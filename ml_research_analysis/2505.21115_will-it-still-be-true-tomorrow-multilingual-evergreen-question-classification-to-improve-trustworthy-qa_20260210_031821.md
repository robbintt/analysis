---
ver: rpa2
title: Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification
  to Improve Trustworthy QA
arxiv_id: '2505.21115'
source_url: https://arxiv.org/abs/2505.21115
tags:
- evergreen
- questions
- question
- answer
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first multilingual evergreen question
  classification task, where questions are labeled as either evergreen (answers remain
  stable over time) or mutable (answers change). To support this, a new dataset called
  EverGreenQA is created, containing 4,757 real-world QA pairs in 7 languages, with
  human-verified labels.
---

# Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA

## Quick Facts
- arXiv ID: 2505.21115
- Source URL: https://arxiv.org/abs/2505.21115
- Reference count: 36
- Introduces first multilingual evergreen question classification task and EverGreenQA dataset (4,757 QA pairs in 7 languages)

## Executive Summary
This work introduces evergreen question classification, distinguishing questions with stable answers from those whose answers change over time. A new multilingual dataset, EverGreenQA, is created and used to train EG-E5, a state-of-the-art classifier that outperforms both LLMs and prior methods. The classifier is applied to improve self-knowledge estimation, filter stale questions from benchmarks, and explain GPT-4o's retrieval behavior, demonstrating practical utility for trustworthy QA systems.

## Method Summary
The authors introduce evergreen question classification and create EverGreenQA, a multilingual dataset with 4,757 QA pairs across 7 languages (Russian, English, French, German, Hebrew, Arabic, Chinese). They train EG-E5, a lightweight multilingual classifier based on E5-Large, and evaluate its performance on classification and downstream tasks. EG-E5 is applied to improve self-knowledge estimation by combining evergreen probability with uncertainty metrics, filter mutable questions from QA benchmarks, and analyze GPT-4o's retrieval behavior. The paper reports strong results across all applications, with EG-E5 achieving F1≈0.91 and outperforming both LLMs and prior methods.

## Key Results
- EG-E5 classifier achieves F1≈0.91 on cross-lingual evergreen classification, outperforming LLMs and prior methods
- Evergreen probability combined with uncertainty metrics improves self-knowledge estimation AUPRC by up to 7.2% (from 0.638 to 0.708)
- Filtering mutable questions from QA benchmarks improves accuracy by 30% relative for retrieval-augmented generation
- Evergreen labels correlate 0.66-0.77 with GPT-4o retrieval decisions, compared to 0.20-0.36 for uncertainty methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adding evergreen probability to uncertainty estimation methods improves LLM self-knowledge detection.
- **Mechanism:** Evergreen probability signals whether static parametric knowledge suffices. If a question is evergreen (stable answer), models with correct training data tend to answer correctly; if mutable, correctness becomes unpredictable without current information. Combining this signal with token entropy, perplexity, or consistency metrics provides complementary information—evergreen features improve AUPRC (identifying when the model knows), while uncertainty captures remaining variance.
- **Core assumption:** The model's parametric knowledge was accurate at training time for evergreen facts; mutable facts depend on information recency.
- **Evidence anchors:**
  - [abstract] "demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation"
  - [section 5.5/Table 4] "evergreen probability is a strong signal... In 16 out of 18 evaluations, the best results are achieved either by the evergreen feature alone or by combining it with an uncertainty estimation method"
  - [corpus] Weak corpus signal; related work on temporality in sketch representation (FMR=0.54) exists but no direct corpus validation of self-knowledge improvement mechanism.
- **Break condition:** If evergreen labels are noisy or model training data contains systematic factual errors, the correlation between evergreen-ness and correctness degrades.

### Mechanism 2
- **Claim:** Filtering mutable questions from QA benchmarks reduces evaluation unfairness caused by outdated gold answers.
- **Mechanism:** Popular QA datasets contain 10-18% mutable questions (Table 6). When evaluated after dataset release, correct model answers may disagree with outdated reference answers, artificially lowering accuracy. Filtering via EG-E5 (F1≈0.91) removes temporally unstable questions, ensuring gold answers remain valid across evaluation timestamps.
- **Core assumption:** Reference answers for evergreen questions remain correct indefinitely; mutable questions have predictable decay patterns.
- **Evidence anchors:**
  - [section 6.1/Table 5] Concrete examples: NQ (2015) "what city is the next winter olympics in" had gold "Beijing," but 2025 answer is "Milan"
  - [section 6.1/Table 6] Mutable questions average ~10% across datasets; model accuracy on mutable questions is consistently lower than evergreen (up to 40% relative gap)
  - [corpus] Corpus validation limited; no prior work directly addresses QA dataset temporal filtering at this scale.
- **Break condition:** If mutable questions contain domain knowledge essential for task coverage (e.g., current events reasoning), filtering may reduce benchmark validity for real-world deployment.

### Mechanism 3
- **Claim:** Question evergreen-ness is the strongest predictor of GPT-4o's retrieval decisions among tested signals.
- **Mechanism:** GPT-4o autonomously decides when to retrieve. Evergreen labels (ground truth or EG-E5 predictions) correlate at 0.66-0.77 with retrieval invocation, while uncertainty metrics (perplexity, entropy) correlate only 0.20-0.36. This suggests GPT-4o either internally models temporality or uses a retrieval policy highly sensitive to temporal markers (e.g., "current," "latest," superlatives).
- **Core assumption:** Retrieval behavior reflects an underlying policy rather than random or purely semantic triggers.
- **Evidence anchors:**
  - [section 7/Table 7] "EverGreen" ground truth correlation = 0.77; EG-E5 = 0.66; all uncertainty methods ≤0.36
  - [abstract] "explaining GPT-4o's retrieval behavior"
  - [corpus] No corpus papers directly validate retrieval-temporality linkage; this appears to be novel observational evidence.
- **Break condition:** If GPT-4o's retrieval policy changes (model update), or if retrieval is triggered by factors not captured (e.g., user intent signals, query complexity), correlation will weaken.

## Foundational Learning

- **Concept: Temporal Knowledge vs. Parametric Knowledge**
  - **Why needed here:** The paper's core distinction—evergreen facts can be stored in static model weights; mutable facts require external retrieval or knowledge editing. Without this, the rationale for classification and downstream applications is unclear.
  - **Quick check question:** "Who is the current US President?" vs. "Who was the first US President?"—which requires retrieval today?

- **Concept: Self-Knowledge Estimation in LLMs**
  - **Why needed here:** Section 5 frames evergreen classification as a feature for improving self-knowledge. Understanding uncertainty quantification (entropy, perplexity, lexical consistency) is prerequisite to seeing why evergreen adds value.
  - **Quick check question:** If an LLM outputs high token entropy on a question, does it necessarily mean the model doesn't know the answer?

- **Concept: Evaluation Fairness in Dynamic Environments**
  - **Why needed here:** Section 6 argues QA benchmarks become stale. Understanding why mutable questions cause false negative evaluations (correct answers marked wrong) is essential for interpreting Table 5 and 6.
  - **Quick check question:** A 2020 dataset asks "Who is the UK Prime Minister?" with gold "Boris Johnson." If a 2025 model answers "Keir Starmer," is the model wrong or the dataset stale?

## Architecture Onboarding

- **Component map:**
  EverGreenQA Dataset -> EG-E5 Classifier -> Downstream Integration Points

- **Critical path:**
  1. Dataset quality -> EG-E5 training -> classifier accuracy (F1≈0.91 cross-lingual)
  2. Classifier output -> downstream task feature engineering
  3. For self-knowledge: ensemble uncertainty + evergreen probability -> train classifier (CatBoost/LogisticRegression) on labeled correctness data

- **Design tradeoffs:**
  - **Lightweight classifier (EG-E5) vs. LLM prompting:** EG-E5 achieves F1 0.91 vs. best LLM (LLaMA 3.1 70B) at 0.88, with lower compute. Tradeoff: EG-E5 requires training data; LLMs generalize zero-shot but underperform.
  - **Synthetic augmentation:** 1,449 synthetic evergreen examples added to reduce class imbalance. Tradeoff: may introduce distribution shift if synthetic examples don't match real user query patterns.
  - **Multilingual translation via GPT-4.1:** Human validation found 0-2 minor errors per language. Tradeoff: translation quality depends on GPT-4.1's multilingual capability; low-resource languages may degrade.

- **Failure signatures:**
  1. **Superlative confusion** -> "What is the biggest star?" misclassified (some superlatives are stable facts, others change with new discoveries)
  2. **Biographical data on living people** -> "How many books has Stephen King written?" treated as static but changes
  3. **False negatives predominate** -> Classifier is cautious; tends to over-predict "mutable" (2× more false negatives than false positives per error analysis)
  4. **Recent-year questions** -> 2023-2024 questions misclassified due to model lacking temporal awareness

- **First 3 experiments:**
  1. **Reproduce EG-E5 training:** Download EverGreenQA, train E5-Large on provided split (10 epochs, lr=4.6e-5), validate F1≈0.91 on test set. Check per-language variance.
  2. **Self-knowledge ablation:** On LLaMA 3.1-8B, run uncertainty estimators (MeanTokenEntropy, SAR, LexicalSimilarity) on SQuAD/NQ subsets with and without evergreen probability feature. Reproduce Table 4 AUROC/AUPRC gains.
  3. **Dataset filtering validation:** Apply EG-E5 to filter mutable questions from HotpotQA. Compare zero-shot accuracy before/after filtering. Check if "RAG gain on mutable" (Table 6, up to 30%) is reproducible with your retrieval setup.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can evergreen classification be effectively leveraged to improve active learning pipelines or search result reranking?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that they "do not explore its potential in tasks such as active learning, answer calibration, or search reranking."
- **Why unresolved:** The paper focused on static tasks like self-knowledge estimation and dataset filtering, leaving dynamic retrieval and learning optimization untested.
- **What evidence would resolve it:** Experiments integrating the EG-E5 classifier into an active learning framework or a RAG reranking module, measuring improvements in efficiency or retrieval precision.

### Open Question 2
- **Question:** Can integrating external entity metadata (e.g., alive/deceased status) resolve the misclassification of biographical questions and superlatives?
- **Basis in paper:** [inferred] The error analysis (Table 8) highlights that the classifier struggles with "biographical/life data on alive people" and superlatives (e.g., "biggest star") because it lacks external context to determine if a fact is stable.
- **Why unresolved:** The current lightweight classifier relies solely on text embeddings without access to real-time or structured knowledge graphs to verify entity status.
- **What evidence would resolve it:** A study comparing the current EG-E5 model against a version augmented with Wikidata lookups for entities, specifically evaluating performance on the "False Positive" error categories listed in the paper.

### Open Question 3
- **Question:** How robust is multilingual evergreen classification when transferred to low-resource languages outside the current training distribution?
- **Basis in paper:** [explicit] The Limitations section notes that while seven languages are covered, "performance in truly low-resource settings remains unexplored" because the dataset does not span all major language families.
- **Why unresolved:** It is unclear if the linguistic features learned by EG-E5 for Latin and select non-Latin scripts transfer effectively to languages with distinct morphological structures or scarce training data.
- **What evidence would resolve it:** Benchmarking the zero-shot performance of the multilingual EG-E5 model on a newly translated test set of low-resource languages (e.g., from African or Austronesian families).

## Limitations

- **Coverage limits:** 7 languages (Russian, English, French, German, Hebrew, Arabic, Chinese) exclude low-resource languages and many major language families
- **Error patterns:** Classifier struggles with superlatives and biographical data on living people, producing more false negatives than false positives
- **Temporal assumptions:** Relies on training data accuracy at time of model training; systematic errors could degrade evergreen-correctness correlation

## Confidence

- **High confidence:** EG-E5 classifier performance (F1≈0.91 cross-lingual), quantitative improvements in self-knowledge estimation when combining evergreen features with uncertainty metrics, concrete evidence of QA dataset staleness (10-18% mutable questions), GPT-4o retrieval correlation with evergreen labels (0.66-0.77 vs. ≤0.36 for uncertainty methods)
- **Medium confidence:** Generalization of error patterns (superlatives, biographical data) across languages, synthetic augmentation's impact on class balance without introducing bias, whether the observed GPT-4o retrieval correlation reflects deliberate policy or incidental correlation
- **Low confidence:** Claims about real-world deployment utility beyond controlled experiments, long-term stability of evergreen labels as global knowledge continues to evolve, whether filtering mutable questions from benchmarks maintains task validity for dynamic reasoning

## Next Checks

1. **Temporal drift validation:** Re-evaluate EG-E5's F1 score on 2023-2024 question subsets to quantify performance decay on recent-year questions, and test whether retraining with temporally extended data improves robustness.

2. **Low-resource language robustness:** Apply EG-E5 to classify questions in two additional low-resource languages (e.g., Swahili, Hindi) using machine translation, then measure classification F1 and error patterns to assess multilingual generalization limits.

3. **Downstream task ablation:** On a held-out QA benchmark (e.g., NaturalQuestions), compare zero-shot model accuracy with and without filtering mutable questions, and measure whether the "RAG gain on mutable" effect (up to 30% relative improvement) replicates with alternative retrieval systems.