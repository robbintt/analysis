---
ver: rpa2
title: 'SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large
  Language Models'
arxiv_id: '2510.13836'
source_url: https://arxiv.org/abs/2510.13836
tags:
- methods
- similarity
- generations
- language
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty quantification
  (UQ) for large language models (LLMs) by developing methods to estimate confidence
  in generated outputs. The core contribution is a similarity-based aggregation framework
  that leverages the consistency among multiple sampled generations to infer confidence,
  avoiding reliance on model internals.
---

# SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models

## Quick Facts
- **arXiv ID:** 2510.13836
- **Source URL:** https://arxiv.org/abs/2510.13836
- **Reference count:** 40
- **Primary result:** Black-box confidence estimation for LLM outputs using similarity aggregation outperforms baselines in calibration (ACE) across 9 datasets spanning QA, summarization, and text-to-SQL tasks.

## Executive Summary
This paper addresses uncertainty quantification for large language models by developing a similarity-based aggregation framework that estimates confidence in generated outputs without requiring model internals. The core insight is that consistency among multiple sampled generations serves as a proxy for correctness - correct responses tend to cluster together while incorrect ones are more variable. The framework includes Bayesian aggregation treating similarities as probabilistic evidence and classification-based aggregation learning confidence as a function of similarity features. Experiments demonstrate superior calibration performance compared to baselines while maintaining strong selection accuracy.

## Method Summary
The method generates 5 samples per query across 6 temperature settings (0.25-1.5), computes pairwise similarities using metrics like Jaccard and ROUGE, then aggregates these similarities into confidence scores via three approaches: simple arithmetic mean, Bayesian aggregation with Beta distributions, and supervised random forest classification. For classification, the model is trained on 50% of the data and evaluated on the remainder, with 5 repeated runs. Only samples from lower temperatures (0.25-0.75) are used for evaluation. Correctness thresholds are task-specific (e.g., Rouge-L≥0.5 for QA, execution match for SQL).

## Key Results
- Supervised classification-based aggregation (clf-pairs) achieves ACE of 0.041 ± 0.012 on CoQA, significantly outperforming the baseline arithmetic mean (0.272 ± 0.019)
- Jaccard and ROUGE similarity metrics effectively capture variation in structured SQL outputs, not just natural language
- Random forest classifiers consistently outperform Bayesian aggregation and simple averaging across all metrics (ACE, ATS, AUROC)
- The approach maintains strong performance across diverse tasks including question answering, summarization, and text-to-SQL

## Why This Works (Mechanism)

### Mechanism 1: Consistency Hypothesis
When LLMs sample multiple responses, correct answers cluster together (high pairwise similarity) while incorrect ones are more variable and dissimilar from the consensus. This consistency provides a black-box signal for confidence without requiring model internals. The core assumption is that correct generations are statistically more similar to other generations than incorrect ones.

### Mechanism 2: Similarity Aggregation
Individual pairwise similarities are aggregated into a single confidence score for each generation. Rather than clustering generations, the method computes a vector of similarities for each sample and applies an aggregation function (arithmetic mean, Bayesian update, or learned classifier) to map this to confidence.

### Mechanism 3: Supervised Learning
Confidence estimation is treated as binary classification using similarity features. A random forest classifier learns the mapping from similarity vectors to correctness labels, achieving better calibration than unsupervised aggregation by leveraging labeled training data.

## Foundational Learning

- **Concept: Consistency-based UQ** - Why needed: The entire framework rests on the assumption that response consistency signals correctness. Quick check: If 9 out of 10 responses are identical while 1 differs dramatically, what does the consistency hypothesis predict about that outlier?

- **Concept: Calibration metrics (ACE)** - Why needed: The paper optimizes for Adaptive Calibration Error, not just accuracy. Understanding calibration is essential to interpret results. Quick check: If a model assigns 0.8 confidence to 100 predictions and 80 are correct, is it well-calibrated?

- **Concept: Similarity metrics for structured outputs** - Why needed: The paper shows Jaccard and ROUGE work even for SQL, which is non-obvious. Understanding why token-overlap metrics capture meaningful variation is key. Quick check: Would Jaccard capture semantic equivalence between "SELECT name FROM users WHERE age > 30" and "SELECT u.name FROM users u WHERE u.age > 30"?

## Architecture Onboarding

- **Component map:** Input Query → [Sampler: temp sweep 0.25-1.5, 5 samples/temp] → [Similarity Computer: Jaccard/ROUGE/SBERT] → [Aggregator: Bayesian/Classifier/Arithmetic mean] → Confidence Score ∈ [0,1]

- **Critical path:** Temperature sampling strategy (higher temps provide diversity), similarity metric choice (Jaccard default, but ROUGE-L and SBERT perform better on some datasets), classifier training (requires 50% train/test split, 5 runs for variability)

- **Design tradeoffs:** Black-box vs. white-box (clf-pairs is purely black-box; clf-pairs+gen adds generative scores), unsupervised vs. supervised (arith-agg requires no training data; clf-pairs needs labeled samples but achieves lower ACE), complexity vs. calibration (Bayesian is theoretically principled but underperforms due to independence assumption violations)

- **Failure signatures:** High ACE with low ATS (calibrated but poor selection), Bayesian aggregation with high ACE (independence assumption likely violated), Random forest overfitting (check train vs. test ACE gap), SQL-specific metrics underperforming (Makiyama metric shows ACE of 0.652 vs. 0.093 for Jaccard with clf-rf)

- **First 3 experiments:** 1) Implement arith-agg with Jaccard on CoQA to validate sampling and similarity computation (target ACE ≈ 0.27). 2) Compare clf-pairs vs. clf-mean+gen vs. clf-pairs+gen to quantify feature value. 3) Train classifier on CoQA, evaluate on Spider to probe cross-domain performance.

## Open Questions the Paper Calls Out

- **Cross-domain performance:** How does SIMBA UQ perform under distribution shift compared to in-domain settings? The paper notes this involves challenges like distribution shift and domain adaptation that warrant separate study.

- **Bayesian independence assumption:** Can relaxing the conditional independence assumption in Bayesian aggregation improve its performance on calibration metrics? The authors hypothesize this assumption may not hold in practice.

- **Theoretical limits:** What are the theoretical limits of the consistency hypothesis in distinguishing correct from incorrect responses? The paper calls for further studies to understand fundamental limitations of consistency-based UQ.

## Limitations

- The consistency hypothesis lacks theoretical guarantees and may fail when incorrect responses cluster or when correct responses are inherently diverse.
- Bayesian aggregation's conditional independence assumption is explicitly violated in practice, leading to suboptimal calibration performance.
- Supervised methods require labeled training data with unambiguous correctness labels, which is challenging for subjective tasks.

## Confidence

- **High Confidence:** Experimental methodology is well-specified with clear metrics and reproducible sampling parameters; superiority of supervised classification over simple averaging is robustly demonstrated.
- **Medium Confidence:** Effectiveness of similarity metrics for structured outputs is demonstrated empirically but lacks theoretical explanation.
- **Low Confidence:** Generalizability across domains and conditions under which consistency hypothesis breaks down are not empirically validated.

## Next Checks

1. **Cross-domain transfer validation:** Train the random forest classifier on CoQA and evaluate on Spider text-to-SQL to measure degradation in ACE and identify feature distribution shifts.

2. **Consistency hypothesis stress test:** Generate responses at higher temperatures (1.25-1.5) and analyze cases where correct responses are diverse or incorrect responses cluster to measure correlation between pairwise similarity and correctness.

3. **SQL-specific metric comparison:** Implement the Makiyama similarity metric for SQL and compare its calibration performance against Jaccard and ROUGE-L to validate whether task-specific metrics provide meaningful improvements.