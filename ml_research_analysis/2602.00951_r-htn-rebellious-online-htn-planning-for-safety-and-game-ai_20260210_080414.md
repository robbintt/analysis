---
ver: rpa2
title: 'R-HTN: Rebellious Online HTN Planning for Safety and Game AI'
arxiv_id: '2602.00951'
source_url: https://arxiv.org/abs/2602.00951
tags:
- agent
- agents
- planning
- task
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces rebellious online Hierarchical Task Network
  (HTN) planning agents that operate under directives and can disobey user commands
  when necessary. The R-HTN algorithm enables two types of agents: Nonadaptive agents
  stop when directives are violated, while Adaptive agents replan to avoid violations
  while still attempting to achieve user goals.'
---

# R-HTN: Rebellious Online HTN Planning for Safety and Game AI

## Quick Facts
- arXiv ID: 2602.00951
- Source URL: https://arxiv.org/abs/2602.00951
- Authors: Hector Munoz-Avila; David W. Aha; Paola Rizzo
- Reference count: 6
- Key outcome: Adaptive R-HTN agents achieve most goals (4-4.5/5) with no directive violations, while Compliant agents violate directives and die frequently (>50% episodes)

## Executive Summary
This paper introduces R-HTN, a rebellious online Hierarchical Task Network (HTN) planning algorithm that enables agents to operate under directives while having the capability to disobey user commands when necessary for safety or goal achievement. The approach distinguishes between Nonadaptive agents that stop when directives are violated and Adaptive agents that replan to avoid violations while still pursuing user goals. The system was evaluated in two domains: O-RESCHU for safety-critical navigation and MONSTER for personality-driven behavior, demonstrating that Adaptive agents achieve superior performance by balancing directive compliance with goal completion.

## Method Summary
The R-HTN algorithm extends traditional HTN planning by incorporating directive monitoring and adaptive replanning capabilities. When a directive violation is detected, Nonadaptive agents halt execution while Adaptive agents trigger replanning to find alternative paths that avoid the violation. The system evaluates agent performance across two metrics: goal achievement and directive compliance. Agents can be configured as Compliant (ignoring directives), Nonadaptive (stopping on violations), or Adaptive (replanning to avoid violations). The evaluation framework measures goal completion rates, directive violation counts, and penalty points incurred during execution.

## Key Results
- Adaptive agents achieve 4-4.5 out of 5 goals in O-RESCHU while incurring no directive violations
- Compliant agents frequently violate directives and achieve only 2-3 goals in O-RESCHU
- Adaptive agents collect the most gold in MONSTER without dying, while Compliant agents die in over 50% of episodes
- Adaptive agents incur approximately half the penalty points of Compliant agents

## Why This Works (Mechanism)
The R-HTN algorithm works by continuously monitoring directive compliance during plan execution and dynamically adjusting behavior when violations are detected. The key mechanism is the replanning capability of Adaptive agents, which allows them to find alternative action sequences that satisfy both user goals and directive constraints. This approach bridges the gap between rigid compliance and complete disobedience by enabling intelligent disobedience when following directives would prevent goal achievement or cause harm.

## Foundational Learning
- HTN Planning: Hierarchical decomposition of complex tasks into primitive actions; needed for structured goal achievement in complex domains
- Directive Monitoring: Real-time checking of agent behavior against specified constraints; needed to detect when compliance must be sacrificed
- Adaptive Replanning: Dynamic generation of alternative plans when current path is blocked; needed to maintain progress toward goals while respecting constraints
- Online Planning: Execution and planning occurring simultaneously; needed for responsive behavior in dynamic environments
- Goal-Directive Trade-off: Balancing competing objectives of goal achievement and constraint satisfaction; needed for realistic agent behavior
- Performance Metrics: Quantitative measures of success including goal completion and violation counts; needed to evaluate different agent configurations

## Architecture Onboarding

Component Map: User Commands -> R-HTN Planner -> Action Executor -> Environment -> Monitor -> Violation Handler -> Planner

Critical Path: Plan Generation → Execution → Monitoring → Violation Detection → Replanning (if Adaptive) → Continued Execution

Design Tradeoffs: Compliant vs. Adaptive vs. Nonadaptive agent types; real-time replanning overhead vs. goal achievement; directive strictness vs. flexibility

Failure Signatures: Compliant agents die frequently due to directive violations; Nonadaptive agents fail to achieve goals by stopping early; all types may struggle with scalability in complex domains

First Experiments:
1. Run O-RESCHU domain with all three agent types to verify goal achievement and violation patterns
2. Implement MONSTER domain to test personality-driven behavior and survival rates
3. Vary directive strictness parameters to observe threshold effects on agent behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation limited to two specific domains with relatively small state spaces
- Performance in domains with stochastic dynamics or partial observability remains untested
- Computational overhead analysis and runtime comparisons with traditional HTN planning are absent
- Scalability to larger, more complex environments has not been demonstrated

## Confidence
R-HTN algorithm effectiveness (High): Strong experimental support across both domains with clear performance differences between agent types
Scalability and generalizability (Medium): Limited testing scope raises questions about performance in more complex environments
Computational efficiency (Low): No detailed analysis of runtime overhead or resource usage compared to baseline approaches

## Next Checks
1. Test R-HTN algorithm in a larger domain with significantly more states and actions to evaluate scalability and identify performance bottlenecks
2. Implement approach in a stochastic environment with probabilistic action outcomes to assess robustness to uncertainty
3. Conduct ablation studies varying directive violation thresholds to understand their impact on goal achievement rates and agent behavior