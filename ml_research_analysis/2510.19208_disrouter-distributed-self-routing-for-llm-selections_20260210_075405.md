---
ver: rpa2
title: 'DiSRouter: Distributed Self-Routing for LLM Selections'
arxiv_id: '2510.19208'
source_url: https://arxiv.org/abs/2510.19208
tags:
- cost
- routing
- answer
- performance
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiSRouter introduces a distributed self-routing framework that
  replaces centralized routing with a network of LLM agents, each using self-awareness
  to decide whether to answer or route queries. This approach enhances flexibility
  and scalability while enabling scenario adaptability (performance-first to cost-first)
  without retraining.
---

# DiSRouter: Distributed Self-Routing for LLM Selections

## Quick Facts
- **arXiv ID:** 2510.19208
- **Source URL:** https://arxiv.org/abs/2510.19208
- **Reference count:** 23
- **Primary result:** Distributed self-routing achieves up to 74.29% of oracle utility, outperforming centralized baselines across in-domain and out-of-domain tasks.

## Executive Summary
DiSRouter introduces a distributed self-routing framework that replaces centralized routing with a network of LLM agents, each using self-awareness to decide whether to answer or route queries. This approach enhances flexibility and scalability while enabling scenario adaptability (performance-first to cost-first) without retraining. Self-Awareness Training, comprising supervised fine-tuning and reinforcement learning with localized rewards, improves agents' ability to accurately assess their own capabilities. Extensive experiments show DiSRouter achieves up to 74.29% of oracle utility, outperforming centralized baselines across in-domain and out-of-domain tasks. The system demonstrates strong generalization, modularity, and dynamic adjustment of routing strategies based on user-defined preferences, validating that intrinsic self-awareness is more effective than external assessment for efficient multi-agent routing.

## Method Summary
DiSRouter employs a two-stage Self-Awareness Training process: supervised fine-tuning (SFT) to align model responses with capability assessment, followed by reinforcement learning (RL) with localized rewards to optimize routing policies. The system uses a cascade of LLM agents (0.5B→14B) where each agent either answers or routes to the next larger model based on self-assessed capability. Training involves generating SFT data by sampling responses to determine correctness frequency, then applying Reinforce++ in Verl with scenario-conditioned rewards. The α parameter controls the trade-off between performance and cost, allowing dynamic adjustment without retraining.

## Key Results
- DiSRouter achieves 0.80 accuracy in self-assessment, outperforming BERT-based external router (0.56) and 8B LLM-based router (0.71)
- System achieves up to 74.29% of oracle utility across various scenarios and domains
- Maintains utility (0.60) superior to baselines without retraining when reducing agent pool size
- Successfully shifts query distribution toward smaller models (60% on 0.5B/1.5B) under cost-first scenario

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intrinsic self-assessment yields higher routing accuracy than external proxy models
- **Mechanism:** By integrating routing decisions into the model's generation process, the model leverages internal latent representations to detect knowledge boundaries
- **Core assumption:** Internal probability of generating correct answers correlates with explicit rejection behavior
- **Evidence anchors:** Table 5 shows 0.80 accuracy vs. 0.56 (BERT) and 0.71 (8B LLM router)
- **Break condition:** Poor calibration where model confidently generates wrong answers without triggering rejection

### Mechanism 2
- **Claim:** Localized reward function with preference factor α aligns agent policies with global utility without communication
- **Mechanism:** Reward function reward(x) = (1-α)^γ for rejection sets dynamic capability threshold
- **Core assumption:** Local optimizations result in globally efficient routing path
- **Evidence anchors:** Equation 10 derivation and Figure 4 showing query distribution shifts
- **Break condition:** Complex coordination requirements beyond simple cascading utility

### Mechanism 3
- **Claim:** Decoupling agent training enables plug-and-play modularity
- **Mechanism:** Each agent learns policy π_i to maximize local utility U_i, allowing addition/removal without retraining
- **Core assumption:** Routing path remains valid despite topology changes
- **Evidence anchors:** Reduced 3-agent system experiment maintains utility (0.60) without retraining
- **Break condition:** New agent with overlapping capability but higher cost routes unnecessarily

## Foundational Learning

- **Concept: Reinforcement Learning Policy Optimization (PPO/Reinforce++)**
  - **Why needed here:** Second stage uses RL to optimize self-awareness policy
  - **Quick check question:** How does the localized reward (1-α)^γ change the probability of selecting "I don't know" token?

- **Concept: Hallucination and Uncertainty Quantification**
  - **Why needed here:** Core failure mode is model failing to reject unanswerable queries
  - **Quick check question:** Does SFT teach new knowledge or map existing uncertainty states to "I don't know"?

- **Concept: Cascade Systems**
  - **Why needed here:** DiSRouter instantiated as cascade with latency vs. accuracy tradeoffs
  - **Quick check question:** What guarantees query won't loop indefinitely in distributed network?

## Architecture Onboarding

- **Component map:** Query → Agent 1 → (Answer/Forward) → Agent 2 → (Answer/Forward) → ... → Agent N → Fallback
- **Critical path:** 1) Input: Query + Scenario Preference (α) 2) Agent generation 3) Self-check for "I don't know" 4) Forward if rejected 5) Repeat until answer or fallback
- **Design tradeoffs:** Latency vs. Cost (sequential latency vs. single forward pass); Training Cost vs. Maintenance (high upfront vs. low maintenance)
- **Failure signatures:** Cascading Rejection (all reject, forced largest model answer); Over-Aggressive Small Models (hallucinate instead of reject); Distribution Shift (accuracy plummet under cost-first on difficult dataset)
- **First 3 experiments:** 1) Calibration Validation: Test binary classification accuracy of single aligned agent 2) Scenario Sensitivity: Run full system with α=0.2 vs. α=0.8 3) Modularity Stress Test: Remove 3B model, re-evaluate utility

## Open Questions the Paper Calls Out
- Can framework scale to complex network topologies (tree, mesh) beyond cascade structure?
- How to enhance self-awareness training for smaller models (0.5B-1.5B)?
- Does incorporating system-level information about downstream agents improve routing efficiency?

## Limitations
- Single controlled comparison lacks ablation studies isolating "intrinsic vs. external" mechanism contribution
- SFT data generation process requires unspecified sampling count N
- Cascade structure's latency penalty acknowledged but not quantified
- System behavior under severe distribution shifts or overlapping capability agents untested

## Confidence
- **High confidence:** Distributed architecture and modular training approach are technically sound
- **Medium confidence:** Self-Awareness Training methodology improves calibration accuracy
- **Medium confidence:** Scenario adaptability through α parameter works as described

## Next Checks
1. Replicate binary classification accuracy comparison (Table 5) between DiSRouter's self-assessment and external LLM router using identical base models
2. Systematically vary CoT sampling count N during SFT data construction (N ∈ {4, 8, 16}) and measure resulting calibration accuracy
3. Introduce agent with capabilities overlapping 3B model but higher cost, measure routing efficiency and accuracy degradation