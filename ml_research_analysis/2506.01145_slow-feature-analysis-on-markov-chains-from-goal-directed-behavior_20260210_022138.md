---
ver: rpa2
title: Slow Feature Analysis on Markov Chains from Goal-Directed Behavior
arxiv_id: '2506.01145'
source_url: https://arxiv.org/abs/2506.01145
tags:
- behavior
- features
- correction
- figure
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the effects of goal-directed behavior on
  slow feature analysis (SFA) for representation learning in reinforcement learning
  settings. The authors formulate SFA on ergodic Markov chains and derive optimal
  solutions as generalized eigenvectors, connecting to spectral graph theory.
---

# Slow Feature Analysis on Markov Chains from Goal-Directed Behavior

## Quick Facts
- arXiv ID: 2506.01145
- Source URL: https://arxiv.org/abs/2506.01145
- Authors: Merlin Schüler; Eddie Seabrook; Laurenz Wiskott
- Reference count: 11
- Primary result: Goal-directed behavior causes scaling distortion in slow features, harming value function approximation; scale correction resolves this.

## Executive Summary
This work investigates how goal-directed behavior affects slow feature analysis (SFA) for representation learning in reinforcement learning. The authors formulate SFA on ergodic Markov chains and derive optimal solutions as generalized eigenvectors. They find that goal-directed behavior induces scaling distortions in slow features due to state occupancy differences, which can impair value function approximation. Three correction mechanisms are proposed and evaluated: behavior modification, learning rate adaptation, and scale correction. Experiments show that scale correction applied to Boltzmann behavior generally yields the best results for approximating optimal value functions.

## Method Summary
The paper formulates SFA on ergodic Markov chains by constructing matrices M (transition weights) and D (stationary distribution) from the induced policy. The optimal slow features are derived as solutions to a generalized eigenvalue problem (D-M)Y = DYΛ. The method involves computing the stationary distribution μ, building M and D matrices, solving for eigenvectors corresponding to smallest eigenvalues, and optionally applying correction mechanisms. Value function approximation is evaluated by linear regression of the features against the optimal value function V*, with performance measured by mean squared error.

## Key Results
- Goal-directed behavior causes significant scaling distortion in slow features proportional to the inverse square root of state occupancy
- Scale correction (multiplying features by D^(1/2)) effectively recovers approximation performance
- Goal-averse features often outperform goal-directed ones for value function approximation without correction
- Learning rate adaptation correction showed limited benefit compared to scale correction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Goal-directed behavior induces a scaling distortion in slow features proportional to the inverse square root of state occupancy, impairing value function approximation.
- **Mechanism:** SFA enforces a unit variance constraint (y^T Dy = 1) weighted by the stationary distribution μ. In goal-directed settings, states near the reward have high occupancy (μ). To satisfy the constraint, feature amplitudes (y) must shrink in high-occupancy regions and expand in low-occupancy regions. This results in "flat" features exactly where the value function peaks (the goal), increasing regression error.
- **Core assumption:** The environment is spatially connected and the agent's behavior creates a non-uniform stationary distribution (ergodic Markov chain).
- **Evidence anchors:**
  - [abstract] Mentions goal-directed behavior leads to significant differences in state occupancy, affecting value-function approximation.
  - [section 5] Derives the bound y_i ≤ ±1/√μ_i (Eq. 24) and visualizes flattened features near the goal (Fig. 2).
  - [corpus] "Goals and the Structure of Experience" discusses how purposeful behavior structures experience, supporting the premise that behavior shapes representations.
- **Break condition:** If the behavior policy is uniform (μ is constant), the scaling effect disappears, and features resemble standard Laplacian eigenmaps.

### Mechanism 2
- **Claim:** Rescaling features by the square root of the stationary distribution (√μ) corrects the amplitude distortion and recovers approximation performance.
- **Mechanism:** The scaling effect suggests features are "squeezed" by √μ. By multiplying the feature vectors by D^(1/2) (where D contains μ on the diagonal), the optimization feasible region shifts from Y^T D Y = I to Y^T Y = I. This restores uniform scaling across states, better matching the magnitude profile of optimal value functions.
- **Core assumption:** An accurate estimate of the stationary distribution μ is available or calculable from the transition statistics.
- **Evidence anchors:**
  - [section 6] Proposes scale correction D^(1/2)Y and shows it counteracts extreme scaling (Fig. 4).
  - [section 7.1] Shows scale correction is "exclusively beneficial" for goal-directed features in the linear environment (Fig. 11).
  - [corpus] No direct corpus evidence for this specific correction method.
- **Break condition:** If the distribution μ is estimated poorly (e.g., insufficient samples during exploration), the correction may introduce noise or over-correction.

### Mechanism 3
- **Claim:** Goal-averse behavior naturally aligns feature scaling with value functions, outperforming goal-directed behavior without correction.
- **Mechanism:** Goal-averse policies actively avoid the reward, resulting in low state occupancy (μ) near the goal. The SFA variance constraint then forces feature amplitude to increase near the goal (low μ → high y). This "divergent" scaling coincidentally matches the peak of the value function V*, allowing a linear combination of features to fit the target more easily than the "flat" features from goal-directed policies.
- **Core assumption:** The value function has high magnitude in states that are least visited by the behavior policy.
- **Evidence anchors:**
  - [section 7.1] Notes "goal-averse behavior leads to features that perform better in value function approximation" (Fig. 8, 14).
  - [section 5] Links feature amplitude bounds inversely to state occupancy.
  - [corpus] "Real-World Robot Control..." discusses the balance of goal-directed vs. exploratory actions, relevant to the behavioral spectrum.
- **Break condition:** If the reward structure is dense or uniform rather than sparse/goal-based, the correlation between low-occupancy and high-value breaks down.

## Foundational Learning

- **Concept: Ergodic Markov Chains & Stationary Distribution**
  - **Why needed here:** The paper formulates SFA specifically on ergodic chains to ensure a unique stationary distribution μ exists. μ is the mathematical driver of the scaling distortion.
  - **Quick check question:** Can you explain why a non-ergodic chain (with absorbing states) would break the SFA derivation used here?

- **Concept: Generalized Eigenvalue Problem**
  - **Why needed here:** The optimal slow features are derived as the solution to (D - M)Y = DYΛ. Understanding this linear algebra connection is required to see why SFA produces these specific features and how it relates to graph Laplacians.
  - **Quick check question:** In the equation (D - M)Y = DYΛ, what do the smallest eigenvalues λ represent in terms of the "slowness" objective?

- **Concept: Value Function Approximation (V*)**
  - **Why needed here:** This is the downstream task used to evaluate representation quality. The paper measures success based on the Mean Squared Error (MSE) between the linear projection of features and the optimal value function V*.
  - **Quick check question:** Why is "off-policy" learning relevant here (approximating V* using features learned from a different exploratory behavior)?

## Architecture Onboarding

- **Component map:** Environment (MDP) -> Policy Module (generates transitions) -> SFA Solver (constructs M, D matrices and solves eigenproblem) -> Correction Layer (applies scale correction or LRA) -> Evaluator (linear regression to fit V*)

- **Critical path:** The construction of the M matrix from the induced Markov chain. If the behavior policy does not generate sufficient coverage (violating ergodicity or poor estimation of μ), the eigen-decomposition fails or the scaling bounds become inaccurate.

- **Design tradeoffs:**
  - **Boltzmann vs. ζ-greedy:** Boltzmann provides a softer stationary distribution, mitigating extreme scaling naturally, but requires tuning the temperature parameter β. ζ-greedy is simpler but creates extreme occupancy differences.
  - **LRA vs. Scale Correction:** LRA attempts to fix the objective function but showed limited benefit in experiments. Scale correction is a post-hoc fix that requires estimating μ but proved highly effective.

- **Failure signatures:**
  - **"Flat Features" at Goal:** Features have near-zero amplitude at the reward location (indicates high occupancy/directed behavior without correction).
  - **High MSE with few features:** Suggests the scaling of the basis functions does not match the shape of V* (likely goal-directed features uncorrected).

- **First 3 experiments:**
  1. **Baseline Verification:** Generate a random walk (θ=0.5) on a linear graph. Solve for SFA features. Verify they match standard Fourier-like basis functions (uniform amplitude).
  2. **Distortion Reproduction:** Switch to a goal-directed policy (θ=0.48). Plot the slowest feature. Confirm the amplitude is suppressed at the goal and exaggerated at the opposite end of the chain.
  3. **Correction Validation:** Apply the scale correction (D^(1/2)) to the features from step 2. Run linear regression against V* and confirm MSE drops compared to the uncorrected version.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed scaling effects and correction mechanisms transfer to SFA implementations restricted by fixed function approximator families (e.g., neural networks)?
- **Basis in paper:** [explicit] The authors state, "SFA is typically bound to a fixed family of architectures, and thus can generally not realize optimal features. It is unclear to what extent the features in such a setting exhibit the same effects."
- **Why unresolved:** The analysis relies on deriving theoretically optimal features, which assumes the model has the capacity to represent any function, unlike practical neural networks.
- **What evidence would resolve it:** Empirical evaluations comparing scaling effects in deep SFA or hierarchical networks against the optimal theoretical baseline derived in the paper.

### Open Question 2
- **Question:** Can "over-correcting" features toward goal-averse scaling properties yield superior value function approximation compared to standard uniform scale correction?
- **Basis in paper:** [explicit] The authors suggest, "Instead of correcting goal-directed features to be more uniform, an over-correction toward generally better performing goal-averse features is possible."
- **Why unresolved:** The study focused on normalizing scaling, but results showed goal-averse features often outperformed uniform ones; the potential of biasing features toward this "better performing" state was not quantified.
- **What evidence would resolve it:** Experiments treating the degree of correction as a hyperparameter to identify if an optimal bias exists between uniform and goal-averse scaling.

### Open Question 3
- **Question:** How does the accuracy of stationary distribution estimation in finite-sample settings impact the stability and performance of the proposed scale correction?
- **Basis in paper:** [inferred] The paper analyzes an idealized "infinite-sample" case. The authors list "ease" of application as a limitation, noting that scale correction "requires at least an estimate of the stationary distribution."
- **Why unresolved:** In practical RL, the stationary distribution is unknown and must be estimated from limited data; the sensitivity of the correction to estimation errors was not assessed.
- **What evidence would resolve it:** Analysis of approximation error when the scale correction is applied using a noisy distribution estimate derived from finite trajectory lengths.

## Limitations

- The effectiveness of LRA correction was limited in experiments compared to scale correction
- The analysis assumes accurate estimation of the stationary distribution, which may be challenging in practice
- Results are primarily demonstrated on simple linear and lattice graph environments

## Confidence

- **High:** The mathematical derivation of scaling effects (Mechanism 1) and the effectiveness of scale correction (Mechanism 2) are well-established through both theory and experiments.
- **Medium:** The performance advantage of goal-averse features (Mechanism 3) is demonstrated but may depend heavily on the specific sparse reward structure used in experiments.
- **Low:** The practical utility of LRA correction is questionable given experimental results showing minimal benefit compared to scale correction.

## Next Checks

1. Test scale correction on environments with non-uniform reward distributions (not just single-goal settings) to verify generalizability beyond the current experimental setup.
2. Implement a robustness analysis for LRA correction by varying the weighting function and temperature parameter β to identify if any parameter regimes show improved performance.
3. Conduct experiments with estimated (vs. exact) stationary distributions to quantify sensitivity to distribution estimation errors in real-world applications.