---
ver: rpa2
title: 'Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement
  Learning'
arxiv_id: '2511.08749'
source_url: https://arxiv.org/abs/2511.08749
tags:
- learning
- query
- arxiv
- queries
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the traditional RL paradigm of optimizing
  only for control by proposing architectures designed explicitly for answering diverse
  queries about the environment. The key insight is that inference accuracy (e.g.,
  reachability prediction) can decouple from control performance - QDIN achieves near-perfect
  reachability IoU (99%) even when returns remain poor (31%), suggesting different
  representations are optimal for knowledge versus control.
---

# Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.08749
- Source URL: https://arxiv.org/abs/2511.08749
- Reference count: 40
- Key outcome: Near-perfect reachability inference (99% IoU) can decouple from poor control performance (31% returns), demonstrating specialized neural modules can achieve superior query answering while maintaining competitive control.

## Executive Summary
This paper challenges the traditional RL paradigm of optimizing only for control by proposing architectures designed explicitly for answering diverse queries about the environment. The key insight is that inference accuracy (e.g., reachability prediction) can decouple from control performance - QDIN achieves near-perfect reachability IoU (99%) even when returns remain poor (31%), suggesting different representations are optimal for knowledge versus control. The proposed Query-Conditioned Deterministic Inference Networks (QDIN) feature specialized neural modules (convolutional for spatial queries, attention for comparisons, sequential for paths) conditioned on query types from the earliest layers. Experiments show QDIN outperforms monolithic and post-hoc methods on inference tasks while maintaining competitive control, and generalizes to composite queries never seen during training. The work establishes a research agenda for designing RL systems as queryable knowledge bases rather than just action selectors.

## Method Summary
QDIN learns a unified network that answers structured queries about deterministic MDPs through query-specific neural modules. The architecture features a shared state encoder (3-layer convolutional hierarchy with skip connections), a query encoder (type embedding + parameter MLP), and cross-attention fusion that conditions state features on query type. Four specialized heads handle different query families: ConvTranspose+skip for spatial reachability masks, LSTM-Pointer for sequential paths, Siamese MLP for relative comparisons, and linear+softmax for policy. Training uses multi-objective optimization with normalized losses balanced via curriculum sampling from simple to complex query types, combining TD loss for control with query-specific losses for inference accuracy.

## Key Results
- QDIN achieves 99% reachability IoU compared to 41% for monolithic baselines, with only 40% parameter increase
- Reachability accuracy and control performance decouple: near-ceiling inference (99% IoU) occurs alongside poor returns (31%)
- Composite query generalization reaches 73% accuracy vs 41% monolithic baseline on never-seen query types
- Specialized heads contribute most significantly to performance: removing them causes -0.18 Reach IoU drop and +5.1 Path MAE increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-conditioned cross-attention enables selective feature extraction based on query type.
- Mechanism: The network encodes both state and query representations, then uses cross-attention (Q=hq, K=hs, V=hs) to let the query "attend" to relevant state features—spatial patterns for reachability, sequential structure for paths, value-relevant features for comparisons.
- Core assumption: Different query types require fundamentally different features from the same state representation.
- Evidence anchors:
  - [Section 4.2] "Cross-attention mechanism enables the query to selectively attend to relevant state features—spatial patterns for reachability, sequential structure for paths, or value-relevant features for comparisons."
  - [Abstract] "specialized neural modules optimized for each inference pattern"
  - [Corpus] Weak direct support; neighbor papers focus on symbolic reasoning (SymDQN) and reachability analysis rather than query-conditioned architectures.
- Break condition: If query types share most features (high feature overlap), the overhead of attention may not justify the cost; a shared representation could suffice.

### Mechanism 2
- Claim: Architectural specialization per query type improves inference accuracy over monolithic processing.
- Mechanism: Each query head uses architecture matched to its computational pattern—ConvTranspose for spatial reachability masks, LSTM-Pointer for sequential paths, Siamese MLP for relative comparisons. This embeds inductive biases directly into the computation path.
- Core assumption: Query types have structurally distinct answer spaces requiring different computational primitives.
- Evidence anchors:
  - [Section 5.3] "Specialized heads contribute most significantly to performance... confirming that different query types benefit from different architectural inductive biases."
  - [Table 2] Removing specialized heads causes -0.18 Reach IoU drop, +5.1 Path MAE increase.
  - [Corpus] SymDQN (2504.02654) supports modular neuro-symbolic design, but does not evaluate query-specific specialization.
- Break condition: If query types can be reformulated into a shared output space (e.g., all as classification), specialization may be unnecessary overhead.

### Mechanism 3
- Claim: Inference accuracy and control performance can decouple—near-perfect world knowledge does not guarantee good policies.
- Mechanism: Query-only training optimizes representations for accurate prediction tasks but not for action selection under value optimization. The representations that encode reachability topology differ from those that encode action-value gradients.
- Core assumption: Knowledge representation and control policy have partially non-overlapping optimal representations.
- Evidence anchors:
  - [Section 5.2, Figure 3] "Reachability prediction can achieve near-ceiling accuracy (99% IoU) even when returns remain poor (31%)."
  - [Abstract] "representations needed for accurate world knowledge differ from those required for optimal control."
  - [Corpus] DHP (2502.01956) shows reachability checks improve hierarchical planning but does not test the decoupling claim directly.
- Break condition: In stochastic environments or under dense reward shaping, the decoupling may weaken as control requires more complete world models.

## Foundational Learning

- Concept: Cross-attention mechanisms (query, key, value)
  - Why needed here: Core fusion mechanism between query embeddings and state representations; enables query-conditioned feature selection.
  - Quick check question: Can you explain why cross-attention (Q from query, K/V from state) differs from self-attention (Q, K, V all from same source)?

- Concept: Multi-objective optimization with gradient balancing
  - Why needed here: Training jointly on multiple query types requires balancing losses; uses normalized losses (Lq/EMA[Lq]) to prevent domination by any single objective.
  - Quick check question: Why might simple weighted sum of losses fail when one loss has much larger magnitude or gradient variance?

- Concept: Deterministic MDPs and reachability sets
  - Why needed here: Formalizes the query types; reachability R_H(s) is well-defined only when transitions are deterministic.
  - Quick check question: In a stochastic MDP, what would need to change about the reachability query definition?

## Architecture Onboarding

- Component map: State Encoder -> Cross-Attention Fusion -> Specialized Heads (Reachability, Path, Comparison, Policy)
- Critical path: State → hierarchical convs → cross-attention with query → specialized head. The skip connection from h1 to reachability head is essential for spatial fidelity.
- Design tradeoffs:
  - Parameter efficiency: ~40% increase over monolithic, but modular execution means only relevant heads run per query.
  - Training complexity: Requires curriculum sampling and gradient normalization; ablation shows consistency loss contributes minimally (-0.03 IoU).
  - Generalization vs specialization: Specialized heads improve accuracy but may reduce transfer; composite query generalization tested at 73% vs 41% monolithic baseline.
- Failure signatures:
  - Reachability mask blurry/low IoU: Check skip connection from h1; may need higher spatial resolution in early conv layers.
  - Path predictions invalid (non-contiguous waypoints): LSTM pointer may be undertrained; increase path query sampling in curriculum.
  - Comparison accuracy near random: Siamese features may not be discriminative; check contrastive training signal.
  - Policy degrades while inference improves: Loss balance skewed toward query losses; reduce αq weights or increase αcontrol.
- First 3 experiments:
  1. Ablate specialized heads (use linear heads for all query types) to reproduce -0.18 Reach IoU, +5.1 Path MAE from Table 2.
  2. Train query-only (no TD loss) and confirm decoupling: expect ~0.99 Reach IoU, ~0.31 Return per Figure 3.
  3. Test zero-shot composite queries (set unions, constrained paths) and compare to monolithic baseline; expect ~73% vs 41% accuracy per Figure 5a.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the QDIN architecture maintain its computational efficiency and accuracy when scaled to high-dimensional continuous spaces?
- Basis in paper: [explicit] The authors state that "application to high-dimensional continuous spaces remains unexplored" despite favorable scaling on grid worlds.
- Why unresolved: Current experiments are restricted to deterministic grid worlds up to 32x32, which may not reflect the sample complexity or approximation errors in continuous domains.
- What evidence would resolve it: Successful deployment of query-conditioned modules in high-dimensional continuous control benchmarks (e.g., robotics simulators) with comparable IoU and latency metrics.

### Open Question 2
- Question: What specific representational differences cause the observed decoupling where agents possess near-perfect world knowledge but fail at control?
- Basis in paper: [explicit] The paper identifies "The Representation Learning Puzzle" regarding why optimal representations for inference differ from those for control.
- Why unresolved: The paper empirically observes the decoupling (e.g., 99% reachability accuracy with 31% returns) but only hypothesizes that control requires "actionable" vs. "complete" information.
- What evidence would resolve it: Mechanistic interpretability studies or probing classifiers that isolate specific features present in inference-trained representations but absent in control-trained representations.

### Open Question 3
- Question: What is the principled taxonomy for mapping complex query types (e.g., temporal logic, causality) to specific neural module architectures?
- Basis in paper: [explicit] The authors note that developing a "principled taxonomy of queries and corresponding architectures is an open challenge."
- Why unresolved: The current work implements four "fundamental" query families, leaving complex types like temporal logic undefined and unarchitected.
- What evidence would resolve it: The derivation of a formal mapping between the computational structure of temporal queries and specific inductive biases (e.g., recurrent memory or attention mechanisms).

## Limitations

- The architectural design choices (depth of LSTM in path heads, exact ConvTranspose configuration, Siamese MLP architecture) are not fully specified, requiring reasonable engineering assumptions
- The claim of near-perfect decoupling (99% reachability accuracy with 31% returns) relies on deterministic environments; performance in stochastic settings remains untested
- The 73% vs 41% composite query generalization gap assumes the monolithic baseline uses only linear heads, which may not represent the strongest possible monolithic approach

## Confidence

- High confidence: The architectural framework (query-conditioned cross-attention with specialized heads) is clearly specified and the inference accuracy improvements over monolithic baselines are well-supported by ablation studies
- Medium confidence: The decoupling claim (knowledge vs control representations) is supported by the experimental data but requires careful interpretation given the deterministic environment assumption
- Medium confidence: The composite query generalization results show promising but potentially fragile performance that depends on the specific baseline comparison

## Next Checks

1. Test the QDIN architecture on stochastic grid worlds where reachability sets become probabilistic, validating whether the core query-conditioned approach generalizes beyond deterministic MDPs
2. Implement a stronger monolithic baseline (e.g., using attention or hierarchical processing) to establish whether the 73% vs 41% composite query performance gap persists against state-of-the-art monolithic approaches
3. Conduct ablation studies on the consistency loss (λ·L_consistency) and curriculum design to quantify their contribution to the reported performance, given the paper notes consistency loss contributes minimally to reachability IoU