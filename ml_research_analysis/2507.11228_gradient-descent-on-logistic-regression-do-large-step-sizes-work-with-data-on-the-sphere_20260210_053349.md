---
ver: rpa2
title: 'Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data
  on the Sphere?'
arxiv_id: '2507.11228'
source_url: https://arxiv.org/abs/2507.11228
tags:
- step
- dataset
- convergence
- have
- logistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies gradient descent (GD) convergence on logistic\
  \ regression with large step sizes when data is restricted to the unit sphere. While\
  \ separable data allows arbitrarily large step sizes, non-separable data requires\
  \ step sizes below the stability threshold 2/\u03BB."
---

# Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?

## Quick Facts
- arXiv ID: 2507.11228
- Source URL: https://arxiv.org/abs/2507.11228
- Authors: Si Yi Meng; Baptiste Goujaud; Antonio Orvieto; Christopher De Sa
- Reference count: 22
- Primary result: Unit-norm data enables global convergence for all γ < 2 in 1D logistic regression, but fails in higher dimensions due to stable cycles

## Executive Summary
This paper investigates when gradient descent (GD) with large step sizes converges globally for logistic regression with data constrained to the unit sphere. While separable data allows arbitrarily large step sizes, non-separable data requires step sizes below 2/λ (where λ is the Hessian's largest eigenvalue at the minimizer). The authors prove that in one dimension with unit-norm data, global convergence is guaranteed for all γ < 2, extending the known γ ≤ 1 bound. However, they construct counterexamples showing that in higher dimensions, global convergence fails for any γ < 2, even with unit-norm data. The counterexample involves "smearing" a 2D cycle onto a high-dimensional sphere.

## Method Summary
The authors study gradient descent on logistic regression with step size η = γ/λ, where γ < 2 and λ = λmax(∇²L(w*)) is computed at the minimizer w*. They analyze both 1D and higher-dimensional settings with unit-norm data (||xi|| = 1). The 1D analysis uses a two-step contraction argument to prove global convergence for all γ < 2, while the higher-dimensional analysis constructs counterexamples using a "schmearing" technique that preserves cycle dynamics from 2D base datasets. The method involves computing power spectra of iterate norms to detect cycles and analyzing Hessian eigenvalue structures.

## Key Results
- Global convergence is guaranteed for all γ < 2 in 1D logistic regression with unit-norm data
- Convergence is oscillatory for γ ∈ (1, 2) in 1D, achieved via two-step contraction
- In higher dimensions, counterexamples exist for any γ < 2, even with unit-norm data
- The counterexamples are constructed by "schmearing" 2D base cycles onto high-dimensional spheres

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For non-separable logistic regression, local stability of GD at the minimizer requires η < 2/λ, but this condition does not guarantee global convergence.
- Mechanism: The threshold η = 2/λ is a bifurcation point for the discrete-time map T(w) = w − η∇L(w). When γ := ηλ < 2, the fixed point w* is locally attracting (linearized system stable). However, the global dynamics can contain attracting cycles because the map is nonlinear.
- Core assumption: Non-separable data with unique finite minimizer w*.
- Evidence anchors:
  - [abstract] "GD can exhibit a cycling behaviour even when the step sizes is still below the stability threshold $2/\lambda$"
  - [section 1, page 2] "Above this threshold, GD will diverge, but below it, convergence is still possible when initialized sufficiently close to the minimizer"
  - [corpus] "Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region" confirms fractal/cycle structures near criticality in related settings
- Break condition: Data is linearly separable (then arbitrarily large step sizes are safe and implicit bias drives directional convergence).

### Mechanism 2
- Claim: In 1D with unit-norm data (|xi| = 1), GD converges globally for all γ < 2 via oscillatory two-step contraction.
- Mechanism: The GD update T(w) = w − (γ/σ'(w*))(σ(w) − σ(w*)) has exactly one fixed point w*. For γ ∈ (1,2), iterates starting right of w* may cross left but Lemma 2 guarantees they cross back right; Lemma 3 bounds the two-step contraction factor by 1 − γ(2−γ)R(w)²/σ'(w*)² < 1. Oscillation is sustained but distance to w* shrinks every two steps.
- Core assumption: d = 1 and all examples satisfy |xi| = 1.
- Evidence anchors:
  - [abstract] "global convergence is guaranteed for all γ < 2... They show convergence is oscillatory for γ ∈ (1, 2)"
  - [section 2, page 5] "wt+2 − w∗ / wt − w∗ ≤ 1 − γ(2 − γ) R(wt)²/σ'(w*)² < 1"
  - [corpus] Corpus evidence on this specific 1D oscillatory contraction result is weak/missing
- Break condition: d > 1 (the ordering of iterates around w* and the two-step argument do not generalize).

### Mechanism 3
- Claim: For any γ < 2, there exist high-dimensional unit-norm datasets where GD converges to a nontrivial cycle.
- Mechanism: Take a 2D base dataset known to yield a stable cycle at given γ. "Schmear" each point xi onto the d-sphere by creating (d−2) pairs [xi; ±si·ej] with si = √(1−||xi||²). The solution w* pads the 2D solution with zeros. The Hessian becomes block-diagonal; for large d, the max eigenvalue is dominated by the 2D block λb, so the cycle dynamics persist.
- Core assumption: A 2D base cycle exists and is stable; d can be chosen large enough so λb ≥ cb/(d−2).
- Evidence anchors:
  - [abstract] "in higher dimensions, they construct counterexamples demonstrating that global convergence fails for any γ < 2, even with unit-norm data"
  - [section 3, page 6–7] "∇²L(·)|w* = blockdiag(∇²Lb(w*b), cb/(d−2)·I)" and "take d large enough so that λb ≥ cb/(d−2)"
  - [corpus] Corpus evidence on this construction technique is weak/missing
- Break condition: Low dimensions where the cb/(d−2) block may exceed λb; the paper notes the dimension may need to be large.

## Foundational Learning

- Concept: **Local vs. global stability in discrete-time maps**
  - Why needed here: The paper's central insight is that satisfying the local stability condition (η < 2/λ) does not prevent global pathologies (cycles).
  - Quick check question: Why can a fixed point be locally stable yet fail to attract all initializations?

- Concept: **