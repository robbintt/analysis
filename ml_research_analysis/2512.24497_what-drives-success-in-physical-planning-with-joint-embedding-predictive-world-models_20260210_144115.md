---
ver: rpa2
title: What Drives Success in Physical Planning with Joint-Embedding Predictive World
  Models?
arxiv_id: '2512.24497'
source_url: https://arxiv.org/abs/2512.24497
tags:
- planning
- success
- rate
- action
- predictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the design choices that make Joint-Embedding
  Predictive World Models (JEPA-WMs) effective for planning in physical environments.
  The authors conduct a comprehensive study on several key components, including planning
  optimizer selection, multistep rollout training, proprioceptive input, encoder type,
  predictor architecture, and model scaling.
---

# What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?

## Quick Facts
- arXiv ID: 2512.24497
- Source URL: https://arxiv.org/abs/2512.24497
- Reference count: 40
- Primary result: Optimal JEPA-WM model with DINOv2 encoder, AdaLN conditioning, multistep rollout loss achieves up to 83.9% success on Metaworld Reach and 48.2% on DROID Place tasks

## Executive Summary
This paper investigates the design choices that make Joint-Embedding Predictive World Models (JEPA-WMs) effective for planning in physical environments. Through systematic ablations, the authors identify key components: using proprioception, multistep rollout training, DINOv2/V3 encoders, and AdaLN conditioning in the predictor architecture. They propose an optimal model that outperforms baselines on both navigation and manipulation tasks, with planning success rates reaching 83.9% on Metaworld Reach and 48.2% on DROID Place. The study reveals important scaling effects where larger models benefit real-world data but not simulated environments.

## Method Summary
The authors train JEPA-WMs using frozen DINOv2/V3 encoders to map observations into compact embeddings, with a trainable predictor learning dynamics in this space. The predictor uses ViT architecture with AdaLN conditioning to incorporate actions at every layer. Training employs multistep rollout losses (typically 2-step for simulated environments, 6-step for DROID) using TBPTT with "Last-gradient only" variant. Planning is performed via CEM or gradient-based optimizers optimizing L2 distance in embedding space. The framework processes video, proprioception, and actions through separate encoders, combining them for trajectory optimization in the latent space.

## Key Results
- AdaLN conditioning in predictor architecture provides superior action incorporation compared to sequence or feature concatenation
- Multistep rollout training significantly improves planning success by exposing predictor to its own errors
- DINOv2/V3 encoders outperform V-JEPA encoders for precise spatial localization tasks
- Model scaling benefits real-world data but degrades performance in simulated environments
- Optimal configuration achieves 83.9% success on Metaworld Reach and 48.2% on DROID Place tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Planning in learned representation space is more efficient than pixel-space planning by filtering irrelevant visual details while preserving task-relevant features.
- Mechanism: JEPA framework trains encoder to map observations into compact embedding space where predictor learns dynamics (z_{t+1} = P(z_t, a_t)). Planning in embedding space avoids noise and computational cost of pixel reconstruction, focusing on abstract state essence.
- Core assumption: Pretrained visual encoder (DINOv2) captures semantically sufficient spatial features without fine-tuning on target environment.
- Evidence anchors:
  - [abstract] "Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space... abstracting irrelevant details yields more efficient planning."
  - [section 4] "We optimize to find an action sequence without theoretical guarantees... closer to trajectory optimization."
  - [corpus] Related work (V-JEPA 2) supports self-supervised video models enable planning by understanding world dynamics.

### Mechanism 2
- Claim: Multistep rollout losses improve planning success by simulating error accumulation during actual planning.
- Mechanism: During planning, predictor recursively consumes its own predictions. Teacher-forcing training causes distribution shift when feeding on own errors. Multistep rollout exposes predictor to its mistakes during training, teaching robustness to error propagation.
- Core assumption: "Last-gradient only" TBPTT variant captures temporal dependencies for stable unrolling.
- Evidence anchors:
  - [section 4] "We observe... that the performance increases when going from pure teacher-forcing models to 2-step rollout loss models."
  - [appendix B] "What matters is to train the predictor to receive as input a mix of encoder outputs and predictor outputs. This makes the predictor more aligned with the planning task."

### Mechanism 3
- Claim: AdaLN is superior for injecting action information compared to sequence or feature concatenation.
- Mechanism: Standard conditioning adds actions at input layer, but signal dilutes through transformer layers. AdaLN modulates normalization statistics (scale/shift) of every transformer block based on action, ensuring control signal influences representation transformation directly at every depth stage.
- Core assumption: Computational overhead of generating modulation parameters is justified by preservation of action signal.
- Evidence anchors:
  - [section 5.2] "AdaLN's per-layer modulation should provide the most consistent action conditioning throughout the predictor depth, which may explain its superior empirical performance."
  - [figure 5a] Shows AdaLN with RoPE achieving highest average performance across tasks.

## Foundational Learning

- **Vision Transformers (ViT) as Spatial Encoders**
  - Why needed here: Paper relies on frozen DINOv2/V3 encoders to extract "local features" and "fine-grained object segmentation." Understanding patch embeddings and spatial tokens is essential for debugging predictor input.
  - Quick check question: Can you explain why a frozen DINOv2 encoder might capture spatial relationships (like cup position) better than video encoder like V-JEPA in this specific planning context?

- **Cross-Entropy Method (CEM) for Trajectory Optimization**
  - Why needed here: Paper identifies CEM L2 as optimal planner. CEM is derivative-free optimization that samples trajectories, evaluates using world model, refines sampling distribution based on best performers.
  - Quick check question: Why would sampling-based method like CEM succeed in "Wall" tasks where gradient-based methods (Adam/GD) fail due to local minima?

- **Proprioception-State Alignment**
  - Why needed here: Paper highlights proprioception as key success driver but notes misalignment between datasets (DROID vs. Robocasa) prevents transfer.
  - Quick check question: If visual encoder is frozen, how does adding learnable proprioception encoder to input tokens help predictor distinguish between visually identical arm positions with different joint angles?

## Architecture Onboarding

- **Component map**: Video/Proprioception/Action -> Frozen Encoder E_vis (DINOv2/V3) + Learnable Encoders E_prop, A -> Predictor P (ViT with AdaLN + RoPE) -> Predicted embeddings z_{t+1...H} -> CEM/L2 Planner -> Action sequence

- **Critical path**: Predictor Unroll is critical. If predictor drifts even slightly in embedding space, L2 planning cost becomes ambiguous, causing CEM optimizer to select sub-optimal trajectories. Multistep training specifically designed to stabilize this path.

- **Design tradeoffs**:
  - Encoder: DINOv2 vs. V-JEPA. Use DINOv2 for tasks requiring precise spatial localization (manipulation), V-JEPA if temporal dynamics are primary feature.
  - Planner: CEM vs. Gradient Descent. Use CEM for tasks with obstacles/complex dynamics (multimodal costs), Gradient Descent only if cost landscape is convex/smooth.
  - Context Window (W): Must be W ≥ 2 to infer velocity. Too large (W > 5) may degrade performance due to fewer unique trajectory samples during training.

- **Failure signatures**:
  - Gradient Planner Stagnation: Adam/GD planner stuck in walls/oscillating - cost landscape has local minima. Switch to CEM.
  - Action Vanishing: Predictor ignores actions (predicts same future regardless of input) - check AdaLN implementation or action ratio.
  - Real-to-Sim Gap: Model trained on DROID fails on Robocasa - likely proprioception misalignment (set α=0 to ignore proprioception during planning).

- **First 3 experiments**:
  1. Overfit Single Step: Train predictor to predict z_{t+1} from z_t for 10 batches on Push-T. Verify loss drops to near zero to confirm pipeline is connected.
  2. Visualize Rollouts: Use visual decoder (trained separately) to decode predictor's output z_{t+1...H}. Visually confirm predicted frames move agent toward goal state plausibly.
  3. Planner Ablation: Set up CEM planner with H=3, N=300. Compare success rates using L1 vs L2 distance costs to replicate paper's finding that L2 is generally superior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism causing task-dependent performance differences between feature and sequence conditioning in predictor architectures?
- Basis in paper: [inferred] Authors observe in Appendix E.1 that feature conditioning outperforms sequence conditioning on 2D navigation but not manipulation, admitting they "cannot provide a precise explanation."
- Why unresolved: Authors establish empirical trend but lack theoretical justification for how action-to-token pathway influences learned dynamics for different spatial complexities.
- What evidence would resolve it: Analysis of latent space geometry (smoothness, gradient paths) induced by each conditioning method across diverse tasks.

### Open Question 2
- Question: Why does scaling model capacity improve real-world planning success while degrading performance in simulated environments?
- Basis in paper: [inferred] Section 5.2 notes this divergence and hypothesizes that larger embedding spaces make it harder for planners to distinguish nearby states in simple domains, but this remains unconfirmed.
- Why unresolved: Observation contradicts intuition that higher capacity generally improves modeling, suggesting unexplained interaction between embedding space dimensionality and planner efficiency.
- What evidence would resolve it: Study correlating embedding resolution (distance between distinct states) with planning horizon and success rate across varying model sizes.

### Open Question 3
- Question: How can JEPA-WMs be trained to accurately predict contact-rich interactions and avoid "hallucinating" physical success?
- Basis in paper: [explicit] Appendix E.1 notes that during Metaworld manipulation tasks, model's visual decoding "hallucinates grasping the object," creating gap between predicted and actual outcomes.
- Why unresolved: Visual generation capabilities of decoder appear decoupled from physical constraints of contact, leading to misleading planning feedback.
- What evidence would resolve it: Integration of physical priors or auxiliary contact-based losses that penalize predictions violating object permanence or collision dynamics.

## Limitations
- Transfer learning analysis is limited - paper identifies proprioception misalignment as blocker between DROID and Robocasa but doesn't explore domain adaptation techniques (fine-tuning encoders, action space alignment) to mitigate gap.
- Ablation studies use different model scales (6 layers for sim, 12 for real), making it difficult to isolate whether differences are due to architectural choices or increased model capacity.
- Analysis limited to 2-step vs. teacher-forcing for multistep training, doesn't explore longer rollout horizons or alternative training strategies.

## Confidence
- **High confidence**: Core claim that planning in learned embedding space is more efficient than pixel-space planning, supported by consistent CEM L2 success across tasks and ablation showing single-step teacher-forcing fails.
- **Medium confidence**: Superiority of AdaLN over concatenation-based conditioning, though ablation shows strong performance, paper doesn't explore whether specific to ViT architectures or generalizable to other predictor types.
- **Medium confidence**: Finding that multistep rollout training is crucial, as analysis limited to 2-step vs. teacher-forcing and doesn't explore longer rollout horizons or alternative training strategies.

## Next Checks
1. **Cross-dataset Transfer Test**: Train model on DROID with proprioception, then systematically vary proprioception encoder (freeze, fine-tune, or remove) and test transfer to Robocasa to isolate misalignment effect.
2. **Architectural Scaling Control**: Re-run key ablations (AdaLN, multistep loss, proprioception) using same model scale (e.g., 6 layers) to confirm observed differences aren't artifacts of scale.
3. **Alternative Conditioning Comparison**: Implement and compare against other conditioning mechanisms (e.g., FiLM, cross-attention) in same predictor architecture to verify AdaLN's specific advantages beyond ViT depth.