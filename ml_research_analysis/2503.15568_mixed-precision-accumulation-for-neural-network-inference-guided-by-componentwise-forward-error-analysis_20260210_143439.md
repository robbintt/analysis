---
ver: rpa2
title: Mixed precision accumulation for neural network inference guided by componentwise
  forward error analysis
arxiv_id: '2503.15568'
source_url: https://arxiv.org/abs/2503.15568
tags:
- precision
- mixed
- fp16
- error
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mathematically founded mixed precision
  accumulation strategy for neural network inference. The core idea is to use a componentwise
  forward error analysis to identify which components of inner products should be
  computed in low precision versus high precision.
---

# Mixed precision accumulation for neural network inference guided by componentwise forward error analysis

## Quick Facts
- arXiv ID: 2503.15568
- Source URL: https://arxiv.org/abs/2503.15568
- Reference count: 40
- Key outcome: Mixed precision accumulation strategy for MLP inference that achieves accuracy matching uniform high precision while using only ~20-25% high precision computations, guided by componentwise forward error analysis

## Executive Summary
This paper introduces a mathematically founded mixed precision accumulation strategy for neural network inference. The core idea is to use a componentwise forward error analysis to identify which components of inner products should be computed in low precision versus high precision. The analysis shows that errors in each component are proportional to both the condition number of the inner product and the condition number of the activation function. Based on this, the paper proposes an algorithm that first computes all components in low precision, estimates condition numbers, and then recomputes only those components with large condition numbers in higher precision.

## Method Summary
The method computes all inner products in low precision (fp8), then estimates the condition number κ_ℓ = κ_φ ⊘ |v_ℓ| for each component, where κ_φ is the activation function condition number and κ_v is approximated as c/|v|. Components with κ_ℓ > threshold τ are recomputed in high precision (fp16) and requantized to fp8. For ReLU activations, κ_φ = 0 for negative pre-activations, making this method particularly effective. The algorithm is implemented using mptorch for fp8 simulation and tested on MNIST and Fashion MNIST with pre-trained MLPs.

## Key Results
- Mixed precision strategy achieves accuracy matching uniform high precision (fp16) while using only ~20-25% high precision computations for ReLU networks
- For ReLU activations, 77-90% of condition numbers are zero, enabling massive low-precision retention
- Cost model shows c_mixed = clow + ρ·chigh, with efficiency requiring ρ < 1 - (clow/chigh)
- The method is particularly effective for ReLU activations where a large percentage of condition numbers are zero

## Why This Works (Mechanism)

### Mechanism 1: Componentwise Error Decomposition via Condition Numbers
The forward error in each output component scales with the product of two condition numbers: the inner product condition number and the activation function condition number. The error bound satisfies a recurrence εhℓ = κϕℓ(vℓ) ◦ κvℓ ◦ (εWℓ + ‖εhℓ−1‖∞1) + εϕℓ, where κvℓ = (|Wℓ||hℓ−1|)/|Wℓhℓ−1| captures cancellation sensitivity and κϕℓ captures activation sensitivity. Errors are scaled componentwise, not uniformly.

### Mechanism 2: ReLU Zero Condition Numbers Enable Massive Low-Precision Retention
For ReLU activations, negative pre-activations yield κϕℓ = 0, meaning these components require no high-precision recompute regardless of the inner product condition number. ReLU(v) = max(0,v). When v < 0, the output is identically zero and the condition number κϕℓ = |v·ϕ′(v)/ϕ(v)| = 0 because the derivative is zero in this region. Zero condition numbers completely dampen error amplification.

### Mechanism 3: Low-Precision Condition Number Estimation Preserves Classification Decisions
Computing condition numbers in low precision (ulow) preserves sufficient ordering to correctly classify which components need high-precision recompute. κϕℓ estimation uses only |v ◦ ϕ′(v)|/|ϕ(v)| computed from low-precision outputs. κvℓ is approximated as c/|v| because the numerator |W||h| has much lower variance than the denominator |Wh|—cancellation drives the condition number, not magnitude.

## Foundational Learning

- Concept: **Componentwise vs normwise error analysis**
  - Why needed here: The key insight is that errors vary across components by orders of magnitude; normwise analysis "smudges" these differences together, hiding mixed-precision opportunities.
  - Quick check question: If all errors were bounded only in ∞-norm, could you identify which specific inner products need higher precision?

- Concept: **Condition number of an inner product (κv)**
  - Why needed here: κv = (|w||x|)/|w·x| measures cancellation—when positive and negative terms nearly cancel, small perturbations cause large relative output changes.
  - Quick check question: What is κv when w = [1, -1, 1, -1] and x = [1, 1, 1, 0.99]?

- Concept: **Condition number of an activation function (κϕ)**
  - Why needed here: κϕ = |v·ϕ′(v)/ϕ(v)| quantifies how input perturbations amplify through the nonlinearity; for ReLU this is 0 or 1, for tanh it varies with input magnitude.
  - Quick check question: Why is κϕ = ∞ when ϕ(v) = 0 but ϕ(v(1+Δv)) ≠ 0?

## Architecture Onboarding

- Component map: Input hℓ−1 → [Low-precision MMA: vℓ = Wℓhℓ−1] → [Activation: hℓ = ϕℓ(vℓ)] → [Condition number estimation: κℓ = κϕℓ ⊘ |vℓ|] → [Threshold check: κℓ > τ?] → [Keep fp8 or Recompute fp16 → Requantize to fp8] → [Combine → hℓ passed to layer ℓ+1]

- Critical path:
  1. Full low-precision forward pass (dominant cost: clow)
  2. κϕℓ estimation from low-precision outputs (O(nℓ) per layer, negligible)
  3. Threshold comparison per component
  4. Selective high-precision recompute for components where κℓ > τ

- Design tradeoffs:
  - **τ selection**: Lower τ → higher accuracy, higher ρ (recompute fraction). For ReLU, τ ≈ 0.1 works; for tanh, τ ≈ 1-5.
  - **Cost model**: cmixed = clow + ρ·chigh. Efficiency requires ρ < 1 - (clow/chigh). For fp8/fp16 with clow/chigh ≈ 0.5, need ρ < 0.5.
  - **Static vs dynamic**: Algorithm is input-dependent (dynamic); static variants could precompute precision masks but lose adaptivity.

- Failure signatures:
  - Accuracy drops below uniform low precision: τ too high, or κ estimation produces systematic errors
  - Cost exceeds uniform high precision: ρ > 0.5 (for fp8/fp16 setup); occurs easily with tanh for low τ
  - ReLU not showing efficiency gains: Check that >75% of pre-activations are negative; network may not have suitable activation patterns

- First 3 experiments:
  1. **Validate κ estimation fidelity**: Compute κℓ in both fp8 and fp32 across all layers; plot correlation. Target: R² > 0.9 for order-of-magnitude preservation.
  2. **Measure zero κϕ percentage for ReLU networks**: On validation set, compute fraction of negative pre-activations. Target: >75% for efficiency gains.
  3. **Cost-accuracy tradeoff curve**: Sweep τ ∈ {0.01, 0.1, 0.5, 1.0, 5.0} on held-out test set; plot accuracy vs ρ. Identify τ where accuracy matches uniform high precision with ρ < 0.5.

## Open Questions the Paper Calls Out

- **Static allocation strategy**: Does a static allocation strategy, where precision is determined a priori based on a representative dataset, achieve a better performance-accuracy tradeoff than the proposed dynamic runtime strategy? The authors suggest that the overhead cost of the dynamic algorithm could be removed by "learning" the precision configuration statically, which may be easier to implement efficiently.

- **Hardware implementation**: To what extent can the theoretical cost-accuracy tradeoff be realized in high-performance hardware implementations given the overhead of data movement and precision switching? The authors explicitly state that an important question left for future work is the high-performance implementation, noting that simulated experiments cannot assess the practical translation of the theoretical potential.

- **Transformer adaptation**: How can the componentwise error analysis and mixed-precision accumulation strategy be effectively adapted for transformer architectures, specifically regarding the softmax function? The authors note that the extension to transformers is a natural perspective, but requires further investigation because the softmax function "amplifies variations in magnitude," potentially complicating the error bounds.

- **Training phase extension**: Can the componentwise error analysis be rigorously extended to the training phase of neural networks to guide online mixed-precision accumulation? The authors list the "extension of our approach for online mixed-precision training" as a meaningful direction for future research in the conclusion.

## Limitations

- The assumption that low-precision condition number estimation preserves ordering lacks rigorous theoretical justification and quantitative analysis of misclassification rates.
- The approximation κv ≈ c/|v| relies on heuristic arguments about the relative variance of numerator and denominator without theoretical bounds.
- The method's effectiveness for ReLU networks critically depends on the assumption that low-precision negative pre-activations remain negative, with no quantitative analysis of perturbation magnitude distribution or sign flip probability.

## Confidence

- **High confidence**: The componentwise error decomposition framework and the relationship εhℓ ∝ κϕℓ(vℓ) ◦ κvℓ ◦ (εWℓ + ‖εhℓ−1‖∞1) are mathematically rigorous (Theorem 2.4). The observation that ReLU yields κϕℓ = 0 for negative pre-activations is exact and well-established.
- **Medium confidence**: The empirical validation on MNIST/Fashion MNIST MLPs demonstrates the approach works in practice, but the results are limited to specific network architectures (3-8 layer MLPs) and datasets. The generalizability to deeper networks, different architectures (CNNs, transformers), or tasks remains untested.
- **Low confidence**: The approximation κv ≈ c/|v| and the claim that low-precision condition number estimation preserves ordering lack rigorous theoretical justification. The paper relies heavily on empirical observations without providing error bounds or probability estimates.

## Next Checks

1. **Misclassification rate analysis**: For a held-out validation set, compute the exact classification error rate when using fp8-estimated κℓ versus fp32-ground-truth κℓ. Report the fraction of components that are: (a) correctly classified as safe (fp8 and fp32 both below τ), (b) correctly classified as needing recomputation (fp8 and fp32 both above τ), (c) false negatives (fp8 above τ, fp32 below τ), and (d) false positives (fp8 below τ, fp32 above τ). Target: false positive/negative rates < 5%.

2. **Perturbation stability for ReLU**: For a diverse set of negative pre-activations from the validation set, compute the minimum perturbation magnitude needed to flip sign in fp8 arithmetic. Report the distribution of these perturbation thresholds and the fraction of pre-activations that would flip under the worst-case fp8 error. Target: >95% of negative pre-activations require perturbations >100% (i.e., |Δv| > 1) to flip sign.

3. **Cross-architecture generalization**: Implement the same mixed-precision strategy on a CNN (e.g., LeNet-5) trained on CIFAR-10 or MNIST. Compare the fraction ρ of high-precision recomputations and the accuracy-cost tradeoff curve against the MLP results. Target: Demonstrate that ρ < 0.5 for at least one architecture while maintaining accuracy within 0.5% of uniform high precision.