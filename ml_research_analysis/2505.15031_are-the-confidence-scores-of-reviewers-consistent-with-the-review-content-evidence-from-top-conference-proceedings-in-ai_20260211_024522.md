---
ver: rpa2
title: Are the confidence scores of reviewers consistent with the review content?
  Evidence from top conference proceedings in AI
arxiv_id: '2505.15031'
source_url: https://arxiv.org/abs/2505.15031
tags:
- review
- dence
- hedge
- scores
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the consistency between reviewers\u2019\
  \ confidence scores and their textual review content at fine-grained levels (word,\
  \ sentence, and aspect). Using peer review data from top AI conferences, a deep\
  \ learning model is trained to detect hedge sentences, which are then annotated\
  \ with aspects."
---

# Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI

## Quick Facts
- **arXiv ID:** 2505.15031
- **Source URL:** https://arxiv.org/abs/2505.15031
- **Reference count:** 40
- **Primary result:** Higher reviewer confidence scores correlate with longer reviews, more hedge sentences, and higher rejection rates

## Executive Summary
This study investigates whether reviewers' confidence scores align with their textual review content at fine-grained levels (word, sentence, and aspect) using peer review data from top AI conferences. A deep learning model is trained to detect hedge sentences, which are then annotated with aspects to analyze their relationship with confidence scores. Statistical analysis reveals that reviews with higher confidence scores contain more hedge sentences and are significantly longer. Regression analysis shows a negative correlation between confidence scores and paper acceptance, suggesting that higher confidence is associated with rejection decisions. These findings support the reliability and transparency of the peer review process.

## Method Summary
The study employs a deep learning approach to classify hedge sentences in peer reviews, followed by statistical analysis of consistency between text features and reviewer confidence scores. The methodology involves training a Sentence-BERT encoder with Bi-LSTM and MLP layers on the HedgePeer dataset to detect hedging language. The trained model is then applied to peer review texts from ICLR and various NLP conferences to identify hedge sentences. These sentences are filtered using a predefined list of 378 hedge words and annotated with eight aspects using a tagger from prior work. Statistical analyses including Spearman correlation and Mann-Whitney U tests examine the relationship between hedge frequency, review length, aspects, and confidence scores. Multinomial logistic regression is used to assess how confidence scores and aspects predict paper acceptance decisions.

## Key Results
- Review reports with higher confidence scores contain significantly more hedge sentences and longer text
- Consistency between confidence scores and review content is maintained across word, sentence, and aspect levels
- Higher confidence scores show a negative correlation with paper acceptance rates, indicating association with rejection
- The correlation between confidence scores and hedge frequency is statistically significant across multiple datasets

## Why This Works (Mechanism)
The mechanism works because hedge sentences serve as linguistic markers of uncertainty that reviewers use when expressing confidence levels. By training a deep learning model to detect these hedge sentences, the study can quantify uncertainty expressions in review texts. The Sentence-BERT encoder captures semantic meaning of sentences, while the Bi-LSTM layer processes sequential dependencies in hedging expressions. The MLP with sigmoid activation enables binary classification of hedging behavior. Statistical analysis then reveals systematic patterns between these linguistic features and reviewers' self-reported confidence levels.

## Foundational Learning
- **Hedge sentence detection**: Understanding how hedging functions as epistemic modality in academic writing is crucial for interpreting reviewer uncertainty. Quick check: Verify hedge word list captures common epistemic hedges in computer science discourse.
- **Sentence-BERT embeddings**: These provide semantic representations that capture sentence meaning beyond surface-level features. Quick check: Compare cosine similarity between hedge and non-hedge sentences in embedding space.
- **Bi-LSTM architecture**: Processes sequential information bidirectionally to capture context around hedging expressions. Quick check: Visualize attention weights to confirm model focuses on hedging cues.
- **Statistical correlation analysis**: Spearman correlation measures monotonic relationships between confidence scores and text features. Quick check: Plot scatter plots to visually inspect correlation patterns.
- **Aspect-based annotation**: Categorizing hedge sentences by content area (Originality, Motivation, etc.) enables granular analysis. Quick check: Calculate inter-annotator agreement on aspect labels.
- **Multinomial logistic regression**: Models the relationship between multiple predictors (confidence, aspects) and categorical outcomes (acceptance decisions). Quick check: Examine odds ratios to interpret predictor effects.

## Architecture Onboarding
- **Component map:** Sentence-BERT -> Bi-LSTM -> MLP -> Sigmoid (classification) -> Hedge word filter -> Aspect tagger -> Statistical analysis
- **Critical path:** Text preprocessing → Sentence embedding → Sequential processing → Classification → Aspect annotation → Statistical correlation
- **Design tradeoffs:** The model balances between dictionary-based filtering (high precision, low recall) and deep learning predictions (higher recall, potential noise). Using both approaches mitigates false negatives while maintaining interpretability.
- **Failure signatures:** Poor hedge detection occurs when sentences contain implicit hedging without explicit cue words, or when hedging serves rhetorical rather than epistemic functions. Aspect annotation fails when hedge sentences span multiple categories or when domain-specific terminology is ambiguous.
- **First experiments:**
  1. Train the Bi-LSTM classifier on HedgePeer and evaluate on held-out test set to verify 0.88 accuracy claim.
  2. Apply the trained model to a sample of review texts and manually verify hedge sentence predictions.
  3. Run Spearman correlation analysis on a small subset to confirm consistency between confidence scores and hedge frequency.

## Open Questions the Paper Calls Out
The authors identify three key open questions: First, whether the observed correlations between confidence scores, hedge frequency, and paper decisions hold across academic disciplines outside of Computer Science, given that open peer review data primarily pertains to computer science. Second, whether large language models can improve detection of implicit hedging that dictionary-based deep learning models miss, as some hedging is implicit and difficult to detect. Third, whether other linguistic characteristics beyond hedging serve as significant predictors for reviewer confidence, as the study narrowed its scope to hedge words, sentences, and aspects.

## Limitations
- The study relies on open peer review data primarily from computer science conferences, limiting generalizability to other disciplines.
- Cross-annotator agreement for hedge sentence identification shows only moderate reliability (Cohen's kappa = 0.54).
- The negative correlation between confidence and acceptance could reflect reviewer bias rather than objective quality assessment.
- The hedge word list of 378 words may not capture all hedging behavior and could introduce false positives.

## Confidence
- **High confidence:** The statistical finding that higher confidence scores correlate with longer reviews and more hedge sentences is robust across multiple datasets and significance tests.
- **Medium confidence:** The interpretation that hedge usage reflects epistemic uncertainty is reasonable but not definitively proven, as hedges can serve multiple rhetorical functions beyond expressing uncertainty.
- **Low confidence:** The claim that these findings "support the reliability and transparency of the peer review process" overstates the evidence, as the study doesn't directly measure review quality or decision accuracy.

## Next Checks
1. Conduct an inter-annotator reliability study on a subset of reviews to validate the hedge sentence annotations and aspect assignments, particularly given the moderate kappa score.
2. Perform ablation studies testing whether the hedge word list filter or the DL model predictions drive the observed correlations more strongly.
3. Analyze whether the negative correlation between confidence and acceptance rates persists when controlling for reviewer experience, paper quality indicators, or field-specific norms.