---
ver: rpa2
title: How to Enhance Downstream Adversarial Robustness (almost) without Touching
  the Pre-Trained Foundation Model?
arxiv_id: '2504.10850'
source_url: https://arxiv.org/abs/2504.10850
tags:
- adversarial
- training
- robust
- contrastive
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving adversarial robustness
  in downstream tasks without updating the foundation model's weights. The authors
  theoretically show that the downstream adversarial loss can be bounded by the downstream
  clean loss plus the adversarial contrastive loss.
---

# How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?

## Quick Facts
- **arXiv ID**: 2504.10850
- **Source URL**: https://arxiv.org/abs/2504.10850
- **Reference count**: 40
- **Primary result**: Simple-yet-effective robust auto-encoder pre-processor improves downstream adversarial robustness without updating foundation model weights

## Executive Summary
This paper addresses the challenge of improving adversarial robustness in downstream tasks without updating the foundation model's weights. The authors theoretically show that the downstream adversarial loss can be bounded by the downstream clean loss plus the adversarial contrastive loss. Based on this insight, they propose a simple-yet-effective robust auto-encoder as a data pre-processor trained with adversarial contrastive loss, which purifies downstream data before feeding it to the foundation model. Extensive experiments on multiple datasets demonstrate that their method significantly improves downstream robustness compared to baselines, while maintaining low computational costs. The results validate the connection between feature robustness (small adversarial contrastive loss) and downstream task robustness.

## Method Summary
The method trains a robust auto-encoder as a data pre-processor that transforms adversarial inputs into purified representations before feeding them to a frozen foundation model. The auto-encoder is trained with a combined loss: reconstruction loss (MSE) plus adversarial contrastive loss weighted by λ. During training, FGSM attacks are used to generate adversarial examples, and the contrastive loss explicitly pulls clean and adversarial representations together while pushing apart representations of different classes. The foundation model (ViT-MAE-Large) remains frozen throughout. After pre-processor training, a linear classifier is trained on top of the foundation model's outputs, with optional robust fine-tuning using adversarial examples.

## Key Results
- CRoPD significantly improves robust accuracy on CIFAR-10 (+13.68% PGD-10), CIFAR-100 (+5.56% PGD-10), and SVHN (+5.84% PGD-10) compared to Identity baseline
- CRoPD outperforms reconstruction-only auto-encoder baseline, validating the importance of contrastive loss
- Cross-dataset transfer shows CRoPD can improve robustness even when trained on different data (e.g., CIFAR-10→CIFAR-100 improves from 28.72% to 34.28% PGD-10)
- Clean accuracy degrades by only 0.1-1.7% compared to Identity baseline, maintaining natural performance

## Why This Works (Mechanism)

### Mechanism 1: Theoretical Bound Between Contrastive and Downstream Robustness
The paper derives that downstream adversarial loss is upper-bounded by clean downstream loss plus a scaled adversarial contrastive loss (κ·L_con). By minimizing L_con during pre-processor training, the bound tightens, limiting how much adversarial perturbations can degrade downstream performance. This assumes the encoder produces representations where clean/adversarial pairs stay close while different-class pairs remain separated, and the classifier loss is bounded.

### Mechanism 2: Robust Auto-Encoder as Adversarial Purifier
The auto-encoder learns to map adversarial inputs to reconstructions whose latent representations align with clean inputs. This "purification" removes adversarial structure before the foundation model processes the data. The contrastive objective explicitly forces fen(x_adv) ≈ fen(x) while pushing apart representations of different classes, creating robust features that transfer to downstream tasks.

### Mechanism 3: Why Reconstruction-Only Training Fails
Auto-encoders trained with only adversarial reconstruction loss do not guarantee downstream robustness because they only ensure the output visually resembles the input—they do not constrain the latent representation to be robust. An identity auto-encoder can achieve zero reconstruction loss while passing adversarial perturbations unchanged to the foundation model.

## Foundational Learning

- **Adversarial Training and PGD Attacks**
  - Why needed here: CRoPD uses FGSM during training but is evaluated against stronger PGD-10/20 and AutoAttack. Understanding the threat model (L∞ ball, ε budget) is essential for interpreting results.
  - Quick check question: Can you explain why PGD-20 is a stronger attack than FGSM, and what the ε=8/255 budget means for image perturbations?

- **Contrastive Learning Objective**
  - Why needed here: The core insight is that contrastive loss (pulling similar pairs together, pushing dissimilar pairs apart) transfers to robustness when applied adversarially.
  - Quick check question: In standard contrastive learning, what constitutes a "positive pair" vs. a "negative pair"? How does CRoPD modify this for adversarial inputs?

- **Auto-Encoder Architecture (Encoder-Decoder)**
  - Why needed here: CRoPD uses a ViT-MAE variant as the preprocessor. Understanding latent representations and reconstruction is critical for debugging.
  - Quick check question: What does the reconstruction loss ‖f_de(f_en(x)) - x‖² optimize, and why might minimizing it alone fail to improve robustness?

## Architecture Onboarding

**Component map:**
Input x → Robust Auto-Encoder (f_en, f_de) → Purified x̂ → Frozen Foundation Model (ViT-MAE) → Features → Linear Classifier → Prediction

**Critical path:**
1. Train auto-encoder on downstream dataset with combined loss: reconstruction + λ·adversarial contrastive
2. Freeze auto-encoder; feed all downstream data through it to foundation model
3. Train only the linear classification head on labeled downstream data (clean or adversarial)

**Design tradeoffs:**
- λ (contrastive weight): Higher λ → better robustness, worse clean accuracy. Paper uses λ∈{0.1, 1, 10}; optimal varies by dataset.
- Pre-processor size: Smaller pre-processor reduces training cost but may limit representation capacity. Paper uses 6-28% of foundation model parameters.
- Attack strength during training: FGSM used for efficiency; stronger attacks (PGD) may improve robustness further but increase cost.

**Failure signatures:**
- Low robust accuracy despite high λ: Check if per-class data is sufficient (CIFAR-100 underperforms due to 600 samples/class vs. 5000 for CIFAR-10)
- High clean accuracy drop: Reduce λ or increase reconstruction weight
- Pre-processor doesn't transfer: Re-train on target domain data (Table 3 shows cross-dataset transfer is limited)

**First 3 experiments:**
1. **Baseline replication**: Run Identity + linear layer on CIFAR-10 with PGD-10 attack. Verify ~2-3% robust accuracy matches Table 2a.
2. **Ablation on λ**: Train CRoPD with λ∈{0.1, 1, 10} on CIFAR-10. Plot clean vs. robust accuracy tradeoff. Expect λ=10 to achieve ~48% PGD-10 accuracy.
3. **Transfer test**: Train CRoPD on CIFAR-10, evaluate on CIFAR-100 (or vice versa). Compare to Table 3 results to validate generalization claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CRoPD framework be effectively adapted to enhance the robustness of Large Language Models (LLMs) in Natural Language Processing (NLP)?
- Basis in paper: The Conclusion states that while this work focuses on image data, the ideas could be borrowed for NLP to enhance the robustness of large language models.
- Why unresolved: The discrete nature of text (tokens) contrasts with the continuous image data used in the study, potentially complicating the application of contrastive auto-encoders and gradient-based adversarial training.
- What evidence would resolve it: A successful application of a text-based robust auto-encoder that improves downstream robustness in an LLM without accessing the foundation model's weights.

### Open Question 2
- Question: Does the theoretical bound connecting adversarial contrastive loss to downstream robustness hold under alternative data assumptions, such as sparse coding models?
- Basis in paper: The Conclusion identifies deepening the theoretical understanding via different data assumptions (specifically mentioning sparse coding models) as a primary future direction.
- Why unresolved: The current theoretical analysis relies on specific assumptions about the data distribution and feature reconstruction that may not generalize to sparse coding structures without further derivation.
- What evidence would resolve it: A theoretical extension of Theorem 1 that proves the bound holds under sparse coding assumptions, supported by empirical validation on data generated from such models.

### Open Question 3
- Question: Can CRoPD maintain high robustness in downstream tasks where the number of classes is high but the number of training samples per class is very low?
- Basis in paper: Appendix B.2 notes that CRoPD performance dipped on CIFAR-100 due to "significantly fewer samples per class," making it harder to learn well-separated robust features.
- Why unresolved: The paper attributes the performance drop to data scarcity but does not propose a solution or verify if the method fundamentally fails in extreme few-shot, high-cardinality settings.
- What evidence would resolve it: Experiments on datasets specifically designed for fine-grained classification (high class count, low sample density) showing that the auto-encoder can still form robust representations.

## Limitations
- The theoretical bound relies on assumptions about representation geometry that may not hold for datasets with few samples per class or extremely fine-grained distinctions
- Cross-dataset transfer results show limited generalization, raising questions about applicability to significantly different domains
- Claims about computational efficiency relative to robust fine-tuning are difficult to verify without implementation details and precise timing measurements

## Confidence

**High Confidence**: The empirical demonstration that CRoPD improves robust accuracy over baselines across multiple datasets and attack types is well-supported by Table 1-2 results.

**Medium Confidence**: The theoretical bound connecting adversarial contrastive loss to downstream robustness is formally derived, but its tightness in practice depends on unverified assumptions about representation geometry.

**Low Confidence**: Claims about computational efficiency relative to robust fine-tuning are difficult to verify without implementation details of alternative methods and precise timing measurements across different hardware configurations.

## Next Checks
1. **Bound Sensitivity Analysis**: Systematically vary ε in FGSM attacks during pre-processor training and measure how downstream robust accuracy scales. This tests whether the theoretical bound holds empirically across different perturbation magnitudes.
2. **Per-Class Robustness Audit**: For CIFAR-100, analyze robust accuracy per class to identify whether performance degradation correlates with class-specific sample size or semantic similarity to other classes. This validates the data-efficiency limitation claim.
3. **Pre-Processor Capacity Scaling**: Train CRoPD with pre-processor architectures of varying depth (4, 8, 12 encoder layers) on CIFAR-10 and measure the impact on both clean and robust accuracy. This quantifies the tradeoff between computational cost and performance.