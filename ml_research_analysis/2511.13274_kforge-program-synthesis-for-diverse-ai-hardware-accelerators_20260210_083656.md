---
ver: rpa2
title: 'KForge: Program Synthesis for Diverse AI Hardware Accelerators'
arxiv_id: '2511.13274'
source_url: https://arxiv.org/abs/2511.13274
tags:
- torch
- kernel
- performance
- tensor
- program
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'KForge introduces an agent-based framework for synthesizing high-performance
  GPU kernels across diverse accelerators like CUDA and Metal. It uses two LLM agents:
  a generation agent that iteratively refines programs via compilation and correctness
  feedback, and a performance analysis agent that interprets profiling data to guide
  optimization.'
---

# KForge: Program Synthesis for Diverse AI Hardware Accelerators

## Quick Facts
- arXiv ID: 2511.13274
- Source URL: https://arxiv.org/abs/2511.13274
- Reference count: 7
- One-line primary result: Agent-based framework synthesizes GPU kernels across CUDA/Metal with up to 90% correctness using iterative refinement and profiling feedback

## Executive Summary
KForge introduces a dual-agent framework for synthesizing high-performance GPU kernels across diverse accelerators like CUDA and Metal. It uses a generation agent that iteratively refines programs via compilation and correctness feedback, paired with a performance analysis agent that interprets profiling data to guide optimization. The approach leverages cross-platform knowledge transfer from reference implementations and works with varied profiling inputs including programmatic APIs and GUI tools. Experiments on KernelBench show that reasoning models achieve up to 90% correctness and 20% kernels running 1.5× faster than baselines.

## Method Summary
KForge implements a two-agent program synthesis framework where a generation agent produces and iteratively refines GPU kernels while a performance analysis agent interprets profiling data to guide optimization. The system supports single-shot generation, iterative refinement with reference implementations, and profiling-guided optimization. It works across CUDA and Metal backends using different profiling tools (Nsight Systems and Xcode Instruments) and employs Jinja2 templates for structured prompt engineering. The framework synthesizes kernels that wrap into PyTorch nn.Module classes and verifies them through compilation, runtime, and correctness checks.

## Key Results
- Iterative refinement with reasoning models achieves up to 90% correctness across KernelBench difficulty levels
- Cross-platform transfer from CUDA to Metal improves single-shot correctness by 20-30% points
- 20% of synthesized kernels achieve 1.5× speedup over PyTorch eager baselines
- Specialized performance analysis agent improves optimization quality for complex workloads despite inconsistent profiling feedback results

## Why This Works (Mechanism)

### Mechanism 1
Separating generation and performance analysis into specialized agents improves both correctness rates and optimization quality compared to a single monolithic agent. The generation agent F produces kernels and iteratively refines them via compilation/correctness feedback. The performance analysis agent G processes profiling data and generates targeted optimization recommendations r_t that feed back into the next synthesis iteration: F: (p, k_{t-1}, r_{t-1}) → k_t. This assumes LLM performance degrades on long contexts (>32K tokens for relevant information retrieval) and that profiling data contains sparse optimization signals buried in extensive metrics.

### Mechanism 2
Providing reference implementations from a source accelerator (e.g., CUDA) improves kernel synthesis correctness and performance for a target accelerator (e.g., Metal). Reference implementations expose language-agnostic parallel patterns (thread indexing, synchronization, memory access) that transfer across hardware paradigms. The model translates structural similarities—CUDA thread blocks to Metal threadgroups, warps to SIMD-groups—while adapting syntax and hardware-specific optimizations. This assumes parallel execution hierarchies and computational logic remain largely invariant across GPU architectures.

### Mechanism 3
Incorporating profiling feedback (textual metrics or visual screenshots) improves performance optimization for functionally correct kernels, particularly for complex workloads. The performance analysis agent interprets profiling data—Nsight Systems CSV exports or Xcode Instruments screenshots—and generates a single optimization recommendation targeting the highest-impact bottleneck. This recommendation conditions the next generation iteration. This assumes LLMs can extract actionable optimization signals from raw profiling data (hundreds of metrics) or visual representations, and these signals translate to performance improvements when incorporated into synthesis prompts.

## Foundational Learning

- **GPU execution model (thread hierarchies, memory coalescing, occupancy)**: Understanding why KForge's profiling feedback works requires knowing what metrics (warp occupancy, memory bandwidth, arithmetic intensity) indicate performance bottlenecks. *Quick check: Given a kernel with low occupancy but high memory bandwidth utilization, what optimization strategy would likely help most?*

- **Iterative refinement with feedback loops**: KForge's core architecture relies on closed-loop iteration where compilation errors, runtime failures, and profiling data condition subsequent generations. *Quick check: If a kernel passes functional correctness but profiling shows poor cache utilization, what information should feed back into the next iteration?*

- **Cross-platform kernel translation**: The CUDA→Metal transfer mechanism depends on recognizing that thread blocks, warps, and synchronization primitives have analogues across GPU architectures despite syntax differences. *Quick check: What CUDA concepts map to Metal's threadgroups and SIMD-groups, and what optimizations would NOT transfer directly?*

## Architecture Onboarding

- **Component map**: Problem input → Single-shot generation → Compilation check → Runtime check → Correctness check → (if correct) Profiling → Performance analysis → Optimization recommendation → Next iteration generation → Repeat until N iterations or target speedup achieved.
- **Critical path**: Generation agent produces kernel code using Jinja2 templates, verification module executes and classifies states (generation failure / compilation failure / runtime error / shape mismatch / correct), performance analysis agent interprets profiling data and generates optimization recommendations that feed back into generation.
- **Design tradeoffs**: Single-shot vs. iterative (single-shot is faster but achieves lower correctness), with vs. without reference implementation (reference improves correctness but requires available source kernels), textual vs. visual profiling (textual works with text-only models, visual requires multimodal capabilities).
- **Failure signatures**: Compilation failure (syntax errors, missing includes), runtime error (segmentation faults, aborts), numerical/shape mismatch (incorrect output values), performance degradation after optimization (local optima), "cheating" optimizations (constant output recognition).
- **First 3 experiments**: 1) Baseline single-shot synthesis on 10 Level 1 problems without reference or iteration, 2) Iterative refinement with CUDA reference + 5 iterations, 3) Profiling-guided optimization on 5 correct-but-slow kernels with Nsight data.

## Open Questions the Paper Calls Out

- Which specific profiling metrics most effectively guide optimization, and how can useless or misleading profiling signals be automatically filtered? The paper notes raw profiling data often contains hundreds of metrics, creating overwhelming information, and profiling sometimes leads to performance degradation.

- How can iterative refinement escape local optima to discover fundamentally better algorithmic restructuring rather than marginal improvements? The paper observes models become trapped in local optima where each iterative refinement produces marginal improvements while missing opportunities for more fundamental algorithmic restructuring.

- Can synthesized programs be extended to support backward passes for training workloads across diverse accelerators? Current framework only targets inference kernels; backward pass synthesis requires handling gradient computation, memory optimization for activations, and potentially different optimization strategies.

## Limitations

- Profiling feedback shows inconsistent effectiveness across model types, with only gpt-5 demonstrating reliable performance gains
- Cross-platform transfer from CUDA to Metal has not been validated across fundamentally different architectures like SIMD vs. dataflow accelerators
- Jinja2-based prompt engineering is presented as superior but lacks comparative validation against other prompt engineering approaches

## Confidence

- **High confidence**: Dual-agent architecture's separation of concerns demonstrates measurable improvements in correctness rates for reasoning models
- **Medium confidence**: Cross-platform knowledge transfer shows strong correctness gains but limited performance optimization evidence
- **Low confidence**: Profiling-guided optimization shows inconsistent results across models and lacks systematic validation of which profiling metrics drive improvements

## Next Checks

1. **Profiling feedback ablation study**: Run 20 Level 3 problems with and without profiling feedback across all model types to quantify when profiling helps vs. hurts.

2. **Architecture transfer stress test**: Generate CUDA→Metal kernels for a heterogeneous set including SIMT, SIMD, and dataflow examples to identify where cross-platform transfer breaks down.

3. **Prompt engineering comparison**: Implement the same two-agent pipeline using standard zero-shot prompts instead of Jinja2 templates to isolate the impact of structured prompt engineering on correctness and performance.