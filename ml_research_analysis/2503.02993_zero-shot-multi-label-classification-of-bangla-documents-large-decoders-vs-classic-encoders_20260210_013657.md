---
ver: rpa2
title: 'Zero-Shot Multi-Label Classification of Bangla Documents: Large Decoders Vs.
  Classic Encoders'
arxiv_id: '2503.02993'
source_url: https://arxiv.org/abs/2503.02993
tags:
- bangla
- language
- label
- arxiv
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large decoder-based language
  models (LLMs) and classic encoder-based models for zero-shot multi-label classification
  of Bangla documents. It establishes a benchmark by testing 32 state-of-the-art models
  on a newly created BanglaNewsNet dataset of 7,245 news articles with 21 labels.
---

# Zero-Shot Multi-Label Classification of Bangla Documents: Large Decoders Vs. Classic Encoders

## Quick Facts
- **arXiv ID:** 2503.02993
- **Source URL:** https://arxiv.org/abs/2503.02993
- **Reference count:** 40
- **Key outcome:** While a few models achieve F1 scores around 60%, most struggle with high accuracy, indicating that both LLMs and encoders face challenges in handling Bangla's linguistic complexities. Notably, no LLM surpasses the best-performing encoder, LaBSE, in embedding-based classification.

## Executive Summary
This study establishes a benchmark for zero-shot multi-label classification of Bangla documents by evaluating 32 state-of-the-art models on a newly created BanglaNewsNet dataset of 7,245 news articles with 21 labels. The evaluation focuses on F1 score, precision, and recall, comparing large decoder-based language models (LLMs) and classic encoder-based models. Results show that while a few models achieve F1 scores around 60%, most struggle with high accuracy, indicating that both LLMs and encoders face challenges in handling Bangla's linguistic complexities. Notably, no LLM surpasses the best-performing encoder, LaBSE, in embedding-based classification.

## Method Summary
The study evaluates zero-shot multi-label classification using two approaches: encoder-based methods that compute semantic similarity between document and label embeddings, and decoder-based LLMs that follow structured prompts for generative classification. The BanglaNewsNet dataset contains 7,245 news articles with 21 labels, averaging 1.345 labels per article. Models are evaluated using micro-averaged F1, precision, and recall metrics. Encoder models (LaBSE, LASER, BanglaTransformer) generate fixed-dimensional vectors, while LLMs (Gemini, GPT, Llama variants) generate label predictions through prompt-following.

## Key Results
- LaBSE achieves F1=0.404 with Explicit-Mentions label embedding, outperforming all <8B parameter LLMs in embedding-based classification
- Gemini-1.5-Flash (8B) achieves F1=0.571, outperforming larger models including GPT-3.5 (175B, F1=0.537)
- Larger models do not consistently outperform smaller models on Bangla zero-shot classification (e.g., Mixtral-56B F1=0.305 vs. Gemma-2 27B F1=0.593)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity via Sentence Embeddings
Encoder-based zero-shot classification operates by measuring cosine similarity between document and label embeddings in a shared semantic space. Document text is encoded into fixed-dimensional vectors via transformer encoders, while labels are embedded using either "Label name + Keywords" or "Explicit-Mentions" approaches. Classification occurs via threshold-based similarity matching without task-specific training. The embedding space must preserve cross-lingual semantic relationships such that Bangla documents and their conceptual labels cluster together regardless of explicit training signal.

### Mechanism 2: Instruction-Following for Generative Classification
Decoder-based LLMs perform zero-shot multi-label classification by following structured prompts that define label taxonomy and output format constraints. A system prompt specifies 21 predefined Bangla labels and JSON output format. The model generates label predictions autoregressively based on learned instruction-following capabilities from RLHF/fine-tuning, without accessing internal representations. The model must map unseen Bangla text to semantically appropriate labels from the provided list.

### Mechanism 3: Scale-Quality Mismatch in Low-Resource Settings
Parameter count alone does not predict zero-shot performance on low-resource languages; data quality and architectural efficiency mediate this relationship. The study observes that smaller well-optimized models (Gemma-2 27B: F1=0.593, Gemini-1.5-Flash 8B: F1=0.571) match or exceed larger models (GPT-3.5 175B: F1=0.537, Mixtral 56B: F1=0.305). This suggests training data composition, distillation techniques, and architectural choices interact with multilingual capability in non-obvious ways.

## Foundational Learning

- **Concept: Zero-Shot Classification**
  - **Why needed here:** The entire benchmark operates without task-specific fine-tuning, relying on transferred knowledge from pre-training. Understanding what "zero-shot" means operationally is prerequisite to interpreting why F1 scores cluster around 40-60% rather than 90%+.
  - **Quick check question:** Given a new Bangla article about cricket with no training examples, how would LaBSE classify it using only label name embeddings?

- **Concept: Multi-Label vs. Multi-Class Classification**
  - **Why needed here:** BanglaNewsNet has 1.345 labels per article on average, meaning documents can belong to multiple categories simultaneously. This requires threshold-based decision boundaries rather than softmax argmax, affecting both evaluation and architecture design.
  - **Quick check question:** If an article about women's health policy receives labels ["Women", "Health", "Politics"], should precision be computed per-label or per-document?

- **Concept: Encoder-Decoder Architectural Difference**
  - **Why needed here:** The paper compares bidirectional encoders (LaBSE, LASER—designed for representation learning) against autoregressive decoders (GPT, LLaMA—designed for generation). The finding that encoders remain competitive for embedding-based classification reflects fundamental architectural trade-offs in how attention operates.
  - **Quick check question:** Why can you extract embeddings from LaBSE but must rely on prompting for GPT-4o Mini?

## Architecture Onboarding

- **Component map:** Input Layer: BanglaNewsNet article → Embedding Branch (Encoders: LaBSE/LASER/BanglaTransformer) or Generative Branch (LLMs: Gemini, GPT, Llama) → Label Embedding: Label+Keywords or Explicit-Mentions → Similarity Computation: Cosine similarity or Autoregressive Generation → Evaluation: Micro-averaged Precision, Recall, F1

- **Critical path:** The Explicit-Mentions label embedding approach requires finding articles that contain ≥3 informative keywords per label. If this step fails (insufficient keyword coverage), label embeddings become noisy, propagating errors through the entire similarity computation.

- **Design tradeoffs:**
  - **Embedding vs. Prompting:** Embedding-based methods require model internals but are deterministic and computationally cheaper. Prompting works with black-box APIs but introduces output parsing fragility and higher inference costs.
  - **Label embedding strategy:** Label+Keywords is simpler but requires manual keyword curation. Explicit-Mentions is automated but depends on dataset quality and explicit label mentions being present in training articles.
  - **Model selection:** Larger models achieve highest F1 but with severe precision-recall imbalance. Smaller models offer better balance at lower cost.

- **Failure signatures:**
  - **High recall, low precision:** Models over-generate labels (Gemini-1.5-Pro: 91.8% recall, 46.3% precision). Indicates threshold miscalibration or training bias toward coverage over specificity.
  - **Language-specific fine-tuning shows minimal gain:** BanglaLlama variants show marginal improvement over base LLaMA. Suggests architectural factors or training methodology matter more than domain-specific pre-training.
  - **Large models underperform:** Mixtral-56B (F1=0.305) and Qwen-1.5-72B (F1=0.429) fall far below mid-sized models. Check for MoE routing failures or insufficient multilingual pre-training.

- **First 3 experiments:**
  1. **Establish encoder baseline:** Run LaBSE with Explicit-Mentions label embedding on 100-article subset. Compute per-label F1 to identify which of the 21 categories have highest/lowest performance.
  2. **Ablate threshold sensitivity:** For LaBSE, sweep similarity threshold from 0.3 to 0.8 in 0.05 increments. Plot precision-recall curve to determine optimal operating point.
  3. **Prompt format comparison:** Test 3 prompt variations on Gemini-1.5-Flash: (a) current JSON format, (b) comma-separated labels, (c) binary yes/no per label.

## Open Questions the Paper Calls Out

- **Dataset Generalization:** The authors explicitly state that broader evaluations are necessary to determine whether findings generalize across different domains, as the entire study relies on a newly crawled dataset from a single news source.

- **Supervised Fine-Tuning Potential:** Hardware limitations prevented efficient fine-tuning, leaving unquantified the potential gains from updating model weights on Bangla-specific data.

- **Language-Specific Adaptation Failure:** BanglaLlama 3.2 (11B) and Llama 3.2 (11B) achieve close scores (0.513 and 0.481), showing minimal benefits from language-specific tuning, which is counter-intuitive.

- **Scale-Quality Relationship:** The finding