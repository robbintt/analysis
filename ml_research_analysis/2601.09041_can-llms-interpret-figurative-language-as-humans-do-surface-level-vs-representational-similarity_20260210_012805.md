---
ver: rpa2
title: 'Can LLMs interpret figurative language as humans do?: surface-level vs representational
  similarity'
arxiv_id: '2601.09041'
source_url: https://arxiv.org/abs/2601.09041
tags:
- sensible
- non-sensible
- human
- sentences
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared human and LLM interpretations of figurative
  language across six categories (idiomatic, sarcastic, emotional, funny, slang, conventional)
  using 240 sentences rated on 40 interpretive dimensions. Both humans and four LLMs
  (GPT-4, Gemma-2-9B, Llama-3.2, Mistral-7B) provided 10-point Likert ratings.
---

# Can LLMs interpret figurative language as humans do?: surface-level vs representational similarity

## Quick Facts
- **arXiv ID**: 2601.09041
- **Source URL**: https://arxiv.org/abs/2601.09041
- **Reference count**: 31
- **Primary result**: GPT-4 most closely approximated human representational patterns for figurative language, but all models struggled with context-dependent categories.

## Executive Summary
This study investigates whether large language models (LLMs) interpret figurative language similarly to humans by comparing surface-level and representational alignment across six figurative categories. Using 240 sentences rated on 40 interpretive dimensions by both humans and four LLMs (GPT-4, Gemma-2-9B, Llama-3.2, Mistral-7B), the research found that while GPT-4 showed the closest representational similarity to humans for conventional and emotional sentences, all models struggled with context-dependent categories like sarcasm and slang. Representational similarity remained below human-human consistency, indicating that current LLMs do not fully capture human-like semantic organization of figurative language.

## Method Summary
The study employed a comparative framework using 240 sentences across six figurative language categories (idiomatic, sarcastic, emotional, funny, slang, conventional). Both humans and four LLMs provided 10-point Likert ratings across 40 interpretive dimensions. Surface-level alignment was measured via Pearson correlation, while representational alignment was assessed using representational similarity analysis (RSA) based on pairwise Euclidean distances between interpretations. This dual approach allowed researchers to examine both direct rating correspondences and the underlying semantic structures of interpretations.

## Key Results
- GPT-4 demonstrated the closest representational alignment to humans, particularly for conventional and emotional figurative language
- All models showed significantly weaker representational similarity than human-human consistency
- LLMs struggled most with context-dependent categories like sarcasm, slang, and idiomacy
- Llama-3.2 showed the weakest alignment across all figurative language types

## Why This Works (Mechanism)
The study's dual measurement approach (surface-level correlation and representational similarity) captures both direct rating correspondences and the underlying semantic structures that humans use to organize figurative language interpretations. By using RSA with Euclidean distances, the research reveals whether models organize figurative meanings in ways that mirror human cognitive structures, not just whether they produce similar ratings. The 40-dimensional interpretive framework provides granular measurement of how humans and models process different aspects of figurative language simultaneously.

## Foundational Learning

**Representational Similarity Analysis (RSA)**: A method for comparing the representational geometries of different systems by analyzing the similarity structure of their internal representations. Needed to assess whether models organize semantic information similarly to humans beyond simple correlation. Quick check: verify that distance matrices between all sentence pairs preserve relative similarity rankings across systems.

**Surface-level vs. Representational Alignment**: Surface-level measures direct correspondence between ratings, while representational alignment examines whether the underlying semantic structures are similar. Needed to distinguish between superficial agreement and genuine semantic understanding. Quick check: ensure Pearson correlation and RSA yield complementary insights about model-human alignment.

**Interpretive Dimension Framework**: The 40-point Likert scale across multiple interpretive dimensions captures nuanced aspects of figurative language understanding. Needed to provide granular measurement beyond binary classification. Quick check: validate that dimensions capture meaningful variance in human interpretations across different figurative categories.

## Architecture Onboarding

**Component Map**: Human ratings -> LLM ratings -> Pearson correlation -> RSA distance matrices -> Representational similarity matrices -> Category-wise alignment comparison

**Critical Path**: Sentence presentation → 40-dimension rating generation (human/LLM) → Euclidean distance calculation → RSA computation → Category-based alignment analysis

**Design Tradeoffs**: Using Likert scales provides quantitative measurement but may oversimplify nuanced interpretations; RSA assumes linear relationships that may not capture complex semantic structures; focusing on four LLM architectures limits generalizability

**Failure Signatures**: Poor alignment in context-dependent categories (sarcasm, slang, idiomacy) indicates models rely more on surface patterns than pragmatic inference; lower representational similarity than human-human consistency suggests models lack full semantic organization; model-specific weaknesses (Llama-3.2) reveal architectural differences in figurative language processing

**First 3 Experiments**: 1) Replicate with alternative distance metrics (cosine similarity) in RSA to test robustness of representational findings; 2) Test additional LLM architectures to assess generalizability across model types; 3) Apply the framework to other figurative language categories not covered in the original study

## Open Questions the Paper Calls Out
None

## Limitations
- Small corpus of 240 sentences may not capture full diversity of figurative language use
- Likert-scale ratings introduce potential subjectivity and cultural bias
- Focus on four specific LLM architectures may not represent broader landscape
- Euclidean distance assumptions may not accurately reflect complex semantic structures

## Confidence

**High confidence**: GPT-4 shows closest representational alignment to humans for conventional and emotional sentences

**Medium confidence**: All models struggle with context-dependent categories like sarcasm and slang

**Low confidence**: Generalizability of representational similarity results to other LLM architectures or figurative language types

## Next Checks
1. Replicate the study with a larger and more diverse corpus of figurative language sentences, including wider range of cultural contexts and speaker backgrounds
2. Conduct comparison study using alternative distance metrics in RSA (e.g., cosine similarity) to assess robustness of representational alignment findings
3. Extend analysis to include broader range of LLM architectures and fine-tuned models to evaluate whether representational similarity patterns hold across different model types and training approaches