---
ver: rpa2
title: Explaining deep learning for ECG using time-localized clusters
arxiv_id: '2509.15198'
source_url: https://arxiv.org/abs/2509.15198
tags:
- clusters
- explanations
- figure
- uncertainty
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a novel interpretability method for deep learning
  models applied to ECG analysis, addressing the challenge of understanding and trusting
  AI-driven diagnostics. The proposed method extracts time-localized clusters from
  internal network representations, segmenting the ECG according to learned characteristics
  and quantifying uncertainty.
---

# Explaining deep learning for ECG using time-localized clusters

## Quick Facts
- arXiv ID: 2509.15198
- Source URL: https://arxiv.org/abs/2509.15198
- Reference count: 38
- This work presents a novel interpretability method for deep learning models applied to ECG analysis, addressing the challenge of understanding and trusting AI-driven diagnostics.

## Executive Summary
This paper introduces a post-hoc interpretability method for 1D-CNNs analyzing ECG signals. The method extracts time-localized clusters from internal network representations, segmenting the ECG according to learned characteristics and quantifying uncertainty. Applied to both classification and regression tasks, it provides intuitive visualizations and structured explanations, revealing how different waveform regions contribute to model predictions. The approach demonstrates effectiveness in uncovering physiologically meaningful patterns and model behaviors, with strong correlations between cluster assignments and diagnostic labels.

## Method Summary
The method applies K-means clustering to feature activations from the last three residual blocks of a 1D-ResNet encoder. These activations are upsampled to the input time dimension, L2-normalized per timestep, scaled by depth-dependent factors, and concatenated across channels. For each ECG timestep, the method computes soft assignment probabilities to K=20 clusters, deriving both hard segmentation (argmax) and uncertainty (entropy). The cluster proportions serve as compressed representations that can recover model predictions, demonstrating that the clustering preserves decision-relevant features.

## Key Results
- Cluster assignments align with physiological ECG key points (P-wave, QRS, T-wave) and specialize in specific diagnostic categories
- Uncertainty heatmaps reveal different information than gradient-based importance maps, highlighting representation ambiguity
- Random Forest classifiers trained on cluster proportions achieve accuracy comparable to the original 1D-ResNet models
- The method is robust across K values from 10-50 and training set sizes from 50-2,198 samples

## Why This Works (Mechanism)

### Mechanism 1: Latent State Quantization via Clustering
If a 1D-CNN encoder learns robust features for ECG classification, then clustering these feature activations using k-means approximates a hidden Markov model, revealing physiologically meaningful "states" (e.g., QRS complex, T-wave) without explicit supervision. The method extracts feature maps from the encoder's final residual blocks, upsamples them to input time dimension, and concatenates them. K-means clusters these high-dimensional vectors into K centroids, where each centroid becomes a learned "state" representing specific waveform morphology. The core assumption is that Euclidean distance in normalized latent space correlates with semantic similarity in the input signal.

### Mechanism 2: Uncertainty as a Proxy for Representation Ambiguity
The entropy of soft cluster assignment probabilities serves as a distinct signal of model confidence compared to gradient-based importance (saliency), specifically highlighting areas of internal representation conflict or data ambiguity. Instead of hard assignment, the method computes probability of each timestep belonging to each cluster based on distance to centroids (softmax). High entropy suggests the encoder sees features of multiple states simultaneously, reflecting meaningful clinical ambiguity rather than clustering artifacts.

### Mechanism 3: Sufficient Information Encoding in Cluster Proportions
The relative frequency of time-localized clusters captures sufficient diagnostic information to recover the original model's predictions, implying the clustering preserves decision-relevant features. By calculating proportions of time-steps assigned to each cluster across a full ECG signal, the method creates a fixed-dimensional summary vector. A classifier trained solely on these summaries can predict labels with accuracy comparable to the end-to-end deep model, suggesting temporal ordering is less critical than aggregate morphology presence.

## Foundational Learning

- **1D Convolutions and Receptive Fields**: The method relies on extracting activations from specific residual blocks (layers 3, 4, 5). Deeper layers have larger receptive fields, capturing longer-range temporal context (full heartbeat vs. single spike). Quick check: Why did authors choose last three blocks rather than first layer? (Answer: Needed receptive field large enough to capture full waveform context).

- **K-Means Clustering and Soft Assignment**: The core is grouping similar neural activations. Distinguish between hard assignment (argmax, used for visualization) and soft assignment (probabilities, used for uncertainty/entropy). Quick check: How is uncertainty metric u mathematically derived from clustering? (Answer: Entropy of probability distribution over cluster centroids for a specific time point).

- **Upsampling/Resizing in Deep Learning**: Internal feature maps are downsampled compared to input ECG. The method requires aligning clusters back to original signal to say "this specific millisecond is this cluster." Quick check: In Algorithm 1, why is linear upsample(z, D) necessary before clustering? (Answer: To align internal representation R with input timestamps so clusters map directly to ECG signal regions).

## Architecture Onboarding

- **Component map**: Base Model (1D-ResNet) -> Feature Extractor (hooks into blocks 3,4,5) -> Processor (upsampler + L2 normalizer + weighting scalar) -> Explainer (K-Means -> Softmax -> Entropy/Argmax)

- **Critical path**: Accurate upsampling of feature maps r_i to input dimension D is critical. Misalignment breaks time-localization logic. Normalization prevents deeper layers with larger activation magnitudes from dominating clustering.

- **Design tradeoffs**: Number of Clusters (K) - performance plateaus after K=20; lower K is interpretable but coarse, higher K offers granularity but risks overfitting. Post-hoc vs Inline - applied after training, doesn't improve performance, only interpretability. Layer Selection - using only last layer might lose mid-level features, using too many introduces computational overhead and noise.

- **Failure signatures**: "Checkerboard" Artifacts from incorrect upsampling, Dominant Cluster from imbalanced training data, High Global Uncertainty from under-trained encoder or non-discriminative layers.

- **First 3 experiments**: 
  1. Sanity Check: Train RF on cluster proportions vs raw signal; if accuracy is significantly lower, clustering K or layer selection is losing too much information.
  2. Visual Alignment: Run explanation on "Normal" ECG; verify cluster boundaries align with P-waves, QRS complexes, and T-waves.
  3. Hyperparameter Sweep: Vary K (5,10,20,50) and plot "Elbow Curve" of cluster-predictor accuracy to find stable plateau for your dataset.

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise relationship between the entropy-based uncertainty of the encoder's representations and gradient-based feature importance (e.g., Grad-CAM), and can they be mathematically integrated? The authors note the relationship between uncertainty of explanations and feature importance is not clear, providing only qualitative visual comparisons without unified theoretical model. Resolution would require formal analysis quantifying correlation and divergence across various pathologies and layer depths.

### Open Question 2
Does the observed increase in representation uncertainty with patient age correlate with measurable degradation in model robustness or predictive calibration? The paper visualizes uncertainty increase with age but doesn't quantify direct impact on error rates or calibration for older cohorts. Resolution would require calibration plots and error analysis stratified by age groups.

### Open Question 3
Does integration of time-localized cluster explanations into clinical workflows significantly improve diagnostic accuracy or inter-rater agreement among cardiologists compared to standard saliency maps? While method is shown to be quantitatively valid, practical utility and cognitive load for medical experts compared to existing explainability standards remain unverified. Resolution would require randomized user study measuring diagnostic speed and accuracy.

## Limitations
- The "bag-of-clusters" approach may lose critical temporal information for sequence-dependent pathologies like specific arrhythmias
- The uncertainty mechanism lacks ablation studies comparing against established uncertainty quantification methods
- The core assumption about Euclidean distance correlation with semantic similarity remains untested quantitatively across diverse ECG morphologies

## Confidence

- **Mechanism 1 (Latent State Quantization)**: Medium-High - Strong visual evidence but relies on untested Euclidean correlation assumption
- **Mechanism 2 (Uncertainty Proxy)**: Medium - Internal validation shows differences from saliency, but lacks comparison to established uncertainty methods
- **Mechanism 3 (Sufficient Information Encoding)**: High - Direct empirical evidence via RF classifier performance matching base model

## Next Checks

1. **Temporal Dependency Test**: Apply method to sequence-dependent pathology (e.g., specific arrhythmia) and verify whether cluster proportions alone capture diagnostic signal or if temporal ordering is critical.

2. **Uncertainty Method Comparison**: Implement alternative uncertainty quantification (e.g., Monte Carlo dropout) on same ECG samples and compare correlation patterns with proposed cluster-entropy method.

3. **Latent Space Correlation Validation**: Quantitatively measure correlation between Euclidean distance in latent space and ground-truth morphological similarity (e.g., using dynamic time warping distance between ECG segments assigned to same cluster).