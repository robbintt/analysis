---
ver: rpa2
title: Global Group Fairness in Federated Learning via Function Tracking
arxiv_id: '2503.15163'
source_url: https://arxiv.org/abs/2503.15163
tags:
- fairness
- learning
- group
- local
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ensuring global group fairness
  in federated learning, where the goal is to train a model that treats different
  demographic groups fairly across the entire population without exchanging sensitive
  data between clients. The authors propose a novel approach based on Maximum Mean
  Discrepancy (MMD) regularization to achieve this goal.
---

# Global Group Fairness in Federated Learning via Function Tracking

## Quick Facts
- **arXiv ID**: 2503.15163
- **Source URL**: https://arxiv.org/abs/2503.15163
- **Reference count**: 40
- **Key outcome**: Novel function-tracking scheme enables global group fairness in federated learning with small communication overhead, performing comparably to centralized training

## Executive Summary
This paper addresses the challenge of achieving global group fairness in federated learning, where clients must ensure fairness across the entire population without sharing sensitive data. The authors propose a Maximum Mean Discrepancy (MMD) regularization approach that approximates global fairness through a function-tracking scheme. Their method shares small sampled prediction sets between rounds to enable clients to compute unbiased fairness gradients, while seamlessly integrating into existing federated learning algorithms like FedAvg with rigorous convergence guarantees.

## Method Summary
The method introduces an MMD-regularized federated learning algorithm where fairness is measured globally across all clients rather than locally. A key innovation is the function-tracking scheme: at each communication round, the server samples prediction sets Y₀ and Y₁ from the current global model's predictions on protected groups, then broadcasts these (typically 50-100 scalar values) to selected clients. Clients use these samples to compute local fairness gradients during their SGD updates. The approach handles differential privacy through kernel convolution interpretation and maintains convergence guarantees despite biased gradient estimates during local updates.

## Key Results
- Achieves comparable performance to centralized training on global datasets (Communities & Crime, COMPAS)
- Outperforms local fairness methods that fail due to Simpson's paradox when client biases oppose globally
- Demonstrates theoretical convergence rate O(1/T + 1/(T+1)^(1/2) + 1/(T+1)^(2/3)) matching standard FedAvg
- Shows differential privacy can be implemented as kernel modification with intuitive bandwidth interpretation

## Why This Works (Mechanism)

### Mechanism 1: Client-Indecomposable MMD Fairness Regularizer
Global fairness metrics like MMD cannot be decomposed as weighted sums over local client objectives. Instead, an auxiliary function f(θ; Y₀, Y₁) is constructed using sampled prediction sets, whose gradient provides an unbiased estimator of the fairness gradient. This impossibility result for client decomposition is proven mathematically and forms the theoretical foundation for the function-tracking approach.

### Mechanism 2: Function-Tracking with Resampling Bias Control
Sharing small sampled prediction sets (Y₀, Y₁) enables clients to compute local fairness gradients that are unbiased at communication rounds but become biased during local updates. The key insight is that resampling at each round prevents bias accumulation across rounds. This allows the method to maintain convergence while incurring minimal communication overhead.

### Mechanism 3: Differential Privacy as Kernel Convolution
Adding DP noise to shared predictions is equivalent to replacing the original kernel κ with a convolved kernel κ̃ = κ * μDP. For Gaussian mechanisms with Gaussian kernels, this simply increases kernel bandwidth. This interpretation provides an intuitive framework for analyzing privacy-utility tradeoffs in kernel-based fairness regularization.

## Foundational Learning

- **Maximum Mean Discrepancy (MMD)**: Core metric measuring unfairness as distance between prediction distributions. Understanding kernel-based distribution distances is essential to interpret what the regularizer optimizes. *Quick check*: Given two sample sets {xi} and {yj}, can you compute the U-statistic estimator for MMD²?

- **Federated Averaging (FedAvg)**: Algorithm 1 extends FedAvg; understanding local SGD, client sampling, and model aggregation is prerequisite to seeing where fairness components integrate. *Quick check*: In FedAvg with E local epochs and client subsampling rate S/K, what is the relationship between local and global step sizes?

- **Statistical Parity vs. Other Fairness Notions**: The paper focuses on statistical parity but claims extension to equal opportunity, equalized odds, etc. Understanding these distinctions clarifies when the method applies. *Quick check*: For equal opportunity (vs. statistical parity), what conditioning sets Cj replace {X × Y} in Table 1?

## Architecture Onboarding

- **Component map**:
  Server: Global model θt → Prediction sampler (generates Yt₀, Yt₁) → Weight estimator (computes αka) → Aggregator (weighted average of client updates)
  Client k: Local dataset (X, Y, A) ~ Pk → Local loss Ek[L(hθ(X), Y)] → Fairness term fk(θ; Yt₀, Yt₁) using received Y sets → Local SGD for E epochs

- **Critical path**: One-time setup: Compute αka via secure aggregation. Per-round: Sample Yt₀, Yt₁ → broadcast to selected clients → local SGD with combined loss → aggregate updates. Failure-critical: Wrong αka estimates cause biased fairness gradients; too small Y sets cause variance explosion.

- **Design tradeoffs**: Communication vs. variance (larger |Y| reduces gradient variance but increases overhead); fairness vs. accuracy (controlled by λ); local epochs E vs. convergence (more local epochs increase stale Y bias).

- **Failure signatures**: Bias accumulation if Y sets aren't resampled; local fairness decreases but global fairness doesn't (Simpson's paradox); numerical instability with small kernel bandwidths.

- **First 3 experiments**:
  1. Replicate Figure 2 with K=10 clients, varying λ to confirm local fairness fails when client biases are heterogeneous but cancel globally
  2. Vary |Yt₀|, |Yt₁| ∈ {20, 50, 100, 200} on Communities & Crime to verify 50-100 is sufficient
  3. Track train/test loss over 100 rounds to verify objective (4) is minimized and compare convergence to Theorem 5.1 prediction

## Open Questions the Paper Calls Out

- **Acceleration integration**: Can momentum acceleration or variance reduction techniques be effectively integrated to achieve faster convergence rates? The authors note this for future work regarding the biased nature of fairness gradient estimators.

- **Failure modes on biased distributions**: What are specific failure modes when applied to heavily biased distributions? The conclusion warns of unintended effects but doesn't characterize them experimentally or theoretically.

- **Adaptive sample size tuning**: How should the size of sample sets Y₀ and Y₁ be adaptively tuned to minimize communication overhead while preventing performance collapse under high regularization strengths? Fixed sample sizes are used despite variance-sensitivity to λ.

## Limitations

- Kernel formulation for energy distance MMD is not fully specified (distance kernel κ assumed but exact form not given)
- Neural network architecture details (activation functions, output layer, loss specification) remain implicit
- Extension claims to fairness notions beyond statistical parity lack experimental validation

## Confidence

- **High Confidence**: Mechanism 1 (client-indecomposable MMD) and Mechanism 3 (DP as kernel convolution) have rigorous mathematical proofs with clear conditions
- **Medium Confidence**: Mechanism 2 (function-tracking with bias control) - while convergence proof exists, empirical validation of bias-reset effectiveness across heterogeneous settings is limited
- **Low Confidence**: Extension claims to fairness notions beyond statistical parity - theoretical framework provided but no experimental validation shown

## Next Checks

1. **Synthetic data replication**: Recreate Figure 2 with K=10 clients to demonstrate Simpson's paradox failure of local fairness methods
2. **Gradient variance sensitivity**: Systematically vary |Y₀|, |Y₁| ∈ {20, 50, 100, 200} on real datasets to verify paper's claim about sufficient sample sizes
3. **DP-utility tradeoff validation**: Implement Gaussian mechanism with Gaussian kernel and verify Corollary 5.3's claim that noise simply increases kernel bandwidth