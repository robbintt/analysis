---
ver: rpa2
title: 'From Information to Generative Exponent: Learning Rate Induces Phase Transitions
  in SGD'
arxiv_id: '2510.21020'
source_url: https://arxiv.org/abs/2510.21020
tags:
- learning
- complexity
- sample
- update
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how learning rate choices affect sample complexity\
  \ in gradient-based algorithms for learning Gaussian single-index models. The authors\
  \ develop a general framework for analyzing online iterative algorithms where the\
  \ update oracle includes both correlational and non-correlational terms modulated\
  \ by a learning rate \u03B7."
---

# From Information to Generative Exponent: Learning Rate Induces Phase Transitions in SGD

## Quick Facts
- arXiv ID: 2510.21020
- Source URL: https://arxiv.org/abs/2510.21020
- Reference count: 40
- Key outcome: Learning rate choices induce phase transitions in sample complexity for Gaussian single-index models, shifting from information exponent $p$ to generative exponent $p^*$ regimes.

## Executive Summary
This paper establishes a general framework for analyzing online gradient-based algorithms learning Gaussian single-index models, revealing how learning rate choices can fundamentally alter sample complexity requirements. The authors demonstrate that algorithms with non-correlational update terms exhibit phase transitions: below a critical learning rate threshold, sample complexity scales with the information exponent $p$ (matching standard online SGD bounds), while above it, complexity scales with the potentially smaller generative exponent $p^*$. This framework is applied to batch reuse SGD and a novel alternating SGD algorithm that achieves near-linear sample complexity for certain targets without requiring batch reuse. The theoretical analysis extends to deeper networks, showing that network depth plays an analogous role to polynomial degree in controlling sample complexity.

## Method Summary
The authors develop a unified framework for analyzing online iterative algorithms where updates include both correlational and non-correlational terms modulated by learning rate $\eta$. They study three algorithms: standard online SGD, batch reuse SGD (which reuses samples with different learning rates), and alternating SGD (which updates layers sequentially with different learning rates). The framework leverages Hermite polynomial expansions of the target link function and analyzes alignment dynamics between student and teacher weights. Sample complexity is characterized through phase transitions in $\eta$ that shift the governing exponent from information ($p$) to generative ($p^*$) regimes.

## Key Results
- Batch reuse SGD and alternating SGD exhibit phase transitions in sample complexity as learning rate $\eta$ varies
- Below critical threshold $\eta \sim d^{-1}$, sample complexity follows $O(d^{p-1})$ bound; above it, follows $O(d^{p^*-1})$ bound
- Alternating SGD achieves near-linear sample complexity for certain targets without batch reuse
- Depth $D$ in deeper networks plays analogous role to polynomial degree in batch reuse SGD

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing the learning rate of non-correlational terms can induce a phase transition in sample complexity, shifting the governing difficulty from the Information Exponent ($p$) to the Generative Exponent ($p^*$).
- **Mechanism:** In algorithms like batch reuse or alternating SGD, the learning rate $\eta$ scales higher-order terms (e.g., $y^2$) in the effective update oracle $\psi_\eta$. When $\eta$ exceeds a critical threshold, these terms—often dependent on a lower Hermite coefficient $p^*$—dominate the gradient dynamics, reducing the required sample size from $O(d^{p-1})$ to $O(d^{p^*-1})$.
- **Core assumption:** Assumption 3.2 (alignment of student activation $\sigma$ with target $\sigma_*$) and sufficient learning rate stability ($\gamma \lesssim \max \mu_i d^{-(i/2 \lor 1)}$).
- **Evidence anchors:**
  - [abstract] "We demonstrate that, in certain cases, there is a phase transition from an 'information exponent regime' with small learning rate to a 'generative exponent regime' with large learning rate."
  - [Section 4.2] Equation 4.6 explicitly derives the phase transition threshold $\eta \le d^{\frac{[(p_j-1)\vee 1]-[(p_i-1)\vee 1]}{2(j-i)-1}}$.
  - [corpus] Weak direct validation; neighboring papers on SGD limits focus on scaling limits rather than LR-induced exponent shifts.
- **Break condition:** If $\eta$ is set too small ($\eta \ll d^{-1}$ typically), the higher-order terms are suppressed, and the algorithm defaults to the standard online SGD complexity governed by $p$.

### Mechanism 2
- **Claim:** Alternating SGD (layer-wise training) implicitly transforms a correlational query into a non-correlational query by injecting powers of the label $y$ into the first layer update.
- **Mechanism:** By updating the second layer weight $a$ first using $\eta$ and then immediately using this updated $\tilde{a}$ for the first layer update with $\gamma$, the effective update rule acquires a term proportional to $y^2 \sigma(z)\sigma'(z)$. This allows the algorithm to leverage the information exponent of $\sigma_*^2$ rather than just $\sigma_*$.
- **Core assumption:** The correlation between the student's activation derivatives and the target's squared Hermite coefficients (e.g., $u_{i-1}(\sigma\sigma')u_i(\sigma_*^2) \neq 0$).
- **Evidence anchors:**
  - [Section 4.3] "Given the second-layer gradient update... the update for $w$ is $\psi(y,z) = ya\sigma'(z) + \eta y^2\sigma(z)\sigma'(z)$."
  - [Figure 1] Visualization of the phase transition where alignment improves significantly once $\eta$ crosses a threshold.
  - [corpus] Not applicable (specific algorithm introduced in this paper).
- **Break condition:** If the first and second layers are updated simultaneously or on separate batches, the sequential dependency generating the $y^2$ term is broken.

### Mechanism 3
- **Claim:** The "curse of information exponent" is linked to the Correlational Statistical Query (CSQ) class, but large learning rates allow SGD to exceed CSQ limitations without changing the loss function.
- **Mechanism:** Standard SGD with small LR behaves like a CSQ algorithm (querying $y h(x)$). The framework shows that specific algorithmic structures (batch reuse, alternating layers) with large $\eta$ effectively query $h(x, y)$, placing them in the broader Statistical Query (SQ) class which is bounded by the potentially smaller Generative Exponent.
- **Core assumption:** The learning rate acts as a hyperparameter that scales the strength of these non-correlational queries.
- **Evidence anchors:**
  - [Section 1] "CSQ lower bounds... depends on the information and the latter [SQ] depends on the generative exponent."
  - [Section 3] "Only $\eta$ will induce the aforementioned phase transition of interest."
- **Break condition:** If the link function $\sigma_*$ is not learnable by SQ algorithms (rare for polynomial targets) or if noise overwhelms the $y^k$ terms at large $\eta$.

## Foundational Learning

- **Concept: Information Exponent ($p$)**
  - **Why needed here:** It is the baseline difficulty metric for the target function $\sigma_*$. You must understand that $p$ is the index of the first non-zero coefficient in the Hermite expansion to grasp what the "hard" regime looks like.
  - **Quick check question:** If $\sigma_*(z) = He_3(z)$, what is its information exponent? (Answer: 3).

- **Concept: Generative Exponent ($p^*$)**
  - **Why needed here:** It represents the target complexity after optimal transformations. Understanding $p^* \le p$ explains why the "fast" regime is theoretically possible.
  - **Quick check question:** Why does $GE(\sigma) \le IE(\sigma)$ always hold?

- **Concept: Two-Timescale Dynamics**
  - **Why needed here:** The "Alternating SGD" mechanism relies on updating one layer (fast time-scale $\eta$) before another (slow time-scale $\gamma$) to create the specific label transformation.
  - **Quick check question:** In Algorithm 2, which layer uses the learning rate $\eta$ and which uses $\gamma$?

## Architecture Onboarding

- **Component map:**
  - Target: Single-index model $y = \sigma_*(\langle x, \theta^* \rangle)$
  - Student: Two-layer network $f(x) = \frac{1}{N}\sum a_j \sigma(\langle x, w_j \rangle + b_j)$
  - Optimizer: Online SGD variants (Standard, Batch Reuse, or Alternating)
  - Metrics: Alignment $\langle \theta^*, w \rangle$ (Weak Recovery)

- **Critical path:**
  1. Initialize weights $w$ uniformly on the sphere (alignment $\approx d^{-1/2}$)
  2. Select the algorithm variant (e.g., Alternating SGD)
  3. Tune the critical learning rate $\eta$ based on the dimension $d$ and expected exponents
  4. Run online iterations (fresh samples each step)
  5. Monitor alignment $\kappa(t)$; verify it crosses the weak recovery threshold $1/\text{polylog}(d)$

- **Design tradeoffs:**
  - **Batch Reuse:** Good for maximizing $p^*$ extraction but requires storing/handling samples twice
  - **Alternating SGD:** Fully online (no sample storage) but relies on the student architecture having a specific two-layer structure where the second layer can be "pre-updated"
  - **Learning Rate:** Large $\eta$ is necessary for the phase transition but risks divergence (normalization error)

- **Failure signatures:**
  - **Stagnation:** Alignment $\kappa(t)$ remains at $O(d^{-1/2})$. *Likely cause:* $\eta$ is too small (stuck in Information Exponent regime) or $\gamma$ is too small
  - **Divergence:** Alignment fluctuates wildly or norm explodes. *Likely cause:* $\gamma$ is too large relative to the noise or $\eta d \gg 1$
  - **No Transition:** Increasing $\eta$ does not speed up recovery. *Likely cause:* $p^* = p$ (no statistical advantage to be gained) or student activation $\sigma$ is misaligned with target

- **First 3 experiments:**
  1. **Baseline Check (Online SGD):** Train on $He_3$ target with vanilla SGD. Verify sample complexity scales as $\sim d^{p-1}$
  2. **Batch Reuse Phase Transition:** Train on $He_3$ using Algorithm 1. Sweep $\eta \in [d^{-1}, 1]$. Plot sample complexity vs. $\eta$ to observe the transition from $O(d^2)$ to $O(d)$ (if $p^*$ allows)
  3. **Alternating SGD Validation:** Implement Algorithm 2. Compare sample complexity against Batch Reuse. Specifically, check if Alternating SGD achieves the "fast" regime without holding onto data samples

## Open Questions the Paper Calls Out

- **Extension to multi-index models:** The current framework relies heavily on the rotational symmetry of single-index models. Extending these phase transition results to multi-index models where the target depends on multiple latent directions would require new theoretical tools to handle the higher-dimensional latent subspaces.

- **Non-Gaussian input distributions:** The analysis depends critically on Gaussian input properties and Hermite polynomial expansions. Investigating how phase transitions behave under different input distributions (e.g., sub-Gaussian, heavy-tailed) would reveal the robustness of these learning rate effects and potentially lead to broader applications.

- **General activation functions for deep alternating SGD:** The current deep network analysis requires restrictive assumptions on target link functions. Finding activation strategies that guarantee weak recovery without oracle knowledge of the target would make the approach more practical and widely applicable.

## Limitations

- The framework assumes Gaussian input distributions and single-index model structure, limiting applicability to real-world non-Gaussian data
- The alternating SGD algorithm's effectiveness is tightly coupled to specific two-layer architecture and sequential update structure
- The theoretical analysis relies heavily on Hermite polynomial expansions which may not capture full complexity of realistic data distributions

## Confidence

- **High Confidence:** The core mechanism of learning rate-induced phase transitions in batch reuse SGD is well-supported by theoretical derivations and explicit threshold formulas
- **Medium Confidence:** The alternating SGD mechanism is theoretically sound but relies on specific architectural assumptions that may not hold in practice
- **Medium Confidence:** The CSQ-SQ framework connection provides valuable conceptual insight but practical implications for algorithm design are less direct

## Next Checks

1. **Robustness to Activation Mismatch:** Test alternating SGD with student activation σ ≠ σ* to verify whether the phase transition still occurs and quantify the impact on alignment threshold

2. **Multi-Pass Stability Analysis:** For batch reuse SGD, systematically study the trade-off between learning rate magnitude and stability as the number of passes increases, particularly for non-polynomial link functions

3. **Extension to Multi-Layer Networks:** Implement a depth-D version of alternating SGD where each layer is updated sequentially with appropriate learning rates, and empirically verify whether depth D plays an analogous role to polynomial degree in controlling sample complexity