---
ver: rpa2
title: Semantic search for 100M+ galaxy images using AI-generated captions
arxiv_id: '2512.11982'
source_url: https://arxiv.org/abs/2512.11982
tags:
- images
- image
- galaxy
- search
- astronomical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work demonstrates how Vision-Language Models (VLMs) can generate\
  \ informative, free-form descriptions of galaxy images that enable semantic search\
  \ at scale without requiring human labels. By using GPT-4.1-mini to caption millions\
  \ of unlabeled galaxy images and aligning a frozen astronomy foundation model with\
  \ these synthetic captions via contrastive learning, the authors create AION-Search\u2014\
  a zero-shot semantic retrieval system."
---

# Semantic search for 100M+ galaxy images using AI-generated captions

## Quick Facts
- **arXiv ID**: 2512.11982
- **Source URL**: https://arxiv.org/abs/2512.11982
- **Reference count**: 25
- **Key outcome**: AION-Search achieves nDCG@10 scores of 0.941, 0.554, and 0.180 for spirals, mergers, and gravitational lenses respectively, demonstrating semantic retrieval capabilities for astronomical image archives at scale

## Executive Summary
This work presents AION-Search, a zero-shot semantic retrieval system for astronomical images that uses Vision-Language Models (VLMs) to generate free-form captions for unlabeled galaxy images. By aligning a frozen astronomy foundation model with these synthetic captions through contrastive learning, the system enables semantic search without requiring human-annotated labels. The approach significantly outperforms similarity-based baselines on three scientific targets: spirals, mergers, and gravitational lenses, with particular promise for discovering rare objects like gravitational lenses through VLM-based re-ranking.

## Method Summary
The approach combines VLMs and contrastive learning to create a semantic search system for astronomical images. First, GPT-4.1-mini generates descriptive captions for millions of unlabeled galaxy images. These synthetic captions are then used to align a frozen astronomy foundation model (AION-Foundation) with text representations through contrastive learning, creating a joint embedding space. The resulting AION-Search model enables zero-shot retrieval by matching galaxy images to user queries based on their semantic content rather than pixel similarity. The system demonstrates scalability to 140 million images and generalizability to other scientific image archives where manual labeling is impractical.

## Key Results
- AION-Search achieves nDCG@10 scores of 0.941, 0.554, and 0.180 for spirals, mergers, and gravitational lenses respectively, significantly outperforming similarity-based baselines
- VLM-based re-ranking nearly doubles the number of rare gravitational lenses found in the top-100 results compared to baseline methods
- The approach successfully scales to 140 million galaxy images while maintaining retrieval performance

## Why This Works (Mechanism)
The method works by leveraging VLMs to generate semantically rich captions that capture scientific features humans would use to describe astronomical phenomena. These captions bridge the gap between visual features learned by the astronomy foundation model and the textual descriptions needed for semantic search. The contrastive learning framework aligns the frozen astronomy model with text embeddings in a shared space, enabling direct comparison between images and queries without requiring labeled training data. This zero-shot approach bypasses the labeling bottleneck that limits traditional supervised methods, while the foundation model provides domain-specific feature extraction capabilities that generic vision models lack.

## Foundational Learning
**Vision-Language Models (VLMs)**: AI models that can process both images and text to generate descriptive captions or answer questions about visual content. Needed because they provide a scalable way to generate semantic descriptions of astronomical images without human annotation. Quick check: Can the VLM generate scientifically accurate descriptions of astronomical phenomena?

**Contrastive Learning**: A training approach that learns to bring similar items (images and their captions) closer together in embedding space while pushing dissimilar items apart. Essential for aligning the frozen astronomy model with text representations without fine-tuning the foundation model. Quick check: Does the alignment create meaningful associations between astronomical features and their textual descriptions?

**Foundation Models**: Large-scale models pre-trained on vast datasets that serve as starting points for downstream tasks. The astronomy foundation model provides domain-specific feature extraction that generic vision models cannot match. Quick check: Does the foundation model capture relevant astronomical features like spiral arms or merger signatures?

**Zero-shot Retrieval**: Search capability that operates without task-specific training data by leveraging semantic understanding learned during pre-training. Critical for scaling to millions of unlabeled images where supervised learning would be impractical. Quick check: Can the system retrieve relevant images for novel scientific queries it hasn't seen during training?

## Architecture Onboarding

**Component Map**: User Query -> Text Encoder -> Joint Embedding Space <- Frozen Astronomy Foundation Model <- Galaxy Image

**Critical Path**: Query text → text encoder → joint embedding → similarity scoring → ranked results

**Design Tradeoffs**: The system trades perfect caption accuracy for scalability by using synthetic rather than human-generated captions. While this introduces potential errors, it enables processing millions of images that would be infeasible to label manually. The frozen foundation model preserves astronomical domain knowledge but limits fine-tuning capabilities.

**Failure Signatures**: Poor retrieval performance on rare classes suggests caption quality issues or insufficient alignment between visual features and textual descriptions. High similarity scores for visually similar but scientifically different objects indicate the embedding space may not capture discriminative scientific features effectively.

**First Experiments**: 1) Test caption generation quality on a small labeled subset with expert review, 2) Validate retrieval performance on a held-out test set with ground truth labels, 3) Evaluate retrieval for additional rare astronomical phenomena beyond the three studied classes.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies entirely on synthetic captions without human validation of caption quality or scientific accuracy
- Performance on rare object detection (gravitational lenses with nDCG@10 of 0.180) remains limited despite improvements over baselines
- The scalability to 140 million images is theoretically supported but not empirically demonstrated at that scale

## Confidence
- **High confidence**: The zero-shot retrieval framework using frozen astronomy foundation models and synthetic captions is technically sound and implementation details are clearly specified
- **Medium confidence**: Retrieval performance improvements over similarity-based baselines are demonstrated, but absolute performance on rare object detection remains limited
- **Medium confidence**: Scalability claim to 140 million images is supported by methodology, though actual performance at this scale is not empirically demonstrated

## Next Checks
1. Conduct expert astronomer review of a random sample of generated captions to assess scientific accuracy and relevance for the three target classes
2. Test AION-Search on a held-out test set of images with ground truth labels to independently verify the reported nDCG@10 scores
3. Evaluate retrieval performance on additional rare astronomical phenomena beyond the three studied classes to assess generalizability of the approach