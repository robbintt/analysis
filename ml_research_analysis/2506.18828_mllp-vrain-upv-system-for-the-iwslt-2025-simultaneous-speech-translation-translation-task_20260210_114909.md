---
ver: rpa2
title: MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation
  task
arxiv_id: '2506.18828'
source_url: https://arxiv.org/abs/2506.18828
tags:
- translation
- system
- computational
- speech
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a cascade system for the IWSLT 2025 Simultaneous
  Speech Translation task that adapts pre-trained Whisper ASR and NLLB MT models for
  streaming translation of long-form speech. The system employs document-level prefix
  training and adaptive emission policies including wait-k and RALCP to manage translation
  latency while maintaining coherence.
---

# MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task

## Quick Facts
- arXiv ID: 2506.18828
- Source URL: https://arxiv.org/abs/2506.18828
- Reference count: 17
- Primary result: Lightweight adaptation of pre-trained Whisper and NLLB models achieves BLEU 31.96 with StreamLAAL latency 2.94s for simultaneous English-to-German speech translation

## Executive Summary
This paper presents a cascade system for the IWSLT 2025 Simultaneous Speech Translation task that adapts pre-trained Whisper ASR and NLLB MT models for streaming translation of long-form speech. The system employs document-level prefix training and adaptive emission policies including wait-k and RALCP to manage translation latency while maintaining coherence. Experimental results on the ACL60/60 dataset show the adapted system achieves BLEU 31.96 with StreamLAAL latency of 2.94 seconds, outperforming a naive baseline (26.10 BLEU, 3.61 latency). The work demonstrates that lightweight adaptation of strong pre-trained components can create effective simultaneous translation systems without requiring extensive in-domain parallel data or specialized end-to-end training.

## Method Summary
The proposed system follows a cascade architecture where Whisper ASR transcribes speech into text, which is then translated by an adapted NLLB MT model. The adaptation process involves training on reconstructed document-level prefixes extracted from existing text corpora (News Commentary and Europarl) to improve coherence for long-form translation. For simultaneous translation, the system implements wait-k and RALCP policies to balance translation latency and quality. The RALCP policy allows the model to emit tokens even when input is available, improving efficiency. The adaptation uses document-level prefixes rather than individual sentences to better handle discourse-level phenomena and maintain coherence across longer speech segments.

## Key Results
- Adapted system achieves BLEU score of 31.96 with StreamLAAL latency of 2.94 seconds on ACL60/60 dataset
- Outperforms naive baseline with BLEU 26.10 and StreamLAAL latency 3.61 seconds
- Demonstrates lightweight adaptation of pre-trained models can achieve competitive simultaneous translation performance without extensive in-domain parallel data

## Why This Works (Mechanism)
The system's effectiveness stems from leveraging pre-trained models' strong representations while adapting them for streaming contexts. Document-level prefix training enables the MT model to maintain coherence across longer speech segments, addressing a key challenge in simultaneous translation. The RALCP policy optimizes the trade-off between translation latency and quality by allowing token emission even when input is available, improving responsiveness without sacrificing accuracy. By avoiding end-to-end training, the approach reduces computational requirements while maintaining the strong performance characteristics of the base models.

## Foundational Learning

**Document-level prefix training**: why needed - maintains coherence across longer speech segments; quick check - compare BLEU scores with and without document-level prefixes

**RALCP (Read-After-Look-and-Check Policy)**: why needed - balances translation latency and quality in streaming scenarios; quick check - measure latency differences between RALCP and wait-k policies

**StreamLAAL metric**: why needed - evaluates both translation quality and latency simultaneously; quick check - verify that lower latency correlates with acceptable BLEU degradation

**Cascade architecture**: why needed - leverages specialized pre-trained models for ASR and MT tasks; quick check - compare performance against end-to-end simultaneous translation models

## Architecture Onboarding

**Component map**: Whisper ASR -> Document-level prefix preprocessing -> NLLB MT (adapted) -> Simultaneous translation emission (RALCP/wait-k)

**Critical path**: Speech input → Whisper transcription → Prefix extraction → NLLB translation → Token emission policy → Output translation

**Design tradeoffs**: Uses lightweight adaptation instead of full end-to-end training, reducing computational requirements but potentially limiting optimization for simultaneous translation-specific challenges

**Failure signatures**: 
- High latency with poor translation quality suggests RALCP parameters need adjustment
- Low BLEU scores despite reasonable latency may indicate insufficient prefix training data
- Sudden performance drops could indicate issues with document-level reconstruction

**3 first experiments**:
1. Evaluate baseline BLEU and latency without any adaptation to establish reference points
2. Test different document-level prefix lengths to optimize coherence vs. latency trade-off
3. Compare wait-k and RALCP policies across various latency thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the incorporation of synthetic document-level bitext data or speech data improve the robustness of this lightweight adaptation methodology for simultaneous translation?
- Basis in paper: The authors state in the conclusion that future work could "expand to see the robustness of this methodology, such as the usage of synthetic document level bitext data... or speech data."
- Why unresolved: The current study relied on reconstructed document-level information from existing text corpora (News Commentary and Europarl) rather than generating synthetic data to augment training.
- What evidence would resolve it: A comparative evaluation showing performance changes (BLEU/LAAL) when the NLLB model is fine-tuned with synthetic document-level data compared to the current Paradise-extracted dataset.

### Open Question 2
- Question: Can Large Language Models (LLMs) serve as effective all-in-one transcribers, translators, or re-scorers in a cascaded pipeline for long-form speech translation without exceeding practical computational budgets?
- Basis in paper: The authors note that LLMs show promising results for long-context usage "if computational costs can be taken into account," but they found early results "unsatisfactory for our computing budget."
- Why unresolved: The authors performed only preliminary tests on LLMs (discarding them due to cost/quality trade-offs) and did not conduct a full integration into the simultaneous streaming architecture.
- What evidence would resolve it: Experiments benchmarking LLM-based cascades against the Whisper-NLLB system, demonstrating specific latency and VRAM usage profiles that remain within production constraints.

### Open Question 3
- Question: Does the proposed cascade adaptation methodology generalize effectively to language pairs other than English-to-German?
- Basis in paper: In the Limitations section, the authors mention they restricted themselves to English-to-German but "think that our approach could be generalized to the other language pairs in the competition."
- Why unresolved: The system components, including the NLLB-3.3B model and specific prefix-training heuristics, were selected and tuned specifically for the En-De task direction.
- What evidence would resolve it: Applying the identical system architecture and adaptation pipeline to other IWSLT task directions (e.g., English-to-Japanese or English-to-Chinese) and reporting the resulting quality-latency trade-offs.

## Limitations
- Evaluation restricted to ACL60/60 dataset, limiting generalizability to other domains or language pairs
- Comparison with only a "naive baseline" lacks benchmarks against other state-of-the-art simultaneous translation systems
- Does not address potential degradation in translation quality under extreme latency constraints
- Does not explore the trade-off between coherence preservation and real-time responsiveness in depth

## Confidence
The claim that lightweight adaptation suffices for effective simultaneous translation carries **Medium confidence** given the controlled experimental conditions and limited dataset scope. The assertion that the approach works without extensive in-domain parallel data is supported by the methodology but not empirically validated through ablation studies or comparisons with data-intensive alternatives. The performance metrics (BLEU and StreamLAAL) provide quantitative evidence, though the paper does not discuss potential measurement limitations or alternative evaluation criteria for simultaneous translation quality.

## Next Checks
1. Test the system on additional simultaneous speech translation datasets with varying domain characteristics to assess robustness beyond ACL60/60
2. Conduct ablation studies comparing the adapted system against end-to-end simultaneous translation models trained from scratch on the same data
3. Evaluate performance under different latency thresholds to determine the breaking points where translation quality significantly degrades