---
ver: rpa2
title: 'GTA: Supervised-Guided Reinforcement Learning for Text Classification with
  Large Language Models'
arxiv_id: '2509.12108'
source_url: https://arxiv.org/abs/2509.12108
tags:
- uni00000013
- uni00000011
- guess
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Guess-Think-Answer (GTA) framework to
  address the efficiency-capability trade-off between supervised fine-tuning (SFT)
  and reinforcement learning (RL) for text classification with large language models.
  GTA structures the reasoning process into three stages: an initial intuitive guess
  optimized via cross-entropy loss, a reflective thinking step, and a final answer
  optimized through RL rewards.'
---

# GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models

## Quick Facts
- arXiv ID: 2509.12108
- Source URL: https://arxiv.org/abs/2509.12108
- Reference count: 11
- Primary result: GTA framework achieves superior text classification performance by integrating supervised fine-tuning with reinforcement learning through a three-stage guess-think-answer structure

## Executive Summary
This paper introduces the Guess-Think-Answer (GTA) framework to address the efficiency-capability trade-off between supervised fine-tuning (SFT) and reinforcement learning (RL) for text classification with large language models. GTA structures the reasoning process into three stages: an initial intuitive guess optimized via cross-entropy loss, a reflective thinking step, and a final answer optimized through RL rewards. The framework integrates SFT and RL in a unified training paradigm, using loss masking and gradient constraints to mitigate conflicts between the two training signals. Experiments on four text classification benchmarks demonstrate that GTA significantly accelerates convergence while outperforming both standalone SFT and GRPO baselines in accuracy and F1 scores.

## Method Summary
The GTA framework trains LLMs for text classification through a hybrid approach combining supervised fine-tuning and reinforcement learning. The model processes input text through three stages: first making an intuitive guess labeled with the ground-truth, then reasoning about the input, and finally producing an answer. During training, the guess stage receives cross-entropy loss based on the ground-truth label, while the think and answer stages receive reinforcement learning rewards based on answer correctness and format compliance. A key innovation is the use of loss masking to apply SFT only to guess tokens and RL only to think-answer tokens, combined with gradient conflict resolution that skips SFT updates when gradients point in opposite directions. The framework is implemented using ModelScope Swift for SFT and TRL for GRPO, with bfloat16 precision and DeepSpeed ZeRO Stage 2 optimization.

## Key Results
- GTA achieves F1 scores of 92.47% (Qwen2.5), 92.94% (Qwen3), and 93.36% (Llama3.2) on the Emotion dataset, surpassing other methods
- The framework demonstrates faster convergence compared to standalone GRPO while maintaining or improving final accuracy
- GTA shows robustness in reasoning, as models can correct incorrect initial guesses through the reflective thinking stage
- The method outperforms both pure SFT and GRPO baselines across four text classification benchmarks (SST-5, Amazon, Emotion, BBC News)

## Why This Works (Mechanism)
The framework works by decomposing the classification task into distinct reasoning stages that can be optimized separately. The initial guess provides a supervised signal that guides early learning and prevents the model from drifting into random exploration. The think stage allows the model to refine its reasoning before producing the final answer, which receives reinforcement learning rewards. The loss masking strategy ensures that each stage receives appropriate training signals without interference, while the gradient conflict resolution prevents contradictory updates from destabilizing training.

## Foundational Learning
- **Cross-entropy loss for classification**: Used to train the guess stage on ground-truth labels; needed to provide supervised guidance in early training phases; quick check: verify loss decreases on training set
- **Reinforcement learning with GRPO**: Optimizes the think-answer stages based on reward signals; needed to enable flexible reasoning and policy improvement; quick check: monitor reward progression during training
- **Loss masking**: Applies different losses to different token sequences; needed to prevent interference between SFT and RL signals; quick check: inspect token-level loss distributions
- **Gradient conflict resolution**: Skips SFT updates when gradients oppose RL gradients; needed to maintain training stability when objectives conflict; quick check: monitor gradient cosine similarity between stages
- **Prompt engineering for structured outputs**: Requires specific format with guess/think/answer tags; needed to enable stage-specific loss application; quick check: validate output format compliance
- **Token-level masking in sequence models**: Allows selective application of losses to specific parts of generated sequences; needed to implement the three-stage training approach; quick check: verify mask application matches intended stages

## Architecture Onboarding

**Component Map:**
Input Text -> Guess Stage (SFT/CE loss) -> Think Stage -> Answer Stage (RL/GRPO loss) -> Final Output

**Critical Path:**
The critical path flows from input through guess generation, reasoning, and final answer production. The loss masking and gradient conflict resolution mechanisms are critical for stable training, as they determine which gradients are applied at each stage.

**Design Tradeoffs:**
The framework trades implementation complexity for performance gains. The three-stage structure requires careful prompt engineering and loss management but enables better integration of supervised and reinforcement learning signals. The gradient conflict resolution adds computational overhead but prevents training instability.

**Failure Signatures:**
- Poor initial accuracy indicates SFT loss masking may not be correctly applied to guess tokens
- Unstable training or plateauing performance suggests frequent gradient conflicts not being properly resolved
- Slow convergence indicates insufficient supervised guidance from the guess stage

**3 First Experiments:**
1. Verify loss masking by checking that cross-entropy loss is only applied to guess tokens and zero elsewhere
2. Test gradient conflict resolution by monitoring cosine similarity between SFT and RL gradients during training
3. Conduct ablation by training with only the guess stage (pure SFT) versus only think-answer stages (pure RL) to quantify each component's contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Several critical hyperparameters are underspecified including learning rates, KL reference update frequency, and GRPO clipping parameters
- The gradient conflict resolution threshold (dot product < 0) lacks empirical justification and may be overly simplistic
- Claims of faster convergence are supported by convergence curves but lack statistical significance testing across multiple runs
- The framework's performance gains are demonstrated on four datasets but generalizability to other domains is not extensively tested

## Confidence

**High Confidence:**
- Core GTA framework architecture and three-stage training paradigm
- Experimental results showing superiority over baselines across four datasets
- Fundamental loss masking strategy implementation

**Medium Confidence:**
- Gradient conflict resolution mechanism implementation details
- Integration of SFT and RL through token-level masking

**Low Confidence:**
- Exact training dynamics including learning rates and scheduler settings
- KL reference model update frequency
- GRPO clipping parameter and other fine-grained hyperparameters

## Next Checks
1. Implement gradient similarity monitoring to verify conflict resolution activates appropriately and assess impact on convergence stability
2. Conduct ablation studies comparing GTA with and without the guess-think-answer structure to isolate component contributions
3. Test the framework across additional text classification datasets beyond the four reported to evaluate generalizability and identify domain-specific limitations