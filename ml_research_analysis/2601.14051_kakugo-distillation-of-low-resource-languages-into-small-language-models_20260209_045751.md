---
ver: rpa2
title: 'Kakugo: Distillation of Low-Resource Languages into Small Language Models'
arxiv_id: '2601.14051'
source_url: https://arxiv.org/abs/2601.14051
tags:
- language
- data
- languages
- training
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Kakugo, a pipeline that trains general-purpose small
  language models for low-resource languages using only the language name as input.
  By generating synthetic prompts and translating instruction datasets with a large
  teacher model, we create training data and SLMs for 54 low-resource languages.
---

# Kakugo: Distillation of Low-Resource Languages into Small Language Models

## Quick Facts
- arXiv ID: 2601.14051
- Source URL: https://arxiv.org/abs/2601.14051
- Reference count: 27
- Key outcome: Distills general-purpose small language models for 54 low-resource languages using only the language name as input, achieving average accuracy gains of 9.9% on Belebele, 1.9% on GlobalMMLU, and 8.8% on SIB200 at under $50 per language.

## Executive Summary
Kakugo introduces a pipeline for training general-purpose small language models (SLMs) for low-resource languages using only the language name as input. The approach generates synthetic prompts across topic-, scenario-, and context-based categories, then uses a large teacher model to produce instruction-following data in the target language. This data is combined with translated instruction datasets and used to fine-tune small student models. The method demonstrates consistent improvements across translation, classification, and question answering tasks for 54 low-resource languages, with total generation and training costs under $50 per language.

## Method Summary
Kakugo trains SLMs for low-resource languages through knowledge distillation from a large teacher model (GPT-OSS 120B). The pipeline generates synthetic prompts using topic-based (macro-topics and subtopics), scenario-based (broad and detailed scenarios), and context-based (from FineWeb2 samples) approaches. These prompts are processed by the teacher model to create instruction-following data in the target language, with reasoning traces captured for some data. The synthetic data is combined with translated versions of English instruction datasets (15K rows from InfinityInstruct "7M_core" subset). The combined dataset is used to fine-tune IBM Granite 4 Micro models using Llama Factory with DeepSpeed ZeRO-3 for one epoch, achieving improvements across multiple evaluation benchmarks.

## Key Results
- Average accuracy gains of 9.9% on Belebele (closed QA), 1.9% on GlobalMMLU (open QA), and 8.8% on SIB200 (classification)
- Consistent improvements across 54 low-resource languages with cost under $50 per language
- Token-limited models show comparable performance to full models, suggesting efficient use of generated data

## Why This Works (Mechanism)
The approach works by leveraging a large teacher model's knowledge to generate instruction-following data in target languages where native resources are scarce. By creating synthetic prompts across multiple categories and combining them with translated instruction data, Kakugo provides diverse training examples that enable small models to learn task-specific capabilities. The inclusion of reasoning traces captures the model's thought process, potentially improving the quality of responses. The distillation process allows the small student models to inherit the teacher's capabilities while being optimized for the specific characteristics of each low-resource language.

## Foundational Learning
- Knowledge distillation: Why needed - Transfer capabilities from large models to small, efficient models; Quick check - Verify teacher model produces coherent responses in target language
- Synthetic data generation: Why needed - Create training data where native resources are limited; Quick check - Inspect sample quality and diversity of generated prompts
- Prompt engineering: Why needed - Design effective prompts to elicit useful responses from teacher model; Quick check - Test prompt variations on sample generation
- Multilingual fine-tuning: Why needed - Adapt small models to specific language characteristics; Quick check - Monitor loss curves during training for convergence
- Evaluation metrics selection: Why needed - Measure performance across different task types; Quick check - Verify benchmark implementations match original specifications

## Architecture Onboarding

Component Map: GPT-OSS 120B (teacher) -> Synthetic prompt generator -> Response generator -> Granite 4 Micro (student) -> Evaluation benchmarks

Critical Path: Synthetic prompt generation → Teacher response generation → Data combination → Student fine-tuning → Evaluation

Design Tradeoffs:
- Model size vs. performance: Using Granite 4 Micro balances efficiency with capability
- Synthetic vs. translated data: Combines novel content generation with existing resources
- Full fine-tuning vs. parameter-efficient methods: Chosen for better performance despite higher computational cost

Failure Signatures:
- Teacher model produces poor quality or incoherent text in target languages
- Translated data fails parsing due to formatting issues
- Repetitive or hallucinated content in generated data
- Overfitting to synthetic data patterns, poor generalization

First Experiments:
1. Generate and evaluate sample synthetic prompts for a test language to assess teacher model quality
2. Translate a small subset of instruction data and verify parsing works correctly
3. Fine-tune Granite 4 Micro on combined data for one epoch and check baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on teacher model's ability to generate meaningful content in languages it may have limited training exposure to
- Success depends on prompt engineering quality and assumption that synthetic data can compensate for limited native resources
- Requires access to substantial computational resources and capable teacher model, potentially limiting accessibility

## Confidence
- Confidence in methodology: High - well-documented and reproducible approach
- Confidence in magnitude of improvements: Medium - gains vary significantly across languages and tasks
- Confidence in accessibility claim: Medium - technical approach is sound but resource requirements may be prohibitive

## Next Checks
1. Apply Kakugo to additional low-resource languages not included in the original 54 to assess robustness
2. Repeat experiments using different teacher models of varying sizes/capabilities to quantify dependence on teacher quality
3. Conduct human evaluation studies with native speakers to assess real-world utility of improvements