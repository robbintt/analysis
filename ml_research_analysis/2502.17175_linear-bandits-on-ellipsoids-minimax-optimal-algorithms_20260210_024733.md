---
ver: rpa2
title: 'Linear Bandits on Ellipsoids: Minimax Optimal Algorithms'
arxiv_id: '2502.17175'
source_url: https://arxiv.org/abs/2502.17175
tags:
- algorithm
- regret
- bound
- linear
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of stochastic linear bandits\
  \ over ellipsoidal action sets, where the learner selects actions from an ellipsoid\
  \ and observes noisy linear rewards. The main contributions are two-fold: First,\
  \ the authors derive a novel information-theoretic lower bound on the regret of\
  \ any algorithm, which is $\\Omega(\\min(d\\sigma\\sqrt{T} + d\\|\u03B8\\|A, \\\
  |\u03B8\\|AT))$, where $d$ is the dimension, $T$ is the time horizon, $\\sigma^2$\
  \ is the noise variance, $A$ is the matrix defining the action set, and $\\theta$\
  \ is the unknown parameter."
---

# Linear Bandits on Ellipsoids: Minimax Optimal Algorithms

## Quick Facts
- **arXiv ID**: 2502.17175
- **Source URL**: https://arxiv.org/abs/2502.17175
- **Reference count**: 40
- **Primary result**: Proposes a computationally efficient, minimax optimal algorithm (E2TC) for stochastic linear bandits over ellipsoidal action sets.

## Executive Summary
This paper addresses the problem of stochastic linear bandits over ellipsoidal action sets, proposing both a novel information-theoretic lower bound and a matching upper bound algorithm. The authors establish that the regret of any algorithm must be at least Ω(min(dσ√T + d‖θ‖_A, ‖θ‖_A T)), where d is dimension, T is time horizon, σ² is noise variance, A defines the action set, and θ is the unknown parameter. They then introduce E2TC (Explore-Explore-Then-Commit), a non-classical algorithm that achieves this lower bound up to constants, making it minimax optimal. E2TC is computationally efficient (O(dT + d²log(T/d) + d³) time, O(d²) memory) compared to optimistic algorithms that are not polynomial-time implementable. The algorithm also achieves local asymptotic minimax optimality, a stronger notion of optimality.

## Method Summary
The authors develop E2TC (Explore-Explore-Then-Commit), a novel algorithm that first estimates ‖θ‖ using a sequential procedure, then applies an explore-and-commit strategy based on this estimate. Unlike optimism-based or sampling-based approaches, E2TC combines estimation of the norm of the unknown parameter with a strategic exploration phase followed by commitment to an optimal action. The algorithm's design leverages the specific geometry of ellipsoidal action sets to achieve computational efficiency while maintaining theoretical optimality guarantees. The method requires exact knowledge of certain problem parameters like noise variance, and its analysis assumes i.i.d. Gaussian noise.

## Key Results
- Establishes a tight information-theoretic lower bound of Ω(min(dσ√T + d‖θ‖_A, ‖θ‖_A T)) for ellipsoidal linear bandits
- Proposes E2TC algorithm that matches this lower bound up to constants, proving minimax optimality
- Demonstrates E2TC is computationally efficient (O(dT + d²log(T/d) + d³) time, O(d²) memory) versus non-implementable optimistic algorithms
- Proves local asymptotic minimax optimality, a stronger optimality notion than standard minimax guarantees

## Why This Works (Mechanism)
The algorithm exploits the specific geometry of ellipsoidal action sets through a two-phase approach: first estimating ‖θ‖ via a novel sequential procedure, then using this estimate to inform an explore-and-commit strategy. This avoids the computational intractability of optimism-based methods while achieving optimal regret. The mechanism works because the ellipsoidal structure allows for efficient exploration that simultaneously estimates both the norm of θ and identifies near-optimal actions, with the commitment phase leveraging this information to minimize regret.

## Foundational Learning
- **Ellipsoidal action sets**: Why needed - provides geometric structure enabling efficient algorithms; Quick check - verify action set can be expressed as {x: xᵀAx ≤ 1} for some positive definite A
- **Information-theoretic lower bounds**: Why needed - establishes fundamental limits on achievable performance; Quick check - confirm lower bound derivation uses change-of-measure arguments
- **Explore-then-commit strategies**: Why needed - enables computational efficiency by avoiding continuous exploration; Quick check - verify algorithm commits to action after finite exploration phase
- **Local asymptotic minimax optimality**: Why needed - stronger optimality notion than standard minimax; Quick check - confirm asymptotic analysis holds as T → ∞ for fixed problem instance
- **Sequential parameter estimation**: Why needed - allows adaptive exploration based on learned information; Quick check - verify estimation procedure converges to true ‖θ‖ value
- **Ellipsoidal geometry in optimization**: Why needed - enables polynomial-time computation of optimal actions; Quick check - verify action selection can be computed via convex optimization

## Architecture Onboarding

**Component Map**: Action selection -> Sequential norm estimation -> Explore phase -> Commit phase -> Regret accumulation

**Critical Path**: The algorithm's performance depends critically on accurate estimation of ‖θ‖ during the sequential phase, which then determines the duration and strategy of the explore phase. Poor norm estimation leads to suboptimal explore duration, which directly impacts regret. The commit phase must execute optimally once exploration is complete.

**Design Tradeoffs**: The algorithm trades off between exploration thoroughness and computational efficiency. More exploration improves ‖θ‖ estimation but increases regret during the explore phase. The ellipsoidal structure enables this tradeoff to be managed efficiently, whereas general convex sets would require computationally expensive optimization at each step.

**Failure Signatures**: Algorithm failure manifests as linear regret growth (O(T)) rather than the optimal √T rate, typically due to inaccurate norm estimation causing premature commitment or excessive exploration. The algorithm is also vulnerable to noise misspecification and deviations from the ellipsoidal assumption.

**First Experiments**: 1) Test norm estimation accuracy on synthetic ellipsoidal problems with known θ; 2) Verify computational time scales as O(dT) on problems of varying dimension; 3) Compare regret trajectories against theoretical predictions across multiple problem instances.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis restricted to ellipsoidal action sets, not extending to general convex or non-convex sets
- Assumes i.i.d. Gaussian noise; robustness to other distributions (sub-Gaussian, heavy-tailed) not addressed
- Requires exact knowledge of problem parameters like noise variance, which may not be available in practice
- Lacks empirical validation of practical performance in non-asymptotic regimes or under model misspecification

## Confidence
- Minimax optimality claims: High
- Computational efficiency claims: High
- Asymptotic optimality claims: High
- Practical applicability: Medium

## Next Checks
1. Conduct experiments comparing E2TC with existing practical algorithms on synthetic and real-world ellipsoidal bandit problems to assess empirical regret and computational efficiency
2. Extend the analysis to other noise distributions (e.g., sub-Gaussian, heavy-tailed) and verify if the algorithm and bounds remain valid or require modification
3. Investigate the robustness of E2TC under model misspecification, such as unknown noise variance or slight deviations from the ellipsoidal action set assumption