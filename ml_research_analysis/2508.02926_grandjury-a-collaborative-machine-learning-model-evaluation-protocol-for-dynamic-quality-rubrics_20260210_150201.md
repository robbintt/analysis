---
ver: rpa2
title: 'GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for
  Dynamic Quality Rubrics'
arxiv_id: '2508.02926'
source_url: https://arxiv.org/abs/2508.02926
tags:
- evaluation
- grandjury
- arxiv
- vote
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GrandJury introduces a structured evaluation protocol for generative\
  \ AI outputs without ground truth. It uses time-decayed aggregation to weight recent\
  \ votes more heavily while maintaining historical context, with exponential decay\
  \ controlled by parameter \u03BB."
---

# GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics

## Quick Facts
- arXiv ID: 2508.02926
- Source URL: https://arxiv.org/abs/2508.02926
- Reference count: 35
- Key outcome: Introduces time-decayed aggregation for generative AI evaluation without ground truth, using exponential decay controlled by λ to weight recent votes more heavily while maintaining historical context.

## Executive Summary
GrandJury introduces a structured evaluation protocol for generative AI outputs without ground truth. It uses time-decayed aggregation to weight recent votes more heavily while maintaining historical context, with exponential decay controlled by parameter λ. The protocol includes traceability of all votes with timestamps and voter IDs, ambiguity flagging for high-variance judgments, and transparent voter prompts defining evaluation criteria. The open-source grandjury Python package implements this protocol, and a public dataset of 149 jokes from 31 models across 7 platforms demonstrates its application. The approach transforms disagreement from noise into analytical signal, enabling pluralistic, accountable evaluation that captures evolving consensus rather than optimizing toward static benchmarks.

## Method Summary
GrandJury implements a two-phase workflow for evaluating generative AI outputs without ground truth. Phase 1 collects human votes (0-1 scale) on model outputs with required metadata: inference ID, vote value, voter ID, timestamp (ISO 8601), and voter prompt ID. Phase 2 analyzes these votes using the grandjury Python package to generate time-decayed aggregate scores. The scoring uses exponential decay S_t = αS_{t-1} + (1-α)⟨v_t⟩ where α = e^(-λΔt), with λ controlling responsiveness (typical: 0.001-0.01 time^-1). The protocol also flags high-variance judgments (σ² > σ²_crit, suggested 0.05) for additional review and maintains full traceability of all votes.

## Key Results
- Introduces time-decayed aggregation with exponential decay controlled by λ parameter
- Implements ambiguity flagging for high-variance judgments (σ² > 0.05) to surface contested outputs
- Provides transparent voter prompts and full vote traceability with timestamps and voter IDs
- Demonstrates protocol on public dataset of 149 jokes from 31 models across 7 platforms

## Why This Works (Mechanism)

### Mechanism 1: Time-Decayed Aggregation
- Claim: Weighting recent votes more heavily than older ones may produce scores that better reflect evolving consensus while preserving historical context.
- Mechanism: Exponential decay formula S_t = αS_{t-1} + (1-α)⟨v_t⟩ where α = e^(-λ∆t). The decay factor α controls how quickly older votes lose influence, with λ as the configurable decay constant and ∆t as elapsed time.
- Core assumption: Consensus on acceptable outputs shifts over time and recent judgments are more representative of current standards.
- Evidence anchors:
  - [abstract] "time-decayed aggregation to weight recent votes more heavily while maintaining historical context, with exponential decay controlled by parameter λ"
  - [Section 3.2] Provides worked example: "Let S_{t-1} = 0.72, ∆t = 3 days, λ = 0.1, so α = e^-0.3 ≈ 0.741" showing 25.9% weight to new batch, 74.1% to history
  - [corpus] KBE-DME (arXiv:2510.21182) addresses similar dynamic benchmark evolution concerns but through knowledge graphs rather than vote aggregation
- Break condition: If evaluation criteria are truly stable (not evolving), time-decay adds unnecessary complexity without benefit. Very high λ values may cause scores to oscillate with small vote batches.

### Mechanism 2: Ambiguity Flagging via Variance Thresholds
- Claim: Flagging high-variance judgments surfaces genuinely contested outputs for additional review rather than averaging away disagreement.
- Mechanism: Compute variance σ²_t = Var(v_t,∗) across votes in batch t. Flag as ambiguous if σ²_t > σ²_crit (suggested threshold 0.05). Flags trigger curator review or additional voting rounds.
- Core assumption: High variance indicates meaningful evaluative disagreement, not just noisy or low-quality annotations.
- Evidence anchors:
  - [abstract] "ambiguity flagging for high-variance judgments"
  - [Section 3.4] Worked example: votes {1, 0, 0} yield σ²_t = 0.111 > 0.05, triggering flag; votes {0.90, 0.80, 0.60} yield σ²_t = 0.016 < 0.05, no flag
  - [corpus] Related work on disagreement (CrowdTruth referenced in paper) models annotator disagreement as signal rather than noise, but corpus lacks direct validation of variance thresholds
- Break condition: If voter pool has systematically different interpretive frameworks, variance may reflect rubric ambiguity rather than legitimate pluralism.

### Mechanism 3: Structured Voter Prompts with Full Traceability
- Claim: Explicit, shared rubrics (voter prompts) combined with audit trails may improve judgment consistency and enable retrospective analysis.
- Mechanism: Each evaluation task begins with a publicly shared voter prompt specifying what to evaluate, how, and why. Every vote records: inference ID, vote value, voter ID, timestamp (ISO 8601), and voter prompt ID.
- Core assumption: Explicit rubrics reduce interpretive variance enough to make aggregated scores meaningful, while preserving legitimate disagreement.
- Evidence anchors:
  - [abstract] "transparent voter prompts defining evaluation criteria" and "traceability of all votes with timestamps and voter IDs"
  - [Section 3.1] "this structured rubric aligns diverse judgments under a shared framework while retaining interpretive differences"
  - [corpus] Pairwise or Pointwise paper (arXiv:2504.14716) examines feedback protocol bias in LLM evaluation but does not validate human rubric design
- Break condition: If rubrics are too vague or too prescriptive, they either fail to reduce variance or artificially suppress legitimate pluralism.

## Foundational Learning

- Concept: **Exponential Decay in Sequential Aggregation**
  - Why needed here: The time-decay formula uses e^(-λ∆t) to compute weights. Without understanding how λ and ∆t interact, parameter selection becomes arbitrary.
  - Quick check question: If λ = 0.1/day and ∆t = 10 days, what fraction of the previous score is retained? (Answer: e^-1 ≈ 0.368, so ~37%)

- Concept: **Variance as Disagreement Signal**
  - Why needed here: Ambiguity flagging relies on interpreting σ² relative to a threshold. Confusing variance with standard deviation or misinterpreting scale will break flag logic.
  - Quick check question: For binary votes {1, 0, 1, 0, 1}, what is the variance? (Answer: mean = 0.6, variance = 0.24)

- Concept: **Inter-Annotator Agreement Limitations**
  - Why needed here: The paper explicitly notes κ < 0.3 in open-ended tasks. Understanding why agreement is low informs how to interpret flagged ambiguity.
  - Quick check question: Why might low κ not indicate poor data quality in subjective tasks? (Answer: Legitimate pluralism—multiple valid interpretations exist)

## Architecture Onboarding

- Component map: Python client (grandjury PyPI package) -> Server-side scoring engine -> Vote data schema -> Inference collection
- Critical path:
  1. Install client: `pip install grandjury`
  2. Phase 1: Collect human votes on model outputs using defined voter prompts
  3. Phase 2: Load vote data → analyze distributions → apply time-decay scoring (requires API key for server-side computation)
  4. Review ambiguity flags and freshness metrics

- Design tradeoffs:
  - **λ selection**: High λ (e.g., 0.1) = responsive to recent votes but potentially unstable; low λ (e.g., 0.001) = stable but slow to reflect consensus shifts
  - **σ²_crit threshold**: Lower = more items flagged (more review burden); higher = fewer flags but may miss genuine ambiguity
  - **Client vs. server computation**: Local analysis (histograms, distributions) requires no auth; time-decay scoring requires API key and server access

- Failure signatures:
  - Scores oscillating wildly: λ too high for vote volume
  - No items ever flagged: σ²_crit too high or votes artificially uniform
  - Freshness metric always near 1.0: ∆t values too large relative to λ
  - Unable to reproduce scores: Missing voter_prompt_id linkage or timestamp drift

- First 3 experiments:
  1. **Validate decay behavior**: Create synthetic vote sequence with known timestamps; vary λ and verify S_t matches manual calculation. Confirms implementation matches formula.
  2. **Calibrate ambiguity threshold**: Run the jokes dataset with 3+ independent raters; histogram variance distribution to identify natural σ² clustering. Informs σ²_crit selection.
  3. **Test freshness interpretation**: Compare scores at different query times for same vote history; verify freshness metric correctly reflects recency weight. Confirms time-unit handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal methodology for selecting the time-decay constant (λ) to balance responsiveness to new votes against historical stability?
- Basis in paper: [explicit] The authors invite "protocol dialogues that refine metadata logging, **decay parameters**... and ambiguity thresholds" in Section 5.2.
- Why unresolved: While the paper suggests typical values (0.01 to 0.001), it leaves the specific selection criteria to "domain-specific tuning" without defining a validation metric for the tuning process itself.
- What evidence would resolve it: Empirical studies comparing different λ values against known ground-truth shifts in longitudinal evaluation tasks.

### Open Question 2
- Question: How should the critical variance threshold (σ²_crit) for ambiguity flagging be calibrated to distinguish signal from noise?
- Basis in paper: [explicit] Section 5.2 explicitly calls for community dialogue to refine "**ambiguity thresholds**."
- Why unresolved: The paper provides a worked example using 0.05 as a threshold but does not justify this specific value or provide a method for determining appropriate thresholds for different domains.
- What evidence would resolve it: Analysis of variance distributions across diverse evaluation datasets to identify natural breakpoints that correlate with qualitative assessments of ambiguity.

### Open Question 3
- Question: How can voter reputation weights (r_i) be objectively calculated and integrated without introducing centralization or bias?
- Basis in paper: [inferred] Section 3.2 defines the notation for reputation weights (r_i > 0) but explicitly defaults them to 1, leaving the mechanism for calculating reputation undefined.
- Why unresolved: The protocol currently treats all voters as equal, failing to address how to weight expertise or reliability in a decentralized "pluralistic" system.
- What evidence would resolve it: Simulation results comparing evaluation accuracy using various reputation algorithms (e.g., peer-consensus vs. gold-standard validation) against the default uniform weighting.

## Limitations
- API dependency: Core time-decay scoring requires server-side computation with authentication, creating verification black box
- Threshold calibration: Ambiguity threshold σ²_crit=0.05 lacks empirical justification or sensitivity analysis
- Parameter selection: Optimal λ selection depends on task-specific dynamics without concrete decision criteria

## Confidence
- High Confidence: Mathematical formulation of exponential decay and variance computation is correct and reproducible
- Medium Confidence: Conceptual framework of treating disagreement as analytical signal is well-grounded in related work
- Low Confidence: Practical effectiveness of full protocol relies on untested assumptions about voter behavior and rubric interpretation

## Next Checks
1. **Decay Implementation Verification**: Create synthetic vote sequences with known timestamps; compare computed scores against manual exponential decay calculations for multiple λ values
2. **Variance Threshold Calibration**: Run the jokes dataset with 4+ independent raters; analyze variance distribution to empirically determine natural clustering and optimal σ²_crit
3. **API Independence Test**: Implement the time-decay formula locally (as specified) and compare results with server-side computation for identical vote histories to verify consistency