---
ver: rpa2
title: Reinforcement Learning for Self-Improving Agent with Skill Library
arxiv_id: '2512.17102'
source_url: https://arxiv.org/abs/2512.17102
tags:
- skill
- library
- task
- agent
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving LLM-based agents'
  ability to continuously learn and adapt when deployed in new environments, specifically
  through skill libraries that allow agents to learn, validate, and apply new skills.
  The authors propose a reinforcement learning framework called Skill Augmented GRPO
  for self-Evolution (SAGE) that incorporates a novel Sequential Rollout process and
  Skill-integrated Reward.
---

# Reinforcement Learning for Self-Improving Agent with Skill Library

## Quick Facts
- **arXiv ID:** 2512.17102
- **Source URL:** https://arxiv.org/abs/2512.17102
- **Reference count:** 36
- **Primary result:** 8.9% higher Scenario Goal Completion, 26% fewer steps, 59% fewer tokens vs baselines on AppWorld

## Executive Summary
This paper presents SAGE (Skill Augmented GRPO for self-Evolution), a reinforcement learning framework that enables LLM-based agents to continuously improve through skill libraries. The framework addresses the challenge of enabling agents to learn, validate, and apply new skills when deployed in new environments. SAGE combines sequential rollout across task chains with skill-integrated rewards to create an end-to-end credit assignment mechanism that reinforces skill generation from successful skill utilization.

## Method Summary
SAGE builds upon Group Relative Policy Optimization (GRPO) by introducing Sequential Rollout and Skill-integrated Reward components. The agent generates reusable skills as programmatic functions and immediately calls them for task processing. During training, the agent is deployed across chains of similar tasks, accumulating skills from previous tasks in the library for subsequent tasks. The Skill-integrated Reward provides explicit incentives for both high-quality skill generation and effective skill utilization, beyond standard outcome-based rewards. The approach requires supervised fine-tuning (SFT) initialization using expert demonstrations before RL training can begin.

## Key Results
- SAGE achieves 8.9% higher Scenario Goal Completion on AppWorld compared to existing approaches
- Requires 26% fewer interaction steps to complete tasks
- Generates 59% fewer tokens during task execution
- Outperforms baseline methods in both accuracy and efficiency metrics

## Why This Works (Mechanism)

### Mechanism 1: Sequential Rollout with Skill Accumulation
The framework deploys agents across chains of similar tasks, allowing skills generated during one task to be reused in subsequent tasks. This creates an end-to-end credit assignment path where rewards from successful skill usage in later tasks reinforce the behavior that generated the skill in earlier tasks.

### Mechanism 2: Skill-integrated Reward
A composite reward function that explicitly incentivizes skill generation and utilization, in addition to task completion. The reward includes bonuses when skills generated in previous tasks are successfully used, and when skills generated in the current task are successfully used in subsequent tasks.

### Mechanism 3: Unified Skill-Task Format with SFT Initialization
The agent generates a skill function and immediately calls it for task processing in a unified format. Base models often fail at this complex instruction-following, so SFT is performed first using expert demonstrations to teach the basic format and behavior before RL begins.

## Foundational Learning

- **Policy Gradient Reinforcement Learning (GRPO)**: Understanding how policy gradient methods optimize policies through reward-based gradient estimation is essential for grasping how SAGE learns from sampled trajectories.
  - Quick check: Can you explain, in simple terms, how a policy gradient algorithm updates a model's weights based on the reward of a generated action sequence?

- **Credit Assignment in Reinforcement Learning**: SAGE addresses the challenge of determining which early actions (skill generation) were responsible for later rewards (successful skill usage). The Sequential Rollout and Skill-integrated Reward are designed to solve this credit assignment problem.
  - Quick check: In a sequence of actions that leads to a final reward, why is it difficult for an RL agent to know which specific early actions were most important for that success?

- **Skill Libraries in AI Agents**: Understanding the motivation for skill libraries (efficiency, transfer learning, composability) is key to understanding the problem SAGE solves.
  - Quick check: What are the potential advantages of storing a solution as a reusable "skill" function versus re-solving a similar problem from scratch each time?

## Architecture Onboarding

- **Component map:**
  - Skill Library Agent (Qwen2.5-32B-Instruct model) -> Environment (AppWorld) -> SAGE Training Loop (Sequential Rollout + Reward Computation + GRPO Updater)

- **Critical path:**
  1. SFT Initialization: Fine-tune base model on expert-generated trajectories using the skill-library agent format
  2. Sequential Rollout Implementation: Persist skill library across task chains during rollout
  3. Reward Function Implementation: Precisely implement skill-integrated reward formulas with conditional bonuses

- **Design tradeoffs:**
  - Task chain length: Uses 2-task chains (longer chains perform worse due to reward imbalance and gradient variance)
  - Unified vs. separate format: Unified format (generate-then-call) is more constrained but more effective than free-form agents
  - On-policy vs. off-policy: Strictly on-policy GRPO variant is more stable but less sample-efficient

- **Failure signatures:**
  - Low skill usage rate: Indicates poor SFT data quality or incorrect reward shaping
  - Reward hacking: Excessive skill generation producing useless skills if bonus is too high
  - Gradient instability: Longer chains create divergent skill libraries across rollout groups

- **First 3 experiments:**
  1. Reproduce baseline (GRPO) on AppWorld to establish performance baseline
  2. Ablate SFT initialization: Compare training from base model vs. SFT model
  3. Ablate reward components: Compare outcome-only, full skill-integrated, and simplified rewards

## Open Questions the Paper Calls Out

### Open Question 1
Does SAGE generalize to diverse tool-using agent benchmarks beyond AppWorld? The authors plan to extend evaluation to other tool-using agent datasets, but current scope is limited to AppWorld's simulated environment.

### Open Question 2
Can specialized tool-usage retrievers outperform general text-embedding models for skill library retrieval? Current Qwen3-Embedding underperformed compared to simpler methods, suggesting general embeddings fail to capture tool semantics.

### Open Question 3
Can reward balancing or variance reduction techniques stabilize training for longer sequential task chains (K > 2)? Experiments with 3-task chains showed reward distribution imbalance and gradient variance as key issues.

## Limitations
- Narrow scope of skill applicability requires tasks to be organized into chains where skills transfer effectively
- 2-task chain length may not be optimal for all domains and requires empirical tuning
- Substantial computational cost requiring 4 GPUs per example during training

## Confidence

- **High Confidence (8-10):** Core experimental results on AppWorld are well-documented and reproducible
- **Medium Confidence (6-7):** Proposed mechanisms are theoretically sound but unproven in diverse environments
- **Low Confidence (3-4):** Practical applicability to real-world scenarios is uncertain due to assumptions about expert data availability

## Next Checks

1. **Ablation Study on Chain Length**: Systematically test chain lengths of 1, 2, 3, and 4 tasks on AppWorld to understand optimal configuration and identify performance degradation points.

2. **Cross-Domain Transfer**: Apply SAGE to a different environment (e.g., ALFWorld) to test whether skill library mechanism generalizes beyond AppWorld domain.

3. **Resource Efficiency Analysis**: Measure wall-clock time and GPU-hour requirements for training SAGE versus baselines to analyze practical feasibility of performance gains.