---
ver: rpa2
title: 'Un-considering Contextual Information: Assessing LLMs'' Understanding of Indexical
  Elements'
arxiv_id: '2506.01089'
source_url: https://arxiv.org/abs/2506.01089
tags:
- sentence
- chris
- will
- context
- indexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) demonstrate strong performance interpreting
  indexicals like "I," but struggle with others such as "you," "here," and "tomorrow."
  A dataset of 1,600 multiple-choice questions was created to test how LLMs resolve
  indexicals in English under varying syntactic (quotation vs. non-quotation) and
  contextual conditions.
---

# Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements

## Quick Facts
- **arXiv ID**: 2506.01089
- **Source URL**: https://arxiv.org/abs/2506.01089
- **Reference count**: 16
- **Primary result**: LLMs achieve near-perfect "I" interpretation but struggle with "you," "here," and "tomorrow" due to inconsistent handling of quotation-based context shifting

## Executive Summary
This study evaluates how four frontier LLMs interpret indexicals (I, you, here, tomorrow) under varying syntactic and contextual conditions. The research reveals that while LLMs perform nearly optimally on first-person indexical "I," they struggle significantly with other indexicals, particularly when quotation marks create context-shifting scenarios. The findings demonstrate that LLMs inappropriately apply context-based reasoning heuristics to indexicals that should be resolved purely by syntactic rules, and that quotation affects different indexicals inconsistently across model architectures.

## Method Summary
The study created a dataset of 1,600 multiple-choice questions testing indexical interpretation across four conditions: quotation presence, context prime (shifted vs. non-shifted), and sentence type. Four frontier models (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, DeepSeek V3) were evaluated zero-shot using a standardized prompt template. The dataset was curated via GPT-4o and includes 100 base sentences × 4 conditions per indexical, with balanced gender representation in named entities.

## Key Results
- "I" interpretation achieves near-perfect accuracy (99%) across all models in non-quotation conditions
- "You," "here," and "tomorrow" show significantly lower accuracy, with context priming causing incorrect interpretations
- Quotation improves "here" accuracy but reduces performance for "you" and "tomorrow"
- DeepSeek V3 shows anomalous context sensitivity for "I" under quotation (17% accuracy)
- Models exhibit a strong non-shifted bias for "tomorrow" regardless of syntactic context

## Why This Works (Mechanism)

### Mechanism 1: Distributional Frequency Drives "I" Performance
- **Claim**: High token frequency and consistent self-referential patterns in training data enable robust learning of speaker-reference mapping for "I"
- **Evidence**: 99% accuracy across models; performance linked to training data consistency
- **Break condition**: Edge cases with nested quotations or multi-speaker dialogue causing accuracy drops

### Mechanism 2: Context Priming Interference with Indexical Resolution
- **Claim**: Models inappropriately apply context-sensitive reasoning heuristics (developed for ambiguous pronouns) to unambiguous indexicals
- **Evidence**: Incorrect context-prime-based selections for "you" and "here" in non-quotation conditions
- **Break condition**: Explicit syntactic fine-tuning eliminating context priming effects

### Mechanism 3: Quotation Triggers Inconsistent Indexical Shifting
- **Claim**: Direct quotation acts as context-shift operator but models apply shifting inconsistently across indexical types
- **Evidence**: Quotation improves "here" but degrades "you" and "tomorrow" accuracy; strong non-shifted bias for "tomorrow"
- **Break condition**: Models correctly applying uniform shifting rules when prompted

## Foundational Learning

- **Indexicals vs. Third-Person Pronouns**
  - Why needed: Indexicals have unambiguous reference mechanisms unlike ambiguous pronouns
  - Quick check: In "John told Bill that I should get a promotion," can "I" refer to Bill? Why or why not?

- **Direct Quotation as Context-Shifting Operator**
  - Why needed: Experimental design hinges on quotation triggering shifted interpretations
  - Quick check: In "Mary said 'I am here now'", who is "I" and where is "here"?

- **Context Priming vs. Syntactic Binding**
  - Why needed: Paper tests whether models incorrectly let semantic context override syntactic rules
  - Quick check: If model changes interpretation based on "arrogant" vs. "supportive" context, what failure mode?

## Architecture Onboarding

- **Component map**: Input Text → Context Prime → Stimulus Sentence → Syntactic Marker → Model Inference → Choice
- **Critical path**: Detect indexical → Check quotation → Apply shift if quoted → Suppress context prime → Resolve coordinates
- **Design tradeoffs**: 1,600 samples provide statistical power but may miss edge cases; binary choice simplifies analysis but may miss partial understanding
- **Failure signatures**: 
  - "you": Incorrectly selects shifted in non-quotation
  - "here": Incorrectly selects non-shifted (~2% accuracy)
  - "tomorrow": Strong non-shifted bias (83-100%) regardless of rule
  - DeepSeek "I": Context sensitivity spike (17% accuracy)

- **First 3 experiments**:
  1. **Quotation ablation test**: Replace quotation with "that" clauses; expect "here" accuracy drops
  2. **Context prime randomization**: Shuffle primes; expect accuracy drops to ~50% if context-dependent
  3. **Per-indexical attention analysis**: Visualize attention on 20 samples per indexical to identify processing patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do specific internal model components encode syntactic constraints for indexicals, or is performance driven by statistical correlations?
- **Basis**: Study uses "black-box setting" and calls for white-box analysis
- **What evidence resolves**: Probing classifiers or causal tracing identifying specific attention heads for quotation vs. context

### Open Question 2
- **Question**: Does language pro-drop nature versus training data volume explain first-person indexical performance gaps across languages?
- **Basis**: Performance divergence noted between English and Turkish studies
- **What evidence resolves**: Controlled cross-linguistic study comparing high-resource vs. low-resource language pairs

### Open Question 3
- **Question**: Why do LLMs exhibit specific non-shifted bias for "tomorrow" distinct from other indexicals?
- **Basis**: Strong non-shifted bias reported for "tomorrow" across conditions
- **What evidence resolves**: Analysis of temporal shifting frequency in pre-training corpora compared to other indexicals

## Limitations

- Binary forced-choice evaluation may oversimplify nuanced indexical understanding
- GPT-4o dataset generation introduces potential circularity concerns
- Study limited to English indexicals, restricting cross-linguistic generalizability
- 1,600 samples may not capture edge cases where interpretation becomes genuinely ambiguous

## Confidence

- **High Confidence**: "I" interpretation near-perfect (99% accuracy) supported by consistent results across all four models
- **Medium Confidence**: Context priming interference hypothesis has strong behavioral evidence but lacks mechanistic validation
- **Low Confidence**: DeepSeek V3's anomalous context sensitivity for "I" (17% accuracy) lacks explanatory evidence

## Next Checks

1. **Attention Mechanism Analysis**: Use integrated gradients or attention visualization to examine how models process quotation marks, context primes, and indexical tokens across all four types

2. **Cross-Lingual Replication**: Evaluate same four indexicals in morphologically rich languages (Turkish/Finnish) to test distributional frequency advantages for first-person pronouns

3. **Syntactic Fine-Tuning Experiment**: Train models on synthetic data emphasizing syntactic rules of quotation-based context shifting without contextual interference to determine if context priming effects can be eliminated