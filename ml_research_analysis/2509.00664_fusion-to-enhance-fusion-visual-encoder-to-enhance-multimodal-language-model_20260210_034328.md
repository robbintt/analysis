---
ver: rpa2
title: 'Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model'
arxiv_id: '2509.00664'
source_url: https://arxiv.org/abs/2509.00664
tags:
- visual
- vision
- zhang
- language
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal large language models
  (MLLMs) struggling with basic visual perception tasks despite excelling at high-level
  semantic understanding. The core issue is that current MLLMs rely on single vision
  encoders optimized for semantic alignment, which sacrifices fine-grained visual
  detail perception.
---

# Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model

## Quick Facts
- arXiv ID: 2509.00664
- Source URL: https://arxiv.org/abs/2509.00664
- Authors: Yifei She; Huangxuan Wu
- Reference count: 40
- Key outcome: FtZ framework fuses CLIP and DINOv2 via cross-attention to improve MLLM visual perception, achieving 43.6% accuracy on TextVQA (vs 35.3% baseline) and 85.3% average on POPE (vs 81.2% baseline)

## Executive Summary
This paper addresses the fundamental limitation of MLLMs that rely on single vision encoders optimized for semantic alignment, which sacrifices fine-grained visual detail perception. The authors propose Fusion to Enhance (FtZ), a novel framework that composes two heterogeneous expert vision encoders - a semantically powerful anchor encoder (CLIP) and a perception-rich augmenting encoder (DINOv2) - through a lightweight Multi-Head Cross-Attention mechanism. This allows the anchor encoder to dynamically query the augmenting encoder for precise visual details. Experimental results demonstrate that FtZ significantly outperforms baselines on several challenging benchmarks including TextVQA, POPE, MMMU, and MME, showing superior performance in fine-grained visual understanding and hallucination mitigation.

## Method Summary
FtZ employs a two-stage LLaVA-style training procedure with frozen vision encoders. The framework uses CLIP ViT as a semantically-aligned anchor and DINOv2 ViT as a perception-rich augmenter. Lightweight Multi-Head Cross-Attention modules are inserted at mapped layer pairs, where the anchor generates queries and the augmenter provides keys and values. Features flow unidirectionally from augmenter to anchor, with MHCA outputs integrated via residual connections. A 2-layer MLP projector maps final visual tokens to the LLM embedding space. The method is trained on LAION-CC-SBU subset for pretraining and LLaVA-v1.5-mix for fine-tuning, with different batch sizes and learning rates for each stage.

## Key Results
- TextVQA accuracy improves from 35.3% (CLIP-Only) to 43.6% (FtZ)
- POPE average accuracy improves from 81.2% (CLIP-Only) to 85.3% (FtZ)
- MMMU accuracy improves from 28.4% (CLIP-Only) to 29.6% (FtZ)
- MME score improves from 1385.9 (CLIP-Only) to 1502.7 (FtZ)

## Why This Works (Mechanism)

### Mechanism 1: Complementary Inductive Bias Fusion
Combining a semantically-aligned encoder (CLIP) with a self-supervised perceptual encoder (DINOv2) recovers fine-grained visual details that single-encoder architectures discard. CLIP's contrastive image-text pretraining optimizes for semantic abstraction (the "what"), which inherently compresses away texture, boundary, and spatial information. DINOv2's self-supervised pretraining preserves these low-level features. The cross-attention allows semantic queries to retrieve specific perceptual details on demand.

### Mechanism 2: Unidirectional Query-Based Enrichment
A unidirectional cross-attention flow (augmenter → anchor) preserves anchor stability while allowing selective detail injection. The anchor encoder generates Query vectors from its semantic features. The augmenter provides Key and Value vectors from its perceptual features. This asymmetry means the anchor actively "pulls" relevant details rather than having foreign features pushed into it, reducing representation drift.

### Mechanism 3: Hierarchical Depth-Aligned Fusion
Fusing at multiple semantically-comparable depths enables progressive enrichment rather than late-stage patching. Layers are mapped by relative depth (e.g., 50% through anchor → 50% through augmenter) rather than absolute index. This exchanges information between processing stages of comparable abstraction levels.

## Foundational Learning

- **Concept: Vision Encoder Inductive Biases**
  - Why needed here: Understanding why CLIP discards details while DINOv2 preserves them is essential for diagnosing fusion failures.
  - Quick check question: Can you explain why contrastive image-text training tends to compress away texture information while self-supervised local discrimination preserves it?

- **Concept: Multi-Head Cross-Attention**
  - Why needed here: The core fusion operation; misunderstanding here leads to incorrect implementation of Q/K/V assignment.
  - Quick check question: Given anchor features H_anchor and augmenter features H_augment, which generates Q and which generates K, V in this architecture—and why does directionality matter?

- **Concept: Residual Gating Behavior**
  - Why needed here: The residual connection H'_anchor = H_anchor + H_cross implicitly learns how much fusion signal to admit.
  - Quick check question: If the learned cross-attention output H_cross is near-zero at initialization, what happens to the anchor's forward pass during early training?

## Architecture Onboarding

- **Component map:**
  - Image → CLIP ViT (anchor, frozen) → DINOv2 ViT (augmenting, frozen) → MHCA modules (trainable) → 2-layer MLP projector → LLM backbone

- **Critical path:**
  1. Image preprocessed for both encoders
  2. Parallel forward passes through frozen encoders
  3. At each mapped layer pair: augmenter features → projection → K, V; anchor features → Q
  4. MHCA output added via residual to anchor stream
  5. Enriched anchor features continue through remaining layers
  6. Final visual tokens → projector MLP → prepended to text embeddings → LLM

- **Design tradeoffs:**
  - Number of fusion points: More points increase enrichment capacity but add parameters and computation
  - Frozen vs. fine-tuned encoders: Freezing preserves pre-trained knowledge but limits adaptability
  - Unidirectional vs. bidirectional: Unidirectional preserves anchor stability; bidirectional could enable mutual refinement

- **Failure signatures:**
  - Hallucination on POPE: Model claims objects exist when absent → visual grounding failure
  - Poor OCR on TextVQA: Text recognition fails → DINOv2 perceptual signal may not be reaching final representations
  - No improvement over CLIP-Only: Fusion may be inactive; verify MHCA weights are training

- **First 3 experiments:**
  1. Run inference with FtZ vs. CLIP-Only on a single image; visualize attention weights from MHCA to confirm augmenter is being queried
  2. Train with only one fusion point (final layer) vs. full hierarchical fusion to quantify depth-aligned contribution
  3. Replace learned W_proj with random initialization frozen during training; if performance collapses, dimensionality alignment is critical

## Open Questions the Paper Calls Out
- Does the FtZ framework's relative performance advantage persist when integrated with large-scale LLM backbones (e.g., 7B+ parameters) compared to the small-scale models tested?
- What are the specific inference latency and computational overhead trade-offs introduced by the dual-encoder architecture and Multi-Head Cross-Attention mechanism?
- Can the fusion mechanism effectively scale to incorporate three or more heterogeneous expert vision encoders without feature interference or saturation?

## Limitations
- The unidirectional fusion design choice is justified theoretically but lacks empirical comparison against bidirectional alternatives
- The specific layer mapping strategy (relative depth) is plausible but not validated against alternatives like absolute index mapping
- The paper does not analyze whether the DINOv2 features are actually being attended to in practice (attention weight distributions)
- No ablation studies examine the impact of different numbers of fusion points or different anchor/augmenter pairs

## Confidence

**High Confidence:** The core experimental results showing FtZ outperforming baselines on all reported benchmarks.

**Medium Confidence:** The mechanism claim that unidirectional fusion preserves anchor stability while enabling enrichment; this is logically sound but lacks ablation evidence.

**Medium Confidence:** The hierarchical depth-aligned fusion providing progressive enrichment; reasonable but not empirically validated against other fusion strategies.

**Low Confidence:** The claim that CLIP sacrifices fine-grained details while DINOv2 preserves them is asserted rather than demonstrated through feature analysis.

## Next Checks
1. **Attention Visualization:** Run FtZ inference on sample images and visualize the cross-attention weights from anchor to augmenter at each fusion point to confirm that DINOv2 features are actually being queried and attended to.
2. **Ablation Study:** Train FtZ with different numbers of fusion points (0, 1, 2, 4, 8) to quantify the marginal benefit of each additional fusion layer and identify optimal configuration.
3. **Encoder Swap Experiment:** Replace DINOv2 with another self-supervised perceptual encoder (e.g., MAE) while keeping CLIP as anchor to test whether the fusion mechanism itself, rather than the specific encoder choice, drives performance improvements.