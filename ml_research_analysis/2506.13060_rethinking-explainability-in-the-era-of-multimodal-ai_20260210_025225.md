---
ver: rpa2
title: Rethinking Explainability in the Era of Multimodal AI
arxiv_id: '2506.13060'
source_url: https://arxiv.org/abs/2506.13060
tags:
- multimodal
- explanation
- modality
- explanations
- unimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that unimodal explanation techniques fail for
  multimodal AI systems because they miss critical cross-modal interactions. The author
  identifies three key shortcomings: unimodal explanations fail to capture the multimodal
  attribution gap, attention weights misrepresent causal influence, and mechanistic
  interpretability tools don''t scale to multimodal architectures.'
---

# Rethinking Explainability in the Era of Multimodal AI

## Quick Facts
- arXiv ID: 2506.13060
- Source URL: https://arxiv.org/abs/2506.13060
- Reference count: 12
- Key outcome: Unimodal explanation techniques fail for multimodal AI systems because they miss critical cross-modal interactions, requiring new modality-aware explanation methods.

## Executive Summary
This paper identifies fundamental limitations in current explainability methods when applied to multimodal AI systems. The author demonstrates that unimodal techniques fail because they cannot capture cross-modal interactions, attention weights misrepresent causal influence, and mechanistic interpretability tools don't scale to multimodal architectures. The work proposes three formal desiderata—Granger-style modality influence, synergistic faithfulness, and unified stability—as both evaluation criteria and design guidelines for developing truly multimodal explanation methods.

## Method Summary
The paper proposes three formal desiderata for multimodal explanations without implementing them. The methods include Granger-style modality influence (ablating modalities to measure dependency), synergistic faithfulness (testing sufficiency and necessity of explanations), and unified stability (applying Lipschitz continuity to cross-modal perturbations). The paper applies Logit Lens analysis to TEA-GLM on Cora dataset and uses concept-based explanations via Splice to demonstrate current methods' failures.

## Key Results
- Unimodal explanations cannot capture the "Attribution Gap" where one modality's influence depends on another's presence
- Attention weights reflect alignment rather than causal influence, making them unreliable for multimodal explanations
- Standard mechanistic interpretability tools like Logit Lens fail when applied across heterogeneous modalities with entangled fusion layers

## Why This Works (Mechanism)

### Mechanism 1: Granger-style Modality Influence
**Claim**: Unimodal explanations misrepresent behavior by missing dependencies between modalities.
**Mechanism**: Quantifies dependency by ablating (removing) modality $m$ and measuring how the remaining modality $n$'s explanation shifts relative to output change. If output changes significantly but explanation doesn't update, it's unfaithful.
**Core assumption**: The model's true reliance on a modality can be approximated by the difference in output probability when that modality is removed.
**Evidence anchors**: Abstract identifies failure to capture cross-modal interactions; section 2.1 explains the interpretability vacuum; section 3 defines ablated output change; MultiSHAP validates interaction measurement.

### Mechanism 2: Synergistic Faithfulness
**Claim**: Feature attribution highlights active features without verifying necessity for decisions.
**Mechanism**: Enforces sufficiency and necessity tests by constructing perturbed inputs—$x_{keep}$ (retains highlighted features) and $x_{remove}$ (removes highlighted features). Faithful explanations should preserve performance on $x_{keep}$ and degrade on $x_{remove}$.
**Core assumption**: Explanation objects can be precisely mapped back to structural input perturbations.
**Evidence anchors**: Abstract proposes synergistic faithfulness; section 3 defines sufficiency and necessity constraints; EvalxNLP discusses benchmarking faithfulness.

### Mechanism 3: Unified Stability
**Claim**: Explanation methods are unstable to small input perturbations, worsened in multimodal settings.
**Mechanism**: Applies Lipschitz continuity constraints—small perturbation in modality $m$ should induce bounded change in modality $n$'s explanation when model output remains constant.
**Core assumption**: Semantic equivalence implies internal reasoning should remain relatively constant.
**Evidence anchors**: Section 3 defines cross-modal stability; section 2.3 shows graph token representations exhibit minimal evolution; Logit Lens demonstrates stability failures.

## Foundational Learning

- **Concept: Attention vs. Causality**
  - Why needed here: Paper argues attention weights are misused as explanations since they reflect alignment, not causal influence
  - Quick check: If model attends heavily to an image region, does masking it necessarily change prediction? (If no, attention ≠ causal explanation)

- **Concept: Ablation Studies**
  - Why needed here: Granger-style influence relies on controlled ablations to measure dependency
  - Quick check: When removing text, should you use empty string, [MASK], or random noise? (Paper suggests "neutral reference")

- **Concept: Lipschitz Continuity**
  - Why needed here: Unified stability formalizes cross-modal stability using Lipschitz constants
  - Quick check: High Lipschitz constant means small input change causes small or large output change? (Large)

## Architecture Onboarding

- **Component map**: Heterogeneous Encoders (ResNet/Transformer/GNN) -> Fusion Layer -> Explanation Function ($E$)
- **Critical path**: The Fusion Layer is highest-risk—standard mechanistic tools fail because they cannot trace cross-modal signal flow
- **Design tradeoffs**: Modularity vs. Entanglement (fusion layers often irreversibly entangle modalities); Unimodal vs. Multimodal Explainers (concatenating unimodal explanations misses interaction effects)
- **Failure signatures**: 
  - "Plausible" Shortcut (heatmap looks correct but model uses text shortcut)
  - Static Token Evolution (graph tokens remain static across layers while text tokens evolve)
- **First 3 experiments**:
  1. Baseline Ablation Test: Ablate image in multimodal model and check if text explanation changes
  2. Semantic Stability Check: Apply output-invariant perturbation to image and measure shift in text explanation
  3. Sufficiency Validation: Mask non-highlighted features and verify prediction accuracy collapses

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can the three formal desiderata be operationalized into quantitative metrics for comparing multimodal explanation methods?
**Basis in paper**: [explicit] Proposes desiderata as evaluation criteria and design guidelines but doesn't implement or validate them
**Why unresolved**: Provides formal definitions but stops short of demonstrating feasibility or showing how to compute constants in practice
**What evidence would resolve it**: Working evaluation suite computing these metrics across existing methods on standardized benchmark

### Open Question 2
**Question**: How can mechanistic interpretability tools be adapted to capture cross-modal information flow in heterogeneous encoder stacks?
**Basis in paper**: [explicit] States we lack analogous tools for multimodal systems where methodological assumptions don't transfer
**Why unresolved**: Multimodal models fuse modalities through entangled representations where perturbing one modality has unpredictable effects
**What evidence would resolve it**: Demonstrating modified tool can identify and intervene on cross-modal circuits predictably

### Open Question 3
**Question**: What synthetic multimodal tasks can be constructed where ground-truth modality contributions are known for benchmarking?
**Basis in paper**: [explicit] Suggests creating synthetic tasks where ground-truth contribution is known to test explanation recovery
**Why unresolved**: No benchmark exists; existing datasets lack labeled modality contributions
**What evidence would resolve it**: Released benchmark dataset with controlled ground-truth modality dependencies

## Limitations

- Proposed desiderata lack empirical validation across diverse multimodal architectures
- Key implementation details for perturbation constructors and threshold values remain unspecified
- No quantitative benchmarks showing proposed methods outperform existing techniques

## Confidence

- **High Confidence**: Core observation that unimodal explanations fail to capture cross-modal interactions is well-supported
- **Medium Confidence**: Granger-style modality influence metric is methodologically sound but needs more empirical validation
- **Low Confidence**: Unified stability framework lacks corpus evidence and concrete threshold specifications

## Next Checks

1. Cross-Architecture Validation: Test Granger-style influence across vision-language, graph-language, and audio-language architectures
2. Empirical Benchmarking: Compare synergistic faithfulness tests against Deletion/Insertion curves and Region Perturbation metrics
3. Stability Quantification: Implement unified stability on clinically deployed multimodal model and measure actual Lipschitz constants