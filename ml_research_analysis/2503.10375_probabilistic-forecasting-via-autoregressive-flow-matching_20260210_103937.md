---
ver: rpa2
title: Probabilistic Forecasting via Autoregressive Flow Matching
arxiv_id: '2503.10375'
source_url: https://arxiv.org/abs/2503.10375
tags:
- flow
- forecasting
- autoregressive
- training
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Autoregressive Flow Matching (AFM), a probabilistic
  forecasting method that combines autoregressive modeling with flow matching to predict
  multivariate time series. AFM decomposes the forecasting problem into a sequence
  of conditional distributions, each modeled via a shared flow that transforms a simple
  base distribution into the next observation distribution conditioned on past observations
  and covariates.
---

# Probabilistic Forecasting via Autoregressive Flow Matching

## Quick Facts
- arXiv ID: 2503.10375
- Source URL: https://arxiv.org/abs/2503.10375
- Authors: Ahmed El-Gazzar; Marcel van Gerven
- Reference count: 40
- Autoregressive Flow Matching (AFM) achieves state-of-the-art probabilistic forecasting performance with significant improvements in CRPS scores across multiple datasets

## Executive Summary
This paper introduces Autoregressive Flow Matching (AFM), a probabilistic forecasting method that combines autoregressive modeling with flow matching to predict multivariate time series. AFM decomposes the forecasting problem into a sequence of conditional distributions, each modeled via a shared flow that transforms a simple base distribution into the next observation distribution conditioned on past observations and covariates. The method leverages flow matching to learn these transformations efficiently without requiring simulation or handcrafted noise schedules.

Empirical evaluation on both simulated dynamical systems and real-world datasets shows AFM achieves state-of-the-art performance. On real-world forecasting tasks, AFM outperforms baselines with an average CRPS of 0.042 on Electricity data (vs 0.045 for the second-best model), 0.009 on Exchange data, and 0.284 on Solar data (vs 0.343 for the second-best). The method demonstrates superior extrapolation capabilities with 93-98% error reduction compared to non-autoregressive baselines, particularly in challenging scenarios.

## Method Summary
AFM uses an autoregressive architecture where each step in the sequence is conditioned on all previous observations and covariates. The core innovation is using flow matching to learn the conditional distributions at each timestep, transforming a simple base distribution (typically Gaussian) into the target distribution. This approach combines the flexibility of autoregressive models with the efficient training of flow matching, avoiding the need for expensive simulations during training. The model maintains a shared flow network across all timesteps, conditioned on the relevant history at each step.

## Key Results
- Achieves state-of-the-art performance on real-world forecasting tasks with average CRPS scores of 0.042 (Electricity), 0.009 (Exchange), and 0.284 (Solar)
- Demonstrates superior extrapolation capabilities with 93-98% error reduction compared to non-autoregressive baselines
- Provides well-calibrated uncertainty estimates while simplifying the optimization problem through flow matching

## Why This Works (Mechanism)
AFM works by decomposing multivariate forecasting into a sequence of simpler, conditional density estimation problems. The autoregressive structure allows the model to build up predictions step-by-step, with each prediction conditioned on all previous observations. Flow matching provides an efficient way to learn these conditional distributions by optimizing a deterministic objective rather than requiring expensive simulations. This combination enables the model to capture complex temporal dependencies while maintaining computational efficiency during training.

## Foundational Learning
- Autoregressive modeling: Why needed - to decompose complex multivariate distributions into tractable conditional distributions; Quick check - verify that conditioning on sufficient history captures relevant dependencies
- Flow matching: Why needed - to learn complex conditional distributions efficiently without simulation; Quick check - confirm that the flow transformation is invertible and density-preserving
- Conditional density estimation: Why needed - to model the probability distribution of the next observation given past data; Quick check - evaluate calibration of predicted uncertainties
- Multivariate time series forecasting: Why needed - to handle interdependent variables evolving over time; Quick check - assess performance on datasets with varying degrees of inter-variable correlation

## Architecture Onboarding

**Component map:** Input history -> Encoder -> Shared Flow Network -> Conditional Distribution -> Prediction

**Critical path:** Past observations and covariates → Encoder → Flow network → Base distribution transformation → Output distribution

**Design tradeoffs:** Autoregressive structure enables handling variable horizons and extrapolation but increases sequential computation; Flow matching simplifies training but requires careful design of the flow architecture

**Failure signatures:** Poor performance on highly non-linear dynamics; degradation with very long sequences; miscalibration of uncertainty estimates

**3 first experiments:**
1. Evaluate CRPS and calibration metrics on synthetic dynamical systems with known ground truth
2. Compare performance against autoregressive baselines (RNN, Transformer) on Electricity dataset
3. Test extrapolation capability by evaluating on sequences longer than training data

## Open Questions the Paper Calls Out
The paper identifies several areas for future work, including extending AFM to handle irregular time series, incorporating hierarchical structures for long-range dependencies, and exploring alternative base distributions beyond Gaussian. The authors also note the need for more extensive evaluation on diverse real-world datasets to assess generalizability across different data characteristics and forecasting scenarios.

## Limitations
- Computational complexity for long sequences not thoroughly discussed
- Scalability to very high-dimensional time series remains an open question
- Generalizability across diverse real-world scenarios needs more extensive validation

## Confidence

**Major Claim Confidence Assessment:**
- Theoretical framework and methodology: High
- Empirical performance improvements: Medium
- Extrapolation capabilities: Medium
- Computational efficiency: Low

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (autoregressive structure vs flow matching) to the overall performance.

2. Evaluate AFM's performance on additional real-world datasets with varying characteristics (e.g., higher dimensionality, different noise distributions, non-stationary patterns).

3. Benchmark AFM against other state-of-the-art probabilistic forecasting methods on long-sequence forecasting tasks to assess scalability and efficiency.