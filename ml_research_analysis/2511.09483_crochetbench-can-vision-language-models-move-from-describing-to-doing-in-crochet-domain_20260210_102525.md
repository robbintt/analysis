---
ver: rpa2
title: 'CrochetBench: Can Vision-Language Models Move from Describing to Doing in
  Crochet Domain?'
arxiv_id: '2511.09483'
source_url: https://arxiv.org/abs/2511.09483
tags:
- next
- join
- crochet
- each
- around
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CrochetBench is a new benchmark designed to evaluate whether multimodal\
  \ large language models can generate executable crochet procedures from images.\
  \ It introduces four tasks of increasing difficulty\u2014stitch recognition, instruction\
  \ selection, natural-language instruction generation, and translation into an executable\
  \ crochet DSL\u2014with 6,085 patterns integrated with the CrochetPARADE DSL for\
  \ automated validation."
---

# CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?

## Quick Facts
- arXiv ID: 2511.09483
- Source URL: https://arxiv.org/abs/2511.09483
- Authors: Peiyu Li; Xiaobao Huang; Nitesh V. Chawla
- Reference count: 40
- Primary result: VLMs can recognize stitches and retrieve instructions but fail to generate executable crochet procedures

## Executive Summary
CrochetBench introduces a new benchmark to evaluate whether multimodal large language models can move beyond describing crochet patterns to generating executable procedures. The benchmark features four progressively challenging tasks—stitch recognition, instruction selection, natural-language generation, and translation to an executable crochet DSL—with 6,085 patterns. Across all tasks, models show a fundamental gap: they excel at perception and retrieval but struggle with procedural synthesis and execution, highlighting a critical limitation in current VLM capabilities for translating visual understanding into actionable instructions.

## Method Summary
The benchmark parses 6,085 crochet patterns from Yarnspirations using GPT-4o-mini into structured JSON with associated images. Four tasks are defined: multi-label stitch recognition from images (6,009 examples), 4-way multiple-choice instruction selection (6,003 examples), natural-language instruction generation from images (6,009 examples), and translation to an executable CrochetPARADE DSL (119 step-level examples, 100 project-level examples). Nine VLMs are evaluated using standardized prompts, with DSL outputs validated through compilation and optional rendering. Finetuning experiments use Qwen2-VL-7B with full-parameter updates on Task C data.

## Key Results
- Models achieve 50-68% accuracy on stitch recognition and instruction selection but <6% BLEU on instruction generation
- DSL compilation yields <10% valid programs across all models, with undefined stitches and unbalanced brackets as primary failure modes
- Larger models (Qwen2-VL-72B, Gemma-3-27B) show 3-5x higher undefined-stitch rates compared to smaller counterparts
- Finetuning improves BLEU scores by 238% but fails to improve executable program validity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Procedural competence decomposes into a hierarchy of capabilities, with perception preceding synthesis
- Mechanism: Four-task progression (recognition → selection → generation → formalization) isolates failure points; performance degrades sharply as tasks require active synthesis rather than passive retrieval
- Core assumption: Models that cannot recognize stitches or ground instructions will fail at generating procedures
- Evidence anchors: Task A/B scores (50-68%) drop to <6% BLEU on Task C; ISO-Bench finds multimodal causal reasoning breaks on procedural dependencies

### Mechanism 2
- Claim: DSL-based validation exposes structural errors that text metrics miss
- Mechanism: CrochetPARADE DSL provides syntax checking, state validation, and rendering; compilation failure directly indicates unexecutable output
- Core assumption: A procedure that compiles and renders is closer to correct than one that does not
- Evidence anchors: Error taxonomy shows undefined stitches and unbalanced brackets dominate failures; DINO similarity 0.10-0.17 even for valid programs

### Mechanism 3
- Claim: Larger models hallucinate more out-of-domain symbols in constrained DSLs
- Mechanism: Increased model capacity correlates with higher undefined-stitch rates; larger models trained on broader distributions generate plausible-sounding but DSL-invalid tokens
- Core assumption: Scale improves general reasoning fluency but not constrained symbolic generation without domain-specific training
- Evidence anchors: Qwen2-VL-72B: 72.0% undefined stitches vs Qwen2-VL-7B: 13.6%; TAMA notes tool-constrained procedural domains challenge VLMs

## Foundational Learning

- **Domain-specific languages (DSLs) and formal grammars**
  - Why needed here: CrochetPARADE DSL is the evaluation target; understanding why exact string matching fails is essential for interpreting metrics
  - Quick check question: Why does the benchmark use compilation success + DINO similarity instead of exact DSL match?

- **Stateful procedural reasoning**
  - Why needed here: Crochet patterns have state (stitch counts, position, round/row structure); errors propagate forward
  - Quick check question: Why do early-step DSL translations (steps 1-2) have lower valid pattern rates than late-step (steps 1-6) given prefix context?

- **Multi-label classification vs sequence generation**
  - Why needed here: Task A is multi-label (multiple stitches per image); Task C/D are sequence generation; different inductive biases and error modes
  - Quick check question: Why might high precision but low recall on Task A lead to different downstream failures than low precision but high recall?

## Architecture Onboarding

- **Component map:**
  Data pipeline -> GPT-4o-mini JSON parser -> Four task datasets (A: 6,009; B: 6,003; C: 6,009; D: 119 step / 100 project)
  -> VLM inference with task-specific prompts -> text or DSL output
  -> CrochetPARADE compiler -> valid/invalid + error category
  -> Optional rendering -> DINO similarity to ground-truth image

- **Critical path:**
  1. Parse patterns and construct four datasets
  2. Run inference with standardized prompts across all models
  3. For Task D, feed DSL output through CrochetPARADE validator
  4. Categorize failures using error taxonomy
  5. For valid DSL, render and compute DINO similarity

- **Design tradeoffs:**
  - Step-level vs project-level DSL: Step-level gives prefix context (tests continuation); project-level tests full synthesis
  - Text metrics vs execution: BLEU is fast but misses structural errors; execution is slow but catches fatal flaws
  - Finetuning target: Task C finetuning improves BLEU (+238%) but not valid pattern rate

- **Failure signatures:**
  - Undefined stitches: Model generates plausible-sounding but DSL-undefined tokens
  - Unbalanced brackets: Missing closing parentheses in repeat structures
  - Fluency without correctness: High BLEU with 0% valid patterns (finetuned models)
  - Scale-induced hallucination: Larger model → more undefined stitches (Qwen2-VL-72B: 72% vs 7B: 13.6%)

- **First 3 experiments:**
  1. Replicate Table 2 baseline on Tasks A/B/C with 2-3 open-source models to verify setup
  2. Run Task D step-level with varying prefix lengths; correlate context depth with valid pattern rate
  3. Finetune on Task C and measure both BLEU improvement and any cross-task transfer to Task D

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can execution-feedback training or program-guided learning paradigms improve procedural correctness in multimodal models beyond supervised finetuning?
- Basis in paper: In the Limitations section, the authors state: "Exploring alternative training paradigms, such as program guided learning or learning with execution feedback, remains an important direction for future work."
- Why unresolved: Only supervised finetuning on a single architecture was tested; no execution-guided or reinforcement learning approaches were explored
- What evidence would resolve it: Comparing supervised finetuning against methods that incorporate DSL compilation feedback or reinforcement learning with validity rewards

### Open Question 2
- Question: Why do larger models produce higher rates of undefined-stitch errors, and can this symbolic hallucination be mitigated without sacrificing scale benefits?
- Basis in paper: Results show larger models exhibit substantially higher undefined-stitch errors (Qwen2-VL-72B: 72.0% vs. Qwen2-VL-7B: 13.6%)
- Why unresolved: The mechanism driving increased hallucination with scale is unclear, and no mitigation strategies were tested
- What evidence would resolve it: Ablation studies analyzing whether constrained decoding, vocabulary restriction, or calibration techniques reduce undefined-stitch errors in larger models

### Open Question 3
- Question: How can models bridge the gap between generating syntactically valid DSL programs and producing semantically correct outputs that match target visual structures?
- Basis in paper: DINO similarity scores between DSL-rendered outputs and ground-truth images remain uniformly low (0.10–0.17) across all models
- Why unresolved: The benchmark validates compilation success but not semantic equivalence; models may produce valid programs that construct incorrect geometries
- What evidence would resolve it: Training models with visual similarity feedback, or developing intermediate representations that better capture topological and geometric constraints

### Open Question 4
- Question: Does improving procedural reasoning on CrochetBench transfer to other procedural domains requiring executable synthesis, such as robotics or CAD?
- Basis in paper: The authors acknowledge: "the benchmark focuses on a single creative domain, crochet, which although representative of structured symbolic procedures, may not capture all forms of procedural reasoning found in domains such as robotics"
- Why unresolved: No cross-domain experiments were conducted; crochet-specific findings may not generalize
- What evidence would resolve it: Evaluating models trained or improved on CrochetBench on procedural benchmarks in other domains (e.g., CAD code generation, robot task planning)

## Limitations

- The benchmark's scope is limited to a single craft domain with relatively standardized patterns; results may not generalize to more variable or less-structured procedural domains
- Validation relies on a custom DSL that may reject valid human patterns due to grammatical constraints, creating false negatives
- No human baseline is provided for comparison, making it difficult to assess whether the observed model failures are truly fundamental or simply reflect domain expertise gaps

## Confidence

- **High confidence**: Models can recognize stitches and retrieve plausible instructions but fail at synthesis (well-supported by consistent performance drops across tasks)
- **Medium confidence**: DSL-based validation is superior to text metrics for procedural correctness (reasonable but assumes no false negatives from DSL constraints)
- **Medium confidence**: Larger models hallucinate more DSL-invalid symbols (data shows correlation but doesn't establish causation or test finetuning effects)
- **Low confidence**: Four-task hierarchy cleanly isolates failure modes (assumed but not experimentally validated through ablation studies)

## Next Checks

1. Test whether models trained on CrochetBench can generalize to patterns from other sources or different crochet traditions to assess domain transferability
2. Implement human evaluation of DSL-rejected patterns to quantify false negative rate and refine the grammar if needed
3. Conduct ablation studies on model size vs. finetuning data volume to disentangle whether scale effects are due to capacity or exposure