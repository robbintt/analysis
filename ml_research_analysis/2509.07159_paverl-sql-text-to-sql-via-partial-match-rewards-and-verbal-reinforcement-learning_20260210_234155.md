---
ver: rpa2
title: 'PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement
  Learning'
arxiv_id: '2509.07159'
source_url: https://arxiv.org/abs/2509.07159
tags:
- training
- arxiv
- execution
- text-to-sql
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents PaVeRL-SQL, a framework combining partial-match
  rewards and verbal reinforcement learning to improve Text-to-SQL accuracy on industry-scale
  databases. The authors introduce two complementary pipelines: a verbal self-evaluation
  workflow that uses large language models to generate and rank SQL candidates, and
  a chain-of-thought reinforcement learning pipeline that trains a small OmniSQL-7B
  model with execution feedback.'
---

# PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2509.07159
- **Source URL**: https://arxiv.org/abs/2509.07159
- **Reference count**: 40
- **Primary result**: Combines partial-match rewards and verbal RL to achieve state-of-the-art Text-to-SQL accuracy on industry-scale databases.

## Executive Summary
PaVeRL-SQL introduces a framework that combines partial-match rewards and verbal reinforcement learning to improve Text-to-SQL accuracy on industry-scale databases. The authors propose two complementary pipelines: a verbal self-evaluation workflow that uses large language models to generate and rank SQL candidates, and a chain-of-thought reinforcement learning pipeline that trains a small OmniSQL-7B model with execution feedback. Key innovations include a fractional execution accuracy metric (EXf) that provides denser training signals than binary exact match, a two-stage GRPO training schedule that achieves strong performance within 20 epochs, and mixed-dialect training that yields threefold gains for low-resource SQL dialects.

## Method Summary
PaVeRL-SQL presents two complementary Text-to-SQL pipelines: a verbal self-evaluation workflow for inference-only deployment, and a chain-of-thought reinforcement learning pipeline for on-prem model training. The verbal pipeline generates multiple SQL candidates, executes them against the target database, and uses a large language model to score and select the best candidate. The RL pipeline trains a small OmniSQL-7B model using Group Relative Policy Optimization (GRPO) with execution-based rewards, including a novel fractional execution accuracy metric (EXf) that provides denser training signals than binary exact match. The framework employs a two-stage GRPO training schedule with best-checkpoint restart and achieves strong performance within 20 epochs, while mixed-dialect training enables significant improvements for low-resource SQL dialects.

## Key Results
- Verbal RL pipeline achieves 7.4% higher execution accuracy than state-of-the-art on Spider2.0-SQLite
- CoT RL pipeline improves execution accuracy by 1.4% over baseline
- Two-stage GRPO training schedule achieves strong performance within 20 epochs
- Mixed-dialect training yields threefold gains for low-resource SQL dialects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Partial-match rewards (EXf) provide denser gradients than binary execution accuracy, improving sample efficiency.
- **Mechanism**: The fractional execution accuracy metric (EXf) scores column-by-column matches between generated and gold tables, yielding values in [0,1]. The reward function scales payoff by EXf (10 × EXf for valid results), converting sparse pass/fail into graded feedback that reduces gradient variance during GRPO updates.
- **Core assumption**: Partial correctness (some columns right) correlates with being on a productive optimization path toward full correctness.
- **Evidence anchors**:
  - [abstract] "fractional execution accuracy metric (EXf) that provides denser training signals than binary exact match"
  - [section III-A] "EXf ∈ [0,1] measures... the proportion of the result column matching the golden result"
  - [section III-C2] "R = 10·EXf, if SQL executes and returns results; 0.5, if SQL executes but is incorrect; 0, otherwise"
  - [corpus] Weak direct evidence on partial-match rewards; Reasoning-SQL [97335] also explores partial rewards but with different formulation.

### Mechanism 2
- **Claim**: Verbal self-evaluation approximates group-relative preference signals without gradient updates, lifting zero-shot accuracy.
- **Mechanism**: For each question, sample SQL candidates until K=10 execute successfully (or 200 attempts cap). The same backbone LLM scores all candidates via a judging prompt; the highest mean score (20 repetitions) is selected. This approximates GRPO's group-wise advantage estimation at inference time.
- **Core assumption**: The backbone LLM can reliably discriminate SQL quality when comparing candidates side-by-side, even if it cannot generate perfect SQL in one shot.
- **Evidence anchors**:
  - [abstract] "verbal self-evaluation workflow that uses large language models to generate and rank SQL candidates"
  - [section III-B] "This 'generate-and-judge' loop approximates a group-relative signal without gradient updates"
  - [table II] Verbal RL achieves 37.0 EX on Spider2.0-SQLite vs. 29.6 for CHESS (same GPT-5 mini backbone)
  - [corpus] Related work (ReFoRCE [42], MCTS-SQL [90958]) explores iterative refinement, but verbal self-judging specifically is less studied externally.

### Mechanism 3
- **Claim**: Two-stage GRPO with best-checkpoint restart stabilizes training and accelerates convergence within ~20 epochs.
- **Mechanism**: Stage 1 uses warm-up (3% steps), linear ramp (3–10%), then cosine decay. Stage 2 restarts from the best Stage 1 checkpoint (not necessarily the final one), with learning rate adjusted based on convergence behavior (raised if plateauing, halved if fluctuating). This avoids oscillation and exploits early high-performing regions.
- **Core assumption**: The best early checkpoint captures a policy in a favorable basin that benefits from fresh optimization with adjusted step size.
- **Evidence anchors**:
  - [abstract] "two-stage GRPO training schedule that achieves strong performance within 20 epochs"
  - [section III-C1b] "Stage two: Always start from the best model (highest greedy decoding accuracy) of stage one"
  - [figure 2] Training curves show EX improving through both stages with distinct LR schedules
  - [corpus] DeepSeekLLM [28] introduces two-stage training for LLMs; PaVeRL adapts it specifically for Text-to-SQL.

## Foundational Learning

- **Concept**: Group Relative Policy Optimization (GRPO)
  - **Why needed here**: GRPO estimates advantages by comparing outputs within a group of rollouts from the same prompt, removing need for a separate value function. This simplifies RL for text-to-SQL where execution feedback is the primary signal.
  - **Quick check question**: Can you explain why GRPO uses group-wise advantages instead of a learned critic?

- **Concept**: Execution-based reward design
  - **Why needed here**: SQL is directly verifiable by execution against a database. Designing rewards that reflect correctness, syntax validity, and partial progress is central to stabilizing RL training in this domain.
  - **Quick check question**: What problems arise if rewards are purely binary (0 for wrong, 1 for correct) in SQL generation?

- **Concept**: In-context self-evaluation
  - **Why needed here**: The verbal RL pipeline relies on an LLM judging its own candidates. Understanding how to prompt for comparative scoring and aggregate judgments is essential for the inference-only track.
  - **Quick check question**: Why might generating multiple SQL candidates and scoring them outperform single-shot generation?

## Architecture Onboarding

- **Component map**:
  - **Verbal RL Pipeline**: Generation prompt → Candidate sampling (up to 200 attempts) → Execution filter → Scoring prompt (20× repetition) → Mean aggregation → Selection
  - **CoT RL Pipeline**: Prompt (dialect + schema + context + question) → GRPO rollouts (G=10) → Execution → Reward computation (EXf-based) → Policy update with KL penalty

- **Critical path**:
  1. Precompute golden SQL execution results for all training samples before GRPO (avoids repeated execution during rollouts).
  2. Build schema strings with enriched metadata (PK/FK, MIN/MAX, top-3 text modes) for both training and inference.
  3. For Verbal RL, tune the cap (K=10, 200 attempts) based on cost/latency constraints; for CoT RL, monitor Stage 1 curve to trigger Stage 2 restart correctly.

- **Design tradeoffs**:
  - **Verbal RL vs. CoT RL**: Verbal RL requires no training but relies on a strong backbone LLM and multiple inference passes (higher latency/cost per query). CoT RL trains a compact 7B model for on-prem deployment but requires GPU hours and an executable database environment.
  - **Reward function**: The paper found that mixing positive and negative rewards caused instability; non-negative only (0, 0.5, 10×EXf) stabilized convergence. Assumption: this is dataset/architecture specific—verify on your data.
  - **Majority voting group size**: Figure 3 shows optimal around 32 samples; larger groups did not help and added cost.

- **Failure signatures**:
  - **Training instability**: Mean rewards oscillate wildly → check reward scale, ensure non-negative, reduce learning rate.
  - **Verbal judge drift**: Selected SQL scores high but fails execution → inspect scoring prompt, add execution-aware criteria, or clamp/normalize scores.
  - **Schema overflow**: Context window exceeded on large databases → apply schema compression (retain PK/FK, gold-referenced columns, random subset).

- **First 3 experiments**:
  1. **Baseline verification**: Run zero-shot Prompt 1 (with enriched schema) on your target database; compare to Verbal RL (K=10) to quantify lift from generate-and-judge.
  2. **Reward ablation**: Train CoT RL with binary reward (0/1) vs. EXf-scaled reward on a held-out validation split; log convergence speed and final EX.
  3. **Dialect transfer test**: If targeting a low-resource dialect (e.g., MariaDB), run mixed-dialect training (rich SQLite + limited target) and evaluate zero-shot on target dialect before/after training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can enriching reward signals beyond column-level matching (e.g., tuple-set comparisons, constraint satisfaction) further improve training stability and final accuracy?
- Basis in paper: [explicit] The conclusion states: "we will (i) enrich rewards beyond column-level signals (e.g., tuple-set comparisons, constraint satisfaction)."
- Why unresolved: Current EXf rewards only measure column-level matches, ignoring row-level precision and constraint violations that matter in production.
- What evidence would resolve it: Ablation experiments comparing column-level vs. tuple-set rewards on Spider 2.0 and BIRD, measuring both accuracy and training curve stability.

### Open Question 2
- Question: How sensitive is the verbal RL pipeline's judge-scoring mechanism to backbone model biases, and can cross-model calibration mitigate this?
- Basis in paper: [explicit] The conclusion lists as a limitation "residual sensitivity of judge-style scoring to backbone biases" and plans to "strengthen cross-model judging and calibration."
- Why unresolved: Using the same LLM as both generator and judge may introduce systematic scoring biases that favor syntactically similar outputs regardless of correctness.
- What evidence would resolve it: Systematic comparison of same-model vs. cross-model judging, measuring correlation between LLM scores and ground-truth execution accuracy.

### Open Question 3
- Question: How can PaVeRL-SQL be extended to multi-turn conversational Text-to-SQL while maintaining efficient RL training?
- Basis in paper: [explicit] The paper excludes "multi-turn dialogues style questions" from SynSQL training and states in the conclusion: "we also plan to extend PaVeRL-SQL to multi-turn interactions."
- Why unresolved: Current CoT RL pipeline assumes single-turn queries; multi-turn requires maintaining dialogue state and contextual grounding across turns.
- What evidence would resolve it: Extending GRPO training to multi-turn datasets (e.g., SParC, CoSQL) and evaluating on conversational benchmarks.

### Open Question 4
- Question: What schema compression strategies preserve sufficient context for accurate SQL generation on databases with thousands of tables?
- Basis in paper: [explicit] The conclusion states plans to "improve schema retrieval/compression for very large databases."
- Why unresolved: Current approach samples columns for long schemas; no systematic study of how retrieval quality affects RL training efficiency and generalization.
- What evidence would resolve it: Benchmarking schema retrieval methods (embedding-based, graph-based) on industrial-scale schemas with RL training curves and final accuracy.

## Limitations

- **Model access uncertainty**: The verbal RL pipeline relies on GPT-5-mini and gpt-oss-20B/120B models, which appear to be proprietary or unreleased.
- **Hyperparameter gaps**: Critical GRPO settings (exact max LR per dataset, KL penalty β, clipping threshold ε, precise learning rate schedules) are underspecified.
- **Dataset execution caching**: Precomputing golden SQL execution results for all training samples is assumed but not detailed for large datasets like BIRD.

## Confidence

- **High confidence**: The verbal RL pipeline's mechanism (generate-and-judge loop with mean aggregation) and its Spider2.0-SQLite result (37.0 EX vs. 29.6 CHESS) are directly supported by table evidence and prompt structure.
- **Medium confidence**: The two-stage GRPO schedule's effectiveness is well-described conceptually, but the exact trigger conditions for Stage 2 restart and LR adjustments are underspecified.
- **Medium confidence**: The EX_f metric's design and role in denser gradients are clearly explained, but its correlation with task utility (partial columns being useful) is assumed rather than empirically validated across domains.

## Next Checks

1. **Zero-shot baseline verification**: Run Prompt 1 (with enriched schema) on your target database and compare to the verbal RL pipeline (K=10) to quantify the generate-and-judge lift.
2. **Reward function ablation**: Train CoT RL with binary (0/1) vs. EXf-scaled rewards on a held-out validation split. Log convergence speed and final EX to confirm the partial-match reward improves sample efficiency.
3. **Schema overflow handling test**: For large schemas (e.g., BIRD), implement and evaluate the proposed column sampling strategy (PK/FK + gold-referenced + random subset). Measure token usage and EX impact to ensure the context window constraint is managed.