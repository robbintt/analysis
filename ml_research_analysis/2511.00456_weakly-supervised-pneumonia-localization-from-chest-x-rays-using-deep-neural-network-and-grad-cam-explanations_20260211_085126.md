---
ver: rpa2
title: Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural
  Network and Grad-CAM Explanations
arxiv_id: '2511.00456'
source_url: https://arxiv.org/abs/2511.00456
tags:
- pneumonia
- grad-cam
- image
- class
- localization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a weakly supervised deep learning framework
  for pneumonia classification and localization using chest X-rays. Instead of relying
  on costly pixel-level annotations, the method utilizes image-level labels and Gradient-weighted
  Class Activation Mapping (Grad-CAM) to generate clinically meaningful heatmaps highlighting
  pneumonia-affected regions.
---

# Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations

## Quick Facts
- arXiv ID: 2511.00456
- Source URL: https://arxiv.org/abs/2511.00456
- Authors: Kiran Shahi; Anup Bagale
- Reference count: 27
- Primary result: Weakly supervised framework achieves 96–98% classification accuracy and clinically meaningful pneumonia localization without pixel-level annotations

## Executive Summary
This study presents a weakly supervised deep learning framework for pneumonia classification and localization using chest X-rays. Instead of relying on costly pixel-level annotations, the method utilizes image-level labels and Gradient-weighted Class Activation Mapping (Grad-CAM) to generate clinically meaningful heatmaps highlighting pneumonia-affected regions. Seven pretrained models, including a Vision Transformer, were evaluated under identical training conditions using focal loss and patient-wise splits to prevent data leakage. Experimental results showed high classification accuracy (96–98%) across all models, with ResNet-18 and EfficientNet-B0 performing best. Grad-CAM visualizations confirmed that the methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. MobileNet-V3 emerged as an efficient lightweight alternative suitable for real-time deployment. The study demonstrates the potential of weakly supervised, interpretable models to enhance transparency and clinical trust in AI-assisted pneumonia screening.

## Method Summary
The proposed method employs a transfer learning approach using seven pretrained convolutional neural networks (CNNs) and one Vision Transformer (ViT) to classify chest X-rays as normal or pneumonia. Image-level labels are used for training, eliminating the need for expensive pixel-level annotations. Focal loss is employed to handle class imbalance. Grad-CAM is applied to generate heatmaps that highlight regions of the lung most relevant to the model's classification decision. Patient-wise data splitting is implemented to prevent data leakage. Model performance is evaluated using accuracy, precision, recall, and F1-score metrics, while localization effectiveness is assessed through the Lung Attention Ratio (LAR), which measures the proportion of attention focused on the lung region. The framework prioritizes interpretability by producing visual explanations alongside predictions.

## Key Results
- Classification accuracy: 96–98% across all models with ResNet-18 and EfficientNet-B0 achieving the highest performance
- Lung Attention Ratio (LAR): All models demonstrated high attention to lung regions with ResNet-18 showing the highest focus at 87.94%
- MobileNet-V3 emerged as an efficient lightweight alternative suitable for real-time deployment with competitive accuracy
- Grad-CAM visualizations consistently highlighted clinically relevant pneumonia-affected regions in chest X-rays

## Why This Works (Mechanism)
The framework's effectiveness stems from leveraging transfer learning with pretrained models that have learned general visual features, combined with focal loss to address class imbalance in pneumonia datasets. Grad-CAM exploits the gradient information flowing into the final convolutional layer to generate class-discriminative visualizations, enabling localization without explicit segmentation annotations. The use of patient-wise splitting prevents information leakage across training and test sets, ensuring robust evaluation. The combination of high classification accuracy with interpretable heatmaps addresses both the diagnostic need and the clinical requirement for explainable AI in medical imaging.

## Foundational Learning
- Transfer Learning: Why needed - Enables effective model training with limited medical imaging data; Quick check - Model converges faster than training from scratch
- Focal Loss: Why needed - Addresses class imbalance between pneumonia and normal cases; Quick check - Improves recall for minority pneumonia class
- Grad-CAM: Why needed - Generates interpretable heatmaps without requiring pixel-level annotations; Quick check - Heatmap highlights coincide with visible pneumonia regions
- Patient-wise Data Splitting: Why needed - Prevents data leakage when multiple images come from the same patient; Quick check - Patient IDs are properly randomized and separated
- Lung Attention Ratio: Why needed - Quantifies localization quality by measuring focus on lung regions; Quick check - Higher values indicate better localization performance

## Architecture Onboarding
**Component Map:** Chest X-ray image -> CNN/ViT feature extractor -> Classification head -> Focal loss training -> Grad-CAM visualization

**Critical Path:** Image input → Pretrained backbone → Global average pooling → Fully connected layer → Sigmoid output → Grad-CAM computation

**Design Tradeoffs:** The framework balances accuracy (using deeper models like ResNet-18) against computational efficiency (using MobileNet-V3), with Grad-CAM providing interpretability at minimal computational overhead.

**Failure Signatures:** Poor localization indicated by low Lung Attention Ratio; misclassification potentially due to focal loss hyperparameters or insufficient pretraining domain adaptation.

**First Experiments:** 1) Compare classification accuracy across all seven models under identical training conditions, 2) Generate and qualitatively assess Grad-CAM heatmaps for representative cases, 3) Calculate and compare Lung Attention Ratios across models to evaluate localization quality.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the proposed weakly supervised framework maintain high localization fidelity when applied to larger, multi-institutional datasets?
- Basis in paper: [explicit] The authors state the research is "limited to a single dataset" and explicitly suggest future evaluation on "RSNA Pneumonia and NIH ChestX-ray14" to enhance robustness.
- Why unresolved: The current results are derived exclusively from the Kermany dataset, which may not represent the full diversity of clinical scan quality and patient demographics found in larger benchmarks.
- What evidence would resolve it: Reporting of classification accuracy and Lung Attention Ratio (LAR) scores on the RSNA or NIH datasets using the identical training protocol.

### Open Question 2
- Question: Can the Grad-CAM attention mechanism effectively disentangle features in multi-label thoracic disease scenarios?
- Basis in paper: [explicit] The conclusion lists "multi-label thoracic disease localization" as a specific extension to increase the framework's clinical utility.
- Why unresolved: The current study is limited to binary classification (Normal vs. Pneumonia); it is unclear if the model can generate distinct heatmaps for co-occurring pathologies in the same image.
- What evidence would resolve it: Successful generation of spatially distinct heatmaps for different disease labels in a multi-label dataset.

### Open Question 3
- Question: Do the visual explanations align sufficiently with expert reasoning to foster clinical trust?
- Basis in paper: [explicit] The authors identify "radiologist reader studies" as a necessary step to validate the framework's translational impact.
- Why unresolved: While quantitative metrics like LAR show the model attends to the lung region, automated metrics cannot fully verify if the specific opacity highlighted is clinically meaningful to a human expert.
- What evidence would resolve it: Results from a clinical reader study where radiologists rate the relevance and diagnostic value of the generated heatmaps.

## Limitations
- No ground-truth segmentation masks available for direct quantitative validation of Grad-CAM localization accuracy
- Evaluation limited to single publicly available dataset (Kermany dataset) without testing on multi-institutional data
- Focal loss hyperparameter sensitivity not extensively explored across different model architectures

## Confidence
- Classification accuracy claims (96-98%): **High confidence** - Multiple models tested under identical conditions with consistent results
- Grad-CAM localization validity: **Medium confidence** - Visual agreement with clinical expectations, but no quantitative validation against expert annotations
- MobileNet-V3 deployment efficiency: **High confidence** - Based on established architectural properties and computational benchmarks
- Vision Transformer performance: **Medium confidence** - Limited to single ViT model without architectural variations

## Next Checks
1. Conduct radiologist review of Grad-CAM heatmaps using Dice coefficient or Intersection-over-Union metrics against expert-annotated pneumonia regions
2. Test model generalization on multi-institutional datasets with varying acquisition parameters and patient demographics
3. Perform ablation studies with different loss functions and focal loss hyperparameters to assess robustness to training configuration changes