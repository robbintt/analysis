---
ver: rpa2
title: Can LLMs Extract Frame-Semantic Arguments?
arxiv_id: '2502.12516'
source_url: https://arxiv.org/abs/2502.12516
tags:
- frame
- performance
- elements
- frame-semantic
- identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates large language models (LLMs) for frame-semantic
  parsing, a critical task in natural language understanding. The authors conduct
  a comprehensive evaluation of LLMs across different input formats, model scales,
  and datasets, revealing that JSON-based representations significantly improve performance.
---

# Can LLMs Extract Frame-Semantic Arguments?

## Quick Facts
- arXiv ID: 2502.12516
- Source URL: https://arxiv.org/abs/2502.12516
- Authors: Jacob Devasier; Rishabh Mediratta; Chengkai Li
- Reference count: 15
- One-line primary result: Fine-tuned smaller models (Qwen 2.5-3B) outperform larger ones (Llama 3.3-70B) on frame-semantic parsing; JSON-based output formats significantly improve performance.

## Executive Summary
This paper investigates large language models for frame-semantic parsing, a critical task in natural language understanding. The authors conduct a comprehensive evaluation across different input formats, model scales, and datasets, revealing that JSON-based representations significantly improve performance. Fine-tuned smaller models, such as Qwen 2.5 (3B), outperform much larger models like Llama 3.3 (70B). Additionally, the study introduces a novel frame identification method using predicted frame elements, achieving state-of-the-art performance on ambiguous targets (+1.2% accuracy). However, LLMs still struggle with out-of-domain data and unseen frame elements, highlighting limitations in generalization.

## Method Summary
The study fine-tunes open-source LLMs (Qwen 2.5, Phi-4, Llama) using LoRA adapters on FrameNet 1.7 full-text annotations. Inputs consist of sentences with targets marked in markdown (`**target**`), along with frame definitions and frame element (FE) descriptions. Four output formats are compared: Markdown, XML, JSON-Existing, and JSON-Complete. Models are evaluated on argument identification (precision, recall, F1) and frame identification accuracy. A novel method identifies frames by comparing predicted arguments across candidate frames, resolving ambiguity by selecting the frame with the most predicted elements.

## Key Results
- JSON-Existing format yields highest F1 (+3.9% over prior SOTA).
- Base models outperform instruction-tuned versions (Qwen 2.5-7B base: 0.768 vs instruct: 0.703 F1).
- Qwen 2.5 (3B) fine-tuned with LoRA outperforms Llama 3.3 (70B) on frame-semantic parsing.
- Novel frame identification method achieves +1.2% accuracy on ambiguous targets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured JSON output formats improve frame-semantic argument extraction accuracy compared to free-text or positional XML tagging.
- **Mechanism:** JSON enforces a key-value structure that maps directly to frame element names and text spans. This reduces the "cognitive load" on the model by allowing it to select from defined keys rather than generating open-ended text or managing precise span indices required by XML tags.
- **Core assumption:** The model's pre-training data contains sufficient code or data structures (JSON) to leverage this format bias effectively.
- **Evidence anchors:**
  - [abstract]: "reveal that JSON-based representations significantly enhance performance"
  - [section 4.2]: "We found that using JSON-Existing results in the highest precision, F1 score, and accuracy... likely due to the simplified cognitive load."
  - [corpus]: Neighboring paper "FRASE" supports the efficacy of structured representations for generalization, though specific JSON vs. XML comparisons are absent in neighbors.
- **Break condition:** If the output requires strict nesting or overlapping spans (which JSON objects do not natively support without complex escaping), performance may degrade or require post-processing.

### Mechanism 2
- **Claim:** Targeted fine-tuning (LoRA) allows smaller models to outperform significantly larger models on specific semantic tasks by encoding domain-specific schema knowledge.
- **Mechanism:** Fine-tuning adapts model weights to the specific FrameNet schema (definitions and frame-to-element mappings). General pre-training, even at 70B+ parameters, does not sufficiently encode these specific definitions, leading to poor zero-shot performance. Parameter-efficient tuning (LoRA) bridges this gap without full retraining.
- **Core assumption:** The performance gain stems from schema acquisition rather than mere exposure to the sentence structures in the training set.
- **Evidence anchors:**
  - [abstract]: "smaller models like Qwen 2.5 (3B) outperforming larger ones like Llama 3.3 (70B) through fine-tuning"
  - [section 4.4]: "Qwen 2.5 (3B) notably outperforming the much larger Llama 3.3 (70B)."
  - [corpus]: Neighbor "Do LLMs Encode Frame Semantics?" suggests LLMs have latent but insufficient frame knowledge, supporting the need for fine-tuning.
- **Break condition:** If the model is evaluated on frames or elements semantically distant from the fine-tuning distribution (unseen FEs), the mechanism fails (evidenced by the -27% F1 drop on unseen FEs).

### Mechanism 3
- **Claim:** Frame disambiguation is more accurate when conditioned on predicted argument spans rather than relying solely on the target word and context.
- **Mechanism:** Instead of classifying a target word directly, the model generates potential arguments for candidate frames. The "fit" of these arguments serves as a disambiguation signalâ€”correct frames allow for semantically coherent argument fills, whereas incorrect frames result in incoherent or empty predictions.
- **Core assumption:** The model can accurately predict arguments *given* a candidate frame even if it struggles to select the frame initially.
- **Evidence anchors:**
  - [abstract]: "introduce a novel approach to frame identification leveraging predicted frame elements, achieving state-of-the-art performance on ambiguous targets"
  - [section 4.8]: "This method showed strong performance, particularly on ambiguous targets... higher than any previous approach."
  - [corpus]: "Enhancing Frame Detection with Retrieval Augmented Generation" mentions RAG for frame detection, providing a contrast to this argument-prediction approach.
- **Break condition:** If the input sentence is too short to provide distinct arguments, or if multiple candidate frames share identical argument structures, the disambiguation signal fails (random selection is used as a fallback).

## Foundational Learning

### Concept: Frame Semantics (FrameNet)
- **Why needed here:** The paper assumes familiarity with the triad of Target (trigger word), Frame (scenario), and Frame Elements (semantic roles). Understanding that a target like "began" evokes a specific schema (Activity_start) is the foundation of the extraction task.
- **Quick check question:** Can you distinguish between a Lexical Unit (the word) and the Frame (the conceptual structure) it evokes?

### Concept: LoRA (Low-Rank Adaptation)
- **Why needed here:** The paper relies on LoRA to fine-tune models up to 72B parameters on limited hardware (single A100/H100). Understanding that LoRA freezes base weights and injects trainable rank-decomposition matrices is crucial for reproducing these results.
- **Quick check question:** Why would LoRA allow a 3B model to surpass a 70B model in a low-data domain?

### Concept: Instruction Tuning vs. Base Models
- **Why needed here:** The paper finds that instruction-tuned versions of models (e.g., Qwen 2.5-7B) perform *worse* than base models for this task. Understanding the trade-off between "following constraints" and "semantic reasoning" is vital for model selection.
- **Quick check question:** Why might strict instruction following (high IFEval score) negatively correlate with frame-semantic reasoning?

## Architecture Onboarding

### Component map:
Input Layer -> Processor (Base LLM + LoRA) -> Output Layer (JSON object with frame elements and spans)

### Critical path:
1. Pre-processing text to identify and mark targets.
2. Injecting frame definitions into the context window (critical for defining valid keys).
3. Generating JSON completion.
4. Parsing JSON to map strings to spans (exact match).

### Design tradeoffs:
- **Representation:** `JSON-Existing` (only present keys) maximizes Precision/F1; `JSON-Complete` (all keys, empty strings) maximizes Recall but lowers F1.
- **Model Selection:** Base models are preferred over Instruction-Tuned models for this specific task (negated correlation with IFEval).
- **Data:** Random subsampling performs better than "Diverse" or "Most-FE" subsampling, likely due to preserving natural distribution.

### Failure signatures:
- **Unseen Elements:** Massive performance drop (-27% F1) on elements not seen during training.
- **Out-of-Domain (OOD):** Performance collapses on datasets like YAGS (slang, poor grammar).
- **Instruction Drift:** Using GPT-4 to generate instructions resulted in significant performance degradation (F1 0.225 vs 0.471).

### First 3 experiments:
1. **Format Validation:** Run GPT-4o-mini (or similar) in few-shot mode to compare `Markdown`, `XML`, `JSON-Existing`, and `JSON-Complete` representations to confirm the JSON advantage.
2. **Architecture Baseline:** Fine-tune a small model (e.g., Qwen 2.5-0.5B or 3B) using LoRA on the full-text annotations to verify that smaller models can become competitive.
3. **Generalization Stress Test:** Evaluate the fine-tuned model specifically on "Unseen Frame Elements" to quantify the generalization gap and ensure the system fails gracefully.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the tie-breaking mechanism for ambiguous frame identification targets be improved beyond random selection?
- **Basis in paper:** [explicit] The authors state in the Limitations section that their "current method of handling multiple frames with predicted frame elements requires refinement" and that the "randomized prediction approach can lead to inconsistent outputs."
- **Why unresolved:** The current implementation uses a fixed random seed (0) for reproducibility but does not explore deterministic or semantic methods to resolve ties when multiple candidate frames have predicted elements.
- **What evidence would resolve it:** Implementing alternative tie-breaking strategies (e.g., based on element confidence scores or semantic similarity) and evaluating their impact on consistency and accuracy on ambiguous targets.

### Open Question 2
- **Question:** Do the performance benefits of JSON-based representations and LLM fine-tuning generalize to non-English FrameNet datasets?
- **Basis in paper:** [explicit] The authors note their research was "limited to the English FrameNet dataset" and consequently, their "findings may not generalize to other languages or semantic frameworks."
- **Why unresolved:** The study did not validate the transferability of its findings across different linguistic structures or semantic annotation standards.
- **What evidence would resolve it:** Replicating the experimental setup on multilingual FrameNet corpora or other semantic frameworks and comparing the relative performance of input representations.

### Open Question 3
- **Question:** Can fine-tuning larger closed-source models like GPT-4o surpass the state-of-the-art results achieved by open-source models?
- **Basis in paper:** [explicit] The authors state they were "unable to explore fine-tuning on certain high-performing models like GPT-4o" due to computational costs, acknowledging these models "may have achieved even stronger results."
- **Why unresolved:** High training and inference costs prevented the inclusion of the largest proprietary models in the fine-tuning experiments.
- **What evidence would resolve it:** Conducting fine-tuning experiments on GPT-4o (or equivalent frontier models) using the FrameNet full-text annotations and comparing F1 scores against the Qwen 2.5 (72B) baseline.

## Limitations
- **Generalization Gap:** Performance collapses on out-of-domain data (YAGS) and unseen frame elements (-27% F1), indicating poor robustness to real-world language variation.
- **Frame Identification Scope:** The novel frame identification method is only validated on ambiguous targets within FrameNet, not on truly novel or out-of-domain targets.
- **Language Dependency:** Findings are limited to English FrameNet and may not transfer to other languages or semantic frameworks.

## Confidence

### High Confidence:
- JSON-based representation advantage (+3.9% F1 over SOTA) is supported by ablation and multiple baselines.
- Base models outperform instruction-tuned models for this task is consistently demonstrated across metrics.

### Medium Confidence:
- LoRA fine-tuning results are compelling, but exact hyperparameter sensitivity is not explored.
- "Unseen frame elements" failure mode is well-documented but not deeply analyzed for mitigation strategies.

### Low Confidence:
- Novel frame identification method is only tested on ambiguous FrameNet targets; its accuracy on truly novel, out-of-domain targets remains unverified.

## Next Checks
1. **Generalization Stress Test:** Evaluate the best fine-tuned model (Qwen 2.5-3B) on a non-FrameNet frame-semantic dataset (e.g., PropBank or a custom corpus of informal text) to quantify schema-transfer limits.
2. **Output Format Robustness:** Re-run the format ablation with JSON-Complete, but modify the parsing logic to handle nested or overlapping spans. Compare F1 to JSON-Existing to see if structured parsing overhead is justified.
3. **Instruction Tuning Counterfactual:** Fine-tune an instruction-tuned base model (e.g., Qwen 2.5-7B-Instruct) with LoRA on FrameNet and compare its F1 to the base model. This will isolate whether the instruction-tuning penalty is due to the model's weights or the fine-tuning process itself.