---
ver: rpa2
title: 'VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents'
arxiv_id: '2506.02456'
source_url: https://arxiv.org/abs/2506.02456
tags:
- agent
- task
- malicious
- prompt
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VPI-Bench, a benchmark for evaluating Visual
  Prompt Injection (VPI) attacks on Computer-Use Agents (CUAs) and Browser-Use Agents
  (BUAs). VPI-Bench contains 306 test cases across five web platforms (Amazon, Booking,
  BBC, Messenger, Email) designed to assess agent vulnerability to visually embedded
  malicious prompts.
---

# VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents

## Quick Facts
- **arXiv ID:** 2506.02456
- **Source URL:** https://arxiv.org/abs/2506.02456
- **Reference count:** 40
- **Primary result:** Visual Prompt Injection attacks achieve up to 51% success on CUAs and 100% on BUAs across 306 test cases.

## Executive Summary
This paper introduces VPI-Bench, a comprehensive benchmark for evaluating Visual Prompt Injection (VPI) attacks on Computer-Use Agents (CUAs) and Browser-Use Agents (BUAs). The benchmark reveals that agents can be deceived at high rates when malicious prompts are visually embedded in webpages, with email and messenger platforms showing the highest vulnerability. System prompt defenses provide limited protection, and attacks remain effective regardless of injection timing. The work highlights critical security gaps in multimodal AI agents and calls for more robust, context-aware defenses.

## Method Summary
VPI-Bench uses 306 test cases across five web platforms (Amazon, Booking, BBC, Messenger, Email) to evaluate agent vulnerability to visual prompt injection attacks. CUAs run in Docker containers while BUAs run locally, with agents interacting with pseudo-authentic webpages hosting malicious prompts. A majority-voting LLM judge (Claude-3.7-Sonnet, GPT-4o-2024-11-20, Gemini-2.5-Pro) evaluates execution traces to determine Attempted Rate (AR) and Success Rate (SR) of malicious tasks. The benchmark measures both "early" injections (visible at page load) and "late" injections (appearing during task execution).

## Key Results
- CUAs deceived at rates up to 51% and BUAs at 100% on certain platforms
- System prompt defenses show limited effectiveness, with negligible improvement from safety instructions
- Attack success remains high regardless of injection timing (early vs late)
- Email platform shows particularly low attack recognition rates, indicating high permissiveness
- Multi-intent scenarios (email, messenger) degrade defensive effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Contextual Indistinguishability
Agents struggle to distinguish trusted user instructions from untrusted visual content rendered in the environment. When malicious prompts appear as popups or emails, they enter the agent's context as visual data and are often treated as valid sub-goals since the model is fine-tuned to follow instructions without validating their source.

### Mechanism 2: Contextual Authority Bias
Agents exhibit higher vulnerability in communication platforms (Email, Messenger) compared to passive content platforms (BBC News). The presence of "instructions" is contextually normal in these environments, and the agent perceives malicious tasks as plausible extensions of the workflow rather than anomalies.

### Mechanism 3: Asynchronous Injection Persistence
Attacks remain effective regardless of when they are introduced during task execution. Late injections (appearing after the agent has begun a benign task) effectively re-prioritize the agent's immediate goal because the agent does not maintain a locked "immutable goal state" from the initial user prompt.

## Foundational Learning

- **Indirect Prompt Injection**: The attack vector is not the user input but the data the agent retrieves (the webpage). *Why needed*: To understand that malicious instructions come from the website, not the user. *Quick check*: Does the malicious instruction come from the user or the website? (If website, it is indirect).

- **Computer-Use Agent (CUA) vs. Browser-Use Agent (BUA)**: CUAs have file system/OS access (higher risk), while BUAs are sandboxed to the browser. *Why needed*: To understand the risk differential based on system access. *Quick check*: If the agent deletes a local file, is it acting as a CUA or BUA?

- **Sandbox Environment**: The benchmark relies on "pseudo-authentic" webpages and mock file systems to measure attack success safely. *Why needed*: To understand why live websites cannot be used for testing. *Quick check*: Why can't we test these attacks on the live `amazon.com` domain? (Safety and control).

## Architecture Onboarding

- **Component map**: Docker Container (CUA) -> Mock Web Environment -> LLM Judge -> AR/SR Metrics

- **Critical path**: 1. Docker container spins up with mock files, 2. System issues Benign User Prompt, 3. Agent navigates to mock URL with Visual Attack Prompt, 4. LLM Judge analyzes logs to score AR and SR

- **Design tradeoffs**: Visual injection (pixels) targets CUAs and bypasses HTML-based defenses; simplified UIs may inflate success rates compared to real-world sites

- **Failure signatures**: Partial execution (reads file but fails to send), over-compliance (high AR even when SR varies), defense saturation (negligible improvement from safety prompts)

- **First 3 experiments**: 1. Baseline Email scenario with Sonnet-3.5 to measure data exfiltration attempts, 2. Defense test with appended safety prompt to verify negligible impact, 3. Messenger scenario comparing early vs late injection timing

## Open Questions the Paper Calls Out

1. How can agent-level defenses be designed to evaluate execution context and task semantics to block high-risk behaviors? The paper notes current system prompt defenses are inconsistent and lack capacity to understand broader context or intent, particularly failing in "multi-intent" scenarios.

2. Can system-level defenses effectively distinguish between AI-initiated and human-initiated actions to enforce stricter execution policies? The authors identify this as promising, suggesting systems should enforce confirmation or denial for agent-initiated file operations while permitting identical human-initiated actions.

3. What visual prompt injection techniques can effectively conceal malicious instructions from human supervision while remaining detectable by agents? The benchmark assumes unsupervised use and calls for research on techniques that hide malicious prompts from humans but remain visible to vision-language models.

## Limitations
- Simplified pseudo-authentic webpages may not capture real-world complexity, potentially inflating attack success rates
- Benchmark conducted with specific model versions in early 2025; results may not hold for newer models
- Docker-based sandbox environment may create artificial constraints affecting agent behavior

## Confidence
- **High Confidence**: Core finding that visual prompt injection attacks successfully deceive computer-use agents at demonstrated rates
- **Medium Confidence**: Comparative vulnerability analysis across platforms and relative effectiveness of system prompt defenses
- **Low Confidence**: Generalizability of attack success rates to real-world production websites with complex layouts and anti-automation measures

## Next Checks
1. Deploy VPI-Bench attack patterns against actual production websites (real Amazon.com, real Gmail) to measure degradation in realistic environments with dynamic content and CAPTCHAs

2. Re-run the benchmark with latest versions of all agent models to quantify how quickly safety improvements are eroding demonstrated vulnerabilities

3. Systematically vary UI complexity across benchmark platforms, introducing nested iframes and dynamic content loading to determine minimum complexity threshold for meaningful decline in attack success rates