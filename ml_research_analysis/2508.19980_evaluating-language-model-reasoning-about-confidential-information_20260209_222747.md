---
ver: rpa2
title: Evaluating Language Model Reasoning about Confidential Information
arxiv_id: '2508.19980'
source_url: https://arxiv.org/abs/2508.19980
tags:
- password
- information
- confidential
- reasoning
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language models fail to reliably withhold confidential information
  without correct passwords, even in simple scenarios. The authors create PasswordEval,
  a benchmark where models must reveal a secret only when given a correct password.
---

# Evaluating Language Model Reasoning about Confidential Information

## Quick Facts
- **arXiv ID:** 2508.19980
- **Source URL:** https://arxiv.org/abs/2508.19980
- **Reference count:** 22
- **Primary result:** Language models fail to reliably withhold confidential information without correct passwords, even in simple scenarios

## Executive Summary
Language models fail to reliably withhold confidential information without correct passwords, even in simple scenarios. The authors create PasswordEval, a benchmark where models must reveal a secret only when given a correct password. All frontier models perform well on compliant requests but poorly on non-compliant ones, with reasoning capabilities providing no significant benefit. Reasoning traces frequently leak passwords or secrets, raising security concerns. Simple jailbreak prompts drastically reduce compliance, exposing brittleness in alignment. Performance degrades further with multi-turn password verification. Results suggest current models are ill-suited for high-stakes access control and that reasoning traces should not be exposed publicly.

## Method Summary
The PasswordEval benchmark consists of 500 scenarios with password-confidential info pairs and compliant/non-compliant user prompts. Models are evaluated using exact string matching to detect leaks in both outputs and reasoning traces. The study tests standard and reasoning models (LLaMA-3, Qwen-3, GPT-4o, Gemini-2.5 variants) with temperature=1.0, top_p=1.0, and 100 tokens (400 for reasoning). Multi-turn verification tests sequential password entry (2-10 passwords), while adversarial testing uses template jailbreaks, GCG (500 steps, top_k=256), and PAIR (Mistral-7B-Instruct attacker, LLaMA-3 8B target). CompliantAcc, NonCompliantAcc, ConfInfoLeak, and PasswordLeak metrics are computed via exact string matching.

## Key Results
- Frontier models achieve high NonCompliantAcc (refusing unauthorized access) but low CompliantAcc (releasing with correct password)
- Reasoning traces leak confidential information at rates of 72-97.6%, while output leakage remains low (0-4.6%)
- Simple jailbreak prompts drastically reduce compliance, exposing brittleness in alignment
- Multi-turn password verification further degrades performance, with compliant accuracy dropping to 23-45%

## Why This Works (Mechanism)

### Mechanism 1: Contextual Robustness Failure
- **Claim:** Current alignment methods fail to instill "contextual robustness," defined as the ability to adhere to rules that depend on specific context variables (e.g., verifying a password) rather than universal prohibitions.
- **Mechanism:** Post-training (RLHF/DPO) optimizes for broad refusal of harmful content but does not robustly bind conditional logic (if password -> release; if no password -> refuse) to specific data pieces within the prompt.
- **Core assumption:** Models prioritize general helpfulness or pattern matching over strict logical verification of authorization states.
- **Evidence anchors:**
  - [abstract] The study measures "contextual robustness" and finds models struggle with the "seemingly simple task" of conditional release.
  - [section 4.1] "Frontier Models Struggle with Password Verification" notes that while models refuse non-compliant requests, they often fail to release information for compliant ones, indicating a lack of nuanced control.
  - [corpus] *AbstentionBench* supports the finding that models struggle with context-dependent constraints, specifically "knowing when not to answer" in underspecified scenarios.
- **Break condition:** If models were trained on data specifically emphasizing conditional access control and logical implication over generic refusal.

### Mechanism 2: Reasoning-Induced Information Leakage
- **Claim:** Inference-time reasoning traces act as an unguarded intermediate state where confidential information is processed and leaked, even if the final output is compliant.
- **Mechanism:** Chain-of-thought reasoning requires the model to explicitly manipulate the confidential data (the password and the secret) to determine the correct response. Without specific training to sanitize the "thought process," these values appear in the trace.
- **Core assumption:** Reasoning capabilities are currently decoupled from safety constraints; the model reasons *about* the secret rather than reasoning *to protect* the secret.
- **Evidence anchors:**
  - [abstract] "Reasoning traces frequently leak confidential information, which calls into question whether reasoning traces should be exposed."
  - [section 4.4] Figure 4 shows that while output leakage is low (0-4.6%), reasoning trace leakage is extremely high (72-97.6%) for certain models.
  - [corpus] *Privacy Meets Explainability* highlights similar risks where "intermediate states" or explanations in LLM tools inadvertently expose confidential data.
- **Break condition:** If process reward models (PRMs) were used to enforce that reasoning traces must not contain the secret, essentially training the model to "reason silently" or mask sensitive tokens.

### Mechanism 3: Adversarial Override of Instruction Hierarchy
- **Claim:** Models lack a robust instruction hierarchy, allowing adversarial user prompts (jailbreaks) to override system-level rules regarding access control.
- **Mechanism:** Jailbreak templates frame the request in a new context (e.g., "educational," "fictional") or use optimized adversarial suffixes (GCG/PAIR) that elevate the user prompt's effective priority above the system prompt's access rules.
- **Core assumption:** The attention mechanism attends strongly to the immediate user context (the jailbreak) and fails to maintain inhibition derived from the system context (the password rule).
- **Evidence anchors:**
  - [abstract] "Simple jailbreak prompts drastically reduce compliance, exposing brittleness in alignment."
  - [section 3.2] Describes how template-based jailbreaks "encourage the model to ignore existing preferences" and deny "password-based access controls."
  - [corpus] *Reasoning Up the Instruction Ladder* explicitly discusses the difficulty of enforcing instruction hierarchies, where user instructions can override system directives.
- **Break condition:** If architectures implemented strict, non-learnable priority layers where system rules could hard-filter outputs based on unauthorized content presence.

## Foundational Learning

### Concept: Contextual Robustness
- **Why needed here:** To understand that safety is not just binary (safe/unsafe) but conditional (safe for user A, unsafe for user B). PasswordEval tests this specifically.
- **Quick check question:** Can the model distinguish between a generic refusal (safety) and a conditional refusal (access control) based on session state?

### Concept: Instruction Hierarchy
- **Why needed here:** To diagnose why jailbreaks work. The failure implies the user prompt (low priority) successfully competed with the system prompt (high priority).
- **Quick check question:** Does the model treat the system prompt as a hard constraint or just soft context?

### Concept: Side-Channel Leakage (Reasoning Traces)
- **Why needed here:** To recognize that a secure output does not mean a secure process. Exposing the reasoning trace invalidates the access control mechanism.
- **Quick check question:** If the user sees the reasoning trace, is the system secure? (The paper suggests **No**).

## Architecture Onboarding

### Component map:
System Prompt -> User Prompt -> Model Inference -> Output/Reasoning Trace Analysis

### Critical path:
Generating the specific system prompt with embedded secrets -> Running inference (standard vs. reasoning mode) -> Applying string-based detection to the output and trace

### Design tradeoffs:
- **Exact Match vs. Semantic Match:** The authors use exact string matching for `ConfInfoLeak` and `PasswordLeak`. This is strict but might miss paraphrased leaks. *Assumption:* The secrets are generated to be specific enough that paraphrasing is unlikely to be accidental.
- **Trace Visibility:** Evaluating reasoning traces is critical here. Systems using reasoning models must architecturally hide these traces from end-users to maintain the security properties found in the final output.

### Failure signatures:
- **Over-refusal:** Model refuses to give info even with correct password (Low Compliant Correctness).
- **Context Leakage:** Model refuses the request correctly but outputs the password in the refusal message (e.g., "I can't tell you without 'password99'").
- **Reasoning Leakage:** Final output is secure, but the intermediate reasoning block contains the confidential info.

### First 3 experiments:
1. **Baseline Conditional Logic:** Run PasswordEval (Standard) to measure the gap between Non-Compliant Accuracy (refusing correctly) and Compliant Accuracy (releasing correctly).
2. **Trace Inspection:** Run reasoning models on PasswordEval and specifically query the reasoning trace API/field for the presence of the secret string (Fig 4 replication).
3. **Adversarial Stress Test:** Apply the "Template Jailbreak" (Appendix D.3) to see if simple prompting drops Non-Compliant Accuracy (i.e., the model reveals the secret).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reasoning capabilities be trained differently to improve contextual robustness and rule-following?
- **Basis in paper:** [explicit] The abstract states "reasoning capabilities may need to be trained in a different manner to make them safer for release in high-stakes settings."
- **Why unresolved:** Current reasoning training (e.g., for math and coding) does not transfer to rule-following; reasoning models show marginal or negative effects on PasswordEval.
- **What evidence would resolve it:** Demonstrated improvements on PasswordEval (especially non-compliant correctness) after training reasoning models with rule-following objectives.

### Open Question 2
- **Question:** Should reasoning traces be exposed to users, and how can training prevent information leakage in traces?
- **Basis in paper:** [explicit] The authors find that "reasoning traces frequently leak confidential information" and explicitly question "whether reasoning traces should be exposed to users in such applications."
- **Why unresolved:** Even when outputs correctly withhold secrets, reasoning traces often contain passwords or confidential information.
- **What evidence would resolve it:** Training methods (e.g., process reward modeling) that demonstrably reduce or eliminate leakage in reasoning traces.

### Open Question 3
- **Question:** What mechanisms underlie the gap between models' compliance on authorized vs. unauthorized requests?
- **Basis in paper:** [inferred] While models achieve high non-compliant accuracy (often 100%), compliant accuracy lags substantially, indicating a surface-level understanding without consistent contextual discrimination.
- **Why unresolved:** The paper does not investigate internal representations or attention patterns that might explain this asymmetry.
- **What evidence would resolve it:** Probing or mechanistic analysis identifying where context-dependent authorization fails in model computation.

### Open Question 4
- **Question:** How can language models be integrated with external verification mechanisms for robust access control?
- **Basis in paper:** [explicit] The discussion suggests "stronger integration between language models and structured authentication systems" via "tool-use for password validation or API-level access controls."
- **Why unresolved:** PasswordEval tests native language model authentication; external tool integration remains unexplored in this benchmark context.
- **What evidence would resolve it:** Evaluations showing that tool-augmented models maintain confidentiality robustly under the same adversarial pressure.

## Limitations
- Exact in-context examples and reasoning trace API parameters are unspecified
- Evaluation relies on exact string matching, potentially missing paraphrased leaks
- Benchmark focuses on simple password-based access control, may not generalize to complex authorization schemes
- Adversarial jailbreak templates may not represent full attack space

## Confidence
- **High Confidence:** Models struggle with conditional access control; reasoning traces leak confidential information
- **Medium Confidence:** Reasoning capabilities provide no benefit for this task; instruction hierarchy vulnerability to jailbreaks
- **Low Confidence:** Broader claim about models being ill-suited for high-stakes access control; recommendation against exposing reasoning traces

## Next Checks
1. **Multi-modal and Contextual Access Control:** Test PasswordEval with additional security dimensions including multi-modal verification (biometric + password), time-based access windows, and contextual factors (location, device fingerprinting) to assess whether simple password-based failures generalize to richer access control schemes.

2. **Process Reward Model Validation:** Implement a PRM that penalizes reasoning traces containing confidential information and retrain or fine-tune reasoning models on PasswordEval. Measure whether this architectural intervention closes the reasoning trace leakage gap while maintaining task performance.

3. **Instruction Hierarchy Stress Testing:** Systematically vary the relative position, formatting, and explicit priority markers of system vs. user prompts to map the boundary conditions where jailbreaks succeed or fail. This would quantify the precise failure modes in the instruction hierarchy and inform architectural mitigations.