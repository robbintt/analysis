---
ver: rpa2
title: 'Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search'
arxiv_id: '2512.25065'
source_url: https://arxiv.org/abs/2512.25065
tags:
- systems
- search
- heuristics
- these
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VULCAN, a framework that uses large language
  models (LLMs) to synthesize instance-optimal systems heuristics. VULCAN addresses
  the challenge of manually designing performant heuristics by automating their discovery
  through evolutionary search over LLM-generated code.
---

# Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search

## Quick Facts
- arXiv ID: 2512.25065
- Source URL: https://arxiv.org/abs/2512.25065
- Reference count: 40
- Synthesizes instance-optimal heuristics outperforming human-designed algorithms by up to 69% on cache eviction and 7.9% on memory tiering

## Executive Summary
VULCAN is a framework that automates the discovery of high-performance systems heuristics through LLM-driven evolutionary search. By separating policy logic from mechanism using VALUE and RANK interfaces, VULCAN transforms complex heuristic synthesis into a tractable single-function generation problem. The system achieves instance-optimal performance by searching for heuristics specialized to specific workload-hardware pairs, outperforming all human-designed state-of-the-art algorithms in cache eviction and memory tiering applications.

## Method Summary
VULCAN automates heuristic synthesis by constraining LLMs to generate stateless scoring functions (VALUE or RANK) while scaffolding handles all data structures and state management. The framework uses evolutionary search over LLM-generated candidates, maintaining a population and iteratively refining heuristics based on performance feedback. Users specify the task, instance (workload/hardware pair), and evaluation harness, while VULCAN searches for optimal policies. The approach dramatically reduces human effort in heuristic creation while discovering specialized solutions tailored to deployment contexts.

## Key Results
- Synthesized cache eviction heuristics outperformed all human-designed state-of-the-art algorithms by up to 69% in performance
- Memory tiering heuristics achieved 7.9% improvement over existing solutions
- Demonstrated that instance-specific specialization captures workload-hardware combinations that generic heuristics miss
- Showed LLMs can efficiently discover high-performance heuristics through evolutionary search over constrained interfaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating policy from mechanism via constrained interfaces makes LLM-driven heuristic synthesis tractable
- Mechanism: VULCAN restricts the LLM to generating only a stateless scoring function (`value()` or `rank()`), while scaffolding handles all data structures, state management, and action execution. This collapses a complex multi-file codebase problem into a single-function generation task
- Core assumption: Most systems heuristics can be reformulated as either computing a value from state or ranking objects for selection, without loss of expressiveness
- Evidence anchors:
  - [abstract] "VULCAN separates policy and mechanism through LLM-friendly, task-agnostic interfaces... sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code"
  - [Section 2.3] Direct synthesis fails because "heuristics maintain internal state, involve intricate state-transition logic, and are intertwined with mechanisms spread across multiple files"
  - [Section 3.1] "By sharply constraining the 'attack surface' of the LLM to a single, stateless function... every function that returns a real value is a well-formed policy"
  - [corpus] Related work (Barbarians, Glia) uses LLMs for heuristic mutation but lacks this principled interface abstraction
- Break condition: Tasks requiring rich interactions between policy and mechanism (e.g., policies that fundamentally change data structures) may not fit these interfaces

### Mechanism 2
- Claim: Evolutionary search with performance feedback drives heuristic quality improvements without explicit supervision
- Mechanism: A population of candidate heuristics is maintained. In each iteration, the LLM generates new candidates conditioned on top performers from the database. The evaluation harness scores each candidate on the target instance, and high performers are retained as exemplars for future generations
- Core assumption: The heuristic space is smooth enough that iterating on high-performing examples yields progressively better solutions, and the evaluation harness captures real deployment behavior
- Evidence anchors:
  - [abstract] "VULCAN searches for performant policies via evolutionary search over LLM-generated code"
  - [Section 3.3] "At a high level, this evolutionary search algorithm maintains a population of candidate heuristics... high-performing candidates are preferentially selected, allowing the search to gradually discover increasingly effective heuristics"
  - [Section 4.1.3] Cache eviction search "seeded with LRU... top two performing heuristics across all rounds are retained and used as examples in the next round"
  - [corpus] LLaMEA and AlphaEvolve demonstrate evolutionary search for algorithm discovery, supporting general feasibility
- Break condition: Sparse reward landscapes or noisy evaluators may prevent convergence; evaluation cost can become prohibitive (Section 3.3.2 notes hundreds of evaluations may be needed)

### Mechanism 3
- Claim: Instance-specific specialization captures workload-hardware combinations that generic heuristics miss
- Mechanism: Users define an instance as a specific (workload, hardware) pair. VULCAN searches for heuristics optimized for that exact context, rather than attempting generalization. An automated instance generator can cluster workloads by features and trigger re-search when runtime signals indicate distribution shift
- Core assumption: Different instances have fundamentally different optimal heuristics, and the cost of per-instance search is acceptable given performance gains
- Evidence anchors:
  - [abstract] "synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed"
  - [Section 2.1, Figure 1] "no single algorithm performs the best on even half of the traces" across 106 CloudPhysics traces
  - [Section 3.2] "VULCAN fundamentally changes this cost model: by dramatically reducing the human cost of heuristic creation, it becomes practical to synthesize heuristics for much more narrower instances"
  - [corpus] Limited direct corpus support; instance-optimality framing appears novel in this formulation
- Break condition: If workload distributions shift faster than search can complete, or if instances are poorly defined, specialization provides limited benefit

## Foundational Learning

- Concept: **Evolutionary search / genetic programming**
  - Why needed here: VULCAN's core loop is evolutionary search over code. You need to understand population management, selection pressure, and convergence criteria
  - Quick check question: Can you explain how retaining top-k performers as examples biases the LLM toward high-performing regions of the search space?

- Concept: **Policy vs. mechanism in systems design**
  - Why needed here: The paper's central abstraction relies on cleanly separating *what* decision is made (policy) from *how* it's executed (mechanism)
  - Quick check question: For a CPU scheduler, what is the policy logic and what is the mechanism? Would a `rank()` function over runnable tasks capture the policy adequately?

- Concept: **LLM code generation capabilities and limitations**
  - Why needed here: The approach depends on LLMs generating syntactically correct, logically coherent single functions but not complex stateful systems
  - Quick check question: Why would asking an LLM to generate a complete cache eviction algorithm with ghost lists, multiple queues, and synchronization likely fail, while generating a `score()` function succeeds?

## Architecture Onboarding

- Component map:
  - Template (natural language description + function prototype) -> Generator (LLM) -> Policy Module (scaffolding with injected function) -> Evaluation Harness (simulator or real system) -> Heuristic Database (stores candidates with performance scores)

- Critical path:
  1. Define VALUE or RANK interface and implement scaffolding (data collection + action logic)
  2. Create template with function prototype, constraints, and example heuristics
  3. Implement evaluation harness balancing speed vs. fidelity
  4. Configure evolutionary search (population size, iterations, selection strategy)
  5. Run search, extract best heuristic, deploy to production

- Design tradeoffs:
  - **Evaluation fidelity vs. speed**: Simulators (libcachesim) enable hundreds of evaluations quickly; real hardware (CloudLab) is slower but more accurate. Table 4 shows this spectrum
  - **Interface expressiveness vs. tractability**: Narrow interfaces (single scoring function) simplify search but may constrain policy space. Queue Topology (Section 4.2) shows an alternative interface trading some generality for built-in efficiency
  - **Instance granularity**: Finer instances (individual traces) enable more specialization but increase search cost; coarser clustering reduces search overhead but may miss optimization opportunities

- Failure signatures:
  - **Poor interface design**: LLM generates functions that are syntactically correct but semantically meaningless (e.g., `return 42;`). Usually indicates insufficient feature exposure or unclear template guidance
  - **Slow evaluation**: Search takes days instead of hours. Often caused by using real hardware when simulation suffices, or inefficient harness implementation
  - **Search stagnation**: Performance plateaus early. May indicate insufficient population diversity, overly constrained template, or evaluator noise masking signal
  - **Deployment divergence**: Synthesized heuristic performs well in evaluation but poorly in production. Indicates fidelity gap in evaluation harness

- First 3 experiments:
  1. **Cache eviction with RANK interface**: Use libcachesim with CloudPhysics traces. Start with LRU as seed, run 25 iterations with 25 candidates each. Goal: reproduce >10% improvement over LRU on at least one cluster
  2. **Memory tiering with RANK interface**: Use ARMS scaffolding on CloudLab CXL-emulation setup. Define instance as single application (e.g., GUPS). Run 50 iterations targeting throughput metric. Goal: observe bandwidth-aware penalties emerging in synthesized code
  3. **Custom task (congestion control) with VALUE interface**: Use Mahimahi emulator. Template defines `value()` returning cwnd based on RTT/loss signals. Seed with simple AIMD. Goal: validate interface generality beyond the paper's two case studies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the evaluation harness be optimized to balance the speed of simulation against the fidelity of real-hardware execution to prevent "simulation-to-reality" gaps in the synthesized heuristics?
- Basis in paper: [explicit] Section 3.3.2 states, "Designing an effective evaluation harness requires balancing two competing objectives: efficiency and fidelity," and notes that users must anticipate running hundreds of evaluations
- Why unresolved: The paper demonstrates both simulated (caching) and real-hardware (tiering) evaluation but does not provide a systematic method for combining them or determining the optimal trade-off point
- What evidence would resolve it: A study showing a hybrid evaluation strategy (e.g., using simulators for early generations and hardware for refinement) that converges faster with higher final fidelity than single-mode approaches

### Open Question 2
- Question: How can the system handle rapid, real-time workload phase shifts where the overhead of the offline evolutionary search loop exceeds the duration of the workload phase?
- Basis in paper: [explicit] Section 3.2.1 notes that if no instance matches, the classifier triggers an "offline VULCAN policy search loop," implying a latency that may be incompatible with fast workload transients
- Why unresolved: The framework currently relies on offline search; the authors do not quantify the search latency relative to workload change speeds or propose an online adaptation mechanism
- What evidence would resolve it: An analysis of search convergence times relative to workload phase durations, or a mechanism for incremental, low-latency policy updates

### Open Question 3
- Question: To what extent does the requirement for users to manually define the scaffolding and exposed signals (features) limit the search space and prevent the discovery of novel policies requiring unexposed state?
- Basis in paper: [inferred] Section 3.1 states users must define "data collection logic" and signals, and Section 3.3.1 mentions templates can constrain the search space
- Why unresolved: While the LLM combines signals, it cannot utilize information the user did not explicitly expose in the scaffolding, potentially bounding performance by the user's feature engineering
- What evidence would resolve it: A comparison of heuristics discovered using minimal raw signals vs. those using hand-crafted features, or a method for the LLM to request new signals dynamically

### Open Question 4
- Question: How can the framework guarantee semantic safety invariants (e.g., lack of starvation, fairness) in the generated code when the search is guided solely by a scalar performance metric?
- Basis in paper: [inferred] Section 2.4 notes that while the interface prevents "invalid" code (syntax/type errors), it allows "poor" policies, and Section 3.3.2 mentions combining multiple objectives into a single metric
- Why unresolved: A scalar metric may average out rare but catastrophic safety violations (like starvation) that are not explicitly penalized or constrained in the template
- What evidence would resolve it: Incorporation of formal verification constraints or multi-objective penalties into the search loop that mathematically guarantee safety properties alongside performance

## Limitations
- The effectiveness of VULCAN's evolutionary search is bounded by the fidelity of the evaluation harness, which may not capture production complexity
- The VALUE/RANK interface design may not generalize across all systems problems requiring richer state interactions
- The assertion that instance-optimal heuristics are fundamentally necessary across systems domains lacks systematic validation beyond the two case studies

## Confidence
- **High confidence**: The core mechanism of separating policy from mechanism via constrained interfaces is well-supported by the evidence and theoretical soundness
- **Medium confidence**: The performance claims (69% improvement for cache eviction, 7.9% for memory tiering) are based on specific experimental setups and may not generalize across all instances or systems
- **Low confidence**: The assertion that instance-optimal heuristics are fundamentally necessary across systems domains lacks systematic validation beyond the two case studies

## Next Checks
1. Apply VULCAN to a CPU scheduler design problem where the interface requires ranking runnable threads, comparing synthesized heuristics against established algorithms like Completely Fair Scheduler across diverse workload mixes

2. Evaluate VULCAN's performance on a real-world production cache system (e.g., Memcached or Redis) where evaluation harness fidelity cannot be perfectly controlled, measuring deployment-to-simulation performance correlation

3. Test VULCAN with different LLM sizes and model families (including smaller, specialized models) to quantify the relationship between model capability and heuristic quality, particularly examining whether the constrained interface truly enables smaller models to perform competitively