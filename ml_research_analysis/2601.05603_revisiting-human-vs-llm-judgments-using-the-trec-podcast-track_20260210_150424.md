---
ver: rpa2
title: Revisiting Human-vs-LLM judgments using the TREC Podcast Track
arxiv_id: '2601.05603'
source_url: https://arxiv.org/abs/2601.05603
tags:
- trec
- llms
- relevance
- judgments
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of large language models
  (LLMs) as relevance assessors by comparing their judgments to human assessments
  in the TREC Podcast Track collections from 2020 and 2021. The authors reassessed
  18,284 query-segment pairs using five different LLM models, resulting in 91,420
  total judgments.
---

# Revisiting Human-vs-LLM judgments using the TREC Podcast Track
## Quick Facts
- arXiv ID: 2601.05603
- Source URL: https://arxiv.org/abs/2601.05603
- Reference count: 0
- Primary result: LLM assessments may be more reliable than previously thought for podcast relevance evaluation

## Executive Summary
This study investigates the reliability of large language models as relevance assessors by comparing their judgments to human assessments in the TREC Podcast Track collections from 2020 and 2021. The authors reassessed 18,284 query-segment pairs using five different LLM models, resulting in 91,420 total judgments. They found that system rankings were relatively stable in 2020 but showed significant volatility in 2021, with some systems experiencing rank changes of up to 12 positions. When three senior IR experts re-assessed pairs with highest disagreement between LLM and TREC assessors, the experts showed higher agreement with LLMs than with the original TREC assessors.

## Method Summary
The researchers collected human judgments from TREC Podcast Track 2020-2021 (18,284 query-segment pairs) and generated new judgments using five different LLM models. They compared system rankings between human and LLM assessments, then conducted expert re-assessments on the 800 pairs showing highest disagreement. The study analyzed agreement levels, ranking stability, and expert preferences to evaluate LLM reliability as relevance assessors.

## Key Results
- System rankings showed stability in 2020 but significant volatility in 2021 (up to 12 position changes)
- Expert assessors agreed more with LLM judgments than with original TREC assessors on high-disagreement pairs
- LLM assessments demonstrated reliability comparable to or exceeding human assessments in podcast domain

## Why This Works (Mechanism)
The study demonstrates that LLM assessments can capture relevance patterns effectively when properly prompted and evaluated against established human judgments. The mechanism relies on LLMs' ability to understand query-segment relationships and apply consistent relevance criteria across large datasets.

## Foundational Learning
1. **Relevance Assessment** - The process of determining if a document satisfies a query's information need; critical for understanding evaluation metrics and agreement calculation.
2. **Inter-annotator Agreement** - Measurement of consistency between different assessors; needed to establish baseline reliability and compare LLM performance.
3. **System Ranking Volatility** - Changes in relative system performance across different assessment sets; indicates stability and reliability of evaluation methods.
4. **Domain-specific Evaluation** - Assessment methods may vary by content type (podcasts vs web documents); affects generalizability of findings.
5. **Prompt Engineering** - Techniques for eliciting desired responses from LLMs; impacts consistency and quality of automated assessments.
6. **Expert Re-assessment** - Process of having domain experts review disputed judgments; serves as gold standard for validation.

## Architecture Onboarding
- **Component Map**: TREC data -> LLM judgments -> System rankings -> Expert re-assessment -> Agreement analysis
- **Critical Path**: Human judgments → LLM generation → Ranking comparison → Expert validation → Reliability conclusions
- **Design Tradeoffs**: Automation efficiency vs assessment quality, model selection vs computational cost, expert time vs sample size
- **Failure Signatures**: High disagreement rates, ranking instability, systematic biases in LLM judgments
- **First Experiments**: 1) Run same LLM models on 2020 data only 2) Vary temperature settings systematically 3) Test additional LLM models with different architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to two years of TREC Podcast Track data, may not generalize across domains
- Five LLM models represent a small sample of available models
- Expert re-assessment sample (800 pairs) is small relative to total judgments

## Confidence
- LLM assessments more reliable than thought: Medium
- 2020 vs 2021 ranking volatility difference: Medium
- LLM judgments reinforce relevance assessment subjectivity: High

## Next Checks
1. Conduct cross-domain validation using collections from different TREC tracks to assess generalizability
2. Implement controlled experiments varying prompt engineering approaches and model parameters
3. Perform longitudinal analysis tracking same systems across multiple years for pattern consistency