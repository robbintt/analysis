---
ver: rpa2
title: 'Banking on Feedback: Text Analysis of Mobile Banking iOS and Google App Reviews'
arxiv_id: '2503.11861'
source_url: https://arxiv.org/abs/2503.11861
tags:
- reviews
- topic
- topics
- data
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed mobile banking app reviews from Canadian banks
  on iOS and Google Play to identify user satisfaction factors and improvement areas.
  It used text preprocessing with NLTK, topic modeling with Latent Dirichlet Allocation
  (LDA), and sentiment analysis comparing LSTM, Multinomial Naive Bayes, TextBlob,
  and VADER.
---

# Banking on Feedback: Text Analysis of Mobile Banking iOS and Google App Reviews

## Quick Facts
- arXiv ID: 2503.11861
- Source URL: https://arxiv.org/abs/2503.11861
- Reference count: 40
- Primary result: Supervised LSTM (82% accuracy) and MNB (77% accuracy) outperform lexicon-based sentiment analysis on mobile banking app reviews.

## Executive Summary
This study analyzed mobile banking app reviews from Canadian banks on iOS and Google Play to identify user satisfaction factors and improvement areas. It used text preprocessing with NLTK, topic modeling with Latent Dirichlet Allocation (LDA), and sentiment analysis comparing LSTM, Multinomial Naive Bayes, TextBlob, and VADER. LSTM achieved 82% accuracy for iOS reviews and Multinomial Naive Bayes achieved 77% accuracy for Google Play reviews. Topic modeling with bi-grams (15 topics) performed best for both platforms. Positive reviews highlighted ease of use, reliability, and features like budgeting tools and free credit score checks, while negative reviews focused on login issues, glitches, update-related problems, and poor customer service. Findings emphasize the need for user-friendly designs, stable updates, and improved customer support. Advanced text analytics offer actionable insights for enhancing user satisfaction and app experience.

## Method Summary
The study collected 11,980 iOS reviews (capped at 4,000 per app) and 89,343 Google Play reviews from five Canadian banks, filtering non-English reviews. Reviews underwent NLTK preprocessing (lowercase, extended stop words, lemmatization). Sentiment labels were assigned using a ±1 SD rule from mean ratings. Sentiment analysis compared TextBlob, VADER, LSTM (5-layer architecture with 40% embedding dropout, 20% recurrent dropout), and Multinomial Naive Bayes. Topic modeling used LDA with uni/bi/trigrams (5-15 topics), selecting models via weighted perplexity-coherence scores. The best sentiment models were LSTM for iOS (82% accuracy) and MNB for Google Play (77% accuracy), while bi-gram LDA with 15 topics provided optimal topic coherence.

## Key Results
- Supervised LSTM (82% accuracy) and MNB (77% accuracy) significantly outperform lexicon-based methods (24-33% accuracy) for sentiment classification.
- Bi-gram topic modeling with 15 topics achieved the highest coherence scores for both iOS and Google Play datasets.
- Negative reviews primarily focused on login issues, app glitches, problematic updates, and poor customer service experiences.
- Positive reviews emphasized ease of use, reliability, and valuable features like budgeting tools and free credit score checks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bigrams provide superior topic coherence compared to unigrams or trigrams for mobile app reviews.
- **Mechanism:** Pairing words (bigrams) preserves contextual relationships (e.g., "crash" vs. "crash loop") that unigrams sever, while avoiding the data sparsity and overfitting risks of trigrams.
- **Core assumption:** User feedback in banking apps relies heavily on specific two-word technical phrases (e.g., "face id", "log in") rather than single keywords.
- **Evidence anchors:**
  - [abstract] Abstract mentions topic modeling results generally.
  - [section] Page 4, Fig. 2/3 and text: "bi–grams perform the best... coherence... increases... bi–gram model with 15 topics is the best overall."
  - [corpus] "Mobile Application Review Summarization..." supports the density of information in app reviews requiring nuanced extraction.
- **Break condition:** If the vocabulary becomes too large or reviews are very short (e.g., "Great app"), bigram frequency may drop below statistical significance.

### Mechanism 2
- **Claim:** Supervised classification outperforms lexicon-based sentiment analysis for domain-specific banking reviews.
- **Mechanism:** Generic sentiment dictionaries (TextBlob, VADER) fail to detect nuances in technical complaints or sarcasm. Training a supervised model (LSTM/MNB) on manually labeled "ground truth" data allows the algorithm to learn domain-specific correlations between text patterns and sentiment.
- **Core assumption:** The heuristic used for manual labeling (rating ± 1 standard deviation) accurately captures the semantic sentiment of the text.
- **Evidence anchors:**
  - [abstract] "Sentiment analysis compared methods... LSTM achieving 82\% accuracy... Multinomial Naive Bayes 77\%."
  - [section] Page 5, Table I: Shows TextBlob labeling specific complaints as "neutral" or "positive" incorrectly.
  - [corpus] "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment..." explicitly validates the misalignment between simple ratings/lexicons and actual text sentiment.
- **Break condition:** If the manual labeling heuristic is flawed (e.g., a 2-star review with sarcastic praise), the model learns incorrect patterns (Garbage In, Garbage Out).

### Mechanism 3
- **Claim:** App updates function as a primary causal trigger for negative sentiment clusters.
- **Mechanism:** Updates disrupt user muscle memory and introduce new bugs. Users frequently employ temporal language ("since update", "bring back") that links their dissatisfaction directly to a specific version change.
- **Core assumption:** Negative reviews coinciding with updates are caused by the update itself, rather than external factors (e.g., bank rates).
- **Evidence anchors:**
  - [abstract] "...negative reviews identified... dissatisfaction with updates."
  - [section] Page 6: Negative topics highlight "Problematic Update" and phrases like "worked great before" or "since the update."
  - [corpus] No direct corpus evidence provided in neighbors regarding update mechanics; assumption based on paper's topic extraction.
- **Break condition:** The mechanism weakens if the update interval is longer than the review collection period, masking the temporal correlation.

## Foundational Learning

- **Concept: Latent Dirichlet Allocation (LDA)**
  - **Why needed here:** This is the engine used to discover hidden topics (e.g., "login issues") from raw text without prior labeling.
  - **Quick check question:** How does the "perplexity" score change as you increase the number of topics, and why might lower perplexity not always mean "better" topics?

- **Concept: Heuristic Labeling Thresholds**
  - **Why needed here:** The paper creates its "ground truth" training data by using Standard Deviation (SD) on ratings. You must understand this to replicate the data prep.
  - **Quick check question:** Why did the authors choose 1 SD rather than a simple cutoff (e.g., <3 stars = negative) to label the training data?

- **Concept: LSTM vs. Multinomial Naive Bayes (MNB)**
  - **Why needed here:** The paper achieves best results with different models for iOS (LSTM) vs. Google (MNB). Understanding the difference explains why.
  - **Quick check question:** MNB relies on word frequency independence; why might LSTM (which handles sequence) capture more nuance in shorter, more emotional iOS reviews?

## Architecture Onboarding

- **Component map:**
  1. Ingest: Web Scrapers (App Stores) → Raw Reviews.
  2. Preprocess: NLTK (Lowercase, Stopwords, Lemmatization) → N-grams (Uni/Bi/Tri).
  3. Topic Engine: LDA (Gensim) → 15 Topics (Coherence scoring).
  4. Labeling Engine: Manual (Rating ± SD) → Training Set.
  5. Sentiment Classifier: LSTM (iOS) / MNB (Google) → Positive/Negative/Neutral.

- **Critical path:** The **Manual Labeling Heuristic**. The entire supervised learning accuracy (82%) depends entirely on the assumption that "ratings 1 SD below average = negative text." If this correlation is weak, the models train on noise.

- **Design tradeoffs:**
  - **Accuracy vs. Interpretability:** LDA topics are clusters of words; human interpretation is required to name them (e.g., "Topic 1 = Login Issues").
  - **Model Complexity:** LSTM is heavier but necessary for iOS; MNB is lighter and sufficient for Google Play data in this specific study.

- **Failure signatures:**
  - **TextBlob/VADER failure:** Detecting "neutral" sentiment in text that is clearly frustrated but lacks "negative" keywords (e.g., "update turns screen white" → labeled Neutral).
  - **Topic overlap:** LDA may produce distinct topics that are semantically the same (e.g., "Login" vs "Password").

- **First 3 experiments:**
  1. **Validate the Labeler:** Sample 100 reviews labeled "Negative" by the SD method and manually verify they contain negative sentiment text to confirm ground truth.
  2. **N-gram A/B Test:** Run the LDA model on the same dataset using Unigrams vs. Bigrams and compare the top 10 words of the top topic for readability.
  3. **Update Correlation:** Plot the volume of reviews containing the word "update" against the specific dates of known bank app releases to verify the causal link.

## Open Questions the Paper Calls Out
None

## Limitations
- The manual labeling heuristic (±1 SD rule) may introduce bias if the correlation between ratings and text sentiment is weak.
- LSTM architecture details (epochs, batch size, hidden units) are unspecified, preventing exact reproduction.
- The non-English review detection method is not documented, which could affect the Google Play dataset's representativeness.

## Confidence

- **High Confidence:** The finding that TextBlob and VADER perform poorly (24-33% accuracy) compared to supervised models is robust, as it is directly supported by Table I showing clear misclassification examples.
- **Medium Confidence:** The superiority of bigrams for topic modeling (15 topics) is credible but relies on coherence scores that may not fully capture human interpretability.
- **Medium Confidence:** The causal link between updates and negative sentiment is plausible based on temporal language patterns but lacks direct validation against release dates.
- **Low Confidence:** The exact accuracy values (82% for LSTM, 77% for MNB) depend heavily on the unverified labeling heuristic and unspecified hyperparameters.

## Next Checks

1. **Validate the Labeler:** Sample 100 reviews labeled "Negative" by the SD method and manually verify they contain negative sentiment text to confirm ground truth accuracy.
2. **N-gram A/B Test:** Run the LDA model on the same dataset using Unigrams vs. Bigrams and compare the top 10 words of the top topic for readability and coherence.
3. **Update Correlation:** Plot the volume of reviews containing the word "update" against the specific dates of known bank app releases to verify the temporal correlation between updates and negative sentiment spikes.