---
ver: rpa2
title: 'The Affine Divergence: Aligning Activation Updates Beyond Normalisation'
arxiv_id: '2512.22247'
source_url: https://arxiv.org/abs/2512.22247
tags:
- affine
- these
- which
- divergence
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates a fundamental mismatch between ideal and
  effective gradient updates for neural network activations during training. While
  parameters update in their direction of steepest descent, propagated activation
  updates do not, exhibiting sample-wise scaling biases across affine, convolutional,
  and attention layers.
---

# The Affine Divergence: Aligning Activation Updates Beyond Normalisation

## Quick Facts
- arXiv ID: 2512.22247
- Source URL: https://arxiv.org/abs/2512.22247
- Reference count: 24
- Primary result: Affine-like correction achieves 50.56% CIFAR-10 accuracy vs 49.08% for BatchNorm

## Executive Summary
This paper investigates a fundamental mismatch between ideal and effective gradient updates for neural network activations during training. While parameters update in their direction of steepest descent, propagated activation updates exhibit sample-wise scaling biases across affine, convolutional, and attention layers. The author proposes structural corrections to realign activation updates with their mathematically ideal gradient directions, which naturally yield normalization-like functional forms derived from first principles. Extensive experiments on fully-connected networks show the affine-like correction consistently outperforms conventional normalizers across various architectures and activation functions, challenging the scale-invariance hypothesis for normalization's effectiveness.

## Method Summary
The method introduces structural corrections to address the "affine divergence" - a mismatch between mathematically ideal and effective activation updates during gradient descent. The affine-like correction divides affine layer outputs by √(‖x‖² + 1), preserving all representational degrees of freedom while realigning effective updates with ideal gradients. Experiments compare this correction against BatchNorm, LayerNorm, RMSNorm, and de-parameterized normalizers on fully-connected networks trained on CIFAR-10. The correction is also derived for convolution (PatchNorm) and identified in attention mechanisms, though practical implementations remain challenging. The approach challenges traditional explanations for normalization's success by showing that scale-invariance is not necessary for improved optimization.

## Key Results
- Affine-like correction achieves 50.56% CIFAR-10 accuracy vs 49.08% for BatchNorm on fully-connected networks
- The divergence theory predicts and validates a negative correlation between batch size and performance for structural corrections
- Results challenge the scale-invariance hypothesis for normalization success, showing effective alternatives exist
- Normalizers can be algebraically decomposed into activation-function-like maps with parameterizable scaling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: There exists a systematic mismatch between mathematically ideal and effective activation updates during gradient descent.
- Mechanism: For affine layer z = Wx + b, parameter updates propagate to activations as Δz/η = g(‖x‖² + 1), not Δz/η = g. The (‖x‖² + 1) term causes sample-wise scaling bias—high-magnitude inputs receive disproportionately large effective updates.
- Core assumption: First-order learning rate approximation and single-layer propagation (corrections don't account for multi-layer nonlinear interactions).
- Evidence anchors:
  - [abstract] "propagated activation updates do not, exhibiting sample-wise scaling biases across affine, convolutional, and attention layers"
  - [Section 2.1, Eqn. 11] "∆L/∆zi = ∂L/∂zi (‖x⃗‖² + 1)"
  - [corpus] Weak corpus support—this is a novel theoretical contribution not directly tested in neighbors.
- Break condition: Multi-layer residual connections violate single-layer assumption (acknowledged in App.B.3); very deep networks may compound approximation errors.

### Mechanism 2
- Claim: The affine-like structural correction z' = (Wx + b)/√(‖x‖² + 1) exactly cancels the divergence and outperforms conventional normalizers.
- Mechanism: Dividing by √(‖x‖² + 1) counteracts the (‖x‖² + 1) scaling factor, realigning effective updates with ideal gradients. Unlike norm-based methods, this preserves all representational degrees of freedom (no projection to hypersphere).
- Core assumption: The divergence correction is causally responsible for improved optimization, not scale-invariance.
- Evidence anchors:
  - [Section 2.3, Eqn. 19] "⃗z′ = W⃗x + ⃗b / √(‖⃗x‖² + 1)"
  - [Section 3 results] "affine-like correction (red) outperforms all other normalisers...50.56% CIFAR-10 accuracy vs 49.08% for BatchNorm"
  - [corpus] No direct corpus validation of this specific functional form.
- Break condition: Convolutional PatchNorm (analogous correction) underperforms (App.B.2, Figs. 8-9)—patch interdependencies violate single-sample assumptions.

### Mechanism 3
- Claim: Structural corrections should exhibit negative correlation between batch size and performance due to off-diagonal interference terms.
- Mechanism: Batched corrections introduce Mbk = (xk·xb + 1b) mixing matrix. Off-diagonal terms represent "interference" from other samples in the batch. Larger batches → more interference → worse alignment with individual ideal updates.
- Core assumption: Samples are IID and the diagonal (ideal) term should dominate off-diagonal (interference) terms.
- Evidence anchors:
  - [App.B.1, Eqn. 41] "y′_bi = y_bi − η′g_bi [Ideal] − η′Nbkgbk [Effective Interference]"
  - [App.B.1, Table 1] "Affine-Like Correction...Slope: (−2.45±0.17)×10⁻²" (negative for Standard Tanh)
  - [corpus] No corpus papers test batch-size correlations for normalizers.
- Break condition: LayerNorm and RMSNorm show inconsistent correlations—their more complex gradient mixing doesn't yield clean predictions.

## Foundational Learning

- Concept: **Gradient descent optimizes parameters, not activations directly**
  - Why needed here: The paper's central claim is that parameter-centric optimization neglects activation-space geometry. Understanding that activations are "proxies" for loss reduction is prerequisite.
  - Quick check question: Can you explain why we update weights instead of activations during training?

- Concept: **Normalizers as degree-of-freedom reducers**
  - Why needed here: The paper reframes normalizers as maps that irreversibly project out radial information (R^n → S^(n-1) → R^n), distinct from scale-invariance explanations.
  - Quick check question: What information is lost when applying LayerNorm to a 2D vector?

- Concept: **First-order Taylor approximation in learning rate**
  - Why needed here: All derivations assume η → 0, ignoring η² terms. Understanding this boundary clarifies where theory may break down with large learning rates.
  - Quick check question: Why would Adam's adaptive learning rates complicate this analysis?

## Architecture Onboarding

- Component map:
  - **Standard affine**: z = Wx + b (divergence present)
  - **Norm-like correction**: z = W(x/‖x‖) + b (RMSNorm-like, loses radial DOF)
  - **Affine-like correction**: z = (Wx + b)/√(‖x‖² + 1) (preserves DOF, recommended)
  - **PatchNorm (conv)**: Patch-wise analog—currently underperforms

- Critical path:
  1. Compute ‖x‖² per sample
  2. Apply sqrt: s = √(‖x‖² + 1)
  3. Divide output: z = (Wx + b) / s
  4. Backward pass: ∂L/∂x = g·(W/s − y·x/s²) [Eqn. 28]

- Design tradeoffs:
  - Affine-like vs norm-like: Affine-like preserves DOF and avoids ‖x‖→0 singularity but lacks scale-invariance
  - Implementation: Requires per-sample norm computation—similar cost to LayerNorm
  - Batch size: Smaller batches reduce interference (validated prediction)

- Failure signatures:
  - Very small inputs (‖x‖≈0) cause norm-like to explode (requires ε); affine-like is more stable
  - Convolution PatchNorm underperforms due to patch interdependencies violating assumptions
  - Residual networks: Skip connections break single-layer approximation (App.B.3)

- First 3 experiments:
  1. **Baseline comparison**: Implement affine-like correction on MLP with CIFAR-10, compare against BatchNorm/LayerNorm/RMSNorm across widths [16, 32, 64, 128]
  2. **Batch size sweep**: Validate negative correlation prediction—test batch sizes [8, 16, 32, 64, 128] with affine-like correction
  3. **Ablation on deeper networks**: Test if single-layer approximation degrades—compare 2-layer vs 5-layer vs 10-layer networks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the affine-like correction perform in larger-scale architectures (e.g., deep CNNs, transformers) where the single-layer approximation may compound errors across many layers?
- Basis in paper: [explicit] "This potentially positions the affine-like correction as a good and novel form of normalisers to be explored in larger architectures, where the founding approximations continue to hold."
- Why unresolved: Experiments were limited to small fully-connected networks (widths up to 256, depths up to 3 hidden layers); the first-order, single-layer approximation accuracy degrades as depth increases.
- What evidence would resolve it: Systematic evaluation of affine-like corrections on ImageNet-scale CNNs and transformer architectures, comparing against BatchNorm/LayerNorm baselines with matched compute budgets.

### Open Question 2
- Question: How do adaptive optimizers (e.g., Adam, AdamW) interact with structural corrections for the affine divergence?
- Basis in paper: [explicit] "Future work could also investigate the relation to the choice of optimiser, as these are likely non-trivial in effect, and the ramifications of rescaling by running statistics on representation updates could be further studied, such as element-wise rescaling by ADAM."
- Why unresolved: Adam's per-parameter adaptive learning rates introduce additional sample-dependent scaling that may amplify, cancel, or interact non-trivially with the affine divergence corrections.
- What evidence would resolve it: Controlled ablations comparing SGD vs. Adam with affine-like corrections across architectures, measuring whether Adam's rescaling partially obviates or conflicts with structural corrections.

### Open Question 3
- Question: Can convolution-specific structural corrections be derived that account for patch interdependencies while preserving translational equivariance?
- Basis in paper: [explicit] "Generalising the ideal-effective misalignment theory to convolution may require substantial further, nuanced considerations to construct functions that produce approximations that better respect the construction of convolutional layers, and this is encouraged for future work."
- Why unresolved: The derived PatchNorm is algebraically equivalent to the affine correction but underperforms because patches nonlinearly interact within a single sample, violating the independence assumption used in the derivation.
- What evidence would resolve it: Derivation of alternative convolutional corrections that incorporate multi-patch coupling analytically or via efficient approximations, followed by empirical validation on standard CNN benchmarks.

## Limitations

- The theoretical framework relies on first-order learning rate approximations and single-layer propagation, which break down in multi-layer networks with residual connections and attention mechanisms.
- The PatchNorm correction for convolutions currently underperforms, suggesting fundamental limitations in the current approach.
- The affine divergence concept and its structural corrections are novel theoretical contributions with limited external validation and no corpus support for the specific functional forms.

## Confidence

**High Confidence**: The empirical observation that affine-like correction outperforms conventional normalizers on fully-connected networks (50.56% vs 49.08% CIFAR-10 accuracy). The mathematical derivation of the affine divergence from gradient update mismatch is internally consistent.

**Medium Confidence**: The theoretical prediction that structural corrections should show negative batch-size correlation. The core assumption that this divergence explains normalization's empirical success. The claim that affine-like correction preserves representational degrees of freedom better than norm-based methods.

**Low Confidence**: The applicability of these corrections to convolutional networks (PatchNorm underperforms). The theory's predictions for attention mechanisms (computationally challenging). The broader claim that this framework provides a complete mechanistic explanation for normalization's success across all architectures.

## Next Checks

1. **Batch Size Correlation Validation**: Systematically test the affine-like correction across batch sizes [8, 16, 32, 64, 128] to empirically verify the predicted negative correlation. This directly tests the theory's most distinctive prediction and would provide strong evidence for or against the divergence framework.

2. **Multi-Layer Network Evaluation**: Test whether the affine-like correction maintains its advantage in deeper networks (5-10 hidden layers) compared to BatchNorm/LayerNorm. This validates whether the single-layer approximation holds or breaks down with increased depth and residual connections.

3. **Attention Mechanism Correction**: Attempt to derive and implement the analogous correction for multi-head attention mechanisms. Success or failure here would test whether the divergence framework extends beyond fully-connected layers and could explain attention normalization practices.