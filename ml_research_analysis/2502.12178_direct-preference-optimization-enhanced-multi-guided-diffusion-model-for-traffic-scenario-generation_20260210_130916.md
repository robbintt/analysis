---
ver: rpa2
title: Direct Preference Optimization-Enhanced Multi-Guided Diffusion Model for Traffic
  Scenario Generation
arxiv_id: '2502.12178'
source_url: https://arxiv.org/abs/2502.12178
tags:
- diffusion
- traffic
- mudi
- arxiv
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Direct Preference Optimization-Enhanced Multi-Guided Diffusion Model for Traffic Scenario Generation

## Quick Facts
- arXiv ID: 2502.12178
- Source URL: https://arxiv.org/abs/2502.12178
- Authors: Seungjun Yu; Kisung Kim; Daejung Kim; Haewook Han; Jinhan Lee
- Reference count: 40
- Primary result: A diffusion model fine-tuned with DPO can generate diverse, realistic, and controllable traffic scenarios that balance realism with rule compliance.

## Executive Summary
This paper presents MuDi-Pro, a diffusion-based model for generating diverse and controllable traffic scenarios. It introduces a multi-task learning framework with a guidance conditional layer that allows a single model to handle multiple and combined guidance functions. The model is fine-tuned using Direct Preference Optimization (DPO) to optimize preferences based on guide scores, avoiding the need for expensive and often non-differentiable gradient calculations.

## Method Summary
MuDi-Pro uses a two-stage training process. First, a Diffusion Transformer (DiT) backbone is trained to predict clean trajectories from noisy inputs, using a unicycle dynamics model for physical feasibility and classifier-free sampling via dropout. Second, the model is fine-tuned with DPO and a guidance conditional layer. This layer transforms guidance inputs into a latent space that modulates the transformer blocks, enabling the model to process various guidance functions. The DPO algorithm constructs preference datasets based on heuristic guide scores to optimize the model without a separate reward model.

## Key Results
- MuDi-Pro outperforms baseline models in realism (measured by comfort metrics and collision rates) and controllability (measured by guidance loss).
- The model achieves higher diversity (measured by L2 distance and angle difference from ground truth) while maintaining data-driven alignment.
- Fine-tuning with DPO and the guidance conditional layer improves performance over training separate single-guide models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning the diffusion model using Direct Preference Optimization (DPO) enhances guided sampling precision by directly optimizing preferences based on guide scores, circumventing expensive and non-differentiable gradient calculations.
- Mechanism: The DPO algorithm constructs a preference dataset of winning and losing samples based on heuristic guide scores. It then directly optimizes the model parameters to increase the likelihood of preferred trajectories without learning a separate reward model, reparameterizing the optimization to align with these scores.
- Core assumption: Guide scores effectively serve as a proxy for task preferences (e.g., rule compliance), and the DPO loss function can successfully align the model with these preferences without a dedicated reward model.
- Evidence anchors:
  - [abstract]: "...fine-tuned using the Direct Preference Optimization (DPO) algorithm. This algorithm optimizes preferences based on guide scores, effectively navigating the complexities and challenges associated with the expensive and often non-differentiable gradient calculations..."
  - [section]: Section III.C "Fine-tuning via DPO" details the construction of the preference dataset D and the application of the DPO loss function (Eq. 5).
  - [corpus]: Related work (e.g., InPO, RoDiF) confirms applying DPO to diffusion models is a viable alignment strategy, though its specific use for *guide scores* is a novel application here.
- Break condition: If guide scores do not correlate well with desired outcomes or if the DPO optimization leads to over-fitting on specific guide combinations at the expense of general realism.

### Mechanism 2
- Claim: A multi-task learning framework with a guidance conditional layer enables a single diffusion model to handle multiple and combined guidance functions effectively.
- Mechanism: The model adds a "guidance conditional layer" analogous to a task conditional layer. A "guidance embedding layer" transforms the input guidance into a vector, which is processed by a "guidance encoding network" into a latent space. This latent is integrated into the transformer blocks, allowing the model to modulate its processing based on the specific guidance provided.
- Core assumption: The transformer architecture can effectively condition its output on the guidance latent to produce distinct, high-quality results for different guidance types without suffering from interference or catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "This model adopts a multi-task learning framework, enabling a single diffusion model to process various guide inputs."
  - [section]: Section III.C "Guidance Conditional Layer" and Figure 3a describe the architecture and its integration into the transformer blocks.
  - [corpus]: Weak/missing direct evidence for this specific conditional architecture in traffic generation within the provided corpus.
- Break condition: If the model fails to generalize across different guidance combinations or if performance degrades significantly compared to training separate, single-guide models.

### Mechanism 3
- Claim: Combining Classifier-Free Sampling (CFS) with clean trajectory-guided sampling allows for a tunable balance between realism/diversity and controllability.
- Mechanism: The model is trained with future-conditioned and non-future-conditioned inputs using dropout. At inference, a CFS weight w blends the outputs: w=1.0 uses future info (reconstruction), w=0.0 ignores it (sampling). This predicted clean trajectory is then perturbed by guidance gradients during the denoising steps to enforce rules.
- Core assumption: The blending of future-conditioned and non-future-conditioned predictions provides a controllable spectrum of trajectory diversity, and the guidance perturbation effectively steers generation without destroying underlying priors.
- Evidence anchors:
  - [section]: Section III.B "Classifier-free Sampling and Clean Trajectory Guided Sampling" (Eq. 3 & 4) defines the blending and perturbation.
  - [corpus]: This builds directly on the TRACE framework [16], validating the use of clean trajectory guidance.
- Break condition: If guidance strength is too high, it overrides learned priors, leading to unrealistic artifacts. If the CFS weight is not well-calibrated, it may produce overly constrained or overly random trajectories.

## Foundational Learning

- Concept: **Denoising Diffusion Probabilistic Models (DDPMs)**
  - Why needed here: The system's backbone is a diffusion model that learns to reverse a noise-adding process to generate data.
  - Quick check question: Can you explain the forward (adding noise) and reverse (denoising) processes in a standard diffusion model?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: This is the core fine-tuning method used to align the model with "preferences" derived from guide scores, replacing more complex RL methods.
  - Quick check question: How does DPO differ from Reinforcement Learning from Human Feedback (RLHF), particularly regarding the need for a separate reward model?

- Concept: **Classifier-Free Guidance/Sampling**
  - Why needed here: This technique allows the model to be conditioned on context while controlling the influence of that context (specifically future information) at inference time.
  - Quick check question: How does training with random dropout of a condition enable guidance strength control during sampling?

## Architecture Onboarding

- Component map: **MuDi-Pro Architecture**
  - **Input Encoders:** Tokenize noisy trajectories, maps, past/future trajectories.
  - **Backbone:** Transformer blocks process these tokens. Includes **AdaLN** (Adaptive Layer Norm) for context scaling.
  - **Guidance Conditional Layer:** (Added during fine-tuning) Guidance Embedding -> Guidance Encoding Network -> Guidance Latent. This latent modulates the transformer blocks.
  - **Decoder:** Reconstructs the clean action/trajectory.
  - **DPO Fine-tuning Loop:** Uses a frozen reference model and the trainable model to compute loss based on winning/losing sample pairs.

- Critical path: The critical path for performance is the **DPO fine-tuning stage**. The quality of the preference dataset (derived from guide scores) directly determines the model's ability to follow rules while maintaining realism. Errors in guide score calculation or pairing logic here will propagate into a misaligned model.

- Design tradeoffs:
  - **Single vs. Multi-Guide Models:** The paper argues for a single model using a conditional layer for efficiency and flexibility, but this adds architectural complexity and may face optimization challenges.
  - **DPO vs. RLHF:** DPO is chosen for simplicity (no reward model) and to avoid reward hacking, but requires a reliable way to generate preference pairs.
  - **Reconstruction (w=1.0) vs. Sampling (w=0.0):** Reconstruction aligns with ground truth (high realism, low diversity), while sampling explores possibilities (high diversity). The choice of `w` is a key inference-time tradeoff.

- Failure signatures:
  - **Guide over-fitting:** The model becomes too rigid, generating identical or unrealistic trajectories just to satisfy a rule.
  - **Catastrophic forgetting:** After fine-tuning for guides, the model forgets basic traffic priors, leading to collisions or off-road driving even without guides.
  - **Poor CFS calibration:** Trajectories are either too random or too deterministic, failing to strike the desired balance.

- First 3 experiments:
  1. **Ablation on Guidance Conditional Layer:** Test performance when the conditional layer is removed or replaced with a simpler method (e.g., concatenation) to validate its architectural contribution.
  2. **Sensitivity Analysis on CFS weight `w`:** Run inference across a range of `w` values (0.0 to 1.0) and plot metrics for realism, diversity, and controllability to characterize the tradeoff.
  3. **DPO vs. Baseline Fine-Tuning:** Compare DPO performance against a baseline that uses the same guide scores as a direct loss for fine-tuning, to demonstrate the advantage of the preference-based approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending the classifier-free sampling mechanism from trajectory space to latent space improve the model's generation capabilities?
- Basis in paper: [explicit] The authors state in the Conclusion that "its classifier-free sampling is limited to trajectory space" and that "Future work will explore latent space sampling... to extend its capabilities."
- Why unresolved: The current architecture operates directly on trajectory tokens during sampling, and the paper does not investigate the potential benefits of latent space manipulation.
- What evidence would resolve it: A comparison of generation diversity and fidelity between the current trajectory-space implementation and a modified latent-space sampling approach.

### Open Question 2
- Question: How can the efficiency of the guided fine-tuning process be improved to facilitate broader application?
- Basis in paper: [explicit] The Conclusion explicitly lists "improve the efficiency of the guided fine-tuning process" as a direction for future work to extend the model's capabilities.
- Why unresolved: The paper introduces the DPO fine-tuning method but does not provide an analysis of its computational cost or propose methods to optimize the training speed.
- What evidence would resolve it: A study reporting training times and resource utilization, alongside proposed algorithmic optimizations that reduce convergence time without degrading performance.

### Open Question 3
- Question: Does the reliance on heuristic guidance scores for preference optimization fail to capture nuances of realism that human feedback would capture?
- Basis in paper: [inferred] The paper mentions using guidance scores to create preference pairs "rather than relying on human preference feedback" (Section III.C) to avoid reward hacking and cost.
- Why unresolved: While the method avoids the cost of human labeling, it is unclear if optimizing for simple rule-based scores perfectly aligns with the complex, subjective human perception of "realism."
- What evidence would resolve it: A user study comparing scenarios generated by guide-score-based DPO against those generated by human-feedback-based RLHF, specifically assessing subjective realism.

### Open Question 4
- Question: Does the necessary exclusion of collision-prone training data limit the model's ability to simulate reactive safety-critical scenarios?
- Basis in paper: [inferred] The authors exclude 13.9% of agents with pre-existing collisions to prevent "meaningless movements" (Section IV.A), potentially creating a bias away from collision-adjacent interactions.
- Why unresolved: While filtering improves basic metrics, it creates a domain gap where the model may not learn the specific dynamics of avoiding accidents that have already begun.
- What evidence would resolve it: An evaluation of the model's performance on a test set specifically composed of safety-critical scenarios where collision avoidance is required.

## Limitations
- The paper does not specify key hyperparameters (learning rates, batch sizes, diffusion steps, noise schedule) or architecture dimensions (hidden size, number of layers, number of heads), making faithful reproduction difficult.
- The specific mathematical formulations for the rules and the exact calculation method for the "preference score" used to sort DPO pairs are not explicitly defined.
- The exclusion of 13.9% of agents with pre-existing collisions may create a bias away from collision-adjacent interactions, potentially limiting the model's ability to simulate reactive safety-critical scenarios.

## Confidence
- MuDi-Pro architecture design: High
- DPO fine-tuning mechanism: High
- Guidance conditional layer functionality: Medium
- Specific implementation details: Low

## Next Checks
1. **Validate the unicycle dynamics model integration**: Confirm the conversion from predicted actions to states is correctly implemented and physically feasible.
2. **Test the guidance score calculation**: Verify the heuristic guide scores are correctly computed and correlate with desired outcomes before using them for DPO.
3. **Run sensitivity analysis on DPO scaling factor**: Experiment with different $\beta$ values to find the optimal balance between rule compliance and realism.