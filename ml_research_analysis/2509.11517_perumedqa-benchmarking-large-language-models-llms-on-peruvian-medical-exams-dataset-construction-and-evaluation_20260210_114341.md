---
ver: rpa2
title: 'PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams
  -- Dataset Construction and Evaluation'
arxiv_id: '2509.11517'
source_url: https://arxiv.org/abs/2509.11517
tags:
- test
- medgemma-4b-it
- surgery
- medgemma-27b-text-it
- biomistral-7b-dare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed PeruMedQA, a dataset of 8,380 medical multiple-choice
  questions from Peruvian specialty exams, to evaluate eight medical LLMs in Spanish.
  Fine-tuning the smallest model, medgemma-4b-it, using parameter-efficient fine-tuning
  and LoRA improved its accuracy significantly, enabling it to outperform other models
  under 10B parameters and rival a 70B-parameter model.
---

# PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation

## Quick Facts
- **arXiv ID**: 2509.11517
- **Source URL**: https://arxiv.org/abs/2509.11517
- **Reference count**: 0
- **Primary result**: Developed PeruMedQA dataset of 8,380 medical multiple-choice questions from Peruvian specialty exams, evaluated eight medical LLMs in Spanish, and demonstrated that fine-tuning smaller models can achieve performance rivaling larger models in specialized medical domains

## Executive Summary
This study introduces PeruMedQA, a comprehensive dataset of 8,380 multiple-choice medical questions derived from Peruvian specialty examinations, designed to evaluate the performance of large language models in Spanish medical contexts. The researchers benchmarked eight medical LLMs, including both general-purpose and specialized models, using this dataset. A key finding was that fine-tuning the smallest model, medgemma-4b-it, through parameter-efficient techniques and LoRA adapters significantly improved its accuracy, enabling it to outperform other models under 10B parameters and rival a 70B-parameter model. The largest model, medgemma-27b-text-it, achieved over 90% accuracy in several exams, establishing it as optimal for Spanish medical AI applications.

## Method Summary
The study constructed PeruMedQA by collecting multiple-choice questions from Peruvian medical specialty exams, creating a dataset of 8,380 questions across various medical disciplines. Eight medical LLMs were evaluated on this dataset, including both general and specialized models ranging from 4B to 70B parameters. The researchers employed parameter-efficient fine-tuning techniques, specifically LoRA adapters, to optimize the smaller medgemma-4b-it model. The evaluation methodology involved assessing model performance across different medical specialties and comparing results between pre-trained and fine-tuned versions of the models.

## Key Results
- Fine-tuned medgemma-4b-it model outperformed other models under 10B parameters and rivaled a 70B-parameter model
- Medgemma-27b-text-it achieved over 90% accuracy in several medical specialty exams
- Parameter-efficient fine-tuning with LoRA adapters enabled smaller models to excel in specialized medical domains

## Why This Works (Mechanism)
The success of fine-tuned smaller models in medical domains can be attributed to the focused nature of the PeruMedQA dataset, which captures the specific knowledge and reasoning patterns required for Peruvian medical specialty exams. The parameter-efficient fine-tuning approach allows the model to adapt to this specialized domain without requiring full retraining, preserving the general language capabilities while enhancing medical-specific knowledge. The LoRA adapters enable efficient adaptation by modifying the model's behavior through low-rank matrix decomposition, making the fine-tuning process computationally feasible while achieving significant performance gains.

## Foundational Learning
- **Spanish medical terminology** - why needed: The dataset contains Spanish-language medical questions requiring understanding of both general Spanish and medical vocabulary; quick check: Can the model correctly translate and interpret specialized medical terms in Spanish
- **Multiple-choice question reasoning** - why needed: Medical exams use specific question formats requiring pattern recognition and elimination strategies; quick check: Does the model correctly identify distractors and select optimal answers
- **Peruvian medical education standards** - why needed: The dataset reflects specific curriculum and examination standards of Peruvian medical education; quick check: Can the model demonstrate knowledge of local medical protocols and practices
- **Parameter-efficient fine-tuning concepts** - why needed: Understanding LoRA and adapter-based methods is crucial for model optimization; quick check: Can the model maintain performance while using reduced parameter counts
- **Medical specialty knowledge domains** - why needed: Different specialties require distinct knowledge bases and reasoning approaches; quick check: Does the model show consistent performance across various medical specialties
- **Benchmark evaluation methodology** - why needed: Proper evaluation requires understanding of metrics and comparative analysis; quick check: Are the performance metrics statistically significant and comparable across models

## Architecture Onboarding
Component map: PeruMedQA dataset -> Model evaluation pipeline -> Fine-tuning process -> Performance comparison
Critical path: Data collection → Model selection → Fine-tuning implementation → Benchmark execution → Results analysis
Design tradeoffs: Larger models provide better baseline performance but require more computational resources, while smaller fine-tuned models offer cost-effective alternatives with competitive accuracy
Failure signatures: Poor performance on questions requiring integration of multiple medical concepts, inability to handle questions outside the training distribution, inconsistent results across different medical specialties
First experiments: 1) Test model performance on held-out questions from the same specialty exams, 2) Evaluate cross-specialty generalization by testing on questions from different medical fields, 3) Assess model calibration by comparing confidence scores with actual accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset represents only Peruvian medical examination system, limiting generalizability to other Spanish-speaking healthcare contexts
- Evaluation focuses exclusively on multiple-choice questions, which may not capture full breadth of clinical reasoning required in practice
- Study does not address potential biases in training data or examine model performance on real-world medical scenarios

## Confidence
- Fine-tuned smaller models rivaling larger models: **Medium** confidence - Limited comparison to small set of models under 10B parameters
- Medgemma-27b-text-it as optimal for Spanish medical AI: **High** confidence - Consistently superior performance across multiple examinations

## Next Checks
1. Evaluate the models on medical examination datasets from other Spanish-speaking countries to assess generalizability
2. Test the fine-tuned models on clinical case scenarios and open-ended medical questions to evaluate real-world applicability
3. Conduct a bias analysis of both the PeruMedQA dataset and the fine-tuned models to ensure equitable performance across different medical specialties and demographic groups