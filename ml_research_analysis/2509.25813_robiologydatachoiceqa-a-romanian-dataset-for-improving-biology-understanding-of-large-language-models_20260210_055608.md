---
ver: rpa2
title: 'RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding
  of Large Language Models'
arxiv_id: '2509.25813'
source_url: https://arxiv.org/abs/2509.25813
tags:
- questions
- dataset
- answer
- data
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoBiologyDataChoiceQA, a Romanian-language
  dataset of approximately 14,000 multiple-choice biology questions curated from national
  olympiads and medical school admission exams. The dataset is designed to benchmark
  and improve the performance of large language models (LLMs) on domain-specific scientific
  tasks in low-resource languages.
---

# RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding of Large Language Models

## Quick Facts
- arXiv ID: 2509.25813
- Source URL: https://arxiv.org/abs/2509.25813
- Reference count: 40
- Introduces 14K+ Romanian biology MCQs to benchmark and improve LLM domain performance

## Executive Summary
This paper introduces RoBiologyDataChoiceQA, a Romanian-language dataset of approximately 14,000 multiple-choice biology questions curated from national olympiads and medical school admission exams. The dataset is designed to benchmark and improve the performance of large language models (LLMs) on domain-specific scientific tasks in low-resource languages. Extensive experiments evaluate various LLMs across zero-shot and fine-tuning settings, with prompt engineering and model adaptation techniques. Results show significant performance variation across models, with fine-tuning notably improving accuracy. The study highlights both the strengths and limitations of current LLMs in handling specialized biology knowledge and linguistic nuances in Romanian, offering insights for future research in domain-specific and multilingual NLP applications.

## Method Summary
The study uses the RoBiologyDataChoiceQA dataset with 14,109 questions split into 11,347 train / 1,374 val / 1,388 test. Models are evaluated in zero-shot, few-shot, and fine-tuning settings. Zero-shot uses temperature=0; fine-tuning Gemini 1.5 Flash via Google AI Studio CSV upload and Gemma 2 9B Instruct via LoRA with Unsloth. Experiments test three question types (single-choice, group-choice, multiple-choice) across different model tiers, measuring accuracy per type/grade/stage/source.

## Key Results
- Fine-tuning improves accuracy across all question types
- Few-shot prompting degrades performance due to overfixation on specific letters
- Domain reasoning ability, not language proficiency, appears to be the primary bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on domain-specific Romanian biology questions improves model accuracy across all question types.
- Mechanism: Domain-specific fine-tuning adjusts model weights to better represent specialized biological terminology and reasoning patterns in Romanian, creating stronger associations between technical concepts and their correct applications.
- Core assumption: The training distribution is representative of evaluation questions and sufficient signal exists in ~11K examples to shift model behavior.
- Evidence anchors:
  - [abstract] "Extensive experiments evaluate various LLMs across zero-shot and fine-tuning settings... with fine-tuning notably improving accuracy"
  - [section 4.3-4.4] Fine-tuned Gemini 1.5 Flash achieved 0.752 single-choice accuracy (vs 0.668 baseline); Gemma 2 9B improved by 12+ percentage points across categories
  - [corpus] MedQARo paper (arXiv:2508.16390) similarly found fine-tuning Romanian medical QA improved performance, suggesting domain adaptation is transferable across Romanian scientific domains
- Break condition: If training data contains systematic errors from OCR artifacts or incorrect answer keys, fine-tuning may reinforce wrong associations.

### Mechanism 2
- Claim: Few-shot prompting degrades or fails to improve performance on Romanian biology MCQs.
- Mechanism: Models exhibit overfixation on specific answer letters when given examples, interfering with actual reasoning. The examples may not transfer well to Romanian-specific phrasing or biology reasoning patterns.
- Core assumption: The few-shot examples are correctly formatted and representative, but models still fail to generalize the task structure.
- Evidence anchors:
  - [section 4.1] "Running the models with a few-shot approach did not yield substantial improvements... in fact, some models performed worse"
  - [section 4.1] "certain LLMs exhibited a tendency to overfixate on specific letters after being presented with examplesâ€”interestingly, not necessarily the ones included in the prompt"
  - [corpus] No direct corpus support for this specific Romanian biology phenomenon; related work on MCQ ordering effects exists (Pezeshkpour and Hruschka 2024 cited in paper) but mechanism for few-shot degradation in low-resource languages is underexplored
- Break condition: If few-shot examples were poorly selected or formatted, the degradation may be an artifact rather than a general phenomenon.

### Mechanism 3
- Claim: Domain reasoning ability, not language proficiency, is the primary performance bottleneck for Romanian biology QA.
- Mechanism: Romanian-adapted models showed marginal improvements over multilingual models because they already handle Romanian syntax adequately; the constraint is applying biological knowledge to complex reasoning scenarios.
- Core assumption: Romanian-trained models genuinely have stronger Romanian language understanding, which is not directly measured.
- Evidence anchors:
  - [section 4.1] "Romanian-trained models (Rogemma2, Rollama3-8B-Instruct-Imat, and Romistral-7B-Instruct) did not show a significant advantage... performance appears to be primarily constrained by the models' ability to reason about biological concepts"
  - [section 4.6] Models performed better on molecular biology (grades IX, XII) than physiology (grades X, XI), suggesting topic-specific knowledge gaps
  - [corpus] Weak corpus evidence; RoQLlama paper showed Romanian medical QA improvements but didn't isolate language vs domain effects
- Break condition: Without cross-lingual evaluation (e.g., testing Romanian models on English biology), language and domain effects cannot be fully disentangled.

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: The paper uses LoRA via Unsloth to fine-tune Gemma 2 9B efficiently on limited GPU resources (RTX 3070, 8GB VRAM)
  - Quick check question: Can you explain why LoRA reduces memory requirements compared to full fine-tuning, and what the rank hyperparameter controls?

- Concept: **Stratified Data Splitting**
  - Why needed here: The dataset uses stratified sampling across grades, difficulty tiers, and institutions to ensure balanced coverage; improper splits could leak difficulty patterns
  - Quick check question: Why would random splitting potentially bias evaluation if some grades are systematically harder?

- Concept: **Group-Choice Question Encoding**
  - Why needed here: Romanian Olympiads use a unique format where 4 numbered options map to 5 letter answers (A-E) based on combination rules; models must learn this mapping
  - Quick check question: Given the mapping rules (A=1,2,3 correct; B=1,3; C=2,4; D=4 only; E=all), how would you reformulate this as a standard classification task?

## Architecture Onboarding

- Component map:
  Data Pipeline: PDF extraction (PyMuPDF4LLM for text PDFs) -> OCR (Gemini 1.5 Flash for scanned images) -> Regex parsing -> Grammar correction (Gemini/Gemma validation) -> Deduplication (jina-embeddings-v3)
  Question Types: Single-choice (1 correct), Group-choice (combinatorial mapping A-E), Multiple-choice (any subset correct)
  Evaluation Framework: Zero-shot/few-shot prompting with temperature=0, accuracy by type/grade/stage/source
  Fine-tuning Pipeline: LoRA via Unsloth (batch sizes 16-64, 1-5 epochs) for open models; Google AI Studio CSV upload for Gemini

- Critical path:
  1. Data quality validation (OCR errors, answer key verification) -> 2. Baseline zero-shot evaluation -> 3. Fine-tuning with stratified training split -> 4. Group-choice heuristic optimization (Section 4.5)

- Design tradeoffs:
  - **OCR accuracy vs scale**: AI-based OCR (Gemini) better than Tesseract but introduces hallucination risk; manual verification required
  - **Duplicate retention vs removal**: Duplicates marked but kept for pattern analysis; may inflate training signal
  - **Romanian-specific vs multilingual models**: Romanian-adapted models didn't justify their additional training cost for this task

- Failure signatures:
  - **Overfixation on letters**: Model outputs same letter repeatedly after few-shot examples
  - **Superficial associative reasoning**: Correct answers require nuanced clinical reasoning but models default to common co-occurrences (Section 4.9)
  - **Lexical confusion**: Models conflate similar terms (bronchi vs bronchioles) without contextual disambiguation

- First 3 experiments:
  1. Establish zero-shot baselines on test split for single/group/multiple-choice across 3 model tiers (large API models, mid-range open models, Romanian-adapted models) to identify which question type is hardest
  2. Fine-tune Gemma 2 9B with LoRA (batch 32, 3 epochs) and evaluate at 100-step intervals to find convergence point before overfitting
  3. Implement group-choice reformulation as multiple-choice (Section 4.5) with heuristics for invalid combinations to determine if format confusion limits performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can cross-lingual evaluation effectively disentangle the performance degradation caused by the Romanian language barrier from that caused by the inherent complexity of biological reasoning?
- Basis in paper: [explicit] The limitations section states the authors "do not perform a cross-lingual evaluation (e.g., testing models on an English version of the dataset) to isolate the impact of language from domain complexity."
- Why unresolved: The current study benchmarks only in Romanian, making it impossible to separate linguistic struggles from domain knowledge gaps.
- What evidence would resolve it: A comparative study evaluating the same LLMs on a translated English version of the dataset versus the original Romanian version.

### Open Question 2
- Question: To what degree does data contamination (presence of Romanian olympiad or admission questions in pre-training corpora) artificially inflate the accuracy of state-of-the-art models?
- Basis in paper: [explicit] The authors acknowledge they "do not explicitly verify whether the dataset's questions appear in the training data of the evaluated language models" and list "Potential Data Leakage" as a key limitation.
- Why unresolved: Lack of transparency regarding the training data of closed-source and large open-weight models prevents the verification of benchmark validity.
- What evidence would resolve it: Implementing n-gram overlap checks or membership inference attacks to detect if specific questions were present in the models' pre-training data.

### Open Question 3
- Question: Does the inclusion of "thinking tokens" or chain-of-thought prompting significantly reduce the "superficial associative reasoning" errors identified in complex biology questions?
- Basis in paper: [explicit] In the error analysis, the authors note that models rely on superficial reasoning and suggest a "potential need for models to also ponder their responses," a technique they "did not sufficiently investigate."
- Why unresolved: The current experiments relied on standard zero-shot and few-shot prompts without allowing for internal reasoning steps before the final output.
- What evidence would resolve it: Re-evaluating the specific subset of "hard" questions using reasoning-optimized inference strategies to see if accuracy improves without fine-tuning.

## Limitations
- OCR errors from scanned PDFs introduce noise and potential incompleteness in the dataset
- Modest training size (~11K examples) may limit fine-tuning effectiveness for complex reasoning
- Highly imbalanced baseline performance across question types suggests evaluation may not equally challenge all reasoning capabilities

## Confidence

- **High confidence**: The finding that fine-tuning improves accuracy across all question types is well-supported by experimental results showing consistent gains (e.g., Gemma 2 9B improving by 12+ percentage points). The zero-shot baseline results and per-type performance breakdowns are clearly reported.

- **Medium confidence**: The claim that few-shot prompting degrades performance due to overfixation on specific letters is supported by observed performance drops, but the exact mechanism remains speculative without controlled experiments isolating letter bias from other factors.

- **Low confidence**: The assertion that domain reasoning ability, not language proficiency, is the primary bottleneck is weakly supported. While Romanian-adapted models showed marginal improvements, the paper lacks cross-lingual evaluation to definitively separate language from domain effects.

## Next Checks
1. **Cross-lingual evaluation**: Test the best-performing models on equivalent English biology MCQs to isolate whether performance gaps reflect language proficiency versus domain knowledge limitations.

2. **Controlled few-shot experiments**: Systematically vary example letter distribution and question types in few-shot prompts to determine if letter fixation is the true cause of degradation or an artifact of prompt construction.

3. **Data quality audit**: Conduct a random sample verification of 100+ questions to quantify OCR error rates and identify systematic issues in the dataset that could bias model training and evaluation.