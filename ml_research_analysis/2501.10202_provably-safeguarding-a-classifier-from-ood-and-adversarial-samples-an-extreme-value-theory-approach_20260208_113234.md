---
ver: rpa2
title: 'Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme
  Value Theory Approach'
arxiv_id: '2501.10202'
source_url: https://arxiv.org/abs/2501.10202
tags:
- samples
- adversarial
- detection
- extreme
- spade
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel method, Sample-efficient Probabilistic\
  \ Detection using Extreme Value Theory (SPADE), which transforms a classifier into\
  \ an abstaining classifier, offering provable protection against out-of-distribution\
  \ (OOD) and adversarial samples. The approach is based on a Generalized Extreme\
  \ Value (GEV) model of the training distribution in the classifier\u2019s latent\
  \ space, enabling the formal characterization of OOD samples."
---

# Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach

## Quick Facts
- **arXiv ID**: 2501.10202
- **Source URL**: https://arxiv.org/abs/2501.10202
- **Reference count**: 40
- **Primary result**: Transforms a classifier into an abstaining classifier with provable protection against OOD and adversarial samples using EVT

## Executive Summary
This paper introduces SPADE, a novel method that leverages Extreme Value Theory (EVT) to transform a pre-trained classifier into an abstaining classifier. The approach fits a Generalized Pareto Distribution (GPD) to the tail of the training distribution in the classifier's latent space, enabling formal characterization of out-of-distribution (OOD) and adversarial samples. Under mild assumptions, the GEV model allows for provably rejecting adversarial samples whose perturbation exceeds a theoretical bound. Empirical validation on CIFAR-10, CIFAR-100, and ImageNet demonstrates the method's frugality, stability, and efficiency compared to state-of-the-art approaches.

## Method Summary
SPADE transforms a pre-trained neural network into an abstaining classifier by modeling the tail of the distance distribution in the latent space using Extreme Value Theory. The method extracts normalized latent embeddings from the teacher model, computes the k-th nearest neighbor distance for each class, and fits a GPD to distances exceeding a threshold t. During inference, samples are rejected as OOD or adversarial if their distance is statistically extreme relative to the fitted model. The approach relies on two key assumptions: well-separated class clusters in latent space and Lipschitz continuity of the embedding function for adversarial robustness.

## Key Results
- SPADE provides provable guarantees for rejecting OOD samples based on formal EVT characterization
- Under mild assumptions, the same GEV model formally characterizes adversarial samples for rejection
- Empirical validation shows SPADE outperforms state-of-the-art methods on CIFAR-10, CIFAR-100, and ImageNet while being more sample-efficient
- The method demonstrates stability across different neural architectures (ResNet, VGG, Vision Transformer)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling the tail of the distance distribution in the classifier's latent space provides probabilistic detection of OOD samples
- **Mechanism:** SPADE uses the Peak Over Threshold (POT) method from EVT to fit a GPD to the tail of training distances in latent space. Samples are flagged as OOD if their distance is statistically extreme relative to the fitted model.
- **Core assumption:** The latent embedding of ID data forms compact clusters such that the tail distribution can be characterized by EVT
- **Break condition:** If latent space is poorly structured (overlapping clusters or sparse), the tail distribution may flatten, causing false negatives

### Mechanism 2
- **Claim:** The EVT-based test used for OOD detection can provably reject adversarial samples if perturbation amplitude is sufficiently large
- **Mechanism:** Adversarial samples cross decision boundaries in latent space. Under K-Lipschitz assumption, a perturbation of magnitude ε can move the latent representation at most Kε. If this required movement exceeds the distance to the EVT-defined "abstain" boundary, the attack is detected.
- **Core assumption:** The teacher embedding h is K-Lipschitz, and adversarial perturbation ε exceeds the lower bound defined in Equation (6)
- **Break condition:** If adversarial perturbation is very small (below the bound), the sample may stay inside the GEV boundary and fool the classifier

### Mechanism 3
- **Claim:** The EVT models are sample-efficient and stable, allowing aggressive subsampling without significant performance degradation
- **Mechanism:** EVT characterizes the tail rather than full density, relying only on extremal properties of data. Maximum Likelihood Estimation on tail samples is robust even when trained on subsets.
- **Core assumption:** The tail behavior of the distance distribution is representative of class boundaries regardless of total sample count
- **Break condition:** If threshold t is set too high relative to available data, insufficient extreme samples remain to fit GPD parameters reliably

## Foundational Learning

- **Concept: Extreme Value Theory (EVT) & Generalized Pareto Distribution (GPD)**
  - **Why needed here:** EVT models the tail rather than bulk of data, allowing formal definition of what is "statistically extreme" (anomaly/OOD)
  - **Quick check question:** If you fit a Gaussian to data, does it accurately model the probability of events 5 standard deviations away? (Hint: EVT argues it does not)

- **Concept: Abstaining Classifier (Selective Classification)**
  - **Why needed here:** The goal is to refuse classification on risky inputs rather than classify everything
  - **Quick check question:** What is the operational behavior of classifier fτ when the OOD probability exceeds 1-τ?

- **Concept: Latent Space Geometry (Lipschitz Continuity)**
  - **Why needed here:** The theoretical guarantee against adversarial attacks relies on the embedding function h being Lipschitz continuous
  - **Quick check question:** Why does a smaller Lipschitz constant K make the classifier easier to safeguard against adversarial perturbations?

## Architecture Onboarding

- **Component map:**
  - Teacher Model (f = g ∘ h) -> Embedding Extractor (h) -> Distance Calculator -> GEV Model Store -> Decision Engine

- **Critical path:**
  1. Offline Training: Iterate through training set D, find k-NN distance, filter by threshold t, fit GPD parameters via MLE
  2. Online Inference: Extract embedding for query x, compute distance to class prototypes, consult GEV Model Store, return "Abstain" if extreme or f(x) otherwise

- **Design tradeoffs:**
  - Threshold t (POT): Too low = modeling non-extremes (bias); Too high = too few samples to fit parameters (variance)
  - Subsampling rate: Paper claims frugality, but extreme subsampling might lose rare edge cases of ID distribution
  - Lipschitz constraint: If teacher h is very rough (high Lipschitz constant), the bound on adversarial perturbation becomes very tight

- **Failure signatures:**
  - High False Positive Rate (FPR95): Model rejects too many valid ID samples, usually implies threshold or confidence misconfigured
  - Silent Adversarial Success: Adversarial examples with perturbation below Eq (6) bound are accepted and misclassified
  - GEV Fit Failure: MLE fails to converge (returns ξ < 0 or instability), indicating class distance distribution lacks defined tail

- **First 3 experiments:**
  1. Sanity Check (Latent Geometry): Visualize latent space and compute Variation (V) and Informativeness (I) for teacher. If V ≫ I, SPADE will likely fail
  2. Hyperparameter Sensitivity (k and t): Run Algorithm 1 on validation set, vary k and t, plot stability of estimated shape parameter ξ
  3. Adversarial Bound Verification: Generate adversarial samples (FGSM) with increasing ε, plot "Abstention Rate" vs ε, verify abstention spikes when ε crosses theoretical bound

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the GEV test be effectively extended to other empirical OOD detection criteria, such as the score margin used in MSP?
  - Basis: Section 7 suggests extending GEV test to empirical criteria like score margins in MSP
  - Why unresolved: Current study focuses exclusively on distance-based criteria in latent space
  - What evidence would resolve it: Experimental results demonstrating GEV model performance on MSP score margins

- **Open Question 2:** Does explicitly optimizing the Lipschitz constant of the classifier embedding during training improve SPADE's robustness and detection performance?
  - Basis: Section 7 suggests enhancing training loss to optimize Lipschitz constant
  - Why unresolved: Paper analyzes Lipschitz constant theoretically but doesn't implement training regime that actively optimizes it
  - What evidence would resolve it: Empirical evaluation of OOD detection performance on classifiers trained with Lipschitz regularization

- **Open Question 3:** Can "safe example behaviors" be formally identified and certified within the latent space to achieve broader certification of neural networks?
  - Basis: Section 7 states long-term goal is investigating whether these behaviors can be identified and certified
  - Why unresolved: Current work provides probabilistic rejection guarantees but not formal certification of network's global properties
  - What evidence would resolve it: Theoretical framework defining "safe behaviors" in latent space and proof-of-concept formal verification

## Limitations

- The core EVT-based detection mechanism critically depends on well-structured latent clusters, which the paper validates empirically on limited datasets
- The Lipschitz continuity assumption for adversarial robustness is rarely verified for modern deep networks, making the theoretical bound difficult to assess in practice
- The claimed "frugality" benefit is not robustly demonstrated with systematic ablations across different subsampling rates

## Confidence

- **Mechanism 1 (OOD Detection):** Medium - EVT framework is established but application to high-dimensional neural embeddings needs more empirical grounding
- **Mechanism 2 (Adversarial Defense):** Low - Lipschitz assumption is rarely true for modern networks, and perturbation bound is very tight in practice
- **Mechanism 3 (Sample Efficiency):** Medium - Theoretical argument for EVT's tail modeling is sound but empirical validation is limited

## Next Checks

1. **Latent Geometry Analysis:** Compute Variation (V) and Informativeness (I) metrics for each dataset-architecture pair to verify the core assumption that class clusters are well-separated

2. **Lipschitz Verification:** Empirically estimate the Lipschitz constant of the teacher network using randomized smoothing or spectral normalization techniques to assess validity of adversarial robustness claim

3. **Subsampling Sensitivity:** Systematically evaluate SPADE's performance while varying subsampling rate (10%, 25%, 50%, 100% of training data) to quantify true "frugality" benefit versus computational cost