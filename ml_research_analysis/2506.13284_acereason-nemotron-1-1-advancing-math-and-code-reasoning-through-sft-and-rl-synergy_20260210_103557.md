---
ver: rpa2
title: 'AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and
  RL Synergy'
arxiv_id: '2506.13284'
source_url: https://arxiv.org/abs/2506.13284
tags:
- training
- performance
- math
- code
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates the interplay between supervised
  fine-tuning (SFT) and reinforcement learning (RL) in building advanced reasoning
  models. It explores two SFT scaling strategies: increasing the number of prompts
  and increasing the number of responses per prompt.'
---

# AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy

## Quick Facts
- arXiv ID: 2506.13284
- Source URL: https://arxiv.org/abs/2506.13284
- Reference count: 40
- **Key outcome:** AceReason-Nemotron-1.1-7B achieves state-of-the-art among Qwen2.5-7B-based models, scoring 72.6% on AIME24, 64.8% on AIME25, and over 52% on LiveCodeBench benchmarks.

## Executive Summary
This paper systematically investigates the interplay between supervised fine-tuning (SFT) and reinforcement learning (RL) in building advanced reasoning models. The authors explore two SFT scaling strategies—increasing the number of prompts and increasing the number of responses per prompt—and find both improve reasoning performance, with prompt scaling yielding larger gains. RL training is then applied starting from different SFT models, revealing that stronger SFT models consistently produce better results after RL, although performance gaps narrow significantly during RL. A key finding is that RL initially degrades performance but is crucial for enabling the model to generate more concise reasoning, which benefits later RL stages. The study also shows that math-only RL significantly improves code reasoning performance, even when starting from strong SFT models. The final AceReason-Nemotron-1.1-7B model achieves state-of-the-art results among Qwen2.5-7B-based models.

## Method Summary
The authors build on Qwen2.5-Math-7B (with modified rope_theta for 128K context) using a combined SFT and RL approach. For SFT, they scale data through two strategies: increasing the number of unique prompts and increasing the number of responses per prompt, training for 5-6 epochs on 383K total prompts (247K math, 136K code). Responses are generated by DeepSeek-R1 with 9-gram decontamination applied. For RL, they use GRPO with token-level advantages, on-policy generation (8-16 rollouts per batch of 128), and a stage-wise curriculum: Math-8K→16K→24K→32K→Code-24K→32K→Math-32K final. Training temperature is tuned to maintain temperature-adjusted entropy around 0.3. Overlong filtering is applied in early stages only.

## Key Results
- SFT scaling: Both prompt and response scaling improve performance, with prompt scaling yielding larger gains.
- RL from SFT: Stronger SFT models consistently produce better RL results, but performance gaps narrow significantly during RL training.
- Stage-1 necessity: RL initially degrades performance but enables reasoning compression essential for later stages.
- Cross-domain transfer: Math-only RL significantly improves code reasoning performance, achieving over 52% on LiveCodeBench.

## Why This Works (Mechanism)
The combination of SFT and RL creates a powerful synergy where SFT provides a strong initial reasoning capability and RL refines it through reward-based optimization. The stage-wise curriculum with progressive context lengths allows the model to gradually handle more complex reasoning chains. The temperature-adjusted entropy constraint ensures appropriate exploration-exploitation balance during RL. The math-only RL improving code performance suggests transfer of abstract reasoning patterns across domains.

## Foundational Learning
- **GRPO (Group Relative Policy Optimization):** A variant of policy gradient methods that optimizes group-relative rewards. Needed for stable RL training with multiple rollouts. Quick check: Verify loss decreases monotonically during RL training.
- **Token-level advantages:** Computing rewards at the token level rather than sequence level for finer-grained optimization. Needed for detailed credit assignment. Quick check: Monitor advantage distribution to ensure proper scaling.
- **Temperature-adjusted entropy:** Tuning sampling temperature to maintain specific entropy levels during RL. Needed to balance exploration and exploitation. Quick check: Monitor entropy during training to ensure it stays near 0.3.
- **Stage-wise curriculum:** Progressive training with increasing context lengths and difficulty levels. Needed to build up reasoning capabilities gradually. Quick check: Verify performance improves at each stage transition.
- **Overlong filtering:** Removing excessively long responses during early training stages. Needed to encourage concise reasoning. Quick check: Compare response lengths with/without filtering.

## Architecture Onboarding
**Component Map:** Qwen2.5-Math-7B (base) -> SFT (5-6 epochs) -> RL Stage 1 (8K) -> RL Stage 2 (16K) -> RL Stage 3 (24K) -> RL Stage 4 (32K) -> RL Code stages (24K→32K) -> Final Math 32K

**Critical Path:** The sequence of RL stages is critical, with Stage-1 (8K) being essential despite initial performance degradation. The temperature-adjusted entropy constraint is crucial for stable training.

**Design Tradeoffs:** 
- SFT vs. RL investment: The paper suggests diminishing returns for initial SFT quality as RL progresses.
- Context length vs. efficiency: Longer contexts enable better reasoning but increase computational cost.
- Filtering vs. freedom: Early-stage overlong filtering encourages conciseness but may limit complex reasoning initially.

**Failure Signatures:** 
- Skipping Stage-1 causes suboptimal Stage-2 performance and prevents reasoning compression.
- Training temperature too low causes entropy collapse and over-exploitation.
- Applying overlong filtering at Stage-4 degrades performance by preventing learning of comprehensive generation.

**Three First Experiments:**
1. Train SFT with prompt scaling only and measure performance against response scaling baseline.
2. Run RL starting from weak SFT model with extended training to match strong SFT baseline performance.
3. Apply math-only RL to code tasks and measure cross-domain transfer benefits.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Is it more computationally efficient to invest resources in scaling SFT data for a stronger initialization, or to start with a weaker SFT model and rely on RL to bridge the performance gap?
**Basis in paper:** [explicit] Section 4.5.1 explicitly observes that "the performance gap between initial SFT models narrows significantly throughout the RL process," suggesting diminishing returns for initial SFT quality.
**Why unresolved:** While the paper confirms that stronger SFT models still win, it does not quantify the compute trade-off (data curation/SFT vs. RL steps) required to achieve a specific performance target.
**What evidence would resolve it:** A comparative study measuring total training FLOPs for a "Weak SFT + Extended RL" pipeline against a "Strong SFT + Short RL" pipeline to reach equivalent benchmark accuracy.

### Open Question 2
**Question:** What is the mechanistic explanation for the cross-domain generalization where math-only reinforcement learning substantially improves code reasoning performance?
**Basis in paper:** [inferred] Section 4.5.6 validates that Math-Only RL significantly boosts LiveCodeBench scores (Figure 11), but the paper provides only high-level conjectures regarding this transfer.
**Why unresolved:** The empirical result is counter-intuitive (improving code without code data), and the specific reasoning patterns or latent skills transferred (e.g., logic planning vs. syntactic robustness) remain unidentified.
**What evidence would resolve it:** An interpretability analysis comparing activation patterns or attention heads during code tasks before and after math-only RL to identify shared computational substrates.

### Open Question 3
**Question:** Does the "temperature-adjusted entropy ~0.3" heuristic generalize to larger model scales (e.g., 70B+) or different architectures (e.g., Llama)?
**Basis in paper:** [explicit] Section 4.5.2 proposes a "rule of thumb" to set sampling temperature such that entropy remains around 0.3, based on trials with Qwen2.5-7B.
**Why unresolved:** The finding is derived empirically from a specific 7B model; the optimal exploration-exploitation balance (and associated entropy) likely shifts with model capacity or architecture variance.
**What evidence would resolve it:** Applying the specific entropy 0.3 constraint during RL training on different base models and scales, comparing the results against baseline temperature tuning strategies.

## Limitations
- **Missing hyperparameters:** Learning rates, optimizers, and exact training durations for RL stages are not specified.
- **Filter criteria ambiguity:** Exact thresholds for "appropriate difficulty" based on reward distribution per rollout group are not defined.
- **Computational requirements:** The cluster configuration and total compute needed for full reproduction are not stated.

## Confidence

**High Confidence:** The core empirical findings about SFT scaling benefits (both prompt and response scaling improving performance, with prompt scaling having larger impact) and the observation that RL training from stronger SFT models yields better final performance are well-supported by the reported benchmark results across multiple datasets.

**Medium Confidence:** The claim that RL initially degrades performance but is crucial for enabling concise reasoning is supported by the training curriculum design, though the exact mechanism connecting Stage-1 to Stage-2 improvements could be more rigorously demonstrated.

**Medium Confidence:** The finding that math-only RL improves code reasoning performance is supported by the results, but the underlying transfer mechanisms and generalizability to other model architectures remain unexplored.

## Next Checks

1. **Stage-1 Necessity Validation:** Systematically compare AceReason-Nemotron-1.1 performance when starting RL directly at Stage-2 (16K) versus following the full 8K→16K→24K→32K curriculum, measuring both benchmark performance and average response length compression.

2. **Temperature Sensitivity Analysis:** Conduct controlled experiments varying the temperature-adjusted entropy target (e.g., 0.2, 0.3, 0.4) during RL training to determine optimal entropy levels and quantify the impact of temperature tuning on final performance and reasoning quality.

3. **Overlong Filtering Impact Study:** Compare final model performance with and without overlong filtering applied in Stage-4 (32K context), isolating the effect of this filtering decision on the model's ability to generate concise versus comprehensive reasoning chains.