---
ver: rpa2
title: First and Second Order Approximations to Stochastic Gradient Descent Methods
  with Momentum Terms
arxiv_id: '2504.13992'
source_url: https://arxiv.org/abs/2504.13992
tags:
- lemma
- theorem
- then
- proof
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops continuous approximations to stochastic gradient
  descent (SGD) methods with momentum terms, addressing the gap between empirical
  observations and rigorous mathematical analysis. The key innovation is extending
  existing diffusion approximation results to scenarios with both time-varying learning
  rates and momentum parameters.
---

# First and Second Order Approximations to Stochastic Gradient Descent Methods with Momentum Terms

## Quick Facts
- **arXiv ID**: 2504.13992
- **Source URL**: https://arxiv.org/abs/2504.13992
- **Reference count**: 4
- **Primary result**: Develops continuous ODE/SDE approximations to SGD with momentum, achieving O(h) and O(h²) error bounds

## Executive Summary
This paper develops continuous approximations to stochastic gradient descent methods with momentum terms, addressing the gap between empirical observations and rigorous mathematical analysis. The key innovation is extending existing diffusion approximation results to scenarios with both time-varying learning rates and momentum parameters. Under weak assumptions on the objective functions and their gradients, the paper establishes both ODE and SDE approximations that converge to the discrete process as the learning rate approaches zero.

The framework provides theoretical justification for common empirical practices like using constant momentum values around 0.9, showing that momentum-based methods achieve O(1/k²) convergence in convex settings compared to standard SGD's O(1/k) rate. The work bridges the gap between the continuous-time analysis of optimization algorithms and their discrete implementations used in practice.

## Method Summary
The paper treats discrete SGD with momentum as a 2d-dimensional system tracking consecutive iterates, enabling application of diffusion approximation techniques. It establishes weak convergence of the discrete process to continuous ODE and SDE approximations with quantified error bounds. The first-order approximation achieves O(h) error while a modified second-order SDE achieves O(h²) error uniformly in time. The framework accommodates various learning rate schedules and momentum parameter choices, with the continuous limit capturing both gradient and momentum effects through a modified drift term.

## Key Results
- Establishes O(h) weak approximation error for first-order ODE/SDE approximations to momentum-based SGD
- Proves O(h²) error bound for second-order SDE with modified drift term
- Shows momentum-based methods achieve O(1/k²) convergence in convex settings, improving upon standard SGD's O(1/k) rate
- Provides theoretical justification for empirical momentum practices like using constant values around 0.9

## Why This Works (Mechanism)

### Mechanism 1: 2d-Dimensional State Augmentation for Momentum
- Claim: Momentum-based SGD can be reformulated as a first-order Markov process by embedding consecutive iterates into an augmented state space.
- Mechanism: The Heavy Ball iteration is transformed into a 2d-dimensional process where the momentum term becomes structural rather than an additive correction. This enables application of standard diffusion approximation techniques that require Markov structure.
- Core assumption: Momentum parameter schedule and learning rate are bounded and vary smoothly (A1).
- Evidence anchors:
  - [Section 2.2]: "By studying the discrete process χ̊ := (χ_{n+1}, χ_n), we develop a diffusion approximation to momentum-based SGD."
  - [Section 4.2]: The transformation explicitly constructs J_γ^(n)(χ_n) = ∇j_γ^(n)(χ_n) where j encodes both the objective f and momentum coupling terms.
- Break condition: If momentum parameters vary discontinuously or learning rates are non-diagonalizable, the 2d embedding may not produce the required Lipschitz structure.

### Mechanism 2: Weak Approximation via Feynman-Kac Connection
- Claim: The expected value of test functions under the discrete process converges to expectations under continuous approximations with quantified error O(h).
- Mechanism: Using the Feynman-Kac formula, the paper connects discrete iterate expectations to solutions of PDEs driven by the limiting ODE/SDE. Taylor expansion of the value function accumulates local approximation errors; summation over k yields the global error bound.
- Core assumption: H_γ(x) ∈ G¹(R^d) (polynomially bounded gradients) and Σ(x) has Lipschitz continuous derivatives up to order 2 (A2, A3).
- Evidence anchors:
  - [Section 4, Lemma 5]: "Eg(χ_{T/h}^h) - g(X_T) = h² Σ EΦ_{kh}(χ_k^h) + h²ξ(h)" establishes the error expansion.
  - [Section 5.1]: The SDE case follows similarly with additional h-dependence in y_t^h requiring Lemma 11 to remove.
- Break condition: If objective gradients grow faster than polynomial (violating G¹), or if the covariance Σ is singular, the Feynman-Kac PDE solution may not exist or error bounds fail.

### Mechanism 3: Second-Order Correction via Modified Drift
- Claim: A modified SDE with drift correction term achieves O(h²) weak approximation error uniformly in time.
- Mechanism: Theorem 7 introduces an SDE with drift correction accounting for discretization error accumulated in the first-order approximation, analogous to Stratonovich-to-Itô correction in stochastic calculus.
- Core assumption: All assumptions from first-order case plus additional smoothness required for Taylor remainder bounds.
- Evidence anchors:
  - [Section 6, Theorem 7]: States the modified SDE achieving max_n |Eg(χ_n^h) - Eg(X_{nh}^h)| ∈ O(h²).
  - [Section 6, Lemma 12]: Bounds the difference between discrete and continuous moment expectations using Taylor expansions up to order ℓ+1.
- Break condition: If test function g lacks sufficient smoothness (∉ G^∞), the O(h²) error bound does not hold.

## Foundational Learning

- **Itô's Lemma and Stochastic Integration**
  - Why needed here: Core tool for deriving the Feynman-Kac PDE and computing derivatives of stochastic processes in the proof of Lemma 2 and Proposition 1.
  - Quick check question: If dX_t = μdt + σdW_t and f(t,x) = x², what is df(t,X_t)?

- **Weak Convergence of Markov Chains to SDEs**
  - Why needed here: The paper's main contribution is proving weak convergence of discrete SGD to continuous limits; understanding this paradigm is essential for interpreting Theorems 3, 5, and 7.
  - Quick check question: Explain why weak convergence (convergence of expectations E[g(X_n)] → E[g(X_t)]) differs from pathwise convergence.

- **Function Spaces G^κ and Polynomial Growth Bounds**
  - Why needed here: All assumptions and lemmas rely on H, J, and test functions being in G¹ or G^∞ (polynomially bounded with bounded derivatives). Understanding these spaces is necessary to verify assumptions (A2), (A3).
  - Quick check question: Is the function f(x) = e^{|x|²} in G^κ(R^d) for any κ? Why or why not?

## Architecture Onboarding

- **Component map:**
  ```
  Discrete SGD Process (χ_n)
         ↓ [2d embedding]
  Augmented Process (χ_n, χ_{n-1})
         ↓ [Taylor expansion + Feynman-Kac]
  Continuous Approximation (ODE: dX_t = U_t J̄(X_t)dt)
         ↓ [Add diffusion term √h Σ dW_t]
  SDE Approximation (dX^h_t = U_t J̄(X^h_t)dt + U_t√h Σ dW_t)
         ↓ [Drift correction for 2nd order]
  Second-Order SDE (modified drift)
  ```

- **Critical path:**
  1. Verify assumptions (A1)-(A3) for your objective and noise structure
  2. Construct the augmented 2d system and derive J from your momentum scheme
  3. Choose between ODE approximation (deterministic analysis) vs SDE approximation (captures noise)
  4. Determine if first-order error O(h) suffices or if second-order O(h²) is needed

- **Design tradeoffs:**
  - ODE vs SDE: ODE is simpler but ignores gradient noise; SDE captures stochasticity but requires specifying covariance Σ
  - First vs second order: Second order provides better approximation but requires modified drift implementation and smoother test functions
  - Time horizon T: Longer T amplifies accumulated error; Lemma 4 bounds grow as (1+Ch)^{T/h}

- **Failure signatures:**
  - Divergent iterates: May indicate H ∉ G¹ (gradients grow too fast)
  - Approximation error not decreasing as h→0: Check Lipschitz assumptions on Σ
  - Momentum benefits not appearing: Verify momentum coupling in J construction (Section 4.2)

- **First 3 experiments:**
  1. Reproduce Lemma 4 numerically: Run SGD with momentum on a simple convex objective (quadratic), plot ||χ_n||_p vs n for various h, verify bound C(1+|x|)
  2. Validate ODE approximation: Compare discrete iterate distribution mean to ODE solution X_t for known f, measure error scaling with h
  3. Test second-order correction: Implement both first and second-order SDEs from Theorems 5 and 7, plot max_n |E[g(χ_n)] - E[g(X_{nh})]| vs h on log-log scale to confirm O(h) vs O(h²) scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the diffusion approximation framework be rigorously extended to Nesterov accelerated gradient methods, and does the accelerated structure require modifications to the 2d-dimensional tracking approach?
- Basis in paper: [explicit] Conclusion states: "The generalization of learning rate schedules opens up a wide variety of applications and extension to other methods such as Nesterov Acceleration."
- Why unresolved: The paper develops the framework for Heavy Ball momentum, but Nesterov's "lookahead" gradient evaluation creates fundamentally different dynamics that may not fit the current 2d-dimensional formulation.
- What evidence would resolve it: A theorem analogous to Theorem 2 specifically for Nesterov iterates, with explicit drift and diffusion terms capturing the anticipation step.

### Open Question 2
- Question: How do the O(h) and O(h²) approximation guarantees degrade when learning rates are not small relative to the Lipschitz constants of the objective function?
- Basis in paper: [inferred] The theoretical bounds are proven as h→0, but practical deep learning uses learning rates that may violate the small-h regime implicit in assumption (A1).
- Why unresolved: The error expansions assume scaled learning rates with |ut| ∈ (0,1], but no analysis addresses the transition regime or provides explicit constants for practical h values.
- What evidence would resolve it: Empirical validation showing when Eg(χ^h) − Eg(X^h) deviates from O(h) predictions, with explicit bounds involving problem-dependent constants.

### Open Question 3
- Question: Can the momentum-based SDE approximation explain the empirically observed superiority of specific constant momentum values (e.g., 0.9) beyond the O(1/k²) convergence rate improvement?
- Basis in paper: [explicit] The paper notes "it is typical to set momentum to a constant, around 0.9" and provides "theoretical justification for common empirical practices," but the approximation itself does not derive optimal momentum schedules.
- Why unresolved: The framework accommodates arbitrary momentum schedules ζn but does not solve for optimal ζn or explain why 0.9 emerges as empirically preferred.
- What evidence would resolve it: Analysis showing that the continuous approximation's diffusion term or its variance is minimized near ζ ≈ 0.9 for typical loss landscapes.

## Limitations

- The polynomial growth assumptions (H_γ ∈ G¹) exclude objectives with exponential or super-polynomial growth, limiting applicability to many modern deep learning scenarios
- The diffusion approximation assumes sufficiently smooth learning rate and momentum schedules, which may not capture discrete updates common in practice
- The 2d-dimensional embedding approach introduces complexity that may not translate directly to implementation guidance
- The second-order correction term lacks empirical validation in the momentum SGD context

## Confidence

- **High Confidence**: The weak approximation error bounds O(h) for first-order and O(h²) for second-order approximations are well-established through the Feynman-Kac framework
- **Medium Confidence**: The practical implications for SGD with momentum are valid but require careful verification, particularly the connection between continuous approximations and actual training dynamics
- **Low Confidence**: The second-order correction term in Theorem 7, while mathematically derived, lacks empirical validation in the momentum SGD context

## Next Checks

1. **Empirical Error Scaling Verification**: Implement the continuous ODE/SDE approximations from Theorems 3 and 5 for a simple convex problem (e.g., quadratic objective). Measure approximation error E[g(χ^h_{T/h})] - E[g(X_T)] across different h values on a log-log plot to confirm O(h) and O(h²) scaling respectively.

2. **Assumption Compliance Testing**: For a practical non-convex objective (e.g., neural network training), numerically verify whether assumptions (A1)-(A3) hold. Specifically, check gradient growth bounds, Lipschitz continuity of Σ, and smoothness of learning rate/momentum schedules in actual training runs.

3. **Momentum Benefit Isolation**: Compare the convergence rates of discrete momentum SGD versus its continuous ODE approximation for both convex and non-convex objectives. Quantify the gap between theoretical O(1/k²) convergence and observed training dynamics to assess the practical value of the continuous framework.