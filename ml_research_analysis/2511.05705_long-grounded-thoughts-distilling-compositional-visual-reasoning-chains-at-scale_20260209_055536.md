---
ver: rpa2
title: 'Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at
  Scale'
arxiv_id: '2511.05705'
source_url: https://arxiv.org/abs/2511.05705
tags:
- reasoning
- data
- image
- question
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Long Grounded Thoughts, a large-scale vision-centric
  reasoning dataset synthesized through a two-stage process combining object-level
  metadata and compositional problem hardening. The method leverages VLMs and reasoning
  LLMs to produce high-quality synthetic data spanning over 1M multiple-choice questions
  and reasoning traces with complex structures.
---

# Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale

## Quick Facts
- arXiv ID: 2511.05705
- Source URL: https://arxiv.org/abs/2511.05705
- Authors: David Acuna, Chao-Han Huck Yang, Yuntian Deng, Jaehun Jung, Ximing Lu, Prithviraj Ammanabrolu, Hyunwoo Kim, Yuan-Hong Liao, Yejin Choi
- Reference count: 36
- Key outcome: Introduces Long Grounded Thoughts, a 1M+ synthetic vision-centric reasoning dataset synthesized through metadata-grounded MCQ generation and compositional hardening, achieving state-of-the-art vision reasoning performance and positive transfer to text/audio tasks

## Executive Summary
This paper introduces Long Grounded Thoughts, a large-scale vision-centric reasoning dataset synthesized through a two-stage process combining object-level metadata and compositional problem hardening. The method leverages VLMs and reasoning LLMs to produce high-quality synthetic data spanning over 1M multiple-choice questions and reasoning traces with complex structures. Experiments demonstrate that fine-tuning Qwen2.5-VL-7B on this data yields state-of-the-art results on vision-centric benchmarks and surprisingly transfers positively to text-only and audio reasoning tasks. Ablation studies reveal that SFT on high-quality data is essential for effective online RL, and staged offline RL matches online RL performance with less compute. The work provides the first comprehensive analysis of VLM post-training across reasoning, multi-hop tasks, and cross-modal transfer.

## Method Summary
The approach synthesizes vision-centric reasoning data through a two-stage pipeline: Stage 1 generates object-centric multiple-choice questions using dense captions and bounding-box metadata from DOCCI images, preventing question-collapse through semantic deduplication filters. Stage 2 composes 2-5 simpler MCQs into multi-hop problems using compositional hardening. Reasoning traces are synthesized via a two-step process where a VLM first generates simple CoTs, then a reasoning LLM expands them with richer cognitive behaviors while maintaining distribution compatibility. The method produces 1M+ training examples and 130K preference pairs for preference learning or RL fine-tuning.

## Key Results
- SOTA performance on vision-centric benchmarks (V* Bench, CV-Bench, MMVP, RealWorldQA, MMStar-V) with 0.757 accuracy
- Surprising positive transfer to text-only reasoning (+2.98% MMLU-Pro) and audio reasoning (+1.32% MMAU)
- GRPO from SFT base peaks at 70K samples (0.757) vs 10K from base (0.695), demonstrating SFT enables RL scaling
- Staged offline RL (SFT→DPO) achieves 0.740 accuracy matching online RL with less compute

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Object-level metadata (bounding boxes + tags) prevents question-collapse at scale, enabling diverse MCQ synthesis beyond 1M examples.
- **Mechanism:** Dense captions alone cause LLMs to generate redundant questions about the same salient objects. Bounding-box coordinates force the generator to target specific regions, distributing questions across all detected objects per image (median 10.7 boxes/image). A semantic similarity filter (composite score over question, answer, and category embeddings with τ_dup=0.82) removes near-duplicates.
- **Core assumption:** The LLM can meaningfully interpret normalized coordinates to guide question targeting despite operating purely in text modality.
- **Evidence anchors:** [abstract] "scale, where imagery and metadata (captions, bounding boxes) are used to generate diverse, verifiable visual questions"; [section] Figure 2 shows scaling comparison vs LPT; Section 2.1 describes metadata integration and saturation hypothesis; [corpus] LongPerceptualThoughts (LPT) uses caption-only generation and does not report similar scale (30K dataset)
- **Break condition:** If base VLM already saturates performance on simpler questions (Figure 4a), additional grounded-MCQ scale may yield diminishing returns without Stage 2 hardening.

### Mechanism 2
- **Claim:** Two-stage reasoning trace synthesis (VLM draft → Reasoning LLM expansion) preserves output-distribution compatibility while injecting complex cognitive behaviors.
- **Mechanism:** Direct sampling from reasoning LLMs (M_Reason) produces traces that deviate from VLM output distribution, degrading downstream fine-tuning. The method first samples a simple CoT from M_VLM (in-distribution), then prompts M_Reason to *continue* rather than rewrite, structurally anchoring expansion in VLM-compatible token patterns.
- **Core assumption:** Distribution mismatch between VLM and reasoning LLM outputs materially harms SFT effectiveness.
- **Evidence anchors:** [abstract] "Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs"; [section] Section 2.3: "naively sampling from M_Reason often produces CoTs that deviate significantly from those of the VLM, which we experimentally observed significantly degrades downstream performance"; [corpus] GRIT and VTool-R1 explore related distribution/grounding issues but do not isolate this specific two-stage expansion mechanism
- **Break condition:** If M_VLM already exhibits target cognitive behaviors (subgoal setting, backtracking, verification), expansion may add cost without proportional benefit.

### Mechanism 3
- **Claim:** SFT on non-linear reasoning traces functions as "skill teaching" that enables effective online RL scaling; without it, RL degrades beyond ~10K samples.
- **Mechanism:** Base instruct models lack structured cognitive behaviors. Online RL (GRPO) from this starting point peaks early (0.695 at 10K) then declines. SFT on high-quality traces with verification/backtracking patterns provides behavioral priors that RL can then optimize, shifting the peak to ~70K samples (0.757).
- **Core assumption:** The cognitive behaviors in SFT data (not just correctness) are the critical enabler for RL scaling, though this is confounded with data quality.
- **Evidence anchors:** [abstract] "SFT on high-quality data with non-linear reasoning traces is essential for effective online RL"; [section] Table 3 and Figure 5 (right): GRPO from base peaks at 10K (0.695) and degrades; from SFT base peaks at 70K (0.757); [corpus] No direct corpus comparison for this specific SFT→RL enabling mechanism
- **Break condition:** If SFT data quality is poor (low verification/backtracking rates), RL scaling benefit may not materialize. Staged offline RL (SFT→DPO) provides a compute-efficient alternative with comparable performance (0.740 vs 0.757).

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) on Synthetic Reasoning Traces**
  - Why needed here: The entire pipeline depends on SFT as the primary transfer mechanism and as the prerequisite for effective RL. Understanding how SFT on synthesized CoTs differs from standard instruction tuning is essential.
  - Quick check question: Given a VLM that outputs linear reasoning traces, what would you expect to happen if you fine-tune it on synthetic traces with 3x more backtracking and verification steps?

- **Concept: Preference Learning (DPO vs GRPO)**
  - Why needed here: The paper compares offline preference learning (DPO) and online RL (GRPO) as post-SFT stages. Understanding the compute/accuracy tradeoff (DPO: 0.740, GRPO: 0.757) is critical for resource-constrained deployments.
  - Quick check question: If you have 129K preference pairs but can only afford synchronous RL compute for 70K samples, which training path would you choose and why?

- **Concept: Cross-Modal Transfer in Multimodal Models**
  - Why needed here: The paper reports surprising positive transfer from vision-centric training to text-only (+2.98% MMLU-Pro) and audio (+1.32% MMAU) reasoning. The mechanism is hypothesized but not proven—understanding this uncertainty prevents overclaiming.
  - Quick check question: A colleague claims "training on visual reasoning universally improves text reasoning." Based on this paper alone, is this claim supported? What evidence would you need to verify it?

## Architecture Onboarding

- **Component map:** DOCCI images → Grounded-SAM-2 (bounding boxes + tags) → Stage 1 MCQ Generator → Semantic filter → Stage 2 Composer → Reasoning Trace Synthesizer (M_VLM Simple CoT → M_Reason expansion → Local verifier) → Training Pipeline (SFT → DPO/GRPO)
- **Critical path:** Metadata quality from Grounded-SAM-2 (confidence cutoff 0.9, max 9 same-category instances/image) → Prompt engineering for vision-centric MCQ generation (role-playing, coordinate interpretation) → Distribution-compatible trace synthesis (VLM-first, then reasoning expansion) → SFT before RL (non-optional for GRPO scaling)
- **Design tradeoffs:** Scale vs Complexity (Stage 1: 750K simpler questions vs Stage 2: 250K harder compositions); Offline vs Online RL (DPO: 0.740, decoupled compute vs GRPO: 0.757, synchronized compute, plateaus at 70K); Model tier for synthesis (Stage 1 uses 7B/32B models; Stage 2 reserves 72B/671B for complex compositions)
- **Failure signatures:** Question-collapse (scaling MCQ generation without metadata produces redundant questions); Distribution mismatch (direct M_Reason sampling yields traces with explicit caption references); RL degradation without SFT (GRPO from base model declines after 10K samples); Negative cross-modal transfer (training on Virgo degrades MMAU audio scores)
- **First 3 experiments:** (1) Reproduce scaling curve: Generate 100K grounded MCQs with and without bounding-box metadata; measure question diversity via embedding clustering and base VLM solve-rate; (2) Ablate trace synthesis: Compare VLM-only traces, reasoning-LLM-only traces, and two-stage expansion on held-out benchmark; isolate distribution-mismatch effect; (3) Validate SFT→RL dependency: Run GRPO from base instruct model, SFT on linear traces, and SFT on non-linear traces; plot performance vs sample count to confirm plateau shift

## Open Questions the Paper Calls Out
The paper identifies several open questions including how to overcome performance plateaus and degradation in online RL beyond 70k examples, and what specific mechanisms enable the positive transfer of vision-centric reasoning data to out-of-domain modalities like text-only and audio reasoning.

## Limitations
- Reliance on high-quality human-annotated dense captions limits generalizability to domains where such data is scarce
- Performance plateaus and degrades in online RL beyond 70k examples, with no proposed solution for scaling further
- Cross-modal transfer mechanism remains speculative despite empirical demonstration, with domain-dependent effects (positive for some benchmarks, negative for others)

## Confidence
- **High Confidence:** Data synthesis pipeline effectiveness, SFT→RL dependency, staged offline RL performance
- **Medium Confidence:** Scale benefits of grounded metadata, cross-modal transfer generality
- **Low Confidence:** Exact contribution of specific cognitive behaviors in SFT data, necessity of complex reasoning traces for all VLM reasoning tasks

## Next Checks
1. **Isolate transfer mechanism:** Train identical models on (a) visual reasoning data with same cognitive behaviors but no visual content, (b) visual data with only linear reasoning; measure cross-modal transfer to determine if modality or reasoning complexity drives generalization.
2. **Test SFT behavior dependency:** Create synthetic SFT data with (a) high correctness but linear reasoning, (b) lower correctness but rich verification/backtracking; compare GRPO scaling to determine whether behavioral priors or data quality primarily enable RL scaling.
3. **Evaluate metadata saturation point:** Systematically increase MCQ generation scale with and without bounding-box metadata; measure diversity via embedding clustering and base VLM solve-rate to identify where metadata benefits plateau or saturate.