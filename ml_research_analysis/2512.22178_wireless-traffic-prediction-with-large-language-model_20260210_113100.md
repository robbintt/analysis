---
ver: rpa2
title: Wireless Traffic Prediction with Large Language Model
arxiv_id: '2512.22178'
source_url: https://arxiv.org/abs/2512.22178
tags:
- uni00000013
- traffic
- prediction
- uni00000015
- uni0000001d
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes TIDES, a novel LLM-based framework for wireless
  traffic prediction that addresses the limitations of existing approaches by incorporating
  spatial-temporal correlations at city scale. TIDES employs a two-phase process:
  spatial-aware region clustering to group base stations by traffic patterns and spatial
  autocorrelation, followed by LLM-driven prediction using DeepSeek.'
---

# Wireless Traffic Prediction with Large Language Model

## Quick Facts
- **arXiv ID:** 2512.22178
- **Source URL:** https://arxiv.org/abs/2512.22178
- **Reference count:** 40
- **Primary result:** LLM-based TIDES framework achieves MAE 0.2193, RMSE 0.2958, MAPE 2.7481% on real-world 4G datasets

## Executive Summary
This paper introduces TIDES, a novel LLM-based framework for wireless traffic prediction that addresses the limitations of existing approaches by incorporating spatial-temporal correlations at city scale. TIDES employs a two-phase process: spatial-aware region clustering to group base stations by traffic patterns and spatial autocorrelation, followed by LLM-driven prediction using DeepSeek. The method introduces prompt engineering to convert numerical traffic data into structured language inputs and a DeepSeek module with cross-domain attention to capture spatial dependencies between regions. By fine-tuning only lightweight components while freezing core LLM layers, TIDES achieves efficient adaptation without excessive training overhead. Experiments on real-world 4G datasets demonstrate that TIDES significantly outperforms state-of-the-art baselines.

## Method Summary
TIDES uses a two-phase approach: first, it performs spatial-aware region clustering using K-means on features including geographic coordinates, traffic statistics, and Local Moran's I spatial autocorrelation scores. Second, for each cluster, it trains a specialized model that combines prompt engineering (converting traffic data to structured text with statistical descriptors) with a DeepSeek LLM. The model employs cross-domain attention to fuse spatial-temporal traffic features with LLM text representations, while freezing core LLM layers to reduce training overhead.

## Key Results
- TIDES achieves MAE of 0.2193, RMSE of 0.2958, and MAPE of 2.7481% in Zone A
- Significantly outperforms state-of-the-art baselines across all metrics
- Demonstrates scalability and spatial intelligence suitable for next-generation 6G networks
- Shows consistent improvements across different zones in the real-world 4G dataset

## Why This Works (Mechanism)

### Mechanism 1: Spatial-Aware Region Clustering with Autocorrelation
- **Claim:** Grouping base stations by both traffic patterns and spatial autocorrelation enables more accurate, region-specific prediction by capturing heterogeneous dynamics that a single global model misses.
- **Mechanism:** Construct per-region feature vector including geographic coordinates, traffic statistics, and Local Moran's I statistic. Apply enhanced K-means on normalized features to partition regions into clusters. Train personalized TIDES model for each cluster.
- **Core assumption:** Chosen features sufficiently capture traffic patterns and spatial dependencies for K-means to yield meaningful groupings.
- **Evidence anchors:** [abstract], [section III-A-1, IV-A-1], and corpus works validating regional heterogeneity focus.

### Mechanism 2: Prompt Engineering for Domain Alignment
- **Claim:** Converting numerical traffic into structured natural language prompts with rich statistical descriptors allows LLMs to apply reasoning/pattern recognition for forecasting.
- **Mechanism:** Apply reversible instance normalization, extract fundamental statistics/trends/time-of-day patterns/wireless-specific metrics, concatenate into text prompt fed to LLM.
- **Core assumption:** LLMs can interpret these descriptors and map them to temporal dynamics for forecasting.
- **Evidence anchors:** [abstract], [section IV-B-1], and corpus Time-LLM baseline supporting concept.

### Mechanism 3: Cross-Domain Attention for Spatial Dependency Fusion
- **Claim:** Cross-domain attention fuses spatial-temporal traffic features with LLM text representations to explicitly incorporate spatial context from neighbors.
- **Mechanism:** Compute symmetric normalized Laplacian of spatial graph. Use spatial attention with Laplacian-derived mask, then multi-head cross-domain attention (query: spatial features, key/value: LLM embeddings).
- **Core assumption:** Graph accurately represents influencing spatial dependencies and aligning features with LLM semantic space is effective.
- **Evidence anchors:** [abstract], [section IV-B-2], and corpus models using graph attention for spatial-temporal fusion.

## Foundational Learning

- **Concept: Local Moran's I (Spatial Autocorrelation)**
  - **Why needed here:** Quantifies how similar a region's traffic is to its neighbors, serving as a core feature for clustering mechanism.
  - **Quick check question:** Given traffic values for a region and its neighbors, can you explain if a high positive Local Moran's I indicates a traffic hotspot or coldspot?

- **Concept: Reversible Instance Normalization (RevIN)**
  - **Why needed here:** Stabilizes training by normalizing input traffic data, preventing gradient issues while allowing recovery of original scale.
  - **Quick check question:** Why is a reversible normalization scheme preferred over standard normalization when final output must be in original traffic scale?

- **Concept: Cross-Domain Attention**
  - **Why needed here:** Core fusion component aligning features from spatial-temporal traffic domain with textual semantic domain of LLM.
  - **Quick check question:** In TIDES cross-domain attention, which domain provides Query vectors and which provides Key/Value vectors, and what's the intuition?

## Architecture Onboarding

- **Component map:** Region Feature Extractor (coordinates, stats, Moran's I) -> K-means Clustering. RevIN Normalized Input -> Prompt Feature Extractor -> Text Prompt -> LLM (frozen) -> H_Prompt. In parallel: Spatial Attention (SA) with Graph Laplacian Mask -> H_SA. Fusion: Cross-Domain Attention (H_SA as Query, LLM embeddings as Key/Value) -> H_DA. Concatenate(H_Prompt, H_DA) -> LLM -> Final Linear Layer -> Prediction.

- **Critical path:** Quality of initial clustering (Phase 1) determines which specialized model is trained and applied. Within Phase 2, cross-domain attention is the central innovation integrating spatial awareness into LLM's predictive process.

- **Design tradeoffs:**
  - **Specialization vs. Complexity:** Clustering into many groups allows specialized models but increases system complexity and training overhead.
  - **Efficiency vs. Capability:** Freezing core LLM layers reduces training cost but may limit adaptation of internal representations.
  - **Spatial Graph Definition:** Using k-nearest neighbors is simple but may miss non-physical dependencies.

- **Failure signatures:**
  - **Poor Clustering:** If features are uninformative, clusters may be incoherent, leading to poorly specialized models.
  - **Attention Masking Issues:** Incorrectly constructed Laplacian mask could block relevant neighbor information or allow irrelevant noise.
  - **Prompt Mismatch:** If prompt format is unclear to LLM, it may not leverage statistical context effectively.

- **First 3 experiments:**
  1. **Cluster Ablation:** Run TIDES with K=1 vs. K=4 vs. other K values to quantify performance gain from clustering-based specialization.
  2. **Attention Ablation:** Compare full TIDES against variant without cross-domain attention to isolate spatial fusion contribution.
  3. **Feature Ablation:** Systematically remove feature groups from prompt to determine which components contribute most to predictive accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- K-means clustering assumes extracted features sufficiently capture true underlying traffic dynamics and spatial dependencies
- Choice of K=4 is somewhat arbitrary and may not be optimal for all scenarios
- Cross-domain attention depends heavily on accuracy of spatial graph G, which uses simple k-nearest neighbors
- Prompt engineering requires careful crafting of statistical descriptors that may be insufficient or cause the LLM to fail in mapping to future numerical values

## Confidence
- **High Confidence:** Core concept of using LLMs for wireless traffic prediction, overall two-phase architecture, and reported experimental results
- **Medium Confidence:** Effectiveness of specific prompt engineering scheme and cross-domain attention mechanism for spatial fusion
- **Medium Confidence:** Choice of K=4 for clustering and use of k-nearest neighbors for spatial graph

## Next Checks
1. **Cluster Sensitivity Analysis:** Conduct experiments with different values of K (K=2, K=4, K=8, K=16) to determine optimal number of clusters and quantify clustering-based specialization performance gain.

2. **Attention Mechanism Ablation:** Compare full TIDES model against variant without cross-domain attention to isolate contribution of spatial fusion mechanism.

3. **Prompt Feature Ablation:** Systematically remove or modify different components of the prompt to determine which contribute most to predictive accuracy and identify potential overfitting or redundancy.