---
ver: rpa2
title: Missing Data Multiple Imputation for Tabular Q-Learning in Online RL
arxiv_id: '2510.10709'
source_url: https://arxiv.org/abs/2510.10709
tags:
- missingness
- imputation
- state
- missing
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores multiple imputation ensembles for online reinforcement
  learning with missing state space data. The key insight is that maintaining multiple
  imputation pathways can better capture uncertainty under missingness while remaining
  computationally efficient.
---

# Missing Data Multiple Imputation for Tabular Q-Learning in Online RL

## Quick Facts
- arXiv ID: 2510.10709
- Source URL: https://arxiv.org/abs/2510.10709
- Reference count: 40
- Key outcome: Multiple imputation ensembles outperform single imputation and simple baselines in online RL with missing state data

## Executive Summary
This paper addresses the challenge of missing state space data in online reinforcement learning by proposing a multiple imputation ensemble approach. The authors maintain multiple imputation pathways to capture uncertainty under missingness while remaining computationally efficient. The framework generates imputations, learns Q-values, and selects actions using these pathways with fractional updates to ensure proper normalization. Experiments on a custom 8×8 grid world demonstrate that the multiple imputation approach is robust across various missingness mechanisms and outperforms baseline methods.

## Method Summary
The proposed method uses multiple imputation ensembles for online reinforcement learning with missing state data. Multiple imputation pathways are maintained to capture uncertainty, with fractional updates ensuring proper normalization. The approach generates imputations, learns Q-values, and performs action selection through these pathways. The framework is designed to be computationally efficient while providing better handling of missingness compared to single imputation methods.

## Key Results
- Multiple imputation ensembles outperform single imputation and simple baselines across total reward, steps in the river, and path length metrics
- The approach is particularly robust as missingness increases
- Treating missingness as a state may be beneficial under extreme missingness rates but adds state space complexity
- Performance evaluated across multiple missingness mechanisms (MCAR, MCOLOR, MFOG)

## Why This Works (Mechanism)
The approach works by maintaining multiple imputation pathways that capture the uncertainty inherent in missing data. By learning Q-values across these diverse imputation streams and using fractional updates for normalization, the agent can better handle the ambiguity of missing observations. The ensemble nature allows the agent to explore different plausible completions of the missing state space, leading to more robust decision-making under uncertainty.

## Foundational Learning
- **Multiple imputation**: Generating several plausible values for missing data to capture uncertainty - needed to avoid overconfidence in single imputed values; quick check: verify multiple distinct imputation pathways are maintained
- **Fractional updates**: Partial parameter updates to ensure proper normalization across multiple pathways - needed to maintain consistent value estimates; quick check: confirm update fractions sum to 1
- **Tabular Q-learning**: Traditional RL algorithm with state-action value tables - needed for baseline comparison and computational tractability; quick check: verify Q-table dimensions match state-action space
- **Missing data mechanisms**: MCAR (Missing Completely at Random), MCOLOR, MFOG - needed to test robustness across different missingness patterns; quick check: confirm missingness rates and mechanisms are properly implemented
- **Online RL**: Learning while interacting with environment rather than from pre-collected data - needed for real-world applicability; quick check: verify agent updates Q-values after each step

## Architecture Onboarding

**Component map**: Missing data -> Multiple imputation pathways -> Q-value learning -> Action selection -> Environment interaction

**Critical path**: Observation → Imputation → Q-value update → Action selection → Reward/transition → Next observation

**Design tradeoffs**: Multiple pathways capture uncertainty but increase computational cost; tabular approach is interpretable but doesn't scale to high-dimensional spaces; fractional updates ensure normalization but may slow convergence

**Failure signatures**: Poor performance when missingness rate is very high (>80%); convergence issues with improper fractional updates; sensitivity to number of imputation pathways chosen

**First experiments**:
1. Implement single imputation baseline on 8×8 grid world with varying missingness rates
2. Test multiple imputation with 2 pathways vs 5 pathways to identify optimal ensemble size
3. Compare performance across MCAR, MCOLOR, and MFOG missingness mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to single custom 8×8 grid world environment
- Narrow comparison with baseline methods, lacking modern deep RL approaches
- Computational overhead of maintaining multiple imputation pathways not thoroughly characterized
- Potential convergence issues with fractional update scheme in non-stationary environments

## Confidence

**High confidence**: The core algorithmic framework for multiple imputation ensembles in tabular RL is sound and the basic mathematical formulation is correct.

**Medium confidence**: The experimental results showing benefits of multiple imputation over single imputation and simple baselines, though limited by single environment.

**Medium confidence**: The claim that treating missingness as a state is beneficial under extreme missingness rates, though this requires more rigorous testing.

## Next Checks

1. Test the approach on at least two additional RL environments with different characteristics (e.g., continuous state spaces, higher-dimensional observations) to assess scalability and robustness.

2. Conduct ablation studies varying the number of imputation pathways to identify the optimal ensemble size and quantify the computational trade-offs.

3. Compare against modern deep RL baselines that handle missingness through learned embeddings or attention mechanisms to establish whether the tabular approach offers advantages beyond simple domains.