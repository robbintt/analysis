---
ver: rpa2
title: 'Reflective Personalization Optimization: A Post-hoc Rewriting Framework for
  Black-Box Large Language Models'
arxiv_id: '2511.05286'
source_url: https://arxiv.org/abs/2511.05286
tags:
- user
- arxiv
- personalization
- personalized
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reflective Personalization Optimization (RPO),
  a novel post-hoc rewriting framework for personalizing black-box large language
  models (LLMs). The core idea is to decouple content generation from personalization
  by first having a base model generate a generic response, then using a trained reflection
  module to rewrite it in alignment with user preferences.
---

# Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models

## Quick Facts
- arXiv ID: 2511.05286
- Source URL: https://arxiv.org/abs/2511.05286
- Authors: Teqi Hao; Xioayu Tan; Shaojie Shi; Yinghui Xu; Xihe Qiu
- Reference count: 40
- Primary result: RPO achieves state-of-the-art personalization on LaMP benchmark, outperforming zero-shot by 0.141 accuracy on LaMP-2 and best baseline by 0.025 R-1 on LaMP-5

## Executive Summary
This paper introduces Reflective Personalization Optimization (RPO), a novel post-hoc rewriting framework for personalizing black-box large language models (LLMs). The core idea is to decouple content generation from personalization by first having a base model generate a generic response, then using a trained reflection module to rewrite it in alignment with user preferences. This reflection module is trained using structured rewriting trajectories via supervised fine-tuning and refined with reinforcement learning under a progressive multi-context curriculum. Experiments on the LaMP benchmark show RPO significantly outperforms state-of-the-art baselines across classification, regression, and generation tasks, with improvements such as 0.355 accuracy (vs 0.214 for zero-shot) on LaMP-2, and 0.498 R-1 score (vs 0.473 for best baseline) on LaMP-5. RPO also demonstrates high modularity, compatibility with any base model, and interpretability through explicit reasoning steps.

## Method Summary
RPO operates through a two-stage pipeline: first, a black-box base model generates a high-quality generic response to a user query. Second, an external reflection module explicitly rewrites this output to align with the user's preferences using retrieved user history. The reflection module is trained on structured rewriting trajectories that contain explicit reasoning chains linking user profile attributes to specific rewriting actions. Training proceeds in two phases: supervised fine-tuning (SFT) on these trajectories, followed by reinforcement learning (RL) with a progressive multi-context curriculum that gradually increases the number of retrieved user examples from 2 to 6 shots. The framework uses semantic retrieval to select relevant user history and employs task-specific rewards during RL optimization.

## Key Results
- LaMP-2 (Movie Tagging): 0.355 accuracy vs 0.214 zero-shot, 0.340 RAG w/ ICL
- LaMP-3 (Product Rating): 1.197 MAE vs 1.297 RAG w/ ICL
- LaMP-5 (Title Generation): 0.498 R-1 vs 0.473 RAG w/ ICL, 0.475 zero-shot
- LaMP-7 (Tweet Paraphrasing): 0.449 R-1 vs 0.439 RAG w/ ICL
- Progressive curriculum (2-6 shots) consistently outperforms fixed-shot configurations

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Generate-Then-Rewrite Pipeline
Separating content generation from personalization reduces the dual burden where models must simultaneously produce accurate content and align with user style. A black-box base model first generates a semantically complete generic response. A separate, trainable reflection module then rewrites this response using retrieved user history, transforming an open-ended generation problem into a constrained rewriting task. The base model's generic output preserves semantic integrity well enough that the reflection module can focus primarily on style alignment without degrading factual content.

### Mechanism 2: Structured Rewriting Trajectories Make Latent Policy Explicit
Supervised fine-tuning on explicit reasoning chains that show step-by-step transformation from generic to personalized responses enables learning of user-specific decision-making patterns. A teacher model (GPT-4.1-mini) generates structured trajectories containing reasoning traces that explicitly link user profile attributes to rewriting actions. The reflection model is trained to predict these trajectories, internalizing the logic rather than surface patterns. The teacher model can reliably infer and articulate the reasoning behind personalization, and this reasoning transfers to the smaller reflection model.

### Mechanism 3: Progressive Multi-Context Curriculum Prevents Style Drift
Gradually increasing the number of retrieved user history examples during RL training helps the model learn to synthesize preferences from noisy contexts without overfitting to irrelevant history. RL training begins with 2-shot examples (forcing focus on core style signals) and progressively increases to 6-shot (teaching synthesis from complex, potentially noisy histories). This curriculum is formalized via a scheduling function K(e) based on training epoch. Core user preferences can be extracted from a small number of highly salient examples, and the model can learn to filter noise as context expands.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: RPO's User Retriever component uses semantic retrieval (Contriever/BGE) to select relevant user history. Understanding RAG helps grasp how personalized context is constructed and why retrieval quality matters (Table 4 shows random vs. semantic retrieval has 50%+ MAE difference on LaMP-3).
  - Quick check question: Can you explain why the paper concatenates query + base response for retrieval instead of using query alone?

- **Concept: Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF)**
  - Why needed here: The RL stage uses REINFORCE++_baseline with KL regularization against the SFT policy. Understanding reward shaping, KL penalties, and policy gradients is essential for debugging training instability or reward hacking.
  - Quick check question: What does the KL divergence term in equation (3) prevent, and what happens if β is set too high or too low?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Structured rewriting trajectories are fundamentally CoT traces that externalize reasoning. The reflection model generates `<thinkation>...</thinkation>` blocks before producing personalized outputs. Understanding CoT helps diagnose whether the model is reasoning or pattern-matching.
  - Quick check question: In the LaMP-7 case study, how does the reasoning trace differ from simply copying words from user history examples?

## Architecture Onboarding

- **Component map:**
  User Query → Base Model M_base → Generic Response A_base → Query ⊕ A_base → User Retriever → Top-k User History P_rel → Reflection Model M_reflect → <thinkation> reasoning </thinkation> → Personalized Response A_final

- **Critical path:**
  1. Data preparation: Generate structured rewriting trajectories using teacher model with strict quality filtering
  2. SFT stage: Train reflection model on trajectories (1 epoch, lr=5e-6, top-1 retrieved example)
  3. RL stage: Fine-tune with progressive 2→6 shot curriculum (1 epoch, generate 16 candidates, task-specific rewards)
  4. Inference: Base model → retrieval → reflection model rewriting

- **Design tradeoffs:**
  - Reflection model size vs. capability: Paper uses 7B (Qwen2.5-7B-Instruct) for efficiency. Larger models may improve reasoning but increase latency.
  - Curriculum step size (E_step): Too fast → model doesn't learn from sparse contexts; too slow → overfitting to simple patterns.
  - Number of candidates in RL: 16 candidates used; more candidates improve reward signal but increase compute 16×.
  - Retriever choice: Contriever vs. BGE showed minimal difference (Table 4), but retriever quality is critical—random retrieval nearly halves performance.

- **Failure signatures:**
  - Generic outputs unchanged after reflection → SFT failed to transfer reasoning; check trajectory quality
  - Personalization inconsistent across similar queries → RL curriculum not converged or reward signal noisy
  - Factual errors introduced during rewriting → Base model output too poor; consider stronger base model
  - Style drift with long histories → Curriculum max shots (k_max) too high for this user's history quality

- **First 3 experiments:**
  1. Reproduce SFT-only ablation: Train reflection model on trajectories without RL stage. Compare to Table 3 "RPO w/o RL" to validate data pipeline.
  2. Retriever sensitivity test: Swap Contriever for random retrieval on LaMP-2. Expect ~10% accuracy drop per Table 4. This validates the retrieval dependency.
  3. Base model swap test: Use same trained reflection model with 2 different base models (e.g., GPT-4o-mini vs. DeepSeek-V3) on LaMP-5. Per Table 6, ROUGE-1 should stay within 0.475±0.005 range if framework is truly model-agnostic.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does RPO's performance scale with the size of the reflection module beyond the 7B model tested? Only a 7B reflection model was tested for deployment efficiency; no experiments with larger (e.g., 70B) or smaller models were conducted.

- **Open Question 2:** How robust is RPO when user history is extremely sparse (1-2 examples) or highly noisy? Experiments consistently use 2+ retrieved examples; performance degradation curves for k=0 or k=1 are not reported.

- **Open Question 3:** Is the structured trajectory generation dependent on the specific teacher model (GPT-4.1-mini), or can weaker models produce comparable training data? All SFT trajectories are generated using gpt-4.1-mini-2025-04-14, but no analysis of teacher model sensitivity is provided.

## Limitations

- Framework's dependence on high-quality structured rewriting trajectories is a significant limitation—if the teacher model's reasoning chains are noisy or fail to capture user-specific decision logic, the entire SFT stage may fail to generalize.
- Progressive curriculum's hyperparameters (E_step, k_max) appear sensitive to task and user profile quality, suggesting limited out-of-the-box applicability without tuning.
- Claims of superior performance vs. state-of-the-art are strong but based on a single benchmark (LaMP); generalization to real-world personalization scenarios remains unproven.

## Confidence

- **High Confidence:** The decoupled generate-then-rewrite mechanism is well-validated by ablation studies showing SFT-only performance significantly exceeds zero-shot baselines.
- **Medium Confidence:** The progressive multi-context curriculum shows consistent gains across tasks, but lacks external corpus validation and may overfit to LaMP's specific structure.
- **Medium Confidence:** Claims of superior performance vs. state-of-the-art are strong but based on a single benchmark (LaMP); generalization to real-world personalization scenarios remains unproven.

## Next Checks

1. Test curriculum sensitivity: Run RL with fixed-shot (2, 4, 6) configurations on LaMP-2 to verify that progressive curriculum provides consistent gains across diverse user profiles.
2. Stress-test base model independence: Replace Qwen2.5-7B-Instruct with a significantly smaller reflection model (3B) and larger base model (DeepSeek-V3) to confirm performance stability across the claimed modularity spectrum.
3. Validate trajectory quality impact: Systematically degrade reasoning chain quality in the SFT dataset (e.g., remove explicit reasoning traces) and measure degradation in personalization accuracy to confirm trajectory quality is indeed critical.