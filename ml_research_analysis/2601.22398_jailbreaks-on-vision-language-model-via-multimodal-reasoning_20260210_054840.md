---
ver: rpa2
title: Jailbreaks on Vision Language Model via Multimodal Reasoning
arxiv_id: '2601.22398'
source_url: https://arxiv.org/abs/2601.22398
tags:
- image
- safety
- react
- prompt
- rewriting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates jailbreak attacks on vision-language models
  (VLMs) by combining adaptive prompt rewriting and image noising strategies. It introduces
  a ReAct-driven framework that iteratively refines both prompts and adversarial noise
  based on VLM feedback, using reasoning traces to adaptively bypass safety filters.
---

# Jailbreaks on Vision Language Model via Multimodal Reasoning

## Quick Facts
- arXiv ID: 2601.22398
- Source URL: https://arxiv.org/abs/2601.22398
- Authors: Aarush Noheria; Yuguang Yao
- Reference count: 12
- The study introduces a ReAct-driven framework that achieves up to 52.08% attack success rates on harmful prompts by iteratively refining prompts and adversarial noise based on VLM feedback.

## Executive Summary
This paper presents a novel jailbreak attack framework targeting vision-language models (VLMs) by exploiting the "modality gap" between text and image safety alignment. The approach combines adaptive prompt rewriting using the ReAct paradigm with dynamic image noising, iteratively refining both modalities based on VLM safety feedback. Experiments demonstrate that this dual-strategy method significantly outperforms single-modality baselines, achieving up to 52.08% attack success rates on harmful prompts while maintaining output plausibility. The framework highlights critical vulnerabilities in current VLM safety mechanisms and demonstrates how coordinated multimodal attacks can effectively bypass safeguards.

## Method Summary
The framework employs a ReAct (Reasoning + Acting) loop that iteratively refines both prompts and adversarial image noise based on VLM feedback. The process begins with a harmful query (text + image), which is evaluated by the VLM's safety filters. If blocked, the system classifies the violation into one of eight categories and uses ReAct's Thought-Action-Observation paradigm to either rewrite the prompt into a contextually benign framing or apply category-specific visual transformations (Gaussian blur for violence, DCT filter for nudity, recoloring for weapons). This loop runs for up to five iterations, with each iteration potentially adjusting either modality based on which triggered the refusal. The method uses Gemini-2.0-Flash via Google's Generative AI API with safety filters set to "BLOCK NONE" for controlled experimentation.

## Key Results
- Dual-strategy approach achieves attack success rates of up to 52.08% on harmful prompts in SPA-VL dataset
- ReAct-driven iterative prompt rewriting alone improves bypass rates to 49.81%, significantly outperforming static baselines (13.96% for image-only, 24.29% for static rewriting)
- Category-specific image filtering provides measurable benefits over uniform noising approaches, with improvements shown across all harm categories in VLGuard dataset

## Why This Works (Mechanism)

### Mechanism 1: ReAct-driven Iterative Prompt Rewriting
The framework uses ReAct's Thought-Action-Observation loop to iteratively rewrite harmful prompts, progressively reducing harm signals detected by safety filters. By classifying blocked prompts into violation categories (Criminal, HateSpeech, PrivateHealth, etc.) and decomposing harmful intent into contextually benign framings (e.g., academic research, professional training), the system exploits the assumption that VLM safety filters rely on surface-level semantic patterns rather than deep intent analysis. This allows harmful requests disguised in innocuous contexts to evade detection.

### Mechanism 2: Category-Specific Adaptive Image Filtering
The attack applies targeted visual transformations based on detected harm category in images, using Gaussian blur for violence/hate symbols, DCT filter for skin/nudity, and recoloring for weapons. This approach exploits the assumption that VLM safety classifiers depend on specific visual feature patterns that can be selectively degraded without triggering suspicion from the modified image itself, while preserving overall image coherence.

### Mechanism 3: Cross-Modal Coordination Exploiting Modality Gap
By combining prompt rewriting and image noising in a coordinated ReAct loop, the attack achieves higher success rates than either modality alone. This exploits the "modality gap" where safety alignment learned in text-only settings may not transfer when both modalities interact, allowing each ReAct iteration to adjust either text or image based on which modality triggered refusal, creating coordinated cross-modal perturbations.

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting) Paradigm**
  - Why needed here: The attack framework uses ReAct's Thought-Action-Observation loop to interpret safety feedback and generate refined attacks; understanding this control flow is essential for implementing or modifying the approach.
  - Quick check question: In this framework, what happens during the "Observation" phase after a prompt is rewritten?

- **Concept: Vision-Language Model Safety Alignment**
  - Why needed here: The paper's core thesis depends on understanding how VLMs are safety-trained and why the "modality gap" creates vulnerabilities; this informs both attack design and potential mitigations.
  - Quick check question: Why might a prompt that passes text-only safety filters trigger refusals when paired with certain images?

- **Concept: Frequency-Domain Image Filtering (DCT)**
  - Why needed here: The image noising component uses DCT filtering alongside spatial methods; understanding frequency-domain transformations helps explain why different filters suit different harm categories.
  - Quick check question: Why might a DCT filter preserve more structural information than Gaussian blur while still obscuring specific content?

## Architecture Onboarding

- **Component map:**
  Safety Feedback Simulator -> Prompt Rewriting Module -> Image Analysis Module -> Adaptive Filtering Module -> ReAct Orchestrator -> Evaluation Bench

- **Critical path:**
  Original query (image + text) → VLM safety check → If blocked, classify category → ReAct Thought: select strategy → Action: rewrite prompt OR apply filter → Observation: VLM re-evaluation → Loop or output final response

- **Design tradeoffs:**
  - Iteration count: More iterations increase bypass rate but multiply API costs (paper uses 5)
  - Filter aggressiveness: Stronger filters improve bypass but risk semantic content loss
  - Margin threshold: The 10-point safety/unsafety gap trades false positives for conservative flagging
  - Static fallback vs. adaptive: Static is faster but drops ASR from ~52% to ~24%

- **Failure signatures:**
  - Semantic drift: After multiple rewrites, prompt may request different information than originally intended
  - Filter over-application: Excessive blurring/recoloring makes images unusable
  - Category misclassification: Wrong filter type applied to harm signal
  - Refusal loop exhaustion: All 5 iterations fail, falling back to static template

- **First 3 experiments:**
  1. Replicate single-modality baselines (Table 1: Original, Static Rewriting, Image-only) on a 100-sample subset of VLGuard to validate environment setup against reported ASRs
  2. Ablate iteration count (1 through 5) and plot ASR convergence curve to identify diminishing returns threshold
  3. Test individual filter types (Gaussian blur vs. DCT vs. recoloring) on category-specific subsets to validate category-filter matching assumptions from Figure 6

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the attack success rate change under default or stricter safety filter configurations rather than the "BLOCK NONE" setting used in experiments?
- Basis in paper: [explicit] The authors state "further tests to bypass Gemini's adjustable safety filters on more rigorous block settings should be performed to verify our results."
- Why unresolved: Experiments were conducted with safety filters disabled to ensure all prompts returned answers, leaving real-world deployment conditions untested.
- What evidence would resolve it: Replicate experiments with Gemini's default and "BLOCK MOST" safety settings, reporting ASR across all baselines.

### Open Question 2
- Question: Do the adaptive jailbreak techniques transfer effectively to other state-of-the-art VLMs beyond Gemini-2.0-Flash?
- Basis in paper: [inferred] Only a single closed-source VLM was evaluated, limiting claims about the framework's generalizability across architectures.
- Why unresolved: Different VLMs (e.g., GPT-4V, Claude, LLaVA) may have different safety alignment strategies, visual encoders, or reasoning capabilities that affect vulnerability.
- What evidence would resolve it: Test the same attack framework on at least 2-3 additional VLMs with comparable experimental setup and report comparative ASR.

### Open Question 3
- Question: What defensive mechanisms can effectively detect or mitigate adaptive ReAct-driven multimodal jailbreaks without degrading benign performance?
- Basis in paper: [explicit] The conclusion states "Our findings highlight the need for improved defenses in vision language models when it comes to blocking unsafe outputs."
- Why unresolved: The paper focuses entirely on attack methodology without proposing or testing countermeasures.
- What evidence would resolve it: Develop and benchmark defense strategies (e.g., adversarial training, reasoning trace monitoring, multi-round safety verification) showing reduced ASR while maintaining task performance on benign queries.

### Open Question 4
- Question: Does using the target VLM to score its own outputs introduce systematic bias in safety evaluation?
- Basis in paper: [inferred] The evaluation bench relies on Gemini scoring its own responses via factual and counterfactual safety scores.
- Why unresolved: Model self-assessment may exhibit overconfidence or inconsistency compared to human annotation or independent classifiers.
- What evidence would resolve it: Compare VLM-based safety scores against human expert judgments or external toxicity classifiers on a held-out subset of outputs.

## Limitations

- Single VLM evaluation limits generalizability across different architectures and safety implementations
- "BLOCK NONE" safety settings used for attack generation may not reflect real-world deployment conditions
- Critical assumptions about VLM safety filter mechanisms and region detection accuracy remain unverified

## Confidence

**High Confidence Claims:**
- The dual-strategy framework architecture and evaluation methodology are clearly specified and reproducible
- The reported ASR improvements over single-modality baselines are statistically significant and methodologically sound
- The identification of modality gaps as exploitable vulnerabilities is well-supported by experimental evidence

**Medium Confidence Claims:**
- The ReAct-driven iterative approach consistently improves bypass rates across different harm categories
- Category-specific image filtering provides measurable benefits over uniform noising approaches
- The coordination between text and image attacks creates synergistic effects beyond simple combination

**Low Confidence Claims:**
- The generalizability of attack effectiveness across different VLM architectures and safety configurations
- The persistence of identified vulnerabilities in VLMs with more sophisticated safety training
- The real-world impact of these attacks given that commercial VLMs typically employ multiple defense layers

## Next Checks

1. **Cross-Model Validation**: Test the attack framework against at least three additional VLM architectures (e.g., GPT-4V, Claude-3-Vision, LLaVA) to assess whether the reported ASR improvements generalize beyond Gemini-2.0-Flash. This addresses the model-specificity concern and validates the claimed modality gap exploitation.

2. **Defense Robustness Testing**: Implement and evaluate three common VLM defense mechanisms (input sanitization, adversarial training, and multimodal safety alignment) against the dual-strategy attacks to measure actual bypass rates under realistic security conditions. This quantifies how much the "BLOCK NONE" experimental setup inflates success rates.

3. **Human Evaluation of Semantic Drift**: Conduct a blinded human study where participants assess whether iteratively rewritten prompts maintain semantic equivalence to the original harmful requests. This directly validates the critical assumption that prompts remain "semantically aligned" while evading safety filters, addressing concerns about semantic drift during iterative rewriting.