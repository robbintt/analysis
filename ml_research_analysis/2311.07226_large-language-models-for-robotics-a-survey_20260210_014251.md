---
ver: rpa2
title: 'Large Language Models for Robotics: A Survey'
arxiv_id: '2311.07226'
source_url: https://arxiv.org/abs/2311.07226
tags:
- language
- robot
- robotics
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of Large Language
  Models (LLMs) in robotics, covering their applications, challenges, and future directions.
  LLMs have significantly enhanced robot capabilities in perception, decision-making,
  control, and interaction by leveraging their natural language understanding and
  reasoning abilities.
---

# Large Language Models for Robotics: A Survey

## Quick Facts
- arXiv ID: 2311.07226
- Source URL: https://arxiv.org/abs/2311.07226
- Authors: Fanlong Zeng; Wensheng Gan; Zezheng Huai; Lichao Sun; Hechang Chen; Yongheng Wang; Ning Liu; Philip S. Yu
- Reference count: 40
- Key outcome: Comprehensive survey of LLM applications in robotics, covering perception, decision-making, control, and interaction with emerging challenges and future directions

## Executive Summary
This survey provides a comprehensive overview of Large Language Models (LLMs) in robotics, covering their applications, challenges, and future directions. LLMs have significantly enhanced robot capabilities in perception, decision-making, control, and interaction by leveraging their natural language understanding and reasoning abilities. The integration of LLMs with robotics has led to advancements such as vision-language-action models (VLAs), multi-agent systems, and embodied intelligence. These models enable robots to perform complex tasks, interact with humans, and adapt to dynamic environments. However, challenges such as dataset scarcity, training transfer, security, and ethical considerations remain. The survey highlights the potential of LLMs to revolutionize robotics while addressing critical issues for future development.

## Method Summary
The survey systematically categorizes LLM applications in robotics across four main domains: perception, decision-making, control, and interaction. It analyzes architectural patterns including vision-language-action alignment, hierarchical task planning, and multi-robot collaboration frameworks. The authors synthesize findings from 40+ references to identify key mechanisms, challenges, and emerging research directions. The methodology emphasizes conceptual frameworks over empirical validation, providing a roadmap for understanding how LLMs enhance robotic capabilities through semantic grounding and reasoning.

## Key Results
- LLMs enable hierarchical task planning by decomposing abstract natural language instructions into executable sub-goals
- Vision-Language-Action (VLA) models align visual perception with language understanding to generate continuous action sequences
- Multi-robot systems leverage LLMs for collaborative task assignment and conflict resolution through natural language communication

## Why This Works (Mechanism)

### Mechanism 1: Semantic Grounding for Hierarchical Task Planning
- **Claim:** LLMs function as high-level planners by decomposing abstract natural language instructions into actionable sub-goals, relying on their pre-trained semantic knowledge rather than hard-coded rules.
- **Mechanism:** The LLM acts as a "brain" that parses a high-level command (e.g., "clean the kitchen") into a sequence of logical steps. It utilizes **Chain-of-Thought (CoT)** reasoning to bridge the gap between abstract intent and executable logic. The process maps linguistic concepts to world knowledge to generate a "Standardized Action Procedure" (SAP) or sub-goal list, which is then passed to lower-level controllers.
- **Core assumption:** The assumption is that the linguistic associations learned from massive text corpora (web data) map effectively to physical world logic and object affordances, and that the model can maintain **dialogue consistency** over long horizons without catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] Mentions LLMs facilitate "efficient interaction and collaboration" and "decision-making... planning."
  - [section 4.2.1] States LLMs can "parse high-level abstract instructions to perform complex tasks without requiring step-by-step guidance" and "break down problems into smaller, manageable steps."
  - [corpus] The neighbor paper *"Agentic Reasoning for Large Language Models"* supports the underlying reasoning shift, but specific robotics grounding mechanisms are not detailed in the provided snippet.
- **Break condition:** This mechanism fails when instructions are highly ambiguous or require physical nuance (e.g., force profiles) that text-only pre-training does not capture.

### Mechanism 2: Vision-Language-Action (VLA) Alignment
- **Claim:** VLA models enable direct robotic control by aligning visual perception and language understanding into a shared latent space that maps to continuous action sequences.
- **Mechanism:** This approach integrates a **Visual Encoder** (e.g., DINOv2) with a **Language Model**. Visual features are projected into the linguistic embedding space (often via an MLP). The model outputs "action tokens" or flow-matching trajectories rather than text. By training on large-scale robot datasets (like Open X-Embodiment), the model learns to condition these action outputs on the semantic context of the visual and text inputs.
- **Core assumption:** The assumption is that a unified Transformer architecture can simultaneously master visual feature extraction, language semantics, and the temporal dynamics of motion planning (the "Scaling Law" applied to robotics).
- **Evidence anchors:**
  - [section 4.3.2] Describes how models like OpenVLA and $\pi_0$ use a "standard Vision-Language Model... to process visual and linguistic inputs" while an "Action Expert Module" handles robot states.
  - [section 4.1.1] Notes VLMs process "images and text at the same time" to resolve complex queries.
  - [corpus] Weak or missing support in provided corpus neighbors regarding specific VLA alignment architectures.
- **Break condition:** The mechanism breaks if the visual encoder fails to capture fine-grained spatial relationships necessary for manipulation, or if the "action tokens" do not adequately represent the high-frequency control required (latency issues).

### Mechanism 3: Multi-Robot Collaboration via LLM Communication
- **Claim:** LLMs enable heterogeneous multi-robot systems (MAS) to collaborate by acting as a centralized "Task Assigner" that uses natural language as a shared communication protocol to resolve conflicts and decompose tasks.
- **Mechanism:** A centralized LLM receives a high-level goal and generates a proposal, decomposing it into subtasks for specific robot types (e.g., quadrotors vs. robotic dogs). It employs a **Proposal-Execution-Feedback-Adjustment (PEFA)** loop. If an executor fails, it reports "self-reflection feedback" back to the LLM assigner, which adjusts the plan dynamically. This overcomes the difficulty of hand-crafting coordination policies for diverse hardware.
- **Core assumption:** It assumes the LLM has a sufficient context window to track the state of multiple agents and that "hallucinations" can be filtered out by the feedback loop before they cause physical collisions or deadlocks.
- **Evidence anchors:**
  - [section 3.1.1] Discusses "COHERENT," a framework where a "centralized task assigner... decomposes the complex task into subtasks" using a feedback mechanism.
  - [section 4.2.2] Notes LLMs can "enhance mathematical and strategic reasoning abilities through multi-agent debate."
  - [corpus] The neighbor paper *"Large Language Models for Multi-Robot Systems: A Survey"* validates the relevance of this specific sub-field.
- **Break condition:** This fails if the communication latency between agents and the central LLM is too high for real-time coordination, or if the LLM assigns a task to a robot lacking the physical capability (embodiment mismatch) without effective verification.

## Foundational Learning

- **Concept: Transformers and Context Embeddings**
  - **Why needed here:** LLMs in robotics are almost exclusively Transformer-based. To understand how a robot "remembers" a previous instruction or integrates a camera feed, one must understand how attention mechanisms weigh different parts of the input context (text + image patches) to predict the next action token.
  - **Quick check question:** Can you explain how "Self-Attention" allows a model to decide that the word "apple" in an instruction refers to the red object in the image patch, rather than the banana?

- **Concept: Zero-Shot vs. Few-Shot Generalization**
  - **Why needed here:** The survey emphasizes that LLMs allow robots to generalize to new tasks without retraining (Zero-Shot) or with minimal demos (Few-Shot). This is the primary value proposition over traditional Reinforcement Learning (RL), which often requires millions of iterations for a single task.
  - **Quick check question:** How does providing a few "demonstrations" in the context window (Few-Shot) alter the behavior of the LLM compared to a standard prompting approach?

- **Concept: Sim-to-Real Transfer**
  - **Why needed here:** Many foundational VLA models (like $\pi_0$) are pre-trained on simulation or web data. Understanding the "Sim-to-Real" gap—the difference in physics, lighting, and sensor noise between a simulator and the physical world—is critical for debugging why a robot might fail upon deployment.
  - **Quick check question:** Why might a robot trained in a perfect simulation fail to pick up a cup in a real room with shadows?

## Architecture Onboarding

- **Component map:** Visual Encoder -> Language Model -> Action Expert -> Actuator Control
- **Critical path:** Input Text/Image -> **VLM Alignment** (Projecting vision to language space) -> **LLM Inference** (Planning/Reasoning) -> **Action Token Generation** -> **Actuator Control**
- **Design tradeoffs:**
  - **Cloud vs. Edge:** Cloud-based "Super Brains" (Section 7.4) offer massive reasoning power but introduce latency and connectivity risks. Local "brains" are faster but less capable.
  - **Generalist vs. Specialist:** Training a single "Generalist" policy (like Gato/RT-2) is data-hungry and computationally expensive, whereas specialized policies are brittle to new environments.
  - **Reasoning Depth vs. Latency:** "Thinking" models (CoT/Slow thinking) provide better error recovery but are too slow for high-frequency control loops.
- **Failure signatures:**
  - **Hallucination:** The robot attempts to interact with an object that does not exist or is not visible in the scene.
  - **Catastrophic Forgetting:** In long-horizon tasks, the robot loses track of the initial goal due to context window limits (Section 7.6).
  - **Grounding Error:** The robot correctly identifies "cup" but fails to compute the correct grasp point (spatial reasoning failure).
- **First 3 experiments:**
  1. **Visual Grounding Test:** Prompt a pre-trained VLM (e.g., CLIP or LLaVA) to identify the bounding box of specific objects in a cluttered image. This validates the Perception module's alignment before adding motion.
  2. **Scripted Planning Check:** Give a text-only LLM (e.g., GPT-3.5/4) a high-level household task and ask it to output a JSON plan of sub-goals. Manually check for logical errors or impossible steps to evaluate the Decision module.
  3. **Low-Level Control Imitation:** Collect a small dataset (demonstrations) of a simple task (e.g., pushing a block) and train a small policy (Behavior Cloning) to connect the VLM outputs to actuator movements. This tests the Control module in isolation.

## Open Questions the Paper Calls Out
- How can LLMs maintain consistent task execution over long-horizon, multi-step robotic tasks without catastrophic forgetting?
- What are the fundamental limitations of using pre-trained language models for physical reasoning and object manipulation in diverse environments?
- How can security and safety concerns be addressed when granting autonomous decision-making capabilities to LLMs with known hallucination tendencies?
- What are the optimal strategies for balancing computational resources between cloud-based "super brains" and edge computing for real-time robotic applications?

## Limitations
- Analysis relies on reported capabilities from primary sources without independent experimental validation, introducing potential optimism bias
- Many cited mechanisms lack rigorous quantitative benchmarks in real-world robotics settings, particularly for multi-agent coordination
- The survey focuses on conceptual frameworks rather than empirical performance comparisons across different robotic platforms and tasks

## Confidence

**High confidence**: Basic VLA alignment mechanisms and perception integration (supported by multiple implementations like OpenVLA, RT-2)
**Medium confidence**: High-level planning decomposition and task abstraction (mechanism plausible but limited real-world validation)
**Low confidence**: Multi-robot collaboration and emergent reasoning claims (few published implementations, theoretical assumptions dominant)

## Next Checks

1. **Grounding robustness test**: Evaluate a VLA model's object recognition and manipulation success rate across 50+ physically diverse household environments with varying lighting, occlusion, and object configurations

2. **Long-horizon consistency validation**: Deploy an LLM planner on a 10+ step household task, measuring task completion rate and error accumulation over time compared to traditional hierarchical planners

3. **Cross-embodiment transfer assessment**: Test a single VLA policy (trained on one robot morphology) across three different robot platforms, quantifying performance degradation and required fine-tuning parameters