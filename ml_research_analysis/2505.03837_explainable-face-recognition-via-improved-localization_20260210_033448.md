---
ver: rpa2
title: Explainable Face Recognition via Improved Localization
arxiv_id: '2505.03837'
source_url: https://arxiv.org/abs/2505.03837
tags:
- class
- face
- image
- recognition
- regions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of transparency in deep learning-based
  face recognition systems by proposing a method for explainable face recognition
  using the Scaled Directed Divergence (SDD) technique. The SDD technique performs
  fine localization of class-specific features in scenarios with overlapping classes,
  which is common in face recognition.
---

# Explainable Face Recognition via Improved Localization

## Quick Facts
- arXiv ID: 2505.03837
- Source URL: https://arxiv.org/abs/2505.03837
- Reference count: 21
- Primary result: SDD-CAM outperforms random CAM in deletion evaluation (62.33% vs 42.50% confidence drop)

## Executive Summary
This paper addresses the lack of transparency in deep learning-based face recognition by proposing Scaled Directed Divergence (SDD), a method for generating explainable class activation maps. The approach fine-tunes a pre-trained AdaFace model on FaceScrub dataset and applies SDD to highlight discriminative facial features specific to each predicted class. The method outperforms random baselines in deletion-and-retention evaluation, demonstrating that SDD successfully localizes class-specific features in overlapping class scenarios.

## Method Summary
The method fine-tunes a pre-trained AdaFace (ResNet100) model on the FaceScrub dataset with 530 classes, achieving 96.33% test accuracy. For explainability, SDD computes class activation maps by scaling the target class CAM, subtracting competing class CAMs, and applying exponential amplification. The deletion-and-retention evaluation scheme validates explanation quality by measuring prediction changes when masking the most salient regions identified by SDD.

## Key Results
- SDD-CAM achieves 62.33% average confidence drop vs 42.50% for random CAM in deletion evaluation
- SDD-CAM shows 51.36% prediction change percentage vs 29.73% for random CAM in deletion evaluation
- Fine-tuned AdaFace model achieves 96.33% test accuracy on FaceScrub dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaled Directed Divergence (SDD) narrows explanation maps by explicitly comparing target-class features against competing classes.
- Mechanism: For a target class t, SDD computes CAM_SDD^t = exp(α(x_t·F - Σ x_k)) where x_t is the target CAM, x_k are competing class CAMs, F is a scaling factor balancing magnitude differences, and α controls amplification. This subtraction suppresses regions common across classes, leaving only discriminative features.
- Core assumption: The method assumes that class-relevant regions unique to the target class will survive subtraction, while shared regions will cancel out.
- Evidence anchors:
  - [abstract] "SDD method performs fine localization of the face features relevant to the deep learning model for its prediction, highlighting class-specific discriminative regions in a very narrow/specific manner compared to traditional CAM."
  - [section 3.2] Formula (3) defines SDD CAM computation with scaling and exponentiation steps.
  - [corpus] Weak direct evidence; corpus papers address biometric security and explainability broadly but not SDD specifically.
- Break condition: If target and competing classes share nearly identical discriminative features (extreme class overlap), subtraction may suppress all relevant regions, producing empty or misleading maps.

### Mechanism 2
- Claim: Deletion-based evaluation validates explanation quality by measuring prediction sensitivity to removed regions.
- Mechanism: The top 20% of SDD CAM values are masked (set to 0), and model confidence drop is measured. Larger drops indicate identified regions are truly influential. SDD CAMs cause 62.33% average confidence drop vs 42.50% for random regions.
- Core assumption: Regions highlighted by a good explanation should be causally necessary for the model's prediction.
- Evidence anchors:
  - [abstract] "SDD CAM outperforms random CAM in terms of average confidence drop (62.33% vs 42.50%) and prediction change percentage (51.36% vs 29.73%)."
  - [section 5.1] Table 1 shows deletion metrics across 4,494 test images.
  - [corpus] No corpus papers directly validate deletion metrics for face recognition explainability.
- Break condition: If the model uses distributed representations with no localized critical regions, deletion may cause gradual rather than sharp confidence drops regardless of explanation quality.

### Mechanism 3
- Claim: Fine-tuning pre-trained face recognition models preserves discriminative feature learning while enabling class-specific CAM generation.
- Mechanism: AdaFace (ResNet100, pre-trained on WebFace12M) is fine-tuned on FaceScrub (530 classes) with AdamW optimizer, achieving 96.33% test accuracy. The final convolutional layer's feature maps, combined with output layer weights, generate CAMs.
- Core assumption: Pre-trained face embeddings transfer effectively to classification tasks, and gradient/weight flow to final conv layer remains interpretable after fine-tuning.
- Evidence anchors:
  - [section 4.1] "The overall test accuracy of the model is 96.33%" with training details specified.
  - [section 3.1] Equation (2) shows how output layer weights w_k^c combine with feature maps f_k(x,y) to form CAM.
  - [corpus] Corpus paper "Deep CNN Face Matchers Inherently Support Revocable Biometric Templates" supports CNN face matcher effectiveness but not explainability mechanisms.
- Break condition: If fine-tuning collapses feature diversity or creates adversarial shortcuts, CAM-based explanations may highlight spurious correlations rather than semantic features.

## Foundational Learning

- Concept: Class Activation Mapping (CAM)
  - Why needed here: SDD is built on CAM; understanding how CAM computes spatial importance via weighted feature map summation is prerequisite.
  - Quick check question: Given feature maps f_k(x,y) and class weights w_k^c, write the CAM formula for class c.

- Concept: Global Average Pooling (GAP)
  - Why needed here: GAP aggregates spatial activations before the output layer, enabling weight backprojection for CAM generation.
  - Quick check question: Why does GAP (vs. max pooling) preserve spatial importance information for CAM?

- Concept: Deletion-Retention Evaluation
  - Why needed here: Critical for validating that explanation maps identify causally relevant regions, not just correlated features.
  - Quick check question: In deletion evaluation, why should a good explanation cause larger confidence drops than random masking?

## Architecture Onboarding

- Component map: ResNet100 backbone -> Global Average Pooling -> Fully Connected layer -> Softmax -> Output layer weights
- Critical path:
  1. Forward pass through ResNet100 to final conv layer (feature maps)
  2. GAP → FC layer → softmax (class scores)
  3. Extract w_k^c for target class and top-N competing classes
  4. Compute CAMs for all selected classes
  5. Apply SDD: scale target, subtract others, amplify, exponentiate
  6. Upsample SDD CAM to input resolution, overlay on image

- Design tradeoffs:
  - **Scaling factor F**: Controls target CAM magnitude relative to others. F ∈ [|S|, 50]; too low causes underflow, too high over-amplifies noise. Paper uses adaptive F based on mean absolute values.
  - **Amplification parameter α**: α ∈ [0.2, 0.4] adaptively; higher α sharpens differences but may overfit to spurious features.
  - **Number of competing classes**: Paper uses top-5; more classes increase computation and may dilute discriminative signal.

- Failure signatures:
  - **Empty SDD CAM**: All values near zero → likely extreme class overlap or incorrect F scaling
  - **Diffuse SDD CAM (similar to CAM)**: Subtraction not removing shared regions → check competing class selection or increase α
  - **High retention confidence drop**: SDD regions not more informative than random → model may use non-localized features

- First 3 experiments:
  1. **Sanity check**: Generate CAM and SDD CAM for a correctly classified test image; visually verify SDD is narrower. Measure confidence drop when masking top-20% SDD regions vs. random regions.
  2. **Ablation on competing classes**: Vary number of competing classes (3, 5, 10) and measure deletion metric changes. Identify point of diminishing returns.
  3. **Cross-dataset validation**: Test SDD on a held-out face dataset (e.g., LFW) with the fine-tuned model to assess generalization of explanation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the visual explanations generated by Scaled Directed Divergence (SDD) be effectively utilized to retrain face recognition models to mitigate learned biases or improve classification accuracy?
- **Basis in paper:** [explicit] The conclusion states that the method "can be used to improve the performance of the models by indicating the flaws and biases," suggesting this as a future application of the technique.
- **Why unresolved:** The current work focuses solely on generating and evaluating the explanations themselves (transparency), but does not conduct experiments to validate if these explanations can close the loop to refine the model's learning process.
- **What evidence would resolve it:** Experimental results showing improved test accuracy or reduced demographic bias in a model that has been fine-tuned using a loss function weighted by the SDD feature maps.

### Open Question 2
- **Question:** How does the SDD CAM quantitative performance compare against other state-of-the-art explainability methods (e.g., Grad-CAM, LIME) using the deletion-and-retention evaluation schemes?
- **Basis in paper:** [inferred] The evaluation section limits its quantitative comparison to "Random CAM," while qualitatively reviewing methods like Grad-CAM and LIME in the Related Work section without benchmarking them against SDD in the results.
- **Why unresolved:** Establishing that SDD outperforms "random" baselines does not confirm it outperforms established gradient-based or perturbation-based explanation methods in preserving model confidence.
- **What evidence would resolve it:** A comparative table showing the Average Confidence Drop and Prediction Change Percentage for Grad-CAM, LIME, and SDD on the same FaceScrub test set.

### Open Question 3
- **Question:** Is the SDD technique effective for open-set face verification tasks where a fixed set of class weights (necessary for divergence calculation) is not available in the final layer?
- **Basis in paper:** [inferred] The methodology relies on a multiclass classification setup (FaceScrub) with a fixed number of classes (530) to compute divergence against top predicted classes; this setup differs from standard face verification which often operates on embeddings rather than class probabilities.
- **Why unresolved:** The current method depends on subtracting the CAMs of the "top 5 classes" to highlight discriminative features, a mechanism that may not function in verification systems that compare feature vectors rather than classify against a gallery.
- **What evidence would resolve it:** Successful application and evaluation of SDD on a pair-based verification task (e.g., LFW) without relying on fixed output layer class weights.

## Limitations
- SDD mechanism may fail with extreme class overlap where facial features are nearly identical
- Adaptive parameter α calculation has circular dependency that could introduce instability
- Deletion evaluation method lacks clarity on how "removing" pixels affects model predictions

## Confidence

- **High Confidence:** The foundational mechanism of CAM generation from final convolutional layer feature maps combined with output weights is well-established and correctly implemented.
- **Medium Confidence:** The SDD subtraction mechanism is theoretically sound but depends critically on proper scaling (F) and amplification (α) parameters, which have adaptive but potentially unstable calculations.
- **Low Confidence:** The deletion evaluation results showing 62.33% vs 42.50% confidence drops are compelling but the implementation details for pixel removal are insufficiently specified to verify the methodology.

## Next Checks

1. Test SDD CAM performance when applied to face pairs with intentionally high visual similarity to quantify the method's robustness to class overlap scenarios.
2. Implement a controlled ablation study varying the number of competing classes (3, 5, 10) while measuring deletion metrics to identify optimal trade-offs between computational cost and explanation quality.
3. Apply SDD CAMs to an independent face recognition dataset (not used in training) to assess whether the explanation quality generalizes beyond the specific FaceScrub dataset characteristics.