---
ver: rpa2
title: Spoken Language Understanding on Unseen Tasks With In-Context Learning
arxiv_id: '2505.07731'
source_url: https://arxiv.org/abs/2505.07731
tags:
- fine-tuning
- tasks
- arxiv
- language
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling spoken language
  understanding (SLU) tasks on unseen domains without task-specific annotations. Traditional
  SLU models require supervised data for each task, while speech-text large language
  models (LLMs) show limited zero-shot performance on SLU tasks.
---

# Spoken Language Understanding on Unseen Tasks With In-Context Learning

## Quick Facts
- **arXiv ID:** 2505.07731
- **Source URL:** https://arxiv.org/abs/2505.07731
- **Reference count:** 0
- **Primary result:** Randomized label fine-tuning + text ICL enables zero/few-shot SLU on unseen tasks, with 10.1%-95.3% relative gains over regular fine-tuning.

## Executive Summary
This paper tackles the challenge of enabling spoken language understanding (SLU) on unseen tasks without task-specific annotations. Traditional SLU models require supervised data for each task, while speech-text LLMs show limited zero-shot performance. The authors propose randomized label fine-tuning, where class definitions are randomly permuted across mini-batches during fine-tuning, forcing the model to focus on following instructions rather than memorizing semantic class meanings. Combined with in-context learning using textual demonstrations, this approach significantly improves zero-shot and few-shot transfer to unseen SLU tasks across three benchmark datasets.

## Method Summary
The approach uses SALMONN, a speech-text LLM, fine-tuned with LoRA adapters. The key innovation is random label fine-tuning: during fine-tuning on a source task, class definitions are randomly permuted across mini-batches, preventing the model from memorizing semantic associations. This is combined with in-context learning using textual demonstrations (transcriptions of speech queries). The model is evaluated on three SLU tasks: sentiment analysis (SLUE-VoxCeleb), dialogue act classification (SLUE-HVB), and named entity recognition (SLUE-VoxPopuli), comparing regular, symbol-based, and randomized label fine-tuning across matched and mismatched settings.

## Key Results
- Randomized label fine-tuning achieves 10.1% relative improvement on SLUE-VoxCeleb, 95.3% on SLUE-HVB, and 64.3% on SLUE-VoxPopuli over regular fine-tuning in mismatched settings.
- Zero-shot performance on unseen tasks improves significantly with mismatched fine-tuning, even when the fine-tuning task differs from the evaluation task.
- Few-shot in-context learning with textual demonstrations consistently improves performance across all conditions, with gains of 10-40+ points in many mismatched scenarios.

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Following Decoupling via Label Randomization
By randomly permuting class definitions across mini-batches during fine-tuning, the model cannot rely on memorized label-to-semantics mappings from pre-training. This forces the model to learn instruction-following as a general skill—reading class definitions at inference time and mapping inputs accordingly—rather than learning task-specific shortcuts. Evidence shows 95.3% improvement on SLUE-HVB and 64.3% on SLUE-VoxPopuli in mismatched settings.

### Mechanism 2: Text Demonstrations as Cross-Modal ICL Scaffolds
Providing few-shot demonstrations in text form (transcripts) rather than audio improves in-context learning for speech queries. Text demonstrations offer unambiguous semantic content while the model processes the speech query through its multimodal pathway. The model learns to align the spoken input with the semantic pattern established by text examples. Table 1 shows consistent few-shot performance improvements across all conditions.

### Mechanism 3: Mismatched Fine-tuning Unlocks Latent Emergent Abilities
Fine-tuning on a different task than the evaluation task—when combined with randomized labels—activates transferable SLU capabilities that exist but are inaccessible in the base model. The fine-tuning process teaches a meta-skill: "read task instructions, parse class definitions, map input to appropriate output." This meta-skill applies to any SLU task with similar instruction-following structure. Results show significant improvements even when fine-tuning and evaluation tasks differ completely.

## Foundational Learning

- **In-Context Learning (ICL):** The entire approach relies on the model's ability to condition on demonstration examples and instructions without gradient updates at inference time. Quick check: Can you explain why ICL differs from fine-tuning, and what the "demonstrations" contribute to the model's prediction process?

- **Speech-Text Multimodal LLMs:** Understanding how audio encoders, adapters (Q-former), and text LLMs interact clarifies why text demonstrations can guide speech queries. Quick check: Sketch the data flow: speech input → encoder → adapter → LLM. Where does the text instruction enter?

- **LoRA (Low-Rank Adaptation):** The paper fine-tunes with LoRA (r=8, α=32); understanding parameter-efficient tuning helps replicate the setup without full model fine-tuning costs. Quick check: What parameters does LoRA add, and what does "freezing the base model" mean in this context?

## Architecture Onboarding

- **Component map:** Speech encoder -> Q-former adapter -> LLaMA LLM backbone -> Text output
- **Critical path:** 1) Load SALMONN-13B with pre-trained weights. 2) Initialize LoRA adapters (r=8, α=32, dropout=0.1) on LLM attention layers. 3) Prepare fine-tuning data with randomized label definitions per mini-batch. 4) Fine-tune speech encoder + Q-former + LoRA for 15 epochs, batch size 1. 5) At inference, use task-appropriate class definitions and retrieve text demonstrations via embedding similarity.
- **Design tradeoffs:** Randomized labels improve cross-task transfer (10-95% relative gains) but slightly underperforms on matched tasks (~10-15% lower). Text demonstrations are practical but assume ASR quality. Batch size = 1 due to context length constraints.
- **Failure signatures:** No improvement over zero-shot likely due to insufficient fine-tuning epochs, incorrect label randomization, or poor demonstration matching. Matched task performance drops if randomized labels are used at evaluation time. Erratic few-shot scaling indicates demonstration retrieval quality issues.
- **First 3 experiments:** 1) Baseline check: Evaluate base SALMONN on all three SLUE tasks with zero-shot and few-shot without fine-tuning. 2) Regular vs. symbol vs. randomized fine-tuning on matched task: Fine-tune on VoxCeleb with each strategy, evaluate on VoxCeleb validation. 3) Cross-task transfer test: Fine-tune on VoxCeleb with randomized labels, evaluate zero-shot and few-shot on SLUE-HVB.

## Open Questions the Paper Calls Out

- **Open Question 1:** What theoretical mechanisms explain why randomized label fine-tuning improves cross-task generalization compared to regular or symbol-based fine-tuning? The paper empirically demonstrates effectiveness but offers only intuition without formal analysis.
- **Open Question 2:** Can the randomized label fine-tuning approach generalize effectively to speech-text LLMs beyond SALMONN, including smaller or larger architectures? All experiments use only SALMONN-13B, and findings may not transfer across architectures.
- **Open Question 3:** Does using speech demonstrations (instead of text-only demonstrations) improve or degrade ICL performance for SLU tasks in this framework? The paper uses only textual demonstrations, leaving the speech vs. text trade-off unexplored.
- **Open Question 4:** How does randomized label fine-tuning extend to SLU generation tasks (e.g., summarization, translation) rather than classification tasks? All evaluated tasks are classification-based, and randomization may not directly translate to open-ended generation.

## Limitations

- The improvement could stem from either randomized labels or mismatched fine-tuning independently, lacking ablation studies isolating their contributions.
- The approach assumes task instructions and class definitions are available and meaningful at inference time, a constraint not all SLU tasks satisfy.
- Text-based demonstration approach relies on high-quality ASR transcripts, which may not hold in all domains.

## Confidence

- **High confidence:** Empirical improvements on SLUE tasks are reproducible given SALMONN model and specified hyperparameters. Matched-task performance trends are clearly demonstrated.
- **Medium confidence:** Claim that randomized labels force instruction-following rather than memorization is mechanistically sound but not definitively proven through ablation or controlled experiments.
- **Medium confidence:** Assertion that mismatched fine-tuning unlocks emergent abilities is supported by results but lacks theoretical grounding or broader validation.

## Next Checks

1. **Ablation study:** Run fine-tuning with mismatched tasks but without label randomization to quantify its independent contribution to cross-task transfer performance.
2. **Prompt dependency test:** Evaluate performance when class definitions are removed or obfuscated from the prompt to test the instruction-following hypothesis.
3. **Domain robustness check:** Apply the approach to SLU tasks with limited or noisy ASR transcripts to assess the practical limits of text-based demonstrations.