---
ver: rpa2
title: Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue
  Detection
arxiv_id: '2505.23627'
source_url: https://arxiv.org/abs/2505.23627
tags:
- speech
- reading
- miscue
- text
- verbatim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a method to improve reading error annotation
  by extending Whisper ASR with two key modifications: prompting with the target reading
  text and adding special tokens for miscue detection (substitutions, omissions, insertions).
  The approach aims to directly predict both verbatim transcriptions and miscue events,
  addressing limitations of post-hoc ASR-based methods.'
---

# Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection

## Quick Facts
- arXiv ID: 2505.23627
- Source URL: https://arxiv.org/abs/2505.23627
- Reference count: 0
- This paper introduces a method to improve reading error annotation by extending Whisper ASR with two key modifications: prompting with the target reading text and adding special tokens for miscue detection (substitutions, omissions, insertions).

## Executive Summary
This work presents an approach to improve reading error annotation by extending Whisper ASR with two key modifications: prompting with the target reading text and adding special tokens for miscue detection (substitutions, omissions, insertions). The method aims to directly predict both verbatim transcriptions and miscue events, addressing limitations of post-hoc ASR-based methods. Experiments on children's and atypical adult speech show that prompting significantly improves WER over fine-tuning alone, and that end-to-end miscue detection is feasible. The best results achieved WER < 15% and F1 scores for miscue detection ranging from 0.33 to 0.67, outperforming prior work on children's speech.

## Method Summary
The authors modified Whisper ASR by incorporating two key changes: (1) prompting the model with the target reading text to guide transcription, and (2) introducing special tokens to enable end-to-end miscue detection (identifying substitutions, omissions, and insertions). The approach aims to improve both verbatim transcription accuracy and direct prediction of reading errors, moving beyond post-hoc analysis of ASR outputs. Experiments were conducted on speech corpora from children and adults with speech disorders, comparing fine-tuning alone to fine-tuning with prompting, and evaluating both transcription quality (WER) and miscue detection performance (F1 scores).

## Key Results
- Prompting significantly improved WER over fine-tuning alone, achieving WER < 15%.
- End-to-end miscue detection was feasible, with F1 scores ranging from 0.33 to 0.67.
- The approach outperformed prior work on children's speech for both transcription and error detection.

## Why This Works (Mechanism)
The method works by providing Whisper with explicit context (the target reading text) and task-specific signals (special tokens), which helps the model focus on verbatim transcription and recognize speech errors as distinct events rather than treating them as standard ASR errors. This reduces the typical mismatch between spoken and reference text, allowing the model to more accurately transcribe and detect miscues.

## Foundational Learning
- **Automatic Speech Recognition (ASR)**: Converting spoken language into text. *Why needed:* Core task for analyzing reading errors. *Quick check:* Model must produce readable transcripts from speech input.
- **Word Error Rate (WER)**: Standard metric for ASR accuracy, counting substitutions, deletions, and insertions. *Why needed:* Evaluates transcription quality. *Quick check:* Lower WER indicates better verbatim transcription.
- **Miscue Detection**: Identifying reading errors (substitutions, omissions, insertions) during oral reading. *Why needed:* Direct prediction of reading errors is the main goal. *Quick check:* Model should tag each word as correct or error type.
- **Prompting**: Providing the model with the target text to guide transcription. *Why needed:* Reduces mismatch between spoken and reference text. *Quick check:* Prompted model should have lower WER than unprompted.
- **Fine-tuning**: Adapting a pre-trained model to a specific task using labeled data. *Why needed:* Enables ASR model to handle domain-specific speech. *Quick check:* Fine-tuned model should outperform baseline on domain data.
- **Special Tokens**: Adding task-specific tokens to the model vocabulary for error types. *Why needed:* Allows end-to-end miscue detection. *Quick check:* Model should output special tokens for miscues.

## Architecture Onboarding

### Component Map
Whisper ASR model -> Prompting with target text -> Special token tagging for miscue detection -> Output: verbatim transcript + error annotations

### Critical Path
Speech input -> Whisper encoder -> Prompt-guided decoding with special tokens -> Token-level output (words + error tags)

### Design Tradeoffs
- Prompting improves transcription but may introduce bias toward reference text.
- Special tokens enable end-to-end error detection but require careful integration to avoid disrupting standard ASR.
- Fine-tuning is necessary for domain adaptation but risks overfitting on small datasets.

### Failure Signatures
- High WER indicates poor verbatim transcription despite prompting.
- Low F1 for insertions/omissions suggests model struggles with these error types.
- Over-reliance on prompts may reduce robustness to spontaneous speech.

### First Experiments
1. Compare WER with and without prompting on held-out speech data.
2. Evaluate F1 scores for each miscue type (substitutions, omissions, insertions) separately.
3. Test model robustness on spontaneous or disfluent speech excerpts.

## Open Questions the Paper Calls Out
Major uncertainties include whether the observed improvements in WER and miscue detection F1 scores generalize to broader age ranges or speech pathologies beyond the tested children and adults with aphasia. The performance on speech errors, particularly insertions and omissions, remains lower than substitutions, indicating the model may not fully capture all error types. Additionally, the impact of prompting on larger or more diverse datasets is unclear, as is the model's robustness to varying reading speeds and disfluencies.

## Limitations
- Improvements may not generalize to broader age ranges or speech pathologies.
- Lower performance on insertions and omissions compared to substitutions.
- Unclear impact of prompting on larger or more diverse datasets.

## Confidence
- **High** for the improvement in verbatim transcription accuracy when using prompts, as evidenced by consistent WER reductions across experiments.
- **Medium** for the feasibility of end-to-end miscue detection, given the mixed F1 scores and lower performance on insertions and omissions.
- **Medium** for the comparative advantage over post-hoc methods, since the analysis is limited to a specific set of speech corpora.

## Next Checks
1. Test the prompting approach on a more diverse dataset including additional age groups and speech disorders to assess generalization.
2. Evaluate model performance on spontaneous or less structured reading tasks to determine robustness to disfluencies and reading speed variations.
3. Conduct ablation studies to isolate the impact of prompt tuning versus fine-tuning and special tokens on both transcription and miscue detection.