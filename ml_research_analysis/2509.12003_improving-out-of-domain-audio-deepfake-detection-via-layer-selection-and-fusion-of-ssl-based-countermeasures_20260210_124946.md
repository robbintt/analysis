---
ver: rpa2
title: Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and Fusion
  of SSL-Based Countermeasures
arxiv_id: '2509.12003'
source_url: https://arxiv.org/abs/2509.12003
tags:
- layer
- fusion
- performance
- detection
- mhfa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving out-of-domain (OOD)
  generalization in audio deepfake detection systems that rely on frozen self-supervised
  learning (SSL) encoders. The authors propose a systematic analysis of six SSL models
  across four OOD test corpora, examining layer-by-layer contributions and comparing
  single-layer versus multi-head factorized attentive pooling (MHFA) approaches.
---

# Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and Fusion of SSL-Based Countermeasures

## Quick Facts
- **arXiv ID:** 2509.12003
- **Source URL:** https://arxiv.org/abs/2509.12003
- **Reference count:** 26
- **Primary result:** Selecting optimal intermediate layers from frozen SSL encoders and fusing multiple models achieves state-of-the-art OOD audio deepfake detection with up to 80% fewer parameters.

## Executive Summary
This paper addresses the challenge of improving out-of-domain (OOD) generalization in audio deepfake detection using frozen self-supervised learning (SSL) encoders. The authors systematically analyze six SSL models across four OOD test corpora, examining layer-by-layer contributions and comparing single-layer versus multi-head factorized attentive pooling (MHFA) approaches. Their core method involves selecting the most informative intermediate layers for classification and fusing multiple SSL models at the score level. Results show that intermediate layers consistently outperform output layers, and selecting the best single layer achieves state-of-the-art performance with up to 80% fewer parameters. Additionally, fusing complementary SSL models improves OOD generalization, achieving EERs as low as 5.4% on InTheWild and 4.1% on MLAAD datasets.

## Method Summary
The method involves freezing pre-trained SSL encoders (Wav2vec 2.0, WavLM, BEATs, etc.) and attaching either a mean pooling (MP) or multi-head factorized attentive pooling (MHFA) classification head. The study systematically evaluates each transformer layer's contribution by training individual MP heads on every layer and measuring OOD performance. The "Best Single Layer" (BSL) approach selects the layer with optimal validation performance, while MHFA automatically weights all layers. For maximum robustness, the authors fuse the outputs of multiple BSL or MHFA systems using logistic regression trained on ASVspoof5-dev scores. This approach maintains frozen backbones while achieving superior OOD generalization through layer selection and model fusion.

## Key Results
- Intermediate layers consistently provide the most relevant features for audio deepfake detection, with final layers showing degraded performance
- Selecting the best single layer achieves state-of-the-art performance with up to 80% fewer parameters than MHFA
- Score-level fusion of complementary SSL models significantly improves OOD generalization (EERs as low as 5.4% on InTheWild and 4.1% on MLAAD)
- WavLM Large excels on InTheWild (5.4% EER) while BEATs excels on MLAAD (4.1% EER), demonstrating model complementarity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intermediate layers of frozen SSL encoders provide more discriminative features for audio deepfake detection than final output layers.
- **Mechanism:** SSL models build hierarchical representations through successive transformer layers. Final layers specialize toward pre-training objectives, which may discard acoustic artifacts relevant to spoof detection. Intermediate layers retain phonetic and acoustic detail that better distinguishes synthetic from genuine audio. The consistent pattern observed is: high initial error → sharp decrease through intermediate layers → degradation toward output.
- **Core assumption:** Discriminative features for deepfake detection exist in intermediate acoustic representations, not in semantic or speaker-identity representations concentrated in final layers.
- **Evidence anchors:** [abstract], [Section IV-D], [corpus]

### Mechanism 2
- **Claim:** Score-level fusion of diverse SSL encoders improves out-of-domain generalization by leveraging complementary failure modes.
- **Mechanism:** Different SSL models are pre-trained with different objectives (noise robustness, multilingual data, general audio tokenization). This leads them to develop distinct representations with different strengths and blind spots. A spoof that evades one model's features may be detected by another's. Score-level fusion combines independent probabilities, smoothing model-specific weaknesses.
- **Core assumption:** Errors across different SSL-based detectors are partially uncorrelated on OOD data. The fusion training corpus is sufficiently representative to learn useful combination weights.
- **Evidence anchors:** [abstract], [Section IV-C, Table IV], [Section IV-D3], [corpus]

### Mechanism 3
- **Claim:** A simple mean pooling classifier on a single well-chosen intermediate layer achieves performance competitive with complex multi-layer pooling heads like MHFA.
- **Mechanism:** Most discriminative power resides in frozen SSL representations. Once the optimal layer is identified, a minimal head (linear projection + mean pooling) suffices for binary classification. Multi-head factorized attentive pooling (MHFA) primarily automates layer weighting but does not fundamentally add discriminative capacity beyond what exists in the best single layer.
- **Core assumption:** The optimal layer for a given SSL model is relatively stable across OOD datasets, permitting a single "best single-layer" (BSL) selection. The classification boundary is approximately linear in that layer's feature space.
- **Evidence anchors:** [abstract], [Section IV-D2], [corpus]

## Foundational Learning

- **Concept: Self-Supervised Learning (SSL) for Speech**
  - **Why needed here:** The entire method builds on extracting features from frozen SSL encoders (Wav2vec 2.0, WavLM, BEATs, etc.). Understanding that these models learn general-purpose speech/audio representations without labels is foundational.
  - **Quick check question:** What does it mean for an SSL encoder to be "frozen," and why does this paper choose to freeze it? (Answer: The model's weights are not updated during training. This isolates the power of pre-trained representations and simplifies development.)

- **Concept: Transformer Layer Representations**
  - **Why needed here:** The core finding is that intermediate layers outperform output layers. Understanding that each transformer layer sequentially processes and transforms input, building different abstraction levels, is essential.
  - **Quick check question:** Which layer typically provides the best features for audio deepfake detection in this paper's experiments, and why is the final output layer suboptimal? (Answer: Intermediate layers. The final layer may be too specialized for the pre-training task.)

- **Concept: Score-Level Fusion**
  - **Why needed here:** This is the final step for boosting OOD performance. It combines output scores (probabilities/log-likelihoods) of independent systems, not their internal features.
  - **Quick check question:** Why does the paper fuse scores from different models rather than creating one giant model with all their features? (Answer: To maintain low computational cost and leverage complementary strengths (different failure modes) of diverse models.)

## Architecture Onboarding

- **Component map:** Audio Input → Frozen SSL Encoder → Intermediate Layer Extraction → Lightweight Classification Head (MP or MHFA) → Binary Score. For maximum OOD robustness: → Scores from Models A, B, C, D → Fusion → Final Decision.

- **Critical path:** Audio Input → Frozen SSL Encoder → Intermediate Layer Extraction → Lightweight Classification Head (MP or MHFA) → Binary Score. For maximum OOD robustness: → Scores from Models A, B, C, D → Fusion → Final Decision.

- **Design tradeoffs:**
  1. **Single Layer vs. MHFA:** Single Layer (Mean Pooling) is simpler, faster, with up to 80% fewer parameters but requires manual/experimental layer selection. MHFA automates layer weighting but adds complexity.
  2. **Model Selection:** WavLM Large excels on ITW (5.4% EER) but underperforms on MLAAD; BEATs excels on MLAAD (4.1% EER) but struggles on ITW. No single model is universally best.
  3. **Fusion Method:** Logistic regression fusion is more powerful but requires separate fusion corpus (ASVspoof5-dev). Sum of calibrated scores is simpler but may be slightly less performant.

- **Failure signatures:**
  - **High EER on final output layer:** Model is using default output. Switch to intermediate layer.
  - **Good in-domain but very poor OOD performance:** Overfitting to training attacks. Freeze backbone, simplify head, or fuse with complementary model (e.g., add BEATs if using WavLM).
  - **Highly variable performance across OOD test sets:** Inherent property of single SSL models. Solution: model fusion.

- **First 3 experiments:**
  1. **Baseline Layer Analysis:** Take WavLM Base. Train Mean Pooling classifier on each layer individually using ASVspoof5-train-train. Evaluate on ASVspoof5-dev and InTheWild. Plot EER vs. Layer to confirm intermediate layer advantage.
  2. **MHFA vs. Best Single Layer:** Train MHFA head on all layers of same encoder. Compare performance and parameter count to best single-layer model from experiment 1.
  3. **Simple Fusion:** Take two complementary models (e.g., WavLM Large + BEATs). Train their optimal BSL classifiers. Fuse scores via simple sum (after calibration) on ASVspoof5-dev. Evaluate on MLAAD and InTheWild to observe OOD generalization gain.

## Open Questions the Paper Calls Out

- **Question:** Does the effectiveness of intermediate layer selection persist when the self-supervised learning (SSL) encoder is fine-tuned rather than frozen?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that future work should evaluate the validity of their conclusions for "more sophisticated training recipes," specifically noting that "The finetuning of the SSL encoder... could also affect our conclusions."
  - **Why unresolved:** The study strictly freezes backbones to isolate representation quality; fine-tuning updates weights across all layers, potentially shifting the feature hierarchy.
  - **What evidence would resolve it:** A comparative analysis of layer-wise performance (EER) on OOD datasets using identical models trained with frozen versus fine-tuned backbones.

- **Question:** How vulnerable are frozen SSL-based detection systems to adaptive adversarial attacks that exploit the fixed nature of the encoder?
  - **Basis in paper:** [explicit] The Conclusion highlights a security limitation: "it seems obvious that an attacker can use knowledge of the nature of the system, the use of a frozen, pre-trained SSL encoder, in the design of the attack."
  - **Why unresolved:** The paper evaluates generalization against unseen deepfakes but does not test robustness against adversarial examples designed to bypass a static feature extractor.
  - **What evidence would resolve it:** Performance evaluation of the proposed fused systems against white-box adversarial attacks where the attacker has full knowledge of the frozen SSL parameters.

- **Question:** Can we predict the optimal SSL model and layer for a specific target domain based solely on pre-training objectives and datasets?
  - **Basis in paper:** [inferred] The authors observe "wide variation in performance" depending on the test corpus (e.g., BEATs excels on MLAAD but fails on InTheWild) and state that a "fuller and deeper understanding of model behavior" is required.
  - **Why unresolved:** While the paper establishes that different models suit different domains, it does not propose a theoretical framework to predict this mapping without empirical testing.
  - **What evidence would resolve it:** A theoretical analysis or causal study linking specific pre-training data characteristics (e.g., multilingual vs. general audio) to performance on specific OOD attack features.

## Limitations

- Layer selection results are dataset-dependent; the "optimal" layer may shift significantly with different OOD attacks or SSL pre-training objectives
- MHFA implementation details are not fully specified, introducing potential reproducibility gaps
- Fusion effectiveness assumes the ASVspoof5-dev corpus is representative of OOD attack characteristics

## Confidence

- **High Confidence:** The pattern of intermediate layers outperforming output layers (supported by consistent experimental results across 6 SSL models)
- **Medium Confidence:** The claim of achieving "state-of-the-art" performance with single-layer selection, as it is based on this paper's own evaluation and lacks extensive external benchmarking
- **Medium Confidence:** The utility of MHFA as a fully automated alternative to manual layer selection, as performance comparisons are relative and ablation studies on MHFA components are limited

## Next Checks

1. **Cross-Model Layer Stability:** Train the same SSL encoder on two independently constructed OOD datasets (e.g., InTheWild and MLAAD). Verify if the layer with the lowest validation EER is consistent or varies significantly.

2. **MHFA Component Ablation:** Implement MHFA without the attention mechanism (simple sum of layer outputs). Quantify the exact EER drop to assess the true contribution of the "attentive" component versus simple pooling.

3. **Fusion Corpus Robustness:** Construct a "synthetic OOD" test set by removing a known attack type from ASVspoof5-dev and training the fusion model on the remainder. Evaluate on the held-out attack to test fusion's ability to generalize to unseen attack types.