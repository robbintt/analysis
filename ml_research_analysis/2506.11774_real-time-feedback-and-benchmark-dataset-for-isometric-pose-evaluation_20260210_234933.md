---
ver: rpa2
title: Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation
arxiv_id: '2506.11774'
source_url: https://arxiv.org/abs/2506.11774
tags:
- pose
- feedback
- exercise
- isometric
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a real-time feedback system for assessing
  isometric poses, addressing the lack of reliable automated feedback for home workouts.
  The authors release the largest multi-class isometric exercise video dataset to
  date, comprising over 3,600 clips across six poses with correct and incorrect variations,
  including common mistakes.
---

# Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation

## Quick Facts
- arXiv ID: 2506.11774
- Source URL: https://arxiv.org/abs/2506.11774
- Reference count: 40
- Key outcome: Introduces real-time feedback system for isometric poses with largest multi-class dataset (3,600+ clips across 6 poses), benchmarks state-of-the-art models, and proposes safety-aware three-part metric; angle-based classifier achieves weighted F1 of 0.947 with superior reliability.

## Executive Summary
This paper addresses the lack of reliable automated feedback for home isometric exercises by introducing a comprehensive real-time feedback system. The authors release the IMCD dataset, comprising over 3,600 video clips across six isometric poses with correct and two common mistake variations each. They benchmark traditional and advanced models including angle-based classifiers, motion prediction networks, and 2s-AGCN graph networks, revealing that simpler methods often outperform complex ones in safety-critical scenarios. The novel three-part metric explicitly captures classification accuracy, mistake localization, and model confidence, demonstrating that high F1 scores alone don't guarantee safe deployment for exercise feedback.

## Method Summary
The system processes video input through pose estimation to extract 2D keypoints, segments repetitions using motion prominence filtering, and classifies poses using either angle-based histogram features with MLP, motion prediction deviations, or graph convolutional networks. The angle-based approach computes joint triplet angles, builds histograms, and uses the most prominent bins as features for classification. The three-part metric evaluates models on binary correctness detection (M1), confident mistake identification (M2), and uncertainty quantification (M3), providing a safety-focused alternative to traditional accuracy measures.

## Key Results
- Angle-based classifier achieves highest weighted F1 score of 0.947 across all poses
- Three-part metric reveals simpler models outperform complex ones in reliability and safety for real-world deployment
- For Tree Pose, 2s-AGCN achieves multiclass F1 of 0.956 but has 12.9% uncertainty (M3), while angle-based achieves 0.947 with only 0.21% uncertainty
- Binary F1 (M1) for angle-based classifier reaches 0.959, significantly higher than 2s-AGCN's 0.870

## Why This Works (Mechanism)

### Mechanism 1: Joint-Angle Histogram Feature Representation
Representing poses through angle histograms rather than raw coordinates provides body-scale invariance and improves classification robustness for isometric hold-phase evaluation. For each joint triplet, the system computes angle via atan2 of normalized joint vectors, bins into histograms over [0°, 360°], and uses bin centers with maximum counts as features. This data-driven approach learns correct pose patterns from data rather than using trainer-defined thresholds, achieving weighted F1 of 0.947.

### Mechanism 2: Three-Part Safety-Aware Evaluation Metric
Standard multiclass F1 scores can mask dangerous misclassifications; a composite metric explicitly quantifies correctness detection, confident mistake localization, and model uncertainty. M1 measures correct vs incorrect discrimination, M2 filters predictions where model assigns >50% probability to incorrect class, and M3 counts low-confidence mistake predictions where max probability <50%. This reveals that angle-based classifier (M1: 0.959, M3: 0.21%) is safer than 2s-AGCN (M1: 0.870, M3: 12.9%) despite similar multiclass F1.

### Mechanism 3: Prominence-Based Rep Segmentation
Isometric exercises require isolating static hold phase from entry/exit transitions; prominence filtering on motion signals robustly identifies true rep boundaries. Motion time series for reference joint is analyzed for local maxima, retaining only peaks with prominence exceeding threshold (τ=0.2) as hold-phase candidates. Rep start/end are identified as local minima surrounding each prominent peak, effectively separating hold phase from transitional movements.

## Foundational Learning

- **Concept: Skeleton-based Pose Representation**
  - Why needed here: All benchmarked models operate on 2D keypoint coordinates extracted from video. Understanding how pose estimation maps pixels to joint locations is prerequisite to debugging feature extraction and classification failures.
  - Quick check question: Given a video frame with a person in Tree Pose, can you explain how a pose estimator would extract coordinates for right ankle, knee, and hip, and what failure modes might degrade accuracy?

- **Concept: Graph Convolutional Networks for Skeleton Data**
  - Why needed here: 2s-AGCN represents state-of-the-art baseline. It models joints as graph nodes with spatial edges (body connectivity) and temporal edges (same joint across frames). Understanding adaptive graph learning is necessary to interpret why it outperforms on some poses but has higher uncertainty.
  - Quick check question: In a skeleton graph with N joints and T frames, how would you construct adjacency matrix to capture both within-frame spatial relationships and across-frame temporal relationships?

- **Concept: F1 Score vs. Confidence-Weighted Metrics**
  - Why needed here: The paper's core contribution is demonstrating that higher F1 doesn't guarantee safer deployment. Engineers must understand when to optimize for calibration and uncertainty quantification over raw accuracy.
  - Quick check question: A model achieves 95% multiclass F1 on plank pose classification but classifies 10% of incorrect planks as correct with >80% confidence. Why might this be more dangerous than a model with 90% F1 but only 2% high-confidence false negatives?

## Architecture Onboarding

- **Component map:**
  Input Video → Pose Estimator (2D keypoints) → Rep Segmentation (prominence filter) → Angle-Based Classifier (histogram features + MLP) → Three-Part Metric Evaluation → Real-Time Feedback Output

- **Critical path:** Video → Pose Estimation → Angle Feature Extraction → MLP Classification → M1/M2/M3 Scoring. This path achieves best safety profile (lowest M3 uncertainty) and is computationally lightweight for real-time deployment.

- **Design tradeoffs:**
  - Angle-based vs 2s-AGCN: Angle-based offers 61× lower uncertainty (0.21% vs 12.9% M3) and better binary F1 (M1: 0.959 vs 0.870) but may underfit complex pose variations. 2s-AGCN captures richer spatiotemporal patterns but requires more data to calibrate confidence.
  - Predefined thresholds vs data-driven ranges: The paper learns angle distributions from data (±1 std. dev.) rather than hard-coding trainer thresholds, improving generalization across body types but requiring sufficient training diversity.
  - Within-task vs post-task feedback: Referenced HCI work shows users prefer within-set feedback. Real-time deployment requires sub-second inference; angle-based MLP meets this, while 2s-AGCN may not without optimization.

- **Failure signatures:**
  - High M3 on a pose → Model hasn't learned discriminative features for that pose's mistake categories; collect more labeled examples or simplify mistake taxonomy
  - Low M1 despite high multiclass F1 → Dangerous for deployment; model is classifying incorrect poses as correct. Investigate class imbalance or threshold calibration
  - Rep segmentation misses or duplicates reps → Prominence threshold τ may be inappropriate; tune per-exercise or switch to learning-based segmentation
  - Inconsistent performance across subjects → Pose estimator struggling with certain body types or clothing; consider 3D pose estimation or multi-view input

- **First 3 experiments:**
  1. Reproduce angle-based baseline on IMCD: Implement histogram feature extraction, train MLP classifier with 5-fold cross-validation, compute M1/M2/M3. Verify weighted F1 ≈ 0.947 ± margin.
  2. Stress-test M3 uncertainty across error severity levels: Syntheticly perturb correct poses with increasing joint angle deviations (5°, 10°, 20°, 30°) and measure M3 response. Hypothesis: M3 should decrease as deviations become more pronounced.
  3. Cross-subject generalization analysis: Train on N-1 subjects, test on held-out subject. Measure M1/M2/M3 degradation to quantify dataset diversity gap and inform whether production deployment requires per-user calibration.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed isometric feedback system generalize across gender and body type demographics not represented in the current dataset? The authors note the participant pool consists solely of males, which may affect generalizability for other demographics. Biomechanical differences and clothing variations between genders could alter pose estimation accuracy and applicability of learned angle-based thresholds, potentially leading to incorrect feedback for female users. A benchmark evaluation on a newly collected dataset comprising female subjects and diverse body types would verify if weighted F1 scores and M1-M3 metrics remain consistent.

### Open Question 2
Does the deployment of real-time automated feedback systems actually improve long-term exercise adherence and safety for home users compared to following static video tutorials? While the paper proves technical feasibility (high F1 scores), it does not validate the human-computer interaction aspect—specifically, if automated feedback successfully motivates users to continue exercising or if it causes frustration/abandonment over time. A longitudinal user study comparing retention rates and injury incidence between control and experimental groups would resolve this.

### Open Question 3
Can advanced graph-based action recognition models be adapted to minimize uncertainty (M3 metric) to a level safe for clinical deployment, potentially outperforming the simpler angle-based classifier? The authors admit their exploration of action recognition models has been limited and observe that complex models like 2s-AGCN currently suffer from significantly higher uncertainty compared to angle-based methods. It remains unknown if deep learning models can be constrained or regularized to offer both high accuracy and high confidence required for safety-critical feedback. Experiments applying uncertainty quantification techniques to graph convolutional networks would determine if M3 can be reduced below the angle-based baseline while maintaining superior multiclass F1 scores.

## Limitations
- Dataset contains only ~600 clips per pose across correct and two mistake categories with only ~10 male subjects, limiting cross-subject generalization
- All methods rely on 2D keypoint extraction, and errors in keypoint localization directly affect joint angle computations
- Real-time validation gap: angle-based classifier is lightweight enough for deployment, but actual latency measurements and mobile hardware validation are not provided
- Framework tested only on six isometric poses; transferability to dynamic exercises, different body types, or multi-person scenarios is unexplored

## Confidence
- **High confidence**: Angle-based classifier's superior M1 (binary F1) performance and low M3 uncertainty on IMCD dataset
- **Medium confidence**: Three-part metric's ability to reveal safety-relevant differences between models; requires real-world user studies for validation
- **Low confidence**: Generalization claims to diverse populations and exercises; dataset size and diversity may not support broad deployment

## Next Checks
1. Cross-subject generalization test: Train on N-1 subjects, test on held-out subject. Measure M1/M2/M3 degradation to quantify dataset diversity limitations.
2. Pose estimator robustness evaluation: Introduce synthetic occlusions, angle variations, and clothing perturbations to input videos. Measure impact on keypoint accuracy and downstream classification metrics.
3. Real-time deployment validation: Implement angle-based classifier on mobile hardware (e.g., smartphone). Measure inference latency, battery consumption, and user experience during active exercise sessions.