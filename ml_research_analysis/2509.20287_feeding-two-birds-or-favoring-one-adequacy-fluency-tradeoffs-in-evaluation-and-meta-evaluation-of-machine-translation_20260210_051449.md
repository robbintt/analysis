---
ver: rpa2
title: Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation
  and Meta-Evaluation of Machine Translation
arxiv_id: '2509.20287'
source_url: https://arxiv.org/abs/2509.20287
tags:
- adequacy
- fluency
- translation
- metrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the tradeoff between adequacy and fluency
  in machine translation evaluation and meta-evaluation. The authors find that current
  evaluation metrics tend to lean toward adequacy, showing stronger correlation with
  adequacy scores than fluency scores.
---

# Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation

## Quick Facts
- arXiv ID: 2509.20287
- Source URL: https://arxiv.org/abs/2509.20287
- Reference count: 17
- Current evaluation metrics tend to lean toward adequacy, showing stronger correlation with adequacy scores than fluency scores.

## Executive Summary
This work investigates the tradeoff between adequacy and fluency in machine translation evaluation and meta-evaluation. The authors find that current evaluation metrics tend to lean toward adequacy, showing stronger correlation with adequacy scores than fluency scores. More importantly, they reveal that this tradeoff also persists at the meta-evaluation level, where the standard WMT meta-evaluation favors adequacy-oriented metrics due to the composition of systems in the meta-evaluation datasets. To address this bias, the authors propose a method to synthesize translation systems with desired variance in adequacy and fluency scores, enabling more balanced meta-evaluation. Using both pairwise accuracy (PA) and soft pairwise accuracy (SPA) metrics, they analyze several contemporary translation metrics and demonstrate that most lean toward adequacy, with MetricX variants showing relatively more balanced behavior compared to Comet variants.

## Method Summary
The paper analyzes adequacy-fluency tradeoffs in MT evaluation by first categorizing MQM error annotations using Flamich et al.'s taxonomy to create separate Adequacy MQM and Fluency MQM scores. They then compute variance of system-level scores across WMT datasets and use F-statistics to separate extrinsic (system composition) from intrinsic (annotation framework) bias. To create balanced meta-evaluation, they synthesize pseudo-systems by ranking translations per segment based on Adequacy or Fluency MQM scores. They evaluate metrics using pairwise accuracy (PA) and soft pairwise accuracy (SPA) against both adequacy and fluency dimensions separately, measuring sensitivity and bias.

## Key Results
- Most contemporary metrics (including Comet variants) show stronger correlation with adequacy than fluency in WMT meta-evaluation
- Standard WMT meta-evaluation is biased toward adequacy due to higher variance in adequacy scores across candidate systems
- The proposed system synthesis method successfully creates balanced meta-evaluation setups where the adequacy-fluency tradeoff becomes observable
- MetricX variants demonstrate more balanced behavior between adequacy and fluency compared to Comet variants

## Why This Works (Mechanism)

### Mechanism 1: Variance-Driven Meta-Evaluation Bias
Meta-evaluation outcomes are systematically skewed by relative variance in adequacy vs. fluency scores across the candidate systems pool. When Adequacy MQM exhibits higher system-level variance than Fluency MQM, the composite All MQM score (≈ Adequacy + Fluency) is dominated by adequacy. This causes metrics that correlate strongly with adequacy to achieve higher PA/SPA scores, regardless of their fluency measurement capability.

### Mechanism 2: F-Statistic Decomposition of Intrinsic vs. Extrinsic Bias
F-statistics from ANOVA framework can isolate extrinsic bias (system composition) from intrinsic bias (annotation framework preferences). F-statistic = between-system variation / within-system variation. Intrinsic variation (e.g., adequacy errors considered more severe) acts as a multiplicative constant that cancels out in numerator and denominator. Extrinsic variation (choice of systems in meta-evaluation) is captured by normalizing between-system by within-system variation.

### Mechanism 3: Controlled System Synthesis for Debiasing
Synthesizing pseudo-systems with maximal variance in one quality dimension can counteract existing meta-evaluation bias. For K original systems, create K adequacy-oriented systems by selecting each segment's k-th best translation by Adequacy MQM. Similarly, create K fluency-oriented systems. Combining original + synthesized systems (3K total) allows controlled variance manipulation.

## Foundational Learning

- **Concept: MQM Framework and Error Categorization**
  - Why needed here: The entire analysis depends on decomposing MQM annotations into Adequacy MQM and Fluency MQM; understanding error taxonomy is prerequisite for interpreting results.
  - Quick check question: Can you explain why "Mistranslation" is categorized as an adequacy error while "Grammar" is a fluency error?

- **Concept: Meta-Metrics (PA and SPA)**
  - Why needed here: The paper uses pairwise accuracy (PA) and soft pairwise accuracy (SPA) to quantify metric performance; understanding their differences is essential for interpreting Table 6 and Figure 1.
  - Quick check question: Given two systems where Metric scores differ by 0.1 (insignificant) but Human scores differ by 5.0 (highly significant), which meta-metric (PA or SPA) would penalize the metric more heavily?

- **Concept: Adequacy-Fluency Tradeoff in Translation**
  - Why needed here: Flamich et al. (2025) established that optimizing translation for one dimension sacrifices the other; this paper extends the concept to evaluation metrics themselves.
  - Quick check question: If a translation metric is trained to maximize correlation with All MQM, and All MQM is adequacy-dominated, what implicit bias will the metric learn?

## Architecture Onboarding

- **Component map:**
  1. MQM Data Layer (WMT 2023-2024 datasets with error annotations)
  2. Score Computation (Transform MQM annotations → Adequacy MQM / Fluency MQM / All MQM scores)
  3. F-Statistic Calculator (Compute extrinsic bias via between-system / within-system variation)
  4. System Synthesizer (Generate pseudo-systems by ranking segments by Adequacy or Fluency MQM)
  5. Meta-Evaluation Engine (Compute PA and SPA for each metric against Adequacy MQM and Fluency MQM separately)
  6. Sensitivity Analyzer (Measure Δmetric_score / ΔAdequacy_MQM holding Fluency constant)

- **Critical path:**
  1. Load MQM annotations → categorize errors as adequacy/fluency (Tables 8-9)
  2. Compute variance of system-level Adequacy MQM vs. Fluency MQM → calculate B(Δp) to quantify extrinsic bias
  3. Synthesize balanced system pool → re-run meta-evaluation → compare metric rankings (Table 4)

- **Design tradeoffs:**
  - **Intrinsic vs. Extrinsic Bias**: Paper retains intrinsic bias (reflects expert beliefs) but controls extrinsic bias; alternative is to debias both, but this may lose valuable domain knowledge
  - **Original vs. Balanced Setup**: Balanced setup (Row 6) enables clearer tradeoff analysis but may not reflect real-world system distributions
  - **PA vs. SPA**: PA provides interpretable binary agreement; SPA captures confidence/magnitude but harder to interpret; paper uses both to cross-validate

- **Failure signatures:**
  - High variance in results across evaluation sets (Table 15, Figure 3 in Appendix F) indicates noise sensitivity
  - When B(Δp) is near zero but PA/SPA still show strong adequacy alignment (Table 3, Row 5), intrinsic bias is dominant
  - If synthesized systems produce undefined p-values (mentioned in Appendix D), variance-inequality assumption is violated

- **First 3 experiments:**
  1. Replicate variance analysis: Compute Adequacy MQM vs. Fluency MQM variance and B(Δp) for WMT 2023-2024 data; verify Table 2 values match
  2. Synthesize minimal balanced system set: Create synthesized-by-adequacy and synthesized-by-fluency systems for one language pair; confirm B value decreases from original setup
  3. Sensitivity probe: For MetricX and Comet, measure normalized sensitivity to adequacy vs. fluency (Table 7); verify MetricX shows more balanced ratios (0.5–0.6 vs. 0.4–0.5 for adequacy, 0.14–0.23 vs. 0.07–0.14 for fluency)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the adequacy–fluency tradeoff manifest in Large Language Models (LLMs) when used as judges for machine translation?
  - Basis in paper: [explicit] The authors explicitly identify "LLM-as-a-judge" as a future direction to study this tradeoff.
  - Why unresolved: The current study focused exclusively on dedicated neural metrics (MetricX, Comet) and lexical metrics, without evaluating generative LLMs.
  - What evidence would resolve it: An analysis of LLM evaluation prompts and outputs, measuring their correlation specifically against separated Adequacy MQM and Fluency MQM scores.

- **Open Question 2**: Can meta-evaluation bias be mitigated effectively through theoretical post-hoc score normalization without synthesizing pseudo-systems?
  - Basis in paper: [explicit] The authors propose exploring theoretical approaches to debias meta-evaluation outputs via normalization as an alternative to their data synthesis method.
  - Why unresolved: The paper successfully reduced bias by synthesizing translation candidates but did not investigate mathematical normalization techniques to achieve the same result.
  - What evidence would resolve it: A normalization algorithm applied to existing metric scores that successfully balances the influence of adequacy and fluency variances in WMT datasets.

- **Open Question 3**: To what extent does the intrinsic bias of the MQM framework contribute to meta-evaluation skew compared to the extrinsic bias of system composition?
  - Basis in paper: [explicit] The authors note they only analyzed extrinsic bias (system selection) and explicitly flag the need for further efforts to study the impact of intrinsic bias (MQM design/annotator preference).
  - Why unresolved: The study controlled for extrinsic bias but treated the intrinsic severity of error penalties as a fixed constant, leaving its specific influence unquantified.
  - What evidence would resolve it: A comparative analysis using different human evaluation frameworks (e.g., DA vs. MQM) to isolate the bias inherent in the annotation guidelines themselves.

## Limitations

- The analysis relies on MQM error categorization from a specific taxonomy, which may not generalize to other evaluation frameworks
- The system synthesis method creates artificial systems that may not capture realistic translation system behavior patterns
- The F-statistic approach assumes equal variance across systems, which may be violated in practice

## Confidence

- **High Confidence**: The core observation that current meta-evaluation favors adequacy-oriented metrics (supported by Table 4 showing Comet variants outperforming MetricX under original setup)
- **Medium Confidence**: The effectiveness of synthesized systems in creating balanced meta-evaluation (demonstrated in Table 3 Row 6, but relies on synthetic proxy systems)
- **Low Confidence**: The generalizability of findings across different annotation frameworks beyond MQM

## Next Checks

1. **Cross-Framework Validation**: Apply the variance analysis and system synthesis approach to a different annotation framework (e.g., Direct Assessment or Multidimensional Quality Metrics) to test whether the adequacy-fluency tradeoff persists across evaluation paradigms.

2. **Real vs. Synthetic System Comparison**: Select a subset of synthesized systems and compare their behavior to real translation systems from WMT history to assess whether synthesized systems are valid proxies for meta-evaluation purposes.

3. **Intrinsic Bias Sensitivity Analysis**: Systematically vary the error categorization scheme (e.g., reclassify borderline errors between adequacy and fluency) to measure how sensitive the extrinsic bias measurements are to the underlying annotation framework.