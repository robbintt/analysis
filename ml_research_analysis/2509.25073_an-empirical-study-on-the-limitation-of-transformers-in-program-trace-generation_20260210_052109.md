---
ver: rpa2
title: An empirical study on the limitation of Transformers in program trace generation
arxiv_id: '2509.25073'
source_url: https://arxiv.org/abs/2509.25073
tags:
- trace
- program
- longer
- programs
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Transformers trained on program trace generation struggle to generalize
  beyond in-distribution test settings, despite achieving near-perfect performance
  within the training distribution. The study evaluates small Transformer variants
  (154M parameters) on generating step-by-step execution traces for synthetic programs,
  focusing on generalization across four factors: program length, trace steps, number
  of variables, and input size.'
---

# An empirical study on the limitation of Transformers in program trace generation

## Quick Facts
- arXiv ID: 2509.25073
- Source URL: https://arxiv.org/abs/2509.25073
- Reference count: 19
- Primary result: Transformers trained on program trace generation struggle to generalize beyond in-distribution test settings, despite achieving near-perfect performance within the training distribution.

## Executive Summary
This study investigates Transformers' ability to generalize program trace generation across four key dimensions: program length, trace steps, number of variables, and input list size. Using synthetic programs with simple operations (assignment, if/while blocks, list operations), the research finds that while Transformers achieve 98-100% accuracy on in-distribution test cases, they exhibit severe degradation on out-of-distribution scenarios. NaPE positional encoding shows the best overall performance (93.5-97.9% accuracy across generalization splits), while RoPE performs particularly poorly (3.9-65.5%). The results reveal fundamental limitations in Transformers' ability to maintain consistent execution across extended contexts, even for simple operations.

## Method Summary
The study uses synthetic programs generated from a simplified L0-Bench grammar with assignment, if/while blocks, and list operations (append/pop) limited to x+1/x-1, ==, and !=. Training programs range from 5-40 lines with 5-103 trace steps, maximum 10 variables, and input lists of 5-10 integers. The 154M parameter Transformer (12 layers, 16 heads, 1024 dim, SwiGLU) is trained for 20k steps on 4.03B tokens, with line number offset augmentation (1-100 with p=0.5). Evaluation uses seven splits testing longer programs, longer traces, more variables, and longer lists. Whole-trace accuracy (exact match) is reported with mean ± SE across 10 random seeds.

## Key Results
- Transformers achieve 98-100% accuracy on in-distribution test cases but fail to generalize to longer programs (46.5%), longer traces (14.2-91.6%), more variables (94.1%), or larger input lists (90.3%)
- NaPE positional encoding outperforms other schemes, achieving 93.5-97.9% accuracy across generalization splits
- RoPE shows particular weakness with 3.9-65.5% accuracy on generalization tasks
- Hybrid architectures and Canon layers provide modest improvements on specific dimensions but fail to resolve fundamental generalization limitations

## Why This Works (Mechanism)
The study does not explicitly detail the mechanism behind the observed limitations. However, the results suggest that Transformers struggle with maintaining global state consistency over extended sequences, particularly when the number of variables, program length, or trace steps increase beyond training distributions. The positional encoding scheme significantly impacts generalization performance, with NaPE showing better handling of longer traces compared to RoPE.

## Foundational Learning
- **Program Trace Generation**: Understanding step-by-step execution traces of synthetic programs with limited operations (why needed: core task being evaluated; quick check: can generate correct traces for simple 5-line programs)
- **Positional Encoding Schemes**: Different methods for encoding sequence positions in Transformers (NaPE, RoPE, ALiBi, etc.) (why needed: critical architectural component affecting generalization; quick check: compare NaPE vs RoPE performance on simple OOD tasks)
- **Out-of-Distribution Generalization**: Testing model performance on data distributions not seen during training (why needed: reveals fundamental limitations; quick check: measure accuracy drop on longer traces vs in-distribution)
- **Synthetic Program Generation**: Creating controlled program datasets with specific constraints (why needed: enables systematic evaluation; quick check: verify program generation grammar produces valid traces)
- **Whole-Trace Accuracy**: Exact match evaluation metric requiring perfect prediction of every step (why needed: strict measure of execution consistency; quick check: compare with partial credit metrics)
- **Context Window Limitations**: Maximum sequence length constraints (4096 tokens) affecting evaluation scope (why needed: bounds what can be tested; quick check: verify no truncation in evaluation data)

## Architecture Onboarding

**Component Map**: Tokenizer -> Transformer Encoder -> Position Encoding (NaPE/RoPE/NoPE) -> Output Layer -> Trace Generation

**Critical Path**: Synthetic Program Input → Tokenizer → Positional Encoding → Self-Attention Layers → Output Projection → Greedy Decoding → Execution Trace

**Design Tradeoffs**: Small model size (~154M) enables controlled experiments but may limit performance; exact-match accuracy provides strict evaluation but may be overly stringent; synthetic programs enable systematic testing but may not reflect real-world complexity

**Failure Signatures**: Near-perfect in-distribution accuracy (98-100%) followed by severe OOD degradation (46.5-91.6% accuracy drop); specific positional encoding schemes show varying degrees of failure on different generalization dimensions

**Three First Experiments**:
1. Train baseline NoPE Transformer and verify 98-100% in-distribution accuracy
2. Implement NaPE and RoPE positional encodings and compare OOD generalization performance
3. Test longer traces (2x training distribution) to observe specific degradation patterns

## Open Questions the Paper Calls Out

**Open Question 1**: Do non-Transformer architectures or Mixture-of-Experts (MoE) models overcome the systematic out-of-distribution failures observed in standard Transformers on program trace generation? The authors plan to extend their work to non-Transformers and MoE models, but the current study is restricted to dense Transformer architectures.

**Open Question 2**: Does the transduction setting require different modeling capabilities than the generative scratchpad setting used in this study? The authors plan to explore transduction (predicting only final state vs full trace), but it's unknown if the generalization limitations persist when intermediate steps are internalized.

**Open Question 3**: Can the specific strengths of NaPE (handling longer traces) and Canon layers (handling longer programs) be combined to mitigate the trade-offs observed in single-modification models? The paper evaluates modifications in isolation and does not investigate if these distinct inductive biases are complementary.

**Open Question 4**: Will scaling model size or training data volume resolve the specific inability of Transformers to execute simple operations consistently over long sequences? The paper establishes failure modes in small models but leaves open whether this is a fundamental architectural limit or a capacity/data issue.

## Limitations
- Synthetic programs use simplified operations (only increment/decrement and equality checks) that may not reflect real-world program complexity
- Maximum context length of 4096 tokens and program length of 40 lines constrains the scale of programs tested
- The study does not explore alternative optimization strategies, hyperparameter tuning, or scaling laws that might mitigate limitations
- Heavy reliance on exact-match accuracy as the sole metric may be overly stringent for practical applications

## Confidence

**High confidence**: In-distribution results (98-100% accuracy) are robust and reproducible given controlled synthetic data generation and well-specified training setup. Severe OOD generalization failures are well-documented with consistent patterns across positional encoding schemes.

**Medium confidence**: Relative performance of different positional encoding schemes is methodologically sound but may not generalize to other domains. Modest improvements from hybrid architectures and Canon layers are statistically supported but represent incremental gains.

**Low confidence**: Broader implications for reasoning and instruction-following tasks are extrapolated from synthetic program traces and require validation on real-world programming tasks and natural language reasoning benchmarks.

## Next Checks

1. **Real-world program evaluation**: Validate generalization patterns on actual open-source code repositories with complex control flow, nested structures, and diverse operations beyond simple arithmetic and comparisons.

2. **Scaling experiments**: Test whether increasing model size (100M-10B parameters) or training data scale improves OOD generalization on extended traces, following established scaling laws for Transformers.

3. **Alternative evaluation metrics**: Implement more lenient evaluation metrics (partial credit for correct intermediate steps, edit distance between predicted and actual traces) to assess whether exact-match accuracy is too restrictive for practical applications.