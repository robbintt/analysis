---
ver: rpa2
title: QZhou-Embedding Technical Report
arxiv_id: '2508.21632'
source_url: https://arxiv.org/abs/2508.21632
tags:
- data
- training
- arxiv
- retrieval
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QZhou-Embedding is a contextual text embedding model that achieves
  state-of-the-art performance on MTEB and CMTEB benchmarks. Built on Qwen2.5-7B-Instruct,
  it uses a unified multi-task framework with specialized data transformation and
  training strategies.
---

# QZhou-Embedding Technical Report

## Quick Facts
- arXiv ID: 2508.21632
- Source URL: https://arxiv.org/abs/2508.21632
- Reference count: 40
- Primary result: QZhou-Embedding achieves state-of-the-art performance on MTEB and CMTEB benchmarks

## Executive Summary
QZhou-Embedding is a contextual text embedding model that achieves state-of-the-art performance on MTEB and CMTEB benchmarks. Built on Qwen2.5-7B-Instruct, it uses a unified multi-task framework with specialized data transformation and training strategies. The model employs a two-stage training approach (retrieval-focused pretraining followed by full-task fine-tuning) and incorporates data synthesis via LLM for paraphrasing, augmentation, and hard negative generation. These innovations significantly enhance semantic richness and training difficulty. The model ranks first on both MTEB and CMTEB leaderboards as of August 27, 2025, demonstrating superior performance in retrieval, reranking, clustering, and other tasks.

## Method Summary
QZhou-Embedding is built on Qwen2.5-7B-Instruct with bidirectional attention modification. The unified multi-task framework transforms heterogeneous datasets into retrieval, NLI, and classification formats with task-specific losses. Data synthesis via LLM generates 30% of hard negatives through paraphrasing, augmentation, and hard negative generation. The two-stage training approach first establishes retrieval capability (32k steps, lr=3e-5) then expands to full tasks with η=0.72 retrieval ratio (8k steps, lr=2e-5). Mean pooling with L2 normalization generates final embeddings. Full-parameter fine-tuning uses bfloat16 precision with Data Grouping at dataset level.

## Key Results
- Ranks first on MTEB benchmark leaderboard as of August 27, 2025
- Achieves state-of-the-art performance on CMTEB Chinese benchmark
- Superior results across retrieval, reranking, clustering, and STS tasks

## Why This Works (Mechanism)

### Mechanism 1: Unified Multi-task Data Transformation
Converting heterogeneous data into three canonical formats (retrieval, NLI, classification) with task-specific losses enables cross-task knowledge transfer. Title-body pairs become query-document pairs for retrieval; NLI labels map to ordinal scores for Cosent loss; classification uses example-based sampling with masked InfoNCE to prevent false negatives. Semantic information is preserved sufficiently across transformations to benefit multi-task learning.

### Mechanism 2: LLM-Powered Data Synthesis Pipeline
Synthetic data generation via LLM APIs improves semantic richness and sample difficulty through three complementary techniques—paraphrasing for structural diversity, augmentation for semantic/topic diversity, hard negative generation for discriminative challenge. LLM-generated variations maintain semantic equivalence while introducing meaningful diversity.

### Mechanism 3: Two-Stage Training with Controlled Task Ratio
Establishing robust retrieval capability first, then expanding to full tasks with controlled retrieval ratio prevents catastrophic forgetting. Stage 1 trains only on retrieval data (32k steps, lr=3e-5); Stage 2 adds all tasks with η=0.72 retrieval ratio (8k steps, lr=2e-5). Retrieval is a foundational capability that supports rather than competes with other embedding tasks.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: Core training objective for retrieval/classification; understanding how negatives shape the embedding space is essential.
  - Quick check question: Why does adding query-query negatives (in addition to query-document negatives) help retrieval training?

- **Concept: Hard Negative Mining**
  - Why needed here: Central to the paper's data synthesis strategy; understanding why "hard" negatives matter more than random negatives.
  - Quick check question: What makes a negative sample "hard," and why do easy negatives provide diminishing gradient signal?

- **Concept: Bidirectional vs. Causal Attention**
  - Why needed here: The paper modifies Qwen2.5's causal attention to bidirectional—understanding this architectural choice is critical.
  - Quick check question: What semantic information does causal attention miss that bidirectional attention captures for embedding generation?

## Architecture Onboarding

- **Component map**: Qwen2.5-7B-Instruct (decoder-only, 7B params) → Attention Modification: causal → bidirectional → Pooling: mean pooling + L2 normalization → Training Pipeline: Data Transformation → Synthesis → Two-Stage Training

- **Critical path**:
  1. Data preparation: Transform heterogeneous datasets into (retrieval/NLI/CLS) formats
  2. Synthesis: Apply paraphrasing, augmentation, hard negative generation (30% LLM, 70% retrieval-based)
  3. Stage 1: Train 32k steps on retrieval-only data (lr=3e-5, batch=256, 4 hard negatives)
  4. Stage 2: Train 8k steps on all data with η=0.72 retrieval ratio (lr=2e-5)
  5. Evaluate on MTEB/CMTEB benchmarks

- **Design tradeoffs**:
  - Full-parameter fine-tuning vs. LoRA (chose full for maximum performance, higher compute cost)
  - LLM vs. retrieval-based hard negatives (30% LLM due to API cost constraints)
  - Dataset-level vs. task-level grouping (chose dataset-level for domain coherence)
  - Retrieval ratio η=0.72 (tuned value; paper doesn't report sensitivity analysis)

- **Failure signatures**:
  - Retrieval score drops after Stage 2 → η too low or non-retrieval data too dominant
  - Classification performance poor → check masking logic for false negatives
  - Synthetic data causes semantic drift → review LLM prompt constraints and filtering thresholds
  - Training instability with Cosent loss → verify ordinal label mapping is correct

- **First 3 experiments**:
  1. **Ablate data synthesis techniques**: Train three variants—paraphrase-only, augmentation-only, hard-negative-only—to isolate each contribution
  2. **Sweep η parameter**: Test η ∈ {0.5, 0.6, 0.72, 0.8, 0.9} to find optimal retrieval/non-retrieval balance
  3. **Compare grouping strategies**: Benchmark dataset-level vs. task-level batching on retrieval and classification metrics separately

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed data synthesis and multi-task framework be effectively generalized to multimodal and multilingual domains?
- Basis in paper: The conclusion explicitly states the future focus is on "developing multimodal and multilingual embedding models."
- Why unresolved: The current technical report focuses exclusively on text-based English and Chinese benchmarks.
- What evidence would resolve it: Successful application of the framework to image-text retrieval tasks or a broader set of multilingual benchmarks.

### Open Question 2
- Question: How sensitive is the model's final performance to the specific retrieval sampling ratio (η_RET) used during the second training stage?
- Basis in paper: Section 5.2 introduces a global retrieval ratio of 0.72 to prevent degradation but provides no ablation studies on this hyperparameter.
- Why unresolved: It is unclear if 0.72 is a robust default or a brittle setting specific to the current data mixture.
- What evidence would resolve it: Ablation results showing performance metrics across different η_RET values (e.g., 0.5, 0.6, 0.8, 0.9).

### Open Question 3
- Question: What is the specific performance delta between LLM-generated synthetic hard negatives and traditional retrieval-based mined negatives?
- Basis in paper: Section 6.1 notes only 30% of hard negatives were synthetically generated due to "API cost constraints," while the rest were mined.
- Why unresolved: The paper asserts synthetic data is crucial but does not isolate its impact versus the cheaper mined negatives used for the majority of the data.
- What evidence would resolve it: A comparative ablation study training models with 100% mined vs. 100% synthetic hard negatives.

## Limitations
- No ablation studies isolate the contribution of individual mechanisms (data transformation, synthesis, two-stage training, data grouping)
- Limited analysis of synthetic data quality and potential semantic drift in LLM-generated content
- Results demonstrated only on English and Chinese benchmarks without validation on other domains

## Confidence
- **High**: MTEB/CMTEB benchmark results and leaderboard rankings
- **Medium**: Effectiveness of unified multi-task framework and two-stage training
- **Low**: Claims about LLM synthesis quality and its specific contribution to performance gains

## Next Checks
1. **Ablation study**: Train four variants—baseline (no synthesis), paraphrase-only, augmentation-only, hard-negative-only—to quantify each synthesis technique's contribution to MTEB/CMTEB scores
2. **Contamination analysis**: Verify that no training data overlaps with MTEB/CMTEB test sets by computing n-gram overlap statistics and reporting exact overlap percentages
3. **η sensitivity sweep**: Systematically test retrieval ratios η ∈ {0.5, 0.6, 0.72, 0.8, 0.9} to determine optimal balance between retrieval preservation and multi-task learning, reporting task-specific impacts