---
ver: rpa2
title: 'MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction
  Tuning for Emotion-Cause Pair Extraction'
arxiv_id: '2507.14887'
source_url: https://arxiv.org/abs/2507.14887
tags:
- knowledge
- emotional
- emotion
- causal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the Emotion-Cause Pair Extraction (ECPE) task,
  where large language models (LLMs) underperform due to insufficient auxiliary knowledge
  for emotion perception and causal reasoning. To address this, the authors propose
  MEKiT, a multi-source heterogeneous knowledge injection method that integrates internal
  emotional knowledge and external causal knowledge through instruction tuning.
---

# MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction

## Quick Facts
- arXiv ID: 2507.14887
- Source URL: https://arxiv.org/abs/2507.14887
- Reference count: 7
- Primary result: MEKiT achieves 61.49% F1-score on NTCIR-13 dataset, outperforming baseline models.

## Executive Summary
MEKiT addresses the Emotion-Cause Pair Extraction (ECPE) task where large language models underperform due to insufficient auxiliary knowledge for emotion perception and causal reasoning. The method injects multi-source heterogeneous knowledge through instruction tuning, combining internal emotional knowledge (derived from COMET and SBERT) with external causal knowledge (from FLAN dataset). By optimizing the knowledge mixture ratio and leveraging LoRA for parameter-efficient fine-tuning, MEKiT demonstrates significant improvements across different LLM architectures while maintaining strong generality.

## Method Summary
MEKiT employs a three-stage approach: First, emotional knowledge is generated using COMET-BART to produce commonsense emotional reactions (xReact), which are then mapped to emotion label distributions via SBERT similarity or polarity classification for fallback cases. Second, causal knowledge is extracted from the FLAN dataset through similarity matching to the ECPE corpus. Third, these knowledge sources are combined at an optimal 1:5 ratio and used to instruction-tune LLMs via LoRA. The method specifically targets the knowledge gaps in emotion perception and causal reasoning that limit LLM performance on ECPE tasks.

## Key Results
- MEKiT achieves 61.49% F1-score on NTCIR-13 dataset, significantly outperforming baseline models
- Emotional knowledge injection improves F1 by 1.56%, while causal knowledge adds 2.34% improvement
- Optimal causal mixing ratio is 1:5, with performance degrading at higher ratios due to task drift
- Method demonstrates strong generality across different LLM architectures (Vicuna, LLaMA2, Gemma)

## Why This Works (Mechanism)

### Mechanism 1: Emotional Knowledge Injection via Instruction Templates
- Claim: Injecting structured emotional knowledge (label distributions or polarity) into instruction templates improves emotion perception in LLMs.
- Mechanism: COMET generates commonsense emotional reactions (xReact) for each document. SBERT maps these reactions to emotion label distributions via cosine similarity. When COMET returns "none" (43% of cases), a coarse-grained polarity classifier (POSITIVE/NEGATIVE) provides fallback knowledge. This structured knowledge is embedded directly into the instruction template as context.
- Core assumption: Emotion perception benefits from explicit, structured emotional context that LLMs cannot reliably infer from raw text alone.
- Evidence anchors:
  - [abstract] "Emotional knowledge is injected via instruction templates using label distributions or polarity scores derived from COMET and SBERT"
  - [Page 3] "For the non-none xReact relation, we employ the bidirectional encoder SBERT to identify the most semantically similar emotional label"
  - [Page 6, Table 4] Shows label + polarity knowledge combination (59.15% F1) outperforms individual components
- Break condition: If the emotion classification accuracy of the knowledge source drops significantly (as noted with distilroberta/bert-goemotions), noise from misclassification may disrupt reasoning rather than help it.

### Mechanism 2: Causal Knowledge Injection via Data Mixing
- Claim: Mixing general causal reasoning data (FLAN) with task-specific ECPE data at an optimal ratio (1:5) improves causal reasoning without task drift.
- Mechanism: FLAN contains diverse NLP tasks including commonsense reasoning. Causal examples are extracted via similarity matching to the ECPE corpus. These are mixed into the training set, exposing the model to broader causal patterns absent in the original dataset.
- Core assumption: General causal reasoning knowledge transfers to emotion-cause reasoning, and there exists an optimal mixing ratio beyond which task-specific performance degrades.
- Evidence anchors:
  - [Page 4] "extract causal knowledge in the form of natural text from FLAN by calculating the similarity between each entry in the emotion cause dataset and the FLAN corpus"
  - [Page 5, Table 2] Best F1 (61.49%) achieved at 1:5 ratio; 1:10 ratio shows decline (58.84%)
  - [Page 5] "adding too much causal knowledge does not always lead to better results...the optimization objective of the model may shift toward other non-ECPE tasks"
- Break condition: Excessive causal data (>1:10 ratio) causes optimization objective drift away from ECPE task.

### Mechanism 3: Heterogeneous Knowledge Synergy
- Claim: Combining emotional and causal knowledge produces greater gains than either alone, as they address distinct capability gaps.
- Mechanism: Emotional knowledge enhances emotion perception (comprehensive emotional state understanding). Causal knowledge enhances reasoning capability (background knowledge for cognitive appraisal). The appraisal theory of emotion posits that emotion depends on cognitive evaluation of causes.
- Core assumption: Emotion perception and causal reasoning are partially independent capabilities that require different knowledge sources.
- Evidence anchors:
  - [Page 6, Table 3] Ablation shows removing emotional knowledge drops F1 by 1.56%; removing causal knowledge drops F1 by 2.34%; removing both drops F1 by 3.87%
  - [Page 6] "causal knowledge encompasses many contextual scenarios and explicit causal relationships that are absent in the original emotion-cause dataset"
  - [corpus] Limited direct corpus support for this specific synergistic mechanism; related work on multi-source knowledge exists (Wu et al., 2021) but not directly validated for ECPE synergy
- Break condition: If one knowledge source introduces significant noise (e.g., low-accuracy emotion classification), it may counteract benefits from the other source.

## Foundational Learning

- Concept: **Instruction Tuning**
  - Why needed here: The method relies on formatting multi-source knowledge as structured instructions that guide the LLM's attention during fine-tuning.
  - Quick check question: Can you explain how instruction templates differ from standard supervised fine-tuning inputs?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: MEKiT uses LoRA for parameter-efficient fine-tuning of large models without full retraining.
  - Quick check question: What rank and target modules would you select for a 9B parameter model on this task?

- Concept: **Emotion Cognitive Appraisal Theory**
  - Why needed here: The theoretical foundation that emotions arise from evaluation of events/causes, justifying the need for causal knowledge injection.
  - Quick check question: How does this theory motivate the dual knowledge sources in MEKiT?

## Architecture Onboarding

- Component map: COMET-BART -> SBERT/emotion pipeline -> Instruction template -> LoRA fine-tuning
- Critical path:
  1. Generate xReact for each document via COMET
  2. Route to label distribution (non-none) or polarity (none) branch
  3. Construct instruction templates with emotional knowledge
  4. Mix FLAN causal data at 1:5 ratio
  5. Fine-tune LLM with LoRA
- Design tradeoffs:
  - Fine-grained emotion labels vs. polarity: Labels provide more signal but lower accuracy; polarity is coarser but more reliable (43% of cases route here)
  - Causal mixing ratio: Higher ratios add knowledge but risk task drift; optimal is model-dependent (Vicuna: 1:10, LLaMA2: 1:2, Gemma: 1:5)
  - Knowledge source choice: Pre-trained emotion classifiers (distilroberta, etc.) underperform COMET+SBERT due to noise
- Failure signatures:
  - Precision improves minimally while recall increases → model lacks discriminative knowledge for precise pair prediction
  - F1 drops at high causal mixing ratios → optimization objective drift
  - Using single-category emotion classifiers → noise from misclassification disrupts reasoning
- First 3 experiments:
  1. **Baseline validation**: Fine-tune target LLM (e.g., Gemma-2-9B-it) on ECPE data only, no knowledge injection. Expect ~57.6% F1 per Table 1.
  2. **Ablation by knowledge type**: Run three conditions—emotional only, causal only, both—to replicate Table 3 and verify each contributes independently.
  3. **Mixing ratio sweep**: Test 1:1, 1:2, 1:5, 1:10 ratios to find optimal for your specific LLM backbone, as optimal ratios vary by model (Table 2, Figure 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimal mixing ratio for causal knowledge be theoretically predicted or dynamically determined for different LLM architectures?
- Basis in paper: [explicit] The authors observe that "optimal mixing ratios for different LLMs are not necessarily consistent" (e.g., Vicuna required 1:10, while LLaMA2 required 1:2).
- Why unresolved: The paper relies on empirical grid search to find the best ratio and does not propose a mechanism to automate this selection based on model scale or architecture.
- What evidence would resolve it: A scaling law or heuristic algorithm that correlates specific LLM parameters with the ideal knowledge mixture ratio.

### Open Question 2
- Question: How can fine-grained emotional knowledge be integrated without introducing the noise that caused standard pre-trained emotion models to fail?
- Basis in paper: [explicit] The "Discussion on Emotional Knowledge" notes that integrating knowledge from `distilroberta` and `bert-goemotions` decreased performance, forcing a reliance on coarse-grained polarity.
- Why unresolved: The current method defaults to polarity or COMET-based labels to ensure reliability, effectively sacrificing the granularity of distinct emotion categories.
- What evidence would resolve it: A high-accuracy, fine-grained emotion classification method that improves F1 scores over the current polarity baseline.

### Open Question 3
- Question: Does the reliance on narrative-style causal knowledge (FLAN) limit the method's effectiveness in informal domains like social media?
- Basis in paper: [inferred] Experiments are conducted solely on the NTCIR-13 dataset (English novels), while the method utilizes general causal data that may differ structurally from informal text.
- Why unresolved: Narrative causal structures in novels differ significantly from the fragmented syntax and implicit reasoning found in social media or dialogue.
- What evidence would resolve it: Evaluation of MEKiT on informal, short-text datasets (e.g., Twitter or daily dialogues) to demonstrate domain generality.

## Limitations
- Performance degrades at high causal mixing ratios (>1:10) due to optimization objective drift
- Current method sacrifices fine-grained emotion categories for reliability, defaulting to coarse polarity
- Method validated only on narrative text (novels), limiting generality to informal domains like social media

## Confidence
- Mechanism 1 (Emotional Knowledge): High - Well-supported by empirical results and ablation studies
- Mechanism 2 (Causal Knowledge): High - Clear performance patterns across mixing ratios documented
- Mechanism 3 (Synergy): Medium - Synergistic claims partially supported but lack direct validation in corpus

## Next Checks
1. Verify baseline performance (~57.6% F1) on ECPE task without knowledge injection
2. Test 1:1, 1:2, 1:5, 1:10 causal mixing ratios to find optimal for your specific LLM
3. Run ablation study with emotional-only, causal-only, and combined conditions to validate independent contributions