---
ver: rpa2
title: 'Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual
  Corpora'
arxiv_id: '2509.17855'
source_url: https://arxiv.org/abs/2509.17855
tags:
- term
- german
- inflected
- translation
- bavarian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of processing orthographic dialect
  variation in natural language processing by introducing a novel annotation framework
  called DIALEMMA for creating dialect variation dictionaries from monolingual corpora.
  The authors apply this framework to Bavarian and German, automatically extracting
  and manually annotating 100K word pairs, resulting in 11K direct German-Bavarian
  translation pairs and 7K pairs with inflection differences.
---

# Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora

## Quick Facts
- **arXiv ID:** 2509.17855
- **Source URL:** https://arxiv.org/abs/2509.17855
- **Reference count:** 40
- **Primary result:** Introduced DIALEMMA framework to create Bavarian-German dialect dictionaries from monolingual corpora, achieving macro-F1 of 0.567 with Mistral-123b for translation judgment

## Executive Summary
This paper addresses the challenge of processing orthographic dialect variation in NLP by introducing DIALEMMA, a novel annotation framework for creating dialect variation dictionaries from monolingual corpora. The authors apply this framework to Bavarian and Standard German, automatically extracting and manually annotating 100K word pairs to produce 11K direct translations and 7K inflection variants. They evaluate nine state-of-the-art LLMs on two tasks: judging Bavarian-German word pair relationships and translating Bavarian to German. Results show larger models outperform smaller ones, with Mistral-123b achieving the highest macro-F1 of 0.567. Notably, context improves translation but impairs recognition of dialect variants, highlighting LLMs' limitations with orthographic variation.

## Method Summary
The DIALEMMA framework processes monolingual corpora to build dialect variation dictionaries through five steps: (1) lemmatize German corpus with spaCy, (2) extract vocabularies from both dialect and standard corpora, (3) filter shared tokens, (4) find lexically similar dialect terms using Levenshtein distance (k-nearest neighbors), and (5) human annotation of word pairs as translations, inflected variants, or unrelated forms. For evaluation, nine instruction-tuned LLMs (Mistral-7b/123b, Llama3.1-8b/3.3-70b/4-17b, Aya-8b/32b, Gemma3-12b/27b) perform zero-shot inference on translation judgment and translation tasks using selected prompts from development sets.

## Key Results
- Larger models significantly outperform smaller ones, with Mistral-123b achieving macro-F1 of 0.567 versus Mistral-7b's 0.397
- LLMs perform best on nouns and lexically similar word pairs (Levenshtein distance 1-3)
- Providing context improves translation accuracy but reduces ability to recognize dialect variants (F1 trade-off visible in Figure 3)
- Models struggle most with distinguishing between direct translations and inflected variants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lexical similarity filtering via Levenshtein Distance effectively reduces search space for dialect-to-standard mapping in closely related language pairs
- **Mechanism:** Orthographic dialect variations often manifest as phonetically consistent shifts from the standard, so cognates retain high string similarity. Filtering by low Levenshtein distances isolates viable translation candidates
- **Core assumption:** Dialect and standard share high cognate vocabulary rather than being lexically divergent
- **Evidence anchors:** Step 4 of DIALEMMA uses Levenshtein distance; Figure 2 shows successful "Translation" pairs cluster at LD 1-3

### Mechanism 2
- **Claim:** Model scaling improves robustness of lexical semantic matching against orthographic noise
- **Mechanism:** Larger models internalize robust representations that withstand spelling variations better than smaller models, which default to majority class bias
- **Core assumption:** Models exposed to sufficient dialectal or noisy text during pre-training form robust representations
- **Evidence anchors:** Section 5.3 shows Mistral-7b predicts 'no' 93% of time vs Mistral-123b predicting 'yes' 72% for positive instances

### Mechanism 3
- **Claim:** Contextual grounding aids disambiguation for translation but impairs fine-grained morphological classification
- **Mechanism:** Context provides semantic cues that help resolve meaning for translation but biases models toward semantic "lemma" rather than strict morphological adherence required by annotation guidelines
- **Core assumption:** Models prioritize semantic coherence over strict morphological adherence when context is present
- **Evidence anchors:** Abstract notes context improves translation but reduces variant recognition; Figure 3 visualizes this trade-off

## Foundational Learning

- **Concept:** **Levenshtein Distance (Edit Distance)**
  - **Why needed here:** Statistical filter to identify potential translation candidates before annotation; explains why "zweisprachig" and "zwaasprochig" are linked
  - **Quick check question:** If distance threshold is set too low (e.g., 1), will system capture valid dialectal shifts like "a" -> "å", or miss them?

- **Concept:** **Morphological Inflection vs. Lemma**
  - **Why needed here:** DIALEMMA dataset distinguishes "Direct Translation" (Lemma-to-Lemma) from "Inflected Variant" (Lemma-to-Form); annotators must understand German morphology
  - **Quick check question:** Should Bavarian "Häusl" (little house) be treated as direct translation of "Haus" (house) or distinct lexical entry?

- **Concept:** **Macro-F1 Score**
  - **Why needed here:** Dataset heavily skewed (mostly 'no' pairs); accuracy misleading (model predicting 'no' always gets 80%+ accuracy); Macro-F1 measures performance on minority classes
  - **Quick check question:** Why would model with 95% accuracy be considered failure for building translation dictionary?

## Architecture Onboarding

- **Component map:** Corpus Ingestion -> Preprocessing -> Candidate Generator -> Annotator Interface -> Evaluator
- **Critical path:** Candidate Generator - if Levenshtein threshold too tight, miss dialectal variants; if too loose, flood annotators with noise
- **Design tradeoffs:**
  - Monolingual vs. Parallel Data: Chooses monolingual corpora + string matching to avoid parallel dialect-standard scarcity; tradeoff is inability to capture lexical semantic shifts without manual intervention
  - Human vs. LLM Annotator: LLMs faster but struggle with 'Inflected' vs. 'Yes' distinction; humans accurate but slow
- **Failure signatures:**
  - Majority Class Collapse: Model predicts "No" for everything (common in smaller models like Mistral-7b)
  - Instruction Following Error (IF Error): Model hallucinates or chats instead of returning requested label
- **First 3 experiments:**
  1. Threshold Sweep: Run DIALEMMA varying Levenshtein Distance threshold (k=1 to 5) to measure Precision/Recall trade-off
  2. Prompt Sensitivity Check: Replicate "English vs. German prompt" experiment on translation task
  3. Morphology Stress Test: Isolate "Inflected" class and test specifically on suffix-heavy categories (Verbs/Adjectives)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can fine-tuning LLMs on DIALEMMA dataset significantly improve ability to distinguish direct dialect translations from inflected variants? (Basis: Conclusion states authors plan to fine-tune LLMs for identifying translation pairs)
- **Open Question 2:** Why does inclusion of local context improve translation performance while degrading model's ability to judge translation candidates? (Basis: Limitations section notes need to systematically analyze role of contextual information)
- **Open Question 3:** Does DIALEMMA framework generalize to other low-resource language pairs and dialects outside Bavarian-German case study? (Basis: Limitations lists focus on Bavarian as single case study as main limitation)

## Limitations

- Framework relies on orthographic similarity as proxy for semantic equivalence, which works for closely related pairs like Bavarian-German but may fail for dialects with substantial lexical divergence
- Heavy class imbalance (predominantly 'no' pairs) means model performance on minority classes remains relatively weak despite modest improvements in macro-F1
- Exclusive focus on German-Bavarian pairs raises questions about generalizability to language pairs with different orthographic conventions or degrees of lexical similarity

## Confidence

- **High Confidence:** Larger models outperform smaller ones in handling orthographic dialect variation; context improves translation but impairs morphological classification judgment
- **Medium Confidence:** Lexical similarity filtering effectively reduces search space assumes orthographic distance correlates with semantic similarity; LLMs' struggle with distinguishing translations from inflected variants is supported but could benefit from more granular analysis
- **Low Confidence:** Generalizability of DIALEMMA to other language pairs beyond closely related Germanic varieties remains largely untested; specific Levenshtein distance thresholds and k-nearest neighbor parameters may not be optimal

## Next Checks

1. Apply DIALEMMA framework to language pair with less orthographic similarity (e.g., Swiss German to Standard German or Romance dialect to standard) to test limits of lexical similarity filtering mechanism
2. Conduct detailed error analysis specifically on 'Inflected' class to determine whether failures stem from tokenization issues, suffix splitting problems, or genuine morphological ambiguity
3. Systematically vary context window size and content in translation task to find optimal trade-off between semantic disambiguation and morphological classification accuracy