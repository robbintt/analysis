---
ver: rpa2
title: 'Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification
  for Fact-Checking'
arxiv_id: '2512.00267'
source_url: https://arxiv.org/abs/2512.00267
tags:
- verification
- node
- claim
- search
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trification, a novel automated fact-checking
  framework that addresses the limitations of existing LLM-based agents by generating
  a comprehensive set of verification actions and structuring them into a dependency
  graph. The system employs a tree-based strategy planner to ensure complete coverage
  of complex claims, with nodes representing search, refine, think, and judge operations.
---

# Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking

## Quick Facts
- **arXiv ID**: 2512.00267
- **Source URL**: https://arxiv.org/abs/2512.00267
- **Reference count**: 9
- **Primary result**: Tree-based strategy planner with dependency graph achieves 70.63% average Macro-F1, outperforming prior SOTA by 2.23 percentage points on FEVEROUS and HOVER benchmarks.

## Executive Summary
This paper introduces Trification, a novel automated fact-checking framework that addresses the limitations of existing LLM-based agents by generating a comprehensive set of verification actions and structuring them into a dependency graph. The system employs a tree-based strategy planner to ensure complete coverage of complex claims, with nodes representing search, refine, think, and judge operations. The dependency graph enables concurrent execution of independent tasks and supports dynamic modification when verification fails, allowing the system to adapt its strategy. Experiments on FEVEROUS and HOVER benchmarks demonstrate significant improvements over state-of-the-art methods, with Trification achieving 70.63% average macro-F1 compared to 68.40% for the previous best method.

## Method Summary
Trification uses a tree-based strategy planner to generate a comprehensive verification plan represented as a directed acyclic graph (DAG) with four node types: SEARCH (evidence retrieval), REFINE (disambiguation), THINK (intermediate reasoning), and JUDGE (final verdict). The system executes nodes in topological order with parallel processing for independent tasks. When THINK or JUDGE nodes detect insufficient evidence, the dynamic modification mode generates and integrates new verification sub-graphs. The framework operates in two modes: Static Mode using Wiki BM25 retrieval for efficiency, and Dynamic Mode using live search engines with up to 3 graph modifications for challenging claims.

## Key Results
- Trification achieves 70.63% average Macro-F1 across FEVEROUS and HOVER benchmarks, outperforming previous SOTA (68.40%) by 2.23 percentage points.
- Dynamic planning mode shows 40-60% improvements on claims the static mode cannot verify, with 1.92-2.13 average graph modifications per claim.
- Ablation studies confirm REFINE and THINK nodes are essential, with their removal dropping Macro-F1 from 70.63% to 61.63%.

## Why This Works (Mechanism)

### Mechanism 1: Upfront Comprehensive Action Generation
Generating a complete set of verification actions before execution reduces the risk of overlooking implicit or short sub-claims within complex statements. A tree planner produces a full set of verification actions covering all claim components, structured as a directed acyclic graph (DAG), rather than generating actions sequentially which is prone to premature termination. The core assumption is that claims contain multiple verifiable components that can be identified before evidence gathering begins. Weak direct validation exists in related work, but Trification's ablation study shows substantial performance drops when removing intermediate nodes.

### Mechanism 2: Dependency Graph with Intermediate Logical Verification
Structuring actions as a DAG with explicit dependencies enables intermediate logical verification between dependent actions and improves reasoning coherence. Four node types (SEARCH, REFINE, THINK, JUDGE) with explicit dependency edges allow evidence to propagate from parent to child nodes, with REFINE nodes disambiguating queries using parent context and THINK nodes synthesizing multiple parent outputs before passing downstream. The ablation study confirms these intermediate nodes contribute substantively, with their removal dropping Macro-F1 by 9 points.

### Mechanism 3: Dynamic Sub-Tree Generation on Failure
When a THINK or JUDGE node determines evidence is insufficient, generating and integrating a new verification sub-graph improves handling of challenging claims. On failure detection, the system prompts the LLM to generate a new sub-tree targeting missing information, which is integrated into the original graph. Dynamic mode averaged 1.92-2.13 graph modifications per claim and achieved 40-60% Macro-F1 improvements on hard claims the static mode could not verify.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) and Topological Sorting**: The verification plan is represented as a DAG; execution follows topological order with parallel scheduling for nodes whose dependencies are satisfied. *Quick check*: Given nodes A→B, A→C, B→D, C→D, which nodes can execute concurrently?

- **Multi-hop Reasoning**: HOVER and FEVEROUS benchmarks require combining evidence across multiple documents; REFINE and THINK nodes are specifically designed to handle this complexity. *Quick check*: For the claim "X attended university Y, whose chancellor was Z," how many retrieval steps are minimally required?

- **Evidence Propagation in Graph Structures**: Each node receives outputs from its parent nodes; REFINE nodes use parent context to disambiguate queries; THINK nodes synthesize parent evidence. *Quick check*: If a REFINE node has two SEARCH parents, what information should it use to resolve ambiguity?

## Architecture Onboarding

- **Component map**: Tree Planner → DAG with SEARCH, REFINE, THINK, JUDGE nodes → Node Executor (parallel topological execution) → Evidence Propagator (parent-to-child data flow) → Dynamic Modifier (sub-tree generation on failure) → Verdict Aggregator (final JUDGE output)

- **Critical path**: Input claim → Tree Planner generates DAG → Topological execution: SEARCH nodes retrieve evidence in parallel → REFINE nodes disambiguate using parent context → THINK nodes synthesize evidence; if insufficient, trigger dynamic modification → JUDGE node renders final verdict; if uncertain, trigger dynamic modification

- **Design tradeoffs**: Static vs. Dynamic mode (Wiki BM25 vs. live search, lower cost vs. better hard claims); Upfront planning vs. iterative (coverage vs. potential over-generation); Parallel execution vs. dependency complexity (reduced latency vs. careful dependency modeling)

- **Failure signatures**: Search Engine Contradictory Evidence (70% of errors - retrieved evidence contradicts ground truth); Search Engine Incomplete Evidence (partial evidence retrieved); Ambiguous Claims (30% - temporal or quantitative ambiguity confuses planner or judge)

- **First 3 experiments**: 1) Replicate static mode results on HOVER 2-hop subset using Wiki BM25 retrieval; verify Macro-F1 in 74-76% range. 2) Ablate REFINE nodes only; expect ~4-5% Macro-F1 drop. 3) On 20 claims where static mode fails, enable dynamic mode with DDGS; verify graph modification triggers and measure improvement.

## Open Questions the Paper Calls Out

- **Handling conflicting search results**: The framework lacks mechanisms to detect and reconcile contradictory evidence retrieved by the search engine, which accounts for 70% of failures. A comparative study showing improved Macro-F1 when integrating conflict-resolution modules would be valuable.

- **Resolving temporal and quantitative ambiguities**: The current REFINE node struggles with structural semantic ambiguities regarding time and quantity, contributing to 30% of failures. Successful verification of specific ambiguous failure cases using enhanced temporal-reasoning components would demonstrate progress.

- **Optimizing dynamic planning computational cost**: Dynamic modification improves accuracy but requires significant computational overhead, limiting evaluation to 150 claims. Demonstration of complete dataset evaluation with constrained latency comparable to static mode would validate scalability.

## Limitations

- **Unspecified LLM backbone**: The paper does not identify which LLM model is used, making it impossible to determine whether results are model-dependent or reproducible with different LLMs.

- **Weak validation of upfront planning benefits**: The claim that upfront generation reduces premature termination is supported only by weak evidence from related work, lacking direct empirical comparison with sequential generation approaches.

- **Resource-intensive dynamic mode**: While dynamic modification improves accuracy on hard claims, the computational overhead limits scalability and prevents full dataset evaluation.

## Confidence

- **High confidence**: Architectural contribution of combining upfront comprehensive planning with dependency graph execution is well-supported by 9-point Macro-F1 drops in ablation studies.
- **Medium confidence**: Dynamic modification improves performance on hard claims, but unspecified "insufficient evidence" detection mechanism makes it difficult to assess whether improvements are due to the strategy itself or better evidence retrieval.
- **Low confidence**: Claim that upfront generation reduces premature termination is weakly supported, as related work does not empirically validate upfront vs. sequential generation.

## Next Checks

1. **Reproduce static mode results on HOVER 2-hop subset using Wiki BM25 retrieval** to verify Macro-F1 performance claims (74-76% range).
2. **Implement and test the dynamic modification mechanism** on 20 claims where static mode fails, measuring graph modification triggers and accuracy improvements to validate the 40-60% improvement claims.
3. **Evaluate claim decomposition quality** by comparing upfront vs. sequential generation approaches on a held-out set to directly test whether upfront planning reduces premature termination and improves coverage.