---
ver: rpa2
title: 'Scalable Transit Delay Prediction at City Scale: A Systematic Approach with
  Multi-Resolution Feature Engineering and Deep Learning'
arxiv_id: '2601.18521'
source_url: https://arxiv.org/abs/2601.18521
tags:
- feature
- data
- delay
- prediction
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling transit delay prediction
  to city-wide bus networks, tackling issues of feature engineering, computational
  efficiency, and data imbalance. The authors propose a systematic pipeline combining
  multi-resolution feature engineering using Uber's H3 geospatial indexing, dimensionality
  reduction with Adaptive PCA, and a hybrid H3+topology clustering strategy to balance
  data distribution.
---

# Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning

## Quick Facts
- arXiv ID: 2601.18521
- Source URL: https://arxiv.org/abs/2601.18521
- Reference count: 40
- Primary result: LSTM with cluster-aware features achieves R²=0.662 for trip-level delay prediction, outperforming transformers by 18-52% while using 275× fewer parameters

## Executive Summary
This paper addresses the challenge of scaling transit delay prediction to city-wide bus networks, tackling issues of feature engineering, computational efficiency, and data imbalance. The authors propose a systematic pipeline combining multi-resolution feature engineering using Uber's H3 geospatial indexing, dimensionality reduction with Adaptive PCA, and a hybrid H3+topology clustering strategy to balance data distribution. They compare five deep learning architectures (LSTM, XGBoost, xLSTM, PatchTST, Autoformer) on STM Montreal data, with walk-forward temporal validation across elementary segment, segment, and trip levels. LSTM with cluster-aware features achieves the best accuracy (R²=0.662) and outperforms transformer models by 18-52% while using 275× fewer parameters. The system demonstrates practical deployment potential through hierarchical aggregation, multi-level evaluation, and explicit latency/resource profiling, providing a reusable framework for other urban transit networks.

## Method Summary
The authors present a systematic pipeline for city-scale transit delay prediction that addresses key scalability challenges. The approach begins with multi-resolution feature engineering using H3 geospatial indexing to create 1,683 spatiotemporal features from GTFS and GTFS-RT data. To handle computational efficiency and data imbalance, they employ Adaptive PCA to compress features to 83 components (95% variance) and implement a hybrid H3+topology clustering method that groups routes based on both spatial overlap and shared route segments. The system uses a global LSTM model with cluster ID as a categorical feature, trained via walk-forward temporal cross-validation. Predictions are made at the elementary segment level (pace in seconds/meter) and hierarchically aggregated to segment and trip levels, with comprehensive evaluation across all three granularities.

## Key Results
- LSTM with cluster-aware features achieves R²=0.662 for trip-level delay prediction, outperforming transformer models by 18-52% while using 275× fewer parameters
- Hybrid H3+topology clustering reduces data imbalance with coefficient of variation 0.608 compared to CV>2.0 for naive partitioning
- Adaptive PCA compression enables global model training by reducing 1,683 features to 83 components while preserving 95% of variance
- System demonstrates practical deployment potential with hierarchical aggregation, multi-level evaluation, and explicit latency/resource profiling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hybrid H3+topology clustering reduces cluster imbalance compared to naive geographic partitioning.
- **Mechanism**: Weighted Jaccard similarity combines spatial overlap via H3 hexagons with topological overlap via shared route segments. Three-stage algorithm: (1) coarse H3-based geographic clustering, (2) topology-aware subdivision of giant clusters using route connectivity graphs, (3) rebalancing to meet size constraints.
- **Core assumption**: Routes with similar spatial footprints AND shared segments exhibit similar delay dynamics.
- **Evidence anchors**:
  - [abstract]: "hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608)"
  - [Section 4.5.2]: Reports CV=0.608 and imbalance ratio 1.90× vs CV>2.0 for naive partitioning
  - [corpus]: Weak direct corpus support; related transit papers don't address this clustering problem
- **Break condition**: Routes serving same geography but different operational patterns (express vs local, peak-only vs all-day) may cluster incorrectly if topological similarity is insufficient.

### Mechanism 2
- **Claim**: Adaptive PCA compression enables global model training while preserving predictive information.
- **Mechanism**: Generate 1,683 spatiotemporal features via systematic aggregation (23 combinations × 9 statistics), then compress to 83 components retaining 95% variance. This reduces memory/compute burden while preserving the signal learned by downstream models.
- **Core assumption**: Essential delay patterns lie in a lower-dimensional linear subspace.
- **Evidence anchors**:
  - [abstract]: "compresses them into 83 components using Adaptive PCA while preserving 95% of the variance"
  - [Section 3.3]: States Adaptive PCA selected after evaluating 20 reduction methods
  - [corpus]: No direct corpus evidence on this specific compression ratio
- **Break condition**: If feature interactions are highly non-linear, linear compression may lose predictive signal; PCA performance would degrade on networks with qualitatively different delay patterns.

### Mechanism 3
- **Claim**: LSTM with cluster-aware features achieves superior accuracy-efficiency trade-off vs. transformers on this task.
- **Mechanism**: Cluster ID as categorical feature lets a single global LSTM learn cluster-specific patterns without separate models. Bidirectional attention captures short-range temporal dependencies in stop-to-stop sequences. Compact parameter count (31K) prevents overfitting on 6-month data.
- **Core assumption**: Stop-to-stop delay propagation exhibits short-range temporal dependencies amenable to recurrent modeling.
- **Evidence anchors**:
  - [abstract]: "LSTM with cluster-aware features achieves the best trade-off...outperforming transformer models by 18 to 52% while using 275 times fewer parameters"
  - [Section 4.5.4]: R²=0.662 for LSTM vs. 0.566 (PatchTST) and 0.435 (Autoformer); trip-level RMSE 1.85 min
  - [corpus]: Related paper "Real-Time Bus Departure Prediction Using Neural Networks" supports neural approaches but doesn't compare LSTM vs. transformers
- **Break condition**: If longer-range temporal patterns (e.g., morning delays affecting afternoon) dominate, LSTM may underperform; the paper notes xLSTM with extended memory didn't improve results, suggesting such dependencies may be weak in this dataset.

## Foundational Learning

- **Concept**: H3 hexagonal hierarchical indexing
  - **Why needed here**: Core spatial representation enabling multi-resolution aggregation at resolutions 9-10 (~174m to ~66m cells) that align with transit operational scales.
  - **Quick check question**: Can you explain why hexagons are preferred over square grids for spatial indexing in transit applications?

- **Concept**: Walk-forward temporal cross-validation
  - **Why needed here**: Prevents temporal leakage that would overestimate real-world performance; uses rolling time windows instead of random splits.
  - **Quick check question**: Why does random k-fold CV produce optimistic bias for time-series data?

- **Concept**: Pace (seconds/meter) vs. raw travel time
  - **Why needed here**: Normalizes predictions across segments of varying lengths, reducing target variance.
  - **Quick check question**: Given pace predictions and segment distances, how do you compute predicted delay?

## Architecture Onboarding

- **Component map**:
  Data collection -> Feature engineering -> Clustering -> Dimensionality reduction -> Model training -> Inference/Hierarchical aggregation

- **Critical path**: Clustering configuration (H3 resolution, weight w_spatial, number of clusters) -> feature engineering -> model selection. Errors in clustering propagate through entire pipeline.

- **Design tradeoffs**:
  - Global model + cluster features vs. per-cluster models: Paper chooses former (simpler deployment, single model to maintain); trade-off is potential loss of cluster-specific optimization.
  - LSTM vs. transformers: LSTM chosen for accuracy and latency; transformers offer theoretical long-range modeling but overfit here.
  - H3 resolution 7 vs. 8 vs. 9: Resolution 7 used for clustering (5km² hexagons), 9-10 for features; finer clustering increases cluster count and imbalance.

- **Failure signatures**:
  - Giant cluster reappears: Single cluster contains >40% of data -> check spatial weight and topology similarity computation.
  - Transformer outperforms LSTM significantly: May indicate longer-range dependencies or data regime change; re-evaluate sequence length.
  - Trip-level RMSE doesn't improve over segment-level: Error cancellation failing -> check aggregation logic for bugs.

- **First 3 experiments**:
  1. **Clustering validation**: Apply hybrid clustering to your network; verify CV < 1.0 and imbalance ratio < 3× before proceeding. If CV > 2.0, adjust spatial weight and H3 resolution.
  2. **Feature ablation**: Train LSTM with all 1,683 features (no PCA) vs. 83 PCA components; expect similar accuracy with massive compute increase. If PCA hurts significantly, check variance threshold.
  3. **Architecture comparison**: Train LSTM, XGBoost, and one transformer on identical data splits; if transformer wins by >10% R², reconsider LSTM choice—dataset characteristics may differ from Montréal.

## Open Questions the Paper Calls Out

- **Cross-network generalization**: The framework's performance on diverse transit topologies and modes (e.g., rail) remains untested. The authors note validation was limited to a single network and "requires generalization studies across diverse contexts, including... rail systems."

- **Weather data resolution**: Current coarse weather data (daily city-wide) misses microclimates and sudden weather impacts. The authors suggest "1km radar could improve extreme weather performance" but haven't tested this hypothesis.

- **Graph Neural Networks**: While GNNs could model cross-route delay propagation, they "face scalability challenges" at city scale. The current approach excludes explicit cross-route propagation, leaving the trade-off between GNN spatial modeling and computational efficiency unresolved.

## Limitations

- **Network-specific validation**: Results are based on a single transit network (STM Montreal), raising questions about generalizability to different urban topologies, operational patterns, or transit modes like rail.

- **Computational scalability**: While the framework demonstrates efficiency for Montréal's 196 routes, scalability to megacities with thousands of routes and the computational complexity of weighted Jaccard similarity for large networks remain untested.

- **Linear dimensionality assumptions**: Adaptive PCA's effectiveness relies on linear subspace assumptions that may not hold for all transit networks, particularly those with highly non-linear delay patterns or qualitatively different operational characteristics.

## Confidence

- **High confidence**: Clustering methodology (CV=0.608 vs naive CV>2.0), LSTM vs transformer performance comparison (18-52% improvement), and walk-forward validation approach are well-supported by experimental results and align with established best practices.

- **Medium confidence**: Specific PCA compression ratio (83 components from 1,683 features), exact feature engineering combinations, and transferability to other transit networks rely heavily on STM-specific data characteristics without extensive cross-network validation.

- **Low confidence**: Claims about computational efficiency (275× fewer parameters) and specific architecture choices (LSTM over xLSTM) lack broader context about how these would generalize to networks with different operational patterns or delay characteristics.

## Next Checks

1. **Cross-network clustering validation**: Apply the hybrid H3+topology clustering to at least two additional transit networks (different cities/regions) and verify that the method consistently achieves CV < 1.0 and imbalance ratio < 3×. Document how H3 resolution and spatial weight parameters need adjustment across networks.

2. **PCA sensitivity analysis**: Systematically vary the variance retention threshold (90%, 95%, 99%) and document the impact on prediction accuracy, training time, and memory usage. Identify the point where accuracy degradation begins to outweigh computational benefits.

3. **Temporal dependency stress test**: Create synthetic datasets with known long-range temporal patterns (e.g., morning delays affecting afternoon service) and evaluate whether LSTM performance degrades compared to transformers. This would validate the paper's assumption about short-range dominance in real-world transit data.