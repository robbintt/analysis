---
ver: rpa2
title: 'Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on Authorship
  Verification'
arxiv_id: '2503.19099'
source_url: https://arxiv.org/abs/2503.19099
tags:
- authorship
- author
- obfuscation
- attacks
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluated the robustness of an authorship verification
  model to two types of adversarial attacks: authorship obfuscation and authorship
  impersonation. The study used several large language models, including Mistral and
  DIPPER, to perform these attacks on a high-performing authorship verification model.'
---

# Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on Authorship Verification

## Quick Facts
- arXiv ID: 2503.19099
- Source URL: https://arxiv.org/abs/2503.19099
- Reference count: 40
- High-performing authorship verification model successfully fooled with up to 92% attack success rate

## Executive Summary
This study evaluates the robustness of an authorship verification model to two adversarial attacks: authorship obfuscation and authorship impersonation. Using large language models including Mistral and DIPPER, the researchers successfully flipped accurate same-author predictions to different-author predictions with a maximum attack success rate of 92%, and made target author writing styles indistinguishable from source authors with a maximum attack success rate of 78%. The results demonstrate significant vulnerabilities in authorship verification systems to LLM-based adversarial attacks while maintaining semantic preservation.

## Method Summary
The study targets a BigBird-based authorship verification model using two attack strategies on PAN20 FanFiction and CelebTwitter datasets. Obfuscation attacks use PEGASUS, DIPPER (T5-XXL), and Mistral-7B-Instruct to rewrite text while preserving semantics, measured by BLEU, ROUGE, and BERTScore. Impersonation attacks employ a RAG pipeline with Mistral-7B and a GPT-2-based STRAP model to extract and apply target author styles. Attack success is measured as the rate of flipping predictions at the Equal Error Rate threshold (0.29).

## Key Results
- Obfuscation attacks achieved maximum 92% attack success rate using Mistral_zeroshot
- Impersonation attacks achieved maximum 78% attack success rate
- Only 50-60% document paraphrasing required for AV model degradation
- DIPPER achieved best semantic preservation while Mistral achieved highest ASR

## Why This Works (Mechanism)

### Mechanism 1
Paraphrasing-based obfuscation disrupts stylometric features that AV models rely on for author identification. LLMs rewrite text to alter lexical patterns, sentence structure, and stylistic markers while preserving semantics, causing the BigBird AV model to flip accurate "True Trial" predictions to "False Trial." Core assumption: AV models depend primarily on surface-level stylometric features alterable without changing semantic meaning.

### Mechanism 2
RAG-enhanced style extraction enables targeted impersonation by retrieving and applying specific authorial patterns. A multi-step RAG pipeline first extracts stylistic descriptions from target author texts using chain-of-thought prompting, then uses these descriptions to guide Mistral in rewriting source author content while adopting target author style characteristics.

### Mechanism 3
Partial text modification (50-60% threshold) is necessary for reliable AV model misclassification. Ablation studies show AV models remain robust to low/medium obfuscation levels; performance degrades only when approximately half the document is paraphrased, indicating redundancy in stylometric signals.

## Foundational Learning

- **Authorship Verification vs. Attribution**: The paper targets verification (same-author binary classification), not attribution (multi-author identification); attack success metrics depend on this distinction. *Quick check: Can you explain why obfuscation success is measured as flipping "True Trial → False Trial" rather than misattribution to a specific author?*

- **Semantic Preservation Metrics (BLEU, ROUGE, BERTScore)**: Attack success is jointly optimized with semantic consistency; high ASR with low semantic scores indicates gibberish outputs, not useful attacks. *Quick check: Why would an attack achieving 99% ASR but 0.1 BERTScore be considered a failed experiment?*

- **Equal Error Rate (EER) Operating Points**: The AV model's classification threshold (0.29) was set at EER to balance false alarms and misses before attacks; this baseline is critical for interpreting ASR. *Quick check: How would using a different threshold (e.g., 0.5) affect the reported attack success rates?*

## Architecture Onboarding

- **Component map**: Target AV Model (BigBird) <- Obfuscation Attackers (DIPPER, Mistral, PEGASUS) <- Impersonation Attackers (Mistral+RAG, STRAP)

- **Critical path**: Load BigBird AV model and datasets → Generate obfuscated/impostered texts using attack models → Compute AV model predictions, calculate ASR at EER threshold → Score semantic preservation to filter low-quality attacks

- **Design tradeoffs**: DIPPER achieves best semantic preservation but lower ASR on social media data; Mistral achieves higher ASR but more semantic drift; Impersonation requires fewer in-context examples (3 stories optimal) than expected

- **Failure signatures**: PEGASUS baseline achieves only 6% ASR despite being a paraphraser; CelebTwitter impersonation excluded due to potential LLM training data leakage; Low semantic scores (<0.6 BERTScore) indicate attack produced incoherent text

- **First 3 experiments**: 1) Reproduce Mistral_zeroshot obfuscation on PAN20 FanFiction subset (target: ~83% ASR, BERTScore >0.75); 2) Run DIPPER obfuscation ablation across 20%, 40%, 60%, 80%, 100% paraphrasing percentages to validate 50-60% threshold; 3) Implement minimal RAG impersonation pipeline with 3 target author stories, measure ASR against held-out test pairs

## Open Questions the Paper Calls Out

- **Strategic text selection for obfuscation**: Would identifying and targeting the most stylistically "impactful" portions of a document yield higher obfuscation success rates than random paraphrasing selection?

- **PEFT and agent-based approaches**: To what extent can Parameter-Efficient Fine-Tuning (PEFT) or agent-based approaches improve the success rate of authorship impersonation attacks over the current RAG-based method?

- **Multilingual and code attribution robustness**: How robust are these adversarial attack strategies across multilingual datasets and disparate domains such as source code attribution?

- **Evasion of AI-generated text detectors**: Can LLM-based obfuscation techniques successfully evade machine-generated text detectors while maintaining high attack success rates against authorship verification models?

## Limitations

- Results may not generalize beyond English datasets (FanFiction, Twitter) with ~250 token texts
- Semantic preservation was measured but not optimized during attack generation
- Only one AV architecture (BigBird) was tested, limiting transferability conclusions
- Closed-set verification scenario assumed, unlike many real-world open-set applications

## Confidence

**High Confidence**: Paraphrasing disrupts stylometric features (consistent ASR results across multiple LLMs); RAG enables targeted style transfer (validated through pipeline implementation); 50-60% modification threshold (directly measured through ablation studies).

**Medium Confidence**: Generalization across domains (only two datasets tested); Semantic preservation as quality metric (relationship with attack utility remains qualitative); Optimal in-context learning size (3 stories found effective but not systematically varied).

**Low Confidence**: Attack transferability to other AV architectures (only BigBird tested); Real-world attacker capabilities (assumes access to target author samples and system parameters).

## Next Checks

1. **Cross-architecture validation**: Test the same attack methodology against at least two additional authorship verification architectures to assess whether attack success rates remain consistently high.

2. **Semantic preservation optimization**: Implement an attack generation framework that jointly optimizes for ASR and semantic consistency, then measure whether high-quality attacks (>0.75 BERTScore) can still achieve >80% ASR.

3. **Open-set scenario testing**: Design experiments where attackers have limited or no access to target author samples, instead using only publicly available stylistic information or synthetic style generation, to evaluate attack feasibility under realistic constraints.