---
ver: rpa2
title: 'Data Distribution Matters: A Data-Centric Perspective on Context Compression
  for Large Language Model'
arxiv_id: '2602.01778'
source_url: https://arxiv.org/abs/2602.01778
tags:
- data
- compression
- decoder
- encoder
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper adopts a data-centric perspective to investigate how
  data distribution impacts context compression for LLMs. Using an autoencoder framework
  trained from scratch, the authors systematically analyze the roles of input data
  entropy and intrinsic data (pretrained knowledge) on compression quality.
---

# Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model

## Quick Facts
- arXiv ID: 2602.01778
- Source URL: https://arxiv.org/abs/2602.01778
- Reference count: 35
- Authors find that input data entropy and decoder-encoder intrinsic data distribution gaps critically impact context compression quality

## Executive Summary
This paper investigates context compression for large language models from a data-centric perspective, revealing that data distribution characteristics fundamentally constrain compression quality. Through systematic experiments with autoencoder frameworks trained from scratch, the authors demonstrate that input entropy measured by the encoder negatively correlates with compression quality, while decoder-encoder intrinsic data mismatches significantly diminish compression gains. The work provides two practical guidelines: prioritize alignment with the decoder's intrinsic data distribution over pure scaling for compute efficiency, and prefer a larger decoder over a complex encoder when resources are constrained.

## Method Summary
The authors build a controlled autoencoder framework using LLM backbones pretrained from scratch on six distinct distributions (web text, code, math, etc.). The system compresses context into k=128 memory slots at r=4 compression ratio, with a frozen decoder backbone and a fine-tuned encoder backbone connected via a linear projection layer. Experiments systematically vary input entropy levels, decoder-encoder distribution alignment, and model scales (200M-1B parameters) to isolate the effects of data distribution on compression quality. Reconstruction fidelity is evaluated using F1, BLEU, and ROUGE-L metrics.

## Key Results
- Encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship
- The gap between encoder and decoder intrinsic data distributions significantly diminishes compression gains
- Prioritizing alignment with the decoder's intrinsic data distribution yields better performance than pure scaling for compute efficiency

## Why This Works (Mechanism)

### Mechanism 1: Encoder-Side Entropy Predicts Compression Difficulty
- Under a fixed compression budget (k=128 memory slots with r=4 compression ratio), higher-entropy sources require a larger rate to achieve comparable fidelity per rate-distortion theory. When input entropy exceeds the effective rate budget, information loss becomes unavoidable.

### Mechanism 2: Decoder Intrinsic Distribution Dominates Alignment Priority
- The frozen decoder's reachable conditional family (FD) is determined by its pretraining distribution. Aligning pdata with FD directly reduces the KL divergence term in the reconstruction loss, whereas encoder alignment does not change FD.

### Mechanism 3: Scaling Provides Diminishing Returns Under Distribution Mismatch
- Larger decoder capacity expands FD, reducing the mismatch term. However, under fixed compression budget, a large intrinsic gap keeps the mismatch term substantial, yielding diminishing returns from further scaling.

## Foundational Learning

- **Rate-Distortion Theory**
  - Why needed here: Explains why higher-entropy inputs are harder to compress under fixed budgets (Appendix B.1)
  - Quick check question: Given a fixed compression rate R=128 slots, can you reconstruct losslessly if input entropy H(X) > R?

- **Information Entropy (Shannon)**
  - Why needed here: Used as the unified metric for input data complexity across domains; encoder-measured entropy predicts compression difficulty
  - Quick check question: Why might encoder-perceived entropy differ from decoder-perceived entropy for the same input?

- **Autoencoder Reconstruction Loss Decomposition**
  - Why needed here: Equation (7) decomposes loss into H(pdata) + DKL(pdata || q), separating intrinsic complexity from distribution mismatch
  - Quick check question: Which term in the decomposition does decoder alignment reduce? Which term does input entropy affect?

## Architecture Onboarding

- **Component map**: Encoder backbone -> Linear projection layer -> Decoder backbone
- **Critical path**: 
  1. Pretrain encoder and decoder separately on controlled distributions (Di â‰  Dj for mismatch experiments)
  2. Couple via linear projection layer
  3. Fine-tune only encoder on compression task while keeping decoder frozen
  4. Evaluate reconstruction fidelity using F1, BLEU, ROUGE-L

- **Design tradeoffs**:
  - Decoder size vs. encoder size: Larger decoder yields better returns under mismatch (Figure 6)
  - Alignment vs. scaling: Distribution alignment is more parameter-efficient than scaling (Table 4, Figure 5)
  - Compression ratio vs. fidelity: Fixed at r=4, k=128; higher ratios would increase distortion

- **Failure signatures**:
  - Near-zero reconstruction when decoder pretrained on random text outputs mirror noise distribution (Table 5)
  - Persistent degradation under mismatch: BLEU drops from 42.12 (aligned) to 27.02-34.65 (misaligned) in Table 3
  - Entropy-related degradation: High-entropy inputs show systematic F1 decline (Figure 3a)

- **First 3 experiments**:
  1. Sample inputs from D1-D6, compute encoder-perceived entropy, measure F1. Confirm negative correlation.
  2. Fix encoder on D1, decoder on D6. Fine-tune on D1 data. Swap: encoder on D6, decoder on D1, fine-tune on D1. Compare F1/BLEU to verify decoder alignment priority.
  3. Compare 500M-500M aligned pair against 500M-1B misaligned pair. Verify that alignment outperforms 2x decoder scaling under mismatch.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the findings regarding decoder-alignment dominance and entropy correlations hold when compression quality is measured by downstream task performance (e.g., RAG accuracy) rather than reconstruction fidelity?
- Basis in paper: The paper evaluates compression quality exclusively using autoencoder reconstruction metrics while motivating the work with downstream applications like RAG and ICL
- Why unresolved: Reconstruction fidelity measures information retention, but it is unclear if the specific entropy-correlations observed translate linearly to utility in reasoning or retrieval tasks
- What evidence would resolve it: Experiments correlating input entropy and intrinsic data gaps against end-task metrics rather than token-overlap reconstruction scores

### Open Question 2
- Question: Can the decoder-alignment bottleneck be overcome through specialized training strategies, or is distribution mismatch a fundamental limit?
- Basis in paper: The abstract states that the intrinsic data gap "significantly diminishes compression gains, which is hard to mitigate"
- Why unresolved: The authors demonstrate that scaling is inefficient compared to alignment, but do not explore if advanced techniques could effectively "translate" between mismatched intrinsic domains
- What evidence would resolve it: A study showing that a specific intervention allows an encoder and decoder with large intrinsic data gaps to achieve compression quality comparable to an aligned pair

### Open Question 3
- Question: How do the guidelines regarding encoder vs. decoder intrinsic data apply to large-scale, opaque models where the pretraining distribution is unknown or heterogeneous?
- Basis in paper: The methodology notes that "proprietary opacity of pre-training corpora in models such as Qwen precludes a granular understanding"
- Why unresolved: Findings are derived from controlled environments with 1B parameter models trained on specific, distinct distributions; it is unclear if the "frozen decoder" bottleneck persists identically in massive models
- What evidence would resolve it: Replicating the encoder-decoder mismatch experiments using large-scale open-weight models where the pretraining mix is fixed but unknown

## Limitations

- Limited generalization across compression ratios - findings may not hold at different compression budgets
- Static decoder assumption - all experiments freeze the decoder, making decoder alignment superiority dependent on this constraint
- Controlled distribution setup - artificially constructed distributions may not capture real-world data complexity

## Confidence

- High Confidence: The negative correlation between encoder-measured entropy and compression quality is consistently observed with clear statistical patterns
- Medium Confidence: The decoder alignment priority mechanism is well-supported by asymmetric experimental results but relies on frozen decoder assumptions
- Medium Confidence: Diminishing returns from scaling under distribution mismatch are demonstrated empirically but external validation is limited

## Next Checks

1. **Dynamic Compression Budget Test** - Repeat the entropy experiments while varying the compression ratio (r=2, r=8) to verify whether the negative correlation persists across different rate constraints

2. **Joint Fine-tuning Experiment** - Conduct experiments where both encoder and decoder are fine-tuned jointly on the compression task to test whether the claimed asymmetry still holds

3. **Real-world Distribution Validation** - Apply the methodology to naturally occurring datasets rather than controlled synthetic distributions to assess whether theoretical predictions generalize beyond artificial setups