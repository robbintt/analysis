---
ver: rpa2
title: Efficient Reasoning with Hidden Thinking
arxiv_id: '2501.19201'
source_url: https://arxiv.org/abs/2501.19201
tags:
- reasoning
- thinking
- heima
- hidden
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Heima, an efficient reasoning framework that
  accelerates multimodal large language models by encoding Chain-of-Thought reasoning
  into compact hidden representations. The Heima Encoder transforms verbose reasoning
  steps into single thinking tokens while maintaining reasoning accuracy, reducing
  token generation to as little as 6% of the original.
---

# Efficient Reasoning with Hidden Thinking

## Quick Facts
- arXiv ID: 2501.19201
- Source URL: https://arxiv.org/abs/2501.19201
- Reference count: 39
- Primary result: Compresses multimodal chain-of-thought reasoning to 6-15% of original token count while maintaining accuracy

## Executive Summary
This paper introduces Heima, a framework that accelerates multimodal reasoning by encoding verbose Chain-of-Thought (CoT) into compact hidden representations. The key innovation is replacing textual reasoning steps with special "thinking tokens" whose hidden states serve as compressed encodings. The framework achieves 6-15% of original token generation while maintaining comparable or superior accuracy across six reasoning benchmarks. The approach uses progressive encoding to train the model to transition from explicit text to latent representations, and an LLM decoder to reconstruct interpretable reasoning from these hidden states.

## Method Summary
Heima operates through a two-stage process: an encoder that condenses CoT stages into single thinking tokens and a decoder that reconstructs the reasoning process. The encoder fine-tunes an MLLM to output special tokens (e.g., `<Thinking_of_Summary>`) whose last hidden states become compressed representations. Training follows a progressive curriculum, starting with full text generation and gradually replacing stages with tokens. The decoder, a separate LLM, receives explanatory prompts and replaces token embeddings with the encoder's hidden states to generate reconstructed reasoning. Both components use LoRA fine-tuning, and the full pipeline achieves significant token reduction while preserving reasoning accuracy.

## Key Results
- Achieves 6-15% of original token generation compared to baseline MLLMs
- Maintains comparable or superior accuracy across six benchmarks (MMStar, MMBench, MMVet, MathVista, AI2D, HallusionBench)
- Progressive encoding proves essential, with non-progressive approaches showing 2-4% accuracy degradation
- Single thinking token per CoT stage is optimal, with additional tokens providing diminishing returns

## Why This Works (Mechanism)

### Mechanism 1: Latent Compression via Special Token Embeddings
- Claim: Heima encodes verbose Chain-of-Thought (CoT) reasoning into compact hidden representations using single "thinking tokens," significantly reducing token count while preserving reasoning information.
- Mechanism: The framework fine-tunes a Multimodal Large Language Model (MLLM) as a Heima Encoder. Instead of generating full textual CoTs, it generates a special token like `<Thinking_of_Summary>` for each stage. The "last hidden state" of this special token serves as the "hidden representation" that encodes the reasoning content.
- Core assumption: The high-dimensional continuous vector space of the hidden states can represent the semantic content of a potentially long sequence of discrete tokens with sufficient fidelity.
- Evidence anchors: Abstract and section 3.1 describe the encoding process and special tokens.
- Break condition: The hidden representation becomes a lossy bottleneck when critical, non-redundant information cannot be compressed into the fixed-dimensionality of a single hidden state.

### Mechanism 2: Progressive Encoding for Stable Knowledge Transfer
- Claim: A gradual training curriculum is essential for transferring reasoning capabilities from explicit text to latent hidden states without catastrophic forgetting or performance collapse.
- Mechanism: Training progresses through stages, starting with full text for all CoTs, then replacing one stage's text with a thinking token, training, adding another token, training, and so on, until all stages are encoded. A final "recovering stage" optimizes interactions between all encoded representations.
- Core assumption: The model can better learn to internalize complex, multi-stage reasoning if it first masters the output for each stage in textual form and then incrementally learns to pack that information into a latent vector.
- Evidence anchors: Abstract and section 3.1 describe the progressive encoding strategy.
- Break condition: Attempting "one-shot" encoding (non-progressive) leads to significant performance degradation or training instability, as shown in ablation studies.

### Mechanism 3: Adaptive Decoding via Explanatory Prompting
- Claim: A standard LLM (Heima Decoder) can reconstruct the original, variable-length textual reasoning process from the compact hidden representations, provided with the right textual scaffolding.
- Mechanism: The Heima Encoder's frozen hidden state for a thinking token is used to replace the embedding of the special token in the input sequence of a separate Heima Decoder. The decoder is guided by an "explanatory prompt" (e.g., "According to the question: [Q], can you explain the thinking process of <CoT>?").
- Core assumption: The hidden representation from the encoder contains sufficient information about both the original question context and the visual input to guide a text-only LLM.
- Evidence anchors: Abstract and section 3.2 describe the decoder reconstruction process.
- Break condition: The reconstructed reasoning becomes incoherent, irrelevant to the image/question, or factually incorrect, particularly for stages requiring detailed visual grounding.

## Foundational Learning

- **Hidden States (Last Hidden State) in Transformers**
  - Why needed here: The entire mechanism hinges on using the last hidden state of a token as a dense, information-rich vector. Understanding that this state is a summary of all prior context in the sequence is crucial.
  - Quick check question: How does the information encoded in the last hidden state of a token differ from its initial token embedding?

- **Curriculum Learning / Progressive Training**
  - Why needed here: The paper's core method for stable training is a curriculum where the task difficulty is gradually increased (from generating text to generating latent vectors). This principle is key to replicating their results.
  - Quick check question: Why might a model struggle if trained to directly output a compressed latent representation without first being shown the full, explicit reasoning text?

- **Fine-tuning with LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper uses LoRA to train both the encoder and decoder. This implies the core knowledge is already in the base model, and they are learning to reroute it.
  - Quick check question: What are the practical benefits of using LoRA for adapting a large model compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: Input (Image + Query) -> Heima Encoder (MLLM) -> Sequence with thinking tokens -> Extract hidden state -> (Decoder Prompt + Token Embedding replaced with hidden state) -> Heima Decoder (LLM) -> Reconstructed Text

- **Critical path**:
  1. **Dataset Preparation**: Align CoT data into distinct stages (summary, caption, reasoning)
  2. **Encoder Fine-tuning**: Implement the multi-stage progressive encoding loop. Failure to follow the curriculum is the most likely point of failure
  3. **Decoder Fine-tuning**: Freeze the encoder. For each stage, create a dataset of (prompt, question, hidden state from encoder, ground-truth CoT) and fine-tune the decoder

- **Design tradeoffs**:
  - **Efficiency vs. Interpretability**: Using the encoder alone is maximally efficient (few tokens) but opaque. Adding the decoder reconstructs interpretability but adds computational overhead at query time
  - **Compression Granularity**: Ablation studies show a single token per CoT stage is optimal. Increasing the number of tokens per stage adds cost with diminishing returns

- **Failure signatures**:
  - **Performance Collapse**: The model generates gibberish or forgets the task. This suggests the progressive encoding schedule was too aggressive or skipped
  - **Decoder Hallucination**: The reconstructed text does not match the image content. The hidden state bottleneck is too tight or the decoder was poorly trained on the reconstruction task
  - **Token Embedding Mismatch**: A critical implementation error would be passing the text token ID to the decoder instead of replacing its embedding with the hidden state tensor

- **First 3 experiments**:
  1. **Sanity Check - Baseline vs. One-Shot**: Train the encoder without progressive encoding (replace all CoT stages with tokens at once). Compare accuracy on a benchmark against the baseline MLLM. Expect a significant drop, proving the need for the curriculum
  2. **Information Bottleneck - Reconstruction Quality**: Train the decoder for the "summary" stage and the "reasoning" stage. Compare the BLEU/METEOR scores. Expect higher scores for summary, demonstrating that visual reasoning is harder to compress
  3. **Efficiency Measurement**: Run inference on a test set with both the baseline MLLM and the Heima Encoder. Measure the average token count generated. The target is to achieve results reported in the paper (e.g., <15% of original token count)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Heima framework maintain accuracy and reconstruction fidelity when applied to larger MLLM encoders or paired with smaller, more efficient LLM decoders?
- Basis: The conclusion states plans to extend the method to larger models for Heima Encoder and explore smaller LLMs as Heima Decoder.
- Why unresolved: The current experiments utilize a specific configuration; the scalability of the hidden representation transfer across significantly different model sizes remains untested.
- What evidence would resolve it: Benchmark results on standard reasoning datasets using larger encoders (e.g., 70B+ parameters) and smaller decoders (e.g., 1B-3B parameters).

### Open Question 2
- Question: Does the Heima Encoder effectively compress unstructured or free-form chain-of-thought reasoning that lacks the explicit stage definitions used in training?
- Basis: The methodology relies on the LLaVA-CoT-100k dataset which provides distinct "Summary," "Caption," and "Reasoning" stages. The paper does not evaluate the method on datasets where reasoning steps are not explicitly segmented.
- Why unresolved: The progressive encoding strategy trains specific tokens for specific stages; it is unclear if the model can map arbitrary, non-segmented reasoning trajectories into hidden tokens without losing semantic integrity.
- What evidence would resolve it: Zero-shot evaluation results on datasets containing free-form or non-templated chain-of-thought annotations to test generalization capability.

### Open Question 3
- Question: Why does varying the retention ratio of thinking tokens result in irregular accuracy fluctuations rather than a predictable performance trade-off?
- Basis: The ablation study regarding adaptive encoding showed "no consistent pattern" in accuracy as the retention ratio increased, a phenomenon the paper observes but does not explain.
- Why unresolved: The paper establishes that single-token encoding is superior to variable retention but leaves open the theoretical or mechanical reason for the failure of adaptive compression.
- What evidence would resolve it: An analysis of the hidden state manifold geometry or attention distribution dynamics when the model attempts to compress varying amounts of information into the thinking tokens.

## Limitations

- The hidden-state bottleneck may fail for highly visual or detailed content, as evidenced by lower reconstruction metrics (BLEU/METEOR) for visual reasoning and caption stages compared to summary
- The progressive encoding schedule lacks precise step counts per stage, creating ambiguity about the exact training regimen needed to replicate results
- The framework's dependence on special token embeddings as information carriers raises questions about generalizability if future multimodal models change their tokenization or hidden-state architecture

## Confidence

- **High Confidence**: The efficiency gains (6-15% token reduction) are well-supported by experimental design and baseline comparisons
- **Medium Confidence**: The comparable or superior accuracy claims hold for tested benchmarks but may not generalize to more complex reasoning tasks requiring detailed visual analysis
- **Low Confidence**: The scalability claims beyond the tested model sizes (11B encoder, 8B decoder) remain unproven

## Next Checks

1. **Bottleneck Stress Test**: Create a synthetic reasoning benchmark with increasing visual complexity (simple arithmetic → spatial reasoning → multi-object tracking). Measure accuracy and reconstruction quality across all stages to identify the precise complexity threshold where compression fails.

2. **Progressive Schedule Sensitivity**: Systematically vary the number of training steps per progressive encoding stage (1→5→10 epochs per stage). Document accuracy and stability metrics to determine the minimum viable schedule and identify potential overfitting points.

3. **Decoder Independence Test**: Train the Heima Decoder with corrupted or random hidden states from the encoder. If the decoder still produces coherent text, it suggests the explanatory prompts alone drive performance rather than the hidden representations, undermining the core claim about latent compression.