---
ver: rpa2
title: 'Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning
  for Robust Path Tracking'
arxiv_id: '2506.15700'
source_url: https://arxiv.org/abs/2506.15700
tags:
- contraction
- policy
- control
- dynamics
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Contraction Actor-Critic (CAC), a reinforcement
  learning algorithm that integrates control contraction metrics (CCMs) into an actor-critic
  framework to learn robust path-tracking policies under unknown dynamics. CAC jointly
  learns a contraction metric generator (CMG) and a policy, where the CMG outputs
  a distribution over contraction metrics that define a reward function encouraging
  optimal cumulative tracking error minimization.
---

# Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking

## Quick Facts
- arXiv ID: 2506.15700
- Source URL: https://arxiv.org/abs/2506.15700
- Reference count: 29
- Primary result: CAC learns robust path-tracking policies by integrating contraction metrics into RL reward, outperforming baselines in tracking error and sim-to-real transfer.

## Executive Summary
This paper introduces Contraction Actor-Critic (CAC), a reinforcement learning algorithm that integrates control contraction metrics (CCMs) into an actor-critic framework to learn robust path-tracking policies under unknown dynamics. CAC jointly learns a contraction metric generator (CMG) and a policy, where the CMG outputs a distribution over contraction metrics that define a reward function encouraging optimal cumulative tracking error minimization. Theoretical analysis shows that policies trained with this reward exhibit asymptotic convergence if a contracting policy exists. Empirically, CAC outperforms established baselines across four simulated environments and demonstrates successful sim-to-real transfer on a TurtleBot3 robot.

## Method Summary
CAC learns a policy for robust path tracking by jointly optimizing a contraction metric generator and an actor-critic policy. The method pre-trains a dynamics model from data, then alternates between updating the CMG (to satisfy contraction conditions using the dynamics model) and updating the policy via PPO using a reward derived from the contraction metric. The reward encourages minimizing cumulative tracking error while maintaining stability certificates. A freeze-and-learn strategy stabilizes the bi-level optimization by updating the CMG less frequently than the policy.

## Key Results
- CAC achieves lower mean area under the curve (MAUC) for normalized tracking error than C3M, PPO, SD-LQR, and LQR baselines across Car, PVTOL, NeuralLander, and Quadrotor environments.
- In real-world TurtleBot3 experiments, CAC successfully tracks reference trajectories while baselines fail under sim-to-real dynamics deviations.
- Ablation studies show the reward-conditioned entropy regularization and freeze-and-learn strategy are critical for performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bounding the reward function by the contraction metric induces policies that minimize cumulative tracking error while preserving stability certificates.
- Mechanism: The reward R(x) = 1/(1 + δx^T M δx) ties policy optimization directly to the differential Lyapunov function V = δx^T M δx. Maximizing this reward is equivalent (per Lemma 2) to minimizing cumulative weighted tracking error. If a contracting policy exists, Theorem 1 shows the optimal policy under this reward exhibits asymptotic convergence.
- Core assumption: Assumption 1 holds — there exists at least one contracting policy πc with contraction rate λ > 0 that satisfies the CCM conditions.
- Evidence anchors:
  - [section 4] Theorem 1 and Lemmas 1–2 establish the formal connection between reward maximization and asymptotic convergence under contraction.
  - [section 3.2] Equation (10) defines the reward function explicitly in terms of the contraction metric.
  - [corpus] Weak — related work on robust average-reward RL and CCM-based control exists, but no direct corpus evidence on this specific reward-metric coupling.
- Break condition: If no contracting policy exists for the system, the theoretical guarantee collapses; the reward still guides learning but without convergence certificates.

### Mechanism 2
- Claim: Alternating optimization between the CMG and policy stabilizes bi-level learning.
- Mechanism: The freeze-and-learn strategy updates the CMG (Eq. 9), then freezes it for n policy updates (Eq. 6–7). This prevents the CMG from shifting the reward landscape too quickly, giving the actor-critic time to adapt. Reward-conditioned entropy α(rt) = βM e^{-rt} encourages exploration when returns are low and exploitation when returns are high.
- Core assumption: The pre-trained dynamics model provides sufficiently accurate f, B, and B⊥ to evaluate contraction conditions without excessive bias.
- Evidence anchors:
  - [section 3.2] Algorithm 1 and the text describe the freeze-and-learn approach explicitly.
  - [section 5] Figure 2 shows that removing entropy regularization degrades performance, especially on Car and NeuralLander.
  - [corpus] No direct corpus evidence on this specific bi-level strategy in contraction-guided RL.
- Break condition: If dynamics approximation error is too large, contraction condition evaluations become unreliable, potentially destabilizing CMG learning.

### Mechanism 3
- Claim: Model-free RL combined with contraction certificates improves robustness to dynamics mismatch.
- Mechanism: The policy is trained via PPO (model-free) to maximize the contraction-derived reward, while the CMG uses the learned dynamics only to verify contraction conditions. This decouples the policy from direct model dependence. Empirically, CAC outperforms model-dependent baselines (C3M, LQR) under sim-to-real transfer.
- Core assumption: The contraction metric learned from the approximate dynamics transfers to the true system.
- Evidence anchors:
  - [section 5.1] Real-world TurtleBot3 experiments show CAC tracks successfully while C3M fails and PPO diverges.
  - [section 5] Table 1 shows CAC maintains low MAUC across all environments, unlike baselines that vary significantly.
  - [corpus] A related paper on CCM-based robust neural control supports CCM robustness to disturbances, but not the specific RL integration.
- Break condition: Large sim-to-real gaps in actuation or unmodeled dynamics may invalidate the learned contraction metric.

## Foundational Learning

### Concept: Control Contraction Metrics (CCMs)
- Why needed here: CAC's core innovation is embedding CCMs into the RL reward. Without understanding that CCMs certify incremental exponential stability via a Riemannian metric M, the reward design rationale is opaque.
- Quick check question: Given a system ẋ = f(x) + B(x)u, what does the condition B⊥^T (∂_f W + sym(∂_x W · f) + 2λW)B⊥ ≺ 0 imply about the uncontrolled dynamics?

### Concept: Actor-Critic Methods
- Why needed here: CAC builds on PPO, an actor-critic algorithm. Understanding the separation between policy (actor) and value (critic) updates is necessary to follow the training loop.
- Quick check question: In actor-critic, why is the critic trained to minimize Bellman error rather than directly maximizing reward?

### Concept: Differential Lyapunov Analysis
- Why needed here: The contraction framework uses V = δx^T M δx as a differential Lyapunov function. Interpreting δx as the infinitesimal displacement between trajectories is essential for understanding the reward.
- Quick check question: If V ≤ -2λV holds, what does this imply about the distance between any two system trajectories over time?

## Architecture Onboarding

### Component map
Dynamics Model -> CMG -> Reward -> Actor (PPO) + Critic (Bellman error)

### Critical path
1. Collect transition data and pre-train dynamics model to convergence (MSE ≈ 0.1).
2. For each training iteration:
   - Update CMG once (Eq. 9) using current policy samples.
   - Freeze CMG; perform n PPO epochs on actor/critic (Eq. 6–7) using frozen CMG for reward.
3. Evaluate on held-out trajectories; select best checkpoint by MAUC.

### Design tradeoffs
- **CMG update frequency vs. policy stability:** Fewer CMG updates stabilize learning but may slow adaptation; more frequent updates risk oscillation. Paper uses every n-th step.
- **Entropy strength β_M:** Too high → excessive CMG exploration, slow convergence; too low → premature convergence to suboptimal metrics.
- **Reward boundedness:** The 1/(1 + ...) form improves numerical stability but compresses reward dynamic range.

### Failure signatures
- **C3M-like failure on PVTOL/Quadrotor:** High MAUC with large variance suggests dynamics approximation error is propagating into control.
- **PPO divergence in real-world:** Policy overfits to simulator quirks; contraction guidance too weak.
- **CMG loss not decreasing:** Dynamics model may be insufficient; check B⊥ computation via SVD.

### First 3 experiments
1. **Ablation on CMG entropy:** Train CAC with β_M = 0 vs. default 1e-2. Expect higher variance and worse MAUC without entropy, matching Figure 2 trends.
2. **Dynamics model sensitivity:** Vary dynamics pre-training data size (e.g., 10%, 50%, 100% of D) and measure MAUC and CMG loss. Test robustness to model error.
3. **Sim-to-real sanity check:** Train on simulated TurtleBot3 with perturbed friction coefficients (c1, c2, c3 ±20%). Deploy on real robot; compare tracking error to nominal CAC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CAC be extended to offline reinforcement learning settings to eliminate the need for costly online interaction during training?
- Basis in paper: [explicit] The authors state "the training process requires online interaction, which may be costly or impractical in certain real-world settings without reliable simulators."
- Why unresolved: The current formulation relies on online policy rollouts to collect data for both CMG and policy updates. Offline RL introduces distributional shift issues that may conflict with the bi-level optimization structure.
- What evidence would resolve it: Demonstrating CAC trained on fixed offline datasets achieving comparable tracking performance to online CAC, with analysis of how offline constraints affect contraction certificate validity.

### Open Question 2
- Question: How can contraction conditions be certified during training when the learned dynamics model contains approximation errors?
- Basis in paper: [explicit] The limitations section notes "satisfying these conditions in practice can be challenging. As a result, the validity of the theoretical guarantees may not always hold during training or execution."
- Why unresolved: The CMG optimizes contraction conditions using an approximate dynamics model, but there is no mechanism to verify that the true (unknown) dynamics satisfy these conditions.
- What evidence would resolve it: A verification procedure or bound relating dynamics model error to contraction condition violation, potentially using robust CCM formulations.

## Limitations
- The theoretical convergence guarantee requires Assumption 1 (existence of a contracting policy), which is assumed but not verified across environments.
- The method's performance depends on the accuracy of the pre-trained dynamics model, with no systematic analysis of model error sensitivity.
- The freeze-and-learn strategy uses a fixed hyperparameter n for policy update frequency, which may not be optimal across environments.

## Confidence

**High** — Empirical MAUC improvements over baselines (C3M, PPO, SD-LQR, LQR) are clearly demonstrated across four simulated environments and one real-world robot.

**Medium** — The theoretical link between reward maximization and asymptotic convergence (Theorem 1) is sound but contingent on Assumption 1, which is not empirically verified.

**Medium** — The freeze-and-learn strategy is shown to improve stability in ablation studies, but its optimality relative to other bi-level optimization schemes is untested.

## Next Checks

1. **CCM existence verification** — For each environment, verify that a contracting policy exists by checking if the learned CMG satisfies the contraction conditions with λ > 0 on validation trajectories.

2. **Dynamics error sensitivity** — Measure how MAUC degrades as dynamics model error increases (via controlled corruption of f, B predictions), isolating the impact of model fidelity on CMG learning.

3. **Metric transfer analysis** — Compare the learned M(x) across sim and real data to quantify how much the metric changes under domain shift, and correlate with tracking performance.