---
ver: rpa2
title: Refined PAC-Bayes Bounds for Offline Bandits
arxiv_id: '2502.11953'
source_url: https://arxiv.org/abs/2502.11953
tags:
- policy
- reward
- bounds
- probability
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of deriving probabilistic bounds\
  \ on empirical reward estimates for off-policy learning in bandit problems. The\
  \ authors build on previous PAC-Bayes bounds and improve them by using a new parameter\
  \ optimization approach that discretizes the space of possible events to optimize\
  \ the \"in probability\" parameter \u03BB."
---

# Refined PAC-Bayes Bounds for Offline Bandits

## Quick Facts
- arXiv ID: 2502.11953
- Source URL: https://arxiv.org/abs/2502.11953
- Reference count: 28
- Primary result: Parameter-free PAC-Bayes bounds for off-policy learning that recover optimal rates by discretizing KL divergence space

## Executive Summary
This paper addresses the challenge of deriving probabilistic bounds on empirical reward estimates for off-policy learning in bandit problems. The authors improve upon previous PAC-Bayes bounds by developing a new parameter optimization approach that discretizes the space of possible events to optimize the "in probability" parameter λ. This technique yields two parameter-free PAC-Bayes bounds—one based on Hoeffding-Azuma's inequality and another on Bernstein's inequality—that achieve near-optimal rates comparable to what would be obtained by setting the parameter after observing the data.

## Method Summary
The method builds on the PAC-Bayes framework by first establishing that importance sampling estimators form martingale sequences with respect to the bandit history. This martingale structure enables the application of classical concentration inequalities (Hoeffding-Azuma and Bernstein). The key innovation is a discretization technique that partitions the KL divergence space into events and optimizes parameters for each event, then combines them via a union bound. This approach produces parameter-free bounds that simultaneously hold for all policies without requiring pre-data selection of the concentration parameter.

## Key Results
- Parameter-free Hoeffding PAC-Bayes bound achieves |R(π) - R̂^IS(π, H_t)| ≤ 1/(ε√[DKL(π ∥ μ) + ln(4π/(3β))/t)
- Parameter-free Bernstein PAC-Bayes bound achieves |R(π) - R̂^IS(π, H_t)| ≤ 2√[(e - 2)(DKL(π ∥ μ) + ln(4π/(3β)))/(tε)]
- Bounds recover the same rate as would be obtained by setting the "in probability" parameter after data realization
- The discretization technique uses β_k = 6β/(πk²) to ensure ∑β_k ≤ β while maintaining optimal rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance sampling estimates form martingale sequences, enabling concentration bounds on non-i.i.d. sequential bandit data
- Mechanism: The IS estimator M^IS_n(a) = n(ř^IS(a,H_n) - r(a)) satisfies martingale properties because each increment Z^IS_n(a) = 1{a}{A_n}/π_n(A_n)·R_n - r(a) has zero conditional expectation given prior history
- Core assumption: Logging policies π_n(a) are known and bounded below by ε > 0 uniformly
- Evidence anchors: Section III.A establishes the martingale property with reference to [20, Lemma B.1]

### Mechanism 2
- Claim: Donsker-Varadhan variational formula converts policy-dependent expectations into tractable optimization over KL divergence
- Mechanism: The formula log E_{A∼μ}[e^{h(A)}] = sup_{π∈Π}(E_{A∼π}[h(A)] - D_{KL}(π||μ)) upper-bounds policy-dependent reward deviation by terms involving KL divergence between target policy π and prior μ
- Core assumption: Prior policy μ is independent of the observed data H^t
- Evidence anchors: Section III.B, Lemma 1 provides the full Donsker-Varadhan formula used in Proposition 1 proof

### Mechanism 3
- Claim: Discretizing KL divergence space with optimized per-event parameters yields parameter-free bounds achieving oracle-optimal rates
- Mechanism: The technique partitions possible KL values into events E_k = {k-1 < D_{KL}(π||μ) ≤ k}, optimizes parameters (λ_k, β_k) conditioned on each event, and applies union bound
- Core assumption: The grid-based optimization sufficiently approximates the continuum of possible KL values
- Evidence anchors: Abstract describes the discretization technique; Section III.C, Theorem 1 proof details the event construction

## Foundational Learning

- **Importance Sampling for Off-Policy Evaluation**: Why needed here: The core estimator ř^IS uses importance weights π_n(a)/π(a) to correct distribution mismatch between logging and target policies. Quick check: Can you explain why ř^IS(a,h_t) is an unbiased estimator of r(a) when the logging policy π_n covers action a?

- **Martingales and Martingale Difference Sequences**: Why needed here: Bandit data is non-i.i.d.; martingale theory provides the mathematical foundation for concentration bounds under sequential dependence. Quick check: What three conditions must a sequence M_t satisfy to be a martingale with respect to X^t?

- **PAC-Bayes Framework**: Why needed here: Provides "probably approximately correct" guarantees that hold uniformly over a policy class, with complexity measured by KL divergence to a prior. Quick check: Why must the prior μ be chosen independently of the data H^t for PAC-Bayes bounds to hold?

## Architecture Onboarding

- **Component map**: Logged dataset h_t = {(a_n, r_n)}^t_{n=1} with known logging policies π_n -> Importance sampling estimator ř^IS(π, h_t) -> KL divergence D_{KL}(π||μ) computation -> Parameter-free bound application -> High-probability interval [ř^IS - bound, ř^IS + bound]

- **Critical path**: 
  1. Verify logging policy coverage: ε ≤ min_{n,a} π_n(a) for all actions
  2. Select prior μ independent of h_t
  3. Compute KL divergence D_{KL}(π||μ) for candidate policy π
  4. Apply Theorem 1 (Hoeffding) or Theorem 2 (Bernstein) for parameter-free bound

- **Design tradeoffs**:
  - Hoeffding vs. Bernstein: Bernstein bound has tighter ε-dependence (1/√(ε) vs. 1/ε) but restricts maximum KL divergence
  - Prior selection: Tighter priors (closer to target π) reduce KL penalty but require more domain knowledge
  - Coverage ε: Larger ε (better logging coverage) tightens bounds; ε → 0 causes bound explosion

- **Failure signatures**:
  - Bound exceeds reward range [0,1]: Indicates insufficient data (t too small) or poor coverage (ε too small)
  - Negative bound values: Numerical error in KL computation or invalid β > 1
  - Policy optimization ignores bound: May select policies with large KL divergence from prior

- **First 3 experiments**:
  1. **Synthetic bandit validation**: Generate data with known reward distribution, logging policy π_log with uniform coverage ε, verify that |R(π) - ř^IS| ≤ bound holds in ≥ (1-β) fraction of trials across random seeds
  2. **Coverage sensitivity analysis**: Vary ε ∈ {0.1, 0.05, 0.01} by changing logging policy entropy, plot bound tightness vs. ε to confirm 1/ε (Hoeffding) and 1/√ε (Bernstein) scaling
  3. **Prior mismatch study**: Fix target policy π, vary prior μ from uniform (high KL) to π itself (KL=0), measure how bound sharpness changes with D_{KL}(π||μ)

## Open Questions the Paper Calls Out

- **Question**: Can these refined reward estimate bounds be leveraged to derive new PAC-Bayes regret bounds for online learning algorithms?
- **Basis in paper**: [explicit] The conclusion explicitly states: "Future research will focus on leveraging these refined reward estimate bounds to derive new PAC-Bayes regret bounds."
- **Why unresolved**: The current paper focuses entirely on the accuracy of the reward estimate in an offline (batch) setting. Deriving regret bounds requires analyzing the cumulative performance of a sequence of changing policies in an online setting, which introduces additional dependencies not addressed in the current proofs.

## Limitations
- The Bernstein bound requires restrictive conditions on D_KL(π∥μ) that may limit applicability in practice
- No empirical validation provided to confirm theoretical bounds match observed performance
- Prior policy selection strategy remains underspecified beyond independence requirement

## Confidence
- **High confidence**: Martingale structure of IS estimators, Donsker-Varadhan variational application, Hoeffding-Azuma concentration validity
- **Medium confidence**: Bernstein bound's KL restriction practical impact, discretization approximation quality
- **Low confidence**: Optimal rate recovery claim without experimental verification, bound tightness in realistic bandit scenarios

## Next Checks
1. Implement synthetic bandit experiments with known reward distributions to verify that |R(π) - R̂^IS(π, H_t)| ≤ bound holds with frequency ≥ (1-β) across multiple random seeds and sample sizes
2. Conduct coverage sensitivity analysis by varying ε through logging policy entropy and measuring bound scaling with 1/ε (Hoeffding) and 1/√ε (Bernstein)
3. Study prior mismatch effects by fixing target policy π and varying prior μ from uniform to π itself, measuring bound sharpness changes with D_KL(π∥μ)