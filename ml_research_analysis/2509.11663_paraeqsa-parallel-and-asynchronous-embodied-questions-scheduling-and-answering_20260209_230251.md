---
ver: rpa2
title: 'ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering'
arxiv_id: '2509.11663'
source_url: https://arxiv.org/abs/2509.11663
tags:
- questions
- question
- embodied
- exploration
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Embodied Questions Answering
  (EQsA), where an agent must handle multiple, asynchronous questions with varying
  urgencies in a 3D environment, extending beyond the single-question focus of classical
  EQA. To tackle this, the authors propose ParaEQsA, a parallel framework that leverages
  a shared group memory to reduce redundant exploration and a priority-planning module
  to dynamically schedule questions based on urgency, scope, reward, and dependencies.
---

# ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering

## Quick Facts
- **arXiv ID**: 2509.11663
- **Source URL**: https://arxiv.org/abs/2509.11663
- **Reference count**: 33
- **Primary result**: A parallel framework for handling multiple, asynchronous, urgent questions in embodied environments, achieving 9.0% direct answer rate and 0.204 normalized urgency-weighted latency.

## Executive Summary
ParaEQsA introduces a parallel and asynchronous framework for Embodied Questions Answering (EQsA) that extends beyond single-question settings. The system uses a shared group memory to reduce redundant exploration, a priority-planning module to dynamically schedule questions based on urgency, scope, reward, and dependencies, and a VLM-guided targeted exploration strategy. Evaluated on the new PAEQs dataset, ParaEQsA significantly outperforms sequential baselines, demonstrating superior efficiency, responsiveness, and effective knowledge reuse in multi-question scenarios.

## Method Summary
ParaEQsA processes multiple asynchronous questions in parallel by leveraging a shared group memory for knowledge reuse and a priority-based scheduling system. The framework includes a Generator to emit questions, a Parser to extract urgency and scope, a Finishing Module to check for direct answers in memory, and a Question Pool with a DAG-based dependency tracker. Questions are prioritized using a weighted score combining urgency, scope, reward, and dependency, then routed to a Planner for targeted exploration using VLM-guided semantic mapping. A Stopping Module evaluates when sufficient information is gathered, and an Answering Module generates final responses using the group memory context.

## Key Results
- Reduces Normalized Steps (NS) to 0.321 vs. 0.472 (Explore-EQA) and 0.410 (Memory-EQA)
- Achieves Normalized Urgency-Weighted Latency (NUWL) of 0.204 vs. 0.276 (Explore-EQA) and 0.242 (Memory-EQA)
- Direct Answer Rate (DAR) of 9.0% vs. 0% for both baselines
- Accuracy (Acc) of 0.65, comparable to sequential methods but with lower latency and steps

## Why This Works (Mechanism)

### Mechanism 1: Shared Group Memory for Knowledge Reuse
Maintaining a persistent, shared memory across multiple questions enables direct answering of some questions without additional exploration, reducing redundant navigation. As the agent explores for one question, it populates a group memory with visual and textual observations. When new questions arrive, a Finishing Module first queries this memory; if sufficient information exists, the question is answered immediately (zero exploration steps). This contrasts with sequential baselines that treat each question in isolation, forcing re-exploration of previously visited regions.

### Mechanism 2: Priority-Based Dynamic Question Scheduling
A weighted priority score combining urgency, scope, reward, and dependency reduces urgency-weighted latency by processing critical and low-effort questions before exploratory or dependent ones. A Question Pool maintains a buffer with a Directed Acyclic Graph (DAG) of dependencies. An Updater re-scores all questions whenever the pool changes using: `P(q_i) = w_u·Urgency + w_s·Scope + w_r·Reward + w_d·Dependency`. The Planner selects the highest-score question for targeted exploration. High-urgency questions use a non-linear transformation (`-ln(1-u_i)`) to sharply elevate their priority.

### Mechanism 3: VLM-Guided Targeted Exploration
Using a Vision-Language Model to assign semantic values to exploration frontiers focuses navigation on regions likely relevant to the current question, reducing steps needed to gather sufficient information. During exploration, a semantic map overlays spatial information with object labels. A VLM analyzes each observation to identify regions/objects relevant to the active question and assigns them a "semantic value." These values weight frontier-based exploration, biasing the agent toward high-relevance regions.

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) for Dependency Management**
  - **Why needed here**: The Question Pool uses a DAG to track dependencies between questions (e.g., "Is there dirt on my shirt?" depends on "Where is my shirt?"). Understanding DAGs is essential to reason about task ordering and deadlock prevention.
  - **Quick check question**: If question A depends on B, and B depends on C, which question must be answered first? Can a DAG have cycles?

- **Concept: Weighted Multi-Objective Scoring**
  - **Why needed here**: The priority score combines four normalized factors (urgency, scope, reward, dependency) with learnable or tuned weights. Engineers must understand how to balance competing objectives and interpret sensitivity.
  - **Quick check question**: If w_u is increased relative to w_s, what happens to the priority of a high-urgency global-scope question vs. a low-urgency local-scope question?

- **Concept: Vision-Language Models (VLMs) for Semantic Grounding**
  - **Why needed here**: VLMs (GPT-5-mini, Qwen2.5-VL) are used for question parsing, semantic annotation, and stopping decisions. Understanding their capabilities and failure modes is critical for diagnosing exploration and answering errors.
  - **Quick check question**: How might a VLM fail when asked to identify "the red mug on the counter" in a cluttered image? What safety checks could mitigate this?

## Architecture Onboarding

- **Component map**: Generator -> Parser -> Finishing Module -> (Direct Answer / Question Pool) -> Updater -> Planner -> Exploration Loop -> Stopping Module -> Answering Module; Group Memory shared across questions
- **Critical path**: Question arrives → Parser → Finishing Module checks Group Memory. If answerable: → Answering Module → return answer. If not: → Question Pool → Updater scores all questions → Planner selects highest-priority → Targeted Exploration loop (observe → update memory/map → move → check stopping). When stopping triggered: → Answering Module → return answer.
- **Design tradeoffs**: Parallel vs. Sequential (parallel reduces redundancy but adds coordination complexity), LLM/VLM choice (GPT-5-mini balances performance and cost; smaller models reduce latency but may lower accuracy), Exploration continuity (starting from previous pose reduces backtracking but assumes spatial locality), Urgency inference (LLM-based enables dynamic prioritization but introduces noise).
- **Failure signatures**: DAR near 0% (Group Memory not persisting or retrieval failing; questions fully disjoint), NUWL high despite priority (urgency mis-inferred or w_u too low; dependency chains stalling high-urgency questions), NS high with low accuracy (VLM grounding failing; exploration misdirected; stopping module too permissive), Deadlock in Question Pool (DAG has circular dependencies or all questions pending indefinitely).
- **First 3 experiments**: 1) Baseline comparison: Run ParaEQsA vs. Explore-EQA and Memory-EQA on PAEQs, logging Acc, DAR, NS, NUWL. Verify DAR > 0% and NUWL reduction. 2) Ablation on priority components: Disable each factor (urgency, scope, reward, dependency) one at a time. Quantify impact on NS and NUWL to validate weighting scheme. 3) Memory replay test: Inject a question whose answer was fully observed during exploration for a previous question. Confirm it is answered with DAR path (zero steps) and high confidence.

## Open Questions the Paper Calls Out
- **How can the ParaEQsA framework be adapted for decentralized multi-agent coordination to collaboratively solve EQsA tasks?** The Conclusion states, "A major avenue for future effort lies in extending our framework to not only be multi-question, but also multi-agent." The current architecture assumes a single agent with a centralized group memory and planning module; decentralized coordination introduces new challenges in task allocation and information sharing.
- **How does ParaEQsA performance generalize to open-vocabulary or generative answers rather than the four-option multiple-choice format used in the PAEQs dataset?** The evaluation is restricted to multiple-choice questions (A, B, C, D), whereas real-world interactions (described in the Introduction) often require more complex, generative responses.
- **To what extent does real-world VLM inference latency negate the efficiency gains observed in simulation steps (NS)?** Efficiency is measured by Normalized Steps (NS) and step-based latency, ignoring the computational overhead of parallel LLM/VLM calls (e.g., GPT-5-mini, Qwen2.5) which may incur significant wall-clock delays.

## Limitations
- Exact weighting coefficients (w_u, w_s, w_r, w_d) for priority score are not provided, limiting reproducibility.
- Evaluation relies on synthetic PAEQs data with curated urgency labels and multiple-choice answers, not real-world ambiguity.
- Performance claims depend on VLM grounding accuracy, which is not independently validated or quantified.

## Confidence
- **High Confidence**: Shared group memory reduces redundant exploration and improves efficiency in multi-question scenarios (DAR > 0%, NS reduction).
- **Medium Confidence**: Priority-based dynamic scheduling effectively reduces urgency-weighted latency (NUWL reduction), though exact weight values and urgency inference robustness are unclear.
- **Low Confidence**: VLM-guided targeted exploration consistently improves navigation efficiency; performance is highly dependent on VLM grounding accuracy.

## Next Checks
1. **Ablation on Priority Weights**: Systematically vary w_u, w_s, w_r, w_d to determine their impact on NUWL and DAR. Identify if current weight assumptions are optimal or arbitrary.
2. **Generalization to Disjoint Questions**: Test ParaEQsA on a dataset with semantically and spatially disjoint questions. Measure DAR and NS to validate if memory-reuse benefits are conditional on question overlap.
3. **VLM Grounding Robustness**: Replace GPT-5-mini with a weaker VLM (e.g., Qwen2.5-VL-7B) for semantic annotation and stopping decisions. Compare NS and accuracy to quantify sensitivity to VLM performance.