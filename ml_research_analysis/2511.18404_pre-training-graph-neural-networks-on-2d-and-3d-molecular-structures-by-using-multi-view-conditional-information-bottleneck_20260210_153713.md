---
ver: rpa2
title: Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using
  Multi-View Conditional Information Bottleneck
arxiv_id: '2511.18404'
source_url: https://arxiv.org/abs/2511.18404
tags:
- mvcib
- molecular
- information
- representations
- views
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MVCIB, a pre-training method for molecular
  graph neural networks that leverages both 2D and 3D molecular structures through
  a multi-view conditional information bottleneck approach. The key innovation is
  maximizing shared information between views while minimizing irrelevant features,
  using one view as a contextual condition for the other.
---

# Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck

## Quick Facts
- arXiv ID: 2511.18404
- Source URL: https://arxiv.org/abs/2511.18404
- Reference count: 40
- Key outcome: MVCIB achieves 3D Weisfeiler-Lehman expressiveness power and consistently outperforms state-of-the-art baselines across molecular classification and regression tasks

## Executive Summary
This paper introduces MVCIB, a self-supervised pre-training method for molecular graph neural networks that leverages both 2D molecular graphs and 3D conformers through a multi-view conditional information bottleneck approach. The method maximizes shared information between views while minimizing view-specific noise by treating one view as a contextual condition for the other. MVCIB identifies and aligns important substructures like functional groups and ego-networks across views via a cross-attention mechanism. Experiments demonstrate consistent improvements over state-of-the-art baselines in both classification and regression tasks, with particular strength in distinguishing isomers that share identical 2D connectivity but differ in 3D geometry.

## Method Summary
MVCIB pre-trains dual-branch GNNs on 2D and 3D molecular representations using a conditional information bottleneck framework. The 2D branch uses a GIN encoder while the 3D branch employs an EGNN encoder. The method extracts semantic subgraphs (functional groups via BRICS) and contextual subgraphs (ego-networks via k-hop sampling), then aligns them using cross-attention. The MVCIB loss combines mutual information maximization between views with conditional compression to remove view-specific noise. The model is pre-trained on 456k unlabeled ChEMBL molecules and fine-tuned on MoleculeNet benchmarks and QM9 regression tasks, achieving state-of-the-art performance across multiple domains.

## Key Results
- Achieves 3D Weisfeiler-Lehman expressiveness power, successfully distinguishing non-isomorphic graphs and different 3D geometries (isomers) that share identical 2D connectivity
- Consistently outperforms state-of-the-art baselines in classification tasks with significant ROC-AUC improvements across MoleculeNet benchmarks
- Demonstrates superior interpretability in detecting ground-truth substructures like functional groups through cross-attention visualization
- Shows faster convergence during fine-tuning compared to training from scratch, with robust performance across diverse molecular domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditional compression of representations allows the model to discard view-specific noise while retaining task-relevant shared information.
- **Mechanism:** The MVCIB principle treats one molecular view (e.g., 3D) as a condition to compress the other (e.g., 2D). It minimizes conditional mutual information $I(G_{2D}; Z_{2D} | G_{3D})$ to remove features unique to the 2D view that are irrelevant to the 3D view, while maximizing $I(Z_{2D}; G_{3D})$ to ensure the latent code $Z$ remains predictive.
- **Core assumption:** View-specific details (e.g., 2D drawing artifacts or noise in 3D conformer generation) are irrelevant to the underlying chemical identity shared by both views.
- **Evidence anchors:** [page 3]: Definition 2 formalizes the conditional compression objective; [page 10]: Appendix A.1 derives the upper bound using Symmetrized KL Divergence; [corpus]: Paper "Learning Fair Graph Representations with Multi-view Information Bottleneck" (ID 70148) supports IB principles in multi-view settings.

### Mechanism 2
- **Claim:** Aligning fine-grained substructures (functional groups) rather than whole graphs enforces semantic consistency between 2D topology and 3D geometry.
- **Mechanism:** The model samples semantic subgraphs (functional groups via BRICS) and contextual subgraphs (ego-networks). It computes an affinity matrix $S$ and applies cross-attention (Eq. 10) where 2D nodes attend to 3D nodes. This forces the model to map specific chemical moieties (like a benzene ring) to their geometric counterparts.
- **Core assumption:** Functional groups are invariant structural units that maintain chemical meaning regardless of 2D or 3D representation.
- **Evidence anchors:** [page 4]: Section "Subgraph Alignment" and Eq. 9-11 detail the cross-attention mechanism; [page 13]: Fig. 5 shows MVCIB identifying alkane/carbonyl groups that baselines miss; [corpus]: Paper "MV-CLAM" (ID 17484) aligns with multi-view molecular interpretation.

### Mechanism 3
- **Claim:** Incorporating 3D coordinates and k-hop ego-networks allows the model to distinguish isomers (cis/trans) and non-isomorphic graphs that standard 1D-WL GNNs cannot.
- **Mechanism:** Standard GNNs (1D-WL) fail on specific non-isomorphic pairs. By extending the "observation range" to k-hop ego-networks and adding 3D coordinate inputs to an EGNN encoder, MVCIB creates distinct feature maps for these previously indistinguishable graphs (Fig. 3b).
- **Core assumption:** The "3D Weisfeiler-Lehman" power is derived from the specific combination of local topology (ego-networks) and continuous geometric input.
- **Evidence anchors:** [abstract]: Mentions achieving "3D Weisfeiler-Lehman expressiveness power"; [page 10]: Appendix A.2 proves MVCIB is strictly more powerful than 1D-WL; [corpus]: No direct corpus evidence for the "3D-WL" specific claim.

## Foundational Learning

- **Information Bottleneck (IB):**
  - **Why needed here:** The core loss function (Eq. 14) modifies standard IB for self-supervision. You must understand the trade-off between compression (minimizing $I(X;Z)$) and sufficiency (maximizing $I(Z;Y)$).
  - **Quick check question:** If you remove the compression term ($\alpha$ in Eq. 15), does the model overfit to noise or underfit the data?

- **Molecular Representations (2D vs 3D):**
  - **Why needed here:** The method relies on the fact that 2D topology (bonds) and 3D geometry (conformers) offer complementary views. Crucially, Isomers share identical 2D graphs but differ in 3D space.
  - **Quick check question:** Why would a 2D-only GNN fail to predict the difference in solubility between Cis- and Trans-isomers?

- **Weisfeiler-Lehman (WL) Test:**
  - **Why needed here:** The paper claims "3D-WL expressiveness." You need to know that standard GNNs are limited to 1D-WL expressiveness, meaning they cannot distinguish certain non-isomorphic graphs.
  - **Quick check question:** Does the WL test check graph isomorphism based on node degrees or iterative neighbor label aggregation?

## Architecture Onboarding

- **Component map:** ChEMBL molecules -> BRICS decomposition + k-hop sampling -> GIN (2D) + EGNN (3D) -> Cross-Attention block -> MVCIB Loss -> Pre-trained representations
- **Critical path:** The synchronization of the subgraph sampling between views. If a functional group is identified in the 2D view, the corresponding nodes must be correctly identified in the 3D view for the cross-attention alignment to function.
- **Design tradeoffs:** The paper uses BRICS for "semantic" subgraphs. This is chemically rigorous but rigid; a "soft" learned subgraph mask might capture non-standard functional groups but would be harder to train.
- **Failure signatures:**
  - **Uniform Attention:** Cross-attention maps look like noise → Subgraph sampling likely misaligned or encoder dimensions too low.
  - **Reconstruction Collapse:** 2D→3D reconstruction loss is low but downstream classification is random → Model is memorizing geometry without learning shared semantics.
- **First 3 experiments:**
  1. **Isomer Ablation:** Run the model on the Cis/Trans isomer dataset (Table 13) with only the 2D view enabled vs. full MVCIB to verify the 3D expressiveness claim.
  2. **Sensitivity Analysis:** Vary the k-hop radius (Fig. 6a) to find the optimal "observation range" for ego-networks on your specific dataset.
  3. **Interpretability Check:** Visualize the cross-attention weights on a sample molecule to see if the "Ring" functional group actually aligns with the correct 3D atom coordinates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the fixed BRICS algorithm for semantic subgraph extraction be replaced by an end-to-end differentiable learning module?
- **Basis in paper:** [inferred] The Methodology section states the model "employ[s] the BRICS algorithm (Degen et al. 2008) to decompose molecules," relying on hard-coded chemical rules rather than learning the substructures directly from data.
- **Why unresolved:** Rule-based decomposition may fail to identify non-standard or implicit functional groups that are relevant to specific downstream tasks, limiting the model's ability to generalize beyond human-defined chemical heuristics.
- **What evidence would resolve it:** A comparative study where the fixed BRICS module is swapped for a learnable subgraph discovery mechanism, demonstrating superior performance on datasets with novel or rare functional groups.

### Open Question 2
- **Question:** How robust is MVCIB to the quality of the 3D molecular conformations used during pre-training?
- **Basis in paper:** [inferred] The paper notes the use of "456k unlabeled molecules from the ChEMBL database" and discusses distinguishing 3D geometries. However, it does not specify if the 3D structures are computationally minimized (DFT) or rapidly generated (RDKit), leaving the sensitivity to geometric noise unexplored.
- **Why unresolved:** The Conditional Information Bottleneck relies on aligning "shared information" between views; if the 3D input is noisy or physically unrealistic (common in large-scale datasets), the "shared" signal may degrade, leading to misalignment.
- **What evidence would resolve it:** Experiments evaluating downstream performance when pre-training on 3D conformers of varying fidelity (e.g., DFT-optimized vs. force-field vs. random) to determine if MVCIB is robust to geometric noise.

### Open Question 3
- **Question:** Does discarding "view-specific" information via the Information Bottleneck limit predictive power in tasks that rely heavily on unique 2D topological features?
- **Basis in paper:** [inferred] The Introduction posits that minimizing "view-specific information" is desirable to handle noise. However, the paper acknowledges that 2D and 3D views are different; there may be tasks where the discarded view-specific features (e.g., specific 2D connectivity not reflected in 3D geometry) are actually the primary signal.
- **Why unresolved:** The paper demonstrates general improvements but does not isolate scenarios where the aggressive compression of view-specific details results in a loss of critical topological information required for specific reaction or binding predictions.
- **What evidence would resolve it:** An ablation study comparing MVCIB against a non-bottleneck multi-view baseline on specific tasks known to be dependent on topological quirks not easily captured by 3D geometry.

## Limitations

- **Conformer generation quality:** The 3D expressiveness claims depend heavily on the quality and diversity of conformers generated from ChEMBL. If conformers are low-energy local minima, the 3D view may not capture chemically meaningful differences between isomers.
- **Subgraph sampling consistency:** The paper assumes functional groups identified in 2D maps perfectly to their geometric counterparts in 3D. However, BRICS decomposition may fail on non-standard molecules, and there's no explicit validation that cross-attention consistently finds correct atom correspondences across views.
- **Mutual information estimation reliability:** The Jensen-Shannon estimator is known to have high variance in practice. The paper doesn't report stability metrics or sensitivity analysis for the MI estimation component, which is critical for the conditional compression objective.

## Confidence

- **High confidence:** Classification performance improvements (ROC-AUC) across MoleculeNet benchmarks - these are well-established metrics with clear baselines
- **Medium confidence:** 3D-WL expressiveness claim - while theoretically supported, the practical impact on real molecular datasets needs more validation
- **Low confidence:** Interpretability results - qualitative visualization of attention maps is subjective and lacks quantitative validation against ground truth functional group annotations

## Next Checks

1. **Conformer quality validation:** Run the isomer discrimination task (Table 13) with conformers generated using different RDKit parameters (force field, optimization steps). If performance drops significantly, the 3D expressiveness claim is method-dependent rather than inherent to MVCIB.

2. **Cross-view alignment consistency:** For 100 randomly sampled molecules, extract the top-5 cross-attention weights from both views. Manually verify that atoms with high weights in 2D actually correspond to the same functional group in 3D geometry. Report alignment accuracy as a quantitative metric.

3. **MI estimation stability:** Train MVCIB with the MI estimator frozen at different random initializations. Measure the variance in final downstream performance. If variance exceeds 5% ROC-AUC, the MI estimation component is introducing instability that undermines reproducibility.