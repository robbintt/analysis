---
ver: rpa2
title: Generalized Contrastive Learning for Universal Multimodal Retrieval
arxiv_id: '2509.25638'
source_url: https://arxiv.org/abs/2509.25638
tags:
- retrieval
- multimodal
- loss
- vista
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of multimodal retrieval, where
  retrieval models struggle to retrieve keys composed of fused image-text modalities
  (e.g., Wikipedia pages with both images and text). The authors propose Generalized
  Contrastive Learning (GCL), a novel loss formulation that improves multimodal retrieval
  performance by leveraging existing image-caption paired datasets to learn a unified
  representation space.
---

# Generalized Contrastive Learning for Universal Multimodal Retrieval

## Quick Facts
- arXiv ID: 2509.25638
- Source URL: https://arxiv.org/abs/2509.25638
- Reference count: 40
- Primary result: GCL achieves 34.06% Recall@50 on M-BEIR global setting, outperforming models trained with newly composed triplet datasets

## Executive Summary
This paper addresses the challenge of multimodal retrieval when keys contain fused image-text modalities, such as Wikipedia pages with both images and text. Existing retrieval models struggle with these composite keys despite performing well on pure image or text queries. The authors propose Generalized Contrastive Learning (GCL), a novel loss formulation that leverages existing image-caption paired datasets to learn unified representations across text, image, and fused modalities. GCL consistently improves multimodal retrieval performance across diverse benchmarks (M-BEIR, MMEB, CoVR) and model architectures (VISTA, CLIP, TinyCLIP), demonstrating its effectiveness for universal multimodal retrieval.

## Method Summary
The proposed Generalized Contrastive Learning (GCL) method operates by enforcing contrastive learning across all modalities within a mini-batch. Rather than treating image-text pairs as separate entities, GCL integrates text, image, and fused text-image embeddings into a unified representation space. This is achieved by designing a loss function that simultaneously pulls together aligned modalities while pushing apart misaligned ones across the entire batch. The approach leverages readily available image-caption paired datasets without requiring specialized data collection, making it practical for real-world deployment. By considering all possible modality combinations within each training batch, GCL learns robust cross-modal representations that generalize well to retrieval tasks involving fused modalities.

## Key Results
- GCL achieves 34.06% Recall@50 on M-BEIR global setting, significantly outperforming models trained with newly composed triplet datasets
- Consistent improvements across multiple benchmarks (M-BEIR, MMEB, CoVR) and model architectures (VISTA, CLIP, TinyCLIP)
- GCL demonstrates particular effectiveness for Wikipedia-style pages containing both images and text, addressing a critical gap in existing retrieval systems

## Why This Works (Mechanism)
GCL works by creating a unified representation space where text, image, and fused modalities are jointly optimized through contrastive learning. The mechanism leverages the natural structure of image-caption datasets, where captions provide rich textual context that can inform the retrieval of fused modalities. By enforcing similarity between aligned text-image pairs while maintaining separation from negative samples, GCL learns embeddings that capture both fine-grained visual details and semantic textual information. The generalized contrastive loss ensures that the model doesn't just learn to match individual modalities but understands their compositional relationships, which is crucial for retrieving complex multimodal documents.

## Foundational Learning

### Contrastive Learning
- **Why needed**: Forms the basis for learning discriminative representations by pulling together similar samples and pushing apart dissimilar ones
- **Quick check**: Verify that the model can distinguish between matching and non-matching image-text pairs using standard retrieval metrics

### Multimodal Fusion
- **Why needed**: Essential for combining information from different modalities into unified representations
- **Quick check**: Ensure that fused representations capture complementary information from both text and image modalities

### Cross-Modal Alignment
- **Why needed**: Critical for establishing semantic relationships between different modality representations
- **Quick check**: Measure alignment quality using metrics like cosine similarity between matched and mismatched pairs

## Architecture Onboarding

### Component Map
Input Images/Captions -> Image Encoder + Text Encoder -> Modality Embeddings -> GCL Loss Computation -> Unified Representation Space

### Critical Path
Image and text encoders produce embeddings → GCL loss aggregates contrastive signals across all modality combinations → Gradients update encoders to optimize unified space → Trained model performs retrieval using learned embeddings

### Design Tradeoffs
- **Memory vs. Performance**: GCL requires storing additional modality combinations but yields significant performance gains
- **Computational Overhead**: Generalized contrastive learning increases training time but enables zero-shot adaptation to fused modalities
- **Data Efficiency**: Leverages existing image-caption datasets rather than requiring specialized multimodal training data

### Failure Signatures
- Poor performance on pure text or image retrieval indicates over-specialization to fused modalities
- Instability in training suggests improper temperature scaling or batch size issues
- Degraded performance on out-of-domain data indicates overfitting to training modality distributions

### First Experiments
1. **Ablation Study**: Remove GCL loss components to quantify their individual contributions
2. **Generalization Test**: Evaluate on a held-out multimodal dataset not seen during training
3. **Scalability Analysis**: Measure performance and training time across different batch sizes and model scales

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Generalizability beyond tested benchmarks (M-BEIR, MMEB, CoVR) remains uncertain
- Computational overhead of generalized contrastive learning framework not thoroughly quantified
- Robustness to noisy training data (incorrect image-caption pairings) not systematically evaluated

## Confidence
- **Major claims about GCL effectiveness on tested benchmarks**: High
- **Claims about broader applicability to other multimodal retrieval tasks**: Medium
- **Claims about computational efficiency**: Low (not thoroughly discussed)

## Next Checks
1. Test GCL on additional multimodal retrieval benchmarks beyond M-BEIR, MMEB, and CoVR to assess generalizability
2. Evaluate the computational efficiency and scalability of GCL compared to standard contrastive learning approaches
3. Investigate the robustness of GCL to noise in the training data, such as incorrect image-caption pairings or irrelevant text-image combinations