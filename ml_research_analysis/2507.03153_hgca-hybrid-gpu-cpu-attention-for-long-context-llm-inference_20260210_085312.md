---
ver: rpa2
title: 'HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference'
arxiv_id: '2507.03153'
source_url: https://arxiv.org/abs/2507.03153
tags:
- attention
- memory
- hgca
- entries
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HGCA introduces a hybrid CPU-GPU attention mechanism that addresses
  GPU memory limitations in long-context LLM inference. The method performs dense
  attention on recent KV entries in GPU memory and sparse attention on selected salient
  KV entries in CPU memory, merging outputs via log-sum-exp fusion.
---

# HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference

## Quick Facts
- arXiv ID: 2507.03153
- Source URL: https://arxiv.org/abs/2507.03153
- Authors: Weishu Deng; Yujie Yang; Peiran Du; Lingfeng Xiang; Zhen Lin; Chen Zhong; Song Jiang; Hui Lu; Jia Rao
- Reference count: 40
- Primary result: HGCA achieves near-full attention accuracy while enabling long-context inference on commodity GPUs through hybrid CPU-GPU attention computation

## Executive Summary
HGCA introduces a hybrid CPU-GPU attention mechanism that addresses GPU memory limitations in long-context LLM inference. The method performs dense attention on recent KV entries in GPU memory and sparse attention on selected salient KV entries in CPU memory, merging outputs via log-sum-exp fusion. Experiments across multiple models show HGCA achieves near-full attention accuracy, significantly outperforms sparse attention baselines in both performance and memory efficiency, and enables longer sequence and larger batch support on commodity GPUs without model retraining.

## Method Summary
HGCA uses a circular buffer for GPU KV cache and grow-able storage for CPU KV cache, with moving average attention weights tracking entry importance. Recent KV entries remain in GPU memory for dense attention while older entries are offloaded to CPU and processed via per-head sparse attention. The partial attention outputs are merged on GPU using log-sum-exp fusion, avoiding transfer of full KV tensors. The method implements fine-grained, per-head sparsification based on attention score distributions, with each head independently selecting salient KV entries based on a threshold β.

## Key Results
- Achieves perplexity within 0.2% of full attention across LLaMA, GPT-NeoX, and OPT models
- Outperforms sparse attention baselines by 2-3× in throughput while using less memory
- Enables 16K+ token generation on A6000 GPUs with batch size 1
- Maintains performance stability across varying GPU KV ratios and sparsification thresholds

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Attention Partitioning with Compute Offloading
Offloading attention computation (not just storage) to CPU can match or exceed GPU-only throughput when KV cache exceeds GPU memory, because PCIe transfer latency dominates GPU attention time for decode/append stages. Recent KV entries remain in GPU memory for dense attention; older entries offloaded to CPU memory are processed via sparse attention on CPU cores. Partial outputs are merged on GPU using only the compact attention results, not raw KV tensors.

### Mechanism 2: Per-Head Adaptive Sparsification
Fine-grained, per-head KV selection preserves accuracy better than layer-wise fixed top-k approaches because attention distributions vary substantially across heads within the same layer. Each attention head independently applies threshold β to select salient KV entries based on moving average attention weights. Heads with skewed distributions retain few entries; flatter distributions retain more.

### Mechanism 3: Log-Sum-Exp Fusion for Lossless Merging
Partial attention outputs from heterogeneous compute domains can be merged to mathematically equivalent results as single-pass full attention using only compact statistics. GPU and CPU each compute local attention outputs with associated LSE terms. Final output computed via weighted combination using shared normalization factor z = e^(lse_cpu) + e^(lse_gpu), avoiding transfer of full KV tensors.

## Foundational Learning

- **Concept: KV Cache Scaling in Autoregressive Generation**
  - Why needed here: HGCA's entire value proposition depends on understanding why KV cache grows unboundedly and becomes the primary memory bottleneck during decode.
  - Quick check question: During decode, if N=1 new token is generated and N'=10,000 cached KV entries exist, what is the ratio of memory accesses to compute operations?

- **Concept: Roofline Model (Compute-Bound vs Memory-Bound)**
  - Why needed here: The paper's core insight is that decode/append stages are memory-bound, making CPU competitive with GPU when PCIe transfers are included.
  - Quick check question: On a roofline plot, why does a memory-bound kernel with low operational intensity make CPU execution more viable relative to GPU?

- **Concept: Softmax Numerical Stability and Log-Sum-Exp**
  - Why needed here: Understanding how HGCA merges partial attention results requires knowing why naive softmax merging fails and why LSE enables stable fusion.
  - Quick check question: If you computed softmax(Q·K₁ᵀ) and softmax(Q·K₂ᵀ) separately, why can't you simply average the outputs to get the combined result?

## Architecture Onboarding

- **Component map:** GPU KV Cache Manager -> Hybrid Attention Executor -> LSE Merger <- CPU KV Cache Manager
- **Critical path:** Query arrives → GPU loads recent KV entries, CPU loads salient entries from context cache → GPU computes dense attention on recent window → CPU threads compute per-head sparse attention in parallel → Sync CPU threads → transfer O_cpu and lse_cpu via pinned memory → GPU performs in-place LSE fusion merge → Output proceeds to FFN
- **Design tradeoffs:** GPU KV ratio vs. CPU sparsification load; β threshold aggressiveness vs. accuracy; thread merging vs. oversubscription
- **Failure signatures:** Increasing TBT variance during long sequences → CPU thread load imbalance; Accuracy cliff at specific β values → sparsification too aggressive; OOM despite offloading → GPU pre-allocation too conservative
- **First 3 experiments:** 1) Micro-benchmark hybrid vs. GPU-only varying GPU KV count (256-2048) and CPU KV count (512-4096) to validate break-even points. 2) Per-head sparsification sweep across β ∈ {0.25, 0.5, 0.75, 1.0} and GPU ratios {0.25, 0.5, 0.75} on WikiText to find accuracy-stability region. 3) Long-context stress test generating 16K+ tokens with batch size 1, monitoring TBT variance and memory usage.

## Open Questions the Paper Calls Out

### Open Question 1
How can CPU-side sparse attention be optimized to reduce the observed latency variance and load imbalance in multi-threaded execution during long-context decoding? The authors observed large variations and outliers in TBT due to inefficient multi-thread scheduling and load imbalance during CPU attention. A redesigned CPU scheduling strategy demonstrating reduced TBT variance would resolve this.

### Open Question 2
What is the theoretical limit of sparsification aggressiveness (higher β values) before accuracy degrades, and can this threshold be determined dynamically per-model or per-layer? The authors note that best performance resulted in large β values and more selective KV filtering, but lack explanation for when or why degradation occurs. A systematic study correlating β thresholds with attention distribution entropy would resolve this.

### Open Question 3
Can HGCA's per-head sparsification strategy be extended to adapt dynamically within a single decoding session as attention patterns shift? The current design assumes contextual locality is stable during decoding, but attention relevance may evolve as generation progresses. Ablation experiments with periodic re-sparsification during long decoding runs would provide evidence.

## Limitations
- Critical implementation details missing (block size/count, moving average factor, thread-head mapping)
- Numerical stability verification across extreme attention score distributions not empirically validated
- Generalization to encoder-decoder architectures or bidirectional attention unexplored
- Static resource allocation without adaptive strategies for varying sequence lengths

## Confidence

**High confidence (Level 1)**: The core mechanism of offloading attention computation to CPU is validated through direct performance measurements. The mathematical correctness of log-sum-exp fusion for merging partial attention results is established.

**Medium confidence (Level 2)**: Per-head adaptive sparsification's accuracy benefits are demonstrated through perplexity comparisons, but ablation studies isolating per-head versus global strategies are lacking.

**Low confidence (Level 3)**: Long-term numerical stability under continuous generation is not empirically validated. Impact of CPU thread scheduling on tail latency for large models is discussed qualitatively but not quantitatively measured.

## Next Checks

1. **Numerical stability stress test**: Generate 50K+ tokens with HGCA using different β thresholds and GPU ratios, monitoring for NaN/Inf occurrences and LSE precision degradation. Compare against single-precision and half-precision baselines.

2. **Cross-architecture generalization**: Implement HGCA on BERT-base and measure performance degradation when applied to bidirectional attention. Document which architectural assumptions break and quantify accuracy loss.

3. **Adaptive resource allocation**: Replace static β=1.0 with an adaptive threshold that monitors per-head attention entropy and adjusts sparsification dynamically. Measure throughput-accuracy tradeoff improvements across varying sequence lengths (1K-32K tokens).