---
ver: rpa2
title: 'i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents'
arxiv_id: '2509.20971'
source_url: https://arxiv.org/abs/2509.20971
tags:
- latency
- audio
- iterations
- speech
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents i-LAVA, a low-latency voice-to-voice system\
  \ optimized for real-time conversational applications. The key contribution is optimizing\
  \ the TTS component of an end-to-end pipeline (ASR \u2192 LLM \u2192 TTS) by reducing\
  \ the number of Residual Vector Quantization (RVQ) iterations in the CSM-1B model."
---

# i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents

## Quick Facts
- arXiv ID: 2509.20971
- Source URL: https://arxiv.org/abs/2509.20971
- Authors: Anupam Purwar; Aditya Choudhary
- Reference count: 12
- One-line primary result: i-LAVA achieves RTF < 1 by reducing CSM-1B TTS RVQ iterations from 32 to 16, cutting first-chunk latency by ~50% while maintaining telephony-grade audio quality.

## Executive Summary
This work presents i-LAVA, a low-latency voice-to-voice system optimized for real-time conversational applications. The key contribution is optimizing the TTS component of an end-to-end pipeline (ASR → LLM → TTS) by reducing the number of Residual Vector Quantization (RVQ) iterations in the CSM-1B model. This trade-off reduces latency while maintaining acceptable audio quality. Experiments show that reducing RVQ iterations from 32 to 16 decreases first-chunk latency significantly (e.g., from 1382ms to 641ms on GPU) and improves the Real-Time Factor to below 1, enabling real-time voice generation. Streaming the LLM response further reduces overall latency. The optimized architecture achieves stable streaming with average inter-chunk latency close to two-thirds of chunk length, ensuring a responsive user experience.

## Method Summary
i-LAVA implements a 3-component pipeline: Silero-VAD for speech endpoint detection, Whisper-v3-large-turbo for ASR, and gpt-4o-mini for LLM processing. The CSM-1B TTS component is optimized by reducing RVQ iterations from 32 to 16 and synchronizing Mimi tokenizer codebooks accordingly. Torch.compile (JIT) is applied to the Llama 3.2 1B backbone and decoder. LLM output is streamed in 10-token chunks to TTS. The pipeline includes 1.5s silence window for VAD, warm-up generations, and hardware-optimized configurations targeting RTF < 1.

## Key Results
- Reducing RVQ iterations from 32 to 16 cuts TTS latency by ~50% (GPU: 1.2s → 0.65s) while maintaining telephony-grade audio quality
- Streaming LLM tokens to TTS every 10 tokens reduces end-to-end latency by 61% (GPU: 3.3s → 1.3s)
- RTF improves from 0.785x to 0.480x (GPU) and 1.489x to 0.934x (CPU) when reducing RVQ iterations to 16

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing RVQ iterations from 32 to 16 cuts TTS latency by ~40-50% while maintaining acceptable quality for telephony applications.
- **Mechanism:** Residual Vector Quantization (RVQ) performs sequential quantization stages where each stage processes residual errors from the previous one. Fewer iterations mean fewer sequential computations, directly reducing inference time. The Mimi tokenizer's codebook count is synchronized with RVQ iterations to match the decoder's expected input dimensions.
- **Core assumption:** The audio quality degradation from fewer quantization stages remains within acceptable bounds for the target use case (telephone-grade audio).
- **Evidence anchors:**
  - [abstract] "most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi"
  - [Table III/IV] RTF improves from 0.785x to 0.480x (GPU) and 1.489x to 0.934x (CPU) when reducing from 32 to 16 iterations
  - [corpus] SpeakStream (arXiv:2505.19206) confirms streaming TTS latency as fundamental bottleneck in conversational AI
- **Break condition:** If target application requires high-fidelity audio (SNR >30 dB), reduced RVQ iterations produce unacceptable quality degradation (SNR drops to 7-15 dB at 16 iterations).

### Mechanism 2
- **Claim:** Streaming LLM output to TTS every 10 tokens reduces end-to-end latency by 61% (GPU: 3.3s → 1.3s) compared to one-shot generation.
- **Mechanism:** Rather than waiting for complete LLM response generation, partial text tokens are forwarded to TTS immediately. This parallelizes LLM token generation with TTS audio synthesis, overlapping compute-intensive stages. First audio chunk reaches user while LLM continues generating subsequent tokens.
- **Core assumption:** TTS can produce coherent audio from incomplete sentence fragments without semantic discontinuity across chunks.
- **Evidence anchors:**
  - [Table I] Total latency with streaming: 1.28s vs without streaming: 3.33s on GPU
  - [Section III-A] "streaming the LLM response as an input for the TTS component provides significant speedup"
  - [corpus] PredGen (arXiv:2506.15556) demonstrates similar input-time speculation for LLM speech interaction
- **Break condition:** If semantic coherence across chunks degrades (e.g., mid-sentence prosody shifts), user experience suffers despite lower latency.

### Mechanism 3
- **Claim:** Inter-chunk latency lower than chunk playback duration maintains non-empty audio buffer for seamless streaming.
- **Mechanism:** Audio chunks are generated faster than they play. GPU achieves 685ms inter-chunk latency for 1600ms average chunks (ratio ~0.43), ensuring the audio queue never empties mid-stream. This creates the perception of continuous real-time response.
- **Core assumption:** Network jitter and client-side buffer management don't introduce additional delays beyond generation latency.
- **Evidence anchors:**
  - [Table II] GPU: Avg Inter-Chunk Latency 1008.9ms vs Avg Chunk Size 1550ms (ratio 0.65)
  - [Section II-B-2] "inter-chunk generation time is lesser than the length of the chunks, hence maintaining a non-empty queue"
  - [corpus] ChipChat (arXiv:2509.00078) validates cascaded streaming architecture for on-device voice agents
- **Break condition:** CPU environment shows gaps between chunks even at 16 iterations—ratio exceeds 1.0, causing audible pauses.

## Foundational Learning

- **Concept: Real-Time Factor (RTF)**
  - **Why needed here:** Primary metric determining if voice generation keeps pace with playback. RTF <1 means generation is faster than real-time.
  - **Quick check question:** If generating 10 seconds of audio takes 8 seconds, what is the RTF? (Answer: 0.8x—acceptable for real-time)

- **Concept: Residual Vector Quantization (RVQ)**
  - **Why needed here:** Core bottleneck in CSM-1B TTS. Understanding sequential quantization stages explains why iteration count directly impacts latency.
  - **Quick check question:** Why does RVQ parallelize poorly compared to standard transformer layers? (Answer: Each stage depends on residuals from previous stage—sequential dependency)

- **Concept: Voice Activity Detection (VAD) Silencing Window**
  - **Why needed here:** Determines when user has finished speaking. The 1.5s silence window balances false turn-cutting vs response delay.
  - **Quick check question:** What happens if VAD silence window is too short? (Answer: Premature response interrupts user mid-sentence)

## Architecture Onboarding

- **Component map:**
  [User Audio] → Silero-VAD → [Valid Speech] → Whisper ASR → [Transcript] → GPT-4o-mini (streaming) → [Context Audio + Text] → CSM-1B TTS ← [Partial LLM tokens every 10] → RVQ Decoder (configurable iterations: 16-32) → Mimi Audio Decoder → [Audio Chunks] → User

- **Critical path:**
  1. VAD end-of-speech detection (1.5s silence threshold)
  2. ASR transcription (parallelizable during silence window)
  3. LLM first-token generation → immediate TTS handoff
  4. TTS first-chunk generation (target: <650ms on GPU)

- **Design tradeoffs:**
  - RVQ iterations: 16 = lowest latency, lowest quality | 32 = highest quality, highest latency
  - LLM choice: gpt-4o-mini balances reasoning vs latency; larger models increase Time-to-First-Token
  - Hardware: GPU required for RTF<1 at higher quality; CPU only viable at 16 iterations
  - Streaming chunk size: Smaller = faster first audio, but more overhead per chunk

- **Failure signatures:**
  - Audio gaps during playback: Inter-chunk latency exceeds chunk duration (CPU scenario)
  - Robotic/artifacts: SNR drops below ~10 dB (16 RVQ iterations with certain audio lengths)
  - Cold start lag: First 2 inferences slow; requires warmup
  - Context incoherence: TTS produces jarring prosody if context window misconfigured

- **First 3 experiments:**
  1. **Baseline latency profile:** Run end-to-end pipeline on GPU with 32 RVQ iterations, measure each component's contribution. Expect: LLM dominant, TTS ~0.67s.
  2. **RVQ sweep:** Test 16/20/24/32 iterations on identical input text. Measure RTF, first-chunk latency, and SNR. Identify acceptable quality floor for your use case.
  3. **Streaming vs one-shot:** Compare total latency with LLM streaming (10-token chunks) vs waiting for complete response. Quantify the ~60% improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generating custom CUDA kernels for the sequential Residual Vector Quantization (RVQ) process significantly reduce latency compared to the current Python-based implementation?
- Basis in paper: [explicit] The authors state in Future Work that the RVQ process is currently sequential Python code causing CPU bottlenecks, and suggest optimizing it by generating CUDA Kernels for native GPU execution.
- Why unresolved: The current implementation relies on standard Python/PyTorch operations which involve handoffs between GPU and CPU, but the specific performance gains of a custom CUDA implementation were not tested in this study.
- What evidence would resolve it: Benchmarking the i-LAVA architecture after replacing the standard RVQ loop with a custom CUDA kernel to compare RVQ iteration times and overall RTF.

### Open Question 2
- Question: Does implementing streaming ASR effectively reduce the speech recognition latency contribution to near-zero by fitting processing within the VAD silence window?
- Basis in paper: [explicit] The paper hypothesizes in Future Work that streaming ASR could eliminate effective latency by processing audio chunks immediately upon VAD detection, rather than waiting for the complete audio capture.
- Why unresolved: The current methodology waits for the VAD to determine the end of speech before sending the full audio to ASR, leaving the optimization of overlapping ASR processing with VAD silence windows untested.
- What evidence would resolve it: A time-profiling analysis of a modified pipeline where ASR initializes chunk-by-chunk processing immediately after VAD flags speech start, measuring if the ASR completion time falls within the 1.5-second silence window.

### Open Question 3
- Question: What is the cause of the high variance in Signal-to-Noise Ratio (SNR) observed when using 32 Mimi codebooks with reduced RVQ iterations?
- Basis in paper: [explicit] In Section III.B, the authors note "high variance in SNR values going as low as -2.2341" when using padding methods to maintain 32 codebooks with reduced iterations, stating explicitly that "the cause of which is unclear."
- Why unresolved: The paper documents the anomaly as an observation but does not investigate the theoretical or implementation-level interaction between the padded codebooks and the decoder that leads to signal degradation.
- What evidence would resolve it: An ablation study analyzing the spectral characteristics of the audio generated with different padding strategies (Mean vs. Concat) to identify if specific frequencies or artifacts correlate with the SNR drops.

### Open Question 4
- Question: Can the Llama 3.2 backbone and decoder be replaced by more efficient models to further decrease TTS latency without losing contextual understanding?
- Basis in paper: [explicit] The Future Work section suggests that "further experimentation may be conducted to replace the Llama 3.2 Backbone and Decoder... with more efficient models."
- Why unresolved: While the authors optimized the *usage* of the existing CSM-1B model (RVQ iterations), they did not explore substituting the underlying model architecture, which remains a major component of the processing time.
- What evidence would resolve it: Comparative benchmarks of the V-2-V pipeline using smaller or distilled decoder models against the current Llama 3.2 100M decoder to measure latency reduction versus conversational quality.

## Limitations

- CSM-1B TTS configuration details (RVQ iterations and Mimi codebook synchronization) are not fully documented, creating reproducibility challenges
- Hardware dependency: RTF < 1 only achievable on GPU; CPU implementation shows audio gaps and higher latency
- Quality degradation at 16 RVQ iterations may not meet requirements for applications beyond telephony

## Confidence

- **High Confidence:** Streaming LLM tokens to TTS reduces end-to-end latency by 61% (supported by Table I experimental data)
- **Medium Confidence:** Reducing RVQ iterations from 32 to 16 significantly cuts TTS latency while maintaining telephony-grade audio quality (supported by RTF and latency metrics, but SNR quantification missing)
- **Low Confidence:** Inter-chunk latency lower than chunk playback duration maintains seamless streaming (theoretically sound but CPU validation shows gaps)

## Next Checks

1. **Reproduce RVQ Configuration:** Verify the exact configuration of CSM-1B with 16 RVQ iterations and 16 Mimi codebooks. Measure the impact on RTF, first-chunk latency, and SNR to ensure the claimed trade-offs between latency and quality are achievable.

2. **Hardware Performance Validation:** Conduct experiments on both GPU and CPU to confirm the RTF < 1 on GPU and identify the inter-chunk latency issues on CPU. This will help validate the hardware dependency claim and explore potential optimizations for CPU environments.

3. **Streaming Coherence Testing:** Evaluate the semantic coherence of TTS output when streaming LLM tokens every 10 tokens. Assess whether the audio maintains continuity and natural prosody across chunks, ensuring the user experience is not compromised by the reduced latency.