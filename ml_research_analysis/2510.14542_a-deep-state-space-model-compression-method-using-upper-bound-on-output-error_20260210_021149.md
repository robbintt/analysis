---
ver: rpa2
title: A Deep State-Space Model Compression Method using Upper Bound on Output Error
arxiv_id: '2510.14542'
source_url: https://arxiv.org/abs/2510.14542
tags:
- deep
- output
- error
- reduced
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a compression method for deep state-space
  models (Deep SSMs) using model order reduction (MOR) with a provable output error
  guarantee. The key contributions include: (1) Deriving an upper bound on the output
  error between two Deep SSMs, showing that minimizing the h2 error of each internal
  linear quadratic-output (LQO) system reduces the overall output error; (2) Formulating
  an optimization problem to minimize this bound while preserving Deep SSM-specific
  properties; (3) Developing a gradient-based MOR method with a stationary point guarantee.'
---

# A Deep State-Space Model Compression Method using Upper Bound on Output Error

## Quick Facts
- arXiv ID: 2510.14542
- Source URL: https://arxiv.org/abs/2510.14542
- Authors: Hiroki Sakamoto; Kazuhiro Sato
- Reference count: 21
- Reduces ~80% of trainable parameters without retraining, with only 4-5% performance drop

## Executive Summary
This paper presents a compression method for deep state-space models (Deep SSMs) using model order reduction (MOR) with a provable output error guarantee. The key contributions include: (1) Deriving an upper bound on the output error between two Deep SSMs, showing that minimizing the h2 error of each internal linear quadratic-output (LQO) system reduces the overall output error; (2) Formulating an optimization problem to minimize this bound while preserving Deep SSM-specific properties; (3) Developing a gradient-based MOR method with a stationary point guarantee. On the IMDb task from the Long Range Arena benchmark, the proposed method achieves strong performance.

## Method Summary
The method compresses pretrained Deep SSMs by reducing the state dimension of each LQO system layer while preserving Deep SSM properties (diagonal stable A, LQO outputs, residual connections, LayerNorm). It derives an upper bound on the output error and minimizes this bound using gradient descent with Armijo backtracking. The optimization problem balances h² error minimization with stability constraints. For diagonal A matrices, the h² error computation simplifies to O(nm) elementwise operations via Sylvester equations. The method initializes reduced models using TLBT or TLH2 techniques and provides a stationary point guarantee.

## Key Results
- Reduces roughly 80% of trainable parameters without retraining, with only a 4-5% performance drop
- Achieves strong performance on IMDb task from Long Range Arena benchmark
- Provides provable upper bound on output error between original and compressed Deep SSMs

## Why This Works (Mechanism)
The method works by leveraging the structure of Deep SSMs to derive a tight upper bound on output error. By minimizing the h² error of each internal LQO system, the overall output error is reduced. The gradient-based optimization ensures local optimality while preserving the diagonal stable A property crucial for Deep SSMs.

## Foundational Learning
- **Model Order Reduction (MOR)**: Reducing system complexity while preserving essential dynamics; needed to compress Deep SSMs; quick check: verify reduced model retains input-output behavior.
- **h² error**: Measure of system approximation quality; needed to quantify compression error; quick check: confirm h² error decreases during optimization.
- **Sylvester equation**: AX + XB = C; needed for efficient h² error computation; quick check: verify O(nm) elementwise computation for diagonal A.
- **Armijo backtracking**: Line search method ensuring sufficient decrease; needed for stable gradient descent; quick check: confirm objective decreases each iteration.
- **HiPPO initialization**: Structured initialization for SSMs; needed for pretraining; quick check: verify convergence from HiPPO init.

## Architecture Onboarding
**Component map**: HiPPO-initialized Deep SSM -> LQO layers -> h² error computation -> gradient-based MOR -> compressed Deep SSM
**Critical path**: Input sequence -> Deep SSM layers -> output -> h² error bound -> gradient computation -> parameter update
**Design tradeoffs**: Higher compression (smaller r) vs. accuracy retention; stability constraints vs. flexibility in parameter space
**Failure signatures**: If A becomes unstable → backtracking stalls; if h² error increases → learning rate too high; if TestAcc drops significantly → poor initialization
**First experiments**: 1) Verify h² error computation for diagonal A matches O(nm) elementwise formula; 2) Test Armijo backtracking with synthetic objective; 3) Confirm TLBT/TLH2 initialization produces stable reduced models

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown pretraining details (optimizer, LR, batch size) not specified
- TLBT/TLH2 initialization procedures from external works may require adjustment for diagonal A case
- Rank parameter c for cross-gramian objective unclear in selection criteria
- LayerNorm ε value only specified as >0 without exact value

## Confidence
- High confidence in theoretical framework and gradient computation for diagonal A cases
- Medium confidence in practical implementation of Algorithm 1 (stability constraints, backtracking)
- Medium confidence in baseline comparisons (TLBT/TLH2) due to external method dependencies

## Next Checks
1. Verify the h²_L objective reduction on a small-scale example (e.g., r=4) and confirm that gradient descent with Armijo backtracking decreases the objective monotonically before applying to full model.
2. Check the eigenvalues of A matrices after TLBT/TLH2 initialization for each r in r_list; if any |λ| ≥ 1, implement corrective initialization or adjust target dimensions.
3. Compare the final compressed model's accuracy (before retraining) against the HiPPO-initialized baseline to ensure the objective reduction correlates with task performance, particularly for r=32 and r=16 where largest parameter savings occur.