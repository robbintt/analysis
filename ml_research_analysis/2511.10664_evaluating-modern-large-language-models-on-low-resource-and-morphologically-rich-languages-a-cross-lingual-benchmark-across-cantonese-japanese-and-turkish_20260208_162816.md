---
ver: rpa2
title: Evaluating Modern Large Language Models on Low-Resource and Morphologically
  Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish
arxiv_id: '2511.10664'
source_url: https://arxiv.org/abs/2511.10664
tags:
- language
- cantonese
- evaluation
- languages
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new cross-lingual benchmark to evaluate
  seven state-of-the-art large language models on low-resource and morphologically
  rich languages, focusing on Cantonese, Japanese, and Turkish. The benchmark includes
  four tasks: open-domain question answering, document summarization, English-to-X
  translation, and culturally grounded dialogue.'
---

# Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish

## Quick Facts
- arXiv ID: 2511.10664
- Source URL: https://arxiv.org/abs/2511.10664
- Reference count: 40
- Key outcome: New benchmark shows large proprietary models outperform smaller open-source models on low-resource languages, with significant gaps in culturally nuanced understanding and morphological generalization.

## Executive Summary
This paper introduces a comprehensive cross-lingual benchmark to evaluate seven state-of-the-art large language models on low-resource and morphologically rich languages, specifically Cantonese, Japanese, and Turkish. The benchmark covers four tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. Through both human evaluations and automated metrics, the study reveals that while the largest proprietary models (GPT-4o, GPT-4, Claude 3.5) generally lead across languages and tasks, significant performance gaps remain in culturally nuanced understanding and morphological generalization, particularly for Cantonese and Turkish. The study provides detailed quantitative results, qualitative error analysis, and releases the benchmark and evaluation data to foster reproducibility and further research.

## Method Summary
The benchmark employs standardized prompts across seven models (three proprietary, four open-source) with temperature=0 greedy decoding for reproducibility. Four tasks are evaluated: open-domain QA using 100 questions per language from TyDi QA, XQuAD, and custom sources; document summarization with 50 pairs per language from Wikinews, MLSUM, and Hong Kong news; English-to-X translation using 100 pairs per language from WMT, TED talks, and custom sources; and culturally grounded dialogue with 15 scenarios per language written by native speakers. Automated metrics (BLEU, ROUGE, COMET, BERTScore) are computed alongside human evaluations by three native speakers per language using 5-point Likert scales for fluency, factual accuracy, and cultural appropriateness, with inter-annotator agreement measured at ρ≈0.76.

## Key Results
- GPT-4o achieves 78% accuracy on Cantonese QA versus 74% for GPT-4, showing superior language consistency
- All models struggle with Turkish agglutinative morphology, with smaller models often failing to produce correct case endings or plural suffixes
- Cultural appropriateness ratings average 4.5/5 for top proprietary models but only 2-3/5 for smaller open-source models
- LLaMA-2 13B hallucinated "Vecihi Hürkuş" (male aviator) instead of the correct "Sabiha Gökçen" (female pilot) on a Turkish question

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Larger proprietary models achieve better cross-lingual performance through scale-enabled transfer from high-resource training data to low-resource target languages.
- **Mechanism:** Increased parameter count and training data diversity enable implicit cross-lingual representations to emerge, allowing knowledge acquired in English/high-resource languages to transfer to underrepresented languages through shared semantic embeddings.
- **Core assumption:** The paper assumes that scale and multilingual training data exposure are the primary drivers of cross-lingual capability, not architectural innovations specific to low-resource languages.
- **Evidence anchors:**
  - [abstract] "the largest proprietary models (GPT-4o, GPT-4, Claude 3.5) generally lead across languages and tasks"
  - [Section 4.1] "GPT-4o answered 78% of Cantonese questions correctly versus 74% for GPT-4... GPT-4o almost never made that mistake [answering in wrong language]"
  - [corpus] Related work on Paramanu (arxiv 2401.18034) explicitly addresses "tokenizer oversegmentation for morphologically rich low-resource languages" with architecture-level solutions, suggesting scale alone is insufficient.

### Mechanism 2
- **Claim:** Morphological richness degrades LLM performance through tokenization fragmentation and compositional generalization failures.
- **Mechanism:** Agglutinative languages produce long word forms via root+affix combinations that standard subword tokenizers fragment excessively, reducing token efficiency and requiring models to learn compositional morphology implicitly—a capability that emerges unevenly across model scales.
- **Core assumption:** Assumption: Tokenizer design, rather than architectural attention mechanisms, is the primary bottleneck for morphologically rich languages.
- **Evidence anchors:**
  - [abstract] "all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology"
  - [Section 2] "Turkish is a highly agglutinative language with rich morphology, often yielding very long word forms from root+affix combinations. This can stress a model's ability to generalize compositionally to unseen word forms."
  - [corpus] Tokenization Standards for Linguistic Integrity (arxiv 2502.07057) directly addresses Turkish tokenization as a benchmark, confirming tokenization as a causal factor.

### Mechanism 3
- **Claim:** Cultural appropriateness requires explicit cultural context in training data; it does not emerge purely from linguistic competence.
- **Mechanism:** Cultural grounding operates through exposure to culture-specific dialogues, idioms, and pragmatic conventions during training. Models default to culturally agnostic or Western-centric behavior when training data lacks cultural diversity, regardless of fluency in the target language.
- **Core assumption:** Assumption: Cultural competence is learned from cultural content in training data, not derived from general reasoning capabilities.
- **Evidence anchors:**
  - [abstract] "significant gaps remain in culturally nuanced understanding... particularly for Cantonese and Turkish"
  - [Section 4.2] "GPT-4o showed superior adaptation to local norms (choice of words, humor). Nonetheless, even top models are imperfect"
  - [Section 5] "Smaller models often defaulted to generic, culture-agnostic behavior, indicating that cultural sensitivity may emerge only beyond a certain model capacity and training diversity."

## Foundational Learning

- **Concept: Agglutinative Morphology**
  - **Why needed here:** Turkish morphology creates compositional word forms (e.g., "ev-ler-iniz-den" = "from your houses") that test whether LLMs can decompose and generate morphologically complex tokens correctly.
  - **Quick check question:** Can you explain why a subword tokenizer might produce 8+ tokens for a single Turkish word while producing 1-2 tokens for an equivalent English phrase?

- **Concept: Cross-Lingual Transfer**
  - **Why needed here:** The benchmark assumes knowledge can transfer from English/high-resource pretraining to low-resource evaluation; understanding this transfer explains why larger models outperform smaller ones on languages they weren't explicitly trained on.
  - **Quick check question:** If a model trained primarily on English achieves 78% accuracy on Cantonese QA, what does this imply about the relationship between its English and Cantonese representations?

- **Concept: Human Evaluation Reliability for Multilingual NLG**
  - **Why needed here:** Automated metrics (BLEU, ROUGE) fail to capture cultural appropriateness and nuanced fluency; this paper relies on native-speaker Likert ratings (inter-annotator agreement ρ=0.76) as ground truth.
  - **Quick check question:** Why might a translation receive high BLEU but low cultural appropriateness scores from native speakers?

## Architecture Onboarding

- **Component map:**
```
Benchmark Pipeline
├── Task Modules (4)
│   ├── Open QA (100 prompts/lang) → Exact-match accuracy
│   ├── Summarization (50 docs/lang) → ROUGE-L, BERTScore
│   └── Translation (100 EN→X pairs/lang) → BLEU, COMET
│   └── Cultural Dialogue (15 scenarios/lang) → Human ratings only
├── Model Layer (7 models)
│   ├── Proprietary: GPT-4o, GPT-4, Claude 3.5 (API access)
│   └── Open-source: LLaMA 3.1 70B, Mistral Large 2, LLaMA-2 13B, Mistral 7B
├── Evaluation Layer
│   ├── Automated: SacreBLEU, ROUGE, COMET, BERTScore
│   └── Human: 3 native speakers/lang, 5-point Likert (fluency, accuracy, cultural)
└── Output: Per-language, per-task performance tables + qualitative error analysis
```

- **Critical path:**
  1. Prompt standardization across models (minimize prompt-design confounds)
  2. Temperature=0 greedy decoding for reproducibility
  3. Human evaluation with hidden model identity to prevent bias
  4. Inter-annotator agreement measurement (Spearman's ρ)
  5. Discrepancy resolution through discussion

- **Design tradeoffs:**
  - **Parallel vs. natural datasets:** Paper chose naturalness over strict parallelism across languages (cultural authenticity > cross-lingual comparability)
  - **Automated vs. human metrics:** Paper prioritizes human judgment for final conclusions, using automated metrics only for initial screening
  - **Scale vs. architecture testing:** Paper tests scale (7B to 405B params) but not architectural variations (all are transformer-based)

- **Failure signatures:**
  - **Language mixing:** LLaMA-3.1 produced Cantonese output with Mandarin vocabulary (的 instead of 嘅)
  - **Register mismatch:** LLaMA-2 13B used casual speech in formal Japanese contexts
  - **Hallucination:** LLaMA-2 13B answered "Vecihi Hürkuş" (male aviator) instead of "Sabiha Gökçen" (correct female pilot)
  - **Complete failure:** Mistral 7B returned "No answer" or irrelevant responses on Cantonese QA

- **First 3 experiments:**
  1. **Reproduce baseline:** Run the 4 benchmark tasks on GPT-4o and Mistral 7B for Turkish only; verify you observe the ~30-point QA accuracy gap reported (85% vs. 55%). This validates your evaluation pipeline.
  2. **Tokenizer analysis:** Extract and count tokens for 10 Turkish agglutinative words vs. their English equivalents across all 7 models' tokenizers. Hypothesis: Models with lower token fertility on Turkish will show better QA/translation scores.
  3. **Ablate prompt language:** For Cantonese QA, provide prompts in English with "Answer in Cantonese" instruction vs. prompts in Cantonese. Measure whether GPT-4o's advantage persists, testing whether its performance stems from multilingual understanding vs. translation-as-crutch strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific training data or architectural modifications enable LLMs to achieve culturally grounded dialogue performance comparable to native speakers in low-resource languages like Cantonese?
- **Basis in paper:** [explicit] Authors state that "cultural sensitivity may emerge only beyond a certain model capacity and training diversity" and call for research on "culturally aware fine-tuning and evaluation strategies."
- **Why unresolved:** The paper demonstrates that even top proprietary models (GPT-4o, Claude 3.5) average only ~4.5/5 on cultural appropriateness, with smaller models scoring 2-3/5, but does not identify which specific factors drive this performance gap.
- **What evidence would resolve it:** Ablation studies varying training data composition, model scale, and fine-tuning approaches, measuring cultural appropriateness scores systematically across conditions.

### Open Question 2
- **Question:** How can tokenization schemes be optimized for morphologically rich languages like Turkish to reduce the morphological generalization errors observed in current LLMs?
- **Basis in paper:** [explicit] The authors note that "morphological complexity and cultural nuance remain difficult even for the largest models" and call for "building adaptive tokenization and morphology-sensitive architectures."
- **Why unresolved:** The error analysis shows smaller models "often failed to produce correct case endings or plural suffixes" and even large models made "minor morphological omissions," but the paper does not test whether alternative tokenization approaches would help.
- **What evidence would resolve it:** Comparative evaluation of models trained with morphology-aware tokenizers (e.g., Paramanu-style or contextual morphological tokenization) versus standard tokenizers on Turkish morphological generalization benchmarks.

### Open Question 3
- **Question:** To what extent does the Cantonese performance gap stem from data scarcity versus interference from Mandarin-dominant training corpora?
- **Basis in paper:** [inferred] The paper notes that "training open models on Mandarin-dominant corpora limits their Cantonese ability" and that smaller models "mixed Mandarin phrasing" (e.g., using 的 instead of 嘅), but does not disentangle whether the primary barrier is insufficient Cantonese data or negative transfer from Mandarin.
- **Why unresolved:** The study evaluates only final model performance, not the underlying causes of errors specific to Cantonese-Mandarin interference.
- **What evidence would resolve it:** Controlled experiments measuring Cantonese performance after fine-tuning on varying amounts of Cantonese data, with and without Mandarin data ablation, quantifying interference effects.

### Open Question 4
- **Question:** How reliably can automated evaluation metrics (BLEU, ROUGE, COMET) or LLM-as-a-judge approaches substitute for human evaluation in low-resource, culturally nuanced tasks?
- **Basis in paper:** [explicit] Authors observe that "metrics and human judgments diverged" and that "LLM-as-a-judge approaches show promise for scalable evaluation but risk bias if the judge shares the evaluated model's limitations."
- **Why unresolved:** The paper reports moderate correlation (Pearson r ≈ 0.6) for GPT-4-as-judge but does not systematically test alternative automated evaluation methods or establish thresholds for when automated metrics become unreliable.
- **What evidence would resolve it:** Large-scale comparison of multiple automated metrics against human judgments across all benchmark tasks, with analysis of error types where automation fails most severely.

## Limitations

- Dataset size remains relatively small (100 QA questions, 50 summaries, 15 dialogue scenarios per language), potentially limiting statistical robustness
- Evaluation focuses on only three morphologically rich languages from different families, raising questions about generalizability to other language groups
- Temperature=0 greedy decoding may not reflect typical deployment scenarios where stochastic sampling could yield different quality outcomes

## Confidence

**High Confidence (8/10):**
- Proprietary models (GPT-4o, GPT-4, Claude 3.5) consistently outperform open-source alternatives across all tasks and languages
- Significant performance gaps exist between largest models (405B+ parameters) and smaller open models (7B-70B parameters)
- Cantonese presents unique challenges due to limited training data and mixed-script nature, reflected in consistently lower performance across all models

**Medium Confidence (6/10):**
- Morphological richness directly degrades performance through tokenization fragmentation effects
- Cultural appropriateness emerges primarily from training data exposure rather than general reasoning capabilities
- Scale is the dominant factor explaining cross-lingual transfer success, outweighing architectural innovations

**Low Confidence (4/10):**
- The specific numerical performance gaps would persist with larger evaluation datasets
- The identified error patterns (language mixing, register mismatch, hallucination) represent the full spectrum of model failures
- The benchmark's task distribution adequately captures the full range of cross-lingual challenges

## Next Checks

1. **Scale the evaluation dataset**: Reproduce the entire benchmark using 3× the current dataset size (300 QA questions, 150 summaries, 45 dialogue scenarios per language) to verify whether the reported performance gaps and error patterns persist under increased statistical power.

2. **Architectural ablation study**: Test morphology-aware tokenizers (e.g., Paramanu-style or contextual morphological tokenization) on LLaMA 3.1 70B specifically for Turkish and Japanese tasks, comparing performance against the standard tokenizer results reported.

3. **Cross-linguistic generalization test**: Apply the same benchmark methodology to a fourth morphologically rich low-resource language from a different family (e.g., Tamil or Swahili) to assess whether the observed patterns (scale advantages, tokenization challenges, cultural grounding issues) generalize beyond the current language sample.