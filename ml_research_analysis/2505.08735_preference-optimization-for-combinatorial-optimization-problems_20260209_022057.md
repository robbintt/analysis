---
ver: rpa2
title: Preference Optimization for Combinatorial Optimization Problems
arxiv_id: '2505.08735'
source_url: https://arxiv.org/abs/2505.08735
tags:
- optimization
- preference
- reward
- learning
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Preference Optimization (PO), a novel method
  for solving combinatorial optimization problems using reinforcement learning. PO
  addresses key challenges in existing RL approaches, such as diminishing reward signals
  and inefficient exploration in large action spaces.
---

# Preference Optimization for Combinatorial Optimization Problems

## Quick Facts
- arXiv ID: 2505.08735
- Source URL: https://arxiv.org/abs/2505.08735
- Reference count: 40
- Primary result: Preference Optimization (PO) outperforms existing RL methods on TSP, CVRP, and FFSP benchmarks with faster convergence and better solution quality.

## Executive Summary
This paper introduces Preference Optimization (PO), a novel method for solving combinatorial optimization problems using reinforcement learning. PO addresses key challenges in existing RL approaches, such as diminishing reward signals and inefficient exploration in large action spaces. It transforms quantitative rewards into qualitative preference signals, enabling more stable learning and better exploration. PO is derived from an entropy-regularized RL objective, which aligns the policy with preferences while avoiding intractable computations. Additionally, PO integrates local search techniques into the fine-tuning process, improving solution quality without extra inference time.

## Method Summary
Preference Optimization (PO) replaces traditional REINFORCE with a preference-based learning framework. The method samples multiple solutions per problem instance, computes pairwise preference labels based on reward comparisons, and optimizes a loss that maximizes the likelihood of these preferences. PO is derived from an entropy-regularized RL objective where rewards are reparameterized in terms of policy log-probabilities, avoiding the need to compute intractable partition functions. The method also incorporates local search during fine-tuning, treating LS outputs as expert demonstrations to improve policy performance without incurring inference-time costs.

## Key Results
- PO achieves faster convergence and superior solution quality compared to REINFORCE on TSP, CVRP, and FFSP benchmarks
- The method successfully integrates local search techniques during fine-tuning, improving solution quality without extra inference time
- PO demonstrates stability across different problem scales, with optimal performance achieved using problem-specific temperature parameters (α)

## Why This Works (Mechanism)

### Mechanism 1: Qualitative Signal Stabilization
Transforming quantitative rewards into qualitative preference signals stabilizes training by mitigating the vanishing gradient problem common in late-stage RL training. Standard REINFORCE suffers as advantage values approach zero when the policy improves, causing gradients to vanish. PO maintains stable gradient magnitude regardless of reward scale by using binary preference labels.

### Mechanism 2: Entropy-Regularized Reparameterization
Reparameterizing the reward function in terms of policy log-probability allows tractable optimization without explicitly computing the partition function Z(x). By expressing reward differences as α(log π(τ₁) - log π(τ₂)), the method bypasses intractable calculations through pairwise comparisons.

### Mechanism 3: Imitation-Style Local Search Fine-Tuning
Integrating local search into the fine-tuning phase allows the policy to internalize heuristic improvements without inference-time costs. PO uses LS to generate high-quality preference pairs during training, treating LS outputs as expert demonstrations in an imitation learning setup.

## Foundational Learning

- **Entropy-Regularized Reinforcement Learning**
  - Why needed here: The entire PO derivation depends on the maximum entropy objective, which encourages stochastic policies and controls the exploration-exploitation trade-off through temperature parameter α.
  - Quick check question: What happens to the optimal policy π* if the reward r is scaled by a constant factor k? (Answer: The optimal policy remains the same if α is adjusted accordingly, or essentially the "shape" of the distribution is preserved relative to the scaling).

- **Bradley-Terry Model**
  - Why needed here: This statistical model provides the link between latent reward difference and observed preference probability, justifying why maximizing preference likelihood implicitly maximizes reward difference.
  - Quick check question: In the Bradley-Terry model, if the probability of preferring τ₁ over τ₂ is 0.5, what is the difference in their underlying "strength" scores? (Answer: 0).

- **Auto-regressive Decoding in Neural CO**
  - Why needed here: PO requires calculating log π(τ|x), which is the product of step-wise probabilities. Understanding sequential solution construction is necessary for correct gradient calculation.
  - Quick check question: When applying PO to TSP, how is the probability of a full tour τ calculated from the decoder's outputs? (Answer: It is the product of the probabilities of selecting the next city at each step).

## Architecture Onboarding

- **Component map:**
  Problem Encoder -> Solution Decoder -> Reward Function -> Preference Module -> Optimization Head

- **Critical path:**
  1. Batch Sampling: Sample B instances and N solutions each
  2. Preference Calculation: Compute N × N preference matrix based on rewards
  3. Log-Prob Extraction: Extract log π(τ) for all samples
  4. Loss Calculation: Compute pairwise likelihood loss using preference matrix and log-probs

- **Design tradeoffs:**
  - Bradley-Terry vs. Exponential Model: BT for stable convergence on easier tasks; Exponential for final performance or complex problems
  - Alpha (α) Tuning: Low α (0.01-0.05) for models with built-in exploration; High α (1.0-2.0) for models needing entropy injection

- **Failure signatures:**
  - Gradient Saturation: Bradley-Terry with large α causes gradient vanishing
  - Slow Convergence: Too small α causes early greedy behavior and local optima
  - Numerical Instability: Very negative log-prob values require high precision

- **First 3 experiments:**
  1. Sanity Check (TSP-20): Train POMO backbone with PO, verify faster convergence than REINFORCE, check optimality gap < 0.1%
  2. Alpha Ablation: Sweep α ∈ {0.01, 0.1, 1.0} on TSP-100, plot trajectory entropy over time
  3. Fine-tuning Integration: Train base model, run "Fine-tuning with Local Search" phase, verify policy improvement without LS at inference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How stable is the reparameterized reward function across a more diverse range of Combinatorial Optimization Problems (COPs)?
- Basis in paper: The conclusion states, "The stability of our reparameterized reward function across diverse COPs warrants comprehensive investigation."
- Why unresolved: Current experiments focus on routing and scheduling, but theoretical assumptions may not hold uniformly across all COP structures.
- What evidence would resolve it: Theoretical analysis or empirical benchmarks on non-routing COPs demonstrating consistent convergence behavior.

### Open Question 2
- Question: Can Preference Optimization be effectively adapted for multi-objective optimization problems?
- Basis in paper: Authors identify applying PO to "problems where reward signals are difficult to design... such as multi-objective optimization" as valuable direction.
- Why unresolved: Method relies on scalar rewards to generate clear preference pairs; conflicting objectives may introduce transitivity issues.
- What evidence would resolve it: Successful extension to multi-objective benchmarks without requiring explicit scalarization.

### Open Question 3
- Question: What is the theoretical rationale for selecting specific preference models based on problem complexity?
- Basis in paper: Section 4.2 suggests "exploring the rationale behind the choice of preference models on different problems" as research direction.
- Why unresolved: Empirical results show Exponential outperforms Bradley-Terry on large-scale problems, but theoretical cause regarding landscape optimization remains unclear.
- What evidence would resolve it: Formal analysis linking preference function properties to optimization landscape curvature of specific COPs.

## Limitations
- Evaluation Scope: Experiments focus on standard benchmarks with fixed instance distributions; real-world variants remain unverified
- Scalability Assumptions: O(N²) preference computation may become prohibitive for large N; computational efficiency gains lack direct empirical support
- Generalization of α Tuning: Heuristic α ranges provided without systematic sensitivity analysis across diverse problem classes

## Confidence
- **High:** Core mechanism of transforming rewards into preferences to avoid vanishing gradients is well-supported by entropy-regularized RL derivation and experimental results
- **Medium:** Integration of local search via preference learning is plausible but long-term stability across diverse COPs not fully validated
- **Low:** Claims about computational efficiency gains lack direct empirical support; runtime overhead of pairwise preference computation not benchmarked

## Next Checks
1. **α Sensitivity Analysis:** Systematically sweep α ∈ {0.01, 0.05, 0.1, 1.0} across TSP, CVRP, and FFSP; measure final optimality gap and training stability
2. **Runtime Benchmarking:** Compare wall-clock training time of PO versus REINFORCE on identical hardware, isolating O(N²) preference computation cost
3. **Cross-COP Generalization:** Apply PO to a non-standard combinatorial problem (e.g., Job Shop Scheduling with blocking constraints); evaluate whether same α ranges generalize or require task-specific tuning