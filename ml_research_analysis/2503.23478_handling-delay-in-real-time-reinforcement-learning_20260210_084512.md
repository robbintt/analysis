---
ver: rpa2
title: Handling Delay in Real-Time Reinforcement Learning
arxiv_id: '2503.23478'
source_url: https://arxiv.org/abs/2503.23478
tags:
- skip
- layers
- connections
- delay
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses delays in real-time reinforcement learning
  caused by parallel computations of neurons. The authors propose temporal skip connections
  combined with history-augmented observations to reduce observational delay and improve
  training stability.
---

# Handling Delay in Real-Time Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.23478
- Source URL: https://arxiv.org/abs/2503.23478
- Authors: Ivan Anokhin; Rishav Rishav; Matthew Riemer; Stephen Chung; Irina Rish; Samira Ebrahimi Kahou
- Reference count: 40
- Primary result: Temporal skip connections with history augmentation reduce observational delay regret and restore Markovian property, enabling strong RL performance under parallel computation constraints.

## Executive Summary
This paper addresses the challenge of observational delay in real-time reinforcement learning caused by parallel layer computations. When neural network layers execute in parallel, each layer processes data from a different timestep, creating a delay between observation and action. The authors propose temporal skip connections combined with history-augmented observations to mitigate this delay. Skip connections create temporal shortcuts that reduce the effective delay, while history augmentation makes the delayed decision process Markovian by including recent actions in the state. The approach is theoretically grounded with regret bounds showing exponential improvement over vanilla feedforward networks, and empirically validated across MuJoCo, MinAtar, and MiniGrid environments.

## Method Summary
The approach introduces temporal skip connections in parallel-computed neural networks to reduce observational delay, combined with history augmentation to restore the Markov property. During parallel execution, each layer processes the next input immediately after producing output for the current timestep, creating an N×δ delay for N-layer networks. Skip connections allow recent observations to influence actions through fewer layers, reducing effective delay from O(Nδ) to O(δ). History augmentation includes recent actions in the state representation, making transitions stationary during learning. The method is implemented in SAC for continuous control (MuJoCo) and PPO for discrete control (MinAtar/MiniGrid), with critic trained without delay and actor trained via backpropagation through unrolled sub-trajectories under parallel computation constraints.

## Key Results
- Skip connections reduce observational delay regret from O(Nδ) to O(δ), exponentially improving performance in stochastic environments
- History augmentation stabilizes training by restoring Markovian property in delayed decision processes
- Parallel execution achieves 6-350% inference acceleration on GPU while maintaining competitive RL performance
- Agents match or exceed baseline methods across MuJoCo, MinAtar, and MiniGrid environments despite computational delays

## Why This Works (Mechanism)

### Mechanism 1: Temporal Skip Connections Reduce Delay Regret
Skip connections reduce observational delay from O(Nδ) to O(δ), exponentially improving regret bounds. In parallel computation, each layer processes data from a different timestep. Skip connections create temporal shortcuts—allowing recent observations to influence actions through fewer layers. This reduces the effective delay between observation and action. Core assumption: The environment has measurable stochasticity (p_minimax); deterministic environments show less pronounced gains.

### Mechanism 2: History Augmentation Restores Markovian Property
Augmenting observations with recent actions makes the delayed decision process Markovian, stabilizing training. Without access to intermediate actions, transition probabilities depend on the changing policy π(a_t-1|s_t-2), which is non-stationary during learning. Including past actions in the state removes this dependency. Core assumption: The agent has access to its own action history; requires sufficient memory buffer.

### Mechanism 3: Parallel Layer Computation Increases Throughput
Pipelined layer execution accelerates inference by 6-350% on GPU hardware while creating the delay problem that mechanisms 1-2 solve. Each layer processes the next input immediately after producing output for the current one. Layers work in parallel on different timesteps, maximizing hardware utilization. Core assumption: Hardware supports parallel matrix operations; layer execution times are approximately uniform.

## Foundational Learning

- **Markov Property in MDPs**: Understanding why non-Markovian transitions destabilize RL is essential. The delayed observation problem creates hidden state dependencies. Quick check: Can you explain why knowing only s_t-Nδ (without intermediate actions) makes transition probabilities non-stationary during policy updates?

- **Pipelining and Throughput vs. Latency**: The parallel computation framework increases throughput but increases effective latency per decision. Understanding this trade-off is essential. Quick check: If each layer takes 2ms and you have 5 layers, what is the latency and throughput for sequential vs. pipelined execution?

- **Skip Connections (Residual vs. Temporal)**: Traditional skip connections aid gradient flow; temporal skip connections additionally reduce delay. The paper leverages both properties differently. Quick check: How does a skip connection from layer 1 to layer 3 in a pipelined system reduce the observation-to-action delay?

## Architecture Onboarding

- **Component map**: Actor network (N-layer MLP/CNN with temporal skip connections) -> History buffer (stores recent states/actions) -> Parallel execution engine (manages layer-wise pipelining) -> Critic network (trained independently without delay)

- **Critical path**: 1) Initialize hidden activations h⁰_0, ..., h^N_0 to zeros; 2) At each timestep: receive observation s_t, query actor with current hidden states; 3) Actor produces action a_t and updated hidden states for each layer; 4) Store transition in buffer; sample sub-trajectories for training; 5) Update critic online (no delay); update actor via backpropagation through unrolled sub-trajectory

- **Design tradeoffs**: More layers provide higher expressivity but increase delay (mitigated by skip connections); skip connection type balances speed and capacity; augmentation depth improves Markovian property but increases input dimensionality

- **Failure signatures**: Performance collapse without augmentation (non-stationary transitions cause unstable Q-values); no speed-up on CPU (thread synchronization overhead for >20 layers); performance gap in complex environments (reduced expressivity bottlenecks performance)

- **First 3 experiments**: 1) Baseline comparison: Run MLP with and without skip connections on HalfCheetah-v4 with neuron execution time δ=1; 2) Ablation of augmentation: Compare skip-connection agent with/without action history on Walker2d-v4; 3) Layer count sensitivity: Test 2-layer vs. 3-layer vs. 4-layer architectures with skip connections on Ant-v4 with δ=4

## Open Questions the Paper Calls Out

- **How does the parallel computation framework perform under stochastic neuron execution times (δ)?**
The authors state: "One important assumption we make... is that we have a fixed neuron execution time (δ) which is not the case in real world environments where δ can be stochastic. We propose this as a future line of work..." The theoretical proofs rely on fixed delay intervals, but real-world hardware latencies fluctuate, potentially destabilizing the Markov property or regret bounds.

- **Can the temporal skip connection approach scale effectively to deep neural networks (beyond 5 layers) without requiring specialized training tricks?**
The Limitations section notes: "we limit our experiments to at most a 5-layer neural network, as scaling vanilla RL methods to deeper architectures is non-trivial and often requires additional losses or training tricks." While the theory suggests exponential reduction in delay regret, the optimization difficulties associated with training deep RL agents may prevent the realization of these benefits.

- **Can the performance gap between parallel agents and instantaneous oracle agents be closed in complex environments where skip connections alone are insufficient?**
The Conclusion states: "when neuron execution times are bigger, or environments are more complex (e.g., MinAtar), the performance gap... widens. Further research is needed to either mitigate this gap or identify cases where it may be unavoidable." In environments like MinAtar, the reduced expressivity of the "fast path" through skip connections appears to bottleneck performance.

## Limitations
- Theoretical analysis assumes stochasticity but doesn't fully characterize performance degradation in deterministic environments
- Experimental validation covers three domains but with limited hyperparameter ablation, particularly for skip connection types and augmentation depths
- Parallel computation speed-up claims depend heavily on specific GPU architecture characteristics that may not generalize across hardware

## Confidence

- **High Confidence**: Skip connections reduce observational delay (empirically verified across all domains, theoretical regret bounds provided)
- **Medium Confidence**: History augmentation restores Markovian property (proven formally, but practical implementation details are underspecified)
- **Medium Confidence**: Parallel execution provides inference acceleration (measured directly, but dependent on hardware-specific optimizations)

## Next Checks

1. **Mechanism Isolation Test**: Run controlled experiments on HalfCheetah-v4 varying only skip connection presence/absence and augmentation depth while holding neuron execution time constant at δ=2. Verify individual contribution of each mechanism to performance.

2. **Determinism Sensitivity Analysis**: Test agent performance on deterministic variants of MuJoCo tasks (e.g., fixed random seeds, no dynamics noise) to quantify theoretical claim about stochasticity dependence.

3. **Hardware Portability Evaluation**: Reproduce speed-up measurements on CPU and different GPU architectures (NVIDIA vs. AMD) to assess generalizability of parallel execution claims beyond the specific hardware used in experiments.