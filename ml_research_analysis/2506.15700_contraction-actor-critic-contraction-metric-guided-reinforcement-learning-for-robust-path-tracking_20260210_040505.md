---
ver: rpa2
title: 'Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning
  for Robust Path Tracking'
arxiv_id: '2506.15700'
source_url: https://arxiv.org/abs/2506.15700
tags:
- contraction
- policy
- control
- dynamics
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Contraction Actor-Critic (CAC), a reinforcement
  learning algorithm that integrates control contraction metrics (CCMs) into an actor-critic
  framework to learn robust path-tracking policies under unknown dynamics. CAC jointly
  learns a contraction metric generator (CMG) and a policy, where the CMG outputs
  a distribution over contraction metrics that define a reward function encouraging
  optimal cumulative tracking error minimization.
---

# Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking

## Quick Facts
- **arXiv ID**: 2506.15700
- **Source URL**: https://arxiv.org/abs/2506.15700
- **Reference count**: 29
- **Key outcome**: CAC outperforms C3M, PPO, SD-LQR, and LQR baselines in simulated and real-world path tracking, achieving lower mean area under the curve (MAUC) for normalized tracking error while maintaining low inference overhead.

## Executive Summary
This paper introduces Contraction Actor-Critic (CAC), a reinforcement learning algorithm that integrates control contraction metrics (CCMs) into an actor-critic framework to learn robust path-tracking policies under unknown dynamics. CAC jointly learns a contraction metric generator (CMG) and a policy, where the CMG outputs a distribution over contraction metrics that define a reward function encouraging optimal cumulative tracking error minimization. Theoretical analysis shows that policies trained with this reward exhibit asymptotic convergence if a contracting policy exists. Empirically, CAC outperforms established baselines (C3M, PPO, SD-LQR, LQR) across four simulated environments (Car, PVTOL, NeuralLander, Quadrotor), achieving lower mean area under the curve (MAUC) for normalized tracking error in most cases while maintaining low inference overhead. Real-world TurtleBot3 experiments further demonstrate CAC's robustness to sim-to-real dynamics deviations, where it successfully tracks reference trajectories while baselines fail.

## Method Summary
CAC addresses robust path tracking for nonlinear control-affine systems with unknown dynamics by combining contraction certificates with RL optimality. The method pre-trains separate neural networks to approximate the drift dynamics f̂ and actuation matrix B̂, then jointly learns a CMG (outputting a distribution over contraction metrics M) and an actor-critic policy. A "freeze-and-learn" strategy alternates between updating the CMG and performing multiple policy updates to stabilize training. The reward function R(x) = 1/(1 + δx⊤M δx) + β_π H(π_θ(x)) penalizes tracking error based on the differential length defined by the metric, while the CMG loss penalizes violations of contraction conditions. The system is evaluated on MAUC = (L/T)·∑||x(t)-xᵈ(t)||²/||x(0)-xᵈ(0)||².

## Key Results
- CAC achieves lower MAUC than C3M, PPO, SD-LQR, and LQR baselines across four simulated environments
- Real-world TurtleBot3 experiments show CAC successfully tracks reference trajectories while baselines fail
- CAC maintains low inference overhead compared to optimization-based methods
- The freeze-and-learn strategy stabilizes the simultaneous training of CMG and policy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Contraction Actor-Critic (CAC) improves tracking robustness by using a Control Contraction Metric (CCM) to shape the reward signal, implicitly encoding stability constraints into the policy optimization.
- **Mechanism:** The Contraction Metric Generator (CMG) outputs a Riemannian metric M(x). Instead of a standard Euclidean tracking error, the reward function R(x) = 1 / (1 + δx⊤M δx) penalizes the system based on the differential length defined by the metric. This creates a gradient that drives the policy not just toward the target, but toward contraction-certifying regions of the state space.
- **Core assumption:** The learned metric M(x) sufficiently satisfies the CCM conditions (Eq. 4-5) to act as a valid proxy for incremental exponential stability during training.
- **Evidence anchors:** [abstract] "CCMs provide dynamics-informed feedback for learning control policies that minimize cumulative tracking error..."; [section 3.2] "We therefore use RL to optimize a policy... by defining a reward function based on the differential Lyapunov function given by Equation (2)."; [corpus] Paper 60622 ("A Robust Neural Control Design...") supports the viability of using CCMs for robust neural control in physical systems.
- **Break condition:** If the CMG fails to produce a positive-definite metric or the metric is ill-conditioned, the reward signal creates erratic gradients, preventing policy convergence.

### Mechanism 2
- **Claim:** A "freeze-and-learn" bi-level optimization strategy stabilizes the simultaneous training of the contraction metric and the control policy.
- **Mechanism:** The system alternates between updating the CMG and the policy. After updating the CMG (which changes the reward function landscape), the CMG parameters are frozen for n steps. This allows the actor-critic algorithm to adapt the policy to the new metric without the "moving goalpost" instability caused by a constantly shifting reward definition.
- **Core assumption:** The update interval n is sufficient for the policy to partially converge to the new metric before the metric changes again.
- **Evidence anchors:** [section 3.2] "We address this issue by implementing a freeze-and-learn strategy, where for each update of the CMG, we freeze its parameters and then perform n policy updates..."; [corpus] Corpus links regarding general Actor-Critic stability (Paper 6883, 72708) suggest optimization stability is a known bottleneck in robust RL, supporting the need for this stabilization.
- **Break condition:** If n is too small, the policy oscillates or diverges; if n is too large, the system wastes computation and may overfit to a suboptimal metric state.

### Mechanism 3
- **Claim:** Using a pre-trained approximation of unknown dynamics enables the calculation of CCM conditions (specifically the null space of actuation) required for the CMG loss.
- **Mechanism:** Since analytical dynamics are unavailable, separate neural networks approximate the drift f(x) and actuation matrix B(x). The CMG loss (Eq. 9) relies on these approximations to calculate the constraints C_{W1} and C^j_{W2} (Eq. 4-5). This allows the system to verify contraction feasibility regions even in a model-free setting.
- **Core assumption:** The approximated dynamics f̂ and B̂ are accurate enough to compute the required Jacobians and singular value decompositions (for null spaces) without significant error propagation.
- **Evidence anchors:** [section 3.1] "We employ neural network function approximators to model f and B... We directly compute B_⊥ using singular value decomposition (SVD)..."; [abstract] "...given a pre-trained dynamics model, CAC simultaneously learns a CMG..."; [corpus] No specific corpus papers address the specific error propagation from learned-dynamics to CCMs; general robustness is inferred primarily from the provided text.
- **Break condition:** If the dynamics model has high approximation error (specifically in the Jacobian or actuation null space), the CMG optimizes for a physically impossible metric, invalidating the stability certificate.

## Foundational Learning

- **Concept: Control Contraction Metrics (CCM)**
  - **Why needed here:** This is the theoretical core. You must understand that CCMs generalize the idea of stability (all trajectories converging to a point) to incremental stability (all trajectories converging to each other/ a reference).
  - **Quick check question:** Can you explain why a positive-definite Riemannian metric M(x) ensures that trajectory errors δx contract exponentially?

- **Concept: Actor-Critic (PPO)**
  - **Why needed here:** CAC builds on PPO. You need to understand how the "Actor" (policy) proposes actions and the "Critic" (value function) evaluates them using the shaped reward.
  - **Quick check question:** How does the critic use the reward R(x) to update the actor's weights in standard PPO, and how does the custom R(x) here change that dynamic?

- **Concept: Model-Based vs. Model-Free RL**
  - **Why needed here:** CAC is a hybrid. It uses a model to define the *reward* (via CMG) but is trained via model-free RL. Distinguishing "planning with a model" vs. "learning a metric with a model" is crucial.
  - **Quick check question:** In CAC, does the agent use the dynamics model to simulate future states (planning), or does it use the model strictly to evaluate the validity of a metric (certification)?

## Architecture Onboarding

- **Component map:** Input: State x, Reference x_d → Dynamics Learner: Pre-trained f̂_ξ, B̂_ζ → CMG Network: Takes x, outputs M(x) → Reward Engine: Computes R(x) = 1/(1 + δx⊤M δx) → Actor Network: Takes x, outputs action u → Critic Network: Estimates value V(x) → Environment: Sim/Real robot returns x'

- **Critical path:** The pre-training of f̂, B̂ is the blocking step. Once ready, the "Freeze-and-Learn" loop (CMG update → Metric Freeze → Actor/Critic update) is the execution loop.

- **Design tradeoffs:**
  - **Inference Speed:** CAC is fast (feed-forward networks) compared to LQR/SD-LQR (requires solving Riccati equations online), but slower than pure PPO due to the CMG head.
  - **Robustness vs. Accuracy:** The entropy regularizer in the CMG prevents premature convergence but might slow down the certification of a strict contraction metric.

- **Failure signatures:**
  - **Metric Collapse:** The CMG loss (Eq. 9) goes to infinity or NaNs, often due to B̂ approximation errors leading to invalid null-space calculations.
  - **Freeze-Failure:** The tracking error oscillates heavily, indicating n (freeze steps) is too small for the policy to adapt to the changing metric.
  - **Sim-to-Real Gap:** The policy tracks well in sim but diverges in reality; check if the dynamics model f̂ was trained on data covering the operational domain.

- **First 3 experiments:**
  1. **Dynamics Validation:** Train f̂, B̂ and visualize B_⊥ against ground truth (if available) to ensure the SVD step isn't amplifying noise.
  2. **Ablation on Freeze Cycles:** Run CAC with n=1 (no freeze) vs. n=10 (default) on the "Car" environment. Plot MAUC divergence to verify the stability mechanism.
  3. **Metric Visualization:** Visualize the eigenvalues of M(x) along a trajectory. Ensure they remain bounded within [0.1, 10.0] (hyperparameters) to confirm valid positive-definiteness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can CAC be adapted to offline reinforcement learning settings to eliminate the need for costly online interaction in real-world deployment scenarios?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "the training process requires online interaction, which may be costly or impractical in certain real-world settings without reliable simulators."
- **Why unresolved:** The current CAC algorithm relies on online data collection during training, which limits applicability in safety-critical or resource-constrained environments where simulators may be unavailable or inaccurate.
- **What evidence would resolve it:** Demonstrating CAC variants that learn effectively from fixed offline datasets while maintaining contraction guarantees and comparable tracking performance to online CAC.

### Open Question 2
- **Question:** What are the necessary and sufficient conditions under which the contraction metric generator (CMG) will produce metrics that satisfy the contraction and CCM conditions during training, ensuring theoretical guarantees hold in practice?
- **Basis in paper:** [explicit] The authors acknowledge: "although our theoretical results provide convergence guarantees under contraction conditions, satisfying these conditions in practice can be challenging. As a result, the validity of the theoretical guarantees may not always hold during training or execution."
- **Why unresolved:** The theoretical analysis assumes the existence of a contracting policy and that the CMG learns a valid contraction metric, but the loss function in Equation (9) does not guarantee strict satisfaction of these constraints—only penalizes violations.
- **What evidence would resolve it:** Formal analysis of CMG convergence conditions or empirical verification that learned metrics satisfy contraction conditions with probability 1 under specific training conditions.

### Open Question 3
- **Question:** How sensitive is CAC's performance to the accuracy of the pre-trained dynamics model, and what is the minimum model fidelity required to maintain both optimality and contraction guarantees?
- **Basis in paper:** [inferred] The method requires pre-training dynamics models (Equation 8), and Appendix C.1 reports minimizing MSE to ~0.1, but the paper does not systematically characterize performance degradation as model accuracy decreases or establish thresholds for acceptable approximation error.
- **Why unresolved:** While the paper demonstrates robustness to model approximation errors compared to C3M and LQR-based methods, it does not quantify the relationship between dynamics model error and CAC's ability to learn valid contraction metrics and optimal policies.
- **What evidence would resolve it:** Ablation studies varying the dynamics model accuracy (e.g., by limiting training data or network capacity) and measuring the resulting impact on tracking error, contraction condition satisfaction, and real-world transfer success.

### Open Question 4
- **Question:** Does the reward-conditioned entropy regularizer in the CMG loss function (α(rt) = βM·e^(-rt)) provide principled exploration-exploitation trade-offs, or are there alternative regularization strategies that could improve convergence speed and final policy performance?
- **Basis in paper:** [inferred] The paper introduces a heuristic entropy regularizer that decays exponentially with reward, motivated by the intuition that "the CMG should explore the CCM space when the agent receives low returns and exploit when high returns are achieved," but provides no theoretical justification or comparison to alternative schedules.
- **Why unresolved:** The hyperparameter βM is set empirically, and Figure 2 shows that removing entropy regularization entirely degrades performance, but the specific functional form α(rt) = βM·e^(-rt) is not justified against alternatives such as fixed entropy coefficients or adaptive schemes based on contraction constraint satisfaction rather than reward.
- **What evidence would resolve it:** Comparative experiments with alternative entropy regularization strategies (constant, linear decay, constraint-based adaptation) across multiple environments, measuring both convergence speed and final MAUC.

## Limitations

- **Online interaction requirement:** The training process requires online interaction, which may be costly or impractical in certain real-world settings without reliable simulators.
- **Theoretical guarantee gaps:** Although theoretical results provide convergence guarantees under contraction conditions, satisfying these conditions in practice can be challenging, potentially invalidating theoretical guarantees during training or execution.
- **Dynamics model sensitivity:** CAC's performance depends on the accuracy of the pre-trained dynamics model, though it demonstrates robustness to model approximation errors compared to C3M and LQR-based methods.

## Confidence

- **High:** The mechanism of using CCMs to shape the reward function for stability (Mechanism 1) is well-supported by the theoretical framework and empirical results.
- **Medium:** The freeze-and-learn bi-level optimization strategy (Mechanism 2) is logically sound but the optimal frequency n is unspecified, leaving a gap in reproducibility.
- **Low:** The accuracy of the pre-trained dynamics model for computing CCM conditions (Mechanism 3) is a significant assumption; without validation, the CMG's loss calculations could be unreliable.

## Next Checks

1. **Dynamics Model Validation:** Pre-train the dynamics model and compare its predictions of the actuation null space against ground truth (if available) to ensure the SVD step is not amplifying noise.
2. **Freeze Frequency Ablation:** Run CAC with varying freeze intervals (n=1 vs. n=10) on the "Car" environment and plot the MAUC divergence to quantify the stability benefit of the freeze-and-learn strategy.
3. **Metric Condition Monitoring:** During training, monitor the eigenvalues of the learned metric M(x) along a trajectory to ensure they remain within the specified bounds [0.1, 10.0], confirming that the CMG is producing a valid positive-definite metric.