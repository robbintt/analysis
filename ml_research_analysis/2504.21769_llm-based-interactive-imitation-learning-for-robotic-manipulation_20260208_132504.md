---
ver: rpa2
title: LLM-based Interactive Imitation Learning for Robotic Manipulation
arxiv_id: '2504.21769'
source_url: https://arxiv.org/abs/2504.21769
tags:
- feedback
- agent
- llm-iteach
- teacher
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LLM-iTeach, an interactive imitation learning
  framework that leverages large language models as cost-effective, human-like teachers
  for robotic manipulation tasks. The framework uses hierarchical prompting to generate
  a task-specific CodePolicy, which is then used in a similarity-based feedback mechanism
  to provide corrective and evaluative guidance to the learning agent.
---

# LLM-based Interactive Imitation Learning for Robotic Manipulation

## Quick Facts
- arXiv ID: 2504.21769
- Source URL: https://arxiv.org/abs/2504.21769
- Authors: Jonas Werner; Kun Chu; Cornelius Weber; Stefan Wermter
- Reference count: 37
- Key outcome: LLM-iTeach outperforms Behavior Cloning and achieves performance comparable to the state-of-the-art human-supervised CEILing method on RLBench tasks.

## Executive Summary
This work introduces LLM-iTeach, an interactive imitation learning framework that leverages large language models as cost-effective, human-like teachers for robotic manipulation tasks. The framework uses hierarchical prompting to generate a task-specific CodePolicy, which is then used in a similarity-based feedback mechanism to provide corrective and evaluative guidance to the learning agent. Experiments on RLBench tasks demonstrate that LLM-iTeach outperforms Behavior Cloning and achieves performance comparable to the state-of-the-art human-supervised CEILing method. The approach shows promise in reducing human effort while maintaining high success rates, and it generalizes well to additional complex tasks. Ablation studies confirm that combining both feedback types with a warm-start policy yields the best results.

## Method Summary
LLM-iTeach generates a CodePolicy through hierarchical prompting, breaking tasks into executable Python functions. During training, the agent receives feedback based on the angular difference between its actions and the CodePolicy's actions. Corrective feedback replaces the agent's action when the angle exceeds a threshold (20°), while evaluative feedback reinforces successful agent actions when the angle is below threshold. The agent learns through a weighted loss function that combines both feedback types, allowing it to explore variations the teacher might have missed.

## Key Results
- Achieves 83.5% success rate on CloseMicrowave task, comparable to CEILing's 87.0%
- Outperforms Behavior Cloning baseline on all tested RLBench tasks
- Demonstrates strong generalization to complex tasks beyond initial training set
- Ablation studies show warm-start + mixed feedback yields optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Code Policy Generation
Decomposing task reasoning into executable Python code (CodePolicy) allows an LLM to serve as a low-latency reference policy for feedback generation. The system uses a two-level hierarchical prompt: (1) a Planner Prompt to break the task into steps, and (2) Action/Check Prompts to generate Python functions that calculate actions and verify step completion. This code is executed locally during training to determine the "teacher's" preferred action without repeated LLM inference calls.

### Mechanism 2: Angle-Based Feedback Disentanglement
Distinguishing between evaluative (approval) and corrective (intervention) feedback based on the angular difference between agent and teacher actions creates an effective learning signal. The system compares the agent's action vector with the CodePolicy's action vector. If the angle between them is below a threshold (optimized at 20°), the agent receives positive evaluative feedback. If the angle exceeds threshold, the CodePolicy's action overrides the agent's action as corrective feedback.

### Mechanism 3: Weighted Loss for Exploration-Exploitation
Combining corrective data with evaluative data in the loss function allows the agent to outperform the pure LLM teacher by exploring successful variations the teacher missed. The loss function weights state-action pairs based on feedback type. Corrective feedback is weighted inversely proportional to its frequency, while evaluative feedback gets a fixed weight. This prevents the agent from overfitting to potentially sub-optimal corrective demonstrations while reinforcing successful agent-discovered behaviors.

## Foundational Learning

- **Concept: Interactive Imitation Learning (IIL)**
  - Why needed here: The paper builds on IIL (specifically the CEILing framework) rather than standard Behavior Cloning. You must understand that IIL involves an agent acting and a teacher correcting *during* execution to solve the distribution shift problem.
  - Quick check question: How does IIL mitigate the "covariate shift" problem inherent in standard Behavior Cloning?

- **Concept: Hierarchical Prompting / Code as Policies**
  - Why needed here: The "teacher" here is not a neural network but generated Python code. Understanding how an LLM translates logic into executable functions (via chain-of-thought) is vital to debugging the CodePolicy.
  - Quick check question: Why is executing a pre-generated Python policy more practical for real-time feedback than querying an LLM at every inference step?

- **Concept: Markov Decision Process (MDP) & Loss Functions**
  - Why needed here: The agent learns via gradient descent on a negative log-likelihood loss. Understanding the mapping of State → Action and how "expert" corrections are integrated into the training buffer is required to implement the learning loop.
  - Quick check question: In Eq. 7, how does the weighting factor q change the gradient update for a state-action pair that received corrective feedback versus evaluative feedback?

## Architecture Onboarding

- **Component map:** LLM (Llama3-70b) -> CodePolicy (Teacher) -> Similarity Checker -> Agent (Policy Network) -> Trainer
- **Critical path:** The generation of a valid `CodePolicy`. If the LLM generates code with syntax errors or logic that fails to call the API correctly (e.g., wrong object names), the entire loop fails.
- **Design tradeoffs:** Low β (strict) → High correction rate (micromanagement). High β (lenient) → High evaluation rate (risk of reinforcing bad habits). Paper finds 20° is the sweet spot.
- **Failure signatures:** "Inverse Kinematics Error": The CodePolicy commands a valid target pose, but the robot arm cannot reach it physically. "Semantic Hallucination": The LLM plans to "push the handle" when pushing the door surface would suffice.
- **First 3 experiments:** 1) Run the `CodePolicy` in "open loop" (direct control) on the target task. 2) Train the agent with *only* corrective feedback vs. *only* evaluative feedback. 3) Run a hyperparameter sweep on β (e.g., 0°, 20°, 90°, 180°) on a simple task.

## Open Questions the Paper Calls Out

**Open Question 1:** Can LLM-iTeach maintain performance when adapted for 7-dimensional action spaces in robotic manipulation? The authors identify the "limited action space" as a limitation and suggest "advancing LLM-iTeach to a 7-d action space" as a specific avenue for future work to unlock more complex policies.

**Open Question 2:** Does integrating Vision-Language Models (VLMs) improve the LLM Teacher's ability to account for physical constraints? The paper notes the LLM Teacher failed in specific instances (like closing a microwave) due to a "restricted observation space" and lack of "external contextual knowledge" regarding physical constraints.

**Open Question 3:** Can LLM-iTeach operate effectively in real-world environments without relying on privileged ground truth state information? The authors state that the "reliance on ground truth data... limits the approach's applicability to real-world scenarios" and identify "moving towards real-world sensory inputs" as a necessary future step.

## Limitations

- **Limited action space:** Currently uses 4-dimensional action space, with 7-dimensional action space as future work
- **Ground truth state dependency:** Relies on API providing exact object positions, not available in real-world deployment
- **Physical constraint unawareness:** LLM lacks awareness of robot's physical constraints like joint limits and self-collision

## Confidence

**High Confidence Claims:**
- LLM-iTeach outperforms standard Behavior Cloning on RLBench tasks
- The hierarchical CodePolicy generation mechanism successfully converts task descriptions into executable Python code
- Angle-based similarity threshold at 20° provides effective balance between corrective and evaluative feedback

**Medium Confidence Claims:**
- Performance comparable to CEILing method (human-supervised baseline) across all tasks
- The weighted loss function effectively combines corrective and evaluative feedback to outperform pure LLM teacher
- Warm-start with 10 demonstrations significantly accelerates convergence

**Low Confidence Claims:**
- Generalization to complex tasks beyond the 5 tested (no systematic evaluation of scalability)
- Robustness to perception noise (all experiments use ground-truth state API)
- Performance without the warm-start demonstrations (theoretical claims not empirically validated)

## Next Checks

1. **Architecture Verification:** Implement the agent with exact network specifications from the code repository and verify training curves match paper results.

2. **Perception Noise Injection:** Modify the CodePolicy to receive simulated noisy object positions and evaluate success rate degradation.

3. **Zero-Shot Generalization Test:** Apply the CodePolicy generation pipeline to a novel RLBench task not in the training set and measure CodePolicy success rate before agent training.