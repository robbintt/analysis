---
ver: rpa2
title: Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and
  Hyper-Parameter Optimisation
arxiv_id: '2510.07052'
source_url: https://arxiv.org/abs/2510.07052
tags:
- speech
- gp-bo
- search
- emotion
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a CPU-only speech emotion recognition (SER)
  pipeline that combines a pre-trained wav2vec2.0 encoder with automated hyperparameter
  optimisation (HPO). The approach fine-tunes the encoder on IEMOCAP and applies HPO
  using Gaussian Process Bayesian Optimisation (GP-BO) and Tree-structured Parzen
  Estimators (TPE) to optimise learning rate, training epochs, encoder unfreezing
  point, and input sequence length.
---

# Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation

## Quick Facts
- arXiv ID: 2510.07052
- Source URL: https://arxiv.org/abs/2510.07052
- Reference count: 0
- Primary result: GP-BO achieves 96% BCA in 11 minutes on EmoDB, outperforming AutoSpeech 2020 baseline (85% in 30 minutes on GPU)

## Executive Summary
This study presents a CPU-only speech emotion recognition (SER) pipeline that combines a pre-trained wav2vec2.0 encoder with automated hyperparameter optimisation (HPO). The approach fine-tunes the encoder on IEMOCAP and applies HPO using Gaussian Process Bayesian Optimisation (GP-BO) and Tree-structured Parzen Estimators (TPE) to optimise learning rate, training epochs, encoder unfreezing point, and input sequence length. On the German EmoDB corpus, GP-BO achieved 96.0% balanced class accuracy in 11 minutes, while TPE attained 97.0% in 15 minutes. These results significantly outperform the best AutoSpeech 2020 baseline (85% in 30 minutes on GPU) and exceed native German listeners' performance (84%). Results demonstrate that efficient HPO with pre-trained encoders delivers competitive SER performance on commodity hardware.

## Method Summary
The method employs a SpeechBrain EncoderClassifier with a wav2vec2-base encoder pre-fine-tuned on IEMOCAP, followed by a linear classifier head. Training uses a two-stage strategy where the encoder is initially frozen, then unfrozen at an HPO-controlled epoch (u ∈ {0,...,5}). The HPO search space includes learning rate (log-uniform 10⁻⁶ to 10⁻³), number of epochs (1-10), unfreeze epoch (0-5), and input sequence length (32k-160k samples). Three HPO engines are evaluated: Ax (GP-BO), Hyperopt (TPE), and Optuna (TPE with pruning). The objective function is balanced class accuracy on validation data, with efficiency measured as BCA per wall-clock minute.

## Key Results
- GP-BO achieves 96.0% BCA on EmoDB in 11 minutes using only 15 trials
- TPE reaches 97.0% BCA in 15 minutes, exceeding native German listeners (84%)
- HPO-tuned models improve cross-lingual generalization by 25% on CREMA-D and 26% on RAVDESS
- Grid search requires 143 trials and 1680 minutes to reach 90% accuracy

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained wav2vec 2.0 representations provide transferable acoustic features that, when combined with targeted HPO, enable rapid adaptation to new emotion corpora. The encoder, pre-trained on IEMOCAP (English), captures generalizable paralinguistic patterns that require only task-specific fine-tuning rather than full training. HPO identifies the optimal point to unfreeze the encoder and appropriate learning rates, enabling efficient adaptation to EmoDB (German) within 10-15 trials. The acoustic features relevant to emotion recognition are partially language-independent, allowing cross-lingual transfer.

### Mechanism 2
Sequential model-based optimization (SMBO) methods achieve better accuracy-efficiency trade-offs than grid search by modeling hyperparameter-response relationships. GP-BO fits a Gaussian process surrogate to observed (hyperparameter, BCA) pairs and selects promising configurations via acquisition functions (Expected Improvement). TPE models densities over "good" vs. "bad" configurations and samples by maximizing ℓ(λ)/g(λ). Both prioritize exploration in high-uncertainty, high-potential regions. The hyperparameter space has exploitable structure (smoothness for GP-BO, separability for TPE).

### Mechanism 3
Controlled encoder unfreezing stabilizes fine-tuning on small corpora by allowing the classifier head to converge before updating representations. Freezing the encoder for initial epochs prevents catastrophic forgetting of pre-trained features while the linear classifier learns the emotion mapping. Unfreezing at epoch u (HPO-optimized between 0-5) enables task-specific adaptation once the classifier stabilizes. The small target corpus (EmoDB: 535 utterances) provides insufficient signal for joint encoder-classifier training from initialization.

## Foundational Learning

- **Self-supervised speech representations (wav2vec 2.0)**: The encoder provides frame-level features from masked prediction pre-training, not emotion-specific features, clarifying why fine-tuning is essential. Can you explain why a model pre-trained on speech recognition tasks might still require fine-tuning for emotion classification?

- **Balanced Class Accuracy (BCA)**: BCA accounts for class imbalance by averaging per-class recall; standard accuracy could be misleading on skewed emotion distributions. Given 7 emotion classes with uneven sizes, why is BCA preferred over raw accuracy for model selection?

- **Bayesian Optimization vs. TPE**: GP-BO models the objective function directly; TPE models hyperparameter densities. This affects which performs better on different search space structures. If your hyperparameter space includes conditional parameters (e.g., learning rate only relevant when optimizer=Adam), which method might handle this better?

## Architecture Onboarding

- **Component map**: Input audio (16kHz, padded/truncated) -> wav2vec2.0 encoder (SpeechBrain, IEMOCAP-fine-tuned) -> Temporal pooling → z ∈ R^d -> Linear classifier → ŷ ∈ R^7 (emotion logits) -> Cross-entropy loss → AdamW + cosine annealing. HPO loop (Ax/Hyperopt/Optuna) wraps training, optimizing: lr, #epochs, unfreeze epoch, maxlen.

- **Critical path**: 1) Load SpeechBrain encoder checkpoint (ensure IEMOCAP-fine-tuned version) 2) Define search space per Table 2 (log-uniform lr, discrete epochs/unfreeze, categorical maxlen) 3) Run SMBO with 15-trial budget; evaluate on held-out 20% EmoDB split 4) Select λ* by max validation BCA; report test BCA

- **Design tradeoffs**: GP-BO (Ax): Faster convergence (11 min), slightly lower peak accuracy (0.96); best for rapid iteration. TPE (Hyperopt): Higher peak accuracy (0.97), modest time increase (15 min); best when accuracy is critical. TPE (Optuna): Supports pruning but required 185 min in this study; potentially better for larger budgets.

- **Failure signatures**: Zero-shot BCA near 1/C (0.14 for 7 classes) → encoder not fine-tuned or wrong checkpoint. Training loss diverges → learning rate too high; check lr sampling (10⁻⁶ to 10⁻³). Validation BCA plateaus < 0.80 → unfreeze too early (u=0) on small data; constrain u ≥ 2. Cross-lingual transfer fails → emotion label mismatch; verify unified taxonomy mapping (Table 1).

- **First 3 experiments**: 1) Reproduce Ax GP-BO baseline: 15 trials on EmoDB 80/20 split, target ≥0.95 BCA in <15 min 2) Ablate unfreeze strategy: Compare u ∈ {0, 2, 5} with fixed lr=2e-5, epochs=8; expect u=2-4 optimal 3) Test cross-lingual transfer: Train HPO-tuned model on EmoDB, evaluate zero-shot on CREMA-D/RAVDESS with label mapping; target ≥0.35 BCA improvement over pre-trained baseline

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed CPU-only HPO workflow maintain its efficiency when extended to multilingual corpora beyond the German EmoDB and English transfer tasks? The conclusion states that "Future work will extend to multilingual corpora." This remains unresolved as the current study evaluates performance on a single German dataset with zero-shot transfer to only two English datasets. Applying the GP-BO or TPE pipeline to a diverse multilingual SER benchmark would resolve this.

### Open Question 2
Can the objective function be reformulated to include energy consumption and inference latency without degrading the balanced class accuracy? The authors identify the need to "integrate energy and latency objectives for deployment in resource-constrained settings" as future work. The current optimization focuses solely on BCA and wall-clock training time. Results from a multi-objective HPO search mapping the Pareto frontier between accuracy and resource constraints would resolve this.

### Open Question 3
Do parameter-efficient fine-tuning (PEFT) methods provide better computational efficiency or cross-lingual generalization than the current two-stage freezing strategy? The paper notes that future work will "explore parameter-efficient fine-tuning." The current architecture relies on unfreezing the entire wav2vec 2.0 encoder, which may be computationally wasteful compared to adapter-based approaches like LoRA. A comparative study benchmarking PEFT techniques against full unfreezing would resolve this.

## Limitations

- Cross-lingual generalization claims rely on a single fine-tuning run; robustness to different initializations is untested
- Efficiency comparisons with AutoSpeech 2020 use different hardware (CPU vs GPU) without normalization for computational resources
- The assumed language-independence of emotion-related acoustic features is not directly validated beyond cross-corpus performance gains

## Confidence

- **High confidence**: HPO efficiency gains over grid search (directly measured, large effect size)
- **Medium confidence**: Cross-lingual transfer improvements (positive results but limited to two corpora)
- **Medium confidence**: Optimal unfreeze timing mechanism (ablation not performed, assumption-based)

## Next Checks

1. Run 5 independent seeds of the full HPO pipeline on EmoDB to establish variance in BCA and timing metrics
2. Perform head-to-head efficiency comparison on identical hardware by implementing a GPU version of the AutoSpeech baseline
3. Conduct ablation studies varying the pre-training corpus (IEMOCAP vs neutral speech) to quantify transfer dependence on pre-training data emotion content