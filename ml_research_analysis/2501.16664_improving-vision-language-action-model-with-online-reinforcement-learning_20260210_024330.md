---
ver: rpa2
title: Improving Vision-Language-Action Model with Online Reinforcement Learning
arxiv_id: '2501.16664'
source_url: https://arxiv.org/abs/2501.16664
tags:
- tasks
- learning
- arxiv
- large
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving vision-language-action
  (VLA) models for robotic control using online reinforcement learning (RL). The authors
  propose the iRe-VLA framework, which iteratively alternates between RL stages (freezing
  the VLM backbone to stabilize training) and supervised learning stages (fine-tuning
  the entire model on successful trajectories).
---

# Improving Vision-Language-Action Model with Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.16664
- Source URL: https://arxiv.org/abs/2501.16664
- Reference count: 40
- Primary result: iRe-VLA framework improves VLA model training stability and performance on robotic manipulation tasks using online RL with frozen VLM backbone

## Executive Summary
This paper addresses the challenge of training large Vision-Language-Action (VLA) models for robotic control using online reinforcement learning. The authors propose the iRe-VLA framework, which iteratively alternates between RL stages (freezing the VLM backbone to stabilize training) and supervised learning stages (fine-tuning the entire model on successful trajectories). This approach stabilizes the unstable RL process while leveraging the large VLM's expressive power. Experiments on MetaWorld, FrankaKitchen, and real-world manipulation tasks show that iRe-VLA outperforms standard PPO, achieving higher success rates on both seen and unseen tasks.

## Method Summary
iRe-VLA iteratively alternates between two training stages. Stage 1 freezes the VLM backbone and trains only the action head via RL (PPO or SACfD) on new tasks, collecting successful trajectories. Stage 2 performs supervised fine-tuning on the entire model using both expert data and collected trajectories to prevent catastrophic forgetting. The framework uses BLIP-2 3B as the VLM backbone with LoRA adapters for parameter-efficient fine-tuning. Training starts with an initial supervised stage on expert data, then iterates between RL and SL stages for new tasks.

## Key Results
- Outperforms standard PPO on MetaWorld, FrankaKitchen, and real-world manipulation tasks
- Achieves 0.83 left-door-open success rate in FrankaKitchen (vs 0.43 for PPO)
- Improves real-world eggplant picking success rate from 0.35 to 0.80
- Maintains expert task performance while improving on RL-trained tasks

## Why This Works (Mechanism)

### Mechanism 1
Freezing VLM parameters during RL prevents training instability and model collapse in large VLA models. Standard RL gradients introduce noise that disrupts pre-trained VLM representations. By freezing VLM (billions of parameters) and training only the lightweight action head, RL exploration proceeds without degrading the semantic embeddings that enable generalization.

### Mechanism 2
Iterating between RL exploration and supervised consolidation prevents catastrophic forgetting while expanding task coverage. RL stage discovers novel successful trajectories for new tasks. SL stage distills these into the full model using both expert data (De) and collected trajectories (DRL), maintaining original competencies while incorporating new skills.

### Mechanism 3
Two-stage architecture enables local-machine RL deployment by isolating computation demands. RL stage requires only forward passes through frozen VLM plus action head training—feasible on consumer GPUs (e.g., RTX 4090). Full-model SL on combined datasets requires cloud resources but happens less frequently.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: The paper uses PPO as the baseline RL algorithm; understanding its stability properties helps contextualize why VLA-specific modifications are needed.
  - Quick check question: Can you explain why PPO's clipping mechanism might be insufficient for stabilizing billion-parameter VLM training?

- **Concept: Catastrophic Forgetting in Multi-Task Learning**
  - Why needed here: The iRe-VLA design explicitly addresses forgetting when learning new tasks; understanding this problem clarifies why the SL stage replays expert data.
  - Quick check question: What would happen to original task performance if we skipped the De replay in Stage 2?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The VLA uses LoRA for parameter-efficient fine-tuning; this is critical for understanding what "trainable parameters θ" means in practice.
  - Quick check question: How does LoRA's rank parameter affect the tradeoff between fine-tuning expressiveness and overfitting risk?

## Architecture Onboarding

- **Component map**: Image + Text instruction -> VLM Backbone (BLIP-2 3B) -> Token Learner -> MLP Action Head -> Continuous Action
- **Critical path**: Stage 0: SFT on expert dataset De → baseline VLA policy; Stage 1: Freeze VLM, train action head + critic via RL on new task; Collect successful trajectories into DRL; Stage 2: Full model SL on De ∪ DRL; Iterate stages 1-2 across task curriculum
- **Design tradeoffs**: Action head capacity vs. VLM expressiveness; RL iteration count vs. forgetting risk; Sparse vs. dense rewards
- **Failure signatures**: Performance degradation on original tasks → insufficient De replay ratio in Stage 2; RL stage collapse → VLM accidentally unfrozen, or learning rate too high for action head; No improvement on new tasks → success rate too low to collect viable trajectories
- **First 3 experiments**: 1) Train SFT policy on MetaWorld subset, verify it achieves ~0.8 success on seen tasks; 2) Apply PPO directly to full VLA on one MetaWorld task; confirm performance drop, then freeze VLM and verify stability recovers; 3) Run one RL→SL cycle on a held-out task, confirm RL finds successes, SL doesn't degrade original tasks, and task improves vs. zero-shot

## Open Questions the Paper Calls Out
The paper identifies limitations in its ability to learn entirely new motor skills under sparse-reward conditions, rather than merely refining skills within types seen during pre-training.

## Limitations
- Primarily validated on relatively simple manipulation tasks, not complex long-horizon planning scenarios
- Performance on tasks requiring high-precision control is not explicitly evaluated
- Computational advantage assumes VLM inference is the dominant bottleneck, which may vary by architecture

## Confidence

| Claim | Confidence |
|-------|------------|
| Freeze-VLM-during-RL mechanism stabilizes training | High |
| Catastrophic forgetting mitigation through SL stage | High |
| Computational efficiency claims for local deployment | Medium |

## Next Checks

1. Apply iRe-VLA to a more complex manipulation task suite (e.g., TDW or RoboTHOR) to evaluate scalability beyond current benchmarks
2. Systematically vary action head capacity and VLM parameter count to quantify impact on RL stability and final performance
3. Deploy trained iRe-VLA model on a physical robot for multi-task manipulation challenge, measuring inference latency and control accuracy under real-world conditions