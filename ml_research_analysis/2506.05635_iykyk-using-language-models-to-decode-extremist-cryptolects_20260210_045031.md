---
ver: rpa2
title: 'IYKYK: Using language models to decode extremist cryptolects'
arxiv_id: '2506.05635'
source_url: https://arxiv.org/abs/2506.05635
tags:
- language
- task
- in-group
- incels
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether current language technologies can
  detect and decode the complex in-group language (cryptolects) of online extremist
  platforms. Evaluating eight models across six tasks on data from Stormfront and
  Incels, the research finds that general-purpose large language models cannot consistently
  detect or decode extremist language.
---

# IYKYK: Using language models to decode extremist cryptolects

## Quick Facts
- **arXiv ID:** 2506.05635
- **Source URL:** https://arxiv.org/abs/2506.05635
- **Reference count:** 16
- **Primary result:** General-purpose LLMs cannot consistently detect or decode extremist cryptolects, but performance improves significantly through domain adaptation and specialized prompting techniques.

## Executive Summary
This study investigates whether current language technologies can detect and decode the complex in-group language (cryptolects) of online extremist platforms. Evaluating eight models across six tasks on data from Stormfront and Incels, the research finds that general-purpose large language models cannot consistently detect or decode extremist language. However, performance improves significantly through domain adaptation and specialized prompting techniques. Domain adaptation of encoder models on extremist data boosts macro-F1 scores by up to 5.2% for radicalization detection tasks and achieves state-of-the-art results. The study also develops and releases novel datasets including 19.4M posts and expert-validated lexicons, providing valuable resources for future work on automated moderation technologies.

## Method Summary
The study evaluates LLMs on three zero-shot tasks (classification, retrieval, and decoding of in-group language) and supervised classification of radical content across two extremist platforms. For zero-shot evaluation, seven instruction-tuned LLMs are tested with four different prompt framings on candidate lexicons. Supervised classification uses an XLM-T encoder model with multi-task fine-tuning, including domain adaptation via MLM on forum data before classification fine-tuning. The IYKYK dataset contains 19.4M posts from Incels.is and Stormfront.org, with expert-annotated gold standards for evaluation. Domain adaptation is performed on filtered forum data, followed by supervised fine-tuning with auxiliary tasks like ideology prediction.

## Key Results
- Domain adaptation improves radical content detection by up to 5.2% macro-F1 on Counter dataset tasks
- Specialized prompting techniques (e.g., 10-example framing) significantly improve zero-shot decoding performance
- Morphological regularity in cryptolects (like "-cel" suffixes) impacts classification performance more than decoding
- Encoder models with domain adaptation achieve state-of-the-art results on extremist content classification

## Why This Works (Mechanism)

### Mechanism 1: Context Framing Improves Zero-Shot Decoding
Providing example posts and full instructions to instruction-tuned LLMs significantly improves their ability to correctly define extremist cryptolect terms. Context-rich prompts supply usage patterns that help the model disambiguate polysemous terms and override default semantic associations from general pretraining. The mechanism works by grounding the model in the specific semantic domain through contextual examples.

### Mechanism 2: Domain Adaptation Enhances Supervised Classification
Continued pre-training (domain adaptation) of an encoder-only model on raw extremist forum data improves its performance on downstream radical content classification tasks. The MLM objective on in-domain text refines the model's representations to capture the statistical regularities, vocabulary, and syntactic structures of the cryptolect, creating a better initialization for supervised fine-tuning.

### Mechanism 3: Morphological Regularity in Cryptolects
Cryptolects that systematically reuse morphemes to create new terms are easier for LLMs to classify than those using opaque, symbolic, or acronym-based language. Models can leverage tokenization and pattern-matching capabilities on known morphemes (e.g., "-cel", "-maxx") to infer the in-group status of unseen compound words.

## Foundational Learning

### Concept: Cryptolects and Dogwhistles
**Why needed here:** The entire paper is predicated on understanding that extremist groups develop specialized, in-group language (cryptolects) that is not just "slang." This includes dogwhistlesâ€”coded language with one meaning to outsiders and another to insiders.
**Quick check question:** If a word has a standard dictionary definition, can it still be a cryptolect term? (Answer: Yes, if the group has imbued it with a secret, coded meaning, it's a dogwhistle.)

### Concept: Domain Adaptation vs. Fine-Tuning
**Why needed here:** The paper evaluates two distinct phases. Understanding the difference between continued pre-training (domain adaptation with MLM) and supervised fine-tuning for a specific classification task is essential to grasp the results.
**Quick check question:** What is the primary difference in the objective function used for domain adaptation (MLM) versus the final classification fine-tuning? (Answer: MLM predicts masked tokens from context; fine-tuning uses a classification head with cross-entropy loss over class labels.)

### Concept: Zero-Shot Prompting & Framing
**Why needed here:** A core finding is that model performance is highly sensitive to how the prompt is constructed ("framing"). Distinguishing between providing a definition, full instructions, or example posts is key to interpreting the results.
**Quick check question:** According to the paper, which prompt framing strategy generally yielded the best performance for the decoding task? (Answer: Providing 10 example posts along with full instructions.)

## Architecture Onboarding

### Component map:
Data Sources (19.4M posts from Incels and Stormfront) -> LISTN-C processing (candidate lexicon induction) -> Expert annotation (gold-standard test sets) -> Zero-Shot Evaluator (7 instruction-tuned LLMs via API) -> Supervised Classifier (XLM-T encoder with classification head) -> Training Pipeline (MLM domain adaptation -> Multi-task supervised fine-tuning)

### Critical path:
The most impactful path for a new engineer is the supervised classification pipeline. Start with the XLM-T model, apply MLM domain adaptation on the combined IYKYK forum data, then fine-tune for the target task (e.g., Radicalization Level) alongside an auxiliary task like ideology prediction.

### Design tradeoffs:
- Prompting vs. Fine-Tuning: Prompting is cheaper and faster for experimentation but provides lower peak performance. Fine-tuning requires more data and compute but achieves state-of-the-art results.
- Platform-Specific vs. Combined Adaptation: Adapting on a single platform's data might capture more specific nuances, but combining data from multiple extremist platforms often yields more robust, generalizable improvements.
- Assumption: The choice of model (XLM-T) is based on its prior success with social media data. The tradeoff is using an encoder-only model, which is better for classification but unsuitable for generative decoding tasks.

### Failure signatures:
- Overfitting to prompts: High recall but low precision when adding example posts, suggesting the model is biased towards labeling terms as positive.
- Refusal to answer: Models refusing to define terms due to safety filters, especially in the "definition-only" framing that lacks research context.
- Polysemy errors: Incorrectly defining words with common standard meanings when context is insufficient.

### First 3 experiments:
1. **Reproduce Task 1 (Classification) Baseline:** Select 50 terms from the generated lexicon and test a model like Llama-3.3-70B with the "definition-only" framing vs. "instructions" framing to confirm the reported ~10% F1 improvement.
2. **Domain Adaptation Ablation:** Take the XLM-T model and fine-tune the Radicalization Level classifier with three different pre-training configurations: (a) no adaptation, (b) adaptation on Incels data only, (c) adaptation on combined Incels+Stormfront data. Compare macro-F1 scores.
3. **Prompting Strategy Test:** For Task 3 (Decoding), select 10 confirmed positive terms and test a model with the "1-example" vs. "10-examples" framing to observe the impact on definition accuracy and the handling of polysemous terms.

## Open Questions the Paper Calls Out

### Open Question 1
**How does temporal evolution of extremist cryptolects affect model detection and decoding performance over time?**
The study uses a static snapshot approach without analyzing how model performance degrades as extremist terminology evolves.

### Open Question 2
**To what extent do findings generalize to other extremist ideologies with different linguistic characteristics?**
Only two ideologies were tested; the paper notes large performance differences between them.

### Open Question 3
**What is the optimal balance between toxic content removal during pre-training and retaining sufficient representations for extremist language monitoring tasks?**
The tension between content safety and monitoring capability is acknowledged but not empirically quantified.

### Open Question 4
**How does annotation quality and annotator agreement affect gold standard creation for subjective in-group language identification?**
The study acknowledges single-annotator limitations but doesn't quantify inter-annotator variability's impact on model evaluation.

## Limitations

- Evaluation relies on expert annotation which introduces potential subjectivity and limits scalability
- API-based evaluation using commercial LLMs may not be reproducible in self-hosted deployments
- Focus on only two specific platforms (Incels and Stormfront) limits generalizability to other extremist communities

## Confidence

**High Confidence:**
- Domain adaptation significantly improves supervised classification performance (F1 improvements of 2.0-5.2%)
- Specialized prompting techniques substantially improve zero-shot decoding performance (jump from 44.7% to 90.3% for Incels)
- Morphological regularity impacts classification more than decoding

**Medium Confidence:**
- Cross-platform generalization of domain adaptation (limited to two platforms)
- Prompt framing sensitivity (results depend on specific prompt templates)
- Generalization across different extremist ideologies (primarily tested on right-wing extremism)

**Low Confidence:**
- Scalability of expert annotation approach
- Performance on cryptolects from completely different linguistic families
- Real-world deployment effectiveness given safety filter interactions

## Next Checks

1. **External Platform Validation:** Apply the domain-adapted XLM-T model to extremist content from a third, previously unseen platform to test cross-platform generalization. Compare performance to models without domain adaptation.

2. **Safety Filter Robustness Test:** Systematically evaluate how different instruction-tuned LLMs handle refusal scenarios by testing with and without explicit research context in prompts. Measure refusal rates and quality of non-refusal responses.

3. **Zero-Shot Prompting Ablation:** Conduct a controlled experiment varying prompt framing components to isolate which specific elements drive the 45.6% performance improvement observed between definition-only and 10-example framing. Test with a broader range of polysemous terms.