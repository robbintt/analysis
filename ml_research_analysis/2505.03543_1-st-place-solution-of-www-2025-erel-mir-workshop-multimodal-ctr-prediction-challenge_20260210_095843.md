---
ver: rpa2
title: 1$^{st}$ Place Solution of WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction
  Challenge
arxiv_id: '2505.03543'
source_url: https://arxiv.org/abs/2505.03543
tags:
- multimodal
- item
- learning
- embedding
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the 1st place solution to the WWW 2025 EReL@MIR
  Workshop Multimodal CTR Prediction Challenge, focusing on improving click-through
  rate prediction by integrating multimodal embeddings into recommender systems. The
  proposed approach combines sequential modeling with feature interaction learning,
  using a Transformer-based architecture to capture user interest patterns and DCNv2
  for modeling high-order feature interactions.
---

# 1$^{st}$ Place Solution of WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction Challenge

## Quick Facts
- arXiv ID: 2505.03543
- Source URL: https://arxiv.org/abs/2505.03543
- Reference count: 15
- Primary result: Achieved 0.9839 AUC on WWW 2025 EReL@MIR Multimodal CTR Prediction Challenge

## Executive Summary
This paper presents the 1st place solution to the WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction Challenge, focusing on improving click-through rate prediction by integrating multimodal embeddings into recommender systems. The proposed approach combines sequential modeling with feature interaction learning, using a Transformer-based architecture to capture user interest patterns and DCNv2 for modeling high-order feature interactions. The solution demonstrates that proper integration of multimodal information with sequential modeling and feature interactions is crucial for optimal performance in multimodal recommendation scenarios.

## Method Summary
The model combines sequential modeling and feature interaction learning to capture user-item interactions effectively. It uses an embedding layer that concatenates frozen multimodal embeddings (PCA of BERT+CLIP embeddings) with learnable ID/tag embeddings, followed by a Transformer for sequential modeling where the target item embedding is concatenated with each history item. DCNv2 cross networks are used in parallel with deep MLPs for feature interactions, and a prediction MLP outputs the CTR probability. The architecture uses k=16 for short-term interest outputs with max pooling for long-term aggregation.

## Key Results
- Achieved 0.9839 AUC on the challenge test set, significantly outperforming baseline models
- DCNv2 component showed largest impact - removing it dropped AUC from 0.9776 to 0.9632 with multimodal
- Extensive ablation studies confirmed effectiveness of both Transformer and DCNv2 components
- Simple concatenation of multimodal embeddings was used due to time constraints, with better alignment methods planned for future work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating target item embeddings with each historical item before Transformer encoding improves sequential interest capture compared to encoding history independently.
- Mechanism: By concatenating $e_{target}$ with each $e_{item}$ in the sequence, the Transformer attends to history items in the *context of the current prediction target*, allowing attention weights to emphasize historically similar items to what's being recommended now.
- Core assumption: User interest is context-dependent; relevance of past clicks depends on what's currently being evaluated.
- Evidence anchors:
  - [section 2.2.2] "The target item embedding $e_{target}$ is concatenated with every item embedding in the history sequence to form the input sequence"
  - [abstract] "combining sequential modeling and feature interaction learning to effectively capture user-item interactions"
  - [corpus] TransAct (KDD '23) reference [13] inspired this approach for real-time user action modeling
- Break condition: If target concatenation increases sequence length beyond memory limits or if history items become redundant (all attending to same target), the mechanism degrades.

### Mechanism 2
- Claim: Combining explicit cross-layer interactions (DCNv2) with deep MLP in parallel captures both interpretable bounded-degree interactions and implicit high-order patterns.
- Mechanism: The cross network $c_{l+1} = f_i \odot (W_l c_l + b_l) + c_l$ explicitly models feature crosses at each layer, while the parallel deep MLP learns arbitrary nonlinear combinations. Concatenating both outputs preserves interpretability and expressiveness.
- Core assumption: Optimal CTR prediction requires both memorizing specific feature combinations AND generalizing to unseen interactions.
- Evidence anchors:
  - [section 2.2.3] "DCNv2 for its efficiency and effectiveness of modeling high-order feature interactions"
  - [Table 2] Removing DCNv2 drops AUC from 0.9776 to 0.9632 (with multimodal), the largest single-component drop
  - [corpus] DCN V2 (WWW '21) demonstrated web-scale effectiveness; limited direct corpus comparison available
- Break condition: If cross-layer count is too low, high-order interactions are missed; if too high, overfitting on sparse features occurs.

### Mechanism 3
- Claim: Frozen multimodal embeddings, when simply concatenated, can *degrade* performance if the base architecture lacks capacity to align them with the CTR task.
- Mechanism: Frozen embeddings from BERT+CLIP are trained for general semantic similarity, not CTR prediction. Without learnable alignment layers, models like DIN cannot reconcile the distribution mismatch between semantic and collaborative signals.
- Core assumption: Multimodal embeddings encode semantic relevance but not behavioral relevance; alignment is required.
- Evidence anchors:
  - [Table 2] DIN baseline AUC drops from 0.9326 to 0.8577 when multimodal embeddings are added
  - [section 3.2] "we suspect that the frozen multimodal embeddings are not well aligned with the CTR prediction task"
  - [corpus] Neighbor papers (QIN, MARS) explore alignment strategies but limited comparative evidence on frozen vs. fine-tuned
- Break condition: Performance degradation when adding multimodal features indicates misalignment; requires architectural intervention (not just concatenation).

## Foundational Learning

- Concept: **Transformer self-attention for user sequences**
  - Why needed here: Understanding how attention weights distribute over history items (and why target concatenation changes this) is essential for debugging interest modeling.
  - Quick check question: If attention weights collapse to a single position, what does this imply about position encodings or sequence construction?

- Concept: **Cross networks and bounded-degree feature interactions**
  - Why needed here: DCNv2's cross layers are not standard MLPs; understanding their inductive bias helps diagnose why removing them causes the largest performance drop.
  - Quick check question: How does the number of cross layers relate to the maximum interaction degree that can be explicitly modeled?

- Concept: **Frozen vs. learnable embeddings in transfer learning**
  - Why needed here: The negative result with DIN + multimodal embeddings illustrates a common failure mode when injecting pretrained features without alignment.
  - Quick check question: If frozen embeddings hurt performance, what are three architectural interventions to try before unfreezing?

## Architecture Onboarding

- Component map: Input Features → Embedding Layer (ID embeddings || frozen multimodal) → Sequential Module (Transformer with target concatenation) → User Interest Vector (last-k flatten + max-pool) → Feature Interaction (DCNv2 cross || deep MLP) → Prediction MLP → Sigmoid → CTR

- Critical path: The concatenation of target embedding to each history item (Eq. 4) is the key differentiator from standard sequential models. Trace this carefully when debugging attention patterns.

- Design tradeoffs:
  - **Simple concatenation vs. alignment layers**: Authors chose concatenation due to time constraints; explicitly note this as suboptimal.
  - **k=16 for short-term interest**: Balances recency bias against context; ablation shows sensitivity (Fig. 2).
  - **2 Transformer layers, 3 cross layers**: Relatively shallow; likely tuned for inference latency constraints.

- Failure signatures:
  - Adding multimodal embeddings decreases AUC → check base model capacity and alignment
  - Large gap between validation and test AUC → overfitting to validation set; reduce early stopping patience
  - Attention weights uniform across history → check padding mask implementation

- First 3 experiments:
  1. **Ablation by component**: Remove Transformer, then DCNv2, then multimodal embeddings separately to establish contribution of each (replicate Table 2 on your data).
  2. **Multimodal alignment test**: Add a single learnable projection layer between frozen multimodal embeddings and concatenation; compare against raw concatenation.
  3. **Target concatenation validation**: Run Transformer with and without target concatenation; visualize attention distributions to confirm context-dependent weighting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can quantization methods effectively transform frozen multimodal embeddings into semantic, learnable item embeddings that outperform simple concatenation?
- Basis in paper: [explicit] "In the future, we will explore quantization methods to transform the frozen multimodal embeddings into semantic and learnable item embeddings."
- Why unresolved: Simple concatenation was used due to time constraints; quantization was explicitly deferred as future work.
- What evidence would resolve it: Ablation experiments comparing quantization approaches (e.g., VQ-VAE, product quantization) against concatenation on the same benchmark.

### Open Question 2
- Question: How can prior knowledge from multimodal embeddings guide the learning of collaborative ID embeddings?
- Basis in paper: [explicit] "How to effectively utilize prior knowledge of the multimodal embeddings to guide the learning of collaborative ID embeddings is also a worthy research direction."
- Why unresolved: Current architecture learns ID embeddings independently from multimodal features; no transfer mechanism was implemented.
- What evidence would resolve it: Experiments showing multimodal-informed ID embeddings achieve higher AUC than independently learned embeddings.

### Open Question 3
- Question: What causes multimodal embeddings to degrade performance in simpler architectures (DIN, Transformer-free models)?
- Basis in paper: [inferred] Table 2 shows DIN AUC drops 0.9326→0.8577 and w/o Transformer drops 0.9741→0.9688 when multimodal features are added; authors speculate alignment issues.
- Why unresolved: Only speculation is provided; no systematic investigation of architectural compatibility with frozen multimodal features.
- What evidence would resolve it: Controlled ablations identifying which architectural components (attention, cross-interaction layers) enable multimodal feature utilization without degradation.

## Limitations

- Multimodal embedding integration approach is acknowledged as suboptimal - simple concatenation was used due to time constraints rather than proper alignment methods
- Frozen embedding strategy limits model's ability to adapt multimodal features to CTR prediction task
- Lack of direct comparisons with alternative multimodal integration strategies and alignment mechanisms
- Limited ablation studies on sequence length and specific side feature interactions

## Confidence

**High Confidence**: The sequential modeling with target concatenation mechanism is well-supported by the ablation results (Table 2) and the theoretical motivation is sound. The performance degradation when removing the Transformer component is substantial and consistent.

**Medium Confidence**: The DCNv2 component's effectiveness is strongly evidenced by the largest performance drop upon removal, but the paper lacks direct comparison with alternative feature interaction methods (e.g., FiBiNET, AutoInt) that would strengthen the claim about DCNv2's superiority.

**Low Confidence**: The multimodal embedding integration claims are the weakest, as the paper itself acknowledges the approach is preliminary. The negative result with DIN suggests fundamental alignment issues that are not fully resolved, and the simple concatenation approach may be leaving significant performance gains unrealized.

## Next Checks

1. **Multimodal Alignment Validation**: Implement a learnable projection layer (single linear transformation) between the frozen multimodal embeddings and the concatenation point. Compare performance against the current baseline to quantify the alignment gap and determine if this simple fix recovers the performance lost in the DIN baseline.

2. **Attention Pattern Analysis**: Visualize and analyze the attention weight distributions from the Transformer when using target concatenation versus standard sequential encoding. Quantify whether attention becomes more focused on historically relevant items and whether weights remain diverse across the sequence.

3. **Sequence Length Sensitivity**: Systematically vary the history sequence length N (e.g., 10, 20, 50, 100) and measure the impact on AUC performance and attention patterns. This would reveal whether the model's effectiveness scales with more historical context or if there's an optimal range where attention quality degrades.