---
ver: rpa2
title: 'Self-Improving Pretraining: using post-trained models to pretrain better models'
arxiv_id: '2601.21343'
source_url: https://arxiv.org/abs/2601.21343
tags:
- pretraining
- quality
- suffix
- rollouts
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Improving Pretraining, a method that
  enhances language model pretraining by leveraging a post-trained judge model to
  provide superior supervision signals. The core idea is to treat pretraining as a
  sequence generation task, where the model generates high-quality suffixes given
  prefixes, and uses the judge to evaluate and reward generations for quality, safety,
  and factuality.
---

# Self-Improving Pretraining: using post-trained models to pretrain better models

## Quick Facts
- arXiv ID: 2601.21343
- Source URL: https://arxiv.org/abs/2601.21343
- Authors: Ellen Xiaoqing Tan; Shehzaad Dhuliawala; Jing Xu; Ping Yu; Sainbayar Sukhbaatar; Jason Weston; Olga Golovneva
- Reference count: 40
- Primary result: Up to 86.3% win rate in generation quality, 36.2% relative improvement in factuality, and 18.5% relative improvement in safety over standard pretraining

## Executive Summary
This paper introduces Self-Improving Pretraining, a method that enhances language model pretraining by leveraging a post-trained judge model to provide superior supervision signals. The core idea is to treat pretraining as a sequence generation task, where the model generates high-quality suffixes given prefixes, and uses the judge to evaluate and reward generations for quality, safety, and factuality. The method dynamically balances between using the original suffix, rewrites, and rollouts from the current policy, adapting as training progresses. Experiments show significant improvements: up to 86.3% win rate in generation quality, 36.2% relative improvement in factuality, and 18.5% relative improvement in safety over standard pretraining. The approach also achieves strong performance in from-scratch pretraining settings, demonstrating its effectiveness in building safer, more factual, and higher-quality models from the ground up.

## Method Summary
Self-Improving Pretraining re-envisions pretraining as prefix-conditioned suffix generation. A post-trained model acts as judge and rewriter to provide superior supervision signals for quality, safety, and factuality over standard next-token prediction. The method trains three components: a judge model (via GRPO on synthetic safety/quality pairs), a rewriter model (via GRPO to copy safe suffixes or rewrite unsafe ones), and a policy model (via online DPO or RF-NLL). For each prefix, the policy generates multiple rollouts and rewrites, the judge scores them, and the policy is updated to prefer higher-scoring suffixes. Training uses SlimPajama/RedPajama datasets, chunk size N=128, batch 256, 16 rollouts, T=1.0, top_p=1.0, lr=5e-6 cosine, for 2000 steps continual or 21000 steps scratch.

## Key Results
- 86.3% win rate in generation quality versus baseline (GPT pairwise comparison)
- 36.2% relative improvement in factuality (FActScore, HaluEval, TruthfulQA)
- 18.5% relative improvement in safety (weighted avg across RealToxicityPrompts, XStest, ToxiGen)

## Why This Works (Mechanism)
The method works by leveraging a post-trained judge model to provide richer, higher-quality supervision signals during pretraining. Instead of simply predicting the next token, the model learns to generate high-quality, safe, and factual suffixes conditioned on prefixes. The judge evaluates multiple candidate suffixes (original, rewritten, and rollouts), allowing the policy to learn from the best examples rather than just the ground truth. This dynamic supervision adapts as training progresses, helping the model improve in areas where standard pretraining struggles, such as safety and factuality.

## Foundational Learning
- **Prefix-conditioned suffix generation**: Why needed? Enables generation of contextually appropriate suffixes. Quick check: Model generates coherent suffixes for diverse prefixes.
- **Judge-based supervision**: Why needed? Provides higher-quality, multi-dimensional feedback than ground truth alone. Quick check: Judge consistently ranks high-quality suffixes above low-quality ones.
- **Online DPO training**: Why needed? Allows policy to learn from judge feedback without pre-collected data. Quick check: Policy loss decreases and generation quality improves over training.
- **Multiple rollouts and rewrites**: Why needed? Increases diversity of training signals and prevents collapse to safe but meaningless outputs. Quick check: Generated suffixes vary meaningfully and maintain coherence.
- **Synthetic data for judge/rewriter**: Why needed? Enables scalable, targeted training of supervision models. Quick check: Judge/rewriter accuracy on synthetic validation sets is high.

## Architecture Onboarding
- **Component map**: Data stream -> Prefix/Suffix split -> Policy generation (rollouts + rewrites) -> Judge scoring -> Online DPO update -> Policy model
- **Critical path**: Prefix input -> multiple suffix candidates (rollouts, rewrites, original) -> judge evaluation -> online DPO update -> policy model
- **Design tradeoffs**: Judge model size vs. accuracy; number of rollouts vs. computational cost; online vs. offline DPO.
- **Failure signatures**: Model collapse to safe but meaningless sequences; judge favoring "complete" over coherent suffixes early in training.
- **First experiments**: 1) Train judge on synthetic pairs and evaluate accuracy. 2) Train rewriter to copy safe, rewrite unsafe suffixes. 3) Train policy with online DPO on a small dataset and monitor generation quality.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of generating multiple rollouts and requiring judge evaluations for each training example could become prohibitive for larger models.
- Reliance on synthetic data for judge training introduces potential distribution shift issues that may affect real-world performance.
- The paper doesn't extensively explore the impact of different judge model sizes or alternative training paradigms for the judge and rewriter components.

## Confidence
- **High Confidence**: Core methodology and experimental results within tested parameter range are robust and well-supported.
- **Medium Confidence**: Claims about from-scratch pretraining and safety improvements warrant additional validation across different model scales and architectures.
- **Low Confidence**: Assertion that this represents the first successful integration of post-trained models into pretraining needs careful verification.

## Next Checks
1. **Scalability Assessment**: Reproduce the experiments with larger model sizes (7B-70B parameters) to verify that the computational overhead remains manageable and the quality/safety/factuality improvements scale proportionally.
2. **Distribution Shift Analysis**: Evaluate the judge model's performance on out-of-distribution prompts and real-world data to quantify the impact of synthetic training data on downstream task performance.
3. **Resource Efficiency Benchmarking**: Measure the wall-clock time, GPU memory usage, and energy consumption per training step compared to standard pretraining to provide a complete cost-benefit analysis of the approach.