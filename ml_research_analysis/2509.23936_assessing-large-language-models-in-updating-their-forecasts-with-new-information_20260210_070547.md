---
ver: rpa2
title: Assessing Large Language Models in Updating Their Forecasts with New Information
arxiv_id: '2509.23936'
source_url: https://arxiv.org/abs/2509.23936
tags:
- confidence
- news
- information
- forecasts
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EVOLVECAST, a framework for evaluating how
  large language models update forecasts and confidence in response to new information.
  The key idea is to measure belief dynamics by comparing the model's confidence updates
  against reference human forecasts, rather than against final outcomes, to assess
  rational belief updating under uncertainty.
---

# Assessing Large Language Models in Updating Their Forecasts with New Information

## Quick Facts
- **arXiv ID**: 2509.23936
- **Source URL**: https://arxiv.org/abs/2509.23936
- **Authors**: Zhangdie Yuan; Zifeng Ding; Andreas Vlachos
- **Reference count**: 22
- **Primary result**: Large language models exhibit conservative, inconsistent belief updating when exposed to new information, failing to match human reference standards despite showing directional reasoning capability.

## Executive Summary
This paper introduces EVOLVECAST, a framework for evaluating how large language models update forecasts and confidence in response to new information. Rather than measuring final prediction accuracy, the framework assesses rational belief updating by comparing confidence changes against reference human forecasts. Using 1,613 question-news pairs from Metaculus, the authors find that models update conservatively and inconsistently, with neither verbalized nor logit-based confidence methods consistently outperforming the other. The results reveal that current approaches like RAG-based methods are insufficient for probabilistic reasoning, as models treat new information as context rather than evidence that should shift posterior probabilities.

## Method Summary
The study evaluates 12 LLMs (DeepSeek-R1-Distill-Qwen-1.5B/7B, DeepSeek-R1-Distill-LLaMA-8B, and base counterparts) on 1,613 question-news pairs from 203 Metaculus binary questions. Models are prompted to forecast at two timestamps: T0 (question-only) and T1 (question + news), with date-anchored prompts to prevent leakage. Confidence is extracted via verbalized 1-10 scale or logit-based mean token probability. Updates are compared against reference human forecasts aggregated at corresponding timestamps using normalized confidence scores. Evaluation metrics include directional agreement (MDA, F1), magnitude alignment (MSE, SMSPE), and calibration change (Brier score, ΔBrier).

## Key Results
- Models show systematic conservative bias, with larger models producing excessive "Still" predictions across true labels
- Directional prompting ("Up/Down/Still") yields markedly higher MDA (0.47) compared to probability-based methods (0.34), validating qualitative reasoning strength
- Accumulated news context degrades performance for 7B/8B models, indicating failure to weigh evidence hierarchically
- Neither verbalized nor logit-based confidence methods consistently outperform the other, and both fall short of human reference standards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models exhibit systematic conservative bias because parametric memory dominates over context-based evidence integration
- Mechanism: Pre-training creates a strong prior that functions as an "overwhelming anchor," preventing context tokens from shifting probability distributions proportionally to rational Bayesian updates
- Core assumption: Tension between parametric knowledge and external context causes under-reaction rather than random noise
- Evidence anchors:
  - "The massive weight of pre-training data acts as an overwhelming anchor, preventing the short context window from shifting the probability distribution as significantly as a rational update would require" (Page 6)
  - "Models treat new information as retrieval context rather than evidence that shifts posterior probability"
  - Related work on confidence calibration finds over-reasoning impairs calibration

### Mechanism 2
- Claim: Accumulated news context degrades performance because models lack hierarchical evidence weighting
- Mechanism: Models fail to prioritize recent or relevant signals over older context, instead "accumulating noise" and treating all updates with similar salience
- Core assumption: Humans discount outdated news dynamically; models lack this selective attention mechanism
- Evidence anchors:
  - "Both DS R1 Qwen-7B and DS R1 LLaMA-8B see declines in MDA and F1 when given the full sequence of updates" (Table 3, Page 8)
  - "Models struggle to weigh multiple temporally ordered snippets, often overemphasizing earlier or less relevant updates rather than the most recent signal" (Page 7)

### Mechanism 3
- Claim: Direct directional prompting outperforms probability elicitation because it bypasses calibration errors while retaining signal detection
- Mechanism: Subtracting two noisy probability estimates amplifies calibration errors; direct classification isolates qualitative reasoning from quantitative calibration
- Core assumption: Directional reasoning and magnitude calibration are partially separable cognitive operations in LLMs
- Evidence anchors:
  - "Compared to the probability-based approach, the direct method yields markedly higher MDA and F1, especially for larger models. DS R1 LLaMA-8B achieves nearly 0.49 MDA... compared to only 0.34 under verbalized confidence" (Page 8)
  - "Models are substantially better at reasoning about the qualitative direction of change than at producing well-calibrated probabilities" (Page 8)

## Foundational Learning

- **Concept: Bayesian belief updating**
  - Why needed here: The framework evaluates whether LLMs approximate rational posterior updating when new evidence arrives
  - Quick check question: If a forecaster moves from 55% to 70% confidence after news, what implicit assumption are they making about the likelihood ratio of that news?

- **Concept: Confidence calibration**
  - Why needed here: The paper distinguishes between correct predictions and calibrated confidence
  - Quick check question: Why does evaluating against final binary outcomes rather than contemporaneous human aggregates encourage overconfidence?

- **Concept: Black-box vs. white-box confidence elicitation**
  - Why needed here: The paper compares verbalized probabilities against logit-based probabilities
  - Quick check question: Why might logit-based confidence be more sensitive to prompt length than verbalized confidence?

## Architecture Onboarding

- **Component map:**
  - Metaculus questions → News retrieval (Google Search API + SBERT) → Question-news pairs → Reference forecasts (T0/T1) → Model inference (T0/T1) → Confidence extraction (verbalized/logit) → Evaluation metrics

- **Critical path:**
  1. Filter Metaculus questions (≥100 forecasts, binary, unambiguous resolution)
  2. Identify update timestamps via comment stream monitoring
  3. Retrieve news within 1-week window; rank by semantic similarity to comment text
  4. Run inference at T0 (question-only) and T1 (question+news) with date-anchored prompts
  5. Extract confidence via verbalized or logit-based method
  6. Compute Δp and compare to reference Δh across all metrics

- **Design tradeoffs:**
  - Verbalized confidence is more interpretable but subject to default-to-conservative bias
  - Logit-based confidence is mechanically grounded but highly sensitive to prompt length and decoding hyperparameters
  - Single vs. accumulated news: single updates isolate signal; accumulated better simulates real forecasting but degrades performance

- **Failure signatures:**
  - Excessive "Still" predictions indicate conservative bias (larger models)
  - Scattered Up/Down confusion indicates noise rather than systematic reasoning (smaller models)
  - Accumulated context increases spurious directional movement without improving accuracy
  - Providing human forecast references as context yields no improvement—models cannot exploit external anchors

- **First 3 experiments:**
  1. **Baseline replication**: Run DeepSeek-R1-Qwen-7B on single-update setting with both verbalized and logit-based confidence extraction. Verify MDA ≈ 0.35, ΔBrier ≈ +0.021
  2. **Directional prompting comparison**: Switch to direct "Up/Down/Still" classification on the same data. Confirm MDA improves to ≈ 0.47
  3. **Accumulated news ablation**: Provide full temporal news sequences to the same model. Expect MDA decline (≈ 0.32) and increased false directional predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectural modifications enable Bayesian-like belief updating, rather than treating new information merely as additional context tokens?
- Basis in paper: [explicit] "Current approaches (e.g., RAG-based methods) for 'updating' model knowledge are insufficient for probabilistic reasoning; models treat new information as retrieval context rather than evidence that shifts posterior probability."
- Why unresolved: The paper identifies this fundamental mismatch but does not propose or test architectural solutions
- What evidence would resolve it: Demonstration that a modified architecture (e.g., explicit latent belief state, posterior updating mechanism) produces calibrated forecast revisions aligned with human references

### Open Question 2
- Question: Why does providing accumulated news context degrade rather than improve belief updating in larger models?
- Basis in paper: [explicit] "Both Qwen-7B and LLaMA-8B degrade when given the full sequence of updates."
- Why unresolved: The paper documents the phenomenon but does not isolate whether the issue is evidence weighting, context length, attention dilution, or failure to discount outdated information
- What evidence would resolve it: Controlled experiments varying evidence relevance, recency, and quantity to identify which factors cause degradation, paired with attention analysis

### Open Question 3
- Question: What mechanisms allow reasoning-tuned models (DeepSeek-R1) to outperform base models in belief updating, and can these be isolated and strengthened?
- Basis in paper: [explicit] "Reasoning-tuned models consistently outperform their base counterparts, particularly at larger scales"
- Why unresolved: The comparison conflates multiple post-training differences without ablation
- What evidence would resolve it: Controlled training experiments isolating specific reasoning interventions to measure their individual contributions to belief updating performance

### Open Question 4
- Question: Why are models unable to exploit explicit human forecast references as calibration anchors?
- Basis in paper: [explicit] The ablation where "prompts are augmented with a line such as 'Human forecast for this question on t is ht%'" showed "no measurable improvements"
- Why unresolved: The paper documents the failure but does not determine whether models ignore, misinterpret, or actively override such anchors
- What evidence would resolve it: Probing studies examining how models represent and use numerical anchors, plus experiments with alternative anchor presentation formats

## Limitations
- Dataset construction variability: Automated news retrieval via Google Search API may introduce temporal and semantic inconsistencies across re-runs
- Confidence extraction method sensitivity: Logit-based confidence is highly sensitive to prompt length, decoding parameters, and token sampling strategies
- Model-specific calibration effects: Conservative bias may be partially attributable to prompt templates rather than inherent model limitations

## Confidence
- **High confidence**: Models exhibit systematic conservative bias in belief updating; directional prompting improves qualitative reasoning but not calibration; accumulated news degrades performance in current models
- **Medium confidence**: The primary failure mode is parametric memory dominance over context-based evidence integration; logit-based and verbalized confidence methods perform comparably and both fall short of human standards
- **Low confidence**: Direct directional prompting is fundamentally superior to probability elicitation for belief updating tasks; current RAG-based approaches are categorically insufficient for probabilistic reasoning

## Next Checks
1. **Dataset alignment verification**: Reproduce the news-question alignment pipeline using SBERT and Google Search API on a subset of Metaculus questions. Compare retrieved snippets to the original paper's alignment to quantify semantic overlap and temporal consistency
2. **Confidence method ablation study**: Run a within-model comparison of verbalized vs. logit-based confidence extraction on the same instances, varying prompt length and decoding parameters systematically. Quantify how much of the performance gap is due to method sensitivity vs. genuine calibration differences
3. **Temporal evidence weighting experiment**: Modify the prompt structure to explicitly prioritize recent news over older context (e.g., by appending a recency score or reordering snippets). Measure whether MDA and directional accuracy improve in the accumulated news condition