---
ver: rpa2
title: Homophily-aware Heterogeneous Graph Contrastive Learning
arxiv_id: '2501.08538'
source_url: https://arxiv.org/abs/2501.08538
tags:
- graph
- heterogeneous
- learning
- node
- self-expressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the performance gap of heterogeneous graph pre-training
  (HGP) methods when applied to real-world graphs with low homophily. The authors
  propose HGMS, a contrastive learning framework that improves node representations
  by enhancing the homophily of augmented views through a heterogeneous edge dropping
  strategy and inferring homophily via multi-view self-expression.
---

# Homophily-aware Heterogeneous Graph Contrastive Learning

## Quick Facts
- **arXiv ID:** 2501.08538
- **Source URL:** https://arxiv.org/abs/2501.08538
- **Reference count:** 40
- **One-line primary result:** Improves heterogeneous graph pre-training performance on low homophily graphs through homophily-aware augmentation and self-expression

## Executive Summary
This paper addresses the performance gap of heterogeneous graph pre-training methods when applied to real-world graphs with low homophily. The authors propose HGMS, a contrastive learning framework that improves node representations by enhancing the homophily of augmented views through a heterogeneous edge dropping strategy and inferring homophily via multi-view self-expression. The method effectively tackles the challenge of heterophily in heterogeneous graphs, achieving significant improvements over state-of-the-art baselines.

## Method Summary
HGMS is a self-supervised pre-training framework for heterogeneous graphs that operates through three key mechanisms: (1) heterogeneous edge dropping based on metapath-based connection strength to create homophily-enhanced views, (2) multi-view self-expression to infer homophily relationships without labels, and (3) modified contrastive loss that uses the self-expressive matrix to identify and handle false negative pairs. The method consists of an augmentation module, a metapath-specific GCN encoder with semantic attention, a self-expressive module (either network-based or closed-form), and a contrastive loss function that incorporates the homophily information.

## Key Results
- Achieves up to 19.07% and 100% relative improvements in node classification and clustering tasks respectively compared to state-of-the-art baselines
- Outperforms eleven state-of-the-art baselines across six benchmark datasets (ACM, DBLP, Aminer, Freebase, IMDB, Academic)
- Theoretical analysis confirms tighter mutual information bounds compared to standard contrastive learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Heterogeneous edge dropping based on connection strength probabilistically increases the homophily of augmented graph views
- **Mechanism:** The strategy drops edges on the raw heterogeneous graph rather than the metapath subgraph. Edges involved in multiple metapath instances (high MCS) have higher survival probability, filtering out weak semantic connections
- **Core assumption:** Nodes connected by multiple paths (high MCS) are more likely to share the same label than those connected by single paths
- **Evidence anchors:** Abstract states "enhancing the homophily of augmented views through a heterogeneous edge dropping strategy"; Section 4.2 shows "metapath-based neighbor pairs with higher connection strength tend to exhibit higher homophily"; Fig 2(c) demonstrates HE_Random enhances MHR with increasing augmentation ratio

### Mechanism 2
- **Claim:** A self-expressive matrix $S$ can infer homophily relationships without labels by solving a regularized reconstruction problem
- **Mechanism:** The model enforces that a node's representation can be linearly reconstructed from its neighbors ($X = SX$) with Elastic Net regularization, forcing $S$ to become sparse and low-rank with block-diagonal structure
- **Core assumption:** Nodes in the same class reside in the same low-dimensional subspace, allowing them to linearly reconstruct each other better than nodes in different classes
- **Evidence anchors:** Abstract mentions "inferring homophily via multi-view self-expression"; Section 5.3 defines the optimization objective; Fig 5 visualizations show the learned matrix exhibits a "block-diagonal pattern"

### Mechanism 3
- **Claim:** Using the self-expressive matrix to penalize similarity of negative pairs creates tighter MI bounds than standard contrastive loss
- **Mechanism:** Weights negative terms by $(1 - \hat{S}_{ij})$ in the loss function, reducing pressure to separate representations of likely same-class nodes
- **Core assumption:** The self-expressive matrix $\hat{S}$ provides a sufficiently accurate proxy for ground-truth class similarity
- **Evidence anchors:** Abstract states the self-expressive matrix "is used to identify false negatives in contrastive loss"; Eq (16) modifies the denominator of the loss function; Theorem 1 proves $-L_s \leq I(X; U, V)$

## Foundational Learning

- **Concept:** **Metapath-based Homophily vs. Heterophily**
  - **Why needed here:** The paper explicitly challenges the assumption that metapaths imply semantic similarity
  - **Quick check question:** If a graph has low homophily, does a random neighbor aggregation (like standard GCN) help or hurt classification accuracy?

- **Concept:** **Self-Expressive Learning (Subspace Clustering)**
  - **Why needed here:** The paper imports this technique from computer vision/clustering to build the homophily matrix
  - **Quick check question:** In the equation $X = SX$, what does a non-zero entry $S_{ij}$ imply about the relationship between node $i$ and node $j$?

- **Concept:** **False Negatives in Contrastive Learning**
  - **Why needed here:** In standard SSL, negative samples are assumed to be dissimilar, but in graphs, random negatives might actually be same-class nodes
  - **Quick check question:** What happens to the representation space if a positive pair is incorrectly treated as a negative pair in the loss function?

## Architecture Onboarding

- **Component map:** Augmentation Module -> Encoder -> Self-Expressive Head -> Loss Module
- **Critical path:** The dependency flow is **Augmentation → Encoder → Self-Expressive Matrix $S$**. You cannot calculate the final loss without solving for $S$ first.
- **Design tradeoffs:**
  - **HGMS-N vs. HGMS-C:** HGMS-C (Closed-form) is mathematically exact and stable but requires holding matrices in memory; HGMS-N (Network) scales better for massive graphs but requires iterative training
  - **Augmentation Ratio $p$:** Too low, heterophilic edges persist; too high, graph becomes too sparse/fragmented
- **Failure signatures:**
  - **Performance Collapse:** Likely caused by mismatched self-expressive regularization coefficients, resulting in dense/uniform $S$ matrix
  - **Over-smoothing:** If augmentation drops too many edges, the "connection strength" bias disappears, and performance reverts to random dropping baselines
- **First 3 experiments:**
  1. **MCS Distribution Analysis:** Before training, plot "Connection Strength" vs. "Label Similarity" for your specific dataset
  2. **Matrix Visualization:** After 1 epoch, visualize the self-expressive matrix $S$ to check for block-diagonal structures
  3. **Ablation on Edge Dropping:** Compare "Random Dropping" vs. "Heterogeneous Dropping" on a validation set to isolate the specific contribution of the homophily enhancement strategy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the homophily inference mechanism be adapted for metapath-free heterogeneous graphs where semantic relationships must be discovered automatically?
- **Basis:** The methodology explicitly relies on predefined metapaths to define connection strength and construct subgraphs
- **Why unresolved:** The current augmentation and self-expression modules depend on metapath-specific adjacency matrices and attention weights, limiting applicability to domains lacking expert knowledge
- **What evidence would resolve it:** A reformulation of the encoder and connection strength metrics using general relation types, demonstrating comparable performance on datasets without predefined metapaths

### Open Question 2
- **Question:** Is the positive correlation between Metapath-based Connection Strength (MCS) and homophily robust in graphs where high connectivity implies heterophily?
- **Basis:** The edge dropping strategy is predicated on assuming high MCS reliably indicates high homophily
- **Why unresolved:** In domains like fraud detection, nodes may deliberately form strong connections with dissimilar entities, violating the core assumption
- **What evidence would resolve it:** Experiments on adversarial or noise-heavy datasets where high-degree connections are predominantly heterophilous, comparing HE_Random against content-based augmentation strategies

### Open Question 3
- **Question:** How does the framework perform on dynamic heterogeneous graphs where homophily patterns evolve over time?
- **Basis:** The model treats the heterogeneous graph as a static snapshot and learns a fixed self-expressive matrix
- **Why unresolved:** The static self-expressive matrix captures instantaneous subspace relationships but cannot model temporal drift in node semantics
- **What evidence would resolve it:** An extension incorporating a temporal encoder or dynamic self-expression module, validated on benchmark datasets with timestamped edge sequences

## Limitations
- The MCS-based edge dropping assumes strong edges correlate with label similarity, but this relationship isn't universally valid across all graph domains
- The self-expressive matrix inference relies on subspace assumptions that may fail with noisy or high-dimensional features
- Theoretical bounds assume ideal conditions not always met in practice (e.g., perfect reconstruction)

## Confidence
- **High Confidence:** Experimental results showing HGMS outperforms baselines (Sections 6.2.1-6.2.3)
- **Medium Confidence:** Theoretical analysis of mutual information bounds (Section 5.6)
- **Medium Confidence:** Mechanism validity (requires domain-specific verification)

## Next Checks
1. **MCS Validation:** Plot MCS vs. label similarity for your specific dataset to verify the core augmentation assumption holds
2. **Matrix Structure Analysis:** Visualize the learned self-expressive matrix after training to confirm block-diagonal patterns emerge
3. **False Negative Impact:** Conduct ablation studies comparing standard contrastive loss vs. the $S$-weighted version to quantify the false negative mitigation effect