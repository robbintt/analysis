---
ver: rpa2
title: 'Paper2Video: Automatic Video Generation from Scientific Papers'
arxiv_id: '2510.05096'
source_url: https://arxiv.org/abs/2510.05096
tags:
- video
- presentation
- generation
- slide
- slides
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces Paper2Video, the first benchmark for automatic
  academic presentation video generation, featuring 101 research papers paired with
  author-created videos, slides, and speaker metadata. It also proposes four tailored
  evaluation metrics: Meta Similarity, PresentArena, PresentQuiz, and IP Memory, designed
  to assess video quality, information coverage, and impact on author visibility.'
---

# Paper2Video: Automatic Video Generation from Scientific Papers

## Quick Facts
- **arXiv ID:** 2510.05096
- **Source URL:** https://arxiv.org/abs/2510.05096
- **Reference count:** 40
- **Primary result:** Introduces Paper2Video benchmark and PaperTalker system achieving 10% higher PresentQuiz accuracy than human-made videos

## Executive Summary
Paper2Video introduces the first benchmark for automatic academic presentation video generation, featuring 101 research papers paired with author-created videos, slides, and speaker metadata. The authors propose four tailored evaluation metrics—Meta Similarity, PresentArena, PresentQuiz, and IP Memory—designed to assess video quality, information coverage, and impact on author visibility. To address this task, they develop PaperTalker, a multi-agent framework that integrates slide generation with fine-grained layout refinement using Tree Search Visual Choice, cursor grounding via GUI-grounded models and WhisperX, subtitle generation, speech synthesis, and talking-head rendering. Experiments show that PaperTalker outperforms existing baselines, producing content comparable to human-created presentations while reducing generation time by over 6×.

## Method Summary
PaperTalker is a multi-agent framework that automatically generates academic presentation videos from scientific papers (LaTeX source). The pipeline consists of a Slide Builder (GPT-4.1 generates LaTeX Beamer code refined by Tree Search Visual Choice), Subtitle and Cursor builders (VLM generates scripts and visual-focus prompts; UI-TARS grounds cursor coordinates; WhisperX aligns timestamps), and a Talker Builder (F5-TTS clones voice; Hallo2/FantasyTalking renders talking head, parallelized per slide). The system takes as input a paper's LaTeX project, speaker portrait, and voice sample, producing a complete video with slides, subtitles, cursor trajectories, speech, and a talking head.

## Key Results
- PaperTalker outperforms existing baselines on all four proposed evaluation metrics
- Achieves 10% higher PresentQuiz accuracy (50.0%) compared to human-made videos (40.0%)
- Reduces generation time by over 6× through slide-wise parallelization
- Produces content comparable to human-created presentations on Meta Similarity and PresentArena metrics

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Visual Layout Search (Tree Search Visual Choice)
Instead of relying on an LLM to predict exact numerical parameters, the system generates multiple visual variants using rule-based parameter sweeps and uses a Vision-Language Model as a "judge" to select the candidate with the best visual coverage and no overflow. This approach improves slide structural quality compared to direct numerical prompting, which models handle insensitively.

### Mechanism 2: Parallelized Slide-wise Rendering
The pipeline treats each slide as an isolated video clip, generating talking-head videos for each slide in parallel rather than processing the full video sequentially. This approach reduces total video generation time significantly without visibly disrupting narrative flow, as the visual discontinuity of a presenter between slides is acceptable to viewers.

### Mechanism 3: Spatial-Temporal Cursor Grounding via GUI Agents
The system connects speech to cursor movement by generating "visual focus prompts" and using GUI-grounded models (UI-TARS) to find pixel coordinates, combined with WhisperX for timing alignment. This mapping of abstract narration to specific screen coordinates benefits from using "computer-use" models rather than pure text-based heuristics.

## Foundational Learning

- **Concept: Beamer (LaTeX) for Slide Generation**
  - Why needed here: This architecture relies on the declarative nature of LaTeX to auto-arrange content, unlike PPTX or XML formats used in baselines.
  - Quick check question: Can you explain why a declarative syntax (like LaTeX) might be safer for automated generation than a positional drag-and-drop coordinate system?

- **Concept: Computer-Use Grounding (GUI Agents)**
  - Why needed here: The Cursor Builder requires mapping text descriptions to specific (x,y) pixels on a slide image, solved here using GUI-interaction models rather than OCR or heuristics.
  - Quick check question: How does a GUI-grounding model differ from a standard object detection model when interpreting a screenshot?

- **Concept: Talking Head Synthesis (Audio-Driven Video)**
  - Why needed here: The "Talker Builder" component is critical for the "IP Memory" metric; understanding the trade-off between lip-sync fidelity and identity preservation is key.
  - Quick check question: What are the visual artifacts typically associated with long-duration talking head generation, and how does slide-wise parallelization mitigate them?

## Architecture Onboarding

- **Component map:** Paper (PDF/LaTeX) + Author Avatar + Voice Sample → Slide Builder (Coder Agent → Beamer Compiler → Tree Search Visual Choice → Final Slides) → Subtitle Builder (VLM → Scripts + Cursor Prompts) → Cursor Builder (UI-TARS + WhisperX) → Talker Builder (F5-TTS + Hallo2/FantasyTalking) → Composited Video

- **Critical path:** The Slide Builder is the primary bottleneck. If the LaTeX code fails to compile or the Tree Search fails to find a valid layout, the downstream Subtitle and Cursor builders cannot execute.

- **Design tradeoffs:** LaTeX offers higher formal structure and automated layout but is brittle compared to the resilience of template-based PPTX editing. Parallelization is 6x faster but sacrifices cross-slide temporal continuity of the avatar.

- **Failure signatures:** Overfull \hbox indicates Tree Search Visual Choice failed to resolve text overflow; Cursor Drift suggests GUI-grounding model misinterpreted the slide region; Uncanny Valley in talking head often caused by misalignment between audio duration and video generation constraints.

- **First 3 experiments:** 1) Run Slide Builder on 5 papers with complex tables to verify Tree Search reduces "Overfull" errors vs. baseline LLM editor. 2) Generate slides with dense figures, run Cursor Builder, and calculate IoU between predicted cursor points and actual visual targets. 3) Measure total generation time for 5-minute video using parallel vs. sequential Talker Builder to confirm 6× speedup.

## Open Questions the Paper Calls Out
1. **IP Memory validation:** How can the "IP Memory" metric be validated against real-world measures of scholarly impact, rather than relying solely on VideoLLM proxy simulations? The current implementation uses a VideoLLM to associate authors with questions, which acts as a synthetic proxy rather than a measurement of actual human academic recall or citation impact.

2. **PresentQuiz accuracy interpretation:** Why does PaperTalker outperform human-created videos on PresentQuiz accuracy, and does this indicate a trade-off between natural presentation flow and information density? It's unclear if AI-generated content is genuinely better at teaching or if the VideoLLM evaluator simply prefers denser, text-heavy OCR-friendly slides.

3. **Cross-slide continuity impact:** Does the "hard scene change" approach to slide transitions degrade viewer immersion compared to maintaining temporal continuity of the presenter? While efficient, cutting between independent talking-head clips might introduce cognitive load or visual jarring that disrupts the viewing experience.

## Limitations
- **Evaluation subjectivity:** PresentArena and IP Memory rely on model-based judgments that may inherit biases from training data, potentially overestimating perceived video quality.
- **Domain specificity:** System is optimized for LaTeX-based academic papers; performance on PPTX/PDF papers or non-STEM domains remains unvalidated.
- **Visual complexity handling:** Tree Search Visual Choice may struggle with highly dense or unconventional slide layouts where parameter sweeps cannot find valid solutions.

## Confidence
- **Tree Search Visual Choice effectiveness:** High (multiple ablation studies and qualitative comparisons provide strong evidence)
- **6× speedup from parallelization:** Medium (figure reported but exact baseline configuration not fully specified)
- **10% higher PresentQuiz accuracy vs. human videos:** Low (requires careful interpretation due to uncontrolled comparison with heterogeneous human content)

## Next Checks
1. **Cross-domain generalization test:** Evaluate PaperTalker on 20 PPTX-based academic presentations from non-STEM fields to measure performance degradation and identify domain-specific failure patterns.

2. **Visual complexity benchmark:** Create controlled test suite of 50 slides with progressively complex layouts and measure Tree Search success rates and compilation errors across difficulty levels.

3. **Cross-slide continuity assessment:** Conduct user study comparing parallel-generated videos against sequentially-generated versions (with continuous talking head) to quantify perceptual impact of hard cuts on viewer engagement and comprehension.