---
ver: rpa2
title: 'HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based
  Human Activity Recognition'
arxiv_id: '2512.10807'
source_url: https://arxiv.org/abs/2512.10807
tags:
- activity
- data
- learning
- recognition
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HAROOD, the first comprehensive benchmark
  for out-of-distribution (OOD) generalization in sensor-based human activity recognition
  (HAR). The benchmark defines four realistic OOD scenarios: cross-person, cross-position,
  cross-dataset, and cross-time, and evaluates 16 comparative methods across six public
  datasets using both CNN and Transformer architectures.'
---

# HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition

## Quick Facts
- **arXiv ID:** 2512.10807
- **Source URL:** https://arxiv.org/abs/2512.10807
- **Reference count:** 40
- **Key outcome:** HAROOD introduces the first comprehensive benchmark for OOD generalization in sensor-based HAR, defining four realistic scenarios and evaluating 16 methods across six datasets.

## Executive Summary
This paper introduces HAROOD, the first comprehensive benchmark for out-of-distribution (OOD) generalization in sensor-based human activity recognition (HAR). The benchmark defines four realistic OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and evaluates 16 comparative methods across six public datasets using both CNN and Transformer architectures. Key findings include: no single method consistently outperforms others across all scenarios; CORAL, Fish, and Fishr are recommended when uncertain; model architecture choice significantly impacts OOD performance; and oracle-based model selection outperforms training-domain validation by nearly 2 percentage points. The benchmark is open-source and designed for extensibility, enabling future research in OOD-based HAR.

## Method Summary
The benchmark defines four OOD scenarios: cross-person (variability across individuals), cross-position (variability across sensor placements), cross-dataset (variability across data collection protocols), and cross-time (variability across temporal segments). It evaluates 16 OOD methods including ERM, Mixup, CORAL, DANN, MMD, VREx, LAG, MLDG, RSC, GroupDRO, ANDMask, Fish, Fishr, URM, ERM++, and DDLearn across six public datasets (DSADS, USC-HAD, UCI-HAR, PAMAP2, EMG, WESAD). Experiments use leave-one-domain-out protocols with both CNN (2 conv blocks) and Transformer backbones, training for up to 150 epochs with Adam optimizer. Results are aggregated across 3 random seeds and ranked using a point-based system (2 points for best, 1 for second, 0 for third).

## Key Results
- No single method consistently outperforms others across all four OOD scenarios
- CORAL, Fish, and Fishr show relatively stable performance across scenarios and are recommended when uncertain
- Model architecture choice (CNN vs Transformer) creates substantial performance variation depending on task characteristics and shift type
- Oracle-based model selection outperforms training-domain validation by nearly 2 percentage points

## Why This Works (Mechanism)

### Mechanism 1: Distribution Shift Formalization via Structured OOD Scenarios
The benchmark enables systematic evaluation of distribution shifts by decomposing real-world variability into four distinct scenarios (cross-person, cross-position, cross-dataset, cross-time). Each scenario defines explicit domain splits reflecting specific real-world distribution shifts: individual biomechanics (cross-person), sensor placement physics (cross-position), data collection pipelines (cross-dataset), and temporal non-stationarity (cross-time). These four scenarios capture the dominant sources of distribution shift in sensor-based HAR deployments.

### Mechanism 2: Correlation/Gradient-Based Distribution Alignment
CORAL, Fish, and Fishr achieve relatively stable cross-scenario performance compared to other OOD methods. These methods align feature distributions across source domains—CORAL through second-order correlation matching, Fish through gradient matching across domains, Fishr through gradient variance alignment—promoting domain-invariant representations without target data access. Domain-invariant representations enable better generalization to unseen domains.

### Mechanism 3: Architecture-Task Affinity Effects
Model architecture choice (CNN vs Transformer) creates substantial performance variation depending on task characteristics and shift type. Different architectures encode different inductive biases—CNNs capture local temporal patterns via convolution, Transformers capture global dependencies via self-attention—which align differentially with HAR task structures and distribution shift patterns. The optimal architecture depends on how temporal structure interacts with the specific distribution shift.

## Foundational Learning

- **Concept: Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** The entire benchmark formalizes OOD generalization—learning from multiple source domains to generalize to unseen target domains without any target data during training. Understanding this paradigm is essential to interpret results.
  - **Quick check question:** Why does standard empirical risk minimization (ERM) often fail when test data comes from a different distribution, even with abundant training data?

- **Concept: Domain Shift Taxonomy in Time Series**
  - **Why needed here:** The four scenarios (cross-person, cross-position, cross-dataset, cross-time) represent fundamentally different shift mechanisms. Understanding what changes in each scenario is critical for selecting methods and interpreting failure modes.
  - **Quick check question:** How does the underlying cause of distribution shift differ between cross-person (user biomechanics) and cross-time (temporal drift), and what might this imply for method selection?

- **Concept: Domain-Invariant Representation Learning**
  - **Why needed here:** Most competitive OOD methods (CORAL, MMD, DANN, Fish) work by learning representations invariant to domain identity. Understanding what "invariance" means and how alignment objectives achieve it is necessary to debug method performance.
  - **Quick check question:** If a model learns domain-invariant features, what trade-off might it face regarding task-relevant information that varies across domains?

## Architecture Onboarding

- **Component map:**
  datasets/ -> scenarios/ -> algorithms/ -> models/ -> protocols/ -> analysis/

- **Critical path:**
  1. Configure experiment via YAML (dataset, scenario, algorithm, backbone, hyperparameters)
  2. Load dataset and apply scenario-specific domain splits
  3. Initialize backbone (CNN or Transformer) with algorithm-specific modifications
  4. Run leave-one-domain-out training: train on all but one domain, test on held-out
  5. Apply model selection protocol (validation or oracle)
  6. Aggregate results across 3 random seeds and compute rankings

- **Design tradeoffs:**
  - Modularity vs. method-specific optimization: Unified API ensures fair comparison but may disadvantage methods sensitive to implementation details (e.g., LAG, DDLearn underperformed relative to original papers)
  - Comprehensive vs. tractable search: Limited to 20 hyperparameter combinations—may miss optimal configurations for sensitive methods (noted for MLDG)
  - Oracle as upper bound: Oracle selection shows ~2% improvement but is unrealizable in deployment; reveals gap between current and achievable performance

- **Failure signatures:**
  - Method rankings differ substantially across scenarios (expected behavior, not a bug—reflects scenario-specific effectiveness)
  - HAR-specific methods (LAG, DDLearn) underperform relative to their original papers (due to standardized implementation for fairness—authors invite original authors to contribute implementations)
  - Large CNN vs. Transformer performance gaps on same task (indicates architecture-task mismatch—should explore both)
  - MLDG showing extreme performance variance (hyperparameter sensitivity—needs expanded search)

- **First 3 experiments:**
  1. Establish ERM baseline: Run ERM with CNN backbone on DSADS cross-person scenario (4 domains, leave-one-out). This establishes baseline performance and validates your pipeline.
  2. Validate recommended methods: Compare CORAL, Fish, and Fishr against ERM on the same DSADS cross-person task. This tests the benchmark's primary recommendation and builds intuition for alignment method behavior.
  3. Assess architecture sensitivity: Run your best-performing method from #2 with both CNN and Transformer backbones on two different scenarios (e.g., DSADS cross-person and EMG cross-time). This reveals architecture-scenario interactions before broader experimentation.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can online or meta-adaptive model selection strategies be developed to approximate oracle performance without accessing target domain data?
  - **Basis in paper:** The conclusion states future work includes "developing online or meta-adaptive selection strategies that operate without oracle data."
  - **Why unresolved:** Oracle selection (using test data) is unattainable in deployment, yet training-domain validation underperforms by nearly 2 percentage points, leaving a significant gap in practical hyperparameter tuning.
  - **What evidence would resolve it:** A selection protocol that consistently achieves accuracy within a tight margin of the oracle baseline across all four HAROOD scenarios.

- **Open Question 2:** Why do existing HAR-specific OOD methods (e.g., LAG, DDLearn) fail to replicate their reported dominance when standardized in a unified benchmark codebase?
  - **Basis in paper:** The authors note that "OOD methods designed for activity recognition do not necessarily maintain stability," and their performance falls short of original reports, potentially due to modifications for consistency.
  - **Why unresolved:** The specific implementation details or data processing quirks that contributed to original reported gains are unclear, hindering reproducibility.
  - **What evidence would resolve it:** An ablation study isolating the specific codebase modifications to identify which components degrade performance for HAR-specific algorithms.

- **Open Question 3:** Can integrating dedicated temporal backbones (e.g., RNN, LSTM) improve OOD generalization compared to the current CNN and Transformer architectures?
  - **Basis in paper:** The authors list "integrating temporal backbones" as a key direction for future work to better align with sensor-based tasks.
  - **Why unresolved:** Current experiments rely on general-purpose CNNs/Transformers which may not fully capture the sequential dependencies inherent in sensor time-series data.
  - **What evidence would resolve it:** Benchmark results showing temporal architectures outperforming current baselines, particularly in the cross-time scenario.

## Limitations
- The benchmark focuses exclusively on time-series sensor data, so findings may not transfer to vision-based or multimodal HAR systems
- The standardized implementation approach may not optimize methods for their specific strengths, potentially understating their true capabilities
- The limited hyperparameter search (20 combinations) may miss optimal configurations for sensitive methods like MLDG

## Confidence
- **High confidence:** The benchmark's systematic evaluation framework, open-source implementation, and core finding that no single OOD method dominates across all scenarios
- **Medium confidence:** The specific performance rankings of individual methods, which depend on standardized implementations that may not capture method-specific optimizations
- **Medium confidence:** The architecture-sensitivity finding (CNN vs Transformer performance varies substantially by scenario)

## Next Checks
1. **Dataset preprocessing validation:** Verify that downloaded datasets match the benchmark's preprocessing (sliding window parameters, normalization method) by checking tensor shapes against Table 4 specifications before training

2. **Architecture-scenario interaction:** Test the same OOD method (e.g., CORAL) with both CNN and Transformer backbones across two different scenarios to empirically validate the architecture-task affinity effects reported in the paper

3. **Method sensitivity analysis:** For methods showing high variance (particularly MLDG), expand the hyperparameter search beyond the default 20 combinations to determine if performance improves with more thorough optimization