---
ver: rpa2
title: 'PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic
  Speech Recognition'
arxiv_id: '2509.12647'
source_url: https://arxiv.org/abs/2509.12647
tags:
- speech
- recognition
- context
- inproc
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Pronunciation-Aware Contextualized (PAC)
  framework for LLM-based ASR that addresses two key challenges: effective pronunciation
  modeling and robust homophone discrimination. The core idea is a two-stage learning
  paradigm that first employs Pronunciation-Guided Context Learning (PGCL) to interleave
  grapheme and phoneme annotations with distractor words, and then uses Pronunciation-Discriminative
  Reinforcement Learning (PDRL) with perturbed label sampling to enhance homophone
  discrimination.'
---

# PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2509.12647
- Source URL: https://arxiv.org/abs/2509.12647
- Authors: Li Fu; Yu Xin; Sunlu Zeng; Lu Fan; Youzheng Wu; Xiaodong He
- Reference count: 0
- One-line primary result: Achieves 30.2% and 53.8% relative WER reductions on Librispeech and AISHELL-1 datasets compared to pre-trained LLM-based ASR models

## Executive Summary
This paper introduces PAC, a pronunciation-aware contextualized framework for LLM-based ASR that addresses two key challenges: effective pronunciation modeling and robust homophone discrimination. The approach employs a two-stage learning paradigm: first using Pronunciation-Guided Context Learning (PGCL) to interleave grapheme and phoneme annotations with distractor words, then applying Pronunciation-Discriminative Reinforcement Learning (PDRL) with perturbed label sampling to enhance homophone discrimination. Experiments on Librispeech and AISHELL-1 datasets demonstrate significant improvements over strong baselines, particularly for long-tail words.

## Method Summary
PAC uses a FireRed-LLM 7B backbone with LoRA adapters, training in two stages. Stage 1 (PGCL) optimizes a loss combining grapheme-only, grapheme-phoneme, and grapheme-phoneme-with-distractor contexts, sampled with equal probability. Stage 2 (PDRL) applies reinforcement learning with perturbed labels (replacing keywords with homophones) and contexts, using N-best sampling with B-WER rewards. The model includes an auxiliary CTC module for keyword filtering during decoding. Training uses Adam optimizer with 2e-5 learning rate and 1000-step warmup, requiring 8Ã— NVIDIA G2 140G GPUs.

## Key Results
- 30.2% and 53.8% relative reductions in WER on Librispeech and AISHELL-1 datasets
- 31.8% and 60.5% relative reductions in biased WER for long-tail words compared to strong baselines
- Significant improvements in keyword recognition accuracy while maintaining overall transcription quality
- Effective handling of homophones through pronunciation-aware context modeling

## Why This Works (Mechanism)

### Mechanism 1: Interleaved Grapheme-Phoneme Context with Distractor Forcing
The model pairs graphemes with phoneme annotations and adds grapheme-only homophone distractors, forcing attention to phonemic cues. Given context "speech, PAC", it receives "speech (S P IY1 CH), PAC (P AE1 K)" with distractor "pack". Since "pack" and "PAC" share identical phonemes but differ graphemically, the model must leverage phonemic annotation to resolve ambiguity. The cross-entropy loss aggregates three terms: grapheme-only, grapheme-phoneme, and grapheme-phoneme with distractor, sampled with probabilities 1/3 each.

### Mechanism 2: Perturbed Label Sampling with B-WER Reward Shaping
Reinforcement learning uses adversarially perturbed (keyword, context) pairs to strengthen homophone discrimination. For each training instance, generate perturbed transcript by replacing target keyword with homophone, and construct perturbed context by swapping (keyword, transcription, homophone) with (homophone, transcription, keyword). The biased MWER loss assigns positive rewards to hypotheses with B-WER below average and negative rewards to those above. The PDRL loss combines both original and perturbed pairs.

### Mechanism 3: Two-Stage Progressive Refinement
Supervised PGCL pre-training before PDRL reinforcement learning provides stable initialization, preventing RL divergence while enabling fine-grained homophone discrimination. Stage 1 establishes grapheme-phoneme associations, while Stage 2 applies RL with small CE loss weight (0.01) to prevent divergence. The CTC module filters irrelevant keywords to mitigate hallucination from long contexts.

## Foundational Learning

- **Grapheme-to-Phoneme (G2P) Conversion**: Why needed - PAC requires phoneme annotations for context construction. Quick check - Can you explain why "psalm" might be misrecognized without explicit phonemic modeling?

- **Minimum Word Error Rate (MWER) Training**: Why needed - PDRL uses B-WER as reward signal. Quick check - How does MWER differ from cross-entropy loss in handling hypotheses with similar probabilities but different error profiles?

- **Biased WER (B-WER) Evaluation**: Why needed - Standard WER obscures keyword-level performance. Quick check - Why might overall WER improve while B-WER degrades, and what would this indicate about model behavior?

## Architecture Onboarding

- **Component map**: Audio Encoder -> Adapter -> LLM Backbone (FireRed-LLM, 7B params) with LoRA -> Context input (grapheme-only Cg OR grapheme-phoneme Cgp OR Cgpgd with distractors) -> Auxiliary CTC module for keyword filtering -> Beam search decoder (beam size 4)

- **Critical path**: Speech X -> Encoder -> speech representations -> Adapter maps speech to LLM embedding space -> Context C (with phoneme annotations + distractors) concatenated with instruction prompt -> LLM generates transcription Y -> During training: compute LPGCL (Stage 1) or LPDRL with N-best sampling (Stage 2) -> During inference: CTC filters irrelevant keywords, beam search produces final output

- **Design tradeoffs**: Context list size N: Larger N improves coverage but increases hallucination risk; PAC uses N up to 2000 (English) / 600 (Mandarin); Distractor selection: Hand-crafted rules for English vs. pypinyin for Mandarin; LoRA vs. full fine-tuning: LoRA chosen for efficiency

- **Failure signatures**: Homophone confusion persists: Check if distractor words are phonemically identical to targets; Hallucinated keywords: CTC filtering threshold may be too permissive; Training divergence: Reduce PDRL learning rate or increase CE loss weight

- **First 3 experiments**: 1) Baseline sanity check: Run pre-trained FireRed-LLM with grapheme-only context on small validation set; 2) Ablation by context type: Train separate models with only grapheme-only, only grapheme-phoneme, and full grapheme-phoneme-with-distractor; 3) Homophone stress test: Construct synthetic evaluation set with high homophone density

## Open Questions the Paper Calls Out
- Can the interleaved grapheme-phoneme modeling strategy effectively generalize to code-switching scenarios where pronunciation rules conflict? The paper states plans to extend to multilingual and code-switching ASR tasks, but current validation is limited to monolingual datasets.

## Limitations
- The framework relies on external G2P tools (g2p-en, pypinyin) that may produce errors for irregular pronunciations and proper nouns
- The perturbation strategy assumes synthetic homophone replacements reflect real-world error patterns without validation
- Computational overhead of the two-stage training approach is not quantified relative to baseline methods

## Confidence
- **High Confidence**: WER/B-WER improvements over baselines are well-supported by experimental results
- **Medium Confidence**: Mechanism explanations for PGCL and PDRL effectiveness rely on assumptions not fully empirically validated
- **Low Confidence**: Claims about framework generality to other languages and handling of "any" long-tail words lack sufficient experimental support

## Next Checks
1. Conduct systematic ablation studies isolating each component (grapheme-only, grapheme-phoneme, distractor-enhanced contexts) to quantify individual contribution margins
2. Perform error analysis on G2P-generated pronunciations, particularly focusing on irregular words and domain-specific terminology
3. Test the perturbation strategy on real error patterns from baseline ASR systems rather than synthetic homophone replacements