---
ver: rpa2
title: Reinforced Visual Perception with Tools
arxiv_id: '2509.01656'
source_url: https://arxiv.org/abs/2509.01656
tags:
- visual
- tools
- tool
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReVPT, a reinforcement learning approach
  that trains multimodal language models to use visual tools for enhanced visual reasoning.
  The method employs a two-stage training process combining cold-start supervised
  fine-tuning with group-relative policy optimization (GRPO) to enable models to reason
  about and select appropriate visual tools.
---

# Reinforced Visual Perception with Tools

## Quick Facts
- arXiv ID: 2509.01656
- Source URL: https://arxiv.org/abs/2509.01656
- Reference count: 28
- Primary result: ReVPT-7B achieves +9.82% accuracy on CV-Bench versus instruct-tuned counterpart

## Executive Summary
This paper introduces ReVPT, a reinforcement learning approach that trains multimodal language models to use visual tools for enhanced visual reasoning. The method employs a two-stage training process combining cold-start supervised fine-tuning with group-relative policy optimization (GRPO) to enable models to reason about and select appropriate visual tools. ReVPT achieves state-of-the-art performance on perception-heavy benchmarks, with the 3B and 7B models outperforming their instruct-tuned counterparts by 9.03% and 9.44% on CV-Bench respectively. The approach demonstrates significant improvements in visual perception tasks while maintaining general capabilities, addressing limitations of supervised fine-tuning approaches that struggle with generalization and expensive data generation.

## Method Summary
ReVPT employs a two-stage training pipeline: (1) Cold-start supervised fine-tuning on 1.5k GPT-4.1-synthesized tool-use trajectories for 2 epochs, then (2) Group-Relative Policy Optimization (GRPO) on 20k filtered questions for ~200 steps. The method uses 4 visual tools (object detection, zoom, edge detection, depth estimation) that transform images into task-relevant representations. GRPO samples 8 responses per prompt, computes normalized advantages via group-relative scoring, and updates the policy to favor responses with higher relative rewards. The approach addresses the progressive decline in tool propensity observed in R1-Zero-style training by establishing baseline tool-use patterns before RL exploration.

## Key Results
- ReVPT-7B achieves +9.82% accuracy on CV-Bench versus instruct-tuned counterpart
- ReVPT-3B achieves +9.03% accuracy on CV-Bench versus instruct-tuned counterpart
- Tool ablation shows object detection contributes ~5 points to BLINK relation accuracy
- Cold-start initialization prevents the "progressive decline in tool propensity" observed in R1-Zero-style training

## Why This Works (Mechanism)

### Mechanism 1: GRPO-Driven Tool Selection Optimization
RL with group-relative advantages enables adaptive tool selection that outperforms static supervised trajectories. GRPO samples N responses per query, computes normalized advantages via `A_i = (r_i - mean) / std`, then updates policy to favor responses with higher relative rewards—allowing exploration of diverse tool combinations rather than imitating fixed trajectories. Core assumption: Multiple valid tool-use paths exist for visual problems, and exploration improves over single-trajectory imitation. Evidence: ReVPT-7B achieves +9.82% on CV-Bench vs. instruct model; Qwen-SAT-SFT degrades on BLINK/MMStar.

### Mechanism 2: Cold-Start Initialization Prevents Tool Abandonment
Supervised pre-training on tool-use traces is necessary to establish tool invocation before RL exploration. SFT on GPT-4.1-synthesized trajectories (1.5k samples) creates baseline tool-use policy, preventing the "progressive decline in tool propensity" observed in R1-Zero-style training. Core assumption: Smaller models (3B-7B) cannot discover tool-use from scratch without demonstration. Evidence: "during the training process, we observe a progressive decline in the agent's propensity to utilize tools" when attempting R1-Zero.

### Mechanism 3: Specialized Visual Tools Compensate for MLLM Perceptual Gaps
External vision tools (depth, detection, edge, zoom) provide structured perceptual signals the base model cannot reliably generate. Tools transform images into task-relevant representations (depth maps, bounding boxes, edge maps, crops) that the language model can reason over. Core assumption: Tool outputs are sufficiently accurate and interpretable for downstream reasoning. Evidence: Removing object detection drops BLINK relation by 5 points (8.2% decrease).

## Foundational Learning

- Concept: **Group-Relative Policy Optimization (GRPO)**
  - Why needed here: Core RL algorithm enabling tool-use learning without neural reward models.
  - Quick check question: How does GRPO's group normalization differ from PPO's advantage estimation using a value function?

- Concept: **Tool-Augmented Reasoning Chains**
  - Why needed here: Understanding interleaved thinking-tool-answer sequences.
  - Quick check question: Given a spatial relationship query, what signals determine whether the model calls depth estimation vs. object detection?

- Concept: **Cold-Start vs. Zero-RL Training Paradigms**
  - Why needed here: Critical design choice for tool-use acquisition in smaller models.
  - Quick check question: What specific failure mode does the paper observe when skipping cold-start training?

## Architecture Onboarding

- **Component map:**
  - Policy Model (Qwen2.5-VL-3B/7B-Instruct) -> Tool Controller -> Visual Tools (Depth, Detection, Edge, Zoom) -> Model input
  - Reward Function -> Policy Update (GRPO) -> Frozen Reference Model (KL penalty)

- **Critical path:**
  1. Cold-start SFT: 2 epochs, lr=1e-5, batch=64 on 1.5k GPT-4.1-synthesized trajectories
  2. GRPO training: 200 steps, lr=2e-6, 8 generations per prompt, on 20k filtered questions
  3. Inference: Up to 5 turns of model-tool interaction before final answer

- **Design tradeoffs:**
  - Tool count vs. model capacity: 4 tools selected; initial experiments with more tools showed "extremely low utilization"
  - Cold-start quality vs. generalization: GPT-4.1 synthesis required careful filtering; direct SAT-SFT degraded general benchmarks
  - Binary reward simplicity vs. potential shaping: Paper uses naive rewards citing prior work on rule-based RL effectiveness

- **Failure signatures:**
  - Tool abandonment during RL (addressed by cold-start)
  - Incorrect tool output propagation (Figure 7: misclassification, depth map misinterpretation)
  - Inappropriate tool selection (Figure 7: edge detection for "flat vs. standing" question)
  - Interference from tool outputs (Figure 7: zoom-in provides useless information, degrades answer)

- **First 3 experiments:**
  1. **Baseline comparison**: Train ReVPT-3B with cold-start only vs. cold-start + GRPO; evaluate on CV-Bench depth/distance subsets to isolate RL contribution.
  2. **Tool ablation**: Remove depth estimation from both training data and tool set; measure impact on BLINK-Depth and CV-Bench distance tasks (expect ~5-10 point drop based on Table 4).
  3. **Cold-start data analysis**: Replicate Figure 5 analysis—compare tool usage proportions before/after RL phase across CV-Bench, BLINK, MMVP, and MMStar to validate adaptive tool selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the utility of visual tools scale with model capability—will larger models outgrow tool dependence or discover new synergistic uses?
- Basis in paper: Section 4 asks "Do we really need visual tool specialist?" and notes a "non-monotonic relationship" where tools help small models but "the marginal benefit diminishes" at moderate scales, yet advanced models like GPT-o3 may experience a "tool renaissance."
- Why unresolved: The authors only tested 3B and 7B models; the scaling behavior beyond this range is unknown, and the mechanism for potential tool renaissance in larger models remains speculative.
- What evidence would resolve it: Training ReVPT on larger model families (e.g., 13B, 70B, 200B+) and measuring per-tool utility curves; analyzing whether larger models develop qualitatively different tool-use strategies versus simply outgrowing tools.

### Open Question 2
- Question: Can reinforcement learning overcome cold-start bias in tool selection, or does initial SFT permanently constrain the learned tool repertoire?
- Basis in paper: Section 4 states "The core challenge lies in the cold-start nature of tool learning, which necessitates committing to a specific tool repertoire during initial training" and that this "constrains generalizability."
- Why unresolved: The authors acknowledge this as a fundamental limitation but do not propose or test solutions to enable learning of new tools post-cold-start.
- What evidence would resolve it: Experiments with incremental tool addition after cold-start; comparing models trained with different initial tool sets then exposed to the same expanded toolkit during RL.

### Open Question 3
- Question: How can models learn to identify and reject unreliable tool outputs rather than accepting them uncritically?
- Basis in paper: Section 3.3 and Figure 7 document failure cases where "visual tools often hinder rather than enhance model performance" including object detection misclassifying pillows and models failing to "correct erroneous tool outputs."
- Why unresolved: The current reward design only signals final answer correctness, providing no mechanism for models to learn meta-reasoning about tool reliability during training.
- What evidence would resolve it: Introducing intermediate rewards for tool-output verification; analyzing whether models develop uncertainty calibration about when to trust tools versus their own perception.

## Limitations

- Tool output interpretation challenges: Models fail to correctly interpret structured visual information from external tools, with fundamental limitations in processing depth maps and object detection results
- Cold-start data quality dependency: Reliance on GPT-4.1-synthesized trajectories with limited detail on prompt templates and filtering criteria introduces uncertainty about data diversity and potential biases
- Binary reward function limitations: The +1/-1 reward structure may provide insufficient gradient signal for fine-grained tool selection improvements, potentially limiting learning of nuanced tool usage strategies

## Confidence

**High confidence**: The core claim that ReVPT achieves state-of-the-art performance on perception-heavy benchmarks (CV-Bench +9.82%, BLINK +4.48%) is well-supported by quantitative results in Table 2 and the ablation study in Table 4 showing individual tool contributions.

**Medium confidence**: The assertion that GRPO-driven exploration outperforms static supervised trajectories is supported by relative performance gains, but the paper provides limited analysis of what specific tool selection strategies emerge from the RL phase versus what was encoded in cold-start training.

**Low confidence**: The claim that cold-start initialization is necessary to prevent tool abandonment lacks direct empirical validation—the paper doesn't report what happens when skipping cold-start training entirely, only comparing to "R1-Zero-like" approaches without showing specific failure modes.

## Next Checks

1. **Cold-start ablation experiment**: Train ReVPT-3B without cold-start SFT (direct GRPO from instruct model) and measure tool usage frequency and accuracy on CV-Bench to directly validate the necessity of cold-start training.

2. **Reward shaping analysis**: Implement a simple neural reward model that predicts tool selection quality and compare GRPO performance with binary vs. learned rewards on a subset of tasks to assess whether richer reward signals could improve tool selection.

3. **Tool output encoding study**: Systematically vary how tool outputs are encoded and presented to the model (text vs. image overlays, different formatting) and measure impact on accuracy for tasks where tool interpretation was previously problematic (spatial relationships, depth perception).