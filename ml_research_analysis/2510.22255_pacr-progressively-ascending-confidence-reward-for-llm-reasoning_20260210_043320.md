---
ver: rpa2
title: 'PACR: Progressively Ascending Confidence Reward for LLM Reasoning'
arxiv_id: '2510.22255'
source_url: https://arxiv.org/abs/2510.22255
tags:
- reasoning
- reward
- confidence
- steps
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sparse, outcome-based rewards
  in Reinforcement Learning with Verifiable Rewards (RLVR) for LLM reasoning, which
  provides no guidance for intermediate steps and slows exploration. The authors propose
  Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic reward
  signal that encourages ground-truth confidence growth along reasoning trajectories.
---

# PACR: Progressively Ascending Confidence Reward for LLM Reasoning

## Quick Facts
- **arXiv ID**: 2510.22255
- **Source URL**: https://arxiv.org/abs/2510.22255
- **Reference count**: 21
- **Primary result**: Dense, model-intrinsic reward (PACR) improves LLM reasoning performance by 3.0 percentage points over Dr.GRPO baseline across math benchmarks

## Executive Summary
This paper addresses the challenge of sparse, outcome-based rewards in Reinforcement Learning with Verifiable Rewards (RLVR) for LLM reasoning, which provides no guidance for intermediate steps and slows exploration. The authors propose Progressively Ascending Confidence Reward (PACR), a dense reward signal that encourages ground-truth confidence growth along reasoning trajectories. PACR is based on the inductive bias that a correct reasoning path should progressively increase the model's probability of the ground-truth answer. Experiments on multiple math benchmarks show PACR consistently outperforms the Dr.GRPO baseline, achieving 52.6% average accuracy compared to 49.6% for Dr.GRPO with Qwen2.5-Math-7B.

## Method Summary
PACR introduces two variants of dense rewards based on ground-truth confidence growth: Sparse-PACR provides a single trajectory-level reward based on the proportion of steps with positive confidence gain, while Dense-PACR provides step-wise rewards based on the magnitude of each confidence change. The method tracks the model's confidence in the correct answer at each reasoning step and rewards positive confidence growth. Theoretical analysis shows that an ideal oracle policy will, on average, increase confidence in the ground truth, validating PACR as a strong inductive bias. The combined reward uses λ1=0.9 for GRPO outcome and λ2=0.1 for PACR. Training is performed on MATH dataset with Qwen2.5-Math models, evaluating on five math benchmarks.

## Key Results
- Dense-PACR achieves 52.6% average accuracy across all benchmarks versus 49.6% for Dr.GRPO (3.0 percentage point improvement)
- PACR accelerates exploration and reaches higher final performance faster than sparse reward baselines
- Training dynamics show PACR provides better guidance for intermediate reasoning steps
- Ablation study confirms positive-only rewards are crucial for sustained exploration

## Why This Works (Mechanism)
PACR works by providing dense, model-intrinsic rewards that capture the inductive bias that correct reasoning paths should exhibit monotonically increasing confidence in the ground-truth answer. Unlike sparse outcome-based rewards that only signal success or failure at the end, PACR provides immediate feedback at each reasoning step based on whether the model's confidence in the correct answer has increased. This dense signal guides the policy toward reasoning trajectories that progressively build confidence, effectively shaping the exploration process and accelerating learning.

## Foundational Learning
- **Ground-truth confidence computation**: Computing log-probability of the correct answer given reasoning steps using the policy model. Why needed: Forms the basis for PACR reward signal. Quick check: Verify confidence increases monotonically along correct reasoning paths.
- **Trajectory segmentation**: Splitting generated reasoning into steps at '\n' or '. ', merging fragments <5 tokens. Why needed: Defines the granularity of reward application. Quick check: Ensure segmentation preserves meaningful reasoning units.
- **Reward normalization**: Min-Max normalization across groups at each step (not LOO centering). Why needed: Prevents premature convergence and maintains exploration. Quick check: Verify rewards stay in [0,1] range and preserve relative ordering.
- **Discount factor usage**: Weighting future confidence gains in Dense-PACR. Why needed: Controls temporal credit assignment. Quick check: Test different γ values to find optimal balance.
- **Positive-only reward signal**: Using only positive confidence gains, not penalizing decreases. Why needed: Maintains sustained exploration. Quick check: Compare performance with and without negative advantages.
- **Reward combination**: Mixing GRPO outcome reward (λ1=0.9) with PACR (λ2=0.1). Why needed: Balances final outcome with intermediate guidance. Quick check: Verify combined reward improves over either component alone.

## Architecture Onboarding

**Component Map**: Ground-truth confidence computation -> PACR reward generation -> Dr.GRPO policy update

**Critical Path**: For each training example: generate reasoning trajectory -> segment into steps -> compute ground-truth confidence at each step -> calculate PACR rewards -> combine with GRPO outcome reward -> update policy via PPO/GRPO

**Design Tradeoffs**: Dense rewards provide better guidance but require more computation per step; positive-only rewards maintain exploration but may miss some learning signals from negative steps; Min-Max normalization preserves relative ordering but requires group-level computation.

**Failure Signatures**: 
- Using LOO centering instead of Min-Max normalization causes premature convergence and performance plateau
- Penalizing steps with below-average confidence gains reduces sustained exploration and final accuracy
- Incorrect answer prefix formatting leads to wrong ground-truth confidence computation

**3 First Experiments**:
1. Verify ground-truth confidence computation by checking that confidence increases along known correct reasoning paths
2. Test Sparse-PACR reward computation by comparing trajectory-level rewards with manual confidence gain counting
3. Implement Min-Max normalization and verify it produces [0,1] rewards while preserving relative ordering across a group

## Open Questions the Paper Calls Out
None

## Limitations
- Key hyperparameters (discount factor γ) are not specified, affecting reproducibility
- Ground-truth confidence computation details (answer prefix format and tokenization) are under-specified
- Evaluation is limited to math benchmarks without testing generalization to other domains
- No comparison to more recent RLVR methods that may outperform Dr.GRPO

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical insight about confidence growth in correct reasoning paths | High |
| PACR improves performance over Dr.GRPO on math benchmarks | Medium |
| PACR generalizes beyond math to other verifiable reward domains | Low |

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically test different discount factors γ (e.g., 0.9, 0.95, 0.99) for Dense-PACOR to determine their impact on training stability and final performance.

2. **Answer Prefix Standardization**: Implement and test multiple answer prefix formats (e.g., different LaTeX delimiters, plain text) to determine how sensitive the ground-truth confidence computation is to this formatting choice.

3. **Cross-Domain Validation**: Apply PACR to a non-math domain with verifiable rewards, such as code synthesis (verifiable by compilation) or fact verification, to test whether the confidence growth inductive bias generalizes beyond mathematical reasoning.