---
ver: rpa2
title: Adapting a World Model for Trajectory Following in a 3D Game
arxiv_id: '2504.12299'
source_url: https://arxiv.org/abs/2504.12299
tags:
- future
- trajectory
- agent
- learning
- radius
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using Inverse Dynamics Models (IDM) with
  various encoders and policy heads for trajectory following in the 3D video game
  Bleeding Edge. Six model configurations are evaluated across General, Specific,
  and Fine-tuning settings.
---

# Adapting a World Model for Trajectory Following in a 3D Game

## Quick Facts
- arXiv ID: 2504.12299
- Source URL: https://arxiv.org/abs/2504.12299
- Reference count: 38
- Primary result: ConvNeXt-GPT architecture achieves mean AUC 0.86 in General setting for trajectory following

## Executive Summary
This paper investigates using Inverse Dynamics Models (IDM) with various encoders and policy heads for trajectory following in the 3D video game Bleeding Edge. Six model configurations are evaluated across General, Specific, and Fine-tuning settings. The study finds that ConvNeXt-GPT performs best in the General setting (mean AUC 0.86), DINOv2 variants excel in the Specific setting (mean AUC 0.96), and ConvNeXt models achieve high performance in Fine-tuning (mean AUC 0.96). Visual inputs are essential for accurate trajectory following, while action inputs provide marginal gains. The paper also explores future alignment strategies to address distribution shifts, with the Radius strategy yielding optimal results. Despite advancements, trajectory-following remains challenging, particularly for complex behaviors.

## Method Summary
The paper evaluates six IDM configurations using different encoder-policy head combinations (ConvNeXt-GPT, DINOv2-GPT, DINOv2-LSTM, DINOv2-Dense, ResNet-GPT, ResNet-LSTM) across three settings: General (pretrained models on random game data), Specific (pretrained models on target trajectory data), and Fine-tuning (models trained on specific trajectories). Models are tested on three trajectory types in Bleeding Edge: Circle, Random, and Figure-8. The Area Under the Curve (AUC) metric measures trajectory-following accuracy by comparing agent and expert positions. The study also examines distribution shift strategies (Radius, Rectangle, Cylinder, Area) for handling input distribution changes between training and evaluation.

## Key Results
- ConvNeXt-GPT achieves highest mean AUC (0.86) in General setting across all trajectory types
- DINOv2 variants excel in Specific setting with mean AUC of 0.96
- ConvNeXt models achieve highest mean AUC (0.96) in Fine-tuning setting
- Visual inputs are essential for trajectory following; action inputs provide minimal performance gains
- Radius strategy is optimal for handling distribution shifts in alignment strategies

## Why This Works (Mechanism)
The Inverse Dynamics Model approach works by learning the relationship between current state (visual observations) and action sequences needed to follow trajectories. Visual inputs provide rich spatial information about the game environment and agent position, while action inputs offer minimal additional benefit in this controlled setting. The pretrained models leverage learned representations from game data to efficiently adapt to trajectory-following tasks. The ConvNeXt-GPT architecture's transformer-based approach enables better handling of long-range dependencies in trajectory sequences compared to LSTM or Dense alternatives.

## Foundational Learning
- Inverse Dynamics Models: Predict actions from current state and next state; needed for learning control policies from observations without explicit reward signals; quick check: verify model correctly predicts actions from state transitions
- Trajectory Following Metrics: AUC measures area between expert and agent paths; needed for quantitative comparison of following accuracy; quick check: confirm perfect following yields AUC close to 1
- Distribution Shift: Changes in input data distribution between training and deployment; needed to ensure models generalize beyond training conditions; quick check: test model on held-out trajectories with different starting positions
- Encoder-Policy Head Architecture: Separates feature extraction from decision-making; needed for modularity and transfer learning; quick check: verify encoder outputs consistent embeddings across similar states
- Pretraining Strategies: Using random game data to initialize models; needed to leverage large-scale learning before fine-tuning; quick check: compare convergence speed with and without pretraining

## Architecture Onboarding

**Component Map:** Visual Inputs -> Encoder -> Policy Head -> Action Outputs; Optional: Action Inputs -> Encoder

**Critical Path:** The encoder processes visual observations into latent representations, which the policy head uses to predict action sequences for trajectory following. Visual inputs are the primary driver of performance.

**Design Tradeoffs:** The choice between transformer-based (GPT) and recurrent (LSTM) policy heads involves balancing long-range dependency handling against computational efficiency. Dense policy heads offer simplicity but may struggle with complex temporal patterns.

**Failure Signatures:** Poor performance on sharp turns or rapid acceleration changes indicates the model struggles with high-frequency trajectory features. Failure to generalize across trajectory types suggests overfitting to specific patterns.

**3 First Experiments:**
1. Compare AUC scores across the six model configurations on Circle trajectories in the General setting
2. Evaluate the impact of including action inputs alongside visual inputs on trajectory following accuracy
3. Test each distribution shift strategy (Radius, Rectangle, Cylinder, Area) on Fine-tuning models with Figure-8 trajectories

## Open Questions the Paper Calls Out
The paper identifies several open questions: How can trajectory-following models better handle complex behaviors like sharp turns and rapid acceleration changes? What are the most effective strategies for aligning models when facing distribution shifts between training and deployment environments? Can the performance gains observed in the Specific setting be achieved without extensive pretraining on target trajectory data? How do different encoder-policy head combinations perform on trajectory-following tasks in games with fundamentally different mechanics than Bleeding Edge?

## Limitations
- Results are based on a single 3D game (Bleeding Edge), limiting generalizability to other games or real-world scenarios
- High performance in Specific setting (mean AUC 0.96) may indicate overfitting to particular game mechanics
- The claim that visual inputs are "essential" is based on marginal gains from action inputs in a controlled environment
- Distribution shift strategy evaluation was limited to the constrained Bleeding Edge environment

## Confidence
- General model performance claims: High
- Distribution shift alignment strategy effectiveness: Medium
- Claims about essential visual inputs: Medium
- Generalization across games/scenarios: Low

## Next Checks
1. Test the ConvNeXt-GPT and DINOv2 architectures on at least two additional 3D games with different mechanics to assess generalization.
2. Conduct ablation studies systematically varying trajectory complexity (sharp turns, acceleration changes, obstacle navigation) to identify specific failure modes.
3. Implement and evaluate the Radius distribution shift strategy on a non-game domain such as autonomous vehicle path following or robotic manipulation tasks.