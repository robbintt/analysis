---
ver: rpa2
title: Enhancing ML Models Interpretability for Credit Scoring
arxiv_id: '2509.11389'
source_url: https://arxiv.org/abs/2509.11389
tags:
- features
- feature
- credit
- regression
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a hybrid approach to enhance ML model interpretability
  for credit scoring by using post-hoc XAI methods (SHAP) to guide feature selection,
  then training transparent glass-box models (EBM and PLTR). Using the Lending Club
  dataset with 87 features, the method reduces features to 10 (88.5% reduction) while
  maintaining predictive performance comparable to a black-box XGBoost benchmark.
---

# Enhancing ML Models Interpretability for Credit Scoring

## Quick Facts
- **arXiv ID:** 2509.11389
- **Source URL:** https://arxiv.org/abs/2509.11389
- **Reference count:** 10
- **Primary result:** Hybrid SHAP-guided feature selection with EBM achieves XGBoost-level performance while reducing 87 features to 10 (88.5% reduction)

## Executive Summary
This paper proposes a hybrid approach to enhance ML model interpretability for credit scoring by combining post-hoc XAI methods with transparent glass-box models. The method uses SHAP values from a black-box XGBoost model to identify the most important features, then trains interpretable models like Explainable Boosting Machine (EBM) on this reduced feature set. Using the Lending Club dataset, the approach reduces features from 87 to 10 while maintaining predictive performance comparable to the black-box benchmark. The work addresses regulatory demands for transparency in credit risk modeling and demonstrates that interpretable models can achieve parity with complex models when the feature space is optimized.

## Method Summary
The methodology follows a three-phase approach: first, a black-box XGBoost model is trained on the full feature set (87 features) with class weights to handle imbalance. Second, TreeSHAP is used to calculate global feature importance rankings. Third, the top 10 features are selected and used to train an Explainable Boosting Machine (EBM), a type of generalized additive model. The process includes refinement steps where feature interactions and correlations are analyzed to improve model robustness and avoid counter-intuitive explanations. The framework maintains performance parity with the black-box benchmark while providing significantly greater transparency.

## Key Results
- EBM consistently achieved performance on par with XGBoost (AUPRC, AUROC, F1)
- Feature reduction from 87 to 10 features (88.5% reduction) with no substantial performance loss
- Correlation analysis and interaction checks improved model robustness
- Glass-box models provide transparent decision-making suitable for regulatory compliance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Post-hoc feature importance rankings from high-performance black-box models can effectively guide feature selection for interpretable "glass-box" models without significant loss of predictive accuracy.
- **Mechanism:** A complex black-box model (e.g., XGBoost) captures non-linear relationships across a wide feature set. By calculating SHAP values, researchers identify the subset of features that contribute most to the model's decision boundary. Training a constrained glass-box model (e.g., EBM) on only this subset allows the simpler model to approximate the black-box's performance by focusing its limited capacity on the most informative signals.
- **Core assumption:** The predictive power of the black-box model is concentrated in a relatively small subset of features (sparsity), and the glass-box model is sufficiently expressive to model the non-linearities of these specific features.
- **Evidence anchors:**
  - [abstract] "...post-hoc interpretations of black-box models guide feature selection, followed by training glass-box models..."
  - [section 4.5] "...we observe that adding more than 10 features yields no substantial improvement in model performance... reduce the dimensionality from 86 features to just 10."
  - [corpus] Related work supports the utility of Shapley values for feature attribution in XAI, though specific implementations for feature selection pipelines vary.
- **Break condition:** If the underlying data generation process relies heavily on complex, high-order feature interactions distributed across hundreds of variables, reducing the feature set based on main effect importance will likely cause a significant performance drop.

### Mechanism 2
- **Claim:** Generalized Additive Models (GAMs), specifically Explainable Boosting Machine (EBM), can achieve performance parity with ensemble tree methods in credit scoring when the feature space is optimized.
- **Mechanism:** EBM operates by training small decision trees on each feature individually in a round-robin fashion (cyclic gradient boosting). This builds a sum of non-linear functions (one per feature). While typically less expressive than a full Random Forest or XGBoost (which model feature interactions natively), EBM achieves parity in this context because the preceding feature selection step removes noise and dimensions where EBM would struggle to compete.
- **Core assumption:** The selected features exhibit strong main effects that do not strictly require complex, opaque pairwise interactions to be predictive.
- **Evidence anchors:**
  - [abstract] "EBM consistently achieved performance that is at least on par with XGBoost."
  - [section 4.5] "Notably, EBM consistently achieves performance that is at least on par with XGBoost, while offering significantly greater transparency."
  - [corpus] Corpus suggests growing interest in GAMs for interpretability, though general parity with boosting is not guaranteed across all datasets.
- **Break condition:** If the top-ranked features exhibit strong, non-additive interaction effects (e.g., high income is only good if debt is low, but bad if debt is high), a purely additive EBM (without interaction terms) will underperform the black-box benchmark.

### Mechanism 3
- **Claim:** Restricting feature interactions and analyzing correlations mitigates counter-intuitive explanations (model instability) often found in complex models.
- **Mechanism:** Unconstrained models can produce "correct" predictions for the wrong reasons (e.g., inferring that low loan amounts correlate with higher default due to confounding). By analyzing feature interactions (Section 4.6.1) and correlations (Section 4.6.2), the authors prune features that introduce confusing signals or spurious correlations. This forces the model to rely on robust main effects, aligning outputs with financial logic.
- **Core assumption:** Domain experts can validate model behavior by inspecting marginal contribution plots, and deviations from domain logic indicate model instability rather than a novel insight.
- **Evidence anchors:**
  - [section 4.3] "...even when using inherently interpretable models like EBM, introducing feature interactions can result in patterns that defy financial intuition."
  - [section 4.6.1] "...practitioners may confidently exclude all feature interactions and still obtain a glass-box model that outperforms the logistic regression benchmark."
  - [corpus] "Fair and Explainable Credit-Scoring under Concept Drift" highlights that explanations must be robust to evolving populations; interaction-heavy models often fail this robustness check.
- **Break condition:** If the true physical laws of the system are deeply interactive and counter-intuitive, strictly enforcing intuitive main effects will result in under-fitting and loss of predictive power.

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations) Values**
  - **Why needed here:** This is the engine of the proposed architecture. It transforms the black-box model into a ranking system. Without understanding that SHAP assigns contribution values based on the marginal contribution of a feature across all coalitions, one cannot justify why specific features are selected for the glass-box phase.
  - **Quick check question:** If Feature A and Feature B are perfectly correlated (multicollinearity), how might SHAP attribute importance between them compared to a feature importance metric based purely on "gain" or "split count"?

- **Concept: Generalized Additive Models (GAMs)**
  - **Why needed here:** The paper advocates for EBM, a form of GAM. Understanding that a GAM models the target as a *sum* of individual feature functions (f1(x1) + f2(x2) + ...) is critical to understanding why these models are considered "glass-box" (interpretable) compared to the nested interactions of XGBoost.
  - **Quick check question:** Can a standard GAM capture the relationship "High income reduces default risk *only if* debt-to-income ratio is low"? Why or why not?

- **Concept: The Accuracy-Interpretability Trade-off**
  - **Why needed here:** The central premise of the paper is challenging the assumption that this trade-off is linear. The paper posits that by optimizing the *input space* (feature selection), we can use simpler models without paying the accuracy tax.
  - **Quick check question:** In the context of this paper, does the reduction in features come from discarding "noise" or from discarding "weak signals"?

## Architecture Onboarding

- **Component map:** Ingest: Lending Club Data (87 raw features) -> Black-Box Proxy: Train XGBoost on full feature set -> Translator: Tree-SHAP algorithm extracts global feature importance -> Filter: Select Top N features (e.g., 10) based on SHAP rankings; apply Correlation Filter -> Glass-Box Core: Train EBM (GAM with boosting) on reduced set -> Validation: Human-in-the-loop checks on interaction plots and marginal effects

- **Critical path:** The "Translation" phase (Step 3) is the leverage point. If the XGBoost model overfits to noise, the SHAP values will select garbage features, and the downstream Glass-Box model will fail.

- **Design tradeoffs:**
  - **Feature Count vs. Robustness:** The paper settles on 10 features. Dropping to 5 loses too much signal; keeping 20 introduces complexity and interaction noise.
  - **Interaction vs. Intuition:** Including pairwise interactions in EBM offers a theoretical performance boost (~0.4% F1) but introduces significant risk of producing "counter-intuitive" results that violate regulatory expectations.

- **Failure signatures:**
  - **Performance Collapse:** Glass-box model AUC drops >2% below Black-box. (Likely cause: Critical features were correlated with top features and dropped during correlation analysis).
  - **Regulatory Reject:** Model attributes positive default probability to "high income" interactions. (Likely cause: Interaction terms capturing spurious correlations; disable interactions).

- **First 3 experiments:**
  1. **Sensitivity Scan:** Replicate the "Top-N" analysis. Plot AUPRC/AUROC as features are added 1-by-1 to verify the "elbow" at 10 features.
  2. **Correlation Stress Test:** Inject a highly correlated copy of the top feature into the dataset. Verify if SHAP splits importance equally or if one dominates, and check if the correlation filter removes the duplicate correctly.
  3. **Interaction Ablation:** Train the EBM with 0 interactions vs. top-9 interactions. Compare the F1 score gain against the complexity of explaining the interaction plots to a domain expert.

## Open Questions the Paper Calls Out

- **Question:** Does the hybrid approach maintain predictive parity with black-box models across diverse financial datasets and economic cycles?
  - **Basis in paper:** [explicit] The Conclusion explicitly recommends "testing the effectiveness and robustness of this approach across diverse datasets to validate its generalizability and applicability in varied financial contexts."
  - **Why unresolved:** The study relies exclusively on the Lending Club dataset (consumer lending); results may not generalize to corporate credit scoring or different regulatory jurisdictions.
  - **What evidence would resolve it:** Replication of the methodology on distinct datasets (e.g., corporate default data, European credit datasets) showing similar feature reduction rates without performance degradation.

- **Question:** How does SHAP-guided feature selection compare to traditional selection methods when training glass-box models?
  - **Basis in paper:** [inferred] The paper selects SHAP for feature ranking due to its popularity and visualization capabilities, but does not benchmark it against alternative selection algorithms (e.g., Lasso, mutual information).
  - **Why unresolved:** It is unclear if the computational overhead of SHAP is necessary for selecting the top 10 features, or if simpler statistical metrics would suffice for the glass-box training phase.
  - **What evidence would resolve it:** An ablation study comparing the performance of EBM/PLTR models trained on features selected via SHAP versus those selected via standard filter or wrapper methods.

- **Question:** Does the exclusion of feature interactions to ensure intuitive explanations consistently result in negligible performance loss?
  - **Basis in paper:** [inferred] The paper notes that including interactions caused "counterintuitive" patterns (Fig 1) but yielded only a marginal 0.4% F1 score improvement, suggesting a trade-off that may vary by dataset.
  - **Why unresolved:** The finding that interactions are dispensable may be specific to the Lending Club dataset; other credit domains might rely more heavily on non-linear interactions for accuracy.
  - **What evidence would resolve it:** Testing the performance gap between additive-only and interaction-inclusive glass-box models on datasets with known complex non-linear dependencies.

## Limitations
- The methodology relies on a single dataset (Lending Club) without validation across different credit domains or economic cycles
- Specific hyperparameters for XGBoost and EBM models are not provided, making exact reproduction challenging
- The analysis of feature interactions and correlations lacks quantitative thresholds for feature exclusion
- No comparison with alternative feature selection methods to validate SHAP's superiority for this task

## Confidence
- **High Confidence:** The core mechanism of using SHAP for feature selection followed by EBM training is well-established in the XAI literature. The performance claim of EBM achieving parity with XGBoost is supported by the presented results.
- **Medium Confidence:** The assertion that reducing to 10 features maintains performance requires validation on different datasets. The interaction analysis conclusions are reasonable but not rigorously tested across multiple scenarios.
- **Low Confidence:** The claim about avoiding "counter-intuitive" explanations through interaction restrictions needs more systematic evaluation, as what constitutes "counter-intuitive" can vary by domain expert.

## Next Checks
1. **Feature Stability Test:** Perform k-fold cross-validation on the feature selection process to assess whether the same top-10 features consistently emerge across different data splits.
2. **Time-Delay Analysis:** Apply the methodology to sequential time periods (e.g., quarterly data) to evaluate model performance degradation and feature stability over time.
3. **Interaction Sensitivity Study:** Systematically vary the number of allowed interactions in EBM (0, 1, 2, 5, 9) across multiple runs to quantify the actual performance-cost trade-off of interpretability.