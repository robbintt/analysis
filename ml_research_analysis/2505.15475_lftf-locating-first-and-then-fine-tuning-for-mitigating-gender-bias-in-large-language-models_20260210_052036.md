---
ver: rpa2
title: 'LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large
  Language Models'
arxiv_id: '2505.15475'
source_url: https://arxiv.org/abs/2505.15475
tags:
- gender
- bias
- llms
- layer
- lftf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in large language models (LLMs),
  particularly regarding profession-related associations. The authors propose two
  datasets, GenBiasEval and GenHintEval, along with corresponding evaluation metrics
  (AFGB-Score and UB-Score) to quantify gender bias.
---

# LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models

## Quick Facts
- **arXiv ID**: 2505.15475
- **Source URL**: https://arxiv.org/abs/2505.15475
- **Reference count**: 17
- **Primary result**: Reduces gender bias in LLMs by 70% through targeted fine-tuning of specific transformer blocks

## Executive Summary
This paper introduces LFTF (Locating First and Then Fine-Tuning), a novel algorithm that mitigates gender bias in large language models by identifying and fine-tuning only the most bias-relevant transformer blocks. The approach uses BMI (Block Mitigating Importance) scores to locate blocks that disproportionately transform hidden states during biased predictions, then applies a balanced probability loss function to those blocks. Experiments on Qwen2.5-7B and Meta-Llama3-8B demonstrate significant reductions in gender bias while preserving general capabilities, outperforming full-parameter fine-tuning and prompt-based methods.

## Method Summary
LFTF operates in two stages: (1) **Locating Stage** - computes BMI scores for each block using cosine similarity between consecutive hidden states, identifying the block with highest BMI (typically the final layer); (2) **Fine-Tuning Stage** - fine-tunes only the identified block using a loss function that sums the probabilities of gendered pronouns: L = P("he") + P("she"). The method uses GenBiasEval and GenHintEval datasets with 2,358 and 786 samples respectively, and employs learning rate 1e-5, 2 epochs, batch size 32, and Adam optimizer.

## Key Results
- Reduces AFGB-Score from ~0.26 to ~0.08 on Qwen2.5-7B while maintaining UB-Score of 0.67
- Outperforms full-parameter fine-tuning (FPFT) which achieves 0.01 AFGB but only 0.006 UB-Score
- Preserves general capabilities with minimal degradation (<1%) on most benchmarks
- Maintains gender-hint following capability while reducing unwarranted associations

## Why This Works (Mechanism)

### Mechanism 1: Block-Level Importance Localization via BMI
The BMI score identifies gender bias concentration in specific transformer blocks by measuring hidden state transformation. Higher BMI indicates greater involvement in bias computations. Evidence shows final layer consistently has highest BMI across models, and BMI robustness is demonstrated across random samples.

### Mechanism 2: Balanced Probability Loss for Targeted Debiasing
The dual-component loss function `L = P("he") + P("she")` reduces bias by equalizing gender-token probabilities. Both terms are necessary - removing either causes anti-bias failures. The loss converges toward balanced probabilities (~0.5 each) rather than complete suppression.

### Mechanism 3: Dual-Metric Evaluation for Bias-Hint Trade-off
AFGB-Score measures absolute fairness (equal P(he)≈P(she) for neutral prompts) while UB-Score measures hint-following consistency. Good debiasing minimizes AFGB while maintaining high UB, ensuring models remain gender-aware when explicitly prompted.

## Foundational Learning

- **Transformer Block Architecture (Attention + MLP)**: Needed because LFTF operates at block granularity, fine-tuning both attention and MLP sub-components. Quick check: What happens if only MLP is updated?
- **Probability Distributions from Logits via Softmax**: Essential for computing P("he") and P("she") for evaluation metrics and loss function. Quick check: Why must logits be converted via softmax before computing the loss?
- **Fine-Tuning vs. Full-Parameter Training Trade-offs**: Critical for understanding why selective fine-tuning preserves general capabilities. Quick check: Table 4 shows FPFT reduces AFGB but catastrophically fails on UB-Score. What does this imply?

## Architecture Onboarding

- **Component map**: GenBiasEval/GenHintEval datasets → [BMI Calculation] → [Block Ranking] → [Select Top Block] → [Fine-Tune with L = P(he) + P(she)] → Evaluate: AFGB-Score↓ + UB-Score↑ + MMLU≈
- **Critical path**: Verify BMI calculation correctness, confirm selected block has significantly higher BMI than neighbors, monitor loss convergence to ~1.0
- **Design tradeoffs**: Single-block vs. multi-block fine-tuning; loss function simplicity vs. sophistication; general capability preservation vs. bias reduction
- **Failure signatures**: Anti-bias (UB-Score→0, AFGB→1.0), capability collapse (>2% MMLU drop), no effect (unchanged AFGB)
- **First 3 experiments**: 1) BMI robustness check across random samples, 2) Single-component loss ablation to confirm anti-bias failure, 3) Block position validation (top vs. random vs. first layer)

## Open Questions the Paper Calls Out
The paper explicitly acknowledges limitations regarding binary gender assumptions in its evaluation framework and does not call out additional open questions beyond this constraint.

## Limitations
- Binary gender assumption in evaluation framework (male/female only)
- Potential degradation of mathematical reasoning capabilities (-12.8% on GSM8K)
- Limited testing across different model architectures and bias types
- Correlation vs. causation uncertainty in BMI localization mechanism

## Confidence
- **High Confidence**: Dual-metric evaluation framework (AFGB-Score + UB-Score) provides meaningful signal about debiasing effectiveness
- **Medium Confidence**: BMI localization hypothesis is supported by consistent results but remains correlational rather than causal
- **Low Confidence**: Claims about general capability preservation are questionable given significant degradation on mathematical reasoning benchmarks

## Next Checks
1. Test BMI localization across 3-4 additional LLM architectures to verify consistent block identification
2. Conduct mechanistic analysis comparing final block activation patterns during mathematical reasoning vs. gender prediction
3. Develop modified loss function that maintains UB-Score while reducing AFGB-Score to test if both can be optimized simultaneously