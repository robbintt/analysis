---
ver: rpa2
title: Analysis of instruction-based LLMs' capabilities to score and judge text-input
  problems in an academic setting
arxiv_id: '2509.20982'
source_url: https://arxiv.org/abs/2509.20982
tags:
- evaluation
- answer
- reference
- student
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the use of large language models (LLMs) to\
  \ automatically assess student answers to text-input problems in higher education.\
  \ Five evaluation systems were tested using three models\u2014JudgeLM, Llama-3.1-8B,\
  \ and DeepSeek-R1-Distill-Llama-8B\u2014on a dataset of 110 computer science answers."
---

# Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting

## Quick Facts
- arXiv ID: 2509.20982
- Source URL: https://arxiv.org/abs/2509.20982
- Reference count: 35
- Five evaluation systems tested on a dataset of 110 computer science answers

## Executive Summary
This study investigates whether large language models can reliably score and judge student-written answers to text-input problems in higher education. Using a dataset of 110 computer science responses, the researchers tested five different LLM-based evaluation methods with three models: JudgeLM, Llama-3.1-8B, and DeepSeek-R1-Distill-Llama-8B. The methods included Reference Aided Evaluation (using a reference answer), No Reference Evaluation, Additive Evaluation (using predefined criteria), and Adaptive Evaluation (with dynamically generated criteria). Results show that Reference Aided Evaluation most closely matched human grading, with the lowest median absolute deviation and root mean square deviation. The study suggests that, with appropriate methodology, LLMs can serve as effective complementary tools for academic assessment.

## Method Summary
The researchers assembled a dataset of 110 computer science answers and designed five evaluation systems to assess them using three different LLMs. The systems included JudgeLM-based scoring, Reference Aided Evaluation (comparing against a reference answer), No Reference Evaluation (using a single rubric), Additive Evaluation (applying atomic criteria), and Adaptive Evaluation (generating criteria dynamically). Each answer was scored by each system, and results were compared against human-graded scores. Performance was measured using median absolute deviation and root mean square deviation to determine alignment with human grading.

## Key Results
- Reference Aided Evaluation achieved the best alignment with human grading (median absolute deviation: 0.945; root mean square deviation: 1.214)
- This method provided both fair scoring and insightful feedback
- Other methods underperformed due to rigid criteria, over-reliance on references, or insufficient contextual understanding

## Why This Works (Mechanism)
The effectiveness of LLMs in academic assessment depends on the evaluation methodology employed. Reference Aided Evaluation leverages a reference answer to provide contextual understanding, enabling more nuanced scoring that aligns with human judgment. The mechanism works because LLMs can parse semantic content and map it against established standards, especially when guided by explicit reference material. The scoring process benefits from the model's ability to understand language nuances, compare responses to benchmarks, and provide explanatory feedback.

## Foundational Learning
- **Large Language Models (LLMs)**: Neural networks trained on vast text corpora to generate and understand human language. Why needed: Form the core technology for automated text assessment. Quick check: Can the model generate coherent, contextually appropriate responses.
- **Evaluation Systems**: Structured methodologies for applying LLMs to assess responses. Why needed: Determine how effectively LLMs can score academic answers. Quick check: Does the system produce consistent scores across similar responses.
- **Reference-based Scoring**: Using a model answer as a benchmark for evaluation. Why needed: Provides a concrete standard for comparison. Quick check: How closely do scores align with human graders using the same reference.
- **Statistical Deviation Measures**: Metrics like median absolute deviation and root mean square deviation to quantify scoring accuracy. Why needed: Enable objective comparison between LLM scoring and human grading. Quick check: Are deviations within acceptable educational thresholds.
- **Rubric-based Assessment**: Structured criteria for evaluating responses. Why needed: Provides consistent evaluation framework. Quick check: Can the system apply rubric criteria reliably across diverse responses.

## Architecture Onboarding

### Component Map
Dataset -> Evaluation System -> LLM -> Scoring Output -> Statistical Analysis

### Critical Path
The critical path for effective LLM-based assessment runs from the quality of the reference answer through the evaluation system design to the LLM's scoring output. Reference Aided Evaluation represents the most efficient path, minimizing processing steps while maximizing alignment with human judgment.

### Design Tradeoffs
Reference Aided Evaluation trades some flexibility for accuracy, as it depends on the quality of the reference answer. Alternative methods like No Reference or Adaptive Evaluation offer more adaptability but sacrifice scoring precision. The choice between methods involves balancing computational efficiency, scoring accuracy, and the ability to provide meaningful feedback.

### Failure Signatures
Evaluation systems fail when they apply rigid criteria that don't account for valid alternative answers, over-rely on potentially flawed reference answers, or lack sufficient contextual understanding. Systems also fail when statistical measures show high deviation from human grading or when they produce inconsistent scores across similar responses.

### First Experiments
1. Test Reference Aided Evaluation on a cross-disciplinary dataset to assess generalizability
2. Compare multiple rubrics for the same problem to evaluate robustness
3. Measure inter-rater reliability between human graders to establish baseline variability

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to computer science discipline and 110 answers, constraining generalizability
- Only three open-weight models tested, leaving uncertainty about performance with other architectures
- Human grading used as ground truth is subjective and may introduce bias

## Confidence

| Claim | Confidence |
|-------|------------|
| Reference Aided Evaluation achieves superior alignment with human grading | Medium |
| LLMs can serve as effective complementary assessment tools | Medium |
| Performance comparisons between evaluation systems are valid | High |

## Next Checks
1. Replicate the study across multiple disciplines (e.g., humanities, natural sciences) and problem types (e.g., essay, calculation, diagram-based)
2. Expand the dataset size by an order of magnitude and include multiple rubrics per problem to test generalizability
3. Conduct a multi-rater human grading study to quantify inter-rater reliability and assess whether LLM scoring variance is comparable to or better than human assessors