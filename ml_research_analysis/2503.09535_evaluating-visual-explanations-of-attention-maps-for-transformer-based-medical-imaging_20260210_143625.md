---
ver: rpa2
title: Evaluating Visual Explanations of Attention Maps for Transformer-based Medical
  Imaging
arxiv_id: '2503.09535'
source_url: https://arxiv.org/abs/2503.09535
tags:
- attention
- maps
- medical
- interpretability
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of attention maps as visual
  explanations for transformer-based models in medical imaging. The authors compare
  attention maps to GradCAM and a transformer-specific interpretability method (Chefer)
  across four medical imaging datasets using ViT-B/16 models with different pretraining
  strategies (random, supervised, DINO, MAE).
---

# Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging

## Quick Facts
- arXiv ID: 2503.09535
- Source URL: https://arxiv.org/abs/2503.09535
- Reference count: 36
- Primary result: Attention maps outperform GradCAM but underperform Chefer method for transformer interpretability in medical imaging

## Executive Summary
This study systematically evaluates attention maps as visual explanations for transformer-based models in medical imaging, comparing them against GradCAM and a transformer-specific interpretability method (Chefer) across four medical datasets. Using ViT-B/16 models with different pretraining strategies, the authors find that attention maps provide better localization than GradCAM but are outperformed by Chefer's gradient-relevance propagation approach. The research also reveals that self-supervised pretraining generally improves attention map interpretability, though the optimal strategy varies by dataset, and highlights significant limitations of bounding box annotations for evaluation.

## Method Summary
The study uses ViT-B/16 models (224×224 input, 16×16 patches) with four initialization strategies: random, supervised ImageNet, DINO, and MAE. Models are fine-tuned for binary classification on four medical imaging datasets (CP-Child, DUKE, Kvasir Z-line, MURA) using grid-searched hyperparameters. Interpretability methods extract: CLS token attention from the final layer, GradCAM on the final attention layer, and Chefer's gradient-relevance maps. Evaluation uses pointing game accuracy and IoU with top 5% threshold against expert bounding box annotations (50 disease-positive images per dataset).

## Key Results
- Attention maps generally outperform GradCAM but are outperformed by the Chefer method
- Self-supervised pretraining improves attention map interpretability in most cases, though DINO does not always perform better than supervised or MAE pretraining
- Bounding box annotations have significant limitations for evaluating interpretability in medical datasets

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pretraining improves attention map interpretability through SSL objectives (DINO's discriminative learning, MAE's reconstruction) shaping attention weight distribution across image patches. Models learn to attend to semantically meaningful regions during pretraining, transferring to downstream medical tasks. Core assumption: Features learned on natural images transfer to medical imaging despite domain shift. Evidence: SSL improves interpretability in most cases, though DINO doesn't always outperform supervised pretraining. Break condition: Substantial domain differences between natural and medical images diminish SSL transfer benefits.

### Mechanism 2
Attention maps outperform GradCAM because they capture token-level importance through query-key inner products, but underperform Chefer due to discarding gradient information and relevance propagation across layers. The Chefer method aggregates relevance scores backward through all layers, capturing how attention contributes to final predictions. Core assumption: Attention weights correlate with decision-relevant regions; gradient flow provides additional importance signal. Evidence: Attention maps outperform GradCAM but underperform Chefer across all datasets. Break condition: For tasks with uniform attention or spurious correlations, attention maps fail regardless of method quality.

### Mechanism 3
Bounding box annotations inflate interpretability evaluation scores because the pointing game only requires maximum activation to fall within the box. Large boxes (bone abnormalities in MURA, esophageal regions in Kvasir) increase chance hits regardless of method precision. Core assumption: Disease regions can be meaningfully bounded by rectangular annotations. Evidence: Authors explicitly state bounding boxes have significant shortcomings and inflate results. Break condition: For focal pathologies, bounding boxes remain adequate.

## Foundational Learning

- **Vision Transformer tokenization and self-attention**: Understanding how ViT-B/16 processes images as 196 patch tokens plus [cls] token is essential for interpreting attention map extraction. Quick check: Can you explain why the [cls] token attention pattern is used for classification-relevant interpretability?

- **Self-supervised learning paradigms (contrastive vs. masked reconstruction)**: DINO (discriminative/contrastive) and MAE (generative/masked) produce different attention patterns; knowing why helps predict which works for specific datasets. Quick check: Would you expect MAE to learn better local texture features or global semantic structure, and why?

- **Pointing game vs. IoU as evaluation metrics**: The paper shows these metrics can contradict; understanding their failure modes prevents misleading conclusions. Quick check: A method achieves 98% pointing game accuracy but 35% IoU—what does this indicate about the interpretability map?

## Architecture Onboarding

- **Component map**: ViT-B/16 (224×224 input, 16×16 patches, 196 tokens + [cls]) → Interpretability extractors (GradCAM, Attention maps, Chefer) → Evaluation (Pointing game, IoU)

- **Critical path**: 1) Select pretraining (random/supervised/DINO/MAE), 2) Fine-tune on medical dataset with binary classification head, 3) Extract interpretability maps on validation positives, 4) Compare against expert bounding boxes using both metrics

- **Design tradeoffs**: Attention maps are computationally cheap and architecture-native but provide incomplete signal; Chefer method is more comprehensive but requires backward pass through all layers; bounding box evaluation is fast but low precision compared to segmentation maps

- **Failure signatures**: High pointing game + low IoU indicates method localizes broadly but imprecisely; GradCAM near-zero scores indicate incompatibility with transformer architecture; random-level interpretability despite high accuracy suggests spurious feature usage

- **First 3 experiments**: 1) Replicate attention vs. Chefer comparison on held-out medical dataset with both bounding box and segmentation annotations, 2) Ablate attention layer depth to test if earlier layers capture different diagnostic features, 3) Compare DINOv2 against original DINO to validate whether newer SSL methods improve medical interpretability

## Open Questions the Paper Calls Out

### Open Question 1
Do relative performance rankings of interpretability methods remain consistent when evaluated against pixel-level segmentation maps rather than bounding box annotations? The authors explicitly state bounding boxes have "significant shortcomings" and advise using segmentation maps for more precise assessment. This remains unresolved because the current study relied on bounding boxes that conflate target areas with background tissue.

### Open Question 2
Which self-supervised pretraining objectives (DINO vs. MAE) most reliably enhance attention map interpretability for specific medical imaging modalities? The authors note SSL generally helps but DINO doesn't always outperform supervised or MAE pretraining, suggesting "certain pretraining tasks may be more suitable for specific medical datasets." This remains unresolved due to inconsistent results across datasets indicating unknown interactions between pretraining strategy and data modality.

### Open Question 3
To what extent do evaluated interpretability methods rely on spurious correlations rather than clinically relevant features? The authors caution that evaluation based on expert annotations might be misleading due to "spurious correlations" and advise assessing potential biases. This remains unresolved because high IoU or pointing game scores indicate correlation with disease location but don't prove clinically valid reasoning.

## Limitations

- Evaluation metric ambiguity: IoU and pointing game can yield contradictory rankings, limiting confidence in method comparisons
- Domain transfer uncertainty: Optimal self-supervised pretraining strategy varies by dataset with incompletely understood transfer mechanisms
- Bounding box annotation quality: Large, imprecise bounding boxes inflate pointing game scores regardless of method precision

## Confidence

- Attention maps outperform GradCAM but underperform Chefer: High confidence - clear quantitative support across all datasets
- Self-supervised pretraining improves interpretability: Medium confidence - generally supported but with dataset-dependent variations and limited theoretical explanation
- Bounding boxes are inadequate for evaluation: High confidence - demonstrated through metric discrepancies and supported by medical annotation literature

## Next Checks

1. **Metric validation with segmentation masks**: Apply interpretability methods on datasets with both bounding boxes and pixel-level segmentations to quantify how annotation quality affects metric reliability and method rankings

2. **Multi-layer attention analysis**: Compare interpretability quality across different ViT layers to determine if diagnostic features emerge at specific depths, potentially explaining why Chefer outperforms single-layer attention maps

3. **Domain-specific SSL pretraining**: Test whether medical imaging-specific self-supervised pretraining improves interpretability transfer compared to natural image SSL, addressing fundamental domain shift questions