---
ver: rpa2
title: 'See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model'
arxiv_id: '2509.16087'
source_url: https://arxiv.org/abs/2509.16087
tags:
- spatial
- arxiv
- visual
- motion
- trek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SEE&TREK, the first training-free prompting
  framework designed to enhance spatial understanding in multimodal large language
  models (MLLMs) under vision-only constraints. The method addresses two key limitations
  in existing MLLMs: visual homogeneity from uniform frame sampling and lack of motion
  information in selected keyframes.'
---

# See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model

## Quick Facts
- arXiv ID: 2509.16087
- Source URL: https://arxiv.org/abs/2509.16087
- Reference count: 40
- Authors: Pengteng Li; Pinhao Song; Wuyang Li; Weiyu Guo; Huizai Yao; Yijie Xu; Dugang Liu; Hui Xiong
- One-line result: Training-free framework achieving up to 3.5% accuracy improvement on spatial reasoning benchmarks by addressing visual homogeneity and motion information limitations in MLLMs

## Executive Summary
This paper introduces SEE&TREK, the first training-free prompting framework designed to enhance spatial understanding in multimodal large language models (MLLMs) under vision-only constraints. The method addresses two key limitations in existing MLLMs: visual homogeneity from uniform frame sampling and lack of motion information in selected keyframes. SEE&TREK employs Maximum Semantic Richness Sampling using an off-the-shelf perception model to extract semantically diverse keyframes, and Motion Reconstruction using Visual Odometry to simulate camera trajectories and encode spatial-temporal context. Extensive experiments on VSI-BENCH and STI-BENCH benchmarks show that SEE&TREK consistently boosts performance across diverse spatial reasoning tasks.

## Method Summary
SEE&TREK operates through two main components: Maximum Semantic Richness Sampling and Motion Reconstruction. The first component uses YOLOv8n to detect objects in subsampled frames, then selects keyframes using Balanced-TopK to maximize semantic diversity while ensuring temporal distribution. The second component employs monocular visual odometry with ORB features and RANSAC to estimate camera trajectories, which are rendered as BEV and 3D visualizations. Spatiotemporal encoding overlays frame indices and trajectory-mapped color markers onto keyframes. The augmented frames, trajectory plots, and text prompts are fed into the MLLM for spatial reasoning tasks.

## Key Results
- Achieves up to 3.5% improvement in average accuracy across spatial reasoning benchmarks
- Best performance at N=4 sampling interval (43.2% accuracy, 82s vs. N=1's 410s)
- Maximum gain with K=8 keyframes (+3.0%); diminishing returns at higher K
- YOLOv8n tiny model achieves comparable accuracy with 5-10× speedup over larger detectors

## Why This Works (Mechanism)

### Mechanism 1: Semantic Diversity via Balanced-TopK Keyframe Selection
Selecting frames that maximize detected object categories while distributing temporally improves spatial scene representation compared to uniform sampling. A lightweight object detector (YOLOv8n) scans subsampled frames; Balanced-TopK selects the frame with maximum object count globally, then iteratively picks frames from K-1 temporal segments that minimize overlap with already-selected categories (|C_t ∩ C_sel|), maximizing new object discovery. Frames containing more detected objects provide richer spatial semantics and better capture scene structure than visually homogeneous frames (walls, ceilings, object fragments).

### Mechanism 2: Camera Motion Reconstruction via Monocular Visual Odometry
Explicitly reconstructing and visualizing camera trajectories compensates for MLLMs' inability to infer ego-motion from static frames alone. ORB features are extracted and matched between consecutive frames; the essential matrix E is estimated via RANSAC-robust 8-point algorithm; SVD decomposition yields relative rotation R_t and translation direction T_t (up to scale); poses are chained to form global trajectory, then rendered as BEV and 3D plots with color-coded temporal progression. MLLMs rely primarily on commonsense priors rather than grounded visual evidence when motion information is absent, leading to speculative spatial predictions.

### Mechanism 3: Spatiotemporal Encoding Bridges Semantic and Motion Information
Overlaying frame indices and color-coded trajectory markers directly onto keyframes establishes explicit correspondence between scene content and camera position. Each selected keyframe f_τi is augmented with a filled circle (color c_i = ColorMap(t_i/K)) and frame index at a fixed corner position, using the same colormap as trajectory visualizations. This creates a shared visual vocabulary linking keyframes to their positions in the motion sequence. MLLMs cannot natively associate disconnected visual inputs (keyframes vs. trajectory plots) without explicit visual markers indicating their relationship.

## Foundational Learning

- **Concept: Epipolar Geometry and Essential Matrix**
  - Why needed here: Motion reconstruction fundamentally relies on understanding how 2D point correspondences constrain 3D camera motion through the epipolar constraint (x'^T E x = 0)
  - Quick check question: Given matched points between two frames and camera intrinsics K, what does the essential matrix encode and how do you decompose it?

- **Concept: Object Detection Recall vs. Speed Trade-offs**
  - Why needed here: The method's efficiency hinges on using YOLOv8n (tiny model) for frame scoring; understanding detector failure modes directly impacts keyframe quality
  - Quick check question: Why might a detector optimized for COCO classes fail on indoor scene objects, and how would this bias keyframe selection?

- **Concept: Monocular Scale Ambiguity**
  - Why needed here: VO recovers translation direction only (up to scale); understanding this limitation is critical when interpreting trajectory visualizations and absolute distance reasoning tasks
  - Quick check question: If the essential matrix gives you rotation R and translation direction T, what additional information would you need to recover metric-scale camera positions?

## Architecture Onboarding

- **Component map:**
```
Input Video V
    ↓ [Subsample: every N frames]
Sampled Frames S
    ↓ YOLO Detection → Per-frame class sets C_t
    ↓ ORB Feature Extraction → Keypoints K_t, Descriptors D_t
    ↓ Feature Match + RANSAC → Matched pairs M_{t-1,t}
    ↓ Essential Matrix Est. → E matrix → (R_t, T_t)
    ↓ Pose Chaining → Global trajectory T_world
    ↓ Balanced-TopK Selection (uses C_t + temporal seg) → K keyframe indices τ_i
    ↓ Spatiotemporal Encoding → Augmented frames F'_τi
    ↓ Render BEV/3D → P_BEV, P_3D visualizations
    ↓ MLLM Input: [F'_τ1, ..., F'_τK, P_BEV, P_3D] + Text Prompt
```

- **Critical path:** Feature matching quality → Essential matrix accuracy → Trajectory coherence. Poor feature matches cascade into incorrect pose estimates, producing misleading trajectory visualizations that harm rather than help spatial reasoning.

- **Design tradeoffs:**
  - N=4 sampling interval (Table 5): Best accuracy/efficiency balance (43.2% accuracy, 82s vs. N=1's 410s); denser sampling increases computation 5× for marginal gains
  - YOLOv8n vs. larger detectors (Table 9): Tiny model achieves comparable accuracy with 5-10× speedup; larger models (YOLOv11S) show no consistent accuracy improvement
  - K=8 keyframes (Table 10): Maximum gain (+3.0%); diminishing returns at higher K due to visual redundancy

- **Failure signatures:**
  - Object count regression (Table 1, LLaVA-ONEVISION-7B: 34.7% → 32.0%): Suggests some models may be confused by augmented visual information or biased toward trajectory cues over scene content
  - YOLO generalization gaps (Appendix B.6): Fails in multi-object/multi-class scenes, missing objects that uniform sampling might have captured incidentally
  - Relative direction conflicts (Table 1, InternVL3-1B: 47.8% → 46.4%): Motion cues may contradict learned spatial priors in some configurations

- **First 3 experiments:**
  1. Reproduce core ablation (Table 3) on VSI-BENCH with InternVL3-8B: Validate that MSRS alone improves static tasks (object size) while Motion Reconstruction improves dynamic tasks (route planning). Use N=4, K=8, YOLOv8n as specified.
  2. Trajectory quality stress test: Synthetic videos with known camera poses (e.g., ScanNet trajectories). Measure VO drift vs. ground truth; correlate drift magnitude with spatial task performance degradation to establish failure threshold.
  3. Detector swap experiment: Replace YOLOv8n with GroundingDINO (open-vocabulary detector) on failure cases from Appendix B.6. Hypothesis: Better generalization should improve multi-object scene performance without requiring GPU acceleration.

## Open Questions the Paper Calls Out
- How does the performance-latency trade-off shift when replacing the lightweight YOLOv8n detector with higher-capacity models like GroundingDINO or DepthAnything?
- How does the reliance on monocular Visual Odometry (ORB features) impact performance in low-texture or highly dynamic environments where feature tracking is prone to failure?
- Do the visual trajectory overlays induce a robust internal 3D geometric understanding in the MLLM, or does the model simply learn to associate visual trajectory cues with specific answer types?

## Limitations
- Performance varies across model scales with some models experiencing accuracy regression when augmented with trajectory visualizations
- Requires camera intrinsics which are not specified for arbitrary video inputs, limiting real-world applicability
- YOLOv8n's limited generalization in multi-object/multi-class scenes can lead to suboptimal keyframe selection

## Confidence
- **High Confidence**: The visual odometry pipeline (ORB + RANSAC + SVD) is well-established and correctly implemented
- **Medium Confidence**: Semantic richness sampling using YOLOv8n improves spatial understanding, though performance varies by model scale and scene complexity
- **Low Confidence**: Claims about MLLMs' inability to infer ego-motion from static frames lack direct empirical validation beyond performance metrics

## Next Checks
1. Test the framework on synthetic videos with ground-truth camera trajectories to quantify the relationship between VO drift and spatial reasoning accuracy degradation
2. Compare YOLOv8n with open-vocabulary detectors (GroundingDINO) on failure cases to validate whether detection generalization limits semantic richness sampling
3. Conduct cross-model ablation studies to identify which MLLM architectures benefit most from spatiotemporal encoding versus which are confused by trajectory visualizations