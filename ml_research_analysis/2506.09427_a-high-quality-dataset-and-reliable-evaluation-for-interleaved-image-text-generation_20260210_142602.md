---
ver: rpa2
title: A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation
arxiv_id: '2506.09427'
source_url: https://arxiv.org/abs/2506.09427
tags:
- image
- question
- answer
- what
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InterSyn, the first large-scale, multi-turn,
  instruction-following dataset for interleaved image-text generation, built using
  a Self-Evaluation with Iterative Refinement (SEIR) pipeline. The dataset contains
  1.8 million single-turn and 50,000 multi-turn samples across 8 domains and 3,500
  topics, with rigorous automated quality control to ensure semantic completeness
  and image-text synergy.
---

# A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation

## Quick Facts
- **arXiv ID:** 2506.09427
- **Source URL:** https://arxiv.org/abs/2506.09427
- **Reference count:** 40
- **Primary result:** First large-scale, multi-turn interleaved image-text dataset (1.8M samples) with iterative refinement and specialized evaluator showing consistent model quality gains across four dimensions.

## Executive Summary
This paper introduces InterSyn, a large-scale dataset for interleaved image-text generation built using a Self-Evaluation with Iterative Refinement (SEIR) pipeline. The dataset contains 1.8 million single-turn and 50,000 multi-turn samples across 8 domains and 3,500 topics, with rigorous automated quality control to ensure semantic completeness and image-text synergy. To evaluate outputs, the authors propose SynJudge, an automatic evaluator scoring responses along four dimensions: text content, image content, image quality, and image-text synergy. Human evaluation shows that SEIR improves question quality by 32% and answer quality by up to 19% after three refinement iterations. Models fine-tuned on InterSyn show consistent gains across all metrics, with maximum improvements of 29.7% in text content, 10.3% in image content, 6% in image quality, and 52.1% in synergy, demonstrating the dataset's effectiveness for advancing multimodal generation.

## Method Summary
The method employs a three-stage Markovian refinement pipeline (SEIR) where a Language Model generates content, an evaluator provides specific improvement suggestions, and the model regenerates content based on feedback. The pipeline iterates through Question Refinement, Answer Refinement, and Image Refinement stages for three iterations each. A fine-tuned multimodal evaluator (SynJudge) scores outputs across four dimensions: Text Content Completeness (TCC), Image Content Completeness (ICC), Image Quality (IQ), and Image-Text Synergy (ITS). The dataset is built using proprietary models (Qwen2.5-32B, InternVL2.5-26B, FLUX.1-dev) and covers 8 domains with 3,500 topics. SynJudge is trained on 38,400 human-annotated samples to provide reliable automatic evaluation.

## Key Results
- SEIR pipeline improves question quality by 32% and answer quality by up to 19% after three refinement iterations
- Models fine-tuned on InterSyn achieve maximum improvements of 29.7% in text content, 10.3% in image content, 6% in image quality, and 52.1% in synergy
- SynJudge evaluation shows RMSE reduction from 1.16 (GPT-4o baseline) to 0.47 (fine-tuned QwenVL) for text content completeness
- Dataset contains 1.8 million single-turn and 50,000 multi-turn samples across 8 domains and 3,500 topics

## Why This Works (Mechanism)

### Mechanism 1
Iterative self-evaluation and refinement improves semantic completeness and instruction-following capability. A three-stage Markovian process where LM generates content, evaluator provides specific suggestions for improvement, and LM regenerates based on feedback. Core assumption: base generative models possess latent capability to produce high-quality outputs but require explicit iterative guidance to align with complex constraints. Evidence: SEIR equations define suggestion generation and refinement steps; Table 1 shows AR iterations improve TCC from 3.85 to 4.42; synthetic refinement pipelines supported by related work. Break condition: refinement suggestions become empty or iteration count reaches fixed limit (K=3).

### Mechanism 2
Optimizing for "image-text synergy" rather than simple consistency enhances utility of interleaved outputs. Refinement prompts explicitly penalize redundancy by separating "answer" from "caption" to ensure text provides core explanation while image adds complementary visual details. Core assumption: high-quality multimodal responses require distinct but aligned roles for text and images. Evidence: Synergy defined as rewarding "complementary alignment" and penalizing "redundancy"; Table 3 shows 52.1% improvement in Synergy versus 6% in Image Quality. Break condition: text generation fails to distinguish between "answer" and "caption" fields.

### Mechanism 3
Fine-tuning multimodal evaluator on human-aligned scores reduces deviation from human judgment. Vision-Language Model (InternVL or QwenVL) fine-tuned on 38,400 human-annotated samples to predict scores across four dimensions. Core assumption: human preference for "interleaved" quality follows learnable distribution that general-purpose models don't natively capture. Evidence: SynJudge achieves lower RMSE (0.47 for TCC) compared to base GPT-4o (1.16); reduces gap to human judgment to 5% versus 13% for non-fine-tuned methods. Break condition: evaluation benchmark distribution drifts from training data or subjective errors exceed 1-point threshold.

## Foundational Learning

- **Concept: Unified Multimodal Models (Uni-MLLMs)**
  - Why needed: Paper targets "interleaved" generation where text and images are mixed. Traditional LLMs only output text; diffusion models only output images. Must understand architectures that handle both sequentially or in parallel.
  - Quick check: Can a standard LLM generate an image token natively, or does it require an external adapter?

- **Concept: Markovian Refinement**
  - Why needed: SEIR pipeline relies on Markov property, meaning state at step k depends only on step k-1. Crucial for understanding how "history" is managed versus local refinement.
  - Quick check: In SEIR process, does quality of question at iteration 3 depend on question at iteration 2 or original question?

- **Concept: Semantic Alignment vs. Aesthetic Quality**
  - Why needed: Paper separates "Image Quality" from "Image Content Completeness" and "Synergy". High aesthetic scores don't guarantee image answers user's prompt.
  - Quick check: If image is high resolution but depicts wrong object, which metric fails: IQ or ICC?

## Architecture Onboarding

- **Component map:** Seed (Topic Hierarchy & Question Templates) -> SEIR Pipeline (LM: Qwen2.5-32B, VLM: InternVL2.5, GM: FLUX) -> SynJudge (Fine-tuned QwenVL2.5-7B) -> Fine-tuned Uni-MLLMs (Anole, VILA-U)

- **Critical path:**
  1. Load Topic Hierarchy & Question Templates
  2. Run SEIR loop (QR -> AR -> IR) for K=3 iterations
  3. Use SynJudge to score and validate dataset (InterSyn)
  4. Fine-tune downstream Uni-MLLMs on InterSyn

- **Design tradeoffs:**
  - Iteration Cost: Increasing K improves quality but increases inference time/cost linearly. Paper fixes K=3 as "sweet spot" where gains plateau
  - Judge Size: Larger judge models (78B) may be more accurate but slower. Paper uses 7B/8B variants for efficiency
  - Image Generation: IQ gains limited (6%) by intrinsic constraints of generator (FLUX) regardless of data quality

- **Failure signatures:**
  - Semantic Drift: Image content shifts away from question topic during iterative regeneration
  - Redundancy: Model generates text that simply describes image, failing "complementarity" requirement
  - Caption Misalignment: Generated image doesn't match intermediate caption used to guide generation

- **First 3 experiments:**
  1. SEIR Ablation: Generate 100 samples with K=0 and K=3, run SynJudge to verify quality gap in Table 1
  2. Judge Validation: Compare SynJudge scores against small set of human-labeled "golden" responses to calculate RMSE
  3. Fine-tuning Run: Fine-tune small interleaved model (Anole) on subset of InterSyn (10k samples) and evaluate on benchmark

## Open Questions the Paper Calls Out

### Open Question 1
How can the InterSyn dataset and SEIR pipeline be extended to support scenarios requiring multiple images within a single dialogue turn? The authors state the dataset is restricted to one image per turn, which diverges from real-world scenarios requiring simultaneous multi-image generation. Future work needs multi-image samples and analysis of model performance on multi-image generation tasks.

### Open Question 2
What modifications are required for SynJudge to accurately assess image-text synergy in responses containing multiple images? Section I notes that SynJudge is designed for single-image evaluation and extending it to multi-image contexts is a future direction. The current evaluation metrics and model architecture are tailored to single-image inputs.

### Open Question 3
Does training on InterSyn improve performance on highly structured tasks or complex multi-modal reasoning chains? The paper acknowledges that future work is needed to enhance the dataset's coverage of structured tasks and deep reasoning. Current dataset prioritizes general instruction-following, potentially lacking density in complex reasoning structures.

## Limitations
- Dataset construction relies on proprietary base models (Qwen2.5-32B, InternVL2.5-26B, FLUX.1-dev) whose performance may differ significantly from open alternatives
- Quality of synthetic refinement bottlenecked by capability of base models to follow complex refinement instructions
- 38,400 human-annotated samples for SynJudge training are not publicly available, requiring synthetic distillation or proxy datasets
- Effectiveness of synergy-focused approach depends critically on evaluator's ability to detect subtle redundancy versus complementarity

## Confidence

- **High Confidence:** SEIR pipeline architecture and iterative refinement mechanism are clearly specified and logically sound. Separation of evaluation dimensions (TCC, ICC, IQ, ITS) is methodologically rigorous.
- **Medium Confidence:** Reported quality improvements (32% question quality, 19% answer quality) based on human evaluation but exact methodology only partially specified. 52.1% synergy improvement impressive but depends heavily on SynJudge reliability.
- **Low Confidence:** Scalability claims (1.8M samples, 90,000 H100 hours) difficult to verify without exact seed data and full pipeline implementation. Generalization of gains across different multimodal models beyond those tested (Anole, VILA-U) is unproven.

## Next Checks

1. **Ablation on Base Model Quality:** Run SEIR with different base model configurations (smaller LMs, open-source VLMs) to quantify dependency of quality gains on proprietary model performance.

2. **Synergy Detection Robustness:** Create adversarial test cases where text and images are intentionally redundant versus complementary, and measure whether SynJudge correctly discriminates between them across multiple domains.

3. **Cost-Quality Tradeoff Analysis:** Systematically vary number of refinement iterations (K=0,1,2,3,4) and measure marginal quality improvements versus computational cost to verify claimed "sweet spot" at K=3.