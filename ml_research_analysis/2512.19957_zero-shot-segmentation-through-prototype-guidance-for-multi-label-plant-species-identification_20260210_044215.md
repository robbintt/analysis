---
ver: rpa2
title: Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species
  Identification
arxiv_id: '2512.19957'
source_url: https://arxiv.org/abs/2512.19957
tags:
- attention
- training
- classification
- species
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-label plant species
  identification in high-resolution vegetation plot images from the PlantCLEF 2025
  challenge. The authors propose a zero-shot segmentation approach using class prototypes
  obtained from training data as proxy guidance for training a segmentation Vision
  Transformer (ViT) on test set images.
---

# Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification

## Quick Facts
- arXiv ID: 2512.19957
- Source URL: https://arxiv.org/abs/2512.19957
- Reference count: 12
- Primary result: 5th place in PlantCLEF 2025 challenge with F1 score of 0.33331 on private leaderboard

## Executive Summary
This paper introduces a zero-shot segmentation approach for multi-label plant species identification in high-resolution vegetation plot images. The method uses class prototypes obtained from training data as proxy guidance for training a segmentation Vision Transformer (ViT) on test set images without pixel-level labels. By clustering DINOv2 embeddings of training images and training a narrow ViT to reconstruct these prototypes from test image features, the approach learns to generate attention scores that identify plant-relevant regions. The method achieved competitive performance in the PlantCLEF 2025 challenge, scoring only 0.03 lower than the top submission.

## Method Summary
The approach involves extracting DINOv2 embeddings from training images and clustering them via K-Means (K=7806 species) to create class prototypes. A narrow ViT (6 blocks, 12 heads, 768-dim) is trained to reconstruct this prototype matrix from frozen DINOv2 features of test images. The reconstruction objective forces the model to identify which patches correspond to known plant representations. For inference, attention scores from all ViT blocks are aggregated, normalized, and thresholded to create segmentation masks. These masks are used with a K×K grid assembly heuristic for species classification, achieving significant improvement over direct patch-wise classification.

## Key Results
- Achieved 5th place in PlantCLEF 2025 challenge with F1 score of 0.33331 on private leaderboard
- Early stopping at epoch 10-15 was critical, as later epochs showed attention degradation toward background features
- Grid assembly (K=9) classification improved F1 from ~0.09 (patch-wise) to 0.33, demonstrating the importance of contextual information

## Why This Works (Mechanism)

### Mechanism 1
Class prototypes from clustered training embeddings provide sufficient supervision signal for the ViT to learn plant-relevant attention patterns without pixel-level labels. DINOv2 embeddings from training images are clustered via K-Means (K=7806 species) to form class prototypes. A narrow ViT is trained to reconstruct this prototype matrix from test image features, forcing it to identify which patches correspond to known plant representations. The reconstruction of training prototypes from test features requires attending to plant regions and suppressing background.

### Mechanism 2
Aggregating attention across transformer blocks and heads yields spatial maps usable as zero-shot segmentation masks. Attention scores from all ViT blocks are averaged across heads and blocks, then normalized. Patches exceeding threshold t are retained; others discarded before classification. Attention magnitude correlates with plant presence, not just reconstruction-relevance, making it suitable for segmentation.

### Mechanism 3
Early stopping before loss convergence is essential; continued training causes attention collapse toward irrelevant features. The reconstruction objective admits trivial solutions (e.g., attending to constant features like frame edges). Over-training overfits to shortcuts, causing attention to shift from plants to background elements like quadrat frames.

## Foundational Learning

- Concept: Vision Transformer (ViT) patch embeddings and multi-head self-attention
  - Why needed here: The architecture uses a ViT to process image patches and generate attention scores; understanding token-wise attention is essential for interpreting segmentation outputs.
  - Quick check question: Given an image split into 64×64 patches, what does the attention score for patch i represent in a ViT?

- Concept: DINOv2 self-supervised features and their semantic properties
  - Why needed here: DINOv2 provides frozen features for both prototype creation and patch extraction; its pretrained representations determine cluster quality and reconstruction feasibility.
  - Quick check question: Why might self-supervised ViT features cluster well by semantic class even without labeled fine-tuning?

- Concept: Prototype learning and K-Means cluster centroids as class representatives
  - Why needed here: The reconstruction target is a fixed matrix of cluster centroids; understanding prototype formation clarifies what the ViT learns to reconstruct.
  - Quick check question: If K-Means clusters two visually similar species together, how would this affect prototype-guided reconstruction?

## Architecture Onboarding

- Component map: Frozen DINOv2 encoder -> K-Means (K=7806) -> Narrow ViT (6 blocks, 12 heads) -> Attention aggregation -> Threshold filter -> Classification heuristics
- Critical path: Test image -> 64×64 crop grid -> DINOv2 features -> ViT -> attention map -> threshold -> assemble K×K grids -> classify with DINOv2+classifier -> aggregate species predictions
- Design tradeoffs:
  - Higher resolution (3072×2048) yields more patches but limited GPU memory (~2048 tokens max with A100 80GB)
  - Larger grid K improves context (K=9 outperforms K=5) but increases compute
  - Later epochs achieve lower loss but worse attention maps—early stopping is mandatory
- Failure signatures:
  - Attention highlights quadrat frames instead of plants → model has collapsed; use earlier checkpoint
  - F1 near zero on patch-wise classification → insufficient context; switch to grid assembly heuristic
  - Loss plateau coincides with attention degradation → stop training immediately
- First 3 experiments:
  1. Reproduce prototype creation: Extract DINOv2 embeddings from training images, run K-Means with K=7806, verify cluster-to-species alignment via sample inspection.
  2. Train narrow ViT for 15 epochs on 2048×2048 test images, save checkpoints at epochs 5, 10, 15; visualize attention maps to confirm plant localization before loss convergence.
  3. Compare patch-wise vs. grid assembly (K=9) classification at epoch 10 checkpoint with attention threshold t=0.6, probability threshold prob=0.5; expect >3× F1 improvement with grid assembly.

## Open Questions the Paper Calls Out

### Open Question 1
What specific regularization techniques could prevent the degradation of attention maps during the convergence of the reconstruction loss? The authors observe a "clear degradation in the quality of the attention maps" coinciding with loss convergence and state that "explicit regularization was not provided" to prevent the model from collapsing to trivial solutions. Experiments implementing attention-keeping regularizers showing stable attention maps and improved F1 scores at later epochs would resolve this.

### Open Question 2
Does the proxy task of reconstructing class prototypes from test features inherently force the model to learn semantic segmentation boundaries? The reconstruction target is a constant matrix derived from training data; the mechanism linking test-patch attention to this target is theoretically sound but empirically results in model collapse, implying the semantic link is weak or brittle. Ablation studies analyzing the correlation between reconstruction error gradients and ground-truth segmentation masks would resolve this.

### Open Question 3
Is enforcing a single cluster per species (K=7806) optimal for representing the multi-modal visual variance of plant organs in the prototype reconstruction task? The authors set K to the number of classes, but acknowledge training data includes "multiple organs and perspectives." A single centroid per species may fail to represent distinct visual modes (e.g., flower vs. leaf), potentially confusing the reconstruction ViT. Comparative experiments using over-clustering (K > 7806) would resolve this.

## Limitations
- Prototype Quality: Effectiveness depends on DINOv2 embeddings having sufficient semantic separation across plant species; poor clustering could propagate errors to attention maps.
- Attention Collapse Mechanism: The observation that attention maps degrade after loss convergence is empirical and unexplained; the mechanism is not theoretically justified.
- Generalizability: The method is tailored to high-resolution vegetation plots with specific image statistics; performance on different plant imaging contexts is unknown.

## Confidence
- **High**: The 5th place ranking (F1=0.33331) on the PlantCLEF 2025 private leaderboard is a verifiable outcome.
- **Medium**: The reported improvement from patch-wise (F1~0.09) to grid assembly (F1=0.33) classification is detailed and significant.
- **Low**: The theoretical basis for why early stopping prevents attention collapse is anecdotal; the claim that attention scores directly indicate plant presence is not independently validated.

## Next Checks
1. **Cluster Validation**: After creating class prototypes via K-Means, compute and report the adjusted Rand index or similar metric comparing cluster assignments to species labels on a held-out validation set of training images.
2. **Attention Map Analysis**: For a fixed test image, generate and visualize attention maps from ViT checkpoints at epochs 5, 10, and 15. Quantify the IoU between the top-attention patches and ground-truth plant regions (if available in a validation subset).
3. **Hyperparameter Sensitivity**: Systematically vary the K-Means cluster count (K ∈ {5000, 7806, 10000}) and the attention threshold (t ∈ {0.5, 0.6, 0.7}) on a validation set, reporting F1 score changes to identify the most robust configuration.