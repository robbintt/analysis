---
ver: rpa2
title: Best Agent Identification for General Game Playing
arxiv_id: '2507.00451'
source_url: https://arxiv.org/abs/2507.00451
tags:
- game
- best
- each
- general
- identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently identifying the
  best-performing algorithm for each sub-task in a multi-problem domain, specifically
  within the context of general game playing. The authors propose a novel approach
  that frames this challenge as a set of best arm identification problems for multi-armed
  bandits, where each bandit corresponds to a specific game and each arm represents
  a specific game-playing agent.
---

# Best Agent Identification for General Game Playing

## Quick Facts
- arXiv ID: 2507.00451
- Source URL: https://arxiv.org/abs/2507.00451
- Authors: Matthew Stephenson; Alex Newcombe; Eric Piette; Dennis Soemers
- Reference count: 22
- Primary result: Proposed Optimistic-WS method achieves 35.5-47.0% lower average simple regret than UCB-E in GV-GAI and Ludii domains

## Executive Summary
This paper addresses the challenge of efficiently identifying the best-performing algorithm for each sub-task in multi-problem domains, specifically within general game playing. The authors propose a novel approach that frames this as a set of best arm identification problems for multi-armed bandits, where each bandit corresponds to a specific game and each arm represents a specific game-playing agent. Their core contribution is an optimistic selection process based on the Wilson score interval (Optimistic-WS), which ranks each arm across all bandits by potential regret reduction to efficiently identify the best agent for each game within limited trials.

The primary results demonstrate substantial performance improvements, with the Optimistic-WS approach showing average simple regret reductions of 35.5-47.0% compared to the second-best approach (UCB-E) when evaluated on two popular general game domains: the General Video Game AI (GVGAI) framework and the Ludii general game playing system. This method can significantly improve the quality and accuracy of agent evaluation procedures for general game frameworks and other multi-task domains with high algorithm runtimes.

## Method Summary
The authors propose a novel approach to best agent identification by framing the problem as a set of best arm identification problems for multi-armed bandits. Each bandit corresponds to a specific game, while each arm represents a specific game-playing agent. The core method uses an optimistic selection process based on the Wilson score interval (Optimistic-WS) to rank each arm across all bandits by their potential regret reduction. This allows for efficient identification of the best agent for each game within a limited number of trials. The Wilson score interval provides a statistically sound way to estimate confidence intervals for binomial proportions, making it particularly suitable for the bandit setting where reward outcomes are binary (win/loss).

## Key Results
- Optimistic-WS achieved average simple regret that was 35.5-47.0% smaller than UCB-E across tested datasets
- The method was evaluated on two popular general game domains: GVGAI and Ludii
- Results show significant performance improvements over previous best arm identification algorithms for multi-armed bandits

## Why This Works (Mechanism)
The method works by leveraging the Wilson score interval to create an optimistic estimate of each arm's performance across all bandits. This approach effectively balances exploration and exploitation by considering both the observed mean performance and the uncertainty in that estimate. The Wilson score interval provides a more accurate confidence interval for binomial proportions compared to simpler methods, especially when sample sizes are small or proportions are near 0 or 1. By ranking arms based on their potential regret reduction, the algorithm can efficiently identify the best agent for each game while minimizing unnecessary trials.

## Foundational Learning

**Multi-armed bandits** - Sequential decision-making framework where an agent chooses from multiple options (arms) to maximize cumulative reward
*Why needed:* Provides the theoretical foundation for sequential experimentation in agent selection
*Quick check:* Understand the explore-exploit tradeoff and regret minimization concepts

**Best arm identification** - Subfield of multi-armed bandits focused on finding the optimal arm with high confidence rather than maximizing cumulative reward
*Why needed:* Directly applicable to the problem of identifying best agents for each game
*Quick check:* Distinguish between regret minimization and pure identification objectives

**Wilson score interval** - Statistical method for calculating confidence intervals for binomial proportions
*Why needed:* Provides more accurate confidence estimates than normal approximation, especially for small samples or extreme proportions
*Quick check:* Compare Wilson interval to normal approximation and Agresti-Coull intervals

**Simple regret** - Measure of the difference between the reward of the chosen arm and the optimal arm
*Why needed:* Key performance metric for best arm identification algorithms
*Quick check:* Understand how simple regret differs from cumulative regret

## Architecture Onboarding

**Component map:** Games (bandits) -> Agents (arms) -> Trials -> Wilson score interval calculations -> Regret reduction estimates -> Agent selection

**Critical path:** Game selection → Agent selection → Trial execution → Reward observation → Wilson score update → Regret calculation → Next agent/game selection

**Design tradeoffs:** The method trades computational complexity of Wilson score interval calculations for more accurate confidence estimates and faster convergence to optimal agents. This is particularly beneficial when agent evaluation is expensive but computation per trial is relatively cheap.

**Failure signatures:** If Wilson score intervals are poorly calibrated, the method may over-explore suboptimal agents or fail to identify the true optimal agent. This could manifest as slower convergence or identification of incorrect best agents.

**3 first experiments:**
1. Verify Wilson score interval coverage properties on synthetic binomial data with varying sample sizes
2. Compare simple regret convergence rates on a small-scale game/agent testbed
3. Test sensitivity to different confidence level parameters in the Wilson score calculation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only two specific game playing frameworks (GVGAI and Ludii), constraining generalizability
- No analysis of computational overhead differences between the proposed method and baselines
- Performance not explored in domains with varying numbers of arms or different reward distributions

## Confidence
- Core claims for GV-GAI and Ludii results: Medium-High
- Generalizability claims to other multi-task domains: Medium-Low

## Next Checks
1. Test the method on additional game playing frameworks or multi-task domains to assess generalizability
2. Conduct ablation studies to quantify the computational overhead of Wilson score interval calculation compared to baseline methods
3. Evaluate performance with varying numbers of agents and games to understand scalability limits