---
ver: rpa2
title: 'Towards AI-Supported Research: a Vision of the TIB AIssistant'
arxiv_id: '2512.16447'
source_url: https://arxiv.org/abs/2512.16447
tags:
- research
- platform
- tools
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the TIB AIssistant, a domain-agnostic, human-machine
  collaborative platform for AI-supported research. The platform addresses challenges
  in AI literacy, prompt engineering, tool integration, and agent orchestration by
  providing modular components including a prompt library, tool library, and shared
  data store.
---

# Towards AI-Supported Research: a Vision of the TIB AIssistant
## Quick Facts
- arXiv ID: 2512.16447
- Source URL: https://arxiv.org/abs/2512.16447
- Reference count: 26
- One-line primary result: Introduces a modular, domain-agnostic platform for AI-supported research with human-machine collaboration

## Executive Summary
The TIB AIssistant is a proposed platform addressing challenges in AI literacy, prompt engineering, tool integration, and agent orchestration for research workflows. The vision emphasizes flexibility through modular components including a prompt library, tool library using Model Context Protocol (MCP), and shared data store. The platform aims to support researchers across disciplines throughout the research lifecycle while maintaining human control and critical evaluation. An initial prototype demonstrates feasibility by integrating domain-agnostic assistants with external scholarly tools.

## Method Summary
The approach involves building a web-based platform with modular architecture: a GUI wrapper, Prompt Library, Tool Library (MCP servers), and Data Store. The prototype uses external LLMs (specific models unspecified) rather than training models. The critical path requires deploying the web application, configuring MCP server connections, seeding the prompt library, initializing the data store, and implementing agent handoff logic. The system is designed for both centralized and local deployment options.

## Key Results
- Modular architecture successfully integrates domain-agnostic research assistants with external tools via MCP
- Shared data store enables cross-agent coordination without context window limitations
- Prototype demonstrates feasibility of human-machine collaborative research workflows
- Platform addresses multiple research lifecycle stages from ideation through paper writing

## Why This Works (Mechanism)
### Mechanism 1
- Claim: A curated prompt library reduces the barrier to effective LLM use for researchers lacking AI literacy.
- Mechanism: Pre-designed prompts encode domain-agnostic best practices for research tasks (ideation, literature analysis, writing). Users select rather than author prompts, bypassing trial-and-error iteration. Metadata tags enable task-to-prompt matching.
- Core assumption: Prompt quality transfers across users and domains without extensive customization.
- Evidence anchors: Platform addresses challenges in AI literacy, prompt engineering by providing modular components including a prompt library. A list of prompts minimizes the need for researchers to create their own prompts, often relying on time-consuming trial-and-error.

### Mechanism 2
- Claim: Model Context Protocol (MCP) enables dynamic, standardized integration of external scholarly tools without hard-coding.
- Mechanism: MCP servers wrap external APIs (Crossref, ORCID, Semantic Scholar) as discoverable tools. The LLM autonomously decides tool invocation based on descriptions and user input. New tools integrate via MCP server setup rather than platform modification.
- Core assumption: LLMs can reliably select appropriate tools and construct valid API calls from natural language context.
- Evidence anchors: Integrating external tools like Crossref and ORCID via Model Context Protocol. To ensure tools can be added dynamically, a Model Context Protocol (MCP) can be used. MCP provides a standardized approach to provide external access to LLMs.

### Mechanism 3
- Claim: A centralized data store enables cross-agent coordination without context window bloat.
- Mechanism: Agents write intermediate artifacts (research questions, bibliographies) to a shared store under predefined keys. Subsequent agents retrieve only relevant data rather than maintaining full conversation history. This decouples agent execution and enables isolated debugging.
- Core assumption: Research artifacts can be meaningfully stored as discrete, key-addressable units.
- Evidence anchors: Modular components including shared data store facilitate ideation, literature analysis, methodology development. The constraint of context window size is less problematic since the data is stored in a separate store, and the agents are self-contained.

## Foundational Learning
- Concept: **Model Context Protocol (MCP)**
  - Why needed here: MCP is the integration layer for all external tools. Understanding client-server architecture, tool discovery, and invocation patterns is essential for extending the platform.
  - Quick check question: Can you explain how an LLM determines whether to invoke a tool versus respond directly?

- Concept: **Retrieval-Augmented Generation (RAG) and Tool Calling distinction**
  - Why needed here: The platform uses both patterns—RAG for context injection (data store) and tool calling for dynamic API access. Confusing these leads to architecture errors.
  - Quick check question: When would you use RAG versus a tool call for fetching publication metadata?

- Concept: **Human-in-the-loop orchestration**
  - Why needed here: The vision explicitly rejects full automation. Interfaces must support intermediate result modification, not just prompt-response flows.
  - Quick check question: How would you design a UI that allows users to edit agent outputs before the next agent processes them?

## Architecture Onboarding
- Component map: User -> GUI (IDE-like interface) -> Agent Orchestration Layer -> Prompt Library + Tool Library (MCP Servers) -> Data Store (shared state) -> External APIs (Crossref, ORCID, Semantic Scholar, ORKG)

- Critical path:
  1. Deploy base web application (GitLab source available)
  2. Configure MCP server connections for at least 2 external tools
  3. Seed prompt library with domain-agnostic research task prompts
  4. Initialize data store schema (research questions, bibliography, etc.)
  5. Implement agent handoff logic with intermediate state persistence

- Design tradeoffs:
  - **Centralized vs. local deployment**: Paper proposes both, but local deployment requires user-managed API keys and model access.
  - **Agent autonomy vs. user control**: Full automation reduces friction but violates the human-machine collaboration principle.
  - **Predefined workflows vs. flexibility**: Fixed pipelines (like "The AI Scientist") are easier to build but less adaptable than the modular approach proposed here.

- Failure signatures:
  - Tool calls silently fail due to API rate limits—platform must gracefully degrade (Error-Tolerance principle).
  - Context mismatch when data store keys don't align with agent expectations—schema versioning needed.
  - Prompt outputs inconsistent across LLM providers—testing matrix required if BYOK model selection enabled.

- First 3 experiments:
  1. Run the prototype through one complete research cycle (ideation → research questions → literature review → outline) to validate agent handoffs and data persistence.
  2. Add a new MCP server for a tool not in the prototype (e.g., arXiv API) to test extensibility claims.
  3. Compare outputs from 3 different prompt variants for the same task (e.g., research question formulation) to assess whether "no one-size-fits-all" assumption holds.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can a shared prompt library effectively bridge the gap in AI literacy across different scholarly domains?
- Basis in paper: The authors identify "lacking domain-specific AI literacy" (Challenge 1) and propose a prompt library as a solution, but do not validate if this successfully transfers expertise to non-experts.
- Why unresolved: The paper describes the library's structure but presents no user studies assessing whether non-experts can effectively utilize shared prompts for complex tasks.
- What evidence would resolve it: User studies measuring task completion rates for researchers with varying AI expertise using the library versus creating prompts from scratch.

### Open Question 2
- Question: Does the centralized data store architecture improve the coherence of multi-step workflows better than standard LLM context windows?
- Basis in paper: Section 4.1 asserts that a centralized data store offers benefits over context-based memory, yet this architectural trade-off lacks empirical testing regarding agent coordination.
- Why unresolved: The prototype demonstrates the architecture, but there is no analysis of whether this storage method effectively preserves logical consistency across the research life cycle.
- What evidence would resolve it: Quantitative evaluation of context consistency and error propagation in multi-agent interactions compared to context-window-only approaches.

### Open Question 3
- Question: Does the human-machine collaboration model yield higher quality scientific outputs than fully autonomous pipelines?
- Basis in paper: The authors critique fully automated frameworks (Section 3.2), arguing for a hybrid approach where humans "orchestrate and validate," but offer no comparative performance data.
- Why unresolved: The preference for human-in-the-loop is stated as a design principle, yet the paper provides no evidence that this orchestration improves the quality of the final research artifacts.
- What evidence would resolve it: A benchmark comparison of research output quality (e.g., accuracy, rigor) between the TIB AIssistant and fully autonomous systems.

## Limitations
- Claims about reducing AI literacy barriers and improving research efficiency lack empirical validation
- No quantitative performance metrics provided for any core mechanisms
- System's reliance on external LLM providers and APIs introduces potential failure points not addressed in prototype
- Assumption that domain-agnostic prompts work across disciplines without significant customization is untested

## Confidence
- **High Confidence**: The architectural patterns (MCP integration, modular agent design, shared data store) are technically sound and align with established best practices in AI engineering. The human-in-the-loop orchestration principle is well-founded.
- **Medium Confidence**: The feasibility of the vision is demonstrated through the prototype, but real-world effectiveness across diverse research domains remains unproven. The extensibility claims depend on MCP server reliability, which varies by external tool provider.
- **Low Confidence**: Claims about reducing AI literacy barriers and improving research efficiency lack empirical validation. The "no one-size-fits-all" prompt assumption is acknowledged but not tested across disciplines.

## Next Checks
1. **Cross-Domain Prompt Efficacy Test**: Deploy the prompt library across three distinct research domains (e.g., humanities, natural sciences, social sciences) and measure the modification rate required for acceptable outputs. Track time savings versus researchers creating prompts from scratch.

2. **MCP Tool Integration Stress Test**: Implement five diverse MCP servers (including complex APIs like arXiv with nested queries) and conduct 100 tool invocations each. Measure success rate, latency, and error handling under rate limiting conditions.

3. **Multi-Agent Coordination Benchmark**: Execute a complete research cycle with 10+ agent handoffs, introducing deliberate user modifications at each stage. Measure context preservation accuracy and identify failure points in the data store schema design.