---
ver: rpa2
title: 'RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft
  Trees'
arxiv_id: '2512.14069'
source_url: https://arxiv.org/abs/2512.14069
tags:
- draft
- radar
- calls
- arxiv
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating large language
  model (LLM) inference, which is expensive and slow due to the autoregressive generation
  process requiring access to entire model parameters for each generated token. The
  authors propose RADAR, a novel speculative sampling method that uses reinforcement
  learning (RL) to dynamically generate draft trees, allowing for more effective generation
  and utilization of candidate tokens.
---

# RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees

## Quick Facts
- arXiv ID: 2512.14069
- Source URL: https://arxiv.org/abs/2512.14069
- Authors: Junjie Ma; Jinlong Li
- Reference count: 0
- Achieves 3.17x–4.82x speedup over auto-regressive decoding baseline

## Executive Summary
This paper introduces RADAR, a reinforcement learning-based approach for accelerating large language model inference through dynamic draft tree generation. The method addresses the computational bottleneck of autoregressive decoding by using speculative sampling with intelligent draft tree management. RADAR employs reinforcement learning to make real-time decisions about draft model calls, reducing redundant computations while maintaining high-quality token generation. The approach demonstrates significant speedups across multiple LLMs and tasks while preserving generation quality.

## Method Summary
RADAR formulates draft tree generation as a Markov Decision Process and uses offline RL to train a prediction model for real-time decision making. The method dynamically controls the number of draft model calls based on learned policies, rather than using fixed strategies. This RL-based approach allows RADAR to adaptively generate and prune candidate tokens, optimizing the trade-off between computational cost and generation quality. The system operates by maintaining a dynamic tree structure where the RL agent decides when to expand nodes and how many draft tokens to generate at each step.

## Key Results
- Achieves 3.17x–4.82x speedup over auto-regressive decoding baseline
- Reduces average draft model calls by 18.7% compared to EAGLE-3
- Maintains high average acceptance length (only ~1.2% lower than EAGLE-3)
- Validated across three LLMs (Llama-Instruct 3.1 8B, Vicuna 13B, DeepSeek-R1-Distill-Llama 8B)
- Tested on four diverse tasks (MT-bench, GSM8K, Alpaca, MBPP)

## Why This Works (Mechanism)
RADAR's effectiveness stems from its ability to make intelligent, learned decisions about draft tree generation rather than using fixed patterns. The reinforcement learning component learns optimal policies for when to generate more candidates and when to prune, adapting to the specific characteristics of each generation context. By reducing redundant draft model calls while maintaining sufficient candidate diversity, RADAR achieves better computational efficiency without sacrificing generation quality.

## Foundational Learning

**Markov Decision Process (MDP)**
- Why needed: Provides the mathematical framework for modeling sequential decision-making in draft tree generation
- Quick check: Verify that state transitions in the draft process can be modeled as Markovian

**Reinforcement Learning (RL)**
- Why needed: Enables learning optimal policies for dynamic draft tree management without explicit supervision
- Quick check: Ensure the reward function properly balances speed and quality trade-offs

**Speculative Sampling**
- Why needed: The core technique for accelerating inference by generating multiple candidate tokens in parallel
- Quick check: Validate that acceptance rates remain high while reducing draft model calls

## Architecture Onboarding

**Component Map**
LLM -> Draft Model -> RL Agent -> Dynamic Tree Manager -> Output Filter

**Critical Path**
Input prompt → Draft model generation → RL-based pruning decisions → Acceptance/rejection → Final output

**Design Tradeoffs**
- Computational overhead of maintaining draft trees vs. speedup benefits
- Quality of offline RL training data vs. generalization to new prompts
- Complexity of MDP formulation vs. ease of implementation

**Failure Signatures**
- Poor RL policy leading to excessive draft calls or insufficient candidate quality
- Tree structure becoming too complex, causing memory issues
- Acceptance rates dropping significantly due to aggressive pruning

**3 First Experiments**
1. Measure acceptance rates and draft model call counts on a held-out validation set
2. Profile memory usage during draft tree maintenance across different tree depths
3. Compare speedups with and without the RL-based decision component

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Computational overhead of maintaining and traversing draft trees during inference is not fully characterized
- Offline RL training requires substantial computational resources and may not generalize well to out-of-distribution prompts
- MDP formulation assumes specific draft model architecture that may not be optimal for all LLM pairs

## Confidence
- High confidence in technical methodology and experimental design
- Medium confidence in generalizability across different LLM sizes and tasks
- Medium confidence in claimed speedups relative to existing methods

## Next Checks
1. Conduct runtime profiling to measure actual overhead of maintaining draft trees and verify claimed speedups account for all computational costs
2. Test RADAR with larger language models (70B+ parameters) to assess scalability and whether 18.7% draft call reduction holds at different scales
3. Perform ablation study isolating contribution of RL-based decision policy versus dynamic tree structure to quantify individual impact on performance