---
ver: rpa2
title: 'Human-Centered Evaluation of RAG outputs: a framework and questionnaire for
  human-AI collaboration'
arxiv_id: '2509.26205'
source_url: https://arxiv.org/abs/2509.26205
tags:
- metrics
- questionnaire
- evaluation
- human
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-centered questionnaire for evaluating
  retrieval-augmented generation (RAG) outputs. Building on Gienapp's utility-dimension
  framework, the authors iteratively refined a 12-item questionnaire through human
  ratings and discussions, ultimately incorporating feedback from both human raters
  and human-LLM pairs.
---

# Human-Centered Evaluation of RAG outputs: a framework and questionnaire for human-AI collaboration

## Quick Facts
- **arXiv ID:** 2509.26205
- **Source URL:** https://arxiv.org/abs/2509.26205
- **Reference count:** 34
- **Primary result:** Introduces a 12-item human-centered questionnaire for evaluating RAG outputs, showing good inter-rater reliability (ICC = 0.73) and balanced human-AI assessment capabilities.

## Executive Summary
This paper presents a human-centered framework for evaluating Retrieval-Augmented Generation (RAG) outputs through a 12-item questionnaire. Building on Gienapp's utility-dimension framework, the authors iteratively refined the instrument through human ratings and discussions, incorporating feedback from both human raters and human-LLM pairs. The questionnaire assesses RAG outputs across multiple dimensions including logical coherence, stylistic consistency, user intent alignment, and verifiability.

The evaluation shows good inter-rater reliability and demonstrates that while LLMs excel at focusing on metric descriptions, they struggle with detecting format variations. The framework aims to balance human perception with computational efficiency in RAG evaluation, providing a structured approach to quality assessment that can guide both human-AI collaboration and system improvements.

## Method Summary
The authors developed the questionnaire through an iterative process involving human ratings and discussions, refining items based on feedback from both human raters and human-LLM pairs. The framework draws on Gienapp's utility-dimension theory as its theoretical foundation. Evaluation involved testing the instrument with human raters and comparing results with LLM assessments using GPT-4. The study measured inter-rater reliability and examined agreement between human and LLM evaluations across various RAG output dimensions.

## Key Results
- The 12-item questionnaire demonstrated good inter-rater reliability with ICC = 0.73
- LLMs showed strength in focusing on metric descriptions but struggled with format variations
- Human raters found the questionnaire understandable and useful, though LLM ratings showed only partial agreement with human judgments
- The framework successfully balanced human perception with computational efficiency in RAG evaluation

## Why This Works (Mechanism)
The framework works by translating abstract quality dimensions into concrete, actionable assessment criteria that both humans and AI systems can apply consistently. The iterative refinement process ensured that questions were understandable to humans while remaining computationally tractable for LLMs. By grounding the instrument in established utility-dimension theory, the questionnaire captures multiple facets of output quality that matter to users, from logical coherence to stylistic consistency.

## Foundational Learning
**Utility-dimension framework** - Why needed: Provides theoretical foundation for breaking down complex output quality into measurable dimensions. Quick check: Can you map each questionnaire item to a specific utility dimension?

**Inter-rater reliability metrics** - Why needed: Quantifies consistency of human judgments to validate questionnaire effectiveness. Quick check: Does ICC > 0.7 indicate acceptable reliability for this context?

**Human-AI collaborative evaluation** - Why needed: Leverages complementary strengths of human perception and LLM processing speed. Quick check: Can you identify which dimensions each agent type evaluates better?

## Architecture Onboarding

**Component map:** User Query -> RAG System -> Output -> Questionnaire Assessment (Human/LLM) -> Quality Score

**Critical path:** The evaluation flow follows: (1) Generate RAG output, (2) Apply questionnaire items systematically, (3) Aggregate scores across dimensions, (4) Compare human vs. LLM assessments.

**Design tradeoffs:** The framework balances comprehensiveness (12 dimensions) against cognitive load on raters, while maintaining computational feasibility for LLM evaluation. The moderate ICC value (0.73) reflects the inherent subjectivity in quality assessment versus the need for standardized metrics.

**Failure signatures:** Low inter-rater reliability indicates ambiguous question phrasing; systematic LLM-human disagreement suggests questions that exploit different evaluation strengths; consistently poor scores across dimensions indicate fundamental RAG system issues rather than evaluation problems.

**First experiments:** 
1. Apply questionnaire to diverse RAG outputs across multiple domains to test generalizability
2. Compare LLM performance using different model families (not just GPT-4) to identify system-specific patterns
3. Conduct A/B testing with and without specific questionnaire items to determine which dimensions drive most variance in quality assessment

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to a small sample of human raters and single LLM system (GPT-4), restricting generalizability
- ICC of 0.73 indicates moderate variability in human judgments, suggesting need for refinement
- Focus primarily on technical domains with limited exploration of broader user needs and other application areas

## Confidence

| Claim | Confidence |
|-------|------------|
| Questionnaire development process is methodologically sound | High |
| Framework captures relevant quality dimensions for RAG outputs | Medium |
| Results generalize across different LLM systems and user populations | Low |
| Instrument maintains effectiveness over time as RAG technology evolves | Medium |

## Next Checks
1. Test the questionnaire with a larger, more diverse group of human raters across different domains and technical expertise levels to establish broader reliability metrics.

2. Evaluate the framework with multiple LLM systems (beyond GPT-4) to assess consistency of results and identify potential system-specific biases.

3. Conduct a longitudinal study to determine if the questionnaire maintains its effectiveness and reliability over time, and whether it captures evolving user needs as RAG technology advances.