---
ver: rpa2
title: 'Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM'
arxiv_id: '2512.12868'
source_url: https://arxiv.org/abs/2512.12868
tags:
- diagnosis
- fbpr
- questions
- question
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how much of the performance of large language
  models (LLMs) on clinical diagnosis benchmarks can be attributed to simple probabilistic
  reasoning over concept-diagnosis co-occurrence statistics from pretraining corpora.
  The authors introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight
  method that scores diagnosis options using a smoothed Naive Bayes over co-occurrence
  frequencies extracted from the Dolma and RedPajama corpora.
---

# Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM
## Quick Facts
- arXiv ID: 2512.12868
- Source URL: https://arxiv.org/abs/2512.12868
- Authors: Furong Jia; Yuan Pu; Finn Guo; Monica Agrawal
- Reference count: 25
- A lightweight probabilistic method achieves comparable performance to LLMs on clinical diagnosis tasks

## Executive Summary
This paper investigates how much of the performance of large language models (LLMs) on clinical diagnosis benchmarks can be attributed to simple probabilistic reasoning over concept-diagnosis co-occurrence statistics from pretraining corpora. The authors introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores diagnosis options using a smoothed Naive Bayes over co-occurrence frequencies extracted from the Dolma and RedPajama corpora. When applied to a diagnosis subset of MedQA, FBPR achieves accuracies of 46.7% (Dolma) and 44.5% (RedPajama), comparable to the LLMs OLMo Instruct 7B (44.1%) and LLaMA 65B (47.0%) trained on the same corpora. The methods largely make different correct predictions, indicating complementary strengths. These results suggest that simple frequency-based methods can account for a substantial portion of LLM performance on medical diagnosis tasks, highlighting the value of explicit probabilistic baselines and potential for hybrid approaches.

## Method Summary
The Frequency-Based Probabilistic Ranker (FBPR) is a lightweight probabilistic method that scores diagnosis options using a smoothed Naive Bayes approach over co-occurrence frequencies extracted from pretraining corpora. The method identifies medical concepts (diseases, symptoms, procedures, medications) in clinical case questions, then computes the log probability of each diagnosis option given the identified concepts using smoothed frequency counts from the corpus. The frequency counts are smoothed using add-one smoothing to handle unseen concept-diagnosis pairs. FBPR processes MedQA questions by first extracting medical concepts, then scoring each diagnosis option based on its co-occurrence frequency with those concepts in the pretraining corpus, selecting the highest-scoring option as the prediction.

## Key Results
- FBPR achieves 46.7% accuracy on MedQA diagnosis subset using Dolma corpus
- FBPR achieves 44.5% accuracy on MedQA diagnosis subset using RedPajama corpus
- These results are comparable to OLMo Instruct 7B (44.1%) and LLaMA 65B (47.0%) trained on the same corpora

## Why This Works (Mechanism)
FBPR works by leveraging the rich co-occurrence statistics present in large pretraining corpora. When medical concepts appear together frequently in training data, they form probabilistic associations that can be captured through simple frequency counting. The method exploits the fact that certain diseases co-occur with specific symptoms, procedures, and medications at predictable rates. By counting these co-occurrences across millions of documents, FBPR builds a statistical model that captures these relationships without requiring complex neural architectures or expensive inference.

## Foundational Learning
- Medical concept extraction: Why needed - to identify relevant features from clinical text; Quick check - can the system accurately identify diseases, symptoms, and procedures from free-text questions
- Co-occurrence frequency counting: Why needed - to build statistical associations between concepts and diagnoses; Quick check - are the frequency counts accurate and representative of the corpus
- Naive Bayes smoothing: Why needed - to handle unseen concept-diagnosis pairs; Quick check - does add-one smoothing appropriately balance observed frequencies with unseen combinations
- Corpus selection: Why needed - different corpora contain different medical knowledge distributions; Quick check - does the method perform consistently across multiple pretraining corpora
- Soft voting: Why needed - to combine predictions from multiple corpora; Quick check - does combining predictions improve overall accuracy

## Architecture Onboarding
The FBPR architecture follows a linear processing pipeline: Text Preprocessing -> Medical Concept Extraction -> Frequency Lookup -> Probability Scoring -> Diagnosis Selection. The critical path involves extracting concepts from the question, looking up their frequencies with each diagnosis option, computing smoothed probabilities, and selecting the highest-scoring diagnosis. The main design tradeoff is between simplicity and comprehensiveness - FBPR uses simple frequency counting rather than complex neural models, trading potential expressiveness for interpretability and computational efficiency. Failure signatures include poor concept extraction (leading to missing or incorrect medical terms), sparse frequency counts (due to corpus limitations), and over-reliance on common but incorrect diagnoses. Three first experiments would be: 1) Evaluate concept extraction accuracy on a manually annotated subset of MedQA questions, 2) Compare performance using different smoothing parameters, 3) Test the impact of corpus size on prediction accuracy.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The study is limited to a single clinical diagnosis benchmark (MedQA), raising questions about generalizability to other medical reasoning tasks
- Performance comparisons are restricted to LLMs trained on the same pretraining corpora, limiting understanding of FBPR's effectiveness against a broader range of models
- The method's reliance on co-occurrence statistics introduces potential bias from corpus composition and quality, which is not fully addressed

## Confidence
- Performance parity claim: Medium confidence - results show comparable accuracy but are based on a single benchmark and specific corpus constraints
- Attribution of LLM performance claim: Medium confidence - plausible interpretation but cannot definitively establish causation or quantify exact contributions
- Complementary strengths claim: Low confidence - observation is interesting but lacks systematic analysis of prediction patterns

## Next Checks
1. Cross-dataset validation: Evaluate FBPR on multiple clinical reasoning benchmarks beyond MedQA, including datasets with different question formats, medical specialties, and difficulty levels to assess generalizability.
2. Corpus sensitivity analysis: Systematically test FBPR's performance using different combinations and subsets of pretraining corpora to understand how corpus composition affects results and identify optimal training data characteristics.
3. Hybrid method evaluation: Implement and test hybrid approaches that combine FBPR's probabilistic reasoning with LLM inference, measuring whether the complementary strengths identified in the study translate to improved overall performance on clinical diagnosis tasks.