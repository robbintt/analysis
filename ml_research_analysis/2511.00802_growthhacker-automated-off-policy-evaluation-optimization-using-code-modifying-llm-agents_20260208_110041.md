---
ver: rpa2
title: 'GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying
  LLM Agents'
arxiv_id: '2511.00802'
source_url: https://arxiv.org/abs/2511.00802
tags:
- code
- optimization
- agent
- evaluation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigate whether LLM-based agents can automatically optimize
  off-policy evaluation (OPE) through code modification. We propose GrowthHacker,
  a benchmark system that iteratively refines code, executes OPE, and compares results
  across iterations.
---

# GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents

## Quick Facts
- **arXiv ID:** 2511.00802
- **Source URL:** https://arxiv.org/abs/2511.00802
- **Reference count:** 40
- **Primary result:** A two-agent LLM framework achieves 100% reliability and 106.7% average improvement for automated off-policy evaluation optimization.

## Executive Summary
This paper investigates whether large language models can automatically optimize off-policy evaluation (OPE) through iterative code modification. The authors introduce GrowthHacker, a benchmark system that iteratively refines code, executes OPE, and compares results across independent iterations. Their custom two_agent framework, featuring specialized Prompter and Coder agents, demonstrates significantly higher reliability and performance compared to existing multi-agent systems like AutoGen and CrewAI. The work establishes a new approach for scalable, automated optimization of decision-making algorithms in production environments.

## Method Summary
GrowthHacker implements a two-agent architecture where a Prompter/Analyzer agent generates modification instructions from execution results, and a Coder agent applies these instructions to create new code versions. The system runs 7 independent iterations in parallel, each starting from the original baseline code to avoid error accumulation. A final selection step identifies the best-performing configuration. The framework uses file-based communication to prevent context degradation and focuses on minor function adjustments and hyperparameter tuning to maintain execution reliability. Gemini-2.5-Flash serves as the underlying LLM model.

## Key Results
- Two_agent framework achieves 100% reliability compared to 45% for CrewAI and 34% for AutoGen
- Highest average improvement of 106.7% among positive outcomes for two_agent system
- Parallel exploration strategy prevents error accumulation common in sequential refinement approaches
- System successfully optimizes OPE across multiple estimator types (DM, IPW, DR) and datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dual-agent architecture separating analysis from code synthesis reduces compilation failures by preventing context degradation.
- **Mechanism:** The Prompter/Analyzer reads results and generates text-based instructions (`instruction_i.md`), while the Coder consumes this static instruction to modify code. This file-based state passing prevents the API hallucination observed in conversational frameworks.
- **Core assumption:** LLM possesses sufficient domain knowledge to propose valid OPE optimizations when explicitly prompted without managing conversational state.
- **Evidence anchors:** Section 3.2 identifies compilation failures and context degradation as primary limitations; Section 3.3 describes the file-based architecture; corpus papers discuss OPE applications but not agent architecture reliability.
- **Break condition:** Ambiguous instructions like "optimize the model" without specific parameter constraints may cause the Coder to hallucinate invalid APIs.

### Mechanism 2
- **Claim:** Parallel exploration of independent optimization trajectories prevents error accumulation from sequential refinement.
- **Mechanism:** System spawns 7 independent iterations, each deriving from original baseline code rather than previous outputs. Final selection identifies best performer, isolating failures so syntax errors in one iteration don't prevent others from succeeding.
- **Core assumption:** Optimal configuration is reachable via single-step modification from baseline or exists within distribution of independent attempts.
- **Evidence anchors:** Section 3.4 states parallel exploration avoids context degradation and error accumulation; Section 5.1 notes other frameworks suffer from File Corruption and Syntax Errors which parallel isolation bypasses.
- **Break condition:** If optimization landscape requires multi-step hill-climbing path, parallel approach will fail to find global optimum.

### Mechanism 3
- **Claim:** Constraining optimization scope to minor adjustments maintains executable integrity while improving OPE metrics.
- **Mechanism:** Prompts explicitly discourage architectural rewrites, focusing LLM on safe modifications like adjusting gamma, bandwidth, or estimator selection. This keeps code within LLM's reliable execution boundary for OBP/Scope-RL libraries.
- **Core assumption:** Baseline code is structurally sound and performance gains are primarily accessible via parameter adjustment rather than algorithmic innovation.
- **Evidence anchors:** Section 3.4 mentions Targeted Optimization Scope and prompt design enabling consistent compilation success; Section 5.3.3 discusses Code Modification Reliability favoring agent-applied patches over whole-code regeneration.
- **Break condition:** If dataset requires novel estimator structure not present in libraries, constrained agent will flatline at 0% improvement rather than innovate.

## Foundational Learning

- **Concept: Off-Policy Evaluation (OPE)**
  - **Why needed here:** This is the "loss function" agents try to minimize. Understanding that OPE estimates policy value using logged data (avoiding costly A/B tests) is essential to interpret the 106.7% improvement metric.
  - **Quick check question:** Why can't we just calculate the mean reward from logged data to evaluate a new policy? (Answer: Distribution shift/Selection bias)

- **Concept: Estimators (IPW, DM, DR)**
  - **Why needed here:** These are specific code components agents modify. The paper references Doubly Robust and Inverse Probability Weighting as targets for hyperparameter tuning.
  - **Quick check question:** Which estimator combines reward model with importance weighting to reduce variance? (Answer: Doubly Robust / DR)

- **Concept: Agentic "Tool Use" vs. Code Generation**
  - **Why needed here:** Paper distinguishes between agents that "talk" (AutoGen) and agents that "write files" (two_agent). Understanding this distinction is key to reliability improvements.
  - **Quick check question:** Why might a "chat" based agent fail to modify a 500-line Python file correctly compared to an agent that reads/writes files?

## Architecture Onboarding

- **Component map:** original_code.py + baseline_result.txt → Analyzer → instruction_i.md → Coder + original_code.py → newcode_i.py → Executor → results.txt → Selector

- **Critical path:** The instruction file. If Analyzer suggests deprecated parameter (e.g., gamma outside constructor), Coder will inject syntax error. Reliability depends entirely on Analyzer's ability to map textual strategy to valid library APIs.

- **Design tradeoffs:**
  - **Reliability vs. Depth:** two_agent achieves 100% reliability by limiting itself to "minor adjustments," trading potential for complex refactoring (which might yield higher gains) for execution stability.
  - **Isolation vs. Learning:** Iterations are independent (parallel). System does not "learn" from failure; it survives via redundancy.

- **Failure signatures:**
  - **Parameter Placement Errors:** gamma=0.99 appearing outside constructor parenthesis (Section 5.1)
  - **Diff Injection:** Agent outputting `--- a/main.py` diff syntax directly into executable Python file (AutoGen specific)
  - **Parameter Explosion:** Continuous action space parameters (bandwidth < 1.0) causing metric spikes > 9,999%

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run provided OBP notebooks (multiclass.ipynb) without agents to establish ground truth MSE for DM/IPW/DR estimators
  2. **Failure Mode Trigger:** Attempt to optimize Scope-RL notebook using generic "improve this code" prompt to reproduce File Corruption or Syntax Error failure modes
  3. **Iterative Tuning:** Execute two_agent framework on synthetic dataset and verify instruction.md correctly identifies hyperparameter to tune and newcode.py reflects this change syntactically

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating iteration-aware memory systems improve optimization convergence compared to current independent exploration strategy?
- **Basis in paper:** [explicit] Authors note current methods prevent agents from getting feedback from previous runs and suggest future work should "investigate developing memory systems for agents to learn from iteration history."
- **Why unresolved:** Preliminary attempts to add iteration history confused models, leading to failures; robust method for integrating history without degrading performance remains undiscovered.
- **What evidence would resolve it:** Demonstration of memory-augmented agent achieving statistically higher success rates or faster convergence than baseline independent runs.

### Open Question 2
- **Question:** How can agent frameworks dynamically balance exploration and exploitation to prevent local optima in OBP and parameter explosions in Scope-RL?
- **Basis in paper:** [explicit] Paper identifies limitation regarding "imbalance of exploration and exploitation," noting OBP requires escaping local optima while Scope-RL requires safer exploration to avoid failures.
- **Why unresolved:** Current "shooting in the dark" approach causes instability and inconsistent results across different datasets; unified strategy for conflicting needs is undefined.
- **What evidence would resolve it:** Study showing dynamic exploration strategy reduces "EXTREME" value occurrences in Scope-RL and improves median improvement metrics in OBP simultaneously.

### Open Question 3
- **Question:** What is optimal design for human-in-the-loop systems to balance automation safety with efficiency in high-stakes OPE scenarios?
- **Basis in paper:** [explicit] Authors conclude "human-in-the-loop systems could be considered" and suggest future work investigate "how to reach better balances between automation and human interference."
- **Why unresolved:** Full autonomy currently risks severe parameter explosions, but specific trade-offs and checkpoint mechanisms for human validation are not established.
- **What evidence would resolve it:** Defined human-agent workflow that mitigates critical failures (e.g., bandwidth reductions below safety thresholds) while maintaining acceptable optimization latency.

## Limitations

- Reliability claims hinge on proprietary prompt templates and specific API parameter constraints not fully disclosed
- Success rates based on small sample size (8 notebooks) with particularly limited Scope-RL testing (3 notebooks) showing no positive outcomes
- Parallel exploration strategy assumes optimal configuration is reachable via single-step modifications, which may not hold for optimization landscapes requiring sequential refinement
- Approach highly specialized for OPE codebases with constrained modifications to hyperparameter tuning of specific estimator classes

## Confidence

- **High Confidence:** Parallel isolation strategy's effectiveness in preventing error accumulation, supported by clear evidence comparing sequential vs. independent iteration failures
- **Medium Confidence:** 106.7% average improvement metric, based on positive outcomes only and excluding Scope-RL results where system failed to produce improvements
- **Low Confidence:** Generalizability of approach beyond OPE codebases given highly specialized nature of modifications and constrained prompt design

## Next Checks

1. **Prompt Template Validation:** Request and test exact Prompter/Analyzer and Coder prompts to verify claimed 100% reliability holds when applied to new OPE codebases not included in original evaluation
2. **Failure Mode Replication:** Systematically reproduce File Corruption and Syntax & Code Errors failure modes by attempting code optimization without constrained prompt design, then measure reliability improvement provided by two_agent approach
3. **Multi-Step Optimization Test:** Design controlled experiment where optimal solution requires at least two sequential modifications to test whether parallel exploration strategy can find solutions requiring coordinated multi-step changes