---
ver: rpa2
title: Making Universal Policies Universal
arxiv_id: '2502.14777'
source_url: https://arxiv.org/abs/2502.14777
tags:
- agent
- agents
- learning
- action
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends universal policies to enable training a single
  policy that can control multiple agents with different action spaces but shared
  observation spaces. The method builds on a diffusion-based planner that generates
  observation sequences, paired with agent-specific inverse dynamics models to assign
  actions.
---

# Making Universal Policies Universal

## Quick Facts
- arXiv ID: 2502.14777
- Source URL: https://arxiv.org/abs/2502.14777
- Reference count: 40
- One-line primary result: Diffusion-based planner conditioned on agent type enables single policy to control multiple agents with different action spaces, achieving up to 42.20% improvement in task completion accuracy.

## Executive Summary
This paper extends universal policies to enable training a single policy that can control multiple agents with different action spaces but shared observation spaces. The method builds on a diffusion-based planner that generates observation sequences, paired with agent-specific inverse dynamics models to assign actions. By pooling datasets from different agents, the approach achieves positive transfer, outperforming policies trained on individual datasets. The planner is conditioned on agent information—such as action space encoding or example trajectories—with action space encoding yielding the best results. Experiments on BabyAI tasks show up to 42.20% improvement in task completion accuracy compared to single-agent policies. The method also demonstrates generalization to unseen agents when agent diversity is increased.

## Method Summary
The method trains a single conditional diffusion planner that generates observation sequences, combined with agent-specific inverse dynamics models (IVDs) to predict actions from consecutive observations. The planner is conditioned on agent information (action space encoding, agent ID, or example trajectories) and trained on pooled datasets from multiple agents. Each agent has its own IVD model trained on agent-specific data. At inference, the planner generates an observation sequence conditioned on the current observation, instruction, and agent type, then consecutive observation pairs are passed to the corresponding IVD to predict executable actions. This decouples task planning from action execution, allowing positive transfer across agents with different action capabilities.

## Key Results
- Up to 42.20% improvement in task completion accuracy compared to training on a single agent dataset
- Action space encoding conditioning outperforms agent ID and example trajectory conditioning
- Generalization to out-of-distribution agents improves from 0.494 to 0.906 when training diversity increases from 6 to 6,595 agents
- Positive transfer observed across all tested BabyAI environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling planning from action selection enables a single shared planner to serve multiple agents with different action capabilities.
- Mechanism: The diffusion-based planner generates observation sequences conditioned on agent type information. Agent-specific inverse dynamics models then translate consecutive observation pairs into executable actions. This separation allows the planner to learn general task completion strategies from pooled data while delegating action-space constraints to specialized, smaller models.
- Core assumption: Observation sequences encode sufficient information about task progress that any agent capable of reaching those observations can complete the task, regardless of specific action primitives.
- Evidence anchors: [abstract] "diffusion-based planner that generates observation sequences conditioned on agent type information, combined with agent-specific inverse dynamics models"; [Section 3.1] Defines CA-UDPD framework where observation space is shared (X) but policies (πk) are agent-specific.

### Mechanism 2
- Claim: Conditioning the planner on action space encodings (rather than arbitrary agent IDs) improves in-distribution performance and enables limited generalization to unseen action combinations.
- Mechanism: Binary vector encoding of available actions (v ∈ {0,1}^|A|) provides structured information about agent capabilities. This allows the planner to learn correlations between available actions and feasible observation transitions, rather than memorizing arbitrary agent-ID mappings.
- Core assumption: The action space encoding captures the relevant constraints for plan generation; agents with similar action spaces can execute similar observation sequences.
- Evidence anchors: [Section 3.3] Describes action space representation as binary vector where "if v_i = 1 the agent is capable of action i"; [Figure 4] Shows "Mixed-ActionSpace" consistently outperforms "Mixed-AgentID" across environments.

### Mechanism 3
- Claim: Positive transfer emerges from exposing the planner to diverse trajectories across agents, increasing effective training data for task-completion patterns.
- Mechanism: Pooling datasets from agents with different action spaces provides more examples of goal-reaching observation sequences. Even though agents take different actions, the shared observation space allows the planner to learn general navigation and task-completion strategies that transfer across agent types.
- Core assumption: Task-completion patterns in observation space are partially independent of the specific actions used to achieve them.
- Evidence anchors: [abstract] "improving task completion rates by up to 42.20% compared to training on a single agent dataset"; [Section 4.2] "For all environments UCAP conditioned on any type of the agent representation shows positive transfer".

## Foundational Learning

- Concept: **Diffusion models for conditional generation**
  - Why needed here: The planner uses denoising diffusion to generate observation sequences; understanding the ODE formulation (Eq. 1-2) and Heun sampling is essential.
  - Quick check question: Can you explain how conditioning is injected into the denoising network D_θ, and why classifier-free guidance was set to zero?

- Concept: **Inverse dynamics models**
  - Why needed here: These agent-specific models bridge generated observation sequences to executable actions; training requires paired observation-action data.
  - Quick check question: Given two consecutive observations, how would you train a model to predict the discrete action that caused the transition?

- Concept: **Transfer learning in imitation learning**
  - Why needed here: Understanding positive/negative transfer helps diagnose when pooled training helps vs. hurts performance.
  - Quick check question: What conditions would cause negative transfer when pooling data from agents with very different action spaces?

## Architecture Onboarding

- Component map:
  1. **Shared Diffusion Planner**: Takes (x_0, c, k) → generates observation sequence; conditioned on start observation, instruction, and agent info
  2. **Conditioning Encoders**: T5 for instructions; linear/embedding layers for agent ID, action space encoding, or example trajectories
  3. **Agent-Specific Inverse Dynamics Models**: One per agent type; maps (x_t, x_{t+1}) → action
  4. **Training Data**: Pooled dataset D with agent IDs; individual datasets D_n for inverse dynamics

- Critical path:
  1. Prepare pooled observation sequences from all agents (random windows of size 4)
  2. Train shared planner with conditional diffusion loss (Eq. 2)
  3. Train individual inverse dynamics models on agent-specific data
  4. At inference: generate plan → extract consecutive pairs → predict actions via IVD

- Design tradeoffs:
  - **Conditioning type**: Action space encoding generalizes better but requires action-level metadata; Agent ID is simplest but cannot generalize to new agents; Example trajectories are most flexible but require more data
  - **Planning granularity**: 1-step planning requires strict action compatibility; 2-step planning allows more flexibility (Table 4 shows OOD performance improves significantly)
  - **Dataset pooling**: Increases data diversity but may introduce conflicting optimization signals

- Failure signatures:
  - **Unreachable plans**: Planner generates observations the agent cannot reach → inverse dynamics produces invalid actions
  - **Negative transfer**: Performance worse than single-agent training → check if action space conditioning is active
  - **OOD generalization failure**: Novel agent types perform poorly → consider increasing agent diversity (Table 5 shows 6,595 agents improves generalization from 0.494 to 0.906)

- First 3 experiments:
  1. **Baseline validation**: Train single-agent policies on individual datasets to establish lower bounds; verify inverse dynamics models achieve >95% accuracy on action prediction
  2. **Ablation on conditioning**: Compare Agent ID, Action Space encoding, and Example Trajectory conditioning on a held-out agent type; measure task completion and analyze failure modes
  3. **Scaling test**: Gradually increase number of agent types in training pool (2 → 4 → 6 → 10) while keeping total data constant; plot transfer efficiency vs. diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Universal Cross-Agent Policy (UCAP) framework maintain positive transfer performance when extended to agents with heterogeneous observation spaces?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion (Page 8): "Future work should extend the approach to agents with heterogenous observation spaces."
- Why unresolved: The current study is restricted to a shared observation space (BabyAI gridworld), where the diffusion planner generates plans in a unified visual format. It is unclear if a single diffusion planner can simultaneously model valid future observation sequences for agents with fundamentally different sensors (e.g., a static camera vs. an egocentric camera).
- What evidence would resolve it: Experimental results from training UCAP on a dataset like Open X-Embodiment (OXE), evaluating whether the shared planner successfully improves performance across agents with diverse visual inputs compared to single-agent baselines.

### Open Question 2
- Question: Does the positive transfer observed in grid-world environments persist in complex, real-world robotic domains with high visual variety?
- Basis in paper: [explicit] The authors note in the Limitations (Page 8): "It is unclear whether this transfer will hold when environment observations vary more significantly, such as in the OXE dataset... Future work should attempt to scale the approach to larger datasets."
- Why unresolved: The experiments were conducted in the BabyAI environment, which utilizes relatively simple, structured grid-based observations. It remains unverified if the diffusion model can effectively pool data from complex, high-dimensional robotic manipulation scenarios without succumbing to visual artifacts or mode collapse.
- What evidence would resolve it: A demonstration of UCAP trained on a multi-embodiment robotic dataset (e.g., OXE), showing improved task completion rates over single-agent policies on real-world manipulation tasks.

### Open Question 3
- Question: Can progressive distillation or consistency models effectively mitigate the computational overhead of the diffusion planner without sacrificing cross-agent generalization?
- Basis in paper: [explicit] The authors state in the Limitations (Page 8): "A drawback of using the diffusion planner is the increased training and inference time... Techniques like progressive distillation [40] and consistency models [42] improve sampling speed with minimal loss in generative ability."
- Why unresolved: While the authors suggest these techniques, they do not implement them. It is unknown if the sample efficiency and diversity required for cross-agent positive transfer can be preserved when compressing the 128 neural function evaluations down to a few steps.
- What evidence would resolve it: A comparative study measuring the task completion accuracy and latency of a distilled UCAP model versus the standard diffusion implementation on the BabyAI cross-agent tasks.

### Open Question 4
- Question: Does the "Example Trajectory" conditioning method require scaling to thousands of agent types to robustly outperform explicit action-space encoding for zero-shot generalization?
- Basis in paper: [inferred] The paper shows that while action-space encoding generalizes well to Out-of-Distribution (OOD) agents, the "Example Trajectory" method failed with 6 agent types but succeeded with 6,595 agents (Page 7). This infers a scaling requirement or data density threshold that was not fully mapped.
- Why unresolved: The ablation on "Increased Agent Diversity" showed a massive jump in performance for the example-based method (Table 5), but the authors did not determine the minimum diversity or data volume required to make this method reliable compared to the more consistent action-space encoding.
- What evidence would resolve it: A study plotting the zero-shot OOD generalization performance of "Example Trajectory" conditioning against an increasing number of in-distribution agent types (e.g., 10, 50, 100, 1000) to find the intersection point where it matches or exceeds action-space encoding.

## Limitations
- The approach assumes shared observation spaces, limiting applicability to agents with different sensors
- Diffusion planner introduces significant computational overhead during training and inference
- Performance on out-of-distribution agents is highly sensitive to training diversity levels
- Requires structured action metadata (action space encoding) that may not be available in all domains

## Confidence

**High Confidence:** The technical framework and training methodology are clearly specified. The empirical results on BabyAI tasks (up to 42.20% improvement) are well-documented and reproducible.

**Medium Confidence:** The claim that pooling datasets from different agents leads to positive transfer is supported by the experiments, but the underlying mechanism could vary with different task complexities and agent diversity levels.

**Medium Confidence:** The generalization to unseen agents when increasing training diversity is demonstrated, but the results are highly dependent on the specific diversity implementation and may not transfer to other domains.

## Next Checks

1. **Negative Transfer Analysis:** Systematically test scenarios where agent action spaces differ substantially (e.g., one agent can teleport while others cannot) to identify conditions that cause performance degradation below single-agent baselines.

2. **OOD Generalization Stress Test:** Train on a minimal set of agents (2-3) and test on agents with novel action combinations not seen during training to measure the limits of action-space encoding generalization.

3. **Planner-Independent Validation:** Replace the diffusion planner with a simpler planner (e.g., behavior cloning) to isolate whether improvements come from the planner architecture or the general framework of decoupling planning from action selection.