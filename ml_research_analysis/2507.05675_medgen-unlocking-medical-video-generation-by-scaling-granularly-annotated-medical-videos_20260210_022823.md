---
ver: rpa2
title: 'MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated
  Medical Videos'
arxiv_id: '2507.05675'
source_url: https://arxiv.org/abs/2507.05675
tags:
- video
- medical
- generation
- videos
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedVideoCap-55K, the first large-scale, caption-rich
  dataset for medical video generation, comprising over 55,000 curated clips spanning
  diverse real-world medical scenarios. Built upon this dataset, the authors develop
  MedGen, a specialized medical video generation model that significantly outperforms
  other open-source models in both visual quality and medical accuracy.
---

# MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos

## Quick Facts
- arXiv ID: 2507.05675
- Source URL: https://arxiv.org/abs/2507.05675
- Reference count: 4
- Primary result: MedGen achieves leading performance on Med-VBench and rivals commercial systems like Sora and Pika, with a total score of 70.93.

## Executive Summary
This paper introduces MedVideoCap-55K, the first large-scale, caption-rich dataset for medical video generation, comprising over 55,000 curated clips spanning diverse real-world medical scenarios. Built upon this dataset, the authors develop MedGen, a specialized medical video generation model that significantly outperforms other open-source models in both visual quality and medical accuracy. MedGen achieves leading performance on Med-VBench and rivals commercial systems like Sora and Pika, with a total score of 70.93. Human evaluations by medical experts further validate MedGen's superiority in text alignment, medical accuracy, and visual quality. The dataset and model are shown to be effective for downstream medical video analysis tasks, demonstrating strong generalization across clinical practice, medical animation, and imaging domains.

## Method Summary
MedGen is built on top of the open-source latent diffusion video model HunyuanVideo with Low-Rank Adaptation (LoRA) for efficient domain adaptation. The model is trained on MedVideoCap-55K, a large-scale medical video dataset created through systematic filtering of YouTube videos. The dataset construction involves medical keyword filtering, CLIP-based medical frame classification, temporal consistency checks, and multi-stage quality filtering to remove low-quality frames and ensure medical relevance. LoRA adapters are added to attention layers of the base model to specialize it for medical video generation while preserving general video generation capabilities.

## Key Results
- MedGen achieves a total score of 70.93 on Med-VBench, outperforming all open-source baselines and rivaling commercial systems.
- Human expert evaluations show MedGen significantly outperforms baseline models in text alignment (score: 73.75 vs 63.00), medical accuracy (score: 73.25 vs 61.00), and visual quality (score: 78.75 vs 62.50).
- The model demonstrates strong generalization across clinical practice, medical animation, and imaging domains.
- MedGen shows lower warping error (38.50) compared to HunyuanVideo (45.00) and better text alignment scores across all dimensions.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Data Scaling for Medical Visual Semantics
Training on large-scale, caption-rich medical videos enables generation of medically accurate content that general-domain models fail to produce. General video diffusion models learn priors from natural scenes that do not encode anatomical correctness, procedural logic, or instrument semantics. By exposing the model to 55K diverse, text-aligned medical clips, the denoising trajectory is shaped toward medically plausible outputs rather than generic visual patterns.

### Mechanism 2: Multi-Stage Quality Filtering Reduces Training Noise
Systematic removal of low-quality frames (black borders, subtitles, compression artifacts) improves model convergence and output fidelity. Noisy data introduces conflicting gradients and teaches the model to attend to irrelevant features. The pipeline—CLIP-based medical frame classification → temporal consistency filtering → aesthetic/technical scoring—ensures each clip is visually coherent, medically relevant, and artifact-free.

### Mechanism 3: LoRA-Based Domain Adaptation Preserves Base Capacity
Low-Rank Adaptation enables efficient transfer from a strong general video prior (HunyuanVideo) to medical domain while retaining temporal coherence and visual quality. LoRA adds trainable low-rank matrices to attention layers, updating only ~1–2% of parameters. This allows the model to specialize on medical data distribution without catastrophic forgetting of general motion and texture priors learned from massive general corpora.

## Foundational Learning

- **Concept**: Latent Video Diffusion Models (LVDMs)
  - Why needed here: MedGen inherits this architecture; understanding how video is compressed to latent space, noised, and denoised is essential for debugging and extension.
  - Quick check question: Can you explain why operating in latent space (rather than pixel space) is more efficient for video generation?

- **Concept**: LoRA (Low-Rank Adaptation)
  - Why needed here: The paper uses LoRA for efficient domain adaptation; understanding rank, alpha, and injection points is critical for reproducing or modifying the approach.
  - Quick check question: If LoRA rank is set too low, what symptom would you expect during fine-tuning on a new domain?

- **Concept**: Medical Visual Constraints (Anatomy, Procedure Logic)
  - Why needed here: Unlike natural video, medical content has hard semantic constraints (e.g., liver cannot appear in thorax); evaluation must detect these violations.
  - Quick check question: Why would standard FID or aesthetic scores be insufficient for assessing medical video quality?

## Architecture Onboarding

- **Component map**: YouTube collection → medical keyword + text classifier → CLIP frame classifier + temporal filtering → caption generation (GPT-4o) → multi-stage quality filters → HunyuanVideo backbone + LoRA adapters → MedGen

- **Critical path**:
  1. Video segmentation must produce coherent clips (≥6 frames, medical label, temporal consistency); breaks here cause caption misalignment.
  2. Caption generation must be grounded (uses title, description, transcript); hallucinations in captions propagate to model training.
  3. LoRA training must converge without overfitting; monitored via validation loss and visual inspection of generated samples.

- **Design tradeoffs**:
  - Dataset size vs. diversity: 55K clips is large for medical domain but still sparse for some specialties (only 2.39% imaging videos); generalization to unseen subdomains may be limited.
  - Filtering strictness: Aggressive subtitle/quality filtering removes noise but may exclude rare but valuable content (e.g., educational slides with overlays).
  - LoRA vs. full fine-tuning: LoRA is efficient but may not capture deep domain shifts; full fine-tuning is costly and risks overfitting.

- **Failure signatures**:
  - Distortion/warping errors: High Dover scores or low aesthetic scores in training data correlate with warped outputs (MedGen achieves 38.50 warping error vs. HunyuanVideo's 45.00).
  - Medical hallucination: Generated instruments or anatomy not present in training set; detectable via expert review or out-of-distribution prompt testing.
  - Caption misalignment: If captions describe scenes not visually present, model learns spurious text-video mappings.

- **First 3 experiments**:
  1. Ablation on filtering stages: Train on data with/without subtitle or aesthetic filtering; compare Med-VBench scores and human evaluation.
  2. LoRA rank sensitivity: Test ranks [8, 16, 32, 64] on same training budget; measure loss convergence and medical accuracy.
  3. Cross-domain transfer: Train on a subset (e.g., only surgical videos) and evaluate on held-out categories (e.g., medical animation) to test generalization limits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between medical and non-medical video data in training to maximize medical accuracy while preserving general video generation capabilities like motion smoothness and background consistency?
- Basis in paper: Section 4.3 states: "We hypothesize that incorporating a balanced proportion of non-medical video data could mitigate this issue while preserving performance in these metrics. Exploring this approach to enhance dataset variety and its impact on model robustness remains an avenue for future work."
- Why unresolved: The current experiments show slight declines in background consistency and motion smoothness when training purely on MedVideoCap-55K, but no systematic study of mixed-domain training was conducted.
- What evidence would resolve it: Ablation studies varying the ratio of medical-to-general video data with measurements across all Med-VBench dimensions.

### Open Question 2
- Question: What validation safeguards and accuracy thresholds are needed before medical video generation models can be safely deployed in clinical training and education settings?
- Basis in paper: The Ethics Statement explicitly restricts the dataset "solely and strictly for research purposes and should not be used for non-research settings, especially in clinical practice," yet the applications section targets clinical training and patient education.
- Why unresolved: No validation framework or minimum accuracy requirements for clinical use are defined; the gap between research performance and clinical safety standards remains unspecified.
- What evidence would resolve it: Clinical validation studies with larger expert panels evaluating error rates on critical medical accuracy dimensions, plus definition of acceptable thresholds for deployment contexts.

### Open Question 3
- Question: How well does MedGen generalize to medical domains, procedures, or demographic contexts underrepresented in the YouTube-sourced training data?
- Basis in paper: The dataset relies entirely on publicly available YouTube videos filtered by keywords and classifiers, with no systematic analysis of representation across medical subspecialties, geographic regions, patient demographics, or procedure types.
- Why unresolved: The paper demonstrates generalization across five broad categories but does not analyze performance on rare conditions, underrepresented populations, or procedures less commonly documented on social platforms.
- What evidence would resolve it: Stratified evaluation across demographic groups and medical subspecialties, plus failure analysis on underrepresented scenarios.

## Limitations

- Data Coverage Gaps: While MedVideoCap-55K is the largest medical video dataset reported, it remains domain-sparse for certain specialties (e.g., only 2.39% imaging videos, limited rare pathologies). The model's ability to generalize beyond its training distribution is untested and likely constrained.

- Evaluation Subjectivity: Human expert evaluations, while necessary for medical accuracy, introduce subjective variance. The paper does not report inter-rater reliability or how experts were calibrated, raising questions about evaluation consistency.

- Commercial System Comparison: Claims of rivaling Sora and Pika are based on a single benchmark (Med-VBench) and limited human evaluation. Direct apples-to-apples comparisons are impossible since commercial systems' training data and architectures are proprietary.

## Confidence

- **High Confidence**: Claims about dataset construction methodology, filtering pipeline implementation, and LoRA adaptation mechanics. These are concrete technical details with clear implementation paths.

- **Medium Confidence**: Claims about MedGen outperforming open-source baselines on Med-VBench and human evaluation metrics. While results are reported, limited transparency about evaluation protocols and potential confounds reduces confidence.

- **Low Confidence**: Claims about rivaling commercial systems (Sora, Pika) and achieving "leading performance" across all metrics. These comparisons lack sufficient methodological detail and face inherent reproducibility challenges.

## Next Checks

1. **Cross-Domain Generalization Test**: Train MedGen on a single medical subdomain (e.g., surgical videos only) and evaluate on held-out categories (medical animation, imaging). Measure performance drop to quantify domain generalization limits.

2. **Ablation of Filtering Pipeline**: Create three training variants: (a) raw unfiltered data, (b) filtered for subtitles only, (c) filtered for all quality metrics. Compare Med-VBench scores and human evaluation to isolate filtering impact.

3. **Out-of-Distribution Prompt Challenge**: Design prompts describing medical procedures, anatomy, or scenarios absent from MedVideoCap-55K. Generate videos and have medical experts assess hallucination rates and anatomical plausibility compared to general-domain models.