---
ver: rpa2
title: 'Using large language models to produce literature reviews: Usages and systematic
  biases of microphysics parametrizations in 2699 publications'
arxiv_id: '2503.21352'
source_url: https://arxiv.org/abs/2503.21352
tags:
- parameterizations
- publications
- microphysics
- precipitation
- parameterization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used the large language model GPT-4 Turbo to systematically
  analyze 2,699 publications on precipitation simulations using the WRF model. The
  goal was to understand how different microphysics parameterizations were used and
  their performance biases.
---

# Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications

## Quick Facts
- arXiv ID: 2503.21352
- Source URL: https://arxiv.org/abs/2503.21352
- Reference count: 22
- Using GPT-4 Turbo to analyze 2,699 WRF precipitation studies reveals systematic overestimation biases in 7 of 9 microphysics schemes

## Executive Summary
This study used GPT-4 Turbo to systematically extract and analyze model configurations, performance metrics, and precipitation biases from 2,699 WRF-related publications. The LLM achieved 94% overall extraction accuracy across 8 structured questions about microphysics schemes, regional domains, and performance metrics. Results showed that seven out of nine microphysics parameterizations consistently overestimated precipitation, with biases varying by region. The WSM6 and Thompson schemes were most commonly used. This demonstrates how LLMs can efficiently process large scientific literature to reveal usage patterns and inform model selection, offering a scalable method for synthesizing complex research domains.

## Method Summary
The authors collected 3,958 publications from Web of Science and Scopus using WRF and precipitation-related queries, filtered to 2,699 available PDFs after excluding coupled models. They used GPT-4 Turbo with temperature=0.12 and structured 8-question prompts to extract microphysics schemes, performance metrics (RMSE/CC), and bias classifications. Manual validation on 300 papers achieved 94% average accuracy. The analysis aggregated over/underestimation patterns by scheme and region, revealing systematic biases across the literature.

## Key Results
- GPT-4 Turbo achieved 94% extraction accuracy across 2,699 publications with structured prompts
- Seven of nine microphysics schemes showed tendency to overestimate precipitation, with regional variations
- WSM6 and Thompson schemes were most commonly used, accounting for majority of studies
- Extreme precipitation events showed large RMSEs and negative correlation coefficients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can extract structured model configuration and performance data from scientific PDFs with accuracy sufficient for meta-analysis.
- Mechanism: The paper uses GPT-4 Turbo with an explicit 8-question prompt template targeting microphysics schemes, performance ratings, and regional biases; outputs are normalized to structured fields for aggregation.
- Core assumption: The information is present in extractable text and the prompt unambiguously constrains answers to the target schema.
- Evidence anchors:
  - [abstract] Reports 94% overall extraction accuracy across 2,699 publications.
  - [section 2.4–2.5, Table 1] Average accuracy 94% across 8 questions; per-question accuracies 86–98%; 300-publication manual validation.
  - [corpus] Neighbor work on LLM-assisted SLRs aligns with LLM-based extraction feasibility, but independent replication for this specific atmospheric-science task is limited.
- Break condition: Key data reside only in figures/tables without accessible text; performance drops for RMSE/CC in such cases.

### Mechanism 2
- Claim: Low-temperature decoding reduces variance in structured extraction while preserving usable outputs.
- Mechanism: The authors set temperature=0.12 to make outputs more deterministic and reduce hallucinations, trading off creativity for consistency.
- Core assumption: Deterministic outputs correlate with higher correctness for this extraction task.
- Evidence anchors:
  - [section 2.4] Explicit temperature setting and rationale.
  - [abstract] High-level framing of GPT-4 Turbo for systematic interrogation.
  - [corpus] No direct corpus papers evaluate temperature effects on extraction accuracy; this is an internal design choice without external corroboration here.
- Break condition: Tasks requiring more open-ended synthesis may suffer from overly low temperature.

### Mechanism 3
- Claim: Aggregating over/underestimation signals across many studies reveals consistent regional and scheme-level bias patterns.
- Mechanism: GPT-4 Turbo classifies each paper's precipitation error as overestimate, underestimate, mix, or no answer; the authors compute N_over − N_under by scheme and region.
- Core assumption: Individual study conclusions are comparable enough to aggregate despite heterogeneous configurations, regions, and event types.
- Evidence anchors:
  - [abstract] "Seven out of nine parameterizations tended to overestimate precipitation."
  - [section 5, Fig. 10–11] Regional differences; popular configurations reduce overestimation degree; extreme events yield large RMSEs and negative CCs.
  - [corpus] Corpus papers do not validate these atmospheric-science bias conclusions; they are internal to this study and require domain expert scrutiny.
- Break condition: Systematic reporting biases in the literature could distort aggregated patterns.

## Foundational Learning

- Concept: Prompt engineering for slot-filling extraction
  - Why needed here: Accuracy depends on clear, constrained questions and enumerated answer options.
  - Quick check question: Can you rewrite an open-ended "How did the model perform?" into a set of mutually exclusive, enumerable slots?

- Concept: Validation by stratified manual review
  - Why needed here: The paper's claims rest on 300-paper human–LLM comparison; understanding sampling and error taxonomy is essential.
  - Quick check question: How would you sample papers to ensure coverage across years, venues, and scheme types?

- Concept: Domain-specific metric normalization
  - Why needed here: RMSE in mm/day and correlation coefficients are used; units and definitions must align across studies for aggregation.
  - Quick check question: If some papers report NRMSE or ETS instead of RMSE/CC, how would you handle inclusion?

## Architecture Onboarding

- Component map: Web of Science/Scopus queries -> DOI deduplication -> language/type filtering -> PDFMiner text extraction -> GPT-4 Turbo with 8-question prompt -> structured tables -> manual validation -> bias aggregation

- Critical path: Prompt design -> small-scale manual validation -> full extraction -> bias/metric aggregation. Errors in prompt phrasing propagate to all downstream conclusions.

- Design tradeoffs: Low temperature (0.12) vs. open-ended tasks; deterministic outputs at potential creativity cost. Inclusion thresholds (≥10 uses per scheme) improve robustness but exclude niche schemes. Handling tables/figures: LLM extraction incomplete for RMSE/CC when metrics appear in non-text formats; manual supplementation was used.

- Failure signatures: Hallucinated schemes or metrics not present in the paper. Empty or "Nah" responses when information is present but not recognized. Incomplete multi-value extraction. Region extraction errors when domain is only shown on a map.

- First 3 experiments:
  1. Replicate extraction on 50 papers from the dataset with the same prompts; compute per-question accuracy against manual labels.
  2. Ablate temperature (0.0, 0.12, 0.5) on a 100-paper sample; measure hallucination rate and completeness.
  3. For a single scheme (e.g., WSM6), manually verify regional bias classifications in 30 papers; compare to LLM labels and quantify systematic misclassification patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific physical causes underlying the systematic tendency of most microphysics parameterizations to overestimate precipitation?
- Basis in paper: [explicit] The conclusion states, "although our study highlights the tendency for overestimation in current microphysics parameterizations, we were unable to confirm the exact causes within this paper."
- Why unresolved: While the LLM successfully identified the existence and regional distribution of biases through text analysis, it could not synthesize the complex, quantitative physical mechanisms responsible for these errors from the text alone.
- What evidence would resolve it: Targeted sensitivity experiments or a meta-analysis of model output data linking specific process rates to the observed overestimation patterns.

### Open Question 2
- Question: Do the physical explanations for precipitation bias found in isolated case studies generalize to the broader body of literature?
- Basis in paper: [explicit] Page 27 asks, "whether these kinds of studies can generalize to a larger set of publications to explain the tendency for different microphysics parameterizations to overestimate precipitation is unclear."
- Why unresolved: Existing explanations for overestimation are derived from individual case studies; this review did not extract sufficient mechanistic data to confirm if these reasons apply globally across the 2,699 publications analyzed.
- What evidence would resolve it: A systematic extraction of reported microphysical process contributions from the literature, or a large-scale analysis of raw model diagnostics across diverse climatic regions.

### Open Question 3
- Question: Can this automated LLM-based methodology be effectively adapted to analyze systematic biases in other climate model parameters or distinct scientific domains?
- Basis in paper: [explicit] The authors suggest the method "could be used... to investigate biases in other types of model parameters and analyze the performance of different climate models."
- Why unresolved: This study validated the approach only for WRF microphysics; its efficacy and accuracy when applied to other complex parameters or different fields remain unproven.
- What evidence would resolve it: Replication of this methodology on a separate large-scale literature corpus, such as hydrological modeling or cumulus parameterization studies, achieving similar extraction accuracy.

## Limitations

- The 94% extraction accuracy is based on manual validation of only 300 papers, which may not fully represent edge cases in the entire 2,699-paper corpus
- Systematic bias conclusions depend on aggregating heterogeneous studies with varying configurations and domains, introducing potential uncontrolled variance
- Performance drops significantly when metrics appear only in tables or figures rather than extractable text, with ~23% incomplete retrieval for RMSE/CC values

## Confidence

- **High confidence**: The technical approach of using GPT-4 Turbo with low temperature for structured extraction, the 94% validation accuracy on the sampled papers, and the identification of the WSM6 and Thompson schemes as most commonly used.
- **Medium confidence**: The aggregated bias patterns showing seven of nine schemes tend to overestimate precipitation, as this depends on combining heterogeneous studies with potential publication biases.
- **Medium confidence**: The regional bias variations, as these are derived from classifying studies by geographic domain with potential extraction errors in region identification.

## Next Checks

1. Replicate the extraction on a stratified sample of 50 papers from different years and venues to verify the 94% accuracy claim holds across the full corpus.
2. Conduct an ablation study comparing extraction accuracy at different temperature settings (0.0, 0.12, 0.5) on 100 papers to optimize the balance between determinism and completeness.
3. Manually verify the regional bias classifications for a single microphysics scheme (e.g., WSM6) across 30 papers to quantify systematic misclassification patterns in geographic domain extraction.