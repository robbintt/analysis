---
ver: rpa2
title: A Survey on Agentic Multimodal Large Language Models
arxiv_id: '2510.10991'
source_url: https://arxiv.org/abs/2510.10991
tags:
- arxiv
- reasoning
- preprint
- agentic
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically explores the emerging paradigm of Agentic
  Multimodal Large Language Models (Agentic MLLMs), which represent a shift from static,
  passive AI agents to dynamic, proactive, and generalizable systems. The paper categorizes
  Agentic MLLMs into three fundamental dimensions: (i) Agentic internal intelligence,
  enabling reasoning, reflection, and memory for accurate long-horizon planning; (ii)
  Agentic external tool invocation, extending problem-solving beyond intrinsic knowledge
  through search, code, and visual processing; and (iii) Agentic environment interaction,
  allowing actions and adaptation in virtual and physical environments.'
---

# A Survey on Agentic Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2510.10991
- Source URL: https://arxiv.org/abs/2510.10991
- Reference count: 40
- Primary result: Systematic exploration of Agentic MLLMs across three dimensions (internal intelligence, external tools, environment interaction) with compiled frameworks, datasets, and applications

## Executive Summary
This survey systematically explores the emerging paradigm of Agentic Multimodal Large Language Models (Agentic MLLMs), which represent a shift from static, passive AI agents to dynamic, proactive, and generalizable systems. The paper categorizes Agentic MLLMs into three fundamental dimensions: (i) Agentic internal intelligence, enabling reasoning, reflection, and memory for accurate long-horizon planning; (ii) Agentic external tool invocation, extending problem-solving beyond intrinsic knowledge through search, code, and visual processing; and (iii) Agentic environment interaction, allowing actions and adaptation in virtual and physical environments. It also compiles open-source training frameworks, datasets, and evaluation benchmarks, and reviews downstream applications including Deep Research, Embodied AI, Healthcare, GUI Agents, Autonomous Driving, and Recommender Systems. Finally, the survey identifies key challenges and future directions for the field.

## Method Summary
The paper synthesizes existing research on Agentic MLLMs through systematic literature review, categorizing approaches into three dimensions and compiling relevant resources. The survey identifies a three-stage training pipeline: (1) Agentic CPT (continual pre-training on synthetic corpora), (2) Agentic SFT (cold-start fine-tuning on high-quality reasoning trajectories), and (3) Agentic RL (reinforcement learning using GRPO/PPO with outcome or process rewards). The authors provide comprehensive tables of training frameworks, datasets, and evaluation benchmarks while reviewing specific implementations and their architectural choices.

## Key Results
- Agentic MLLMs shift from static workflows to dynamic policy optimization, enabling adaptive decision-making
- External tool invocation extends problem-solving capacity beyond parametric knowledge limits
- Long-horizon planning relies on the triad of reasoning, reflection, and memory mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Agentic capability emerges when a model shifts from passive sequence prediction to active policy optimization within a dynamic decision process.
- **Mechanism:** The system replaces static, hand-crafted workflows ($f_T \circ \dots \circ f_1$) with a learned policy $\pi(a|s_t)$ that selects actions based on the current state. This allows the model to adapt its strategy dynamically rather than following a fixed prompt pipeline.
- **Core assumption:** The state representation effectively captures the environment and history, and the reward signal is sufficiently dense or well-shaped to guide the policy toward long-horizon goals.
- **Evidence anchors:**
  - [Section 2.2.1] Describes the shift from static workflows to dynamic state transitions $s_{t+1} = \delta(s_t, a_t)$.
  - [Section 2.2.2] Formulates action selection as sampling from a policy $\pi(a|s_t)$.
  - [Corpus] The survey "The Landscape of Agentic Reinforcement Learning for LLMs" reinforces this paradigm shift from generators to decision-makers.
- **Break condition:** If the state space is poorly defined or the reward signal is sparse/noisy, the policy may fail to converge or exhibit "reward hacking."

### Mechanism 2
- **Claim:** External tool invocation extends the model's problem-solving capacity by offloading knowledge retrieval and computation to specialized external modules.
- **Mechanism:** The MLLM acts as a controller that outputs specific tokens (e.g., `<search>`, `<code>`) to invoke tools like search engines or code interpreters. This overcomes the "parametric knowledge limit" by fetching real-time information or performing precise calculations.
- **Core assumption:** The model can reliably map abstract task requirements to specific tool calls and interpret the tool's output correctly.
- **Evidence anchors:**
  - [Abstract] Highlights "Agentic external tool invocation" as a key dimension for extending capabilities beyond intrinsic knowledge.
  - [Section 4.2] Details how agentic search and coding allow models to solve problems requiring up-to-date information or complex computation.
  - [Section 3.2] Describes how actions are embedded via "Specific Tokens" or "Unified Tokens."
- **Break condition:** If the tool outputs are noisy, the model may misinterpret the context, or if the action space is too large, the model may fail to learn when to invoke which tool.

### Mechanism 3
- **Claim:** Robust long-horizon planning relies on a triad of internal intelligence: reasoning to plan, reflection to correct, and memory to maintain context.
- **Mechanism:** The model generates a chain of thought (Reasoning), critiques intermediate steps to detect errors (Reflection), and stores/retrieves relevant history (Memory). This internal loop stabilizes the policy over long trajectories.
- **Core assumption:** The "Reflection" mechanism can effectively identify errors in the reasoning chain before they compound, and memory systems can reliably compress or store relevant context.
- **Evidence anchors:**
  - [Section 4.1] Defines Internal Intelligence as the "system's commander" comprising reasoning, reflection, and memory.
  - [Section 4.1.2] Notes that reflection mitigates the autoregressive limitation where "errors are irreversible and tend to accumulate."
  - [Corpus] "From Perception to Cognition" supports the need for coherent integration of perception and reasoning.
- **Break condition:** If the reflection mechanism produces false positives (critiquing correct steps) or if memory retrieval fails to surface critical historical data, the agent will hallucinate or lose coherence.

## Foundational Learning

- **Concept:** Reinforcement Learning (RL) Fundamentals (Policy, Reward, Value)
  - **Why needed here:** The paper explicitly frames Agentic MLLMs via RL equations (e.g., maximizing cumulative reward in Eq. 5). You cannot understand the training loop without grasping policy gradients or PPO/GRPO.
  - **Quick check question:** Can you explain the difference between "Outcome Reward" (final answer) and "Process Reward" (intermediate steps) in the context of training a visual reasoning model?

- **Concept:** Multimodal Large Language Model (MLLM) Architectures (Dense vs. MoE)
  - **Why needed here:** The choice of base model (e.g., Dense vs. Mixture-of-Experts) affects how the agent handles tool use and reasoning (Section 3.1). MoE models are increasingly preferred for agentic tasks due to their capacity.
  - **Quick check question:** How does the "gating network" in a MoE model differ from the dense feed-forward layers in standard LLMs, and why might this benefit an agent with diverse tools?

- **Concept:** Markov Decision Processes (MDP)
  - **Why needed here:** The paper formalizes the agent's interaction as an MDP (Section 2.2). Understanding state transitions and action spaces is required to design the environment interface.
  - **Quick check question:** In the context of a GUI agent, what constitutes the "State" ($s_t$), the "Action" ($a_t$), and the "Environment" ($E$)?

## Architecture Onboarding

- **Component map:** Base MLLM -> Action Tokenizer (Specific/Unified) -> External Tools (Search, Code) + Memory Bank -> Environment (Virtual/Physical) -> Reward Signal

- **Critical path:**
  1. **Foundational MLLM Selection:** Choose a base model with strong perception capabilities
  2. **Action Space Definition:** Define tokens for tool calls (e.g., `<search>`)
  3. **Cold Start SFT:** Fine-tune on high-quality trajectory data (Section 3.4) to teach the model *how* to use tools
  4. **Agentic RL:** Apply PPO or GRPO (Section 3.5) with outcome/process rewards to optimize the policy for autonomy

- **Design tradeoffs:**
  - **Dense vs. MoE:** MoE offers better scaling for diverse tools but requires complex routing; Dense is simpler but may lack capacity
  - **Unified vs. Specific Tokens:** Unified tokens (JSON-like) are flexible; Specific tokens (`<action_1>`) are more rigid but potentially easier to parse
  - **Outcome vs. Process Reward:** Outcome rewards are easy to scale but prone to "advantage vanishing"; Process rewards offer finer guidance but require expensive supervision (Section 4.1.1)

- **Failure signatures:**
  - **Action Looping:** The model repeatedly calls the same tool without progress (suggests poor state representation)
  - **Reward Hacking:** The model exploits the reward function (e.g., formatting the answer correctly without solving the task)
  - **Context Amnesia:** The agent ignores previous steps in long-horizon tasks (suggests memory compression failure)

- **First 3 experiments:**
  1. **Tool-Use SFT:** Fine-tune a small MLLM on a dataset like "LLaVA-CoT-100K" (Table 5) to verify it can output valid action tokens
  2. **GRPO for Search:** Implement Group Relative Policy Optimization (Eq. 16) to train an agent to perform multi-turn web search (Section 4.2.1), monitoring for reward stability
  3. **Reflection Ablation:** Compare standard RL against a setup with explicit "rethinking triggers" (Section 4.1.2) to quantify the value of reflection in error recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of multi-turn reasoning and tool invocation in agentic MLLMs be reduced to enable real-time, large-scale deployment?
- Basis in paper: [explicit] Section 7.2 highlights that iterative processes impose significant costs, sometimes taking up to thirty minutes per task, and notes that research on accelerating tool invocation specifically remains limited.
- Why unresolved: Current acceleration techniques primarily target text-centric reasoning chains, leaving the latency and energy costs of dynamic, multi-step tool execution largely unaddressed.
- What evidence would resolve it: The development of training or inference frameworks that achieve competitive task performance with significantly lower latency and FLOPs compared to current agentic baselines.

### Open Question 2
- Question: How can persistent memory architectures be designed to support scalable, selective, and multimodal long-term experience accumulation?
- Basis in paper: [explicit] Section 7.3 observes that current memory mechanisms are largely text-centric and constrained in effective length, failing to handle the vast multimodal streams required for long-term continuity.
- Why unresolved: Existing systems struggle to filter, compress, and retrieve relevant multimodal information over extended time horizons without catastrophic forgetting or loss of context.
- What evidence would resolve it: A model demonstrating superior performance on tasks requiring the retrieval and reasoning of multimodal cues from interactions occurring months or years in the past.

### Open Question 3
- Question: What rigorous frameworks are required to ensure the safety and controllability of agentic MLLMs that dynamically generate action sequences and interact with physical environments?
- Basis in paper: [explicit] Section 7.5 argues that the dynamic nature of agentic systems—specifically their ability to call external APIs or physical devices—amplifies risks like unintended consequences and adversarial inputs.
- Why unresolved: Static safety benchmarks are insufficient for systems where ambiguous inputs can propagate across modalities and destabilize physical actions or tool usage.
- What evidence would resolve it: The establishment of adversarial stress-testing benchmarks and normative frameworks that successfully prevent hazardous tool calls or physical actions in open-world, multi-turn scenarios.

## Limitations
- Broad scope rather than depth in specific implementation details and hyperparameter choices
- Lack of standardized, comprehensive evaluation protocols for agentic capabilities
- Does not address safety implications of autonomous tool invocation and physical environment interaction

## Confidence
- **High Confidence:** Categorization framework (internal intelligence, external tools, environment interaction) and three-stage training pipeline (CPT → SFT → RL)
- **Medium Confidence:** Effectiveness of reflection mechanisms and memory systems for long-horizon planning
- **Low Confidence:** Specific performance improvements from architectural choices (Unified vs. Specific tokens, Dense vs. MoE) due to limited systematic comparisons

## Next Checks
1. **GRPO Advantage Stability Test:** Implement GRPO across three different datasets and monitor advantage variance to quantify "advantage vanishing" frequency and mitigation effectiveness
2. **Reflection Mechanism Ablation:** Compare standard GRPO against variants with explicit reflection triggers on long-horizon visual reasoning tasks to measure error recovery impact
3. **Unified vs. Specific Token Performance:** Implement both action token schemes on the same base model and task to measure differences in parsing accuracy, training stability, and end-task performance