---
ver: rpa2
title: Stochastic Shortest Path with Sparse Adversarial Costs
arxiv_id: '2511.00637'
source_url: https://arxiv.org/abs/2511.00637
tags:
- regret
- sparsity
- bound
- vpsr
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies adversarial stochastic shortest path (SSP)\
  \ problems with sparse costs under full-information feedback. Existing methods using\
  \ Online Mirror Descent (OMD) with negative-entropy regularization incur regret\
  \ scaling with \u221A(logSA), where SA is the state-action space size, failing to\
  \ exploit sparsity when only M << SA state-action pairs incur cost."
---

# Stochastic Shortest Path with Sparse Adversarial Costs

## Quick Facts
- **arXiv ID:** 2511.00637
- **Source URL:** https://arxiv.org/abs/2511.00637
- **Reference count:** 40
- **Primary result:** Proposes ℓr-norm regularizers that achieve optimal regret scaling with √(log M) instead of √(log SA) for sparse adversarial SSP, while proving this is optimal and showing unknown transitions limit sparsity benefits.

## Executive Summary
This paper addresses adversarial stochastic shortest path (SSP) problems with sparse cost structures. Standard Online Mirror Descent methods using negative entropy regularization incur regret scaling with √(log SA), failing to exploit when only M << SA state-action pairs have non-zero costs. The authors propose a family of ℓr-norm regularizers (r∈(1,2)) that provably achieve regret scaling with √(log M), capturing the effective dimension of the problem. They establish this is optimal through matching lower bounds. However, they also show that in the unknown transition setting, the benefits of sparsity are limited: minimax regret remains polynomial in SA even for sparse problems.

## Method Summary
The paper proposes Online Mirror Descent with a family of ℓr-norm regularizers (ψ_p(q) = (p/(p+1))‖q‖^(1+1/p)) for sparse adversarial SSP. The method maintains occupancy measures q and updates them via OMD with step-size η = √(pT^(1+1/p)/(KDM^(1/p))). The regularizer parameter p = log(TM) is chosen to balance stability and sparsity adaptation. A sparse-agnostic batching strategy avoids requiring explicit knowledge of M. The approach requires solving a convex projection onto the feasible set of proper occupancy measures Δ(T), with details provided in the appendix.

## Key Results
- Negative entropy regularization provably incurs √(log S) regret even when M=3, failing to adapt to sparsity
- ℓr-norm regularizers achieve optimal √(log M) regret scaling, matching a lower bound
- In unknown transition settings, minimax regret scales polynomially with SA (Ω(√(SAK))) even for M=1
- M acts as the effective dimension in known transitions but not in unknown transitions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Negative entropy regularization fails to adapt to sparse costs because it induces a "skewed" initial policy that cannot easily correct itself.
- **Mechanism:** In SSP, flow constraints force the initial occupancy measure q₁ to distribute mass unevenly across states to satisfy hitting time constraints. Negative entropy applies heavy penalties near zero, creating a gradient that prevents the policy from moving probability mass away from initially dominant (but potentially suboptimal) actions quickly enough when costs are sparse.
- **Core assumption:** Using OMD with negative entropy on SSP instances with specific state-transition structures.
- **Evidence anchors:** Abstract shows negative entropy incurs √(log S) regret; Section 3 describes the failure as stemming from Euclidean distance stretching near the boundary; corpus papers discuss OMD generally but not this specific SSP geometry failure.
- **Break condition:** If costs are dense (M≈SA), the negative entropy penalty is appropriate and no recovery is needed.

### Mechanism 2
- **Claim:** ℓr-norm regularizers (r∈(1,2)) achieve optimal √(log M) regret by allowing easier movement of probability mass near the boundary.
- **Mechanism:** The proposed regularizer ψ_p interpolates between squared Euclidean norm and negative entropy. By tuning p, the gradient remains finite at the boundary, preventing the "trapping" effect of negative entropy and allowing aggressive shifts of probability mass away from sparse costs.
- **Core assumption:** Transition dynamics are known and M is the sparsity level.
- **Evidence anchors:** Abstract shows proposed method achieves √(log M) scaling; Section 4 explains low p makes updates robust to skewed initial occupancy; corpus papers discuss Hedge in combinatorial settings but not ℓr-norm regularizers for SSP sparsity.
- **Break condition:** If step-size η is not tuned correctly relative to M and T*, the stability/penalty trade-off breaks.

### Mechanism 3
- **Claim:** Sparsity cannot be leveraged to reduce dimension dependence in the unknown transition setting because the learner must explore the full state-action space.
- **Mechanism:** Even with sparse costs (M=1), an agent without transition knowledge cannot distinguish "low cost" from "impossible transition" without sampling, so regret lower bound remains polynomial in SA.
- **Core assumption:** Agent has no prior knowledge of transition matrix P.
- **Evidence anchors:** Abstract states minimax regret scales polynomially with SA; Section 5 proves lower bound Ω(D√(SAK)) even for M=1; corpus neighbors focus on regret bounds but don't isolate this unknown transition + sparsity interaction.
- **Break condition:** If agent is given transition model, this barrier is lifted.

## Foundational Learning

- **Concept: Online Mirror Descent (OMD)**
  - **Why needed here:** The entire framework is built on modifying OMD. You must understand Bregman divergences and how the choice of regularizer ψ determines the "geometry" of policy updates.
  - **Quick check question:** How does the choice of regularizer (Euclidean vs. Entropy) affect the update step when policy is near the boundary of the probability simplex?

- **Concept: SSP Occupancy Measures**
  - **Why needed here:** The paper converts the RL problem of finding a policy into a linear optimization problem over occupancy measures q. Sparsity adaptation is analyzed on q, not directly on π.
  - **Quick check question:** Can you map a given occupancy measure vector q back to a valid stationary policy π(a|s)?

- **Concept: Sparsity in Online Learning**
  - **Why needed here:** To distinguish between "worst-case" dimension SA and "effective" dimension M. The paper's contribution is explicitly improving regret bounds based on this M.
  - **Quick check question:** In the "Experts" setting (S=1), how does sparsity typically affect minimax regret compared to standard setting?

## Architecture Onboarding

- **Component map:** Cost vectors c_k, Transition function P (known) -> OMD engine using ℓr-norm regularizer ψ_p -> Convex optimization projection onto Δ(T) -> Occupancy measure q_k -> Policy π_k

- **Critical path:** Implementing the OMD update with ℓr-norm regularizer requires solving the dual problem for the projection step (detailed in Appendix B), where the "sparse-aware" geometry is enforced.

- **Design tradeoffs:**
  - Choice of p: Low p handles sparsity better but may be less stable than high p (entropy) on dense problems
  - Batching: Sparse-agnostic batching strategy adds a small log-log factor to regret

- **Failure signatures:**
  - Negative Entropy Trap: Using standard Entropy regularizers on sparse SSP instance incurs √(log S) regret instead of adapting to smaller sparsity level
  - Unknown Transition Stagnation: In unknown transition setting, expect slow convergence (regret ~ √(SA)) even with sparse costs

- **First 3 experiments:**
  1. Validation of Theorem 3.1: Construct "skewed" MDP from Appendix A. Run standard OMD vs. proposed ℓr-norm OMD. Verify negative entropy incurs √(log S) regret while proposed method scales with M.
  2. Sparsity Scaling: Generate random SSPs with fixed S, A but varying sparsity levels M. Plot regret vs. M for proposed algorithm to confirm √(log M) scaling.
  3. Unknown Transition Probe: Run algorithm in unknown transition environment (Tabular RL setup) with sparse costs. Compare regret against known-transition baseline to confirm "polynomial gap" described in Section 5.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does minimax regret for sparse SSP under bandit feedback scale with √(M) rather than √(SA)?
  - **Basis in paper:** In "Related works" section, authors state: "It is an interesting future direction to study the sparse SSP problem with bandit feedback and understand if the regret scales with √(M) instead of √(SA)."
  - **Why unresolved:** Current study is restricted to full-information feedback; bandit setting presents distinct exploration-exploitation challenges not addressed by proposed regularizers.
  - **What evidence would resolve it:** Algorithm for bandit setting achieving regret ~ O(√(DKT*M)) or matching lower bound proving necessity of √(SA) dependence.

- **Open Question 2:** How does minimax regret behave in the "high-dimensional" regime where K is small relative to T*?
  - **Basis in paper:** In Section 4.1, authors note: "The high-dimensional problem is yet to be explored... and could be an interesting avenue of future research."
  - **Why unresolved:** Current results rely on assumption K ≥ T*, leaving performance guarantees undefined for small K.
  - **What evidence would resolve it:** Regret analysis valid when K << T*, potentially identifying different dependence on sparsity level M in this regime.

- **Open Question 3:** Are there specific structural properties of an MDP that enable polynomial improvements in regret from sparsity?
  - **Basis in paper:** Conclusion states: "There could be structural properties of an MDP that could break this logarithmic limit and achieve polynomial benefits."
  - **Why unresolved:** Paper proves logarithmic improvement is optimal in worst case but suggests under specific structural conditions the effective dimension might scale differently.
  - **What evidence would resolve it:** Identification of structured MDP class and associated algorithm achieving regret scaling polynomially with sparsity (e.g., M/A) rather than √(log M).

## Limitations

- Analysis relies on known transition dynamics, limiting applicability to real-world RL scenarios
- Theoretical improvements depend on precise knowledge of sparsity level M for optimal tuning
- Proof techniques are highly specialized to SSP structure and may not generalize to other RL settings
- Unknown transition setting shows sparsity benefits are limited, suggesting practical constraints

## Confidence

- **High Confidence:** Theorem 3.1 (negative entropy failure) - well-constructed counterexample with clear geometric intuition
- **High Confidence:** Theorem 4.2 (proposed algorithm guarantee) - rigorous analysis with matching lower bound
- **Medium Confidence:** Section 5 (unknown transitions) - valid theoretical result but relies on worst-case assumptions
- **Medium Confidence:** Algorithmic implementation details - some projection step specifics are deferred to appendices

## Next Checks

1. **Robustness to noise:** Test proposed algorithm under noisy or approximate transition knowledge to understand gap between known and unknown transition settings
2. **Hyperparameter sensitivity:** Systematically vary regularizer parameter p around theoretical choice to identify stability-accuracy tradeoff
3. **Scaling experiments:** Evaluate regret scaling on larger state-action spaces with varying sparsity levels to verify theoretical predictions hold empirically