---
ver: rpa2
title: 'Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization'
arxiv_id: '2602.00827'
source_url: https://arxiv.org/abs/2602.00827
tags:
- learning
- generalization
- training
- feature
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how feature learning strength (FLS) affects generalization
  in deep networks. Empirically, we find that excessively large FLS can hurt generalization,
  and an intermediate optimal FLS emerges, contrary to prevailing intuition.
---

# Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization

## Quick Facts
- arXiv ID: 2602.00827
- Source URL: https://arxiv.org/abs/2602.00827
- Authors: Taesun Yeom; Taehyeok Ha; Jaeho Lee
- Reference count: 40
- Primary result: Intermediate feature learning strength minimizes generalization error by balancing over-alignment and over-fitting

## Executive Summary
This work reveals that excessively large or small feature learning strength (FLS) can degrade generalization in deep networks, with an intermediate optimal FLS emerging from a trade-off between over-alignment and over-fitting effects. Empirically, VGG and ResNet architectures on CIFAR-100 and BigGAN data show performance gaps up to 6% when FLS deviates from the optimum. Theoretically, gradient flow analysis of two-layer ReLU networks with logistic loss demonstrates that large FLS causes weights to align with empirical class means rather than true signals (over-alignment), while small FLS restricts feature adaptation (over-fitting).

## Method Summary
The study controls FLS through initialization scale α in theoretical analysis and output multipliers c in empirical experiments, with Proposition B.1 establishing their equivalence. Theoretical analysis tracks two-phase training dynamics: Phase 1 neuron alignment followed by Phase 2 margin maximization. The excess error decomposition E(ŵα) - E* = OA(α) + OF(α) quantifies the trade-off, where OA decreases with α (better alignment at small α) and OF increases with α (better feature learning at large α). Empirical validation uses VGG and ResNet architectures on CIFAR-100 and synthetic BigGAN data with controlled effective dimensions.

## Key Results
- Excessively large FLS induces over-alignment where effective predictors deviate from Bayes-optimal directions
- Excessively small FLS leads to over-fitting by restricting feature adaptation to kernel regime
- Intermediate optimal FLS emerges from trade-off between over-alignment and over-fitting terms
- Performance gaps up to 6% observed when operating away from optimal FLS
- Larger gains from FLS tuning in higher-dimensional tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excessively large FLS causes effective predictor to deviate from Bayes-optimal direction
- Mechanism: Small initialization scale α (large FLS) forces Phase 1 alignment toward empirical class mean x+ rather than true signal s+, creating constrained predictor cone that cannot reach optimal direction. Angular deviation scales as O(√α), so smaller α causes larger deviation.
- Core assumption: Training data satisfies orthogonal separability
- Evidence anchors: Abstract states over-alignment degrades generalization; Corollary 5.3 bounds angle between x+ and weights by O(√α)
- Break condition: When Ψ(tη,α) ≤ cos(ϕ), over-alignment term becomes zero

### Mechanism 2
- Claim: Excessively small FLS leads to over-fitting by constraining model to kernel regime
- Mechanism: Large initialization scale α (small FLS) pushes training toward kernel regime. While alignment improves, over-fitting term OF(α) increases because uniform deviation bound g(α) grows with α, reflecting reduced feature adaptability.
- Core assumption: Logistic loss with Lipschitz property and concentration inequalities
- Evidence anchors: Abstract identifies over-fitting from small FLS; Theorem 5.6 shows OF(α) includes increasing function g(α)
- Break condition: Training may not converge to near-zero loss if FLS too small

### Mechanism 3
- Claim: Intermediate optimal FLS emerges from OA/OF trade-off
- Mechanism: OA(α) decreases with α while OF(α) increases with α, creating U-shaped excess error curve. Sum OA(α) + OF(α) yields data-dependent optimum.
- Core assumption: Gaussian mixture data model with separability parameter κ and noise level σ
- Evidence anchors: Abstract describes OA/OF trade-off; Theorem 5.6 and Equation 24 decompose excess error; Figure 14 demonstrates U-shaped curve
- Break condition: Optimal FLS depends on data dimensionality, noise, and sample size

## Foundational Learning

- Concept: Two-phase training dynamics for ReLU networks
  - Why needed here: Analysis hinges on Phase 1 alignment determining direction inherited by Phase 2 margin maximization
  - Quick check question: Can you explain why neuron alignment occurs before significant loss decrease in strongly feature-learning regimes?

- Concept: Feature learning strength via initialization scaling
  - Why needed here: Understanding FLS ∝ 1/α connects initialization scale to kernel/feature-learning spectrum
  - Quick check question: Why does scaling output by c and adjusting learning rate to η/c produce equivalent dynamics to initializing with scale α?

- Concept: Excess error decomposition
  - Why needed here: OA/OF decomposition provides mechanistic explanation for why intermediate FLS is optimal
  - Quick check question: What does each term (OA, OF) measure, and how do their dependencies on α differ?

## Architecture Onboarding

- Component map: Initialization scale α or output multiplier c -> FLS controller -> Two-phase training dynamics -> Effective predictor ŵα(t) -> Angular alignment Ψ(t) -> Excess error decomposition

- Critical path: Initialize weights with scale α or apply output multiplier c -> Train until training risk reaches target η -> Measure alignment Ψ at stopping time tη,α -> Compute/decompose excess error into OA and OF components

- Design tradeoffs:
  - Small α (large FLS): Strong alignment to empirical direction, risk of over-alignment to wrong direction
  - Large α (small FLS): Better direction alignment, but over-fitting due to limited feature adaptation
  - Higher task dimensionality: Optimal FLS tuning yields larger gains

- Failure signatures:
  - Training fails to reach 99% accuracy: α too large (FLS too small)
  - Test accuracy degrades despite perfect training: α too small (over-alignment)
  - Large variance across seeds: Operating near phase transition between regimes

- First 3 experiments:
  1. Replicate Figure 2: Train VGG/ResNet on CIFAR-100 with output multipliers c ∈ {2⁻¹⁰, ..., 2²} and learning rates η/c. Identify optimal FLS cell.
  2. Validate Phase 2 alignment stability: Train two-layer ReLU network on Gaussian mixture, measure Ψ(tη,α) at different stopping thresholds η. Confirm Ψ ≈ Ψ(tα).
  3. Dimensionality sweep: Generate BigGAN data with effective dimensions 32/64/128, measure test accuracy gap from optimal FLS. Verify larger gaps at higher dimensions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can optimal FLS identified in smaller models be systematically transferred to larger-scale models to improve hyperparameter efficiency?
- Basis in paper: Authors explicitly suggest developing framework for transferring optimal FLS to larger scale models
- Why unresolved: Demonstrate optimal FLS exists empirically but provide no framework for scaling relationships
- What evidence would resolve it: Experiments showing predictable scaling relationships between optimal FLS in small vs. large models

### Open Question 2
- Question: How do stochastic optimization (SGD noise) and explicit regularization interact with over-alignment vs. over-fitting trade-off?
- Basis in paper: Results do not capture effects of stochastic/adaptive gradient methods, data augmentation, and other practical techniques
- Why unresolved: Theory uses deterministic gradient flow without regularization, which may alter observed FLS trade-off
- What evidence would resolve it: Empirical studies measuring how optimal FLS shifts under varying SGD batch sizes, weight decay strengths, and augmentation intensities

### Open Question 3
- Question: Does optimal FLS phenomenon persist when orthogonal separability assumption is relaxed to more realistic data distributions?
- Basis in paper: Theoretical framework relies on strict constraint on empirical distribution (orthogonal separability)
- Why unresolved: Alignment analysis in Phase 1 dynamics critically depends on orthogonal separability; behavior under correlated, overlapping, or adversarially structured data untested
- What evidence would resolve it: Numerical experiments on synthetic mixtures with controlled violations of orthogonal separability

## Limitations

- Theoretical analysis assumes Gaussian mixture data and orthogonal separability, which may not capture complex real-world distributions
- Two-phase dynamics analysis relies on early stopping at fixed training risk η, but optimal stopping criteria in practice remain unclear
- Corollary 5.3 shows O(√α) over-alignment bound but numerical constant's dependence on task difficulty not fully characterized

## Confidence

- High confidence: Empirical observation that intermediate FLS improves generalization across architectures and datasets
- Medium confidence: Theoretical mechanism linking large FLS to over-alignment away from Bayes-optimal direction
- Medium confidence: OA/OF decomposition framework, though precise constants depend on data geometry

## Next Checks

1. Test FLS-optimal training across different loss functions (cross-entropy vs logistic) to verify mechanism generality
2. Extend analysis to multi-class problems beyond binary Gaussian mixture setup
3. Investigate whether adaptive optimizers (Adam) modify FLS-generalization relationship compared to SGD