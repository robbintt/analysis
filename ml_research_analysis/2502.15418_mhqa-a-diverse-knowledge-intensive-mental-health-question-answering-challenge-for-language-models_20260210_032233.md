---
ver: rpa2
title: 'MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge
  for Language Models'
arxiv_id: '2502.15418'
source_url: https://arxiv.org/abs/2502.15418
tags:
- question
- mental
- health
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MHQA, a large-scale mental health question-answering
  dataset derived from PubMed abstracts, targeting four key domains: anxiety, depression,
  trauma, and obsessive-compulsive disorders. The dataset contains ~58.6k QA pairs
  with four answer choices each, including a gold-standard subset of 2,475 expert-verified
  instances.'
---

# MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge for Language Models

## Quick Facts
- **arXiv ID**: 2502.15418
- **Source URL**: https://arxiv.org/abs/2502.15418
- **Reference count**: 12
- **Primary result**: Introduces a 58.6k QA dataset covering anxiety, depression, trauma, and OCD with four question types and expert-verified subset

## Executive Summary
This paper introduces MHQA, a large-scale mental health question-answering dataset derived from PubMed abstracts, targeting four key domains: anxiety, depression, trauma, and obsessive-compulsive disorders. The dataset contains ~58.6k QA pairs with four answer choices each, including a gold-standard subset of 2,475 expert-verified instances. Questions span four types—factoid, diagnostic, prognostic, and preventive—to assess diverse reasoning skills. Using a rigorous LLM-based pipeline with post-hoc validation, the dataset ensures high-quality, medically grounded questions. Benchmarking on models like GPT-4o and BioBERT shows strong performance, with GPT-4o achieving up to 79.8% F1 score, highlighting the dataset's utility for evaluating and advancing mental health-focused language models.

## Method Summary
The MHQA dataset was constructed using a multi-stage LLM-based pipeline starting from PubMed abstracts. The process involved generating questions across four mental health domains (anxiety, depression, trauma, OCD) with four distinct question types: factoid, diagnostic, prognostic, and preventive. The pipeline included question generation, answer choice creation, and quality validation through both automated checks and expert review. A subset of 2,475 instances received expert verification to establish quality benchmarks. The dataset design emphasizes diversity in question types while maintaining medical accuracy and relevance to clinical mental health practice.

## Key Results
- GPT-4o achieved up to 79.8% F1 score on MHQA benchmark
- Dataset contains 58,600 QA pairs across four mental health domains
- 2,475 expert-verified gold-standard instances ensure quality
- Questions span four types: factoid, diagnostic, prognostic, and preventive

## Why This Works (Mechanism)
The dataset's effectiveness stems from its diverse question types that target different cognitive and clinical reasoning skills. By incorporating factoid questions for knowledge recall, diagnostic questions for assessment capabilities, prognostic questions for outcome prediction, and preventive questions for intervention planning, the dataset comprehensively evaluates mental health language models. The PubMed abstract foundation ensures medical grounding while the multi-stage validation process maintains quality standards across the large-scale dataset.

## Foundational Learning
1. **Mental Health Domain Knowledge** (why needed: ensures questions are clinically relevant and accurate; quick check: verify domain coverage matches clinical practice standards)
2. **Question Type Differentiation** (why needed: enables assessment of diverse reasoning capabilities; quick check: confirm all four question types are represented proportionally)
3. **Medical Text Processing** (why needed: PubMed abstracts contain specialized terminology; quick check: validate terminology accuracy in generated questions)
4. **Quality Assurance in LLM Generation** (why needed: maintains dataset reliability at scale; quick check: review expert-verified subset for consistency)
5. **Multi-choice Question Design** (why needed: enables standardized evaluation; quick check: verify distractor quality and answer key accuracy)
6. **Clinical Reasoning Assessment** (why needed: measures practical mental health knowledge; quick check: evaluate alignment with clinical decision-making processes)

## Architecture Onboarding

**Component Map:**
PubMed abstracts -> LLM question generation -> Answer choice creation -> Automated validation -> Expert review -> MHQA dataset

**Critical Path:**
PubMed abstracts → LLM generation pipeline → Quality validation → Expert verification subset → Benchmarking

**Design Tradeoffs:**
The choice of PubMed abstracts provides medical accuracy but may bias toward academic language rather than clinical practice communication. LLM-based generation enables scale but requires extensive validation to ensure quality. The four-domain focus ensures depth but limits breadth of mental health coverage.

**Failure Signatures:**
Poor question quality may manifest as irrelevant distractors, overly technical language, or questions that don't align with clinical reasoning patterns. Performance inconsistencies across question types may indicate model weaknesses in specific reasoning domains.

**First 3 Experiments:**
1. Evaluate model performance across all four question types to identify reasoning strengths and weaknesses
2. Compare expert-verified subset performance versus full dataset to assess quality consistency
3. Test model generalization to clinical scenarios outside the PubMed abstract domain

## Open Questions the Paper Calls Out
None

## Limitations
- PubMed abstract source material may bias toward academic rather than clinical language
- Heavy reliance on LLM generation with limited human validation beyond the expert-verified subset
- Four-domain focus (anxiety, depression, trauma, OCD) represents narrow slice of mental health conditions

## Confidence
- Dataset quality and diversity: Medium
- Benchmark performance significance: Medium  
- Dataset utility for advancing mental health LLMs: Medium

## Next Checks
1. Conduct comprehensive error analysis on the full dataset to assess consistency and identify systematic biases in the LLM-generated questions
2. Evaluate model performance on clinical case scenarios and patient-facing communication tasks to test real-world applicability
3. Expand domain coverage to include additional mental health conditions and validate with diverse clinical experts beyond the current validation subset