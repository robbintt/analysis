---
ver: rpa2
title: Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories
  through the Bernstein Basis
arxiv_id: '2510.26607'
source_url: https://arxiv.org/abs/2510.26607
tags:
- wasserstein
- regression
- distance
- distributions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses distribution regression, where the goal is
  to model conditional distributions of vector-valued responses as smooth probabilistic
  trajectories. Existing methods either ignore the geometry of probability measures
  or are computationally expensive.
---

# Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis

## Quick Facts
- **arXiv ID**: 2510.26607
- **Source URL**: https://arxiv.org/abs/2510.26607
- **Reference count**: 17
- **Key outcome**: Wasserstein regression with Bernstein basis outperforms baselines on synthetic trajectory datasets while maintaining interpretability via explicit control points.

## Executive Summary
This paper proposes a novel method for distribution regression that models conditional distributions as smooth probabilistic trajectories using a mixture of Gaussians parameterized by Bernstein polynomials. The approach optimizes the average squared Wasserstein distance to empirical data, providing a balance between geometric accuracy, computational efficiency, and interpretability. Experiments on synthetic datasets (Spiral, Ellipse, Figure-eight, Lissajous curve, Torus knot) demonstrate competitive performance compared to existing methods, particularly for complex nonlinear trajectories. The method provides explicit interpretability via control points while maintaining trajectory smoothness and robustness.

## Method Summary
The method models conditional distributions P(y|x) as weighted sums of Gaussian components whose means and covariances are parameterized by Bernstein polynomials of degree N. Inputs x_i are normalized to t_i ∈ [0,1], and the model predicts a mixture of K Gaussians at each point. The loss function is the average squared Wasserstein distance (W_2^2) between predicted Gaussians and empirical targets (approximated as Gaussians centered at data points), plus L2 regularization on parameters. The model is optimized using Adam with mini-batches, where the squared Wasserstein distance between Gaussians has a closed-form expression involving matrix square roots of covariance matrices.

## Key Results
- The proposed method achieves competitive Wasserstein distance, Energy Distance, and RMSE metrics compared to baseline methods on synthetic trajectory datasets
- Performance is particularly strong on complex nonlinear trajectories like the Figure-eight and Torus knot
- The Bernstein basis parameterization provides smooth trajectory interpolation while maintaining explicit interpretability via control points
- The approach successfully handles multimodality through the Gaussian mixture model framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing the squared Wasserstein distance ($W_2^2$) preserves the geometric structure of probability distributions better than Euclidean or KL-based losses.
- **Mechanism:** The loss function calculates the "cost" of transporting mass from the predicted Gaussian to the target distribution, explicitly accounting for both mean displacement and covariance geometric deformation.
- **Core assumption:** The target conditional distributions can be adequately approximated by Gaussian mixtures, and the geometry of the probability space is relevant to the regression task.
- **Evidence anchors:** The loss "takes into account the geometry of the distributions" (abstract), and equation 6 explicitly defines the closed-form $W_2^2$ between Gaussians used in optimization.
- **Break condition:** If the true data distribution has sharp discontinuities or is supported on a non-Euclidean manifold, the Gaussian assumption and standard $W_2$ formulation may fail to capture the correct geometry.

### Mechanism 2
- **Claim:** The Bernstein basis enforces smooth trajectory interpolation while maintaining local control via explicit parameters (control points).
- **Mechanism:** The model parameterizes the mean and covariance of the Gaussians as linear combinations of Bernstein polynomials, which have the "convex hull" property ensuring the trajectory stays within control point bounds.
- **Core assumption:** The underlying probabilistic trajectory is smooth (continuous derivatives) with respect to the input variable $x$.
- **Evidence anchors:** The abstract highlights "smooth probability trajectory" and "explicit interpretability via control points," while section 3 defines the degree-$N$ Bernstein basis polynomials for parameterization.
- **Break condition:** If the relationship between $x$ and $y$ is non-smooth or contains sharp discontinuities, the polynomial basis will either require an impractically high degree $N$ or fail to approximate the jump.

### Mechanism 3
- **Claim:** The mixture model approach allows for multimodality and complex distributional shapes within a single regression framework.
- **Mechanism:** Instead of predicting a single point or a single Gaussian, the model predicts a weighted sum of $K$ Gaussians, allowing the trajectory to "split" or represent uncertainty as distinct modes.
- **Core assumption:** The conditional distribution $P(y|x)$ is complex enough to require multimodal representation, but $K$ is small enough to remain tractable.
- **Evidence anchors:** The abstract describes the model as a "weighted sum of Gaussian components," and equation 5 defines the mixture $P(y|t) = \sum w_k \mathcal{N}(...)$.
- **Break condition:** If the number of modes $K$ is underestimated, the model will suffer mode collapse; if the loss landscape is too complex, optimization might get stuck in poor local minima.

## Foundational Learning

- **Concept: Optimal Transport (Wasserstein Distance)**
  - **Why needed here:** This is the core loss function (Eq. 6). Standard regression minimizes error on point estimates; distribution regression minimizes the "distance" between probability shapes.
  - **Quick check question:** If distribution A is two distant spikes and distribution B is one wide bump in the middle, why would Wasserstein distance give a different result than KL-divergence?

- **Concept: Bernstein Polynomials & Bézier Curves**
  - **Why needed here:** This is the architectural backbone of the trajectory. Understanding how control points influence the curve (convex hull property) is necessary to interpret the "explicit interpretability" claim.
  - **Quick check question:** How does increasing the degree $N$ of the Bernstein polynomial affect the stability and flexibility of the curve?

- **Concept: Gaussian Mixture Models (GMMs)**
  - **Why needed here:** The model is essentially a GMM where the parameters are conditional on $x$. You need to know how means, covariances, and weights interact to form a density.
  - **Quick check question:** In a GMM, if two components have identical means but different covariances, can the model distinguish them?

## Architecture Onboarding

- **Component map:** Input Processor -> Basis Layer -> Parameter Heads -> Mixture Aggregator -> Loss Calculator
- **Critical path:** The calculation of the matrix square root $(\Sigma_1^{1/2} \Sigma_2 \Sigma_1^{1/2})^{1/2}$ in Eq. 6 during the backward pass, which is computationally expensive and can be numerically unstable if covariances become degenerate.
- **Design tradeoffs:**
  - **Degree ($N$) vs. Overfitting:** High $N$ captures complex trajectories but may oscillate and overfit noise.
  - **Components ($K$) vs. Speed:** Higher $K$ allows multimodality but linearly scales the computation of distance matrix terms.
  - **Gaussian vs. General:** Using closed-form $W_2$ for Gaussians is fast but limits expressiveness compared to non-parametric approaches.
- **Failure signatures:**
  - **Covariance Collapse:** Covariance matrices $\Sigma_k$ becoming singular or non-positive definite, causing NaN during matrix square root calculation.
  - **Control Point Explosion:** Control points $\mu_{k,i}$ drifting to extreme values if regularization $\lambda$ is too low.
  - **Mode Collapse:** All mixture weights $w_k$ converging to zero except for one, reducing the model to a single Gaussian.
- **First 3 experiments:**
  1. **Sanity Check (Ellipse):** Run on the "Ellipse" dataset with low degree ($N=3$) and $K=1$. Verify that the control points form a rough quadrilateral and the loss converges.
  2. **Gradient Stability Check:** Monitor the gradient norms of the matrix square root term (Eq. 6) on the "Spiral" dataset. Introduce the small $\epsilon$ term (Eq. 8) to $\Sigma$ to verify numerical stability remains > 0.
  3. **Ablation on Basis:** Compare the "Lissajous" trajectory fitting using Degree 3 vs. Degree 10. Observe if the higher degree provides better RMSE/Wasserstein scores or if it begins to overfit noise (check SRI metric).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the method be generalized to non-Gaussian probability distributions while retaining the closed-form gradient benefits?
- **Basis in paper:** The Abstract and Conclusion explicitly identify "extending the method to non-Gaussian distributions" as a primary avenue for future research.
- **Why unresolved:** The current implementation relies on the specific analytical form of the squared Wasserstein distance ($W^2_2$) between Gaussian measures (Eq. 6), which does not hold for arbitrary distributions.
- **What evidence would resolve it:** A reformulation of the loss function and component parameterization that handles heavy-tailed or multimodal base distributions, validated on non-Gaussian synthetic data.

### Open Question 2
- **Question:** Does entropic regularization (e.g., Sinkhorn algorithms) provide a computational speedup large enough to justify potential trade-offs in geometric precision?
- **Basis in paper:** The Conclusion suggests "applying entropy regularization to speed up computations" as a specific next step.
- **Why unresolved:** The paper utilizes autodiff-based optimization on the exact $W_2$ distance, which is robust but computationally intensive; the impact of regularization on the Bernstein control points' stability is unknown.
- **What evidence would resolve it:** A comparative analysis of training time versus error metrics (RMSE, $W_2$) between the proposed exact method and a Sinkhorn-regularized variant.

### Open Question 3
- **Question:** Can the approach effectively scale to high-dimensional data for approximating complex surfaces without succumbing to the curse of dimensionality inherent in optimal transport?
- **Basis in paper:** The Abstract and Conclusion mention the goal of "adapting the approach to working with high-dimensional data for approximating surfaces."
- **Why unresolved:** The experiments were strictly limited to 2D and 3D synthetic trajectories; Wasserstein computations typically scale poorly as dimensionality increases.
- **What evidence would resolve it:** Successful application of the model to datasets with $d > 10$ (e.g., image manifolds or physical simulations) with competitive accuracy and reasonable training time.

## Limitations

- The Gaussian mixture assumption limits expressiveness for highly multimodal or non-smooth target distributions
- The computational cost of matrix square roots in the Wasserstein distance may become prohibitive for high-dimensional outputs
- The Bernstein basis imposes smoothness constraints that may be too restrictive for discontinuous or rapidly varying trajectories

## Confidence

- **High confidence**: The geometric interpretation of Wasserstein loss for distribution regression, the Bernstein polynomial parameterization approach, and the overall methodological framework are well-established and theoretically sound.
- **Medium confidence**: The empirical performance claims on synthetic datasets are reasonable given the methodology, but depend heavily on unspecified hyperparameters and data generation details that affect reproducibility.
- **Low confidence**: The SRI metric evaluation cannot be independently verified without its definition, and the claim about "explicit interpretability via control points" has not been quantitatively validated.

## Next Checks

1. **Numerical stability verification**: Implement the matrix square root computation in equation (6) and test on edge cases where covariance matrices approach singularity to verify the proposed stabilization approach works as intended.
2. **Hyperparameter sensitivity analysis**: Systematically vary N (polynomial degree) and K (number of components) on a single dataset to identify optimal values and determine the sensitivity of final performance metrics to these choices.
3. **Real-world dataset evaluation**: Test the method on a real trajectory dataset (e.g., motion capture data or GPS trajectories) to assess performance beyond synthetic examples and evaluate the interpretability claims with domain experts.