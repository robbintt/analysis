---
ver: rpa2
title: 'PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios'
arxiv_id: '2512.06950'
source_url: https://arxiv.org/abs/2512.06950
tags:
- training
- pruning
- paris
- representer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARIS introduces a novel approach to imbalanced regression by optimizing
  the training set itself rather than modifying loss functions or generating synthetic
  data. The method leverages the representer theorem to compute a closed-form deletion
  residual that quantifies the exact change in validation loss when removing a single
  training point, without requiring retraining.
---

# PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios

## Quick Facts
- **arXiv ID**: 2512.06950
- **Source URL**: https://arxiv.org/abs/2512.06950
- **Reference count**: 11
- **Primary result**: PARIS reduces training set size by up to 75% while improving extreme event prediction in imbalanced regression tasks.

## Executive Summary
PARIS introduces a novel approach to imbalanced regression by optimizing the training set itself rather than modifying loss functions or generating synthetic data. The method leverages the representer theorem to compute a closed-form deletion residual that quantifies the exact change in validation loss when removing a single training point, without requiring retraining. This is combined with efficient Cholesky rank-one downdating for fast iterative pruning. The approach was validated on a challenging space weather forecasting task predicting the geomagnetic Dst index, which follows a severe power-law distribution.

## Method Summary
PARIS optimizes the training set for imbalanced regression by iteratively removing samples that least harm validation performance on rare events. The method extracts penultimate-layer features φ from a neural network, then uses the representer theorem to decompose predictions into training sample contributions. For each pruning step, it computes the exact change in validation loss when removing each training point via a closed-form deletion residual, then removes the sample minimizing this residual. Cholesky rank-one downdating enables O(D²) updates per step instead of O(N³) matrix inversions. An outer retraining loop periodically updates the feature extractor to maintain accuracy as the training set evolves.

## Key Results
- PARIS reduced training set size by up to 75% while maintaining or improving overall RMSE performance compared to state-of-the-art baselines
- Demonstrated superior performance on extreme events, achieving significantly lower conditional RMSE for Dst ≤ -100nT
- Used only 25% of original dataset size while outperforming methods including loss re-weighting, synthetic oversampling, and boosting
- Validated on geomagnetic Dst index forecasting, a challenging imbalanced regression task with severe power-law distribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing specific training samples can reduce validation loss on rare-event targets without model retraining.
- **Mechanism:** The representer theorem decomposes a neural network's prediction into a weighted sum of training sample contributions via learned features φ(x). By computing the influence matrix S = Φ_val(Φ⊙α)^T, each column quantifies how much a training sample contributes to all validation predictions. The deletion residual ΔL = 2r_v* × S_v*,k + S²_v*,k gives the exact change in validation loss if sample k is removed.
- **Core assumption:** The penultimate-layer feature map φ(·) remains approximately constant during the inner pruning loop (addressed by outer-loop retraining).
- **Evidence anchors:** [abstract] "closed-form representer deletion residual, which quantifies the exact change in validation loss caused by removing a single training point without retraining" [section 2.2] Equation 12 derives ΔL_\k^{v*} = 2r_v* S_v*,k + S²_v*,k

### Mechanism 2
- **Claim:** Iterative pruning can be performed at near-linear cost per step instead of O(N³) matrix inversion.
- **Mechanism:** After removing sample k, the Gram matrix A = Φ^TΦ + λI undergoes rank-one downdate: A_new = A - φ_k φ_k^T. Cholesky factor L is updated in O(D²) via standard downdating algorithms. Dual coefficients α = Φw* are recovered without new matrix inversions.
- **Core assumption:** The cached unscaled matrix T = Φ_val Φ^T remains valid while φ is fixed, allowing S reconstruction via column-wise scaling by α.
- **Evidence anchors:** [abstract] "efficient Cholesky rank-one downdating scheme... fast, iterative pruning" [section 2.2.1] "per-step pruning cost dominated by O(D²) downdate"

### Mechanism 3
- **Claim:** Targeting the worst-case validation residual for pruning guidance improves rare-event performance.
- **Mechanism:** At each step, identify validation point v* with largest squared residual. Compute deletion residuals for all training samples against this point. Remove the training sample minimizing ΔL (most beneficial to delete). This biases the retained set toward samples that structurally support tail performance.
- **Core assumption:** Validation set contains representative rare events; pruning to minimize worst-case validation residual generalizes to test tail distribution.
- **Evidence anchors:** [section 2.2] "pruning strategy will be guided by iteratively reducing the residual of the validation point that exhibits the largest current squared residual loss" [section 3.1.1] Table 2 shows PARIS achieves lowest conditional RMSE for extreme percentiles (1-20%)

## Foundational Learning

- **Concept: Representer Theorem for Neural Networks**
  - Why needed here: Enables exact decomposition of predictions into training sample contributions, forming the theoretical basis for deletion residuals without retraining.
  - Quick check question: Given fixed features φ, can you express why a prediction f(x_t) becomes a linear combination of training embeddings?

- **Concept: Ridge Regression Dual/Primal Relationship**
  - Why needed here: PARIS operates in primal (Cholesky on Φ^TΦ) but requires dual coefficients α; understanding the α = Φw* connection is essential for efficient updates.
  - Quick check question: If you add λI to Φ^TΦ, what happens to the condition number and why does this enable stable Cholesky factorization?

- **Concept: Imbalanced Regression Metrics (cRMSE)**
  - Why needed here: Standard RMSE is dominated by majority samples; conditional RMSE on tail thresholds is the proper evaluation criterion for rare-event tasks.
  - Quick check question: Why might a model with lower overall RMSE perform worse on the 1st percentile of target values?

## Architecture Onboarding

- **Component map:** Feature extractor φ(·) -> Ridge solver (Cholesky) -> Influence matrix S -> Deletion residual ΔL -> Pruning selection -> Cholesky downdate -> α update

- **Critical path:** Train → Extract φ → Compute A, factor to L → Solve w*, derive α → Build T, scale to S → Identify worst validation residual → Compute ΔL for all candidates → Remove min ΔL sample → Cholesky downdate → Update α, S → Repeat inner loop → Outer loop: retrain φ on pruned set

- **Design tradeoffs:**
  - Outer loop frequency: More frequent retraining improves accuracy but increases cost
  - Pruning batch size K: Larger K amortizes overhead but increases feature-drift error
  - λ estimation: Closed-form surrogate (Appendix A) avoids cross-validation but may misestimate; fallback λ_min = 10⁻⁵ prevents ill-conditioning

- **Failure signatures:**
  1. Cholesky downdate fails: λ too small or feature matrix rank-deficient; increase regularization
  2. cRMSE degrades on tail: Validation set lacks representative extremes; rebalance or augment validation
  3. Pruning stalls early: Deletion residuals near zero for all candidates; features may have collapsed diversity

- **First 3 experiments:**
  1. Reproduce Dst benchmark: Use LOOCV on 20 strongest storms, compare PARIS vs. MSE/Focal/DW baselines on cRMSE at Dst ≤ -100nT
  2. Ablate outer loop: Run PARIS with no retraining, single retrain, and periodic retrain; plot cRMSE vs. pruning fraction to quantify drift error
  3. Sensitivity to λ: Test surrogate λ vs. fixed λ ∈ {10⁻⁵, 10⁻³, 10⁻¹}; monitor Cholesky stability and final cRMSE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PARIS behave in large-scale regimes where mini-batch training is unavoidable, or under streaming-data conditions?
- Basis in paper: [explicit] The authors state, "we have not explored the regime in which the training set is sufficiently large that mini-batch training becomes unavoidable," noting the need to redefine the influence matrix S under stochastic feature updates.
- Why unresolved: The current implementation assumes full-batch feature extraction for exact Cholesky downdating, which is computationally prohibitive for massive datasets.
- What evidence would resolve it: An approximation of S compatible with stochastic training and benchmarks on datasets significantly larger than the Dst index data.

### Open Question 2
- Question: Can the PARIS framework be generalized to multi-output regression or classification tasks?
- Basis in paper: [explicit] The conclusion identifies "generalizing to multi-output regression or classification by adapting the residual-based pruning criterion to vector-valued losses" as a specific avenue for future research.
- Why unresolved: The current methodology derives the deletion residual specifically for scalar targets using squared error, which does not directly translate to vector-valued outputs or classification likelihoods.
- What evidence would resolve it: A derivation of the deletion residual for vector-valued losses and empirical validation on standard classification benchmarks.

### Open Question 3
- Question: Does incorporating influence information from multiple difficult validation points simultaneously improve pruning stability compared to the single worst-case residual approach?
- Basis in paper: [explicit] The paper suggests "incorporating influence information from multiple 'hard' validation points simultaneously, rather than relying solely on the single worst-case residual."
- Why unresolved: The current algorithm (Algorithm 1, line 14) greedily identifies the single validation point v* with the largest residual to guide the pruning step.
- What evidence would resolve it: Ablation studies comparing the current greedy approach against an aggregation of influence scores from the top-k percentile of high-residual validation points.

## Limitations
- Feature-staleness assumption during iterative pruning requires periodic retraining, but optimal retraining frequency is not specified
- The method assumes validation set contains representative rare events, which may not hold in highly skewed distributions
- The λ surrogate formula for ridge regularization is derived but not experimentally validated against cross-validation

## Confidence

- **High Confidence**: The closed-form deletion residual derivation and Cholesky downdating algorithm are mathematically sound and well-specified
- **Medium Confidence**: The outer-loop retraining strategy effectively mitigates feature drift, but the optimal retraining frequency remains heuristic
- **Medium Confidence**: The Dst dataset and LOOCV setup are clearly described, but exact storm selection criteria are unspecified, limiting exact reproducibility
- **Low Confidence**: The impact of λ estimation via the surrogate formula versus cross-validation is not benchmarked

## Next Checks
1. **Ablation on Outer Loop Frequency**: Run PARIS with pruning batch sizes K = 1, 5, 10 and outer retraining frequencies f = {after every K, after every 2K, no retraining}. Plot cRMSE on extreme events (Dst ≤ -100nT) versus total pruning steps to quantify feature-drift error.
2. **λ Sensitivity Analysis**: Compare PARIS performance using the closed-form λ surrogate versus grid search λ ∈ {10⁻⁵, 10⁻⁴, 10⁻³, 10⁻²}. Monitor Cholesky factorization stability and final cRMSE on both global and conditional metrics.
3. **Validation Set Representativeness**: Create synthetic Dst-like datasets with controlled tail distributions. Test PARIS when the validation set contains 10%, 50%, and 100% of the rarest events. Measure generalization to held-out test tails to assess pruning bias.