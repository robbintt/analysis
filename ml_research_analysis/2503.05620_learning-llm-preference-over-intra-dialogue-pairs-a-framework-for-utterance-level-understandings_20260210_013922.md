---
ver: rpa2
title: 'Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level
  Understandings'
arxiv_id: '2503.05620'
source_url: https://arxiv.org/abs/2503.05620
tags:
- dialogue
- arxiv
- learning
- intent
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework for training small models on noisy
  LLM-generated labels for utterance-level dialogue tasks. The core method uses pairwise
  preference learning with intra-session ranking, leveraging the idea that comparing
  utterances within the same dialogue session reduces noise from LLM labeling errors.
---

# Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings

## Quick Facts
- arXiv ID: 2503.05620
- Source URL: https://arxiv.org/abs/2503.05620
- Reference count: 26
- Primary result: 2-10% improvement on utterance-level dialogue tasks using pairwise preference learning

## Executive Summary
This paper proposes a framework for training small models on noisy LLM-generated labels for utterance-level dialogue tasks. The core method uses pairwise preference learning with intra-session ranking, leveraging the idea that comparing utterances within the same dialogue session reduces noise from LLM labeling errors. The approach calibrates LLM scores through ensemble methods and trains models to match LLM preferences under adaptive margins. Experiments on sentiment detection, dialogue act classification, and dialogue state tracking show improvements of 2-10% over baselines, with the pairwise approach outperforming pointwise knowledge transfer especially in low-data regimes.

## Method Summary
The framework trains small models to match LLM preferences for utterance-level dialogue understanding. It uses intra-session pairwise comparisons to reduce noise from LLM labeling errors, calibrates LLM scores through ensemble methods, and employs adaptive margins during training. The approach works by having LLMs score utterances within dialogue sessions, then training smaller models to reproduce these pairwise preferences rather than absolute scores, which helps mitigate labeling noise.

## Key Results
- 2-10% improvement over baselines on sentiment detection, dialogue act classification, and dialogue state tracking
- Pairwise approach outperforms pointwise knowledge transfer, especially in low-data regimes
- Effective when only small amounts of human-verified labels are available

## Why This Works (Mechanism)
The framework leverages the structure of dialogue sessions to reduce noise in LLM-generated labels. By comparing utterances within the same session rather than across different contexts, the method exploits the relative nature of preferences while avoiding issues with absolute score calibration. The pairwise approach is more robust to noise because it only requires the LLM to rank utterances correctly within a session, not assign globally consistent absolute scores.

## Foundational Learning
- Dialogue session structure: why needed - provides natural grouping for pairwise comparisons; quick check - verify session boundaries in dataset
- LLM score calibration: why needed - reduces systematic bias in LLM preferences; quick check - compare calibrated vs uncalibrated pairwise accuracy
- Adaptive margin training: why needed - handles varying difficulty of preference pairs; quick check - analyze margin distribution during training

## Architecture Onboarding
- Component map: LLM scorer -> Calibrator -> Pairwise comparator -> Small model trainer
- Critical path: Utterance pairs → LLM scoring → Calibration ensemble → Margin scaling → Model training
- Design tradeoffs: Pairwise vs pointwise knowledge transfer (robustness to noise vs simplicity)
- Failure signatures: Poor calibration leads to noisy pairwise labels; session boundaries too large/small affect comparison quality
- First experiments: 1) Calibrate LLM scores on held-out data; 2) Generate pairwise preferences within sessions; 3) Train small model with adaptive margins

## Open Questions the Paper Calls Out
None

## Limitations
- Does not address potential LLM bias in preference labels that could propagate to small models
- Assumes utterance-level understanding can be learned without explicit dialogue context modeling beyond session boundaries
- Relies on synthetic preference data rather than human-verified utterance-level judgments for evaluation

## Confidence
- High confidence: Pairwise preference framework is technically implementable with sound algorithmic components
- Medium confidence: Reported 2-10% improvements due to reliance on synthetic preference data
- Medium confidence: Claims about low-data regime performance based on controlled experiments

## Next Checks
1. Human evaluation of pairwise preferences: Have human annotators verify a sample of the LLM-generated pairwise preferences to quantify actual noise level and potential systematic biases in the labels.

2. Ablation on dialogue context: Test whether including explicit dialogue context features (beyond session ID) improves performance compared to the current session-based approach.

3. Cross-domain generalization: Evaluate the trained models on dialogue datasets from different domains than those used for LLM preference generation to assess robustness to domain shift in the preference labels.