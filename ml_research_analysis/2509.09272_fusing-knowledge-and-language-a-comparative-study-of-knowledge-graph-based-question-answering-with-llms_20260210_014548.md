---
ver: rpa2
title: 'Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based
  Question Answering with LLMs'
arxiv_id: '2509.09272'
source_url: https://arxiv.org/abs/2509.09272
tags:
- bertram
- helena
- love
- relationship
- graphrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comparative study of three methods\u2014\
  spaCy, Stanford CoreNLP-OpenIE, and GraphRAG\u2014for constructing knowledge graphs\
  \ and integrating them with large language models for question answering. Using\
  \ Shakespeare\u2019s play and the RepliQA dataset, the study evaluates ease of use,\
  \ performance, and accuracy."
---

# Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs

## Quick Facts
- **arXiv ID:** 2509.09272
- **Source URL:** https://arxiv.org/abs/2509.09272
- **Reference count:** 0
- **Primary result:** GraphRAG consistently outperformed spaCy and CoreNLP-OpenIE for KG-based QA, achieving average scores of 4.46 (expert) and 4.90 (GPT-4) due to superior reasoning and contextual understanding.

## Executive Summary
This paper presents a comparative study of three methods—spaCy, Stanford CoreNLP-OpenIE, and GraphRAG—for constructing knowledge graphs and integrating them with large language models for question answering. Using Shakespeare’s play and the RepliQA dataset, the study evaluates ease of use, performance, and accuracy. GraphRAG consistently outperformed the others, achieving average scores of 4.46 (expert) and 4.90 (GPT-4), due to superior reasoning and contextual understanding. CoreNLP-OpenIE provided broadest factual coverage, while spaCy offered lightweight, high-precision extraction. GraphRAG’s integration of structured knowledge enhanced thematic and multi-hop reasoning. The study highlights trade-offs in precision, coverage, and reasoning, and recommends hybrid pipelines and incremental graph updates for future KG-QA systems.

## Method Summary
The study compares three KG construction methods (spaCy, CoreNLP-OpenIE, GraphRAG) integrated with LLMs for QA. Triplets are extracted from Shakespeare’s play and RepliQA dataset using dependency parsing (spaCy), OpenIE (CoreNLP), and LLM-based extraction (GraphRAG). KG triplets are used with LangChain’s GraphQAChain for spaCy/CoreNLP, and GraphRAG’s local search pipeline for the third method. Evaluation uses expert human scores and GPT-4 scores on rubric dimensions (Content, Organization, Style, Mechanics), plus quantitative metrics (Exact Match, QA F1, ROUGE-L, METEOR, BERTScore).

## Key Results
- GraphRAG consistently achieved the highest average scores: 4.46 (expert) and 4.90 (GPT-4).
- CoreNLP-OpenIE yielded the broadest factual coverage among the three methods.
- spaCy provided lightweight, high-precision extraction but limited multi-hop reasoning.

## Why This Works (Mechanism)
GraphRAG’s integration of structured knowledge with LLM-driven reasoning enables enhanced contextual understanding and multi-hop inference. The combination of entity-relation extraction (via OpenIE or dependency parsing) and LLM-generated summaries improves answer accuracy. GraphRAG’s use of LLM-generated queries for graph traversal supports deeper reasoning, especially in complex, multi-hop questions. The study shows that structured knowledge graphs augment LLM capabilities by providing explicit factual scaffolding.

## Foundational Learning
- **Triplet extraction patterns:** Essential for converting text into KG nodes/edges; quick check: verify triplet counts and structure for each method.
- **GraphQAChain integration:** Core framework for connecting KG triplets to QA workflows; quick check: ensure triplet format matches LangChain requirements.
- **Evaluation rubric scoring:** Standardizes answer quality across human and LLM evaluators; quick check: test rubric with sample answers before full evaluation.

## Architecture Onboarding
- **Component map:** Text → Triplet Extraction (spaCy/CoreNLP/GraphRAG) → Knowledge Graph → QA Pipeline (GraphQAChain/GraphRAG) → Answer Evaluation
- **Critical path:** Triplet extraction → KG construction → QA query processing → Answer generation
- **Design tradeoffs:** spaCy (lightweight, precise, sparse); CoreNLP (broad coverage, moderate complexity); GraphRAG (rich reasoning, high resource cost)
- **Failure signatures:** Sparse graphs (spaCy); server timeouts (CoreNLP); high resource usage (GraphRAG)
- **First experiments:**
  1. Run triplet extraction on a single document; inspect output structure.
  2. Load a small KG into GraphQAChain; run a test question.
  3. Evaluate a single answer using the rubric with GPT-4.

## Open Questions the Paper Calls Out
None

## Limitations
- Key implementation details for triplet extraction (spaCy patterns) and specific open-source LLM used are unspecified.
- Resource intensity of GraphRAG limits accessibility for smaller research groups.
- Exact triplet filtering logic for matching question triplets is not provided.

## Confidence
- **High Confidence:** Core findings about GraphRAG’s superior reasoning performance and spaCy’s precision advantages.
- **Medium Confidence:** Comparative performance rankings between methods are reliable.
- **Low Confidence:** Exact implementation details for triplet extraction and KG integration remain unclear.

## Next Checks
1. Test CoreNLP server integration and triplet extraction workflows with a small sample before scaling.
2. Verify computational resources (GPU/VRAM) and API costs for GraphRAG deployment.
3. Define and document the specific dependency parsing patterns used for spaCy triplet extraction.