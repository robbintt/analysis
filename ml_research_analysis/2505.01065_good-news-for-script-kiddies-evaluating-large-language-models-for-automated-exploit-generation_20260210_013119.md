---
ver: rpa2
title: Good News for Script Kiddies? Evaluating Large Language Models for Automated
  Exploit Generation
arxiv_id: '2505.01065'
source_url: https://arxiv.org/abs/2505.01065
tags:
- llms
- exploit
- generation
- attacker
- automated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates large language models (LLMs)
  for automated exploit generation (AEG). To mitigate dataset bias, the authors created
  a benchmark using refactored versions of five software security labs and developed
  an LLM-based attacker to systematically guide LLMs in generating exploits.
---

# Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation

## Quick Facts
- arXiv ID: 2505.01065
- Source URL: https://arxiv.org/abs/2505.01065
- Reference count: 18
- No model successfully generated working exploits despite high cooperativeness

## Executive Summary
This study systematically evaluates large language models (LLMs) for automated exploit generation (AEG) using a benchmark of refactored SEED Labs. The authors developed an iterative attacker-guided refinement process where GPT-4o analyzes and improves LLM-generated exploits over up to 15 iterations. While GPT-4 and GPT-4o show high cooperativeness with exploit requests, no model successfully generated working exploits for the refactored labs. GPT-4o made minimal errors (1-2 per attempt), suggesting current LLMs understand exploit structure but fail on critical details like padding calculations and address resolution.

## Method Summary
The study uses five SEED Labs (buffer overflow, return-to-libc, format string, race condition, dirty COW) in both original and refactored versions with renamed variables/functions. An LLM-based attacker (GPT-4o) iteratively guides target LLMs through up to 15 refinement cycles, analyzing errors and generating improved prompts. Target models include GPT-4o, GPT-4o-mini, Llama3, Dolphin-Mistral, and Dolphin-Phi. Generated exploits are manually evaluated against ground-truth solutions, with error counts used to assess effectiveness. Local models run via Ollama while OpenAI models use API access.

## Key Results
- GPT-4 and GPT-4o exhibit high cooperativeness (low refusal rates) with exploit generation requests
- No model successfully generated working exploits for refactored labs despite iterative refinement
- Symbol obfuscation significantly degraded performance, suggesting training data contamination
- GPT-4o made minimal errors (1-2 per attempt) but still failed to produce working exploits
- Llama3 showed highest resistance to exploit generation requests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative attacker-guided refinement increases exploit code quality compared to single-shot prompting.
- Mechanism: An attacker LLM (GPT-4o) analyzes target LLM output, identifies errors, and generates improved prompts. This multi-turn feedback loop allows progressive correction of syntax and logic errors over up to 15 iterations.
- Core assumption: The attacker model can reliably identify what went wrong in the exploit attempt and articulate useful corrections.
- Evidence anchors:
  - [abstract] "we design an LLM-based attacker to systematically prompt LLMs for exploit generation"
  - [section III.C] "The process repeats iteratively, with the improved response sent back to the target LLM, and its output further refined by GPT-4o-mini"
  - [corpus] Weak direct corpus support for iterative refinement mechanisms in AEG; related work focuses on single-pass generation.
- Break condition: If target LLM consistently refuses or produces off-task outputs, iteration yields diminishing returns; GPT-4o still made 1-2 errors per attempt even with iteration.

### Mechanism 2
- Claim: Code symbol obfuscation (renaming variables/functions) significantly degrades LLM exploit generation performance.
- Mechanism: LLMs appear to rely on semantic hints from meaningful variable and function names. When symbols are replaced with generic names (varX, functionX), the model loses contextual cues about program behavior and vulnerability structure.
- Core assumption: Performance degradation on refactored code indicates reliance on surface-level patterns rather than deep vulnerability understanding.
- Evidence anchors:
  - [section III.A] "created a modified version by renaming variables and functions to varX and functionX... to mitigate the risk of LLMs recalling memorized solutions"
  - [section IV.B] "Dolphin Mistral performs the best with the original labs while it made significantly more mistakes on the refactored programs. This indicates that Dolphin Mistral might have been trained with SEED Lab materials"
  - [corpus] Neighbor paper "Prompt to Pwn" addresses smart contract AEG but does not test obfuscation effects.
- Break condition: If an LLM trained on diverse vulnerable code patterns develops true semantic understanding, symbol obfuscation should have minimal impact.

### Mechanism 3
- Claim: LLMs produce syntactically plausible but semantically incorrect exploits because they learn exploit "shape" without understanding underlying mechanisms.
- Mechanism: LLMs generate code that structurally resembles known exploits (correct components, roughly correct ordering) but fail on critical details: padding calculations, address resolution, attack looping for probabilistic exploits.
- Core assumption: Training data contains many exploit examples, enabling pattern matching without causal understanding.
- Evidence anchors:
  - [section IV.C] "current LLMs understand what an attack for a common vulnerability will 'look' like without having formed connections about the underlying mechanisms"
  - [section IV.C] "60% of cases... use of mutexes or semaphores which prevent race conditions" in Dirty COW exploits
  - [corpus] No direct corpus corroboration; this is an observational hypothesis from the study.
- Break condition: If reasoning-focused models (GPT-o1, DeepSeek-r1) are evaluated and show significant improvement, this suggests the limitation is reasoning depth, not training pattern matching.

## Foundational Learning

- Concept: **Symbolic Execution Basics**
  - Why needed here: To understand why LLMs struggle with padding calculations and address resolution—tasks that symbolic execution handles systematically.
  - Quick check question: Can you explain how symbolic execution determines the constraints needed to reach a vulnerable code path?

- Concept: **LLM Alignment and Refusal Behavior**
  - Why needed here: The study's RQ1 directly measures how often models refuse exploit requests; understanding alignment mechanisms explains why Llama3 is resistant while GPT-4o cooperates.
  - Quick check question: What is the difference between training-time alignment (RLHF) and inference-time refusal mechanisms?

- Concept: **Memory Safety Vulnerability Mechanics**
  - Why needed here: The five vulnerability types tested (buffer overflow, return-to-libc, format string, race condition, dirty COW) require understanding stack layout, memory permissions, and race windows.
  - Quick check question: For a buffer overflow exploit, why must the NOP sled length match the unprotected read buffer size?

## Architecture Onboarding

- Component map: Attacker LLM (GPT-4o) -> Target LLM (any evaluated model) -> Attacker Script -> Benchmark (original/refactored SEED Labs) -> Evaluation (manual ground-truth solutions)

- Critical path:
  1. Present vulnerable code to Target LLM with exploit request
  2. Target LLM returns initial exploit attempt
  3. Attacker LLM analyzes errors and generates refinement prompt
  4. Repeat until success or iteration limit
  5. Compare final output against manual solution, count errors

- Design tradeoffs:
  - Using GPT-4o as attacker: Strong reasoning but introduces dependency on OpenAI; could bias results if attacker and target share architectural similarities
  - 15-iteration cap: Empirically determined; may miss rare late-stage successes but bounds evaluation time
  - SEED Labs: Well-documented ground truth, but may not generalize to real-world code complexity

- Failure signatures:
  - High cooperativeness + high error rate = model willing but technically incapable
  - Performance drop on refactored code = likely training data contamination
  - Consistent error types across models (e.g., 100% padding errors in Return-to-LibC) = systematic LLM limitation

- First 3 experiments:
  1. Reproduce the cooperativeness measurement (Table I) on a single vulnerability type to validate the attacker-target interaction pipeline.
  2. Test GPT-4o on one original lab and its refactored version side-by-side; count errors and identify which specific code elements the model relies on.
  3. Manually correct the most common error type (e.g., NOP sled length) in GPT-4o's buffer overflow output and verify if the exploit succeeds—this isolates whether single-error correction is sufficient.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reasoning-focused models (e.g., GPT-o1, DeepSeek-r1) successfully generate exploits for refactored code where standard LLMs failed?
- Basis in paper: [explicit] The authors explicitly identify the release of reasoning models during their study and state, "we plan to evaluate them in future studies."
- Why unresolved: These models were released too recently to be included in the systematic evaluation described in the paper.
- What evidence would resolve it: A replication of the study's benchmark tests using GPT-o1 or DeepSeek-r1 as the target models.

### Open Question 2
- Question: How does LLM performance in automated exploit generation change when applied to real-world vulnerabilities compared to educational labs?
- Basis in paper: [explicit] The authors acknowledge using SEED Labs for ground-truth data and state, "future work will incorporate real-world vulnerabilities and exploits."
- Why unresolved: The study was limited to educational environments with well-documented vulnerabilities to ensure a reliable initial evaluation.
- What evidence would resolve it: Evaluating the models against a dataset of refactored, real-world CVEs (Common Vulnerabilities and Exposures).

### Open Question 3
- Question: Are specific exploit generation failures (e.g., incorrect padding sizes) caused by arithmetic limitations or a failure to semantically link exploit mechanics with code structure?
- Basis in paper: [inferred] The authors speculate in Section IV.C that errors like incorrect padding may stem from arithmetic failures or a failure to connect payload goals with addresses, noting these theories "have not been rigorously tested."
- Why unresolved: The study focused on error rates and cooperativeness rather than isolating the cognitive or computational root causes of specific logic errors.
- What evidence would resolve it: Ablation studies or specific prompting strategies that separate arithmetic tasks from semantic reasoning tasks during exploit construction.

## Limitations

- The SEED Labs benchmark represents a narrow slice of real-world vulnerability scenarios with constrained complexity
- The 15-iteration cap may artificially constrain the exploration space, though empirically determined
- Error categorization methodology is not fully specified, making independent assessment difficult
- No successful exploits were generated, preventing definitive conclusions about fundamental LLM limitations versus benchmark-specific constraints

## Confidence

- **High confidence**: Models exhibit high cooperativeness but fail to generate working exploits; symbol obfuscation degrades performance across models, suggesting training data contamination; GPT-4o made minimal errors (1-2 per attempt) indicating systematic rather than random failure patterns.
- **Medium confidence**: Iterative refinement improves exploit quality compared to single-shot prompting; current LLMs generate structurally plausible but semantically incorrect exploits due to pattern matching rather than causal understanding; Llama3 shows highest resistance to exploit generation requests.
- **Low confidence**: No model successfully generated working exploits, preventing definitive conclusions about whether this reflects fundamental limitations or benchmark-specific constraints; error categorization and refinement quality assessment lack sufficient methodological detail for independent validation.

## Next Checks

1. **Single-error correction validation**: Take GPT-4o's buffer overflow exploit with padding errors, manually correct the padding size and NOP sled length, then execute the modified exploit to determine if the exploit would succeed with a single fix. This isolates whether LLMs are "close" to capability.

2. **Reasoning-focused model evaluation**: Test GPT-o1 or DeepSeek-r1 on the refactored labs using the same attacker-guided iterative process to determine if enhanced reasoning capabilities overcome the systematic errors observed in standard LLMs.

3. **Training data contamination assessment**: Cross-reference the exact code patterns and variable naming conventions in the refactored SEED Labs with public code repositories and training data releases from model providers to quantify the likelihood of memorization versus genuine reasoning.