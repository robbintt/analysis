---
ver: rpa2
title: Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered
  Objects on Edge Devices
arxiv_id: '2509.23647'
source_url: https://arxiv.org/abs/2509.23647
tags:
- pose
- estimation
- object
- tracking
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses robust 6D pose estimation and tracking of novel
  textured objects under challenging illumination, particularly on edge devices. It
  proposes a unified framework that leverages a lighting-invariant color-pair feature
  representation for both initial pose estimation and motion-based tracking.
---

# Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered Objects on Edge Devices

## Quick Facts
- arXiv ID: 2509.23647
- Source URL: https://arxiv.org/abs/2509.23647
- Reference count: 34
- Primary result: 78.6% ADD-S average accuracy on YCB-Video with real-time performance on edge devices

## Executive Summary
This paper presents a unified framework for zero-shot 6D pose estimation and tracking of novel textured objects under challenging illumination conditions, specifically optimized for edge device deployment. The method leverages a lighting-invariant color-pair feature representation extracted from texture edges, which forms a consistent foundation for both initial pose estimation via geometric hashing and Hough voting, and subsequent motion-based tracking using optical flow and a lightweight neural network. Experimental results on YCB-Video and Fast-YCB datasets demonstrate competitive accuracy while maintaining real-time performance on embedded hardware like Jetson AGX Orin.

## Method Summary
The approach operates through a two-stage pipeline: initial pose estimation and continuous tracking. For estimation, objects are detected and segmented using YOLO and NanoSAM, then bilateral filtering and CIELAB conversion prepare the data for edge detection. Color-pair features are extracted from multi-sample points across edges, matched to pre-rendered reference features to construct classified point clouds, and used for semantic triangle-based geometric hashing. Hough voting with Kabsch initialization generates pose hypotheses, refined by weighted multi-class ICP. For tracking, NeuFlow v2 optical flow provides dense correspondences, filtered by color-pair consistency, which are then processed by an Attention-DGCNN network trained on procedurally generated shapes to estimate relative rotation, followed by point-to-plane ICP refinement.

## Key Results
- 78.6% ADD-S average accuracy on YCB-Video dataset
- 60.9% ADD accuracy on YCB-Video
- 7 FPS pose estimation and >20 FPS tracking on Jetson AGX Orin
- Real-time performance maintained for 5 objects simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Lighting-Invariant Color-Pair Feature Representation
The relational characteristics of color pairs across local texture edges remain stable under varying illumination while lightness varies. By sampling colors from both sides of detected texture edges and representing each pair as a triangle in CIELAB color space with the origin, similarity is computed via three geometric comparisons: directional alignment of sides OC₁ and OC₂ with down-weighted lightness, internal contrast between the third sides, and relative luminance ratio. Symmetric matching handles arbitrary sampling order.

### Mechanism 2: Semantic Triangle-Based Geometric Hashing for Pose Initialization
Converting continuous SE(3) alignment into discrete geometric hashing provides robust initial pose hypotheses. Offline database construction selects structurally significant semantic triangles from different color-pair classes, computes 7-dimensional pose-invariant feature keys, and stores in hash table. Online stage extracts scene triangles, queries database, generates hypotheses via Kabsch algorithm, and applies two-stage Hough voting with non-maximum suppression. Point Pair Features supplement when fewer than three classes visible.

### Mechanism 3: Perspective-Normalized Procedural Learning for Tracking
Normalizing point clouds to a canonical reference frame decouples translation from rotation, enabling lightweight network generalization. Consecutive frame point clouds are transformed into canonical "look-at" pose using centering and perspective correction matrix. 15-dimensional feature vectors from normalized 3D coordinates, 2D projections, and displacements train Attention-DGCNN on procedurally generated shapes. Color-pair consistency filters optical flow matches before 3D lifting, with brief ICP refinement mitigating drift.

## Foundational Learning

- **CIELAB Color Space**: Separates lightness (L*) from chrominance (a*, b*), down-weighting L* to achieve illumination invariance. Quick check: Can you explain why separating luminance from chrominance helps with lighting robustness?
- **SE(3) Transformations and Kabsch Algorithm**: Initial pose voting generates hypotheses by aligning semantic triangles; Kabsch provides optimal rotation between corresponding point sets. Quick check: Given two sets of 3D point correspondences, how does Kabsch compute the optimal rigid transformation?
- **Iterative Closest Point (ICP) Variants**: Hierarchical weighted ICP refines initial poses; point-to-plane ICP mitigates tracking drift. Multi-class joint optimization preserves part-level geometry. Quick check: What is the difference between point-to-point and point-to-plane ICP, and when would you prefer each?

## Architecture Onboarding

- **Component map**: YOLO detector → NanoSAM segmentation → bilateral filtering → CIELAB conversion → edge detection → color-pair extraction → feature matching → classified point cloud → semantic triangle hashing → Hough voting → weighted ICP → NeuFlow v2 optical flow → color-pair consistency filter → perspective normalization → Attention-DGCNN rotation estimation → translation derivation → point-to-plane ICP refinement
- **Critical path**: Color-pair extraction quality determines both branches. In estimation, poor classification propagates through voting; in tracking, sparse filtered matches reduce pose accuracy
- **Design tradeoffs**: Accuracy vs. speed (7 FPS vs. higher accuracy methods), texture dependency vs. generality (targets textured objects only), frame interval vs. robustness (5-frame downsampling occasionally requires re-initialization)
- **Failure signatures**: Depth noise sensitivity (accuracy gap partly attributed to depth sensor noise), rapid motion (5-frame intervals occasionally require re-initialization), texture mismatch (method explicitly targets textured objects)
- **First 3 experiments**:
  1. Color-pair filtering ablation: Replicate Fig. 7 by computing inter-frame pose with/without color-pair filtering on held-out YCB-V sequences. Measure rotation and translation error normalized by ground-truth motion magnitude
  2. Edge case stress test: Test on objects with minimal texture (e.g., YCB bleached cleanser with reduced texture regions) to characterize failure thresholds; report classification confidence distributions
  3. Latency profiling on target hardware: Deploy TensorRT-optimized pipeline on Jetson AGX Orin; profile each module (segmentation, color-pair extraction, voting, ICP, optical flow, tracking network) to identify bottlenecks when processing 5+ objects

## Open Questions the Paper Calls Out

- **Open Question 1**: Can integrating 2D positional cues into the refinement stage bridge the accuracy gap with state-of-the-art methods while maintaining real-time efficiency? The authors attribute lower accuracy compared to methods like FoundationPose to reliance on noisy depth maps and suggest future work could focus on integrating these 2D cues during refinement.

- **Open Question 2**: Does extending the tracking module to utilize multiple temporal keyframes improve resilience to abrupt pose changes and motion blur? The paper notes a drop in tracking accuracy when processing sparse frames and suggests future work could incorporate multiple keyframes, establishing a more stable temporal baseline.

- **Open Question 3**: Can the color-pair feature representation be generalized to handle textureless or minimally textured objects? The method is explicitly designed for "textured objects," and the evaluation excludes YCB objects lacking texture, leaving open whether the approach can handle uniform surfaces.

## Limitations

- Lighting-invariant color-pair feature representation lacks direct corpus validation and peer-reviewed benchmarking under varying illumination
- Semantic triangle-based geometric hashing represents a novel approach with no known precedents, making generalizability uncertain
- Accuracy gaps partly attributed to depth sensor noise affecting ICP refinement, suggesting sensitivity to sensor quality

## Confidence

- **High confidence**: Core architectural components (edge-based color-pair extraction, geometric hashing with Hough voting, tracking with optical flow and neural network) are well-specified and reproducible
- **Medium confidence**: Lighting invariance claims rely on proposed metrics without external validation; accuracy numbers (78.6% ADD-S) are competitive but lag behind state-of-the-art (FoundationPose 92.3%)
- **Low confidence**: Procedural training data generation specifics and Attention-DGCNN architecture details are underspecified, potentially affecting tracker generalization

## Next Checks

1. **Color-pair filtering ablation study**: Replicate Fig. 7 by computing inter-frame pose errors with/without color-pair filtering on held-out YCB-V sequences, measuring rotation and translation error normalized by ground-truth motion magnitude
2. **Texture-poor object stress test**: Evaluate on objects with minimal texture (e.g., YCB bleached cleanser) to characterize failure thresholds and report classification confidence distributions
3. **Hardware profiling**: Deploy TensorRT-optimized pipeline on Jetson AGX Orin; profile each module (segmentation, color-pair extraction, voting, ICP, optical flow, tracking network) to identify bottlenecks when processing 5+ objects