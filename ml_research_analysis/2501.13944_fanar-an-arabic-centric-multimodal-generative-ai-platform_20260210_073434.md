---
ver: rpa2
title: 'Fanar: An Arabic-Centric Multimodal Generative AI Platform'
arxiv_id: '2501.13944'
source_url: https://arxiv.org/abs/2501.13944
tags:
- arabic
- data
- fanar
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fanar introduces a comprehensive Arabic-centric multimodal generative\
  \ AI platform featuring two large language models\u2014Fanar Star (7B parameters,\
  \ trained from scratch) and Fanar Prime (9B parameters, fine-tuned from Gemma-2-9B)\u2014\
  trained on 1 trillion Arabic, English, and code tokens. The platform integrates\
  \ speech and image generation, four retrieval-augmented generation systems (Islamic,\
  \ Recency, Biography, and Attribution RAG), and an attribution service for fact\
  \ verification."
---

# Fanar: An Arabic-Centric Multimodal Generative AI Platform

## Quick Facts
- arXiv ID: 2501.13944
- Source URL: https://arxiv.org/abs/2501.13944
- Reference count: 17
- Introduces two Arabic-focused LLMs (7B from scratch, 9B fine-tuned) with multimodal capabilities

## Executive Summary
Fanar introduces a comprehensive Arabic-centric multimodal generative AI platform featuring two large language models—Fanar Star (7B parameters, trained from scratch) and Fanar Prime (9B parameters, fine-tuned from Gemma-2-9B)—trained on 1 trillion Arabic, English, and code tokens. The platform integrates speech and image generation, four retrieval-augmented generation systems (Islamic, Recency, Biography, and Attribution RAG), and an attribution service for fact verification. Fanar addresses Arabic's unique morphological challenges with a novel tokenizer and provides inclusive dialectal Arabic speech recognition. Evaluation shows superior performance on Arabic benchmarks and cultural tasks compared to peer models, with strong bilingual instruction-following capabilities. The platform aims to empower Arabic-speaking communities through sovereign AI development while maintaining cultural alignment and multimodal accessibility.

## Method Summary
The Fanar platform employs a dual-model strategy: Fanar Star (7B parameters) is trained from scratch using OLMo-style architecture with 1 trillion tokens, while Fanar Prime (9B parameters) is continually trained from Gemma-2-9B with vocabulary pruning. Both models undergo two-stage supervised fine-tuning with culturally-aligned data, followed by direct preference optimization and an annealing phase. The platform features a MorphBPE tokenizer designed for Arabic morphology, four RAG systems for domain-specific queries, and multimodal capabilities including dialect-inclusive speech recognition and culturally-aligned image generation. Training used 168 H100 GPUs with curriculum learning across epochs and extensive data filtering.

## Key Results
- Fanar Star Instruct achieves top scores on Arabic benchmarks (ArabicMMLU: 61.89%, MMMLU(Ar): 69.45%, PIQA(Ar): 78.82%)
- Fanar Prime Instruct excels at STEM reasoning (Arabic MMLU: 64.90%, GSM8K: 83.02%)
- MorphBPE tokenizer improves Arabic representation efficiency over vanilla BPE
- Four specialized RAG systems enhance factual accuracy for Islamic, recency, biography, and attribution queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The dual-model deployment strategy achieves complementary coverage across Arabic linguistic and STEM reasoning tasks through specialized routing.
- **Mechanism:** Fanar Star (7B, trained from scratch) excels on general Arabic tasks; Fanar Prime (9B, continually trained from Gemma-2-9B) leverages stronger multilingual pre-training for reasoning-intensive queries. An orchestrator routes prompts based on domain classification.
- **Core assumption:** Knowledge distillation and transfer from a stronger base model (Gemma-2) provides superior reasoning capabilities that cannot be easily replicated from scratch at similar parameter scales with limited Arabic data.
- **Evidence anchors:**
  - [section 3]: "During orchestration, Fanar Prime is designated to handle STEM and reasoning-related prompts, routed through the specialized classifier."
  - [section 8.3.2, Tables 12-13]: Fanar Prime Instruct achieves top scores on Arabic MMLU (64.90%) and GSM8K (83.02%) versus Fanar Star's 51.32% and 42.84% respectively.
  - [corpus]: Weak—neighbor papers (Hala, ALLaM) focus on single-model Arabic LLMs; no direct comparison of dual-model routing strategies found.

### Mechanism 2
- **Claim:** The MorphBPE tokenizer improves Arabic representation efficiency by preserving morphological boundaries during token merging, reducing semantic ambiguity compared to vanilla BPE.
- **Mechanism:** Standard BPE merges based purely on frequency statistics, producing tokens that may span morpheme boundaries in Arabic's root-pattern system. MorphBPE constrains merges to not cross morpheme boundaries, yielding tokens that better align with linguistic structure.
- **Core assumption:** Morphological alignment improves downstream task performance by enabling the model to learn more semantically meaningful embeddings for Arabic words with complex inflectional paradigms.
- **Evidence anchors:**
  - [section 5.3]: "Unlike standard BPE, MorphBPE includes a modified step... which ensures that merges do not cross morpheme boundaries."
  - [section 5.6, Figure 5]: MorphBPE shows lower cross-entropy loss and higher morphological alignment score than vanilla BPE and existing tokenizers (Bloomz, Jais, Llama 3).
  - [corpus]: Not directly tested—neighbor papers do not evaluate alternative tokenization schemes for Arabic.

### Mechanism 3
- **Claim:** The two-stage SFT with progressive quality filtering and cultural data injection produces better Arabic instruction-following than single-stage training.
- **Mechanism:** Stage-1 uses broad capability data; Stage-2 introduces culturally-aligned synthetic data at lower learning rates, reinforcing nuanced value alignment while preserving Stage-1 capabilities. Annealing phase further consolidates learning.
- **Core assumption:** Task interference between general capabilities and culturally-sensitive behaviors can be mitigated through staged training with decreasing learning rates.
- **Evidence anchors:**
  - [section 7.1]: "Our tests revealed that employing this two-stage training approach, rather than utilizing all the data in a single training round, results in a model that achieves higher performance."
  - [section 7.3]: "While this annealing stage led to a slight decrease in automated benchmark performance, it noticeably reduced the user dislike rate."

## Foundational Learning

- **Arabic Morphology (Root-Pattern System)**
  - **Why needed here:** Understanding why vanilla BPE fails for Arabic requires grasping how words derive from consonantal roots through templatic patterns.
  - **Quick check question:** Can you explain why "kitāb" (book), "kātib" (writer), and "maktaba" (library) share the root k-t-b, and why splitting these morphologically would lose semantic information?

- **Byte-Pair Encoding (BPE) and Vocabulary Learning**
  - **Why needed here:** MorphBPE modifies standard BPE; understanding the baseline algorithm reveals what constraint is being added.
  - **Quick check question:** Given a corpus, how does BPE decide which character pairs to merge first, and what happens if merges cross word boundaries?

- **Curriculum Learning in Language Models**
  - **Why needed here:** Fanar's training uses data mixture adaptation across epochs and a cool-down phase with high-quality data.
  - **Quick check question:** Why would presenting easier or more educational content earlier in training, then increasing difficulty, affect final model performance compared to random shuffling?

## Architecture Onboarding

- **Component map:**
  Chat App/API → Safety Filters → Classifier → Orchestrator → Fanar Star/Fanar Prime/RAG Services/Multimodal → Attribution Service → Safety Check

- **Critical path:**
  1. Prompt ingestion through safety filters
  2. Classification determines routing (general vs. STEM/Islamic/recency)
  3. Model selection: Fanar Prime for STEM/reasoning, Fanar Star for general Arabic
  4. RAG injection if domain-specific (Islamic queries → vector retrieval from curated corpus)
  5. Response generation → post-generation attribution (opt-in)
  6. Safety check on output before delivery

- **Design tradeoffs:**
  - From-scratch vs. continual training: Fanar Star offers more control over Arabic data mixture but requires more compute; Fanar Prime inherits stronger reasoning from Gemma-2 but with vocabulary pruning overhead
  - Tokenizer vocabulary size (76,800 for Star vs. 128,256 for Prime): Smaller vocabulary for from-scratch model reduces embedding parameters but may limit code-switching capability
  - Two-stage SFT: Improves cultural alignment at cost of potential capability interference (observed math regression in early trials)

- **Failure signatures:**
  - Wrong language response: Imbalanced DPO data caused model to default to majority language; fixed by balancing languages and adding mismatched-language rejection pairs
  - High dislike rate on factual queries: ~13% for Fanar Star, attributed to smaller pre-training corpus; mitigated via RAG integration and attribution service
  - Western bias in image generation: Base Stable Cascade generates culturally inappropriate images; addressed via fine-tuning on 100K culturally-aligned images and DPO preference tuning

- **First 3 experiments:**
  1. **Tokenizer ablation:** Train two small models (1B params, 50-100B tokens) using MorphBPE vs. vanilla BPE on identical Arabic data; evaluate on ArabicMMLU and morphological segmentation tasks to quantify gains.
  2. **Routing classifier validation:** Collect 500 diverse prompts across domains; measure classifier accuracy for STEM/general/Islamic routing; identify failure modes and edge cases where wrong model receives the query.
  3. **RAG retrieval quality:** For Islamic RAG, measure retrieval precision@4 for 100 religious queries; compare semantic similarity threshold settings and chunk size (2048 tokens with 50-token overlap) effects on context relevance.

## Open Questions the Paper Calls Out

### Open Question 1: Unified Multimodal Architecture
- Question: Can integrating speech, image, and text generation into a single autoregressive model improve performance and cultural alignment compared to the current modular approach?
- Basis in paper: [explicit] Section 11 states: "For the next version, we plan to make speech, image and text generation as part of a unified generative model."
- Why unresolved: Current multimodal capabilities are separate modules coordinated through an orchestrator, which may introduce latency and prevent cross-modal knowledge transfer.
- What evidence would resolve it: Comparative evaluation of a unified model versus the modular approach on benchmarks measuring generation quality, cultural alignment scores, latency, and cross-modal task performance.

### Open Question 2: Test-Time Computation for Arabic Reasoning
- Question: Does scaling test-time computation through multiple response generation with external reward model selection improve Arabic mathematical and logical reasoning beyond parameter scaling alone?
- Basis in paper: [explicit] Section 11 mentions: "An emerging area is to explore the use of enhanced test time computation for hard tasks (e.g., math reasoning) where multiple responses are generated and an external reward models selects the best answer."
- Why unresolved: The paper focuses on pre-training and post-training but has not explored test-time scaling strategies shown effective for English-centric models.
- What evidence would resolve it: Ablation studies comparing Fanar on Arabic reasoning benchmarks using standard inference versus test-time computation strategies with varying compute budgets.

### Open Question 3: Factuality Improvement Beyond Current RAG Systems
- Question: What innovations beyond existing RAG and attribution systems are required to reduce the ~15% user dislike rate primarily caused by factuality errors in Arabic generation?
- Basis in paper: [explicit] Section 11 notes: "most of [the dislike rate] due to factuality related errors... However new approaches maybe required to increase the confidence of using LLMs for critical tasks."
- Why unresolved: Despite four RAG systems and an attribution service, factuality remains the dominant source of user dissatisfaction.
- What evidence would resolve it: Systematic categorization of factuality error types followed by evaluation of novel approaches (e.g., multi-hop verification, retrieval-augmented reasoning) on curated factuality benchmarks.

## Limitations

- MorphBPE tokenizer implementation details (specific morphological analyzer used) are not specified
- Performance on extremely low-resource Arabic dialects remains untested
- Energy efficiency claims lack third-party validation
- Capability interference observed during two-stage SFT (math regression in early trials)

## Confidence

**High Confidence** (Supported by direct experimental evidence):
- Arabic tokenizer efficiency improvements (MorphBPE vs. vanilla BPE)
- Bilingual instruction-following capability on established benchmarks
- Speech recognition accuracy on Arabic Broadcast News and TED Talks
- Cultural alignment improvements via two-stage SFT

**Medium Confidence** (Well-reasoned but with implementation gaps):
- Dual-model routing strategy effectiveness (classification accuracy not fully validated)
- RAG system performance (retrieval precision not extensively tested)
- Attribution service reliability (2.5% error rate is acceptable but not perfect)

**Low Confidence** (Claims lack sufficient validation):
- Energy efficiency comparisons with other models
- Long-form generation stability across diverse domains
- Performance consistency across all Arabic dialects

## Next Checks

1. **Tokenizer Ablation Study**: Train two 1B-parameter models on identical 50-100B token Arabic datasets using MorphBPE versus vanilla BPE, then evaluate on ArabicMMLU and morphological segmentation tasks to quantify the claimed efficiency gains.

2. **Routing Classifier Validation**: Construct a diverse test set of 500 prompts spanning general knowledge, STEM, Islamic studies, and recent events domains. Measure the classifier's accuracy in correctly routing prompts to the appropriate model (Fanar Star vs. Fanar Prime) and analyze failure modes.

3. **RAG Retrieval Quality Assessment**: For the Islamic RAG system, evaluate retrieval precision@4 on 100 religious queries using semantic similarity metrics. Test different chunk sizes (2048 tokens with 50-token overlap) and semantic similarity thresholds to optimize context relevance and minimize retrieval errors.