---
ver: rpa2
title: 'Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey'
arxiv_id: '2507.22920'
source_url: https://arxiv.org/abs/2507.22920
tags:
- quantization
- discrete
- image
- generation
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys discrete tokenization methods for multimodal
  large language models (LLMs), focusing on vector quantization (VQ) techniques that
  convert continuous multimodal data into discrete tokens compatible with LLM architectures.
  The authors present a structured taxonomy of 8 representative VQ variants, analyze
  their algorithmic principles, training dynamics, and integration challenges, and
  review applications across images, audio, video, graphs, and recommendation systems.
---

# Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey

## Quick Facts
- **arXiv ID**: 2507.22920
- **Source URL**: https://arxiv.org/abs/2507.22920
- **Reference count**: 40
- **Primary result**: Comprehensive survey of discrete tokenization methods for multimodal LLMs using vector quantization techniques

## Executive Summary
This survey systematically examines discrete tokenization methods that convert continuous multimodal data into discrete tokens compatible with language model architectures. The authors present a taxonomy of 8 representative vector quantization (VQ) variants, analyzing their algorithmic principles, training dynamics, and integration challenges across diverse modalities including images, audio, video, graphs, and recommendation systems. Key challenges identified include codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. The survey also outlines emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning.

## Method Summary
The survey analyzes discrete tokenization through vector quantization pipelines where continuous multimodal inputs are encoded into latent representations, then mapped to discrete codebook entries via nearest-neighbor search. Training employs reconstruction losses with gradient flow approximated through Straight-Through Estimation (STE) for the non-differentiable quantization operation. Codebook vectors are maintained using Exponential Moving Average (EMA) updates rather than direct gradient descent. The survey covers 8 VQ variants including standard VQ, residual VQ, product VQ, adaptive VQ, and various finite/lattice-free approaches, examining their integration into multimodal LLM architectures.

## Key Results
- Vector quantization enables multimodal LLMs by converting continuous inputs to discrete tokens through nearest-neighbor codebook lookup
- Codebook collapse remains a primary failure mode, with typical utilization rates dropping below 10% without regularization
- EMA-based codebook updates provide more stable training than direct gradient descent on codebook vectors
- Integration challenges include balancing reconstruction fidelity with compression rate and preventing dead codes in large codebooks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Discrete tokenization enables multimodal data processing by LLMs by mapping continuous inputs to a finite set of learned vectors that mimic text tokens.
- **Mechanism**: An encoder transforms raw data into a latent vector z. A vector quantization (VQ) layer then finds the nearest entry in a codebook C (nearest neighbor search). This index acts as a discrete token, allowing the LLM to process non-text modalities using its standard embedding lookup and attention mechanisms.
- **Core assumption**: The discrete codebook captures the semantic essence of the continuous modality sufficiently for the downstream task.
- **Evidence anchors**: [Abstract] "...transform continuous multimodal data into discrete representations suitable for language-based processing." [Page 3, Section 2] "...discrete quantization pipeline begins with input data x... discretized to a specific representation c_q in the codebook..."
- **Break condition**: If the codebook capacity is too small or the encoder fails to cluster similar features, distinct inputs map to identical tokens (information loss), causing semantic confusion in the LLM.

### Mechanism 2
- **Claim**: End-to-end training is enabled by approximating gradients through the non-differentiable quantization bottleneck using Straight-Through Estimation (STE).
- **Mechanism**: The argmin operation used to find the nearest codebook entry is non-differentiable. STE approximates the gradient by treating the quantization operation as an identity function during the backward pass, effectively copying the gradient from the decoder input directly to the encoder output.
- **Core assumption**: The approximation bias introduced by ignoring the quantization step in the backward pass does not destabilize the encoder's learning trajectory.
- **Evidence anchors**: [Page 3, Section Q2] "STE offers a heuristic method... treats the quantization as an identity function during the backward pass..." [Page 3, Fig. 3] Shows the "Straight-Through Estimator (STE)" explicitly bridging the Quantization block.
- **Break condition**: If the codebook entries are far from the encoder outputs, the gradient approximation misdirects the encoder, leading to divergence or "dead" codebooks.

### Mechanism 3
- **Claim**: Codebook relevance is maintained via Exponential Moving Average (EMA) updates rather than direct gradient descent on codebook vectors.
- **Mechanism**: Instead of optimizing codebook vectors via standard gradient descent, EMA updates the vectors based on the running average of encoder outputs assigned to them. This allows the codebook to track the evolving distribution of the encoder's latent space smoothly.
- **Core assumption**: The distribution of encoder outputs changes slowly enough for the EMA decay factor to track it effectively.
- **Evidence anchors**: [Page 4, Section 2.1.3] "EMA strategy updates the codebook by progressively reflecting the distribution of encoder outputs..." [Page 4, Eq. 11-13] Defines the statistical accumulation and update rule for codewords.
- **Break condition**: If the encoder outputs drift rapidly or cluster tightly in a small region, EMA updates may lag, causing codebook collapse where only a few vectors are utilized.

## Foundational Learning

- **Concept**: **Vector Quantization (VQ)**
  - **Why needed here**: This is the fundamental operation transforming continuous data into discrete tokens. Understanding the distance metric (Euclidean) and the "nearest neighbor" lookup is critical for debugging tokenization quality.
  - **Quick check question**: How does the system determine which token represents a specific image patch?

- **Concept**: **Codebook Collapse (Dead Codes)**
  - **Why needed here**: A primary failure mode in discrete tokenization where the model utilizes only a tiny fraction of the available codebook, limiting expressiveness.
  - **Quick check question**: If your codebook has 8192 entries but only 50 are used repeatedly, what regularization technique (e.g., EMA, Code Reset) should you inspect?

- **Concept**: **Stop-Gradient Operator**
  - **Why needed here**: Essential for stabilizing VQ training. It allows the loss function to optimize the codebook to match encoder outputs, and the encoder to match codebook outputs, without creating conflicting gradient loops.
  - **Quick check question**: In the VQ-VAE loss function, where is the stop-gradient operator applied to ensure the codebook "moves" toward the encoder output?

## Architecture Onboarding

- **Component map**: Input Modality → Encoder (CNN/Transformer) → Latent Space → Quantizer (Codebook Lookup) → Discrete Tokens → LLM/Decoder

- **Critical path**: The connection between the Encoder Output and the Codebook Index. If the distance calculation here is flawed or the gradient flow (via STE) is blocked, the LLM receives garbage tokens.

- **Design tradeoffs**:
  - VQ vs. FSQ/LFQ: VQ allows unlimited codebook size (growing with memory) but suffers from collapse. FSQ/LFQ use implicit codebooks to guarantee utilization but limit maximum expressiveness per dimension.
  - Granularity: Higher compression (fewer tokens) improves LLM efficiency but loses fine-grained details necessary for generation tasks.

- **Failure signatures**:
  - Index Collapse: The histogram of token indices shows a spike on a few values (indicating codebook collapse).
  - Reconstruction Drift: The decoder generates blurry outputs, suggesting the commitment loss weight is too low or EMA decay is too fast.
  - Training Instability: Loss spikes indicate the encoder and codebook are "chasing" each other without convergence.

- **First 3 experiments**:
  1. Codebook Utilization Audit: Run inference on a validation set and plot the frequency of codebook index usage. If usage is <20%, investigate EMA settings or linear reparameterization.
  2. Reconstruction Quality Baseline: Train the tokenizer in isolation (without the LLM) to verify it can autoencode the modality before attempting multimodal alignment.
  3. Gradient Flow Check: Monitor the magnitude of gradients passing through the STE mechanism. If they are vanishing, the codebook vectors may be too far from the encoder outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can hybrid architectures be designed to effectively unify discrete and continuous tokens during training and inference to leverage the advantages of both?
- **Basis in paper**: [explicit] Section 6(e) states that while discrete and continuous representations have distinct advantages, existing works often separate them. The authors suggest that "Developing hybrid architectures that unify discrete and continuous tokens... represents a promising direction."
- **Why unresolved**: Current models typically utilize either discrete or continuous spaces, failing to leverage the synergy between the compactness/interpretability of discrete tokens and the gradient-based optimization/fidelity of continuous embeddings.
- **What evidence would resolve it**: The successful training of architectures that use continuous features to inform discrete selection or structure continuous generation with discrete priors, showing improved interoperability.

### Open Question 2
- **Question**: Can adaptive and hierarchical quantization techniques successfully modulate token granularity based on content complexity to align with semantic boundaries?
- **Basis in paper**: [explicit] Section 6(d) highlights the struggle to balance token granularity and align with semantic boundaries. The authors suggest "promising directions include adaptive and hierarchical quantization that modulates granularity based on content complexity and semantics."
- **Why unresolved**: Coarse tokens miss details while fine-grained tokens inflate sequence length, and meaningful units in continuous modalities are often ambiguous or task-specific.
- **What evidence would resolve it**: The implementation of dynamic masking or attention-guided segmentation that demonstrates more efficient and interpretable representations aligned with structural content.

### Open Question 3
- **Question**: What specific approaches can balance token diversity and coverage with training stability to effectively prevent codebook collapse?
- **Basis in paper**: [explicit] Section 6(a) identifies codebook collapse as a key challenge, noting that while existing techniques help, they "often compromise stability." The authors suggest exploring "curriculum-based code activation schedules" or "hybrid codebook designs."
- **Why unresolved**: Current methods to prevent under-utilization frequently introduce instability or convergence issues, creating a trade-off between codebook usage and robust training.
- **What evidence would resolve it**: Novel scheduling or hybrid designs that demonstrate high utilization rates alongside stable convergence metrics without sacrificing downstream performance.

## Limitations

- **Generalizability concerns**: The survey focuses on 8 VQ variants, but rapid evolution may mean emerging techniques don't fit the taxonomy
- **Benchmarking inconsistency**: Different methods use varying datasets, metrics, and implementation details, making direct performance comparisons challenging
- **Capacity trade-off gap**: The survey discusses compression rates but lacks quantitative guidance on optimal codebook sizing for different modalities

## Confidence

**High Confidence**: The fundamental mechanisms of vector quantization, EMA-based codebook updates, and STE gradient approximation are well-established in the literature and consistently described across multiple surveyed papers.

**Medium Confidence**: The taxonomy and categorization of VQ variants is reasonable but somewhat arbitrary, as many methods share overlapping components and could fit into multiple categories.

**Low Confidence**: Claims about relative performance between different VQ variants lack empirical support due to inconsistent benchmarking practices across the surveyed literature.

## Next Checks

1. **Codebook Utilization Benchmark**: Implement a standardized test suite to measure codebook utilization across different VQ variants using the same encoder architecture and dataset. Track utilization over training epochs to identify collapse patterns.

2. **Cross-Modal Generalization**: Test a single VQ implementation (e.g., VQ-VAE) across multiple modalities (images, audio, video) using identical hyperparameters to identify modality-specific failure modes.

3. **Gradient Flow Analysis**: Instrument the STE mechanism to measure gradient magnitudes and variance during training. Correlate these metrics with reconstruction quality and codebook utilization to identify optimal training dynamics.