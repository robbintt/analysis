---
ver: rpa2
title: 1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language
  Model Training
arxiv_id: '2503.19633'
source_url: https://arxiv.org/abs/2503.19633
tags:
- data
- reasoning
- dataset
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors constructed a large-scale reasoning dataset called
  AM-DeepSeek-R1-Distilled containing 1.4 million high-quality problems with thinking
  traces, derived from open-source datasets and distilled from DeepSeek-R1. The data
  underwent semantic deduplication, cleaning, and verification (via reference checking,
  test cases, or reward models) to ensure quality.
---

# 1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training

## Quick Facts
- **arXiv ID:** 2503.19633
- **Source URL:** https://arxiv.org/abs/2503.19633
- **Reference count:** 14
- **Primary result:** AM-Distill-Qwen-32B outperforms DeepSeek-R1-Distill-Qwen-32B on four benchmarks, with average accuracy gains of 1.5 percentage points.

## Executive Summary
The authors present AM-DeepSeek-R1-Distilled, a 1.4 million sample reasoning dataset constructed from open-source sources and distilled from DeepSeek-R1. The dataset underwent rigorous cleaning and verification, including semantic deduplication and quality checks via reference verification, test cases, or reward models. Using simple supervised fine-tuning (SFT) on Qwen2.5 models, the authors trained AM-Distill-Qwen-32B and AM-Distill-Qwen-72B, which achieved superior performance on AIME2024, MATH-500, GPQA-Diamond, and LiveCodeBench compared to DeepSeek-R1 distilled variants, with average accuracy gains of 1.5 and 3.0 percentage points respectively.

## Method Summary
The study involved constructing a large-scale reasoning dataset by combining 0.5M open-source samples with 0.9M distilled from DeepSeek-R1. The data was cleaned and verified through semantic deduplication and quality checks. Two models, AM-Distill-Qwen-32B and AM-Distill-Qwen-72B, were trained via supervised fine-tuning on Qwen2.5-32B and Qwen2.5-72B using this dataset. The training used a specific system prompt format with `<thinkthink>` and `<answer>` tags. Evaluation was performed using majority voting with temperature=0.6, top_p=0.95, and max_tokens=32768, with different sample counts per benchmark (16 for AIME2024, 4 for others).

## Key Results
- AM-Distill-Qwen-32B achieved 73.1% average accuracy across four benchmarks, outperforming DeepSeek-R1-Distill-Qwen-32B.
- AM-Distill-Qwen-72B reached 74.8% average accuracy, surpassing DeepSeek-R1-Distill-Llama-70B.
- The 32B model showed a 1.5 percentage point average accuracy gain, while the 72B model demonstrated a 3.0 percentage point improvement.

## Why This Works (Mechanism)
The performance gains stem from the high-quality, large-scale reasoning dataset that combines open-source samples with distilled reasoning traces from DeepSeek-R1. The rigorous data cleaning and verification process ensures that the training data is both diverse and accurate. The specific system prompt format with thinking trace tags likely encourages the model to generate step-by-step reasoning, which is crucial for solving complex problems in mathematics, coding, and scientific domains.

## Foundational Learning
- **Supervised Fine-Tuning (SFT):** Fine-tuning pre-trained models on labeled data to specialize in a task. *Why needed:* To adapt general-purpose models to reasoning tasks using the curated dataset. *Quick check:* Verify that the model's loss decreases during training on the reasoning dataset.
- **Data Distillation:** Extracting high-quality reasoning traces from larger models (DeepSeek-R1) to create training data. *Why needed:* To leverage the reasoning capabilities of larger models without requiring their full computational cost. *Quick check:* Compare distilled samples against original DeepSeek-R1 outputs for consistency.
- **Majority Voting Evaluation:** Using multiple sampled outputs to estimate model performance. *Why needed:* To account for stochasticity in generation and obtain more robust accuracy estimates. *Quick check:* Ensure that increasing sample count improves stability of reported metrics.
- **Semantic Deduplication:** Removing redundant or near-duplicate samples from the dataset. *Why needed:* To prevent overfitting and ensure diverse training examples. *Quick check:* Verify dataset size reduction after deduplication step.
- **System Prompt Engineering:** Using specific tags (`<thinkthink>`, `<answer>`) to structure model outputs. *Why needed:* To guide the model to generate explicit reasoning traces followed by answers. *Quick check:* Inspect model outputs to confirm proper tag usage and thinking trace generation.
- **Context Window Management:** Handling 32,768 token sequences during training and generation. *Why needed:* To accommodate long reasoning traces and complex problems. *Quick check:* Monitor for truncation warnings or token length errors during data loading.

## Architecture Onboarding
- **Component map:** Dataset (AM-DeepSeek-R1-Distilled) -> SFT Training -> Evaluation (benchmarks)
- **Critical path:** Data preparation with proper formatting → SFT training with appropriate hyperparameters → Evaluation with correct sampling strategy
- **Design tradeoffs:** Larger context window (32k) enables complex reasoning but increases computational cost; majority voting provides robust estimates but requires more inference time
- **Failure signatures:** Context truncation errors, missing thinking traces in outputs, unstable benchmark results across runs
- **Three first experiments:**
  1. Train a smaller model (e.g., 7B) on a subset of the data to verify the training pipeline works
  2. Test the system prompt formatting by generating outputs and checking for proper `<thinkthink>` and `<answer>` tags
  3. Evaluate on a single benchmark with varying sample counts to determine optimal pass@k setting

## Open Questions the Paper Calls Out
None

## Limitations
- Missing training hyperparameters (learning rate, batch size, epochs) make exact reproduction challenging
- Evaluation methodology using temperature=0.6 and top_p=0.95 may introduce variance in reported results
- Comparison primarily against DeepSeek-R1 distilled variants rather than a broader range of reasoning models

## Confidence
- **Claim:** AM-Distill-Qwen models outperform DeepSeek-R1 distilled variants
- **Label:** Medium
- **Reasoning:** Direct benchmark comparisons support the claim, but missing training details and limited competitive analysis reduce reproducibility confidence

## Next Checks
1. Re-run the training pipeline with a fixed hyperparameter set (e.g., lr=3e-5, 2 epochs, batch size 32) and verify if the performance gap is maintained
2. Evaluate model outputs to confirm the correct system prompt format is being used and that the thinking traces are properly generated and parsed
3. Conduct an ablation study comparing AM-Distill-Qwen-32B trained on 0.5M open-source samples versus the full 1.4M dataset to quantify the impact of the distilled DeepSeek-R1 data