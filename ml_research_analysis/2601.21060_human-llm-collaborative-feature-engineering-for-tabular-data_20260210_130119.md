---
ver: rpa2
title: Human-LLM Collaborative Feature Engineering for Tabular Data
arxiv_id: '2601.21060'
source_url: https://arxiv.org/abs/2601.21060
tags:
- feature
- human
- engineering
- performance
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-LLM collaborative framework for feature
  engineering on tabular data. The core idea is to decouple feature operation proposal
  (handled by an LLM) from selection (guided by a Bayesian surrogate model estimating
  utility and uncertainty).
---

# Human-LLM Collaborative Feature Engineering for Tabular Data

## Quick Facts
- arXiv ID: 2601.21060
- Source URL: https://arxiv.org/abs/2601.21060
- Reference count: 40
- One-line primary result: Human-LLM collaborative framework outperforms LLM-only and AutoML baselines on 18 tabular datasets while reducing cognitive load

## Executive Summary
This paper introduces a human-LLM collaborative framework for feature engineering on tabular data that decouples feature operation proposal from selection. The LLM generates candidate transformations while a Bayesian surrogate model with uncertainty quantification guides selection via Upper Confidence Bound (UCB). Human preference feedback is selectively elicited only when it can meaningfully improve selection. Evaluations on 18 datasets and a user study demonstrate consistent performance improvements over baselines while reducing cognitive burden for human collaborators.

## Method Summary
The framework operates iteratively where GPT-4o proposes N=15 candidate feature transformations per round, which are embedded using text-embedding-3-small concatenated with a binary column-usage mask. A Bayesian Neural Network surrogate learns a variational posterior over utility estimates and uncertainties, using UCB selection to balance exploration and exploitation. Human feedback is triggered only when conditions indicate potential performance gains, updating the posterior via probit likelihood. The selected operation is executed and evaluated on a downstream model (MLP or XGBoost), with results added to the history for the next iteration.

## Key Results
- Outperforms LLM-only and AutoML baselines on 18 tabular datasets with 8.96% average error reduction
- User study shows significant performance improvement (p=0.011) and lower cognitive load (p<0.001) compared to control
- Surrogate fitting and UCB computation add only 0.2-0.6s regardless of dataset size
- Framework achieves notable performance jumps while baselines become trapped in local optima

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling feature operation proposal from selection improves exploration efficiency compared to LLM-as-black-box-optimizer approaches.
- **Mechanism:** The LLM generates diverse candidate operations from its internal proposal distribution while a separate surrogate model estimates utility and uncertainty for each candidate. This prevents the LLM from repeatedly exploring low-yield operations based on uncalibrated internal heuristics.
- **Core assumption:** The LLM's proposal distribution contains useful transformations, but its internal selection heuristics are poorly calibrated for utility estimation.
- **Evidence anchors:** [abstract] "current approaches typically assign the LLM as a black-box optimizer... which often lack calibrated estimations of operation utility"; [section 3.1] Equation 3 shows the LLM samples candidates; selection is decoupled via surrogate modeling; [corpus] LLM-FE (arxiv 2503.14434) treats LLM as evolutionary optimizer—complementary approach showing proposal-selection coupling is a known limitation.
- **Break condition:** If the LLM's proposal distribution fails to generate any high-utility operations, the surrogate cannot select what doesn't exist.

### Mechanism 2
- **Claim:** Bayesian Neural Network surrogate with uncertainty quantification enables principled exploration via UCB selection.
- **Mechanism:** The BNN learns a variational posterior q_t(θ) over model parameters, yielding both expected utility μ_t(e) and uncertainty σ²_t(e). UCB selection (μ_t(e) + √β_t σ_t(e)) balances exploiting high-predicted-utility operations with exploring uncertain ones.
- **Core assumption:** The true utility function can be approximately linearly represented in the surrogate feature space (Assumption A.1).
- **Evidence anchors:** [section 3.2] Equation 6 defines expected utility and uncertainty; Equation 8 defines UCB selection; [section 4.3] Figure 1 shows performance trajectories with "notable performance jumps at various points" vs. baselines that "become trapped in local optima"; [corpus] Weak direct corpus evidence on BNN surrogates for feature engineering specifically.
- **Break condition:** If the embedding ϕ(e) fails to capture operation similarity structure, uncertainty estimates become unreliable.

### Mechanism 3
- **Claim:** Selective human preference elicitation improves selection while limiting cognitive cost.
- **Mechanism:** Human feedback is triggered only when (C1) UCB_t(e_b) > LCB_t(e_a) [overlap exists] AND (C2) √β_t(σ_t(e_a) + σ_t(e_b)) ≥ γ_κ [potential gain exceeds cost]. Preference is incorporated via probit likelihood updating the posterior.
- **Core assumption:** Human expertise provides signal about relative utility that the surrogate cannot extract from performance history alone, especially in early rounds.
- **Evidence anchors:** [section 3.3] Equations 13-15 define trigger conditions; Equation 16-17 show posterior update; [section 4.2] "Ours (w/ human) achieves 8.96% [avg error reduction]... compared to the best baseline" (MLP); [section 4.5] User study shows ALG condition significantly outperforms CONTROL (p=0.011) with lower cognitive load (p<0.001); [corpus] Knowledge-Informed Automatic Feature Extraction (arxiv 2511.15074) uses collaborative LLM agents—supports human-in-the-loop value.
- **Break condition:** If human feedback is noisy or systematically biased, posterior updates may degrade rather than improve selection.

## Foundational Learning

- **Concept: Bayesian Optimization with Acquisition Functions**
  - Why needed here: The framework uses UCB as an acquisition function to balance exploration-exploitation. Understanding why UCB = μ + √βσ encourages sampling uncertain regions is essential.
  - Quick check question: Given two operations with μ=0.5, σ=0.1 vs. μ=0.4, σ=0.3, which would UCB prefer with β=2?

- **Concept: Variational Inference for BNNs**
  - Why needed here: The surrogate learns q_t(θ) ≈ P(θ|H_t) via KL divergence minimization. This enables uncertainty quantification without expensive MCMC.
  - Quick check question: Why does minimizing KL(q||P) tend to underestimate posterior variance compared to the true posterior?

- **Concept: Preference-Based Learning with Probit Models**
  - Why needed here: Human feedback Z_t ∈ {+1, -1} is modeled via Φ(ηZ_t[ĝ(e_a) - ĝ(e_b)]), converting preferences to probabilistic constraints.
  - Quick check question: If η→∞, what does the probit likelihood converge to?

## Architecture Onboarding

- **Component map:** LLM Proposal Engine -> Embedding Encoder -> BNN Surrogate -> UCB Selector -> (optional) Human Feedback Module -> Evaluation Loop -> History Update

- **Critical path:** LLM proposal → embedding → BNN forward pass → UCB computation → (optional) human query → posterior update → model evaluation → history update. The surrogate fitting and UCB computation add ~0.2-0.6s regardless of dataset size (Tables 3-4).

- **Design tradeoffs:**
  - BNN vs. GP surrogate: BNN scales better for high-dimensional text embeddings but requires more data to calibrate uncertainty
  - Query cost γ_κ: Set to 4 empirically; higher values reduce queries but may miss valuable feedback
  - Candidates per round N=15: More candidates increase selection quality but raise LLM costs

- **Failure signatures:**
  - Stagnant performance trajectory with no jumps → surrogate uncertainty collapsed prematurely; increase β_t or add noise
  - Excessive human queries → C2 threshold too low; increase γ_κ
  - Selected operations always fail validation → embedding ϕ(e) not capturing utility-relevant structure; consider richer encodings

- **First 3 experiments:**
  1. **Ablate surrogate:** Replace BNN with random selection among LLM proposals to quantify surrogate contribution (expect significant degradation per Figure 1)
  2. **Vary query threshold:** Test γ_κ ∈ {2, 4, 8} on 3 datasets to find cost-performance Pareto frontier
  3. **Stress-test embedding:** On dataset with many semantically similar columns (e.g., adult with demographic features), inspect whether ϕ_column successfully disambiguates operations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the framework's performance when real human experts (rather than GPT-4o as a simulated proxy) provide preference feedback, particularly in domains where LLMs may lack specialized knowledge?
- Basis in paper: [explicit] The paper states in Section 4.1 that they "employ GPT-4o as a proxy as the human expert to provide the preference feedback" and use SHAP-generated feature importance to simulate expertise, acknowledging this as an approximation.
- Why unresolved: The simulated feedback may not capture the full breadth of human domain expertise, reasoning patterns, or the noise/inconsistency inherent in real human judgments.
- What evidence would resolve it: A user study comparing performance when the surrogate model is trained on simulated vs. real human preference feedback across multiple domains, measuring both feature engineering outcomes and feedback quality.

### Open Question 2
- Question: How sensitive is the selective elicitation mechanism to the fixed cost parameter γ_κ = 4, and can this parameter be adaptively tuned for different users or tasks?
- Basis in paper: [explicit] The paper notes that "γ_κ is empirically set to 4 in this study to balance the trade-off between final performance and efficiency," but does not explore whether this value generalizes.
- Why unresolved: Cognitive costs likely vary across users, tasks, and over time within a session; a fixed threshold may over- or under-query in different contexts.
- What evidence would resolve it: Ablation studies varying γ_κ and user studies that measure perceived cognitive burden alongside performance to identify optimal or adaptive cost settings.

### Open Question 3
- Question: What is the impact of the linear utility assumption (Assumption A.1) on the practical reliability of the confidence bounds in Lemma 3.1?
- Basis in paper: [inferred] The theoretical guarantees rely on Assumption A.1 that "the true utility of a feature transformation operation g(e) can be linearly represented in the feature space φ(·)," but this assumption may not hold for complex feature interactions.
- Why unresolved: If the true utility function has significant nonlinear components in the embedding space, the confidence bounds may be misspecified, affecting both UCB selection and human query triggering.
- What evidence would resolve it: Empirical analysis comparing predicted vs. actual utilities across operations, and experiments using alternative surrogate models (e.g., deep ensembles) that relax the linearity assumption.

### Open Question 4
- Question: How does the framework generalize to alternative forms of human feedback beyond pairwise preferences, such as cardinal ratings, natural language explanations, or multi-candidate ranking?
- Basis in paper: [inferred] The paper restricts human input to pairwise preference format, citing prior work (Kahneman & Tversky) on relative judgment, but does not compare against richer feedback modalities that could convey more information per query.
- Why unresolved: Other feedback forms might provide higher information density or better alignment with how experts naturally express domain knowledge.
- What evidence would resolve it: Comparative studies measuring performance-per-query and user cognitive load across different feedback interfaces, particularly for complex feature operations that are difficult to compare directly.

## Limitations
- Simulated human feedback via GPT-4o may not capture real human expertise and judgment patterns
- Fixed query cost parameter γ_κ may not generalize across different users or domains
- Linear utility assumption in embedding space may not hold for complex feature interactions

## Confidence

| Claim | Evidence Strength | Assessment |
|-------|-------------------|------------|
| Performance improvements over baselines | User study p=0.011, avg error reduction 8.96% | High |
| Selective elicitation reduces cognitive load | User study p<0.001 for cognitive load reduction | High |
| BNN surrogate enables efficient exploration | Figure 1 shows performance jumps vs. baseline stagnation | Medium |
| Decoupling proposal from selection is beneficial | Complementary evidence from LLM-FE literature | Medium |

## Next Checks
1. Implement the embedding pipeline combining text-embedding-3-small with binary column-usage mask and verify on sample operations
2. Build and test the BNN surrogate with variational inference to ensure uncertainty estimates are properly calibrated
3. Execute the full iterative loop on a small dataset (e.g., wine) and verify that performance trajectories show expected jumps rather than stagnation