---
ver: rpa2
title: Procedural Fairness in Multi-Agent Bandits
arxiv_id: '2601.10600'
source_url: https://arxiv.org/abs/2601.10600
tags:
- fairness
- procedural
- each
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces procedural fairness as a new fairness objective
  in multi-agent multi-armed bandits, ensuring each agent has equal decision-making
  power. The authors formalize procedural fairness alongside outcome-based fairness
  notions like equality and utilitarian fairness.
---

# Procedural Fairness in Multi-Agent Bandits

## Quick Facts
- arXiv ID: 2601.10600
- Source URL: https://arxiv.org/abs/2601.10600
- Authors: Joshua Caiata; Carter Blair; Kate Larson
- Reference count: 40
- Primary result: Introduces procedural fairness as a new fairness objective ensuring equal decision-making power for all agents in multi-agent multi-armed bandits

## Executive Summary
This paper introduces procedural fairness as a novel fairness objective in multi-agent multi-armed bandits, ensuring each agent has equal decision-making power rather than focusing solely on outcome-based fairness. The authors formalize procedural fairness alongside traditional notions like equality and utilitarian fairness, proving these objectives are fundamentally incompatible and require explicit normative choices. Learning algorithms with sublinear regret guarantees are provided for each fairness type, with empirical results confirming that procedural fairness balances efficiency and equality while maintaining legitimacy through coalitional stability.

## Method Summary
The paper presents a formal framework for fairness in multi-agent multi-armed bandits by introducing procedural fairness as distinct from outcome-based fairness notions. The authors develop theoretical proofs demonstrating the incompatibility between procedural fairness and outcome-based objectives (equality and utilitarian fairness). They design learning algorithms with sublinear regret guarantees for each fairness type and prove that procedurally fair policies lie in the procedural core, ensuring stability against coalitional deviation. Empirical validation is conducted through simulations comparing the performance and fairness trade-offs of different policy approaches.

## Key Results
- Procedural fairness fundamentally conflicts with outcome-based fairness notions (equality and utilitarian), requiring explicit normative choices
- Procedurally fair policies ensure coalitional stability by lying in the procedural core
- Empirical results show procedural fairness balances efficiency and equality with minimal sacrifice in outcome-based fairness objectives
- Learning algorithms achieve sublinear regret guarantees for each fairness type

## Why This Works (Mechanism)
Procedural fairness works by explicitly incorporating equal decision-making power into the bandit algorithm design, rather than treating fairness as solely an outcome property. This mechanism ensures each agent has equal influence over the exploration-exploitation trade-off, creating legitimacy through democratic participation. The theoretical framework proves that this procedural approach necessarily conflicts with outcome-based objectives, as maximizing both equal voice and equal outcomes or total utility simultaneously is impossible. By formalizing this trade-off, the approach forces explicit normative choices about which fairness notion to prioritize.

## Foundational Learning

**Multi-armed bandit problem**: Sequential decision-making framework where agents choose actions to maximize cumulative reward while balancing exploration and exploitation. Needed to understand the core problem setting; quick check: can the learner distinguish between optimal and suboptimal arms.

**Procedural fairness**: Fairness defined by equal decision-making power rather than outcome equality. Needed to distinguish from traditional fairness notions; quick check: does each agent have equal influence on arm selection.

**Equality fairness**: Outcome-based fairness ensuring all agents receive equal cumulative rewards. Needed as a baseline comparison; quick check: do all agents achieve similar reward distributions.

**Utilitarian fairness**: Outcome-based fairness maximizing total cumulative reward across all agents. Needed to represent efficiency-focused objectives; quick check: is the total reward maximized regardless of individual distribution.

**Procedural core**: Game-theoretic solution concept ensuring stability against coalitional deviation. Needed to prove procedural fairness provides legitimate outcomes; quick check: would any coalition benefit by deviating from the proposed policy.

## Architecture Onboarding

**Component map**: Learning algorithm -> Fairness constraint module -> Arm selection policy -> Reward feedback loop -> Agent utility tracking

**Critical path**: Agent preferences → Fairness objective selection → Constraint formulation → Algorithm design → Regret analysis → Empirical validation

**Design tradeoffs**: Equal voice vs. optimal outcomes, theoretical guarantees vs. computational complexity, generality vs. problem-specific optimization

**Failure signatures**: 
- If equality fairness is enforced, some agents may dominate arm selection
- If utilitarian fairness is enforced, minority agent preferences may be ignored
- If procedural fairness is enforced, total reward may be suboptimal

**First experiments**:
1. Compare procedural vs equality vs utilitarian fairness across synthetic reward distributions
2. Test coalitional stability by simulating agent coalitions under each fairness regime
3. Evaluate regret performance while varying the number of agents and arms

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation limited to specific simulation scenarios, reducing generalizability to real-world applications
- Theoretical claims rely on specific game-theoretic assumptions that may not hold in all multi-agent bandit settings
- Normative choice framework could benefit from more explicit discussion of when each fairness notion should be prioritized

## Confidence
High: Procedural fairness fundamentally conflicts with outcome-based fairness (proven theoretically)
Medium: Procedurally fair policies ensure coalitional stability (theoretically sound but assumption-dependent)
Low: Empirical results generalize to real-world applications (limited simulation scope)

## Next Checks
1. Test the algorithms across diverse real-world bandit problems with varying reward distributions and agent numbers
2. Validate the normative choice framework through expert interviews or surveys to determine appropriate fairness trade-offs in practice
3. Extend the theoretical analysis to examine how the fairness incompatibility behaves under different reward correlation structures and agent populations