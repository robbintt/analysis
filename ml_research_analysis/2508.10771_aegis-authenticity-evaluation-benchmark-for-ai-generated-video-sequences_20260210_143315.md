---
ver: rpa2
title: 'AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences'
arxiv_id: '2508.10771'
source_url: https://arxiv.org/abs/2508.10771
tags:
- video
- aegis
- videos
- detection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AEGIS, a novel benchmark designed to address
  the challenges of detecting AI-generated video sequences, which pose significant
  risks to societal trust and digital integrity. Unlike existing benchmarks that often
  focus on image-level AI-generated content or simple animations, AEGIS specifically
  targets hyper-realistic and semantically nuanced videos, providing a comprehensive
  evaluation of detection capabilities.
---

# AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences

## Quick Facts
- arXiv ID: 2508.10771
- Source URL: https://arxiv.org/abs/2508.10771
- Authors: Jieyu Li; Xin Zhang; Joey Tianyi Zhou
- Reference count: 40
- Primary result: Current vision-language models fail to detect AI-generated videos, especially from proprietary models like Sora and KLing

## Executive Summary
This paper introduces AEGIS, a benchmark designed to evaluate video authenticity detection capabilities against AI-generated content. Unlike existing benchmarks focused on images or simple animations, AEGIS specifically targets hyper-realistic and semantically nuanced videos generated by state-of-the-art models including Stable Video Diffusion, CogVideoX-5B, KLing, and Sora. The benchmark includes over 10,000 videos with multimodal annotations (semantic descriptions, optical flow, frequency features) and demonstrates that current VLMs struggle significantly with detection, particularly on challenging subsets. The results highlight the need for more robust detection methods and establish AEGIS as a critical resource for advancing research in video authenticity verification.

## Method Summary
AEGIS is constructed through a systematic pipeline: collecting real videos from Vript, DVF, and YouTube, and synthetic videos from multiple generative models including SVD, CogVideoX-5B, I2VGen-XL, Pika, KLing, and Sora. Videos undergo three filtering stages—removing non-photorealistic content, filtering out easily-detectable AI-generated videos using VLM classification, and ensuring diversity across scenes and resolutions. Multimodal annotations are generated including semantic descriptions via GPT-4V, optical flow via RAFT, and frequency features via FFT/RIO. The dataset is split into training (7,304 videos), validation (2,730 videos), and Hard Test Set (436 videos) that specifically challenges detection capabilities with proprietary model outputs.

## Key Results
- Current VLMs achieve only 0.22-0.23 Macro-F1 on Hard Test Set, demonstrating severe detection limitations
- LoRA fine-tuning improves in-domain performance (F1=0.82) but only marginally helps on Hard Test Set (F1=0.55)
- Structured reasoning prompting actually decreases detection accuracy, dropping from 0.22 to 0.16 Macro-F1
- Proprietary models Sora and KLing pose the greatest detection challenges, with only 218 videos in Hard Test Set

## Why This Works (Mechanism)

### Mechanism 1
Multi-source generative diversity increases detection difficulty by exposing models to heterogeneous artifact distributions rather than model-specific patterns. Different architectures produce distinguishable artifact signatures that single-model benchmarks miss.

### Mechanism 2
Adversarial filtering using VLM pre-screening retains videos that evade current detection capabilities. Borderline cases (labeled "not AI-generated" with low confidence) are kept while confidently detected synthetics are discarded.

### Mechanism 3
Multimodal annotations enable multi-axis detection strategies. Semantic descriptions capture high-level reasoning patterns, optical flow reveals temporal inconsistencies, and frequency features expose low-level artifacts.

## Foundational Learning

**Concept: Temporal coherence vs. frame-level detection**
- Why needed here: AEGIS targets video-specific challenges that image-level benchmarks miss; temporal artifacts require understanding beyond individual frames
- Quick check question: Given a 10-frame video where each frame passes an image-based fake detector, what temporal inconsistencies could still reveal synthesis?

**Concept: Zero-shot vs. fine-tuned generalization gap**
- Why needed here: The dramatic gap between in-domain fine-tuned performance (F1=0.82) and Hard Test Set performance (F1=0.55) requires understanding distribution shift
- Quick check question: Why does LoRA fine-tuning on the training set improve in-domain accuracy but not Hard Test Set accuracy?

**Concept: Difficulty filtering and selection bias**
- Why needed here: AEGIS uses Qwen2.5-VL to filter out easily-detectable videos; understanding this pipeline is critical to interpreting benchmark results
- Quick check question: If you evaluated Qwen2.5-VL on videos it helped filter, what bias would you expect in the results?

## Architecture Onboarding

**Component map:**
Data Sources -> Filtering Pipeline (Reality filter -> Difficulty filter -> Diversity filter) -> Annotation Layer (Semantic + Motion + Frequency) -> Evaluation Splits (Training + Validation + Hard Test)

**Critical path:**
1. Collect raw real/synthetic videos from sources
2. Apply Reality filter (remove non-photorealistic content)
3. Apply Difficulty filter (Qwen2.5-VL zero-shot; retain low-confidence "not AI-generated" predictions)
4. Apply Diversity filter (scene tags, resolution/duration scatter plots)
5. Generate multimodal annotations (RAFT, FFT, GPT-4V descriptions)
6. Split into Train/Val/Hard Test sets

**Design tradeoffs:**
- Using Qwen2.5-VL for filtering creates potential circularity but uses different model sizes for filtering vs. evaluation
- Including proprietary models increases realism but reduces reproducibility (218 videos in Hard Test Set)
- GPT-4o prompt refinement improves semantic precision but introduces cost and potential bias
- Open-Sora exclusion trades coverage for perceived quality threshold

**Failure signatures:**
- Video-LLaVA shows Acc_real=0.0, Acc_ai=1.0 (predicts all synthetic)
- Qwen2.5-VL 7B shows Acc_real=0.89, Acc_ai=0.22 (biased toward "real" predictions)
- Large in-domain vs. Hard Test gap: LoRA fine-tuned Qwen2.5-VL 7B: ID F1=0.82, HT F1=0.55
- Structured reasoning decreases accuracy: Acc_ai drops from 0.22→0.16

**First 3 experiments:**
1. Baseline zero-shot evaluation: Run Qwen2.5-VL-7B with minimal prompt; compute Acc and Macro-F1 on Hard Test Set
2. LoRA fine-tuning generalization test: Fine-tune Qwen2.5-VL-7B on training set; compare in-domain vs. Hard Test Set performance
3. Modality ablation: Train separate detectors using only optical flow, only frequency features, or only semantic descriptions

## Open Questions the Paper Calls Out

**Open Question 1**
Why does structured reasoning prompting degrade detection accuracy for VLMs on the AEGIS Hard Test Set? The paper identifies this counter-intuitive performance drop but does not provide a mechanistic explanation for why guiding a model through explicit reasoning steps harms its ability to detect forgeries.

**Open Question 2**
How can domain adaptation strategies be optimized to bridge the generalization gap between open-source generators and proprietary models like Sora and KLing? While LoRA fine-tuning improved in-domain performance significantly, it failed to transfer effectively to the challenging, high-fidelity videos in the Hard Test Set.

**Open Question 3**
To what extent do the provided explicit annotations (optical flow, FFT) improve detection performance compared to end-to-end raw video analysis? It remains unclear if the failure of current models is due to a lack of data or an inability to perceive the specific low-level visual and motion features provided in the annotations.

## Limitations

- Qwen2.5-VL used for both filtering and evaluation creates potential circularity despite mitigation attempts
- Small Hard Test Set (436 videos) limits statistical power for some analyses
- Proprietary model inclusion (Sora, KLing) enhances realism but reduces reproducibility
- Benchmark focus on hyper-realistic content excludes artistic or low-quality AI-generated videos

## Confidence

**High confidence:** Dataset construction methodology, multimodal annotation approach, and zero-shot detection failure
**Medium confidence:** Difficulty filtering mechanism's effectiveness relies on VLM performance assumptions
**Medium confidence:** Generalizability findings are convincing but limited by small Hard Test Set size

## Next Checks

1. Evaluate the filtered videos using a different VLM than Qwen2.5-VL to verify that retained difficulty is not specific to the filtering model
2. Conduct ablation studies removing individual generative sources to quantify their contribution to detection difficulty
3. Test whether the multimodal annotations actually improve detection performance when used in combination versus individually