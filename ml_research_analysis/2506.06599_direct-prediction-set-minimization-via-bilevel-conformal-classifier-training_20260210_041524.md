---
ver: rpa2
title: Direct Prediction Set Minimization via Bilevel Conformal Classifier Training
arxiv_id: '2506.06599'
source_url: https://arxiv.org/abs/2506.06599
tags:
- training
- dpsm
- prediction
- conformal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of minimizing prediction set sizes
  in conformal prediction by developing a novel bilevel optimization framework called
  Direct Prediction Set Minimization (DPSM). The core idea is to explicitly parameterize
  the quantile of conformity scores using quantile regression in the lower-level subproblem,
  which allows for more accurate estimation compared to stochastic approximation methods.
---

# Direct Prediction Set Minimization via Bilevel Conformal Classifier Training

## Quick Facts
- arXiv ID: 2506.06599
- Source URL: https://arxiv.org/abs/2506.06599
- Reference count: 40
- Key outcome: Achieves O(1/√n) learning bound vs. Ω(1/s) for existing methods, reducing prediction set sizes by 20.46% on benchmark datasets

## Executive Summary
This paper addresses the problem of minimizing prediction set sizes in conformal prediction while maintaining valid coverage. The authors propose Direct Prediction Set Minimization (DPSM), a bilevel optimization framework that explicitly parameterizes the quantile of conformity scores using quantile regression. By decoupling quantile learning from batch sampling, DPSM achieves a theoretical learning bound of O(1/√n), significantly improving over existing methods with bounds of Ω(1/s). The method demonstrates 20.46% reduction in prediction set sizes compared to the best prior baseline while maintaining marginal coverage above 0.9.

## Method Summary
DPSM formulates conformal training as a bilevel optimization problem where the lower-level subproblem learns the conformity score quantile via pinball loss minimization, and the upper-level subproblem minimizes prediction set size conditioned on this learned quantile. The method employs data splitting (D_1, D_2) to compute gradients for different components of the loss, preventing overfitting and stabilizing optimization. The framework is evaluated on CIFAR-100, Caltech-101, and iNaturalist using HPS, APS, and RAPS conformity scores with DenseNet and ResNet architectures.

## Key Results
- Theoretical learning bound of O(1/√n) for DPSM vs. Ω(1/s) for existing conformal training methods
- 20.46% reduction in prediction set sizes compared to ConfTr baseline on CIFAR-100
- Stable convergence with quantile estimation error approaching zero after epoch 35
- Valid marginal coverage maintained at ≥ 0.90 across all datasets and models

## Why This Works (Mechanism)

### Mechanism 1: Bilevel Formulation Decouples Quantile Learning from Batch Sampling
The bilevel formulation eliminates dependence on batch-level stochastic quantiles by learning the quantile as a continuous parameter q over the full training distribution. This removes randomness from mini-batch quantile estimation and improves the learning bound from Ω(1/s) to O(1/√n).

### Mechanism 2: Quantile Regression via Pinball Loss Provides Accurate Empirical Quantile
Pinball loss ρ_α(q, S) provides asymmetric penalty that, when minimized, yields the (1-α)-quantile of conformity scores. This optimization problem satisfies H"olderian error bound conditions, unlike sorting-based or batch-level quantile estimation methods.

### Mechanism 3: Data Splitting Prevents Gradient Correlation and Overfitting
Computing gradients for classification/QR loss on D_1 and conformal loss on disjoint D_2 reduces variance and prevents overfitting. This practical engineering choice stabilizes bilevel optimization despite the lower-level loss violating standard assumptions (non-smooth, non-strongly-convex).

## Foundational Learning

- **Concept: Conformal Prediction (CP) and Coverage Guarantees**
  - Why needed here: DPSM constructs prediction sets C_f(X) = {y : S_f(X,y) ≤ Q̂_f(α)} with guaranteed P(Y ∈ C_f(X)) ≥ 1-α
  - Quick check question: Given calibration scores [0.1, 0.3, 0.5, 0.7, 0.9] and α=0.1, what is the prediction set threshold for a new example?

- **Concept: Quantile Regression and Pinball Loss**
  - Why needed here: DPSM's lower-level problem uses pinball loss to learn quantiles; understanding why pinball loss yields quantiles (not means) is essential
  - Quick check question: Why does minimizing E[ρ_α(q, S)] give the (1-α)-quantile rather than the mean?

- **Concept: Bilevel Optimization Structure**
  - Why needed here: DPSM formulates training as min_f,q L_upper(f,q) s.t. q ∈ argmin_{q'} L_lower(f,q); standard gradient descent doesn't directly apply
  - Quick check question: In bilevel optimization, why can't we simply take gradients of the upper-level loss with respect to f while treating q as fixed?

## Architecture Onboarding

- **Component map:** Classifier f_θ (Deep neural network) → Conformity score S_f(X,Y) → Smooth indicator 1̃[S ≤ q] → Prediction set size → Upper-level loss L_c → Regularized classification loss L_cls

- **Critical path:**
  1. Initialize f_θ randomly, q_0 as small positive value
  2. Split training data into D_1, D_2 (50/50 split)
  3. For each iteration: Sample B_1^t ⊂ D_1, compute ∇_f L_cls and ∇_q L_QR; Sample B_2^t ⊂ D_2, compute ∇_f L_c; Update f and q
  4. Post-training: Apply standard split CP calibration on held-out set to get final Q̂_f

- **Design tradeoffs:**
  - Learning rates (η, γ): γ must be larger than η to ensure q tracks the quantile quickly
  - Regularization λ: Higher λ reduces set size but may harm classification accuracy
  - Temperature τ_sigmoid: Controls smoothness of 1̃[S ≤ q]; smaller τ → sharper approximation but harder optimization
  - Data split ratio: 50/50 is default; asymmetric splits bias gradient estimates

- **Failure signatures:**
  - Diverging q: q → ∞ or negative values → check γ learning rate, ensure pinball loss gradient sign is correct
  - Coverage violation (<1-α): λ too high causing conformal loss to dominate; reduce λ or increase training epochs
  - No convergence in upper-level loss: Early epochs show increasing loss (expected), but should stabilize by epoch ~35
  - Quantile estimation error doesn't decrease: Check D_1/D_2 split is actually random, not stratified

- **First 3 experiments:**
  1. Sanity check: Train on CIFAR-100 with HPS score, λ=0.1, verify marginal coverage ≈0.90±0.01 and compare APSS to ConfTr baseline (target: >15% reduction)
  2. Ablation on data splitting: Compare D_1/D_2 split vs. using same data for all gradients; expect split version to have lower test APSS and more stable training curves
  3. Learning rate sweep: Fix η=0.01, sweep γ ∈ {0.001, 0.01, 0.05, 0.1}; plot final APSS and quantile estimation error to find stable γ range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a stochastic first-order algorithm be developed that provably converges for the DPSM bilevel problem without requiring restrictive assumptions like strong convexity or twice differentiability of the lower-level pinball loss?
- Basis in paper: [explicit] Section 4.2 states, "To the best of our knowledge, there is no prior stochastic optimization algorithm that solves DPSM problem with the assumptions of bilevel optimization literature satisfied... Consequently, it is non-trivial... We leave this challenge for future work."
- Why unresolved: The lower-level function (quantile regression loss) is non-smooth and non-strongly-convex, violating standard assumptions required by existing bilevel optimization theories.
- What evidence would resolve it: A formal convergence proof for a stochastic algorithm solving the DPSM formulation under the specific non-smooth conditions of the pinball loss.

### Open Question 2
- Question: Can the theoretical learning bound of O(1/√n) be maintained if the assumption of "no ties" in non-conformity scores is relaxed?
- Basis in paper: [explicit] The paper frequently notes the assumption, "We assume that there is no tie in non-conformity scores" (Section 3), and Lemma 4.5 explicitly conditions the H"olderian error bound (HEB) on there being no ties.
- Why unresolved: Ties in scores result in a non-unique solution set for the lower-level quantile regression, potentially breaking the HEB condition required for the convergence analysis of the bilevel problem.
- What evidence would resolve it: A theoretical analysis or bound derived for cases with discrete score distributions or density plateaus where ties occur.

### Open Question 3
- Question: Is the heuristic of splitting training data into two disjoint subsets (D_1 and D_2) theoretically necessary for the O(1/√n) bound, or does it strictly reduce statistical efficiency?
- Basis in paper: [inferred] Algorithm 1 randomly splits D_tr into D_1 and D_2. The text notes this "helps to prevent over-fitting" but provides no theoretical justification for how this splitting affects the sample complexity compared to using the full set D_tr for both gradient steps.
- Why unresolved: While practical for regularization, halving the batch size for different gradient estimations might theoretically increase variance or alter the bound, but this trade-off is not analyzed.
- What evidence would resolve it: A comparative theoretical analysis of the algorithm's variance and convergence rate with and without the data splitting constraint.

## Limitations
- Data splitting reduces sample efficiency by requiring disjoint subsets for different gradient computations
- Temperature parameter τ for smooth indicator function is left unspecified, potentially affecting reproducibility
- Convergence guarantees for joint optimization under non-convex DNN landscapes require further empirical validation

## Confidence
- **High confidence:** Theoretical learning bound improvement from Ω(1/s) to O(1/√n) is well-supported by bilevel formulation analysis
- **Medium confidence:** Empirical results showing 20.46% reduction in prediction set sizes are convincing but depend on specific hyperparameter choices
- **Low confidence:** Convergence guarantees for the joint optimization of f and q under data splitting scheme are not rigorously proven

## Next Checks
1. **Convergence sensitivity:** Systematically vary the data split ratio and learning rate γ to map the stability region of the bilevel optimization
2. **Generalization across architectures:** Evaluate DPSM on architectures beyond DenseNet (e.g., Vision Transformers) to test robustness of the learning bound improvement
3. **Sample complexity analysis:** Quantify the performance gap between using full n samples versus the split n/2 approach for both D_1 and D_2 to measure the sample efficiency cost