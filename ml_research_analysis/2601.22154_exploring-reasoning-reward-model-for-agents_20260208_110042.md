---
ver: rpa2
title: Exploring Reasoning Reward Model for Agents
arxiv_id: '2601.22154'
source_url: https://arxiv.org/abs/2601.22154
tags:
- arxiv
- reasoning
- agent
- reward
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of sparse, outcome-based rewards
  in agentic reinforcement learning, which limits learning in long-horizon tasks requiring
  multi-step tool use. It introduces Agent Reasoning Reward Model (Agent-RRM), which
  provides structured feedback including explicit reasoning traces, targeted critiques,
  and holistic scores to differentiate intermediate reasoning quality.
---

# Exploring Reasoning Reward Model for Agents

## Quick Facts
- arXiv ID: 2601.22154
- Source URL: https://arxiv.org/abs/2601.22154
- Reference count: 11
- Key outcome: Agent-RRM provides structured reasoning-aware rewards; Reagent-U achieves 43.7% on GAIA and 46.2% on WebWalkerQA, outperforming baselines.

## Executive Summary
This paper addresses the challenge of sparse, outcome-based rewards in agentic reinforcement learning, which limits learning in long-horizon tasks requiring multi-step tool use. It introduces Agent Reasoning Reward Model (Agent-RRM), which provides structured feedback including explicit reasoning traces, targeted critiques, and holistic scores to differentiate intermediate reasoning quality. Three integration strategies are explored: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 benchmarks demonstrate that Reagent-U achieves state-of-the-art performance, reaching 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of reasoning-aware rewards in enhancing agent reasoning and tool-use capabilities.

## Method Summary
The method involves training a reasoning-aware reward model (Agent-RRM) to provide structured evaluations of agent trajectories, then integrating these rewards into agent training via three variants. Agent-RRM outputs reasoning traces, critiques, and scalar scores. Reagent-C applies critiques at inference time for refinement without training. Reagent-R augments rule-based rewards with Agent-RRM scores during training. Reagent-U pools initial and critique-refined trajectories into unified advantage calculations. Both agent and reward model are initialized from Qwen3-8B, with training proceeding via SFT followed by GRPO.

## Key Results
- Reagent-C achieves consistent inference-time gains across all benchmarks without training.
- Reagent-R with λ=0.3 balances reasoning guidance and outcome correctness.
- Reagent-U yields state-of-the-art performance: 43.7% on GAIA, 46.2% on WebWalkerQA.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Structured, reasoning-aware reward signals mitigate sparse feedback failure modes in long-horizon agentic tasks.
- **Mechanism**: Agent-RRM replaces binary outcome rewards with a three-part judgment (reasoning trace, critique, score). The scalar score provides dense gradient-compatible signals for policy optimization, while the reasoning trace justifies the assessment, improving reward credit assignment across multi-step tool-use trajectories.
- **Core assumption**: Intermediate reasoning quality correlates with eventual task success, and a learned evaluator can approximate this without ground-truth labels.
- **Evidence anchors**:
  - [abstract]: "Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results."
  - [section 3.5.2]: "This variant alleviates the sparsity of rule-based rewards by providing reasoning-aware feedback."
  - [corpus]: Related work (Multimodal Reinforcement Learning with Agentic Verifier, FMR=0.59) similarly argues richer rewards from reasoning tokens improve learning.
- **Break condition**: If Agent-RRM's scores correlate poorly with ground-truth success or exhibit systematic bias (e.g., reward hacking), dense rewards could misguide policy learning.

### Mechanism 2
- **Claim**: Textual critiques enable inference-time error correction without parameter updates.
- **Mechanism**: In Reagent-C, the agent generates an initial response, receives a targeted critique from Agent-RRM, then conditions a second response on this feedback. The critique pinpoints specific flaws (missing tool calls, unverified assumptions, hallucinations), allowing the frozen policy to self-correct via in-context learning.
- **Core assumption**: The agent can interpret and act on natural language feedback to improve subsequent outputs within the same context window.
- **Evidence anchors**:
  - [abstract]: "a focused critique that provides refinement guidance by highlighting reasoning flaws"
  - [section 4.1]: "Reagent-C achieves consistent performance gains across all benchmarks without any parameter updates."
  - [corpus]: PITA (FMR=0.0, limited relevance) explores inference-time alignment but via reward optimization rather than critique conditioning.
- **Break condition**: If critiques are vague, incorrect, or the agent lacks sufficient in-context reasoning capacity to act on them, refinement quality degrades.

### Mechanism 3
- **Claim**: Jointly optimizing scalar rewards and critique-conditioned refinement yields synergistic performance gains.
- **Mechanism**: Reagent-U pools initial and refined trajectories into a unified advantage calculation. By normalizing rewards across both stages, the policy learns to produce high-quality initial outputs while also internalizing refinement capabilities. Critiques are used during training only; inference runs without external guidance.
- **Core assumption**: The two objectives—initial generation quality and refinement capability—share underlying reasoning representations that benefit from mutual reinforcement.
- **Evidence anchors**:
  - [abstract]: "Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA"
  - [section 4.3]: "Reagent-U consistently outperforms all baselines across a diverse spectrum of reasoning and agentic benchmarks."
  - [corpus]: QLASS (FMR=0.55) similarly uses stepwise reward guidance for agents but does not explicitly unify critique and scalar signals.
- **Break condition**: If initial and refined trajectories have fundamentally different reward distributions, unified normalization could introduce noise rather than synergy.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: The paper builds directly on GRPO for RL training. Equations 2–4 define the objective, importance sampling ratio, and advantage computation used in all Reagent variants.
  - **Quick check question**: Can you explain how GRPO computes advantages relative to a group of sampled outputs, and why this avoids training a separate value function?

- **Concept: Reward Modeling for LLMs**
  - **Why needed here**: Agent-RRM is a trained reward model that outputs structured evaluations. Understanding reward model training (SFT → RL pipeline, calibration) is essential to reproduce or modify the approach.
  - **Quick check question**: What is the difference between outcome-based and process-based reward models, and what failure modes does each address?

- **Concept: In-Context Refinement**
  - **Why needed here**: Reagent-C relies on the agent's ability to improve outputs when conditioned on critique feedback. This is a distinct capability from standard supervised or RL training.
  - **Quick check question**: Given a frozen LLM and a critique of its output, what prompt design choices affect whether the model successfully corrects its errors?

## Architecture Onboarding

- **Component map**:
  - Agent Policy (Reagent) -> Agent-RRM -> Tool Suite -> Three Integration Variants (Reagent-C, Reagent-R, Reagent-U)

- **Critical path**:
  1. Train Agent-RRM (SFT → GRPO) to produce reliable structured judgments.
  2. Cold-start agent with SFT on high-quality trajectories.
  3. Select integration variant; for Reagent-U, implement two-stage sampling with critique-conditioned refinement during training.
  4. Evaluate on target benchmarks using automated judge (Qwen2.5-72B-Instruct).

- **Design tradeoffs**:
  - **λ (reward weight)**: Paper finds optimal range [0.2, 0.4]. Higher λ over-emphasizes intermediate reasoning at expense of outcome correctness.
  - **Reagent-C vs. Reagent-R vs. Reagent-U**: C is inference-only (no training cost), R provides dense learning signal but lacks explicit correction, U is most performant but requires more complex training loop.
  - **Agent-RRM training data diversity**: Must cover broad error patterns; paper uses ensemble of multiple model families for trajectory sampling.

- **Failure signatures**:
  - **Reward hacking**: Agent-RRM scores exploited without genuine reasoning improvement (mitigated by GRPO training and structured output format).
  - **Critique unhelpfulness**: Vague or incorrect critiques fail to guide refinement (monitor via Reagent-C ablation).
  - **Generalization gaps**: Strong performance on GAIA/WebWalkerQA but weaker on out-of-distribution tasks (paper acknowledges 8B scale limitation).

- **First 3 experiments**:
  1. **Reagent-C baseline**: Apply frozen Agent-RRM critiques to off-the-shelf Qwen3-8B; measure inference-time gains without training. Validates critique quality.
  2. **λ sensitivity sweep**: Train Reagent-R with λ ∈ {0, 0.1, 0.3, 0.5} on a held-out subset (e.g., AIME24, xbench). Confirm plateau region.
  3. **Reagent-U vs. Reagent-R ablation**: Train both on identical data; isolate contribution of unified critique-and-reward pooling by comparing against Reagent-R (scalar-only) and Reagent-C (critique-only inference).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the Reagent framework scale to larger language models (14B, 32B, 70B+ parameters), and does the relative benefit of unified feedback integration diminish, persist, or amplify?
- **Basis in paper**: [explicit] The limitations section states: "our current experiments primarily focus on models at the 8B parameter scale... its scaling behavior on larger-scale models remains to be explored."
- **Why unresolved**: All reported experiments use Qwen3-8B as the backbone; no results for larger model families are provided.
- **What evidence would resolve it**: Comparative experiments training Reagent-U on Qwen3-14B, Qwen3-32B, and larger models, reporting performance deltas relative to baselines at each scale.

### Open Question 2
- **Question**: How does the Reagent framework perform in open-ended, real-world agentic environments with diverse toolsets and unpredictable task structures beyond standardized benchmarks?
- **Basis in paper**: [explicit] The limitations section notes: "moving beyond standardized benchmarks to handle broader toolsets and more intricate reasoning chains is essential" and suggests exploring "open-ended, real-world applications (e.g., AI for science)."
- **Why unresolved**: All evaluations are conducted on 12 fixed benchmarks with predefined task formats and limited tool sets.
- **What evidence would resolve it**: Deployment studies in domains like automated scientific research, software development, or enterprise workflows with heterogeneous tools, measuring task completion rates and adaptation capability.

### Open Question 3
- **Question**: What is the optimal strategy for tuning the reward weight λ across different task domains, and can it be dynamically adjusted during training?
- **Basis in paper**: [inferred] Figure 3 shows performance varies substantially with λ, plateauing at [0.2, 0.4] but declining at 0.5, yet λ=0.3 is used uniformly across all experiments without task-specific tuning.
- **Why unresolved**: The paper provides no theoretical justification for the λ=0.3 choice or analysis of domain-specific optimal values.
- **What evidence would resolve it**: Systematic ablation studies varying λ per benchmark category (mathematical reasoning vs. web navigation vs. knowledge-intensive tasks), plus experiments with adaptive λ schedules.

### Open Question 4
- **Question**: Does Agent-RRM generalize across different base model architectures, or is its effectiveness contingent on the Qwen family's specific characteristics?
- **Basis in paper**: [inferred] Both Agent-RRM and Reagent are initialized from Qwen3-8B; no cross-architecture experiments are reported (e.g., Llama, Mistral, DeepSeek).
- **Why unresolved**: The training data for Agent-RRM (Reagent-RRM-SFT-28K and Reagent-RRM-RL-90K) may contain distributional biases aligned with Qwen's reasoning patterns.
- **What evidence would resolve it**: Training Agent-RRM from different base models and evaluating its transferability, or applying a Qwen-trained Agent-RRM to agents built on other model families.

## Limitations

- The framework is primarily evaluated on 8B parameter models, with scaling behavior to larger models remaining unexplored.
- Performance is validated only on standardized benchmarks, not on open-ended real-world tasks with diverse toolsets.
- The optimal reward weight λ is not theoretically justified and may require task-specific tuning.

## Confidence

- **High Confidence**: The conceptual framework of replacing sparse outcome rewards with structured reasoning-aware feedback is well-founded and aligns with established reward modeling principles. The ablation showing Reagent-C's inference-time gains without training is directly observable and credible.
- **Medium Confidence**: The performance gains of Reagent-U (43.7% on GAIA, 46.2% on WebWalkerQA) are promising but depend on unverified assumptions about Agent-RRM's consistency and the dataset quality. The claim that unified pooling yields synergistic benefits requires more detailed analysis of reward distributions across initial vs. refined trajectories.
- **Low Confidence**: The claim that Agent-RRM generalizes to out-of-distribution tasks or prevents reward hacking is not empirically validated in the paper. Without ablation studies isolating Agent-RRM's contribution or showing cross-dataset robustness, these claims remain speculative.

## Next Checks

1. **Reward Correlation Analysis**: Compute Pearson/Spearman correlation between Agent-RRM scores and ground-truth correctness on a held-out validation set. Report correlation coefficients per benchmark to quantify reward signal quality.
2. **Dataset Composition Audit**: Document the exact subsampling ratios, difficulty thresholds, and diversity metrics (e.g., error type distribution) for Reagent-RL-709K and Reagent-RRM-RL-90K. Validate that the datasets cover the full spectrum of reasoning errors targeted by Agent-RRM.
3. **Cross-Dataset Generalization Test**: Evaluate Reagent-U on a benchmark not seen during training (e.g., HumanEval for code tasks, or a different multimodal dataset). Compare performance drop to baseline agents to assess generalization limits.