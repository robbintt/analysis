---
ver: rpa2
title: 'Rethinking Safety in LLM Fine-tuning: An Optimization Perspective'
arxiv_id: '2508.12531'
source_url: https://arxiv.org/abs/2508.12531
tags:
- safety
- learning
- utility
- dataset
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether safety risks in large language\
  \ models (LLMs) during fine-tuning are caused by poor optimization choices rather\
  \ than inherent trade-offs. Through systematic experimentation on Llama-2-7B and\
  \ Llama-3.2-1B, the authors show that proper selection of hyper-parameters\u2014\
  such as learning rate, batch size, and gradient accumulation\u2014can reduce unsafe\
  \ model responses from 16% to under 5%, as measured by keyword matching, while maintaining\
  \ utility performance."
---

# Rethinking Safety in LLM Fine-tuning: An Optimization Perspective

## Quick Facts
- **arXiv ID:** 2508.12531
- **Source URL:** https://arxiv.org/abs/2508.12531
- **Reference count:** 31
- **Primary result:** Proper hyperparameter selection and parameter-space EMA can reduce unsafe model responses from 16% to under 5% during fine-tuning while maintaining utility

## Executive Summary
This paper challenges the assumption that safety risks during LLM fine-tuning are inherent trade-offs, proposing instead that they stem from poor optimization choices. Through systematic experimentation on Llama-2-7B and Llama-3.2-1B models, the authors demonstrate that careful selection of learning rates, batch sizes, and gradient accumulation can significantly reduce unsafe outputs during benign fine-tuning. Their parameter-space EMA technique, which maintains an exponential moving average of model weights, achieves approximately 2.7% attack success rate without requiring external safety data, outperforming methods that rely on additional safety datasets. The results suggest that safety alignment degradation during fine-tuning can largely be avoided through optimized training strategies rather than accepting it as an inevitable consequence.

## Method Summary
The authors investigate safety alignment degradation during supervised fine-tuning by systematically varying optimization hyperparameters including learning rate, batch size, and gradient accumulation. They propose a parameter-space EMA technique that maintains an exponential moving average of model weights during training, with the EMA weights used for inference rather than the training weights. The method is evaluated using ASR (Attack Success Rate) measured through keyword matching and utility through MT-Bench scores. Experiments are conducted on Llama-2-7B-Chat and Llama-3.2-1B-Instruct models using benign datasets including Dolly, Alpaca, and ORCA, with AdvBench used for safety evaluation.

## Key Results
- Optimization hyperparameter selection can reduce unsafe model responses from 16% to under 5% as measured by keyword matching
- EMA-based approach achieves approximately 2.7% attack success rate without requiring additional safety data
- Parameter-space EMA outperforms existing methods that rely on external safety datasets while maintaining utility performance
- Proper hyperparameter tuning is critical - poor choices can lead to significant safety degradation even when fine-tuning on benign data

## Why This Works (Mechanism)
The paper's mechanism centers on the observation that safety alignment degradation during fine-tuning occurs because standard optimization methods cause the model to drift too far from the safety-aligned pre-trained weights. The EMA technique works by maintaining a smoothed version of the parameter trajectory that stays closer to the original safety-aligned weights. By updating the EMA weights using a momentum parameter (η=0.25) at each training step, the method creates a compromise between adapting to the new task and preserving the original safety alignment. This approach effectively constrains the fine-tuning process to remain within a "safety basin" in parameter space, preventing the catastrophic forgetting of safety behaviors that typically occurs with standard fine-tuning.

## Foundational Learning
- **Attack Success Rate (ASR):** A metric measuring the percentage of adversarial prompts that successfully elicit unsafe responses from a model. Why needed: Provides quantifiable measure of safety alignment quality. Quick check: ASR below 5% indicates strong safety preservation.
- **Parameter-Space EMA:** Technique maintaining an exponential moving average of model weights during training. Why needed: Enables smooth interpolation between pre-trained and fine-tuned weights while preserving safety properties. Quick check: EMA weights used for inference, not training weights.
- **Safety Basin:** Conceptual region in parameter space where models maintain their safety alignment properties. Why needed: Explains why some hyperparameter choices preserve safety while others cause degradation. Quick check: Models staying within this basin maintain safety during fine-tuning.
- **Catastrophic Forgetting:** Phenomenon where models lose previously learned capabilities (including safety alignment) when trained on new tasks. Why needed: Core problem being addressed by the proposed method. Quick check: EMA reduces forgetting compared to standard fine-tuning.
- **Keyword Matching Evaluation:** Method assessing safety by counting specific unsafe keywords in model responses. Why needed: Provides objective, reproducible safety measurement. Quick check: Lower keyword counts indicate better safety alignment.
- **Adversarial Prompting:** Technique using carefully crafted inputs designed to elicit unsafe responses from models. Why needed: Stress-tests safety alignment beyond benign evaluation. Quick check: Models should maintain low ASR even against adversarial inputs.

## Architecture Onboarding

**Component Map:** Base Model -> Fine-tuning Data -> Optimizer -> EMA Layer -> Evaluation (ASR + Utility)

**Critical Path:** Pre-trained weights → Fine-tuning with EMA → EMA weights for inference → Safety/Utility evaluation

**Design Tradeoffs:** The EMA momentum parameter (η) creates a fundamental tradeoff between adaptation speed and safety preservation. Higher η values allow faster task learning but risk losing safety alignment, while lower values better preserve safety but may limit fine-tuning effectiveness. The authors find η=0.25 provides optimal balance.

**Failure Signatures:** High ASR (>10%) indicates the model has drifted too far from safety-aligned weights, suggesting either poor hyperparameter choices or insufficient EMA momentum. Significant utility degradation (>10% drop in MT-Bench) suggests EMA momentum is too high, preventing adequate task adaptation.

**First Experiments:** 1) Baseline fine-tuning without EMA to establish safety degradation baseline (~16% ASR). 2) Implement EMA with η=0.25 and verify ASR reduction to ~2.7%. 3) Vary learning rate and batch size systematically to identify optimal hyperparameter combinations that preserve safety.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical or heuristic guidelines be established to predict "safe" hyperparameter configurations (e.g., learning rate, batch size) without requiring costly empirical trial-and-error?
- Basis in paper: The authors note that while optimal hyperparameters significantly reduce safety risks, "finding good hyper-parameters is cost-inefficient due to the need for trial and error across multiple parameter pairs."
- Why unresolved: The paper demonstrates that specific hyperparameter choices resolve safety issues but does not provide a formula or theoretical framework to predict these settings a priori for new datasets.
- What evidence would resolve it: The derivation of scaling laws or theoretical bounds linking dataset size, model width, and learning rates to the stability of the "safety basin."

### Open Question 2
- Question: Does the effectiveness of optimization-based safety preservation scale to models significantly larger than 7B parameters (e.g., 70B+ models)?
- Basis in paper: The experimental scope is limited to Llama-2-7B, Llama-3.2-1B, and small Qwen/Phi models (3B).
- Why unresolved: The geometry of the loss landscape (specifically the "safety basin") and the impact of gradient noise may change substantially with model scale, potentially altering the efficacy of EMA or batch size adjustments.
- What evidence would resolve it: Reproduction of the EMA and hyperparameter tuning results on 70B or 405B parameter models, verifying that the 2.7% attack success rate is maintained.

### Open Question 3
- Question: To what extent is the EMA method's success dependent on the dynamic momentum trajectory versus a static interpolation with the aligned base model?
- Basis in paper: The paper states "EMA updates can be also viewed as parameter merging" but frames the contribution as a momentum technique; the specific benefit of the *online* update mechanism over simple weight averaging remains ambiguous.
- Why unresolved: While ablations on the weight η exist, the paper does not isolate whether the safety preservation is purely from remaining close to θ₀ (distance) or from the smoothness of the path taken.
- What evidence would resolve it: A direct comparison between EMA, post-hoc parameter merging (e.g., Task Arithmetic), and L2 regularization constrained to the exact same parameter distance from initialization.

## Limitations
- Safety evaluation relies primarily on keyword matching, which may not capture all forms of unsafe behavior or sophisticated adversarial attacks
- Experiments limited to relatively small models (7B parameters) and synthetic benign datasets, raising questions about generalizability
- Claims about outperforming methods using external safety data are difficult to verify without access to specific baseline implementations

## Confidence
- **High Confidence:** Core finding that optimization choices significantly impact safety alignment is well-supported by ablation studies; EMA mechanism is clearly described and reproducible
- **Medium Confidence:** Claims about reducing ASR from 16% to under 5% are convincing within experimental scope but require validation across different datasets and model families
- **Low Confidence:** Claims about outperforming safety-data-dependent methods lack full verification; long-term stability of EMA-maintained safety alignment is unaddressed

## Next Checks
1. **Adversarial Robustness Validation:** Test EMA-tuned models against adversarial prompts from AdvBench that specifically target the safety keywords identified as triggers to verify whether the 2.7% ASR holds against sophisticated attacks

2. **Cross-Dataset Generalization:** Fine-tune Llama-3.2-1B using the same EMA methodology but with different benign datasets (e.g., UltraChat, WizardLM) to confirm safety preservation generalizes beyond the Dolly dataset

3. **Temporal Stability Analysis:** Implement continued training protocol where EMA-preserved model undergoes additional benign fine-tuning steps and monitor ASR evolution over time to assess whether safety alignment degrades and requires recalibration