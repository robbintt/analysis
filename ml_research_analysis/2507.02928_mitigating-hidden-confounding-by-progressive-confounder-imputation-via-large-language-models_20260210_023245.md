---
ver: rpa2
title: Mitigating Hidden Confounding by Progressive Confounder Imputation via Large
  Language Models
arxiv_id: '2507.02928'
source_url: https://arxiv.org/abs/2507.02928
tags:
- treatment
- confounders
- confounder
- llms
- proci
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProCI, the first framework to leverage large
  language models (LLMs) for mitigating hidden confounding in treatment effect estimation.
  ProCI uses LLMs to iteratively generate, impute, and validate hidden confounders
  by combining semantic reasoning with world knowledge embedded in the models.
---

# Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models

## Quick Facts
- arXiv ID: 2507.02928
- Source URL: https://arxiv.org/abs/2507.02928
- Authors: Hao Yang; Haoxuan Li; Luyu Chen; Haoxiang Wang; Xu Chen; Mingming Gong
- Reference count: 40
- One-line primary result: ProCI consistently improves treatment effect estimation with up to 70% error reduction using LLMs to iteratively generate, impute, and validate hidden confounders.

## Executive Summary
This paper introduces ProCI, the first framework to leverage large language models (LLMs) for mitigating hidden confounding in treatment effect estimation. ProCI uses LLMs to iteratively generate, impute, and validate hidden confounders by combining semantic reasoning with world knowledge embedded in the models. Instead of direct value imputation, it adopts a distributional reasoning strategy to generate diverse, realistic samples. Experiments across multiple datasets (Jobs, Twins) and LLMs (GPT-4o, DeepSeek-R1, LLaMA 3-8B, Qwen2.5-7B) show that ProCI consistently improves treatment effect estimation, with up to 70% error reduction in some cases. The method is robust to varying levels of hidden confounding and demonstrates the potential of LLMs as tools for causal inference under unmeasured confounding.

## Method Summary
ProCI addresses hidden confounding by iteratively generating and imputing latent confounders using LLMs. The framework operates in two phases: (1) Confounder Imputation, where LLMs generate confounder names/explanations, infer distribution types (Gaussian, Bernoulli, etc.), estimate per-unit parameters, and sample values; (2) Unconfoundedness Validation, where LLMs impute counterfactual outcomes and Kernel-based Conditional Independence Test (KCIT) checks whether (Ŷ⁰, Ŷ¹) ⊥ T | X, Û. The loop repeats until KCIT passes. Base CATE estimators include TARNet and CFR-Wass. Experiments use Twins (8,244 twin pairs) and Jobs (297 treated, 425 RCT controls, 2,490 observational controls) datasets with LLMs including GPT-4o, DeepSeek-R1, LLaMA 3-8B, and Qwen2.5-7B.

## Key Results
- ProCI achieves up to 70% error reduction in CATE estimation compared to baselines across multiple datasets
- Distributional reasoning strategy prevents collapsed LLM outputs, producing diverse and realistic confounder values
- DeepSeek-R1 outperforms GPT-4o in semantic reasoning for confounder generation, while open-source models (LLaMA 3-8B, Qwen2.5-7B) show viable performance
- Progressive confounder generation is more effective than batch generation, with optimal temperature setting at 0.7 for maximizing conditional mutual information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate semantically plausible confounders by leveraging world knowledge encoded during pretraining.
- Mechanism: Given structured covariates X, treatment T, and outcome Y, the LLM reasons about what latent factors could plausibly affect both T and Y but are not represented in X. The prompt explicitly requests orthogonal variables with causal justification.
- Core assumption: LLMs encode domain-relevant causal structure that transfers to the specific observational study context.
- Evidence anchors:
  - [abstract] "ProCI leverages two key capabilities of LLMs: their strong semantic reasoning ability, which enables the discovery of plausible confounders from both structured and unstructured inputs, and their embedded world knowledge"
  - [section 3.1] "LLMs can generate meaningful confounder candidates by interpreting the semantic relationships among treatment, outcome, and observed covariates... using domain-level priors encoded in language"
  - [corpus] VIGOR+ (arXiv:2512.19349) confirms LLM-generated confounders can exhibit semantic plausibility but notes validation remains critical
- Break condition: If the LLM lacks domain knowledge (e.g., highly specialized medical or industrial contexts not well-represented in pretraining corpora), generated confounders may be semantically implausible or causally irrelevant.

### Mechanism 2
- Claim: Distributional reasoning prevents collapsed LLM outputs and produces diverse, realistic confounder values.
- Mechanism: Rather than prompting the LLM to directly output individual values (which empirically produces near-identical values across units), ProCI decomposes imputation into: (1) distribution type identification (Gaussian, Bernoulli, etc.), (2) per-unit parameter inference (e.g., μᵢ, σᵢ), then (3) sampling from personalized distributions.
- Core assumption: LLMs can reliably infer distribution families from semantic context and estimate personalized parameters conditioned on observed features.
- Evidence anchors:
  - [abstract] "ProCI adopts a distributional reasoning strategy instead of direct value imputation to prevent the collapsed outputs"
  - [section 3.3] "Direct value imputation... tends to produce collapsed outputs where many individuals receive similar or identical values. To address this issue, we decompose value imputation into distribution identification and parameter inference"
  - [corpus] No direct corpus evidence on this specific distributional decomposition strategy; this appears novel to ProCI
- Break condition: If the true confounder has complex multimodal or heavy-tailed distributions not in the candidate set F, the inferred distribution family will be misspecified, introducing imputation bias.

### Mechanism 3
- Claim: LLM-imputed counterfactual outcomes enable empirical testing of the unconfoundedness assumption via conditional independence tests.
- Mechanism: Since LLMs implicitly condition on latent confounders U* through world knowledge, they can approximate counterfactual outcomes. After imputing Ŷ⁰ and Ŷ¹, KCIT tests whether (Ŷ⁰, Ŷ¹) ⊥ T | (X, Û). Passing indicates sufficient adjustment; failure triggers another confounder generation round.
- Core assumption: Theorem 1 assumes KCIT on imputed variables asymptotically converges to the true test: KCIT(Ŷ⁰, Ŷ¹, T | X, Û) = KCIT(Y⁰, Y¹, T | X, U) + oₚ(1).
- Evidence anchors:
  - [section 3.4] "we propose using LLMs to impute the missing outcomes Y⁰ and Y¹... a kernel-based conditional independence test (KCIT) is applied to check whether (Ŷ⁰, Ŷ¹) ⊥⊥ T | X, Û"
  - [appendix B] Full proof of Theorem 1 under regularity conditions on kernel functions and imputation distributions
  - [corpus] Kernel-based approaches for detecting unobserved confounders appear in "Detecting Unobserved Confounders: A Kernelized Regression Approach" (arXiv:2601.00200), though using different methodology
- Break condition: If sample size is insufficient or imputation quality is poor, KCIT may have low power or produce false positives/negatives. The theorem only guarantees asymptotic validity.

## Foundational Learning

- Concept: **Unconfoundedness (Ignorability) Assumption**
  - Why needed here: ProCI's entire purpose is to restore this assumption when it's violated by hidden confounders. Understanding that (Y¹, Y⁰) ⊥⊥ T | X enables identification is essential.
  - Quick check question: If treatment assignment depends on an unobserved variable U that also affects outcome Y, does unconfoundedness hold? (Answer: No—this is exactly the hidden confounding problem ProCI addresses.)

- Concept: **Potential Outcomes Framework & CATE**
  - Why needed here: The target estimand is τ(x) = E[Y¹ − Y⁰ | X = x]. Only one potential outcome is ever observed per unit, making counterfactual imputation necessary for validation.
  - Quick check question: For a binary treatment, why can't we directly observe individual treatment effects? (Answer: Fundamental problem of causal inference—we only see Y¹ if T=1 or Y⁰ if T=0, never both.)

- Concept: **Conditional Independence Testing (KCIT)**
  - Why needed here: ProCI uses KCIT to empirically validate whether generated confounders restore unconfoundedness. Understanding kernel-based tests for (Y ⊥⊥ T | X) is critical for interpreting results.
  - Quick check question: If KCIT returns p > α, what does this indicate about the relationship between treatment and potential outcomes given the covariates? (Answer: Fail to reject conditional independence—suggests unconfoundedness may hold.)

## Architecture Onboarding

- Component map: Pvar -> Pdist -> Pparam -> Sample Û -> Pout -> KCIT -> [fail: loop] / [pass: TARNet/CFR estimator]
- Critical path: Observed data (X, T, Y) -> Pvar -> Pdist -> Pparam -> Sample Û -> Pout -> KCIT -> [fail: loop] / [pass: TARNet/CFR estimator]. Temperature fixed at 0.7; α is significance threshold.
- Design tradeoffs:
  - Distributional reasoning vs. direct imputation: +robustness, -prompting complexity
  - Progressive vs. batch confounder generation: +diversity/reduced overlap, -more LLM calls
  - LLM choice (GPT-4o vs. DeepSeek-R1 vs. open-source): DeepSeek-R1 shows stronger reasoning but may have access/cost constraints; LLaMA 3-8B/Qwen2.5-7B viable for cost-sensitive deployment
- Failure signatures:
  - **Collapsed outputs**: All Û values identical -> distributional reasoning not triggered or failed
  - **KCIT never passes**: Generated confounders not capturing true hidden structure -> check prompt quality, LLM capability, or dataset context mismatch
  - **Semantic redundancy**: Generated confounders overlap with existing X -> Pvar not enforcing orthogonality
  - **Excessive iterations**: >10 confounders generated -> may indicate fundamental domain mismatch
- First 3 experiments:
  1. **Baseline replication on Jobs/Twins**: Run ProCI-4o and ProCI-R1 with CFR-Wass as base estimator. Verify ε_ATT reduction matches Table 1 (target: ~70% error reduction on Jobs out-sample).
  2. **Ablation on distributional reasoning**: Compare ProCI vs. ProCI w/o DR (direct imputation) on a held-out split. Expect collapsed values and higher variance in w/o DR condition.
  3. **Temperature sensitivity sweep**: Run ProCI-La/ProCI-Qw with λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. Compute CMI I(U, T|X) and I(U, Y|X) for each temperature. Expect peak around 0.5–0.7 per Figure 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ProCI framework generalize to complex, domain-specific datasets beyond the current benchmarks?
- Basis in paper: [explicit] The authors state that evaluation was limited to Twins and Jobs, and suggest future work should "explore a broader range of real-world and domain-specific datasets."
- Why unresolved: Current benchmarks may not capture the high dimensionality or noise inherent in specialized fields like healthcare or economics.
- What evidence would resolve it: Successful application and error reduction demonstrated on diverse, real-world datasets such as clinical EHR or economic microdata.

### Open Question 2
- Question: How can the empirical unconfoundedness test be validated for small sample sizes?
- Basis in paper: [explicit] The paper notes that the KCIT test approximates the true test only with large sample sizes and calls for "more robust or distribution-free statistical tests" for smaller datasets.
- Why unresolved: The theoretical guarantee relies on asymptotic convergence ($o_p(1)$), which may fail in limited data settings.
- What evidence would resolve it: Derivation of a non-asymptotic statistical test that maintains validity for small sample sizes.

### Open Question 3
- Question: Is ProCI effective across different LLM architectures, such as encoder-decoder models?
- Basis in paper: [explicit] The authors highlight that their study focused on instruction-tuned decoders and suggest examining "multilingual models, encoder-decoder frameworks, or smaller-scale LLMs."
- Why unresolved: Different architectures possess varying capabilities in semantic reasoning and world knowledge retrieval critical for confounder generation.
- What evidence would resolve it: Benchmarking ProCI's performance using encoder-decoder (e.g., T5) or smaller-scale models compared to current decoder-only results.

## Limitations
- ProCI's performance depends on LLM access to relevant domain knowledge, limiting generalizability to specialized fields
- The distributional reasoning mechanism lacks direct validation and may introduce bias when true confounder distributions are complex
- KCIT validation assumes asymptotic convergence that may not hold in finite samples, particularly with poor imputation quality

## Confidence
- **High**: ProCI consistently improves CATE estimation across multiple datasets and LLMs; distributional reasoning prevents collapsed outputs; semantic plausibility of generated confounders
- **Medium**: KCIT-based unconfoundedness validation under finite samples; asymptotic guarantees for imputation quality; performance on specialized domains
- **Low**: Claims about LLM-encoded causal structure transfer; robustness when true confounder distributions are complex/multimodal; generalizability to very small datasets

## Next Checks
1. **Domain Transfer Test**: Apply ProCI to a medical dataset (e.g., IHDP) with known hidden confounding and compare performance against VIGOR+ and baseline methods
2. **Distribution Misspecification Analysis**: Simulate data with multimodal or heavy-tailed true confounders; measure bias in ProCI's Gaussian/Bernoulli approximations
3. **Finite Sample KCIT Validation**: Run ProCI on small subsamples of Twins/Jobs; track KCIT p-value stability and CATE estimation variance as n decreases