---
ver: rpa2
title: 'Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems'
arxiv_id: '2507.20491'
source_url: https://arxiv.org/abs/2507.20491
tags:
- reasoning
- score
- language
- logical
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Text-JEPA, a lightweight neuro-symbolic framework
  for converting natural language to first-order logic (NL2FOL) and performing logical
  reasoning using Z3. The system emulates dual-process cognition by generating formal
  logic efficiently (System 1) and applying rigorous symbolic reasoning (System 2).
---

# Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems
## Quick Facts
- **arXiv ID:** 2507.20491
- **Source URL:** https://arxiv.org/abs/2507.20491
- **Reference count:** 14
- **Primary result:** Introduces Text-JEPA, a lightweight neuro-symbolic framework for NL2FOL translation and logical reasoning that achieves competitive accuracy with lower computational overhead than LLM-based systems.

## Executive Summary
This paper introduces Text-JEPA, a dual-process framework for question answering (QA) that mimics human cognition by separating fast, intuitive language processing (System 1) from deliberate, formal logical reasoning (System 2). The system translates natural language into first-order logic (FOL) and uses the Z3 solver to perform rigorous symbolic reasoning. Text-JEPA is designed to be lightweight and interpretable, making it suitable for domain-specific QA tasks where efficiency and explainability are critical. The authors propose custom metrics to evaluate both the quality of the natural language to FOL conversion and the accuracy of the reasoning process.

## Method Summary
Text-JEPA employs a neuro-symbolic approach that first converts natural language questions into first-order logic expressions, then uses the Z3 theorem prover for logical reasoning. The framework separates fast, approximate language processing from slow, precise symbolic reasoning, emulating the dual-process model of cognition. The authors developed three custom metrics—conversion score, reasoning score, and Spearman correlation—to evaluate both the translation fidelity and the accuracy of the logical inference. Experiments were conducted on domain-specific datasets, demonstrating that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to large language model (LLM)-based systems.

## Key Results
- Text-JEPA achieves competitive accuracy on domain-specific QA tasks.
- The system demonstrates significantly lower computational overhead compared to LLM-based QA systems.
- Custom metrics (conversion score, reasoning score, Spearman correlation) effectively capture both translation quality and reasoning accuracy.

## Why This Works (Mechanism)
The framework leverages the complementary strengths of neural and symbolic AI: neural components handle the variability and ambiguity of natural language efficiently, while symbolic reasoning ensures logical rigor and interpretability. By structuring the process into distinct phases—translation and reasoning—Text-JEPA avoids the brittleness of pure neural approaches and the scalability issues of pure symbolic systems. The dual-process design mirrors human cognition, enabling fast, approximate processing for initial understanding, followed by careful, formal reasoning for accurate answers.

## Foundational Learning
- **First-Order Logic (FOL):** A formal system for representing and reasoning about structured knowledge. *Why needed:* Provides a precise, unambiguous language for encoding domain knowledge and queries. *Quick check:* Can you express a simple rule (e.g., "All humans are mortal") in FOL?
- **Z3 Theorem Prover:** A powerful SMT solver for checking logical satisfiability and proving theorems. *Why needed:* Enables automated, rigorous reasoning over FOL expressions. *Quick check:* Can Z3 verify the truth of a given FOL formula?
- **Natural Language to FOL (NL2FOL):** The task of mapping natural language statements to formal logical expressions. *Why needed:* Bridges the gap between human language and machine reasoning. *Quick check:* Given a sentence, can you manually convert it to FOL?
- **Dual-Process Cognition:** A model of human thought that distinguishes fast, intuitive thinking from slow, deliberative reasoning. *Why needed:* Guides the architecture design, separating language understanding from logical inference. *Quick check:* Can you identify examples of System 1 and System 2 thinking in everyday reasoning?
- **Custom Evaluation Metrics:** Domain-specific measures for assessing translation and reasoning quality. *Why needed:* Standard metrics may not capture the nuances of FOL translation and logical correctness. *Quick check:* How would you design a metric to score the accuracy of a FOL translation?
- **Domain-Specific Datasets:** Specialized datasets tailored to particular QA domains. *Why needed:* Enable realistic evaluation and demonstrate practical applicability. *Quick check:* What makes a QA dataset "domain-specific"?

## Architecture Onboarding
- **Component Map:** Natural Language Input -> NL2FOL Translator -> FOL Expression -> Z3 Solver -> Logical Inference -> Answer Output
- **Critical Path:** Natural language question → NL2FOL conversion → FOL reasoning via Z3 → answer generation
- **Design Tradeoffs:** Prioritizes interpretability and efficiency over raw generalization; uses symbolic reasoning for correctness, sacrificing some flexibility of neural-only approaches
- **Failure Signatures:** Incorrect NL2FOL translation leads to invalid logical queries; Z3 solver may fail to find solutions for unsatisfiable or overly complex FOL; domain mismatch can degrade translation quality
- **3 First Experiments:** (1) Test NL2FOL conversion accuracy on a small set of hand-crafted examples; (2) Verify Z3 solver can correctly reason over a known FOL knowledge base; (3) Measure runtime and memory usage on a representative domain dataset

## Open Questions the Paper Calls Out
- How to generalize Text-JEPA to broader, less structured domains without sacrificing efficiency?
- Can the framework be extended to handle uncertainty and probabilistic reasoning?
- What are the limits of NL2FOL translation quality as language complexity increases?

## Limitations
- Limited to domain-specific datasets; scalability to open-domain QA is unclear
- Relies on high-quality NL2FOL translation; errors propagate to reasoning
- May struggle with highly ambiguous or context-dependent natural language

## Confidence
- **Methodology Soundness:** High
- **Reproducibility of Results:** High
- **Generalizability:** Medium
- **Practical Applicability:** High

## Next Checks
- Replicate the core NL2FOL conversion and Z3 reasoning pipeline on a new, small domain dataset
- Compare runtime and memory efficiency against a baseline LLM-based QA system
- Conduct ablation studies to quantify the impact of each component (e.g., NL2FOL quality vs. reasoning accuracy)