---
ver: rpa2
title: 'ChronusOmni: Improving Time Awareness of Omni Large Language Models'
arxiv_id: '2512.09841'
source_url: https://arxiv.org/abs/2512.09841
tags:
- temporal
- audio
- video
- segment
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChronusOmni, an omni large language model
  designed to enhance temporal awareness across both visual and auditory modalities.
  The core method interleaves explicit timestamp tokens with visual and audio representations
  at each time unit, enabling fine-grained cross-modal temporal alignment.
---

# ChronusOmni: Improving Time Awareness of Omni Large Language Models

## Quick Facts
- arXiv ID: 2512.09841
- Source URL: https://arxiv.org/abs/2512.09841
- Authors: Yijing Chen; Yihan Wu; Kaisi Guan; Yuchen Ren; Yuyue Wang; Ruihua Song; Liyun Ru
- Reference count: 40
- Key outcome: Introduces ChronusOmni, an omni large language model with explicit timestamp interleaving and two-stage training that achieves state-of-the-art performance on ChronusAV with over 30% improvement in temporal grounding.

## Executive Summary
This paper introduces ChronusOmni, an omni large language model designed to enhance temporal awareness across both visual and auditory modalities. The core method interleaves explicit timestamp tokens with visual and audio representations at each time unit, enabling fine-grained cross-modal temporal alignment. A two-stage optimization strategy—temporal-aware supervised finetuning followed by reinforcement learning with task-specific reward functions—strengthens the model's ability to reason about temporal boundaries and synchronization. The authors also construct ChronusAV, a large-scale, modality-complete dataset with precise timestamps and separate audio-visual captions to support comprehensive training and evaluation.

## Method Summary
ChronusOmni uses a two-stage training approach with temporal interleaved tokenization. The model processes 64 uniformly sampled video frames through OryxViT, extracts audio features using Whisper-Large-V3 and BEATs encoders (concatenated and downsampled 10x), and generates explicit timestamp tokens in "second{t}" format. These tokens are interleaved with visual and audio tokens in sequence: [T1, V1, A1, T2, V2, A2, ...]. The model is trained in two stages: first with supervised finetuning on 70K samples for dense captioning, then with GRPO reinforcement learning on 4K samples using IoU-based rewards for temporal predictions and Meteor-based rewards for captioning tasks.

## Key Results
- Achieves state-of-the-art performance on ChronusAV dataset with over 30% improvement in temporal grounding metrics
- Excels on visual-only temporal grounding benchmarks with R@0.7 of 45.95
- Maintains strong general audiovisual understanding without significant efficiency loss
- Ablation studies confirm importance of temporal interleaved tokenization (35.95 R@0.7 drop without TIT) and GRPO stage (30.71 R@0.7 drop without RL)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving explicit timestamp tokens with modality-specific tokens enables fine-grained cross-modal temporal alignment
- Mechanism: Timestamps are encoded as text tokens (e.g., "second{126.0}") and interleaved with visual/audio tokens in sequence: [T1, V1, A1, T2, V2, A2, ...]. This creates explicit positional relationships that the LLM can process natively without learned temporal embeddings
- Core assumption: Text-based timestamps provide sufficient temporal information for the model to learn cross-modal synchronization; no additional temporal encoding layers are needed
- Evidence anchors:
  - [abstract]: "we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities"
  - [section 4.1]: "In contrast to methods that rely on learnable temporal embeddings, our explicit textual timestamps do not require aligning a learned embedding space with the language space"
  - [corpus]: Factorized Learning paper notes temporal grounding forms a logical hierarchy with textual response, supporting explicit temporal encoding approaches

### Mechanism 2
- Claim: Reinforcement learning with task-specific reward functions overcomes SFT limitations for temporal boundary localization
- Mechanism: GRPO replaces maximum likelihood with IoU-based rewards for temporal predictions (V2T/A2T) and Meteor-based rewards for captioning tasks (T2V/T2A/V2A/A2V), enabling outcome-driven optimization
- Core assumption: SFT's treatment of time as categorical labels ignores metric structure; reward-based learning can better capture temporal continuity and boundary precision
- Evidence anchors:
  - [abstract]: "reinforcement learning with task-specific reward functions—strengthens the model's ability to reason about temporal boundaries"
  - [section 4.2]: "SFT treats it as a categorical label, ignoring the metric structure—i.e., the magnitude and distance between time points"
  - [corpus]: AHA paper identifies "Temporal Relation Errors" as a distinct hallucination type in audio-language models, suggesting reward-based alignment may address this

### Mechanism 3
- Claim: Modality-separated audio and video captions enable independent assessment and training of cross-modal temporal reasoning
- Mechanism: Dataset provides (timestamp, video caption, audio caption) triplets where each caption describes only its respective modality, preventing information leakage that could mask grounding failures
- Core assumption: Models can be trained to reason across modalities only when training data maintains strict modality separation
- Evidence anchors:
  - [section 5.1]: "99.3% of video captions and 97.5% of audio captions show no or only minor cross-modal leakage"
  - [table 1]: ChronusAV is "the only dataset that simultaneously provides precise timestamps, multimodal annotations across vision, sound/music, and speech, as well as separate captions for audio and video"
  - [corpus]: Daily-Omni paper similarly emphasizes "temporal alignment across modalities" as underexplored; corpus evidence for modality-separated captions specifically is weak

## Foundational Learning

- **Temporal tokenization strategies**
  - Why needed here: The paper assumes familiarity with timestamp encoding approaches (learnable embeddings vs. explicit tokens) and their tradeoffs
  - Quick check question: Can you explain why text-based timestamp tokens ("second{126.0}") might outperform learned positional embeddings for temporal grounding?

- **Reinforcement learning for structured prediction**
  - Why needed here: GRPO requires understanding policy optimization, reward shaping, and KL divergence penalties in the context of temporal prediction
  - Quick check question: What are the tradeoffs between IoU-based rewards for moment retrieval vs. language metrics (Meteor) for captioning?

- **Cross-modal alignment fundamentals**
  - Why needed here: The model must synchronize audio-visual streams with explicit timestamps; understanding multimodal fusion is prerequisite
  - Quick check question: How does explicit timestamp interleaving differ from implicit attention-based cross-modal alignment?

## Architecture Onboarding

- **Component map**:
  Visual encoder (OryxViT) → 64 uniformly sampled frames → visual tokens
  Audio encoders (Whisper-Large-V3 for speech, BEATs for sound/music) → concatenated features → 10x downsampled → audio tokens
  Timestamp text encoder → "second{t}" format tokens
  Token interleaver → [T1, V1, A1, ...] sequence
  LLM backbone (Qwen-2.5-7B) → predictions
  Two-stage trainer (SFT → GRPO with task-specific rewards)

- **Critical path**:
  1. Video/audio preprocessing and frame sampling (64 frames, audio downsampling)
  2. Timestamp token generation with "second{t}" format
  3. Token interleaving in temporal order
  4. Two-stage training: SFT on dense captioning → GRPO on grounding tasks

- **Design tradeoffs**:
  - 64 frames vs. denser sampling: Paper shows 64 frames outperforms 32 and is competitive with 128, suggesting sufficient temporal resolution
  - Frozen encoders vs. end-to-end: Only LLM parameters are finetuned, reducing training cost but potentially limiting encoder adaptation
  - Text timestamps vs. learned embeddings: Explicit tokens provide interpretability but may lack flexibility for irregular temporal patterns

- **Failure signatures**:
  - Temporal ordering errors: Model predicts time intervals outside video duration or in wrong order
  - Cross-modal hallucination: Audio captions describe visual content or vice versa (mitigated by modality-separated training data)
  - Format inconsistency: GRPO format reward enforces "second{start}-second{end}" output; failures indicate reward shaping issues
  - Boundary imprecision: IoU-based rewards should improve this; persistent issues suggest reward function inadequacy

- **First 3 experiments**:
  1. **Tokenization ablation**: Compare temporal interleaved tokenization vs. sequential modality tokens (Table 5 shows R@0.7 drops from 45.95 to 6.80 without TIT)
  2. **Training stage ablation**: Evaluate SFT-only vs. SFT+GRPO to isolate reinforcement learning contribution (Table 5 shows GRPO removal reduces V2A CIDEr from 13.60 to 1.24)
  3. **Frame sampling sensitivity**: Test 32/64/128 frame configurations to determine optimal temporal resolution for target video lengths (Table 6 shows 64 frames optimal for their training setup)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the temporal interleaved tokenization strategy scale effectively to hour-long videos without suffering from context window saturation or attention degradation?
- Basis in paper: [explicit] The conclusion states, "Future work will focus on deploying our model in real-world interactive scenarios and scaling it to hour-long videos."
- Why unresolved: The current evaluation limits video duration to 60–600 seconds with a fixed sample of 64 frames. Hour-long videos require processing significantly more tokens (time, visual, and audio), potentially exceeding the model's effective context length or diluting the attention mechanism's ability to maintain fine-grained alignment over extended periods.
- What evidence would resolve it: Evaluation of ChronusOmni on a dataset of videos averaging 60+ minutes, analyzing performance degradation relative to token count and attention entropy.

### Open Question 2
- Question: Does training on LLM-generated, disentangled captions constrain the model's ability to ground events that exhibit strong, inseparable audio-visual correlations?
- Basis in paper: [inferred] The ChronusAV dataset uses Gemini to generate separate captions for audio and video, a process verified on only 1,000 samples.
- Why unresolved: While the paper argues for separate captions to support specific subtasks, real-world events often fuse audio and visual semantics (e.g., the specific timbre of a visible instrument). Training on forced-disentanglement data might discourage the model from learning natural cross-modal semantic fusion, potentially limiting performance on integrated understanding tasks.
- What evidence would resolve it: A comparative study where the model is trained on entangled vs. disentangled captions and evaluated on complex semantic tasks requiring joint reasoning (e.g., identifying sarcasm where visual cues contradict audio).

### Open Question 3
- Question: Is the reinforcement learning stage capable of correcting temporal hallucinations when the supervised fine-tuning (SFT) stage has already entrenched incorrect boundary patterns?
- Basis in paper: [inferred] The paper notes SFT suffers from "pattern memorization" and "exposure-bias," yet the ablation study shows removing SFT drops T2V CIDEr from 5.07 to 0.69.
- Why unresolved: The drastic drop in the "w/o SFT" ablation suggests the GRPO stage relies heavily on a strong SFT initialization. It is unclear if the RL reward signals are strong enough to override "entrenched" temporal errors from SFT, or if the model simply refines already correct boundaries rather than correcting fundamental misalignments.
- What evidence would resolve it: An experiment introducing synthetic noise into SFT timestamps and measuring the RL stage's success rate in recovering the correct temporal boundaries.

## Limitations

- Dataset availability remains uncertain despite GitHub link, with significant human annotation effort required for reproducibility
- GRPO implementation details lack complete algorithmic specifications, particularly for advantage computation and rollout handling
- The two-stage training approach may be sensitive to initialization, as ablation shows SFT stage is critical for performance

## Confidence

**High Confidence (Mechanism 1 - Temporal Tokenization):** The interleaving approach is well-specified with clear implementation details. The paper provides explicit format specifications ("second{t}") and demonstrates significant performance gains in ablation studies (Table 5 shows R@0.7 drops from 45.95 to 6.80 without TIT).

**Medium Confidence (Mechanism 2 - GRPO Training):** The two-stage training process is clearly described, but the GRPO implementation lacks complete algorithmic details. The ablation showing SFT+GRPO vs SFT-only (Table 5) provides strong evidence, though the exact reward computation procedures remain underspecified.

**Medium Confidence (Mechanism 3 - Dataset Quality):** The modality-separated captioning approach is innovative and well-justified, with quantitative leakage analysis (99.3% video captions clean). However, the dataset creation process involved substantial manual curation, and the actual dataset availability remains uncertain.

## Next Checks

1. **Dataset Availability Verification:** Attempt to access ChronusAV via the provided GitHub link and verify that all required components (timestamps, modality-separated captions, video files) are available for full reproduction. Check licensing terms for academic/commercial use.

2. **GRPO Implementation Replication:** Implement the described reward functions (IoU + format for V2T/A2T, Meteor for captioning) and verify that the advantage computation follows standard PPO methodology. Test with a small subset to confirm optimization stability.

3. **Temporal Tokenization Ablation:** Reproduce the TIT ablation study by comparing the full model against a baseline using sequential modality tokens without interleaving. Measure the impact on temporal grounding metrics (R@0.7) to confirm the ~39 point improvement reported in Table 5.