---
ver: rpa2
title: Optimized Learned Count-Min Sketch
arxiv_id: '2512.12252'
source_url: https://arxiv.org/abs/2512.12252
tags:
- error
- memory
- learned
- sketch
- count-min
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Optimized Learned Count-Min Sketch (OptLCMS),
  a data structure for frequency estimation that analytically derives CMS parameters
  by partitioning the score space. It solves an optimization problem to minimize the
  upper bound on the probability of intolerable estimation error, with CMS parameters
  derived via KKT conditions and thresholds optimized through dynamic programming.
---

# Optimized Learned Count-Min Sketch

## Quick Facts
- **arXiv ID:** 2512.12252
- **Source URL:** https://arxiv.org/abs/2512.12252
- **Authors:** Kyosuke Nishishita; Atsuki Sato; Yusuke Matsui
- **Reference count:** 40
- **Primary result:** OptLCMS achieves up to 20× lower intolerable error probability compared to LCMS while constructing 5× to 10,000× faster through analytical parameter derivation.

## Executive Summary
This paper introduces Optimized Learned Count-Min Sketch (OptLCMS), a data structure for frequency estimation that analytically derives CMS parameters by partitioning the score space. The method formulates parameter selection as a constrained optimization problem to minimize the upper bound on intolerable estimation error probability. By solving this via KKT conditions and optimizing thresholds through dynamic programming, OptLCMS eliminates the need for slow empirical validation while maintaining or improving accuracy guarantees.

## Method Summary
OptLCMS uses a pre-trained ML model to score elements by frequency, then partitions the score space into G groups via dynamic programming that maximizes KL divergence between data and query distributions. Each partition receives its own CMS instance with parameters (ε_g, δ_g) derived analytically via KKT conditions under a memory budget M. High-scoring elements go to a Unique Bucket for exact counting. The analytical solution eliminates empirical grid search, constructing 5× to 10,000× faster than LCMS while achieving up to 20× lower intolerable error probability on the AOL query log dataset.

## Key Results
- Achieves up to 20× lower intolerable error probability compared to LCMS
- Constructs 5× to 10,000× faster by eliminating empirical validation
- Maintains comparable estimation accuracy while providing theoretical guarantees
- Provides explicit control over allowable error thresholds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partitioning the input domain based on learned scores reduces variance of frequency estimation for heavy hitters by isolating them into exact storage.
- **Mechanism:** A pre-trained ML model assigns a frequency score $s_x$ to each element. The architecture defines thresholds $t_1, \dots, t_G$. Elements with scores above the highest threshold ($t_G$) are routed to a Unique Bucket (UB) for exact counting, while lower-scoring elements are routed to $G$ separate Count-Min Sketch (CMS) instances.
- **Core assumption:** The learned model provides a meaningful ranking of element frequencies such that a threshold effectively separates "heavy" (UB-bound) from "light" (CMS-bound) items.
- **Evidence anchors:** [Section 4: Structure] "Elements are routed based on the model score: high-score items go to UB for exact counting, others to the CMS of the corresponding score group."

### Mechanism 2
- **Claim:** Formulating parameter selection as a constrained convex optimization problem allows for analytical derivation of CMS dimensions, eliminating slow empirical grid search.
- **Mechanism:** Instead of testing parameter combinations, the method minimizes the upper bound of the intolerable error probability $\sum \delta_g q_g$ subject to a memory budget $M$. By relaxing integer constraints on table dimensions, the Lagrangian can be solved analytically via Karush-Kuhn-Tucker (KKT) conditions to yield closed-form solutions for failure probability $\delta_g$ and error bounds $\epsilon_g$ for each partition.
- **Core assumption:** The relaxed (continuous) solution for table dimensions provides a sufficiently accurate approximation of the optimal integer-constrained dimensions.
- **Evidence anchors:** [Section 4: Analytical solution] "For fixed thresholds t, the problem is convex in $\delta$ and can be solved analytically via the Karush–Kuhn–Tucker (KKT) conditions."

### Mechanism 3
- **Claim:** Threshold optimization reduces the probability of intolerable error by maximizing the Kullback–Leibler (KL) divergence between the data distribution and the query distribution within partitions.
- **Mechanism:** The optimization objective for finding thresholds $t$ reduces to maximizing the KL divergence term $\sum u_g \ln (u_g/v_g)$, where $u_g$ is the proportion of elements in group $g$ and $v_g$ is the query probability. A Dynamic Programming (DP) approach traverses the score space to find thresholds that maximize this divergence, subject to a feasibility check that ensures the resulting $\delta_g < 1$.
- **Core assumption:** The query distribution used during optimization reflects the actual runtime query workload.
- **Evidence anchors:** [Section B.4] "the partitioning of the score space... should be done in such a way that $\sum u_g \ln u_g/v_g$ is maximized."

## Foundational Learning

- **Concept:** **Count-Min Sketch (CMS) Parameters ($\epsilon, \delta$)**
  - **Why needed here:** OptLCMS relies on the standard CMS guarantee that the estimation error exceeds $\epsilon N$ with probability less than $\delta$. Understanding that $w \propto 1/\epsilon$ (width) and $d \propto \ln(1/\delta)$ (depth) is required to interpret the optimization constraints.
  - **Quick check question:** If you want to lower the probability of a large estimation error, which dimension of the CMS table must you increase, and how does this affect memory?

- **Concept:** **Lagrangian Duality & KKT Conditions**
  - **Why needed here:** The paper claims "analytical derivation" of parameters. This is achieved by solving the Lagrangian of the constrained optimization problem. Readers must recognize that KKT conditions provide the necessary conditions for optimality in constrained convex problems.
  - **Quick check question:** Why can we solve for $\delta_g$ analytically only after "relaxing" the integer constraints on table width and depth?

- **Concept:** **Kullback–Leibler (KL) Divergence**
  - **Why needed here:** The threshold selection heuristic depends on maximizing the divergence between the distribution of stored elements ($u$) and queries ($v$).
  - **Quick check question:** In the context of this paper, what does a high KL divergence between the data distribution and query distribution imply for the efficiency of the resulting OptLCMS?

## Architecture Onboarding

- **Component map:**
  1. Pre-trained Model: Generates scores $s_x$ (assumed available)
  2. Score Statistics: Aggregate validation data to compute distributions ($u_g, v_g, N, n$)
  3. Optimizer (DP + KKT): Takes statistics and memory budget $M$; outputs thresholds $t$ and parameters $\epsilon_g, \delta_g$
  4. Data Structure: Array of $G$ CMS tables (initialized with dimensions derived from optimizer) + 1 Unique Bucket (UB)

- **Critical path:**
  1. Offline Phase: Run the DP algorithm to identify thresholds $t$ that maximize KL divergence. Use these $t$ to solve the KKT equations for optimal $\epsilon_g, \delta_g$.
  2. Build Phase: Initialize $G$ CMS tables with width $w_g = \lceil e/\epsilon_g \rceil$ and depth $d_g = \lceil \ln(1/\delta_g) \rceil$.
  3. Query Time: Input element $x$ -> Score $s_x$ -> Route to UB or specific CMS $g$ -> Return estimate.

- **Design tradeoffs:**
  - **Integer Relaxation:** The optimization treats table dimensions as continuous variables. This may result in a calculated memory usage slightly different from the actual discrete memory usage.
  - **Approximate Feasibility:** The DP checks an approximate condition $\hat{\delta}_g < 1$ to ensure validity. This trades strict optimality for polynomial-time complexity.

- **Failure signatures:**
  - **Construction Failure:** DP fails to find feasible thresholds (returns $\delta_g \ge 1$). This implies the memory budget $M$ is too small for the requested allowable error $\epsilon$ or the model is uninformative.
  - **Runtime Performance Drop:** If the query distribution shifts significantly from the optimization-time assumption, the theoretical guarantee on $\sum \delta_g q_g$ may no longer hold for the new workload.

- **First 3 experiments:**
  1. **Validation of Derivation:** Reproduce the "Construction Time" result (Table 1) to verify that the analytical solution ($\sim$0.003s) is indeed orders of magnitude faster than LCMS grid search ($\sim$10.7s).
  2. **Memory vs. Error:** Reproduce Figure 2 to confirm that analytically derived parameters maintain the "intolerable error probability" guarantee better than empirically tuned baselines.
  3. **Ablation on $G$:** Vary the number of partitions $G$ to observe the trade-off point where increasing partitions (complexity) yields diminishing returns in error reduction.

## Open Questions the Paper Calls Out

- **Question:** How does OptLCMS perform on network packet datasets (CAIDA) compared to confidence-aware LCMS variants?
- **Basis in paper:** [explicit] The conclusion states, "Due to time and scope constraints, we did not evaluate on the CAIDA dataset [9] or compare with confidence-aware LCMS [25]; extending our evaluation to these settings is left for future work."
- **Why unresolved:** The authors restricted experiments to the AOL query log dataset and did not benchmark against the confidence-aware baseline, leaving a gap in validating the method's generality across different data modalities and state-of-the-art competitors.
- **What evidence would resolve it:** Experimental results on the CAIDA dataset showing construction time and intolerable error probability of OptLCMS relative to the confidence-aware LCMS [25].

## Limitations

- **Integer Relaxation Impact:** The paper claims negligible non-convexity from relaxing CMS dimensions to continuous variables, but doesn't quantify the difference between relaxed analytical solutions and actual integer-constrained implementations.
- **Model Dependency:** The method's effectiveness fundamentally depends on the pre-trained model's ability to meaningfully rank elements by frequency, which isn't addressed for cases where the model has poor prediction accuracy.
- **Distributional Assumptions:** The theoretical guarantees assume the query distribution during optimization matches runtime reality, but real-world data streams often experience concept drift that invalidates these bounds.

## Confidence

**High Confidence:** The optimization problem formulation (minimizing $\sum \delta_g q_g$ under memory constraint) is mathematically sound and the KKT-based solution methodology is standard for convex problems. The empirical result showing 20× reduction in intolerable error probability is directly measurable.

**Medium Confidence:** The Dynamic Programming approach for threshold selection follows established combinatorial optimization patterns, but the specific adaptation to KL divergence maximization for this application hasn't been validated through ablation studies.

**Low Confidence:** The paper doesn't address how sensitive the solution is to the choice of $G$ (number of partitions). Without empirical exploration of this hyperparameter, it's unclear whether the theoretical framework provides practical guidance or if the benefits are highly configuration-dependent.

## Next Checks

1. **Integer vs. Continuous Dimensions:** Construct OptLCMS using both the relaxed analytical solution and the corresponding rounded integer dimensions. Compare actual memory usage and error bounds to quantify the approximation gap.

2. **Model Quality Sensitivity:** Evaluate OptLCMS performance across ML models with varying frequency prediction accuracy (e.g., random baseline vs. strong model). Measure whether the error reduction scales with model quality or plateaus regardless of prediction capability.

3. **Distributional Robustness:** Evaluate the data structure under query distributions that differ from the optimization-time assumption (e.g., temporal shifts in query popularity). Measure how quickly the intolerable error probability guarantee degrades as the distribution shifts.