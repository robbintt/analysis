---
ver: rpa2
title: 'REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large
  Reasoning Models'
arxiv_id: '2505.19862'
source_url: https://arxiv.org/abs/2505.19862
tags:
- reflection
- arxiv
- reward
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REA-RL introduces a reflection-aware online reinforcement learning
  approach to address the overthinking problem in large reasoning models. It employs
  a small reflection model to detect and remove overthinking tokens during sequential
  revision, combined with parallel sampling to improve scaling efficiency.
---

# REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models

## Quick Facts
- **arXiv ID:** 2505.19862
- **Source URL:** https://arxiv.org/abs/2505.19862
- **Reference count:** 40
- **Primary result:** 35% response shortening without accuracy loss through reflection-aware overthinking removal

## Executive Summary
REA-RL introduces a reflection-aware online reinforcement learning approach to address the overthinking problem in large reasoning models. It employs a small reflection model to detect and remove overthinking tokens during sequential revision, combined with parallel sampling to improve scaling efficiency. Additionally, it introduces a reflection reward to prevent models from favoring short yet non-reflective responses. Experiments show that the reflection model and reflection reward each reduce inference costs while preserving performance, with their combination achieving a 35% response shortening without compromising accuracy.

## Method Summary
REA-RL addresses overthinking in large reasoning models through a dual-mechanism approach. First, it employs a small reflection model that identifies overthinking tokens by comparing their similarity to reflection embeddings and checking if they appear in final answers. Second, it uses parallel sampling where multiple response candidates are generated simultaneously and refined iteratively. The method introduces a reflection reward that penalizes responses lacking reflection while avoiding reward hacking by setting a quantile-based threshold. These components work together in an online reinforcement learning framework where the reflection model guides token pruning and the reflection reward shapes the policy to favor concise yet reflective reasoning.

## Key Results
- 35% reduction in response length without compromising accuracy when combining reflection model and reflection reward
- Reflection model alone removes overthinking tokens effectively while maintaining performance
- Reflection reward prevents models from favoring short non-reflective responses
- Parallel sampling strategy improves scaling efficiency during training

## Why This Works (Mechanism)
The method works by addressing the overthinking problem through two complementary mechanisms. The reflection model detects overthinking by measuring token similarity to reflection embeddings and ensuring pruned tokens don't appear in final answers. The reflection reward shapes the learning process by penalizing responses that lack reflective reasoning while preventing models from simply outputting shorter but less thoughtful responses. The combination allows the model to maintain reflection ability on complex problems while reducing unnecessary elaboration on simpler ones.

## Foundational Learning
- **Reflection detection**: Identifying tokens that represent overthinking through embedding similarity and final answer presence - needed to distinguish productive reflection from redundant elaboration, quick check: measure token similarity distributions
- **Reinforcement learning with reward shaping**: Using reflection-specific rewards to guide policy learning - needed to prevent models from exploiting reward signals by being overly concise, quick check: analyze reward distribution across different response lengths
- **Parallel sampling in RL**: Generating multiple candidates simultaneously to improve exploration efficiency - needed to handle the computational cost of long reasoning sequences, quick check: measure variance reduction across samples

## Architecture Onboarding

**Component map:** Input -> LRM (Large Reasoning Model) -> Reflection Model -> Token Pruning -> Reflection Reward -> RL Policy Update

**Critical path:** During inference, the reflection model runs in parallel with the LRM to identify overthinking tokens. During training, parallel sampling generates multiple responses that are refined through sequential revision guided by the reflection model's feedback.

**Design tradeoffs:** The approach trades additional computation during training (parallel sampling, reflection model inference) for reduced inference costs and better performance. The reflection model adds complexity but enables targeted overthinking removal.

**Failure signatures:** If the reflection model is too aggressive, it may remove tokens that contain important intermediate reasoning steps. If the reflection reward threshold is set too low, the model may produce overly terse responses lacking necessary reflection.

**First experiments:** 1) Run the reflection model on validation responses to measure overthinking detection accuracy. 2) Test parallel sampling efficiency by comparing wall-clock time for generating multiple responses. 3) Evaluate the reflection reward's impact on response length and reasoning quality using ablation studies.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does REA-RL perform on LRMs pre-trained from scratch versus distilled LRMs, and does the relative effectiveness of the reflection model versus reflection reward change with model scale?
- Basis in paper: [explicit] "Our approach is only validated on distilled 7B LRMs. Due to the large model size and long training time, we do not perform validation on LRMs pre-trained from scratch."
- Why unresolved: The authors acknowledge this gap but do not speculate on how the approach would transfer to different model types or scales.
- What evidence would resolve it: Experiments applying REA-RL to larger LRMs (e.g., 32B, 70B) and to models trained from scratch, comparing performance gains and efficiency improvements.

### Open Question 2
- Question: Can the reflection reward mechanism generalize to non-English or multilingual reasoning tasks, given its reliance on specific English keywords?
- Basis in paper: [inferred] The reflection reward uses keywords ("wait", "alternatively", "check", "but") that are English-specific. All experiments are on English math datasets (GSM8K, MATH500, etc.).
- Why unresolved: The paper does not address whether different keyword sets would be needed for other languages, or whether a language-agnostic reflection detection method exists.
- What evidence would resolve it: Experiments on multilingual reasoning benchmarks, or analysis of reflection patterns across languages to identify universal indicators.

### Open Question 3
- Question: What is the principled method for selecting the optimal quantile threshold (D0.2) in the reflection reward, and how does it interact with problem difficulty distributions?
- Basis in paper: [inferred] The ablation study tests D0.1, D0.2, D0.4, showing trade-offs, but the 0.2 default is not justified theoretically. The paper notes "its impact on performance is relatively small."
- Why unresolved: The threshold appears to be empirically chosen, without clear guidance on how to set it for different training distributions or model sizes.
- What evidence would resolve it: Systematic study correlating optimal threshold with training data difficulty distribution, or theoretical analysis of how the threshold affects the reward landscape.

## Limitations
- The approach has only been validated on distilled 7B LRMs, not on models pre-trained from scratch or larger architectures
- The reflection reward mechanism relies on English-specific keywords, limiting direct applicability to multilingual tasks
- The reflection model's accuracy directly impacts the quality of overthinking removal, introducing potential bias

## Confidence
- **High Confidence**: The core mechanism of using a reflection model to detect and remove overthinking tokens is technically sound and supported by empirical results showing 35% response shortening without accuracy loss
- **Medium Confidence**: The claim that the reflection reward effectively prevents favoring short yet non-reflective responses requires further validation across diverse reasoning tasks and model architectures
- **Medium Confidence**: The generalization of the approach to complex reasoning domains beyond the evaluated mathematical and commonsense tasks needs additional empirical support

## Next Checks
1. Evaluate the approach's performance on long-form reasoning tasks (e.g., multi-step scientific reasoning or code generation) to assess generalization beyond mathematical problems
2. Conduct ablation studies comparing different reflection model architectures and sizes to determine the minimum viable configuration for effective overthinking detection
3. Measure the computational overhead of parallel sampling during training across different model scales (e.g., 8B, 32B, 70B parameters) to establish practical scalability limits