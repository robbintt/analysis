---
ver: rpa2
title: Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics
  and Tabular Data for Forecasting Future Tobacco Use
arxiv_id: '2512.23137'
source_url: https://arxiv.org/abs/2512.23137
tags:
- data
- brain
- graph
- these
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tackles the challenge of predicting future tobacco use
  in adolescents by integrating dynamic brain connectivity data from resting-state
  fMRI with demographic and clinical tabular data. The authors introduce a time-aware
  Graph Neural Network with Transformer Fusion (GNN-TF) that combines non-Euclidean
  brain network data and Euclidean tabular covariates in an end-to-end framework.
---

# Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use

## Quick Facts
- **arXiv ID:** 2512.23137
- **Source URL:** https://arxiv.org/abs/2512.23137
- **Reference count:** 13
- **Primary result:** GNN-TF model achieves AUC 0.697-0.700 for predicting future tobacco use, significantly outperforming GC-LSTM (0.520-0.658) by integrating dynamic brain connectivity with demographic data

## Executive Summary
This study presents a novel Graph Neural Network with Transformer Fusion (GNN-TF) framework for predicting future tobacco use in adolescents using baseline fMRI and tabular data. The method constructs dynamic functional connectivity matrices through a sliding window approach with FDR-based thresholding, then encodes these brain network features using graph neural networks while simultaneously processing demographic covariates through a transformer model. Evaluated on the NCANDA dataset (N=522), the GNN-TF approach demonstrates superior predictive accuracy compared to classical machine learning models and state-of-the-art GC-LSTM methods, achieving AUC values of 0.697 and 0.700 across different brain atlases. The model shows robustness to different brain atlases and benefits from transfer learning with pretrained transformers.

## Method Summary
The GNN-TF framework integrates dynamic brain connectivity data from resting-state fMRI with demographic and clinical tabular data through an end-to-end architecture. The method constructs dynamic functional connectivity matrices using a sliding window approach (8 windows of 130 TR width, step 20 TR) with FDR-based thresholding to create sparse adjacency matrices. A shared-weight 2-layer GCN encodes each graph into 128-dimensional embeddings, which are then fused with projected tabular features (Age, Sex) through a transformer model. The input sequence consists of a projected tabular token followed by the 8 graph embeddings, processed by a standard transformer (3 layers, 4 heads) or pretrained variant (ViT, BERT, GPT-2). The model is trained using stratified 5-fold cross-validation with inner-fold ensembling (5 models) and early stopping.

## Key Results
- GNN-TF achieves AUC values of 0.697 and 0.700 across different brain atlases, significantly outperforming GC-LSTM (0.520-0.658)
- The model demonstrates superior performance compared to classical ML models (LR, RF), GNN-only models, and other transformer-based approaches
- Transfer learning with pretrained transformers (GNN-TF-B/G) provides effective regularization and maintains high predictive accuracy
- Ablation studies confirm the importance of both node features and brain connectivity patterns for prediction
- The approach shows robustness to different brain atlases (Power 264 vs Harvard-Oxford)

## Why This Works (Mechanism)

### Mechanism 1
GNNs provide superior feature extraction for non-Euclidean brain data compared to Euclidean-only methods. The model constructs dynamic functional connectivity (FC) matrices from fMRI time series and applies a Graph Convolutional Network (GCN) that performs message passing over the sparse adjacency matrix, aggregating node features (MNI coordinates, system IDs) based on connectivity strength. This preserves the topological structure of brain networks. The core assumption is that brain functional connectivity relevant to tobacco use is encoded in the graph topology rather than just isolated region intensities.

### Mechanism 2
Transformer fusion allows temporal dynamics and static covariates to jointly influence prediction weights via self-attention. The model projects tabular data (Age, Sex) into an embedding token, prepended to the sequence of GNN embeddings representing 8 sliding windows. The transformer's self-attention mechanism allows the model to weigh the importance of specific time-window graph features relative to subject demographics, integrating them into a classification token. The core assumption is that the relationship between baseline brain dynamics and future tobacco use is non-linear and modulated by demographic factors like age and sex.

### Mechanism 3
FDR-based thresholding enhances model robustness by filtering spurious correlations. Before graph construction, the model applies Benjamini/Hochberg False Discovery Rate (FDR) correction to correlation matrices, creating sparse binary adjacency matrices where only statistically significant connections remain. This reduces noise in the GNN input. The core assumption is that weak or non-significant correlations in resting-state fMRI represent noise or physiologically irrelevant connections that degrade graph learning.

## Foundational Learning

- **Concept: Dynamic Functional Connectivity (dFC) via Sliding Windows**
  - Why needed here: The model relies on the premise that brain connectivity changes over time (non-stationarity). A static correlation matrix would fail to capture the temporal features the transformer is designed to process.
  - Quick check question: If you used a single correlation matrix for the entire scan session, how would the GNN-TF input sequence change? (Answer: It would collapse to a single token, breaking the temporal transformer logic).

- **Concept: Non-Euclidean vs. Euclidean Data Structures**
  - Why needed here: The paper explicitly contrasts graph (non-Euclidean) and tabular (Euclidean) data. Understanding that graphs lack a fixed grid structure (unlike images) clarifies why Standard CNNs are insufficient and GNNs are required.
  - Quick check question: Why can't you feed a correlation matrix directly into a standard Random Forest without feature engineering? (Answer: A matrix implies a specific node ordering/structure that standard models treat as independent rows, losing relational context).

- **Concept: Late Fusion vs. Early/Transformer Fusion**
  - Why needed here: The paper identifies "late fusion" (concatenating outputs) as a suboptimal baseline. Understanding the difference is crucial for seeing why the transformer (jointly processing inputs) works better.
  - Quick check question: In "late fusion," when do the demographics and the brain data interact? (Answer: Only at the final prediction layer, preventing the model from learning interactions like "this specific brain pattern is only risky for this specific age group").

## Architecture Onboarding

- **Component map:** rs-fMRI Time Series + Demographics -> C-PAC pipeline -> Sliding Window (8 windows) -> Correlation Matrices -> FDR Thresholding -> Graphs -> Shared-weight GNN (GCN/GAT) -> 128-dim embeddings -> Projected Tabular token -> Transformer (or Pretrained ViT/BERT/GPT-2) -> CLS token -> Linear Layers -> Softmax

- **Critical path:** The alignment of the GNN output dimension (128) and the Transformer hidden dimension. If the GNN output doesn't match the transformer's expected input size (or lack a projection layer), the fusion fails.

- **Design tradeoffs:**
  - Pretrained vs. Scratch: The paper shows GNN-TF (scratch) works well, but transfer learning (GNN-TF-B/G) allows for fewer trainable parameters, reducing overfitting risk on medium datasets (N=522).
  - GNN Backbone: The paper tests GCN, GAT, GIN. GCN is chosen for computational efficiency and baseline performance, though attention-based GNNs (GAT) might capture node importance better.

- **Failure signatures:**
  - Overfitting: High variance across folds (mitigated in the paper by inner-fold ensembling).
  - Fusion Mismatch: If using "Late Fusion" (ablation baseline), performance drops (AUC drops to ~0.67 vs 0.69), indicating the model failed to capture temporal-tabular interactions.
  - Atlas Sensitivity: While robust, incorrect atlas selection (e.g., too few ROIs) might miss fine-grained connectivity patterns.

- **First 3 experiments:**
  1. Baseline Stability: Run "Late Fusion" vs. "Transformer Fusion" on a single atlas to verify the transformer is actually adding value to the integration (Table 2 shows TF > Late Fusion).
  2. Ablation on Connectivity: Set the adjacency matrix to identity (removing edges) or remove node features to test if the GNN is learning from topology vs. just node attributes.
  3. Pre-training Check: Compare GNN-TF (from scratch) vs. GNN-TF-B (BERT pretrained) specifically on the PRAUC metric to see if transfer learning helps with the minority class (smokers) imbalance.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the integration of high-dimensional behavioral covariates (e.g., questionnaires) improve predictive accuracy without inducing overfitting?
  - Basis in paper: The authors state in Section 4.3, "it's yet to be determined if this would improve performance or if high-dimensional covariates might introduce overfitting problems."
  - Why unresolved: The current study limited tabular inputs to sex and age, leaving the contribution of richer clinical variables untested.
  - What evidence would resolve it: Experiments adding behavioral metrics to the framework, evaluated against validation loss and AUC metrics to check for overfitting.

- **Open Question 2:** Does aligning the sliding window time span with the prediction horizon (e.g., using shorter windows for immediate outcomes) improve model validity?
  - Basis in paper: Section 4.3 posits, "A plausible hypothesis is that immediate future outcomes... could correlate with brain dynamics over shorter time periods," and notes this requires future investigation.
  - Why unresolved: The current window size (130 TR) was chosen for a 3-year forecast; its applicability to different temporal scales remains unvalidated.
  - What evidence would resolve it: Ablation studies varying window sizes and correlating them with prediction accuracy for outcomes occurring at different time lags.

- **Open Question 3:** Can transfer learning from pretrained transformers effectively mitigate overfitting when applying GNN-TF to smaller, single-site fMRI datasets?
  - Basis in paper: The authors note that for datasets with smaller sample sizes, "its applicability... warrants further evaluation."
  - Why unresolved: The NCANDA dataset is described as "medium-sized" (N=522), and it is unclear if the transfer learning strategy is sufficient for the significantly smaller sample sizes typical of single-site studies.
  - What evidence would resolve it: Benchmarking the model's performance on independent, smaller cohorts to observe the variance in AUC and overfitting metrics.

## Limitations
- The model relies on only two tabular variables (Age, Sex), representing a narrow view of potential risk factors
- The dataset size (N=522) and potential class imbalance constrain generalizability
- FDR thresholding may filter potentially meaningful weak connections
- The clinical significance threshold for AUC improvement is not explicitly defined

## Confidence
- **High:** Architectural feasibility of GNN-Transformer fusion for multimodal integration
- **Medium:** Predictive performance advantage (AUC 0.697-0.700) over GC-LSTM (0.520-0.658)
- **Medium:** Transferability to other neuroimaging prediction tasks

## Next Checks
1. **Class Balance Sensitivity:** Retrain the model on balanced subsets (equal smokers/non-smokers) to determine if the AUC advantage persists under class balance conditions.
2. **Connectivity Ablation:** Replace the FDR thresholded adjacency matrix with a fully connected or random graph to quantify how much performance depends on the statistical filtering of connections.
3. **Temporal Resolution Impact:** Vary the sliding window parameters (number of windows, width, step size) to identify the optimal temporal granularity for tobacco use prediction.