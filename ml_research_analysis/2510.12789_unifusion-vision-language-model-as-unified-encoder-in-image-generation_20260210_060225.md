---
ver: rpa2
title: 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation'
arxiv_id: '2510.12789'
source_url: https://arxiv.org/abs/2510.12789
tags:
- image
- generation
- encoder
- unified
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes UniFusion, a diffusion-based generative model
  that uses a frozen vision-language model (VLM) as a unified encoder for both text
  and image inputs. The key innovation is Layerwise Attention Pooling (LAP), which
  extracts features from multiple layers of the VLM to capture both fine-grained visual
  details and high-level semantics.
---

# UniFusion: Vision-Language Model as Unified Encoder in Image Generation

## Quick Facts
- arXiv ID: 2510.12789
- Source URL: https://arxiv.org/abs/2510.12789
- Reference count: 40
- Uses frozen vision-language model as unified encoder for image generation

## Executive Summary
UniFusion introduces a novel diffusion-based generative model that leverages a frozen vision-language model (VLM) as a unified encoder for both text and image inputs. The approach addresses the challenge of cross-modal encoding by extracting features from multiple VLM layers using Layerwise Attention Pooling (LAP), capturing both fine-grained visual details and high-level semantics. The model demonstrates competitive performance on text-to-image generation and editing tasks while being trained on a smaller dataset compared to larger models.

## Method Summary
The method employs a frozen VLM as a unified encoder, with Layerwise Attention Pooling (LAP) extracting features from multiple VLM layers to capture multi-level representations. VLM-Enabled Rewriting Injection with Flexible Inference (VERIFI) is introduced to leverage the VLM's reasoning capabilities for improved prompt following. The model achieves competitive performance through this unified encoding approach, demonstrating effectiveness in both generation and editing tasks while maintaining cross-modal knowledge transfer capabilities.

## Key Results
- Achieves competitive performance on text-to-image generation and editing tasks
- Outperforms larger models like Flux.1 and BAGEL on DPG-Bench with smaller training set
- Exhibits zero-shot generalization to multi-reference editing and image-to-image variations despite single-reference training

## Why This Works (Mechanism)
The unified encoder approach works by leveraging the VLM's pre-trained cross-modal understanding, allowing the model to extract semantically rich features from both text and image inputs. Layerwise Attention Pooling captures multi-level features by aggregating information from different VLM layers, providing both detailed visual information and high-level semantic understanding. The frozen VLM weights ensure stable feature extraction while reducing training complexity.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Pre-trained models that understand both visual and textual information
  - Why needed: Provides cross-modal understanding for unified encoding
  - Quick check: Verify VLM can process both image and text inputs effectively

- **Diffusion Models**: Generative models that denoise random noise into coherent images
  - Why needed: Core generation framework for image synthesis
  - Quick check: Ensure stable training and coherent image output

- **Attention Mechanisms**: Allows selective focus on relevant features from different VLM layers
  - Why needed: Enables multi-level feature extraction from VLM
  - Quick check: Validate attention weights capture meaningful visual-textual relationships

## Architecture Onboarding

**Component Map**: VLM -> LAP -> VERIFI -> Diffusion Model -> Generated Image

**Critical Path**: Text/Image Input → VLM → LAP (multi-layer feature extraction) → VERIFI (prompt injection) → Diffusion Model → Generated Image

**Design Tradeoffs**: 
- Frozen VLM weights provide stable features but limit adaptation to generation-specific tasks
- Multi-layer extraction captures rich information but increases computational complexity
- Smaller training set reduces resource requirements but may limit fine-tuning capabilities

**Failure Signatures**: 
- Poor prompt following indicates VERIFI integration issues
- Visual artifacts suggest LAP feature extraction problems
- Mode collapse indicates diffusion model instability

**First Experiments**:
1. Validate LAP extracts meaningful features across multiple VLM layers
2. Test VERIFI integration with simple prompt variations
3. Verify zero-shot generalization to multi-reference editing

## Open Questions the Paper Calls Out
None

## Limitations
- Frozen VLM weights may restrict adaptation to generation-specific features
- Lack of comprehensive analysis on optimal number of VLM layers for LAP
- Limited quantitative evaluation of zero-shot generalization capabilities

## Confidence
- High confidence: Unified encoder architecture is technically sound
- Medium confidence: VERIFI effectiveness demonstrated but needs more ablation studies
- Low confidence: Cross-modal transfer claims lack rigorous quantitative backing

## Next Checks
1. Conduct ablation studies isolating individual VLM layer contributions to generation quality
2. Evaluate model robustness on out-of-distribution prompts and diverse editing scenarios
3. Compare feature attribution methods to verify VLM semantic features are actively utilized during generation