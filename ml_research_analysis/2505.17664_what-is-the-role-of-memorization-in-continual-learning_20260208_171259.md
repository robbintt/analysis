---
ver: rpa2
title: What is the role of memorization in Continual Learning?
arxiv_id: '2505.17664'
source_url: https://arxiv.org/abs/2505.17664
tags:
- memorization
- learning
- training
- data
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the role of memorization in continual learning,
  where models are trained incrementally on sequences of tasks. Prior studies focused
  on memorization in static settings, but its impact on incremental training remains
  underexplored.
---

# What is the role of memorization in Continual Learning?

## Quick Facts
- arXiv ID: 2505.17664
- Source URL: https://arxiv.org/abs/2505.17664
- Reference count: 40
- This work investigates the role of memorization in continual learning, finding that higher memorization scores increase forgetting risk after task shifts, and that memorization-aware buffer policies can improve performance when buffers are large.

## Executive Summary
This paper investigates memorization's role in continual learning, where models are trained incrementally on sequences of tasks. While prior studies focused on memorization in static settings, its impact on incremental training remains underexplored. The authors find that samples with higher memorization scores are forgotten faster after task shifts than regular samples. They introduce a computationally efficient proxy for memorization score and integrate it into buffer policy design, showing that including such samples improves performance when buffers are large. Their experiments span standard continual learning benchmarks and reveal that memorization-aware strategies can enhance continual learning outcomes.

## Method Summary
The paper proposes a computationally efficient proxy for memorization score based on the training iteration when a sample is first classified correctly and remains correct. This proxy tracks the first iteration where each sample is classified correctly and stays correct for all subsequent iterations. The authors use this proxy to inform buffer selection policies in class-incremental learning settings, comparing bottom-k (regular samples), mid-k, and top-k (memorized samples) selection strategies. The method is implemented within the Mammoth framework using reduced ResNet18 architectures, with experiments on Split-CIFAR10/100 and TinyImageNet datasets.

## Key Results
- Samples with high memorization scores are forgotten faster after task shifts than regular samples
- The importance of high-memorization samples for buffer selection increases with buffer size
- The training iteration proxy correlates strongly (r=0.81, ρ=0.83) with the Feldman memorization estimator while being ~250x faster

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Samples with higher memorization scores are forgotten faster after task shifts than regular samples.
- Mechanism: Memorized samples (often long-tail or mislabeled) rely on specific representation patterns that are overwritten when new tasks introduce different class distributions. The model loses access to the specific features needed for these atypical examples.
- Core assumption: Memorization scores computed offline (stationary training) correlate with what the model actually memorizes during incremental training.
- Evidence anchors:
  - [abstract] "We clarified that learning examples with high memorization scores are forgotten faster than regular samples."
  - [Section 3.3] Accuracy for memorized samples drops significantly after training on two subsequent tasks.
  - [corpus] Limited direct support; related work on stability-plasticity tradeoffs (STAR paper) addresses similar forgetting dynamics but not memorization-specific.
- Break condition: If offline memorization scores do not generalize to incremental training representations (Section 3.4 shows linear probes on frozen representations can still classify memorized data, suggesting the assumption may be weak).

### Mechanism 2
- Claim: The importance of high-memorization samples for buffer selection increases with buffer size.
- Mechanism: With small buffers, the priority is maintaining performance on "typical" (easy/representative) samples that preserve general class structure. With large buffers, there is capacity to also retain atypical/memorized samples needed for peak test accuracy on long-tail data.
- Core assumption: Buffer capacity determines the tradeoff between generalization (regular samples) and coverage (memorized samples).
- Evidence anchors:
  - [abstract] "The importance of high-memorization score sample rises with an increase in the buffer size."
  - [Section 5.2, Table 3] Adding 10% top-k samples improves accuracy only at buffer sizes 10,000 and 20,000, not at 2,000 or 5,000.
  - [corpus] Related buffer-based methods (GCR, GRASP) focus on efficient sample selection but do not explicitly address memorization-aware policies.
- Break condition: If the dataset has few long-tail samples or low memorization requirements, the buffer-size-dependent benefit disappears.

### Mechanism 3
- Claim: Training iteration at first correct classification serves as a computationally efficient proxy for memorization score.
- Mechanism: General patterns are learned early in training; memorization occurs later as the model fits specific examples. Later-learned samples correlate with higher memorization scores.
- Core assumption: The temporal ordering of learning (early vs. late) is consistent across training runs and correlates with the Feldman memorization estimator.
- Evidence anchors:
  - [Section 3.5] Pearson r = 0.81, Spearman ρ = 0.83 between training iteration proxy and Feldman estimator.
  - [Figure 8] Strong correlation between training iteration and Feldman estimator on CIFAR-100 samples.
  - [corpus] No corpus evidence directly validates this specific proxy; prior work on noisy label detection uses similar timing heuristics but in different contexts.
- Break condition: If training dynamics change significantly (different optimizers, learning rate schedules, or architectures), the correlation may weaken or reverse.

## Foundational Learning

- Concept: **Feldman Memorization Score**
  - Why needed here: The paper uses this definition throughout; it quantifies how much a model's prediction on a sample depends on that sample being in the training set.
  - Quick check question: Can you explain why memorization score is defined as the difference between model performance with vs. without the sample in training data?

- Concept: **Class-Incremental Learning (CIL)**
  - Why needed here: The experimental setting; each task introduces disjoint classes, making forgetting more severe than task-incremental scenarios.
  - Quick check question: What distinguishes class-incremental from task-incremental learning in terms of evaluation difficulty?

- Concept: **Rehearsal Buffer Policies**
  - Why needed here: The paper's practical contribution integrates memorization-awareness into buffer selection (reservoir sampling variants).
  - Quick check question: Why does the paper argue that selecting low/mid memorization samples works better for small buffers?

## Architecture Onboarding

- Component map:
  1. **Memorization Score Proxy Calculator**: Tracks first correct classification iteration per sample during training; minimal overhead.
  2. **Buffer Selector**: Chooses top-k, mid-k, or bottom-k samples based on proxy scores; per-class balanced selection.
  3. **Rehearsal Trainer**: Standard experience replay combining current task data with buffer samples.

- Critical path:
  1. During task training, record `v[i]` (first correct iteration) for each sample.
  2. At task end, apply selector (bottom-k, mid-k, or hybrid) per class.
  3. Update buffer by replacing old samples from over-represented classes.

- Design tradeoffs:
  - **Bottom-k vs. Top-k selection**: Bottom-k preserves regular samples (better for small buffers); top-k preserves memorized samples (better for large buffers but risks overfitting to atypical data).
  - **Proxy vs. Feldman estimator**: Proxy is ~250x faster but has moderate (r≈0.81) correlation; Feldman is precise but computationally infeasible for incremental training.
  - **Buffer update timing**: Paper found end-of-task updates work better than per-epoch updates (preliminary experiments, not fully detailed).

- Failure signatures:
  - Using top-k selection with small buffers (<2,000 samples) → accuracy drops below baseline (Table 1: 41.16% vs. 55.18% on Split-CIFAR10).
  - Applying memorization proxy without balancing per-class → over-representation of classes with many late-learned samples.
  - Assuming offline memorization scores transfer to incremental training → Section 3.4 shows linear probes can still classify "memorized" samples, suggesting proxy may not capture true incremental memorization.

- First 3 experiments:
  1. **Validate proxy correlation**: Train ResNet18 on CIFAR-100 subset; compare training-iteration proxy vs. Feldman estimator on 150 random samples. Target: r > 0.75.
  2. **Buffer size sweep**: Run Split-CIFAR-100 with buffer sizes 500, 2,000, 5,000, 10,000, 20,000 using bottom-k and hybrid (bottom-k + 10% top-k) policies. Expect crossover where hybrid outperforms at larger sizes.
  3. **Ablate selector choice**: Compare bottom-k, mid-k, top-k, and random reservoir on Split-CIFAR-10/100/TinyImageNet with buffer=500. Confirm top-k underperforms and bottom-k/mid-k outperform baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we accurately determine which samples are memorized during incremental training, rather than relying on offline pre-computation?
- Basis in paper: [explicit] The Limitations section states, "We do not solve the problem of determining exactly what samples are memorized during incremental training... developing such a method could shed more light on the role of memorization."
- Why unresolved: The standard Feldman definition causes a "combinatorial explosion" in CL settings (Section 3.3), and the authors' experiments show that offline scores may not perfectly align with what is actually memorized in the representation during incremental training (Section 3.4).
- What evidence would resolve it: A computationally tractable metric that identifies memorized samples dynamically online during the task sequence.

### Open Question 2
- Question: Can we localize specific network components responsible for memorization in continual learning and design targeted regularization to protect them?
- Basis in paper: [explicit] The Conclusion proposes "localization of which parts of the network are responsible for memorization... and designing a proper measure to protect these parts from forgetting in incremental training."
- Why unresolved: Prior literature is conflicting (some suggest last layers, others suggest scattered neurons), and no current method specifically targets these "memorization sub-networks" for protection against catastrophic forgetting.
- What evidence would resolve it: Identification of specific neurons or layers that handle memorization, followed by a regularization technique that preserves these weights better than standard methods (e.g., EWC).

### Open Question 3
- Question: What specific training factors influence memorization behavior in the dynamic context of incremental learning?
- Basis in paper: [explicit] The Conclusion notes, "The factors that could impact memorization in incremental learning are unknown. We believe that studying these factors could also be beneficial."
- Why unresolved: While factors like data augmentation and regularization are known to impact memorization in static training, their interaction with shifting data distributions and plasticity-stability trade-offs in CL is unexplored.
- What evidence would resolve it: A systematic ablation study isolating CL-specific variables (e.g., task order, buffer update frequency) to measure their correlation with memorization scores.

## Limitations

- The paper uses a "reduced ResNet18" architecture without explicit architectural parameters, requiring code inspection for exact dimensions.
- The proxy metric's reliability depends on samples remaining correctly classified after first success, with unclear handling of samples that later become incorrect.
- Results may not generalize to non-image domains or different continual learning paradigms beyond class-incremental settings.

## Confidence

**High Confidence:** The computational efficiency claim of the proxy metric (250x faster than Feldman estimator) and its strong correlation (r=0.81, ρ=0.83) are well-supported by the reported experiments.

**Medium Confidence:** The buffer size-dependent benefit of memorization-aware selection is demonstrated but requires careful interpretation - the crossover point where hybrid selection outperforms bottom-k is dataset-dependent.

**Low Confidence:** The mechanism explaining why high-memorization samples are forgotten faster relies on the assumption that offline memorization scores transfer to incremental settings, which Section 3.4 challenges.

## Next Checks

1. **Reproduce Proxy Correlation:** Train ResNet18 on CIFAR-100 subset; compare training-iteration proxy vs. Feldman estimator on 150 random samples. Target: r > 0.75.

2. **Buffer Size Sweep Validation:** Run Split-CIFAR-100 with buffer sizes 500, 2,000, 5,000, 10,000, 20,000 using bottom-k and hybrid (bottom-k + 10% top-k) policies. Confirm crossover where hybrid outperforms at larger sizes.

3. **Architecture Impact Test:** Replace "reduced ResNet18" with standard ResNet18 (no reduction) and repeat the main Split-CIFAR-10 experiment with buffer=500. Measure performance degradation to assess architectural sensitivity.