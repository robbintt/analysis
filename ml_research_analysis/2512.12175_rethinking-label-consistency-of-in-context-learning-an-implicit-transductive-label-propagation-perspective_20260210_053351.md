---
ver: rpa2
title: 'Rethinking Label Consistency of In-Context Learning: An Implicit Transductive
  Label Propagation Perspective'
arxiv_id: '2512.12175'
source_url: https://arxiv.org/abs/2512.12175
tags:
- label
- demonstrations
- arxiv
- learning
- topk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rethinks in-context learning (ICL) from a transductive
  learning perspective, proposing a label propagation framework based on Bayesian
  inference. The authors argue that existing demonstration selection methods fail
  to ensure label consistency, which is critical for guiding latent concepts and estimating
  propagation errors.
---

# Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective

## Quick Facts
- **arXiv ID**: 2512.12175
- **Source URL**: https://arxiv.org/abs/2512.12175
- **Reference count**: 7
- **Primary result**: Proposes transductive label propagation framework for ICL with TopK-SD demonstration selection

## Executive Summary
This paper rethinks in-context learning (ICL) through a transductive learning lens, arguing that label consistency among demonstrations is crucial for effective concept propagation. The authors propose a Bayesian inference framework where semantic and label embeddings are interpolated to synthesize new data, enabling more consistent demonstration selection via TopK sampling with synthetic data (TopK-SD). Their approach addresses the failure of existing methods to ensure label consistency, which can mislead latent concept learning and error estimation. Experiments across nine datasets and four language models demonstrate an average 1.4% accuracy improvement over standard TopK selection, highlighting the importance of label consistency in ICL mechanisms.

## Method Summary
The authors introduce a transductive label propagation framework that reframes ICL as a semi-supervised learning problem where unlabeled test data benefits from label information in demonstrations. The method involves synthesizing new data points by interpolating semantic and label embeddings from existing demonstrations, then using these synthetic examples to perform TopK sampling that prioritizes demonstrations with consistent labels. This TopK-SD approach ensures that selected demonstrations better guide the propagation of latent concepts while enabling more accurate error estimation during inference. The framework is grounded in Bayesian inference principles and addresses the fundamental limitation of traditional ICL where demonstration selection often lacks label consistency guarantees.

## Key Results
- TopK-SD improves ICL accuracy by 1.4% on average across nine datasets and four language models
- Label consistency among demonstrations is critical for effective latent concept propagation
- The framework provides a mathematical foundation for understanding ICL mechanisms through transductive learning principles
- Standard TopK demonstration selection fails to ensure label consistency, potentially degrading performance

## Why This Works (Mechanism)
The method works by reconceptualizing ICL as an implicit transductive learning process where test samples benefit from label information in demonstrations. By synthesizing data through semantic and label embedding interpolation, the framework creates a more robust basis for selecting demonstrations that maintain label consistency. This consistency ensures that latent concepts are properly guided during propagation, preventing the confusion that arises when inconsistent labels are used as demonstrations. The Bayesian inference foundation allows for principled error estimation in the propagation process, making the entire ICL mechanism more reliable and interpretable.

## Foundational Learning

**Transductive Learning**: Learning paradigm where both labeled and unlabeled data from the test set are used during training. *Why needed*: Provides the theoretical framework for understanding how ICL can leverage test data implicitly. *Quick check*: Verify that the method explicitly uses information about the test distribution during demonstration selection.

**Label