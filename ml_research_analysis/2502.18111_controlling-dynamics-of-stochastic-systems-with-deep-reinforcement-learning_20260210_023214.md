---
ver: rpa2
title: Controlling dynamics of stochastic systems with deep reinforcement learning
arxiv_id: '2502.18111'
source_url: https://arxiv.org/abs/2502.18111
tags:
- control
- learning
- network
- neural
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a deep reinforcement learning-based method
  to control the dynamics of stochastic systems, bridging control theory and machine
  learning. The approach uses trained artificial neural networks to compute transition
  probabilities between local states for individually selected agents in agent-based
  simulations.
---

# Controlling dynamics of stochastic systems with deep reinforcement learning

## Quick Facts
- arXiv ID: 2502.18111
- Source URL: https://arxiv.org/abs/2502.18111
- Reference count: 0
- Primary result: Deep reinforcement learning can modify stochastic system dynamics through neural networks controlling transition probabilities between local states

## Executive Summary
This work introduces a deep reinforcement learning method to control stochastic system dynamics, demonstrating effectiveness on particle coalescence and totally asymmetric exclusion processes (TASEP). The approach uses trained neural networks to compute transition probabilities between local states for individually selected agents in agent-based simulations. For coalescence, networks can accelerate or decelerate particle merging by reshaping transition probabilities, creating emergent effective interactions. For TASEP, networks learn control strategies to maximize particle current in heterogeneous systems, with performance improving when using complex reward structures that encourage lane separation based on particle speeds.

## Method Summary
The method trains a shared feed-forward neural network to map observation states to Q-values, which are converted to transition probabilities via softmax normalization. Q-learning with temporal-difference error backpropagation updates the network during training, using ε-greedy exploration. Two stochastic processes are studied: (1) particle coalescence A+A→A on lattice where networks modify decay from asymptotic t^(-1/2) (1D) or t^(-1)log(t) (2D); (2) heterogeneous TASEP with agents of different speeds where networks maximize global current. Agents are selected randomly for sequential updates, with each receiving a bounded observation patch. Post-training, fixed network parameters generate stochastic policies for production simulations.

## Key Results
- Neural networks can speed up or slow down particle coalescence by reshaping transition probabilities, creating emergent effective interactions
- For TASEP, networks learn to maximize particle current with complex reward structures encouraging lane separation by particle speeds
- Control effectiveness is limited by random agent selection scheme, which prevents immediate environmental reactions
- Larger observation radii provide only marginal improvements due to decreasing Q-value sensitivity to distant features

## Why This Works (Mechanism)

### Mechanism 1: Q-Learning as Controller Parameter Optimization
The Q-learning algorithm serves as an automated parameter-tuning procedure for controllers in stochastic systems. The neural network maps observation states to Q-values, updated via temporal-difference error backpropagation. The TD error estimates future reward gain, reinforcing or discouraging state-action pairs. After training, Q-values are converted to transition probabilities via softmax.

### Mechanism 2: Local Observation State Induces Effective Interactions
Providing agents with local observation states enables the neural network to develop emergent interaction strategies resembling modified particle interactions. Each agent receives a bounded observation patch, and the network learns to associate spatial configurations with actions, producing behavior analogous to long-range repulsive interactions or directional preferences.

### Mechanism 3: Reward Function Design Governs Emergent Strategy
The functional form of immediate rewards determines the control outcome, with compound rewards enabling more sophisticated behaviors. Simple rewards produce straightforward acceleration or deceleration, while compound rewards (forward jump reward + lane-position reward) enable emergent strategies like speed-based lane separation.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: The entire control framework rests on modeling state transitions as MDPs where actions influence next-state probabilities
  - Quick check question: Can you explain why the transition probability $p(s', r | s, a)$ must satisfy $\sum_{s', r} p(s', r | s, a) = 1$ for each state-action pair?

- **Concept: Temporal-Difference Learning and Q-Values**
  - Why needed here: Understanding how TD error propagates reward information backward through time is essential for diagnosing training failures
  - Quick check question: If $\gamma = 0$, what happens to the Q-learning update, and what behavior would you expect from the trained controller?

- **Concept: Softmax Policy Extraction**
  - Why needed here: Converting Q-values to stochastic policies enables post-training simulations that retain exploration while biasing toward learned optimal actions
  - Quick check question: Why use $P(a|s) = \exp(-Q(s,a)) / \sum_a \exp(-Q(s,a))$ rather than deterministically selecting $\arg\max_a Q(s,a)$?

## Architecture Onboarding

- **Component map:** [Agent observation state] → [Feed-forward neural network] → [Q-values per action] → [Softmax normalization] → [Transition probability P(a|s)] → [Action sampling] → [State update]
- **Critical path:** Define observation state encoding → Define action space → Design reward function → Train with ε-greedy exploration → Freeze parameters and run production simulations
- **Design tradeoffs:**
  | Decision | Benefit | Cost |
  |----------|---------|------|
  | Larger observation radius | More information for control | Q-values become less sensitive to distant features |
  | Shared network across agents | Memory efficient, consistent behavior | Cannot specialize per-agent strategies |
  | Random sequential updates | Preserves stochastic realism | Agents cannot react until reselected; limits control effectiveness |
- **Failure signatures:**
  - Training loss oscillates/diverges: Learning rate too high or reward variance too large
  - Trained policy matches untrained: Rewards may be misaligned with objective
  - Control works initially then degrades: Agents trapped by environment changes between updates
  - Policy overfits to specific density: Training episodes didn't cover sufficient state diversity
- **First 3 experiments:**
  1. Benchmark with untrained network: Run simulations with random network parameters to establish baseline dynamics
  2. Simple reward sanity check: Train with r=+1 for coalescence events only; verify particle decay accelerates
  3. Observation radius ablation: Repeat experiment 2 with $R_{obs} \in \{2, 5, 10\}$; confirm larger radii produce faster initial coalescence

## Open Questions the Paper Calls Out
- What connections does emergent behavior in multi-agent smart matter share with winning strategies described by game theory?
- Can reward function structures be systematically designed to achieve long-range control objectives that overcome the limited sensitivity of Q-values to distant observations?
- Would alternative agent selection schemes beyond random sequential updates improve control effectiveness in multi-agent systems?

## Limitations
- Random sequential updates prevent immediate environmental reactions, limiting control effectiveness
- Performance degrades at high agent densities, suggesting poor scalability to congested systems
- Neural network architecture details remain unspecified, preventing precise reproduction

## Confidence
**High confidence**: Core demonstration that deep RL can modify stochastic system dynamics through trained neural networks controlling transition probabilities. The Q-learning mechanism for parameter optimization is well-established.

**Medium confidence**: Claim that local observation states enable emergent interaction strategies resembling modified particle interactions. Works for specific cases but needs broader validation.

**Low confidence**: Assertion that complex reward structures significantly improve performance in TASEP. Promising but not thoroughly validated against simpler reward designs.

## Next Checks
1. **Architecture Sensitivity Analysis**: Systematically vary neural network depth/width and training hyperparameters to quantify their impact on control effectiveness across both coalescence and TASEP systems.

2. **Dense Regime Performance**: Test the trained controllers at agent densities of 70-90% occupancy to identify failure modes and determine scaling limits of the approach.

3. **Reward Function Ablation Study**: Compare performance across different reward formulations (simple vs compound) for TASEP control to rigorously establish whether complex rewards provide measurable benefits over simpler alternatives.