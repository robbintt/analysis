---
ver: rpa2
title: 'Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific
  Machine Learning'
arxiv_id: '2509.08759'
source_url: https://arxiv.org/abs/2509.08759
tags:
- e-03
- e-02
- e-01
- e-06
- fourier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Fourier Learning Machine (FLM), a neural
  network architecture designed to represent multidimensional nonharmonic Fourier
  series using a simple feedforward structure with cosine activation functions. The
  FLM learns frequencies, amplitudes, and phase shifts as trainable parameters, creating
  a problem-specific spectral basis adaptable to both periodic and nonperiodic functions.
---

# Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning

## Quick Facts
- arXiv ID: 2509.08759
- Source URL: https://arxiv.org/abs/2509.08759
- Reference count: 40
- One-line primary result: Introduces a neural network architecture that represents complete multidimensional Fourier series using standard feedforward operations with cosine activation functions

## Executive Summary
The Fourier Learning Machine (FLM) is a neural network architecture designed to represent multidimensional nonharmonic Fourier series using a simple feedforward structure with cosine activation functions. The model learns frequencies, amplitudes, and phase shifts as trainable parameters, creating a problem-specific spectral basis adaptable to both periodic and nonperiodic functions. The architecture is the first to represent a complete, separable Fourier basis in multiple dimensions using a standard Multilayer Perceptron-like structure.

## Method Summary
The FLM consists of N parallel sub-networks, each with 2^(m-1) hidden neurons using cosine activation functions. Each sub-network has learnable frequency vectors n^(k) ∈ R^m initialized on integer lattice points, phase shifts φ^(k)_i, and output weights A^(k)_i. The m-Lexi Sign Matrix S systematically generates all sign combinations for multidimensional inputs, enabling the network to span all separable basis functions through phase-shifted cosines. For PDE problems, the architecture uses physics-informed loss functions with automatic differentiation to compute required derivatives, training with Adam optimizer while searching hyperparameters including learning rate, betas, and number of sub-networks.

## Key Results
- Achieves MSE of 6.24×10^-8 for 1D Heat equation, 1.34×10^-7 for 2D Poisson equation, and 3.80×10^-7 for Generalized Black-Scholes equation
- Outperforms SIREN and vanilla feedforward architectures on benchmark PDE problems
- Successfully solves Rock-Paper-Scissors optimal control problem with MAPE < 1% on generalization to varying initial conditions

## Why This Works (Mechanism)

### Mechanism 1: Cosine Phase-Shifted Basis Enables MLP-Compatible Spectral Representation
The FLM represents complete multidimensional Fourier series using standard feedforward operations by converting separable trigonometric bases into cosine phase-shifted form. Rather than using products of sines and cosines (which require multiplicative nodes), each frequency vector generates 2^(m-1) phase-shifted cosines via the m-Lexi Sign Matrix, implicitly representing all 2^m separable basis functions through additive weighted sums followed by cosine activations. This transformation assumes the target function admits a finite Fourier series approximation.

### Mechanism 2: Learnable Frequencies Adapt Spectral Basis to Nonperiodic Functions
Relaxing integer frequency constraints enables the network to discover task-specific spectral bases for both periodic and nonperiodic targets. Frequencies are initialized on integer lattice points but optimized as continuous real-valued parameters, transforming classical Fourier series into nonharmonic series that adapt to the target's spectral characteristics. This approach assumes the frequency parameter optimization landscape is sufficiently smooth to permit gradient-based learning without pathological local minima.

### Mechanism 3: Physics-Informed Loss with Automatic Differentiation for PDE Solutions
Encoding PDE constraints as soft penalties in the loss function enables mesh-free solution learning via automatic differentiation. Collocation points sample the domain; automatic differentiation computes required partial derivatives; the composite loss penalizes PDE residual, initial condition, and boundary condition violations simultaneously. The FLM's spectral structure provides smooth derivatives suitable for this approach, assuming the solution lies within the network's function class and collocation points adequately cover the constraint manifold.

## Foundational Learning

- **Concept: Fourier Series (classical and nonharmonic)** - Why needed here: The FLM is explicitly designed as a learnable nonharmonic Fourier series approximator. Understanding the transition from integer to real-valued frequencies is essential for interpreting what the network learns. Quick check question: Can you explain why a nonharmonic Fourier series can approximate nonperiodic functions on finite intervals while a classical Fourier series cannot?

- **Concept: Phase-Shifted Trigonometric Identities** - Why needed here: The architecture relies on the equivalence between A·cos(ωt - φ) and a·cos(ωt) + b·sin(ωt). The m-Lexi Sign Matrix systematically generates all sign combinations for multidimensional inputs. Quick check question: Given A·cos(n₁x₁ + n₂x₂ - φ), can you expand it into separable products of cos(n₁x₁), sin(n₁x₁), cos(n₂x₂), sin(n₂x₂)?

- **Concept: Automatic Differentiation in Neural Networks** - Why needed here: PDE solutions require computing ∂u/∂t, ∂²u/∂x², etc. These are obtained via autodiff through the network, not numerical finite differences. Quick check question: If a network outputs u(x,t) = Σᵢ Aᵢcos(nᵢ·x - φᵢ), how would you compute ∂u/∂x using backpropagation-style gradient computation?

## Architecture Onboarding

- **Component map**: Input layer -> N parallel sub-networks -> Output aggregation
- **Critical path**: 
  1. Initialize frequency vectors on low integer lattice
  2. Construct S matrix for your dimension m
  3. For each sub-network k: compute pre-activations, apply cos, weight by A^(k), sum
  4. For PDE problems: define composite loss with automatic differentiation
  5. Train with ADAM optimizer; monitor frequency ranges

- **Design tradeoffs**:
  - **N (sub-networks) vs. accuracy**: More sub-networks = more frequency components = better approximation but more parameters
  - **Initialization strategy**: Integer lattice initialization provides stable starting point; random frequency initialization may lead to convergence issues
  - **Frequency constraint relaxation**: Paper initializes in R^m_≥0 but allows training to move frequencies negative—symmetry handling via cosine's even property

- **Failure signatures**:
  - **Frequency explosion**: Learned frequencies grow unboundedly → overfitting to noise
  - **Zero amplitudes**: Sub-networks learn A ≈ 0 → wasted capacity
  - **Slow convergence on PDE residuals**: L_PDE not decreasing → check collocation point coverage
  - **Poor generalization on OCP varying initial conditions**: Network overfits training U_0

- **First 3 experiments**:
  1. **1D function approximation**: Implement FLM with m=1, N=5. Target f(x) = sin(πx) on [0,1]. Verify learned frequencies cluster near {π} and MSE < 10^-6
  2. **2D Poisson equation replication**: Replicate Example 5.2 with N=25 sub-networks, learning rate 0.001, betas (0.95, 0.97). Target MSE < 10^-7
  3. **Frequency initialization ablation**: For Heat equation, compare three initializations: (a) sequential integer lattice, (b) random frequencies in [0,5], (c) frequencies from FFT of target solution

## Open Questions the Paper Calls Out
None

## Limitations
- **Underspecified collocation point counts**: Exact number of interior, boundary, and initial condition points used for training is not specified
- **Frequency implementation ambiguity**: While initialization is specified, preventing frequencies from becoming negative requires careful implementation not fully detailed
- **Limited baseline comparison**: Direct comparison conditions and baseline architectures are not fully detailed, making performance claims medium confidence

## Confidence
- **High**: Core Fourier series representation theory and m-Lexi Sign Matrix construction
- **Medium**: PDE solution accuracy claims and comparison to SIREN
- **Medium**: OCP generalization performance metrics

## Next Checks
1. Implement and validate the m-Lexi Sign Matrix construction for 2D and 3D cases to ensure correct sign pattern generation across all separable basis functions
2. Conduct sensitivity analysis on frequency initialization - compare sequential integer lattice vs. random initialization vs. FFT-based initialization for convergence speed and final accuracy
3. Replicate the Poisson equation experiment with varying collocation point densities to determine minimum sampling requirements for achieving target MSE < 10^-7