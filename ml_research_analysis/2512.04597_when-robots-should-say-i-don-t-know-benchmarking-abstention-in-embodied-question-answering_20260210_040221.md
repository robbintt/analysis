---
ver: rpa2
title: 'When Robots Should Say "I Don''t Know": Benchmarking Abstention in Embodied
  Question Answering'
arxiv_id: '2512.04597'
source_url: https://arxiv.org/abs/2512.04597
tags:
- abstention
- answer
- embodied
- question
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AbstainEQA, a benchmark for evaluating embodied
  agents' ability to recognize and abstain from answering ambiguous or unanswerable
  questions in Embodied Question Answering (EQA). The authors first conduct a large-scale
  survey of 500 human queries, finding that 32.4% require abstention due to missing
  or underspecified context.
---

# When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering

## Quick Facts
- arXiv ID: 2512.04597
- Source URL: https://arxiv.org/abs/2512.04597
- Reference count: 40
- Primary result: Even the best VLMs achieve only 42.79% abstention recall versus human 91.17% in recognizing unanswerable embodied questions

## Executive Summary
This paper introduces AbstainEQA, a benchmark for evaluating embodied agents' ability to recognize and abstain from answering ambiguous or unanswerable questions in Embodied Question Answering (EQA). The authors first conduct a large-scale survey of 500 human queries, finding that 32.4% require abstention due to missing or underspecified context. Drawing on cognitive theories of human communication errors, they define five abstention types and create a benchmark with 1,636 abstention cases paired with answerable cases from OpenEQA.

Evaluation shows that even the best frontier model (Gemini-2.5-Pro) achieves only 42.79% abstention recall, compared to human performance of 91.17%. The paper reveals that models often rely on textual cues rather than visual evidence, leading to unreliable performance when images are randomized. The results indicate that reliable abstention remains a fundamental challenge for embodied AI and is essential for safe real-world human-robot interaction.

## Method Summary
The paper creates AbstainEQA benchmark by conducting a survey of 500 human queries on embodied question-answering, identifying 32.4% as requiring abstention. They derive five abstention categories based on cognitive theories of human error and pair 1,636 abstention cases with 1,636 answerable cases from OpenEQA, with human-annotated evidence frames. The benchmark evaluates VLMs across two settings: Episodic-Memory EQA (EM-EQA) with fixed observations and Active EQA (A-EQA) with navigation. They test multiple models (Gemini-2.5-Pro, GPT-4o, etc.) using coarse/fine prompting strategies and conduct SFT experiments with Qwen2.5-VL-7B.

## Key Results
- Gemini-2.5-Pro achieves only 42.79% abstention recall versus human 91.17% on the AbstainEQA benchmark
- Fine-tuning yields minimal improvements (only 0.57% absolute gain in abstention F1-score) and models often rely on textual cues rather than visual evidence
- Ambiguity causes inefficient exploration in A-EQA, increasing total frames and snapshots while reducing success rates
- Models frequently fail to ground abstention decisions in visual evidence, achieving similar performance when images are randomized

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Taxonomy for Abstention
Categorizing ambiguous queries into distinct failure modes (Actionability Limitation, False Presupposition, etc.) allows agents to apply specific reasoning rules for when to withhold answers rather than using generic uncertainty. This operationalizes Norman's cognitive framework to map agent limitations to systematic abstention categories.

### Mechanism 2: Evidence-Grounded Detection (vs. Textual Bias)
Reliable abstention requires cross-modal verification where visual evidence dictates the response, counteracting LLMs' tendency to rely on linguistic shortcuts. The paper demonstrates that models achieve high abstention scores by detecting textual patterns rather than checking visual frames, highlighting the need for grounding decisions in "Cause Frames."

### Mechanism 3: Navigation Policy Stability under Uncertainty
Abstention acts as a regularization signal for navigation; without it, ambiguous queries cause policy instability, manifesting as oscillation between over-exploration and premature termination. Recognizing ambiguity allows the policy to terminate cleanly rather than executing wandering maneuvers.

## Foundational Learning

- **Cross-Modal Grounding**: Needed because current VLMs often "solve" EQA by memorizing text associations rather than "seeing." Quick check: Does the model change its answer when the image changes but the text remains the same?

- **Selective Prediction (Abstention)**: Standard EQA benchmarks assume all questions are answerable, but this paper introduces the "rejection option." Quick check: Can the model distinguish between "I don't know because I haven't looked" vs. "I don't know because the question is subjective"?

- **Episodic Memory vs. Active Exploration**: The paper evaluates two settings with different abstention mechanisms. Quick check: In Active EQA, should the agent stop exploring immediately if it detects a False Presupposition?

## Architecture Onboarding

- **Component map**: Egocentric Video Frames (Ï„) + Natural Language Query (q) -> VLM Backbone -> Decision Head -> {Answer} or {Abstention, Reason}; Navigator (A-EQA) takes VLM embeddings to generate actions

- **Critical path**: The "Abstention Detection" logic must sit between the VLM Encoder and the Response Generator, acting as a guardrail that blocks the path to the Response Generator if visual evidence is missing

- **Design tradeoffs**: Textual Bias vs. Visual Grounding (fine-tuning on text alone yields high abstention metrics but fails when visual inputs are randomized); Coarse vs. Fine Prompts (coarse increases recall but kills precision)

- **Failure signatures**: Hallucination Trap (agent answers when it hasn't seen evidence); Coward Trap (agent refuses to answer when chair is clearly visible); Textual Shortcutting (agent always refuses questions starting with "Who")

- **First 3 experiments**: 1) Baseline Calibration: Run Gemini-2.5-Pro on validation set, measure gap between Abstention Recall and Response Accuracy; 2) Visual Dependency Test: Randomize image inputs, if abstention performance doesn't drop model is cheating using text; 3) Navigation Efficiency Analysis: In A-EQA, measure Path Length distribution for answerable vs. ambiguous questions

## Open Questions the Paper Calls Out

### Open Question 1
What training paradigms can explicitly couple model abstention decisions with visual evidence retrieval rather than linguistic patterns? Unresolved because SFT models achieve high abstention metrics but perform identically when visual inputs are randomized or removed entirely, showing they learn textual shortcuts rather than genuine multimodal reasoning.

### Open Question 2
How can navigation policies incorporate uncertainty-type awareness to calibrate exploration versus termination? Unresolved because current agents "lack calibrated exploration policies under uncertainty," oscillating between under- and over-searching rather than modulating behavior based on abstention cause.

### Open Question 3
Can constructed datasets with identical questions across varied visual scenes effectively mitigate textual shortcut learning? Unresolved because current SFT models give identical abstention responses across scenes with different visual evidence, revealing decision boundaries dominated by question wording.

## Limitations
- Results are based on specific VLMs and may not generalize to other architectures or smaller models
- The 500-question survey may not capture all forms of ambiguity encountered in real-world robotics deployment
- Human evaluation subjectivity: abstention decisions, while expert-validated, still involve subjective judgment calls

## Confidence

**High Confidence:** Core taxonomy of five abstention types, basic benchmark methodology, and overall finding that current models struggle with reliable abstention

**Medium Confidence:** Specific performance numbers (42.79% recall), efficiency degradation patterns, and relative model rankings

**Medium Confidence:** Textual bias findings, though this requires careful interpretation of the randomized image ablation test

## Next Checks
1. **Generalization test:** Evaluate models on a held-out set of human queries not used in the original survey to assess taxonomy coverage
2. **Real-world deployment simulation:** Test abstention performance in more complex, dynamic environments with partial observability and noisy sensors
3. **Cross-model validation:** Replicate key findings with additional VLMs (including smaller, more deployable models) to verify that textual bias and performance gaps persist across architectures