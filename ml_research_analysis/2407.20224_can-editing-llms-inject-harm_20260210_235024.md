---
ver: rpa2
title: Can Editing LLMs Inject Harm?
arxiv_id: '2407.20224'
source_url: https://arxiv.org/abs/2407.20224
tags:
- editing
- llms
- evaluation
- injection
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether knowledge editing can be exploited
  to inject harmful information into LLMs, a threat they call "Editing Attack." They
  construct a dataset EditAttack to study two risks: misinformation injection and
  bias injection. Their findings show that editing attacks can successfully inject
  both commonsense and long-tail misinformation into LLMs with high effectiveness,
  and that commonsense misinformation injection achieves particularly high effectiveness.'
---

# Can Editing LLMs Inject Harm?

## Quick Facts
- arXiv ID: 2407.20224
- Source URL: https://arxiv.org/abs/2407.20224
- Reference count: 34
- Primary result: Editing attacks can successfully inject harmful misinformation and biases into LLMs with high effectiveness and stealthiness

## Executive Summary
This paper investigates whether knowledge editing techniques can be exploited to inject harmful information into large language models, introducing a threat they term "Editing Attack." The authors construct a dataset called EditAttack to systematically study two risks: misinformation injection and bias injection. Their comprehensive experiments demonstrate that editing attacks can successfully inject both commonsense and long-tail misinformation into LLMs with high effectiveness. They discover that biased sentences can be injected with high stealthiness, and critically, that a single biased sentence injection can degrade overall fairness across multiple unrelated bias dimensions. The attacks show remarkable stealthiness, having minimal impact on the models' general knowledge and reasoning capabilities.

## Method Summary
The authors construct a dataset called EditAttack to study two types of harmful content injection: misinformation and bias. They investigate three representative knowledge editing methods: ROME (Locate-then-Edit), Fine-Tuning (FT), and In-Context Editing (ICE). For each editing attack, they evaluate effectiveness (whether harmful content is injected), generalization (whether it affects outputs beyond the specific edited instance), and stealthiness (whether general capabilities are degraded). The evaluation framework measures effectiveness through task-specific accuracy metrics, generalization through multi-hop inference tasks, and stealthiness through general knowledge and reasoning benchmarks. They systematically test both commonsense and long-tail misinformation, as well as biased content across multiple dimensions.

## Key Results
- Editing attacks successfully inject both commonsense and long-tail misinformation into LLMs with high effectiveness
- One single biased sentence injection can degrade overall fairness across multiple unrelated bias dimensions
- Attacks demonstrate high stealthiness, with minimal impact on LLMs' general knowledge and reasoning capabilities
- Commonsense misinformation injection achieves particularly high effectiveness compared to long-tail misinformation

## Why This Works (Mechanism)
Knowledge editing techniques modify specific internal representations in LLMs to update or correct factual knowledge. These methods typically work by either updating specific weight matrices (like MLP layers in ROME), fine-tuning on specific examples, or using in-context prompts to override default behaviors. The underlying assumption is that these techniques can precisely target and modify only the intended knowledge without affecting unrelated capabilities. However, this paper demonstrates that these editing operations can be exploited to inject harmful content, and that the interconnected nature of LLM representations allows localized edits to have broader impacts on model behavior, particularly in fairness metrics where seemingly unrelated dimensions show degradation.

## Foundational Learning
- Knowledge Editing Methods: Techniques for modifying LLMs' internal representations without full fine-tuning (why needed: to understand the attack surface; quick check: compare ROME, FT, and ICE mechanisms)
- Bias Measurement in LLMs: Quantitative metrics for assessing demographic biases across different dimensions (why needed: to evaluate fairness degradation; quick check: verify consistency across multiple bias benchmarks)
- Stealthiness Evaluation: Methods for detecting unintended side effects of model modifications (why needed: to assess whether attacks can evade detection; quick check: confirm minimal impact on general knowledge tasks)
- Effectiveness Metrics: Task-specific accuracy measures for evaluating successful injection of harmful content (why needed: to quantify attack success; quick check: validate against multiple misinformation types)
- Generalization Assessment: Testing whether injected content affects outputs beyond the specific edited instance (why needed: to understand attack scope; quick check: evaluate multi-hop inference performance)

## Architecture Onboarding

Component Map:
Knowledge Editing Methods (ROME, FT, ICE) -> EditAttack Dataset -> Effectiveness Evaluation -> Generalization Testing -> Stealthiness Assessment

Critical Path:
Editing Attack → Knowledge Modification → Output Generation → Bias/Fairness Metric Computation

Design Tradeoffs:
Precision vs. Coverage: More precise editing methods may be easier to defend against but harder to detect; broader editing methods may cause more collateral damage but be more detectable.

Failure Signatures:
Unexpected fairness degradation across unrelated bias dimensions; successful misinformation injection that persists across multiple query types; minimal performance degradation on general knowledge tasks despite harmful content injection.

First 3 Experiments:
1. Test effectiveness of misinformation injection across different knowledge editing methods using EditAttack dataset
2. Measure fairness degradation across multiple bias dimensions after single biased sentence injection
3. Evaluate stealthiness by comparing general knowledge and reasoning performance before and after editing attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific defense mechanisms can be developed to detect or neutralize editing attacks without compromising the model's general knowledge or legitimate editing capabilities?
- Basis in paper: [explicit] The conclusion states, "Looking ahead, we call for more future research on developing potential defense methods based on the inner mechanisms of editing and enhancing LLMs’ intrinsic robustness against editing attacks."
- Why unresolved: This study focused entirely on characterizing the attack surface (effectiveness, generalization, and stealthiness) rather than proposing or testing mitigation strategies.
- What evidence would resolve it: A proposed defense framework or detection algorithm that successfully differentiates between malicious "Editing Attacks" and benign "Hallucination Correction" or normal usage with high accuracy.

### Open Question 2
- Question: Through what internal mechanisms does a single, localized biased sentence injection cause a global degradation in fairness across unrelated bias dimensions (e.g., increased race bias from a gender bias injection)?
- Basis in paper: [inferred] The paper observes the counter-intuitive finding that "one single biased sentence injection can cause a bias increase in general outputs... highly unrelated to the injected biased sentence," but offers no mechanistic explanation for this "catastrophic" spillover effect.
- Why unresolved: The paper establishes the *existence* of the fairness degradation but does not analyze the representational changes (e.g., shared latent spaces) that link a specific edit to general fairness metrics.
- What evidence would resolve it: Layer-wise or neuron-level analysis identifying how specific edits alter the model's internal representations of demographic concepts beyond the immediate subject of the edit.

### Open Question 3
- Question: Do advanced knowledge editing algorithms (e.g., MEMIT, MEND, meta-learning approaches) exhibit higher or lower susceptibility to editing attacks compared to the "Locate-then-Edit" and Fine-Tuning methods tested?
- Basis in paper: [inferred] The authors limit their investigation to "three representative knowledge editing methods" (ROME, FT, and ICE) and acknowledge they represent only specific subsets of editing techniques.
- Why unresolved: The vulnerability surface may vary significantly depending on whether the editing method modifies MLP layers (ROME), optimizes via gradients (FT), or uses in-context prompting (ICE).
- What evidence would resolve it: Comparative experimental results on the EditAttack dataset using a wider range of editing algorithms to determine if the high effectiveness of attacks is consistent across all editing paradigms.

## Limitations
- The study's findings may not generalize across all LLM architectures beyond those tested
- The evaluation focuses on specific types of harmful content and may not capture the full spectrum of potential editing attack vectors
- The stealthiness evaluation may not fully account for subtle degradations in reasoning capabilities that could emerge in different contexts or over time

## Confidence
- High confidence in the claim that editing attacks can successfully inject both commonsense and long-tail misinformation with high effectiveness
- High confidence in the finding that one single biased sentence injection can degrade overall fairness across multiple bias dimensions
- Medium confidence in the claim about high stealthiness of these attacks due to potential limitations in evaluation criteria

## Next Checks
1. Test the transferability of editing attacks across a broader range of LLM architectures and sizes to assess generalizability
2. Evaluate the persistence and evolution of injected misinformation and biases over extended periods and through additional fine-tuning
3. Develop and test potential defensive mechanisms against editing attacks to understand their effectiveness and limitations