---
ver: rpa2
title: 'Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning'
arxiv_id: '2505.21067'
source_url: https://arxiv.org/abs/2505.21067
tags:
- zero-rl
- distilled
- reasoning
- tokens
- behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether a simple distillation method can
  outperform complex zero-RL methods for improving reasoning in smaller language models.
  The authors use Qwen2.5-32B as a base model and compare two approaches: zero-RL
  using tens of thousands of carefully selected prompts versus distillation using
  only 920 examples generated by DeepSeek R1.'
---

# Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning

## Quick Facts
- arXiv ID: 2505.21067
- Source URL: https://arxiv.org/abs/2505.21067
- Reference count: 40
- Primary result: Distillation using 920 examples outperforms zero-RL with 10k+ prompts on challenging reasoning benchmarks

## Executive Summary
This paper challenges the assumption that reinforcement learning is necessary for improving reasoning in smaller language models. The authors demonstrate that simple supervised fine-tuning (SFT) from a reasoning-capable teacher model can outperform complex zero-RL methods on difficult reasoning tasks, despite using far fewer examples and less computational resources. Through linguistic analysis of model outputs, they discover that distillation transfers specific cognitive behaviors—Multi-Perspective Thinking and Metacognitive Awareness—that enable more flexible reasoning patterns essential for solving complex problems. These behaviors manifest through increased use of anthropomorphic tokens and logical connectors, which serve as reasoning scaffolds for exploration and strategy shifts.

## Method Summary
The study compares distillation versus zero-RL for enhancing reasoning in Qwen2.5-32B. For distillation, 920 AIME problems (1983-2023) are paired with DeepSeek R1-generated responses and used for SFT (5 epochs, lr=1e-5, 16k context). Zero-RL baselines include DAPO, ORZ, and SimpleRL trained on 8k-57k prompts with thousands of steps. Both approaches are evaluated on AIME2024, AIME2025, HMMT, GPQA Diamond, and MATH500 using temperature=1, top_p=0.95, max_length=32768. The authors perform token frequency analysis and cognitive behavior classification to understand why distillation succeeds where zero-RL fails.

## Key Results
- Distilled model achieves 38.3% average on AIME/HMMT vs 29.7-31.3% for best zero-RL models
- Uses only 920 examples versus 10k-57k prompts for zero-RL
- Requires <24 GPU-hours versus >48 hours for zero-RL training
- Distillation transfers anthropomorphic tokens and logical connectors that enable flexible reasoning
- Token-banning experiments show 10.9-14.9 point drops on AIME when distinctive tokens are restricted

## Why This Works (Mechanism)

### Mechanism 1
Distillation transfers flexible cognitive behaviors that zero-RL fails to induce in smaller models. Teacher model outputs contain linguistic markers that seed Multi-Perspective Thinking and Metacognitive Awareness. SFT directly copies these reasoning patterns, whereas zero-RL's gradient signals from sparse rewards cannot efficiently discover and reinforce these low-frequency behaviors before policy entropy collapses.

### Mechanism 2
Linguistic markers serve as reasoning scaffolds enabling more effective search during generation. Anthropomorphic tokens ("wait," "perhaps") and logical connectors ("alternatively," "but") act as decision points for branching, backtracking, or shifting strategies. Banning these tokens forces rigid reasoning paths, degrading performance—yet the model compensates, indicating internalized patterns beyond surface tokens.

### Mechanism 3
Zero-RL on smaller models overfits to reward signals without discovering transferable reasoning strategies. With limited exploration capacity (low policy entropy), RL optimizes for immediate reward patterns—e.g., predicting integer answers or early termination—rather than generalizable reasoning behaviors, manifesting as reward hacking.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) from Reasoning Traces**
  - Why needed: Understanding how token-level supervision transfers high-level reasoning patterns is essential before implementing distillation pipelines
  - Quick check: Can you explain why imitating teacher outputs might transfer cognitive behaviors better than reward optimization?

- **Policy Entropy in Reinforcement Learning**
  - Why needed: Critical for understanding why zero-RL fails to discover rare but valuable reasoning behaviors before entropy collapse
  - Quick check: What happens to exploration when policy entropy collapses, and why does this matter for discovering new reasoning strategies?

- **Chain-of-Thought (CoT) Reasoning Patterns**
  - Why needed: The paper's analysis builds on distinguishing rigid step-by-step vs. flexible exploratory reasoning through linguistic markers
  - Quick check: What token categories distinguish flexible CoT from rigid procedural reasoning?

## Architecture Onboarding

- **Component map:**
  - Base Model: Qwen2.5-32B (pretrained, no instruction tuning)
  - Teacher Model: DeepSeek R1 (generates distillation responses)
  - Distillation Pipeline: 920 AIME problems → R1 responses → SFT (5 epochs, lr=1e-5, 16k context)
  - Zero-RL Baselines: DAPO, ORZ, SimpleRL (8k-57k prompts, thousands of steps)
  - Token Analysis: Frequency counting across anthropomorphic, logical connectors, math reasoning categories
  - Cognitive Behavior Detection: GPT-4o classification of Multi-Perspective Thinking and Metacognitive Awareness

- **Critical path:**
  1. Construct distillation dataset (prompt + teacher response pairs, no correctness filtering)
  2. Run SFT with Qwen2.5-Math prompt template
  3. Evaluate on reasoning benchmarks (temp=1, top_p=0.95, 32k max length)
  4. Perform token frequency analysis comparing distilled, zero-RL, and base models
  5. Validate with token-banning experiments and cognitive behavior counting

- **Design tradeoffs:**
  - Data quantity vs. quality: 920 unfiltered examples outperform 50k+ RL prompts—teacher reasoning style is the critical factor
  - Training cost: Distillation (<24 GPU-hours) vs. zero-RL (>48 hours with 4x GPUs)
  - Generalization: Distillation trained only on math but improves general benchmarks; some zero-RL models show domain-specific overfitting

- **Failure signatures:**
  - Rigid step-by-step outputs without hesitation or strategy shifts
  - Abrupt reasoning termination near training max length (reward hacking)
  - Format overfitting (always outputting integers regardless of question type)
  - Token analysis shows anthropomorphic/connectors near zero

- **First 3 experiments:**
  1. Reproduce distillation: Fine-tune Qwen2.5-32B-base on 920 AIME+R1 pairs; evaluate on AIME2024/HMMT; measure response length and token frequencies.
  2. Run token-banning ablation: Prevent "wait," "perhaps," "alternatively," "but"; compare performance drop across difficulty levels.
  3. Compare cognitive behavior frequencies: Use GPT-4o to classify behaviors in distilled vs. zero-RL outputs; correlate with benchmark scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the efficacy of distillation-induced cognitive behaviors scale with model size, specifically for models significantly larger (e.g., 70B) or smaller than 32B?
- Basis in paper: The authors explicitly state in the Limitations section that future work should extend the investigation to medium-sized models (such as 70B) and smaller models below 32B.
- Why unresolved: The study restricts its empirical analysis to the Qwen2.5-32B model, leaving the generalizability of these findings across different parameter scales unconfirmed.
- What evidence would resolve it: Replicating the distillation vs. zero-RL comparison on Qwen2.5-72B and Qwen2.5-7B, measuring the frequency of Multi-Perspective Thinking and Metacognitive Awareness.

### Open Question 2
- Question: Can a distilled model serve as a more effective initialization point for subsequent reinforcement learning scaling than a raw base model?
- Basis in paper: Section 5 (Discussion) posits that distilled models are likely better suited for subsequent RL because distillation introduces diverse reasoning paths that help RL extract richer feedback signals.
- Why unresolved: The paper compares distillation and zero-RL as separate endpoints but does not experimentally validate the proposed "distillation-then-RL" pipeline.
- What evidence would resolve it: Running a zero-RL experiment initialized from the distilled 32B checkpoint and comparing the learning curve, stability, and final performance against a base model initialized with zero-RL.

### Open Question 3
- Question: Why does zero-RL facilitate self-reflection and flexible reasoning in very large models (e.g., DeepSeek-V3-Base 671B) but fails to induce similar behaviors in smaller models?
- Basis in paper: Appendix E notes the contrasting behavior between large and small models under zero-RL and states that determining the exact reasons—whether base model capacity or training design—is beyond the scope of the paper.
- Why unresolved: It is unclear if the "rigid" reasoning in small zero-RL models is due to a lack of inherent cognitive behaviors in the base model or premature entropy collapse during training.
- What evidence would resolve it: Analyzing the token probability entropy and the presence of reflective keywords in the raw base models of varying sizes prior to RL training.

## Limitations

- The causal link between token patterns and reasoning quality is inferred but not experimentally proven
- Comparison assumes all zero-RL models are equally well-tuned without standardized hyperparameter sweeps
- Base model's reasoning capacity is assumed to be fixed, but lack of instruction tuning may bottleneck both methods

## Confidence

- **High confidence:** The empirical observation that 920 distillation examples outperform tens of thousands of zero-RL prompts is directly measurable and reproducible
- **Medium confidence:** The mechanism linking anthropomorphic tokens and logical connectors to flexible reasoning is plausible but requires more controlled experiments
- **Low confidence:** The claim that zero-RL fundamentally cannot discover these behaviors at smaller scales is based on observed failures but hasn't been tested across all variants

## Next Checks

1. **Token pattern ablation with synthetic controls:** Generate synthetic responses that mimic the frequency of anthropomorphic tokens and logical connectors but lack actual reasoning structure. Compare performance to the distilled model to determine if token patterns alone drive the improvement.

2. **Zero-RL hyperparameter sweep:** Systematically test all zero-RL methods across a wide range of entropy regularization strengths, learning rates, and curriculum learning schedules to establish whether the distillation advantage persists under optimal RL configurations.

3. **Cross-domain generalization test:** Train distilled and zero-RL models on AIME and evaluate on completely different reasoning tasks (e.g., code generation, commonsense reasoning) to determine if the cognitive behaviors transferred are truly domain-general or AIME-specific patterns.