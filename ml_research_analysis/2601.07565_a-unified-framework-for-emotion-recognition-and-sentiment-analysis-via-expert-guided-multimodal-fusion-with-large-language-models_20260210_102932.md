---
ver: rpa2
title: A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided
  Multimodal Fusion with Large Language Models
arxiv_id: '2601.07565'
source_url: https://arxiv.org/abs/2601.07565
tags:
- multimodal
- emotion
- recognition
- framework
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EGMF, a unified framework for multimodal emotion
  recognition and sentiment analysis that combines expert-guided multimodal fusion
  with large language models. The method addresses the challenge of integrating heterogeneous
  text, audio, and visual modalities for both discrete emotion classification and
  continuous sentiment regression tasks.
---

# A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models

## Quick Facts
- arXiv ID: 2601.07565
- Source URL: https://arxiv.org/abs/2601.07565
- Reference count: 35
- Primary result: 87.09% F1 on MOSEI, 73.90% WF1 on CHERMA, 87.30% F1 on MELD, 82.43% F1 on SIMS-V2 with unified multimodal fusion and LLM integration

## Executive Summary
This paper introduces EGMF, a unified framework that combines expert-guided multimodal fusion with large language models (LLMs) for emotion recognition and sentiment analysis across text, audio, and visual modalities. The method employs a hierarchical dynamic gating mechanism that adaptively selects among three specialized expert networks (fine-grained local, semantic correlation, and global context) to produce context-aware multimodal representations. These representations are integrated with frozen LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework to handle both classification and regression tasks through parameter-efficient LoRA fine-tuning. Experiments on bilingual benchmarks demonstrate state-of-the-art performance with strong cross-lingual robustness, revealing universal patterns in multimodal emotional expressions across English and Chinese.

## Method Summary
EGMF processes multimodal inputs through four modules: (1) an AudioVisualEncoder extracts features from audio and visual data while text uses LLM embeddings; (2) cross-modal attention fuses these representations with text as the primary semantic carrier; (3) three expert networks with hierarchical dynamic gating adaptively enhance the fused features based on context; (4) enhanced representations are projected as pseudo tokens and integrated with frozen LLMs (GLM3-6B) via prompt-based conditioning using LoRA for efficient fine-tuning. The framework handles both discrete emotion classification and continuous sentiment regression through a unified generative approach.

## Key Results
- Achieves 87.09% F1 score on MOSEI multimodal sentiment analysis dataset
- Attains 73.90% weighted F1 on CHERMA emotion recognition classification benchmark
- Reaches 87.30% F1 on MELD emotion recognition dataset
- Scores 82.43% F1 on SIMS-V2 multimodal sentiment analysis dataset

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical dynamic gating enables context-aware selection among specialized expert networks, improving multimodal representation expressiveness over static fusion. The two-stage weighting process uses feature-driven weights `w_i = GateNetwork(f_fusion_i)` to generate initial expert preferences, then refines with context-aware coefficients `α_i = Softmax(MLP(Concat(f_fusion_i, w_i)))`. The final enhanced representation combines weighted expert outputs with residual connection: `f_enhanced = Σ α_i,k · E_k(f_fusion_i) + β_i · f_fusion_i`. This mechanism assumes different emotional contexts benefit from different feature processing scales, and learned gating can identify which expert(s) to prioritize per input.

### Mechanism 2
Pseudo token injection enables multimodal feature integration into frozen LLMs without architectural modification, preserving generative reasoning capabilities. Enhanced multimodal representations are projected to pseudo tokens `T_pseudo = Repeat(Linear_proj(f_enhanced), n_tokens)`, then wrapped with task-specific prompts: `I_wrapped = [E_prefix; T_pseudo; E_suffix; P_task]`. The LLM processes this unified sequence via `P(y|I_wrapped) = LLM(I_wrapped; θ_frozen, θ_LoRA)`, generating emotion labels or sentiment scores. This mechanism assumes LLMs can interpret pseudo tokens as meaningful semantic representations when properly conditioned.

### Mechanism 3
Text-centric cross-modal attention captures inter-modal dependencies while preserving textual semantic primacy, with audio-visual modalities providing complementary signals. Audio-visual features concatenated as `H_av ∈ R^(2×d_h)` attend to text via bidirectional cross-attention: `Z_cross = CrossAttention(H_text, H_av, H_av) + H_text`. Self-attention refines: `Z_self = SelfAttention(Z_cross) + Z_cross`. Global pooling produces fused representation. This mechanism assumes text carries primary semantic content for emotion understanding, while audio-visual cues provide supplementary paralinguistic and visual context.

## Foundational Learning

- **Mixture of Experts (MoE) with Gating Networks**: Essential for understanding how expert selection works and debugging why certain experts activate for specific emotional contexts. Quick check: Can you explain why the gating network takes `f_fusion` as input rather than raw multimodal features?

- **LoRA (Low-Rank Adaptation)**: Critical for understanding the parameter-efficient LLM fine-tuning used in this framework. Quick check: What happens to trainable parameter count when LoRA rank increases from 8 to 32?

- **Cross-Attention for Multimodal Fusion**: The primary fusion mechanism before expert processing; understanding query-key-value roles helps diagnose modality alignment issues. Quick check: If you swapped text and audio-visual roles (audio-visual as query), what semantic assumption would change?

## Architecture Onboarding

- **Component map**: Raw multimodal input → Cross-modal attention (Eq. 1-2) → Fused representation `f_fusion` → Expert processing with gating (Eq. 3) → Enhanced representation `f_enhanced` → Pseudo tokens → LLM generation

- **Critical path**: Raw multimodal input → Cross-modal attention (Eq. 1-2) → Fused representation `f_fusion` → Expert processing with gating (Eq. 3) → Enhanced representation `f_enhanced` → Pseudo tokens → LLM generation

- **Design tradeoffs**: GLM3-6B vs. larger models (33% fewer parameters than GLM4-9B with competitive performance); LoRA efficiency vs. cross-lingual adaptation (improves English +0.74%-1.40% but degrades Chinese); expert bottleneck ratios (1:8, 1:4, 1:2) trade capacity vs. specialization.

- **Failure signatures**: Gating collapse (all α_i converge to uniform distribution); modality over-reliance (text ablation causes >20% drop); cross-lingual degradation (Llama3-8B on Chinese data: 46.52% vs 73.90% WF1); LoRA mismatch (performance drops on Chinese datasets).

- **First 3 experiments**: (1) Modality ablation baseline—remove each modality individually to establish contribution on target dataset; (2) Expert contribution analysis—remove each expert individually to verify hierarchical importance pattern; (3) Gating distribution visualization—log α_i distributions across validation samples to verify context-dependent selection.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can language-aware or multilingual LoRA initialization strategies resolve performance degradation on Chinese datasets caused by English-centric LoRA fine-tuning? The paper identifies the problem but does not propose solutions.

- **Open Question 2**: Do cross-lingual differences in multimodal dependency (stronger audio-visual reliance in Chinese vs. English) generalize to other language families beyond Chinese and English? The paper tests only bilingual benchmarks.

- **Open Question 3**: What causes severe performance collapse of Llama3-8B on Chinese datasets (46.52% vs. 73.90% WF1 on CHERMA), and can this be mitigated? The paper reports this failure but provides no explanation or remediation strategy.

- **Open Question 4**: Can manually specified expert network configurations (bottleneck ratios 1:8, 1:4, 1:2; activations Mish/GELU/Swish) be replaced with learnable alternatives without sacrificing performance? The design space remains unexplored.

## Limitations

- Experimental scope is narrow, limited to four datasets without cross-dataset transfer evaluation or robustness testing against domain shifts.

- AudioVisualEncoder architecture is unspecified, preventing exact replication and raising questions about performance scalability with different feature extraction pipelines.

- LoRA shows efficiency gains but causes language-specific degradation on Chinese datasets, suggesting framework may not generalize well beyond tested language pair.

## Confidence

- **High Confidence**: Expert network contributions (E1 > E3 > E2 for classification tasks) and modality importance (text > audio > visual) are well-supported by ablation studies and consistent across datasets.

- **Medium Confidence**: Cross-lingual robustness claims are supported but limited to English-Chinese pair; pseudo token injection efficacy is claimed but not directly compared against alternatives.

- **Low Confidence**: Universal pattern claims in multimodal emotional expressions lack empirical validation beyond tested language pair; computational efficiency claims are not quantified against baseline methods.

## Next Checks

1. **Modality Contribution Validation**: Conduct comprehensive modality ablation across all four datasets to verify reported text > audio > visual importance hierarchy, expecting text removal to cause >20% performance drops on ERC tasks and >15% on MSA tasks.

2. **Expert Selection Dynamics**: Log and visualize hierarchical gating coefficients (α_i distributions) across validation samples to confirm context-dependent expert activation patterns correlate with emotional intensity, sentiment polarity, or conversational context.

3. **Cross-Lingual Generalization Test**: Evaluate framework on a third language (e.g., Spanish or French) using GLM3-6B backbone to test whether reported cross-lingual robustness extends beyond English-Chinese pair, or if performance degrades similarly to Llama3-8B Chinese results.