---
ver: rpa2
title: Deep reinforcement learning-based longitudinal control strategy for automated
  vehicles at signalised intersections
arxiv_id: '2505.08896'
source_url: https://arxiv.org/abs/2505.08896
tags:
- vehicle
- reward
- function
- acceleration
- speed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a deep reinforcement learning-based longitudinal
  vehicle control strategy for signalised intersections. It addresses the complex
  decision-making challenges by formulating a comprehensive reward function focusing
  on distance headway-based efficiency, amber light decision-making, asymmetric acceleration/deceleration
  response, safety, and comfort.
---

# Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections

## Quick Facts
- **arXiv ID**: 2505.08896
- **Source URL**: https://arxiv.org/abs/2505.08896
- **Reference count**: 3
- **Key outcome**: DRL-based longitudinal control improves traffic safety, efficiency, and comfort at signalized intersections while maintaining traffic signal compliance

## Executive Summary
This study develops a deep reinforcement learning-based longitudinal vehicle control strategy for signalized intersections using DDPG and SAC algorithms. The approach addresses complex decision-making challenges through a comprehensive reward function focusing on distance headway efficiency, amber light decision-making, asymmetric acceleration/deceleration response, and traditional safety/comfort criteria. Models are trained on real-world pNEUMA data plus Ornstein-Uhlenbeck simulated trajectories, then tested against human-driven vehicles. Results demonstrate successful collision avoidance, traffic signal compliance, and improved performance metrics while maintaining lower distance headway and jerk compared to human drivers.

## Method Summary
The method formulates longitudinal control as a Markov Decision Process with state vector containing speed, relative speed, spacing, distance to stop line, and traffic light status. Two DRL algorithms (DDPG with 3×30 neural network, SAC with 5×128) are trained on 95 real-world trajectories from pNEUMA dataset plus 200 Ornstein-Uhlenbeck simulated trajectories. The reward function combines seven components weighted according to Table 1, with asymmetric action bounds [-4, +2] m/s² reflecting human comfort preferences. Training uses experience replay buffers (150K for DDPG, 300K for SAC) with soft target network updates (τ=0.001).

## Key Results
- Both DDPG and SAC models successfully maintain lower distance headway and jerk compared to human-driven vehicles without compromising safety
- DDPG model demonstrates smoother action profiles and better overall performance in safety, efficiency, and comfort metrics
- Both models comply with traffic signals and handle safety-critical scenarios effectively, including amber light decision-making and emergency braking situations
- Models maintain collision-free operation across 63 test trajectories while reducing headway below human driver levels

## Why This Works (Mechanism)

### Mechanism 1: Multi-Objective Reward Function with Distance Headway Formulation
The comprehensive reward function enables balanced learning across safety, efficiency, comfort, and signal compliance. Using distance headway formulation with log-normal PDF centered on speed-proportional desired gap (S* = S₀ + vₙT) rather than fixed time headway better handles stop-and-go conditions at signals. The amber light decision uses stopping sight distance (SSD = vₙTᵣ + vₙ²/2a_comf) to determine stopping feasibility.

### Mechanism 2: Asymmetric Acceleration/Deceleration Response via Reward Shaping
Encoding asymmetric bounds [-4, +2] m/s² in the reward function produces more realistic behavior. Square-root scaling of comfort penalties (steeper for acceleration than deceleration) reflects physiological reality that passengers tolerate braking more than rapid acceleration, preserving asymmetric response observed in human car-following literature.

### Mechanism 3: Real-World + OU-Process Simulated Data Augmentation
Training on combined real (95 trajectories) and OU-process simulated data (200 trajectories) improves robustness to safety-critical scenarios underrepresented in naturalistic data. The OU process generates leader trajectories with controlled volatility, producing hard braking scenarios (up to -6 m/s²) that exceed the following vehicle's maximum deceleration capability, enhancing safety-critical event coverage.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - **Why needed here:** The paper formulates vehicle control as an MDP with state s(t) = (vₙ, Δvₙ₋₁,ₙ, Sₙ₋₁,ₙ, dₜₗ, TL) and continuous action a(t) ∈ [-4, 2] m/s². Understanding state observability and action space is prerequisite to debugging reward shaping.
  - **Quick check question:** Can you identify which state variables are directly observable via standard AV sensors (camera, lidar, V2I) versus which require estimation?

- **Concept: Actor-Critic Architecture with Target Networks**
  - **Why needed here:** Both DDPG and SAC use separate actor (policy) and critic (value) networks with soft target updates θ̄ ← τθ + (1-τ)θ̄. The paper uses τ = 0.001 for both algorithms.
  - **Quick check question:** Why does SAC use two Q-networks (Q₁, Q₂) while DDPG uses one, and how does this affect overestimation bias?

- **Concept: Off-Policy Learning with Experience Replay**
  - **Why needed here:** Both algorithms use replay buffers (DDPG: 150K, SAC: 300K capacity) to break temporal correlation of samples. The paper's training used batch sizes of 256 (DDPG) and 512 (SAC).
  - **Quick check question:** If collision events are rare (penalty -50), how does the replay buffer ratio affect learning stability, and should collision experiences be oversampled?

## Architecture Onboarding

- **Component map:** Kinematic vehicle model -> State encoder (5D vector + traffic light categorical) -> Actor network (DDPG: 3×30, SAC: 5×128) -> Environment step -> Critic network (DDPG: 1 Q-network, SAC: 2 Q-networks) -> Reward aggregator (weighted sum) -> Experience replay buffer -> Target network updates

- **Critical path:** Initialize replay buffer with trajectory data (60:40 real:simulated) -> For each episode: sample LV trajectory, reset ego state -> At each timestep: actor outputs action → action masking check → environment step → compute 6 reward components → store transition -> Sample mini-batch, update critic via TD error, update actor via policy gradient -> Soft update target networks every step

- **Design tradeoffs:** DDPG (smaller network, single Q-function) showed smoother acceleration profiles; SAC (larger network, entropy regularization) had slightly higher training rewards but more action fluctuation. Paper recommends DDPG for this task. Distance headway formulation handles v→0 at signals better than time headway; time headway is speed-invariant for highway. Manual reward weight tuning used (Table 1); w₇=50 for collision dominates other rewards.

- **Failure signatures:** Collision during training indicates incorrect action masking threshold d_safe computation; red light violation suggests flawed amber light decision logic; oscillatory acceleration may indicate excessive exploration noise or unstable critic; large jerk values require checking comfort reward normalization.

- **First 3 experiments:** 1) Reproduce training curve: Train DDPG with 95 real + 200 OU trajectories for 1200 episodes; verify rolling average reward stabilizes and no collisions on 63 test trajectories. 2) Ablate asymmetric bounds: Train DDPG with symmetric bounds [-4, +4] m/s²; compare jerk CDF and acceleration smoothness. 3) Test amber light boundary conditions: Generate 128 scenarios with FV start times 15-20s and initial speeds 0-14 m/s; verify stop/go decisions match SSD criterion.

## Open Questions the Paper Calls Out

- **How does incorporating fuel efficiency into the reward function affect the longitudinal control strategy?** The current study excludes energy consumption metrics from optimization. A study showing modified reward function reduces fuel consumption while maintaining safety and comfort standards would resolve this.

- **Can the trained DRL models transfer effectively to real-world physical vehicles?** The conclusion suggests future work should apply models "in real-world settings... to analyse the transferability and adaptability." Successful deployment on a physical test vehicle navigating a signalized intersection without collisions would provide evidence.

- **How robust are the models when trained on large-scale, diverse trajectory datasets?** The authors note the study used a "limited set of real-world trajectories" and suggest expanding to "diverse locations." Demonstrating consistent performance across new datasets with varying intersection geometries and traffic cultures would resolve this.

## Limitations

- Asymmetric action bounds [-4, +2] m/s² are assumed optimal without sensitivity analysis across different vehicle classes
- OU-process augmentation effectiveness depends critically on parameter selection that may not generalize to different traffic cultures
- Real-world validation beyond the pNEUMA intersection is not demonstrated, limiting generalizability claims

## Confidence

- **High confidence**: Safety performance claims (collision avoidance in test scenarios), traffic signal compliance, basic comfort metrics
- **Medium confidence**: Efficiency improvements (distance headway reduction), comparative performance against human drivers
- **Low confidence**: Generalization to different intersection geometries, weather conditions, and aggressive driving cultures

## Next Checks

1. **Cross-cultural validation**: Test trained models on aggressive driving datasets (e.g., Beijing or São Paulo traffic) to assess OU-process parameter sensitivity
2. **Vehicle class transfer**: Evaluate performance when transferring from passenger vehicle bounds to heavy truck dynamics with different comfortable deceleration limits  
3. **Real-world deployment**: Conduct on-road testing at multiple intersection types to validate simulation-to-reality transfer, particularly for amber light decision boundaries under varying weather and visibility conditions