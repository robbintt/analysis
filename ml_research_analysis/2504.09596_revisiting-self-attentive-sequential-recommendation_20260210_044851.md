---
ver: rpa2
title: Revisiting Self-Attentive Sequential Recommendation
arxiv_id: '2504.09596'
source_url: https://arxiv.org/abs/2504.09596
tags:
- sequential
- item
- recommender
- more
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the self-attentive sequential recommendation
  (SASRec) model and identifies several areas for improvement. The authors highlight
  issues with personalization, embedding usage, and positional encoding in SASRec.
---

# Revisiting Self-Attentive Sequential Recommendation

## Quick Facts
- arXiv ID: 2504.09596
- Source URL: https://arxiv.org/abs/2504.09596
- Authors: Zan Huang
- Reference count: 9
- Primary result: Identifies foundational issues in SASRec (positional encoding, personalization, tokenization) and proposes new experimental methodologies for future research.

## Executive Summary
This paper revisits the self-attentive sequential recommendation (SASRec) model to identify several foundational issues that may limit its effectiveness and scalability. The authors highlight problems with positional encoding alignment, lack of explicit user embeddings, and inappropriate tokenization strategies when treating item sequences like language tokens. Rather than presenting new results, the paper proposes a roadmap of experiments to validate these claims and motivate future research on scaling up self-attentive recommenders. The primary contribution is identifying critical implementation details that require empirical validation.

## Method Summary
SASRec uses a Transformer decoder architecture for next-item prediction in sequential recommendation. The model takes user interaction sequences as input, applies item and positional embeddings, then uses masked self-attention to predict the next item. Training employs negative sampling by contrasting the target item against sampled negatives. The paper proposes several modifications: correcting positional embeddings to be relative to prediction position rather than absolute, adding explicit user embeddings for personalization, exploring autoregressive prediction, comparing tokenization methods, and testing different sampling algorithms. The exact datasets and negative sampling procedures are not specified.

## Key Results
- Identifies misalignment between positional embeddings and prediction positions in standard SASRec implementations
- Highlights absence of explicit user embeddings leading to non-personalized recommendations
- Proposes hierarchical tokenization to address vocabulary size limitations when scaling to large item catalogs
- Suggests experimental roadmap for validating foundational issues in self-attentive sequential recommenders

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Positional embeddings in SASRec implementations are misaligned relative to prediction position, potentially degrading sequence modeling effectiveness.
- **Mechanism:** During batched training with causal masking, items at position *k* in a padded sequence receive positional embedding *ep_k* based on absolute position in the padded tensor, rather than relative distance from the prediction target.
- **Core assumption:** Relative positional information encodes meaningful recency signals that absolute positions cannot capture for variable-length sequences.
- **Evidence anchors:**
  - [Section 3.3]: "The position of the interacted item should be relative to the position of the prediction, but the absolute position was used instead."
  - [Section 3.3]: Provides explicit transformation examples showing expected vs. actual embedding assignments.
  - [corpus]: Weak direct validation. Neighbor papers address SASRec improvements but do not confirm this specific positional embedding claim.

### Mechanism 2
- **Claim:** SASRec lacks explicit user embeddings, making it a non-personalized recommender despite operating in a high-dimensional sequential space.
- **Mechanism:** The model represents users solely through their interaction sequences. Given identical sequence *[ix, iy]*, any user receives the same prediction *iz* deterministically.
- **Core assumption:** Explicit user embeddings capture stable preferences that interaction sequences alone cannot express.
- **Evidence anchors:**
  - [Section 3.1]: "There is no personalization in the original SASRec model, as it is without explicit use of user embeddings."
  - [Section 3.1]: "Anyone who interacts with [ix, iy] would receive the same recommended item iz deterministically."
  - [corpus]: Related work on bias-aware and cross-domain sequential recommendation implicitly assumes user-specific signals matter, but does not directly test this claim.

### Mechanism 3
- **Claim:** Item-ID sequences differ fundamentally from language token sequences in sparsity and randomness, requiring different tokenization strategies for effective scaling.
- **Mechanism:** Language tokens follow statistical regularities (n-gram patterns, syntax). Item interaction sequences are sparser and more random. Treating them identically may limit model learning.
- **Core assumption:** Item co-occurrence patterns have lower statistical structure than language, requiring vocabulary reduction strategies.
- **Evidence anchors:**
  - [Section 4.3]: "We expect the item-id sequences to be more random, and much more sparse than language token sequences."
  - [Section 4.3]: Proposes "reverse process of tokenization" to reduce "trillions of items into combinations of more basic elements."
  - [corpus]: CoVE paper (arXiv:2506.19993) addresses vocabulary compression for LLM-based recommenders, providing tangential support for tokenization challenges.

## Foundational Learning

- **Concept: Transformer decoder with causal self-attention**
  - **Why needed here:** SASRec uses the Transformer decoder structure where each position attends only to earlier positions, enabling next-item prediction without bidirectional information leakage.
  - **Quick check question:** Given sequence [A, B, C], which positions can position 3 attend to in a causal decoder?

- **Concept: Negative sampling for recommendation training**
  - **Why needed here:** SASRec trains by contrasting the target item against sampled negative items. The paper notes this introduces biasâ€”interacted items are excluded from the negative pool.
  - **Quick check question:** If item X appears in a user's history, should X be allowed in that user's negative sample pool? Why or why not?

- **Concept: Embedding concatenation vs. addition**
  - **Why needed here:** Positional embeddings are *added* to item embeddings. When padding tokens have non-zero embeddings, they contribute spurious signal to the output.
  - **Quick check question:** If padding token embedding = [0.1, 0.2] and positional embedding at that position = [0.3, 0.4], what is the effective contribution to the attention computation?

## Architecture Onboarding

- **Component map:**
  Input sequence [i_x1, i_x2, ..., i_xl] -> Item embedding lookup [e_x1, e_x2, ..., e_xl] -> + Positional embeddings [ep_1, ep_2, ..., ep_l] -> Causal self-attention layers (Transformer decoder) -> Final hidden state -> Linear projection -> Softmax over item vocabulary -> Negative sampling loss

- **Critical path:** Item embedding quality -> Positional encoding correctness -> Attention pattern learning -> Prediction distribution. Errors in positional encoding propagate through all attention layers.

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Explicit user embeddings | Personalization | Storage overhead, cold-start handling |
  | Relative positional encoding | Correct sequence semantics | Implementation complexity, batching inefficiency |
  | Hierarchical tokenization | Scalability to large catalogs | Loss of fine-grained item distinctions |
  | Autoregressive prediction | Richer training signal | Slower inference |

- **Failure signatures:**
  - **Positional misalignment:** Model performs well only on sequences near maximum length (where absolute/relative positions converge).
  - **Non-personalization:** Users with similar recent history receive identical recommendations regardless of long-term preference differences.
  - **Padding contamination:** Non-zero padding embeddings cause attention to attend to "nothing" positions.

- **First 3 experiments:**
  1. **Correct positional embedding usage:** Batch sequences by length, apply position indices relative to prediction target. Ablate padding entirely for variable-length sequences.
  2. **Add explicit user embeddings:** Concatenate or add learned user embedding to sequence representation. Compare against sequence-only baseline on per-user metrics.
  3. **Compare tokenization strategies:** Train identical architectures on (a) raw item IDs, (b) hierarchical item clusters, (c) text tokens from item metadata. Measure entropy and sparsity of each representation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does correcting the positional embedding application to be relative to the prediction position, rather than the sequence start, significantly improve SASRec performance?
- **Basis in paper:** [explicit] The authors identify that SASRec uses absolute positional embeddings due to training batching tricks, causing $i_{x1}$ to incorrectly receive $ep_{l-1}$ instead of $ep_1$ when predicting $i_{xl-1}$.
- **Why unresolved:** The standard practice of using masks to pack variable-length sequences into batches forces absolute positional encoding, a structural flaw that has persisted since the model's inception.
- **What evidence would resolve it:** A comparison experiment where training samples are batched by length to ensure semantically correct positional embeddings, evaluated against the standard implementation.

### Open Question 2
- **Question:** Can a "duality" approach, where items are represented by sequences of user embeddings, improve the consistency and robustness of sequential recommenders?
- **Basis in paper:** [explicit] Section 4.4 proposes enforcing consistency between original item embeddings and high-order embeddings derived from the sequence of users who interacted with them.
- **Why unresolved:** Standard models only represent users via item sequences ("you are what you interacted"), neglecting the potential cyclic consistency of representing items via user sequences.
- **What evidence would resolve it:** Implementing a loss term that enforces consistency between item embeddings and the aggregated embeddings of interacting users, followed by robustness testing.

### Open Question 3
- **Question:** Can "reverse tokenization" effectively reduce the massive item space into combinations of basic elements to allow self-attentive recommenders to scale up?
- **Basis in paper:** [explicit] Section 4.3 suggests that item-id sequences are sparser than language tokens and hypothesizes that breaking down trillions of items into basic combinations could prevent the model from suffering "ADHD".
- **Why unresolved:** The vocabulary size and sparsity of item IDs compared to language tokens remain a fundamental blocker for scaling sequential recommenders to the size of Large Language Models.
- **What evidence would resolve it:** Architectures implementing a hierarchical or combinatorial item representation compared against raw ID-based models on datasets with extremely large item catalogs.

## Limitations
- No empirical validation provided for proposed corrections - claims remain theoretical
- Missing critical implementation details including dataset specifications and negative sampling procedures
- Evidence anchors rely primarily on logical arguments rather than experimental results

## Confidence
- **Mechanism 1 (Positional embedding misalignment):** Medium confidence - logical argument is sound but untested empirically
- **Mechanism 2 (Lack of personalization):** Medium confidence - deterministic behavior is mathematically correct but practical impact requires validation
- **Mechanism 3 (Tokenization differences):** Low confidence - claims about sequence properties lack empirical measurement

## Next Checks
1. **Measure positional embedding impact:** Train SASRec with corrected relative positional encoding versus baseline absolute positional encoding on identical datasets. Quantify performance differences across sequence lengths.
2. **Test personalization effect:** Add explicit user embeddings to SASRec and measure per-user recommendation diversity and accuracy. Compare against sequence-only baseline on metrics like NDCG@k and user coverage.
3. **Characterize sequence entropy:** Compute entropy and n-gram statistics for item interaction sequences versus language corpora. Measure vocabulary size growth and sparsity patterns. Test whether hierarchical tokenization reduces effective vocabulary size while maintaining recommendation quality.