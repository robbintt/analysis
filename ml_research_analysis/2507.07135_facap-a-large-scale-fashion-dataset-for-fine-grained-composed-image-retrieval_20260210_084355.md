---
ver: rpa2
title: 'FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval'
arxiv_id: '2507.07135'
source_url: https://arxiv.org/abs/2507.07135
tags:
- image
- fashion
- images
- dataset
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FACap, a large-scale fashion dataset for composed
  image retrieval (CIR), addressing the challenge of insufficient high-quality data
  in the fashion domain. The dataset is constructed using a two-stage annotation pipeline
  powered by a vision-language model (VLM) and a large language model (LLM), generating
  detailed modification texts for image pairs.
---

# FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval

## Quick Facts
- arXiv ID: 2507.07135
- Source URL: https://arxiv.org/abs/2507.07135
- Authors: François Gardères; Shizhe Chen; Camille-Sovanneary Gauthier; Jean Ponce
- Reference count: 40
- Primary result: FACap dataset and FashionBLIP-2 achieve state-of-the-art results on Fashion IQ benchmark for composed image retrieval with fine-grained modification texts

## Executive Summary
This paper introduces FACap, a large-scale fashion dataset for composed image retrieval (CIR), addressing the critical shortage of high-quality data in the fashion domain. The dataset is constructed using a two-stage annotation pipeline powered by a vision-language model (VLM) and a large language model (LLM), generating detailed modification texts for image pairs. The authors also propose FashionBLIP-2, a model that fine-tunes BLIP-2 with lightweight adapters and multi-head query-candidate matching to better capture fine-grained fashion-specific information. Experimental results demonstrate that pretraining on FACap significantly improves performance in fashion CIR, especially for retrieval with fine-grained modification texts, achieving state-of-the-art results on the Fashion IQ benchmark.

## Method Summary
FACap is built from Fashion200k and DeepFashion-MultiModal, pairing images by CLIP similarity within categories and generating modification texts through a two-stage VLM+LLM pipeline. FashionBLIP-2 uses BLIP-2 ViT-G with adapter modules in the image encoder and a multi-head query-candidate matching mechanism. The model is trained in two stages: first on FACap with CIR and CTR losses, then fine-tuned on Fashion IQ with frozen encoder+adapters. The multi-head matching reduces 32 query tokens to 12 through token mixing and projects from 768 to 256 dimensions through channel mixing, computing cosine similarities that are summed for the final score.

## Key Results
- FACap pretraining improves FashionBLIP-2 performance on Fashion IQ from 41.04 to 43.31 Recall@10
- Multi-head query-candidate matching achieves 65.97 Recall@10 on Fashion IQ val (vs 64.38 without)
- FACap achieves comparable faithfulness to Fashion IQ (4.40 vs 4.48 on 5-point scale) with higher detail and saliency scores
- FashionBLIP-2 outperforms state-of-the-art models including UniFashion on enhFashionIQ benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage VLM+LLM annotation pipeline produces higher-quality modification texts than human crowdsourcing for fashion CIR.
- Mechanism: Stage 1 uses InternVL to generate detailed single-image captions with category and product context. Stage 2 uses GPT-4o mini to synthesize concise modification texts by analyzing differences between reference and target captions. This division of labor leverages VLMs' strength in visual description and LLMs' strength in text reasoning.
- Core assumption: VLM captioning errors are comparable to human annotation mistakes, and LLMs can reliably identify salient differences from caption pairs.
- Evidence anchors:
  - [abstract] "a two-stage annotation pipeline powered by a VLM and a large language model (LLM) to generate accurate and detailed modification texts"
  - [Section 3.2, Table 2] Human evaluation shows FACap achieves comparable faithfulness (4.40 vs 4.48) and higher details (4.09 vs 3.03) and saliency (4.29 vs 3.60) than Fashion IQ on 5-point scale.
  - [corpus] Weak corpus evidence—related papers focus on CIR methods rather than automated annotation quality evaluation.
- Break condition: If VLM hallucinations increase beyond human error rates, or if LLM fails to identify discriminative differences, annotation quality degrades.

### Mechanism 2
- Claim: Lightweight adapters in the image encoder enable fashion-domain adaptation while preserving general visual knowledge.
- Mechanism: Adapter modules with bottleneck architecture (downsampling factor 16) are inserted into each transformer layer. Only adapters and Q-Former are trained while the pretrained BLIP-2 image encoder remains frozen. The residual connection preserves original features.
- Core assumption: Fashion-specific visual features can be learned through low-rank adaptations without modifying the full encoder.
- Evidence anchors:
  - [abstract] "fine-tunes the general-domain BLIP-2 model on FACap with lightweight adapters"
  - [Section 4.2, Eq. 1] Adapter(x) = x + W_u(σ(W_d·x)) with c_b ≪ c
  - [Section 5.3, Table 5] Adapters improve performance from 41.04 to 42.62 (no fine-tuning) and 64.36 to 64.38 (with fine-tuning) on Fashion IQ average recall.
  - [corpus] Related papers (FineCIR, TMCIR) do not specifically validate adapter-based domain adaptation for CIR.
- Break condition: If fashion features require high-rank representations or if adapters cause catastrophic forgetting of general features, performance degrades.

### Mechanism 3
- Claim: Multi-head query-candidate matching captures fine-grained details better than global averaging.
- Mechanism: Instead of averaging tokens to a single global vector, token mixing reduces 32 query tokens to 12 tokens, then channel mixing projects from 768 to 256 dimensions. Each resulting vector pair computes cosine similarity, and scores are summed. This preserves multiple aspects of the input.
- Core assumption: Multiple smaller representations capture complementary fine-grained features that averaging destroys.
- Evidence anchors:
  - [abstract] "multi-head query-candidate matching to better capture fine-grained fashion-specific information"
  - [Section 4.3, Eq. 4] s_qc = Σ cos(x_q,i, x_c,i) over n_t heads
  - [Section 5.3, Table 5] Multi-head matching improves from 42.62 to 43.31 (no fine-tuning) and 64.38 to 65.97 (with fine-tuning).
  - [corpus] TMCIR (arXiv 2504.10995) explores token merging for CIR, suggesting token-level processing is an active research direction.
- Break condition: If token mixing removes discriminative information or if heads become redundant, fine-grained matching fails.

## Foundational Learning

- Concept: **Vision-Language Models (VLMs)**
  - Why needed here: The data pipeline uses InternVL for captioning. Understanding how VLMs encode visual information into text is essential for diagnosing annotation quality.
  - Quick check question: Given an image of a dress, what types of visual details would a VLM likely capture versus miss?

- Concept: **Contrastive Learning for Retrieval**
  - Why needed here: FashionBLIP-2 uses contrastive loss (Eq. 5) where positive pairs are pulled together and negative pairs pushed apart in embedding space.
  - Quick check question: In a batch of 512 triplets, how many negative pairs exist for each reference image?

- Concept: **BLIP-2 Architecture**
  - Why needed here: FashionBLIP-2 builds on BLIP-2's Q-Former which bridges frozen image encoders and LLMs using learnable queries.
  - Quick check question: Why does the Q-Former use 32 query tokens rather than processing all image patches directly?

## Architecture Onboarding

- Component map:
  Reference Image → [ViT-G + Adapters] → Feature Map
                                             ↓
  Modification Text ──────────────────→ [Q-Former] → Query Embeddings (32×768)
                                                                  ↓
  Candidate Image → [ViT-G + Adapters] → [Q-Former, no text] → Candidate Embeddings (32×768)
                                                                  ↓
                                            [Multi-head Matching] → Similarity Score
                                                                  ↓
  Token Mixing (32→12) → Channel Mixing (768→256) → Sum of 12 Cosine Similarities

- Critical path:
  1. Image encoder adapters must learn fashion-specific features during FACap pretraining (Stage 1)
  2. Q-Former must fuse visual and textual information through contrastive learning
  3. Multi-head matching parameters (W_tm, W_cm) are shared between query and candidate processing

- Design tradeoffs:
  - **Adapter bottleneck (c/16)**: Smaller = fewer parameters (faster training) but may underfit complex fashion features
  - **Number of heads (12)**: More heads capture more details but increase redundancy risk
  - **CTR auxiliary task**: Improves zero-shot performance but may overfit to detailed text when fine-tuning on noisy data (Table 5, row 4 shows drop from 65.97 to 65.62 with CTR)

- Failure signatures:
  - **Retrieved images match text but aren't target**: Indicates false negatives in candidate set (Figure 6, row 3)
  - **Performance drops on enhFashionIQ after FashionIQ fine-tuning**: Model overfitting to noisy annotations (SPRC drops 79.59→80.29, FashionBLIP-2 maintains 87.93)
  - **Low recall on dress category**: High diversity in descriptions (length, pattern, neckline) requires more training data

- First 3 experiments:
  1. **Validate data quality**: Randomly sample 50 FACap triplets, manually score faithfulness/details/saliency on 1-5 scale. Compare to Fashion IQ samples.
  2. **Ablation adapters**: Train FashionBLIP-2 on FACap with adapters disabled (freeze full image encoder). Measure gap on Fashion IQ validation.
  3. **Scaling test**: Train on 25%, 50%, 75% of FACap data. Plot recall vs. dataset size to identify diminishing returns point (Figure 5 suggests gains slow after 50%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Composed Text Retrieval (CTR) auxiliary task introduce a textual bias that hinders adaptation to noisy downstream datasets?
- Basis in paper: [explicit] The authors state in the ablation study discussion: "We hypothesize that the CTR task may introduce a bias towards detailed textual descriptions, which might hinder adaptation to noisier datasets like Fashion IQ."
- Why unresolved: While the paper observes that the CTR task improves performance in zero-shot settings but degrades it when fine-tuning on Fashion IQ, the underlying mechanism of this bias remains a hypothesis rather than a proven conclusion.
- What evidence would resolve it: An analysis of the embedding space geometry when training with and without CTR on datasets with controlled noise levels, measuring the alignment shift between visual and textual features.

### Open Question 2
- Question: Can the FACap dataset and FashionBLIP-2 architecture be effectively combined with UniFashion's generative pretraining tasks?
- Basis in paper: [explicit] The authors mention in the results discussion that "UniFashion... utilizes more image-caption pairs... As our FACap dataset and method are complementary to UniFashion, we will leave it to future work."
- Why unresolved: The paper demonstrates that FashionBLIP-2 is competitive but often second-best to UniFashion in fine-tuned settings, but it does not explore if the two approaches (CIR triplets vs. generative tasks) are mutually exclusive or synergistic.
- What evidence would resolve it: Experiments incorporating FACap triplets into the UniFashion training pipeline to observe if retrieval metrics improve beyond the current state-of-the-art.

### Open Question 3
- Question: To what extent does data quality and diversity contribute to performance gains compared to sheer dataset volume?
- Basis in paper: [explicit] Section 5.3 notes that the "performance gain from training on 50% to 100% of the dataset is relatively small" and suggests "further improvements may require focusing on data quality and diversity rather than sheer volume."
- Why unresolved: The experiments only scale the volume of data, showing diminishing returns; the specific contribution of the "quality" aspect of FACap was not isolated against a volume baseline.
- What evidence would resolve it: A controlled ablation where models are trained on subsets of FACap explicitly curated for high diversity versus large subsets with redundant samples, holding the sample count constant.

### Open Question 4
- Question: Can direct multi-image processing by advanced Vision-Language Models (VLMs) replace the two-stage caption-then-diff generation pipeline?
- Basis in paper: [inferred] The paper justifies the two-stage pipeline by stating "currently, only a few VLMs are capable of accurately comparing two images in detail," implying that future VLM capabilities might negate this design choice.
- Why unresolved: The pipeline was designed to work around the limitations of current VLMs regarding multi-image context; the paper does not test if a stronger, native multi-image VLM could generate modification texts with comparable fidelity in a single step.
- What evidence would resolve it: A comparative evaluation where a state-of-the-art multi-image VLM (e.g., a future iteration of LLaVA or GPT-4V) directly generates modification texts, bypassing the separate captioning and LLM synthesis steps.

## Limitations

- The automated annotation pipeline's quality critically depends on unstated prompts and few-shot examples, making faithful reproduction challenging
- CTR auxiliary task shows inconsistent benefits, suggesting potential overfitting when fine-tuning on noisy data
- Performance gains from dataset scaling show diminishing returns, suggesting quality and diversity may be more important than volume

## Confidence

- **High confidence**: Adapter-based domain adaptation - well-specified architecture with measurable performance gains across multiple ablations
- **Medium confidence**: Multi-head matching effectiveness - shows consistent gains but relies on unvalidated architectural choices
- **Medium confidence**: Annotation quality claims - human evaluation supports claims but lacks comparison to baseline automated methods

## Next Checks

1. **Error rate analysis**: Compare VLM hallucination frequency against human annotation error rates on fashion CIR tasks using blind human evaluation of 100 random FACap triplets
2. **CTR task sensitivity**: Systematically vary CTR loss weight (0, 0.1, 0.5, 1.0) during Fashion IQ fine-tuning and measure zero-shot vs fine-tuned performance trade-offs
3. **Annotation robustness**: Generate FACap data using alternative VLMs (e.g., BLIP-2, Flamingo) and measure downstream retrieval performance variance to establish annotation method sensitivity