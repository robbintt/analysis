---
ver: rpa2
title: 'Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online
  Preference Optimization'
arxiv_id: '2509.21718'
source_url: https://arxiv.org/abs/2509.21718
tags:
- languages
- speech
- grpo
- language
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a GRPO-based framework for adapting multilingual
  TTS models to low-resource languages by leveraging a pretrained ASR model as a reward
  signal. The method proceeds in three stages: multilingual pretraining with IPA tokens,
  fine-tuning on limited paired data, and online preference optimization using ASR-derived
  CER, speaker similarity, and PESQ scores as rewards.'
---

# Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization

## Quick Facts
- **arXiv ID:** 2509.21718
- **Source URL:** https://arxiv.org/abs/2509.21718
- **Reference count:** 0
- **Primary result:** GRPO adaptation of multilingual TTS yields >8× CER reduction and improved speaker similarity for low-resource languages

## Executive Summary
This work introduces a GRPO-based framework for adapting multilingual TTS models to low-resource languages by leveraging a pretrained ASR model as a reward signal. The method proceeds in three stages: multilingual pretraining with IPA tokens, fine-tuning on limited paired data, and online preference optimization using ASR-derived CER, speaker similarity, and PESQ scores as rewards. Experiments show that combining fine-tuning with GRPO yields substantial gains in intelligibility (CER drops from 33.00% to 3.94% for Portuguese with 30 min data) and speaker consistency (SSIM improves from 0.50 to 0.79), outperforming fine-tuning alone. GRPO also surpasses offline alignment (DPO) in high-resource languages, delivering superior intelligibility, speaker similarity, and audio quality.

## Method Summary
The method adapts a multilingual TTS model to low-resource languages through three stages: (1) pretraining a Koel-TTS model on 21k hours of multilingual data with IPA tokenization, (2) fine-tuning on limited target language data mixed with original data at 5× upsampling, and (3) applying GRPO for ~2k iterations using a weighted combination of ASR-derived CER, speaker similarity (SSIM), and PESQ scores as rewards. The reward is normalized to [0,1] via piecewise linear mapping and optimized without KL penalty using group-relative advantages from 12 samples per prompt.

## Key Results
- With 30 minutes of Portuguese data, CER drops from 33.00% to 3.94% after fine-tuning and GRPO (8× reduction)
- SSIM improves from 0.50 to 0.79 for Portuguese, indicating better speaker consistency
- GRPO outperforms fine-tuning alone and offline DPO in high-resource language settings
- The multi-objective reward signal effectively balances intelligibility, speaker consistency, and audio quality

## Why This Works (Mechanism)

### Mechanism 1: IPA-Based Phonetic Transfer Enables Zero-Shot Cross-Lingual Generalization
Representing text as International Phonetic Alphabet (IPA) tokens rather than language-specific orthography allows a multilingual TTS model to synthesize speech in unseen languages by mapping universal acoustic units. The baseline model learns phoneme-to-acoustic-code mappings from six source languages using a 256-token byte-level IPA tokenizer. Because IPA standardizes speech sounds across languages, phonotactic patterns from training languages can recombine to approximate target language pronunciations during inference.

### Mechanism 2: Multi-Objective Reward Signal Provides Differentiable Feedback Without Human Annotation
A weighted combination of ASR-derived character error rate (CER), speaker similarity (SSIM), and perceptual quality (PESQ) creates a proxy reward that correlates with human preferences for intelligibility, speaker consistency, and audio quality. For each generated sample, the system computes CER via Whisper Large V3, SSIM via Titanet-large cosine similarity, and PESQ via a neural estimator, then combines them with weights w_cer = w_ssim = 0.45, w_pesq = 0.1.

### Mechanism 3: Group-Relative Advantages Enable Stable Policy Optimization Without KL Penalty
Computing advantages relative to the mean reward of sampled outputs (rather than against a fixed reference model) stabilizes RL training and eliminates the need for KL divergence constraints. For each prompt, K=12 audio samples are generated. The advantage A_i,k = r_i,k - μ_i, where μ_i is the mean reward of the K samples. The GRPO objective maximizes log-likelihood weighted by these centered advantages.

## Foundational Learning

- **Concept: Reinforcement Learning from Feedback (RLHF paradigm)**
  - **Why needed here:** GRPO is an RL algorithm adapted from LLM alignment. Understanding the core idea—that a policy is updated to maximize expected reward from a judge—helps distinguish online methods (iterative sampling + reward) from offline methods (fixed preference pairs).
  - **Quick check question:** Can you explain why DPO is called "offline" while GRPO is "online," and what tradeoff this creates?

- **Concept: Codec-Based Discrete Speech Representation**
  - **Why needed here:** The TTS model generates audio codec tokens (NanoCodec at 21.5 FPS) rather than waveforms directly. This tokenization is what makes the problem tractable as a sequence modeling task similar to language modeling.
  - **Quick check question:** If the codec has 1024 codebook entries and operates at 21.5 FPS, approximately how many tokens represent a 5-second utterance?

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed here:** CFG is used during both training (random dropout of conditioning) and inference (combining conditioned and unconditioned predictions). Results show CFG substantially improves SSIM and CER, but the paper evaluates both with and without it.
  - **Quick check question:** During inference with CFG scale 2.5, how are the conditioned and unconditioned logit distributions combined?

## Architecture Onboarding

- **Component map:** Text → IPA conversion → byte-level tokenizer (256 tokens); Reference audio → NanoCodec encoder → acoustic codes → NAR encoder → AR decoder → acoustic codes → NanoCodec decoder → waveform; Reward models: Whisper Large V3 (CER), Titanet-large (SSIM), neural PESQ estimator

- **Critical path:** 1. Pretrain baseline on 6 languages (~21k hours) with IPA tokens, next-frame prediction loss; 2. Fine-tune on target language data (30 min–5 hours) mixed with pretraining data, upsampled 5×; 3. Apply GRPO for ~2k iterations with 15k prompts per language, K=12 samples per prompt, learning rate 2e-7

- **Design tradeoffs:** CER and SSIM weighted equally (0.45), PESQ lower (0.1)—the paper does not ablate these; K=12 balances compute cost against advantage estimate quality; KL penalty omission removes regularization to reference model, speeding training but potentially allowing larger distribution shifts; 5× upsampling ensures exposure but may overfit if target data is very small or noisy

- **Failure signatures:** High CER with good SSIM indicates overfitting to speaker characteristics at expense of intelligibility; reward hacking if PESQ improves but human listeners report artifacts; catastrophic forgetting if performance degrades on original training languages; training instability from noisy reward normalization or insufficient sample diversity

- **First 3 experiments:** 1. Reproduce baseline → fine-tune → GRPO on Portuguese 30-min split targeting CER <5%; 2. Ablate GRPO vs. fine-tuning only on same held-out test set expecting >2× CER reduction; 3. Compare GRPO vs. DPO on English high-resource setting replicating Table 1 conditions

## Open Questions the Paper Calls Out

- **Open Question 1:** How well does the GRPO-based adaptation approach generalize to languages with typological features substantially different from the six Indo-European languages used in pretraining and evaluation?
  - Basis: Experiments only cover Hindi, Portuguese, and Polish—all Indo-European languages sharing phonological and prosodic characteristics with the pretraining languages
  - Why unresolved: The IPA representation provides language-agnostic phonetic encoding, but the paper does not validate whether this sufficiently captures radically different phoneme inventories, tonal systems, or morphological patterns
  - What evidence would resolve it: Experiments on languages from diverse families (e.g., Mandarin, Arabic, Swahili, Navajo) showing comparable CER/SSIM/PESQ improvements

- **Open Question 2:** What is the lower bound on ASR model quality required for the reward signal to improve rather than degrade TTS outputs?
  - Basis: The method relies on Whisper Large V3 for CER-based rewards, but many truly low-resource languages have substantially weaker ASR models, potentially providing noisy or biased reward signals
  - Why unresolved: The paper assumes accessible ASR models but does not ablate how ASR errors or biases propagate through the GRPO optimization, nor how robust the reward normalization is to systematic ASR failures
  - What evidence would resolve it: Ablation studies using degraded ASR models (higher WER) or synthetic ASR noise, measuring resulting TTS quality degradation

- **Open Question 3:** How sensitive is GRPO performance to the specific weighting of multi-objective rewards, and are the chosen weights (w_cer=0.45, w_ssim=0.45, w_pesq=0.1) optimal across different language-resource scenarios?
  - Basis: "where we set w_cer = w_ssim = 0.45 and w_pesq = 0.1"—no justification or ablation is provided for these hyperparameters
  - Why unresolved: Different low-resource scenarios may prioritize intelligibility over speaker similarity or quality, and the paper provides no analysis of weight sensitivity or principled selection criteria
  - What evidence would resolve it: Systematic hyperparameter sweeps across weight combinations with Pareto frontier analysis for different target languages and data regimes

## Limitations

- **IPA Coverage Assumption:** The method assumes target languages share substantial phonological overlap with training languages, but this is not quantified for truly low-resource languages with rare phonemes
- **Reward Model Reliability:** The multi-objective reward combines proxy metrics without validating their correlation with human preferences in the target domain; no human evaluation is reported
- **Reproducibility Barriers:** Key implementation details are missing or inaccessible, including the exact IPA tokenization pipeline, baseline normalization scores, and full architecture specifications; pretrained multilingual baseline is not publicly released

## Confidence

- **High Confidence:** The core GRPO framework and reward formulation are clearly specified and theoretically sound; the mechanism of using group-relative advantages for stable online optimization is well-established in the literature
- **Medium Confidence:** The reported performance gains are substantial, but without independent replication or human evaluation, the practical significance is uncertain; the IPA-based transfer mechanism is plausible but not empirically validated for unseen phoneme inventories
- **Low Confidence:** The long-tail generalization to languages with minimal phonological overlap to training sets is asserted but not tested; the omission of KL penalty and its impact on distribution stability is reported as beneficial but not rigorously justified

## Next Checks

1. **IPA Coverage Validation:** For a target low-resource language, enumerate its phoneme inventory and compare against the union of phonemes in the 6 training languages. Measure the fraction of target phonemes absent from training. Synthesize test utterances and assess intelligibility gaps for unseen phonemes.

2. **Reward Model Correlation Study:** Collect human ratings (intelligibility, speaker similarity, naturalness) for a subset of generated samples. Compute Pearson/Spearman correlation between human ratings and the automated reward scores (CER, SSIM, PESQ). Identify if any reward component systematically misaligns with human judgment.

3. **Generalization Stress Test:** Apply the fine-tuned+GRPO model to a language with known phonological gaps (e.g., a language with clicks or tones absent from training). Measure CER and perform qualitative analysis to determine if the model produces recognizable speech or fails catastrophically.