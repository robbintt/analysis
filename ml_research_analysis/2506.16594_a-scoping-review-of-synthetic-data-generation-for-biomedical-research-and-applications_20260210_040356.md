---
ver: rpa2
title: A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications
arxiv_id: '2506.16594'
source_url: https://arxiv.org/abs/2506.16594
tags:
- data
- synthetic
- clinical
- generation
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review systematically analyzed 59 studies on synthetic
  data generation for biomedical research, published between 2020 and 2025. The review
  found that large language models (LLMs) are predominantly used for generating synthetic
  biomedical data, with 72.9% of studies employing prompting-based approaches.
---

# A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications

## Quick Facts
- arXiv ID: 2506.16594
- Source URL: https://arxiv.org/abs/2506.16594
- Reference count: 40
- Primary result: 59 studies analyzed showing LLMs dominate synthetic biomedical data generation with prompting-based approaches (72.9%) and unstructured text as the primary data type (78.0%)

## Executive Summary
This scoping review systematically analyzed 59 studies on synthetic data generation for biomedical research published between 2020 and 2025. The review found that large language models (LLMs) are predominantly used for generating synthetic biomedical data, with prompting-based approaches being the most common generation method. The majority of studies focused on unstructured text data, followed by tabular data and multimodal sources. Quality assessment varied significantly across studies, with most relying on task-specific extrinsic metrics and human-in-the-loop evaluations. The findings highlight both the potential of synthetic data to address data scarcity and privacy concerns in biomedical research, as well as the need for standardized evaluation frameworks and improved accessibility practices.

## Method Summary
The review followed PRISMA-ScR guidelines, conducting systematic searches across PubMed, ACM, Web of Science, Google Scholar, arXiv, and MedRxiv using Boolean keyword queries combining "Language Model," "synthetic data," and biomedical terms. After de-duplication and screening, 59 peer-reviewed studies met inclusion criteria for full-text analysis. Data extraction focused on generation methods, data modalities, evaluation approaches, and accessibility practices, with quantitative synthesis of trends and patterns across the included literature.

## Key Results
- Large language models dominate synthetic data generation, with 72.9% of studies using prompting-based approaches
- Unstructured text represents the primary data type (78.0%), followed by tabular data (13.6%) and multimodal sources (8.4%)
- Quality assessment relies heavily on extrinsic metrics and human-in-the-loop evaluations (55.9%), with limited use of intrinsic evaluation methods
- Accessibility remains a concern, with 49.2% of studies not clearly specifying data availability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompting-based generation using general-purpose LLMs (e.g., GPT-4) may effectively augment scarce biomedical training data for specific downstream tasks.
- **Mechanism:** General-domain LLMs encode broad linguistic and latent biomedical patterns from pre-training. By conditioning on instructions (prompts) and optionally few-shot examples, the model navigates its latent space to output structured or unstructured clinical text that statistically resembles real patient data, thereby increasing training set volume.
- **Core assumption:** The pre-trained model possesses sufficient latent domain knowledge to produce clinically coherent output without parameter updates.
- **Evidence anchors:** [abstract] "Prompting-based methods (72.9%) [were] being the most common generation approach." [section] "Prompt-based approaches... rely on meticulously crafted textual instructions... for diverse biomedical tasks." [corpus] Corpus signals indicate "Retrieval-Augmented Generation in Medicine" as a related trend.
- **Break condition:** If the target domain requires highly specialized jargon or logic not well-represented in the pre-training corpus, prompting alone may yield factually incorrect or "hallucinated" data.

### Mechanism 2
- **Claim:** Fine-tuning open-source models on local datasets provides a mechanism for generating privacy-preserving synthetic data, though potentially at the cost of raw reasoning performance.
- **Mechanism:** By updating the weights of a model (e.g., GPT-2, Llama) on a specific, localized dataset, the model learns the exact statistical distribution and style of that data. This allows generation of new samples without querying external, third-party APIs, reducing data exposure risks.
- **Core assumption:** The local fine-tuning data is of sufficient quality and quantity to overfit the model to the specific domain style without simply memorizing training examples.
- **Evidence anchors:** [section] "Fine-tuning... involves additional training... enabling the model to specialize its outputs... Fine-tuning open-source models demonstrates advantages in privacy-sensitive... contexts." [table 1] Multiple studies list "Fine-tuning" for tasks like de-identification and readmission prediction. [corpus] "Privacy Challenges and Solutions... for Healthcare Chatbots" is noted as a neighbor paper.
- **Break condition:** If fine-tuning is under-resourced (compute or data), the resulting model may produce lower-fidelity text compared to state-of-the-art proprietary baselines (e.g., GPT-4).

### Mechanism 3
- **Claim:** Synthetic data quality is primarily established through extrinsic utility (performance on a downstream task) rather than intrinsic similarity metrics.
- **Mechanism:** Rather than measuring how "similar" synthetic text looks to real text (intrinsic), validation relies on the "Train on Synthetic, Test on Real" paradigm. If a classifier trained on synthetic data performs well on real data, the synthetic data is deemed high-fidelity.
- **Core assumption:** High performance on a specific downstream task correlates with general data realism and generalizability.
- **Evidence anchors:** [abstract] "Quality assessment relied heavily on extrinsic metrics (task-specific)... with human-in-the-loop evaluations in 55.9% of studies." [table 2] Shows the vast majority of studies using task metrics rather than just "Intrinsic+" checks. [corpus] No direct corpus neighbor contradicts this, but the reliance on "Evaluation" is a standard theme in AI reviews.
- **Break condition:** If the synthetic data overfits to the specific features required for the downstream task (shortcut learning), it may score high on utility but fail to capture the true diversity of clinical reality.

## Foundational Learning

- **Concept: Zero-shot vs. Few-shot Prompting**
  - **Why needed here:** The review identifies these as the dominant generation methods (72.9%). Understanding how providing examples (shots) conditions the model is essential for replicating the reviewed results.
  - **Quick check question:** If I need to generate synthetic discharge summaries, would I use zero-shot (just instructions) or few-shot (instructions + 3 example summaries)?

- **Concept: Extrinsic vs. Intrinsic Evaluation**
  - **Why needed here:** The paper highlights a gap in evaluation standards. One must distinguish between checking data quality via statistical similarity (intrinsic) vs. utility in a model (extrinsic).
  - **Quick check question:** If a synthetic dataset has low perplexity (intrinsic) but fails to improve a diagnostic classifier (extrinsic), is it considered high quality in this review's context?

- **Concept: Closed-source vs. Open-source Model Risks**
  - **Why needed here:** The review explicitly flags the trade-off between the performance of closed-source models (GPT-4) and the privacy/transparency of open-source models (Llama, GPT-2).
  - **Quick check question:** Why might a hospital prohibit the use of GPT-4 for generating synthetic data from raw patient records, even for augmentation purposes?

## Architecture Onboarding

- **Component map:** Source Data -> Generator -> Prompt Interface -> Validator -> Output
- **Critical path:**
  1. Define Schema: Identify the specific clinical concept to generate (e.g., "PTSD symptoms")
  2. Prompt Engineering: Construct few-shot examples or knowledge-infused prompts (e.g., using ICD codes)
  3. Generation: Run inference via API or locally
  4. Filtering: Use automated metrics or human review to discard low-quality outputs
  5. Validation: Train a downstream model on the synthetic data and evaluate on a held-out *real* set

- **Design tradeoffs:**
  - **API (GPT-4) vs. Local (Llama):** API offers superior reasoning/generation quality but fails data sovereignty requirements (MIMIC-IV cannot be uploaded). Local offers privacy but requires significant compute for fine-tuning/inference and may underperform on complex reasoning.
  - **Few-shot vs. Fine-tuning:** Few-shot is faster to iterate on but limited by context window. Fine-tuning is expensive to set up but better captures distinct styles/long-tail terms.

- **Failure signatures:**
  - **Mode Collapse:** Synthetic data repeats the same few examples provided in the prompt
  - **Hallucination:** Clinical text is grammatically correct but contains medically impossible logic or fake drug names
  - **Privacy Leakage:** Synthetic data reproduces unique identifiers or rare combinations from the training data verbatim

- **First 3 experiments:**
  1. **Baseline Generation (Zero-shot):** Use a closed-source model to generate 100 synthetic clinical notes based solely on a task description; evaluate via human expert review for realism
  2. **Few-shot Augmentation:** Provide 5 real (anonymized) examples to the model; generate 500 notes; compare the performance of a classifier trained on these 500 vs. the 5 real ones
  3. **Privacy Stress Test:** Train a membership inference attack model on the synthetic dataset to check if it can distinguish between the original seed data and unrelated data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a standardized evaluation framework be established to allow for direct comparison of synthetic data quality across different biomedical studies and domains?
- **Basis in paper:** [inferred] The authors identify a "lack of standardized evaluation frameworks" as a key challenge, noting that heterogeneity in metrics complicates comparisons and limits the transferability of findings.
- **Why unresolved:** Current studies rely on varied, task-specific extrinsic metrics rather than a unified set of intrinsic and extrinsic standards.
- **What evidence would resolve it:** The development and adoption of a consensus benchmark or toolkit that provides consistent scoring for fidelity, utility, and privacy across diverse datasets.

### Open Question 2
- **Question:** Can open-source LLMs be optimized to match the synthetic data generation capabilities of closed-source models like GPT-4 in specialized clinical contexts?
- **Basis in paper:** [inferred] The review notes that while open-source models offer customization and privacy advantages, they "typically underperform closed-source models in general language tasks and clinical reasoning benchmarks."
- **Why unresolved:** There is a trade-off between the accessibility/transparency of open-source models and the high performance of proprietary models.
- **What evidence would resolve it:** Studies demonstrating fine-tuning or architectural modifications that enable open-source models to achieve statistical parity with GPT-4 on specific biomedical generation tasks.

### Open Question 3
- **Question:** What is the optimal methodology for integrating intrinsic metrics with extrinsic, task-specific evaluations to ensure synthetic data fidelity?
- **Basis in paper:** [inferred] The paper states that studies "still underutilize intrinsic evaluation," relying heavily on extrinsic metrics that may not fully capture the utility or limitations of the synthetic data.
- **Why unresolved:** The field lacks a clear understanding of how intrinsic statistical properties of synthetic data correlate with downstream clinical performance.
- **What evidence would resolve it:** Research validating specific intrinsic metrics as reliable predictors of performance on downstream clinical tasks like phenotype classification or entity extraction.

## Limitations

- The scoping methodology relies on keyword-based searches that may miss relevant studies using alternative terminology or published in non-indexed venues
- Classification of generation methods and data types depends on author self-reporting, potentially not capturing hybrid approaches
- Quality assessment metrics vary significantly across studies, making direct comparisons challenging
- The review doesn't account for domain-specific nuances in biomedical subfields, potentially overgeneralizing findings

## Confidence

- **High Confidence:** The predominance of LLM-based generation methods (72.9% prompting-based) and unstructured text as the primary data type (78.0%) are well-supported by systematic analysis
- **Medium Confidence:** Identified trends toward multimodal generation and increased use of open-source models may reflect recent developments that could shift as the field evolves
- **Low Confidence:** The assertion that fine-tuning provides superior privacy preservation lacks empirical validation across reviewed studies

## Next Checks

1. **Replication Study:** Conduct a follow-up systematic review using the same methodology but with an extended timeframe (2026-2027) to validate whether identified trends persist and strengthen

2. **Empirical Comparison:** Design a controlled experiment comparing privacy-preserving capabilities of prompting-based vs. fine-tuning approaches using standardized metrics on identical biomedical datasets

3. **Evaluation Framework Development:** Develop and pilot a standardized evaluation protocol that combines intrinsic metrics with extrinsic task performance and human expert validation across multiple biomedical subdomains