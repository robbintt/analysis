---
ver: rpa2
title: 'Dynamic Template Selection for Output Token Generation Optimization: MLP-Based
  and Transformer Approaches'
arxiv_id: '2511.20683'
source_url: https://arxiv.org/abs/2511.20683
tags:
- token
- template
- routing
- across
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Dynamic Template Selection (DTS) addresses the problem of token\
  \ inefficiency in LLM deployments by adaptively matching response templates to query\
  \ complexity, reducing output token costs which are 4-8\xD7 more expensive than\
  \ input tokens. The core method uses neural routing to classify queries and select\
  \ from five verbosity-tuned templates, with a dual-layer token control mechanism\
  \ combining soft prompting and hard API-level token caps."
---

# Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches

## Quick Facts
- arXiv ID: 2511.20683
- Source URL: https://arxiv.org/abs/2511.20683
- Authors: Bharadwaj Yadavalli
- Reference count: 20
- Key outcome: Achieved 90.5% routing accuracy and 32.6-33.9% token reduction across 9,000 production API calls on 1,000 MMLU questions, maintaining 100% success rates while significantly lowering costs

## Executive Summary
Dynamic Template Selection (DTS) addresses token inefficiency in LLM deployments by adaptively matching response templates to query complexity. The system uses neural routing to classify queries and select from five verbosity-tuned templates, combining soft prompting and hard API-level token caps for robust token control. Across major LLM providers, DTS reduces output token costs (which are 4-8× more expensive than input tokens) while maintaining high success rates.

## Method Summary
DTS employs a dual-layer token control mechanism: soft prompts guide response verbosity through system instructions, while hard caps enforce absolute token ceilings per template. The router uses either an MLP classifier (90.5% accuracy, ~5ms latency) or RoBERTa transformer (89.5% accuracy, ~50ms latency) to classify queries based on pre-computed embeddings. Five templates (minimal, standard, verbose, technical, executive) are applied with corresponding max_tokens parameters, achieving 32.6-33.9% token reduction across three major LLM providers.

## Key Results
- 90.5% routing accuracy with MLP router, marginally exceeding RoBERTa's 89.5% performance
- 32.6-33.9% reduction in output tokens across three major LLM providers
- 100% API success rate maintained across all template types
- Cost reduction of 32.6-33.9% while maintaining task performance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Embedding Sufficiency for Template Classification
Pre-computed 1536D embeddings capture sufficient semantic information to classify queries into appropriate template categories, making complex feature engineering unnecessary. The MLP operates on frozen embeddings from text-embedding-3-small, applying nonlinear transformations to map semantic representations directly to 5 template classes.

### Mechanism 2: Dual-Layer Token Control via Soft-Hard Constraint Composition
Combining instruction-based style guidance with API-level hard caps produces predictable token reduction across heterogeneous LLM providers. System prompts shape response verbosity tendencies while hard caps enforce absolute ceilings per template.

### Mechanism 3: Provider-Agnostic Routing via Semantic Query Classification
Routing decisions based purely on query semantics generalize across LLM providers without retraining, because template appropriateness is query-intrinsic rather than model-dependent. The router is trained once on MMLU labels and applied identically to all providers.

## Foundational Learning

- **Embedding-based classification with frozen representations**: The MLP consumes pre-computed vectors rather than learning embeddings. If you switched embedding models, you'd need to retrain the MLP classifier since the semantic features would change.
- **Cost-constrained optimization with quality thresholds**: DTS formalizes the problem as minimizing expected cost subject to quality constraints. If quality threshold increases, the feasible template set shrinks and expected token savings decrease.
- **Generalization bounds and PAC learning**: Section 3.4 justifies the 11,100 sample size via VC dimension arguments. To strengthen provider-agnostic claims beyond testing on three providers, evaluation on diverse model tiers and specialized domains would be needed.

## Architecture Onboarding

- **Component map**: Query Input → Router Layer (MLP: Embedding API → 1536D vector → 3-layer MLP → softmax; RoBERTa: Tokenizer → 512 tokens → 125M param transformer → softmax) → Template Selection → 5 templates (minimal/standard/verbose/technical/executive) → LLM Backend (OpenAI/Gemini/Claude API calls) → Response + Token Count

- **Critical path**: 1) Query → Embedding extraction (5ms for MLP) or tokenization (50ms for RoBERTa) 2) Classification → confidence score; if < 0.3 threshold, fallback to verbose 3) Template application → system prompt injection + max_tokens parameter 4) API call → response generation 5) Token counting → savings calculation

- **Design tradeoffs**:
| Factor | MLP | RoBERTa |
|--------|-----|---------|
| Latency | ~5ms | ~50ms |
| Infrastructure | CPU only | GPU recommended |
| Privacy | Requires external embedding API | Fully offline |
| Cost/1M queries | ~$0.40 (embeddings) | ~$65.75 (GPU inference) |
| Accuracy | 90.5% | 89.5% |

- **Failure signatures**: Confidence score < 0.3 → triggers verbose fallback; Template mismatch → correct answer but wrong verbosity; Cap exhaustion → response truncated mid-sentence; Provider drift → token savings vary unexpectedly

- **First 3 experiments**:
  1. Baseline replication: Run always-verbose vs. DTS on 100 MMLU samples with GPT-4o-mini. Measure token reduction and verify routing accuracy.
  2. Confidence threshold sweep: Test θ_conf values [0.1, 0.2, 0.3, 0.4, 0.5] on held-out data. Plot accuracy vs. fallback rate.
  3. Domain shift probe: Evaluate router on non-MMLU data (customer support tickets, code documentation queries). Assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DTS routing generalize across languages and cultural contexts?
- Basis in paper: [explicit] "investigating DTS applicability beyond English-language contexts presents important challenges."
- Why unresolved: All experiments used English-language MMLU benchmark; no multilingual evaluation conducted.
- What evidence would resolve it: Evaluation on multilingual benchmarks (mMLU, XM3600) showing routing accuracy consistency across languages.

### Open Question 2
- Question: Can the router improve through online learning from production usage data?
- Basis in paper: [explicit] "Online learning would let the router keep improving from real usage data and user feedback."
- Why unresolved: Current router is trained once on static MMLU data; no adaptive learning mechanism implemented or tested.
- What evidence would resolve it: Deployment study showing sustained or improved routing accuracy over time with online learning.

### Open Question 3
- Question: Can the DTS framework extend to routing queries between different LLM models?
- Basis in paper: [explicit] "We could also expand this to route between different LLMs entirely, not just templates."
- Why unresolved: Current architecture only routes to templates within a single LLM provider; cross-model routing unexplored.
- What evidence would resolve it: Implementation and evaluation showing model-selection router achieving cost savings while maintaining task performance.

## Limitations
- Evaluation limited to MMLU benchmark data, which may not reflect real-world query distributions
- Quality measurement limited to MMLU correctness without assessing coherence, completeness, or user satisfaction
- Provider-specific behavior assumptions not rigorously tested across different model tiers or specialized domains

## Confidence

**High Confidence**: Token savings measurements and API success rates (empirically validated across 9,000 production API calls with consistent results)

**Medium Confidence**: Routing accuracy and generalization claims (demonstrated on MMLU data but limited scope and lack of domain diversity)

**Low Confidence**: Provider-agnostic behavior and cost optimization guarantees (theoretically plausible but not rigorously tested across different model tiers or specialized domains)

## Next Checks

1. **Domain shift validation**: Evaluate the router on non-MMLU data including customer support tickets, code documentation queries, and domain-specific technical questions. Measure routing accuracy and token savings across at least 5 different domain types.

2. **Cross-tier provider comparison**: Test the same routing system across different model tiers within each provider (e.g., GPT-3.5 vs GPT-4, Claude Instant vs Claude 2, Gemini Pro vs Ultra). Measure whether template selection remains optimal.

3. **Quality trade-off analysis with human evaluation**: Conduct human assessment of responses generated using different templates on the same queries. Measure user satisfaction, completeness, and correctness to quantify actual quality degradation.