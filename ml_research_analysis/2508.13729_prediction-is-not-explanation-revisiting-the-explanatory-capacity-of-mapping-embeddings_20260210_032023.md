---
ver: rpa2
title: 'Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping
  Embeddings'
arxiv_id: '2508.13729'
source_url: https://arxiv.org/abs/2508.13729
tags:
- norms
- features
- embeddings
- feature
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines property inference methods for explaining
  word embeddings, challenging the common assumption that accurate prediction of semantic
  features implies genuine feature-based interpretability. The authors apply Partial
  Least Squares Regression and Feed Forward Neural Networks to map BERT embeddings
  to three feature norms (McRae, Buchanan, Binder) and conduct extensive ablation
  experiments.
---

# Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings

## Quick Facts
- arXiv ID: 2508.13729
- Source URL: https://arxiv.org/abs/2508.13729
- Reference count: 40
- Primary result: High prediction accuracy in mapping embeddings to feature norms reflects geometric similarity and methodological artifacts rather than genuine semantic interpretability.

## Executive Summary
This paper critically examines property inference methods for explaining word embeddings, challenging the common assumption that accurate prediction of semantic features implies genuine feature-based interpretability. The authors apply Partial Least Squares Regression and Feed Forward Neural Networks to map BERT embeddings to three feature norms (McRae, Buchanan, Binder) and conduct extensive ablation experiments. They demonstrate that prediction accuracy is predominantly determined by algorithmic upper bounds and geometric similarity rather than meaningful semantic representation. The results show that high prediction accuracy does not reliably indicate information overlap, and that random or nonsensical features can be predicted to some degree. The study concludes that mapping embeddings primarily reflects geometric similarity within vector spaces rather than the emergence of semantic properties, calling into question the explanatory capacity of such methods.

## Method Summary
The authors evaluate property inference methods by mapping BERT-base-uncased layer 0 embeddings (768-dim) to three feature norms: McRae (541 concepts × 2526 features, categorical), Buchanan (4436 concepts × 3981 features, categorical), and Binder (535 concepts × 62 features, continuous ratings). They use Partial Least Squares Regression (PLSR) and Feedforward Neural Networks (FFNN) with hyperparameter tuning via 10-fold cross-validation and MSE-based selection of latent dimensionality. The evaluation includes standard metrics (F1@10 for sparse norms, Spearman's ρ for dense norms), geometric similarity measures (Neighborhood Accuracy@10), and ablation baselines (Rand, Shuffle, CDiff, Upper Bound). The study compares prediction accuracy against theoretical upper bounds calculated by mapping each norm to itself.

## Key Results
- Prediction accuracy for McRae and Buchanan is statistically indistinguishable from their respective upper bounds (0.25 vs 0.27 F1), suggesting dataset structure limits performance rather than embedding quality.
- Neighborhood accuracy (geometric similarity) remains high even when specific feature prediction accuracy is low, indicating the methods capture relational structure rather than semantic content.
- Random and nonsensical features (CDiff: character length difference) can be predicted with accuracy comparable to real semantic features, demonstrating metric vulnerability to artifacts.
- Dense norms (Binder) show spurious high correlation on nonsensical rankings, further undermining claims of semantic interpretability.

## Why This Works (Mechanism)

### Mechanism 1: Upper Bound Constraint on Predictability
If a property inference model achieves high accuracy, it may reflect the methodological upper bound of the dataset rather than genuine semantic knowledge in the embeddings. The paper demonstrates that the maximum possible prediction accuracy is calculated by mapping the target matrix to itself (PLSR(Y, Y)). For sparse norms, this upper bound is surprisingly low (e.g., F1=0.27 for McRae). Since standard mapping performance (F1=0.25) is statistically indistinguishable from this bound, the results suggest the limiting factor is the dataset's geometry, not the embedding's information content.

### Mechanism 2: Geometric Similarity vs. Feature Knowledge
Mapping embeddings to feature norms primarily measures global geometric similarity (neighborhood structures) rather than the emergence of specific local semantic properties. The authors assessed "Neighborhood Accuracy" (retaining nearest neighbors in vector space) alongside feature accuracy. While specific feature prediction is noisy and sensitive to artifacts, the geometric similarity between the source (embeddings) and target (norms) spaces remains a recoverable signal.

### Mechanism 3: Artifact Sensitivity (Sparsity and Randomness)
Prediction metrics (like F1 or Spearman's ρ) are vulnerable to dataset artifacts, allowing models to "predict" random or nonsensical features if the matrix structure permits. The paper uses ablation studies (Shuffle, CDiff). "Shuffle" retains sparsity but randomizes features; "CDiff" replaces values with character counts. High performance on these nonsense targets indicates the regression method exploits matrix structure (e.g., density patterns) rather than semantic meaning.

## Foundational Learning

- **Concept: Property Inference (Linear Mapping)**
  - **Why needed here:** This is the standard technique the paper critiques. It involves training a regressor (e.g., PLSR) to project embedding vectors onto human-annotated feature vectors.
  - **Quick check question:** Can you explain why mapping a matrix to itself (Y → Y) serves as an "upper bound" for this technique?

- **Concept: Data Sparsity in Feature Norms**
  - **Why needed here:** The paper attributes much of the misleading performance to the sparse nature of datasets like McRae (where most entries are zero).
  - **Quick check question:** How does the "Shuffle" baseline isolate the effect of sparsity from the effect of semantic content?

- **Concept: Geometric Similarity (Neighborhood Accuracy)**
  - **Why needed here:** The paper distinguishes between knowing a "property" (is it a bird?) vs. knowing a "neighbor" (is this vector close to other animals?).
  - **Quick check question:** Why might a model preserve the neighbors of a concept vector while failing to predict the specific features of that concept?

## Architecture Onboarding

- **Component map:** BERT embeddings (X) → Mapper (PLSR/FFNN) → Feature norms (Y) → Evaluator (F1/ρ/NA)
- **Critical path:**
  1. Extract embeddings for concepts.
  2. **Hyperparameter Tuning (Crucial):** Use MSE on test data to find the optimal latent dimensionality k.
  3. Train Mapper (X → Y).
  4. Run Baselines (Upper Bound, Shuffle, CDiff) to validate if results exceed artifacts.
- **Design tradeoffs:**
  - PLSR vs. FFNN: The paper finds them equivalent if hyperparameters are tuned correctly. PLSR is preferred for interpretability of the latent space, but FFNNs offer equivalent performance.
  - F1 vs. Correlation: F1 is unsuitable for dense norms; Correlation is misleading for small dense norms with non-distinct rankings.
- **Failure signatures:**
  - Overfitting: Large gap between Training and Test MSE (indicates k is too high).
  - Artifact Exploit: High prediction accuracy on "CDiff" or "Shuffle" baselines (indicates the model is learning matrix structure, not semantics).
  - Upper Bound Saturation: System performance ≈ Upper Bound performance (indicates the dataset is the bottleneck, not the embedding).
- **First 3 experiments:**
  1. **Tuning Validation:** Plot MSE vs. k for PLSR on McRae/Buchanan to identify the "elbow" and verify overfitting in high-k regimes.
  2. **Upper Bound Test:** Implement PLSR(Y, Y) for a chosen norm to establish the theoretical maximum accuracy for that specific dataset structure.
  3. **Nonsense Baseline:** Map embeddings to the "CDiff" (Character Difference) matrix on the Binder dataset to observe if correlation scores (ρ) remain misleadingly high.

## Open Questions the Paper Calls Out

### Open Question 1
How do different degrees of sparsity and geometric structures in feature norms quantitatively influence the explanatory potential of property inference methods? The authors state in the Discussion that "Relating those metrics to explanatory potential is an important subject of future work" and in the Conclusion that future work should analyze "its dependence on sparsity, geometric structures, properties of scores."

### Open Question 2
Under what specific conditions, if any, can property inference methods provide faithful explanations of emergent property knowledge rather than merely capturing geometric similarity? The Discussion states: "We do not give a complete account of what else could be explained to what degree, and under which circumstances emerging property knowledge could be analyzed with inference methods. This will be subject to a range of future analyses."

### Open Question 3
Do the limitations of property inference methods generalize to contextualized embeddings from transformer layers beyond layer 0 and to embedding models other than BERT? The paper acknowledges using only "layer 0 of BERT embeddings" and states their "core assertions are more related to the limits of the feature norms and the methods than to the source embeddings," but this claim remains empirically unverified across diverse embedding architectures.

## Limitations

- The paper's core claim about geometric similarity versus semantic knowledge depends on the assumption that "matrix-to-self" upper bounds accurately capture dataset limitations, though sensitivity to preprocessing choices remains unclear.
- Alternative explanations (e.g., different model architectures or evaluation metrics) were not exhaustively explored, leaving open the possibility that modified approaches might overcome identified limitations.
- The practical recommendation against using property inference for interpretability may be overly pessimistic if future work develops more robust evaluation frameworks.

## Confidence

- **High Confidence:** The experimental finding that prediction accuracy correlates strongly with algorithmic upper bounds and geometric similarity measures. The ablation studies (Shuffle, CDiff) provide robust evidence that current metrics are artifact-sensitive.
- **Medium Confidence:** The broader theoretical claim that property inference methods fundamentally cannot distinguish between semantic knowledge and geometric similarity. While supported by evidence, alternative explanations were not exhaustively explored.
- **Medium Confidence:** The practical recommendation against using property inference for interpretability. This follows logically from the empirical results but may be overly pessimistic if future work develops more robust evaluation frameworks.

## Next Checks

1. **Upper Bound Sensitivity Analysis:** Systematically vary preprocessing parameters (normalization, filtering) for McRae/Buchanan/Binder and measure how upper bounds change. This would test whether the bounds reflect dataset structure or arbitrary preprocessing choices.
2. **Alternative Geometric Measures:** Implement and compare alternative geometric similarity metrics (e.g., Procrustes distance, cosine similarity on reduced dimensions) to verify that PLSR/FFNN mappings capture all recoverable geometric information.
3. **Robustness to Semantic Perturbations:** Create controlled perturbations of the feature norms that preserve geometric structure but introduce semantic noise (e.g., systematically swapping related features) and measure whether mapping accuracy degrades more than neighborhood accuracy.