---
ver: rpa2
title: 'Beyond Black-Box Benchmarking: Observability, Analytics, and Optimization
  of Agentic Systems'
arxiv_id: '2503.06745'
source_url: https://arxiv.org/abs/2503.06745
tags:
- agentic
- systems
- system
- analytics
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies critical gaps in agentic system evaluation,
  demonstrating that traditional benchmarking fails to capture non-deterministic execution
  flows and performance variability. Through empirical experiments on a calculator
  agentic system and a user study of 38 practitioners, the authors show that identical
  inputs can produce divergent execution paths and outputs (mean CV 63% in execution
  flow, 19% in accuracy for natural language inputs).
---

# Beyond Black-Box Benchmarking: Observability, Analytics, and Optimization of Agentic Systems

## Quick Facts
- arXiv ID: 2503.06745
- Source URL: https://arxiv.org/abs/2503.06745
- Reference count: 29
- This paper identifies critical gaps in agentic system evaluation, demonstrating that traditional benchmarking fails to capture non-deterministic execution flows and performance variability.

## Executive Summary
This paper addresses the fundamental challenge of evaluating agentic systems that exhibit non-deterministic behavior across repeated executions. Traditional black-box benchmarking that focuses solely on final outputs misses critical internal variations in execution flows, task decomposition, and inter-agent interactions. Through empirical experiments on a calculator agentic system and a user study of 38 practitioners, the authors demonstrate significant variability in execution paths (mean CV 63%) and outputs (19% CV for natural language inputs). They propose a novel Agentic System Behavioral Benchmarking approach that extends observability frameworks with GenAI Events to capture lifecycle and state changes, and introduce ABBench - a benchmark dataset of 30 structured logs with ground truth analytics outcomes.

## Method Summary
The authors developed a behavioral benchmarking methodology for agentic systems using a LangGraph-based calculator with decomposition, parallel execution, and validation. They extended OpenTelemetry with GenAI Events to capture lifecycle transitions for agentic entities, and implemented task flow discovery using hierarchical directed acyclic graphs to enable comparative analysis across runs. The approach was validated using ABBench, a dataset of 30 structured logs with ground truth analytics outcomes, and tested with TAMAS, a proprietary analytics system that correctly identified issues in 60% of benchmark cases. Experiments involved running 50 calculator examples (5 repetitions each) across two datasets (Pure Math and Pure NL) while measuring accuracy (MSE), cost, execution time, LLM calls, and flow variability using Graph Edit Distance.

## Key Results
- Identical inputs produce divergent execution paths with mean CV of 63% in flow variability
- Natural language inputs show 19% CV in accuracy across repeated runs
- Only 16% of practitioners trust automated interventions in agentic systems
- TAMAS correctly identified issues in 60% of benchmark cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral benchmarking may expose execution path variability that black-box outcome evaluation misses.
- Mechanism: Rather than evaluating only final outputs, behavioral benchmarking captures execution traces, task decomposition patterns, and inter-agent interactions. The paper reports mean coefficient of variation of 63% in execution flow and 19% in accuracy for natural language inputs across identical queries, suggesting outcome-only metrics can mask significant internal variation.
- Core assumption: Execution path divergence correlates with system reliability issues that practitioners need to diagnose.
- Evidence anchors:
  - [abstract]: "identical inputs can produce divergent execution paths and outputs (mean CV 63% in execution flow, 19% in accuracy for natural language inputs)"
  - [Section 3.3.1]: "results for the 'pure math' dataset exhibit a mean CV of 63% across all examples"
  - [corpus]: LumiMAS paper confirms monitoring challenges in multi-agent systems but does not validate this specific variability metric.
- Break condition: If execution path variability does not meaningfully correlate with failure modes or user-impacting outcomes, behavioral benchmarking adds complexity without diagnostic value.

### Mechanism 2
- Claim: Extending OpenTelemetry with GenAI Events could standardize observability for agentic systems.
- Mechanism: The paper proposes GenAI Events that bind to spans, capturing lifecycle transitions (creation, update, start, end, suspension, abortion, failure, deletion) for agentic entities. This extends OTeL's existing span-based tracing without requiring a new observability infrastructure.
- Core assumption: Practitioners will adopt manual instrumentation for deeper behavioral insights.
- Evidence anchors:
  - [Section 4.3]: "This aims to extend OTeL's semantic conventions for GenAI Agentic Systems by introducing GenAI Events"
  - [Section 4.3]: "This approach supports manual instrumentation by developers, ensuring deeper insights"
  - [corpus]: No corpus papers validate OTeL extension adoption; corpus signals weak on this mechanism.
- Break condition: If instrumentation overhead exceeds practitioner tolerance (survey suggests 60% find current tools insufficient), adoption will remain limited.

### Mechanism 3
- Claim: Task flow discovery using hierarchical directed acyclic graphs may enable comparative analysis across runs.
- Mechanism: Task flows decompose execution into parent-child relationships with attached metrics (LLM calls, token usage, failure rates). Graph-edit distance quantifies structural differences between flows, enabling automated comparison across traces.
- Core assumption: Graph-based representation captures the failure-relevant structure of agentic execution.
- Evidence anchors:
  - [Section 4.4]: "task flow analysis represents the hierarchical breakdown of tasks into subtasks, structured as a directed acyclic graph"
  - [Section 3.3]: "Variability between multiple executions of the same input was quantified using the mean graph-edit distance"
  - [corpus]: CREW-WILDFIRE benchmarks multi-agent collaboration but does not validate graph-based flow comparison.
- Break condition: If task flows cannot reliably distinguish intended behavioral variation from problematic divergence, comparison metrics become noisy.

## Foundational Learning

- Concept: **Coefficient of Variation (CV)**
  - Why needed here: The paper uses CV to quantify variability across repeated runs of identical inputs. Understanding that CV = standard deviation / mean helps interpret the 63% flow variability claim.
  - Quick check question: If execution time has mean 10s and CV 45%, what is the standard deviation? (Answer: 4.5s)

- Concept: **OpenTelemetry Spans and Traces**
  - Why needed here: The proposed GenAI Events build on OTeL's span model. Without understanding parent-child span relationships, the extension mechanism is opaque.
  - Quick check question: How does a trace differ from a span? (Answer: A trace is the complete request lifecycle; spans are individual operations within it.)

- Concept: **Graph-Edit Distance**
  - Why needed here: Used to quantify execution flow differences. Understanding that GED counts node/edge insertions, deletions, and substitutions enables interpretation of variability metrics.
  - Quick check question: If two task flows differ by one additional subtask and one reordered edge, what is the minimum GED? (Answer: At least 2)

## Architecture Onboarding

- Component map:
  - Resources: Inputs/outputs (text, images, templates, databases)
  - Tools: Programmatic units invoked by agents (LLM inference, vector DB, human-in-the-loop)
  - Workflows: Structured graphs coordinating tool execution (support sequencing, conditionals, forks)
  - Tasks: Discrete work units with metrics attached
  - Agents: Autonomous entities using tools/workflows with memory
  - Organizations: Multi-agent structures with defined roles

- Critical path: Input → Task Decomposition → Workflow Generation → Tool Execution → Validation → Output. The calculator example shows failures can occur at decomposition, input parsing, or validation stages.

- Design tradeoffs:
  - **Enhanced monitoring vs. overhead**: Detailed logging impacts latency and cost; paper recommends selective activation
  - **Automation vs. trust**: Only 16% of practitioners trust automated interventions; manual approval gates recommended
  - **Behavioral depth vs. standardization**: Richer instrumentation improves diagnostics but reduces cross-framework portability

- Failure signatures:
  - **Instruction violation**: Task decomposition ignores prompt constraints (detected in benchmark)
  - **Incorrect input failures**: Parsing/syntax errors propagate through workflow
  - **Validation failures**: Correct intermediate results fail validation checks
  - **Validator failures**: False negatives from validation logic (TAMAS cannot detect these)

- First 3 experiments:
  1. Run the calculator system on 5 identical inputs 5 times each; compute CV for execution time, cost, and LLM calls to establish baseline variability.
  2. Enable detailed logging on one failure mode (e.g., instruction violation) to validate that GenAI Event capture identifies the failure stage.
  3. Compare TAMAS output against ABBench ground truth on 10 logs to identify which failure types your implementation can/cannot detect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the four failure types identified in the calculator benchmark (instruction violation, incorrect input, validation, and validator failures) generalize to agentic systems in other domains, or do different system architectures require distinct failure taxonomies?
- Basis in paper: [explicit] The authors state they "plan to expand [the ABBench dataset] by logs from other agentic systems in the future" and currently categorize failures based solely on the calculator system.
- Why unresolved: The current dataset is restricted to a specific calculator agentic system; the applicability of this specific taxonomy to domains like software engineering or data analysis remains untested.
- What evidence would resolve it: An extension of the ABBench dataset with logs from diverse agentic applications showing similar failure distributions, or the identification of novel, domain-specific failure categories.

### Open Question 2
- Question: Can automated interventional analytics significantly reduce the high coefficient of variation (CV) observed in execution flows (mean 63%) without negatively impacting task accuracy or increasing operational costs?
- Basis in paper: [explicit] The Discussion section calls for "developing automated techniques... to mitigate variability" and exploring "adaptive optimization strategies" to improve robustness in future work.
- Why unresolved: The paper quantifies the high variability but does not demonstrate a concrete solution or "interventional analytics" method that successfully reduces this variability while preserving the system's functional success rate.
- What evidence would resolve it: Experimental results showing a statistically significant reduction in mean CV for execution flow and latency across repeated trials following the application of automated interventions.

### Open Question 3
- Question: How can agent analytics systems be enhanced to reliably detect "validator failures" (where correct agent actions are erroneously rejected) and input syntax errors, which the current TAMAS implementation fails to identify?
- Basis in paper: [explicit] The authors note that TAMAS "cannot detect the validator failures" and "does not provide correct report for logs with syntax errors," limiting its alignment with ground truth to 60%.
- Why unresolved: Distinguishing between a genuine agent error and a "validator failure" (a false negative by the system's own validation module) requires semantic analysis or observability data not currently captured or analyzed by the proposed method.
- What evidence would resolve it: An updated analytics algorithm capable of cross-referencing validation rejections with ground truth correctness, successfully identifying and flagging instances where the validator rejected a correct action.

## Limitations
- The generalizability of the behavioral benchmarking approach beyond calculator systems remains untested
- The 60% TAMAS accuracy on ABBench may reflect dataset bias rather than robust performance
- Practitioner survey findings rely on self-reported tool preferences rather than observed usage patterns

## Confidence

- **High confidence**: Non-deterministic execution variability in agentic systems (63% CV in flow, 19% in accuracy) - directly measured from controlled experiments
- **Medium confidence**: GenAI Events extension to OTeL provides sufficient behavioral observability - theoretically sound but lacks practitioner adoption evidence
- **Low confidence**: ABBench represents a comprehensive benchmark for agentic system evaluation - limited sample size and narrow domain coverage constrain external validity

## Next Checks

1. Reproduce CV measurements across three distinct agentic system types (calculator, code generation, and text summarization) to test domain generalizability
2. Conduct A/B testing where practitioners use behavioral vs. outcome-only metrics to diagnose failures, measuring diagnostic accuracy and time-to-resolution
3. Expand ABBench to 100+ logs spanning multiple failure modes and system architectures, then re-evaluate TAMAS performance to assess robustness