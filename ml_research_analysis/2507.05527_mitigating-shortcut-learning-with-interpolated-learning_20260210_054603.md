---
ver: rpa2
title: Mitigating Shortcut Learning with InterpoLated Learning
arxiv_id: '2507.05527'
source_url: https://arxiv.org/abs/2507.05527
tags:
- interpoll
- minority
- examples
- pages
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InterpoLL addresses shortcut learning in NLP by interpolating representations
  of majority and intra-class minority examples to weaken shortcut influence and improve
  minority generalization. The method identifies majority/minority examples via an
  under-parameterized auxiliary model, then interpolates their representations during
  training, with interpolation ratios sampled from Uniform(0, 0.5) to avoid substantial
  alterations.
---

# Mitigating Shortcut Learning with InterpoLated Learning

## Quick Facts
- **arXiv ID**: 2507.05527
- **Source URL**: https://arxiv.org/abs/2507.05527
- **Reference count**: 36
- **Primary result**: InterpoLL improves minority generalization by 3.9-4.2% over ERM across six NLU datasets and three model architectures while maintaining majority accuracy.

## Executive Summary
InterpoLL addresses shortcut learning in NLP by interpolating representations of majority and intra-class minority examples during training. The method identifies majority examples (correctly classified by a weak auxiliary model) and replaces their representations with weighted combinations of themselves and minority examples (misclassified by the auxiliary) from the same class. This forces the model to learn features predictive across both shortcut-reliant and robust examples, reducing spurious correlation dependence. Evaluated across six English NLU datasets and three model architectures (BERT, T5, GPT-2), InterpoLL consistently improves minority generalization by 3.9-4.2% over ERM and state-of-the-art methods while maintaining majority accuracy, demonstrating robustness to noisy labels and effectiveness across model scales.

## Method Summary
InterpoLL mitigates shortcut learning by interpolating representations during training. First, an under-parameterized auxiliary model (TinyBERT) is trained with ERM to identify majority examples (correctly classified) and minority examples (misclassified). During main model training, for each majority example, InterpoLL samples an intra-class minority example and replaces the majority's representation with a linear combination: z_interp = (1-λ)z_i + λz_j where λ ~ Uniform(0, 0.5). The CLS token representation from the final encoder layer is used for interpolation. Minority examples use their original representations. This process injects shortcut-mitigating features into majority examples while preserving semantic validity, forcing the classifier to find robust features predictive across both groups.

## Key Results
- Improves minority generalization by 3.9-4.2% over ERM and state-of-the-art methods
- Maintains majority accuracy while improving minority performance
- Demonstrates effectiveness across six datasets, three model architectures, and scales from BERT-tiny to T5-3B
- Reduces shortcut extractability from representations while showing robustness to noisy labels

## Why This Works (Mechanism)

### Mechanism 1
Interpolating majority representations with intra-class minority representations reduces reliance on spurious correlations. For a majority example x_i, InterpoLL replaces its representation z_i with z_interp = (1-λ)z_i + λz_j, where z_j is a minority example's representation from the same class. This injects shortcut-mitigating features into the majority representation, forcing the classifier to find features predictive across both groups. The method assumes minority examples contain true label features negatively correlated with shortcut features in majority distributions.

### Mechanism 2
An under-parameterized auxiliary model effectively identifies shortcut-reliant examples without manual annotations. A small model (TinyBERT) trained with ERM overfits to easy superficial patterns, correctly classifying majority examples that fit shortcuts but misclassifying minority examples requiring robust reasoning. These misclassifications serve as a proxy for the "minority" group. The core assumption is that shortcut features are easier to learn than core task features, so small models learn them preferentially and fail on non-shortcut examples.

### Mechanism 3
Constraining λ ∈ U(0, 0.5) preserves semantic validity while sufficiently perturbing shortcut features. The constraint prevents the interpolated representation from drifting too far into the minority manifold, acting as a regularizer that perturbs the "easy" path just enough to break spurious correlation without losing core semantic signal. The assumption is that shortcut features are non-robust and can be disrupted by small linear perturbations in representation space.

## Foundational Learning

**Spurious Correlations (Shortcuts)**: The entire method relies on distinguishing "shortcuts" (e.g., high lexical overlap) from "core features" (e.g., logical entailment). Without this distinction, the interpolation logic is meaningless. *Quick check*: Can you explain why high word overlap between premise and hypothesis is a "shortcut" in NLI rather than a valid signal?

**Manifold Mixup / Latent Interpolation**: InterpoLL operates on z = f_enc(x), not raw text. You must understand that linear combinations in latent space act as semantic blends/augmentations. *Quick check*: What happens if you interpolate between the latent vectors of a "cat" image and a "dog" image? (Answer: The decoder sees a hybrid).

**Group Robustness (GroupDRO/JTT)**: To position this work, you need to know the baseline struggle: standard ERM optimizes average accuracy by sacrificing performance on small subgroups (minorities). *Quick check*: Why does minimizing average loss often hurt minority group performance?

## Architecture Onboarding

**Component map**: Auxiliary Model (TinyBERT) -> Majority/Minority Classification -> Learner Model (BERT/T5) with Interpolation Hook -> Classification Head

**Critical path**: 1) Pre-processing: Run auxiliary model on training set, save misclassified examples as minority group. 2) Training Step: Sample batch, identify majority examples, sample matching intra-class minority examples. 3) Interpolation: Replace majority representations with interpolated versions. 4) Loss: Compute cross-entropy using interpolated representations for majority, original for minority.

**Design tradeoffs**: Lambda range U(0, 0.5) is safer for ID accuracy; U(0, 1) increases OOD robustness but risks ID collapse. Auxiliary size: smaller is better for sharper shortcut detection. Architecture agnostic: claims work on Encoder-only, Encoder-Decoder, and Decoder-only.

**Failure signatures**: ID Accuracy Collapse: if λ too high or auxiliary model fails to identify shortcuts, model trains on noise. No Improvement over ERM: suggests auxiliary model too capable or dataset lacks distinct shortcut patterns.

**First 3 experiments**: 1) Auxiliary Sensitivity: Compare TinyBERT vs BERT-base on MNLI (ID vs OOD HANS accuracy). 2) Lambda Sweep: Grid search on λ distributions to plot ID/OOD tradeoff. 3) Layer Probing: Verify InterpoLL reduces shortcut extractability in hidden states vs ERM.

## Open Questions the Paper Calls Out

**Cross-Lingual Applicability**: How effective is InterpoLL in multilingual settings where spurious correlation structures may differ significantly from English NLU classification tasks? The paper's experiments focus on English-language tasks, leaving broader applicability unexplored.

**Impact on Non-Shortcut Representations**: Does targeted suppression of shortcut features inadvertently degrade the model's ability to capture other distinct linguistic properties (e.g., syntax, semantics) within learned representations? The analysis does not examine whether suppression impacts other aspects of learned representations.

**Scaling to Larger Models**: How effective is InterpoLL for models significantly larger than 3B parameters? While results show consistent gains across scales, effectiveness for modern LLMs (7B+ parameters) that may handle distribution shifts differently remains unverified.

## Limitations

- **Auxiliary Model Dependency**: Effectiveness depends on correctly identifying shortcut-using examples through auxiliary model misclassification, creating potential circular dependency if the auxiliary fails to identify true shortcut groups.
- **Noisy Label Testing**: Claims of noise robustness are based on synthetic label noise on only three datasets with mild 10% noise settings, not real-world noisy label scenarios.
- **Computational Overhead**: Requires running auxiliary model over entire training set for preprocessing, which could become a bottleneck for very large datasets.

## Confidence

**High Confidence**: Core mechanism of interpolating majority representations with intra-class minority representations to reduce shortcut reliance is well-supported by consistent experimental results across six datasets and three model architectures.

**Medium Confidence**: Claims of "architecture agnostic" effectiveness and "works across scales" are supported by experiments but underlying reasons for uniform effectiveness across diverse architectures aren't fully explained.

**Medium Confidence**: Noise robustness claims are supported by synthetic experiments but real-world noise patterns could be more complex, and the paper doesn't test higher noise rates (15-30%) or naturally occurring noisy datasets.

## Next Checks

1. **Auxiliary Model Capacity Sensitivity**: Systematically vary auxiliary model size from TinyBERT to BERT-base across multiple datasets to identify optimal under-parameterization threshold and failure points when auxiliary becomes too strong or too weak.

2. **Real-World Noisy Label Testing**: Apply InterpoLL to naturally occurring noisy label datasets with inherent annotation errors and higher noise rates (15-30%) with different noise patterns (uniform vs class-conditional) to validate robustness claims under realistic conditions.

3. **Cross-Dataset Shortcut Transferability**: Train auxiliary model on one dataset with known shortcuts, then use its classifications to train main model on a different dataset to test whether shortcut patterns are transferable across domains and whether the method captures universal shortcut characteristics.