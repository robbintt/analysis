---
ver: rpa2
title: End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled
  Data
arxiv_id: '2506.16251'
source_url: https://arxiv.org/abs/2506.16251
tags:
- speech
- data
- translation
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of developing end-to-end speech-to-text
  translation (ST) systems for low-resource languages, which suffer from a lack of
  high-quality annotated data. The authors propose using weakly labeled data automatically
  mined from multilingual speech corpora to train ST models.
---

# End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data

## Quick Facts
- arXiv ID: 2506.16251
- Source URL: https://arxiv.org/abs/2506.16251
- Reference count: 0
- Primary result: Weakly labeled data from multilingual corpora enables competitive ST for low-resource Indian languages, with larger noisier datasets outperforming smaller cleaner ones.

## Executive Summary
This paper addresses the challenge of developing end-to-end speech-to-text translation (ST) systems for low-resource languages by leveraging weakly labeled data automatically mined from multilingual speech corpora. The authors created a new dataset, Shrutilipi-anuvaad, by mining parallel text and speech pairs from the Shrutilipi corpus for four Indian language pairs (bn-hi, ml-hi, or-hi, te-hi) using similarity scores to curate training, development, and test sets of varying quality. Their approach demonstrates that ST models trained on larger, lower-quality datasets outperform those trained on smaller, higher-quality data, and that pre-trained bilingual ASR models significantly boost performance. The best systems achieve BLEU scores up to 32.6, outperforming baseline multilingual models in several directions.

## Method Summary
The authors mine parallel data from the Shrutilipi corpus using SONAR sentence embeddings and cosine similarity thresholds (0.5-0.8 for training, >0.8 for dev/test). They create quality-stratified training splits (S1-S5) and pre-train bilingual ASR models using joint CTC/attention architecture. ST models are initialized from pre-trained ASR and fine-tuned on each split independently. The approach uses ESPnet with transformer architectures, speed perturbation for data augmentation, and evaluates using BLEU and chrF2 metrics with 95% confidence intervals.

## Key Results
- ST models trained on larger, lower-quality datasets (S1) consistently outperform those trained on smaller, higher-quality data (S5) across 7/8 language directions
- Pre-trained bilingual ASR models boost ST performance by 10-20 BLEU points compared to training from scratch
- Best systems achieve BLEU scores up to 32.6 (bn→hi) and 32.6 (or→hi), outperforming SONAR and SeamlessM4T in several directions
- hi→xx directions consistently underperform xx→hi due to weaker source language representations

## Why This Works (Mechanism)

### Mechanism 1
Larger volumes of weakly labeled data outperform smaller high-quality datasets for ST training. Noisy parallel pairs mined via semantic similarity provide sufficient signal at scale while the model learns to be robust to label noise. Noise in weak labels is approximately random/benign rather than systematically misleading.

### Mechanism 2
Transfer learning from bilingual ASR provides strong initialization for low-resource ST. Pre-trained ASR encoder learns language-agnostic acoustic representations; fine-tuning on ST adapts decoder to translation while retaining acoustic priors. Source-target language pairs share sufficient phonetic/structural overlap.

### Mechanism 3
Bitext mining via sentence encoders can automatically curate pseudo-parallel ST data. SONAR projects multilingual text into shared embedding space; high cosine similarity indicates semantic equivalence across languages. Broadcast news across languages covers overlapping events, enabling parallel alignment.

## Foundational Learning

**Sentence embeddings and semantic similarity**: Core to mining parallel pairs from monolingual corpora using cosine similarity thresholds. *Quick check*: Can you explain why cosine similarity >0.8 is used for dev/test vs. 0.5–0.8 for training?

**Joint CTC/attention architecture for sequence transduction**: The ASR and ST models use this hybrid objective; CTC regularizes alignment learning. *Quick check*: What role does CTC weight (0.3) play during training and decoding?

**Transfer learning from ASR to ST**: ST is initialized from ASR encoder-decoder; only target-language layers are retained. *Quick check*: Why are source-language CTC, embedding, and output layers discarded for ST?

## Architecture Onboarding

**Component map**: Shrutilipi corpus → SONAR encoder → similarity bucketing → S1–S5 + dev/test splits → Bilingual ASR → ST model → BLEU/chrF2 evaluation

**Critical path**: 1. Mine bitext using SONAR embeddings; create quality-stratified splits. 2. Train bilingual ASR on all available speech-text pairs. 3. Initialize ST from ASR; fine-tune on S1–S5; evaluate BLEU/chrF2.

**Design tradeoffs**: S1 vs. S5: Quantity vs. quality; paper finds S1 superior except hi-or shows diminishing returns. ASR language balance: Hindi WER (7.9) far lower than regional languages (19–22%) due to data imbalance. Baseline fine-tuning: Seamless prefers S5; proposed model prefers S1.

**Failure signatures**: Low BLEU with high chrF2 suggests correct words but wrong ordering. hi→xx directions consistently underperform xx→hi due to weaker source language representations. Domain mismatch: All data from broadcast news; generalization to other domains untested.

**First 3 experiments**: 1. Replicate S1 vs. S5 comparison for one language pair; verify BLEU gap and statistical significance. 2. Ablate ASR pre-training: train ST from scratch on S1; expect 10–20 BLEU drop. 3. Vary similarity thresholds for S1 (e.g., 0.4–0.8) to test noise tolerance limits.

## Open Questions the Paper Calls Out

**Out-of-domain generalization**: How does performance degrade when applied to out-of-domain speech data? All datasets are from single-domain radio broadcasts, which may influence performance due to consistent speaking styles. Evaluation on diverse test sets like conversational speech or lectures would resolve this.

**Architecture response to noise**: Why do bilingual ASR-initialized models and massive multilingual models (e.g., SeamlessM4T) exhibit opposite preferences for data quality versus quantity? The paper documents this divergence but doesn't investigate underlying mechanisms. Comparative analysis of noise robustness in pre-trained multilingual versus bilingual encoder layers would help.

**Optimal similarity threshold**: Can a specific similarity score threshold be determined to optimize the trade-off between data quantity and noise? The authors observe hi-or performed worse on S1 vs S2, suggesting diminishing returns. Granular ablation studies mapping performance curves against continuous similarity thresholds would resolve this.

## Limitations
- Data quality vs. quantity tradeoff: Exact quality thresholds where diminishing returns begin remain unclear; relationship may be more complex than "more is better"
- ASR pre-training imbalance: Stark disparity between Hindi (7.9 WER) and regional languages (19-22% WER) raises questions about true language-pair balance
- Domain specificity: All evaluation data comes from All India Radio broadcast news; no testing on other domains like conversational speech

## Confidence

**High confidence** in: ASR pre-training significantly improves ST performance over training from scratch (10-20 BLEU point improvement is substantial and well-supported).

**Medium confidence** in: Larger, lower-quality datasets consistently outperform smaller, higher-quality ones. Generally supported but hi-or exception and lack of intermediate thresholds suggest need for more exploration.

**Low confidence** in: Comparative advantage over baseline systems across all directions. Mixed results—better in some directions, worse in others—without clear explanations for when and why the approach succeeds.

## Next Checks

1. **Quality threshold sensitivity**: Systematically vary similarity thresholds between S1 and S5 to identify optimal quality-quantity tradeoff point for each language pair, revealing whether S1 advantage is universal or pair-specific.

2. **Cross-domain generalization**: Evaluate best-performing ST models on held-out test set from different domain (e.g., conversational speech or different Indian language broadcast content) to assess whether weakly-labeled approach transfers beyond broadcast news.

3. **Target language representation analysis**: Replicate ASR pre-training with balanced Hindi-regional language data (equalizing WERs) to determine whether hi→xx performance gap is primarily due to Hindi's dominance in pre-training data or reflects deeper asymmetries in language pairs.