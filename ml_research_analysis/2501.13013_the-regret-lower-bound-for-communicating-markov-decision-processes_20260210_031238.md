---
ver: rpa2
title: The regret lower bound for communicating Markov Decision Processes
arxiv_id: '2501.13013'
source_url: https://arxiv.org/abs/2501.13013
tags:
- lower
- regret
- bound
- optimal
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes a regret lower bound for communicating
  Markov decision processes (MDPs) in the problem-dependent setting. The key insight
  is that consistent learners must exhibit three complementary behaviors: (1) exploration
  - gathering information in suboptimal regions; (2) co-exploration - over-visiting
  all optimal regions compared to suboptimal ones; and (3) second-order navigation
  constraints that govern how the learner moves through the environment.'
---

# The regret lower bound for communicating Markov Decision Processes

## Quick Facts
- **arXiv ID:** 2501.13013
- **Source URL:** https://arxiv.org/abs/2501.13013
- **Reference count:** 40
- **Primary result:** Establishes a computationally hard (Σ₂^P-complete) regret lower bound for communicating MDPs in the problem-dependent setting

## Executive Summary
This paper establishes a fundamental regret lower bound for communicating Markov decision processes in the problem-dependent setting. The bound captures three essential behaviors of consistent learners: exploration (visiting suboptimal pairs), co-exploration (over-visiting optimal regions), and navigation constraints (how the learner moves through the environment). The main result shows that computing this lower bound is Σ₂^P-complete in general, making it intractable for large MDPs. However, the paper provides constructive algorithms to approximate the bound in specific cases where local modifications to MDPs can create confusing instances, leading to computationally tractable approximations in many classical settings like multi-armed bandits and ergodic MDPs.

## Method Summary
The paper derives a regret lower bound for communicating MDPs through information-theoretic arguments. The bound is expressed as an optimization problem over exploration measures that must satisfy two types of constraints: invariant measure constraints (navigation constraints) and information constraints (ensuring confusing models can be rejected). The proof relies on change-of-measure arguments showing that consistent learners must visit pairs sufficiently often to distinguish the true MDP from alternatives where optimal policies become suboptimal. The bound is then analyzed for computational complexity, showing it's Σ₂^P-complete in general, and constructive approximations are provided for specific MDP structures.

## Key Results
- The regret lower bound K*(M) requires learners to exhibit exploration, co-exploration, and navigation constraints
- Computing the exact bound is Σ₂^P-complete, making it intractable for large MDPs
- For multi-armed bandits, the bound recovers the classical Lai-Robbins result: K*(M) = Σ_x Δ*(x)/kl(m(x)||max(m))
- For optimally recurrent models (e.g., ergodic MDPs), navigation constraints vanish and the bound simplifies to pair-wise decoupled constraints

## Why This Works (Mechanism)

### Mechanism 1: Mandatory Exploration Through Confusing Model Rejection
Consistent learners must visit suboptimal state-action pairs logarithmically often to statistically distinguish the true MDP from "confusing" alternatives. The paper proves via change-of-measure arguments that for any consistent learner and confusing model M†, the expected cumulative KL-divergence ∑NT(x)KLx(M||M†) must grow at least as log(T). This forces visits to pairs where M and M† differ—specifically, pairs outside X⋆(M) that could be optimal in alternative environments.

### Mechanism 2: Co-Exploration of Optimal Regions
Consistent learners must visit all optimal pairs (X⋆(M)) super-logarithmically often—strictly more than suboptimal pairs—even though these pairs incur zero Bellman gap. For any x⋆∈X⋆(M), if there exist arbitrarily close models M' where x⋆ becomes strictly better (higher reward), then the learner cannot distinguish M from M' without over-visiting x⋆.

### Mechanism 3: Second-Order Navigation Constraints
The visit measure μ (normalized visit counts at log(T) scale) must be an invariant measure of the MDP, not an arbitrary distribution over state-action pairs. In communicating MDPs, learners cannot teleport between pairs—they must navigate via the transition structure. The truncated visit vector converges to invariant measures of the MDP's minors, constraining which exploration strategies are physically realizable.

## Foundational Learning

- **Concept: Invariant Measures of Markov Chains**
  - **Why needed here:** The regret lower bound K*(M) is expressed as an optimization over μ∈Inv(M). Understanding flow conservation—∑p(s|x)μ(x) = ∑μ(s,a)—is essential to interpret why arbitrary exploration strategies are infeasible.
  - **Quick check question:** Given a 2-state MDP with deterministic transitions s₁→s₂ and s₂→s₁, what constraint does Inv(M) impose on μ(s₁,a₁) vs. μ(s₂,a₂)? (Answer: They must be equal.)

- **Concept: KL-Divergence and Change-of-Measure**
  - **Why needed here:** The information constraint inf[M†∈Cnf*(M)] ∑μ(x)KLx(M||M†) ≥ 1 is the core lower bound driver. The proof relies on Radon-Nikodym derivatives and log-likelihood ratios.
  - **Quick check question:** Why must the expected cumulative KL-divergence grow as log(T) rather than being constant? (Answer: To maintain distinguishability as T grows; see Proposition 4 proof using discriminating random variable U.)

- **Concept: Complexity Classes (Σ₂^P, coNP, NP-completeness)**
  - **Why needed here:** Theorem 14 proves CONFUSING-MODEL is NP-complete; Theorem 15 proves checking REGRET solutions is coNP-complete. Understanding that computing the exact bound is generally intractable informs algorithmic approximations.
  - **Quick check question:** Why does NP-hardness of CONFUSING-MODEL imply we cannot efficiently verify if a given exploration measure μ is "informative enough"? (Answer: Verification requires solving the dual—finding if any confusing model M† violates the constraint—which is NP-hard.)

## Architecture Onboarding

- **Component map:** Lower bound formula K*(M) = inf{∑μ(x)Δ*(x) : μ∈Inv(M), inf[M†∈Cnf*(M)] ∑μ(x)KLx(M||M†) ≥ 1}
- **Critical path:**
  1. Classify pairs: Compute X⋆(M), X⁻(M) using Bellman optimality equations
  2. Construct confusing models: For each π∉Π*(M), build M†∈Ben*(π,M) (Algorithm 1)
  3. Compute unlikelihood: For each π, compute U(π,μ,M) = inf[M†]∑μ(x)KLx(M||M†)
  4. Solve optimization: Find μ∈Inv(M) minimizing ∑μΔ* subject to U(π,μ,M)≥1 ∀π
- **Design tradeoffs:**
  - Exact vs. approximate bounds: Exact computation is Σ₂^P-complete; Section 7 proposes local modifications (Algorithm 1) for tractable approximations
  - Full policy enumeration vs. local neighborhoods: k-neighborhoods V*(k) with small k (e.g., k=1 for ergodic MDPs) yield efficient bounds; k=S (maximal) for NP-hard instances
  - Navigation-free simplification: For optimally recurrent models (Definition 9), drop navigation constraints entirely (Proposition 17)
- **Failure signatures:**
  - Infinite bound (K*(M)=∞): No exploration measure can reject all confusing models; typically indicates Cnf*(M)=∅ or structural degeneracy
  - Empty confusing set: If no M† exists with M†=M on X⋆(M) and disjoint optimal policies, the lower bound is trivial (zero regret achievable)
  - Navigation constraint infeasibility: For highly constrained transition structures, Inv(M) may be small, forcing high-regret exploration paths
- **First 3 experiments:**
  1. Validate on multi-armed bandits: Implement K*(M) computation for Bernoulli bandits; verify it recovers the Lai-Robbins bound (Corollary 16): K*(M) = ∑Δ*(x)/kl(m(x)||max(m))
  2. Test navigation constraints on deterministic MDP: Reproduce Figure 3 example (2-state MDP); compare bound with navigation constraints (≈2.6) vs. without (≈1.4) to quantify navigation penalty
  3. Approximate confusing model construction: Implement Algorithm 1 for a small communicating MDP (e.g., 5 states, 2 actions each); measure gap between approximate K*(M) via local modifications vs. brute-force policy enumeration over V*(1)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does there exist a consistent learning agent whose theoretical regret guarantees match the lower bound K*(M) for communicating MDPs? The paper conjectures such an agent exists but postpones algorithm construction to future work.

- **Open Question 2:** If the model class M is convex, does the REGRET problem remain computationally difficult? The reduction proving NP-hardness fails for convex model classes, leaving this as an open problem.

- **Open Question 3:** Is the second-order term in regret lower bounds positive (log log(T)) for Markov decision processes, unlike bandits where it is negative? The paper conjectures that co-exploration creates additional regret not present in bandit settings.

- **Open Question 4:** Can the best-of-both-worlds property (simultaneously optimal model-dependent and minimax regret) achieved in bandits be extended to MDPs? The paper conjectures that navigation constraints and co-exploration requirements may fundamentally conflict with minimax-optimal exploration strategies.

## Limitations

- The exact regret lower bound is Σ₂^P-complete to compute, making it intractable for large MDPs
- Approximation quality of local modification algorithms is not quantified
- Extension to continuous state spaces or infinite MDPs is not addressed
- Computational complexity depends heavily on the choice of neighborhood size V*(k), with limited guidance for choosing k

## Confidence

- **High:** Regret lower bound formula and its derivation via change-of-measure arguments
- **Medium:** Computational complexity results and approximation algorithms
- **Medium:** Special case results for multi-armed bandits and ergodic MDPs

## Next Checks

1. **Numerical validation:** Implement K*(M) computation for small communicating MDPs (3-5 states) and compare against empirical regret of optimal algorithms
2. **Approximation analysis:** Quantify the gap between bounds computed via local modifications versus brute-force policy enumeration for small MDPs
3. **Navigation constraint impact:** Systematically measure how navigation constraints affect the bound across MDPs with varying diameters and transition structures