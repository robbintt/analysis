---
ver: rpa2
title: 'HuAMR: A Hungarian AMR Parser and Dataset'
arxiv_id: '2502.20552'
source_url: https://arxiv.org/abs/2502.20552
tags:
- data
- language
- huamr
- hungarian
- silver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HuAMR, the first Abstract Meaning Representation
  (AMR) dataset for Hungarian. The authors translated the AMR 3.0 dataset into Hungarian
  and generated additional silver-standard AMR annotations using a Llama-3.1-70B model,
  creating HuAMR.
---

# HuAMR: A Hungarian AMR Parser and Dataset

## Quick Facts
- **arXiv ID**: 2502.20552
- **Source URL**: https://arxiv.org/abs/2502.20552
- **Reference count**: 7
- **Primary result**: HuAMR is the first Hungarian AMR dataset, created by translating AMR 3.0 and generating silver-standard annotations with Llama-3.1-70B, achieving up to 76.11 Smatch F1 on Hungarian news data.

## Executive Summary
This paper presents HuAMR, the first Hungarian Abstract Meaning Representation (AMR) dataset. The authors translated AMR 3.0 into Hungarian and generated additional silver-standard AMR annotations using a Llama-3.1-70B model, creating a dataset of 40k training and 3,811 test samples. They developed AMR parsers for Hungarian using mT5 Large and Llama-3.2-1B models, evaluating their performance with Smatch scores. The mT5 Large model consistently outperformed Llama-3.2-1B across all configurations. While additional silver data did not consistently boost overall performance, it effectively improved parsing accuracy on Hungarian news data.

## Method Summary
The authors created HuAMR by translating AMR 3.0 into Hungarian using DeepL and generating silver AMR annotations with a fine-tuned Llama-3.1-70B model on Hunsum-2 corpus data. They developed AMR parsers using mT5 Large and Llama-3.2-1B models, training them on combinations of gold AMRtrans data and silver HuAMR or Europarl data. Model training used LoRA fine-tuning for the generator and full fine-tuning for the parsers, with evaluation using Smatch F1 scores on both AMRtrans and HuAMR test sets.

## Key Results
- mT5 Large consistently outperformed Llama-3.2-1B in Smatch scores across all configurations
- Domain-matched silver data (HuAMR) improved parsing accuracy on Hungarian news data (76.11 F1 vs 69.32 baseline)
- Additional silver data did not consistently boost performance on gold-standard test sets
- Europarl silver data (520k instances) underperformed HuAMR silver data (40k instances) due to domain mismatch

## Why This Works (Mechanism)

### Mechanism 1: Domain-Matched Silver Data Improves In-Domain Parsing
- Claim: Silver-standard AMR annotations from a large LLM can enhance smaller model parsing accuracy when the silver data domain matches the evaluation domain.
- Mechanism: Llama-3.1-70B generates semantically structured AMR graphs for Hungarian news text; smaller models learn domain-specific graph patterns from this data, improving generalization on similar inputs.
- Core assumption: The silver annotations, while imperfect, contain sufficient structural regularities to guide learning without overwhelming the model with noise.
- Evidence anchors: HuAMR silver data improves mT5 Large from 69.32 to 76.11 Smatch F1 on HuAMR test set; Europarl silver data shows inconsistent or negative improvements on HuAMR test.

### Mechanism 2: Encoder-Decoder Architectures Outperform Decoder-Only for Structured AMR Generation
- Claim: Sequence-to-sequence models (mT5 Large) achieve higher Smatch scores than comparably-sized decoder-only models (Llama-3.2-1B) for AMR parsing in low-resource settings.
- Mechanism: The encoder-decoder structure provides separate encoding of input semantics and decoding of structured graph outputs, better aligning with the graph-to-text transformation required for PENMAN-serialized AMR.
- Core assumption: Architectural inductive bias contributes to parsing performance beyond parameter count.
- Evidence anchors: mT5 Large consistently outperforms Llama 3.2 1B across all configurations (72.90 vs 67.43 baseline on AMRtrans; 76.11 vs 73.03 best on HuAMR).

### Mechanism 3: Gold Data Quality Constrains Silver Data Effectiveness
- Claim: Performance saturation with silver data stems from annotation noise, not model capacity—gold-standard data continues to improve scores at scale.
- Mechanism: Silver AMR graphs contain structural errors (e.g., malformed 'and' operators, incomplete graphs) that limit learning signal; gold translations provide cleaner supervision.
- Core assumption: Validation filters do not eliminate all noise sources.
- Evidence anchors: 6,189 HuAMR instances discarded due to validation failures; control experiment shows ~5% higher Smatch F1 when trained on gold vs silver data.

## Foundational Learning

- Concept: **Abstract Meaning Representation (AMR)**
  - Why needed here: AMR is the target formalism—graph-based semantic representation with PENMAN serialization. Understanding nodes (concepts, PropBank frames), edges (semantic roles), and the cross-lingual constraint (English labels regardless of input language) is prerequisite to interpreting all results.
  - Quick check question: Given the PENMAN notation `(w / want-01 :ARG0 (b / boy))`, what does `:ARG0` represent and why is the concept `want-01` in English for Hungarian input?

- Concept: **Silver vs. Gold Annotations**
  - Why needed here: The paper's central intervention is generating silver AMR with Llama-3.1-70B and comparing it to gold-translated AMRtrans. Understanding this quality distinction explains why silver data saturates while gold continues to scale.
  - Quick check question: Why did the authors discard 6,189 HuAMR instances, and what does this imply about silver annotation reliability?

- Concept: **Smatch Evaluation Metric**
  - Why needed here: All performance claims use Smatch F1 scores, which measure triple overlap between predicted and reference AMR graphs. Without this, Table 2 results are uninterpretable.
  - Quick check question: If a model predicts all correct concepts but misses two edge types, would Smatch penalize this, and how severely?

## Architecture Onboarding

- Component map: AMR 3.0 (English) → DeepL translation → AMRtrans (gold Hungarian) → Llama-3.1-70B-LoRA fine-tuning → HuAMR generation → Validation filters → Training data

- Critical path: 1) Silver data quality → validated HuAMR (40k train, 3,811 test) 2) Model selection → mT5 Large preferred for structured output 3) Training data mixing → gold AMRtrans + domain-matched silver HuAMR 4) Domain alignment check → evaluate on both test sets to detect saturation

- Design tradeoffs:
  - Scale vs. quality: 520k Europarl silver instances underperform 40k HuAMR silver—domain match outweighs quantity
  - Architecture vs. accessibility: Llama-3.1-70B requires LoRA+quantization; mT5 Large is more tractable but less flexible for inference
  - Validation strictness: Aggressive filtering reduces dataset size but may miss subtler errors

- Failure signatures:
  - Incomplete AMR graphs: Malformed 'and' operators without proper operands
  - Domain mismatch penalty: Europarl silver data reduces HuAMR test performance
  - Saturation without overfitting: Flat Smatch curves on AMRtrans despite increasing silver data

- First 3 experiments:
  1. **Baseline establishment**: Train mT5 Large on AMRtrans only (40k gold); evaluate on both AMRtrans and HuAMR test sets to measure domain gap
  2. **Silver data ablation**: Add HuAMR silver in increments (10k, 20k, 30k, 40k); plot Smatch curves to identify saturation point and confirm domain-specific gains
  3. **Architecture comparison**: Replicate best silver configuration with Llama-3.2-1B; quantify architecture gap and analyze error patterns on 'and' operators and incomplete graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can improved filtering or noise reduction methods for silver AMR data overcome the performance saturation observed on the gold-standard test set?
- Basis in paper: The Conclusion states that the "limited impact of additional silver data implies that the performance saturation is attributable to the lower quality of silver-standard annotations."
- Why unresolved: The current validation only checks PropBank frames and 'and' operators, leaving other potential noise types unaddressed.
- What evidence would resolve it: Comparing parser performance when trained on manually cleaned silver data versus the current automated validation.

### Open Question 2
- Question: Does increasing the student model capacity beyond 1B parameters improve the effective utilization of noisy silver-standard training data?
- Basis in paper: Experiments are limited to models around 1.2B parameters (mT5 Large, Llama 3.2 1B), while the generator was 70B.
- Why unresolved: The study concludes model capacity is critical but only tests one capacity tier for the student models.
- What evidence would resolve it: Training larger student models (e.g., 7B parameters) on the same silver datasets to observe performance scaling.

### Open Question 3
- Question: To what extent does the domain mismatch between the news-focused HuAMR silver data and the mixed-genre AMRtrans corpus limit the effectiveness of data augmentation?
- Basis in paper: Section 4.2 notes HuAMR is exclusively news data while AMR 3.0 is mixed; results show HuAMR data helps on HuAMR tests but not AMRtrans.
- Why unresolved: The paper does not isolate whether the lack of improvement on AMRtrans is due to data quality or domain discrepancy.
- What evidence would resolve it: Evaluating performance when using silver data matched to the AMR 3.0 domain distribution.

## Limitations

- Evaluation relies entirely on Smatch F1 scores without deeper qualitative error analysis, making it difficult to distinguish whether architectural advantages stem from better handling of semantic structure or from pretraining data differences.
- Silver data generation pipeline lacks transparency regarding quality thresholds and error rates, with no quantification of overall noise rate in accepted HuAMR dataset.
- Domain alignment assumption remains partially validated - while HuAMR silver data improves performance on Hungarian news text, the mechanism by which domain-specific structural patterns transfer to the parser is not empirically demonstrated.

## Confidence

- **High confidence**: The baseline finding that mT5 Large outperforms Llama-3.2-1B across all configurations is well-supported by direct comparison and consistent with related work on encoder-decoder advantages for structured output tasks.
- **Medium confidence**: The claim that domain-matched silver data improves in-domain parsing is supported by the HuAMR test set improvements but lacks systematic exploration of the underlying mechanism.
- **Low confidence**: The assertion that gold data quality fundamentally constrains silver data effectiveness is based on a single control experiment without exploring intermediate quality levels or different validation strategies.

## Next Checks

1. **Error analysis decomposition**: Perform systematic error categorization on mT5 Large outputs comparing gold vs. silver training conditions. Quantify improvements in specific error types (e.g., missing arguments, incorrect PropBank frames, malformed operators) to determine whether silver data improvements are uniform across error categories or concentrated in specific areas.

2. **Silver data quality audit**: Generate a stratified sample of 100 HuAMR instances with their English source AMRtrans graphs. Have bilingual annotators rate silver annotation quality (0-3 scale) and identify systematic error patterns. Calculate inter-annotator agreement and correlate quality ratings with parser performance on those instances.

3. **Mixed-domain silver ablation**: Create training datasets mixing HuAMR and Europarl silver data in ratios from 10:1 to 1:1. Train mT5 Large models on each mixture and evaluate on both HuAMR and Europarl test sets. This would test whether domain transfer is possible and identify optimal domain mixing strategies for low-resource semantic parsing.