---
ver: rpa2
title: Training People to Reward Robots
arxiv_id: '2505.10151'
source_url: https://arxiv.org/abs/2505.10151
tags:
- teaching
- training
- learning
- reward
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using machine teaching (MT) to guide novice
  teachers in providing high-quality demonstrations for reinforcement learning from
  demonstration (RLfD). An MT-derived training framework was designed, featuring visual
  feedback and scaffolding training phases to improve users' reward assignment skills.
---

# Training People to Reward Robots

## Quick Facts
- arXiv ID: 2505.10151
- Source URL: https://arxiv.org/abs/2505.10151
- Reference count: 19
- Primary result: MT-guided training improves teaching quality by 83-98% on training tasks and enables 64-70% improvement on unseen tasks.

## Executive Summary
This paper introduces a machine teaching (MT) framework for training novice users to provide high-quality reward signals for reinforcement learning from demonstration (RLfD). The framework uses scaffolding training phases with visual feedback to help users learn reward assignment skills, focusing on decomposing reward functions into component relationships through progressive curriculum. Experiments on a two-link robot arm show that MT-guided training significantly improves teaching quality and enables successful transfer to unseen tasks, with RLfD-trained teachers achieving better long-term stability than supervised LfD-trained teachers for extended-horizon tasks.

## Method Summary
The method implements a scaffolding curriculum (P3-P7) where users assign rewards via slider interface and receive visual feedback comparing their input to ideal rewards computed from optimal control solutions. The system uses Least Squares Policy Iteration (LSPI) with fixed N=8 demonstrations based on feature dimension. Users are trained on point-reaching (S1) and line-reaching (S2) tasks using a 2-DOF robot arm, with performance measured via Absolute Demonstration Error (ADE), Average Root Mean Square Error (ARMSE), and Average Total Reward (ATR). The framework decomposes reward assignment into component relationships through progressive phases, building implicit understanding through guided practice.

## Key Results
- MT-guided training improved teaching quality by 83-98% on training tasks
- Transfer to unseen tasks showed 64-70% improvement in teaching performance
- RLfD-trained teachers achieved better long-term stability than supervised LfD for longer-duration tasks

## Why This Works (Mechanism)

### Mechanism 1
Scaffolding training with visual feedback enables novices to learn reward assignment skills that transfer to unseen tasks. Progressive curriculum phases (P3-P7) decompose reward assignment into component relationships (magnitude consistency, direction invariance, state-distance scaling, action-magnitude scaling), building implicit understanding through guided practice with immediate visual comparison against ideal rewards. Humans can internalize reward function structure through decomposition and practice, enabling generalization without explicit rules.

### Mechanism 2
LSPI-based RLfD with limited demonstrations (N=8, matching teaching dimension) creates a learnable mapping from user rewards to policy parameters. The closed-form LSPI update produces a linear relationship between reward vectors and value-function parameters, so reducing reward error proportionally reduces parameter error, improving policy quality. The feature basis is sufficiently expressive to represent the target value function, and the teaching dimension defines the minimum sufficient data.

### Mechanism 3
RLfD-trained teachers produce policies with better long-term stability than supervised LfD for extended-horizon tasks. RLfD implicitly encodes cumulative reward optimization, whereas supervised LfD learns myopic state-to-action mappings that accumulate drift over long horizons. The target task has meaningful sequential structure where action quality depends on trajectory context.

## Foundational Learning

- **Action-value function (Q-function)**: Users must understand that rewards shape cumulative expected return, not just immediate outcomes. The Q-function Q^π(x,u) = ω^T ψ(x,u) is what the robot actually learns. Quick check: "If I give the same reward for two different states, how does the robot's policy change?"

- **Teaching dimension**: The framework fixes N=8 demonstrations based on the feature count; users must understand this is a constraint, not arbitrary. Quick check: "Why can't the robot learn from just 3 demonstrations in this system?"

- **Reward function decomposition (state cost + action cost)**: Scaffolding phases P5-P6 explicitly teach users that rewards have independent state and action contributions (j = -φ^T Qφ - u^T Ru). Quick check: "If a state is far from target but the action is small, what should the reward reflect?"

## Architecture Onboarding

- **Component map**: MT Problem Solver -> Scaffolding Controller -> Visual Interface -> RLfD Learner -> Evaluation Metrics
- **Critical path**: 1. Define target skill and compute optimal policy parameters θ̄ 2. Sample state-action pairs (N=8) covering workspace 3. Compute ideal rewards ȷ̄ via Eq. 7 4. Run scaffolding phases with visual feedback 5. Collect user rewards ȷ̃ on test tasks 6. Run LSPI to estimate ω, extract policy, evaluate trajectories
- **Design tradeoffs**: Fixed vs. adaptive sampling (uniform random used; adaptive could improve data efficiency but increases complexity); Feature basis complexity (quadratic features chosen for closed-form solvability; limits expressiveness); Guidance granularity (binary in study; graduated feedback could reduce training time but may reduce skill retention)
- **Failure signatures**: ADE remains high after training (user not internalizing reward scale; check if scaffolding phases were rushed); ARMSE high despite low ADE (reward ambiguity; task design issue); Policy instability on long horizons (supervised LfD accidentally used; verify training mode); No transfer to unseen tasks (scaffolding phases too task-specific; verify P7 was included)
- **First 3 experiments**: 1. Baseline replication: Run control group (no guidance) on S1→S1 and S1→S2; confirm EADE/ARMSE show no significant improvement 2. Ablation on scaffolding phases: Remove P7 only; measure transfer degradation to quantify generalization contribution 3. Horizon sensitivity test: Vary trajectory duration (K=50, 100, 200, 400) on policies from RLfD vs. supervised LfD teachers; identify crossover point where RLfD becomes superior

## Open Questions the Paper Calls Out

- Can the MT training framework be extended to teach complex, non-linear motor skills using local optimal control methods? The current study validated only on simple, linear tasks with known optimal solutions.
- Does the MT-training framework improve teaching performance for Reinforcement Learning algorithms other than LSPI? The teaching dimension and sensitivity to reward noise may vary significantly between algorithms.
- How can the discrepancy between improved demonstration quality (ADE) and statistically insignificant robot performance improvements (ARMSE) in transfer tasks be resolved? The authors suggest "reward ambiguity" as a cause, but this reduces reliability of training transfer for novel tasks.

## Limitations

- Experimental scope is narrow: only 2-DOF planar arm tasks with 20 participants total (10 per condition)
- Fixed N=8 demonstrations constraint may not transfer to tasks with different feature complexities or higher-dimensional state spaces
- Long-horizon stability claims comparing RLfD vs supervised LfD lack theoretical grounding and have weak corpus corroboration

## Confidence

**High Confidence**: Claims about immediate training task improvement (83-98%) - supported by direct experimental measurements with clear metrics (EADE, ARMSE, ATR) and statistical comparison between control and MT-guided groups.

**Medium Confidence**: Claims about skill transfer to unseen tasks (64-70% improvement) - supported by experimental data but limited by small sample size (10 participants per condition) and narrow task diversity.

**Medium Confidence**: Claims about RLfD vs supervised LfD long-term stability - supported by Fig. 6 trajectory data but lacks theoretical grounding and has weak corpus corroboration for the mechanism.

**Low Confidence**: Generalizability claims to other robot platforms, task types, or higher-dimensional problems - not tested experimentally.

## Next Checks

1. Apply the MT framework to a 3-DOF arm task or different skill (e.g., obstacle avoidance) with the same participant pool to test transfer robustness.

2. Systematically vary the teaching dimension T (e.g., 4, 8, 12 features) to empirically validate the relationship between feature count, demonstration requirements, and learning quality.

3. Add subjective workload scales (NASA-TLX) and objective response time metrics during scaffolding phases to quantify the visual feedback mechanism's cognitive benefits.