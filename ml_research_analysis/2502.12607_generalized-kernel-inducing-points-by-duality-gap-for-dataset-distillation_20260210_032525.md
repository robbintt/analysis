---
ver: rpa2
title: Generalized Kernel Inducing Points by Duality Gap for Dataset Distillation
arxiv_id: '2502.12607'
source_url: https://arxiv.org/abs/2502.12607
tags:
- dataset
- distillation
- duality
- dgkip
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Duality Gap Kernel Inducing Points (DGKIP),
  a dataset distillation method that avoids bi-level optimization by leveraging duality
  gap theory. The key innovation is using the duality gap as an optimization objective
  to bound the difference between parameters trained on full and distilled datasets,
  enabling application to various convex loss functions beyond squared loss.
---

# Generalized Kernel Inducing Points by Duality Gap for Dataset Distillation

## Quick Facts
- arXiv ID: 2502.12607
- Source URL: https://arxiv.org/abs/2502.12607
- Reference count: 40
- Primary result: Achieves 85.63% CIFAR-10 accuracy with 1 image per class using duality gap optimization

## Executive Summary
This paper introduces Duality Gap Kernel Inducing Points (DGKIP), a dataset distillation method that circumvents the computational burden of bi-level optimization by leveraging duality gap theory. The key innovation is using the duality gap as an optimization objective to bound the difference between parameters trained on full and distilled datasets, enabling application to various convex loss functions beyond squared loss. DGKIP theoretically guarantees bounds on parameter deviation and prediction consistency while achieving competitive accuracy on standard benchmarks with significantly reduced computational costs.

## Method Summary
DGKIP formulates dataset distillation as an optimization problem where the goal is to find a small set of inducing points that approximate the behavior of a full dataset. Instead of the traditional bi-level optimization approach that requires expensive inner-loop training, DGKIP uses the duality gap - the difference between primal and dual optimization objectives - as its optimization target. This approach theoretically bounds the deviation between parameters trained on the full dataset versus those trained on the distilled set. The method assumes strongly convex and twice differentiable loss functions, allowing for kernel-based approximations that capture the essential information from the full dataset in a compressed form.

## Key Results
- Achieves 85.63% accuracy on CIFAR-10 using only 1 image per class
- Demonstrates significant computational efficiency compared to bi-level optimization methods
- Shows robust transferability across different model architectures
- Maintains competitive performance on MNIST and Fashion-MNIST benchmarks

## Why This Works (Mechanism)
The method works by exploiting the mathematical properties of duality gaps in convex optimization. The duality gap provides a principled way to measure how well a distilled dataset approximates the original dataset's optimization landscape. By minimizing this gap, DGKIP ensures that the parameters learned from the distilled dataset will be close to those learned from the full dataset, both in terms of parameter values and prediction outputs. This theoretical foundation provides guarantees that are absent in many empirical dataset distillation approaches.

## Foundational Learning

**Duality Gap in Convex Optimization**
- Why needed: Provides the theoretical foundation for measuring approximation quality
- Quick check: Verify that primal and dual objectives converge as optimization progresses

**Kernel Methods and Representer Theorem**
- Why needed: Enables efficient computation of the influence of inducing points
- Quick check: Confirm kernel matrix properties (positive semi-definiteness)

**Strong Convexity**
- Why needed: Guarantees unique solutions and enables duality gap bounds
- Quick check: Verify the Hessian is positive definite for the loss function

## Architecture Onboarding

**Component Map**
Input Data -> Kernel Matrix Computation -> Duality Gap Optimization -> Distilled Dataset -> Model Training

**Critical Path**
The critical computational path involves kernel matrix construction and inversion, which scales quadratically with dataset size. This represents the primary bottleneck for scaling to larger datasets.

**Design Tradeoffs**
- Pros: Avoids expensive bi-level optimization, provides theoretical guarantees
- Cons: Limited to strongly convex loss functions, quadratic scaling with dataset size
- Edge cases: Non-convex loss functions may work empirically but lack theoretical guarantees

**Failure Signatures**
- Poor performance when loss function is not strongly convex
- Computational/memory issues with large datasets due to kernel matrix scaling
- Suboptimal distillation when data distribution is highly complex

**First Experiments**
1. Verify duality gap minimization on a simple convex problem with known solution
2. Test scalability by varying dataset size and measuring kernel computation time
3. Validate transferability by training different architectures on distilled datasets

## Open Questions the Paper Calls Out
The paper acknowledges that extending the duality gap approach to non-convex loss functions commonly used in deep learning remains an open challenge. The authors note that while empirical results suggest reasonable performance even with some non-convex scenarios, the theoretical guarantees do not directly extend beyond the strongly convex case.

## Limitations
- Theoretical assumptions require strongly convex and twice differentiable loss functions, limiting applicability to certain
- The quadratic scaling of kernel matrix computation restricts practical use to datasets of moderate size
- The method's performance may degrade on highly complex data distributions that cannot be well-represented by a small number of inducing points

## Confidence
Moderate - The paper presents a theoretically grounded approach with promising empirical results, but the strong convexity requirement and scalability limitations constrain its practical applicability.

## Next Checks
1. Verify the theoretical derivation of duality gap bounds for the specific loss functions used
2. Investigate whether approximate kernel methods could alleviate the quadratic scaling bottleneck
3. Explore empirical performance on non-convex losses despite theoretical limitations