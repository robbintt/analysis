---
ver: rpa2
title: 'ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for
  Time Series Forecasting'
arxiv_id: '2507.00013'
source_url: https://arxiv.org/abs/2507.00013
tags:
- series
- time
- masked
- forecasting
- st-mtm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ST-MTM introduces a seasonal-trend decomposition framework for
  masked time-series modeling to address the challenge of modeling complex temporal
  dependencies in forecasting. Unlike previous methods that mask raw time series,
  ST-MTM decomposes series into seasonal and trend components and applies component-specific
  masking strategies: period masking for seasonal components based on autocorrelation-derived
  periods, and sub-series masking for trend components that preserves local semantic
  information.'
---

# ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting

## Quick Facts
- arXiv ID: 2507.00013
- Source URL: https://arxiv.org/abs/2507.00013
- Reference count: 40
- Primary result: Outperforms state-of-the-art masked modeling and contrastive learning methods on nine benchmark datasets through seasonal-trend decomposition with component-specific masking

## Executive Summary
ST-MTM introduces a seasonal-trend decomposition framework for masked time-series modeling to address the challenge of modeling complex temporal dependencies in forecasting. Unlike previous methods that mask raw time series, ST-MTM decomposes series into seasonal and trend components and applies component-specific masking strategies: period masking for seasonal components based on autocorrelation-derived periods, and sub-series masking for trend components that preserves local semantic information. The method employs a seasonal frequency MLP for encoding periodic patterns and a component-wise gating layer for aggregating seasonal and trend representations. Experiments on nine benchmark datasets demonstrate that ST-MTM consistently outperforms state-of-the-art masked modeling, contrastive learning, and supervised decomposition-based forecasting methods, achieving superior performance in both in-domain and cross-domain forecasting scenarios.

## Method Summary
ST-MTM preprocesses time series by decomposing them into seasonal and trend components using moving average. For seasonal components, it identifies dominant periods via FFT-based autocorrelation and creates multiple masked versions by removing sub-series at period-aligned positions. Trend components are masked by removing contiguous segments with similar local patterns. The seasonal component is encoded using a frequency-domain MLP that processes each frequency independently via complex-valued layers, while the trend component uses a standard Transformer encoder. Component representations are aggregated through a gating layer, and the model is trained with reconstruction loss plus contextual contrastive loss that aligns representations across different masked seasonal views. The architecture is pre-trained with self-supervised masking tasks and fine-tuned for supervised forecasting.

## Key Results
- Consistently outperforms state-of-the-art masked modeling, contrastive learning, and supervised decomposition-based forecasting methods across nine benchmark datasets
- Period masking strategy based on autocorrelation-derived periods creates more semantically meaningful pre-training tasks than raw time series masking
- Component-wise gating layer successfully balances seasonal and trend representations for optimal forecasting performance
- Contextual contrastive learning enhances robustness by aligning representations across multiple masked seasonal series

## Why This Works (Mechanism)

### Mechanism 1: Decomposition-Aware Masking Strategy
Component-specific masking based on seasonal-trend decomposition creates more semantically meaningful pre-training tasks than raw time series masking. The system decomposes time series into seasonal and trend components, then applies period masking for seasonal components (masking sub-series at period-aligned positions) and sub-series masking for trend components (removing contiguous segments sharing similar local patterns). This forces the model to reconstruct by understanding distinct temporal semantics rather than interpolating.

### Mechanism 2: Seasonal Frequency MLP (SFM) for Periodic Pattern Encoding
Frequency-domain processing with per-frequency transformations captures periodic dependencies more effectively than time-domain attention for seasonal components. SFM applies FFT to transform seasonal series to frequency domain, processes each frequency component independently via complex-valued MLP layers, then applies inverse FFT. This operates as global convolution in time domain, capturing periodic dependencies without quadratic attention complexity.

### Mechanism 3: Contextual Contrastive Alignment of Masked Seasonal Representations
Aligning representations across multiple period-masked seasonal series enhances robustness to disrupted patterns and improves generalization. Multiple masked seasonal series are treated as augmentations of the same underlying pattern, with contrastive loss pulling these together while pushing apart representations from other time series, enforcing consistency across different masked views.

## Foundational Learning

- **Seasonal-Trend Decomposition (STL-based)**: Why needed here: The entire ST-MTM architecture depends on separating time series into interpretable components before masking and encoding. Without understanding how moving average extraction works and what residual seasonality represents, you cannot debug decomposition failures or tune kernel sizes. Quick check question: Given a time series with a 24-hour period and a 7-day trend, what kernel size for moving average would isolate the trend while preserving the daily pattern in the seasonal component?

- **Autocorrelation and Period Detection**: Why needed here: Period masking relies entirely on identifying dominant periods via autocorrelation peaks. Understanding FFT-based autocorrelation computation (Wiener-Khinchin theorem) is essential for debugging why certain periods are or aren't detected. Quick check question: If autocorrelation returns peaks at lags 12 and 24 for hourly data, what does this suggest about the underlying periodicity, and which should be used for masking?

- **Frequency-Domain Signal Processing**: Why needed here: The SFM encoder operates entirely in frequency domain with complex-valued parameters. Understanding FFT, spectral leakage, and the relationship between time and frequency representations is necessary to interpret what SFM learns. Quick check question: Why might a frequency-domain MLP struggle with non-stationary signals where frequency content shifts over time?

## Architecture Onboarding

- Component map:
  Raw Input → Mean Normalization → Moving Average Decomposition
                                        ↓
                    ┌───────────────────┴───────────────────┐
                    ↓                                       ↓
            Seasonal Component                       Trend Component
                    ↓                                       ↓
            Period Masking (K masks)              Sub-series Masking
                    ↓                                       ↓
            Seasonal Frequency MLP                 Transformer Encoder
            (FFT → Freq MLP → iFFT)
                    ↓                                       ↓
            K Masked Representations               Single Representation
                    ↓                                       ↓
            Autocorrelation-weighted Aggregation          │
                    ↓                                       │
            ─────────────────→ Component-wise ←────────────┘
                              Gating Layer
                                    ↓
                            Aggregated Representation
                                    ↓
                    ┌───────────────┴───────────────┐
                    ↓                               ↓
            Reconstruction Head              Series Projector
            (Linear Decoder)                 (for Contrastive)
                    ↓                               ↓
            Reconstruction Loss              Contrastive Loss

- Critical path: Decomposition quality → Period detection accuracy → Masking semantic validity → SFM frequency processing → Gating balance. If decomposition mixes trend into seasonal component, period detection will be noisy, masking will be semantically meaningless, and the entire pre-training signal degrades.

- Design tradeoffs:
  - **Kernel size for moving average**: Larger kernels extract smoother trends but may over-smooth, removing genuine seasonal signal. Smaller kernels preserve detail but may leave trend contamination in seasonal component. Paper uses dataset-specific tuning (25-200).
  - **Number of masked seasonal series (K)**: More masks provide richer contrastive signal but increase computational cost and reconstruction difficulty. Paper found K=3 optimal; higher K degraded performance.
  - **Contrastive weight (α)**: Higher weight emphasizes consistency but may over-regularize, suppressing useful variation. Paper found α=0.5 optimal; higher values hurt performance.

- Failure signatures:
  - Indistinguishable attention maps on raw data: Suggests model learning spurious patterns → decomposition not working or masking too easy
  - Gating layer assigning uniform weights: Seasonal-trend distinction not learned → check decomposition quality or encoder capacity
  - Contrastive loss not decreasing: Positive pairs too dissimilar → period detection may be detecting noise rather than true periods
  - Performance degradation with longer input: Frequency resolution improves but may capture irrelevant high-frequency noise → consider low-pass filtering before FFT

- First 3 experiments:
  1. **Decposition sanity check**: Before training, visualize decomposition outputs on sample data. Verify seasonal component has zero mean and trend component is smooth. Tune kernel size until seasonal autocorrelation shows clear peaks and trend shows minimal periodicity.
  2. **Masking difficulty calibration**: Train with reconstruction-only loss (α=0) varying trend masking ratio and number of seasonal masks. Plot reconstruction error vs. forecasting performance to find the difficulty sweet spot where pre-training is challenging but not impossible.
  3. **Component ablation**: Train three variants—seasonal-only, trend-only, and full ST-MTM—on a dataset with strong seasonality (e.g., Electricity) and one with minimal seasonality (e.g., Exchange). This reveals whether both components contribute or if one dominates, informing whether the gating mechanism is functioning as intended.

## Open Questions the Paper Calls Out

### Open Question 1
Can sequential or iterative decomposition methods enhance ST-MTM's ability to capture detailed structured components compared to the current single-pass approach? The authors state they aim to extend their work to masked time-series modeling using sequential decomposition, which could enhance understanding of more detailed structured components. This remains unresolved as the current implementation applies a single decomposition step.

### Open Question 2
Can the period selection mechanism be made adaptive or learnable to eliminate the need for manually selecting the hyperparameter K? The method relies on selecting the "top-K autocorrelations" to identify periods, where K is a fixed hyperparameter set to 3. A fixed K ignores variance in periodicity across datasets, making static selection potentially suboptimal.

### Open Question 3
Is the reliance on a simple moving average for trend decomposition a bottleneck for time series with abrupt trend shifts or non-stationarity? The paper utilizes a basic moving average operation to extract the trend component. Moving averages inherently lag behind real-time changes and smooth out sudden fluctuations, potentially causing the "sub-series masking" strategy to learn from a distorted trend representation.

## Limitations
- Method's reliance on accurate period detection via autocorrelation presents a critical failure mode for non-stationary or chaotic time series where periodicity is unclear or time-varying
- Complex-valued SFM assumes seasonality can be cleanly represented in frequency domain, which may fail for irregular patterns
- Gating mechanism's effectiveness depends on successful decomposition, creating potential cascade failure if trend-seasonal separation is imperfect

## Confidence
- High: Component masking strategy works better than raw time series masking (multiple ablation studies, clear error reductions)
- Medium: Seasonal Frequency MLP provides benefits over time-domain encoding (supported by ablation but lacks comparison to alternative frequency methods)
- Medium: Contrastive alignment improves robustness (mechanism is sound but corpus support is weak)
- Low: Decomposition quality consistently sufficient across all datasets (kernel sizes tuned but methodology for selection not fully specified)

## Next Checks
1. **Decomposition Quality Audit**: Implement per-dataset kernel size selection protocol and visualize decomposition outputs. Verify seasonal components show clear autocorrelation peaks and trend components are smooth before proceeding to full training.
2. **Period Detection Robustness**: Test period masking on synthetic series with known but varying periods (including non-stationary cases). Measure detection accuracy and impact on downstream performance when periods are incorrectly identified.
3. **Frequency Domain Capacity Test**: Compare SFM against time-domain Transformer baselines on datasets with different seasonality characteristics. Include ablation where SFM processes only top-N frequencies to determine if frequency-domain representation is genuinely advantageous.