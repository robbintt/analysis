---
ver: rpa2
title: 'MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft'
arxiv_id: '2504.08388'
source_url: https://arxiv.org/abs/2504.08388
tags:
- game
- actions
- mineworld
- action
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MineWorld is a real-time, open-source interactive world model for
  Minecraft, addressing the efficiency and controllability challenges of video generative
  models in world modeling. It uses an autoregressive Transformer trained on discretized
  game states and actions, with a novel parallel decoding algorithm achieving 4-7
  FPS for real-time interaction.
---

# MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft

## Quick Facts
- arXiv ID: 2504.08388
- Source URL: https://arxiv.org/abs/2504.08388
- Reference count: 10
- Key outcome: MineWorld achieves 4-7 FPS real-time interaction on Minecraft using parallel decoding, outperforming diffusion-based models in both speed and controllability.

## Executive Summary
MineWorld introduces a real-time, open-source interactive world model for Minecraft that addresses the efficiency and controllability challenges of video generative models. The model uses an autoregressive Transformer trained on discretized game states and actions, achieving 4-7 FPS through a novel parallel decoding algorithm. It demonstrates superior performance compared to state-of-the-art diffusion-based models, with controllability assessed through an inverse dynamics model. The approach shows promise for interactive applications while maintaining reasonable visual quality and action-following accuracy.

## Method Summary
MineWorld transforms visual game scenes and actions into discrete token IDs using a VQ-VAE for images and a discrete action tokenizer. These tokens are interleaved and fed into an autoregressive Transformer decoder trained to predict next tokens. The model employs diagonal decoding for parallel generation of spatially redundant tokens, achieving 3-7× speedup. Controllability is evaluated using an inverse dynamics model that infers actions from generated frame pairs. The architecture supports multiple model sizes (300M-1.2B parameters) and is trained on the VPT dataset with a context window of 16 state-action pairs.

## Key Results
- Achieves 4-7 FPS real-time interaction through parallel decoding
- 700M variant achieves 0.73 F1 score on action classification
- Outperforms diffusion-based models on FVD and PSNR metrics
- Human evaluation correlation with IDM controllability metrics: r=0.56, p=0.01

## Why This Works (Mechanism)

### Mechanism 1: Joint State-Action Tokenization
By converting both game scenes and actions into a shared discrete token vocabulary and interleaving them, the Transformer learns conditional dependencies between visual states and control signals. This forces the model to learn p(x_{i+1}|x_{<i}, a_i) through next-token prediction.

### Mechanism 2: Parallel Decoding via Diagonal Attention
Diagonal decoding generates tokens at adjacent spatial positions simultaneously, exploiting local spatial dependencies to achieve 3-7× speedup with minimal quality loss. The theoretical speedup ratio is r = (h × w)/(h + w - 1).

### Mechanism 3: IDM-Based Controllability Evaluation
An inverse dynamics model infers actions from generated frame pairs, comparing predicted vs. ground-truth actions. Classification metrics (precision, recall, F1) on discrete actions and L1 loss on camera angles quantify how well the generated visual outcome matches the conditioned action.

## Foundational Learning

- **Autoregressive Language Modeling**: Understanding causal masking and sampling strategies is essential for training the next-token prediction model.
  - Quick check: Explain why causal masking prevents information leakage during training, and how sampling temperature affects generation diversity.

- **Vector Quantized Variational Autoencoder (VQ-VAE)**: Visual states are discretized via a VQ-VAE with 16× spatial compression and 8k codebook.
  - Quick check: How does codebook size and commitment loss affect the tradeoff between reconstruction quality and token sequence length?

- **Transformer Decoder Architecture**: MineWorld uses RMSNorm, Rotary Embeddings, and SwiGLU-style MLPs.
  - Quick check: What is the computational difference between rotary positional embeddings and absolute sinusoidal embeddings, and why might rotary be preferred for long sequences?

## Architecture Onboarding

- Component map: VQ-VAE (16× compression, 8k codebook) -> Action Tokenizer (11 tokens/action) -> Transformer Decoder (LLaMA-style) -> Parallel Decoding Module
- Critical path: Preprocess VPT dataset → resize to 224×384 → train/fine-tune VQ-VAE → tokenize clips → interleaved sequences → train Transformer → fine-tune with parallel attention mask → inference with diagonal decoding
- Design tradeoffs:
  - Model size vs. FPS: Larger models achieve better FVD/F1 but lower FPS
  - Parallel decoding vs. quality: Without fine-tuning, degrades FVD; fine-tuning recovers quality
  - Resolution vs. sequence length: Downsampling reduces computation but loses details
- Failure signatures:
  - Action mismatch: Generated frames don't reflect conditioned actions
  - Temporal inconsistency: Objects morph or disappear across frames
  - Parallel decoding artifacts: Blurry or misaligned regions
- First 3 experiments:
  1. Evaluate VQ-VAE PSNR/SSIM on validation set to verify reconstruction quality
  2. Train model with action tokens appended (not interleaved) vs. interleaved to isolate inductive bias contribution
  3. Benchmark FPS and FVD for different model sizes with and without parallel decoding fine-tuning

## Open Questions the Paper Calls Out

- Does incorporating video-level tokenizers with temporal compression improve performance over the current image-level tokenizer?
- Does training the model from scratch with the parallel attention mask yield faster convergence than fine-tuning?
- Can the proposed architecture generalize to domains beyond Minecraft or higher resolutions without significant degradation?

## Limitations
- Fixed resolution (224×384) and limited context length (16 state-action pairs)
- Heavy reliance on VQ-VAE compression affects visual fidelity
- Evaluation methodology depends on IDM accuracy, which may have systematic biases

## Confidence
- **High Confidence**: Core technical claims regarding autoregressive training, parallel decoding implementation, and quantitative evaluation metrics
- **Medium Confidence**: Controllability evaluation methodology using IDM, supported by moderate human evaluation correlation
- **Low Confidence**: Claims about model capabilities as a policy model, presented with limited quantitative evidence

## Next Checks
1. Evaluate IDM performance across all action classes in VPT dataset to identify potential biases in controllability assessment
2. Systematically test model performance as context length varies from 8 to 32 state-action pairs to quantify impact of fixed limitation
3. Test pre-trained MineWorld model on a different game environment (e.g., VizDoom) without fine-tuning to assess domain-specific nature of learned representations