---
ver: rpa2
title: '"Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current
  LLMs'
arxiv_id: '2507.08027'
source_url: https://arxiv.org/abs/2507.08027
tags:
- political
- liberal
- more
- they
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study analyzed the political temperaments of seven prominent
  large language models using Moral Foundations Theory, political ideology scales,
  and a new index of current political controversies. Results showed a strong and
  consistent liberal orientation across models, with prioritization of care and fairness
  values.
---

# "Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs

## Quick Facts
- arXiv ID: 2507.08027
- Source URL: https://arxiv.org/abs/2507.08027
- Reference count: 40
- Primary result: Seven prominent LLMs show strong and consistent liberal orientation across multiple political temperament measures

## Executive Summary
This study systematically analyzes the political temperaments of seven current LLMs using Moral Foundations Theory, political ideology scales, and a new political controversies index. The research finds a robust liberal tilt across all models, characterized by prioritization of care and fairness moral foundations. The authors identify four overlapping causal mechanisms: liberal-leaning training corpora, RLHF rewarding care/fairness outputs, the dominance of liberal frameworks in academic ethical discourse, and safety-driven fine-tuning practices that penalize conservative-coded reasoning.

## Method Summary
The study employs a comprehensive methodology using seven prominent LLMs and three measurement approaches: ranking of five moral foundations, 12 established political ideology scales, and a 7-item political controversies index. Researchers used direct prompting with structured questions, requesting both self-scores and estimates of typical American adult responses. A key methodological innovation was comparing base and fine-tuned model pairs (Llama-3.1-8B vs. Llama-3.1-8B-Instruct) to isolate pretraining effects from fine-tuning influences. The analysis includes detailed statistical comparisons and examines how different fine-tuning approaches affect political orientation.

## Key Results
- All seven LLMs show consistent liberal orientation across measurement instruments
- Models consistently prioritize care and fairness moral foundations while ranking loyalty, authority, and purity lowest
- Fine-tuning increases liberal lean by approximately 11% on average compared to base models
- The liberal tilt persists across different model families and evaluation methods

## Why This Works (Mechanism)

### Mechanism 1: Training Corpus Demographic Skew
LLMs exhibit liberal lean partly because training data over-represents liberal-leaning demographics and discourse. Training corpora drawn from books, academic papers, and websites are disproportionately urban, college-educated, and Western—populations that emphasize individual rights, inclusivity, and harm-reduction. These sources underrepresent religious, nationalist, and traditionalist perspectives.

### Mechanism 2: RLHF Rewarding Care/Fairness Outputs
Reinforcement learning from human feedback systematically rewards outputs aligned with liberal moral foundations. Human raters following Western organizational guidelines tend to reward outputs that avoid harm and are inclusive, while penalizing judgmental or exclusionary language. This shapes model behavior toward care and fairness prioritization.

### Mechanism 3: Safety Fine-Tuning Penalizes Authority/Purity Logic
Fine-tuning for safety and public acceptability indirectly shifts models leftward by penalizing conservative-coded moral reasoning. Authority and purity foundations are associated with controversial outputs (nationalism, traditionalism). Fine-tuning penalizes these as "risky," while care/fairness reasoning is "safest" for general-purpose responses.

## Foundational Learning

- Concept: Moral Foundations Theory (Haidt)
  - Why needed here: The paper's primary measurement framework; distinguishes care/fairness (liberal-associated) from loyalty/authority/purity (conservative-associated)
  - Quick check question: Which two foundations do the paper's LLMs consistently rank highest?

- Concept: RLHF (Reinforcement Learning from Human Feedback)
  - Why needed here: Identified mechanism by which human preferences shape model outputs post-pretraining; central to the fine-tuning effect
  - Quick check question: What type of outputs do RLHF raters tend to reward, according to the paper?

- Concept: Base vs. Fine-Tuned Model Comparison
  - Why needed here: Method for isolating pretraining effects from fine-tuning effects; critical for causal attribution
  - Quick check question: What was the approximate percentage liberal shift from base to fine-tuned Llama in Table 6?

## Architecture Onboarding

- Component map: Pretraining layer -> Fine-tuning layer -> Evaluation layer
- Pretraining layer: Base model trained on trillions of tokens (web text, books, academic papers)—captures "street-talk" distributional patterns
- Fine-tuning layer: "Etiquette layer" including RLHF and safety alignment—shifts outputs toward care/fairness and away from controversial positions
- Evaluation layer: Multiple measurement instruments (MFT rankings, 12 political ideology scales, controversy index) applied post-hoc

- Critical path: Pretraining corpora composition → Base model political tendencies → RLHF/safety fine-tuning → Fine-tuned model political output → Evaluation via moral/political scales

- Design tradeoffs:
  - Safety vs. viewpoint diversity: Penalizing authority/purity reduces harmful outputs but may suppress legitimate conservative perspectives
  - Self-report vs. empirical testing: LLM explanations of their own reasoning are unreliable ("post hoc"), requiring paired base/fine-tuned comparisons for validation
  - "Bias" vs. "epistemic difference" framing: Paper argues conflation risks mischaracterizing legitimate value disagreements as defects

- Failure signatures:
  - Base model shows high scores on Right-Wing Authoritarianism and sexism scales; fine-tuned version shows dramatic drops (Table 6: RWA -43%, sexism -95%)
  - Models initially defer ("just an AI"), require nudging to express positions
  - Self-reported fine-tuning influence estimates may not match empirical comparisons

- First 3 experiments:
  1. Replicate base vs. fine-tuned comparison on additional model families (currently only Llama-3.1-8B tested) to validate 11% liberal shift
  2. Test whether explicitly balanced RLHF guidelines (rewarding all five moral foundations equally) reduces liberal tilt
  3. Evaluate whether political controversy index results hold with freshly-designed items not present in training data (addressing "parrot problem")

## Open Questions the Paper Calls Out

### Open Question 1
Does the pattern where fine-tuning increases liberal orientation hold across a wider range of base and fine-tuned model pairs beyond the single Llama pair tested? The finding relies on a single model family (Llama-3.1-8B), leaving open the possibility that the effect is specific to Meta's alignment process rather than a general feature of RLHF.

### Open Question 2
Do LLMs genuinely approximate John Rawls' "veil of ignorance," or is this apparent impartiality merely an artifact of the specific Western, liberal-democratic discourse dominant in their training data? It is conceptually difficult to distinguish between a model acting as an impartial "every-person" (Rawlsian view) and a model simply acting as a statistical average of biased, though dominant, Western internet discourse.

### Open Question 3
What is the relative contribution of training corpora versus RLHF/safety fine-tuning in creating the observed liberal tilt? The paper attributes the tilt to "four overlapping factors" but acknowledges these dynamics are intertwined, noting "The underlying causes... remain unclear."

## Limitations
- Self-report methodology for models estimating their own training influences is unreliable
- "Parrot problem" remains unaddressed: liberal preferences could reflect memorized patterns rather than genuine reasoning
- Single base/fine-tuned pair limits generalizability of fine-tuning effect findings

## Confidence
- High confidence: Observed liberal orientation across multiple measurement instruments is robust and replicable
- Medium confidence: Attribution of liberal tilt to training corpus demographic skew
- Low confidence: Self-reported model estimates of pretraining vs fine-tuning influence

## Next Checks
1. Controlled RLHF experiment: Test whether explicitly balanced RLHF guidelines reduces liberal tilt compared to standard guidelines
2. Fresh controversy index: Replicate political controversy analysis using newly-designed items not present in any training data
3. Cross-cultural validation: Evaluate the same model battery on non-Western training corpora or with cross-cultural moral foundation prompts