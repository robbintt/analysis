---
ver: rpa2
title: 'FilBench: Can LLMs Understand and Generate Filipino?'
arxiv_id: '2508.03523'
source_url: https://arxiv.org/abs/2508.03523
tags:
- filbench
- language
- filipino
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FILBENCH addresses the lack of comprehensive benchmarks for evaluating\
  \ large language models on Filipino, Tagalog, and Cebuano by curating 12 tasks across\
  \ four categories\u2014Cultural Knowledge, Classical NLP, Reading Comprehension,\
  \ and Generation\u2014based on research priorities in Philippine NLP. The authors\
  \ transform existing datasets into a unified task format for standardized LLM evaluation."
---

# FilBench: Can LLMs Understand and Generate Filipino?

## Quick Facts
- **arXiv ID**: 2508.03523
- **Source URL**: https://arxiv.org/abs/2508.03523
- **Reference count**: 40
- **Primary result**: FilBench evaluates 27 state-of-the-art LLMs on Filipino, Tagalog, and Cebuano tasks, revealing significant capability gaps with top models achieving only 72.23% accuracy.

## Executive Summary
FilBench is the first comprehensive benchmark for evaluating large language models on Filipino, Tagalog, and Cebuano, featuring 12 tasks across Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. The authors transform 14 existing datasets into a unified format for standardized LLM evaluation. Testing 27 state-of-the-art models including GPT-4o, SEA-specific models, and multilingual models reveals that even the best-performing GPT-4o achieves only 72.23% accuracy, with generation tasks being particularly challenging. Notably, SEA-specific models like SEA-LION v3 70B underperform compared to general-purpose models despite targeted finetuning, highlighting significant gaps in LLM capabilities for Philippine languages and underscoring the need for language-specific benchmarks.

## Method Summary
FilBench uses the LightEval framework with vLLM backend to evaluate models across 12 tasks transformed from 14 existing datasets. Multiple-choice formulation (MCF) tasks use accuracy metrics, while generation tasks employ ROUGE-L for translation quality. The evaluation suite runs on 2 NVIDIA H100 GPUs, with a 4.93-hour runtime for 27 models under 83B parameters. Models answer prompts formatted as MCF for classification tasks or translation templates for generation tasks, with scores aggregated as weighted averages based on instance counts per category.

## Key Results
- GPT-4o achieves highest score of 72.23% on FilBench, with SEA-specific SEA-LION v3 70B scoring only 61.07%
- Generation tasks are consistently more challenging than classification tasks across all models
- Even top-performing models show significant gaps in understanding Filipino cultural knowledge and reading comprehension

## Why This Works (Mechanism)

### Mechanism 1: Language-Specific Finetuning
Language-specific finetuning improves Filipino benchmark performance, particularly for smaller models. Continuous finetuning on SEA-language data transfers knowledge to target language tasks without full retraining. SEA-LION v3 70B (61.07%) outperforms base Llama 3.1 70B (59.66%), with more pronounced gains for 8B-9B models. This works through cross-lingual transfer from typologically similar SEA languages, though data quality remains a critical factor.

### Mechanism 2: Generation Task Failures
Generation tasks fail primarily due to overgeneration and poor instruction-following, not lack of linguistic knowledge. Short prompts provide insufficient context for LLMs to determine generation boundaries, causing models to continue generating until max length and produce incoherent text. The paper finds 47% of failures are overgeneration and 34% are poor instruction-following across 100 sampled GPT-4o failures, suggesting training data imbalance causes length-related miscalibration.

### Mechanism 3: Model Agreement Patterns
Higher inter-model agreement among SEA-specific models indicates more reliable—but not necessarily more accurate—Filipino representations. SEA-specific finetuning constrains the model's output distribution to consistent patterns for Philippine languages, increasing Fleiss' κ (e.g., 0.639 for NER vs. 0.273 for non-SEA models). However, this consistency does not guarantee correctness—cultural tasks still show low agreement, and SEA models agree on incorrect answers for driving exam questions.

## Foundational Learning

- **Multiple-Choice Formulation (MCF) for LLM evaluation**: Understanding how logits are extracted and compared to gold labels is essential for interpreting scores. Quick check: Given a prompt with options A-D, how would you extract the model's predicted choice from raw token probabilities?
- **ROUGE-L for translation quality**: This metric captures longest common subsequence overlap for comparing generated translations to references. Quick check: If a model translates "The cat sat" as "The cat sat on the mat" (reference: "The cat sat"), will ROUGE-L penalize the extra tokens?
- **Cross-lingual transfer in multilingual models**: Understanding how SEA-specific models leverage transfer from related languages helps explain why SEA-LION outperforms base models despite smaller scale. Quick check: Why might a model trained on Indonesian and Thai still improve on Filipino tasks?

## Architecture Onboarding

- **Component map**: LightEval framework -> vLLM backend -> MCF evaluator/ROUGE-L evaluator -> Benchmark aggregator
- **Critical path**: Load model via vLLM backend with tensor parallelism -> Format prompt using MCF or translation template -> Generate response and extract output -> Compare to gold label and compute metric -> Aggregate per-category scores into weighted FilBench score
- **Design tradeoffs**: Weighted aggregation balances task contribution by instance count but small datasets have minimal impact; ROUGE-L is fast and interpretable but may miss semantic equivalence; zero-shot vs. few-shot evaluation shows k=1 improves generation but deviates from pure zero-shot standard
- **Failure signatures**: Overgeneration (output length vs. reference length ratio > 2.0), wrong target language (language identification detects confusion), hallucination (semantic similarity threshold < 0.5 indicates spurious content)
- **First 3 experiments**: 1) Reproduce baseline on 3 models (GPT-4o, Qwen2.5-72B, SEA-LION v3 8B) to validate evaluation pipeline; 2) Ablate few-shot k on generation tasks (k=0, 1, 3, 5) to quantify diminishing returns; 3) Isolate cultural vs. factual failure modes by classifying 50 incorrect CK responses from SEA-LION v3 70B

## Open Questions the Paper Calls Out

- **Question 1**: What is the precise influence of the proportion of Filipino-centric training data on downstream FILBENCH performance? The authors leave systematic exploration of language-specific training data proportions for future work due to difficulties tracking training data provenance.
- **Question 2**: Why do SEA-specific models underperform on FILBENCH compared to general-purpose LLMs despite targeted finetuning? This finding is noted as significant but unexplained, requiring investigation into data quality, finetuning methodology, or task misalignment.
- **Question 3**: How can the overgeneration and poor instruction-following failure modes in Filipino translation tasks be systematically mitigated? While few-shot prompting shows some improvement, the authors defer ablation to future work as generation performance remains poor.
- **Question 4**: How can models better distinguish between Filipino and Cebuano when processing texts with shared vocabulary? The linguistic similarity facilitates cross-lingual transfer but makes language identification difficult, especially with Cebuano's limited representation.

## Limitations

- The evaluation does not distinguish between insufficient Filipino data quality in finetuning versus fundamental architectural constraints
- High model agreement could indicate shared systematic biases rather than reliable representations
- Generation task failures could stem from prompt engineering rather than linguistic capability gaps

## Confidence

**High Confidence Claims:**
- FilBench provides the first comprehensive benchmark covering Filipino, Tagalog, and Cebuano across 12 distinct tasks
- Even top-performing models achieve only 72.23% accuracy, confirming significant capability gaps
- Generation tasks are consistently more challenging than classification tasks across all models

**Medium Confidence Claims:**
- SEA-specific finetuning improves performance, particularly for smaller models
- Overgeneration and poor instruction-following are primary generation failure modes
- Higher inter-model agreement among SEA-specific models indicates more reliable representations

**Low Confidence Claims:**
- Typological similarity between Filipino and neighboring SEA languages drives finetuning benefits
- Short prompts inherently cause overgeneration regardless of model architecture
- High model agreement correlates with answer correctness

## Next Checks

1. **Ablate finetuning data quality**: Retrain SEA-LION v3 70B using identical architecture but substitute Filipino data with English translations of the same documents. Compare performance to isolate whether finetuning benefits stem from language specificity versus data quality.

2. **Probe generation failure modes**: For 100 generation task failures from GPT-4o, implement automated checks: (a) ratio of generated tokens to reference tokens > 2.0 indicates overgeneration, (b) language identification on output detects target language confusion, (c) semantic similarity threshold < 0.5 indicates hallucination. Classify failures by root cause.

3. **Validate agreement vs. accuracy correlation**: Select 100 instances where SEA models achieve Fleiss' κ > 0.7. Have human experts independently score answer correctness without seeing model outputs. Compute correlation between agreement and human accuracy to determine if high agreement predicts correctness.