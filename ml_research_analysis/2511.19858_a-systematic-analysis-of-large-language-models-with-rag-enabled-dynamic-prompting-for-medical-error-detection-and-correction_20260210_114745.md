---
ver: rpa2
title: A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting
  for Medical Error Detection and Correction
arxiv_id: '2511.19858'
source_url: https://arxiv.org/abs/2511.19858
tags:
- error
- detection
- correction
- sentence
- flag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Clinical documentation contains factual, diagnostic, and management
  errors that can compromise patient safety. This paper evaluates retrieval-augmented
  generation (RAG)-enabled dynamic prompting (RDP) for medical error detection and
  correction across three subtasks: error flag detection, error sentence detection,
  and error correction.'
---

# A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction

## Quick Facts
- **arXiv ID**: 2511.19858
- **Source URL**: https://arxiv.org/abs/2511.19858
- **Reference count**: 40
- **Primary result**: RDP reduced false-positive rates by ~15% and improved recall by 5–10% for error sentence detection in clinical notes.

## Executive Summary
Clinical documentation often contains factual, diagnostic, and management errors that can compromise patient safety. This paper evaluates retrieval-augmented generation (RAG)-enabled dynamic prompting (RDP) for medical error detection and correction across three subtasks: error flag detection, error sentence detection, and error correction. Using the MEDEC dataset and nine large language models, RDP outperformed zero-shot and static prompting strategies by dynamically retrieving semantically relevant exemplars to construct context-aware prompts. Results demonstrate RDP's potential to improve the reliability of LLM-based medical error detection and correction, though expert clinician review remains essential for complex cases.

## Method Summary
The paper evaluates RDP on three subtasks using the MEDEC dataset (3,848 clinical texts). RDP constructs dynamic few-shot prompts by retrieving top-10 semantically similar examples from a Chroma vector store populated with MEDEC training data embeddings (using `text-embedding-3-large`). The retrieved exemplars are appended to prompt templates to guide error detection and correction. RDP is compared against zero-shot prompting (ZSP) and static prompting with 10 random examples (SPR). For error correction, quality is measured using AggScore (mean of ROUGE-1, BERTScore, BLEURT).

## Key Results
- RDP reduced false-positive rates by ~15% compared to static and zero-shot prompting
- RDP improved recall in error sentence detection by 5–10% over baselines
- RDP generated more contextually accurate corrections with higher AggScore metrics
- RDP better handled abbreviations and clinical shorthand compared to baseline approaches

## Why This Works (Mechanism)
RDP works by dynamically retrieving semantically relevant exemplars from the training data to construct context-aware prompts. This approach grounds the model in specific examples of similar clinical errors, reducing hallucination and improving precision in error detection. The retrieval mechanism ensures that the model receives relevant context for each input, particularly important for clinical notes containing abbreviations and domain-specific terminology that may not be well-represented in general training data.

## Foundational Learning
- **Clinical Error Classification**: Understanding the five error types (Diagnosis, Management, Treatment, Pharmacotherapy, Causal Organism) is needed to interpret results. Quick check: Review MEDEC dataset documentation to confirm error labeling schema.
- **RAG (Retrieval-Augmented Generation)**: This technique combines information retrieval with text generation to provide context. Quick check: Verify the Chroma vector store contains diverse error examples across all five categories.
- **Prompt Engineering**: Dynamic prompt construction based on retrieved exemplars. Quick check: Ensure prompt templates in Appendix B are correctly formatted for each subtask.
- **Evaluation Metrics**: Accuracy, Recall, FPR for detection; AggScore (ROUGE-1, BERTScore, BLEURT) for correction. Quick check: Confirm evaluation scripts implement these metrics correctly.
- **Clinical NLP Challenges**: Handling abbreviations, sentence boundary detection, and domain-specific terminology. Quick check: Test RDP on notes with high abbreviation density to validate claims.

## Architecture Onboarding
**Component Map**: Input Clinical Note -> Chroma Vector Store (embeddings) -> Top-10 Retrieval -> Dynamic Prompt Construction -> LLM API -> Output Detection/Correction

**Critical Path**: The retrieval and prompt construction pipeline is critical. RDP's performance depends on retrieving semantically relevant exemplars that accurately represent the error type present in the input.

**Design Tradeoffs**: RDP trades increased inference latency (due to retrieval and prompt construction) for improved accuracy and reduced false positives. The choice of 10 retrieved examples balances context richness against prompt token limits.

**Failure Signatures**: High false-positive rates indicate poor exemplar selection or overly sensitive prompt instructions. Near-miss sentence detection (selecting adjacent sentences) suggests the model needs better boundary distinction, which RDP should address.

**First Experiments**:
1. Implement RDP on a small MEDEC subset and verify FPR reduction compared to SPR
2. Test RDP on clinical notes with adjacent error sentences to confirm reduced near-miss detection
3. Evaluate RDP's handling of abbreviations by comparing performance on notes with high shorthand density

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified text chunking parameters (chunk_size, chunk_overlap) may affect exemplar retrieval quality
- Random seeds for SPR baseline not documented, potentially introducing variability
- Access to specific model versions (GPT-5, o4-mini) may be restricted or future-dated

## Confidence
- **High Confidence**: RDP methodology is clearly described and reproducible with available tools; FPR reduction and recall improvements are well-supported
- **Medium Confidence**: Error correction quality improvements depend on exact prompt construction details that are not fully specified
- **Low Confidence**: Performance claims on future/restricted model versions cannot be independently verified

## Next Checks
1. Implement RDP with MEDEC test data and measure false positive rate compared to SPR to verify ~15% reduction
2. Test RDP on clinical notes with adjacent error sentences to confirm reduced near-miss detection
3. Evaluate RDP's performance on notes with high abbreviation density to validate improved handling of clinical shorthand