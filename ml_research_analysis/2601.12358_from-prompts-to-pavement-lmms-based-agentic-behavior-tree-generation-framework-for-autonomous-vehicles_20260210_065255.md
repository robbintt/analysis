---
ver: rpa2
title: 'From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework
  for Autonomous Vehicles'
arxiv_id: '2601.12358'
source_url: https://arxiv.org/abs/2601.12358
tags:
- agent
- behavior
- pipeline
- vehicle
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an agentic framework that uses large language
  models (LLMs) and vision models to generate behavior trees (BTs) for autonomous
  vehicles. The system includes three specialized agents: a Descriptor that assesses
  scene criticality, a Planner that constructs sub-goals, and a Generator that synthesizes
  executable BTs in XML.'
---

# From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles

## Quick Facts
- arXiv ID: 2601.12358
- Source URL: https://arxiv.org/abs/2601.12358
- Reference count: 29
- One-line result: Agentic framework using LLMs and vision models generates adaptive behavior trees for autonomous vehicles when baseline BT fails, achieving successful obstacle navigation in CARLA simulation.

## Executive Summary
This paper presents an agentic framework that generates behavior trees (BTs) for autonomous vehicles when the baseline navigation system fails. The system employs three specialized agents—a Descriptor that assesses scene criticality using a large language model (LLM) with chain-of-symbols prompting, a Planner that constructs high-level sub-goals, and a Generator that synthesizes executable XML BTs. The framework is triggered only when the baseline Nav2 behavior tree returns Failure, such as when the vehicle is blocked by an obstacle. In CARLA+Nav2 simulation, the system successfully generated and executed a BT to navigate around unexpected obstacles without human intervention, demonstrating adaptability for autonomous driving scenarios.

## Method Summary
The framework integrates with CARLA simulation using ROS2 Humble and Nav2 for autonomous navigation. When the baseline Nav2 BT fails (e.g., vehicle blocked by obstacle), the agentic pipeline activates. The Descriptor agent (using GPT-4o-mini) processes RGB camera data through chain-of-symbols prompting to assess scene criticality and output structured JSON. If critical, the Planner agent generates ordered sub-goals via in-context learning, which the Generator agent then converts into executable XML BT subtrees. These are concatenated under a Sequence node and fed back to Nav2 for execution. The system uses a combination of RGB cameras (1920x1080), LiDAR, and odometry sensors, with BDD-X dataset for Descriptor evaluation.

## Key Results
- Descriptor agent achieved a mean absolute error of 0.5 on criticality labels for binary classification.
- Average generation time across the entire pipeline was 21.36 seconds with 45,111 tokens consumed per scene.
- Generated BTs were valid, executable, and successfully navigated around obstacles (e.g., fire trucks blocking the road) without human intervention.
- Generated BTs aligned with Planner's output, demonstrating the framework's adaptability and viability for autonomous driving scenarios.

## Why This Works (Mechanism)

### Mechanism 1: Failure-Triggered Agent Activation
- Claim: Invoking the agentic pipeline only when the baseline behavior tree fails reduces unnecessary LLM calls and computational overhead.
- Mechanism: The Nav2 baseline BT executes continuously; when its root node returns Failure (e.g., vehicle stuck behind fire truck), this triggers the three-agent pipeline. Formally: B_base(e) = Failure → activate pipeline.
- Core assumption: Baseline BT covers routine driving adequately; only novel edge cases require LLM intervention.
- Evidence anchors: [abstract] "The framework triggers only when a baseline BT fails"; [section III.A.4] "In the condition that the root node of this baseline tree returns Failure, the entire agentic pipeline initiates execution."
- Break condition: If baseline BT fails too frequently (poorly designed) or succeeds in scenarios that are actually unsafe, the trigger logic becomes counterproductive.

### Mechanism 2: Sequential Agent Pipeline with Structured Handoff
- Claim: Decomposing BT generation into three specialized agents (Descriptor → Planner → Generator) with JSON-structured outputs enables modular reasoning and interpretable intermediate states.
- Mechanism: Descriptor (LMM) assesses scene criticality using Chain-of-Symbols prompting → outputs JSON with isCritical, issueExplanation, sceneDescription. If critical, Planner constructs ordered sub-goals in natural language → Generator synthesizes XML subtrees per sub-goal, concatenated under a Sequence node.
- Core assumption: LLMs can reliably decompose high-level plans into BT-executable sub-goals without formal verification.
- Evidence anchors: [abstract] "A specialized Descriptor agent applies chain-of-symbols prompting... Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees."
- Break condition: If any agent hallucinates (e.g., Descriptor misses critical hazard, Planner proposes infeasible maneuver), downstream agents propagate errors.

### Mechanism 3: Chain-of-Symbols Prompting for Spatial Reasoning
- Claim: Using symbolic representations for objects and spatial relations in CoT reduces token consumption while improving spatial reasoning accuracy for the Descriptor agent.
- Mechanism: Instead of verbose natural language descriptions, objects get arbitrary symbols; spatial relations get symbols. This streamlines reasoning before final JSON output.
- Core assumption: Symbolic compression preserves sufficient semantic information for accurate criticality assessment.
- Evidence anchors: [section III.B.1] "Chain of Symbols (CoS), where objects that are detected are given arbitrary symbols, and the physical relations constructed between each object are also given a symbol."
- Break condition: If scene complexity exceeds symbol vocabulary or ambiguous relations arise, symbolic representation may lose critical nuance.

## Foundational Learning

- **Behavior Trees (BTs)**
  - Why needed here: Core representation for autonomous vehicle decision logic; understanding node types (Sequence, Selector, leaf actions) is prerequisite to interpreting generated BTs.
  - Quick check question: Given a Sequence node with children [CheckPath, DriveForward], what happens if CheckPath returns Failure?

- **In-Context Learning (Few-Shot Prompting)**
  - Why needed here: All three agents use few-shot examples rather than fine-tuning; understanding how examples shape LLM output is critical for prompt engineering.
  - Quick check question: Why might temperature=0.0 be necessary for a safety-critical pipeline generating executable behavior trees?

- **ROS2 + Nav2 Architecture**
  - Why needed here: The framework integrates with Nav2's existing BT system; understanding ROS2 nodes, topics, and Nav2's BT structure explains integration points.
  - Quick check question: What ROS2 component in Nav2 would publish the "baseline BT failed" signal that triggers the agentic pipeline?

## Architecture Onboarding

- **Component map:**
  CARLA Sim → Sensors (RGB cam, LiDAR, Odometry) → ROS2 Bridge → SLAM Layer (slam_toolbox) → Map → Nav2 Baseline → BT XML → NavigateToPose → (on Failure) → Agentic Framework (Descriptor → Planner → Generator) → Updated BT XML → Nav2 Execution → Vehicle Control (MPPI Controller)

- **Critical path:** Sensor data → Nav2 baseline BT failure detection (trigger) → Descriptor criticality assessment (filter) → Planner sub-goal decomposition → Generator XML synthesis → Nav2 BT execution. If Descriptor returns isCritical=false, pipeline halts.

- **Design tradeoffs:**
  - Latency vs. Adaptability: 21.36s average generation time is too slow for real-time collision avoidance; suitable only for prolonged blockage scenarios where vehicle can safely wait.
  - API vs. Local LLMs: Remote API calls (GPT-4o-mini) add network latency; local deployment could reduce latency but may trade generation quality.
  - Structured Output vs. Flexibility: Requiring JSON handoff between agents ensures compatibility but constrains information flow; malformed outputs break the pipeline.

- **Failure signatures:**
  - Descriptor MAE=0.5 on criticality labels (relatively high for binary decision)
  - Generated BTs may require parameter tuning (e.g., turn angles) before successful execution
  - No formal verification of generated BTs; unsafe behaviors possible
  - High token consumption (45,111 tokens/scene) may hit rate limits in production

- **First 3 experiments:**
  1. Reproduce baseline failure trigger: Set up CARLA+Nav2, spawn fire truck blocking lane, verify Nav2 baseline BT returns Failure and triggers agentic pipeline. Log trigger latency.
  2. Isolate Descriptor accuracy: Run Descriptor agent on BDD-X subset with ground-truth criticality labels; compute confusion matrix beyond MAE to understand false positive/negative rates.
  3. End-to-end BT execution test: Execute generated BT from paper's scenario; compare trajectory against expected "backup → turn → rejoin path" behavior. Identify any parameter tuning requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can formal verification methods be integrated into the agentic pipeline to guarantee the safety and correctness of LLM-generated behavior trees before execution?
- Basis in paper: [explicit] The authors state that "The generated BTs are not formally verified, and... there is no guarantee that it will always function correctly."
- Why unresolved: The current framework relies on simulation testing and manual review of XML structures, lacking mathematical proofs of safety or liveness properties required for autonomous driving.
- What evidence would resolve it: A modified pipeline that includes a formal verifier agent or tool that validates the generated BT against predefined safety constraints (e.g., avoiding collision) prior to vehicle actuation.

### Open Question 2
- Question: Can locally deployed LLMs achieve the sub-second latency required for real-time navigation without degrading the reasoning quality observed in cloud-based models?
- Basis in paper: [explicit] The paper reports an average generation time of 21.36 seconds, noting this "presents a challenge for deployment" and suggesting future iterations "could significantly reduce latency by using locally deployed LLMs."
- Why unresolved: While local deployment removes network latency, the authors note it may introduce a trade-off in generation quality if smaller, less capable models are used.
- What evidence would resolve it: Benchmarks showing comparable BT validity and scene interpretation accuracy (MAE ~0.5) with generation times under 1 second using on-board hardware.

### Open Question 3
- Question: Does the current agentic architecture possess sufficient reasoning depth to generalize from static lane blockages to dynamic, time-sensitive scenarios?
- Basis in paper: [inferred] While the authors explicitly mention the system "struggles with complex patterns" (e.g., dynamic obstacles), they hypothesize that "a more intricate agentic architecture" is needed to resolve this.
- Why unresolved: The current single-chain pipeline (Descriptor→Planner→Generator) was tested primarily on static obstacles (fire trucks) and may lack the temporal reasoning or feedback loops necessary for moving hazards.
- What evidence would resolve it: Successful generation and execution of BTs in scenarios involving moving obstacles (e.g., merging into traffic or crossing pedestrians) without human intervention.

## Limitations
- Generation latency (21.36s average) renders the system unsuitable for time-critical obstacle avoidance, limiting deployment to scenarios where the vehicle can safely wait.
- No formal verification of generated BTs means safety-critical behaviors could emerge undetected, with only manual review catching obvious errors.
- High token consumption (45,111 tokens/scene) raises concerns about API rate limits and cost in production environments.

## Confidence
- **High Confidence:** The failure-triggered mechanism is clearly defined and technically sound. The sequential agent pipeline with structured JSON handoffs is well-documented and reproducible.
- **Medium Confidence:** The Chain-of-Symbols prompting technique shows promise but lacks comparative evaluation against standard CoT approaches. Descriptor accuracy (MAE=0.5) is reported but not contextualized against baseline alternatives.
- **Low Confidence:** Integration parameters (Nav2 configuration, BT parameter tuning values) are insufficiently specified for exact reproduction. Safety validation of generated BTs is limited to manual review without formal verification.

## Next Checks
1. **Safety Verification Test:** Execute generated BTs in CARLA with safety constraints (minimum clearance, speed limits). Log any near-collision events or unsafe parameter values.
2. **Latency Profiling:** Instrument each agent to measure individual processing times. Identify bottlenecks and test whether batching or caching reduces overall generation time.
3. **Descriptor Robustness:** Run Descriptor agent on BDD-X subset with ground-truth criticality labels. Compute confusion matrix and false positive/negative rates to understand reliability in safety-critical scenarios.