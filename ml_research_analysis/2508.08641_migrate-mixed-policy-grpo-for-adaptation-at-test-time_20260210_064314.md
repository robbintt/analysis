---
ver: rpa2
title: 'MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time'
arxiv_id: '2508.08641'
source_url: https://arxiv.org/abs/2508.08641
tags:
- migrate
- sampling
- wang
- search
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MIGRATE is a method for online test-time training of large language\
  \ models using mixed-policy Group Relative Policy Optimization (GRPO) to solve black-box\
  \ optimization tasks without external data. It combines on-policy sampling with\
  \ two off-policy techniques\u2014greedy sampling of top past completions and neighborhood\
  \ sampling of variations from those completions\u2014to balance exploration and\
  \ exploitation."
---

# MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time

## Quick Facts
- arXiv ID: 2508.08641
- Source URL: https://arxiv.org/abs/2508.08641
- Reference count: 40
- Primary result: MIGRATE achieves up to 25 percentage point improvements in solution rates over inference-only and TTT baselines on word search, molecule optimization, and ARC program synthesis.

## Executive Summary
MIGRATE introduces a mixed-policy Group Relative Policy Optimization (GRPO) framework for online test-time training of large language models to solve black-box optimization tasks without external data. It combines on-policy sampling for exploration, greedy sampling of top past completions for exploitation, and neighborhood sampling of variations from those completions to balance exploration and exploitation. Evaluated on word search (Semantle), molecule optimization (Dockstring), and ARC program synthesis, MIGRATE consistently outperforms inference-only and TTT baselines, achieving up to 25 percentage point improvements in solution rates.

## Method Summary
MIGRATE is an online test-time training method that uses mixed-policy GRPO to optimize LLMs for black-box tasks. It constructs groups of samples from three sources: on-policy completions (exploration), greedy samples from a database of top-performing past completions (exploitation), and neighborhood variations generated from greedy samples (local exploration). The GRPO loss computes advantages relative to the group mean, updating LoRA adapters to maximize task rewards. This approach addresses sparse reward challenges by reusing high-quality past solutions while maintaining exploration through on-policy sampling.

## Key Results
- Semantle: MIGRATE achieves 65.3% solution rate vs. 41.2% for inference-only and 57.6% for TTT
- Dockstring: MIGRATE scores 4.24 (scaled) vs. 3.33 for inference-only and 3.53 for TTT
- ARC: MIGRATE achieves 40.8% Pass@2% vs. 21.8% for inference-only and 23.5% for TTT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed-policy group construction biases policy gradients toward high-reward regions while maintaining exploration diversity.
- Mechanism: By combining on-policy samples with greedy samples and neighborhood sampling, the GRPO loss receives a comparative signal that amplifies high-reward solutions relative to the group mean. The advantage becomes meaningfully positive for strong candidates and negative for weak ones, yielding productive gradients.
- Core assumption: The black-box reward function provides sufficiently discriminative signals; the solution space exhibits local continuity such that neighborhood variations correlate with reward changes.
- Evidence anchors: [abstract] "Together, these components bias the policy gradient towards exploitation of promising regions in solution space, while preserving exploration through on-policy sampling."
- Break condition: If all samples in a group receive identical rewards, advantages collapse to zero and no gradient update occurs.

### Mechanism 2
- Claim: Reusing high-quality past completions as off-policy data addresses sparse reward challenges in limited-budget search.
- Mechanism: Greedy sampling stores top-performing solutions in database and re-injects them into subsequent GRPO groups, ensuring positive advantage signals even if on-policy sampling fails to discover high-reward candidates.
- Core assumption: Past high-reward solutions remain informative for the current policy distribution; no catastrophic distribution shift between iterations.
- Evidence anchors: [abstract] "greedy sampling, which selects top-performing past completions"
- Break condition: If the task requires fundamentally different solution structures across iterations, replaying old solutions may bias search into local optima.

### Mechanism 3
- Claim: Neighborhood sampling generates structured variations that enable local exploration without requiring external data.
- Mechanism: Given greedy samples, NS constructs a prompt instructing the model to produce variations, which are more likely to land in high-reward neighborhoods while providing diversity beyond exact replay.
- Core assumption: Small perturbations to solutions yield correlated reward changes; the LLM can generate semantically meaningful variations via prompting.
- Evidence anchors: [abstract] "neighborhood sampling (NS), which generates completions structurally similar to high-reward ones"
- Break condition: If the reward landscape is discontinuous or highly multimodal, neighborhood variations may be no better than random samples.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO replaces the critic network with group-based advantage estimation, simplifying RL for LLMs while enabling test-time adaptation.
  - Quick check question: Can you explain how $\hat{A}_{i,t} = r_i - \text{mean}(r_{\text{group}})$ differs from a baseline-critic advantage?

- Concept: **On-policy vs. Off-policy RL**
  - Why needed here: MIGRATE mixes on-policy (fresh samples) and off-policy (replayed samples) data; understanding the trade-offs (exploration vs. exploitation, stability) is essential.
  - Quick check question: What are the risks of purely off-policy training (e.g., distribution mismatch) and why does on-policy sampling mitigate them?

- Concept: **Test-Time Training (TTT)**
  - Why needed here: MIGRATE is an online TTT method—updating model weights at inference for a single query—rather than pre-training or fine-tuning on a fixed dataset.
  - Quick check question: How does TTT differ from fine-tuning on a held-out validation set?

## Architecture Onboarding

- Component map: Task Prompt -> Sampler -> Database -> Reward Function -> GRPO Loss -> LoRA Adapter
- Critical path: 1) Initialize database with warmstart samples. 2) For each iteration: sample on-policy completions → retrieve greedy samples → generate NS variations → form group G. 3) Compute rewards; update LoRA via GRPO loss. 4) Store new samples; check for optimal solution. 5) Repeat until budget exhausted or solution found.
- Design tradeoffs:
  - $\alpha, \beta, \gamma$ ratios: High $\alpha$ favors exploration; high $\beta$ risks premature convergence; high $\gamma$ depends on landscape smoothness.
  - Group size N: Larger groups provide more stable advantage estimates but increase compute.
  - Learning rate and LoRA rank: Higher LR accelerates adaptation but may destabilize; rank controls expressivity.
- Failure signatures:
  - Zero gradients: All rewards identical → advantage zero → no update. Consider reward shaping or larger group.
  - Mode collapse: Only greedy samples survive → reduce $\beta$, increase $\alpha$.
  - High invalid rate: NS produces many invalid outputs (e.g., in molecule generation). Consider constrained decoding or filtering.
- First 3 experiments:
  1. Ablation on $\alpha, \beta, \gamma$: Run MIGRATE with $(\alpha=N, \beta=0, \gamma=0)$, $(\alpha=0, \beta=N, \gamma=0)$, $(\alpha=0, \beta=0, \gamma=N)$, and balanced configs on Semantle.
  2. Budget sensitivity: Evaluate solution quality vs. sampling budget (100, 500, 1000) to understand compute-quality tradeoffs.
  3. Generalization test: Apply learned LoRA weights from one ARC task to its "nearest" unsolved task to probe transferability.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does MIGRATE maintain its performance advantages when applied to multi-step decision-making tasks rather than single-step search problems? The paper notes future work may include scaling to multi-step decision-making, as current focus is on search tasks with finite sampling budgets.
- **Open Question 2**: Can uncertainty-aware acquisition strategies be integrated into the mixed-policy sampling to reduce the total sampling budget? The paper identifies integrating stronger uncertainty-aware acquisition strategies as a specific direction for future work.
- **Open Question 3**: Can the mixed-policy composition ($\alpha, \beta, \gamma$) be dynamically adjusted or meta-learned to remove the need for domain-specific tuning? The paper notes sensitivity analysis highlights the importance of tuning mixed-policy composition for each domain.

## Limitations
- **Task dependency**: Performance gains are highly task-dependent, with modest improvements on complex reasoning tasks like ARC.
- **Hyperparameter sensitivity**: Optimal $\alpha:\beta:\gamma$ ratios require domain-specific tuning rather than a universal configuration.
- **Continuity assumption**: Neighborhood sampling effectiveness depends on local smoothness in solution space, which may not hold for all black-box optimization problems.

## Confidence
- **High confidence**: MIGRATE's core GRPO formulation is well-specified and reproducible, with clear performance improvements over baselines.
- **Medium confidence**: Relative contributions of on-policy, greedy, and neighborhood sampling are demonstrated but not precisely quantified.
- **Low confidence**: Claims about generalization across arbitrary black-box optimization tasks are not fully supported by the experimental evidence.

## Next Checks
1. **Ablation on $\alpha, \beta, \gamma$**: Run MIGRATE with $(\alpha=N, \beta=0, \gamma=0)$, $(\alpha=0, \beta=N, \gamma=0)$, $(\alpha=0, \beta=0, \gamma=N)$, and balanced configs on a single task (e.g., Semantle) to quantify each component's contribution.
2. **Budget sensitivity**: Evaluate solution quality vs. sampling budget (100, 500, 1000) to understand compute-quality tradeoffs.
3. **Generalization test**: Apply learned LoRA weights from one ARC task to its "nearest" unsolved task (as in Appendix B.2) to probe transferability of test-time adaptations.