---
ver: rpa2
title: Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward
arxiv_id: '2509.01321'
source_url: https://arxiv.org/abs/2509.01321
tags:
- training
- samples
- data
- selection
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEPO is a data-efficient reinforcement learning framework for large
  reasoning models that combines optimized offline and online data selection strategies.
  The offline phase uses PageRank-weighted Determinantal Point Process pruning and
  difficulty-aware normal distribution sampling to curate high-quality subsets emphasizing
  diversity, influence, and appropriate difficulty.
---

# Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward

## Quick Facts
- arXiv ID: 2509.01321
- Source URL: https://arxiv.org/abs/2509.01321
- Reference count: 33
- Primary result: DEPO achieves 1.85× speedup on AIME24 and 1.66× speedup on AIME25 using only 20% of training data

## Executive Summary
DEPO introduces a data-efficient reinforcement learning framework for large reasoning models that combines optimized offline and online data selection strategies. The framework achieves comparable or superior performance to full-dataset training while using only 20% of the training data, demonstrating significant speedups across five reasoning benchmarks. The approach integrates PageRank-weighted Determinantal Point Process pruning with difficulty-aware sampling in the offline phase, and sample-level explorability metrics with dynamic replay in the online phase.

## Method Summary
DEPO operates through two phases: offline data curation and online training optimization. The offline phase constructs a sample graph from last-token embeddings, applies PageRank-weighted Determinantal Point Process pruning to select diverse and influential samples, generates offline rollouts to estimate difficulty, and samples from a normal distribution centered on moderate difficulty. The online phase dynamically prunes rollouts using sample-level explorability metrics (entropy-based) and incorporates replay for under-explored samples. The framework uses GRPO training with specific hyperparameters and evaluates across multiple reasoning benchmarks.

## Key Results
- 20% data usage achieves 1.85× speedup on AIME24 and 1.66× speedup on AIME25 compared to full-dataset training
- Consistent improvements across five reasoning benchmarks with three different LLMs
- Ablation studies show each component (DPP pruning, difficulty sampling, explorability, replay) contributes meaningfully to performance

## Why This Works (Mechanism)

### Mechanism 1: PageRank-Weighted DPP for Diversity-Influence Pruning
Combines Determinantal Point Process (maximizing determinant of similarity submatrix for diversity) with PageRank weights (capturing representativeness) to select samples that maximize both feature space coverage and pedagogical value. The optimization jointly maximizes diversity and influence through a unified formulation.

### Mechanism 2: Difficulty-Aware Normal Distribution Sampling
Samples training examples whose difficulty follows a normal distribution centered on moderate difficulty, aligning data with model capability. After DPP pruning, generates offline rollouts to compute accuracy as difficulty proxy, then samples with probability proportional to distance from mean difficulty.

### Mechanism 3: Explorability-Guided Rollout Pruning with Dynamic Replay
Prioritizes rollouts on samples with high recent entropy (explorability) while replaying under-explored samples to prevent coverage gaps. Uses sliding window entropy aggregation with linear decay to shift from exploration to refinement over training epochs.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Binary reward framework where models receive rewards based on answer correctness. Essential for understanding rollout generation, policy gradients, and reward sparsity in DEPO.
  - Quick check: Can you explain why rollout computation is the main bottleneck in RLVR training?

- **Determinantal Point Processes (DPP)**: Probability model over subsets where maximizing determinant of similarity submatrix promotes diversity. Core to offline diversity selection in DEPO.
  - Quick check: Why does maximizing the determinant of a similarity submatrix promote diversity?

- **Entropy as Exploration Signal**: Token-level entropy used to identify samples likely to yield diverse reasoning paths. Critical for understanding the online explorability metric.
  - Quick check: How would extremely high-entropy samples differ from moderately high-entropy samples in terms of training value?

## Architecture Onboarding

- Component map:
Raw Dataset D → Offline Phase (Embedding Extraction → Sample Graph G → PageRank-weighted DPP Pruning → Offline Rollouts → Accuracy Estimation → Normal Distribution Sampling → 20% subset D_sub) → Online Phase (Batch from D_sub → Explorability Scoring → Top α_e% + Bottom ρ% replay → Rollout Generation → Policy Gradient Update)

- Critical path: Offline DPP pruning → difficulty sampling → online explorability filtering. Each stage compounds; errors propagate.

- Design tradeoffs:
  - 20% data ratio: Lower = faster but risks missing coverage; higher = diminishing returns
  - Decay rate d=0.05: Higher = faster but under-trains samples; lower = more rollouts
  - Replay ratio ρ=0.05: Higher = redundant compute; lower = coverage gaps on hard samples
  - Threshold λ=1.5: Lower = filters useful exploration; higher = includes noise

- Failure signatures:
  - Performance plateaus early → μ too high (samples too easy)
  - AIME25 underperforms AIME24 → replay ratio too low, hard samples under-trained
  - Rollout count doesn't decrease → explorability scores flat
  - Selected samples cluster in embedding space → DPP kernel degenerate

- First 3 experiments:
  1. Run DEPO-Offline only (no online filtering) on 20% data vs. GRPO full data
  2. Remove one component at a time (no DPP, no difficulty sampling, no explorability, no replay) and quantify drops on AIME24/25
  3. Vary μ ∈ {0.25, 0.5, 0.75} and σ ∈ {0.05, 0.2, 0.5} to find task-optimal difficulty distribution

## Open Questions the Paper Calls Out

### Open Question 1
Can the offline difficulty distribution be made dynamic to align with the model's evolving capability, rather than relying on static normal distribution sampling? The current method fixes the distribution offline, but experiments show a trade-off where medium-difficulty data speeds up early training while including harder examples is necessary for final convergence.

### Open Question 2
Does the high data efficiency of DEPO generalize to domains outside of mathematical and code reasoning where rewards are less strictly verifiable? The experiments are confined to reasoning benchmarks, and it's unclear if the explorability metric remains reliable with noisy or subjective reward signals.

### Open Question 3
Is the removal of KL divergence and entropy losses a prerequisite for the effectiveness of the DEPO pipeline? The authors remove these standard losses following prior work, leaving the interaction between these constraints and the new selection methods untested.

## Limitations
- Embedding-based similarity may not capture task-relevant reasoning structure, potentially optimizing for superficial features
- Offline difficulty estimation becomes stale as model capability shifts during training
- Fixed window size and decay rate for explorability metrics may not adapt well to different learning speeds across tasks

## Confidence
- High confidence: Data efficiency claims (20% training data achieving comparable performance) are well-supported by ablation results
- Medium confidence: The combination of PageRank-weighted DPP and difficulty sampling improves offline selection quality
- Medium confidence: Online explorability metrics reduce compute waste, but optimal parameters may be task-specific

## Next Checks
1. **Embedding relevance validation**: Compare DPP-selected diversity using last-token embeddings versus reasoning-relevant embeddings to quantify correlation with pedagogical value
2. **Dynamic difficulty adaptation**: Implement online difficulty re-estimation during training and measure improvement over fixed normal distribution sampling
3. **Parameter sensitivity analysis**: Systematically vary explorability window size, threshold, and decay rate across all five benchmarks to identify robust configurations