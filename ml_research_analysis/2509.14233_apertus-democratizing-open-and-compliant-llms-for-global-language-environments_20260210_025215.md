---
ver: rpa2
title: 'Apertus: Democratizing Open and Compliant LLMs for Global Language Environments'
arxiv_id: '2509.14233'
source_url: https://arxiv.org/abs/2509.14233
tags:
- data
- training
- arxiv
- tokens
- apertus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Apertus is a fully open LLM suite trained on 15T tokens from 1811
  languages, prioritizing data compliance, multilingual representation, and transparency.
  It uses retroactive robots.txt filtering, PII/toxicity removal, and Goldfish loss
  to suppress memorization while maintaining performance.
---

# Apertus: Democratizing Open and Compliant LLMs for Global Language Environments

## Quick Facts
- arXiv ID: 2509.14233
- Source URL: https://arxiv.org/abs/2509.14233
- Reference count: 40
- Apertus is a fully open LLM suite trained on 15T tokens from 1811 languages, prioritizing data compliance, multilingual representation, and transparency.

## Executive Summary
Apertus is a fully open LLM suite trained on 15T tokens from 1811 languages, prioritizing data compliance, multilingual representation, and transparency. It uses retroactive robots.txt filtering, PII/toxicity removal, and Goldfish loss to suppress memorization while maintaining performance. Post-training yields Apertus-8B/70B-Instruct models with 40% non-English pretraining data and alignment to Swiss constitutional values. Apertus achieves state-of-the-art multilingual performance among open models, with 8B and 70B variants approaching or surpassing open-weight counterparts on cultural, knowledge, and instruction-following benchmarks. All training data, code, and artifacts are publicly released under permissive licenses.

## Method Summary
Apertus trains dense decoder-only Transformers (8B and 70B parameters) on 15T tokens using a curriculum of 5 data stages, with ~40% non-English content from 1811 languages. Goldfish loss masks ~2% of tokens to suppress memorization. Pretraining uses AdEMAMix optimizer, WSD schedule, and custom xIELU activation. Post-training applies SFT on 4.2M examples followed by QRPO alignment with Swiss constitutional values. Data compliance includes retroactive robots.txt filtering, PII/toxicity removal, and license checks.

## Key Results
- Apertus-70B achieves highest score on multilingual XCOPA benchmark among open models
- 8B and 70B models surpass all other fully open models on INCLUDE V1 and V2 benchmarks
- Goldfish loss maintains baseline memorization (Rouge-L≈0.18) while retaining downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token masking during pretraining reduces verbatim memorization without degrading downstream task performance.
- **Mechanism:** Goldfish loss selectively masks ~2% of tokens per sequence using a hash of the preceding 50-token context window. This prevents the model from forming stable, verbatim recall pathways while still learning generalizable patterns.
- **Core assumption:** The specific masking rate (k=50) and context window (h=50) balance memorization suppression and capability retention at scale.
- **Evidence anchors:**
  - [abstract]: "...Goldfish objective...strongly suppressing verbatim recall of data while retaining downstream task performance."
  - [section 2.3]: "The goldfish loss computes the causal language modeling objective on only a subset of tokens..."
  - [section 5.4.1]: "Both Apertus-8B and Apertus-70B remain at baseline memorization (Rouge-L≈0.18)...neither model exhibits memorization across any tested exposure frequency (≤128)."
- **Break condition:** If downstream benchmarks show significant degradation beyond natural variance, the masking parameters may be too aggressive.

### Mechanism 2
- **Claim:** Large-scale multilingual pretraining with balanced data ratios enables cross-lingual transfer and strong performance on non-English benchmarks.
- **Mechanism:** Allocating ~40% of pretraining data to non-English sources from 1811 languages, combined with a tokenizer optimized for multilingual fairness (low Gini coefficient), ensures the model learns language-agnostic representations.
- **Core assumption:** Multilingual diversity in pretraining translates to capability across languages, not just surface-level translation.
- **Evidence anchors:**
  - [abstract]: "~40% of pretraining data allocated to non-English content...1811 languages."
  - [section 5.1]: "Apertus-70B achieves the highest score...on the multilingual XCOPA benchmark...surpass all other fully open models on INCLUDE V1 and V2."
  - [section 2.2]: "Mistral-Nemo achieves the lowest Gini coefficient, indicating more equitable tokenization costs across languages."
- **Break condition:** If performance gaps between high-resource and low-resource languages widen dramatically, the data balance or tokenization fairness may be insufficient.

### Mechanism 3
- **Claim:** Retroactive compliance filtering creates a legally defensible training corpus without requiring new web crawls.
- **Mechanism:** By applying January 2025 robots.txt restrictions to historical crawl data (2013-2024) and filtering for toxic content, PII, and non-permissive licenses, the training corpus respects data owner consent as of a recent, fixed point in time.
- **Core assumption:** Retroactive application of current robots.txt preferences is legally sufficient under frameworks like the EU AI Act (unproven in court).
- **Evidence anchors:**
  - [abstract]: "...retroactively respecting `robots.txt` exclusions and filtering for non-permissive, toxic, and personally identifiable content."
  - [section 3.1.1]: "Any contents blocked by the current robots.txt is removed retroactively from the entire 2013-2024 range."
  - [corpus]: Weak external validation; related work on LLM training data search exists but does not directly address retroactive compliance.
- **Break condition:** If legal challenges arise claiming retroactive filtering is insufficient, the entire compliance approach would need re-evaluation.

## Foundational Learning

- **Concept:** Causal language modeling with selective masking
  - **Why needed here:** Goldfish loss modifies the standard next-token prediction objective; understanding how masking changes gradient flow is essential for debugging or extending the approach.
  - **Quick check question:** Can you explain why masking 2% of tokens would prevent memorization of repeated sequences while still allowing general language learning?

- **Concept:** Data provenance and compliance frameworks (EU AI Act, robots.txt conventions)
  - **Why needed here:** Apertus's core value proposition is compliance; you must understand the legal and ethical reasoning behind retroactive opt-outs and license filtering.
  - **Quick check question:** Why would applying current robots.txt rules to older crawl data be controversial, and what alternatives exist?

- **Concept:** Multilingual tokenization fairness (compression ratio, fertility, Gini coefficient)
  - **Why needed here:** The tokenizer selection was critical for equitable multilingual performance; understanding these metrics helps assess if a tokenizer is biased toward certain languages.
  - **Quick check question:** Given two tokenizers with similar compression ratios, why might the one with a lower Gini coefficient be preferred for a multilingual model?

## Architecture Onboarding

- **Component map:** Data preparation (compliance filters → tokenization → Goldfish masking) → Pretraining (WSD schedule with 5 data stages) → Long-context extension (RoPE θ scaling, context parallelism) → Post-training (SFT → QRPO alignment with Swiss AI Charter)

- **Critical path:** Data preparation → Pretraining → Long-context extension → Post-training

- **Design tradeoffs:**
  - **Openness vs. performance:** Strict compliance filtering (robots.txt, license restrictions) reduced some benchmark scores (e.g., MMLU CoT from 0.513 to 0.253 with full filtering).
  - **Memorization vs. capability:** Goldfish loss adds complexity and may slightly affect convergence, though the paper shows minimal downstream impact.
  - **Multilingual breadth vs. depth:** With 1811 languages, many are extremely low-resource; performance may vary dramatically despite balanced data ratios.

- **Failure signatures:**
  - **Loss spikes during long-context training:** Check RoPE scaling factors and context parallelism configuration.
  - **High memorization on specific sequences:** May indicate near-duplicates in training data that bypass Goldfish hashing (e.g., different whitespace or line breaks).
  - **Gradient instability with AdEMAMix:** This optimizer is more sensitive to gradient clipping values; the paper uses aggressive clipping (0.1).

- **First 3 experiments:**
  1. **Reproduce a small-scale Goldfish ablation:** Train a 1B model on 100B tokens with and without Goldfish loss on a subset of the data. Measure Rouge-L memorization scores and downstream benchmark performance to validate the tradeoff.
  2. **Tokenizer fairness comparison:** Run the tokenizer evaluation (fertility, compression, Gini) on a sample of the pretraining data to understand the multilingual equity of the chosen Mistral-Nemo tokenizer.
  3. **Compliance filter impact analysis:** Starting from a small, fully filtered dataset, progressively relax compliance filters (robots.txt, PII, toxicity) and measure the change in token count, benchmark performance, and memorization to quantify the compliance-performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does aggressive gradient clipping (0.1) with the AdEMAMix optimizer improve convergence stability, or does it hinder model performance at scale?
- **Basis in paper**: [explicit] Section 2.6 notes clipping is applied almost every step but states, "It remains an interesting question to understand its necessity and the effects on training."
- **Why unresolved**: The authors observed stable training without major loss spikes but did not perform ablations on the clipping threshold at the final scale to confirm if it is strictly necessary or detrimental.
- **What evidence would resolve it**: Ablation studies at the 70B scale comparing convergence rates and downstream benchmarks between the 0.1 clipping threshold and higher or disabled thresholds.

### Open Question 2
- **Question**: Does the order of data presentation during pretraining affect verbatim memorization rates (a "primacy effect")?
- **Basis in paper**: [explicit] Section 5.4.2 observes that sequences introduced earlier in pretraining appeared more strongly memorized, but notes this "may be confounded by differences in textual complexity... therefore warrants further investigation."
- **Why unresolved**: The Gutenberg V1 and V2 probe sets differed in content and complexity, making it impossible to isolate the "primacy effect" (timing) from data characteristics.
- **What evidence would resolve it**: Controlled experiments injecting identical probe sequences at different pretraining intervals (e.g., 0T vs. 9T) while controlling for text complexity.

### Open Question 3
- **Question**: Why did the learning rate cooldown phase fail to produce a significant performance jump for the Apertus-70B model?
- **Basis in paper**: [explicit] Section 2.6 states, "It remains unclear why this was the case" regarding the 70B model's lack of cooldown response, hypothesizing the peak learning rate was set too low.
- **Why unresolved**: Project schedules prevented the authors from establishing scaling rules for learning rates or running multiple experimental values at the 70B scale.
- **What evidence would resolve it**: A series of large-scale training runs specifically targeting optimal peak learning rate identification for the stable and cooldown phases of the WSD schedule.

## Limitations

- **Legal Compliance Framework:** The core claim of "full compliance" rests on retroactive robots.txt filtering and license checks, but the legal sufficiency of this approach remains untested. The EU AI Act's requirements for data governance are still being interpreted, and retroactive opt-outs may not withstand future litigation.
- **Generalization to Low-Resource Languages:** While Apertus trains on 1811 languages with 40% non-English data, the paper provides limited analysis of actual performance distribution across language families. Many of these languages have minimal training data, raising questions about whether the model truly serves them or merely includes them for diversity metrics.
- **Evaluation Completeness:** The multilingual benchmark suite is comprehensive but may still underrepresent certain domains. Safety evaluations focus on standard benchmarks without deep qualitative analysis of edge cases or cultural nuances in the 40+ languages tested.

## Confidence

- **High Confidence:** The technical implementation of Goldfish loss, tokenizer fairness metrics, and standard pretraining/post-training procedures. The paper provides sufficient detail and code for verification.
- **Medium Confidence:** Multilingual performance claims relative to other open models. While benchmarks are provided, the paper doesn't fully address whether absolute performance levels are sufficient for practical use across all 1811 languages.
- **Low Confidence:** Legal compliance claims and the assertion that the model serves "global language environments" meaningfully. These depend on factors outside the paper's control and scope.

## Next Checks

1. **Legal Compliance Audit:** Engage legal experts to evaluate whether retroactive robots.txt filtering and license checking meet current and anticipated regulatory requirements under the EU AI Act and other relevant frameworks. Test the approach with a small, controlled dataset where robots.txt preferences are known to have changed over time.

2. **Low-Resource Language Performance Analysis:** Select 10-15 languages from the lower quartiles of training data allocation. Conduct qualitative human evaluation of model outputs in these languages across multiple domains (instruction following, factual knowledge, creative tasks) to determine if the multilingual pretraining actually translates to usable capabilities.

3. **Goldfish Loss Ablation Study:** Reproduce the memorization suppression claims by training two small models (1B parameters) on identical data: one with standard causal language modeling and one with Goldfish loss (k=50, h=50). Measure verbatim recall rates using Rouge-L and assess downstream task performance to validate the memorization-capability tradeoff claimed in the paper.