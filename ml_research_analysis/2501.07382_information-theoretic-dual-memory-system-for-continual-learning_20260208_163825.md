---
ver: rpa2
title: Information-Theoretic Dual Memory System for Continual Learning
arxiv_id: '2501.07382'
source_url: https://arxiv.org/abs/2501.07382
tags:
- memory
- learning
- data
- samples
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Information-Theoretic Dual Memory System
  (ITDMS) for continual learning, addressing catastrophic forgetting by implementing
  rapid and gradual learning mechanisms inspired by the Complementary Learning Systems
  (CLS) theory. The approach uses a fast memory buffer for recent samples (managed
  via reservoir sampling) and a slow memory buffer for informative samples (selected
  using an information-theoretic optimization strategy based on Renyi entropy and
  Cauchy-Schwarz divergence).
---

# Information-Theoretic Dual Memory System for Continual Learning

## Quick Facts
- **arXiv ID:** 2501.07382
- **Source URL:** https://arxiv.org/abs/2501.07382
- **Reference count:** 40
- **Primary result:** Dual memory system using Renyi entropy and Cauchy-Schwarz divergence outperforms state-of-the-art CL methods, especially in imbalanced scenarios.

## Executive Summary
This paper introduces an Information-Theoretic Dual Memory System (ITDMS) for continual learning that addresses catastrophic forgetting through a biologically-inspired dual memory architecture. The system implements rapid learning in a fast memory buffer using reservoir sampling and gradual learning in a slow memory buffer through information-theoretic sample selection. By optimizing for diversity (Renyi entropy) and distributional similarity (Cauchy-Schwarz divergence), ITDMS effectively mitigates forgetting while maintaining performance on imbalanced data streams. The approach shows superior results compared to existing methods like DER++, EWC, and GDUMB across multiple benchmarks including CIFAR-10, Tiny ImageNet, and MNIST variants.

## Method Summary
ITDMS implements a dual-buffer architecture where the Fast Memory buffer uses reservoir sampling to maintain recent samples, while the Slow Memory buffer employs an information-theoretic optimization strategy to select diverse and representative samples. The sample selection process optimizes weights using Renyi entropy and Cauchy-Schwarz divergence with a Concrete distribution relaxation, allowing gradient-based optimization of discrete sample selection. A Balanced Sample Selection method ensures equitable class distribution by removing samples close to class centroids when capacity is exceeded. The model is trained on current data augmented with replayed samples from both buffers, using cross-entropy, distillation, and replay loss terms.

## Key Results
- ITDMS achieves 74.89% average accuracy on Split-CIFAR10 compared to 73.52% for DER++ and 68.83% for EWC
- On imbalanced Split-CIFAR10 (β=0.1), ITDMS maintains 72.71% accuracy versus 62.47% for DER++
- With 200 memory slots on imbalanced Split-CIFAR10, ITDMS achieves 69.31% accuracy compared to 55.28% for DER++
- On Sequential-Tiny ImageNet, ITDMS achieves 48.76% accuracy versus 45.91% for DER++ and 41.29% for GDUMB

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating memory into distinct fast and slow buffers implements a stability-plasticity balance, allowing the system to absorb novel data without destabilizing consolidated knowledge.
- **Mechanism:** The Fast Memory buffer uses reservoir sampling to maintain a uniform representation of the recent data stream (recent task). The Slow Memory buffer preserves "critical" samples selected via optimization. This physical separation prevents the aggressive overwriting typical of single-buffer reservoir sampling.
- **Core assumption:** Assumption: Recent data (fast) and statistically representative data (slow) contribute differently to mitigating catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] "...comprises a fast memory buffer designed to retain temporary and novel samples, alongside a slow memory buffer dedicated to preserving critical and informative samples."
  - [section 3.2] "We implement the fast memory buffer optimization strategy by employing reservoir sampling... a slow memory buffer... preserves long-term critical information."
  - [corpus] "Dynamic Dual Buffer with Divide-and-Conquer Strategy..." supports the efficacy of dual-buffer architectures in OCL.
- **Break condition:** If the training distribution is strictly stationary, the distinction between "recent" and "representative" vanishes, likely reducing the dual system to a redundant single buffer.

### Mechanism 2
- **Claim:** Optimizing sample selection using an information-theoretic cost function selects a subset of data that preserves the statistical properties of the original dataset better than random sampling.
- **Mechanism:** The system minimizes an objective function $L_{info}$ comprising second-order Rényi entropy (maximizing diversity) and Cauchy-Schwarz (CS) divergence (minimizing distributional discrepancy from the original set). It uses a continuous relaxation (Concrete distribution) to optimize discrete sample weights via gradient descent.
- **Core assumption:** Assumption: A subset that maximizes entropy and minimizes CS divergence relative to the full dataset provides a superior training signal for the gradient update step.
- **Evidence anchors:**
  - [abstract] "...selectively identifies and retains diverse and informative data samples... based on Renyi entropy and Cauchy-Schwarz divergence."
  - [section 3.3] "The term $H_2(\tilde{X}_i)$... aims to estimate the diversity... The term $D_{CS}(X_i, \tilde{X}_i)$... is used to quantify the discrepancy between the selected subset and the real training dataset."
  - [corpus] Corpus evidence for this specific optimization mix is weak; neighbors focus on generalization bounds rather than the specific Renyi/CS combination.
- **Break condition:** If the dataset is extremely high-dimensional and sparse, kernel density estimates (used in the entropy calculation) may become unreliable, causing the optimization to select redundant or noisy samples.

### Mechanism 3
- **Claim:** A balanced sample removal strategy (BSS) ensures equitable memory allocation across classes, preventing performance collapse in imbalanced data streams.
- **Mechanism:** Before adding new task data, the system calculates the distance of samples to their class centroids. It removes samples with low diversity scores (those closest to the centroid) to free up space, ensuring an equal number of samples per class remains in the Slow Buffer.
- **Core assumption:** Assumption: Proximity to the class centroid correlates with redundancy, and equal class representation is optimal for preventing bias in the replay buffer.
- **Evidence anchors:**
  - [abstract] "...balanced sample selection method ensures equitable distribution of samples across categories while removing redundancy."
  - [section 3.4] "Eq (16) quantifies the distance between the j-th stored sample... and the central sample... A low diversity score implies the data is similar... and can be eliminated."
  - [corpus] "Non-Uniform Memory Sampling..." relates to improved sampling strategies over standard reservoir methods.
- **Break condition:** In highly complex multi-modal distributions (where a class has distinct sub-clusters), removing samples close to the global centroid might inadvertently prune distinct sub-clusters, reducing intra-class diversity.

## Foundational Learning

- **Concept: Reservoir Sampling**
  - **Why needed here:** This is the baseline algorithm used for the Fast Memory buffer and the ablation study. You must understand its "uniform probability over the stream" property to contrast it with the Slow Memory's "optimized" selection.
  - **Quick check question:** How does reservoir sampling guarantee that any sample in the stream has an equal probability of being retained, regardless of stream length?

- **Concept: Rényi Entropy & Cauchy-Schwarz Divergence**
  - **Why needed here:** These are the mathematical drivers of the sample selection quality. You need to grasp that Rényi entropy (quadratic) measures "peakedness" or diversity, while CS divergence measures the overlap between the stored subset and the full data distribution.
  - **Quick check question:** Why would maximizing the quadratic Rényi entropy of a subset encourage diversity in the selected samples?

- **Concept: Complementary Learning Systems (CLS) Theory**
  - **Why needed here:** The paper explicitly grounds its dual-buffer architecture in this biological theory (Hippocampus vs. Neocortex). Understanding this provides the intuition for why the authors separate "fast" temporary storage from "slow" long-term storage.
  - **Quick check question:** In CLS theory, which system is responsible for the rapid acquisition of new experiences, and which for the gradual consolidation of knowledge?

## Architecture Onboarding

- **Component map:** Fast Memory (reservoir sampling) -> Slow Memory (ITMO optimization) -> Model training with replay -> Balanced Sample Selection (pruning)

- **Critical path:**
  1. **Training:** Model trains on current batch + replay batch (concatenated from $M_{fast}$ and $M_{slow}$).
  2. **Fast Update:** $M_{fast}$ updates online via reservoir sampling.
  3. **Task Transition (End of Task $T_i$):**
     - Run **BSS** on $M_{slow}$ to prune redundant samples and free space (Eq. 18).
     - Run **ITMO** on new task data $D_{i+1}$ to generate selection weights $w$ (Eq. 12).
     - Fill freed space in $M_{slow}$ with samples from $D_{i+1}$ based on $w$.

- **Design tradeoffs:**
  - **Computational Cost:** The ITMO optimization (Section 3.3) requires iterative gradient descent on sample weights *after* each task. This is significantly slower than simple reservoir sampling but yields higher quality samples.
  - **Hyperparameter Sensitivity:** The system relies heavily on balancing $\lambda_{H2}$ (entropy weight) and $\lambda_{CS}$ (divergence weight). Table 4 shows these vary by dataset.

- **Failure signatures:**
  - **Mode Collapse:** If $\lambda_{CS}$ is too high, the selected samples might overfit to the global mean, failing to capture outliers.
  - **Imbalance Amplification:** If the BSS step (centroid calculation) is buggy, minority classes might be under-represented in the Slow Buffer, causing the model to ignore them.

- **First 3 experiments:**
  1. **Ablation vs. Reservoir:** Implement the "ITDMS-reservoir" baseline mentioned in Section 4.5 to confirm that the performance gain comes from the information-theoretic selection, not just the dual-buffer structure.
  2. **Hyperparameter Sweep:** Run a grid search on $\lambda_{CS}$ and $\lambda_{H2}$ using the Gaussian mixture dataset (Fig 5) to visualize how the selected subset shifts between diversity and representativeness.
  3. **Imbalance Stress Test:** Replicate the imbalanced Split-CIFAR10 experiment (Fig 3) with a buffer size of 200 to verify that the BSS mechanism maintains accuracy on minority classes compared to standard DER++.

## Open Questions the Paper Calls Out

- **Question:** How can the computational efficiency of the Information-Theoretic Memory Optimization (ITMO) strategy be improved to reduce the high cost of iterative weight updates?
  - **Basis in paper:** [explicit] The conclusion states that a "significant drawback" is the need for "substantial computational resources" due to "numerous optimization iterations following each transition between tasks."
  - **Why unresolved:** The current method relies on a gradient descent optimization loop (Algorithm 2) to determine sample weights, which is time-consuming.
  - **What evidence would resolve it:** A proposed acceleration technique or convergence analysis showing reduced iterations while maintaining the quality of selected samples.

- **Question:** How does the choice of distance metric in the Balanced Sample Selection (BSS) approach affect the removal of redundant samples?
  - **Basis in paper:** [explicit] The methodology section states, "In this paper, we implement $f_d(\cdot, \cdot)$ using the cosine distance and other distance measures will be investigated in our future study."
  - **Why unresolved:** The paper currently relies solely on cosine distance to identify central samples and redundancy without comparing alternatives.
  - **What evidence would resolve it:** A comparative study evaluating the performance of ITDMS using Euclidean, Manhattan, or learned distance metrics on standard benchmarks.

- **Question:** Can a dynamic memory expansion strategy enable the system to manage an unbounded array of tasks by aligning capacity with data stream complexity?
  - **Basis in paper:** [explicit] The conclusion identifies the "inability to manage an unbounded array of tasks" due to fixed capacity and suggests introducing a "dynamic memory expansion strategy" as a viable approach.
  - **Why unresolved:** The current ITDMS uses a fixed-size buffer which eventually saturates, limiting scalability.
  - **What evidence would resolve it:** An extension of the ITDMS framework that dynamically adjusts buffer size based on task complexity or novelty, validated on extended task sequences.

## Limitations

- The paper relies heavily on empirical performance without providing theoretical guarantees for the information-theoretic optimization's effectiveness in sample selection.
- Key hyperparameters like the Concrete distribution temperature and specific data augmentation parameters are not fully specified, potentially affecting reproducibility.
- The claim that this dual-system is "biologically inspired" via CLS theory is qualitative; the connection to specific hippocampal-neocortical dynamics is not rigorously established.

## Confidence

- **High Confidence:** The dual-buffer architecture and its implementation (reservoir sampling for Fast, BSS for pruning Slow) are clearly described and reproducible.
- **Medium Confidence:** The information-theoretic sample selection (ITMO) is well-defined mathematically, but its superiority over simpler heuristics is primarily supported by benchmark results rather than ablation studies isolating its impact.
- **Low Confidence:** The paper's claim of being "the first biologically inspired IT solution" is difficult to verify without a comprehensive survey of the continual learning literature for prior biologically-motivated work.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Conduct a grid search over λCS and λH2 on the Gaussian mixture dataset (Fig 5) to confirm that the reported performance gains are robust to hyperparameter choices.
2. **Ablation of ITMO vs. Simple Selection:** Implement a baseline where the Slow Memory is filled using reservoir sampling (instead of ITMO) to quantify the exact performance contribution of the information-theoretic optimization.
3. **Robustness to Distribution Shift:** Test the model's performance when the task stream includes sudden, large domain shifts (e.g., from CIFAR-10 to a medical image dataset) to assess the stability of the learned representations.