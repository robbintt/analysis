---
ver: rpa2
title: 'Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts'
arxiv_id: '2507.04569'
source_url: https://arxiv.org/abs/2507.04569
tags:
- arabic
- egyptian
- arxiv
- datasets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Nile-Chat, a family of large language models
  for Egyptian Arabic that supports both Arabic and Latin scripts. The key innovation
  is a novel Branch-Train-MiX (BTX) strategy to merge script-specialized experts into
  a single Mixture-of-Experts model.
---

# Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts

## Quick Facts
- **arXiv ID:** 2507.04569
- **Source URL:** https://arxiv.org/abs/2507.04569
- **Reference count:** 40
- **Key outcome:** Nile-Chat significantly outperforms leading multilingual and Arabic LLMs on Egyptian benchmarks, achieving up to 14.4% improvement over Qwen2.5-14B-Instruct on Latin-script tasks.

## Executive Summary
Nile-Chat introduces a family of large language models tailored for Egyptian Arabic, supporting both Arabic and Latin scripts. The models use a novel Branch-Train-MiX (BTX) strategy to merge script-specialized experts into a single Mixture-of-Experts architecture. Three variants are released: dense 4B and 12B models, and a 3x4B-A6B MoE model. Trained on dual-script Egyptian datasets and fine-tuned for instruction following, Nile-Chat sets a new state-of-the-art for Egyptian Arabic on newly introduced benchmarks.

## Method Summary
Nile-Chat is built using a Mixture-of-Experts (MoE) architecture with a Branch-Train-MiX (BTX) training strategy. The approach trains separate expert models for Arabic and Latin scripts, then merges them into a unified model. The models are pre-trained on a dual-script Egyptian Arabic corpus and fine-tuned for instruction following and alignment. Three model sizes are released: dense 4B and 12B models, and a 3x4B-A6B MoE model.

## Key Results
- Nile-Chat outperforms leading multilingual and Arabic LLMs on Egyptian benchmarks.
- Achieves up to 14.4% improvement over Qwen2.5-14B-Instruct on Latin-script tasks.
- First LLM explicitly supporting both Arabic and Latin scripts for a widely spoken dialect.

## Why This Works (Mechanism)
The Branch-Train-MiX (BTX) strategy allows the model to specialize in each script while maintaining unified performance. By training script-specific experts and merging them, Nile-Chat captures the linguistic nuances of Egyptian Arabic in both scripts. The MoE architecture enables efficient scaling and specialization, leading to superior performance on dialect-specific tasks.

## Foundational Learning
- **Mixture-of-Experts (MoE):** Allows conditional computation by activating only relevant experts for each input, improving efficiency and specialization. *Quick check:* Verify expert activation patterns are script-dependent.
- **Dual-script training:** Exposes the model to both Arabic and Latin representations of Egyptian Arabic, enhancing robustness. *Quick check:* Test performance on code-switched inputs.
- **Instruction fine-tuning:** Adapts the model to follow instructions in Egyptian Arabic, improving usability. *Quick check:* Evaluate zero-shot task completion in dialect.

## Architecture Onboarding
**Component Map:** Input -> Tokenization -> MoE Layer (script-specific experts) -> FFN Layers -> Output
**Critical Path:** Tokenization → MoE Expert Selection → FFN Processing → Output Generation
**Design Tradeoffs:** MoE enables efficient scaling but increases routing complexity; dual-script support broadens usability but requires careful data curation.
**Failure Signatures:** Poor performance on unseen script combinations; routing instability; cultural nuance misinterpretation.
**3 First Experiments:** 1) Test script-specific expert activation on held-out data. 2) Evaluate cross-script generalization on overlapping examples. 3) Perform ablation of BTX strategy.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation test sets for Latin and Arabic scripts have no overlap, making cross-script generalization difficult to assess.
- Training data sources and preprocessing steps are not fully disclosed, limiting reproducibility and risk assessment.

## Confidence
- High confidence in absolute performance gains on proprietary benchmarks.
- Medium confidence in the necessity of the BTX strategy due to lack of ablation studies.
- Low confidence in cultural awareness claims due to absence of human evaluation.

## Next Checks
1. Replicate the evaluation on independent, overlapping Arabic-Latin script test sets to assess true cross-script generalization.
2. Conduct ablation experiments removing the BTX strategy to isolate its impact on performance gains.
3. Perform human evaluation studies with native Egyptian Arabic speakers to validate cultural relevance and practical utility beyond benchmark scores.