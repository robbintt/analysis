---
ver: rpa2
title: Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation
arxiv_id: '2510.08078'
source_url: https://arxiv.org/abs/2510.08078
tags:
- hallucination
- speech
- halcon
- audio
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Insertion Hallucination (IH), a failure mode
  in video-to-audio generation where models generate speech or music without visual
  sources, and proposes a solution called HALCON. The authors develop a systematic
  evaluation framework using a multi-detector ensemble and two metrics (IH@vid and
  IH@dur) to quantify IH prevalence and severity.
---

# Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation

## Quick Facts
- arXiv ID: 2510.08078
- Source URL: https://arxiv.org/abs/2510.08078
- Reference count: 39
- This paper introduces Insertion Hallucination (IH) in video-to-audio generation and proposes HALCON to detect and mitigate it.

## Executive Summary
This paper identifies a critical failure mode in video-to-audio generation called Insertion Hallucination (IH), where models generate speech or music that has no visual source in the input video. The authors develop HALCON, a three-stage inference-time method that detects hallucinated audio segments using a multi-detector ensemble and regenerates them with corrected visual features. Experiments on three benchmarks show HALCON reduces IH prevalence by 40-65% and severity by 40-64% without degrading conventional audio quality metrics.

## Method Summary
HALCON addresses IH through a three-stage process: initial audio generation, hallucination detection with visual feature masking, and regeneration using corrected features. The detection pipeline uses a majority-voting ensemble of three audio event detectors (inaSpeechSegmenter, YAMNet, PANNs-CNN14) to identify speech and music segments lacking visual sources. Detected hallucinated intervals are masked by replacing corresponding video features with empty features during regeneration. The framework introduces two metrics: IH@vid (fraction of videos with hallucinations) and IH@dur (fraction of hallucinated duration). Evaluation on three benchmarks shows HALCON effectively mitigates IH while maintaining conventional audio quality metrics like fidelity, diversity, and synchronization.

## Key Results
- HALCON reduces IH@vid by 40-65% and IH@dur by 40-64% across three benchmarks
- Conventional audio quality metrics (FD, KL, ISC, IB-score, DeSync) remain stable with <5% degradation
- HALCON performs better on out-of-domain datasets compared to in-domain VGGSound due to training bias overfitting
- Detection pipeline achieves 86% precision and 94% recall on human-annotated validation set

## Why This Works (Mechanism)
HALCON works by identifying and masking visual features that correspond to hallucinated audio segments. The multi-detector ensemble provides robust detection of speech and music without visual sources, while feature masking prevents the model from regenerating the same hallucinations. The majority voting mechanism ensures high precision by requiring consensus across detectors, and temporal smoothing prevents spurious detections. By replacing features with empty features rather than removing them entirely, the model maintains temporal alignment while suppressing hallucinations.

## Foundational Learning
- Audio event detection with ensemble voting - Needed to reliably identify speech/music without visual sources; Quick check: Verify detector precision/recall on held-out samples
- Temporal smoothing for event detection - Needed to eliminate spurious detections and merge adjacent segments; Quick check: Ensure min_dur and min_gap parameters produce clean detections
- Feature masking in generative models - Needed to suppress hallucination generation without breaking model architecture; Quick check: Confirm empty feature dimensionality matches original video features
- Video-to-audio generation pipelines - Needed to understand feature extraction and regeneration process; Quick check: Validate video feature extraction produces consistent outputs
- Multi-metric evaluation for generative models - Needed to assess both hallucination reduction and audio quality; Quick check: Verify IH@vid and IH@dur computation accuracy

## Architecture Onboarding
- Component map: Video Input -> Feature Extractor -> HALCON (Detect+Mask) -> V2A Model -> Audio Output
- Critical path: Detection pipeline (9.98s clips, 0.02s grid, temporal smoothing) -> Feature masking (empty features at detected intervals) -> Regeneration (6-12 sampling steps)
- Design tradeoffs: Inference-time correction vs. training-time regularization; majority voting for precision vs. recall; empty feature masking vs. feature removal
- Failure signatures: Over-aggressive masking degrades FD/KL >5%; detector disagreement causes inconsistent IH scores; Stage 1 generation quality too low for reliable detection
- First experiments: 1) Validate detector ensemble outputs on held-out samples, 2) Test HALCON with different empty feature representations, 3) Evaluate conventional audio quality metrics on HALCON outputs

## Open Questions the Paper Calls Out
- Can combining input-level textual guidance with posterior feature-level correction yield a unified method that suppresses hallucinations more effectively than either approach alone? The authors note that text-driven and feature-level approaches address different aspects of hallucination and could be combined for greater effectiveness.
- How can the definition and detection of Insertion Hallucination be expanded beyond speech and music to include general environmental sounds that lack mature detection tools? The current framework is limited to speech and music due to detector availability, leaving broader environmental hallucinations unaddressed.
- Can the "empty feature" masking strategy be learned or optimized during training rather than applied heuristically at inference, to better address the "overfitting to training biases" observed in-domain? The current inference-time approach cannot fully undo learned priors that cause in-domain performance degradation.

## Limitations
- Limited to speech and music hallucinations due to mature detector availability, excluding environmental sounds
- Performance degrades on in-domain datasets due to training bias overfitting that inference-time masking cannot fully address
- Requires access to video feature extraction architecture and empty feature representation details not fully specified

## Confidence
- Detection framework implementation: **High** - Clear specification of thresholds, temporal smoothing, and ensemble voting
- HALCON mitigation effectiveness: **Medium** - Strong empirical results but depends on unknown components like empty features
- Generalization across V2A models: **Low** - Only two models tested; effectiveness on other architectures uncertain

## Next Checks
1. Validate detector ensemble outputs on held-out samples to verify agreement rates and IH@vid/IH@dur computation accuracy
2. Test HALCON with different empty feature representations to determine if the exact pretrained "learned" version is critical
3. Evaluate conventional audio quality metrics (FD, KL, ISC) on HALCON outputs to confirm no degradation beyond the reported <5% threshold