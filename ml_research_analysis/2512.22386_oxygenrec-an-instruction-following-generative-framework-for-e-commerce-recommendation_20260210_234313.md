---
ver: rpa2
title: 'OxygenREC: An Instruction-Following Generative Framework for E-commerce Recommendation'
arxiv_id: '2512.22386'
source_url: https://arxiv.org/abs/2512.22386
tags:
- user
- scenario
- item
- arxiv
- oxygenrec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OxygenREC introduces a Fast-Slow Thinking architecture for generative
  e-commerce recommendation, combining near-line LLM-based reasoning with real-time
  encoder-decoder generation to inject deductive knowledge without incurring latency.
  It employs a semantic alignment mechanism with Instruction-Guided Retrieval to ensure
  instructions effectively control recommendations, and achieves multi-scenario scalability
  via a unified model trained with Soft Adaptive Group Clip Policy Optimization.
---

# OxygenREC: An Instruction-Following Generative Framework for E-commerce Recommendation

## Quick Facts
- arXiv ID: 2512.22386
- Source URL: https://arxiv.org/abs/2512.22386
- Reference count: 40
- Key outcome: OxygenREC introduces a Fast-Slow Thinking architecture for generative e-commerce recommendation, combining near-line LLM-based reasoning with real-time encoder-decoder generation to inject deductive knowledge without incurring latency. It employs a semantic alignment mechanism with Instruction-Guided Retrieval to ensure instructions effectively control recommendations, and achieves multi-scenario scalability via a unified model trained with Soft Adaptive Group Clip Policy Optimization. Deployed at JD.com, OxygenREC significantly increased GMV and order volume in online A/B tests across multiple core scenarios, demonstrating robust flexibility and efficiency in real-world industrial environments.

## Executive Summary
OxygenREC is an industrial-scale generative recommendation system that transforms e-commerce recommendation into a sequence-to-sequence generation task. The framework introduces a Fast-Slow Thinking architecture that separates near-line LLM-based reasoning (slow) from real-time encoder-decoder generation (fast) to avoid latency penalties while maintaining deductive reasoning capability. A semantic alignment mechanism with Instruction-Guided Retrieval ensures that instructions effectively control recommendations, while Soft Adaptive Group Clip Policy Optimization enables multi-scenario unification through a single trainable model. Deployed at JD.com, OxygenREC demonstrates significant improvements in business metrics including GMV and order volume across multiple recommendation scenarios.

## Method Summary
OxygenREC reformulates e-commerce recommendation as autoregressive sequence generation, where items are represented as hierarchical Semantic IDs (SIDs) via residual quantization. The Fast-Slow Thinking architecture uses a near-line LLM pipeline to synthesize Contextual Reasoning Instructions from user context, stored as embeddings and retrieved at inference time by a lightweight encoder-decoder backbone. Instruction-Guided Retrieval (IGR) filters user history using instruction-relevant items, while a unified model trained with SA-GCPO serves multiple scenarios through instruction conditioning. The system achieves real-time performance through prefix-constrained beam search with a Trie index, maintaining 50-80ms latency while generating personalized recommendations.

## Key Results
- Online A/B tests showed significant GMV and order volume increases across multiple JD.com core scenarios
- IGR+Q2I alignment improved HR@1 by 11.4% relative to baseline models
- SA-GCPO outperformed GRPO by 3.8 percentage points in HR@10 with 33% synthetic data
- Multi-scenario unified model outperformed independent scenario-specific models across all six tested scenarios

## Why This Works (Mechanism)

### Mechanism 1: Fast-Slow Thinking Separation for Latency-Free Reasoning
- **Claim**: Offloading deep reasoning to a near-line LLM pipeline while using a lightweight encoder-decoder for online inference enables deductive recommendation without latency penalties.
- **Mechanism**: The "slow" LLM analyzes spatiotemporal context, user profile, and behavior sequences to synthesize Contextual Reasoning Instructions as dense embeddings, stored in Redis keyed by user ID. The "fast" encoder-decoder retrieves these precomputed embeddings at inference time, conditioning generation without online LLM calls.
- **Core assumption**: User intent evolves slowly enough that near-line updates (daily full refresh + 5-minute window aggregation for high-value actions) capture relevant reasoning signals before they become stale.
- **Evidence anchors**:
  - [abstract] "Slow thinking uses a near-line LLM pipeline to synthesize Contextual Reasoning Instructions, while fast thinking employs a high-efficiency encoder-decoder backbone for real-time generation."
  - [section 2.1] "A near-line LLM pipeline (slow thinking) distills complex user intents into compact Contextual Reasoning Instructions, injecting world knowledge and deductive signals without incurring online LLM latency."
  - [section 3.3] "Daily full refresh... Near-line incremental update: When a user performs high-value actions... the system triggers a near-real-time instruction update."
  - [corpus] Weak direct validation; S²GR (arXiv:2601.18664) proposes latent-space reasoning for GR but uses online inference, highlighting the latency tradeoff OxygenREC explicitly addresses.
- **Break condition**: If user intent shifts rapidly within a session (e.g., browsing gifts then immediately searching for self-purchase items) and near-line updates lag, reasoning instructions may misalign with actual intent, causing irrelevant recommendations.

### Mechanism 2: Instruction-Guided Retrieval (IGR) with Query-to-Item Alignment
- **Claim**: Filtering long-term user history via instruction-relevant retrieval improves generation quality by reducing noise from irrelevant past behaviors.
- **Mechanism**: An adapter projects instructions (Is, Ir) and items into a shared embedding space. Q2I loss aligns the instruction embedding with the ground-truth target item during training, enabling the instruction to function as a retrieval query at inference. Top-K relevant history items are retrieved, filtering out irrelevant interactions.
- **Core assumption**: Current intent is better captured by a subset of history semantically aligned with the instruction, not by the full sequence.
- **Evidence anchors**:
  - [section 2.3.3] "We use IGR to filter the history and select interactions that are most relevant to the instruction prompt... this mechanism consists of three components: (1) Adapter... (2) Q2I alignment... (3) IGR."
  - [table 7] "+ IGR+Q2I (Full)" achieves HR@1 of 4.19% vs. "Base Model (w/o IGR/Q2I)" at 3.76%, a 11.4% relative improvement.
  - [corpus] No direct corpus evidence for IGR specifically; this appears to be a novel contribution.
- **Break condition**: If Q2I alignment fails (e.g., embedding collapse or poor projection), retrieved history will be irrelevant. The paper mitigates this with regularization and decorrelation terms (Equation 2), but collapse risk remains if loss coefficients are poorly tuned.

### Mechanism 3: Multi-Scenario Unification via Soft Adaptive Group Clip Policy Optimization
- **Claim**: A unified model can serve multiple scenarios by conditioning on scenario instructions and using a soft-gated RL policy that adaptively handles diverse business objectives.
- **Mechanism**: Scenario information (e.g., Homepage, Cart, Feed) is encoded as scenario instruction tokens. A unified ranking model provides consistent reward signals across scenarios. SA-GCPO replaces hard clipping (as in GRPO) with a sigmoid-based soft gate that decays gradients smoothly, using asymmetric temperatures for positive/negative advantage samples to stabilize training.
- **Core assumption**: Scenarios share transferable representations, and instruction conditioning can replace scenario-specific architectural components (e.g., gating, routing).
- **Evidence anchors**:
  - [abstract] "We transform scenario information into controllable instructions, using unified reward mapping and Soft Adaptive Group Clip Policy Optimization (SA-GCPO)... realizing a train-once-deploy-everywhere paradigm."
  - [table 8] Unified model outperforms independent SFT baselines across all 6 scenarios; e.g., Scenario 1 HR@10: 46.73% (unified) vs. 23.29% (independent).
  - [table 10] SA-GCPO achieves HR@10 of 65.95% vs. GRPO at 62.15% (+3.8pp) with 33% synthetic data.
  - [corpus] OneMall (arXiv:2601.21770) similarly proposes unified generative recommendation for multi-scenario e-commerce, but uses different alignment strategies; provides weak indirect support for the paradigm.
- **Break condition**: If scenarios have fundamentally conflicting objectives (e.g., explore-heavy Homepage vs. convert-heavy Cart) and the unified reward mapping fails to balance them, negative transfer may still occur. The paper does not fully characterize failure modes under extreme objective divergence.

## Foundational Learning

- **Concept: Generative Recommendation (GR)**
  - **Why needed here**: OxygenREC fundamentally reformulates recommendation as sequence generation rather than candidate scoring. Understanding this paradigm shift is prerequisite to grasping why instructions, semantic IDs, and encoder-decoder architectures are central.
  - **Quick check question**: Can you explain why generating item tokens autoregressively differs from scoring a fixed candidate pool, and what advantages the former offers?

- **Concept: Residual Quantization / Semantic IDs**
  - **Why needed here**: Items are represented as hierarchical discrete codes (3 levels, 8192 vocab each) via RQ-KMeans. This tokenization enables transformer-based generation over a manageable vocabulary while preserving semantic structure.
  - **Quick check question**: Given a 3-level residual quantization with codebook size 8192, what is the theoretical maximum number of unique item representations, and why might actual coverage be lower?

- **Concept: Policy Optimization with Clipping (GRPO/PPO)**
  - **Why needed here**: OxygenREC's SA-GCPO modifies GRPO-style clipping. Understanding why clipping matters (preventing large policy updates), and why soft gating might improve upon hard clipping, is essential for the post-training section.
  - **Quick check question**: In PPO/GRPO, what problem does the clipping objective solve, and what is the tradeoff if the clip range is too narrow or too wide?

## Architecture Onboarding

- **Component map**: User Profile + Short-term Behavior + IGR-filtered Long-term Behavior + Scenario Instruction (Is) + Reasoning Instruction (Ir) -> Multimodal Encoder (Qwen3 text + CLIP image) -> Fusion Module -> RQ-KMeans -> Semantic ID (3-level hierarchy, 8192 vocab per level) -> Encoder (4 layers) -> Decoder (6-16 layers) -> Autoregressive Generation -> xGR Inference System

- **Critical path**:
  1. Semantic ID quality determines generation granularity; V4 (multi-source alignment) achieves P99 collision of 9 SKUs (Table 2).
  2. Instruction token position affects controllability; "Insert Right of BOS" is optimal (Table 5).
  3. IGR+Q2I alignment improves history filtering; full model outperforms ablations (Table 7).
  4. SA-GCPO stabilizes multi-scenario RL; outperforms GRPO/GSPO (Table 10).

- **Design tradeoffs**:
  - **MoE active experts**: 0.7B-1.6B models use only 2 active experts per token, causing a performance plateau (Section 4.2); 3.0B model overcomes this via deeper decoder (16 layers).
  - **Beam size vs. latency**: Large beams (256-512) improve recall but increase KV-cache pressure and sorting overhead; xGR optimizes this via specialized kernels.
  - **Near-line update frequency**: More frequent updates capture intent shifts but increase backend load; 5-minute window aggregation balances responsiveness and cost.

- **Failure signatures**:
  - **Codebook collapse**: Low coverage at deeper levels (V2 had 0.013% at L3); mitigated by multi-source contrastive learning.
  - **Negative transfer**: Unified model underperforms in specific scenarios if scenario instructions are insufficiently discriminative.
  - **Reward hacking**: GRPO showed instability with synthetic data (Figure 7); SA-GCPO's asymmetric temperature control mitigates gradient explosion from negative samples.
  - **Embedding collapse**: Q2I loss may produce redundant dimensions; regularization/decorrelation terms (Equation 2) are critical.

- **First 3 experiments**:
  1. **Validate Semantic ID quality**: Compute codebook coverage (L1-L3), cluster purity, P99 collision, and load balance on your item catalog; target V4-level metrics (Table 2).
  2. **Ablate instruction token position**: Replicate Table 5 on your data—test "No Instruction," "Replace BOS," "Add to BOS," "Insert Left of BOS," "Insert Right of BOS"—to confirm optimal placement.
  3. **Compare GRPO vs. SA-GCPO**: Using synthetic data proportions (0%, 20%, 33%, 50%), replicate Figure 7 to validate SA-GCPO's stability and performance gains over GRPO in your scenario mix.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Non-Autoregressive (NAR) generation be implemented in OxygenREC to eliminate the linear latency scaling of autoregressive decoding without degrading the semantic coherence of the generated item sequences?
- Basis in paper: [explicit] Section 5 states that sequential Next Token Prediction (NTP) creates a "fundamental scalability barrier" and identifies transitioning to a Non-Autoregressive (NAR) parallel generation paradigm as a long-term goal to minimize serving latency.
- Why unresolved: The current architecture relies on strict autoregressive dependencies to maintain sequence logic; it is unclear if parallel generation can preserve the instruction-following capability and ranking quality.
- What evidence would resolve it: A comparison of HitRate@K and latency metrics between the current NTP model and a proposed NAR variant on the same dataset.

### Open Question 2
- Question: How can multi-scenario user trajectory modeling be integrated into the Contextual Reasoning Instructions to capture cross-scenario intent evolution without disrupting the current near-line pipeline?
- Basis in paper: [explicit] Section 5 notes that the current system utilizes immediate context well but fails to capture complex decision trajectories spanning multiple scenarios (e.g., Homepage to Checkout), identifying this as a focus for upgrading the instruction system.
- Why unresolved: Aggregating cross-scenario sequences into a single instruction increases input complexity and might introduce noise that the current Instruction-Guided Retrieval (IGR) mechanism is not designed to filter.
- What evidence would resolve it: Evaluations showing improved GMV/Conversion rates when the input includes cross-scenario trajectory embeddings versus the current isolated scenario inputs.

### Open Question 3
- Question: Does the residual quantization of Semantic IDs (SIDs) create a performance bottleneck for long-tail items due to P99/P999 collisions?
- Basis in paper: [inferred] Table 2 shows the best V4 model still has a P999 SID collision count of 35 (meaning 35 items share one SID).
- Why unresolved: While the paper optimizes for collision reduction, it does not analyze if this remaining ambiguity specifically harms the retrieval accuracy for low-frequency items that might map to the same code as high-frequency items.
- What evidence would resolve it: A stratified analysis of HitRate@K specifically for items in the top 1% of frequency vs. the bottom 10%, correlated with their specific collision counts.

### Open Question 4
- Question: Is there a performance disparity between search-driven scenarios (which use explicit queries for instructions) and pure recommendation scenarios (which rely on "default" learnable instructions)?
- Basis in paper: [inferred] Section 2.3.2 states that for recommendation scenarios where textual instructions are unavailable, a "default (learnable) instruction embedding" is used, whereas search scenarios use query-derived instructions.
- Why unresolved: The paper demonstrates overall success but does not isolate whether the "default" instruction mechanism is as semantically rich or effective as the LLM-generated reasoning instructions used in other contexts.
- What evidence would resolve it: An ablation study comparing performance metrics (HR@K) in scenarios where real instructions are swapped for default embeddings versus vice versa.

## Limitations
- Scale dependency: Performance metrics are achieved on JD.com's proprietary dataset with massive scale, and effectiveness may degrade significantly on smaller public datasets.
- Proprietary components: The "JoyAI LLM" pipeline for generating Contextual Reasoning Instructions and the specific Unified Ranking Model architecture are not fully specified, limiting faithful reproduction.
- Real-time reasoning assumption: The Fast-Slow Thinking mechanism assumes user intent evolves slowly enough for near-line updates to remain relevant, without quantitative analysis of intent drift rates.

## Confidence
- **High Confidence**: The core architectural contributions (Fast-Slow Thinking, Instruction-Guided Retrieval with Q2I alignment, SA-GCPO optimization) are well-specified and the empirical results on JD.com's platform are concrete and measurable.
- **Medium Confidence**: The theoretical advantages of the Fast-Slow Thinking architecture (latency-free reasoning) and multi-scenario unification (train-once-deploy-everywhere) are sound, but the paper doesn't provide extensive ablation studies showing performance degradation when individual components fail.
- **Low Confidence**: The scalability claims and transfer learning benefits are primarily validated on a single industrial platform without evidence for how well the unified model would perform on datasets with different user behavior patterns.

## Next Checks
1. **Intent Drift Analysis**: Implement a systematic measurement of how often near-line updated reasoning instructions become misaligned with actual user intent during sessions. Track staleness metrics and measure recommendation quality degradation as a function of update lag time.

2. **Failure Mode Characterization**: Design controlled experiments that stress-test individual components - deliberately introduce embedding collapse in Q2I alignment, simulate rapid intent shifts between near-line updates, and create scenario pairs with conflicting objectives to measure negative transfer effects.

3. **Cross-Dataset Generalization**: Reproduce the core generative architecture on a public e-commerce dataset (e.g., Amazon) with significantly smaller scale. Compare performance against scenario-specific models to quantify the trade-off between unification benefits and scale-dependent effectiveness.