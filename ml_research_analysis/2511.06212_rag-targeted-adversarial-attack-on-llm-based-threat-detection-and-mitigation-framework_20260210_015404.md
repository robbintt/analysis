---
ver: rpa2
title: RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation
  Framework
arxiv_id: '2511.06212'
source_url: https://arxiv.org/abs/2511.06212
tags:
- attack
- adversarial
- description
- mitigation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tests the adversarial robustness of an LLM-based IoT
  attack analysis and mitigation framework by conducting a targeted data poisoning
  attack on its Retrieval-Augmented Generation (RAG) knowledge base. The attack uses
  word-level perturbations generated by TextFooler, applied to a fine-tuned BERT surrogate
  model, to create meaning-preserving adversarial attack descriptions.
---

# RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework

## Quick Facts
- arXiv ID: 2511.06212
- Source URL: https://arxiv.org/abs/2511.06212
- Reference count: 29
- LLM-based IoT threat detection framework compromised through RAG-targeted adversarial data poisoning

## Executive Summary
This work demonstrates a successful adversarial attack on an LLM-based IoT threat detection framework by poisoning its Retrieval-Augmented Generation (RAG) knowledge base. The attack uses TextFooler-generated word-level perturbations on a fine-tuned BERT surrogate model to create meaning-preserving adversarial attack descriptions. These poisoned descriptions are injected into the RAG component and queried during inference, causing ChatGPT-5 Thinking to generate less accurate attack analyses and weaker mitigation recommendations. The attack achieved statistically significant performance degradation across 18 attack classes from two IoT/IIoT datasets, reducing human and judge LLM scores from 9.73-9.85 to 8.43-9.23.

## Method Summary
The attack employs a surrogate model approach where TextFooler generates word-level perturbations on a fine-tuned BERT model to create adversarial attack descriptions. These meaning-preserving modifications are injected into the RAG component's vector store. During inference, the poisoned RAG is queried, and the resulting corrupted context is provided to ChatGPT-5 Thinking for attack analysis and mitigation generation. The framework processes two IoT/IIoT datasets (Edge-IIoTset and CICIoT2023) containing 18 attack classes. Performance is evaluated by human experts and judge LLMs who score pre- and post-attack responses across multiple dimensions including attack-behavior linking accuracy and mitigation specificity.

## Key Results
- Human expert scores dropped from 9.73 pre-attack to 8.43 post-attack
- Judge LLM scores decreased from 9.85 to 9.23 after attack
- Greater degradation observed on CICIoT2023 dataset compared to Edge-IIoTset
- Attack successfully degraded ChatGPT-5 Thinking's performance despite its state-of-the-art capabilities

## Why This Works (Mechanism)
The attack exploits the RAG component's reliance on retrieved contextual information by poisoning the knowledge base with adversarial examples. TextFooler generates perturbations that preserve semantic meaning while evading detection, allowing the poisoned descriptions to be retrieved during inference. When ChatGPT-5 Thinking processes these corrupted contexts, it generates less accurate attack characterizations and weaker mitigations because the foundational attack descriptions have been subtly altered to mislead the reasoning process.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation)**: Combines information retrieval with text generation to enhance LLM responses with external knowledge
  - Why needed: Provides context-specific information beyond the LLM's training data
  - Quick check: Verify retrieval quality and relevance of retrieved passages
- **TextFooler**: Black-box adversarial attack method that generates word-level perturbations
  - Why needed: Creates meaning-preserving modifications that evade detection while degrading performance
  - Quick check: Ensure semantic similarity between original and perturbed text
- **Surrogate modeling**: Uses a proxy model to generate adversarial examples for the target system
  - Why needed: Enables attack development when the target model is inaccessible
  - Quick check: Validate transferability of adversarial examples to the target model
- **IIoT security analysis**: Process of identifying and mitigating attacks in Industrial IoT environments
  - Why needed: Critical for protecting infrastructure from cyber threats
  - Quick check: Assess accuracy of attack classification and mitigation effectiveness

## Architecture Onboarding

Component Map: TextFooler -> BERT surrogate -> RAG vector store -> ChatGPT-5 Thinking -> Human/judge evaluation

Critical Path: Attack description generation → Adversarial perturbation → RAG injection → Context retrieval → Analysis generation → Performance evaluation

Design Tradeoffs: The attack prioritizes stealth through meaning-preserving perturbations at the cost of requiring access to the RAG component. The framework trades computational efficiency for comprehensive threat analysis through RAG augmentation.

Failure Signatures: Degraded attack-behavior linking accuracy, less specific mitigation recommendations, inconsistent performance across datasets, successful attack even on state-of-the-art models.

First Experiments:
1. Test attack transferability across different IoT/IIoT datasets
2. Evaluate defensive preprocessing techniques against the poisoning
3. Assess statistical detectability of adversarial perturbations in RAG data streams

## Open Questions the Paper Calls Out
None

## Limitations
- Attack assumes access to the RAG component's vector store, which may not reflect realistic threat scenarios
- Human evaluation relies on a small panel of three experts, limiting statistical robustness
- Performance degradation represents moderate decline (9.73-9.85 to 8.43-9.23) rather than catastrophic failure
- TextFooler-based attack strategy may be detectable through anomaly detection in textual data streams

## Confidence

**High confidence**: The attack successfully degrades ChatGPT-5 Thinking's performance when poisoned descriptions are injected into RAG

**Medium confidence**: The framework's vulnerability generalizes to other LLM-based systems with RAG components

**Medium confidence**: Performance degradation correlates with dataset characteristics (greater impact on CICIoT2023)

## Next Checks

1. Test the attack on additional IoT/IIoT datasets and non-IoT security domains to assess generalizability

2. Evaluate whether adversarial training or defensive preprocessing can mitigate the poisoning effects

3. Assess the attack's detectability through statistical analysis of textual perturbations in the RAG component