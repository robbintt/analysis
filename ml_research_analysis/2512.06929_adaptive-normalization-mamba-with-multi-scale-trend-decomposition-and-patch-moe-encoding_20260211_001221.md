---
ver: rpa2
title: Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch
  MoE Encoding
arxiv_id: '2512.06929'
source_url: https://arxiv.org/abs/2512.06929
tags:
- forecasting
- temporal
- adamamba
- normalization
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AdaMamba, a unified forecasting architecture
  that tackles the challenges of non-stationarity, multi-scale temporal patterns,
  and distributional shifts in real-world time series forecasting. It integrates adaptive
  normalization, multi-scale trend extraction, and contextual sequence modeling to
  stabilize training and improve accuracy.
---

# Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding

## Quick Facts
- **arXiv ID**: 2512.06929
- **Source URL**: https://arxiv.org/abs/2512.06929
- **Reference count**: 27
- **Primary result**: Achieves state-of-the-art MSE on ETTh1, ETTh2, ETTm1, ETTm2 benchmarks, with particular strength on high-frequency and complex environmental data

## Executive Summary
This paper introduces AdaMamba, a unified forecasting architecture that addresses non-stationarity, multi-scale temporal patterns, and distributional shifts in time series forecasting. The method integrates adaptive normalization, multi-scale trend extraction, and contextual sequence modeling to stabilize training and improve accuracy. Experimental evaluations on benchmark datasets demonstrate consistently lower Mean Squared Error than conventional Transformer-based baselines, particularly on high-frequency and complex environmental data.

## Method Summary
AdaMamba processes multivariate time series through an Adaptive Normalization Block that removes non-stationary components via multi-scale convolutional trend extraction and channel-wise recalibration. The Context Encoder combines patch-wise embeddings, positional encoding, and a Mamba-enhanced Transformer layer with a mixture-of-experts feed-forward module. A lightweight prediction head generates multi-horizon forecasts, with a de-normalization mechanism reconstructing outputs by reintegrating local trends. The architecture is trained end-to-end with combined loss functions including Huber, quantile, and directional losses.

## Key Results
- Achieves lowest MSE on ETTh1 (0.000724), ETTh2 (0.002396), ETTm1 (0.001409), and ETTm2 (0.000862) benchmarks
- Consistently outperforms Transformer-based baselines across all evaluated datasets
- Demonstrates particular strength on high-frequency and complex environmental data
- Shows second-best performance on Weather dataset (0.253790 MSE) behind iTransformer

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Normalization for Stationarization
- **Claim**: Explicitly removing non-stationary trends via multi-scale convolution before modeling stabilizes state-space dynamics and improves convergence.
- **Mechanism**: An Adaptive Normalization Block uses parallel 1D convolutions with different kernel sizes to extract trends at multiple scales. A Squeeze-and-Excitation block recalibrates these features channel-wise to generate a unified trend component that is subtracted from the input before normalization.
- **Core assumption**: Assumes that trend components causing distribution shift can be approximated by convolutional filtering and that removing them leaves a stationary residual easier for sequence models to process.
- **Evidence anchors**: Abstract states "removes non-stationary components through multi-scale convolutional trend extraction and channel-wise recalibration." Section 5.1 describes the Multi-Scale Trend Extraction module followed by SE block recalibration.

### Mechanism 2: Split-Mamba Context Encoding
- **Claim**: Splitting the channel dimension and applying selective state-space modeling (Mamba) to only one branch allows the model to inject temporal context without overwriting static covariates.
- **Mechanism**: The Context Encoder splits the normalized input into two branches. One branch is processed by a Mamba layer to generate context, which is then added to the unprocessed branch, fusing dynamic temporal states with preserved original features.
- **Core assumption**: Assumes that not all channels require complex temporal evolution modeling and that a residual "static" path stabilizes the learning of dynamics.
- **Evidence anchors**: Abstract mentions "Context Encoder that combines... Mamba-enhanced Transformer layer." Section 5.3 describes the split-architecture allowing selective filtering of temporal dynamics.

### Mechanism 3: MoE-Enhanced Temporal Modeling
- **Claim**: Replacing dense feed-forward layers with a Mixture-of-Experts allows the model to scale capacity for non-linear pattern recognition without linear increase in inference cost.
- **Mechanism**: A MoE module routes tokens to specific expert MLPs based on a learned gating network with temperature-scaled softmax. This forces the model to select specialized experts for different temporal patterns.
- **Core assumption**: Assumes that time-series patches exhibit distinct modalities that benefit from specialized processing paths rather than a single monolithic projection.
- **Evidence anchors**: Abstract states "mixture-of-experts feed-forward module, allowing efficient modeling of both long-range dependencies and local temporal dynamics." Section 5.4 describes the MoE layer with temperature-scaled gating.

## Foundational Learning

- **Concept: Non-Stationarity & Stationarization**
  - **Why needed here**: The paper explicitly targets data where statistical properties (mean/variance) change over time. Understanding that the Adaptive Norm block attempts to force the input into a stationary distribution is key to debugging the preprocessing stage.
  - **Quick check question**: If you feed a sine wave with a linearly increasing amplitude into the Adaptive Norm block, what should the output trend look like?

- **Concept: Selective State Space Models (Mamba)**
  - **Why needed here**: The core engine is Mamba, not a standard Transformer. You must understand that Mamba uses input-dependent parameters to filter information, allowing it to ignore noise selectively, unlike RNNs which update states rigidly.
  - **Quick check question**: How does the "selection mechanism" in Mamba theoretically help it ignore irrelevant context compared to a standard RNN?

- **Concept: Mixture of Experts (MoE) Gating**
  - **Why needed here**: The model relies on sparse activation. Understanding how the router selects experts (and the role of temperature τ) is critical for interpreting why the model might fail to learn diverse features.
  - **Quick check question**: What happens to the gradient flow if the Softmax temperature τ approaches infinity?

## Architecture Onboarding

- **Component map**: Input → Adaptive Norm → Patching → Context Encoder (LayerNorm → Split → Mamba Branch + Static Branch → Fuse → MoE FFN) × L → Attention Pooling → MLP Head → De-normalization

- **Critical path**: The Adaptive Normalization Block is the highest risk point. If the trend extraction convolutions are misconfigured (e.g., padding errors), the statistics used for normalization will be wrong, causing the SSM to receive out-of-distribution inputs.

- **Design tradeoffs**:
  - *Patch Size*: Larger patches reduce sequence length (faster Mamba) but may lose fine-grained resolution needed for local dynamics
  - *SE-Block vs. Fixed Weights*: The SE-block adds overhead but allows the model to ignore irrelevant trend scales; fixed averaging is faster but brittle

- **Failure signatures**:
  - **Lagging Predictions**: If the trend removal is too aggressive, the model loses the "level" information and defaults to a moving average
  - **NaN Loss during Norm**: If variance σ approaches zero after detrending, division by σ in normalization can explode

- **First 3 experiments**:
  1. **Ablation on Normalization**: Compare "No Norm" vs. "Standard RevIN" vs. "AdaMamba Norm" on a dataset with strong trend shift (e.g., ETTh2) to validate the specific contribution of the SE-Enhanced detrending
  2. **Patch Size Sensitivity**: Sweep patch sizes {8, 16, 32} to find the balance between computational cost and the Mamba layer's ability to resolve local patterns
  3. **Expert Utilization Check**: Visualize the MoE gating distribution to ensure all experts are being utilized (no collapse) and correlate specific experts with specific time-series regimes

## Open Questions the Paper Calls Out
- **Open Question 1**: Can AdaMamba's deterministic framework be effectively extended to probabilistic forecasting while preserving its handling of non-stationarity?
- **Open Question 2**: Which individual components (SE-based recalibration, multi-scale trend extraction, MoE feed-forward, or Split-Mamba) contribute most to AdaMamba's performance gains?
- **Open Question 3**: What factors explain AdaMamba's relatively weaker performance on the Weather dataset compared to ETT datasets?
- **Open Question 4**: How does AdaMamba's computational efficiency (training time, inference latency, memory) compare to Transformer-based baselines on long-horizon forecasting?

## Limitations
- Critical hyperparameters (patch size, expert count, temperature, SE weighting) are not fully specified, making faithful reproduction challenging
- Performance claims may not generalize to highly irregular or non-periodic time series without additional validation
- The exact impact of the MoE module and its routing strategy on forecasting performance cannot be fully assessed without seeing routing distribution and expert utilization patterns

## Confidence
- **High Confidence**: The core architectural design combining adaptive normalization with Mamba-based context modeling is sound and builds on established principles
- **Medium Confidence**: The specific implementation details of the multi-scale trend extraction and the split-Mamba architecture are well-motivated but require careful hyperparameter tuning
- **Low Confidence**: The exact impact of the MoE module and its routing strategy on forecasting performance cannot be fully assessed without seeing the routing distribution and expert utilization patterns

## Next Checks
1. **Trend Extraction Ablation**: Systematically evaluate the contribution of each component in the Adaptive Normalization Block (multi-scale convolutions alone vs. with SE recalibration vs. full implementation) on datasets with varying degrees of non-stationarity

2. **Channel Split Sensitivity Analysis**: Test different channel split ratios and configurations (e.g., 50/50 vs. 70/30) to determine the optimal balance between static feature preservation and dynamic modeling capacity

3. **MoE Routing Stability**: Monitor and visualize the expert utilization distribution during training across multiple seeds to ensure no routing collapse occurs and that experts specialize to distinct temporal patterns