---
ver: rpa2
title: 'InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient
  Diffusion Model Alignment'
arxiv_id: '2503.18454'
source_url: https://arxiv.org/abs/2503.18454
tags:
- diffusion
- preference
- human
- arxiv
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DDIM-InPO, an efficient method for aligning
  text-to-image diffusion models with human preferences using direct preference optimization.
  The core idea is to treat the diffusion model as a single-step generative model
  and selectively fine-tune latent variables strongly correlated with preference data.
---

# InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment

## Quick Facts
- **arXiv ID:** 2503.18454
- **Source URL:** https://arxiv.org/abs/2503.18454
- **Reference count:** 40
- **Primary result:** DDIM-InPO achieves state-of-the-art preference alignment with just 400 training steps on SDXL/SD1.5 models.

## Executive Summary
This paper introduces DDIM-InPO, an efficient method for aligning text-to-image diffusion models with human preferences using direct preference optimization. The core innovation treats the diffusion model as a single-step generative model and selectively fine-tunes latent variables strongly correlated with preference data through DDIM inversion. Experiments on the Pick-a-Pic v2 dataset demonstrate that DDIM-InPO achieves superior performance compared to all baselines while requiring significantly fewer training steps.

## Method Summary
DDIM-InPO aligns diffusion models by treating them as single-step generators and optimizing noise prediction at specific timesteps. The method uses DDIM inversion to estimate latent variables from preferred/unpreferred images, then applies a reparameterized DPO objective that directly assigns rewards to these latents. Instead of optimizing the full denoising trajectory, the model learns to predict the noise vector difference between winner and loser latents. This selective optimization achieves state-of-the-art results with only 400 training steps on SDXL and SD1.5 models.

## Key Results
- Achieves state-of-the-art preference alignment performance on Pick-a-Pic v2 with just 400 training steps
- Outperforms all baselines in human preference evaluations for aesthetic quality and text-image alignment
- Demonstrates significant efficiency gains (57.6 GPU hours for SD1.5) compared to traditional diffusion alignment methods

## Why This Works (Mechanism)

### Mechanism 1: Single-step Reward Assignment
Treating diffusion as a single-step generative model mitigates sparse reward problems by reparameterizing the DDIM ODE to define an "initial" variable $x_0(t)$. This allows implicit rewards to be assigned directly to latent variable $x_t$ at any timestep, short-circuiting the credit assignment gap between final image quality and intermediate noise predictions.

### Mechanism 2: Targeted Latent Optimization via Inversion
DDIM Inversion enables fine-tuning of latents strongly correlated with preference data by inverting preferred/unpreferred images back to their corresponding latent codes $x_t$. The model learns to shift probability mass toward preferred images by optimizing noise prediction errors relative to these inverted targets.

### Mechanism 3: Selective Latent Optimization for Efficiency
Focusing the loss on noise prediction error differences between winner and loser latents ($\tau^w_t$ vs $\tau^l_t$) avoids backpropagating through the entire sampling chain. This selective approach reduces training requirements to ~400 steps while maintaining alignment quality.

## Foundational Learning

- **Concept: DDIM (Denoising Diffusion Implicit Models)**
  - **Why needed:** DDIM's deterministic property is required for inversion; standard stochastic DDPM cannot easily recover exact latents $x_t$ from images $x_0$.
  - **Quick check:** Can you explain why a deterministic sampler (ODE) is required to find the latent $x_t$ corresponding to a specific image $x_0$, compared to a stochastic Markov chain?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed:** The paper adapts DPO to diffusion, eliminating the need for explicit reward models by reparameterizing the reward function using the optimal policy.
  - **Quick check:** How does DPO eliminate the need for an explicit reward model, and how does InPO adapt the Bradley-Terry model to the image domain?

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed:** CFG interaction with inversion is critical; using CFG during inversion leads to numerical instability in this architecture.
  - **Quick check:** Why might using CFG ($w_{inv} > 0$) during the inversion process lead to numerical instability or training degradation in this specific architecture?

## Architecture Onboarding

- **Component map:** Base Model (SDXL/SD1.5) -> Inversion Engine (DDIM reverse ODE) -> Loss Computer (DPO-adapted loss)
- **Critical path:**
  1. Load preference pair $(x^w_0, x^l_0)$
  2. Run unconditional DDIM inversion on both images to retrieve latents $x^w_t, x^l_t$
  3. Compute target vectors $\tau^w_t, \tau^l_t$ using reference model
  4. Optimize policy model using Eq. 18 (noise matching loss)
- **Design tradeoffs:**
  - Inversion Steps ($n$): $n=10$ balances efficiency (57.6 GPU hours) vs quality ($n=30$ yields best results but requires 136 hours)
  - CFG in Inversion: $w_{inv}=0$ (unconditional) preferred to avoid numerical errors
- **Failure signatures:**
  - Distribution Shift: Dataset quality lower than base model degrades metrics
  - CFG Artifacts: Using CFG during inversion causes inconsistent noise targets
- **First 3 experiments:**
  1. Inversion Validation: Verify reconstructing an inverted image returns the original
  2. Hyperparameter Scan: Test inversion steps $n \in \{3, 5, 10, 30\}$ for trade-off between speed and PickScore/HPS
  3. Baseline Comparison: Train SD1.5 for 400 steps using InPO vs Diffusion-DPO and compare GPU hours and aesthetic scores

## Open Questions the Paper Calls Out
- How can DDIM-InPO be adapted to an online training paradigm to mitigate sensitivity to offline dataset quality?
- What is the performance trade-off between training efficiency and alignment quality when using computationally intensive inversion methods versus standard DDIM inversion?
- To what extent does distributional shift between preference datasets and other architectures limit cross-model transferability?
- Can the DDIM-InPO objective be modified to explicitly filter or down-weight specific undesirable biases like gender stereotypes?

## Limitations
- Sensitive to offline dataset quality and potential distribution shift when applied to other model architectures
- May generate inappropriate outputs (e.g., overly feminine images) due to biases in the preference dataset
- Requires careful tuning of inversion parameters and cannot use CFG during the inversion process

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| 400-step efficiency claim | High |
| Preference alignment superiority | High |
| Mechanism validity (DDIM inversion + DPO) | Medium |
| Generalization to other datasets/models | Medium |
| Avoiding distribution shift | Medium |

## Next Checks
1. **ODE Stability Validation:** Measure reconstruction fidelity across ODE solver configurations and quantify correlation between numerical error and optimization performance.

2. **Inversion Robustness Testing:** Systematically vary CFG during inversion ($w_{inv} \in \{0, 0.5, 1.0\}$) and measure resulting loss landscape and final model quality.

3. **Generalization Stress Test:** Apply DDIM-InPO to preference datasets with varying quality levels and measure performance degradation to validate distribution shift claims.