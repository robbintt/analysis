---
ver: rpa2
title: Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries
arxiv_id: '2511.20854'
source_url: https://arxiv.org/abs/2511.20854
tags:
- recall
- video
- content
- retrieval
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOT2MEM, a large-scale dataset for modeling
  visual content memorability using tip-of-the-tongue (ToT) retrieval queries from
  Reddit. The dataset contains over 470,000 content-recall pairs across multiple domains,
  with a video-based subset of 82,500 videos paired with descriptive recall data.
---

# Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries

## Quick Facts
- arXiv ID: 2511.20854
- Source URL: https://arxiv.org/abs/2511.20854
- Authors: Sree Bhattacharyya; Yaman Kumar Singla; Sudhir Yarram; Somesh Kumar Singh; Harini S; James Z. Wang
- Reference count: 40
- Over 470,000 content-recall pairs across multiple domains, with 82,500 videos paired with descriptive recall data

## Executive Summary
This paper introduces TOT2MEM, a large-scale dataset for modeling visual content memorability using tip-of-the-tongue (ToT) retrieval queries from Reddit. The authors propose two tasks: descriptive recall generation and multimodal ToT retrieval, developing models that outperform state-of-the-art approaches like GPT-4o. Using unsupervised web-scale data, they demonstrate that ToT queries provide rich signals for memorability modeling and can be effectively leveraged for real-world applications.

## Method Summary
The method involves scraping Reddit ToT posts, validating solved answers through moderator bots and LLM verification, extracting YouTube videos, and processing them with scene sampling, OCR, and ASR. For recall generation, QwenVL 2.5 7B is fine-tuned with LoRA on video-recall pairs. For retrieval, VLM2Vec-V2 is trained with contrastive learning using text-based hard negatives mined from top-50 similar recalls. The dataset contains 470K pairs with 82.5K videos specifically annotated with recall descriptions.

## Key Results
- TOT2MEM-RECALL fine-tuned model outperforms GPT-4o in generating open-ended memorability descriptions
- TOT2MEM-RETRIEVAL achieves first multimodal ToT retrieval capability with contrastive training
- Models show strong generalization to human-curated datasets like LAMBDA in zero-shot settings
- Entertainment content dominates dataset (movies, TV, music) but model generalizes across domains

## Why This Works (Mechanism)

### Mechanism 1: ToT Queries as Valid Memorability Proxies
ToT search queries from Reddit provide reliable ground-truth signals for what humans remember about visual content. Users in ToT states post naturalistic recall descriptions when trying to identify content they cannot name. When the community successfully identifies the content (marked "solved"), this validates that the recall description contained sufficient memorable features to enable retrieval by other humans. The paper focuses on solved posts as they show recall descriptions were reliable enough to find correct answers.

### Mechanism 2: Contrastive Alignment for Noisy Recall-to-Video Mapping
Contrastive training with hard negatives enables embedding models to map vague textual recall to correct visual content. The model learns to embed recall descriptions and videos into a shared space where correct pairs are pulled together and semantically-similar-but-incorrect pairs are pushed apart. Hard negatives (top-50 most similar recall statements) force the model to distinguish fine-grained memorability features rather than broad semantic similarity.

### Mechanism 3: VLM Fine-tuning Captures Recall Patterns from Multimodal Context
Fine-tuning vision-language models on recall-video pairs teaches them to predict which content features humans verbalize when remembering. The model receives video scenes, OCR text, and audio transcripts as input, and learns to generate recall descriptions matching human ToT queries. This teaches the model to weight visual and textual features by their "verbalizability" in memory contexts.

## Foundational Learning

- **Tip-of-the-Tongue (ToT) States**: A cognitive phenomenon where a person cannot retrieve a known item's name but can recall partial features. Understanding ToT as a retrieval problem (vague query → specific item) clarifies why this dataset is valuable for memorability research—it captures what features persist in memory when identifiers are lost. Quick check: How does a ToT query differ from a standard search query in terms of specificity and information content?

- **Contrastive Learning with InfoNCE Loss**: Training objective that learns representations by pulling positive pairs together and pushing negative pairs apart in embedding space. The retrieval model uses this to learn shared embeddings for recall text and video content. Quick check: What happens to model performance if all negatives in a batch are easy (semantically distant from the query)?

- **Vision-Language Models (VLMs)**: Models that process both visual and textual inputs, typically using a vision encoder + language model architecture with cross-modal alignment. TOT2MEM-RECALL uses QwenVL 2.5 7B. Quick check: Why would a VLM fine-tuned on this task outperform GPT-4o in zero-shot recall generation?

## Architecture Onboarding

- **Component map**: Reddit API → Post filtering (NSFW, deleted, bots) → "Solved" validation (flair CSS, OP replies, moderator bots, LLM verification) → YouTube link extraction → Video download → Scene sampling + OCR + ASR → Model training

- **Critical path**: Data quality validation → Correct answer resolution → Video download success rate → Training data quality → Model performance. The paper reports >95% agreement between rule-based and LLM-based answer validation, suggesting this path is relatively stable.

- **Design tradeoffs**: Scale vs. noise (web-scale unsupervised data contains noise like episodic memory leakage and OCR errors); Text-based vs. multimodal hard negatives (currently using only text-based hard negatives); Scene sampling limit (30 scenes) ensures context window fit but may lose long-tail content from longer videos.

- **Failure signatures**: Generation - repetition loops, link hallucination, visual-semantic gap; Retrieval - semantically similar but memorability-different content retrieved, niche content harder to retrieve.

- **First 3 experiments**: 1) Reproduce recall generation baseline comparison with fine-tuned QwenVL 2.5 7B vs zero-shot InternVL 2.5 8B; 2) Ablate modality contributions (scenes only, scenes+OCR, scenes+ASR, scenes+OCR+ASR); 3) Hard negative quality analysis (in-batch only, top-50 text-based, top-10 harder).

## Open Questions the Paper Calls Out

- Can multimodal hard negatives (video-based) improve ToT retrieval performance compared to current text-only hard negative mining strategy? The authors note using multimodal hard negatives is challenging due to lack of precedent methods for embedding videos based on memorability.

- Does noisy or ambiguous input from web-scale data induce hallucinations in large vision-language models trained for memorability tasks? The paper notes noise exists but does not systematically study its effects on model hallucination.

- What memorability-related features (emotions, episodic context, semantic vs. non-semantic content) most strongly predict long-term recall descriptions? The paper analyzes genre and popularity correlations but does not systematically decompose which features drive memorability signals.

## Limitations

- Web-scale noise: The unsupervised dataset may contain episodic memory leakage, OCR/ASR errors, and demographic biases from Reddit users, though the paper argues noise doesn't hinder performance.

- Domain generalization: Dataset dominated by entertainment content (movies, TV, music), making it unclear whether the model learns general memorability patterns or overfits to entertainment-specific recall structures.

- Contrastive training assumptions: Current retrieval uses text-based hard negatives (top-50 similar recalls) but multimodal hard negatives could provide stronger supervision, and the choice of threshold remains unexamined.

## Confidence

- **High confidence**: Dataset construction methodology is clearly specified and reproducible; main claims about dataset scale (470K pairs, 82.5K videos) and baseline performance improvements are directly verifiable.

- **Medium confidence**: Claims about zero-shot generalization to LAMBDA and mindmem datasets are supported by quantitative results, but generalization mechanisms require deeper investigation.

- **Low confidence**: Claim that web-scale unsupervised data is "as effective as supervised data" lacks direct comparison with supervised baselines on equivalent-sized curated datasets.

## Next Checks

1. **Generalization stress test**: Evaluate TOT2MEM-RECALL on non-entertainment domains (educational content, sports, news) to determine whether memorability patterns generalize beyond the dataset's entertainment bias.

2. **Hard negative analysis**: Systematically vary hard negative difficulty (top-10 vs top-50 vs top-100 similar recalls) and measure retrieval performance to validate contrastive learning assumptions.

3. **Supervised vs unsupervised comparison**: Train supervised memorability models on equivalent-sized curated datasets (e.g., LAMBDA training split) and compare against TOT2MEM-RECALL performance on the same test sets.