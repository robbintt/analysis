---
ver: rpa2
title: 'Language Models Might Not Understand You: Evaluating Theory of Mind via Story
  Prompting'
arxiv_id: '2506.19089'
source_url: https://arxiv.org/abs/2506.19089
tags:
- room
- story
- characters
- stories
- enters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StorySim, a synthetic benchmark for evaluating
  theory of mind (ToM) in large language models (LLMs) through controllable story
  generation. Unlike prior benchmarks vulnerable to data contamination, StorySim generates
  novel stories from a high-level storyboard specification, enabling precise manipulation
  of character perspectives and events.
---

# Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting

## Quick Facts
- arXiv ID: 2506.19089
- Source URL: https://arxiv.org/abs/2506.19089
- Reference count: 40
- Most models perform better on World Modeling tasks than Theory of Mind tasks, suggesting fundamental limitations in modeling others' mental states.

## Executive Summary
This paper introduces StorySim, a synthetic benchmark for evaluating theory of mind in large language models through controllable story generation. The authors address data contamination concerns in existing ToM benchmarks by generating novel stories from a high-level storyboard specification, enabling precise manipulation of character perspectives and events. Their evaluation of state-of-the-art LLMs on first- and second-order ToM tasks alongside world modeling tasks reveals that models struggle more with reasoning about human agents than inanimate objects, and that larger models generally perform better but still face significant limitations with second-order ToM reasoning.

## Method Summary
The authors developed StorySim, a framework that generates synthetic stories from a Storyboard specification (C, ϕ, G, E, n) where C defines characters, G defines location graphs, and E specifies key events. The system randomizes the path between required events to create novel narratives, reducing data contamination risk. They evaluate models using zero-shot and 3-shot prompting on ToM questions ("Where does S1 think S2 thinks T is?") and matched WM questions ("Where is T currently?"). The framework supports varying "mislead distance" to probe reasoning strategies, and the authors analyze error patterns to identify heuristic failures.

## Key Results
- Most models perform better on World Modeling tasks than Theory of Mind tasks
- Accuracy drops significantly when reasoning about human agents versus inanimate objects
- Error analysis reveals models often rely on heuristics like recency bias and first-common-location memorization rather than genuine ToM reasoning
- Even the best models struggle with second-order ToM tasks

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Story Generation via Storyboards
The framework generates novel, compositional story prompts from a high-level graph specification (Storyboard) with randomized event filling. This reduces data contamination risk by ensuring the probability of the exact story appearing in pre-training data is astronomically small, forcing models to rely on reasoning rather than memorization.

### Mechanism 2: Task Isolation (World Modeling vs. Theory of Mind)
By creating matched pairs of questions—tracking physical state ("Where did T go?") versus modeling mental states ("Where does S1 think S2 thinks T is?")—the framework controls for narrative context processing ability. Performance differences isolate specific deficits in perspective-taking rather than general reasoning capability.

### Mechanism 3: Heuristic Diagnosis via "Mislead Distance"
The authors vary the "mislead distance" (steps between a character's last perception of an event and the event's conclusion). If models used robust ToM, accuracy would remain stable. Instead, errors cluster around specific shortcuts like the "First Common Location" heuristic, revealing reliance on shortcuts rather than persistent mental modeling.

## Foundational Learning

- **Concept: Theory of Mind (ToM) & False Belief Tasks**
  - **Why needed here:** This paper evaluates ToM specifically through "false belief" scenarios where a character holds a belief the model knows is false. Without understanding the difference between "truth" and "belief," the evaluation logic is opaque.
  - **Quick check question:** If a story states "Alice puts a ball in Box A and leaves," and then "Bob moves the ball to Box B," where would a ToM-capable model say Alice *thinks* the ball is?

- **Concept: Data Contamination in Benchmarks**
  - **Why needed here:** The paper's primary motivation is that standard ToM benchmarks (like standard Sally-Anne) are likely in the training data of LLMs, invalidating test results. Understanding contamination is necessary to understand why a synthetic generator (StorySim) is required.
  - **Quick check question:** Why would a model solving the standard Sally-Anne puzzle perfectly not necessarily prove it has Theory of Mind?

- **Concept: World Modeling vs. Social Reasoning**
  - **Why needed here:** The paper distinguishes between tracking the physical state of the world (WM) and tracking the informational state of agents (ToM). This distinction is the basis of their differential analysis.
  - **Quick check question:** Does the question "Where is the ball currently?" require World Modeling, Theory of Mind, or both?

## Architecture Onboarding

- **Component map:** Storyboard (D = C, ϕ, G, E, n) -> StorySim Engine -> Translator -> Natural language story -> LLM Evaluator
- **Critical path:** 1) Define location graph G and required key events E, 2) Generate story text via StorySim, 3) Prompt LLM with ToM question, 4) Compare answer against ground truth to identify heuristic failures
- **Design tradeoffs:** Simple grammar ("Alice enters room 1") minimizes confounding factors related to language understanding but sacrifices narrative nuance for experimental control
- **Failure signatures:** First Common Location Error (guessing first meeting location), Recency Bias (guessing actual current location), Refusal/Context Error (insufficient context for multiple mental states)
- **First 3 experiments:** 1) Compare standard Sally-Anne vs. random 5-event StorySim story to verify contamination, 2) Generate 100 stories and compare WM vs. ToM accuracy, 3) Vary "mislead distance" and plot error types to identify heuristic failures

## Open Questions the Paper Calls Out

- **Open Question 1:** Do LLMs possess a sound representation of goal-directedness in ToM reasoning, or do they rely on heuristics? The current study focused on false-belief tasks; goal-directed reasoning requires different story structures.
- **Open Question 2:** How do simultaneous actions, actions with duration, or periodic actions affect ToM reasoning in LLMs? StorySim currently generates stories with one character performing exactly one action at a time.
- **Open Question 3:** What dimensions of ToM complexity beyond order (first-order, second-order) are most relevant for real-world applications? The paper only manipulated ToM order, leaving other complexity dimensions unexplored.

## Limitations

- Synthetic story generation cannot fully rule out pre-training exposure to similar compositional patterns, though contamination risk is significantly reduced
- Error analysis attributing failures to specific heuristics is based on qualitative inspection rather than systematic categorization
- The distinction between WM and ToM might conflate linguistic complexity with cognitive difficulty, as ToM prompts require nested mental state attribution

## Confidence

- **High Confidence:** Synthetic story generation effectively reduces contamination risk
- **Medium Confidence:** Models perform better on WM than ToM tasks
- **Low Confidence:** Specific heuristic identification (e.g., "First Common Location") as dominant failure mode

## Next Checks

1. Control for Prompt Complexity: Generate matched WM and ToM prompts with identical syntactic structure but varying only mental state attribution depth to test whether the performance gap persists when linguistic complexity is controlled.

2. Heuristic Verification Protocol: Develop an automated classifier to categorize all incorrect responses into heuristic types and validate whether error patterns remain consistent across model families and story parameters.

3. Context Window Validation: Systematically vary story length and measure the point at which "insufficient context length" errors emerge, comparing this threshold to the mislead distance at which heuristic failures dominate.