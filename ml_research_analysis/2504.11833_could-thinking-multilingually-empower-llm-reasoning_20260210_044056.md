---
ver: rpa2
title: Could Thinking Multilingually Empower LLM Reasoning?
arxiv_id: '2504.11833'
source_url: https://arxiv.org/abs/2504.11833
tags:
- language
- multilingual
- languages
- reasoning
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multilingual reasoning can enhance
  large language model (LLM) performance beyond the typical "English bias." The authors
  evaluate three LLMs on two reasoning tasks (GPQA and MGSM) using human-translated
  prompts in 17 languages. By aggregating outputs across multiple languages, they
  show that multilingual reasoning can significantly increase the upper bound of performance
  (by up to ~10 Acc@k points) compared to English-only reasoning.
---

# Could Thinking Multilingually Empower LLM Reasoning?

## Quick Facts
- arXiv ID: 2504.11833
- Source URL: https://arxiv.org/abs/2504.11833
- Reference count: 20
- Primary result: Multilingual reasoning can increase LLM reasoning performance upper bound by ~10 Acc@k points over English-only approaches.

## Executive Summary
This paper challenges the "English bias" in LLMs by demonstrating that multilingual reasoning can significantly boost performance upper bounds on reasoning tasks. The authors evaluate three large language models on scientific (GPQA) and mathematical (MGSM) reasoning tasks using human-translated prompts in 17 languages. They find that aggregating outputs across multiple languages can increase the theoretical maximum performance (Acc@k) by up to 10 points compared to English-only reasoning. However, the paper identifies a critical gap: while the multilingual approach creates a high-quality candidate pool, current answer selection methods (majority voting, LLM-as-a-judge, prompt-based) fail to consistently identify the correct answer from this pool.

## Method Summary
The study uses three LLMs (Qwen2.5-72B, LLaMA3.1-70B, R1-Distill-LLaMA-70B) to perform chain-of-thought reasoning on translated GPQA and MGSM datasets in 17 languages. For each question, the model generates answers in multiple languages, creating a candidate pool. The study compares three approaches: Multilingual (answers in different languages), Repeat (multiple English runs with different seeds), and Paraphrase (paraphrased English inputs). Performance is measured using Acc@k (existence of correct answer in k candidates), Vote@k (majority voting accuracy), and Judge@k (LLM-as-a-judge selection accuracy). The analysis includes comparisons between human and machine translations to assess translation quality impact.

## Key Results
- Multilingual reasoning achieves Acc@4 scores 9.9-10.3 points higher than English-only approaches on GPQA and MGSM.
- The multilingual advantage is distinct from random sampling or paraphrasing, suggesting fundamental benefits from linguistic diversity.
- Answer selection strategies fail to fully exploit the multilingual advantage, with Vote@k and Judge@k significantly underperforming Acc@k.
- The performance upper bound is robust to both translation quality (human vs. machine) and language selection strategy (random vs. optimized combinations).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating reasoning across multiple languages raises the potential upper bound of correct answers by ~10 Acc@k points compared to English-only reasoning.
- Mechanism: Different languages trigger different reasoning paths, with some languages better suited for specific logical structures or knowledge domains, allowing compensation for errors in other languages.
- Core assumption: The model possesses underlying reasoning capability that can be hindered by English bias, which other linguistic representations can circumvent.
- Evidence anchors: [abstract] multilingual reasoning promises significantly higher upper bounds than English-only reasoning; [Page 7] analysis shows different languages match different difficulty levels and key advantage languages compensate for errors.

### Mechanism 2
- Claim: The multilingual performance gain is distinct from gains achieved by simply increasing sampling randomness or paraphrasing in English.
- Mechanism: The paper isolates the effect of language by comparing it to English-only baselines (multiple random seeds and paraphrased prompts), showing multilingual aggregation outperforms both.
- Core assumption: Variations introduced by translation are fundamentally different in nature and more effective at uncovering correct reasoning paths than sampling or paraphrasing variations.
- Evidence anchors: [Page 4] shows Repeat-Mix and Paraphrase-Mix provide additional benefit, but Multilingual provides a unique advantage; [Page 2] multilingual thinking starts to show advantages with only 4 candidates.

### Mechanism 3
- Claim: The high upper bound of multilingual reasoning is robust to language choice and translation quality.
- Mechanism: The benefit comes from introducing linguistic diversity itself, not from a single "magic" language or perfect translation.
- Core assumption: A randomly chosen set of languages is sufficiently diverse to capture potential reasoning benefits.
- Evidence anchors: [Page 4, Table 1] Acc@4 for random language combination (70.0) is much closer to best (74.3) than worst (65.6); [Page 5, Figure 6] performance distributions for human vs. machine translations are very similar.

## Foundational Learning

- **Acc@k (Accuracy at k)**: The probability that at least one of k generated answers is correct. Why needed: This is the core metric defining the "upper bound" of performance, fundamentally different from measuring single answer accuracy or majority vote.
  - Quick check: If a model generates 4 answers for a question (k=4) and 1 is correct and 3 are incorrect, what is the Acc@4 for that question? (Answer: 1 or 100%)

- **Answer Selection / Aggregation Strategy**: Methods to choose one final answer from the multilingual candidate pool. Why needed: The paper establishes a high potential (Acc@k) but shows practical strategies like majority voting and LLM-as-a-judge are currently inadequate.
  - Quick check: If Acc@k represents the potential, what does Vote@k or Judge@k represent? (Answer: The realized performance using a specific selection strategy like majority voting or an LLM judge.)

- **English Bias**: The tendency for LLMs to perform better on tasks presented in English. Why needed: This is the foundational problem the paper addresses, challenging whether non-English languages can surpass English performance.
  - Quick check: The paper challenges the "English bias" by showing that for some tasks, using what kind of languages can yield better performance? (Answer: Non-English languages, or multilingual thinking.)

## Architecture Onboarding

- **Component map**: Input Processor -> Multilingual Generator -> Candidate Pool -> Answer Selector -> Evaluator
- **Critical path**: The full promise depends on improving the Answer Selector. The current path successfully creates a pool with high probability of containing correct answer (Acc@k), but the final step (Selector) fails to reliably pick it.
- **Design tradeoffs**:
  - Number of languages (k): Increasing k raises Acc@k but can make answer selection harder; majority voting can degrade with more candidates.
  - Language Selection Strategy: Optimizing for "best" combination is hard; random selection is often good enough for Acc@k but sensitive for voting (Vote@k).
  - Translation Quality: High-quality human translation is expensive; machine translation yields similarly high upper bound, suggesting viable tradeoff.
- **Failure signatures**:
  - Majority Voting Failure: Most common answer is incorrect even though correct answer exists in minority.
  - LLM-as-a-Judge Bias: Judge consistently favors high-resource languages regardless of correctness.
  - Prompt-based Selection Failure: Model defaults to single dominant language (often English) instead of dynamically choosing best language per query.
- **First 3 experiments**:
  1. Replicate the Upper Bound Gap: Implement Multilingual pipeline using 4 languages, calculate Acc@4 and Vote@4, observe if potential (Acc@k) is significantly higher than realized performance (Vote@k).
  2. Test Selection Biases: Apply LLM-as-a-Judge to candidate pool from Experiment 1, analyze choices for language preferences and correctness.
  3. Explore Hybrid Selector: Create simple new selector that scores internal consistency of reasoning chain for each candidate regardless of language, compare to voting and Acc@k upper bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a universal and stable answer selection method be designed to fully exploit the high upper bound of multilingual reasoning?
- Basis in paper: [explicit] The authors state in the Limitations section that "the absence of a universal and stable method for leveraging multilingualism in reasoning" is a primary gap, as current methods fail to realize theoretical gains.
- Why unresolved: Majority voting degrades with more candidates, prompt-based methods show inconsistency, and LLM-as-a-judge approaches suffer from language bias.
- What evidence would resolve it: Development of a novel aggregation algorithm that consistently achieves Vote@k scores statistically indistinguishable from theoretical Acc@k upper bound across all tested models.

### Open Question 2
- Question: Does the multilingual reasoning advantage persist in LLMs with fewer than 70 billion parameters?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that their focus on models with over 70 billion parameters "may not fully represent the capabilities or challenges faced by smaller models."
- Why unresolved: The study restricts evaluation to 72B, 70B, and 70B parameter models, leaving scalability to smaller architectures untested.
- What evidence would resolve it: Replication of Multilingual vs. Repeat/Paraphrase experiments on smaller model variants (7B or 13B parameters) to observe if ~10 point Acc@k gain remains significant.

### Open Question 3
- Question: Can "key advantage languages" be predicted a priori based on specific linguistic or structural features?
- Basis in paper: [inferred] The paper observes that different languages match questions of different difficulty levels and identifies key advantage languages via post-hoc analysis, but does not determine underlying predictive features.
- Why unresolved: While authors demonstrate some languages compensate for errors in others, the mechanism for why a specific language suits a specific reasoning difficulty remains descriptive.
- What evidence would resolve it: A study correlating specific linguistic typologies with success rates on distinct reasoning categories, enabling prediction of optimal language combinations without prior benchmarking.

## Limitations

- The paper's practical utility is limited by the inability of current answer selection strategies to consistently identify correct answers from the multilingual candidate pool.
- Evaluation is restricted to only two reasoning tasks (GPQA and MGSM), which may not generalize to all LLM capabilities.
- The study focuses exclusively on large language models (over 70 billion parameters), leaving the behavior of smaller models unexplored.

## Confidence

- **High Confidence**: The existence of the multilingual upper bound advantage is well-supported by empirical results showing Acc@k consistently higher than English-only baselines.
- **Medium Confidence**: The claim that this advantage is distinct from English-only diversification is supported by comparisons to Repeat and Paraphrase baselines, though could use more detailed statistical analysis.
- **High Confidence**: The robustness of the upper bound to language choice and translation quality is strongly evidenced by similarity between human and machine translation results.

## Next Checks

1. Implement and test a hybrid answer selector that combines chain-of-thought consistency scoring with language-agnostic features to see if it can bridge the gap between Acc@k and Vote@k.
2. Evaluate the multilingual upper bound on a broader set of tasks beyond GPQA and MGSM, including commonsense reasoning and open-domain question answering, to test generalizability.
3. Conduct a systematic ablation study on language selection by testing not just random combinations but also strategically chosen sets based on linguistic features to determine if certain combinations are more effective.