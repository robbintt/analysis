---
ver: rpa2
title: Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models
arxiv_id: '2511.04800'
source_url: https://arxiv.org/abs/2511.04800
tags:
- prompts
- training
- arxiv
- residual
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key limitation in Group Relative Policy
  Optimization (GRPO)-based reinforcement learning for large language models: residual
  prompts that provide no training signal accumulate as training progresses and models
  scale, reducing training diversity and leaving valuable signals underutilized. To
  address this, the authors propose Explore Residual Prompts in Policy Optimization
  (ERPO), a framework that maintains a history tracker for each prompt and adaptively
  increases the sampling temperature for residual prompts that have previously produced
  all-correct responses.'
---

# Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models

## Quick Facts
- arXiv ID: 2511.04800
- Source URL: https://arxiv.org/abs/2511.04800
- Authors: Chenxi Liu, Junjie Liang, Yuqi Jia, Bochuan Cao, Yang Bai, Heng Huang, Xun Chen
- Reference count: 13
- Primary result: ERPO improves mathematical reasoning performance by addressing residual prompt accumulation in GRPO-based RL, reducing residual prompts from 74.8% to 34.8% and improving AIME2025 mean@32 by ~70%.

## Executive Summary
This paper identifies a key limitation in Group Relative Policy Optimization (GRPO)-based reinforcement learning for large language models: residual prompts that provide no training signal accumulate as training progresses and models scale, reducing training diversity and leaving valuable signals underutilized. To address this, the authors propose Explore Residual Prompts in Policy Optimization (ERPO), a framework that maintains a history tracker for each prompt and adaptively increases the sampling temperature for residual prompts that have previously produced all-correct responses. This encourages the model to generate more diverse reasoning traces, introducing incorrect responses that reactivate training signals. Empirical results on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong baselines across multiple mathematical reasoning benchmarks, with particularly strong improvements on tasks like AIME2025 that are less affected by data contamination.

## Method Summary
ERPO builds on GRPO/DAPO by adding a per-prompt history tracker H_i that counts epochs with all-correct responses. For each prompt, temperature T_i is computed as T_i = min(T_0 + T_s·H_i, T_max), where higher H_i increases sampling temperature to encourage incorrect responses. The framework maintains the GRPO advantage computation but modifies it with a Reactivation Advantage (RA) that filters out all-correct responses when no incorrect responses exist. Training uses a batch of 512 prompts with 16 responses each, with temperatures adjusted per prompt based on historical performance. The method was evaluated on Qwen2.5-3B/7B models using DAPO-Math-17K dataset and tested on AIME 2025/2024, AMC 2023, and MATH500 benchmarks.

## Key Results
- Temperature T=1.0→1.2 reduces residual prompts from 74.8% to 34.8% for Qwen2.5-32B+DAPO
- RA modification alone achieves ~70% improvement on AIME2025 mean@32 (less contaminated benchmark)
- ERPO consistently surpasses strong baselines across multiple mathematical reasoning benchmarks
- The method scales effectively to larger models and provides a promising direction for advancing reinforcement learning with verifiable rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive temperature scaling reactivates training signals from residual prompts
- Mechanism: When a prompt consistently yields all-correct responses (zero-variance rewards), ERPO increases its sampling temperature proportionally to the count of historical successes. Higher temperature increases the probability of generating incorrect responses, which creates non-zero advantage values and restores gradient flow.
- Core assumption: Incorrect responses triggered by higher temperature provide meaningful learning signal rather than noise.
- Evidence anchors:
  - [abstract] "ERPO maintains a history tracker for each prompt and adaptively increases the sampling temperature for residual prompts that previously produced all-correct responses. This encourages the model to generate more diverse reasoning traces, introducing incorrect responses that revive training signals."
  - [Table 1] Temperature T=1.0→1.2 reduces residual prompts from 74.8% to 34.8% for Qwen2.5-32B+DAPO
  - [corpus] Related work "No Prompt Left Behind" addresses similar zero-variance prompt problem but via entropy-guided advantage shaping rather than temperature adaptation

### Mechanism 2
- Claim: History-based temperature scheduling balances exploration-exploitation across prompt difficulty
- Mechanism: The formula T(t)_i = min(T_0 + T_s · H(t)_i, T_max) creates per-prompt exploration that escalates only for prompts proven "too easy" (repeated all-correct), while capping at T_max prevents excessive distribution shift. This reserves aggressive exploration for stubborn residual prompts.
- Core assumption: Prompts that repeatedly yield all-correct responses are "learned" and benefit more from exploration than prompts with mixed correctness.
- Evidence anchors:
  - [Section 4.2] "If H_i is greater than 0, it means that prompt q_i is already easy for the policy... we assign a larger sampling temperature to prompts with H_i > 0"
  - [Figure 2] Average temperature increases exponentially (more prompts become residual), while max temperature caps linearly
  - [corpus] XRPO paper also targets "limited exploration on challenging prompts" but uses different exploration-exploitation targeting

### Mechanism 3
- Claim: Residual prompt training provides retention and novel trace discovery benefits beyond accuracy gains
- Mechanism: Training on residual prompts (via Reactivated Advantage or temperature-induced errors) reinforces already-acquired abilities and may discover alternative reasoning paths. This prevents catastrophic forgetting of "solved" problem types.
- Core assumption: Residual prompts contain learnable structure beyond their current "correct" solutions.
- Evidence anchors:
  - [Section 1] "residual prompts retain learning potential... they help the model retain acquired abilities and may yield novel reasoning traces"
  - [Section 5.2] RA achieves ~70% improvement on AIME2025 mean@32 (less contaminated benchmark)
  - [corpus] Corpus signals indicate this is an emerging research direction with limited prior work

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: ERPO builds directly on GRPO/DAPO; understanding group-normalized advantages (Equation 2) is prerequisite to understanding why zero-variance rewards break the gradient signal.
  - Quick check question: Can you explain why a response group with all-identical rewards produces zero advantage for every response?

- Concept: **Sampling Temperature in Language Models**
  - Why needed here: The entire ERPO mechanism hinges on how temperature affects response diversity; without this, the adaptive temperature strategy is unmotivated.
  - Quick check question: Given a prompt with 95% correct-response probability at T=1.0, what happens to the expected correct-response rate as temperature increases to T=1.4?

- Concept: **On-Policy RL and Distribution Shift**
  - Why needed here: ERPO's T_max cap exists to constrain distribution shift; understanding why on-policy algorithms degrade under large distribution shifts explains this design choice.
  - Quick check question: Why does sampling responses at T=1.4 while training create a different problem than sampling at T=1.0, even if both produce some incorrect responses?

## Architecture Onboarding

- Component map:
  - **History Tracker** -> **Temperature Scheduler** -> **Rollout Generator** -> **Reward Model** -> **Advantage Computer** -> **Policy Updater**

- Critical path:
  1. Prompt batch sampled → History tracker queried for each prompt
  2. Temperature scheduler assigns T_i to each prompt
  3. Rollout generator produces G responses per prompt at assigned temperatures
  4. Reward model evaluates correctness
  5. History tracker updated (increment H_i if all-correct)
  6. Advantage computed (with RA modification if applicable)
  7. Policy update via gradient descent

- Design tradeoffs:
  - **T_max too low**: Residual prompts with high robustness never generate incorrect responses → no training signal recovered
  - **T_max too high**: Excessive distribution shift destabilizes training; may degrade performance on easier prompts
  - **T_s too small**: Slow adaptation, many epochs before residual prompts explored
  - **T_s too large**: Rapid temperature jumps may overshoot optimal exploration level
  - **RA without temperature**: Section 5.2 shows RA alone overfits at scale (40% residual prompts → overrepresentation in training)

- Failure signatures:
  - Training loss plateaus while residual prompt proportion continues rising → temperature not inducing errors (T_max too low or prompt robustness underestimated)
  - Sudden performance drop mid-training → temperature escalation too aggressive, distribution shift
  - History tracker shows 0% prompts with H_i > 0 → base model already underfitting, not reaching residual state
  - Average temperature grows linearly instead of exponentially → new prompts not becoming residual (model not learning)

- First 3 experiments:
  1. **Baseline residual analysis**: Train DAPO on target dataset, log residual prompt proportion per epoch at T=1.0. This quantifies the problem scale before implementing ERPO.
  2. **Temperature sweep on fixed checkpoint**: Take a trained model checkpoint, sample subset of residual prompts at temperatures [1.0, 1.1, 1.2, 1.3, 1.4], measure incorrect-response rate. This calibrates T_s and T_max hyperparameters.
  3. **Ablation: ERPO vs RA vs DAPO**: Train all three on same backbone/dataset with matched compute, evaluate on held-out benchmarks including less-contaminated data (AIME2025-style). This validates the core claim that adaptive temperature outperforms both baseline and naive reactivation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ERPO generalize to reasoning domains beyond mathematics, such as code generation or scientific problem solving?
- Basis in paper: [explicit] The authors evaluate only on mathematical reasoning benchmarks (AIME2024/2025, AMC23, MATH500), leaving other RLVR application domains unexplored.
- Why unresolved: The characteristics of residual prompts may differ across domains; code tasks have deterministic verification while math allows multiple solution paths.
- What evidence would resolve it: Experiments applying ERPO to code benchmarks (e.g., HumanEval, MBPP) and multi-domain reasoning datasets.

### Open Question 2
- Question: Can the temperature schedule hyperparameters (Ts, Tmax) be set adaptively rather than manually tuned per model size?
- Basis in paper: [explicit] The sensitivity analysis shows Ts=0.02, Tmax=1.2 works best for 3B, while different settings cause performance degradation; the paper manually sets different values for 3B vs 7B.
- Why unresolved: Manual tuning doesn't scale and the optimal values may depend on training dynamics not easily predicted a priori.
- What evidence would resolve it: A meta-learning or adaptive scheme that adjusts temperatures based on real-time metrics (e.g., gradient variance, reward distribution).

### Open Question 3
- Question: How does ERPO interact with other exploration-enhancing techniques like tree search or entropy-based sampling?
- Basis in paper: [inferred] The related work mentions methods like TreeRL (Hou et al., 2025) and entropy-based approaches, but ERPO is only tested as a standalone addition to DAPO.
- Why unresolved: These methods target similar diversity issues and may be complementary or redundant with ERPO's temperature-based exploration.
- What evidence would resolve it: Ablation studies combining ERPO with tree search, entropy regularization, or token-level exploration strategies.

## Limitations

- **Model Scale Generalization**: While ERPO shows consistent improvements on 3B and 7B models, the paper lacks validation on larger architectures (13B, 32B+) where residual prompt accumulation dynamics may differ significantly.
- **Generalizability to Non-Mathematical Domains**: The method's efficacy on mathematical reasoning benchmarks doesn't guarantee success in other reasoning domains where residual prompt characteristics may differ.
- **Temperature-Induced Noise vs. Signal**: The core mechanism assumes temperature-triggered incorrect responses provide meaningful learning signal rather than random noise, without analysis of the semantic relationship between high-temperature responses and actual reasoning errors.

## Confidence

- **High Confidence**: The empirical observation that residual prompts accumulate during RL training and reduce training diversity is well-supported by the data (74.8% residual prompts at T=1.0 baseline).
- **Medium Confidence**: The claim that adaptive temperature scaling consistently outperforms baselines is supported across multiple benchmarks, though the magnitude of improvement varies (most notably on AIME2025).
- **Medium Confidence**: The mechanism that history-based temperature scheduling improves exploration-exploitation balance is theoretically sound but lacks ablation studies isolating the temperature component from the RA modification.

## Next Checks

1. **Scale Sensitivity Analysis**: Train ERPO on Qwen2.5-32B with identical hyperparameters to assess whether the method scales proportionally or requires architecture-specific tuning of T_s and T_max.

2. **Domain Transfer Validation**: Apply ERPO to non-mathematical reasoning tasks (e.g., code generation or commonsense reasoning) to test whether residual prompt accumulation and adaptive temperature strategies generalize beyond mathematical domains.

3. **Signal Quality Analysis**: Conduct qualitative analysis of temperature-induced incorrect responses to verify they represent meaningful reasoning errors rather than format failures or random noise, particularly at higher temperatures (T>1.3).