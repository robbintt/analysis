---
ver: rpa2
title: 'JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation
  and Scene Segmentation'
arxiv_id: '2505.10057'
source_url: https://arxiv.org/abs/2505.10057
tags:
- knowledge
- student
- distillation
- multi-task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JointDistill, an adaptive multi-task distillation
  approach for jointly solving depth estimation and scene segmentation. Unlike static
  knowledge transfer from multiple teachers, the method dynamically adjusts the knowledge
  amount from each teacher based on the student model's current learning ability,
  using periodic feedback from a validation dataset.
---

# JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation

## Quick Facts
- arXiv ID: 2505.10057
- Source URL: https://arxiv.org/abs/2505.10057
- Reference count: 40
- This paper introduces JointDistill, an adaptive multi-task distillation approach for jointly solving depth estimation and scene segmentation, achieving significant improvements over state-of-the-art methods with 9.04% and 2.83% average task improvement on Cityscapes and NYU-v2 benchmarks respectively.

## Executive Summary
JointDistill introduces an adaptive multi-task distillation approach that dynamically adjusts knowledge transfer weights from multiple teachers based on the student's current learning ability, using periodic validation feedback. The method addresses knowledge forgetting in multi-teacher settings through a knowledge trajectory mechanism that records essential historical information as attention-based trajectories. Evaluated on Cityscapes and NYU-v2 benchmarks, JointDistill achieves significant improvements over state-of-the-art methods while maintaining computational efficiency.

## Method Summary
JointDistill employs a multi-teacher distillation framework where two single-task teachers (for depth estimation and scene segmentation) provide knowledge to a multi-task student. The key innovations are: (1) feedback-based adaptive weight adjustment that dynamically scales knowledge transfer based on validation performance, (2) knowledge trajectory mechanism that records essential spatial information from teacher features using attention maps, and (3) a connector module that fuses teacher features before distillation. The student is trained with task-specific losses plus distillation losses that include both traditional logits distillation and trajectory-based knowledge preservation.

## Key Results
- Achieves 9.04% average task improvement on Cityscapes benchmark compared to state-of-the-art methods
- Achieves 2.83% average task improvement on NYU-v2 benchmark
- Demonstrates efficiency with lower storage and computation requirements compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Based Multi-Teacher Weight Adjustment
Dynamically adjusting knowledge transfer weights from each teacher based on student validation performance improves multi-task learning over static weighting. The system periodically evaluates the student on a held-out validation set to compute task-specific feedback scores, which determine how much knowledge to transfer from each teacher in the next training period. Higher student performance leads to increased weight for that teacher's knowledge. This prevents one task from dominating learning by ensuring the student receives appropriate guidance from each teacher based on its current learning capacity.

### Mechanism 2: Knowledge Trajectory-Based Distillation
Recording essential historical knowledge as attention-based trajectories and guiding the student to follow them mitigates knowledge forgetting in multi-teacher settings. The method extracts 2D attention maps from features, identifies top-k essential points at each timeframe, and chains them into trajectories. The student is penalized for deviating from these trajectories via L1 distance loss. This preserves the temporal evolution of task-critical spatial information without requiring full parameter storage.

### Mechanism 3: Connector Module for Multi-Teacher Fusion
A learnable connector module that fuses teacher features with adaptive weights enables effective knowledge integration before transfer to the student. Features from N single-task teachers are concatenated and processed through a connector with task-specific dynamic weights. The connector outputs serve as soft targets for student distillation via KL divergence. This creates more coherent knowledge transfer than independent per-teacher distillation by learning meaningful cross-task representations.

## Foundational Learning

- Concept: Knowledge Distillation Fundamentals
  - Why needed here: The entire method builds on KD principles—soft targets, temperature scaling, KL divergence loss. Understanding why teacher logits help students learn is essential.
  - Quick check question: Can you explain why soft targets (logits) often transfer more information than hard labels?

- Concept: Multi-Task Learning Gradient Conflicts
  - Why needed here: The paper addresses knowledge forgetting from conflicting gradient directions in multi-teacher settings. Understanding gradient interference is crucial.
  - Quick check question: What happens when gradients from two tasks point in opposite directions during shared backbone updates?

- Concept: Attention Mechanisms and Feature Maps
  - Why needed here: The trajectory mechanism extracts essential points from 2D attention maps. You need to understand what attention represents spatially.
  - Quick check question: How would you convert a CNN feature tensor [C, H, W] into a 2D spatial attention map?

## Architecture Onboarding

- Component map:
```
Teacher_Seg (frozen) ──┐
                       ├──> [Connector F] ──> Fused Features ──> Distillation Loss
Teacher_Depth (frozen)─┘         ↑                                  ↓
                                 │                            [Student Model]
                          Dynamic Weights ω                    Shared Backbone
                                 ↑                                  ↓
                         [Validation Feedback]            Task Heads (Seg, Depth)
                                 ↑                                  ↓
                         Student Performance              Task Losses + Traj Loss
```

- Critical path:
  1. Initialize student and connector with ω^F_i = 1 for all tasks
  2. Forward pass through teachers (frozen) to get features
  3. Fuse features in connector, compute connector outputs p^F_i
  4. Train student to match connector outputs (KL loss) + task losses
  5. Periodically evaluate student on D_val, update ω^F_i via Eq. 3
  6. Record trajectory points from attention maps, compute L_Traj

- Design tradeoffs:
  - Trajectory length t: Longer captures more history but increases storage; paper doesn't specify optimal value
  - Validation frequency: More frequent feedback is responsive but adds overhead; paper uses periodic checkpoints
  - Number of trajectories K: Top-k essential points; paper uses K=10

- Failure signatures:
  - Weight collapse: If one ω^F_i → 0 early, that task's knowledge is effectively lost. Monitor weight distribution.
  - Trajectory drift: If L_Traj stops decreasing, student may be diverging from teacher learning path.
  - Validation overfitting: If D_val is too small, feedback becomes unreliable—manifests as oscillating weights.

- First 3 experiments:
  1. Reproduce baseline: Train Naive MTL student (no distillation) on Cityscapes with ResNet-50 backbone to establish mIoU and Rel Err baselines from Table I.
  2. Ablate trajectory: Compare +Traj-KD alone vs. +Adap-KD alone on semantic task to verify which component helps which task (Table III pattern).
  3. Hyperparameter sweep α: Test α ∈ {0.5, 1.0, 1.5, 2.0, 3.0} with fixed β=0.001 to reproduce Figure 5 sensitivity curve and confirm optimal range [1, 2].

## Open Questions the Paper Calls Out
- How does JointDistill perform when integrated with more sophisticated multi-modal fusion techniques beyond the simple attention-based text prompt used in the preliminary study? The authors state in Section IV-C: "We believe further improvement could be achieved with advanced fusion and we left the exploration for future."
- How robust is the feedback-based weight adjustment mechanism to the size and distribution of the separate validation dataset? The method relies on periodical feedback from a validation set but does not analyze sensitivity if the validation set is small or unrepresentative.
- Is the "top-k attention points" representation sufficient for capturing complex temporal dependencies in the knowledge trajectory for dense prediction tasks? The paper assumes this captures the "most essential information" but may miss structural context or channel-wise correlations.

## Limitations
- The knowledge trajectory mechanism appears novel but lacks direct corpus evidence for trajectory-based distillation in multi-task settings.
- Critical implementation details like connector architecture specifications are underspecified, making exact reproduction challenging.
- The "periodic" nature of weight updates is not quantified, leaving an important hyperparameter unspecified.

## Confidence
- **High**: Multi-teacher KD framework and loss function formulation; benchmark methodology and evaluation metrics.
- **Medium**: Adaptive weight adjustment mechanism; trajectory loss concept; overall performance claims.
- **Low**: Novel trajectory mechanism uniqueness; exact connector architecture; validation frequency specification.

## Next Checks
1. **Ablation validation**: Replicate Table III to confirm that +Traj-KD vs +Adap-KD shows differential task improvement (depth vs semantic), verifying mechanism complementarity.
2. **Parameter sensitivity**: Reproduce Figure 5 to confirm α ∈ [1,2] is optimal, testing robustness of hyperparameter claims.
3. **Failure mode reproduction**: Intentionally disable trajectory mechanism or use noisy validation feedback to observe weight collapse or gradient conflicts, validating the identified failure signatures.