---
ver: rpa2
title: Why Trust in AI May Be Inevitable
arxiv_id: '2502.20701'
source_url: https://arxiv.org/abs/2502.20701
tags:
- knowledge
- explanation
- explainer
- trust
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining AI systems' decisions
  to human users. The authors model explanation as a search process through knowledge
  networks, where an explainer must find paths between shared concepts and the concept
  to be explained within finite time.
---

# Why Trust in AI May Be Inevitable

## Quick Facts
- arXiv ID: 2502.20701
- Source URL: https://arxiv.org/abs/2502.20701
- Reference count: 40
- Primary result: Explanation can fail even under ideal conditions when shared knowledge exists, because finding connection paths within finite time constraints is not guaranteed.

## Executive Summary
This paper addresses the challenge of explaining AI systems' decisions to human users by modeling explanation as a search process through knowledge networks. The authors show that explanation can fail even under theoretically ideal conditions - when actors are rational, honest, motivated, can communicate perfectly, and possess overlapping knowledge. This is because successful explanation requires not just the existence of shared knowledge but also finding the connection path within time constraints. The result has important implications for human-AI interaction: as AI systems, particularly Large Language Models, become more sophisticated and able to generate superficially compelling but spurious explanations, humans may default to trust rather than demand genuine explanations. This creates risks of both misplaced trust and imperfect knowledge integration.

## Method Summary
The paper formalizes explanation as a sequential search process where an explainer searches their knowledge graph for nodes that overlap with the explainee's knowledge. The model uses Bayesian belief updating to track the explainer's posterior beliefs about the overlap set size after each failed attempt. Expected benefits are calculated based on the probability of success in remaining attempts, and termination decisions are made by comparing expected benefits against continuation costs. The analysis uses negative hypergeometric distribution and derives closed-form expressions for expected search time and benefit decline.

## Key Results
- Explanation can fail even when shared knowledge exists, due to finite search time constraints
- Rational actors may stop explanation attempts before discovering existing shared knowledge when expected benefits fall below continuation costs
- More knowledgeable explainers may see lower benefits to initiating explanation, creating a paradox where expertise hinders explanation motivation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation can fail even when shared knowledge exists, because finding connection paths within finite time constraints is not guaranteed.
- Mechanism: The explainer searches through their knowledge graph R (with N_R nodes) to find a node in the overlap set K that connects to the target node. This is modeled as sampling without replacement. The expected time to success is E(T) = N_R/(N_K + 1), which means larger explainer knowledge networks increase search time proportionally.
- Core assumption: Knowledge structures can be represented as networks where nodes are concepts and edges represent coherence relationships; search proceeds sequentially through connected nodes.
- Evidence anchors:
  - [abstract] "explanation can fail even under theoretically ideal conditions - when actors are rational, honest, motivated, can communicate perfectly, and possess overlapping knowledge"
  - [section 4.1] Equation (2): "E(T) = N_R/(N_K + 1)... this expectation E(T) is a strictly decreasing, convex reciprocal function of N_K + 1"
  - [corpus] Weak direct support; neighbor papers focus on trust/explanation relationships but not the formal search model
- Break condition: If search were not time-bounded, or if the overlap set K were observable rather than hidden, the mechanism would not produce early termination.

### Mechanism 2
- Claim: It becomes rational to stop explanation attempts before discovering existing shared knowledge when expected benefits fall below continuation costs.
- Mechanism: After each failed explanation attempt, the explainer updates their belief about the size of overlap set K using Bayesian updating. The expected benefit E(B_t) decreases over time (except when prior variance is very low relative to mean). When E(B_t) < c(t), rational actors terminate.
- Core assumption: Actors have aligned incentives, can communicate without noise, and make termination decisions based on expected utility maximization.
- Evidence anchors:
  - [abstract] "it can therefore be rational to cease attempts at explanation before the shared knowledge is discovered"
  - [section 4.2] "This result suggests that E(B_t) decreases as t increases... if E(B_t) > c(t) is not met, it will be optimal to stop, even though the set K is non-empty"
  - [corpus] Not directly tested in neighbor papers; remains a theoretical result
- Break condition: If prior beliefs had very low variance (high confidence in overlap size), expected benefits could initially increase, delaying termination.

### Mechanism 3
- Claim: More knowledgeable explainers derive lower expected benefits from initiating explanation, creating a paradox where expertise hinders explanation motivation.
- Mechanism: When explanation begins (t ≪ N_R), the expected benefit of continuing decreases in N_R (explainer's knowledge graph size). From equation (6), E(B_t) = B × μ_K^t / (N_R - t), so larger N_R reduces expected benefit for the same overlap size.
- Core assumption: The explainer's knowledge graph size N_R is known to both parties; benefits and costs are symmetric.
- Evidence anchors:
  - [section 4.2] "An implication of (6) is that when explanation begins... the expected benefits of continuing decrease in N_R, the size of R. This suggests that more knowledgeable agent may see lower benefits to initiating explanation"
  - [section 5.1] "the very depth of expertise that makes an AI system (or indeed human expert) valuable also makes its decisions harder to explain"
  - [corpus] Not empirically validated in corpus; theoretical prediction
- Break condition: If overlap size N_K scaled proportionally with expertise N_R, the effect could be mitigated.

## Foundational Learning

- Concept: **Bayesian belief updating**
  - Why needed here: The model relies on actors updating prior beliefs about overlap size K after each failed explanation attempt. Understanding how p_i^t evolves is essential for grasping why expected benefits decline.
  - Quick check question: If prior belief is uniform over {0, 1, ..., 99} and the first explanation fails, what happens to the expected size of K?

- Concept: **Search theory and optimal stopping**
  - Why needed here: The core insight is that explanation is a sequential search problem with termination conditions. The model formalizes when rational actors should stop searching.
  - Quick check question: In a job search with declining expected offer quality over time, what determines the optimal stopping point?

- Concept: **Knowledge network representation**
  - Why needed here: The paper models knowledge as graphs with nodes (concepts) and edges (coherence relationships). The structure affects search difficulty—complete graphs are easiest, sparse graphs harder.
  - Quick check question: Why might a sparse, hierarchical knowledge network make explanation more difficult than a fully connected one?

## Architecture Onboarding

- Component map:
  - Knowledge graphs R and E -> Search process -> Belief updater -> Expected benefit calculator -> Termination decider
- Critical path: Target node selection → Initial belief specification → Sequential search with belief updating → Benefit-cost comparison → Termination or success
- Design tradeoffs:
  - Complete vs. sparse graph assumption: Complete graphs provide a lower bound on difficulty; real networks are harder
  - Uniform vs. informative priors: Uniform priors cause monotonic benefit decline; low-variance priors can cause initial benefit increases
  - Single vs. multi-agent termination: Model allows either party to terminate; AI-human interactions typically give humans this power
- Failure signatures:
  - Early termination with non-empty K: Rational stopping before overlap discovered (primary failure mode)
  - High-expertise avoidance: Knowledgeable explainers never initiate explanation
  - Spurious explanation acceptance: LLMs generate plausible bridges that don't reflect true reasoning (mentioned but not modeled)
- First 3 experiments:
  1. Replicate Figure 1 simulation: Vary N_R (50, 100, 200, 300) with uniform priors, plot E(B_t) over time to verify monotonic decline
  2. Test variance sensitivity: Fix N_R = 300, μ_K^1 = 10; vary V_K^1/μ_K^1 ratio (0.5, 1.0, 2.0, 5.0) to replicate Figure 2 patterns
  3. Sparse graph extension: Implement partially connected R with BFS/DFS strategies; compare termination rates against complete graph baseline to validate the "lower bound" claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different search strategies (e.g., breadth-first vs. depth-first) affect the likelihood of successful explanation under varying network topologies?
- Basis in paper: [explicit] The authors state that "a systematic investigation of search strategies... under varying network topologies could yield valuable insights" regarding when explainers should persist or pivot.
- Why unresolved: The current model assumes a complete graph for the explainer, which eliminates path dependence and renders search strategy irrelevant, unlike in real-world sparse networks.
- What evidence would resolve it: Simulation results comparing the efficiency and success rates of different search algorithms across networks with varying sparsity and hierarchy.

### Open Question 2
- Question: How do knowledge networks evolve dynamically during the explanation process, and does successful explanation facilitate future search efficiency?
- Basis in paper: [explicit] The paper notes that "successful explanations likely modify these networks by creating new links" and calls for a "dynamic analysis" to formalize how "learning and explainability co-evolve."
- Why unresolved: The current formalization treats knowledge networks as static entities, capturing the search for a path but not the structural changes resulting from learning.
- What evidence would resolve it: A formal model extension or longitudinal data showing if successful integration of a node increases network density or connectivity, thereby reducing expected search time for subsequent explanations.

### Open Question 3
- Question: How do explanation dynamics change in multi-agent settings compared to the dyadic explainer-explainee model?
- Basis in paper: [explicit] The authors suggest "extending our framework to multi-agent settings" to theorize on "collaborative explanation" and how "collective search processes differ from individual ones."
- Why unresolved: The current model is restricted to two actors (dyadic), whereas organizational contexts often involve multiple explainers or explainees interacting simultaneously.
- What evidence would resolve it: Theoretical models or experimental studies measuring whether adding more explainers with overlapping knowledge graphs significantly alters the probability of early termination or success.

## Limitations
- The model assumes knowledge can be represented as a complete graph with uniform search difficulty, which represents a best-case scenario that may not reflect real-world knowledge structures
- The paper does not empirically validate the theoretical predictions about benefit decline and termination decisions
- The extension to AI systems, particularly LLMs generating spurious explanations, is discussed but not formally modeled within the paper's framework

## Confidence

- **High confidence**: The mathematical framework for Bayesian belief updating and expected benefit calculation is sound and internally consistent. The core result that explanation can fail despite shared knowledge existing is well-supported.
- **Medium confidence**: The claim that more knowledgeable explainers have lower motivation to initiate explanation is theoretically supported but lacks empirical validation. The implication for AI explanation is plausible but not rigorously tested.
- **Low confidence**: The discussion of LLM-generated spurious explanations creating a "trust imperative" is speculative and not formally modeled within the paper's framework.

## Next Checks
1. **Empirical validation of termination behavior**: Design human subject experiments where explainers search for shared concepts with varying prior beliefs about overlap size. Measure whether termination decisions align with model predictions when the overlap set is non-empty.
2. **Network structure sensitivity**: Extend simulations to compare complete, random, and hierarchical knowledge graphs. Test whether the monotonic benefit decline pattern holds across different network topologies and search strategies.
3. **Low-variance prior scenarios**: Systematically test the claim that high prior confidence (low variance) can prevent early termination by running simulations with prior variance-to-mean ratios below 0.5, verifying whether E(B_t) increases initially as predicted.