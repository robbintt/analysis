---
ver: rpa2
title: Multimodal Safety Evaluation in Generative Agent Social Simulations
arxiv_id: '2510.07709'
source_url: https://arxiv.org/abs/2510.07709
tags:
- unsafe
- agent
- agents
- plan
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simulation framework for evaluating the
  multimodal safety of generative agents in social environments. The authors construct
  a dataset of 1,000 social activity scenarios with paired safe/unsafe plans and images,
  then simulate five agents using three models (Claude, GPT-4o mini, Qwen-VL) interacting
  over 600 steps.
---

# Multimodal Safety Evaluation in Generative Agent Social Simulations

## Quick Facts
- arXiv ID: 2510.07709
- Source URL: https://arxiv.org/abs/2510.07709
- Reference count: 40
- One-line primary result: Agents achieve only 55% success rate in correcting unsafe plans, with 45% acceptance of unsafe actions when paired with misleading visuals

## Executive Summary
This paper introduces a simulation framework for evaluating the multimodal safety of generative agents in social environments. The authors construct a dataset of 1,000 social activity scenarios with paired safe/unsafe plans and images, then simulate five agents using three models (Claude, GPT-4o mini, Qwen-VL) interacting over 600 steps. Agents perform plan revisions and safety evaluations using a Judge Agent to assess multimodal contexts. Results show that while agents can detect direct contradictions, they often fail to align local revisions with global safety, achieving only a 55% success rate in correcting unsafe plans. Conversion rates varied from 20% (GPT-4o mini, multi-risk) to 98% (Claude, localized fire/heat). Notably, 45% of unsafe actions were accepted when paired with misleading visuals, indicating a strong tendency to overtrust images. The framework provides a reproducible platform for studying multimodal safety, coherence, and social dynamics in agent societies.

## Method Summary
The method involves five generative agents simulating social activities in a virtual environment, using three different large language models (Claude 3.5 Sonnet, GPT-4o-mini, Qwen-VL-2B-Instruct). Agents operate with a memory stream and perform plan revisions every 50 steps, where they assess their current activities against paired images and memories, propose revisions if unsafe, and submit these to a separate Judge Agent for validation. The simulation runs for 600 steps (7 PMâ€“5 AM time window) and measures unsafe-to-safe conversion rates, interaction counts, and acceptance/rejection ratios. The dataset consists of 1,000 scenarios with 21 categories and 192 subcategories, using CLIP embeddings (ViT-L/14, 336px) for image-text alignment verification.

## Key Results
- Agents achieved only 55% success rate in correcting unsafe plans, often failing to align local revisions with global safety constraints
- Conversion rates varied significantly: 98% for Claude in localized fire/heat scenarios vs 20% for GPT-4o mini in multi-risk situations
- 45% of unsafe actions were accepted when paired with misleading visuals, demonstrating strong visual overtrust bias
- Safety improvements were limited by late corrections (Qwen-VL leaving unsafe actions until step 450+) and local-global misalignment

## Why This Works (Mechanism)

### Mechanism 1: Iterative Plan Revision with External Supervision
- **Claim:** Agents improve safety by periodically evaluating local plans against global safety constraints via a separate Judge Agent.
- **Mechanism:** The system introduces a Plan Revision Layer. Every 50 steps, the agent retrieves memories and the current activity-image pair. If an unsafe action is detected, the agent generates a revision, which is validated by a "Judge Agent" before the plan is updated in the memory stream.
- **Core assumption:** Agents possess sufficient reasoning capability to identify local risks when prompted, and the Judge Agent provides a reliable ground truth for safety validation.
- **Evidence anchors:**
  - [abstract]: "Agents perform plan revisions and safety evaluations using a Judge Agent... success rate in correcting unsafe plans."
  - [section 3.2]: "Every 50 steps, agents enter a structured plan revision session... The candidate is then submitted to a separate LLM-as-a-judge agent."
  - [corpus]: Related work (ToolSafe) supports the efficacy of proactive step-level guardrails, though this specific "Judge" architecture is unique to this simulation.
- **Break condition:** If the Judge Agent accepts a "safe" revision that is contextually unsafe (situational misalignment), or if the agent refuses to generate alternatives, the safety improvement loop fails.

### Mechanism 2: Multimodal Situational Grounding
- **Claim:** Safety evaluation relies on cross-modal alignment where visual context modulates the safety classification of text actions.
- **Mechanism:** The dataset provides paired text (activities) and images. Agents must process both; a text-safe action (e.g., "jumping") becomes unsafe when paired with a visual context (e.g., "rooftop").
- **Core assumption:** The vision encoder (CLIP/ViT) correctly extracts semantic features from the image that correspond to the risk categories defined in the text.
- **Evidence anchors:**
  - [abstract]: "45% of unsafe actions were accepted when paired with misleading visuals, indicating a strong tendency to overtrust images."
  - [section 4.3]: "GPT-4o-mini focuses on cross-modal consistency... Qwen-VL emphasizes global narrative consistency but fails to identify high-risk actions."
  - [corpus]: Multimodal situational safety literature (cited as [28]) posits that harmless text can become unsafe in risky visual contexts.
- **Break condition:** If the visual features are ambiguous or the model "overtrusts" the image (ignoring textual risk signals), the mechanism produces false negatives (accepting unsafe plans).

### Mechanism 3: Social Reinforcement of Safety Norms
- **Claim:** Social interactions serve as a mechanism for either propagating unsafe behaviors or reinforcing safety corrections through peer acceptance/rejection.
- **Mechanism:** Agents share plans and reactions. An unsafe plan discussed positively by peers (high acceptance ratio) can reinforce the behavior, while rejection or negative feedback can trigger reflection and revision.
- **Core assumption:** Agents weight social feedback heavily in their planning logic and do not operate in isolation.
- **Evidence anchors:**
  - [abstract]: "The framework... measures social dynamics, including interaction counts and acceptance ratios."
  - [section 4.4]: "Unsafe actions such as rooftop races were repeatedly kept... Cheering reinforces excitement, coherent with plan."
  - [corpus]: Weak direct evidence in provided neighbors; assumes standard social simulation dynamics where peer influence alters state.
- **Break condition:** If agents prioritize internal narrative coherence over social feedback, or if malicious peers collude to validate unsafe acts, social correction fails.

## Foundational Learning

- **Concept: Generative Agent Memory Stream**
  - **Why needed here:** The agents are not stateless; they require a memory of past actions and reflections to determine if a current plan is safe based on history (e.g., "we already did a dangerous activity").
  - **Quick check question:** Does the agent retrieve the most recent observation or the most *relevant* observation when deciding to revise a plan?

- **Concept: Cross-Modal Alignment (CLIP Embeddings)**
  - **Why needed here:** The dataset construction relies on cosine similarity between text and image embeddings to ensure the "unsafe image" actually matches the "unsafe activity."
  - **Quick check question:** If the similarity score is 0.30 (soft threshold) but not 0.35 (hard threshold), is the image retained or flagged for review?

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The system relies on a distinct "Judge Agent" to verify the safety of the "Planner Agent's" revisions, preventing the planner from grading its own homework.
  - **Quick check question:** Is the Judge Agent using the same model backbone as the Planner, or a separate instance/model to ensure objectivity?

## Architecture Onboarding

- **Component map:**
  Environment -> Agent Core (5 agents: PR, KS, JS, CH, AV) -> Cognition (Memory Stream, Plan Revision Layer, Reflection Module) -> Supervision (Judge Agent) -> Data (1,000 Scenarios)

- **Critical path:**
  1. **Initialization:** Agent is seeded with persona + initial unsafe/safe plan.
  2. **Execution:** Agent acts -> Observes Environment -> Updates Memory.
  3. **Revision (Every 50 steps):** Retrieve Plan + Image -> Detect Unsafe -> Propose Alternative -> **Judge Validation** -> Update Plan.
  4. **Social:** Agent interacts -> Accept/Reject proposals -> Log metrics.

- **Design tradeoffs:**
  - **Cost vs. Granularity:** Enabling the full multimodal plan revision loop costs $5-$8/run vs $2-$3 for baseline (Section 4.1).
  - **Model Selection:** Claude offers high conversion (98% in specific categories) but implies higher API costs/latency compared to GPT-4o mini or Qwen-VL.
  - **Rigidity:** Fixed 50-step revision cycle may miss rapid safety violations occurring between intervals.

- **Failure signatures:**
  - **Visual Overtrust:** Agent marks an unsafe action as safe because the image looks "fun" or "socially coherent" (45% false acceptance rate).
  - **Local-Global Misalignment:** Agent fixes a specific action (e.g., "don't run") but fails to see the global context (e.g., "fire in the room") remains unsafe.
  - **Late Correction:** Agent leaves unsafe actions until step 450+ (Qwen-VL behavior), failing to iterate early.

- **First 3 experiments:**
  1. **Baseline Validation:** Run a single agent on 10 scenarios to verify the Judge Agent actually reduces the unsafe count over 600 steps compared to a non-judged run.
  2. **Ablation on Visuals:** Run the simulation with text-only plans vs. text+image plans to quantify the specific degradation caused by "misleading visuals" (targeting the 45% failure mode).
  3. **Social Diffusion Test:** Initialize one agent with a highly unsafe plan and measure how quickly (or if) it propagates to/rejected by peers using the Acceptance Ratio metric.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agent architectures be modified to reduce the overtrust in visual content that leads to the acceptance of unsafe actions?
- Basis in paper: [explicit] The authors state that "45% of unsafe actions were accepted when paired with misleading visuals, indicating a strong tendency to overtrust visual content."
- Why unresolved: The paper identifies this overtrust as a critical limitation but does not propose or validate specific mechanisms to mitigate this bias within the decision-making loop.
- What evidence would resolve it: A demonstration of a modified agent architecture that significantly lowers the acceptance rate of unsafe actions when presented with misleading visual pairings compared to the baseline.

### Open Question 2
- Question: What mechanisms are required to ensure local plan revisions align with global safety constraints rather than treating risks in isolation?
- Basis in paper: [explicit] The abstract notes that agents "frequently fail to align local revisions with global safety, achieving only a 55% success rate in correcting unsafe plans."
- Why unresolved: The current reflection and revision cycles allow agents to correct specific actions but often fail to maintain coherence with the overall safety of the multi-step plan.
- What evidence would resolve it: Simulation results showing a higher unsafe-to-safe conversion rate and fewer residual unsafe actions in the final plan state using a global constraint enforcement module.

### Open Question 3
- Question: How does extending the complexity of social scenarios and introducing adversarial dynamics impact the reproducibility of current safety evaluation metrics?
- Basis in paper: [explicit] The conclusion explicitly lists as future work: "extend the complexity of scenarios and develop more methods for safety assessment and mitigation."
- Why unresolved: The current study utilizes a fixed environment and cooperative social dynamics; it is unknown if the current framework scales to more complex, adversarial, or highly volatile environments.
- What evidence would resolve it: Successful application of the SocialMetrics suite to simulations involving adversarial agents or highly complex environments, yielding consistent and informative safety profiles.

## Limitations
- The framework depends critically on the Judge Agent's reliability, which remains unverified as an objective safety validator
- Social dynamics model may oversimplify peer influence mechanisms and their impact on safety behavior
- The 45% visual overtrust rate, while striking, may be dataset-specific rather than representing a fundamental model limitation

## Confidence
- **Multimodal Safety Detection Claims** (High Confidence): The framework successfully demonstrates that agents can detect direct contradictions between text actions and visual contexts, with clear quantitative metrics showing success/failure rates.
- **Social Dynamics Influence Claims** (Medium Confidence): While the framework measures interaction patterns, the causal relationship between social feedback and safety behavior modification lacks direct experimental validation.
- **Model Performance Comparisons** (Medium Confidence): The reported performance differences between Claude, GPT-4o mini, and Qwen-VL are specific to this simulation environment and dataset.

## Next Checks
1. **Judge Agent Ablation Test**: Run parallel simulations where agents perform plan revisions without Judge Agent validation. Compare unsafe-to-safe conversion rates to isolate the actual contribution of the Judge Agent versus agent self-correction capabilities.

2. **Visual Context Manipulation**: Systematically modify the image-text pairings to create graduated levels of visual ambiguity (maintaining the same text but varying image risk cues). Measure how conversion rates change across this spectrum to better understand the 45% visual overtrust threshold.

3. **Temporal Safety Analysis**: Implement continuous safety monitoring (every 10 steps instead of 50) to identify whether the current 50-step interval allows unsafe actions to persist too long. Track the cumulative safety impact of delayed corrections versus immediate detection.