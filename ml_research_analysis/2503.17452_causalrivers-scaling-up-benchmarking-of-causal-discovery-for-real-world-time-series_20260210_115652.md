---
ver: rpa2
title: CausalRivers -- Scaling up benchmarking of causal discovery for real-world
  time-series
arxiv_id: '2503.17452'
source_url: https://arxiv.org/abs/2503.17452
tags:
- causal
- discovery
- data
- https
- time-series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CausalRivers, the largest real-world benchmark
  for causal discovery from time-series data to date, featuring 1160 river discharge
  measurement stations across eastern Germany and Bavaria with 15-minute temporal
  resolution spanning 2019-2023. The dataset includes two causal ground truth graphs
  constructed using multiple information sources, which can be sampled to generate
  thousands of subgraphs for benchmarking.
---

# CausalRivers -- Scaling up benchmarking of causal discovery for real-world time-series

## Quick Facts
- arXiv ID: 2503.17452
- Source URL: https://arxiv.org/abs/2503.17452
- Reference count: 40
- Largest real-world benchmark for causal discovery from time-series data with 1160 river stations

## Executive Summary
CausalRivers introduces a comprehensive benchmark for evaluating causal discovery methods on real-world time-series data, featuring 1160 river discharge measurement stations across eastern Germany and Bavaria with 15-minute temporal resolution spanning 2019-2023. The dataset includes two causal ground truth graphs constructed using multiple information sources, which can be sampled to generate thousands of subgraphs for benchmarking. The authors evaluated established causal discovery methods alongside naive baselines and found that simple strategies often perform comparably to sophisticated approaches. The experiments revealed that domain adaptation can significantly improve performance, and that proper time-series subsampling strategies depend on the specific causal discovery method used.

## Method Summary
The benchmark consists of three datasets (RiversEastGermany: 666 nodes, RiversBavaria: 494 nodes, RiversElbeFlood: 42 nodes) with corresponding ground truth DAGs constructed through multi-source integration. Subgraphs of sizes 3, 5, and 10 nodes are sampled using various strategies (random, close, root-cause, confounder, disjoint) to create diverse testing conditions. Methods are evaluated using AUROC, with baselines including RP (Reverse Physical) and CC (Cross-Correlation) alongside established algorithms like VAR, PCMCI, and DYNOTEARS. Hyperparameters are optimized through grid search across resolutions and lags, with the best performer reported for each method.

## Key Results
- Simple RP and CC baselines achieve .80 AUROC on Close-3 graphs, matching or exceeding sophisticated methods
- Domain adaptation via fine-tuning improves causal pretraining model performance by 50% on held-out datasets
- Time-series subsampling strategies significantly impact method performance, with no universal optimal approach
- DYNOTEARS performs at chance level (.50 AUROC) on Random-3 and Random-5 graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple domain-informed baselines can match sophisticated causal discovery methods on real-world time-series data.
- Mechanism: The RP baseline exploits the physical constraint that river discharge increases downstream—larger rivers cannot flow into smaller ones. By comparing mean discharge values, this captures true causal direction without statistical modeling. CC uses lag-maximizing correlation to infer temporal precedence.
- Core assumption: Domain-specific physical constraints reliably encode causal direction in this hydrological context.
- Evidence anchors:
  - [section 4.1] Table 3 shows RP achieves .80 AUROC on Close-3 graphs, matching or exceeding VAR (.81) and outperforming PCMCI (.64).
  - [section 3.3] "we simply assume x1 → x2 if the mean of x2 is bigger than the mean of x1"
  - [corpus] Limited corpus support—related papers focus on algorithmic robustness rather than domain-knowledge baselines.
- Break condition: In domains where magnitude or physical properties don't correlate with causal direction (e.g., gene regulatory networks), this mechanism fails completely.

### Mechanism 2
- Claim: Ground truth graphs constructed from multi-source integration enable subgraph sampling for diverse benchmarking conditions.
- Mechanism: The authors combine metadata from state agencies, Wikipedia river information, remote sensing data, and manual verification to construct causal graphs. These graphs are then sampled to create subgraphs with specific characteristics (root-cause chains, hidden confounders, disjoint components) that stress-test different method assumptions.
- Core assumption: River topology (upstream → downstream) reliably represents causal relationships; weather confounding is the primary hidden factor.
- Evidence anchors:
  - [abstract] "Leveraging multiple sources of information and time-series meta-data, we constructed two distinct causal ground truth graphs"
  - [section 3.1] "semi-automatic construction of the graph... all edges were double-checked by hand"
  - [corpus] TimeGraph and KarmaTS papers similarly address benchmark construction but focus on synthetic generation rather than real-world ground truth.
- Break condition: If river topology doesn't match true causal mechanisms (e.g., pump stations, dams, artificial canals altering flow), ground truth labels may be inaccurate.

### Mechanism 3
- Claim: Domain adaptation via fine-tuning significantly improves causal pretraining model performance across graph structures.
- Mechanism: Causal Pretraining (CP) models learn mappings from synthetic time-series to causal graphs. Fine-tuning on domain-specific samples (RiversBavaria) transfers learned representations to target domain (RiversEastGermany), improving performance even on held-out graph types.
- Core assumption: Synthetic pretraining captures transferable causal reasoning patterns; domain fine-tuning adjusts for distributional specificities without catastrophic forgetting.
- Evidence anchors:
  - [section 4.3] "finetuning CP networks with random five variable samples from RiversBavaria strongly boosts its performance... sufficient to outperform the best causal strategy of Experiment Set 1 on 50% of the datasets"
  - [section 3.4] "causal lag of a specific relationship differs throughout the years as it depends on the amount of water"
  - [corpus] SpaceTime paper addresses non-stationarity but doesn't evaluate domain adaptation strategies.
- Break condition: If source and target domains share no structural similarities, fine-tuning may degrade rather than improve performance.

## Foundational Learning

- Concept: **Granger Causality and its Limitations**
  - Why needed here: VAR and other Granger-based methods perform best (Table 3), but the paper highlights that real-world violations (non-linearity, hidden confounding, non-stationarity) challenge theoretical guarantees.
  - Quick check question: Can you explain why Granger causality fails when there are hidden confounders affecting multiple time-series simultaneously?

- Concept: **Graph Sampling Strategies and Their Implications**
  - Why needed here: The benchmark's value comes from generating thousands of subgraphs with specific characteristics (confounder, root-cause, disjoint) to test method robustness.
  - Quick check question: Why would a "confounder" subgraph (where a multi-child node is hidden) be particularly challenging for constraint-based methods like PC?

- Concept: **AUROC vs. F1 for Causal Discovery Evaluation**
  - Why needed here: The paper reports AUROC to avoid threshold selection complications, but notes this may overestimate real-world performance when thresholds are unknown.
  - Quick check question: What practical challenges arise when deploying a causal discovery method that requires threshold selection on unlabeled data?

## Architecture Onboarding

- Component map:
  Data Layer (three datasets with 15-minute resolution) -> Ground Truth Layer (two DAGs via multi-source integration) -> Sampling Layer (subgraphs with specific characteristics) -> Evaluation Layer (AUROC/F1/Accuracy computation) -> Method Interface (standardized input/output)

- Critical path:
  1. Load raw time-series and ground truth graphs
  2. Select sampling strategy based on evaluation goal (e.g., confounder graphs for hidden variable robustness)
  3. Extract subgraph and corresponding time-series
  4. Apply preprocessing (resolution aggregation, normalization)
  5. Run causal discovery method with hyperparameter configuration
  6. Compare predicted edges against ground truth, compute threshold-free metrics

- Design tradeoffs:
  - Full time-series (175k timesteps) vs. subsampled windows: Full data captures seasonal patterns but includes low-innovation periods; flood/rain windows increase effect strength but reduce data volume
  - 15-minute vs. aggregated resolution: High resolution preserves lag structure but increases noise and computation; aggregation may miss short-lag causal effects
  - AUROC vs. F1: AUROC avoids threshold issues but masks deployment challenges; F1 requires threshold selection that's impossible without labels

- Failure signatures:
  - DYNOTEARS performing at chance (.50 AUROC) on Random-3 and Random-5 suggests gradient-based methods struggle with low-innovation data sections
  - PCMCI's poor performance on Close graphs (.64) despite good performance on Random+1 (.83) indicates sensitivity to specific graph structures
  - All methods degrading on Confounder graphs confirms hidden confounding vulnerability

- First 3 experiments:
  1. Replicate the "Close-5" baseline comparison to validate your method pipeline produces consistent AUROC scores with Table 3
  2. Test your method on "Confounder-3" graphs to characterize hidden variable robustness before scaling to larger graphs
  3. Evaluate time-series subsampling strategies (Full vs. No-Rain vs. Flood) on the RiversElbeFlood subset to understand how distributional shifts affect your method specifically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can optimal time-series subselection strategies be determined for specific causal discovery methods in non-stationary real-world environments?
- Basis in paper: [explicit] The authors found that performance on subsections (e.g., "No rain" vs. "Flood") varied significantly by method and encouraged future work to "put explicit effort into finding proper subselection strategies."
- Why unresolved: The experiments showed mixed results; methods like PCMCI benefited from stable periods while Dynotears struggled, suggesting the strategy is method-dependent and lacks a general theory.
- What evidence would resolve it: A systematic study identifying which data regimes (e.g., high variance, stationary, specific seasons) maximize performance for specific algorithm classes.

### Open Question 2
- Question: To what extent can domain adaptation techniques improve causal discovery performance across different real-world geographical distributions?
- Basis in paper: [explicit] Experiment Set 3 showed fine-tuning on "RiversBavaria" improved performance on "RiversEastGermany," leading the authors to conclude that domain adaptation "should be investigated thoroughly in future work."
- Why unresolved: The study provided only a "first example" using Causal Pretraining (CP), leaving the broader applicability and optimal techniques for other methods unexplored.
- What evidence would resolve it: Benchmarks comparing standard methods against their domain-adapted counterparts across the diverse geographical and climatic conditions provided in the dataset.

### Open Question 3
- Question: How can causal discovery methods be modified to handle dynamic causal lags that vary with the state of the system (e.g., water volume)?
- Basis in paper: [inferred] Section 3.4 notes that causal lag in river discharge "differs throughout the years as it depends on the amount of water," which violates the assumption of static lags in many current methods.
- Why unresolved: Established methods generally assume static lag structures, failing to account for physical realities like flow velocity changes.
- What evidence would resolve it: Development and evaluation of algorithms that model time-varying delays, demonstrating superior performance on the CausalRivers dataset compared to fixed-lag baselines.

## Limitations
- Ground truth reliability depends on river topology matching true causal mechanisms, which may not account for artificial interventions
- Results may be domain-specific to hydrological systems where physical magnitude correlates with causal direction
- Synthetic pretraining transfer effectiveness depends on alignment between synthetic and real-world data distributions

## Confidence
- High: Benchmark scale and construction methodology are well-documented and verifiable; comparative performance of established methods is supported by robust statistical evidence
- Medium: Superiority of simple domain-informed baselines is well-supported within hydrological context but needs validation for generalization; effectiveness of domain adaptation demonstrated but needs broader testing
- Low: Claim about most challenging benchmark needs qualification to specific constraints; long-term stability of causal relationships across 2019-2023 period could be affected by climate change

## Next Checks
1. Test RP and CC baseline mechanisms on a non-hydrological domain (e.g., financial time-series or sensor networks) to assess whether physical magnitude-based causal inference is broadly applicable or domain-specific
2. Conduct systematic validation of ground truth accuracy by comparing predicted vs. actual river flow patterns during known extreme events to quantify potential topological-ground truth mismatches
3. Evaluate the causal pretraining approach on a held-out domain (e.g., weather station data from different geographic regions) to test whether synthetic pretraining captures truly transferable causal reasoning patterns rather than overfitting to hydrological features