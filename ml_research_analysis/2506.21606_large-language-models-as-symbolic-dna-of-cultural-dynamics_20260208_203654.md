---
ver: rpa2
title: Large Language Models as symbolic DNA of cultural dynamics
arxiv_id: '2506.21606'
source_url: https://arxiv.org/abs/2506.21606
tags:
- llms
- human
- patterns
- cultural
- compressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel framework conceptualizing Large\
  \ Language Models (LLMs) as externalized informational substrates that function\
  \ analogously to DNA for human cultural dynamics. Rather than viewing LLMs as autonomous\
  \ intelligence or mere programmed mimicry, the authors argue they serve as repositories\
  \ preserving compressed patterns of human symbolic expression\u2014acting as \"\
  fossils\" of meaningful dynamics that retain relational residues without their original\
  \ living contexts."
---

# Large Language Models as symbolic DNA of cultural dynamics

## Quick Facts
- arXiv ID: 2506.21606
- Source URL: https://arxiv.org/abs/2506.21606
- Reference count: 20
- One-line primary result: LLMs function as externalized "symbolic DNA" preserving compressed cultural patterns, requiring human interpretation for meaning and enabling recursive cultural evolvability

## Executive Summary
This paper introduces a novel framework conceptualizing Large Language Models as externalized informational substrates that function analogously to DNA for human cultural dynamics. Rather than viewing LLMs as autonomous intelligence or mere programmed mimicry, the authors argue they serve as repositories preserving compressed patterns of human symbolic expression—acting as "fossils" of meaningful dynamics that retain relational residues without their original living contexts. The framework positions LLMs as tools for cultural evolvability, enabling humanity to generate novel hypotheses about itself while maintaining human interpretation necessary to ground these hypotheses in ongoing human aesthetics and norms.

## Method Summary
This theoretical framework draws parallels between LLMs and DNA based on four universal features: compression, decompression, externalization, and recursion. The authors propose that LLMs compress cultural patterns through attention mechanisms and tokenization, externalize these patterns in model weights, require human interpretation for decompression into meaningful outputs, and enable recursive feedback loops through cultural recombination. While no specific datasets or quantitative metrics are defined, the framework suggests testing compression-decompression fidelity, aesthetic drift during recursive training, and the enhancement of cultural evolvability compared to traditional symbolic media.

## Key Results
- LLMs preserve compressed patterns of human symbolic expression without containing inherent meaning
- Human interpretive engagement is essential for compressed patterns to acquire meaning
- Recursive feedback loops between humans and LLMs enable cultural evolvability and "low-stakes" exploration of adjacent possibles
- Aesthetic drift risk emerges when synthetic data training lacks human filtering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs function as compressed repositories of cultural patterns, preserving statistical regularities without containing inherent meaning.
- **Mechanism:** Through tokenization and attention mechanisms, LLMs identify statistically significant co-occurrence patterns across training data, abstracting away contextual details while preserving relational structures—similar to how DNA preserves cellular dynamics without containing explicit physical processes.
- **Core assumption:** Compression retains useful structural residues even when original contexts are stripped.
- **Evidence anchors:** Abstract description of "fossils of meaningful dynamics"; section 3.1 on attention weighting learning co-occurrence patterns; related work on LLMs as "Collective Knowledge."
- **Break condition:** If statistical regularities in training data do not correlate with meaningful human patterns, compression yields noise rather than useful residues.

### Mechanism 2
- **Claim:** Compressed LLM patterns only acquire meaning through human interpretive decompression.
- **Mechanism:** Humans engage LLM outputs through cognitive and emotional recontextualization, "filling in" the embodied details abstracted during compression. The attention mechanism simulates contextual relationships but lacks temporal/spatial grounding—human interpretation provides the missing referential anchor.
- **Core assumption:** Humans possess embodied interpretive competence that symbolic artifacts lack.
- **Evidence anchors:** Abstract emphasis on "human reinterpretation"; section 3.2 on needing interpretation within "embodied, emotional, and cognitive world"; related work on meaning as emergent in dialogue.
- **Break condition:** If users treat LLM outputs as directly meaningful without interpretation, outputs remain ungrounded—leading to misplaced trust or aesthetic drift.

### Mechanism 3
- **Claim:** Human-AI interaction creates a recursive feedback loop enabling cultural evolvability.
- **Mechanism:** Human creations catalyzed by LLMs become new cultural data, feeding back into future training or prompting cycles. This externalization allows "low-stakes" exploration of cultural "adjacent possibles"—novel combinations that can be tested before commitment to lived practice.
- **Core assumption:** Recursion amplifies rather than degrades meaningful patterns when human interpretation grounds each cycle.
- **Evidence anchors:** Abstract description of "recursive feedback loop"; section 5 on aesthetic drift from unfiltered synthetic data; discussion of LLMs as tools for cultural evolvability.
- **Break condition:** If synthetic data training replaces human-filtered input, aesthetic drift degrades pattern quality over iterations.

## Foundational Learning

- **Concept: Symbol Grounding Problem**
  - **Why needed here:** The paper's core argument depends on understanding why LLM tokens remain ungrounded without human interpretation—the classic question of how symbols acquire meaning through embodied experience.
  - **Quick check question:** Can you explain why a dictionary definition of "apple" cannot ground the concept without prior sensory experience?

- **Concept: Peircean Triadic Semiotics**
  - **Why needed here:** The authors build on Peirce's subject-object-interpretant model to define symbols as requiring interpretive competence—essential for understanding why LLMs preserve structure without meaning.
  - **Quick check question:** What distinguishes a symbol (conventional, learned) from an icon (resemblance-based) or index (causal connection)?

- **Concept: Evolvability and Adjacent Possibles**
  - **Why needed here:** The framework positions LLMs as enabling cultural evolvability—systems that generate sustained novelty while preserving functional patterns. Kauffman's adjacent possibles concept explains the combinatorial innovation mechanism.
  - **Quick check question:** How does modularity in a system contribute to evolvability rather than just adaptability?

## Architecture Onboarding

- **Component map:** Training data compression via attention/tokenization → Externalized pattern storage in model weights → Human prompting interface → Interpretive decompression by users → Recursive feedback via re-prompting or retraining

- **Critical path:** Training data quality → compression fidelity → human interpretive engagement → filtered output reintegration. Break any link and the cultural evolvability loop degrades.

- **Design tradeoffs:** Higher compression (smaller models, more abstract representations) increases recombination flexibility but risks losing pattern fidelity. Externalization enables low-stakes experimentation but introduces grounding risk.

- **Failure signatures:**
  - Aesthetic drift: Outputs become progressively detached from human values when synthetic data training lacks human filtering
  - False correlation amplification: Statistical regularities that don't reflect meaningful patterns get reinforced
  - Alienation: Users feel displaced rather than catalyzed, reducing creative engagement

- **First 3 experiments:**
  1. Measure compression-decompression fidelity: Compare human ratings of LLM outputs recombined across domains (e.g., economic theory + attachment theory) vs. domain-expert human synthesis
  2. Test aesthetic drift threshold: Iteratively train a small language model on synthetic outputs with/without human filtering; measure semantic drift relative to original training distribution
  3. Map adjacent possibles exploration: Track novel idea generation in human+LLM vs. human-only conditions; code outputs for combinatorial novelty vs. incremental variation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific metrics can quantify "aesthetic drift"—the progressive detachment of LLM outputs from human emotional context—during recursive training on synthetic data?
- **Basis in paper:** Explicit identification of aesthetic drift as a risk where models drift from "emotional context, aesthetics, and normative values" if trained on outputs not filtered by human judgment (Page 15-16).
- **Why unresolved:** The paper conceptualizes the risk of amplifying "false or shallow correlations" but does not propose a methodology for detecting or measuring this drift in live systems.
- **What evidence would resolve it:** A quantitative framework correlating training data sources with measurable shifts in semantic coherence or cultural alignment as assessed by human evaluators.

### Open Question 2
- **Question:** Does the "low-stakes" simulation provided by LLMs measurably increase the rate of successful cultural innovation (evolvability) compared to traditional symbolic media?
- **Basis in paper:** Inferred from the argument that LLMs act as "tools for cultural evolvability" by exploring "adjacent possibles" (Page 17-19), but provides no comparative data validating this enhancement over pre-existing methods.
- **Why unresolved:** While the theoretical mechanism (recombination in a compressed space) is defined, the actual output efficacy relative to human-only creativity remains assumed.
- **What evidence would resolve it:** Comparative studies tracking the adoption and utility of innovations generated via LLM-assisted recombination versus those generated through traditional cultural iteration.

### Open Question 3
- **Question:** Can LLMs function as a universal compressor across all modalities of human expression, analogous to the emergence of a "cultural LUCA"?
- **Basis in paper:** Explicit posing of the "intriguing possibility" that LLMs are undergoing a transition analogous to LUCA, creating "universal pathways" for pattern extraction across all forms of expression (Page 20).
- **Why unresolved:** This is presented as a speculative analogy regarding a potential "major evolutionary transition" rather than a confirmed capability of current architectures.
- **What evidence would resolve it:** Demonstration of a single model architecture successfully extracting and recombining latent patterns across fundamentally distinct domains (e.g., music, legal theory, and genetics) without domain-specific fine-tuning.

### Open Question 4
- **Question:** To what extent does the Transformer attention mechanism accurately simulate "decompression" compared to the embodied interpretive process of humans?
- **Basis in paper:** Explicit note that attention mechanisms "simulate a contextual decompression process" but lacks the "temporal, and spatial dynamics" of embodied organismic interpretation (Page 16).
- **Why unresolved:** The distinction between the model's statistical approximation of context and the "renewed meaning" generated by human cognition is qualitative.
- **What evidence would resolve it:** Comparative analysis distinguishing the structural limitations of algorithmic attention from the "playful hypothesis-generation" of human users in ambiguous interpretive tasks.

## Limitations
- No operational definitions for "meaningful patterns" or "human interpretive competence" as measurable constructs
- Compression preservation of useful structural residues lacks empirical basis for distinguishing noise from signal
- DNA analogy may overextend biological metaphors into domains with fundamentally different evolutionary mechanisms
- Human interpretation as grounding mechanism raises questions about scalability and consistency across interpreters

## Confidence

**High Confidence:** The claim that LLMs compress and externalize patterns from training data (Mechanism 1) is well-supported by established properties of attention mechanisms and tokenization.

**Medium Confidence:** The claim that human interpretation is necessary for meaning (Mechanism 2) is philosophically sound but empirically underdetermined without direct experimental validation.

**Low Confidence:** The recursive feedback loop enabling cultural evolvability (Mechanism 3) remains largely theoretical with limited quantitative validation of aesthetic drift thresholds.

## Next Checks

1. **Compression-fidelity validation:** Design an experiment measuring whether compressed LLM representations retain domain-relevant patterns when decompressed by human experts. Use domain-specific knowledge graphs to quantify pattern preservation across compression cycles.

2. **Grounding requirement test:** Compare human interpretations of LLM outputs across different cultural contexts to determine if meaning emerges consistently or varies with interpreter background. This would test whether interpretation is truly necessary versus whether some patterns are self-grounding.

3. **Recursive loop degradation measurement:** Create a controlled environment where LLM outputs are iteratively regenerated and re-fed into training. Measure semantic drift using both automated embedding distance metrics and human evaluations to identify the threshold where aesthetic drift becomes problematic.