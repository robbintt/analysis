---
ver: rpa2
title: 'Inferring Latent Intentions: Attributional Natural Language Inference in LLM
  Agents'
arxiv_id: '2601.08742'
source_url: https://arxiv.org/abs/2601.08742
tags:
- player
- word
- att-nli
- players
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Attributional Natural Language Inference
  (Att-NLI), a framework extending traditional NLI to handle intention-driven reasoning
  in multi-agent environments. Att-NLI employs a two-stage abductive-deductive process:
  first inferring latent intentions from observed actions, then drawing logical conclusions.'
---

# Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents

## Quick Facts
- arXiv ID: 2601.08742
- Source URL: https://arxiv.org/abs/2601.08742
- Authors: Xin Quan; Jiafeng Xiong; Marco Valentino; André Freitas
- Reference count: 40
- Primary result: Neuro-symbolic Att-NLI achieves 17.08% spy win rate, a 24.22% improvement over standard Att-NLI and 78.29% over standard NLI agents

## Executive Summary
This paper introduces Attributional Natural Language Inference (Att-NLI), a framework extending traditional NLI to handle intention-driven reasoning in multi-agent environments. Att-NLI employs a two-stage abductive-deductive process: first inferring latent intentions from observed actions, then drawing logical conclusions. The authors operationalize this framework through Undercover-V, a verifiable social deduction game where agents must identify spies based on word-based clues. Three agent types are evaluated: standard NLI (deductive only), Att-NLI (abductive-deductive), and neuro-symbolic Att-NLI (with external theorem prover). Experiments across four LLMs show a clear performance hierarchy, with neuro-symbolic Att-NLI agents achieving the highest spy win rate of 17.08%, a 24.22% improvement over standard Att-NLI and 78.29% over standard NLI agents. These results demonstrate the effectiveness of Att-NLI in developing agents with sophisticated reasoning capabilities for multi-agent environments.

## Method Summary
The authors operationalize Att-NLI through Undercover-V, a social deduction game where six players (five citizens with the same word, one spy with a different word) must identify the spy through word-based clues. Three agent types are evaluated: standard NLI (deductive only), standard Att-NLI (abductive-deductive), and neuro-symbolic Att-NLI (with Isabelle/HOL theorem prover). Agents play 30 games each using four LLMs (GPT-4o, GPT-4o-mini, Mistral-Medium, Mixtral-8x22B) with word pairs selected for similarity scores between 0.79-0.85. The neuro-symbolic agent autoformalizes descriptions into Neo-Davidsonian event semantics, uses theorem proving for verification, and refines guesses based on TP feedback. Key metrics include spy win rate, Attributional Score (Soundness × Alignment), and average round number.

## Key Results
- Neuro-symbolic Att-NLI achieves 17.08% spy win rate, outperforming standard Att-NLI (13.86%) and standard NLI (9.57%)
- Att-NLI agents show 24.22% improvement over standard Att-NLI and 78.29% improvement over standard NLI agents
- Attributional Score demonstrates strong correlation with spy win rate (0.57 Pearson correlation)
- Average round number for neuro-symbolic Att-NLI is 3.83, lower than standard Att-NLI (4.63) and standard NLI (5.10)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage abductive-deductive pipeline improves intention inference over purely deductive NLI.
- Mechanism: Stage 1 (abduction) generates the most plausible intention hypothesis h* from observed descriptions P via argmax Pr(h|P). Stage 2 (deduction) derives a conclusion C such that P ∪ {h*} ⊨ C, ensuring decisions rest on both observations and inferred intentions.
- Core assumption: Intentions can be modeled as discrete hypotheses with posterior probabilities approximated by LLMs.
- Evidence anchors:
  - [abstract] "Att-NLI... framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference... and subsequent deductive verification"
  - [Section 2.1] Formal definition: intention selection chooses h* ∈ H satisfying P |= h*, then conclusion inference derives C where P ∪ H* |= C
  - [corpus] Related work (Probabilistic Modeling of Intentions, 2510.18476) supports maintaining belief distributions over latent intentions, but does not validate the specific abductive-deductive hierarchy claimed here.
- Break condition: If intention hypotheses are highly ambiguous or priors are misspecified, abduction may select incorrect h*, propagating errors into deduction.

### Mechanism 2
- Claim: External theorem prover (TP) verification and iterative guess refinement substantially improve Att-NLI performance.
- Mechanism: The neuro-symbolic agent autoformalizes descriptions into Neo-Davidsonian event semantics in Isabelle/HOL, constructs theories Θ = (A, τ), and uses Sledgehammer for proof search. TP feedback (Traces(Θ')) updates the guessed opponent word g_i across rounds when proofs fail.
- Core assumption: Autoformalization preserves semantic content and TP proof status reliably indicates logical validity.
- Evidence anchors:
  - [Section 3.1.3] "The neuro-symbolic agent applies Isabelle/HOL for automated theorem proving and refinement... uses TP feedback to refine the guess word"
  - [Results] Neuro-symbolic Att-NLI achieves 17.08% spy win rate, +24.22% over standard Att-NLI, +78.29% over standard NLI
  - [Appendix B.1] Eq. 12–13 formalizes guess update: g^{r+1}_i = LLM(g^r_i, Traces(Θ'), θ^r_i, p_up) if valid(Θ') = 0
  - [corpus] Theorem-of-Thought (2506.07106) explores multi-agent abductive/deductive reasoning but does not integrate external theorem provers; corpus lacks direct validation of TP-augmented Att-NLI.
- Break condition: If autoformalization introduces systematic semantic drift or TP timeouts become frequent (>5 iterations per theory), the neuro-symbolic advantage degrades.

### Mechanism 3
- Claim: Undercover-V's no-lying constraint ensures cumulative logical consistency, enabling testable intention attribution.
- Mechanism: Each description must satisfy T(s_i, Δ^r_i) = True, meaning parsed description Δ is consistent with state axioms φ(s_i). By induction, cumulative descriptions remain consistent: φ(s_i) ∪ Δ^{(R)}_i ⊭ ⊥ for all rounds R.
- Core assumption: A reliable parsing function parse: d → Form(L) exists, and φ(s) correctly encodes verifiable features of each state.
- Evidence anchors:
  - [Section 3] "Undercover-V explicitly prohibits lying... adheres to the principle (ex falso sequitur quodlibet)"
  - [Appendix C] Proof by induction that cumulative descriptions remain consistent, enabling external verifiers to converge on true states
  - [corpus] Bayesian Social Deduction (2506.17788) evaluates LLMs in Avalon but involves deception; no corpus paper validates the no-lying testability claim.
- Break condition: If players produce vague descriptions that are technically consistent but uninformative, convergence may stall; if parse fails to capture semantic nuances, false contradictions may arise.

## Foundational Learning

- Concept: **Abductive Inference**
  - Why needed here: Core to intention selection—LLM must generate the best explanation for observed actions, not just derive conclusions from given premises.
  - Quick check question: Given "Player describes their word as 'often infused with bergamot'", can you generate multiple intention hypotheses (e.g., "player holds Earl Grey Tea" vs. "player is bluffing")?

- Concept: **Neo-Davidsonian Event Semantics**
  - Why needed here: The autoformalization pipeline represents descriptions as predicate-argument structures with events, agents, and patients before Isabelle verification.
  - Quick check question: How would you formalize "A tea often infused with bergamot gives a citrusy aroma" using event predicates (Infuse, Agent, Patient)?

- Concept: **Attributional Score (AS · AA)**
  - Why needed here: Novel metric quantifying Att-NLI capability: Attributional Soundness (similarity to citizen vs. spy word) × Attributional Alignment (similarity to others' descriptions).
  - Quick check question: If an agent's AS = 0.9 and AA = 0.7, what does AttScore = 0.63 indicate about their intention inference vs. blending ability?

## Architecture Onboarding

- Component map:
  - Game Engine -> Description Phase -> Autoformalization Module -> Isabelle/HOL Server -> TP Verification -> Logical Record V
  - LLM Agents (NLI/Att-NLI/Neuro-Symbolic) -> Abduction/Deduction -> Description/Vote Generation
  - Post-Vote Phase -> TP Feedback -> Guess Refinement Loop

- Critical path:
  1. Description phase → Autoformalize each player's descriptions → TP verification → Build logical record V
  2. Abduction: LLM infers identity distribution {θ_j} and self-identity θ_i using V and guessed word g^r_i
  3. Deduction: Generate description or vote based on inferred intention
  4. Post-vote: If TP flags mismatch between voted-out player's description and g^r_i, refine guess

- Design tradeoffs:
  - **TP integration latency**: Avg. solving time ~15–25s per theory (Fig. 9); scales with axiom count
  - **Early-stop majority vote**: Reduces TP inconsistency but may miss edge cases
  - **Word pair selection**: Similarity <0.79 yields trivial games; >0.79 needed for meaningful Att-NLI evaluation (Appendix E)

- Failure signatures:
  - Spy win rate = 0% across all models → word pair too easy (low similarity)
  - TP returns "syntax error" repeatedly (>5 iterations) → autoformalization prompt needs adjustment
  - Neuro-symbolic agent underperforms Att-NLI → check Isabelle server connectivity, theory construction bugs

- First 3 experiments:
  1. **Baseline replication**: Run Standard NLI vs. Standard Att-NLI on Ceylon Tea–Earl Grey pair with GPT-4o-mini, verify ~25–50% win rate improvement (Fig. 4a)
  2. **TP integration stress test**: Vary axiom count (2–14) and measure solving time; confirm <30s for typical game theories
  3. **Ablation on guess refinement**: Disable Eq. 12–13 updates; expect drop in neuro-symbolic advantage (particularly for Mixtral-8x22b per results section)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Att-NLI capabilities generalize to multi-agent environments with more than two latent intention types and temporally extended behaviors beyond lexical semantic discrimination?
- Basis in paper: [explicit] The authors state "Undercover-V reduces attribution to discriminating between two latent roles (spy vs. citizen) that are tightly tied to a pair of lexical anchors, so that 'intention' is proxied by which of two nearby word-level concepts best explains an agent's utterances; this design still privileges local lexical semantics over richer, temporally extended behaviour or social norms, which are central to attribution in real multi-agent systems."
- Why unresolved: Current experiments only test binary intention classification (spy/citizen) with static word pairs, which oversimplifies real-world attribution.
- What evidence would resolve it: Experiments in multi-role games (3+ roles) with dynamic, temporally-evolving intentions across extended interaction sequences.

### Open Question 2
- Question: How does Att-NLI performance correlate with broader agent alignment properties such as trustworthiness, safety, and social calibration in open-ended multi-agent environments?
- Basis in paper: [explicit] The paper acknowledges "this operationalisation... does not yet address how Att-NLI interacts with broader desiderata such as social calibration, trustworthiness, safety, or alignment with human judgments in more open-ended multi-agent environments."
- Why unresolved: Attributional Score measures only soundness and alignment within game constraints, not real-world alignment properties.
- What evidence would resolve it: Correlation studies between Att-NLI performance and established safety/trustworthiness benchmarks in unconstrained dialogue.

### Open Question 3
- Question: Can the neuro-symbolic Att-NLI approach maintain performance gains when scaling beyond ~12 axioms per Isabelle theory, given theorem prover latency increases?
- Basis in paper: [inferred] Figure 9 shows solving time ranges from 15-24 seconds with 2-14 axioms, suggesting potential scalability concerns for longer games with accumulated descriptions.
- Why unresolved: The paper demonstrates effectiveness but does not test extended multi-round games that would generate significantly larger knowledge bases.
- What evidence would resolve it: Performance and latency measurements in games extending beyond 5 rounds with 20+ accumulated axioms per player.

## Limitations
- The paper does not provide code-level details for the Isabelle/HOL integration pipeline, making exact reproduction challenging
- Human-defined reference sentences for Attributional Soundness calculation are not provided, creating ambiguity in metric computation
- The "early-stop majority vote" mechanism for TP verification has underspecified thresholds and retry logic

## Confidence
- High confidence: The core two-stage abductive-deductive framework and its formalization in Section 2.1 are well-specified and theoretically sound
- Medium confidence: The neuro-symbolic agent's performance improvements (17.08% spy win rate) are well-documented, but the exact Isabelle integration details remain unclear
- Low confidence: The Attributional Score metric calculation lacks complete implementation details, particularly for the human reference sentences

## Next Checks
1. Implement the full Isabelle/HOL integration pipeline with Neo-Davidsonian autoformalization and verify that TP feedback can successfully refine guess words across multiple game iterations
2. Conduct ablation studies disabling the guess refinement mechanism (Eq. 12-13) to quantify its contribution to the neuro-symbolic agent's performance advantage
3. Test the robustness of word pair selection by varying similarity thresholds (0.79-0.85) and measuring impact on spy win rates across all three agent types