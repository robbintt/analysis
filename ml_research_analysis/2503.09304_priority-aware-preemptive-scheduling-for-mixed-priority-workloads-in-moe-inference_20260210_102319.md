---
ver: rpa2
title: Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference
arxiv_id: '2503.09304'
source_url: https://arxiv.org/abs/2503.09304
tags:
- jobs
- qllm
- inference
- batch
- scheduling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QLLM, an inference system for Mixture-of-Experts
  models that addresses the challenge of efficiently serving mixed-priority workloads
  in data centers. The key innovation is expert-level preemption and priority-aware
  scheduling, which enables latency-sensitive jobs to preempt best-effort jobs at
  any layer, minimizing head-of-line blocking.
---

# Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference

## Quick Facts
- arXiv ID: 2503.09304
- Source URL: https://arxiv.org/abs/2503.09304
- Reference count: 39
- Reduces latency-sensitive job time-to-first-token by up to 101.6x and maintains SLO compliance at up to 7 requests/sec

## Executive Summary
This paper introduces QLLM, an inference system for Mixture-of-Experts (MoE) models that addresses the challenge of efficiently serving mixed-priority workloads in data centers. The key innovation is expert-level preemption and priority-aware scheduling, which enables latency-sensitive jobs to preempt best-effort jobs at any layer, minimizing head-of-line blocking. The system employs a redesigned MoE layer with per-expert queues and a lightweight state management mechanism to enable fine-grained control without sacrificing throughput. QLLM is modular and integrates seamlessly with Hugging Face models.

## Method Summary
QLLM builds a priority-aware scheduler with four queues (LS/BE × prefill/decode) and implements per-expert FIFO queues within MoE layers to enable fine-grained preemption. The system uses a unified Sequence and Batch abstraction with facade pattern to avoid costly tensor split-merge operations during preemption. The batch selection logic prioritizes LS prefill/decode over BE, and expert-level preemption with lightweight state management preserves KV cache, routing metadata, and partial expert outputs. The method was evaluated on a Nvidia A100 80GB GPU with Mixtral 8×7B (4-bit quantization) and ShareGPT dataset, comparing against Hugging Face TGI baseline.

## Key Results
- LS job time-to-first-token reduced by up to 101.6x (average 65.2x)
- Meets SLOs at up to 7 requests/sec while baseline cannot
- Maintains comparable or higher throughput and reduces turnaround time by up to 12.8x

## Why This Works (Mechanism)

### Mechanism 1: Fine-Grained Expert-Level Preemption
The system inserts a control loop between the scheduler and inference engine, checkpointing BE batches mid-layer or mid-expert when LS jobs arrive. This avoids waiting for full iteration completion (300-400ms) before scheduling high-priority tasks. The core assumption is that state save/restore overhead is significantly lower than remaining BE execution time.

### Mechanism 2: Per-Expert Queuing
QLLM replaces monolithic tensor processing with per-expert FIFO queues, decoupling token execution from rigid batch synchronization. This allows draining high-priority tokens from specific expert queues while leaving BE tokens buffered, prioritizing specific computation paths without dismantling the entire batch structure.

### Mechanism 3: Unified State Management (Facade Pattern)
The system encapsulates sequence metadata into unified object abstractions rather than physically splitting tensors. This enables constant-time context switching logic by simulating contiguous batch tensors from fragmented sequence objects, avoiding expensive tensor split-merge operations.

## Foundational Learning

- **Head-of-Line (HOL) Blocking**: The specific pathology QLLM fixes - understanding how long BE jobs block subsequent LS jobs in FCFS queues is essential. Quick check: In FCFS, if Request A (BE) takes 5 seconds and Request B (LS) arrives 1 second later, how long does Request B wait?

- **Mixture-of-Experts (MoE) Routing**: QLLM exploits MoE structure where tokens are routed to specific "experts" rather than visiting every expert. Quick check: Does every token in an MoE layer visit every expert, or only a subset (e.g., top-k)?

- **Iteration-Level vs. Fine-Grained Scheduling**: The paper moves scheduling checkpoints from "iteration" boundaries to "expert/layer" boundaries. Quick check: In standard LLM inference, when can the scheduler typically interrupt a running batch to add a new job?

## Architecture Onboarding

- **Component map**: Dispatcher -> Batch Engine -> Inference Engine -> Unified Dynamic Cache
- **Critical path**: LS Arrival → Preemption Signal → Checkpoint BE → Process LS → Restore BE
- **Design tradeoffs**: Trades BE latency (up to 2.04× increase) and memory overhead for LS responsiveness (65.5× improvement)
- **Failure signatures**: Starvation of BE jobs under high LS arrival rates; memory fragmentation from Unified Dynamic Cache
- **First 3 experiments**:
  1. SLO Compliance Test: Replicate arrival rate sweep (1-8 req/sec) with fixed LS/BE ratio
  2. Preemption Overhead Micro-benchmark: Measure checkpoint/restore wall-clock time in isolation
  3. Starvation Stress Test: Flood with 100% LS traffic to verify graceful degradation

## Open Questions the Paper Calls Out

- **Memory Optimization**: How can memory management be optimized to reduce overhead of caching additional states required for fine-grained preemption? The authors are actively exploring methods to reduce memory overhead.

- **Starvation Mitigation**: What mechanisms can effectively mitigate potential starvation of BE jobs under heavy LS loads? The strict prioritization may indefinitely delay BE