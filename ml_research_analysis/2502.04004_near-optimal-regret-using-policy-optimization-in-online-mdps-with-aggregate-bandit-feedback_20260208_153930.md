---
ver: rpa2
title: Near-optimal Regret Using Policy Optimization in Online MDPs with Aggregate
  Bandit Feedback
arxiv_id: '2502.04004'
source_url: https://arxiv.org/abs/2502.04004
tags:
- lemma
- regret
- bound
- feedback
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online finite-horizon Markov Decision Processes
  (MDPs) with adversarial losses and aggregate bandit feedback, where the agent only
  observes the total loss of each trajectory rather than individual losses at each
  step. The key challenge is that standard policy optimization methods rely on estimating
  the Q-function, which requires observing individual step losses.
---

# Near-optimal Regret Using Policy Optimization in Online MDPs with Aggregate Bandit Feedback

## Quick Facts
- arXiv ID: 2502.04004
- Source URL: https://arxiv.org/abs/2502.04004
- Reference count: 40
- Primary result: Near-optimal regret bounds for online MDPs with aggregate bandit feedback using policy optimization

## Executive Summary
This paper addresses the challenge of learning in online finite-horizon MDPs with adversarial losses when only aggregate bandit feedback is available. Unlike standard settings where per-step losses are observed, here the agent only sees the total loss of each trajectory. The authors introduce the U-function - the expected total trajectory cost conditioned on visiting a particular state-action pair - which enables regret decomposition using only aggregate feedback. Two algorithms are presented: one for known dynamics achieving optimal regret O(H²√(SAK)), and one for unknown dynamics with regret O(H³S√(AK)), improving prior work by factors of H²S⁵A².

## Method Summary
The method centers on the U-function, which represents the expected total trajectory cost given visitation to a state-action pair. For known dynamics, exact occupancy measures are computed via dynamic programming. For unknown dynamics, optimistic transition models are maintained. The algorithm estimates U-values using importance sampling on aggregate trajectory losses with a bias-variance tradeoff parameter γ. Policy updates use multiplicative weights on an estimate of the U-function minus a bonus term. The bonus is computed via Bellman backup to cancel distribution mismatch in the regret analysis. Hyperparameters are set to η = √(log(HSAK/δ))/(H√(SAK) + H²√K) and γ = 2ηH.

## Key Results
- Known dynamics algorithm achieves optimal regret bound of Θ(H²√(SAK))
- Unknown dynamics algorithm achieves O(H³S√(AK)) regret, improving prior work by H²S⁵A²
- Lower bound of Ω(H²√(SAK)) establishes optimality of known dynamics algorithm
- First policy optimization algorithm for aggregate bandit feedback in online MDPs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The U-function enables regret decomposition that only requires aggregate trajectory loss for estimation, unlike Q-functions which require per-step losses.
- Mechanism: The U-function represents the expected total trajectory cost conditioned on visiting a state-action pair. By Lemma 2, U(s,a) - Q(s,a) = W(s), which is action-independent. This term cancels in the regret decomposition because policy probability differences sum to zero. Corollary 1 shows regret decomposes as a weighted sum over U-values rather than Q-values.
- Core assumption: Markov property and Markovian policies ensure the pre-visit trajectory is independent of the action taken at time h.
- Evidence anchors: [section 3]: "The U-function at a given state and action is the expected cost on the entire trajectory given that we visit this state-action pair"; [section 3]: Lemma 2 proof showing U - Q = W(s) which is action-independent.

### Mechanism 2
- Claim: The importance sampling estimator Û can be computed using only aggregate feedback while remaining nearly unbiased with controlled variance.
- Mechanism: The estimator Û(s,a) = I{visited} × L₁:H / (μ(s,a) + γ) uses the full trajectory loss L₁:H in the numerator and adds exploration parameter γ to the denominator. The γ term ensures variance stays bounded when μ(s,a) is small, at the cost of small bias. Lemma 3 shows E[Û] = μ/(μ+γ) × U, which is nearly unbiased when γ << μ.
- Core assumption: Accurate occupancy measure μ(s,a) is available (via dynamic programming for known dynamics, or confidence bounds for unknown dynamics).
- Evidence anchors: [section 4, Eq. 3]: Definition of Û estimator with γ parameter; [section 4, Lemma 3]: Derivation showing the bias is μ/(μ+γ) × U.

### Mechanism 3
- Claim: Bonus terms cancel the distribution mismatch between the optimal policy's state occupancy and the algorithm's occupancy in the regret analysis.
- Mechanism: The bonus B(s,a) is computed as a Q-function with respect to an artificial loss b(s) = Σₐ 3γHπ(a|s)/(μπ(s)π(a|s)+γ). The Bonus term in the regret decomposition equals Σμ^k × b - Σμ* × b. The negative part cancels the problematic mismatch terms from Bias1 and Reg. The positive part is bounded by O(γH²SAK).
- Core assumption: The value difference lemma applies to the bonus function; known dynamics allow exact computation (unknown dynamics use optimistic transitions from confidence sets).
- Evidence anchors: [section 4]: Definition of b(s) and B(s,a) via Bellman backup; [section 4]: "The negative term exactly cancels out (i) and (ii) from Bias1 and Reg".

## Foundational Learning

### Concept 1: Value Difference Lemma
- Why needed here: The entire regret analysis builds on extending this lemma from Q-functions to U-functions. Without understanding how V^π - V^π' decomposes into per-state-action terms, the algorithm design and analysis are inaccessible.
- Quick check question: For policies π and π', write the decomposition of V^π_1(s_init) - V^π'_1(s_init) as a sum over states and actions.

### Concept 2: Multiplicative Weight Updates (Exponential Weights)
- Why needed here: The policy update π_{k+1}(a|s) ∝ π_k(a|s) exp(-η(Û - B)) is exponential weights applied independently per state. The regret bound for OMD (Lemma 25) directly controls the "Reg" term.
- Quick check question: What is the regret bound for exponential weights with learning rate η, K rounds, and d actions?

### Concept 3: Importance Sampling for Bandit Estimation
- Why needed here: The core challenge is estimating U-values from bandit feedback. Importance sampling with the trajectory-level observation L₁:H is how the algorithm obtains unbiased estimates without per-step losses.
- Quick check question: If you play action a with probability p and observe total loss L, what is the unbiased importance sampling estimator for action a's loss?

## Architecture Onboarding

### Component Map:
1. **Policy Store**: Maintains π_k(a|s) for all (h,s,a); initialized uniform, updated via multiplicative weights
2. **Trajectory Buffer**: Stores the episode trajectory {(s_h, a_h)} and aggregate loss L₁:H
3. **Occupancy Computer**: Computes μ_k(s,a) via forward dynamic programming (known dynamics) or optimistic bounds via confidence set optimization (unknown dynamics)
4. **Estimator Module**: Constructs Û(s,a) = I{visited} × L / (μ + γ) for visited state-action pairs
5. **Bonus Computer**: Backward Bellman recursion computing B(s,a) with artificial loss b(s)
6. **Transition Model** (unknown only): Empirical transition counts and confidence set P^k

### Critical Path:
1. **Episode execution**: Sample actions from π_k, observe trajectory and aggregate loss L₁:H
2. **Occupancy computation**: Forward pass computing μ_k(s,a) for all (h,s,a)
3. **Estimation**: Construct Û(s,a) for each (h,s,a) using visitation indicators
4. **Bonus computation**: Backward pass: B_H = 0, then B_h(s,a) = b_h(s) + Σ_{s',a'} p(s'|s,a)π_{h+1}(a'|s')B_{h+1}(s',a')
5. **Policy update**: For each (s,a,h): π_{k+1}(a|s) ∝ π_k(a|s) exp(-η(Û - B))

### Design Tradeoffs:
1. **γ (exploration parameter)**: Larger γ → lower variance but bias ~γ/μ. Paper sets γ = 2ηH ≈ 1/√K.
2. **η (learning rate)**: Set to (H√(SAK) + H²√K)^{-1}. Larger η increases the second-order regret term ηH⁵K.
3. **Known vs unknown dynamics**: Unknown requires maintaining confidence sets (O(HSA) space) and optimistic optimization (O(HS²A) per-episode computation vs O(HSA) for known).
4. **Closed-form vs convex optimization**: This algorithm uses closed-form updates; prior work (Cohen et al. 2021) required solving convex programs each episode.

### Failure Signatures:
1. **Exploration starvation**: If μ(s,a) becomes extremely small for some (s,a), Û explodes even with γ. Symptom: occasional episodes with extreme policy shifts. Fix: increase γ or add explicit exploration.
2. **Bonus over-optimism**: In unknown dynamics, if confidence sets are too loose (few visits), B becomes too large and policy ignores Û. Symptom: policy plateaus with suboptimal performance. Fix: ensure sufficient early exploration.
3. **Transition model divergence**: If transition estimates are wrong (non-stationary environment), both μ and B are corrupted. Symptom: regret grows linearly rather than √K. Fix: add change-point detection or use sliding windows.

### First 3 Experiments:
1. **Estimator bias-variance validation**: Fix a policy, run 10K episodes on a small MDP (S=3, A=2, H=3). Compare empirical mean of Û estimates to true U-values (computed analytically). Vary γ ∈ {0.001, 0.01, 0.1} to plot bias-variance tradeoff curve.
2. **Regret scaling verification**: Run Algorithm 1 on a medium MDP (S=10, A=5, H=5) with K ∈ {1000, 5000, 10000, 50000}. Plot regret vs √K and fit the scaling coefficient. Compare to the bound Õ(H²√(SAK)).
3. **Unknown dynamics transition error**: Implement Algorithm 2 on an MDP with a bottleneck transition (one state-action pair is rare). Track the sum of |μ^k - μ̱^k| (occupancy uncertainty) over episodes. Verify it scales as Õ(H³S√(AK)) per Lemma 27. Compare regret between known and unknown dynamics versions to quantify the H√S gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regret bound for the unknown dynamics setting be tightened to match the lower bound of Ω(H²√(SAK)), eliminating the current gap factor of H√S?
- Basis in paper: [explicit] Page 12 states that "determining the exact optimal bound for the unknown dynamics case remains an open problem" and identifies a multiplicative gap of H√S between the algorithm's upper bound and the lower bound.
- Why unresolved: The current analysis for Policy Optimization with unknown dynamics relies on Bernstein-style confidence sets and occupancy measure estimation, which introduce additional dependence on H and S.
- What evidence would resolve it: An improved analysis or a new algorithm achieving Õ(H²√(SAK)) regret for unknown dynamics, or a higher lower bound specific to the unknown transition setting.

### Open Question 2
- Question: Can the U-function framework be extended to environments with function approximation, such as Linear MDPs, despite the U-function not being linear under these assumptions?
- Basis in paper: [explicit] Page 13 (Discussion and Future Work) asks whether the U-function could be useful for "environments with infinitelyely many states," explicitly noting the challenge that the "U-function is not linear" in the Linear MDP setting.
- Why unresolved: Standard function approximation techniques typically rely on the linearity of the value or Q-function, properties which the U-function lacks in this setting.
- What evidence would resolve it: A modified algorithm or analysis demonstrating sub-linear regret for aggregate bandit feedback in Linear MDPs, potentially by identifying specific properties of the U-function that allow for efficient estimation.

### Open Question 3
- Question: Does aggregate bandit feedback strictly increase the minimax regret dependence on the horizon H compared to semi-bandit feedback in the unknown dynamics setting?
- Basis in paper: [inferred] The paper establishes a lower bound of Ω(H²√(SAK)) for aggregate feedback (Page 13), distinguishing it from the semi-bandit lower bound of Ω(H^(3/2)√(SAK)) cited in the text (Page 3).
- Why unresolved: While the lower bounds suggest aggregate feedback is harder (quadratic vs. 3/2-power dependence on H), the best known Policy Optimization upper bounds for both settings currently coincide at Õ(H³ S √A).
- What evidence would resolve it: An upper bound for aggregate feedback that matches the semi-bandit's horizon dependence, or a formal proof establishing the separation of the optimal rates.

## Limitations

- The unknown dynamics algorithm has a regret bound O(H³S√(AK)) that is H√S worse than the lower bound, with the gap arising from occupancy measure uncertainty analysis.
- The estimator introduces a bias-variance tradeoff through parameter γ that is not extensively validated empirically, and practical performance could degrade when some state-action pairs have small occupancy.
- The confidence set construction for unknown dynamics relies on Bernstein concentration bounds that may be loose in practice, particularly for rare state-action pairs that dominate the regret bound's H³S√A term.

## Confidence

- High confidence in the known dynamics algorithm and its O(H²√(SAK)) regret bound, given the clean U-function decomposition and established multiplicative weights analysis.
- Medium confidence in the unknown dynamics algorithm's regret bound due to the complexity of the occupancy measure uncertainty analysis and the potential looseness of confidence set bounds.
- High confidence in the theoretical framework and proof techniques, but medium confidence in practical implementation details (numerical stability of log-sum-exp, exploration parameter tuning).

## Next Checks

1. **Estimator Behavior**: Implement the U-function estimator on a small MDP with controlled visitation frequencies. Plot the empirical bias vs γ and variance vs γ curves to verify the theoretical tradeoff.
2. **Unknown Dynamics Scaling**: Run the unknown dynamics algorithm on MDPs with varying bottleneck structures (e.g., different numbers of rare state-action pairs). Measure how the regret scales with S and √A to validate the H³S√A term.
3. **Lower Bound Construction**: Attempt to construct MDPs with non-uniform initial transitions that achieve regret matching the Ω(H²√(SAK)) lower bound. This would test the tightness of the reduction argument.