---
ver: rpa2
title: 'Questioning Representational Optimism in Deep Learning: The Fractured Entangled
  Representation Hypothesis'
arxiv_id: '2505.11581'
source_url: https://arxiv.org/abs/2505.11581
tags:
- cppn
- picbreeder
- skull
- representation
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the assumption that better performance in
  deep learning necessarily leads to better internal representations. The authors
  propose the Fractured Entangled Representation (FER) hypothesis, suggesting that
  networks trained via conventional stochastic gradient descent (SGD) often exhibit
  fragmented and redundant internal representations, where the same concepts are represented
  multiple times and entangled with unrelated functions.
---

# Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis

## Quick Facts
- arXiv ID: 2505.11581
- Source URL: https://arxiv.org/abs/2505.11581
- Reference count: 38
- Key outcome: Challenges assumption that better performance leads to better internal representations, proposing Fractured Entangled Representation (FER) as a fundamental limitation of SGD-trained networks

## Executive Summary
This paper challenges the assumption that better performance in deep learning necessarily leads to better internal representations. The authors propose the Fractured Entangled Representation (FER) hypothesis, suggesting that networks trained via conventional stochastic gradient descent (SGD) often exhibit fragmented and redundant internal representations, where the same concepts are represented multiple times and entangled with unrelated functions. This contrasts with representations from open-ended search processes, which tend to be more unified and modular. Through experiments comparing evolved CPPNs with SGD-trained networks, the authors demonstrate that while both produce identical outputs, their internal representations differ dramatically, with evolved networks showing unified factored representations and SGD networks exhibiting FER symptoms.

## Method Summary
The paper compares networks evolved through open-ended search (Picbreeder) with those trained via conventional SGD on the task of generating single images using CPPNs. Both network types produce identical outputs but differ in internal representation structure. The method involves training layerized MLP CPPNs with SGD for 100,000 iterations using MSE loss, then visualizing intermediate neuron activations and performing weight sweeps to assess semantic meaningfulness. The key comparison isolates training method while controlling for architecture, allowing direct observation of how different optimization approaches affect internal representation organization.

## Key Results
- SGD-trained networks exhibit fractured entangled representations (FER) where the same concepts are redundantly encoded across multiple disconnected neural circuits
- Evolved Picbreeder networks show unified factored representations (UFR) where concepts are represented once and reused appropriately
- FER can negatively impact generalization, creativity, and continual learning by hindering the network's ability to build upon and extend its understanding of concepts
- Open-ended search processes may be more conducive to developing UFR than conventional SGD optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conventional SGD optimization tends to produce fractured representations where the same conceptual information is encoded in multiple disconnected neural circuits.
- **Mechanism:** SGD follows a greedy, direct optimization path that independently learns to produce each output feature without incentives to discover and reuse underlying regularities. When the same pattern appears in multiple contexts, SGD may encode separate solutions rather than discovering the unifying principle.
- **Core assumption:** The gradient descent trajectory toward a minimum does not inherently favor representations that capture regularities; it only minimizes loss.
- **Evidence anchors:**
  - [abstract] "SGD-trained networks exhibit a form of disorganization that we term fractured entangled representation (FER)"
  - [section 4] Figure 5b shows the SGD skull CPPN "seems never to encode the two sides of the image as symmetric until the very final (output) layer, failing to take advantage of this regularity"
  - [corpus] Evidence is weak—corpus papers primarily address unrelated Facial Expression Recognition (FER) topics; no direct corpus validation of this hypothesis
- **Break condition:** If SGD were combined with explicit regularity-discovery objectives, or if grokking phenomena consistently unified fractured representations at scale, fracture would be mitigated

### Mechanism 2
- **Claim:** FER includes entanglement where semantically independent functions become coupled, causing unintended side effects when modifying one capability.
- **Mechanism:** Without explicit factorization pressure, gradient updates that reduce loss for one function may incidentally create dependencies with unrelated functions. The paper visualizes this as neurons that "change hair color might also cause the foliage in the background to change."
- **Core assumption:** Loss minimization alone does not enforce modularity; shortcuts that couple functions are often lower-loss than learning separate mechanisms.
- **Evidence anchors:**
  - [abstract] "entangled with unrelated functions... behaviors that should be independent and modular end up influencing each other"
  - [section 5.1] GPT-4o generates correct extra-thumb images for humans but fails for apes, suggesting "fractured and redundant procedures for generating an image of a hand"
  - [corpus] No direct corpus evidence for entanglement hypothesis
- **Break condition:** Architectural innovations like Mixture of Experts or explicit disentanglement regularizers could reduce entanglement (though the paper notes this remains unverified)

### Mechanism 3
- **Claim:** Open-ended search processes (like Picbreeder's human-guided evolution) naturally produce UFR by discovering fundamental regularities early, which become scaffolding for later elaborations.
- **Mechanism:** Human selection for interestingness tends to favor images with regularities (symmetry, compressible patterns). Once discovered, these regularities persist in the lineage and are inherited by descendants, creating a curriculum where simple abstractions precede complex ones.
- **Core assumption:** Human aesthetic preferences correlate with representational regularities; serendipitous exploration discovers regularities that objective-directed search misses.
- **Evidence anchors:**
  - [abstract] "networks evolved through an open-ended search process... tend to be more unified and modular"
  - [section 6.1] "In the ancestry of the Picbreeder Skull CPPN, symmetry emerged long before the skull did... That is why the skull that later emerged was able to avoid FER: it inherited these previously learned regularities"
  - [corpus] "Decoupling Search and Learning" paper (arxiv:2509.10973) suggests benefits of exploring diverse minima, tangentially supporting search diversity
- **Break condition:** If open-endedness is not the causal factor but rather the NEAT topology evolution or heterogeneous activation functions, the mechanism would require revision

## Foundational Learning

- **Concept: Compositional Pattern Producing Networks (CPPNs)**
  - **Why needed here:** CPPNs map (x, y, d) coordinates to (h, s, v) color values, making the full input-output behavior of the network visualizable as a single image—critical for the paper's evidence
  - **Quick check question:** Can you explain why a CPPN's output image represents "all possible outputs for all possible inputs" rather than a single generation instance?

- **Concept: Weight sweeps as representation probes**
  - **Why needed here:** The paper's core evidence comes from visualizing how perturbing individual weights changes outputs; UFR networks show semantically meaningful changes while FER networks show incoherent distortions
  - **Quick check question:** If sweeping a weight causes "mouth opening" in one network but "random distortion" in another with identical outputs, what does this suggest about their internal representations?

- **Concept: Representational optimism**
  - **Why needed here:** The paper challenges the implicit assumption that better benchmark performance implies better representations; understanding this framing is essential for evaluating the paper's contribution
  - **Quick check question:** Why might a network that perfectly reproduces training data still have poor generalization according to the FER hypothesis?

## Architecture Onboarding

- **Component map:** NEAT-evolved CPPNs (arbitrary graph topology, heterogeneous activations) → layerized MLP CPPNs (dense, layerwise, same computation) → SGD-trained MLP CPPNs (same architecture, random init, gradient optimization)

- **Critical path:**
  1. Extract a Picbreeder CPPN (available in paper's code repo)
  2. Layerize it to dense MLP format
  3. Train a randomly-initialized copy with SGD to reproduce the same output image
  4. Visualize intermediate neuron activations for both
  5. Compare weight sweeps for semantic meaningfulness

- **Design tradeoffs:**
  - CPPN domain is intentionally simple (one image = entire input-output space) but may not scale to high-dimensional domains
  - Visual inspection of representations is subjective; the paper provides weight sweeps as quantitative probes but acknowledges this limitation
  - The hypothesis remains untested at LLM scale; evidence is behavioral (counting failures, image generation inconsistencies) rather than mechanistic

- **Failure signatures:**
  - If PCA or other basis transformations revealed UFR in SGD networks, FER diagnosis would be an artifact of visualization choice (addressed in Appendix F—evidence suggests FER persists)
  - If grokking consistently unified representations at scale, FER would be a transient issue only

- **First 3 experiments:**
  1. Replicate the skull CPPN experiment: layerize, train with SGD, visualize intermediate representations, and compare weight sweep semantics
  2. Test whether curriculum ordering affects FER: train SGD on multiple related images (skulls at various angles) and measure whether symmetry representation improves
  3. Probe an LLM for FER signatures: test whether capabilities transfer across contexts (e.g., can the model apply a learned algorithm in a superficially different domain?) following the GPT-3 counting examples

## Open Questions the Paper Calls Out
None

## Limitations
- Evidence is primarily drawn from CPPNs in a narrow image-generation domain, with behavioral observations from LLMs remaining largely anecdotal
- The mechanistic explanation for why SGD produces FER lacks rigorous theoretical grounding
- Claims about FER's negative impact on generalization and creativity remain speculative without direct empirical validation

## Confidence
- CPPN experiments showing different internal representations: Medium
- Claims about LLMs and generalization impact: Low
- Mechanism 1 (fractured representations): Medium
- Mechanism 2 (entanglement): Low
- Mechanism 3 (open-ended search benefits): Medium

## Next Checks
1. **CPPN replication and scaling**: Replicate the skull CPPN experiment (SGD vs. evolved) and extend to a larger set of target images to test whether FER patterns persist across diverse visual concepts.
2. **SGD with regularization**: Train SGD networks with explicit regularization for modularity/disentanglement and measure whether FER symptoms reduce while maintaining performance.
3. **LLM behavioral probe**: Design controlled experiments to test whether LLM capabilities transfer across superficially different contexts, following the hand-generation examples in the paper, to provide quantitative evidence for behavioral FER in large models.