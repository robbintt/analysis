---
ver: rpa2
title: Flowing Datasets with Wasserstein over Wasserstein Gradient Flows
arxiv_id: '2506.07534'
source_url: https://arxiv.org/abs/2506.07534
tags:
- wasserstein
- gradient
- then
- cited
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled framework for optimizing functionals
  over the space of probability measures on probability measures, using the Wasserstein
  over Wasserstein (WoW) gradient flow approach. The authors represent labeled datasets
  as mixture distributions supported on class-conditional distributions, and endow
  this space with the WoW distance, deriving a differential structure to define WoW
  gradient flows that decrease a given objective functional.
---

# Flowing Datasets with Wasserstein over Wasserstein Gradient Flows

## Quick Facts
- arXiv ID: 2506.07534
- Source URL: https://arxiv.org/abs/2506.07534
- Reference count: 40
- Primary result: Achieves 100% accuracy on MNIST-to-FashionMNIST domain adaptation with 200 samples per class using Wasserstein over Wasserstein gradient flows

## Executive Summary
This paper introduces a principled framework for optimizing functionals over the space of probability measures on probability measures, using the Wasserstein over Wasserstein (WoW) gradient flow approach. The authors represent labeled datasets as mixture distributions supported on class-conditional distributions, and endow this space with the WoW distance, deriving a differential structure to define WoW gradient flows that decrease a given objective functional. The core method involves defining a Wasserstein differentiable functional (specifically, a Maximum Mean Discrepancy with Sliced-Wasserstein based kernels) and simulating gradient flows on the WoW space to optimize it. This enables structured transitions of classes toward other classes, with applications to transfer learning and dataset distillation.

## Method Summary
The proposed method represents a labeled dataset as a mixture distribution P = (1/C) Σ δ_{μ_c}, where each μ_c is a probability measure over the feature space representing a class. This structure is embedded in P₂(P₂(ℝᵈ)), the space of probability measures over probability measures, equipped with the Wasserstein over Wasserstein (WoW) distance. The paper derives a differential structure on this space and defines WoW gradient flows that minimize a given objective functional. The functional is constructed as an MMD with a Sliced-Wasserstein based kernel, making it tractable for gradient computation. The gradient flow is simulated using a forward Euler discretization scheme with particle updates driven by the WoW gradient, computed via automatic differentiation. This framework enables the optimization of datasets for tasks such as domain adaptation and dataset distillation by manipulating the geometric structure of the hierarchical probability space.

## Key Results
- On MNIST-to-FashionMNIST domain adaptation, the method achieves 100% accuracy with 200 samples per class
- On dataset distillation, the method outperforms existing approaches by 2-4% accuracy with 1-10 samples per class
- The approach is significantly faster (13-294x speedup) than comparable methods for transfer learning tasks

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Representation of Labeled Datasets
- Claim: Representing a labeled dataset as a mixture of class-conditional distributions enables a hierarchical, geometry-aware optimization over datasets.
- Mechanism: A labeled dataset with C classes is modeled as a probability measure P = (1/C) Σ δ_{μ_c}, where each μ_c is itself a probability measure over the feature space. This casts datasets as points in P(P(ℝᵈ)), the space of probability measures over probability measures.
- Core assumption: The structure of the data is meaningfully captured by representing each class as a distribution, and the relationships between classes can be manipulated via the geometry of this higher-order space.
- Evidence anchors: [abstract] "we propose to represent each class by the associated conditional distribution of features, and to model the dataset as a mixture distribution supported on these classes"; [section 3] Defines the space P₂(P₂(M)) and its structure; [corpus] Corpus papers discuss Wasserstein geometry for probability measures, but do not address the specific hierarchical P(P(ℝᵈ)) space; evidence is weak for the specific WoW construction.
- Break condition: If classes are not meaningfully separable or their internal distributions are trivial, the hierarchical representation may not provide geometric advantage.

### Mechanism 2: WoW Gradient Flow via Forward Euler Discretization
- Claim: A forward Euler scheme provides a tractable simulation of gradient flows on the WoW space, enabling particle-based optimization.
- Mechanism: The paper derives a differential structure on P(P(ℝᵈ)) with the WoW distance. A functional F (e.g., MMD) is minimized by updating discrete distributions (particles) via x^{c}_{i,k+1} = x^{c}_{i,k} - τ ∇_{WW2} F(P_k)(μ^{c}_k)(x^{c}_{i,k}). This leverages autodifferentiation on the discrete functional F to compute the WoW gradient.
- Core assumption: The chosen functional F is Wasserstein differentiable on P(P(ℝᵈ)), and the discretization error of the forward scheme is manageable for the application.
- Evidence anchors: [abstract] "derive a differential structure on this space, and define WoW gradient flows... simulating gradient flows on the WoW space to optimize it"; [section 4.2] Derives the WoW gradient of the MMD with Sliced-Wasserstein kernel and outlines the practical update using autodiff; [corpus] Related papers on gradient flows in probability space support the general discretization concept but not the specific WoW gradient.
- Break condition: If the functional F lacks proper differentiability or the learning rate τ is poorly chosen, the flow may diverge or fail to converge.

### Mechanism 3: Task-Driven Functional with Sliced-Wasserstein Kernels
- Claim: Defining the objective as an MMD with a Sliced-Wasserstein (SW) based kernel provides a tractable, differentiable functional whose gradient flow performs structured dataset manipulation.
- Mechanism: The paper proposes minimizing F(P) = (1/2) MMD²_K(P, Q) where K(μ, ν) = exp(-SW²(μ, ν)/(2h)) (Gaussian SW kernel) or K(μ, ν) = -SW(μ, ν) (Riesz SW kernel). The SW distance reduces the kernel computation to manageable 1D OT problems. The gradient of this functional w.r.t. particle positions drives the flow, inducing both intra-class and inter-class particle interactions.
- Core assumption: The SW distance serves as an effective proxy for full Wasserstein distance in defining a kernel that captures relevant distributional differences for the target task.
- Evidence anchors: [abstract] "leveraging... novel tractable functionals that take the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels"; [section 4.2 & 5] Details kernel choices, gradient computation, and their application in experiments; [corpus] Corpus papers discuss MMD gradient flows and SW distances separately, but not their combination for hierarchical dataset flows.
- Break condition: If the SW approximation with finite projections L is too crude, or if the kernel bandwidth h is misspecified, the flow may produce poor alignments or fail to decrease the MMD effectively.

## Foundational Learning

- **Optimal Transport (OT) & Wasserstein Distance**:
  - Why needed here: The entire framework is built on the Riemannian geometry of the Wasserstein space. The WoW distance is OT with W₂ as a ground cost, and the SW kernel relies on 1D OT.
  - Quick check question: Can you explain why the Wasserstein-2 distance defines a Riemannian structure on P₂(ℝᵈ)?

- **Gradient Flows in Probability Space**:
  - Why needed here: The core method is defining and simulating a gradient flow over the space P(P(ℝᵈ)) to minimize a functional. Understanding continuity equations and discretization (JKO vs. forward Euler) is crucial.
  - Quick check question: What is the difference between an implicit (JKO) and explicit (forward Euler) scheme for simulating a Wasserstein gradient flow?

- **Maximum Mean Discrepancy (MMD) & Kernel Methods**:
  - Why needed here: The objective functional minimized by the flow is an MMD with a specific kernel between probability distributions. This connects the geometric flow to a practical machine learning objective.
  - Quick check question: Given a kernel k, how is the squared MMD between two empirical distributions computed, and how can its gradient be derived?

## Architecture Onboarding

- **Component map**: Data Representation -> Functional Definition -> Gradient Computation -> Flow Integration

- **Critical path**:
  1. Implement the SW distance computation (Monte Carlo projections, 1D sorting).
  2. Implement the MMD loss and its automatic differentiation.
  3. Implement the particle update loop with learning rate and momentum.
  4. Validate on synthetic data (e.g., rings) before scaling to image datasets.

- **Design tradeoffs**:
  - **Kernel choice**: Riesz SW kernel (-SW) avoids bandwidth tuning but is not positive definite; Gaussian SW kernel is positive definite but requires tuning h.
  - **Projection count (L)**: Higher L improves SW approximation but increases compute cost per step. Ablation on L is critical.
  - **Discretization**: Forward Euler is fast but may require small τ and momentum for stability. JKO would be more stable but much slower.

- **Failure signatures**:
  - **Mode collapse**: Particles within a class collapse to a few points.
  - **Misalignment**: Classes from source P flow to wrong classes in target Q (verify with classifier accuracy).
  - **Non-convergence**: Loss plateaus or oscillates without decreasing significantly.
  - **Numerical overflow**: Can occur with exponential kernels if SW distances are large relative to bandwidth.

- **First 3 experiments**:
  1. **Synthetic validation**: Replicate the "three rings" experiment to verify that particles form rings and converge correctly. Ablate over kernels (Gaussian vs. Riesz) and learning rates.
  2. **Domain adaptation sanity check**: Flow MNIST → FashionMNIST with n=50-100 samples/class, using a pretrained MNIST classifier. Monitor classifier accuracy on flowed samples every N steps (aiming for 100% alignment).
  3. **Distillation baseline**: Compare the proposed MMDSW (Riesz kernel, ambient space) against standard Distribution Matching (DM) on MNIST with p=1,10 samples/class, measuring test accuracy of a small ConvNet trained on the synthetic data.

## Open Questions the Paper Calls Out
None

## Limitations
- The hierarchical dataset representation assumes classes can be meaningfully modeled as probability measures, which may break down for imbalanced datasets or disjoint class supports
- The forward Euler discretization lacks rigorous convergence guarantees and relies on heuristic hyperparameter choices
- The SW approximation with finite projections introduces a trade-off between computational efficiency and approximation accuracy without theoretical bounds

## Confidence

**High Confidence** (Evidence Strong):
- The WoW gradient flow framework is mathematically well-defined on P(P(ℝᵈ)) with the WoW distance
- Forward Euler discretization provides a practical simulation method
- The MMD with SW kernel is a tractable functional for gradient computation

**Medium Confidence** (Evidence Moderate):
- The SW kernel effectively captures distributional differences for transfer learning tasks
- The method achieves superior results compared to baseline approaches in reported experiments
- Computational speedup claims are reproducible with the proposed implementation

**Low Confidence** (Evidence Weak):
- Generalization to datasets with more than two classes or complex inter-class relationships
- Robustness to hyperparameters (L, h, τ) across different dataset domains
- Theoretical convergence guarantees for the forward Euler scheme on the WoW space

## Next Checks

1. **Ablation on Projection Count**: Systematically vary L (e.g., L=50, 200, 500) and measure both approximation accuracy of the SW distance and downstream task performance to quantify the trade-off.

2. **Convergence Analysis**: Track the MMD loss and WoW gradient norm during optimization across different learning rates to identify convergence patterns and potential instability regimes.

3. **Robustness Testing**: Apply the method to datasets with varying class balance ratios and verify whether the hierarchical representation maintains effectiveness when class priors deviate significantly from uniform.