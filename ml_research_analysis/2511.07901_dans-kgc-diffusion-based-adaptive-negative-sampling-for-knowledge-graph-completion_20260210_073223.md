---
ver: rpa2
title: 'DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion'
arxiv_id: '2511.07901'
source_url: https://arxiv.org/abs/2511.07901
tags:
- negative
- knowledge
- graph
- sampling
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DANS-KGC is a novel negative sampling framework for knowledge graph
  completion that addresses the limitations of existing methods in terms of false
  negatives, generalization, and lack of control over sample hardness. The method
  introduces a Difficulty Assessment Module (DAM) that quantifies entity-specific
  learning difficulty by integrating semantic and structural features.
---

# DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion

## Quick Facts
- **arXiv ID**: 2511.07901
- **Source URL**: https://arxiv.org/abs/2511.07901
- **Reference count**: 10
- **Primary result**: Achieves state-of-the-art MRR of 0.972 on UMLS and 0.572 on YAGO3-10 datasets

## Executive Summary
DANS-KGC introduces a novel negative sampling framework for knowledge graph completion that addresses limitations in existing methods regarding false negatives, generalization, and sample hardness control. The method combines a Difficulty Assessment Module that quantifies entity-specific learning difficulty with an Adaptive Negative Sampling Module using conditional diffusion models. A Dynamic Training Mechanism progressively adjusts negative sample hardness during training following a curriculum-style progression. Extensive experiments on six benchmark datasets demonstrate significant performance improvements over traditional and diffusion-based baselines.

## Method Summary
DANS-KGC is a three-component system for knowledge graph completion. First, the Difficulty Assessment Module (DAM) computes entity-specific difficulty scores by integrating semantic embeddings (from TransE pre-training) with structural graph features like centrality and clustering coefficients through an MLP. Second, the Adaptive Negative Sampling Module (ANS) employs a conditional diffusion model where the noise schedule is modulated by the difficulty score, generating negative samples at multiple timesteps with varying hardness levels. Third, the Dynamic Training Mechanism (DTM) applies curriculum learning by partitioning negatives into four difficulty bands and adjusting sampling weights progressively during training. The framework uses a combined loss function balancing knowledge graph completion objectives with diffusion loss.

## Key Results
- Achieves state-of-the-art MRR of 0.972 on UMLS dataset
- Achieves state-of-the-art MRR of 0.572 on YAGO3-10 dataset
- Shows consistent improvements across all evaluation metrics (MRR, Hit@1, Hit@10) on six benchmark datasets compared to both traditional and diffusion-based baselines

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Aware Noise Scheduling
DAM computes normalized difficulty scores ζ(e) by combining semantic and structural features, which modulate the maximum noise level βmax(x) = βlow + (βglobal − βlow)(ζ(x))^μ. This ensures harder entities receive stronger corruption during forward diffusion. Ablation shows removing DFS causes MRR to drop from 0.994 to 0.965 on Family dataset.

### Mechanism 2: Condition-Constrained Reverse Denoising
The reverse denoising process conditions on semantic type (K-means cluster centroids) and neighborhood information to generate negatives that are semantically plausible yet distinguishable from positives. Ablation shows Hit@10 drops from 0.995 to 0.983 without conditional constraints.

### Mechanism 3: Curriculum-Based Dynamic Training
DTM partitions negatives into four difficulty bands based on diffusion timestep and applies curriculum-style sampling weights that increase probability of harder examples as training progresses. Removing DTM causes Hit@1 to drop sharply from 0.991 to 0.921, indicating weakened discriminative boundary learning.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPM)**: Understanding forward noise injection and reverse denoising is essential for grasping how DANS-KGC generates negatives. Quick check: Can you explain why sampling at earlier denoising timesteps produces harder negatives?
- **Knowledge Graph Embedding Loss Functions**: The margin-based ranking loss and how negatives interact with it is central to why negative sampling quality matters. Quick check: What happens to embedding quality if all negative samples are too easy to distinguish?
- **Graph Structural Features (Centrality, Clustering)**: DAM relies on computing metrics like PageRank and Betweenness Centrality to quantify entity difficulty. Quick check: Why might high-degree entities be considered "harder" to learn in a KG context?

## Architecture Onboarding

- **Component map**: Knowledge Graph → TransE Pre-training → Semantic Features → DAM (MLP) → Difficulty Scores ζ(e) → Forward Diffusion (entity-specific βt) → Noisy Embedding xT → Reverse Denoising (with type/neighborhood constraints) → Negatives at T/20, T/10, T/5, T/2 → DTM (curriculum weights) → Sample negative band κ → KGC Loss (L_KGC1 + L_KGC2) + Diffusion Loss (L_Diff)
- **Critical path**: Difficulty score computation → Adaptive noise schedule → Conditioned denoising → Curriculum-weighted sampling
- **Design tradeoffs**: More diffusion steps (T) increase negative diversity but raise computation cost; higher η emphasizes DANS negatives over random but may overfit to synthetic negatives; more K-means clusters provide finer type granularity but risk over-fragmentation
- **Failure signatures**: Low variance in difficulty scores across entities; cluster assignments that don't align with semantic types; curriculum weights not shifting across epochs; random negatives outperforming DANS negatives
- **First 3 experiments**: 1) Replicate ablation on Family dataset to validate independent component contributions; 2) Hyperparameter sweep on μ and η using validation set; 3) Visualize generated negatives at each timestep for a fixed triple to verify hardness gradient

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational overhead of calculating global structural metrics (e.g., Betweenness Centrality) in the Difficulty Assessment Module scale to massive knowledge graphs? Experiments are restricted to medium-sized datasets, leaving the feasibility of computing these metrics on industrial-scale graphs unverified.

### Open Question 2
To what extent does the quality of pre-trained embeddings impact the accuracy of the Difficulty Assessment Module? Poor pre-trained embeddings could lead to inaccurate difficulty assessments, degrading final negative sampling quality.

### Open Question 3
Is the specific selection of diffusion timesteps (T/20, T/10, T/5, T/2) optimal for all relational complexities? Different relation types may require different hardness distributions that the current static partitioning might not capture.

## Limitations

- The DAM module's reliance on expensive structural metrics (centrality, clustering coefficients) raises scalability concerns for large-scale graphs
- The framework requires extensive hyperparameter tuning (μ, η, T, cluster count) with no theoretical guidance on optimal settings
- Performance gains come with increased computational overhead from diffusion-based generation compared to simpler negative sampling methods

## Confidence

- **Overall method effectiveness**: Medium-High (consistent state-of-the-art results across 6 datasets)
- **Difficulty assessment mechanism**: Medium (plausible but lacks direct validation of difficulty-feature correlation)
- **Curriculum learning for NS**: Medium (well-established concept but novel application to KGC hardness progression)

## Next Checks

1. Analyze correlation between DAM-computed difficulty scores (ζ(e)) and actual model training patterns (loss convergence rates per entity) to validate the difficulty assessment mechanism
2. Conduct controlled experiment comparing entity-specific vs global noise scheduling while holding all other factors constant, measuring both negative sample quality and downstream KGC performance
3. Perform qualitative analysis of generated negatives at each difficulty level (T/20, T/10, T/5, T/2) for representative triples, verifying claimed hardness gradient and semantic coherence