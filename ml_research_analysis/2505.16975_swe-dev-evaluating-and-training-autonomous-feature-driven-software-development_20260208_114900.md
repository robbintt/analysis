---
ver: rpa2
title: 'SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development'
arxiv_id: '2505.16975'
source_url: https://arxiv.org/abs/2505.16975
tags:
- swe-dev
- code
- training
- test
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SWE-Dev is the first large-scale dataset designed to evaluate and
  train autonomous coding systems on feature-driven development tasks. It comprises
  14,000 training and 500 test instances derived from real-world open-source projects,
  each equipped with runnable environments and executable unit tests for reliable,
  functionality-based evaluation.
---

# SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development

## Quick Facts
- **arXiv ID:** 2505.16975
- **Source URL:** https://arxiv.org/abs/2505.16975
- **Reference count:** 40
- **One-line result:** SWE-Dev is the first large-scale dataset for autonomous feature-driven development, enabling small models to achieve GPT-4o-level performance on hard tasks through fine-tuning.

## Executive Summary
SWE-Dev introduces a novel dataset and evaluation framework for autonomous feature-driven software development (FDD), addressing the gap between existing bug-fixing benchmarks and the challenge of implementing new features in real-world codebases. The dataset comprises 14,000 training and 500 test instances from 1,000+ PyPI repositories, each equipped with runnable Docker environments and executable unit tests that provide reliable binary reward signals. Evaluations across 17 chatbot LLMs, 10 reasoning models, and 10 multi-agent systems reveal profound difficulty, with even leading models like Claude-3.7-Sonnet achieving only 22.45% Pass@3 on hard tasks. The dataset enables advanced training paradigms including SFT, RL, and multi-agent training, demonstrating that fine-tuning a 7B model on SWE-Dev yields performance comparable to GPT-4o on challenging FDD tasks.

## Method Summary
SWE-Dev employs a systematic approach to create feature-driven development tasks by masking target functions in real-world Python repositories and generating Project Requirement Descriptions (PRDs). Each task includes a Dockerized environment with the incomplete codebase and executable unit tests serving as ground truth. The dataset construction uses developer-authored tests as binary reward signals, transforming subjective generation into objective verification. Complexity is controlled through dynamic call-tree analysis, measuring execution depth and node count to create difficulty gradients. Training involves Supervised Fine-Tuning with LoRA (r=16, Î±=16, dropout=0.05), Reinforcement Learning via PPO and DPO using test pass rates as rewards, and multi-agent systems with role-based collaboration. Inference selects "relevant code context" rather than full repositories to fit context windows.

## Key Results
- SWE-Dev achieves only 22.45% Pass@3 on hard tasks even with leading models like Claude-3.7-Sonnet
- Fine-tuning a 7B model on SWE-Dev yields performance comparable to GPT-4o on hard tasks
- Reasoning models underperform chatbots not due to logic errors but due to poor instruction following (high IFR)
- Pass@3 metric shows strong correlation with capability scaling, while CodeBLEU and other metrics show minimal variance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Executable unit tests provide reliable binary reward signals that correlate better with functional correctness than similarity-based metrics
- **Core assumption:** Unit tests fully capture functional requirements and are free of flakiness
- **Evidence anchors:** Abstract states RL enabled by accurate reward signals; Section 5 shows Pass@3 reflects capability scaling while CodeBLEU shows minimal variance
- **Break condition:** If unit tests have low coverage or fail to account for edge cases, models may achieve high pass rates while producing invalid code

### Mechanism 2
- **Claim:** Dynamic call-tree complexity acts as predictable proxy for task difficulty and model failure rates
- **Core assumption:** Structural dependency depth correlates positively with reasoning load required
- **Evidence anchors:** Section 3.1 explains depth and width reflect feature logic complexity; Section 5/Fig 8a shows GPT-4o performance declines with depth and node count
- **Break condition:** May fail if shallow tree contains algorithmically dense logic harder than deep tree with simple logic

### Mechanism 3
- **Claim:** Verifiable training set with runnable environments enables small models to distill capabilities of larger models
- **Core assumption:** Ground truth code is optimal solution path and execution success implies correctness
- **Evidence anchors:** Abstract notes 7B model comparable to GPT-4o on hard split after fine-tuning; Section 4.2.3 describes rejection sampling ensuring retention of beneficial data
- **Break condition:** If Docker environments are unstable or expensive to run at scale, verifiable feedback volume diminishes

## Foundational Learning

- **Concept: Feature-Driven Development (FDD) vs. Bug Fixing**
  - **Why needed here:** Prior benchmarks focus on bug fixing; FDD requires generating new logic and integrating it into existing codebase
  - **Quick check question:** Can you explain why a model might succeed at filling a deleted line but fail at adding a new method and updating `__init__.py` exports?

- **Concept: Pass@k vs. Pass@1**
  - **Why needed here:** Generative models are stochastic; Pass@1 is strict while Pass@k measures potential correctness from best of k samples
  - **Quick check question:** If a model has 20% Pass@1 but 40% Pass@3, what does that imply about consistency of its reasoning process?

- **Concept: Instruction Following Rate (IFR)**
  - **Why needed here:** Reasoning models often underperform chatbots not due to logic errors but failure to adhere to file structure or PRD constraints
  - **Quick check question:** Why might a "smarter" model perform worse than a "dumber" model if task requires strictly modifying only specified files?

## Architecture Onboarding

- **Component map:** PRD + Masked Codebase -> LLM Agent (Single/Multi-Agent) -> Docker Environment (runs `pytest`) -> Rejection Sampler
- **Critical path:** Target Function Masking logic determines difficulty; if too shallow task is trivial, if it breaks imports environment fails
- **Design tradeoffs:** Simple MAS often outperforms complex MAS due to reduced overhead and hallucination risks; Verifiability vs Scale requires reliable Docker environments
- **Failure signatures:** Incomplete Error (implements logic but fails to export/connect), Logic Error (code runs but wrong outputs), Hallucination (invents libraries not present)
- **First 3 experiments:**
  1. Baseline Validation: Run GPT-4o-mini on Easy split to ensure Docker execution pipeline matches environment
  2. Error Analysis: Run Qwen-1.5B and categorize errors (Incomplete vs Logic) to reveal bottleneck type
  3. Overfitting Test: Fine-tune 7B model on small subset (500 samples) and validate against test set for data leakage issues

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can complexity-aware training using call-tree signals systematically improve model performance on harder FDD tasks?
- **Basis in paper:** Appendix C.4 states SWE-Dev offers fine-grained complexity signals for complexity-aware training and suggests curriculum learning
- **Why unresolved:** Paper reports training gains but does not experiment with curriculum learning or complexity-ordered training
- **What evidence would resolve it:** Curriculum learning experiment training low-depth tasks before high-depth tasks, comparing to random training order

### Open Question 2
- **Question:** Can lightweight agent architectures with improved context-sharing mechanisms outperform current MAS on repository-level FDD while reducing communication overhead?
- **Basis in paper:** Page 2 notes current MAS suffer from unnecessary communication overhead and limited coordination efficiency
- **Why unresolved:** Paper evaluates existing MAS frameworks but does not propose or test new architectures optimized for repository-scale coordination
- **What evidence would resolve it:** New MAS with shared context memory measuring Pass@k performance and total API calls/cost against baselines

### Open Question 3
- **Question:** How can multi-agent training be scaled effectively to leverage full 14,000-sample training set with stable learning dynamics?
- **Basis in paper:** Page 2 states future work could investigate multi-agent training and long-context RL with SWE-Dev after noting initial experiments show headroom
- **Why unresolved:** Multi-agent training experiments used limited subset and role-wise SFT; scaling to full dataset with end-to-end RL unexplored
- **What evidence would resolve it:** Training multi-agent system on full training set with RL and reporting convergence stability and final Pass@k

## Limitations

- Quality and completeness of unit tests used as ground truth remains uncertain and difficult to verify at scale across 1,000+ repositories
- Context selection mechanism for "relevant code context" during inference remains underspecified, making precise reproduction challenging
- Evaluation focuses exclusively on Python repositories with pytest-based testing, limiting generalizability to other languages or frameworks

## Confidence

**High Confidence (8-10/10):** SWE-Dev provides challenging benchmark for autonomous FDD is well-supported by 22.45% Pass@3 on hard tasks and comparison with existing metrics

**Medium Confidence (5-7/10):** Effectiveness of executable tests as training signals and correlation between call-tree complexity and difficulty are supported but rely on assumptions about test quality

**Low Confidence (1-4/10):** Specific performance improvements from fine-tuning require precise reproduction of training pipeline including context selection heuristic and Docker environment stability

## Next Checks

1. **Test Quality Validation:** Select 10 random test suites and manually verify they comprehensively cover PRD requirements, checking for edge cases and boundary conditions

2. **Context Selection Replication:** Implement and test multiple context selection strategies (file import graph analysis, keyword matching, random sampling) to determine if unreported heuristic is critical

3. **Cross-Domain Transfer:** Apply SWE-Dev-trained models to small set of JavaScript/TypeScript repositories with Jest/Mocha testing to measure performance degradation and assess language/framework dependency