---
ver: rpa2
title: 'PAL: Probing Audio Encoders via LLMs -- Audio Information Transfer into LLMs'
arxiv_id: '2506.10423'
source_url: https://arxiv.org/abs/2506.10423
tags:
- audio
- tokens
- plits
- text
- integration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of efficiently integrating audio
  encoders with large language models (LLMs) for machine listening applications. We
  propose LAL, a lightweight integration method that injects audio tokens solely through
  the attention mechanism as keys and values, bypassing the feedforward module, thereby
  reducing computational complexity and memory usage while maintaining performance
  comparable to the state-of-the-art PLITS approach.
---

# PAL: Probing Audio Encoders via LLMs -- Audio Information Transfer into LLMs

## Quick Facts
- **arXiv ID:** 2506.10423
- **Source URL:** https://arxiv.org/abs/2506.10423
- **Reference count:** 40
- **One-line primary result:** LAL achieves comparable performance to PLITS with 60% lower memory and 190% higher throughput through attention-only audio injection.

## Executive Summary
This paper addresses the challenge of efficiently integrating audio encoders with large language models (LLMs) for machine listening applications. The authors propose LAL (Lightweight Audio-Language integration), which injects audio tokens solely through the attention mechanism as keys and values, bypassing the feedforward module. This approach achieves substantial computational efficiency gains while maintaining performance comparable to the state-of-the-art PLITS method. Building on LAL, the authors introduce PAL (Progressive Audio-Language integration), a hybrid approach that combines PLITS and LAL to achieve superior performance on unified speech, sound, and music understanding benchmarks.

## Method Summary
The method introduces two integration strategies: LAL injects audio representations solely through the attention mechanism at selected LLM layers, bypassing the feed-forward module to reduce computational complexity and memory usage. PAL combines LAL with PLITS by injecting full-resolution audio tokens via LAL into the initial one-third of layers (where fine-grained acoustic detail is most valuable) while processing a compact representation through all layers via PLITS. The unified training curriculum uses multi-stage training on the OpenAQA corpus (1.2M→5.6M samples), first pretraining the connector with frozen LLM+encoder, then jointly training connector+LLM using AdamW optimizer with cosine learning rate schedule.

## Key Results
- LAL achieves comparable performance to PLITS across multiple tasks while reducing memory usage by ~60% and increasing throughput by ~190%
- PAL outperforms both LAL and PLITS on unified speech, sound, and music understanding benchmarks
- Ablation studies show LAL injection depth is critical, with diminishing returns beyond the initial one-third of layers
- Attention analysis reveals complementary specialization: LAL peaks in early layers for acoustic detail, while PLITS increases in later layers for semantic reasoning

## Why This Works (Mechanism)

### Mechanism 1: Attention-Only Audio Injection (LAL)
- **Claim:** Injecting audio tokens solely as keys and values in attention blocks, without passing them through feed-forward networks (FFNs), is sufficient for text tokens to gather cross-modal information.
- **Mechanism:** Audio tokens from a Q-Former are projected into each LLM layer's input space via a small MLP. Text tokens (only) form queries, while keys and values include both text and audio. Audio modulates the attention context without FFN processing, allowing text representations to incorporate audio-derived features through attention-based modulation alone.
- **Core assumption:** Attention is the necessary and sufficient pathway for audio cues to influence text representations; FFN processing of audio tokens is redundant since text tokens can "piggy-back" audio context into the LLM's parametric knowledge via their own FFN passage.
- **Evidence anchors:**
  - [abstract] "LAL injects audio representations solely through the attention mechanism at selected LLM layers, bypassing the feed-forward module."
  - [Page 3, Section 3.2] "Since attention mediates all inter-token interactions, it is the necessary pathway for audio to influence text, and we posit that it is also sufficient."
  - [Page 17, Table 9] Shows LAL with frozen FFN maintains ~93-98% of trainable-FFN performance, supporting that FFN modification is not essential.
  - [corpus] Weak/absent - no external corpus papers validate the attention-only hypothesis directly.

### Mechanism 2: Computational Complexity Reduction via Asymmetric Attention
- **Claim:** Eliminating audio-to-audio attention interactions and FFN routing for audio tokens yields substantial efficiency gains that scale with audio sequence length.
- **Mechanism:** In PLITS, causal attention over N_a + N_t tokens costs O((N_a + N_t)²). In LAL, only text tokens issue queries; audio serves only as keys/values, reducing cost to O((N_a + N_t)N_t). Since N_a ≫ N_t typically, this removes the dominating N_a² term. Additionally, audio tokens skip FFN sublayers entirely, reducing per-layer FLOPs and activation memory.
- **Core assumption:** Audio-to-audio self-attention is unnecessary for most audio-understanding tasks; the relevant information can be extracted via text-to-audio attention alone.
- **Evidence anchors:**
  - [abstract] "reducing computational complexity and memory usage while maintaining performance comparable to... PLITS"
  - [Page 2, Section 3.2.1] "LAL scales as (N_a + N_t)N_t, which is linear in N_a. Thus, the compute and memory gap widens with longer or more finely segmented audio."
  - [Page 3, Figure 1] Shows ~60% memory reduction and ~190% throughput increase vs. PLITS baseline.

### Mechanism 3: Hybrid Integration with Layer-Specialized Pathways (PAL)
- **Claim:** Combining PLITS (for compact summary tokens through all layers) with LAL (for full-resolution tokens in early layers only) provides complementary information pathways that outperform either method alone.
- **Mechanism:** Full-resolution audio tokens are injected via LAL into the initial one-third of layers, where fine-grained acoustic detail is most valuable. A separate compact representation (via strided convolution with r=3) is processed through all layers via PLITS, providing high-level semantic summaries that align with later text-oriented reasoning. The pathways interleave in K/V space to preserve temporal alignment.
- **Core assumption:** Early transformer layers benefit most from fine-grained acoustic context, while later layers benefit more from abstracted, language-aligned summaries.
- **Evidence anchors:**
  - [Page 5, Section 3.3] "We hypothesize that... LAL is most valuable in the early transformer layers, where preserving fine-grained acoustic detail is critical."
  - [Page 13, Tables 5-6] Ablation shows masking LAL hurts music more; masking PLITS hurts speech more, indicating complementary specialization.
  - [Page 14, Figure 5] Attention mass analysis shows LAL attention peaks in early layers, PLITS attention increases in later layers.

## Foundational Learning

- **Concept: Transformer Attention with Separate Q, K, V Sources**
  - **Why needed here:** LAL fundamentally relies on asymmetric attention where queries come from text only, but keys and values include audio. Understanding that Q, K, V can originate from different sequences (as in cross-attention) while being processed in a unified operation is essential.
  - **Quick check question:** Given text hidden states H_t and audio features A, can you write the attention operation where text queries attend to concatenated [text, audio] keys and values?

- **Concept: Rotary Positional Embeddings (RoPE) with Non-Sequential Token Insertion**
  - **Why needed here:** LAL injects audio tokens into attention without physically inserting them into the text sequence. Position IDs must be managed to preserve relative ordering—a "gap" is reserved for audio positions.
  - **Quick check question:** If system prompt ends at position k and audio has N_a tokens, what position ID should the first user-prompt text token receive in LAL vs. PLITS?

- **Concept: Strided Temporal Convolution for Token Compression**
  - **Why needed here:** PAL uses stride-r convolution to create summary tokens from the full audio sequence. Understanding how this preserves temporal structure while reducing sequence length is critical.
  - **Quick check question:** Given audio tokens of shape [T, d] and stride r=3, what is the output shape after 1D strided convolution? How are the summary tokens aligned with original tokens in K/V interleaving?

## Architecture Onboarding

- **Component map:** Audio → Encoder → Q-Former → Per-Layer Projectors → Attention K/V injection (LAL path) || Audio → Encoder → Strided Conv → MLP → Text sequence prepend (PLITS path in PAL). Loss computed only on text tokens.

- **Critical path:** Audio → Encoder → Q-Former → Per-Layer Projectors → Attention K/V injection (LAL path) || Audio → Encoder → Strided Conv → MLP → Text sequence prepend (PLITS path in PAL). Loss computed only on text tokens.

- **Design tradeoffs:**
  - **LAL depth:** More layers = better performance but diminishing returns and higher memory. Paper shows 1/3 of layers is sufficient for PAL.
  - **Summary stride r:** Larger r = more compression but potential information loss. Paper uses r=3 throughout.
  - **Freezing FFN:** Possible with LAL (Table 9) for parameter efficiency, but slight performance drop (~5-7% relative).
  - **Position ID strategy:** Must match PLITS-style ordering for fair comparison; incorrect gaps break temporal reasoning.

- **Failure signatures:**
  - **Attention all to text, ignoring audio:** Check that audio K/V are actually concatenated and position IDs are assigned correctly.
  - **Memory not reduced vs. PLITS:** Likely audio tokens are still passing through FFN; verify routing logic.
  - **PAL underperforms LAL-only:** Summary tokens may not be aligned with LAL tokens in K/V ordering (verify interleaving per Equation 5).
  - **Position-related artifacts:** Text tokens after audio have wrong relative positions; check gap reservation in position IDs.

- **First 3 experiments:**
  1. **LAL vs. PLITS single-model comparison:** Implement both on same LLM backbone (e.g., Llama-3.2-1B) with identical training curriculum (OpenAQA Stages 1-2). Measure ESC-50, DCASE, AudioCaps performance + memory/throughput. Expected: LAL matches PLITS within 2-5% with ~50% memory reduction.
  2. **Ablate LAL injection depth:** Test LAL injection at layers {first 1/4, first 1/3, first 1/2, all layers}. Plot accuracy vs. memory on MMAU benchmark. Expected: Diminishing returns beyond 1/3; early layers most critical.
  3. **PAL pathway masking:** Train PAL-3B, then at inference mask LAL tokens or PLITS summary tokens separately. Measure per-domain (sound/music/speech) performance drop on MMAU/MMAR. Expected: LAL masking hurts music more; PLITS masking hurts speech more (per Tables 5-6).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does LAL/PAL maintain efficiency advantages and performance gains when scaled to larger LLMs (7B, 13B, 70B+)?
- **Basis in paper:** [explicit] Conclusion states: "For future work, we plan to scale to larger LLMs"
- **Why unresolved:** All experiments use 1B-3B models; the attention complexity reduction scales linearly with Na but interaction with larger model capacities is unknown
- **What evidence would resolve it:** Systematic evaluation of LAL/PAL on Llama-3.1-8B, Qwen2.5-14B, and larger backbones under identical training curricula

### Open Question 2
- **Question:** How does LAL/PAL perform on streaming audio or extremely long audio contexts (hours-long sequences)?
- **Basis in paper:** [explicit] Conclusion states plan to "explore streaming and long context audio"
- **Why unresolved:** Current experiments use fixed-length clips; LAL's O((Na+Nt)Nt) complexity suggests linear scaling with audio length but memory and latency characteristics in streaming scenarios are untested
- **What evidence would resolve it:** Benchmarks on long-form audio (e.g., multi-hour recordings, real-time streaming) measuring latency, memory, and accuracy

### Open Question 3
- **Question:** Is the "first third of layers" heuristic for LAL injection in PAL optimal, or does the ideal depth vary by task, audio type, or model architecture?
- **Basis in paper:** [inferred] Section 3.3 states LAL is restricted to "initial one-third" based on attention analysis, but this threshold was not systematically ablated
- **Why unresolved:** Appendix A shows attention patterns shift across layers, but no grid search over different layer fractions was reported
- **What evidence would resolve it:** Ablation study varying LAL injection depth (1/4, 1/3, 1/2, all layers) across MMAU/MMAR/MMSU benchmarks

### Open Question 4
- **Question:** Can the LAL/PAL architecture generalize to other modalities (vision, video) with comparable efficiency-performance tradeoffs?
- **Basis in paper:** [inferred] The method is motivated by mechanistic interpretability insights about feature activation that should apply across modalities, but only audio was tested
- **Why unresolved:** No experiments with visual encoders; vision tokens may have different length/compression characteristics than audio
- **What evidence would resolve it:** Applying LAL/PAL to vision-language models (e.g., replacing visual token prepending) with efficiency and accuracy measurements

## Limitations

- **LFST Connector Implementation Gaps:** The paper specifies LFST as the default connector but only provides high-level architecture details (3 cross-attention layers, fusion of SSLAM+CLAP). Critical hyperparameters including latent token count, local region sizes, and dimensionality are unspecified, making exact reproduction challenging.

- **Position ID Management Complexity:** LAL requires careful handling of position IDs to maintain temporal relationships when audio tokens are injected as K/V without physical sequence insertion. The paper describes the strategy but doesn't provide implementation details for different LLM architectures or sequence lengths.

- **Unified Training Curriculum Details:** While multi-stage training is described (connector pretraining → joint training), specific data mixing ratios across speech/sound/music domains and curriculum progression are not fully specified, potentially affecting reproducibility of the reported performance gains.

## Confidence

- **High Confidence:** LAL's computational efficiency claims (60% memory reduction, 190% throughput increase) are well-supported by the complexity analysis and Table 9 showing frozen FFN performance preservation.

- **Medium Confidence:** The hybrid PAL performance improvements are demonstrated empirically but the optimal layer split (1/3) and stride choice (r=3) are not theoretically justified or experimentally validated across different model scales.

- **Low Confidence:** The attention-only sufficiency hypothesis lacks external validation - while ablation studies show frozen FFN maintains most performance, no comparison exists against audio-query attention variants or tasks requiring fine-grained acoustic discrimination.

## Next Checks

1. **Cross-Encoder Validation:** Reproduce LAL performance using three different audio encoders (AFWhisper, LFST, and Q-Former alone) on ESC-50 to verify the attention-only mechanism works consistently across architectures, not just with the default LFST setup.

2. **Audio-Query Attention Ablation:** Implement a variant where audio tokens also contribute queries (not just K/V) in LAL to test whether the attention-only sufficiency claim holds - measure performance drop on tasks requiring fine-grained acoustic discrimination like AudioCaps and music understanding benchmarks.

3. **Layer-Split Sensitivity Analysis:** Systematically vary the PAL layer split from 1/4 to 3/4 of total layers while measuring performance on MMAU domains separately to determine if the 1/3 allocation is optimal or task-dependent.