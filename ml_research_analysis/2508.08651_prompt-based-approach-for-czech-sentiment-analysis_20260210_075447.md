---
ver: rpa2
title: Prompt-Based Approach for Czech Sentiment Analysis
arxiv_id: '2508.08651'
source_url: https://arxiv.org/abs/2508.08651
tags:
- sentiment
- czech
- fine-tuning
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first prompt-based approach for sentiment
  analysis in Czech, focusing on aspect-based sentiment analysis (ABSA) and sentiment
  classification (SC). The authors propose using sequence-to-sequence models with
  prompt-based fine-tuning to solve ABSA tasks (ACD, ATE, ACTE, TASD) simultaneously
  and Czech monolingual BERT-like models for APD and SC tasks.
---

# Prompt-Based Approach for Czech Sentiment Analysis

## Quick Facts
- arXiv ID: 2508.08651
- Source URL: https://arxiv.org/abs/2508.08651
- Reference count: 28
- First prompt-based approach for Czech sentiment analysis, showing significant improvements in both ABSA and SC tasks

## Executive Summary
This paper introduces the first prompt-based approach for sentiment analysis in Czech, focusing on aspect-based sentiment analysis (ABSA) and sentiment classification (SC). The authors propose using sequence-to-sequence models with prompt-based fine-tuning to solve ABSA tasks (ACD, ATE, ACTE, TASD) simultaneously and Czech monolingual BERT-like models for APD and SC tasks. Their experiments demonstrate that prompting significantly outperforms traditional fine-tuning for ABSA tasks, with micro F1 scores increasing by up to 19 points. For SC, prompting excels in few-shot scenarios, achieving 60.4% and 65.2% accuracy with 10-20 training examples compared to 50.3% and 51.5% for traditional fine-tuning. The FERNET model achieved state-of-the-art results with 88.2% accuracy.

## Method Summary
The authors employ sequence-to-sequence models (mT5, mBART) with prompt-based fine-tuning for ABSA tasks and Czech monolingual BERT-like models (Czert, RobeCzech, FERNET) with masked prompt completion for APD and SC tasks. For ABSA, they use sentinel tokens in the prompt template to align with the pre-training objective, enabling simultaneous solving of multiple subtasks. For SC, they leverage the masked language modeling head by mapping sentiment labels to vocabulary words and completing prompts like "Je to <mask> film". The approach also incorporates domain-specific continued pre-training on restaurant reviews for ABSA and movie reviews for SC, significantly improving zero-shot performance.

## Key Results
- Prompting improved micro F1 scores by up to 19 points for ABSA tasks compared to traditional fine-tuning
- In few-shot SC scenarios (10-20 examples), prompting achieved 60.4% and 65.2% accuracy versus 50.3% and 51.5% for traditional fine-tuning
- FERNET achieved state-of-the-art 88.2% accuracy on SC task
- Domain pre-training significantly improved zero-shot performance, with FERNET improving from 5.8% to 59.2% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based fine-tuning aligns sequence-to-sequence model predictions with their pre-training objective, improving ABSA task performance.
- Mechanism: The approach replaces discrete labels with structured text templates using sentinel tokens (e.g., `<extra_id_0>`) that match the span prediction objective from mT5 pre-training. The model generates aspect category, sentiment polarity, and aspect term in the format it was pre-trained to produce, rather than learning an arbitrary output mapping.
- Core assumption: Models perform better when fine-tuning tasks closely resemble their pre-training objective (span reconstruction for T5, sequence denoising for BART).
- Evidence anchors:
  - [abstract] "We employ the sequence-to-sequence models to solve the aspect-based tasks simultaneously and demonstrate the superiority of our prompt-based approach over traditional fine-tuning."
  - [section 5.5] "The reason why the sequence-to-sequence models perform better with prompting than with traditional fine-tuning may be that the prompting matches these models' pre-training objectives closely."

### Mechanism 2
- Claim: Prompt-based fine-tuning exploits the masked language modeling head of encoder-only models for few-shot sentiment classification.
- Mechanism: Rather than training a new linear classification layer, the method appends a task-specific prompt with a `[MASK]` token and maps sentiment labels to vocabulary words (e.g., "dobrý" for positive, "špatný" for negative). The pre-trained MLM head directly predicts the sentiment word.
- Core assumption: The pre-trained model has already learned semantic associations between contexts and sentiment words during MLM pre-training, requiring fewer examples to activate this knowledge.
- Evidence anchors:
  - [abstract] "We conduct zero-shot and few-shot learning experiments for sentiment classification and show that prompting yields significantly better results with limited training examples."
  - [section 5.4, Table 5] With 10 examples: prompting achieves 60.4%/65.2% accuracy vs. 50.3%/51.5% for traditional fine-tuning across models.

### Mechanism 3
- Claim: Domain-specific continued pre-training enhances zero-shot prompt-based performance by injecting task-relevant linguistic patterns.
- Mechanism: Additional MLM pre-training on domain corpora (restaurant reviews for ABSA, movie reviews for SC) before prompting exposes the model to domain vocabulary and sentiment expressions, improving its ability to predict masked sentiment tokens without task-specific fine-tuning.
- Core assumption: Domain-specific pre-training transfers to downstream tasks even without labeled examples, because the model learns domain-typical sentiment expressions.
- Evidence anchors:
  - [abstract] "We also demonstrate that pre-training on data from the target domain can lead to significant improvements in a zero-shot scenario."
  - [section 5.4, Table 5] FERNET with domain pre-training achieved 59.2% zero-shot accuracy vs. 5.8% without; Czert improved from 11.8% to 48.2%.

## Foundational Learning

- **Sequence-to-sequence vs. encoder-only architectures**
  - Why needed here: The paper uses mT5/mBART (encoder-decoder) for generation tasks and BERT-like models (encoder-only) for classification. Understanding which architecture suits which task is essential for reproducing results.
  - Quick check question: Can you explain why the authors use mT5 for ATE/ACD tasks but FERNET for APD classification?

- **Prompt template design and label verbalization**
  - Why needed here: The paper constructs specific templates (e.g., "c je [MASK], dáno výrazem: a") and maps labels to vocabulary words. Poor template design would break the method.
  - Quick check question: What would happen if you mapped "neutral" to a rare vocabulary token instead of "ok"?

- **Zero-shot and few-shot evaluation protocols**
  - Why needed here: The paper's key claims depend on proper few-shot sampling (fixed training sets, multiple random seeds) and zero-shot evaluation without any gradient updates.
  - Quick check question: Why does the paper use a fixed subset of training examples rather than random sampling for few-shot experiments?

## Architecture Onboarding

- **Component map**: Input processor -> Backbone selector -> Output decoder -> Domain pre-trainer
- **Critical path**: 
  1. Select appropriate model architecture based on task type
  2. Design prompt template matching model's pre-training format
  3. Define label-to-vocabulary mapping for classification tasks
  4. Optionally continue pre-training on domain data
  5. Fine-tune with prompt-based objective or evaluate zero-shot
- **Design tradeoffs**:
  - mT5 vs. mBART: mT5 uses sentinel tokens matching its span corruption objective; mBART uses `[MASK]` tokens. Paper shows mT5 performs better with prompting.
  - Prompting vs. traditional fine-tuning: Prompting wins in few-shot (<50 examples); traditional fine-tuning catches up with more data.
  - Domain pre-training cost: 2.4M restaurant reviews / 4.2M movie reviews require significant compute but substantially improve zero-shot performance.
- **Failure signatures**:
  - Repeated triplet generation (mT5 with traditional fine-tuning): Model generates duplicate outputs, lacks diversity
  - Random baseline in zero-shot traditional fine-tuning: Linear layer returns uniform predictions (~33% on balanced 3-class)
  - Out-of-vocabulary predictions: Model generates tokens not in label mapping, requiring fallback handling
- **First 3 experiments**:
  1. Replicate the prompt-based mT5 experiment on the Czech ABSA dataset: Compare micro F1 on ACD, ATE, ACTE, TASD against traditional fine-tuning baseline.
  2. Run few-shot SC experiment with FERNET: Test with 10, 20, 50 examples comparing prompting vs. traditional fine-tuning accuracy.
  3. Ablate domain pre-training: Evaluate FERNET zero-shot SC with and without movie review pre-training to isolate the contribution of domain adaptation.

## Open Questions the Paper Calls Out

1. Does expanding the sentiment mapping function (verbalizer) to multiple synonymous words, rather than a single token (e.g., "dobrý"), improve performance in few-shot and zero-shot scenarios?
2. To what extent does prompt template suitability explain the performance discrepancy between the APD task (where prompting excels) and the SC task (where it often underperforms traditional fine-tuning)?
3. Why does additional domain pre-training significantly boost zero-shot performance for FERNET and Czert but degrade it for RobeCzech?

## Limitations
- The Czech ABSA dataset is not publicly available, blocking independent verification of core results
- The paper does not conduct systematic ablation studies on prompt template design, leaving sensitivity to template variations unknown
- Model-specific behavior differences are observed but not fully investigated (why mT5 outperforms mBART, why RobeCzech degrades with pre-training)

## Confidence
- **High confidence**: Prompting significantly improves ABSA performance over traditional fine-tuning; prompting significantly improves few-shot SC performance; domain pre-training substantially improves zero-shot performance
- **Medium confidence**: Prompting vs. traditional fine-tuning tradeoff is data-dependent; specific architecture choices are optimal for Czech prompting
- **Low confidence**: Exact mechanisms by which prompting improves performance are hypothesized but not directly validated; generalization to other languages or domains without adaptation

## Next Checks
1. Obtain or reconstruct the Czech ABSA dataset in SemEval 2016 format and verify the 75:25 train/test split to ensure fair comparison with reported results
2. Systematically vary the prompt templates used for both ABSA and SC tasks to quantify sensitivity and identify robust template designs
3. Apply the same prompting methodology to another language with available aspect-based sentiment analysis data to test generalization beyond Czech restaurant and movie reviews