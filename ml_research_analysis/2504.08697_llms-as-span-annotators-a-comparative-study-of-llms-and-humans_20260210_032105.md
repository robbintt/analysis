---
ver: rpa2
title: 'LLMs as Span Annotators: A Comparative Study of LLMs and Humans'
arxiv_id: '2504.08697'
source_url: https://arxiv.org/abs/2504.08697
tags:
- annotation
- annotations
- span
- human
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) can serve as span annotators, offering
  a scalable alternative to human annotators for tasks requiring fine-grained text
  evaluation. The study compared LLMs with human annotators across three tasks: data-to-text
  generation evaluation, machine translation error detection, and propaganda technique
  identification.'
---

# LLMs as Span Annotators: A Comparative Study of LLMs and Humans

## Quick Facts
- arXiv ID: 2504.08697
- Source URL: https://arxiv.org/abs/2504.08697
- Authors: Zdeněk Kasner; Vilém Zouhar; Patrícia Schmidtová; Ivan Kartáč; Kristýna Onderková; Ondřej Plátek; Dimitra Gkatzia; Saad Mahamood; Ondřej Dušek; Simone Balloccu
- Reference count: 40
- Large language models (LLMs) can serve as span annotators, offering a scalable alternative to human annotators for tasks requiring fine-grained text evaluation

## Executive Summary
This study evaluates large language models as span annotators across three tasks: data-to-text generation evaluation, machine translation error detection, and propaganda technique identification. LLMs achieved moderate agreement with human annotators, comparable to skilled crowdworkers, while producing annotations at a fraction of the cost and time. Reasoning models like DeepSeek-R1 and o3-mini outperformed instruction-tuned models, though both model types struggled with category selection despite correctly identifying problematic spans. The research demonstrates LLMs' potential for efficient, scalable span annotation, though human oversight remains valuable for accuracy.

## Method Summary
The study used structured output generation with detailed annotation guidelines to produce span annotations from LLMs. The system employed constrained JSON decoding with a fixed schema requiring "reason", "text", and "annotation_type" fields. Guidelines were passed directly to the LLM alongside input text and category definitions. The approach was evaluated across three tasks using multiple metrics (F1 soft/hard variants, γ agreement, Pearson ρ, S∅) against human reference annotations. Models included instruction-tuned (Llama 3.3, GPT-4o) and reasoning models (DeepSeek-R1, o3-mini, Gemini 2.0 Flash Thinking).

## Key Results
- LLMs achieved moderate agreement with humans (F1 scores 0.18-0.34), comparable to skilled crowdworkers
- Reasoning models (DeepSeek-R1, o3-mini) outperformed instruction-tuned models by 0.05-0.07 F1 points
- LLMs produced annotations at 2.3% of human cost ($10.5/1k vs $500/1k) and 4% of human time (9-28s vs 227s per output)
- Category confusion was the primary error mode, with models correctly identifying issues but selecting semantically related wrong categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured output generation with detailed guidelines enables LLMs to produce consistent span annotations
- Mechanism: The system uses constrained JSON decoding with a fixed schema requiring "reason", "text", and "annotation_type" fields. Guidelines are passed directly to the LLM alongside the input text and category definitions, providing explicit instructions for handling ambiguous cases and annotation conventions
- Core assumption: LLMs can follow structured output formats and detailed instructions without task-specific training
- Evidence anchors: "with structured outputs and detailed annotation guidelines, LLMs can serve as robust span annotators"; "omitting the guidelines (Pnoguide) lowers the performance of both models"
- Break condition: If guidelines become too complex or categories are too fine-grained (>18 categories), performance degrades significantly (F1 drops to ~0.1)

### Mechanism 2
- Claim: Reasoning models outperform instruction-tuned models on span annotation due to extended inference-time computation
- Mechanism: Reasoning models generate internal thinking traces before producing annotations, allowing them to deliberate on category boundaries and span boundaries. This addresses the inherent ambiguity in span annotation tasks where multiple valid interpretations exist
- Core assumption: The additional computation time enables better reasoning about annotation boundaries and category selection
- Evidence anchors: "Reasoning models like DeepSeek-R1 and o3-mini outperformed instruction-tuned models"; "DeepSeek-R1 generally outperforms the same-sized Llama 3.3"
- Break condition: Reasoning models require significantly longer inference times (227.5s vs 21.6s for local models), making them impractical for time-sensitive applications

### Mechanism 3
- Claim: LLMs and humans exhibit similar error patterns but differ in category calibration
- Mechanism: Manual analysis reveals both LLMs and humans correctly identify problematic spans (~50% accuracy) but struggle with category assignment. LLMs tend to select semantically related but incorrect categories, while also showing attention biases (flagging noise or minor numerical variations as errors)
- Core assumption: Category confusion reflects inherent task ambiguity rather than model incapacity
- Evidence anchors: "while LLMs correctly identified issues, they often misclassified categories or provided incorrect explanations"; "LLM annotations that were marked as correct have only 24% hard character-level overlap with human annotations"
- Break condition: When categories are poorly defined or guidelines lack sufficient examples of edge cases, both human and LLM performance degrades substantially

## Foundational Learning

- Concept: **Span Annotation Protocol**
  - Why needed here: Understanding that span annotation differs from sequence labeling—it requires localizing text spans, assigning categories, and optionally providing reasons, all without fixed annotation units
  - Quick check question: Can you explain why Cohen's κ is unsuitable for span annotation and why partial-credit F1 is preferred?

- Concept: **Inter-Annotator Agreement (IAA) Metrics**
  - Why needed here: The paper uses multiple metrics (F1, γ, Pearson ρ, S∅) to capture different aspects of annotation quality—each has limitations that must be understood for proper interpretation
  - Quick check question: What does the F1∆ = F1(soft) - F1(hard) score reveal about model behavior in multi-category tasks?

- Concept: **Reasoning vs. Instruction-Tuned Models**
  - Why needed here: The performance gap between model families (o3-mini vs GPT-4o, DeepSeek-R1 vs Llama 3.33) stems from architectural differences in inference-time computation that must be considered for deployment
  - Quick check question: Why does few-shot prompting degrade DeepSeek-R1 performance while improving Llama 3.3?

## Architecture Onboarding

- Component map: Input Layer (Text Y + Categories C + Guidelines G) → LLM Layer (Model selection + Constrained decoding) → Parsing Layer (JSON extraction) → Evaluation Layer (Multi-metric comparison)

- Critical path: 1. Guideline design (most impactful on performance—removing guidelines drops F1 by ~50%) → 2. Model selection (reasoning models add +0.05-0.07 F1 over instruction-tuned) → 3. JSON schema enforcement (ensures parseability, enables automated evaluation) → 4. Category granularity calibration (18 categories → F1~0.1; 2-6 categories → F1~0.2-0.4)

- Design tradeoffs: Cost vs. Quality: Claude 3.7 Sonnet ($10.5/1k outputs) vs. crowdworkers ($500/1k outputs) with similar accuracy; Speed vs. Accuracy: DeepSeek-R1 (227.5s/output) vs. Claude 3.7 (9.0s/output) with comparable γ scores; Guideline complexity vs. Model compliance: Detailed guidelines improve accuracy but require more tokens

- Failure signatures: Over-annotation (models flagging noise/markup as errors); Category collapse (models using reduced category vocabulary vs. humans); Reason-annotation mismatch (explanations that contradict span selections); Empty annotation mishandling (γ score undefined when no annotations exist)

- First 3 experiments: 1. Baseline calibration: Run Pbase prompt on Ddev subset with instruction-tuned vs. reasoning model, compute all metrics to establish model-specific baselines; 2. Ablation study: Remove guidelines (Pnoguide) on 50 examples, measure F1 drop to quantify guideline contribution; 3. Error taxonomy analysis: Sample 20 annotations per category, manually classify error types (wrong category vs. incorrect span vs. false positive) to identify systematic biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid annotation approach (LLM pre-annotation with human post-editing) outperform either LLMs or humans alone in span annotation tasks?
- Basis in paper: The authors state "A promising solution seems to be a hybrid approach in which LLMs pre-annotate the text and humans post-edit the annotations" and cite Zouhar et al. (2025), but do not evaluate this approach themselves
- Why unresolved: The paper only compares LLM annotations and human annotations independently, without testing the combined workflow that could leverage LLM efficiency and human accuracy
- What evidence would resolve it: A controlled study measuring annotation quality, time, and cost for the hybrid pipeline versus LLM-only and human-only baselines across multiple tasks

### Open Question 2
- Question: How does span annotation performance scale with the number and granularity of annotation categories?
- Basis in paper: The PROPAGANDA task with 18 categories showed the largest gap between LLM and human performance (γ = 0.16 vs 0.31), while tasks with fewer categories (D2T-EVAL with 6, MT-EVAL with 2) showed smaller gaps
- Why unresolved: The paper does not systematically vary category count to isolate this factor from other task differences such as text type and annotation complexity
- What evidence would resolve it: A controlled experiment holding task type constant while varying the number of categories, or analyzing performance degradation as category count increases within a single task

### Open Question 3
- Question: Can improved annotation guidelines or targeted examples reduce the category confusion patterns observed in LLM span annotations?
- Basis in paper: The authors note "models tend mainly to confuse related categories" and suggest "more descriptive guidelines or additional examples" could help, but also found that few-shot prompting degraded performance for DeepSeek-R1
- Why unresolved: The paper tests guideline ablations but does not explore guideline refinements specifically designed to address the observed confusion patterns between semantically similar categories
- What evidence would resolve it: Iterative guideline development targeting high-confusion category pairs, evaluated through confusion matrix analysis on held-out data

## Limitations

- LLMs often select incorrect categories despite correctly identifying problematic spans, with category confusion rates exceeding 40% for complex tasks
- Performance degrades substantially when category granularity increases (F1 drops from ~0.4 to ~0.1 as categories grow from 2 to 18)
- The reasoning model advantage remains poorly understood - while DeepSeek-R1 and o3-mini outperform instruction-tuned models, the exact mechanisms behind this superiority are not characterized beyond inference-time computation duration

## Confidence

**High Confidence:** LLMs can produce scalable span annotations at significantly lower cost than humans (Claude 3.7 Sonnet at $10.5/1k outputs vs. crowdworkers at $500/1k outputs). The comparative performance across different model families (reasoning vs instruction-tuned) is reproducible, with consistent F1 score differences of 0.05-0.07. The structured output approach with JSON constraints reliably produces parseable annotations.

**Medium Confidence:** LLMs achieve moderate agreement with human annotations, comparable to skilled crowdworkers. This claim depends on the specific metrics used and the quality of human reference annotations, which show substantial variability (F1 > 0.5 threshold for human annotator qualification). The cost-efficiency advantage holds across all three evaluation tasks.

**Low Confidence:** Reasoning models' superior performance stems from extended inference-time computation enabling better category boundary reasoning. While observed empirically, the paper lacks ablation studies isolating the thinking traces' contribution versus other architectural differences. The claim that LLMs' error patterns mirror human annotators' requires more systematic error taxonomy analysis.

## Next Checks

1. **Category Granularity Sensitivity Test:** Systematically vary the number of annotation categories (2, 6, 12, 18) on a single task to quantify the relationship between category complexity and LLM performance, confirming whether F1 scores follow the observed inverse correlation

2. **Reasoning Model Ablation:** Compare reasoning models with and without thinking traces enabled on the same examples, measuring the exact contribution of deliberation time to category selection accuracy versus span boundary identification

3. **Guideline Optimization Experiment:** Conduct a controlled study varying guideline complexity (minimal vs. detailed) across different task types to identify the optimal balance between instruction comprehensiveness and model compliance, measuring the marginal benefit of each additional guideline component