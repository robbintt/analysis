---
ver: rpa2
title: 'LongTail-Swap: benchmarking language models'' abilities on rare words'
arxiv_id: '2510.04268'
source_url: https://arxiv.org/abs/2510.04268
tags:
- words
- sentences
- sentence
- frequency
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LongTail-Swap (LT-Swap), a benchmark designed\
  \ to measure language models' ability to learn and use rare words\u2014a capability\
  \ children excel at but that current language models struggle with. Unlike standard\
  \ benchmarks that focus on frequent words, LT-Swap evaluates semantic and syntactic\
  \ understanding of long-tail words by having models discriminate between correct\
  \ and incorrect sentence pairs where rare words are swapped."
---

# LongTail-Swap: benchmarking language models' abilities on rare words

## Quick Facts
- arXiv ID: 2510.04268
- Source URL: https://arxiv.org/abs/2510.04268
- Authors: Robin Algayres; Charles-Éric Saint-James; Mahi Luthra; Jiayi Shen; Dongyan Lin; Youssef Benchekroun; Rashel Moritz; Juan Pino; Emmanuel Dupoux
- Reference count: 40
- Primary result: Language models perform significantly worse on rare words compared to frequent ones, with architecture differences more pronounced in the long tail

## Executive Summary
This paper introduces LongTail-Swap (LT-Swap), a benchmark designed to measure language models' ability to learn and use rare words—a capability children excel at but that current language models struggle with. Unlike standard benchmarks that focus on frequent words, LT-Swap evaluates semantic and syntactic understanding of long-tail words by having models discriminate between correct and incorrect sentence pairs where rare words are swapped. The authors generate two LT-Swap datasets from the BabyLM corpora (10M and 100M words) and evaluate 16 different language models. Key findings include: (1) language models perform significantly worse on rare words compared to frequent ones, (2) performance differences across architectures are much larger in the long tail than the head, (3) training on larger datasets improves handling of rare words, and (4) a simple RAG-like method boosts semantic performance for rare words. The work highlights the importance of evaluating language models on rare words and offers new insights into model architectures that better handle long-tail generalization. Code is publicly available for generating LT-Swap benchmarks from any English corpus.

## Method Summary
LT-Swap evaluates language models on rare words through sentence pair discrimination tasks. The method generates sentence pairs containing target rare words, then creates incorrect versions by swapping these words between pairs. Three subtasks assess different aspects: WordSwap (semantic discrimination), InflectionSwap (POS tagging), and AgreementSwap (syntactic agreement). Models are evaluated zero-shot by comparing log-probabilities of correct versus incorrect sentences. The benchmark uses quadruplets (S1, S2, S1*, S2*) to control for sentence length and unigram biases. Frequency binning ranges from [0] to [512+] with 10 bins total, ensuring sufficient samples per bin (standard error < 0.02). The approach generates data using Llama3.1-405B and filters pairs using LLM validation prompts.

## Key Results
- Language models perform significantly worse on rare words compared to frequent ones, with accuracy dropping substantially in lowest frequency bins
- Performance differences across architectures are much larger in the long tail than the head, with character-level models showing particular advantage on syntactic tasks
- Training on larger corpora (100M vs 10M words) improves handling of rare words, even when absolute frequency counts are identical
- A simple RAG-like method (adding context sentences as prefix) boosts semantic performance for rare words by 0.049 to 0.13 accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models trained on larger corpora encode rare words better than models trained on smaller corpora, even when the absolute frequency count of those words is identical.
- **Mechanism:** Larger training corpora provide more diverse contextual environments for each word occurrence. A word appearing 5 times in 10M words likely appears in more varied contexts than a word appearing 5 times in 100M words, because the larger corpus exposes the model to richer co-occurrence patterns and semantic relationships.
- **Core assumption:** Word meaning is derived from contextual usage patterns, not just raw frequency counts.
- **Evidence anchors:**
  - [Section 4.3]: "words with frequency count c in BabyLM10M are not encoded as well by LMs as words with the same frequency count c in BabyLM100M"
  - [Figure 4]: Shows LT-Swap scores averaged across LMs, with 100M models outperforming 10M models in lowest frequency bins
  - [corpus]: Related work on data-efficient LMs (BabyLM challenge papers) supports the general finding that training data diversity affects generalization, though this specific frequency-matched comparison is novel to this paper
- **Break condition:** If model capacity is insufficient relative to corpus size, the benefit may saturate or reverse (word-level tokenization shows hapax performance collapse on 100M but not 10M in Appendix F.5)

### Mechanism 2
- **Claim:** Character-level tokenization improves syntactic generalization on rare words while impairing semantic generalization, compared to subword tokenization.
- **Mechanism:** Character-level models explicitly isolate morphological markers (e.g., "-ed", "-s", "-ing") from base words, enabling syntactic pattern recognition even when the base word is unseen. However, longer sequences impair learning of word-level co-occurrence patterns needed for semantics.
- **Core assumption:** Syntactic knowledge is partially compositional (morpheme-level) while semantic knowledge requires word-level embeddings.
- **Evidence anchors:**
  - [Section F.4]: "character-level LMs exhibit only a mild performance drop between high- and low-frequency words on syntactic tasks, while showing a substantial drop on the semantic WordSwap task"
  - [Table 13]: Character-level achieves 0.857 LT-Swap score on 100M vs. BPE-50k at 0.835, driven by syntactic tasks
  - [corpus]: No direct corpus evidence on character vs. subword tokenization for rare words; this is an original contribution
- **Break condition:** If sequences become too long relative to context window, character-level models may fail on tasks requiring long-range dependencies

### Mechanism 3
- **Claim:** Adding a single context sentence containing a rare word from the pretraining corpus as a prefix improves semantic discrimination accuracy without fine-tuning.
- **Mechanism:** The prefix provides in-context evidence about word meaning that the model can use during inference. This leverages the model's existing in-context learning capabilities to augment sparse knowledge from the long tail.
- **Core assumption:** The model has already encoded some information about the rare word during pretraining that can be reactivated with appropriate context.
- **Evidence anchors:**
  - [Section 4.4]: Shows +0.049 to +0.13 accuracy improvement on WordSwap for three lowest frequency bins
  - [Section 4.4]: "This simple experiment suggests that even LMs trained on small datasets have in-context learning abilities"
  - [corpus]: RAG methods are well-established, but this paper applies them specifically to rare word evaluation
- **Break condition:** The method degrades performance on higher frequency bins and on all syntactic tasks (InflectionSwap, AgreementSwap), suggesting it provides noisy signals for well-known words

## Foundational Learning

- **Concept:** Zipf's Law and Long-Tail Word Distributions
  - **Why needed here:** LT-Swap is explicitly designed to evaluate the long tail of word frequency distributions. Without understanding that word frequencies follow a power law (few words are frequent, many words are rare), the benchmark's motivation is unclear.
  - **Quick check question:** If a corpus has 1 million tokens and follows Zipf's law, approximately how many word types appear only once (hapax legomena)?

- **Concept:** Minimal Pair Discrimination
  - **Why needed here:** LT-Swap evaluates models by asking them to distinguish between acceptable and unacceptable sentence pairs. This paradigm (borrowed from BLiMP) requires understanding that linguistic knowledge can be probed via acceptability judgments.
  - **Quick check question:** Given sentences "The cat is sleeping" and "The cat are sleeping," which should receive higher probability from a grammatical English model?

- **Concept:** Zero-Shot Evaluation via Log-Probabilities
  - **Why needed here:** Models are evaluated without task-specific training by comparing log-probabilities of correct vs. incorrect sentences. This requires understanding how to extract and compare likelihoods from different architectures (causal vs. masked LMs).
  - **Quick check question:** For a causal LM, how do you compute the log-probability of sentence "The cat sat"?

## Architecture Onboarding

- **Component map:**
  Pretraining Corpus → Word Frequency Analysis → Frequency Binning
         ↓
  LLM Generator → Sentence Pair Generation (WordSwap/InflectionSwap/AgreementSwap)
         ↓
  LLM Filter → Quadruplet Validation → Benchmark Construction
         ↓
  Candidate LM → Log-Probability Scoring → Accuracy by Frequency Bin

- **Critical path:**
  1. Ensure candidate LM was pretrained on the same corpus used to generate LT-Swap (otherwise frequency counts are meaningless)
  2. Select appropriate scoring method: sum of next-token log-softmax for GPT-like models; pseudo-likelihood (mask one token at a time) for BERT-like models
  3. Average accuracies across all three subtasks and all frequency bins to get final LT-Swap score

- **Design tradeoffs:**
  - **Quadruplets vs. pairs:** Using quadruplets (S1, S2, S1*, S2*) controls for sentence length and unigram biases, but requires more generations and stricter filtering
  - **Frequency bin granularity:** 10 bins from [0] to [512+] provides sufficient resolution to observe frequency effects while maintaining enough samples per bin (standard error < 0.02)
  - **Tokenization choice:** Character-level improves syntactic scores on rare words but degrades semantic scores; BPE offers a middle ground

- **Failure signatures:**
  - Random-chance accuracy (~50%) on lowest frequency bins indicates the model cannot extract any useful information from rare word exposures
  - Large gap between short-distance and long-distance agreement scores (Figure 5) suggests difficulty with hierarchical syntax, not just local patterns
  - WordSwap performance drops to chance for hapax words with word-level tokenization on 100M corpus (Appendix F.5) indicates capacity saturation

- **First 3 experiments:**
  1. **Baseline evaluation:** Run an existing BabyLM model on LT-Swap10M and LT-Swap100M, computing scores broken down by subtask and frequency bin to establish the frequency-effect pattern
  2. **Tokenization ablation:** Train identical OPT-125M architectures on BabyLM with character, BPE (16k), and word-level tokenizations; compare LT-Swap subtask scores to confirm the syntax-semantics tradeoff
  3. **Prefix augmentation test:** Implement the RAG-like prefix method by retrieving one pretraining sentence containing each target rare word and prepending it to evaluation sentences; measure WordSwap accuracy change on the three lowest frequency bins

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific mechanisms allow language models to achieve above-chance accuracy on words seen only once (hapax legomena)?
- **Basis in paper:** [explicit] The authors note in Section 4.2 that accuracy for hapax legomena remains significantly above random chance and state, "Further investigation is needed to better understand what aspects of word learning a LM can acquire from a single exposure."
- **Why unresolved:** It is unclear if the model relies on the semantic meaning of sub-word BPE units to infer the whole word's meaning, or if a single gradient update is sufficient to encode distinct word concepts.
- **What evidence would resolve it:** An ablation study analyzing the correlation between hapax performance and BPE token length, or an analysis of model weights before and after the single exposure to the target word.

### Open Question 2
- **Question:** How do model size and dataset size interact to determine performance on long-tail words?
- **Basis in paper:** [explicit] Section 4.3 states that while increasing dataset size helps long-tail performance, "the combined effect of both larger dataset and model sizes remains unclear, and further research is needed to disentangle the individual contributions."
- **Why unresolved:** The current study fixed model size (approx. 125M parameters) while varying dataset size (10M vs. 100M words), leaving the scaling dynamics of model capacity on rare words unexplored.
- **What evidence would resolve it:** Evaluating the LT-Swap benchmark on a scaling law matrix of models with varying parameter counts trained on the 10M and 100M word datasets.

### Open Question 3
- **Question:** Does the performance drop on long-tail words persist across text corpora with different distributional characteristics than the BabyLM dataset?
- **Basis in paper:** [explicit] The limitations section notes that BabyLM contains transcribed speech, which has a "shorter long tail" than written text, and states, "We do not know how the frequency effect would change when using other datasets with significantly different word distributions."
- **Why unresolved:** The specific Zipfian distribution of the BabyLM corpus may influence the steepness of the performance drop; written text with a fatter tail might result in different architectural comparisons.
- **What evidence would resolve it:** Generating LT-Swap benchmarks from strictly written corpora (e.g., Wikipedia) and comparing the frequency effect slopes against the BabyLM results.

## Limitations

- Benchmark relies entirely on automatically generated data from a single large language model (Llama3.1-405B), introducing potential generator biases
- "Rare words" are defined purely by frequency counts in BabyLM corpora, which may not capture true semantic rarity or domain specificity
- Focus on English nouns and verbs limits generalizability to other parts of speech and languages

## Confidence

**High Confidence Claims:**
- Language models perform significantly worse on rare words compared to frequent words
- Larger training corpora improve handling of rare words
- Character-level tokenization improves syntactic but not semantic rare word performance

**Medium Confidence Claims:**
- The quadruplet filtering approach successfully removes trivial pairs
- The RAG-like prefix method provides meaningful improvements for rare words
- Performance gaps between architectures are larger in the long tail than the head

**Low Confidence Claims:**
- WordSwap task is "primarily semantic" while InflectionSwap is "primarily syntactic"
- The specific frequency binning strategy optimally captures the long-tail distribution

## Next Checks

1. **Human evaluation validation**: Conduct human evaluations on a random sample of 500 LT-Swap sentence pairs to verify that the LLM-generated data accurately reflects true semantic and syntactic distinctions for rare words.

2. **Cross-corpus generalization**: Test LT-Swap models trained on BabyLM against a held-out test set from a different corpus (e.g., Wikipedia or news articles) to assess whether frequency-based difficulty generalizes beyond the training distribution.

3. **Generator model ablation**: Create LT-Swap versions using different generator models (e.g., GPT-4, Claude, open-source alternatives) to determine whether the benchmark results are sensitive to the choice of generation model or reflect genuine linguistic phenomena.