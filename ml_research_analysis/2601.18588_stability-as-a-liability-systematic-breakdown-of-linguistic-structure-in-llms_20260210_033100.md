---
ver: rpa2
title: Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs
arxiv_id: '2601.18588'
source_url: https://arxiv.org/abs/2601.18588
tags:
- training
- stability
- pemp
- mode
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work reveals that training stability, typically viewed as
  essential for reliable optimization, can paradoxically induce systematic degradation
  of generative quality in large language models. Under maximum likelihood estimation
  with stabilized training dynamics, models converge to solutions that minimize forward
  KL divergence to the empirical distribution, implicitly reducing generative entropy
  and concentrating probability mass on a limited subset of observed data modes.
---

# Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs

## Quick Facts
- arXiv ID: 2601.18588
- Source URL: https://arxiv.org/abs/2601.18588
- Reference count: 40
- One-line primary result: Stable training under MLE can induce systematic mode collapse, concentrating probability mass on limited data modes and producing repetitive, low-entropy outputs.

## Executive Summary
This work demonstrates that training stability, typically seen as essential for reliable optimization, can paradoxically degrade generative quality in large language models. Under maximum likelihood estimation with stabilized training dynamics, models converge to solutions that minimize forward KL divergence to the empirical distribution, implicitly reducing generative entropy and concentrating probability mass on a limited subset of observed data modes. This stability-induced mode collapse manifests as repetitive, low-entropy outputs despite smooth loss convergence. Empirical validation using a feedback-based training framework across architectures and random seeds demonstrates a deterministic relationship between stabilization intensity and mode concentration, with extreme stabilization leading to complete collapse (100% high-frequency word proportion).

## Method Summary
The paper introduces a feedback-based training framework (BARRA) implementing the PSU algorithm to study stability-diversity trade-offs. The framework uses DMU (Dynamic MLP Unit) for activation dropout and DTU (Dynamic Transformer Unit) for attention sparsification. Training dynamics follow x_{t+1} = x_t - αf(x_t) + η_t, where stabilization intensity α controls the degree of constraint. The regularized MLE objective includes weight decay λ. Experiments train GPT-2 (1B, 2B, 3B) and BERT (1B) across multiple seeds, measuring high-frequency word proportion as the primary metric for mode collapse severity.

## Key Results
- Stable training under MLE converges to solutions that minimize forward KL divergence, implicitly reducing generative entropy
- Mode collapse manifests as increased high-frequency word proportion, with extreme stabilization reaching 100% concentration
- The relationship between stabilization intensity and mode concentration is deterministic across different random seeds
- Training loss converges smoothly even as generative quality degrades systematically

## Why This Works (Mechanism)
Maximum likelihood estimation under forward KL divergence minimization favors solutions that concentrate probability mass on observed data modes to maximize likelihood. When training is stabilized through constraints like activation dropout and attention sparsification, the model's expressive capacity is restricted, forcing it to allocate probability mass to the most reliable modes in the training distribution. This creates a mismatch between training stability (smooth loss curves) and generative diversity (low entropy outputs), as the model sacrifices exploration of less frequent but potentially meaningful linguistic structures to maintain stable optimization dynamics.

## Foundational Learning
- **Forward KL divergence minimization**: Why needed - explains why MLE favors mode concentration; Quick check - verify that minimizing D_KL(P||Q) pushes Q to cover modes of P
- **Mode collapse**: Why needed - central phenomenon being studied; Quick check - observe output diversity degradation as training progresses
- **Generative entropy**: Why needed - metric for measuring output diversity; Quick check - calculate entropy of model's output distribution
- **Closed-loop feedback training**: Why needed - framework for controlling stabilization intensity; Quick check - verify feedback signal properly scales learning rate and parameter updates
- **Dynamic dropout mechanisms**: Why needed - methods for constraining model expressivity; Quick check - ensure dropout masks are applied consistently during training
- **Attention sparsification**: Why needed - method for reducing model capacity; Quick check - verify attention weights are properly sparsified according to target ratio

## Architecture Onboarding

**Component Map**
PSU Algorithm -> BARRA Framework -> DMU (Dynamic MLP Unit) -> DTU (Dynamic Transformer Unit) -> GPT-2/BERT Models

**Critical Path**
1. Initialize model and training parameters
2. Extract state from gradient statistics and loss dynamics
3. Train discriminator D_ϕ to output adaptive actions (γ_b, S_b)
4. Apply DMU dropout to MLP layers
5. Apply DTU attention sparsification
6. Update parameters with scaled learning rate
7. Measure high-frequency word proportion in generated samples

**Design Tradeoffs**
- Stabilization intensity α vs. generative diversity: Higher α provides smoother training but reduces output variety
- Dropout probability e_c vs. model capacity: Higher dropout constrains expressivity but may impair learning
- Attention sparsity ratio vs. context utilization: Higher sparsity reduces computational cost but may lose important dependencies

**Failure Signatures**
- Mode collapse not observed even at high α: DMU/DTU not properly constraining forward pass
- Training becomes unstable with smooth loss curves: α value too aggressive for architecture
- No correlation between stabilization and diversity: Feedback mechanism not properly implemented

**First Experiments**
1. Implement fixed activation dropout (DMU-style) on a small transformer to test basic stabilization effects
2. Add attention sparsification (DTU-style) to verify combined impact on model expressivity
3. Train with varying dropout/sparsification intensities to observe mode collapse progression

## Open Questions the Paper Calls Out
### Open Question 1
Can explicit entropy regularization or controlled stochasticity mechanisms prevent stability-induced mode collapse without compromising training convergence? The paper concludes this work "lays the foundation for future explorations into balancing stability, expressivity, and controlled stochasticity in generative modeling" but does not propose specific mitigation strategies.

### Open Question 2
Does the stability-diversity trade-off persist under non-MLE objectives, such as Reinforcement Learning from Human Feedback (RLHF)? The theoretical analysis relies specifically on MLE and forward KL divergence properties, leaving open whether alternative objectives resist this degeneration.

### Open Question 3
To what degree does this phenomenon manifest in standard large-scale training pipelines that lack explicit internal stabilization mechanisms? The authors use a controlled feedback-based framework to isolate effects, but it's unclear if standard optimization naturally reaches the "over-stabilization" required to trigger systematic breakdown.

## Limitations
- The experimental claims depend heavily on a specific closed-loop feedback framework (BARRA) that may not be directly applicable to standard training pipelines
- The definition of "high-frequency word proportion" metric is unspecified, making quantitative claims difficult to verify
- Results are based on limited model sizes (1B, 2B, 3B GPT-2, 1B BERT) and may not extend to larger or different model families
- Computational cost of training multiple variants with full PSU algorithm across seeds may be prohibitive for independent verification

## Confidence
- **High confidence**: The theoretical framing that MLE under KL minimization can induce mode collapse by concentrating probability mass on observed modes
- **Medium confidence**: The empirical observation that stable training can coexist with degraded generative quality (based on conceptual alignment with known mode collapse phenomena)
- **Low confidence**: The specific quantitative relationship between stabilization intensity and mode collapse severity, and the claim of deterministic collapse progression across seeds

## Next Checks
1. Implement a simplified version of the PSU algorithm without the full BARRA framework, using fixed activation dropout and attention sparsification to test whether basic stabilization mechanisms alone can induce mode collapse
2. Reproduce the loss-entropy trajectory on a small-scale dataset with a minimal transformer model, comparing standard training to training with various dropout/sparsification intensities to verify the qualitative pattern
3. Measure high-frequency word proportion using multiple definitions (top-100 tokens, tokens above frequency threshold, etc.) to establish robustness of the mode collapse metric across different operationalizations