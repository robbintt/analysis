---
ver: rpa2
title: 'From Behavioral Performance to Internal Competence: Interpreting Vision-Language
  Models with VLM-Lens'
arxiv_id: '2510.02292'
source_url: https://arxiv.org/abs/2510.02292
tags:
- vlms
- arxiv
- color
- layer
- toolkit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLM-LENS is a toolkit enabling systematic benchmarking, analysis,
  and interpretation of vision-language models by extracting intermediate outputs
  from any layer during the forward pass. It provides a unified, YAML-configurable
  interface that abstracts away model-specific complexities and supports user-friendly
  operation across diverse VLMs.
---

# From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens

## Quick Facts
- **arXiv ID**: 2510.02292
- **Source URL**: https://arxiv.org/abs/2510.02292
- **Reference count**: 12
- **Primary result**: VLM-LENS is a toolkit enabling systematic benchmarking, analysis, and interpretation of vision-language models by extracting intermediate outputs from any layer during the forward pass.

## Executive Summary
VLM-LENS is a unified toolkit for analyzing vision-language models (VLMs) that provides a YAML-configurable interface to extract intermediate representations from any layer during the forward pass. The toolkit abstracts away model-specific complexities and currently supports 16 state-of-the-art base VLMs and their variants. Through systematic probing and analysis, the authors demonstrate that VLMs exhibit varying levels of internal competence across different concepts and layers, revealing gaps between behavioral performance and actual representation capabilities.

## Method Summary
The toolkit uses PyTorch forward hooks to intercept intermediate tensors at specified layers during VLM inference, storing them in a SQLite database with standardized schema. Users configure model selection, target layers, and prompts via YAML files. For analysis, the toolkit supports probing with linear classifiers to assess concept encoding and geometric similarity methods to evaluate concept grounding. The probing approach compares main probes against control probes trained on shuffled labels to distinguish genuine representation from memorization. Two experiments demonstrate the toolkit's capabilities: probing primitive concepts (color, material, shape, size, number) across layers and analyzing Stroop-like tasks to reveal internal cue processing.

## Key Results
- The toolkit successfully extracts intermediate representations from 16+ diverse VLM architectures using a unified interface
- Probing reveals systematic differences in hidden representations across layers and target concepts, with some models showing high competence on primitives while others lag despite strong benchmark performance
- Concept similarity analysis demonstrates that lexical content dominates over font color in LLaVA-1.5's internal representations during Stroop-like conflicts

## Why This Works (Mechanism)

### Mechanism 1: Unified Hook Abstraction
A standardized extraction pipeline is achievable by abstracting model-specific preprocessing and hook registration behind a common interface. The base class defines model loading and inference skeletons while subclasses handle idiosyncratic chat templates and visual preprocessing. PyTorch forward hooks dynamically register to specified layers based on YAML configuration, intercepting tensors during the forward pass and routing them to storage backend. The core assumption is that intermediate activations of diverse VLMs share sufficient structural commonalities for unified schema representation.

### Mechanism 2: Competence Decoding via Probing
Linear separability of hidden states serves as a proxy for determining if a VLM encodes specific visual concepts. Hidden states from target layers are extracted, mean-pooled across tokens, and used to train lightweight MLP classifiers. A control probe trained on shuffled labels isolates memorization from genuine representation. The core assumption is that probe success indicates concept encoding, while similar performance between main and control probes suggests reliance on spurious patterns.

### Mechanism 3: Concept Grounding via Geometric Similarity
VLMs resolve ambiguous multimodal cues by encoding conflicting signals into separable subspaces within hidden states. Prototypes for primitive concepts are constructed by averaging hidden states of reference images. Target representations are projected into this space using PCA and cosine similarity to prototypes identifies dominant cues. The core assumption is that color and concept information distributes linearly enough in representation space for cosine similarity over PCA components to reveal internal grounding.

## Foundational Learning

- **PyTorch Forward Hooks**: The entire extraction pipeline relies on registering callable functions to specific module layers to intercept input/output tensors during inference without altering model code. *Quick check*: How do you register a hook to capture the output of `model.layers.0` in a PyTorch module, and what arguments does the hook function receive?

- **VLM Architectures (Encoder + Projector + LLM)**: Effective analysis requires knowing where to hook. One must distinguish between the visual backbone (e.g., CLIP/ViT), the connector (e.g., MLP/Cross-Attention), and the LLM backbone to target the right processing stage. *Quick check*: In a typical VLM like LLaVA, where does the visual feature map get projected into the language model's embedding space?

- **Control Tasks in Probing**: To distinguish between a model "knowing" a concept and the probe simply memorizing dataset statistics, one must understand the logic of comparing a main probe against a probe trained on shuffled labels. *Quick check*: If a probe achieves 90% accuracy on real labels and 85% on shuffled labels, what does this imply about the model's internal representation of the task?

## Architecture Onboarding

- **Component map**: `src/main.py` -> `src/models/base.py` -> `src/models/[model_name].py` -> SQLite database
- **Critical path**: 1) Select model and verify environment 2) Identify target layers using `--log-named-modules` 3) Configure YAML with model path, prompt, and target modules 4) Run extraction; query SQLite DB for tensors
- **Design tradeoffs**: Storage vs. Speed (SQLite I/O bottlenecks for high-dimensional tensors); Generality vs. Specificity (unified interface may hide unique model-specific knobs)
- **Failure signatures**: GPU OOM (batch size must often be 1 for large models); Hook Contamination (failure to unregister hooks causes memory leaks); Preprocessing Mismatch (wrong chat template results in incoherent representations)
- **First 3 experiments**: 1) Module Inspection (verify available hook points with `--log-named-modules`) 2) Layer-wise Probing (extract hidden states from middle and last layers on CLEVR to find where visual concepts are most salient) 3) Ablation Check (compare cosine similarity for identical images with slightly different prompts to test prompt sensitivity)

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural or training-data factors cause the divergence between high behavioral performance on standard benchmarks and lower internal competence on primitive concepts observed in models like LLaVA-1.5? The authors note that "many [models] still fail on simple synthetic data" despite being highly capable on existing benchmarks, and probing results reveal a significant competence gap. While VLM-LENS quantifies this gap, the paper does not isolate underlying causes such as specific architectural bottlenecks or data distribution issues.

### Open Question 2
How can the toolkit's extraction pipeline be adapted to support gradient-based interpretability methods without compromising its current model-agnostic design? The authors explicitly state that their current inference and database storage approach prevents the use of gradient-based saliency analyses such as Grad-CAM. The current implementation focuses on extracting forward-pass activations into a database, which decouples data from the computational graph required for backpropagation.

### Open Question 3
Does the non-linear representation of color concepts found in LLaVA-1.5 generalize to other semantic primitives and across the 16 diverse VLM architectures supported? The concept similarity experiment concludes that "color information is not captured in a single linear direction in the representation space," but this analysis was restricted to color concepts and limited layers. The paper tests specific "primitive color concepts" to demonstrate the tool but does not verify if this non-linearity is universal across VLMs.

## Limitations
- The abstraction layer may face scalability challenges with emerging dynamic architectures or non-PyTorch frameworks
- Probing methodology cannot definitively establish whether decoded information drives generation decisions
- The geometric similarity approach assumes linear separability of complex multimodal concepts, which may not hold for polysemantic representations
- The toolkit's current focus on classification and similarity tasks leaves gaps in analyzing more complex reasoning capabilities

## Confidence

**High Confidence**: The unified hook abstraction mechanism is well-supported by the toolkit's architecture and demonstrated functionality across 16+ models. The SQLite-based storage schema and YAML configuration system are concrete, verifiable components.

**Medium Confidence**: The competence decoding via probing shows statistically significant results in the paper's experiments, but probe accuracy doesn't guarantee causal usage by the model. The control task methodology is standard but its interpretation in the VLM context requires careful consideration.

**Low Confidence**: The geometric similarity approach for concept grounding relies heavily on the paper's internal demonstrations. While the methodology is sound in principle, its effectiveness for complex real-world concepts remains under-validated, particularly for polysemantic representations.

## Next Checks

1. **Architecture Stress Test**: Apply VLM-LENS to a dynamically structured VLM to evaluate whether the static hook registration breaks, and if so, identify the architectural patterns that cause failure.

2. **Causal Probing Extension**: Implement ablation studies where specific layers' representations are selectively removed or masked during generation to determine whether high probe accuracy correlates with actual usage of the represented information.

3. **Geometric Method Robustness**: Test the concept grounding methodology on a dataset with known non-linear concept mixtures to evaluate whether PCA and cosine similarity can adequately disentangle complex representations.