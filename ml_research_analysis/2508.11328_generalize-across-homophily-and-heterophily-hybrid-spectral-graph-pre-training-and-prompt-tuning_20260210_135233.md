---
ver: rpa2
title: 'Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training
  and Prompt Tuning'
arxiv_id: '2508.11328'
source_url: https://arxiv.org/abs/2508.11328
tags:
- graph
- spectral
- prompt
- graphs
- filters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generalizing graph neural
  networks across graphs with varying homophily and heterophily levels under limited
  supervision. Existing graph prompt tuning methods rely on homophily-based low-frequency
  knowledge, limiting their ability to handle diverse spectral distributions.
---

# Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning

## Quick Facts
- arXiv ID: 2508.11328
- Source URL: https://arxiv.org/abs/2508.11328
- Reference count: 40
- Primary result: HS-GPPT achieves up to 12.7% F1-score improvement on heterophilic graphs with 5-shot learning

## Executive Summary
This paper addresses the challenge of generalizing graph neural networks across graphs with varying homophily and heterophily levels under limited supervision. Existing graph prompt tuning methods rely on homophily-based low-frequency knowledge, limiting their ability to handle diverse spectral distributions. The authors propose HS-GPPT, which introduces a hybrid spectral filter backbone to capture abundant spectral knowledge and spectral-aligned prompt tuning to adaptively align the downstream graph's spectrum with the pre-trained filters. Experiments across 10 real-world datasets show HS-GPPT outperforms 15 baselines in both transductive and inductive settings, achieving up to 12.7% improvement in F1-score on heterophilic graphs and demonstrating robust performance under 5-shot learning.

## Method Summary
HS-GPPT is a two-stage framework: (1) Pre-training uses a Beta Wavelet GNN backbone with 3 parallel filters (low/mid/high-pass) trained via multi-view local-global contrastive learning to capture diverse spectral knowledge; (2) Prompt tuning freezes the backbone and optimizes filter-specific prompt graphs to align downstream graph spectra with pre-trained filters. The method addresses the spectral specificity principle: optimal knowledge transfer requires alignment between pre-trained spectral filters and the intrinsic spectrum of downstream graphs.

## Key Results
- Achieves up to 12.7% F1-score improvement on heterophilic graphs compared to 15 baselines
- Outperforms baselines in both transductive and inductive settings across 10 real-world datasets
- Demonstrates robust performance under 5-shot learning conditions
- Ablation studies confirm importance of spectral-aligned prompts and hybrid spectral backbone

## Why This Works (Mechanism)

### Mechanism 1: Spectral Specificity Principle
Effective knowledge transfer requires alignment between pre-trained spectral filters and the intrinsic spectrum of downstream graphs. Theoretical analysis proves filter responses must be positively correlated with signal energy distributions to minimize Spectral Regression Loss.

### Mechanism 2: Hybrid Spectral Filter Backbone
Multi-filter backbone (C+1 filters) captures diverse frequency bands using Beta Wavelet GNN with low-pass, band-pass, and high-pass filters. Multi-view contrastive learning trains these filters and learns adaptive integration weights.

### Mechanism 3: Spectral-Aligned Prompt Tuning
Learnable prompt graphs transform downstream graph spectra to match pre-trained filters. Theorem guarantees prompts can simulate arbitrary spectral transformations. Prompt features normalized to match original graph distribution.

## Foundational Learning

- Concept: Graph Laplacian and Spectral Decomposition
  - Why needed here: Framework operates in spectral domain using L = UΛU^T; high-frequency area S_high quantifies spectral distribution
  - Quick check question: Given a graph's Laplacian L with eigenvalues λ∈[0,2], how would you compute S_high for a feature vector x?

- Concept: Homophily vs Heterophily
  - Why needed here: Edge homophily h determines spectral distribution; S_high = C_base - C_gap·h (linear negative correlation)
  - Quick check question: If a graph has edge homophily h=0.2, would you expect dominant low-frequency or high-frequency components? Why?

- Concept: Graph Prompt Tuning Paradigm
  - Why needed here: HS-GPPT follows "pre-train, freeze, prompt" paradigm; prompts are only tunable components during downstream adaptation
  - Quick check question: In Eq. 1, why are θ* frozen while ω is optimized? What does this imply for knowledge transfer?

## Architecture Onboarding

- Component map: Pre-training Graph → BWGNN Filters (C+1 parallel) → MLP per filter → Weighted Integration → Pooling → Contrastive Loss → Downstream Graph + Prompt Graphs (C+1) → Frozen BWGNN Filters → Frozen Integration → Task Head (MLP) → Classification Loss

- Critical path: 1) Pre-train BWGNN backbone with contrastive learning (2000 epochs, lr=1e-3); 2) Freeze all backbone parameters; 3) For each downstream graph, initialize C+1 prompt graphs (N_p=10 nodes each); 4) Tune only prompt features P^k and task head via cross-entropy (lr=5e-3)

- Design tradeoffs:
  - C order: Higher C increases spectral coverage but adds parameters; C=2 (3 filters) is default
  - Prompt nodes (N_p): More nodes increase alignment capacity but complicate optimization; N_p=10 is default
  - τ_cross threshold: Critical parameter (0.55 homophilic, 0.4 heterophilic); high sensitivity shown in Figure 8
  - Backbone choice: BWGNN provides better spectral locality than Triple GNN

- Failure signatures:
  - Performance collapse on heterophilic graphs: Check if backbone was pre-trained with only low-pass filters
  - Inductive transfer failure: Verify pre-training graph covers diverse spectral distributions
  - Prompt noise injection: If prompts hurt performance, check normalization (Eq. 8)
  - τ_cross mismatch: If homophilic graphs underperform, τ_cross may be too low

- First 3 experiments:
  1. Reproduce transductive 5-shot on heterophilic graphs (Texas, Cornell, Wisconsin): Verify F1 improvements over baselines
  2. Ablation on prompt specificity: Compare HS-GPPT vs "single prompt" to validate filter-specific alignment
  3. Cross-domain inductive transfer: Pre-train on homophilic Pubmed, test on heterophilic Texas

## Open Questions the Paper Calls Out

### Open Question 1
Can the cross-edge threshold (τ_cross) be optimized automatically rather than relying on homophily-driven heuristics?
Basis: Appendix E.1 states performance is "highly sensitive to cross edge thresholds" and uses manual "homophily-driven heuristic"
What evidence would resolve: Mechanism to learn τ_cross via gradient descent or adaptive function achieving comparable performance

### Open Question 2
Can the spectral alignment framework be generalized to graph-level tasks beyond node classification?
Basis: Section 3.1 explicitly restricts scope to node classification as "main challenge on heterophilic graphs"
What evidence would resolve: Modified readout and prompt insertion functions for batches of graphs, validated on graph classification benchmarks

### Open Question 3
Does the required prompt node budget (N_p) scale with spectral complexity or size of downstream graph?
Basis: Theorem 4.1 proves prompt graph exists to approximate any spectral distribution, but implementation fixes N_p=10
What evidence would resolve: Empirical analysis correlating optimal N_p with graph size or spectral entropy, or theoretical bound on N_p required

## Limitations
- Theoretical guarantees rely on Laplacian eigendecomposition assumptions that may not hold for graphs with high multiplicity eigenvalues
- Parameter sensitivity requires careful tuning, particularly τ_cross shows high sensitivity
- Backbone compatibility constraints: experiments only validate BWGNN vs Triple GNN

## Confidence

High confidence in spectral alignment mechanism - Theoretical analysis provides rigorous proof and ablation studies show significant performance drops

Medium confidence in generalization claims - 10 datasets show consistent improvements but limited to node classification

Low confidence in parameter robustness - Limited sensitivity analysis beyond τ_cross; other critical parameters lack systematic evaluation

## Next Checks

1. Cross-task generalization validation - Apply HS-GPPT to link prediction and graph classification tasks to validate if spectral alignment benefits transfer beyond node classification

2. Scalability stress test - Evaluate performance on graphs with 100K+ nodes and varying spectral complexity to validate 10-node prompts remain sufficient

3. Robustness to spectral distribution shift - Create synthetic graphs with controlled spectral distributions to measure performance degradation as spectral gap increases between pre-training and downstream graphs