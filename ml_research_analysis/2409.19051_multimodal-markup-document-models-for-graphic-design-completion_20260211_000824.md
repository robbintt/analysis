---
ver: rpa2
title: Multimodal Markup Document Models for Graphic Design Completion
arxiv_id: '2409.19051'
source_url: https://arxiv.org/abs/2409.19051
tags:
- image
- design
- completion
- text
- graphic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MarkupDM, a multimodal markup document model
  for graphic design completion that represents designs as interleaved markup language
  and images, supporting variable-length elements and type-dependent attributes. Unlike
  grid-based approaches, it employs fill-in-the-middle training to complete missing
  design parts from context.
---

# Multimodal Markup Document Models for Graphic Design Completion

## Quick Facts
- arXiv ID: 2409.19051
- Source URL: https://arxiv.org/abs/2409.19051
- Reference count: 40
- Primary result: MarkupDM achieves 20% accuracy improvement in attribute prediction and 25% in text completion over baselines, with strong performance on instruction-guided design completion

## Executive Summary
This paper introduces MarkupDM, a multimodal markup document model that represents graphic designs as interleaved markup language and images to enable graphic design completion. Unlike grid-based approaches, MarkupDM uses a fill-in-the-middle training objective to complete missing design parts from context, supporting variable-length elements and type-dependent attributes. The model employs a specialized RGBA image tokenizer to handle transparency and variable-sized images, and achieves strong performance across attribute, image, and text completion tasks, with accuracy improvements up to 20% over baselines. On a new instruction-guided design completion task, MarkupDM outperforms state-of-the-art image editing models in text-based edits, demonstrating its flexibility and effectiveness for design automation.

## Method Summary
The method represents graphic designs as SVG-like markup documents with image content replaced by discrete tokens. A causal multimodal LLM (StarCoderBase or Qwen2.5) processes these sequences using fill-in-the-middle training to predict missing spans. The model employs a specialized RGBA image tokenizer trained on transparent images, and uses separate prediction heads for text and image tokens. Training involves 100K steps with FIM probability 0.9, and the model can be fine-tuned for instruction-guided completion on a separate dataset.

## Key Results
- 20% accuracy improvement over baselines in attribute prediction tasks
- 25% improvement in text completion accuracy
- Outperforms state-of-the-art image editing models on text-based instruction-guided design completion
- Specialized RGBA tokenizer shows improved RGB and alpha channel MSE compared to baseline tokenizers

## Why This Works (Mechanism)

### Mechanism 1
Representing graphic designs as interleaved markup language and images enables handling variable-length elements, type-dependent attributes, and text content more naturally than grid-based representations. Designs are converted into an SVG-like document where image content is replaced with discrete tokens, processed by a causal multimodal LLM that learns relationships between markup tags, attributes, and image tokens. The core assumption is that markup language provides a structured, human-readable format that inherently captures the hierarchical and variable nature of design elements.

### Mechanism 2
Fill-in-the-middle (FIM) training enables a single model to perform multiple design completion tasks (attribute, image, text) in a unified manner. During training, random spans of the document are masked and the model learns to predict them from surrounding context (prefix + suffix). This creates a general-purpose completion ability without task-specific heads. The core assumption is that design context provides sufficient information to infer missing spans, similar to code completion patterns.

### Mechanism 3
A specialized image tokenizer trained on RGBA images with transparency enables generation of design elements that standard RGB tokenizers cannot represent. An autoencoder is fine-tuned to encode RGBA images (including alpha channel) into discrete token maps at fixed resolution, then decode back. The alpha channel weights are initialized from RGB means. The core assumption is that transparency information is critical for design elements (overlays, buttons, decorative elements) and must be preserved in tokenization.

## Foundational Learning

- **Concept: Markup Language Representation (SVG/HTML-like structure)**
  - Why needed here: The entire model is built on representing designs as sequences of markup tags with attributes and embedded image tokens. Understanding how SVG elements encode position, style, and content is essential for data preparation, model input/output interpretation, and debugging.
  - Quick check question: Given a design with a red rectangle at position (100, 50) and size (200, 150), how would you represent it as an SVG `<rect>` element with appropriate attributes?

- **Concept: Fill-in-the-Middle (FIM) Training Objective**
  - Why needed here: The model's ability to perform multiple completion tasks stems from this training strategy. Understanding FIM helps explain why no task-specific heads are needed and how to design training data augmentation.
  - Quick check question: If a document has tokens [A, B, C, D, E] and FIM selects span [B, C] to predict, what does the model see as input and what must it generate?

- **Concept: Discrete Image Tokenization (VQ-VAE/VQGAN)**
  - Why needed here: The model generates images by predicting discrete tokens from a codebook. Understanding how images are encoded/decoded, what token resolution means, and how transparency is handled is critical for interpreting image outputs and troubleshooting reconstruction quality.
  - Quick check question: Why does the paper resize all images to a fixed square size before tokenization instead of using variable token map sizes?

## Architecture Onboarding

- **Component map**: Design template -> SVG markup conversion -> FIM transformation -> Tokenization (text via LLM tokenizer, images via RGBA tokenizer) -> Embedding and positional encoding -> LLM layers -> Dual prediction heads -> Token generation -> Decoding (image tokens via tokenizer decoder)

- **Critical path**: 1) Design template converted to SVG markup with image placeholders 2) FIM transformation masks random spans 3) Sequence tokenized (text + image tokens) 4) Embedded and processed through LLM layers 5) Predicted tokens generated with appropriate modality head 6) Tokens decoded back to design elements

- **Design tradeoffs**: Tokenizer resolution vs. sequence length (higher resolution improves quality but increases token count), model size vs. task performance (7B outperforms 1B/3B but requires more compute), caption conditioning (training with captions doesn't improve image completion, but ground-truth captions at inference do), unified vs. task-specific models (single model handles multiple tasks but may underperform specialized models)

- **Failure signatures**: Image generation struggles with complex objects, detailed textures, and visual harmonization with surrounding elements; text completion errors when image understanding fails or text conflicts visually with other elements; instruction following shows lower alignment scores than proprietary models; alpha channel reconstruction degrades for fine details like faces

- **First 3 experiments**: 1) Tokenizer validation: Reconstruct held-out RGBA images and measure MSE (RGB/alpha) and rFID 2) Attribute completion baseline: Train MarkupDM (1B) on Crello dataset and evaluate accuracy on x, y, width, height, font-family, font-size prediction 3) Modality ablation: Test image completion with and without ground-truth captions at inference and measure cosine similarity

## Open Questions the Paper Calls Out

- **Can the model be improved to generate decorative or background elements that visually harmonize with the surrounding content without relying solely on external text-to-image models?**
  - The current model struggles to generate complex images natively and relies on external models which may not capture the specific visual context or transparency needs of decorative design elements

- **What specific domain-specific training techniques or architectural modules are required to enhance the model's spatial reasoning for visually intricate compositional tasks?**
  - The current architecture lacks dedicated spatial modules, limiting its ability to manage complex layering or maintain aesthetic coherence across multiple elements

- **How can the fill-in-the-middle training objective be adapted to support full-fledged editing and refinement of existing design elements?**
  - The current training focuses on inserting missing components from context rather than modifying the attributes or textures of elements already present in the prefix or suffix

## Limitations

- Evaluation focuses on quantitative metrics without qualitative human studies to assess visual quality and usability in real-world scenarios
- Performance on complex design elements with intricate visual details or sophisticated spatial reasoning remains unclear
- Training data of 19K templates may limit generalization to diverse design styles and professional-grade requirements

## Confidence

**High Confidence**: Representation as interleaved markup, specialized RGBA tokenizer, and FIM training approach are well-supported by experimental results with measurable performance improvements.

**Medium Confidence**: Claims about model flexibility and effectiveness for design automation are supported by metrics but lack qualitative validation through human studies or professional designer evaluations.

**Low Confidence**: Claims about the model's ability to generate high-quality designs for professional use cases are not fully substantiated, as evaluation focuses on technical metrics rather than design quality assessments by human experts.

## Next Checks

1. Conduct a human study with professional graphic designers to evaluate the aesthetic quality, usability, and professional viability of completed designs across different task types, comparing human ratings against baseline models.

2. Systematically test the model's performance on designs containing large missing spans, complex visual elements, and elements requiring global design coherence, measuring both quantitative metrics and qualitative reconstruction quality.

3. Evaluate the model's ability to generalize to design templates from different sources, styles, and complexity levels beyond the Crello dataset, testing zero-shot and few-shot performance on professional design tasks.