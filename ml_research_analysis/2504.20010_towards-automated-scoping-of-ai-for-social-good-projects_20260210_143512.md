---
ver: rpa2
title: Towards Automated Scoping of AI for Social Good Projects
arxiv_id: '2504.20010'
source_url: https://arxiv.org/abs/2504.20010
tags:
- problem
- scoping
- methods
- each
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an AI-powered problem scoping agent (PSA)
  for AI for Social Good (AI4SG) projects. The PSA automates the process of identifying
  real-world social problems, selecting appropriate AI methods, and proposing solutions
  by leveraging large language models enhanced with retrieval-augmented generation.
---

# Towards Automated Scoping of AI for Social Good Projects

## Quick Facts
- **arXiv ID:** 2504.20010
- **Source URL:** https://arxiv.org/abs/2504.20010
- **Reference count:** 26
- **Primary result:** PSA-generated proposals are comparable to expert-written proposals on multiple evaluation metrics, with significant improvements over baseline LLM approaches.

## Executive Summary
This paper introduces an AI-powered Problem Scoping Agent (PSA) that automates the process of identifying real-world social problems and proposing AI-based solutions for Social Good (AI4SG) projects. The PSA leverages large language models enhanced with retrieval-augmented generation to systematically retrieve background information, identify challenges, search for relevant AI methods from scientific literature, and generate comprehensive project proposals. Experiments show that PSA-generated proposals are comparable to expert-written proposals on multiple evaluation metrics, with significant improvements over baseline LLM approaches, particularly for models with less domain knowledge.

## Method Summary
The PSA is a multi-stage LLM pipeline that automates AI4SG project scoping through retrieval-augmented generation. Given a list of organization names, it sequentially: (1) retrieves background information from the web, (2) generates diverse challenges and retrieves supporting evidence, (3) searches academic literature for relevant AI methods, and (4) synthesizes a final project proposal grounded in all retrieved context. The pipeline uses Google Custom Search and Semantic Scholar APIs for retrieval, LLM annotation for summarization, and confidence-based selection to balance quality and diversity.

## Key Results
- PSA-generated proposals achieve comparable quality to expert-written proposals on Appropriateness, Thoroughness, Feasibility, and Expected Effectiveness metrics
- PSA provides significant improvements over baseline LLMs, particularly for models with less domain knowledge
- The method encourages diversity in problem identification while maintaining proposal quality
- LLM-based evaluation shows poor correlation with human judgment, highlighting evaluation challenges

## Why This Works (Mechanism)

### Mechanism 1: External Knowledge Grounding via Retrieval-Augmented Generation (RAG)
Sequentially retrieving and annotating external documents before generation reduces hallucination and improves domain relevance compared to naive LLM prompting.

### Mechanism 2: Divergent-Convergent Search with Confidence Estimation
Generating multiple diverse options and selecting/filtering them using verbalized confidence scores improves quality and appropriateness.

### Mechanism 3: Structured Task Decomposition via Chain-of-Thought
Breaking down complex problem scoping into discrete sub-tasks makes the problem tractable for LLMs and yields more coherent proposals than end-to-end generation.

## Foundational Learning

**Concept:** Retrieval-Augmented Generation (RAG)
- **Why needed:** The entire PSA architecture is a specialized RAG pipeline; understanding precision of retrieval, context limits, and synthesis quality is essential for debugging.
- **Quick check:** How would PSA output change if top-k retrieved documents were irrelevant, and how would you diagnose this failure?

**Concept:** Chain-of-Thought (CoT) Prompting
- **Why needed:** PSA's sequential decomposition applies CoT principles to complex real-world tasks; recognizing this helps understand the pipeline design.
- **Quick check:** Which component represents the final "answer" synthesis step in a CoT chain?

**Concept:** LLM-based Evaluation and its Pitfalls
- **Why needed:** The paper critically assesses using LLMs for evaluation, finding them inconsistent; this is crucial for understanding evaluation framework needs.
- **Quick check:** Based on Table 1, why might using an LLM as an automated evaluator be problematic? What are signs of failure?

## Architecture Onboarding

**Component map:**
Orchestrator -> Input (Organization Names) -> Background Retriever (Google Search -> Annotator LLM -> Background $B$) -> Challenge Retriever (LLM queries -> Google Search -> Annotator LLM -> Challenges $C$ with confidence) -> Method Retriever (LLM queries -> Semantic Scholar -> Annotator LLM -> Papers $M$ with confidence) -> Solution Generator (Synthesis LLM -> Final Proposal)

**Critical path:**
Organization Name Input → Google Search for Background → LLM summarization of Background → Google Search for Challenge Evidence → Semantic Scholar Search for Methods → Final Proposal Synthesis

**Design tradeoffs:**
- Pipeline vs. Single Prompt: Multi-step pipeline for control/grounding vs. increased latency and error propagation
- External Search APIs: Real-time verifiable information vs. dependencies and rate limits
- Verbalized Confidence vs. Sampling: Efficient filtering vs. reliance on uncalibrated model self-assessment
- Quality vs. Diversity: Softmax sampling and query pruning balance high-quality solutions with diverse problems

**Failure signatures:**
- Hallucinated Challenge: Proposal references issues not found in retrieved sources
- Generic Solution: Suggests generic AI methods not tied to specific retrieved papers
- Objective Drift: Solution contradicts problem statement (lacks consistency check)
- Low Proposal Diversity: Multiple runs yield identical problem statements

**First 3 experiments:**
1. Baseline Reproduction: Re-implement PSA using open-source LLM and validate against human proposals
2. Ablation Study - RAG Impact: Disable one retrieval component at a time and compare quality scores
3. Evaluation Framework Comparison: Compare human, same-LLM, and different-LLM evaluations for correlation analysis

## Open Questions the Paper Calls Out

**Open Question 1:** How can human-AI collaboration be effectively designed to enhance interactive and iterative AI4SG problem scoping beyond single-pass automated agents?

**Open Question 2:** Can reliable, objective, and automated evaluation criteria for AI4SG problem proposals be formalized to reduce subjectivity and enable scalable assessment?

**Open Question 3:** To what extent can PSA's method of enhancing models with limited domain knowledge be generalized to improve other complex, domain-specific reasoning or planning tasks?

## Limitations
- Evaluation methodology relies heavily on LLM-based assessment, which shows poor correlation with human judgment
- Does not address handling contradictory retrieved documents or interactive clarification needs
- Claims about effectiveness for less-knowledgeable models lack empirical validation across varying baseline knowledge levels

## Confidence
- **High Confidence:** PSA architectural design as RAG-enhanced multi-step pipeline is technically sound
- **Medium Confidence:** PSA outperforms baseline LLMs, but evaluation methodology reliability is questionable
- **Low Confidence:** Claims about effectiveness for models with less domain knowledge lack empirical validation

## Next Checks
1. Replicate Table 1 with three independent human experts versus three different LLM evaluators to quantify evaluation reliability
2. Conduct ablation study on RAG components by disabling retrieval steps and measuring quality degradation
3. Design user study with human stakeholders interacting with PSA in real-time to measure impact on proposal quality