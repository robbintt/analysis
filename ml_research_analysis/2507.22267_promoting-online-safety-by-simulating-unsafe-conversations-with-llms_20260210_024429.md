---
ver: rpa2
title: Promoting Online Safety by Simulating Unsafe Conversations with LLMs
arxiv_id: '2507.22267'
source_url: https://arxiv.org/abs/2507.22267
tags:
- conversations
- llms
- unsafe
- online
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an LLM-based system for simulating scam conversations\
  \ to promote online safety awareness. The system uses two LLMs\u2014an OpenAI model\
  \ as the scammer and Google's Gemini as the target\u2014engaging in realistic unsafe\
  \ conversations."
---

# Promoting Online Safety by Simulating Unsafe Conversations with LLMs

## Quick Facts
- arXiv ID: 2507.22267
- Source URL: https://arxiv.org/abs/2507.22267
- Reference count: 24
- Primary result: LLM-based system simulates scam conversations using OpenAI as scammer and Google Gemini as target, with user feedback promoting learning about online safety

## Executive Summary
This paper presents an innovative approach to online safety education through LLM-mediated scam simulation. The system uses two separate LLMs to create realistic unsafe conversations, with users providing feedback to the target model during simulations. The authors successfully navigated safety constraints by framing the scammer as a "persuader" role-playing a character and declaring both agents as "not value aligned." While qualitative assessments show promising performance, the system lacks quantitative validation of its effectiveness in helping users identify real scam conversations.

## Method Summary
The authors developed an LLM-based system that simulates scam conversations by using OpenAI's model as the scammer and Google's Gemini as the target. Users interact with the target LLM during these simulations, providing feedback based on research showing that hypothetical feedback promotes learning. To bypass safety constraints, the authors framed the scammer as a "persuader" role-playing a character and declared both agents as "not value aligned," allowing them to simulate various scam scenarios including requests for sensitive information like bank passwords and credit card numbers.

## Key Results
- Successfully simulated realistic unsafe conversations between two LLMs for online safety training
- Different models exhibited distinct personalities, requiring combination of two models for desired behavior
- Qualitative assessment shows promising performance in scam scenario simulation
- System operates within safety constraints through creative role-playing framing

## Why This Works (Mechanism)
The system leverages the ability of LLMs to role-play different personas and engage in extended conversations. By positioning one LLM as a "persuader" scammer and another as the target, the system creates realistic interaction scenarios. The feedback mechanism allows users to guide the target LLM's responses, creating an interactive learning environment. The dual-LLM architecture enables more nuanced and persistent scam attempts than a single LLM approach.

## Foundational Learning
- LLM Role-Playing Capabilities - LLMs can convincingly simulate different personas and behaviors; needed to create believable scammer characters; quick check: verify consistent personality across conversation turns
- Prompt Engineering for Safety Bypass - Creative framing can navigate content restrictions; needed to enable realistic scam scenarios; quick check: test multiple framing approaches for consistency
- Interactive Learning Theory - Hypothetical feedback promotes learning; needed to justify user interaction with target LLM; quick check: compare learning outcomes with passive observation

## Architecture Onboarding

**Component Map:** User -> Feedback -> Target LLM (Gemini) <- Scammer LLM (OpenAI)

**Critical Path:** User provides feedback to target LLM during scam conversation -> Target LLM adjusts responses -> Conversation continues with scammer persistence

**Design Tradeoffs:** Single LLM vs dual LLM approach (single simpler but less persistent; dual more complex but realistic); explicit safety bypass vs working within constraints (bypass more flexible but potentially less sustainable)

**Failure Signatures:** Inconsistent scammer personality across turns; target LLM breaking character; safety systems blocking conversation; unrealistic conversation flow

**3 First Experiments:** 1) Test scammer persistence across 10+ conversation turns with different target models; 2) Compare learning outcomes between feedback and no-feedback conditions; 3) Evaluate safety bypass effectiveness across multiple LLM providers

## Open Questions the Paper Calls Out
The paper acknowledges that quantitative validation is needed to establish the system's effectiveness in helping users identify real scam conversations. The authors note ongoing work to measure actual learning outcomes and behavioral changes in participants.

## Limitations
- Lacks quantitative validation of learning outcomes and real-world scam detection effectiveness
- Safety bypass methodology may not generalize across evolving LLM safety systems
- Dual-LLM architecture complexity not fully explored, particularly regarding consistency and emergent behaviors

## Confidence

**High confidence** in technical feasibility of LLM-based scam simulation
**Medium confidence** in pedagogical approach based on hypothetical feedback
**Low confidence** in system's effectiveness for real-world online safety training without quantitative validation

## Next Checks

1. Conduct controlled user studies comparing learning outcomes between LLM simulation approach and traditional online safety training methods, measuring both immediate knowledge gains and retention over time

2. Systematically test the role-playing framing approach across different LLM providers and safety models to assess robustness and identify alternative prompt engineering strategies

3. Implement and evaluate automated metrics for assessing realism and effectiveness of simulated scam conversations, including measures of scammer persistence, target vulnerability patterns, and conversation naturalness