---
ver: rpa2
title: 'AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting
  Exchange'
arxiv_id: '2602.00192'
source_url: https://arxiv.org/abs/2602.00192
tags:
- inpainting
- image
- detectors
- inp-x
- artifacts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AI-generated image detectors fail to identify locally synthesized
  content and instead rely on global V AE reconstruction artifacts that affect the
  entire image. The authors introduce Inpainting Exchange (INP-X), which restores
  original pixels outside the edited region while preserving generated content within
  the mask.
---

# AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange

## Quick Facts
- **arXiv ID:** 2602.00192
- **Source URL:** https://arxiv.org/abs/2602.00192
- **Reference count:** 40
- **Primary result:** Detectors fail on locally synthesized content, relying instead on global VAE reconstruction artifacts

## Executive Summary
AI-generated image detectors fail to identify locally synthesized content and instead rely on global VAE reconstruction artifacts that affect the entire image. The authors introduce Inpainting Exchange (INP-X), which restores original pixels outside the edited region while preserving generated content within the mask. This intervention reveals detectors' overreliance on global artifacts, causing severe performance degradation (e.g., accuracy drops from 91% to 55%). Theoretical analysis links this behavior to high-frequency attenuation from VAE information bottlenecks. Training on INP-X improves both generalization and localization compared to standard inpainting, though detecting INP-X images remains challenging.

## Method Summary
The study introduces INP-X, an intervention that restores original pixels outside inpainting masks while preserving generated content within. The method is evaluated using 90K benchmark images across CelebA-HQ, CityScapes, OpenImages, and SUN-RGBD. Standard inpainting is performed via latent diffusion models (Kandinsky 2.2, OpenJourney, Stable Diffusion v1.4), then INP-X post-processing restores original background pixels. Detection performance is measured using classification metrics (Acc, AUC, Prec, Rec, F1) and localization metrics (mIoU, mAP). Models fine-tuned include ResNet-50, EfficientNet-B0, ViT-B/16, and CLIP ViT-B/32 variants.

## Key Results
- Pretrained detectors show accuracy drops from 91% to 55% when evaluated on INP-X images versus standard inpainting
- VAE information bottlenecks cause systematic high-frequency attenuation, creating detectable spectral anomalies
- Training on INP-X improves both generalization to standard inpainting and localization performance compared to standard inpainting training
- CNN backbones (ResNet-50, EfficientNet) achieve superior localization (mIoU 0.45–0.49) compared to ViTs (0.37–0.41)

## Why This Works (Mechanism)

### Mechanism 1: Global VAE Reconstruction Artifacts as Detection Shortcuts
- **Claim:** Current AI-generated image detectors exploit global spectral artifacts introduced by VAE encoding-decoding, rather than identifying locally synthesized content
- **Mechanism:** Latent diffusion models process entire images through VAE encoder-decoder pairs. This reconstruction introduces pervasive spectral shifts across both masked and unmasked regions, creating a detectable "fingerprint" that provides a trivial classification signal unrelated to actual generated content
- **Core assumption:** Detectors trained on standard inpainting datasets learn spurious correlations between global VAE artifacts and the "fake" label
- **Evidence anchors:** Abstract states detectors "primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content"; Theorem 3.2 proves variance contraction in VAEs; Table 1 shows 91%→55% accuracy drop

### Mechanism 2: High-Frequency Attenuation Creates Forensic Signature
- **Claim:** VAE information bottlenecks systematically attenuate high-frequency content (sensor noise, fine textures), creating detectable spectral anomalies
- **Mechanism:** VAEs optimize for perceptual quality via MSE + perceptual + adversarial losses. Since perceptual losses operate on low-pass VGG features and spatial compression (8× downsampling = 64× dimension reduction) limits information capacity, non-semantic high-frequency content is discarded during encoding and cannot be recovered during decoding
- **Core assumption:** High-frequency sensor noise (PRNU, photon shot noise) is non-semantic and thus not preserved in latent codes
- **Evidence anchors:** Spectral Variance Gap definition and Theorem 3.2 prove spectral power at high frequencies is strictly lower in reconstructions; Figure 13 shows FFT analysis revealing distinct periodic grid-like artifacts in standard inpainting

### Mechanism 3: INP-X Removes Global Divergence While Preserving Local Content
- **Claim:** Restoring original pixels outside the inpainting mask minimizes distributional divergence from real images, eliminating the global detection signal
- **Mechanism:** Standard inpainting introduces artifacts across the entire spatial domain. INP-X sets background pixels to exact original values, forcing KL divergence to zero for background regions. Total divergence reduces to only the conditional divergence of the foreground (masked) region
- **Core assumption:** The detection difficulty scales with distributional divergence; lower divergence implies harder detection
- **Evidence anchors:** Theorem 3.4 formally proves D_KL(P_real || P_ex) < D_KL(P_real || P_std) because background divergence term becomes zero; all 11 pretrained detectors show severe degradation on INP-X

## Foundational Learning

- **Concept: KL Divergence and Distributional Shift**
  - **Why needed here:** Understanding Theorem 3.4 requires grasping how INP-X reduces detectability by minimizing divergence between manipulated and real image distributions
  - **Quick check question:** If background pixels are restored exactly, what happens to the first term in the chain rule decomposition of KL divergence?

- **Concept: Fourier/Wavelet Spectral Analysis**
  - **Why needed here:** The core finding hinges on spectral signatures—understanding why VAEs attenuate high frequencies and how detectors exploit these patterns
  - **Quick check question:** Why would an 8× spatial downsampling factor (64× dimension reduction) preferentially affect high-frequency content preservation?

- **Concept: Shortcut Learning in Neural Networks**
  - **Why needed here:** The paper frames detector failure as shortcut learning—models exploit easy global artifacts rather than learning the intended task of local content detection
  - **Quick check question:** What evidence would distinguish a detector that learned genuine content features from one that learned shortcuts?

## Architecture Onboarding

- **Component map:** Real image x + binary mask M → Standard inpainting via latent diffusion (VAE encoder E → denoiser → VAE decoder D) → INP-X operation: restore original background pixels where M=0 → Detection target: binary classifier (real vs fake) OR localization (manipulated region mask prediction)

- **Critical path:** 1) VAE encoding compresses image to latent space (information bottleneck) 2) Diffusion denoising generates content in masked region 3) VAE decoding reconstructs full image (introduces global spectral artifacts) 4) INP-X intervention: restore original background, remove global artifacts 5) Detector evaluation: classification accuracy and localization mIoU/mAP

- **Design tradeoffs:**
  - Standard inpainting training: Easy to learn (global artifacts = trivial signal), poor generalization, catastrophic transfer failure to INP-X
  - INP-X training: Harder to learn, better generalization to standard inpainting, improved localization, but still challenging detection
  - CNN vs. ViT backbones: CNNs (ResNet-50, EfficientNet) achieve superior localization (mIoU 0.45–0.49) vs. ViTs (0.37–0.41) due to spatial locality inductive bias

- **Failure signatures:**
  - High accuracy on standard inpainting but near-chance on INP-X → detector learned global shortcuts
  - Global attention maps (GradCAM) across entire image rather than localized to mask → shortcut behavior
  - Asymmetric transfer: training on INP fails on INP-X, but INP-X training generalizes to INP

- **First 3 experiments:**
  1. Baseline replication: Run pretrained detectors (Corvi2023, SPAI, CLIP variants) on standard inpainting vs. INP-X to confirm accuracy degradation pattern (target: >30% drop)
  2. VAE artifact isolation: Apply VAE encode-decode to real images without diffusion, measure correlation between VAE reconstruction error and high-frequency content (target: r > 0.5 as reported)
  3. Training ablation: Train ResNet-50 detector on INP vs. INP-X, evaluate cross-transfer and localization (target: INP-X training should improve mIoU by >0.02 and transfer better to standard inpainting than reverse)

## Open Questions the Paper Calls Out

- **Can frequency-preserving VAE architectures or novel decoding strategies that explicitly enforce spectral consistency eliminate the global artifacts that detectors currently exploit, while maintaining generation quality?**
  - **Basis in paper:** Conclusion states: "Future work can focus on model improvement, such as 'frequency-preserving' VAEs or decoding strategies that explicitly enforce spectral consistency"
  - **Why unresolved:** The paper demonstrates the artifact problem theoretically and empirically but does not propose architectural solutions; modern VAEs (SDXL, FLUX.1) still exhibit the issue
  - **What evidence would resolve it:** A modified VAE trained with spectral consistency regularization that produces inpainting outputs indistinguishable from original images in high-frequency bands, validated via spectral MSE analysis

- **How do detectors perform on non-VAE architectures (pixel-space diffusion, component-based models), and do these methods inherently avoid the global artifact problem?**
  - **Basis in paper:** Limitations section: "a deeper analysis of non-VAE architectures, such as component-based or pixel-space models, is warranted"
  - **Why unresolved:** The study focuses exclusively on VAE-based latent diffusion; RePaint is mentioned but dismissed as impractical due to latency and quality issues
  - **What evidence would resolve it:** Systematic evaluation of detectors on pixel-space diffusion outputs with INP-X-style analysis, comparing spectral characteristics and detection accuracy

- **Can detection methods be designed to reliably identify INP-X manipulations without relying on global spectral cues, achieving accuracy substantially above the current 75.3% ceiling?**
  - **Basis in paper:** Even models specifically trained on INP-X achieve only 75.3% accuracy, and the authors characterize detecting INP-X as "challenging" and "underscore[ing] the relevance" of the problem
  - **Why unresolved:** The paper shows training on INP-X improves localization but does not achieve high detection accuracy; the fundamental difficulty of identifying local-only synthetic content remains
  - **What evidence would resolve it:** A detector architecture that achieves >90% accuracy on INP-X images across multiple inpainting models and datasets, with attention maps localized to the manipulated region

## Limitations

- The theoretical analysis of VAE spectral attenuation relies on assumptions about high-frequency content being non-semantic, requiring stronger ablation studies
- The finding is primarily validated through controlled experiments on the INP-X dataset, with limited independent validation from related work
- Detection of INP-X images remains challenging even with models specifically trained on this dataset, achieving only 75.3% accuracy

## Confidence

- **High Confidence:** The INP-X intervention effectively isolates global artifacts from local content, as evidenced by dramatic accuracy drops across all pretrained detectors and theoretical proof of reduced KL divergence
- **Medium Confidence:** VAE reconstruction introduces systematic high-frequency attenuation that creates detectable spectral anomalies, though independent validation is limited
- **Medium Confidence:** Training on INP-X improves localization and generalization compared to standard inpainting, though detection of INP-X images remains challenging

## Next Checks

1. **Ablation on mask sharpness:** Apply soft alpha blending (5×5 Gaussian blur on edges) to INP-X images and verify accuracy remains low, confirming edge artifacts are not driving detector failure
2. **Cross-generator artifact analysis:** Generate INP-X images using different diffusion models (e.g., SD v1.5, SDXL) and measure whether detectors show similar failure patterns, validating the VAE bottleneck hypothesis across architectures
3. **Feature attribution analysis:** Compare GradCAM/Attention Rollout maps between detectors trained on INP vs. INP-X to quantify the shift from global to local attention patterns, providing direct evidence of shortcut learning