---
ver: rpa2
title: 'MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action
  Agent'
arxiv_id: '2511.18810'
source_url: https://arxiv.org/abs/2511.18810
tags:
- task
- merging
- tasks
- mergevla
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MergeVLA addresses the challenge of merging vision-language-action
  (VLA) models trained on different robotic tasks, which typically fails with zero
  success rates due to task-specific parameter conflicts and architectural incompatibility.
  The core method introduces sparsely activated LoRA adapters via task masks to stabilize
  VLM merging and replaces self-attention layers in the action expert with cross-attention-only
  blocks to preserve modularity.
---

# MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent

## Quick Facts
- arXiv ID: 2511.18810
- Source URL: https://arxiv.org/abs/2511.18810
- Reference count: 40
- Primary result: 90.2% success rate on LIBERO tasks via cross-skill VLA model merging

## Executive Summary
MergeVLA addresses the challenge of merging vision-language-action (VLA) models trained on different robotic tasks, which typically fails with zero success rates due to task-specific parameter conflicts and architectural incompatibility. The core method introduces sparsely activated LoRA adapters via task masks to stabilize VLM merging and replaces self-attention layers in the action expert with cross-attention-only blocks to preserve modularity. It also employs a test-time task router to dynamically select task-specific components without prior task knowledge. Across LIBERO, LIBERO-Plus, RoboTwin, and real-world SO101 robotic arm experiments, MergeVLA achieves success rates of 90.2%, 72.2%, and 70.7% respectively, outperforming individually fine-tuned experts and demonstrating robust cross-skill, cross-environment, and cross-embodiment generalization.

## Method Summary
MergeVLA fine-tunes separate VLA models per task using LoRA adapters on a Qwen2.5-0.5B backbone, then merges them using task-specific masks that isolate conflicting parameters. The action expert architecture removes self-attention layers and uses sigmoid gates instead of tanh to preserve VLM signals. A test-time task router infers task identity from hidden state projections onto value subspace principal components. The merged model maintains separate expert heads for final blocks while averaging earlier blocks, achieving high success rates without task labels at inference.

## Key Results
- 90.2% success rate on LIBERO benchmarks, 18.7% higher than prior approaches
- 72.2% success rate on LIBERO-Plus out-of-distribution tasks
- 70.7% success rate on real SO101 robotic arm tasks
- 89.7% average task routing accuracy using value subspace projections

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Masking for LoRA Conflict Resolution
Sparsely activated task masks enable VLM merging by isolating parameters that align with each task while suppressing conflicting updates. For each task m, a binary mask S_m = I[|τ_m| > λ|τ_merge - τ_m|] retains parameters where the task-specific update τ_m is both significant and dominant over residual disagreement with the merged vector. This encourages conflicting parameters to revert toward pretrained weights rather than corrupt shared representations.

### Mechanism 2: Cross-Attention-Only Architecture for Action Expert Modularity
Removing self-attention from action experts prevents task-specific dependencies from propagating across layers, enabling weight averaging to work for shallow blocks. Standard action experts use self-attention that accumulates task-specific biases over depth, causing parameter distance to "explode in the final layers." Cross-attention-only blocks force reliance on shared VLM features instead of scratch-trained parameters. Sigmoid gates ensure VLM signals are always preserved rather than suppressed.

### Mechanism 3: Test-Time Task Router via Value Subspace Projections
Task identity can be inferred from hidden state alignment with value projection principal components, enabling unsupervised task selection. For each candidate task, compute r_m = (1/2)(||P_T h_A||_2 + ||P_A h_T||_2) where P are top-k right singular vectors of value projections. Select task via softmax over scores. Value projections are used because they "directly encode task-dependent information written into hidden states" whereas Q/K "govern attentional selection" and risk collapsing to task-specific subspaces.

## Foundational Learning

- **Concept: Model Merging via Task Vectors**
  - Why needed here: MergeVLA builds on Task Arithmetic (TA) and TIES merging; understanding τ_m = Θ_m - Θ_0 and conflict resolution strategies is prerequisite.
  - Quick check question: Given three fine-tuned models with task vectors τ_1, τ_2, τ_3, what happens when you compute τ_merge = (τ_1 + τ_2 + τ_3)/3 and apply it to a new input?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper analyzes LoRA adapter behavior during VLA fine-tuning and identifies "selfish parameters" in LoRA updates.
  - Quick check question: If LoRA decomposes weight updates as ΔW = BA where B∈R^{d×r}, A∈R^{r×k}, what happens to the rank of merged updates when combining multiple LoRA adapters?

- **Concept: Attention Mechanisms in Transformers**
  - Why needed here: Understanding cross-attention vs. self-attention is critical for grasping why removing self-attention preserves modularity.
  - Quick check question: In cross-attention, what do Q, K, V represent and how does the mechanism differ from self-attention where Q, K, V all come from the same sequence?

## Architecture Onboarding

- **Component map:**
  - VLM Backbone (Qwen2.5-0.5B): Vision encoder + language model with LoRA adapters
  - Action Expert: L cross-attention-only blocks (no self-attention)
  - Expert Head: Final block(s) H_{l→L} kept unmerged per task (typically l=L)
  - Task Masks: Binary masks S_m per task for LoRA parameter selection
  - Task Router: SVD-based value projection scorer using block (l-1) weights

- **Critical path:**
  1. Fine-tune separate models per task with LoRA on VLM, full training on action expert
  2. Compute merged task vector τ_merge via TA/TIES
  3. Generate task masks using λ=0.6 (default)
  4. Average action expert weights for blocks 1 to (l-1); keep expert heads separate
  5. At inference: run router on t=0 observation → select mask + head → execute

- **Design tradeoffs:**
  - λ mask ratio: Lower values (0.2) activate too many parameters causing interference; higher values (0.6-0.9) work better but reduce task-specific adaptation
  - Expert head depth: Single block (L) sufficient for same embodiment; 2-3 blocks needed for cross-embodiment
  - Router subspace: Value projections more stable than key/query but require SVD computation per task

- **Failure signatures:**
  - 0% success rate with standard merging → LoRA conflicts not masked
  - 0% success rate with masked VLM but merged action expert → Self-attention accumulating task dependencies
  - Router misassignment on cross-embodiment → Value subspaces insufficiently discriminative; need deeper expert heads

- **First 3 experiments:**
  1. **Mask ratio ablation**: Train 4 LIBERO tasks, merge with λ∈{0.2, 0.4, 0.6, 0.8}, plot success rate vs. active ratio to verify 0.6-0.9 sweet spot
  2. **Routing subspace comparison**: Implement K-only, V-only, K+V routing on masked merged model; confirm V-only achieves >85% on Object/Goal tasks (Table 5 pattern)
  3. **Architecture validation**: Train VLA-Adapter (with self-attention) vs. MergeVLA (cross-attention-only) on single LIBERO task, test on LIBERO-Plus perturbations to replicate 18.7% OOD improvement claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MergeVLA's architectural modifications (removing self-attention, using cross-attention-only action experts) scale effectively to larger VLM backbones beyond the 0.5B parameter model tested?
- Basis in paper: [explicit] The conclusion states: "Future work will explore this promising direction further, including whether larger VLM backbones remain compatible with our framework."
- Why unresolved: All experiments use Qwen2.5-0.5B as the backbone. Larger VLMs may have different internal representations, attention patterns, or LoRA fine-tuning dynamics that could affect mergeability.
- What evidence would resolve it: Experiments applying MergeVLA to 7B, 13B, or larger VLM backbones on the same benchmarks, comparing success rates and mergeability.

### Open Question 2
- Question: How does MergeVLA's performance and routing accuracy degrade as the number of merged skills increases beyond 4 tasks?
- Basis in paper: [inferred] LIBERO experiments merge only 4 tasks (Spatial, Object, Goal, Long). RoboTwin experiments merge 3 tasks. The paper shows "selfish ratio" increases with task count in Figure 3, suggesting scalability concerns.
- Why unresolved: No experiments demonstrate merging 10, 20, or 100+ skills. The test-time task router selects among M candidates, which may become unreliable as M grows. Parameter conflicts may compound.
- What evidence would resolve it: Systematic evaluation merging 8, 16, 32, and 64 tasks, measuring success rate degradation and router accuracy.

### Open Question 3
- Question: Can the expert heads (final unmergeable blocks) eventually be unified through alternative architectural designs or merging techniques, or is task-specific specialization fundamentally necessary?
- Basis in paper: [inferred] Section 4.2 states: "the deeper blocks... remain unmergeable due to their strong task specialization. Consequently, each task keeps its own expert head." This requires storing M separate heads.
- Why unresolved: The paper treats expert heads as inherently unmergeable but doesn't prove this is fundamental. Alternative approaches (shared low-rank bases, conditional computation) might unify them.
- What evidence would resolve it: Analysis of what information expert heads encode; experiments with alternative architectures that enable head merging without performance loss.

## Limitations

- Architecture details remain unspecified including exact action expert configuration and LoRA target modules
- Expert heads require storing M separate copies, limiting scalability for many tasks
- Test-time task router performance degrades on cross-embodiment tasks with diverse representations

## Confidence

- **High Confidence**: The core mechanism of task-specific masking for LoRA conflict resolution and the empirical success rates on LIBERO benchmarks are well-supported by quantitative results and ablation studies.
- **Medium Confidence**: The claim that cross-attention-only action experts preserve modularity is supported by success rate improvements, but the precise architectural impact requires further validation given unspecified implementation details.
- **Low Confidence**: The test-time task router's effectiveness on cross-embodiment tasks (RoboTwin) is less validated, with routing success dropping in more diverse embodiments, suggesting the mechanism may not generalize robustly.

## Next Checks

1. **Architecture Specification Audit**: Verify the exact number of blocks in the action expert and LoRA placement strategy by reconstructing the model from the codebase or supplementary materials.
2. **Cross-Embodiment Router Robustness**: Test the task router on a broader range of embodiment variations beyond RoboTwin to assess scalability of the value subspace projection method.
3. **Parameter Conflict Analysis**: Conduct a detailed ablation study on λ mask ratios across more than four values to map the boundary between sufficient conflict resolution and loss of task specificity.