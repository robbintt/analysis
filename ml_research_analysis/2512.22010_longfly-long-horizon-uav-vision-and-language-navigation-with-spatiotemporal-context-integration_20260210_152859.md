---
ver: rpa2
title: 'LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal
  Context Integration'
arxiv_id: '2512.22010'
source_url: https://arxiv.org/abs/2512.22010
tags:
- navigation
- longfly
- long-horizon
- historical
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-horizon UAV vision-and-language
  navigation (VLN), where maintaining consistent navigation performance over extended
  trajectories is difficult due to the lack of unified spatiotemporal context. To
  tackle this, the authors propose LongFly, a framework that models historical visual
  observations and flight trajectories as compact, structured representations.
---

# LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration

## Quick Facts
- **arXiv ID:** 2512.22010
- **Source URL:** https://arxiv.org/abs/2512.22010
- **Reference count:** 40
- **Primary result:** LongFly improves UAV VLN success rate by 7.89% and SPL by 6.33% over state-of-the-art baselines in long-horizon navigation.

## Executive Summary
LongFly addresses the challenge of long-horizon UAV vision-and-language navigation (VLN) by integrating historical visual observations and flight trajectories into a unified spatiotemporal context. The framework compresses multi-view images into fixed-length semantic slots using a slot-based attention mechanism, encodes relative motion dynamics into trajectory tokens, and aligns these with current observations and language instructions via a structured prompt. This approach significantly outperforms existing UAV VLN methods, especially in unseen environments, by preserving relevant landmarks and motion patterns over extended trajectories.

## Method Summary
LongFly introduces a three-module framework for long-horizon UAV VLN. First, a Slot-based Historical Image Compression (SHIC) module uses CLIP to encode multi-view images and iteratively aggregates them into K fixed slots via attention-weighted updates and GRU refinement, maintaining constant memory complexity. Second, a Spatiotemporal Trajectory Encoding (STE) module transforms historical waypoints into relative motion descriptors, encodes them with temporal embeddings, and projects them to trajectory tokens. Third, a Prompt-Guided Multimodal (PGM) module serializes the instruction embedding, visual slots, trajectory tokens, and current observation into a structured prompt for a frozen LLM (Qwen-2.5-3B with LoRA), which predicts 3D waypoints. The method achieves strong performance on the OpenUAV dataset across seen and unseen environments.

## Key Results
- LongFly achieves a 7.89% improvement in success rate (SR) over state-of-the-art UAV VLN baselines.
- On the Test Unseen split, LongFly reaches SR 24.19% and SPL 21.46%, outperforming all ablated variants.
- Ablation shows that prompt-guided integration is critical: removing it drops SR from 24.19% to 15.06%.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Slot-based compression enables constant-complexity visual memory that preserves semantically relevant landmarks over long horizons.
- **Mechanism:** Multi-view historical images are encoded via CLIP, then iteratively aggregated into K fixed slots using attention-weighted updates followed by GRU refinement. Slots act as queries attending to incoming visual tokens, selectively absorbing relevant features while maintaining fixed memory footprint.
- **Core assumption:** Instruction-relevant visual semantics can be preserved in a small number of slots; landmark information is more critical than pixel-level detail for navigation decisions.
- **Evidence anchors:**
  - [abstract] "dynamically distills multi-view historical observations into fixed-length contextual representations"
  - [Section III.C.1] "memory and computational complexity during inference is reduced from O(t) to O(1)"
  - [Table VIII] Slot ablation shows K=32 outperforms K=8 (SR 24.19% vs 22.81%)
- **Break condition:** If navigation requires fine-grained texture or small-object discrimination across extended time, slot capacity may bottleneck performance.

### Mechanism 2
- **Claim:** Explicit trajectory encoding provides motion priors that stabilize long-horizon path planning by grounding decisions in movement history.
- **Mechanism:** Historical waypoints are converted to relative motion descriptors (direction + scale), augmented with temporal embeddings, and projected via MLP to trajectory tokens. These tokens serve as explicit priors alongside visual context.
- **Core assumption:** Relative motion representations generalize better than absolute coordinates; temporal ordering carries decision-relevant information.
- **Evidence anchors:**
  - [Section III.C.2] "Instead of directly using absolute coordinates, we first transform waypoints into relative motion representations to reduce sensitivity to global position drift"
  - [Table V] Adding STE alone improves SR from 13.99% to 19.97% on Full split
  - [corpus] StreamVLN (FMR 0.62) uses similar slow-fast context modeling for temporal reasoning
- **Break condition:** If UAV experiences rapid position drift or sensor errors, accumulated relative motion may diverge from true trajectory.

### Mechanism 3
- **Claim:** Prompt-guided integration enables coherent multimodal reasoning by explicitly structuring language, visual memory, and trajectory context before LLM processing.
- **Mechanism:** All modalities (instruction embedding, visual slots, trajectory tokens, current observation) are projected to 2048-dim space and serialized into a structured prompt following Qwen-compatible templates. The LLM generates waypoints directly.
- **Core assumption:** Structured prompts yield better alignment than naive feature concatenation; LLM backbone can perform cross-modal reasoning without specialized fusion modules.
- **Evidence anchors:**
  - [Section III.C.3] "PGM organizes the multimodal context into a structured navigation [prompt], explicitly distinguishing different sources of information"
  - [Table VII] Removing prompt guidance drops SR from 24.19% to 15.06%
  - [corpus] CityNavAgent uses hierarchical planning with global memory—similar structured context approach
- **Break condition:** If prompt context exceeds LLM context window or modalities conflict semantically, reasoning quality may degrade.

## Foundational Learning

- **Concept: Vision-and-Language Navigation (VLN)**
  - Why needed here: LongFly extends VLN from ground/indoor to aerial long-horizon settings; understanding baseline VLN assumptions clarifies why prior methods fail on extended trajectories.
  - Quick check question: Can you explain why atomic instruction following differs from long-horizon instruction decomposition?

- **Concept: Slot-based Memory / Object-Centric Representations**
  - Why needed here: SHIC relies on slot attention principles to compress variable-length visual histories into fixed-capacity memory.
  - Quick check question: How does a slot-based architecture differ from a simple feature buffer or FIFO queue?

- **Concept: Multimodal LLM Prompting**
  - Why needed here: PGM leverages structured prompting to unify modalities; understanding prompt engineering is essential for reproducing and extending this work.
  - Quick check question: What distinguishes a structured multimodal prompt from simple token concatenation?

## Architecture Onboarding

- **Component map:** CLIP ViT-L/14 encoder -> SHIC module (K=32 slots) -> STE module -> BERT encoder (instruction) -> projection layers (2048-dim) -> Qwen-2.5-3B (frozen + LoRA) -> 3D waypoint
- **Critical path:** SHIC slot update -> trajectory token generation -> prompt assembly -> LLM inference -> 3D waypoint. Errors in slot compression propagate to all downstream reasoning.
- **Design tradeoffs:**
  - Slot count (K): Higher K retains more detail but increases memory/compute. Paper shows diminishing returns above K=32.
  - History length: Longer histories improve hard-split performance but require more context window. Paper uses all available frames.
  - Frozen vs. fine-tuned backbone: Only SHIC/STE trained; limits adaptation but improves stability.
- **Failure signatures:**
  - Sudden NE spikes on unseen maps -> visual memory insufficient for novel layouts
  - Low OSR despite high SR -> near-miss navigation, possibly trajectory encoding drift
  - Performance collapse without prompt guidance -> LLM fails to align modalities
- **First 3 experiments:**
  1. Reproduce ablation (Table V) on Test Unseen to verify SHIC vs. STE contribution independently.
  2. Sweep K ∈ {8, 16, 24, 32, 48} to find capacity ceiling on Hard split.
  3. Replace prompt template with naive concatenation to confirm structured prompting gains (per Table VII).

## Open Questions the Paper Calls Out
- **Question:** How can the framework be adapted to bridge the performance gap between generalizing to unseen objects versus unseen maps, where environmental distribution shifts currently cause significant accuracy degradation?
  - **Basis in paper:** [explicit] The authors state that "generalization to novel environments is more challenging than to novel objects" and explicitly suggest that "increasing the diversity of training environments is likely to yield larger gains."
  - **Why unresolved:** While LongFly handles unseen objects well, the Navigation Error (NE) remains high on the Test Unseen Map Set, and the authors note that the model struggles more with environmental distribution shifts than object appearance changes.
  - **Evidence:** A study applying domain adaptation techniques to the trajectory or visual encoders, or evaluating on a dataset with higher geometric diversity, would resolve this.

## Limitations
- Performance gains are concentrated in the Hard split; improvements on Easy splits are modest or negligible.
- Fixed-capacity slot memory (K=32) may bottleneck performance in environments with extreme visual complexity or high semantic density.
- Experimental validation is limited to AirSim simulator; real-world deployment under sensor noise or wind turbulence remains untested.

## Confidence
- **High:** The existence of a measurable performance gap between LongFly and prior UAV VLN methods on long-horizon settings.
- **Medium:** The claim that structured prompt-guided integration yields better alignment than concatenation-based approaches, based on ablation evidence.
- **Medium:** The assertion that slot-based compression is critical for maintaining performance over extended trajectories, though capacity limits are hinted but not fully explored.

## Next Checks
1. Conduct a controlled ablation varying only the history length (keeping K=32) to measure the marginal benefit of longer trajectory encoding.
2. Test whether replacing the frozen CLIP backbone with a fine-tuned version further improves performance, especially on unseen environments.
3. Evaluate the robustness of LongFly to simulated sensor drift or noisy waypoint estimates by perturbing the trajectory history during inference.