---
ver: rpa2
title: An Arbitration Control for an Ensemble of Diversified DQN variants in Continual
  Reinforcement Learning
arxiv_id: '2509.04815'
source_url: https://arxiv.org/abs/2509.04815
tags:
- learning
- agents
- arbitration
- each
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in continual reinforcement
  learning (CRL) by proposing ACED-DQN, an ensemble of diversified DQN variants with
  arbitration control. The core method uses an ensemble of five DQN variants (vanilla
  DQN, Double DQN, Noisy DQN, Dueling DQN, and Distributional DQN), each trained with
  its own value function.
---

# An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.04815
- Source URL: https://arxiv.org/abs/2509.04815
- Reference count: 40
- Primary result: ACED-DQN achieves 50.5% win rate vs. Rainbow (10.6%) in CRL Atari

## Executive Summary
The paper addresses catastrophic forgetting in continual reinforcement learning (CRL) by proposing ACED-DQN, an ensemble of five diversified DQN variants with arbitration control. The core innovation uses MSE-based arbitration to dynamically select the most reliable agent, combined with agency-based sampling to mitigate the "curse of diversity." Tested across 26 Atari games with varying objectives and action randomness, ACED-DQN significantly outperforms state-of-the-art CRL methods, achieving a 50.5% win rate.

## Method Summary
ACED-DQN combines five DQN variants (vanilla, Double, Noisy, Dueling, and Distributional) into an ensemble. Arbitration control dynamically selects the most reliable agent based on prediction error (MSE), while agency-based sampling ensures agents train primarily on experiences they generated. The method maintains a reliability score for each agent, calculated as a softmax over negative MSE, and uses these scores to weight Q-value aggregation during action selection. Transitions are assigned to agents based on which had the lowest error for that transition, reducing interference from highly off-policy data.

## Key Results
- ACED-DQN achieves 50.5% win rate across 26 Atari games, significantly outperforming Rainbow (10.6%), SUNRISE (16.3%), and ACED-Rainbow (22.6%)
- Arbitration control is identified as the key factor driving performance gains
- The ensemble approach effectively addresses catastrophic forgetting across varying objectives and action randomness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Arbitration control dynamically selects the most reliable agent for action selection based on recent prediction error (MSE), mitigating performance drops during environmental shifts.
- **Mechanism:** The system maintains a reliability score $R_i$ for each DQN variant. This score is calculated as a softmax over the negative MSE of recent predictions ($R \propto e^{-L/T}$). During action selection, instead of averaging Q-values or picking randomly, the system weights Q-values by these reliability scores, effectively "listening" to the agent with the lowest prediction error in the current context.
- **Core assumption:** MSE serves as a valid proxy for "reliability" or "fit" in a non-stationary environment; agents with lower current TD error have more accurate value estimates for the active task.
- **Evidence anchors:**
  - [abstract] "Arbitration control dynamically selects the most reliable agent based on prediction error (MSE)."
  - [section 3.2.1] "To adaptively focus on reliable agents, we compute weights... based on reliability $R_i$."
  - [corpus] Weak/missing. Neighbor papers (e.g., GB-DQN) discuss ensembles for non-stationary learning but do not specifically validate MSE-based arbitration.
- **Break condition:** If environmental shifts are so rapid that the "momentum" update of reliability ($\gamma$) lags behind the change, the arbitrator may persistently select an obsolete agent.

### Mechanism 2
- **Claim:** Agency-based sampling mitigates the "curse of diversity" by ensuring agents train primarily on experiences they generated, reducing interference from highly off-policy data.
- **Mechanism:** Rather than sampling from a uniform shared buffer (where Agent A might train on Agent B's incompatible experience), transitions are assigned to agents based on which agent had the lowest error (highest reliability) for that transition. This aligns the training distribution with the agent's behavioral policy.
- **Core assumption:** Enforcing this "sense of agency" reduces gradient conflict and stabilizes the value function updates better than standard prioritized experience replay (PER) alone.
- **Evidence anchors:**
  - [section 2.3] "Curse of diversity... arises from the low proportion of self-generated data."
  - [section 3.2.2] "We refer to this reliability-based sampling method as agency-based sampling."
  - [corpus] Weak/missing. While neighbor papers discuss ensemble stability, they do not explicitly test "agency-based sampling."
- **Break condition:** If the optimal policy requires learning from counter-factual data generated by a completely different strategy, restricting agents to "self-generated" experiences might limit theoretical performance ceilings.

### Mechanism 3
- **Claim:** Heterogeneous DQN variants (e.g., Noisy vs. Distributional) naturally provide robust coverage across varying task contexts without explicit architecture search.
- **Mechanism:** Different DQN variants optimize for different objectives or inductive biases (e.g., Dueling for state-values, Noisy for exploration). In CRL, the "optimal" agent changes as the environment changes. By maintaining structural diversity, the ensemble ensures at least one agent is likely well-suited for any given environmental block.
- **Core assumption:** The diversity of value functions persists even under shared training constraints, and the "optimal" agent for a specific context correlates with the specific architectural strengths (e.g., Distributional DQN for sparse rewards).
- **Evidence anchors:**
  - [section 4.1] "The optimal DQN variant differs not only across tasks but also across blocks."
  - [supplementary 2.4] "Divergence emerges from Learning Dynamics and Variance."
  - [corpus] [Ensemble Elastic DQN] supports the general claim that diverse ensembles address overestimation/stability.
- **Break condition:** If all variants suffer from the same fundamental "blind spot" (e.g., all fail to handle stochasticity), arbitration fails as no agent is reliable.

## Foundational Learning

- **Concept:** Stability-Plasticity Dilemma
  - **Why needed here:** This is the core problem CRL attempts to solve. You must understand the trade-off between "remembering old tasks" (Stability) and "learning new tasks" (Plasticity) to see why ACED-DQN uses an ensemble rather than a single network.
  - **Quick check question:** Does ACED-DQN solve stability by preventing weight overwriting or by selecting a different pre-trained agent?

- **Concept:** The Curse of Diversity (in Ensemble RL)
  - **Why needed here:** This is the specific failure mode of standard ensembles in RL that this paper claims to fix.
  - **Quick check question:** Why does training on a shared replay buffer degrade performance when ensemble members have diverse policies?

- **Concept:** Value Function Approximation (Q-Learning)
  - **Why needed here:** The mechanism relies on comparing Q-value errors (MSE) across variants. You need to know what a Q-value represents to interpret "reliability."
  - **Quick check question:** If an agent has high MSE (prediction error), what does that imply about its Q-value estimates for the current state?

## Architecture Onboarding

- **Component map:** Agents (5 DQN variants) -> Arbitrator (reliability calculator) -> Sampler (agency-based loader) -> Buffer (shared replay memory)

- **Critical path:**
  1. **Inference:** State $s \to$ All 5 Agents predict Q-values.
  2. **Selection:** Arbitrator computes weights $w_i$ based on historical error; executes $a = \text{argmax} \sum w_i Q_i(s, \cdot)$.
  3. **Storage:** Store $(s, a, r, s')$ in Buffer $D$.
  4. **Training:** Sample batch; compute losses; update networks.

- **Design tradeoffs:**
  - **Memory vs. Stability:** You run 5 networks (high memory/compute) to gain CRL stability without complex regularization losses.
  - **Responsiveness vs. Noise:** The temperature $T$ and momentum $\gamma$ in reliability calculation determine how fast the arbitrator switches agents. Low $T$ = jittery switching; High $T$ = slow adaptation.

- **Failure signatures:**
  - **Collapse:** Reliability scores flatten (all agents equally bad/good), causing the system to behave like a naive average.
  - **Oscillation:** Reliability swings wildly between agents every few steps, indicating the temperature $T$ is too low or the environment is too stochastic.

- **First 3 experiments:**
  1. **Baseline vs. Random Arbitration:** Run the ACED architecture but replace the MSE-based arbitrator with a uniform random selector to verify that *intelligent* selection is the causal factor.
  2. **Block-wise Isolation:** Train individual variants on specific environmental blocks to confirm the hypothesis that "No single agent wins all blocks" (validating the need for an ensemble).
  3. **Ablate Agency Sampling:** Disable agency-based sampling (use uniform PER) to measure performance drop specifically attributed to the "curse of diversity."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ACED-DQN framework be effectively adapted for continuous control environments like MuJoCo?
- Basis in paper: [explicit] The discussion section states the framework is generalizable to continuous control domains, although the evaluation focused exclusively on discrete Atari settings.
- Why unresolved: The current implementation relies on discrete DQN variants, whereas continuous domains typically require different algorithmic structures like actor-critic methods.
- What evidence would resolve it: Empirical results from ACED-adapted continuous algorithms (e.g., ACED-DDPG or ACED-SAC) tested on standard continuous CRL benchmarks.

### Open Question 2
- Question: Is there a formal convergence guarantee for ACED-DQN in non-stationary, off-policy CRL settings?
- Basis in paper: [explicit] The authors acknowledge that a general convergence guarantee for practical DQN remains an "open problem" due to unfixed target networks.
- Why unresolved: The provided theoretical analysis is limited to a finite, tabular setting, leaving the stability of the deep neural network implementation theoretically unproven.
- What evidence would resolve it: A mathematical proof establishing bounded error or convergence criteria for the ensemble arbitration mechanism in non-stationary environments.

### Open Question 3
- Question: How does the arbitration control mechanism perform when applied to policy-based reinforcement learning methods rather than value-based variants?
- Basis in paper: [explicit] The authors note that generalizing the framework involves "adapting the reliability metric and arbitration mechanism to corresponding policy and its loss."
- Why unresolved: The current method calculates reliability based on value function error (MSE), which may not translate directly or effectively to policy-gradient loss landscapes.
- What evidence would resolve it: Implementation of ACED with policy gradient variants (e.g., PPO or TRPO) demonstrating comparable mitigation of catastrophic forgetting.

## Limitations

- **Implementation complexity:** Requires running and synchronizing five separate DQN variants, increasing computational overhead significantly
- **Unknown scaling:** The method's effectiveness on more complex, longer-horizon CRL problems with many more than five tasks remains untested
- **Reward shaping ambiguity:** The exact implementation details of the modified reward functions for different Atari games are not fully specified mathematically

## Confidence

- **High confidence** in the core arbitration mechanism (MSE-based reliability weighting) and its contribution to performance gains, as ablation studies isolate this effect
- **Medium confidence** in the novelty claim, as while the combination of arbitration with agency-based sampling is unique, the underlying components (MSE-based selection, PER sampling) are established
- **Medium confidence** in the generalizability, as results show strong performance on 26 Atari games but only within the specific CRL setup with 200K-step blocks

## Next Checks

1. **Reward Function Verification:** Implement and validate the exact reward shaping logic described in Table 1 to ensure fair comparison with baselines
2. **Temperature Sensitivity Analysis:** Systematically test different values of T in the softmax reliability calculation to quantify the tradeoff between responsiveness and stability
3. **Generalization Beyond Atari:** Test the ACED-DQN architecture on non-Atari CRL benchmarks (e.g., DMLab, control tasks) to assess whether the arbitration control provides consistent benefits across domains