---
ver: rpa2
title: 'Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented
  Grammar Books'
arxiv_id: '2506.01796'
source_url: https://arxiv.org/abs/2506.01796
tags:
- rules
- rule
- grammar
- zhuang
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the effectiveness of grammar books in
  translating extremely low-resource languages by decomposing the task into two key
  steps: grammar rule retrieval and application. The authors introduce ZHUANG RULES,
  a modularized dataset of 109 Zhuang grammar rules with parallel sentences, to enable
  controlled and interpretable evaluation.'
---

# Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books

## Quick Facts
- arXiv ID: 2506.01796
- Source URL: https://arxiv.org/abs/2506.01796
- Reference count: 40
- Primary result: Grammar rule retrieval is a bottleneck in low-resource translation; code-based rules significantly improve both retrieval (8.8% recall gain) and application (12.2% BLEU improvement).

## Executive Summary
This paper addresses the challenge of translating extremely low-resource languages by leveraging grammar books as external knowledge sources. The authors decompose the translation task into two distinct steps: retrieving the relevant grammar rule for a given sentence, and then applying that rule to produce the translation. They introduce ZHUANG RULES, a dataset of 109 Zhuang grammar rules with parallel sentences, to enable controlled evaluation. The key innovation is representing grammar rules as code functions rather than natural language, which significantly improves LLM performance on both retrieval and application stages, ultimately achieving a 13.1% BLEU improvement in translation quality.

## Method Summary
The method consists of three main stages: (1) converting textual grammar rules to pseudocode functions using GPT-4o with 5-shot in-context learning, (2) performing rule-by-rule retrieval where an LLM classifies each rule as relevant or irrelevant for a given sentence, and (3) applying the retrieved code rules to translate the sentence using a provided lexicon and optionally two parallel examples. The pipeline uses modularized prompts and experiments with both Qwen-2.5-7B/72B-Instruct and Llama-3.1-70B-Instruct models. The approach is validated on both Zhuang (109 rules, 608 pairs) and Kalamang (97 rules, 152 pairs) languages.

## Key Results
- Rule-by-rule retrieval achieves 79.9% recall compared to 73.2% for full-book retrieval
- Code-based rules improve rule retrieval recall by 8.8% over text rules
- Code-based rules improve translation BLEU by 12.2% over text rules
- The combined approach achieves 13.1% BLEU improvement on average across languages
- Improvements are particularly pronounced for complex "hard" rules requiring multiple operations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing grammar rules as pseudocode aligns linguistic operations with procedural logic, potentially reducing ambiguity in rule application.
- **Mechanism:** Code structures (e.g., `if-else` blocks for conditions, variable reassignment for word reordering) map isomorphically to morphological and syntactic transformations. This structural alignment may allow the model to utilize its code-training priors to execute "functions" rather than loosely interpreting natural language descriptions.
- **Core assumption:** The LLM has sufficient code pre-training to treat the pseudocode as executable logic rather than just text.
- **Evidence anchors:**
  - [Page 2] "Adding affixes... resembles arithmetic addition... selecting different affixes... aligns with an if-else structure."
  - [Page 7] "Code formats enhance rule understanding, especially for difficult rules... average improvement of 8.5% chrF++."
  - [Corpus] Weak direct support; neighbors focus on general low-resource ICL, not code-augmented grammar.
- **Break condition:** If a grammar rule is purely semantic or idiomatic (e.g., metaphor) and lacks a discrete procedural operation, the code representation may fail to capture the nuance.

### Mechanism 2
- **Claim:** Decomposing translation into "Retrieval" and "Application" isolates the context-management bottleneck from the reasoning bottleneck.
- **Mechanism:** Standard prompting dumps all rules into the context, causing the "lost-in-the-middle" phenomenon where models ignore relevant rules. By forcing a binary classification step ("Rule-by-Rule"), the model only needs to process short contexts, significantly reducing retrieval noise before attempting translation.
- **Core assumption:** The retrieval step (identifying the right rule) is a necessary precondition for successful application, and the model cannot implicitly retrieve while generating.
- **Evidence anchors:**
  - [Page 3] "Translation performance declines sharply when we... add more irrelevant rules."
  - [Page 6] "Rule-by-Rule... makes it a more practical solution... achieving nearly 80% recall."
  - [Corpus] "Compensating for Data with Reasoning" supports the general trend of decomposing reasoning in LLMs.
- **Break condition:** If the cost of linearly scanning rules (Rule-by-Rule) becomes prohibitive due to massive grammar books (e.g., >1000 rules), latency becomes a blocker.

### Mechanism 3
- **Claim:** Code-augmented rules improve "hard" rule performance by scaffolding multi-step reasoning.
- **Mechanism:** "Hard" rules require multiple actions (e.g., reorder + affix). Natural language instructions for multi-step operations are often compressed. Pseudocode explicitly serializes these steps (Step 1, Step 2), acting as a Chain-of-Thought prompt that forces the model to execute operations in sequence.
- **Core assumption:** Performance gains are driven by the explicit serialization of logic, not merely the novelty of the code syntax.
- **Evidence anchors:**
  - [Page 7] "Improvement of code rules... is particularly noticeable on the hard subset."
  - [Table 4] Hard rules show a +14.0 chrF++ delta for Code Rules vs Text Rules (Zhuang->Chinese).
  - [Corpus] "Reasoning Transfer" (neighbor paper) aligns with the idea that structured reasoning aids low-resource tasks.
- **Break condition:** If the model's context window is exceeded by the verbose nature of serialized code for very complex rules.

## Foundational Learning

- **Concept:** **In-Context Learning (ICL)**
  - **Why needed here:** This method does not update weights; it relies entirely on the model's ability to map input patterns (rules) to outputs without gradient descent. Understanding the fragility of ICL is key to grasping why "retrieval" is a bottleneck.
  - **Quick check question:** How does the model's performance change if the correct rule is placed in the middle of the prompt versus the beginning?

- **Concept:** **Procedural vs. Declarative Knowledge**
  - **Why needed here:** The paper argues that grammar books (declarative) are hard to apply, whereas code rules (procedural) are easier. Distinguishing between "knowing that" (text description) and "knowing how" (function execution) explains the performance gap.
  - **Quick check question:** Can you distinguish between a declarative grammar rule ("Adjectives follow nouns") and a procedural pseudocode implementation (`order(noun, adj)`)?

- **Concept:** **Interlinear Glossed Text (IGT)**
  - **Why needed here:** The paper uses IGT as an auxiliary element. It serves as a "bridge" representation between the source and target, which is a standard technique in low-resource NLP but distinct from the code-augmentation strategy.
  - **Quick check question:** Why might IGT help a model understand morphology better than a raw parallel sentence?

## Architecture Onboarding

- **Component map:** Grammar Book (Text) -> Converter (GPT-4o) -> Pseudocode Rules -> Retriever (LLM binary classifier) -> Retrieved Code Rules -> Applicator (LLM with lexicon) -> Output Translation
- **Critical path:** The **Converter** is the most critical dependency. If the generated pseudocode is syntactically invalid or logically misrepresents the text rule, the downstream Applicator will fail.
- **Design tradeoffs:**
  - **Accuracy vs. Token Cost:** Code rules significantly increase token count (approx. 5x-7x per rule) compared to text, trading context efficiency for reasoning reliability.
  - **Speed vs. Recall:** "Rule-by-Rule" retrieval requires $N$ forward passes for $N$ rules, whereas "Full-Book" requires 1 pass. The architecture trades inference latency for higher retrieval accuracy.
- **Failure signatures:**
  - **Lexical Leakage:** The model ignores grammar and just uses the provided lexicon/dictionary (addressed by the dataset design but still a risk).
  - **Hallucinated Logic:** The Applicator executes a code rule that doesn't actually match the input sentence structure (e.g., applying a plural affix to a singular noun).
- **First 3 experiments:**
  1. **Ablation on Rule Format:** Compare `Text Rule Only` vs. `Code Rule Only` on the "Hard" subset of ZhuangRules to verify the reasoning scaffolding hypothesis.
  2. **Retrieval Stress Test:** Plot translation quality (BLEU) vs. Number of Distractor Rules to confirm the "Retrieval Bottleneck" curve shown in Figure 2.
  3. **Multi-Rule Composition:** Implement the `INLINE (LLM)` strategy (Figure 3) to test if the model can autonomously merge two code rules for sentences requiring complex transformations.

## Open Questions the Paper Calls Out

- **Question:** Do code-augmented grammar rules generalize to low-resource languages with diverse typological features or slightly higher data availability?
  - **Basis in paper:** [Explicit] The authors explicitly limit the scope to Zhuang and Kalamang and encourage "future work to explore whether our findings generalize to low-resource languages with slightly more data availability."
  - **Why unresolved:** The study focuses on two specific XLR languages; it is unclear if the benefits of code representations transfer to languages with significantly different morphological structures (e.g., polysynthetic languages).
  - **What evidence would resolve it:** Applying the code-augmented retrieval and application framework to a broader benchmark of low-resource languages outside the Kra-Dai and Greater West Bomberai families.

- **Question:** How does the code-based approach scale to open-ended translation scenarios requiring the simultaneous application of multiple complex grammar rules?
  - **Basis in paper:** [Explicit] The authors note their primary experiments use single-rule instances and state, "We leave the exploration of more open-ended and unconstrained translation settings for future work."
  - **Why unresolved:** While a preliminary experiment on two-rule instances was successful, real-world translation often necessitates chaining numerous rules, which may introduce compounding errors or context window limits.
  - **What evidence would resolve it:** Evaluation on a dataset where test instances require 3 or more grammar rules, comparing the efficacy of different code composition strategies (e.g., INLINE vs. FUNC CALL).

- **Question:** Can LLMs effectively distill code-augmented grammar rules directly from unordered parallel corpora without relying on pre-existing grammar books?
  - **Basis in paper:** [Explicit] Appendix D.3 proposes that "future work can explore the possibility of using LLMs to distill descriptive grammar from a larger, unordered corpus of parallel data."
  - **Why unresolved:** The current method relies on converting human-written rules; the ability of LLMs to autonomously induce accurate code-style rules from raw data remains unverified.
  - **What evidence would resolve it:** A study where LLMs generate code rules from parallel text alone, followed by a comparison of translation performance between these "induced" rules and the human-curated gold rules.

## Limitations

- **Data Generalization:** The Zhuang dataset (109 rules, 608 sentences) is substantially smaller than typical MT benchmarks, limiting generalizability to other low-resource languages.
- **Conversion Reliability:** Rule-to-code conversion relies on a single 5-shot GPT-4o prompt without reported error rates or human evaluation of semantic preservation.
- **Retrieval Cost:** Rule-by-rule retrieval requires N forward passes for N rules, making it potentially impractical for grammar books with hundreds of rules.

## Confidence

**High Confidence:** The decomposition of translation into retrieval and application stages is methodologically sound, and the observed retrieval bottleneck is well-supported by controlled experiments showing declining performance as distractor rules increase.

**Medium Confidence:** The code-augmentation mechanism's effectiveness is supported by experimental results but lacks ablation studies on the conversion process itself. The 12.2% BLEU improvement is significant but may not generalize to languages with different grammatical structures.

**Low Confidence:** Claims about code rules being inherently more interpretable or easier for LLMs to reason with are not directly tested. The paper assumes the LLM treats pseudocode as procedural logic without empirical validation of this assumption.

## Next Checks

1. **Conversion Error Analysis:** Implement systematic evaluation of the rule-to-code conversion quality by sampling 50 converted rules and having linguists verify semantic preservation. Report precision, recall, and F1 of the conversion process.

2. **Scaling Study:** Evaluate the retrieval strategy on progressively larger rule sets (50, 100, 200, 500 rules) to measure the trade-off between recall improvement and inference cost. Plot BLEU vs. inference time to identify the practical scaling limit.

3. **Cross-Linguistic Generalization:** Apply the exact methodology to a third low-resource language (e.g., Navajo or Ainu) and report whether the 13.1% average improvement holds. This would test whether the code-augmentation benefit is language-agnostic or specific to Zhuang/Kalamang grammar structures.