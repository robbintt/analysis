---
ver: rpa2
title: 'Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological
  Sequences'
arxiv_id: '2503.16351'
source_url: https://arxiv.org/abs/2503.16351
tags:
- lyra
- sequence
- self
- prediction
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Lyra is a subquadratic deep learning architecture for biological\
  \ sequence modeling that leverages the mathematical structure of epistasis (context-dependent\
  \ interactions between sequence elements) to efficiently capture complex sequence-to-function\
  \ relationships. The model combines projected gated convolutions for local feature\
  \ extraction with state space models (specifically S4D) for long-range dependencies,\
  \ achieving subquadratic O(N log N) scaling compared to the O(N\xB2) complexity\
  \ of Transformers."
---

# Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences

## Quick Facts
- **arXiv ID:** 2503.16351
- **Source URL:** https://arxiv.org/abs/2503.16351
- **Reference count:** 3
- **Primary result:** Achieves state-of-the-art performance across 100+ biological tasks using up to 120,000-fold fewer parameters than Transformers while training in under two hours on two GPUs

## Executive Summary
Lyra is a subquadratic deep learning architecture specifically designed for biological sequence modeling that leverages the mathematical structure of epistasis (context-dependent interactions between sequence elements) to efficiently capture complex sequence-to-function relationships. The model combines projected gated convolutions for local feature extraction with state space models (specifically S4D) for long-range dependencies, achieving subquadratic O(N log N) scaling compared to the O(N²) complexity of Transformers. Lyra outperforms existing approaches across diverse biological domains including protein fitness prediction, RNA function analysis, CRISPR guide design, and biophysical property prediction while dramatically reducing computational requirements.

## Method Summary
Lyra replaces traditional Transformer architectures with a combination of Projected Gated Convolutions (PGC) and Diagonalized State Space Models (S4D). The PGC blocks process local sequence features through depthwise convolutions combined with linear projections via element-wise multiplication, explicitly modeling second-order interactions. The S4D layer captures long-range dependencies by mapping sequence relationships to polynomial functions in the frequency domain, efficiently approximating epistatic interactions without the quadratic cost of attention. The architecture uses standard training procedures with AdamW optimizer and achieves inference speedups by exploiting the algebraic properties of circulant matrices through FFT operations rather than explicit attention computations.

## Key Results
- Achieves state-of-the-art performance across over 100 biological tasks spanning protein fitness, RNA function, CRISPR design, and biophysical properties
- Reduces parameter count by up to 120,000-fold compared to Transformer baselines while maintaining or improving accuracy
- Trains in under two hours on two GPUs versus days required for foundation models
- Demonstrates subquadratic O(N log N) scaling that enables processing of significantly longer sequences (up to 65,536 tokens) than quadratic-attention models

## Why This Works (Mechanism)

### Mechanism 1: Polynomial Epistasis Approximation
Lyra maps biological sequence relationships to polynomial functions in the frequency domain using diagonalized state space models. This mathematical formulation allows the model to efficiently approximate epistatic interactions (where one mutation affects another) as multilinear polynomials, achieving subquadratic complexity compared to attention mechanisms.

### Mechanism 2: Data-Efficient Local Feature Extraction
The Projected Gated Convolution (PGC) block explicitly models second-order interactions by combining depthwise convolutions with linear projections through element-wise multiplication. This gating mechanism captures local epistatic effects with fewer parameters than dense layers, making the model more parameter-efficient.

### Mechanism 3: FFT-Based Efficiency
By formulating global interactions as state space models that reduce to convolutions, Lyra exploits Fast Fourier Transform operations for O(N log N) complexity. This avoids the O(N²) matrix multiplications required by Transformer attention, enabling processing of longer sequences within memory constraints.

## Foundational Learning

- **Epistasis (Biological)**: The phenomenon where the effect of a mutation at one position depends on mutations at other positions. Understanding this non-additive effect is crucial for grasping why Lyra uses polynomial approximators and local interaction mechanisms.

- **State Space Models (SSMs)**: Mathematical frameworks that map sequences to latent state evolution through differential equations. Understanding SSMs is essential for comprehending how Lyra achieves efficiency by converting global interactions to convolution operations.

- **Gating Mechanisms**: Operations that combine two vectors through multiplication (e.g., element-wise product), allowing selective filtering of information. The PGC component relies on gating to capture context-dependent local features.

## Architecture Onboarding

- **Component map:** Input Sequence → Linear Encoder → PGC Blocks → S4D Layer → Pooling → Decoder

- **Critical path:** The initialization of the S4D A-matrix is critical. The real parts control decay and imaginary parts control frequency, and incorrect initialization can cause training instability or failure to capture long-range dependencies.

- **Design tradeoffs:** Lyra trades the "perfect memory" of quadratic attention for subquadratic efficiency. While faster, it may struggle with "needle in a haystack" retrieval tasks compared to full attention models. The model uses deep stacking of small dimensions, requiring careful normalization to prevent vanishing gradients.

- **Failure signatures:** Training divergence often stems from S4D state matrix instability. Underfitting local motifs may occur if PGC kernel size is too small or gating saturates. Slow inference can result from not using optimized FFT kernels.

- **First 3 experiments:**
  1. Train a minimal Lyra instance on synthetic polynomial data to verify the polynomial approximation capability versus Transformer baseline.
  2. Run an ablation study on GFP fluorescence prediction with PGC-only and S4D-only variants to quantify local vs. global interaction contributions.
  3. Benchmark memory usage and latency against ESM-1b on specific hardware at sequence lengths 512, 1024, and 4096 to validate subquadratic scaling claims.

## Open Questions the Paper Calls Out

- Can Lyra leverage pre-training on massive biological datasets to achieve further gains over current foundation models, or are architectural innovations more valuable than increasing model scale?

- Is the polynomial expressivity of Lyra advantageous for non-biological sequence modeling tasks such as natural language or time-series forecasting?

- How does Lyra's performance scale on tasks with significantly longer context windows than those tested, particularly for extremely long genomic sequences?

## Limitations

- The polynomial approximation assumption for epistasis may not hold for all biological systems, particularly those with highly discontinuous or non-polynomial sequence-function relationships.

- Performance on extremely long-range dependencies beyond 65k tokens remains untested, leaving uncertainty about practical scaling limits.

- Some specialized tasks requiring perfect recall or extremely complex motif detection might still favor full attention mechanisms over Lyra's subquadratic approach.

## Confidence

- **High Confidence:** Subquadratic O(N log N) scaling claims and parameter efficiency improvements are well-supported by theoretical analysis and align with established SSM literature.

- **Medium Confidence:** Performance superiority claims across 100+ biological tasks are credible given extensive benchmarking, though specific datasets and protocols lack full detail.

- **Low Confidence:** The assertion that Lyra can fully replace large foundation models in all biological sequence analysis contexts may be overstated for specialized retrieval tasks.

## Next Checks

1. Design a synthetic dataset with known polynomial and non-polynomial epistatic relationships to compare Lyra's approximation capacity against Transformer and pure polynomial regression baselines.

2. Evaluate Lyra's performance on biological sequences exceeding 65k tokens (e.g., full-length chromosomes) to identify practical scaling limits and compare against attention-based models.

3. Reproduce scaling and efficiency claims across multiple hardware configurations (consumer GPUs, cloud instances, specialized accelerators) to verify theoretical O(N log N) improvements consistently manifest in practice.