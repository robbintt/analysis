---
ver: rpa2
title: 'Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1'
arxiv_id: '2508.10173'
source_url: https://arxiv.org/abs/2508.10173
tags:
- reasoning
- learning
- task
- tasks
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that performance improvements in reasoning
  language models, such as DeepSeek-R1-0528, can be attributed not only to algorithmic
  enhancements and model scaling but also to using high-impact benchmarks like Humanity's
  Last Exam as training curricula. By comparing DeepSeek-R1-0528 with Phi-4-reasoning-plus
  and the original DeepSeek-R1, the authors show that DeepSeek-R1-0528's superior
  performance on their sequential decision-making test task largely results from targeted
  improvements on tasks included in the benchmark.
---

# Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1

## Quick Facts
- arXiv ID: 2508.10173
- Source URL: https://arxiv.org/abs/2508.10173
- Authors: Petr Spelda; Vit Stritecky
- Reference count: 5
- Key outcome: Performance improvements in reasoning models can be attributed to using high-impact benchmarks as training curricula rather than purely algorithmic enhancements.

## Executive Summary
This study demonstrates that performance gains in DeepSeek-R1-0528 compared to its predecessor and a contemporaneous model can be attributed to exposure to Humanity's Last Exam (HLE) benchmark data rather than purely algorithmic improvements. By analyzing a specific sequential decision-making task from HLE, the authors show that DeepSeek-R1-0528's superior performance likely results from targeted improvements on tasks included in the benchmark. This "benchmark-driven selection of AI" phenomenon suggests that some public benchmarks may function more as learning curricula than as purely evaluative tools, raising questions about the validity of using such benchmarks to measure model generalization capabilities.

## Method Summary
The study compares three reasoning models (DeepSeek-R1-0528, DeepSeek-R1, and Phi-4-reasoning-plus) on a sequential decision-making task from Humanity's Last Exam using 64 independent samples per model. Human evaluators score responses based on conceptual correctness of answers and explanations. The analysis uses Bayesian credible intervals to quantify uncertainty and posterior probabilities to compare model pairs. The core hypothesis is that DeepSeek-R1-0528's performance gains on this specific task result from exposure to HLE data during training, representing a form of curriculum learning rather than general capability improvement.

## Key Results
- DeepSeek-R1-0528 outperforms both its predecessor (DS-R1) and contemporaneous model (Phi-4-r+) on the specific HLE sequential decision-making task
- The performance gain on this task cannot be explained by model scaling alone, as Phi-4-r+ is smaller but was released after DS-R1
- Human evaluation reveals that models may produce conceptually correct answers that automated judges would miss, particularly for novel tasks

## Why This Works (Mechanism)
The study leverages the chronological release of models and public benchmarks to infer training data exposure. DeepSeek-R1-0528 was released after HLE became public, while DS-R1 was released before. By comparing performance on a specific HLE task, the authors isolate the effect of potential benchmark exposure. The use of human evaluation rather than automated judging is crucial for detecting conceptual correctness that may use non-standard terminology, which is particularly important when comparing models with different training exposures.

## Foundational Learning

**Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
- Why needed here: RLVR is the core training method for modern reasoning models like DeepSeek-R1. It uses tasks with objectively verifiable solutions (like math or code) to provide reward signals, shaping the model's reasoning traces.
- Quick check question: Can you explain how a model learns a "reasoning trace" if the intermediate steps have no ground truth, only the final answer?

**Concept: Chain-of-Thought (CoT) Reasoning**
- Why needed here: This is the behavior the models are trained to produceâ€”generating intermediate steps before a final answer. The paper analyzes these traces to understand model exploration and performance.
- Quick check question: What is the difference between a model simply generating more text and genuinely using a chain of thought to decompose a problem?

**Concept: Distribution Shift in Evaluation**
- Why needed here: The paper's core argument is about generalization. A key challenge is that a model trained on one distribution of tasks (e.g., from a benchmark) may fail when the task distribution shifts slightly, indicating poor generalization.
- Quick check question: If a model is trained on multiple-choice questions, why might its performance drop if the task is changed to require it to generate the answer from scratch?

## Architecture Onboarding

**Component map:** Control Model (DS-R1) -> Experimental Model (DS-R1-0528) -> Test Task (HLE sequential decision-making) -> Evaluation Pipeline (human scoring + Bayesian analysis)

**Critical path:** The analysis hinges on the chronological comparison. If DS-R1-0528 outperforms both Phi-4-r+ (a smaller, contemporaneous model) and DS-R1 (its own predecessor), the primary variable to investigate is exposure to the test task via the public benchmark.

**Design tradeoffs:**
- **Evaluation Method:** The authors chose human evaluation over an AI judge. This is more resource-intensive and less scalable but crucial for detecting conceptual correctness that an automated judge might miss, especially for novel tasks where the model's terminology may differ from the ground truth.
- **Sample Size:** 64 samples per model is small, which informed the choice of Bayesian credible intervals over confidence intervals. This provides better uncertainty estimates for limited data but acknowledges statistical uncertainty.

**Failure signatures:**
- The performance gain on the test task vanishes or is statistically insignificant when comparing DS-R1 and DS-R1-0528
- A strong, positive correlation is found between reasoning token count and success rate across all models, invalidating the claim that more tokens don't equal better performance on this task

**First 3 experiments:**
1. **Re-run with Novel Tasks:** Create and evaluate models on entirely new, private tasks from the same domain (sequential decision-making) that are guaranteed to be unseen. This tests if the "benchmark-driven" effect generalizes.
2. **Automated Evaluation Comparison:** Run the same model responses through an AI judge (like the one used by HLE) and compare the scores to the human evaluation. Quantify what is missed by automation.
3. **Analyze Forgetting:** Test if DS-R1-0528's improvement on the HLE task came at the cost of performance degradation on other, older benchmarks that were not part of its new curriculum. This checks for capability trade-offs.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can benchmark-driven selection of AI enable Large Reasoning Models (LRMs) to generalize to tasks from different distributions within the reasoning domain?
- Basis in paper: [explicit] (Page 9: "Our results beg the following question. Is it possible to make LRMs to generalize to tasks from different distributions...?")
- Why unresolved: The authors note evidence that models trained via benchmark-driven selection struggle with even minor distribution shifts, such as format changes.
- What evidence would resolve it: Evaluating models known to be trained on specific benchmarks against modified versions of those tasks that introduce controlled distribution shifts.

**Open Question 2**
- Question: To what extent does improved generalization on novel mathematical competition tasks transfer to other domains or general reasoning capabilities?
- Basis in paper: [explicit] (Page 10: "It is unclear how improving the generalization capability on this type of task transfers to other areas...")
- Why unresolved: While math competitions are currently used to test frontier capabilities, their utility as a proxy for broader reasoning skills remains unverified.
- What evidence would resolve it: Comparative studies measuring the correlation between performance on unseen math competitions and performance on diverse, non-mathematical reasoning benchmarks.

**Open Question 3**
- Question: How can the processes of benchmark selection and revision be made robust to manipulation and democratically governed?
- Basis in paper: [explicit] (Page 11: "Equally important will be efforts that would make the processes of selection and revision of these benchmarks robust to manipulation...")
- Why unresolved: The paper identifies a lack of transparency in model development and the risk of benchmarks being treated as curricula, necessitating new governance models.
- What evidence would resolve it: The proposal and simulation of benchmark governance frameworks that successfully prevent overfitting or "gaming" by model developers while maintaining diverse task representation.

## Limitations

- The study cannot definitively prove that DeepSeek-R1-0528 was trained on HLE data due to lack of direct access to training data composition
- The analysis relies on a single test task from HLE, representing a limited sample of the benchmark's overall content
- The small sample size (64 per model) introduces statistical uncertainty that necessitates Bayesian methods

## Confidence

**High Confidence:** The empirical finding that DeepSeek-R1-0528 outperforms both its predecessor (DS-R1) and a contemporaneous model (Phi-4-r+) on the specific HLE sequential decision-making task is well-supported by the data and analysis.

**Medium Confidence:** The inference that this performance gain is best explained by exposure to the public benchmark (HLE) as a training curriculum, rather than by general algorithmic improvements, is plausible but not definitively proven due to the lack of direct access to training data.

**Low Confidence:** The broader implications for the validity of public benchmarks as measures of generalization capabilities are speculative. While the single example is concerning, it does not constitute definitive evidence that all or even most public benchmarks function primarily as curricula rather than evaluative tools.

## Next Checks

1. **Re-run with Novel Tasks:** Create and evaluate models on entirely new, private tasks from the same domain (sequential decision-making) that are guaranteed to be unseen. This tests if the "benchmark-driven" effect generalizes beyond the specific HLE task.

2. **Automated Evaluation Comparison:** Run the same model responses through an AI judge (like the one used by HLE) and compare the scores to the human evaluation. Quantify what is missed by automation to assess the necessity and impact of the chosen human evaluation method.

3. **Analyze Forgetting:** Test if DeepSeek-R1-0528's improvement on the HLE task came at the cost of performance degradation on other, older benchmarks that were not part of its new curriculum. This checks for capability trade-offs and provides further evidence for or against the "curriculum learning" hypothesis.