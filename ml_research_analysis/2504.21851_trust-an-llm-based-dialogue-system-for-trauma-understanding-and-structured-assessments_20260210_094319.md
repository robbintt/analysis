---
ver: rpa2
title: 'TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured
  Assessments'
arxiv_id: '2504.21851'
source_url: https://arxiv.org/abs/2504.21851
tags:
- dialogue
- patient
- system
- interview
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRUST is an LLM-based dialogue system that conducts formal diagnostic
  interviews and assessments for PTSD, addressing the challenge of mental healthcare
  accessibility. The system uses a Dialogue Acts schema for clinical interviews and
  patient simulation based on real-life transcripts to evaluate performance.
---

# TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments

## Quick Facts
- arXiv ID: 2504.21851
- Source URL: https://arxiv.org/abs/2504.21851
- Reference count: 36
- Key outcome: TRUST achieves comparable performance to real-life clinical interviews in PTSD diagnostic assessments, with human experts rating its performance equivalent to clinicians.

## Executive Summary
TRUST is an LLM-based dialogue system that conducts formal diagnostic interviews for PTSD using the CAPS-5 protocol. The system addresses mental healthcare accessibility by automating structured clinical interviews while maintaining diagnostic accuracy. It employs a Dialogue Acts schema to control LLM-generated responses and uses patient simulation based on real clinical transcripts for scalable evaluation. Human expert evaluations show TRUST performs comparably to real-life clinical interviews, with average scores indicating equivalent performance to clinicians, though patient simulation exhibits hallucination risks when transcript data is incomplete.

## Method Summary
TRUST uses a modular architecture with Database and Framework modules to conduct PTSD diagnostic interviews following the CAPS-5 protocol. The system processes 92 variables and 241 predefined questions using an 8-tag Dialogue Acts schema (GC, GI, ACK, EMP, VAL, IS, CQ, CA) for response generation control. The Conversation module predicts appropriate DA tags and generates responses, while the Assessment module tracks interview progress and diagnostic scoring. Patient simulation based on real clinical transcripts enables scalable evaluation without requiring continuous clinician involvement. The system uses Claude-3-5-sonnet for all LLM components and integrates an automated PTSD assessment pipeline.

## Key Results
- Human experts rated TRUST's performance equivalent to real-life clinical interviews, with average scores within the -0.3 to 0.3 threshold indicating comparable performance.
- Patient simulation achieved "Acceptable" ratings (0.31-0.32) but showed concerning negative Faithfulness scores (-0.33 to -0.39), indicating hallucination risks.
- Dialogue Act prediction achieved 0.63 F1 score, with the model tending to overproduce clarifying questions and struggle with casual exchanges.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dialogue Acts schema improves controllability of LLM-generated clinical responses by decomposing decision-making into discrete functional categories.
- Mechanism: The 8-tag schema serves as an intermediate layer between conversation history and response generation. The LLM first predicts appropriate DA tags, which then constrain whether the system generates contextual responses or selects from predefined clinical questions.
- Core assumption: Structured diagnostic interviews require interpretable decision points that end-to-end LLM generation cannot reliably produce without intermediate scaffolding.
- Evidence anchors: [abstract] "We propose a Dialogue Acts schema specifically designed for clinical interviews"; [Methods] "This approach decomposes complex decision-making into manageable steps"; [corpus] Related work on clinical dialogue systems emphasizes behavioral constraints for safety-critical systems.
- Break condition: If DA tag prediction accuracy degrades significantly (current F1=0.63), inappropriate tags will cascade into wrong response types.

### Mechanism 2
- Claim: Patient simulation using real-life transcripts enables scalable system evaluation without requiring continuous clinician involvement.
- Mechanism: The system extracts relevant transcript segments for each variable, prompts the LLM to generate adaptive patient responses that preserve clinical information while matching communication patterns. When transcript data is unavailable, the model responds with uncertainty expressions to minimize hallucination risk.
- Core assumption: Real clinical transcripts contain sufficient signal for an LLM to approximate authentic patient behavior across diverse presentations.
- Evidence anchors: [abstract] "We develop a patient simulation approach based on real-life interview transcripts"; [Results] Human evaluators rated simulation at 0.32 and 0.31 (Acceptable range), but Faithfulness scored negative (-0.39, -0.33); [corpus] Voice-Enabled Virtual Patient System uses similar LLM-powered simulation for clinical training.
- Break condition: When original transcripts lack relevant information for specific questions, the LLM "tends to assume an answer and fabricate supporting details."

### Mechanism 3
- Claim: Separating Conversation and Assessment modules enables interview progress tracking and diagnostic scoring while maintaining dialogue coherence.
- Mechanism: The Assessment module evaluates whether interview history provides sufficient information after each patient response. If sufficient, it triggers variable assessment; if insufficient, the dialogue loop continues. History and Score components update dynamically, enabling the Conversation module to maintain context.
- Core assumption: PTSD diagnostic criteria can be decomposed into discrete variables with defined dependencies, questions, and assessment thresholds.
- Evidence anchors: [Methods] "These components actively exchange information with the Framework module"; [Results] Expert evaluation scores (0.03, -0.14 average) indicate "equivalent performance to real-life clinical interviews"; [corpus] Weak direct corpus evidence for this specific modular decomposition.
- Break condition: The paper notes "agent interpretation misalignment" where "the agent occasionally extrapolates beyond what the patient actually stated."

## Foundational Learning

- Concept: **Structured Clinical Interviews (CAPS-5)**
  - Why needed here: The entire system is built around CAPS-5's 92 variables and 241 predefined questions. Understanding variable dependencies and question hierarchies is essential for following the system's decision logic.
  - Quick check question: Can you explain why a dependent variable might skip assessment based on prerequisite variable results?

- Concept: **Dialogue Act Tagging**
  - Why needed here: This is the control mechanism for response generation. The 8 tags determine whether the system acknowledges, empathizes, validates, seeks information, or clarifies—all with different clinical implications.
  - Quick check question: What is the functional difference between ACK (Acknowledgment) and VAL (Validation) in a diagnostic context?

- Concept: **LLM Evaluation Bias**
  - Why needed here: The paper demonstrates that LLM evaluators consistently rate generated content higher than human experts (1.68-1.76 vs 0.03-0.14). This has critical implications for any automated quality assurance.
  - Quick check question: Why might an LLM evaluator favor its own generation, and what mitigation does the paper attempt?

## Architecture Onboarding

- Component map:
  - Variable session initialization → retrieve metadata and previous scores
  - Decision: independent variable (direct to questions) vs. dependent variable (check prerequisites)
  - DA tag prediction (Claude LLM call)
  - Branch: IS tag present → question selection; IS tag absent → contextual response generation
  - Patient response (simulated or real)
  - Assessment module: sufficient information? → score variable; insufficient → loop to step 3
  - Progress to next variable

- Critical path:
  Database Module (Variable/History/Score) → Framework Module (Conversation/Assessment) → Patient Simulation for evaluation

- Design tradeoffs:
  - Modular vs. end-to-end: Modular design sacrifices generation flexibility for interpretability and control
  - Transcript-based simulation vs. synthetic patients: Real transcripts provide clinical authenticity but inherit gaps
  - Human vs. LLM evaluation: Human evaluation is accurate but time-consuming; LLM evaluation is scalable but unreliable

- Failure signatures:
  - Overly long ACK/EMP/VAL responses despite brevity instructions
  - Overproduction of CQ tags, especially after brief patient responses
  - Hallucinated patient details when transcript lacks relevant information
  - Diagnostic drift where agent interpretation exceeds patient statements
  - Conversational rhythm mismatch with natural pause-and-continue patterns

- First 3 experiments:
  1. DA tag prediction error analysis: Inspect cases where predicted tags differ from gold standard to identify cascading errors
  2. Simulation faithfulness stress test: Measure hallucination rates as function of transcript completeness
  3. Communication style calibration: A/B test verbose vs. brief responses to optimize Communication Style scores

## Open Questions the Paper Calls Out
None

## Limitations
- Patient simulation reliability concerns with negative Faithfulness scores (-0.33 to -0.39) indicating systematic hallucination
- Small expert evaluation sample size (5 transcripts) limiting statistical power for "equivalent performance" claims
- Limited generalizability to other diagnostic protocols beyond CAPS-5 PTSD assessment

## Confidence

- High confidence: Modular architecture design, Dialogue Act schema implementation, and basic evaluation methodology are well-documented and reproducible
- Medium confidence: Performance claims of "equivalent to clinicians" are supported by human expert evaluation but limited by small sample size
- Low confidence: Patient simulation reliability and hallucination mitigation strategies, given documented negative Faithfulness scores

## Next Checks

1. **Hallucination threshold quantification**: Systematically measure hallucination rates in patient simulation as a function of transcript completeness, establishing the minimum information density required for reliable simulation.

2. **Cross-diagnostic protocol testing**: Evaluate the system's adaptability to a different clinical interview protocol (e.g., PHQ-9 for depression) to assess generalizability beyond PTSD assessment.

3. **Expanded expert evaluation**: Conduct expert evaluations across a larger, more diverse set of clinical transcripts (minimum 20-30) to strengthen statistical validity of performance claims.