---
ver: rpa2
title: Low-rank surrogate modeling and stochastic zero-order optimization for training
  of neural networks with black-box layers
arxiv_id: '2509.15113'
source_url: https://arxiv.org/abs/2509.15113
tags:
- training
- layer
- layers
- gradient
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating non-differentiable
  physical components, such as photonic layers, into deep learning pipelines for end-to-end
  training. The authors propose a framework called astralora, which combines stochastic
  zeroth-order optimization for updating the physical layer's internal parameters
  with a dynamic low-rank surrogate model to enable gradient propagation through the
  black-box layer.
---

# Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers

## Quick Facts
- arXiv ID: 2509.15113
- Source URL: https://arxiv.org/abs/2509.15113
- Reference count: 40
- Primary result: Successfully trains hybrid neural networks with non-differentiable photonic layers, achieving near-digital baseline accuracy across image, audio, and language tasks

## Executive Summary
This paper addresses the challenge of integrating non-differentiable physical components, such as photonic layers, into deep learning pipelines for end-to-end training. The authors propose a framework called astralora, which combines stochastic zeroth-order optimization for updating the physical layer's internal parameters with a dynamic low-rank surrogate model to enable gradient propagation through the black-box layer. A key component is the implicit projector-splitting integrator algorithm, which efficiently updates the surrogate model after each forward pass with minimal hardware queries. The method is evaluated across three domains: image classification (CIFAR-10), audio classification (UrbanSound8K), and large-scale language modeling (FineWeb corpus).

## Method Summary
The astralora framework trains hybrid neural networks containing non-differentiable black-box (BB) layers by maintaining a differentiable low-rank surrogate model (SM) that approximates the BB's linear operation. During training, forward passes use the actual BB layer while backward passes use the SM to propagate gradients. BB parameters are updated using stochastic zeroth-order optimization with finite-difference gradient estimates, while the SM is updated online using the implicit projector-splitting integrator (I-PSI) algorithm. The approach requires querying the BB for perturbations and random probes to maintain SM accuracy without full matrix reconstruction.

## Key Results
- Achieves near-digital baseline accuracy (91.2% vs 91.3%) on CIFAR-10 with a simulated microring resonator layer
- Successfully trains a 417M parameter language model with Mach-Zehnder interferometer layers, reaching 3.25 validation perplexity
- Demonstrates robust performance across three domains with different physical layer types (MRR, SLM, MZI meshes)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Stochastic zeroth-order (ZO) optimization provides a viable gradient approximation for updating the internal parameters of a non-differentiable black-box layer.
- **Mechanism:** A Monte Carlo estimator uses random perturbations of the black-box's internal parameters to calculate the directional derivative of the loss with respect to those parameters, bypassing the need for explicit gradients.
- **Core assumption:** The function mapping the black-box's parameters to the loss landscape is sufficiently smooth to be approximated by a finite-difference method, and the optimal perturbation scale (µ) and number of samples (M_BB) can be found empirically.
- **Evidence anchors:**
  - [abstract] "...integrates stochastic zeroth-order optimization for updating the physical layer's internal parameters..."
  - [Section 2.1] "...we approximate the gradient using a stochastic finite-difference method...g(x,v)≈ 1/µ Eu (⟨fω+µu(x),v⟩ − ⟨fω(x),v⟩)..."
  - [corpus] "Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization" demonstrates scaling ZO methods to large parameter counts.
- **Break condition:** The method fails if the number of queries per update (M_BB + 1) is too low, resulting in a high-variance gradient estimate that destabilizes training, or if the loss landscape is too noisy/discontinuous.

### Mechanism 2
- **Claim:** A dynamic, low-rank surrogate model (SM) enables gradient propagation *through* a black-box layer to upstream digital layers.
- **Mechanism:** A differentiable approximation of the black-box's linear operation (A_ω) is maintained as a low-rank decomposition (U S V^T). This allows the use of standard autograd for backpropagation through the layer's input-output mapping.
- **Core assumption:** The essential action of the black-box layer can be approximated by a low-rank linear transformation, and this approximation can be kept sufficiently accurate during training.
- **Evidence anchors:**
  - [abstract] "...dynamic low-rank surrogate model that enables gradient propagation through the physical layer."
  - [Section 2] "To pass the gradient through the BB during backpropagation... we utilize a low-rank surrogate model (SM) defined as: y= bfϕ(x) =U SV T x..."
  - [corpus] No direct corpus evidence for this specific combination of a low-rank SM with black-box training.
- **Break condition:** The surrogate model's rank is too low to capture the effective dimensionality of the black-box layer's operation, leading to significant approximation error and poor upstream gradient flow.

### Mechanism 3
- **Claim:** The implicit projector-splitting integrator (I-PSI) algorithm allows for query-efficient online updates of the low-rank surrogate model without full matrix reconstruction.
- **Mechanism:** Instead of recomputing the full matrix from the black-box, the algorithm incrementally updates the existing low-rank factors (U, S, V) by querying the black-box on a small number of vectors (r columns from V, M_sm random probes) to estimate the change in the matrix (ΔA).
- **Core assumption:** The change in the black-box's transfer matrix between parameter updates is small enough that a low-rank approximation can be efficiently updated incrementally, preserving numerical stability.
- **Evidence anchors:**
  - [abstract] "...implicit projector-splitting integrator algorithm, which efficiently updates the surrogate model after each forward pass with minimal hardware queries."
  - [Section 2.2] "The I-PSI algorithm proceeds in four key stages... it exclusively requires forward evaluations of the BB... total query complexity is 2r + 2M_sm..."
  - [corpus] No direct corpus evidence for this specific algorithm in this context.
- **Break condition:** The black-box layer's parameters change too drastically between updates, causing the incremental update to fail to track the true matrix, or the required number of queries (r + M_sm) becomes too high, negating the efficiency benefit.

## Foundational Learning

- **Concept: Zeroth-Order Optimization**
  - **Why needed here:** Essential for understanding how the framework trains the non-differentiable black-box layer by approximating gradients solely from function evaluations (queries).
  - **Quick check question:** How does the variance of a gradient estimate change as you increase the number of random perturbation samples?

- **Concept: Low-Rank Approximation (SVD)**
  - **Why needed here:** Core to the surrogate model's design. Understanding how a matrix can be decomposed into U, S, and V factors is necessary to grasp how the I-PSI algorithm works.
  - **Quick check question:** What does a small singular value in the matrix 'S' signify about the information captured by the corresponding singular vectors?

- **Concept: Hybrid Digital-Physical Training Loop**
  - **Why needed here:** The proposed method is not a standalone optimizer but integrates into a standard deep learning pipeline. The learner needs to understand how forward passes, loss calculation, and backpropagation are modified to accommodate the black-box layer and its surrogate.
  - **Quick check question:** In a standard backpropagation pass, where does the surrogate model's gradient approximation get used?

## Architecture Onboarding

- **Component map:** Digital Neural Network -> Black-Box Layer -> Digital Neural Network -> Loss Function -> Surrogate Model (SM) -> Zeroth-Order Optimizer
- **Critical path:** The training loop's critical path is now: 1) **Forward Pass:** Input flows through digital layers -> hits Black-Box (real query) -> output flows through remaining digital layers. 2) **Loss & Backward:** Loss is computed. Gradients are backpropagated. Crucially, the surrogate model (SM) provides the gradient at the Black-Box layer boundary. 3) **Parameter Updates:** Digital parameters (θ) are updated via standard SGD/Adam. Black-Box parameters (ω) are updated via the ZO Optimizer. 4) **Surrogate Update:** The I-PSI updater uses queries to the newly updated Black-Box to refresh the SM's factors.
- **Design tradeoffs:**
  - **Rank (r) of Surrogate:** Higher rank improves gradient approximation but increases memory and compute for the SM update.
  - **Query Budget (M_bb, M_sm):** More queries reduce ZO gradient variance and improve SM tracking accuracy but slow down the training loop significantly, especially with physical hardware latency.
  - **Perturbation Scale (µ):** A critical hyperparameter. Too small, and numerical noise dominates. Too large, and the finite-difference approximation becomes inaccurate.
- **Failure signatures:**
  - **Training Instability / Divergence:** Likely due to a ZO gradient estimate with excessively high variance. Increase M_bb or tune µ.
  - **Stagnating Accuracy:** The surrogate model may have drifted from the true black-box operation. Increase M_sm or the rank r. Could also indicate the black-box's operation is inherently high-rank.
  - **Slow Convergence:** The total number of queries per step (M_bb + 2r + 2*M_sm) is too high, making each iteration prohibitively expensive.
- **First 3 experiments:**
  1. **Ablation on Surrogate Rank:** Implement the framework on a simple MLP with a simulated black-box layer (e.g., a random matrix). Train for a fixed number of epochs with rank r = [1, 5, 10, 50]. Plot final accuracy vs. rank to establish a baseline for the required SM complexity.
  2. **Ablation on Query Budget:** Using the best rank from experiment 1, train with query budgets M = [1, 10, 100, 1000]. Plot accuracy and training time per epoch vs. M to find the efficiency-performance sweet spot.
  3. **Non-Ideal Hardware Simulation:** Replace the simple black-box with a more realistic simulator (e.g., one that includes additive noise or parameter quantization) to test the framework's robustness before deployment on real hardware. Compare results to an ideal baseline.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions beyond acknowledging that future work could explore multiple physical layers, non-linear physical components, and real hardware implementations.

## Limitations

- Query efficiency remains a concern as the required number of black-box queries per training step (M_BB + 2r + 2M_sm) can become prohibitive, especially with physical hardware latency
- The framework assumes the black-box operation can be well-approximated by a low-rank linear transformation, which may not hold for highly complex physical layers
- All experiments use idealized digital simulations rather than actual hardware, leaving questions about robustness to real-world imperfections like noise, drift, and fabrication variability

## Confidence

- **High Confidence:** The mathematical framework for the low-rank surrogate model and its integration with backpropagation is sound and well-defined. The I-PSI algorithm's theoretical efficiency claims are supported by the query complexity analysis.
- **Medium Confidence:** The zeroth-order optimization component is a well-established technique, but its practical effectiveness here depends heavily on the specific loss landscape characteristics, which vary across tasks and hardware implementations.
- **Low Confidence:** The absolute computational efficiency gains and the framework's robustness to non-ideal hardware conditions are not rigorously quantified. The method's scalability to extremely large models or highly complex photonic architectures remains untested.

## Next Checks

1. **Rank Sufficiency Analysis:** Systematically vary the surrogate rank (r) from 1 to 100 and plot final validation accuracy against rank. Identify the point of diminishing returns to establish a practical upper bound and validate the assumption that low-rank approximation is sufficient for gradient propagation.

2. **Query Budget vs. Performance Trade-off:** For a fixed, optimal rank, conduct an ablation study varying M_BB and M_sm independently. Plot accuracy and training time per epoch against the total query count to find the optimal efficiency-performance balance and to test the claim that I-PSI is "query-efficient."

3. **Robustness to Hardware Imperfections:** Replace the ideal black-box simulator with one that includes realistic hardware imperfections (additive Gaussian noise, parameter quantization to 8/16 bits, slow parameter drift). Measure the degradation in accuracy and the stability of the surrogate model tracking to assess the framework's practical viability for real-world deployment.