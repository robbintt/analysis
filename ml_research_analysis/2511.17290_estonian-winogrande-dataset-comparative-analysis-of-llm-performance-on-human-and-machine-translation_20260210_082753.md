---
ver: rpa2
title: 'Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human
  and Machine Translation'
arxiv_id: '2511.17290'
source_url: https://arxiv.org/abs/2511.17290
tags:
- translation
- dataset
- estonian
- translated
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a manually translated and culturally adapted\
  \ Estonian version of the WinoGrande test set and compares its performance with\
  \ two machine-translated versions across various open and proprietary LLMs. The\
  \ human-translated dataset shows slightly better model performance than machine-translated\
  \ versions, with average gains of 4.7\u20137.2% on non-trivial subsets."
---

# Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation

## Quick Facts
- **arXiv ID**: 2511.17290
- **Source URL**: https://arxiv.org/abs/2511.17290
- **Reference count**: 0
- **Primary result**: Human-translated Estonian WinoGrande dataset outperforms machine-translated versions by 4.7-7.2% on non-trivial subsets

## Executive Summary
This study presents a manually translated and culturally adapted Estonian version of the WinoGrande test set and compares its performance with two machine-translated versions across various open and proprietary LLMs. The human-translated dataset shows slightly better model performance than machine-translated versions, with average gains of 4.7–7.2% on non-trivial subsets. Machine translation introduced semantic shifts in 11.8–15.2% of instances, impacting model accuracy. The detailed prompt designed to address linguistic challenges did not yield significant improvements over a simple prompt. Error correction and cultural adaptation during human translation were crucial for maintaining task integrity, as evidenced by lower performance on uncorrected or culturally distant subsets. Overall, the findings underscore the ongoing limitations of current LLMs for reliable translation in benchmark tasks and highlight the importance of involving language specialists in dataset creation for accurate evaluation of language competency and reasoning.

## Method Summary
The study created three versions of the Estonian WinoGrande dataset: a human-translated version with cultural adaptation, a version translated by Google Translate, and another by DeepL. Researchers evaluated these datasets across multiple open and proprietary LLMs, including encoder-decoder models. They analyzed performance differences, semantic shifts introduced by machine translation, and the impact of various prompt designs. The evaluation included both trivial and non-trivial subsets of the dataset, with particular attention to instances where translation quality affected reasoning task performance.

## Key Results
- Human-translated dataset outperformed machine-translated versions by 4.7-7.2% on non-trivial subsets
- Machine translation introduced semantic shifts in 11.8-15.2% of instances
- Detailed prompts did not significantly improve performance over simple prompts
- Cultural adaptation during human translation was crucial for maintaining task integrity

## Why This Works (Mechanism)
The study demonstrates that human translation, particularly when combined with cultural adaptation and error correction, produces more reliable benchmark datasets for evaluating LLM reasoning capabilities. Machine translation systems introduce systematic semantic shifts that can mislead models and create false impressions of their reasoning abilities. The lack of significant improvement from detailed prompts suggests that the core issue lies in translation quality rather than prompt engineering.

## Foundational Learning
- **Cross-linguistic semantic preservation**: Understanding how meaning transfers between languages is essential for creating valid translation tasks and avoiding false negatives in model evaluation
  - *Why needed*: Machine translation can introduce subtle meaning changes that affect reasoning task performance
  - *Quick check*: Manual verification of translation consistency across multiple language pairs

- **Cultural adaptation in translation**: Adapting content to target culture while preserving original meaning is crucial for benchmark validity
  - *Why needed*: Cultural distance between source and target languages affects task comprehension
  - *Quick check*: Expert review of culturally adapted content for meaning preservation

- **Benchmark dataset quality control**: Ensuring translation accuracy and consistency across dataset versions is fundamental for reliable model evaluation
  - *Why needed*: Small translation errors can significantly impact model performance metrics
  - *Quick check*: Automated consistency checks combined with expert human review

## Architecture Onboarding
**Component Map**: Dataset Creation -> Translation Process -> Model Evaluation -> Performance Analysis -> Quality Assessment
**Critical Path**: Human translation with cultural adaptation → Model evaluation → Performance comparison with machine-translated versions
**Design Tradeoffs**: Human translation provides higher quality but is resource-intensive; machine translation is faster but introduces semantic errors; detailed prompts don't compensate for translation quality issues
**Failure Signatures**: Semantic shifts in 11.8-15.2% of machine-translated instances; performance drops on uncorrected or culturally distant subsets; lack of improvement from detailed prompts
**First Experiments**: 1) Compare performance across different model architectures (encoder-decoder vs decoder-only); 2) Test additional machine translation systems beyond Google Translate and DeepL; 3) Evaluate impact of different prompt engineering approaches on translation-dependent tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Small test set size (280 instances) limits statistical power
- Only three machine translation approaches tested, not representing full spectrum of available systems
- Exclusive focus on encoder-decoder models, excluding more common decoder-only architectures
- Does not account for potential confounding factors like prompt formatting differences or tokenization effects

## Confidence
- **High**: Core finding that human translation outperforms machine translation, supported by consistent results across multiple models
- **Medium**: Specific claims about semantic shifts (11.8-15.2%) and cultural adaptation impact, based on manual analysis of subsets
- **Medium**: Claims about detailed prompt ineffectiveness, as prompt engineering effects may vary across different task types

## Next Checks
1) Replicate study with larger test set and additional machine translation systems to confirm performance gap persists
2) Conduct blinded evaluation where annotators assess translation quality without knowing version to validate claimed semantic shifts
3) Test human-translated dataset across both encoder-decoder and decoder-only model architectures to determine if performance differences persist