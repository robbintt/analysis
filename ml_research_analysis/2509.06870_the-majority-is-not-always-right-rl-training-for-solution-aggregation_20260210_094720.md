---
ver: rpa2
title: 'The Majority is not always right: RL training for solution aggregation'
arxiv_id: '2509.06870'
source_url: https://arxiv.org/abs/2509.06870
tags:
- solutions
- solution
- majority
- aggregation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AggLM, a reinforcement learning method that
  trains an LLM to aggregate multiple candidate solutions into a better final answer.
  Unlike standard majority voting or reward model ranking, AggLM learns to reconcile
  and combine solutions using RL from verifiable rewards.
---

# The Majority is not always right: RL training for solution aggregation

## Quick Facts
- arXiv ID: 2509.06870
- Source URL: https://arxiv.org/abs/2509.06870
- Reference count: 26
- Primary result: AggLM-1.7B achieves 50% accuracy on AIME25 by aggregating eight solutions from Qwen3-1.7B, outperforming majority voting (45%) and reward-model baselines.

## Executive Summary
This paper introduces AggLM, a reinforcement learning method that trains an LLM to aggregate multiple candidate solutions into a better final answer. Unlike standard majority voting or reward model ranking, AggLM learns to reconcile and combine solutions using RL from verifiable rewards. On four challenging math competition datasets (AIME24/25, HMMT24/25), AggLM-1.7B achieves 50% accuracy on AIME25 when aggregating eight solutions from Qwen3-1.7B, outperforming majority voting (45%) and reward-model baselines. It generalizes to stronger models like Qwen3-8B and non-thinking modes, and is more token-efficient than generating more solutions. A key insight is that balancing easy and hard examples during training is crucial for strong performance.

## Method Summary
The method trains an aggregator model using reinforcement learning from verifiable rewards (RLVR) to synthesize correct answers from multiple LLM-generated candidate solutions. The aggregator is initialized from Qwen3-1.7B and trained with Group-Relative Policy Optimization (GRPO) using binary rewards (1 if aggregated solution matches ground truth). Training data is constructed by sampling 128 solutions per problem from Qwen3-1.7B in thinking mode, partitioning into 16 sets of 8, and labeling sets as easy (majority answer correct) or hard (majority answer wrong). The training mixture retains all hard examples and subsamples 5-50% of easy examples. At inference, solutions are partitioned into sets, aggregated, and Pass@1 is computed across aggregated outputs.

## Key Results
- AggLM-1.7B achieves 50% accuracy on AIME25, outperforming majority voting (45%) and reward-model baselines.
- Balanced training mixture (all hard + 50% easy) outperforms hard-only (64.22→70.69 on AIME24) and easy-only (66.20) training.
- AggLM gains are largest when candidate solutions are diverse (small majority answer sets), diminishing as unanimity increases.
- Aggregator is more token-efficient than generating more solutions (Table 7) and generalizes to stronger models (Qwen3-8B) and non-thinking modes.

## Why This Works (Mechanism)

### Mechanism 1
Treating aggregation as a learnable reasoning skill via RLVR outperforms fixed heuristics like majority voting. The aggregator model is trained with GRPO using binary verifiable rewards, allowing it to learn when to trust the majority versus when to recover minority-but-correct answers. This adaptive selection between consensus and minority recovery is behaviors that rule-based methods cannot adaptively select between. Core assumption: base solution model generates sufficiently diverse candidates such that correct solutions appear in at least some of the m samples.

### Mechanism 2
Balanced mixing of easy and hard training examples is critical for aggregation performance. Hard examples (majority answer is wrong) train the model to recover minority-correct answers; easy examples (majority is correct) prevent reward sparsity and teach the model when to trust consensus. The paper retains all hard examples and subsamples 5-50% of easy examples. Core assumption: hard examples alone provide insufficient gradient signal; some majority-correct examples are needed to regularize the policy.

### Mechanism 3
Gains over majority voting are largest when candidate solutions are diverse (small majority answer sets). When the majority answer appears in only 1-3 of 8 candidates, the correct answer is more likely to be in a minority mode. The trained aggregator uses reasoning to evaluate candidates rather than counting frequencies. Core assumption: diversity correlates with uncertainty, and reasoning-based evaluation is more reliable than frequency in high-uncertainty regimes.

## Foundational Learning

- **Reinforcement Learning from Verifiable Rewards (RLVR)**: Why needed: aggregator trained with binary rewards from ground-truth verification rather than supervised labels. Quick check: Can you explain why verifiable rewards enable stable RL training compared to learned reward models?
- **Test-Time Scaling via Self-Consistency / Best-of-N**: Why needed: AggLM is an alternative to majority voting and reward-model selection. Quick check: Why does majority voting fail when correct answers are concentrated in minority modes?
- **Reasoning Models (Chain-of-Thought / Thinking Mode)**: Why needed: paper uses Qwen3 in "thinking mode" and trains aggregator to reason over candidate solutions. Quick check: What is the difference between sampling with temperature 1.5 in thinking mode versus standard autoregressive decoding, and how might this affect solution diversity?

## Architecture Onboarding

- **Component map**: Solution Model p_θ(y|x) -> Aggregator Model p_φ(ỹ|x, y_1:m) -> GRPO Training -> Pass@1 Evaluation
- **Critical path**: Data construction (sampling → grouping → easy/hard labeling → mixing) → GRPO training (forward pass → verify → compute reward → policy update) → Evaluation (partition samples → aggregate → verify → average Pass@1)
- **Design tradeoffs**: Separate vs. shared parameters (multitask training achieves similar performance), number of solution sets s (diminishing returns beyond s=4), easy% ratio (5-50% robust, 50% best), candidate set size m (trained at m=8, generalizes to m=2-16)
- **Failure signatures**: Aggregator regresses to majority voting (check easy% or training convergence), low Pass@1 on harder datasets (candidate quality may be insufficient), generalization gap on stronger solution models (ensure training covers sufficient diversity)
- **First 3 experiments**: 1) Reproduce ablation on easy/hard mixture ratios on DeepScaler subset, 2) Evaluate token efficiency vs. accuracy trade-offs, 3) Test generalization to out-of-distribution solution models (different model family)

## Open Questions the Paper Calls Out
- Can aggregator reasoning skills be distilled back into solution models to improve their single-sample performance? The paper suggests future work could explore distilling better reasoning skills back into original solutions.
- How does aggregator performance scale with model size beyond 1.7B parameters? All experiments use AggLM-1.7B; no larger aggregator variants are tested.
- Does training mixture balancing transfer to non-mathematical reasoning domains? Easy/hard balancing is critical for math, but applicability to code, scientific reasoning, or other verifiable domains is unknown.

## Limitations
- Data provenance uncertainty: DeepScaler dataset is not publicly available, affecting reproducibility and generalizability claims.
- Generalization scope: Improvements are modest (~5% on AIME25) and may not scale to harder problem distributions or fundamentally different reasoning patterns.
- Robustness to model drift: Performance on out-of-distribution solution models beyond Qwen3-8B is unknown.

## Confidence
- **High confidence**: RLVR-based aggregation outperforms majority voting when candidates are diverse; balanced easy/hard training mixture improves performance over hard-only or easy-only training.
- **Medium confidence**: AggLM is more token-efficient than generating more solutions; gains are consistent across multiple math competition datasets; generalization to non-thinking modes and stronger models is demonstrated.
- **Low confidence**: Exact mechanism by which easy examples stabilize training is not fully characterized; optimal easy% ratio may be dataset-dependent.

## Next Checks
1. Ablate easy/hard mixture ratios: Systematically train AggLM with 0%, 10%, 30%, 50%, 100% easy ratios on DeepScaler or comparable dataset to confirm 50% sweet spot and characterize hard-only degradation.
2. Evaluate token efficiency vs. accuracy trade-offs: Compare Pass@1 vs. total tokens for majority voting, reward-model selection, and AggLM across varying solution counts (m=2 to 16) to quantify efficiency gains.
3. Test out-of-distribution generalization: Train AggLM on Qwen3-1.7B outputs and evaluate on solutions from different model family (Llama, Mistral, or Claude) to probe robustness beyond Qwen3-8B experiment.