---
ver: rpa2
title: 'From Retrieval to Generation: Unifying External and Parametric Knowledge for
  Medical Question Answering'
arxiv_id: '2510.18297'
source_url: https://arxiv.org/abs/2510.18297
tags:
- knowledge
- question
- documents
- medical
- retrieved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MedRGAG is a unified retrieval\u2013generation augmented framework\
  \ for medical question answering that integrates external knowledge from medical\
  \ corpora and parametric knowledge from large language models. It comprises two\
  \ core modules: Knowledge-Guided Context Completion (KGCC), which generates complementary\
  \ background documents to fill knowledge gaps identified from retrieved evidence,\
  \ and Knowledge-Aware Document Selection (KADS), which adaptively selects the most\
  \ relevant and reliable subset of retrieved and generated documents."
---

# From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering

## Quick Facts
- arXiv ID: 2510.18297
- Source URL: https://arxiv.org/abs/2510.18297
- Authors: Lei Li; Xiao Zhou; Yingying Zhang; Xian Wu
- Reference count: 40
- Primary result: 12.5% improvement over MedRAG and 4.5% gain over MedGENIE across five medical QA benchmarks

## Executive Summary
MedRGAG is a unified retrieval–generation augmented framework for medical question answering that integrates external knowledge from medical corpora and parametric knowledge from large language models. It comprises two core modules: Knowledge-Guided Context Completion (KGCC), which generates complementary background documents to fill knowledge gaps identified from retrieved evidence, and Knowledge-Aware Document Selection (KADS), which adaptively selects the most relevant and reliable subset of retrieved and generated documents. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE across multiple reader architectures, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning.

## Method Summary
MedRGAG is a three-stage pipeline that combines retrieval, generation, and selection for medical question answering. First, it performs source-balanced BM25 retrieval from medical textbooks and Wikipedia, then reranks with MedCPT to select top-5 documents. Second, KGCC analyzes retrieved documents to identify missing knowledge, then generates complementary background documents using GPT-4o-mini and LLaMA-3.1-8B-Instruct. Third, KADS selects the most relevant subset from retrieved and generated candidates for final answer generation by the reader. The framework is evaluated on five medical multiple-choice benchmarks using various reader models including Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, and Ministral-8B-Instruct.

## Key Results
- 12.5% improvement over MedRAG on five medical QA benchmarks
- 4.5% gain over MedGENIE across multiple reader architectures
- Achieves comparable accuracy to merge-all strategies while using only half as many documents
- Outperforms all baseline models including MedRAG, MedGENIE, and ensemble approaches

## Why This Works (Mechanism)

### Mechanism 1: Gap-Driven Context Generation (KGCC)
The system first summarizes retrieved documents, then uses an "Explorer" module to analyze these summaries against the question to identify specific missing knowledge points. The Generator is then conditioned on these specific points rather than the question alone, producing more relevant background documents than zero-shot generation. This approach assumes the Explorer LLM possesses sufficient parametric knowledge to recognize what is missing from the retrieved context.

### Mechanism 2: Knowledge-Aware Redundancy Reduction (KADS)
Instead of feeding the reader all available evidence, KADS maps documents to specific knowledge requirements and selects a compact set that maximizes coverage while minimizing redundancy. An "Integrator" module uses prompting to distinguish between diverse supporting evidence and redundant noise. This approach assumes an LLM can reliably filter documents based on knowledge coverage requirements.

### Mechanism 3: Source-Balanced Retrieval Strategy
The retriever pulls top-k documents from each source corpus independently before global reranking, preventing over-indexing on a single dominant corpus. This approach assumes distinct medical corpora provide non-overlapping diagnostic value and that semantic similarity alone cannot be trusted to balance the initial pool.

## Foundational Learning

- **Concept**: Parametric vs. Non-Parametric Knowledge
  - **Why needed here**: The architecture relies on the distinct failure modes of these two types—parametric (hallucination) vs. non-parametric (noise/gaps)—to justify the hybrid approach.
  - **Quick check question**: If a model answers "B. Meningioma" because it saw it during pre-training, is that retrieval or generation?

- **Concept**: Evidence Grounding in Medical QA
  - **Why needed here**: The KADS module prioritizes evidence for reasoning. Understanding that medical answers must be defensible via citations is crucial to understanding why simple generation is insufficient.
  - **Quick check question**: Why might a model generate a logically coherent but factually wrong answer in a medical context?

- **Concept**: Reranking vs. Retrieval
  - **Why needed here**: The system uses BM25 for retrieval and MedCPT for reranking. Distinguishing between "finding relevant haystacks" (retrieval) and "ordering needles" (reranking) is necessary to understand the two-stage search.
  - **Quick check question**: Why use a slower Cross-Encoder (MedCPT) only after the faster BM25 step?

## Architecture Onboarding

- **Component map**: Retriever (BM25) -> Reranker (MedCPT) -> Summarizer -> Explorer -> Generator -> Integrator -> Reader
- **Critical path**: The Explorer → Generator loop. If the Explorer fails to identify the correct "missing knowledge," the Generator produces irrelevant filler, negating the benefit of the GAG component.
- **Design tradeoffs**: The multi-stage pipeline adds significant inference latency compared to vanilla RAG. KADS selects only 5 docs from 10 candidates, reducing noise but risking discarding retrieved evidence if the integrator favors generated text.
- **Failure signatures**: "Echo Chamber" Effect occurs if the Integrator favors generated documents too heavily, reverting to GAG behaviors rather than using grounded retrieved evidence. Error Cascade occurs when a weak Summarizer leads to empty Missing Knowledge signals.
- **First 3 experiments**: (1) Run w/o KGCC and w/o KADS to validate module contributions. (2) Measure the ratio of Retrieved vs. Generated docs in final top-5 to ensure retrieval isn't ignored. (3) Swap LLaMA3.1-8B for larger models in the Generator role to test impact on Missing Knowledge documents.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the auxiliary modules (summarizer, explorer, integrator) be distilled into smaller open-source models to maintain performance while reducing dependency on proprietary models like GPT-4o-mini?
- **Open Question 2**: Does MedRGAG generalize to open-ended clinical reasoning tasks where the correct answer is not constrained to a finite set of multiple-choice options?
- **Open Question 3**: How robust is the Knowledge-Aware Document Selection (KADS) module when the generated background documents contain highly plausible but factually hallucinated information?

## Limitations

- The core architecture depends heavily on the quality of the Explorer module's ability to identify missing knowledge, and if weak, the entire KGCC pipeline produces irrelevant generated documents.
- The system's effectiveness assumes the Integrator can reliably distinguish between retrieved and generated documents without bias, but if it systematically favors generated text, the retrieval component becomes ornamental.
- Source-balancing strategies, while theoretically sound, lack strong empirical validation in the medical domain specifically.

## Confidence

- **High confidence**: The basic retrieval-generation pipeline structure (KGCC + KADS) is well-specified and reproducible. The experimental methodology and dataset descriptions are clear enough for faithful replication.
- **Medium confidence**: The mechanism by which KGCC improves relevance (through explicit gap identification) is theoretically sound but depends on unverified assumptions about the Explorer's knowledge quality.
- **Low confidence**: The claim that source-balanced retrieval prevents corpus bias lacks direct empirical support in the medical domain. The specific contribution of the unified approach versus individual component improvements remains unclear.

## Next Checks

1. **Explorer Module Validation**: Test the Explorer's ability to correctly identify missing knowledge by providing it with partial contexts and measuring whether its identified gaps align with human expert assessments.

2. **Source Contribution Analysis**: For a subset of questions, track which documents (retrieved vs. generated) actually contribute to correct answers in the final reader output to verify the system isn't simply ignoring the retrieval component.

3. **Latency vs. Accuracy Tradeoff**: Measure end-to-end inference time for MedRGAG versus baseline systems and correlate with accuracy improvements to quantify whether the architectural complexity is justified by practical gains.