---
ver: rpa2
title: 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for
  Large Language Models'
arxiv_id: '2508.05613'
source_url: https://arxiv.org/abs/2508.05613
tags:
- reward
- cooper
- arxiv
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reward hacking in reinforcement learning for
  large language models, where fixed reward models can be exploited by policy models,
  leading to catastrophic performance degradation. The core method, Cooper, is a framework
  that co-optimizes both the policy and reward models simultaneously, using high-precision
  rule-based rewards to select positive samples and generating negative samples through
  an assistant LLM.
---

# Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models

## Quick Facts
- **arXiv ID:** 2508.05613
- **Source URL:** https://arxiv.org/abs/2508.05613
- **Reference count:** 26
- **Primary result:** Co-optimizing policy and reward models in RL prevents reward hacking and improves robustness, achieving 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct.

## Executive Summary
The paper addresses reward hacking in reinforcement learning for large language models, where fixed reward models can be exploited by policy models, leading to catastrophic performance degradation. The core method, Cooper, is a framework that co-optimizes both the policy and reward models simultaneously, using high-precision rule-based rewards to select positive samples and generating negative samples through an assistant LLM. To support Cooper, the authors introduce VerifyRM, a reference-based reward model trained on a hybrid-annotated dataset combining rule-based and LLM-based verification. This design achieves 89.42% accuracy on VerifyBench, outperforming existing models of similar size. Experiments demonstrate that Cooper significantly improves RL performance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct, while preventing the reward hacking collapse seen with static reward models. The results show that dynamically updating the reward model during RL is an effective strategy for enhancing robustness and stability.

## Method Summary
Cooper is a co-optimization framework for RL that jointly updates a policy model and a reward model to prevent reward hacking. The policy is optimized via GRPO using scores from a reference-based reward model (VerifyRM), which is itself updated online using contrastive learning on positive and negative samples. Positive samples are selected using high-precision rule-based verifiers, while negative samples are generated by an assistant LLM from correct responses. VerifyRM is trained on a hybrid-annotated dataset (rule-based + LLM-as-judge agreement) for 3 epochs using BCE loss. Cooper is implemented in the veRL framework, with policy updates via GRPO (KL penalty 0.001, 16 rollouts/prompt) and reward model updates via contrastive loss. The method is evaluated on math reasoning tasks, demonstrating improved accuracy and stability compared to static reward models.

## Key Results
- VerifyRM achieves 89.42% accuracy on VerifyBench, outperforming vanilla RMs (47-52%) and larger reference-based verifiers (xVerify-9B-C: 84.23%).
- Cooper improves RL performance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct compared to static reward models.
- Cooper prevents reward hacking collapse, maintaining stable accuracy (~80%) while static reward models degrade by ~16% around step 120.

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Precision Exploitation from Rule-Based Verifiers
- **Claim:** Rule-based verifiers exhibit high precision but low recall, enabling reliable positive sample selection while avoiding their brittleness in broader use.
- **Mechanism:** The paper observes that Math-Verify achieves 96% precision (345/360) but only 63% recall on VerifyBench. Cooper exploits this asymmetry: use rules only to select high-confidence positive samples for RM contrastive updates, while the learned RM handles the full response distribution during policy scoring.
- **Core assumption:** The precision asymmetry generalizes across domains and policy output distributions.
- **Evidence anchors:**
  - [abstract] "Cooper leverages the high precision of rule-based rewards when identifying correct responses"
  - [Section 4.1, Table 1] Confusion matrix shows 96% precision / 63% recall for Math-Verify
  - [corpus] Related work "From Accuracy to Robustness" studies rule- vs model-based verifiers but does not report this specific asymmetric exploitation strategy
- **Break condition:** If rule-based precision drops significantly (e.g., <80%) on new domains or evolved policy outputs, positive sample reliability degrades and RM may receive noisy contrastive pairs.

### Mechanism 2: Moving Target Prevention via Synchronized RM Updates
- **Claim:** Jointly updating the RM during RL prevents the policy from exploiting static reward function vulnerabilities.
- **Mechanism:** Each training iteration includes (1) policy optimization via GRPO using current RM scores and (2) RM contrastive learning on fresh positive/negative pairs. As the policy shifts its output distribution, the RM's decision boundaries adapt, closing exploitation opportunities before they accumulate.
- **Core assumption:** The RM update signal (contrastive loss on rule-selected positives + LLM-generated negatives) is sufficiently aligned with true correctness that RM adaptation tracks meaningful distribution shift rather than overfitting to artifacts.
- **Evidence anchors:**
  - [abstract] "demonstrating the effectiveness of dynamically updating reward models in RL"
  - [Figure 3a/3b] Static RM training reward spikes to ~1.0 with test accuracy collapse at step ~120; Cooper maintains rewards ~0.5 with stable accuracy
  - [corpus] "Adversarial Training of Reward Models" identifies reward hacking from static RMs but does not propose synchronized online updates
- **Break condition:** If negative sample quality degrades (e.g., assistant LLM generates implausible negatives that RM learns to reject trivially), contrastive signal weakens and RM may overfit to easy discriminative features without improving verification.

### Mechanism 3: Reference-Grounded Verification for Reasoning Tasks
- **Claim:** Conditioning the RM on reference answers improves verification accuracy compared to query-only RMs.
- **Mechanism:** VerifyRM takes (question, reference_answer, completion) as input, enabling comparative verification rather than absolute quality assessment. This is trained on a hybrid-annotated dataset (rule-based + LLM-as-judge agreement).
- **Core assumption:** Reference answers are available and correct at both training and inference time.
- **Evidence anchors:**
  - [abstract] "a reference-based reward modeling paradigm, where the reward model takes a reference answer as input"
  - [Table 2] VerifyRM-1.5B achieves 89.42% on VerifyBench, outperforming vanilla RMs (47-52%) and larger reference-based verifiers (xVerify-9B-C: 84.23%)
  - [corpus] No direct corpus comparison on reference-conditioned RMs; adjacent works focus on process/outcome reward distinctions
- **Break condition:** In tasks without clear reference answers (open-ended creative tasks), this mechanism does not apply without extension.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Cooper builds on GRPO for policy updates, using within-group normalized advantages and KL regularization.
  - **Quick check question:** Can you sketch how GRPO computes advantages from group-wise rollout rewards and where the KL penalty appears in the objective?

- **Reward Hacking in RLHF/RLVR**
  - **Why needed here:** The core motivation is that static RMs are exploitable; understanding overoptimization helps interpret the collapse dynamics.
  - **Quick check question:** Explain why a policy might achieve increasing reward while test performance degrades.

- **Contrastive Learning for RMs**
  - **Why needed here:** Stage 2 uses pairwise contrastive loss (positive vs negative) to update the RM online.
  - **Quick check question:** What properties must positive/negative pairs satisfy for contrastive learning to yield meaningful representations?

## Architecture Onboarding

- **Component map:** Policy Model (π_θ) -> Reward Model (R_φ, VerifyRM) -> Rule-Based Verifier (e.g., Math-Verify) -> Assistant LLM

- **Critical path:**
  1. Sample batch → generate G rollouts per question
  2. RM scores each rollout (with reference answer)
  3. Compute group-relative advantages → policy update (Stage 1)
  4. Select positive via rule-based verifier; generate negative via assistant LLM
  5. Update RM via contrastive loss (Stage 2)
  6. Repeat

- **Design tradeoffs:**
  - Computational overhead: Dual optimization (policy + RM) increases per-step cost.
  - Dependency on rule-based verifier: Limits generalization to domains without high-precision verifiers.
  - Negative sample quality: Relies on assistant LLM; if negatives are trivially wrong, contrastive signal is weak.

- **Failure signatures:**
  - Training reward spikes toward 1.0 with test accuracy drop → reward hacking (should not occur with Cooper if RM updates are working)
  - RM accuracy on VerifyBench drifts significantly during training → unstable RM updates or low-quality contrastive pairs
  - No valid positive samples selected for many batches → rule-based verifier recall too low for current policy distribution

- **First 3 experiments:**
  1. **Reproduce the collapse baseline:** Run static VerifyRM-based RL on Qwen2.5-1.5B-Instruct with MATH500 evaluation; confirm accuracy drop and reward spike around step 100-150.
  2. **Ablate positive selection strategy:** Replace rule-based positive selection with RM self-selection (e.g., highest-scoring rollout) and compare RM stability and final accuracy.
  3. **Stress test negative quality:** Swap assistant LLM for a simpler perturbation (e.g., random answer substitution) and measure impact on RM accuracy trajectory and policy performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Cooper be effectively applied to domains lacking high-precision rule-based verifiers?
- **Basis in paper:** [Explicit] Section 6 states that "dependency on domain-specific verification tools limits generalization to tasks without clear correctness criteria."
- **Why unresolved:** The framework currently relies on the high precision of rule-based rewards for positive sample selection, a feature absent in open-ended or subjective tasks.
- **What evidence would resolve it:** Experimental results applying Cooper to tasks like creative writing or dialogue using alternative weak-supervision signals (e.g., consistency voting) for the positive sample selector.

### Open Question 2
- **Question:** Is self-supervised contrastive example generation viable for updating the reward model without an external assistant LLM?
- **Basis in paper:** [Explicit] Section 6 identifies reliance on an assistant LLM for negative sample generation as a limitation and suggests future work should "explore self-supervised contrastive example generation."
- **Why unresolved:** The current method depends on an external model to transform correct responses into incorrect ones; a self-contained mechanism is unexplored.
- **What evidence would resolve it:** An implementation of Cooper where negative samples are derived via internal perturbations or self-generated errors, showing comparable stability and performance.

### Open Question 3
- **Question:** Can the co-optimization framework be extended to Process Reward Models (PRMs) for denser supervision?
- **Basis in paper:** [Explicit] Section 6 lists extending Cooper to "process-based rewards for denser supervision" as a specific future direction.
- **Why unresolved:** The current study focuses on Outcome Reward Models (ORMs); it is unclear if the co-optimization dynamics remain stable when applied to intermediate reasoning steps.
- **What evidence would resolve it:** Experiments integrating Cooper with a PRM, analyzing whether dynamic updates prevent reward hacking at the step level and improve final accuracy.

## Limitations

- The asymmetric precision/recall exploitation assumes rule-based verifiers maintain >80% precision across evolving policy distributions; this may not hold for more complex reasoning tasks or less constrained domains.
- The assistant LLM's ability to generate high-quality negative samples is critical but not rigorously validated; if negatives are trivially distinguishable, contrastive learning provides weak supervision.
- The hybrid annotation filtering (rule + LLM agreement) may introduce selection bias, potentially inflating VerifyRM accuracy on VerifyBench.

## Confidence

- **High confidence:** The empirical demonstration that static reward models lead to reward hacking collapse (accuracy drop ~16% at step ~120) is well-supported by controlled experiments.
- **Medium confidence:** The effectiveness of synchronized RM updates in preventing reward hacking is demonstrated but could benefit from longer training horizons and more diverse task domains.
- **Medium confidence:** The reference-based reward modeling approach achieves state-of-the-art accuracy (89.42%) but the improvement over larger models (xVerify-9B-C: 84.23%) should be interpreted with consideration of model size differences.

## Next Checks

1. **Test rule-based precision decay:** Run Cooper with progressively more complex math problems (beyond basic algebra/arithmetic) and measure whether Math-Verify precision drops below the assumed 80% threshold, causing positive sample contamination.
2. **Validate negative sample quality:** Implement an ablation where negative samples are generated via random answer perturbation instead of the assistant LLM, then measure changes in RM accuracy trajectory and policy performance stability.
3. **Extend to non-reference tasks:** Adapt Cooper to open-ended creative tasks without reference answers (e.g., story continuation) and evaluate whether alternative positive selection strategies maintain the no-collapse property.