---
ver: rpa2
title: Graph is a Substrate Across Data Modalities
arxiv_id: '2601.22384'
source_url: https://arxiv.org/abs/2601.22384
tags:
- graph
- across
- structural
- graphs
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of graph representations being
  learned in isolated, task-specific ways and then discarded, which prevents structural
  regularities from being reused across tasks and modalities. The authors propose
  G-Substrate, a framework that treats graph structure as a persistent intermediate
  substrate rather than a task-bound artifact.
---

# Graph is a Substrate Across Data Modalities

## Quick Facts
- arXiv ID: 2601.22384
- Source URL: https://arxiv.org/abs/2601.22384
- Reference count: 25
- Primary result: G-Substrate achieves 51.53 BLEU-4 on molecular graph description, outperforming naive single-task training (48.59 BLEU-4)

## Executive Summary
This paper addresses the problem of graph representations being learned in isolated, task-specific ways and then discarded, which prevents structural regularities from being reused across tasks and modalities. The authors propose G-Substrate, a framework that treats graph structure as a persistent intermediate substrate rather than a task-bound artifact. The core method involves two complementary mechanisms: a unified structural schema to ensure cross-task compatibility of graph representations, and interleaved role-based training that exposes the same graph to multiple functional roles during learning. Experiments across graph algorithmic reasoning, molecular graph description, scene graph generation, and event relation extraction show that G-Substrate consistently outperforms both task-isolated and naive multi-task baselines.

## Method Summary
G-Substrate enables graph structure reuse across modalities via (1) unified structural schema (graphs as triples G={(u,r,v)}) and (2) interleaved role-based training (GENERATE and UNDERSTAND roles). The method uses a VLM backbone (Qwen3-VL-2B-Instruct) with full-parameter fine-tuning, AdamW optimizer (β1=0.9, β2=0.98), learning rate 8×10⁻⁶ with cosine decay, batch size 64 (1×32 grad accum), and 2 epochs. The framework converts all four datasets to unified schema: SGG/ERE tasks produce graphs while GAR/MGD consume them, with ~50% interleaved instances for optimal performance.

## Key Results
- G-Substrate achieves 51.53 BLEU-4 on molecular graph description vs. 48.59 for naive single-task training
- On shortest-path reasoning, G-Substrate outperforms both Unified Multi-Task (schema only) and Naive Multi-Task baselines
- Cross-domain transfer from event graphs to scene graph generation shows 1.5% improvement in R@50
- Excessive interleaving (>50%) degrades performance, indicating need for task-specific optimization preservation

## Why This Works (Mechanism)

### Mechanism 1: Unified Structural Schema Enables Cross-Modal Alignment
Mapping heterogeneous graph representations into a common structural space allows structurally analogous patterns from different modalities to occupy aligned representation regions. Graphs from vision, text, and scientific domains are expressed as structural triples with consistent entity identifiers and typed relations, enabling pattern-level transfer. Core assumption: Local structural motifs encode similar constraint roles across domains despite semantic differences.

### Mechanism 2: Interleaved Role-Based Training Induces Structural Consistency Pressure
Exposing the same graph state to both generation and understanding roles during training biases representations toward structural configurations that remain valid across functional contexts. Training alternates between GENERATE tasks (produce graphs) and UNDERSTAND tasks (consume graphs). Core assumption: Structural coherence—not surface serialization—drives cross-role transferability.

### Mechanism 3: Recurring Local Motifs Provide Transfer-Able Structural Priors
Coarse topological patterns (hub-centered configurations, two-hop chains) recur with non-trivial frequency across domains and encode aligned constraint roles. Hub nodes act as consistency anchors coordinating multiple relations; two-hop chains encode mediated dependencies. These motifs abstract away from semantic content while preserving relational constraints that transfer across modalities.

## Foundational Learning

- **Concept: Graph as Intermediate Representation (vs. Task Input/Output)**
  - Why needed here: G-Substrate treats graphs as persistent state that must survive across task boundaries, not disposable artifacts. This reframes graph learning from task-specific encoding to substrate-level accumulation.
  - Quick check question: Can you articulate why a graph generated in scene graph generation must remain structurally valid for a subsequent shortest-path query?

- **Concept: Functional Role Duality (GENERATE vs. UNDERSTAND)**
  - Why needed here: The framework explicitly separates tasks by whether they produce or consume graph structure, creating complementary supervision pressures.
  - Quick check question: For molecular graph description, does the task act as GENERATE or UNDERSTAND? (Answer: UNDERSTAND—the graph is input for description generation.)

- **Concept: Structural Admissibility Constraints**
  - Why needed here: The unified schema imposes constraints (consistent entity identifiers, typed relations) that graphs must satisfy to participate in cross-role reuse.
  - Quick check question: What minimal structural properties must a graph maintain to be reusable across both algorithmic reasoning and consistency checking tasks?

## Architecture Onboarding

- **Component map:** Raw modality input → Schema-aligned graph construction (if GENERATE) → Gs → Task-specific head (if UNDERSTAND) → Output; graphs persist across timesteps for downstream roles.
- **Critical path:** Unified graph state space (Gs) accepts graphs from all modalities as {(entity, relation, entity)} triples, with role assignment function mapping tasks to GENERATE or UNDERSTAND roles.
- **Design tradeoffs:** Schema granularity affects transfer specificity; interleave proportion balances structural robustness vs. task-specific optimization (optimal ≈50%); backbone choice affects performance but mechanism appears architecture-agnostic.
- **Failure signatures:** Unified schema without interleave degrades performance; excessive interleaving (>50%) causes unstable improvements; structurally incorrect graphs reused reverses gains to losses.
- **First 3 experiments:** 1) Compare Naive Single-Task vs. Unified Single-Task on one domain to confirm schema alone doesn't improve performance; 2) Ablate Unified Multi-Task (schema only) vs. G-Substrate (schema + interleave) on shortest-path reasoning; 3) Pretrain on event graphs only, evaluate zero-shot on scene graph generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What constitutes the optimal balance between structure-producing, structure-consuming, and algorithmic roles during interleaved training?
- Basis in paper: The authors state in the "Limitations and Future Work" section that "the optimal balance among structure-producing, structure-consuming, and algorithmic roles remains open."
- Why unresolved: While the paper demonstrates that moderate interleaving (e.g., 50%) outperforms extremes, it does not provide a theoretical or empirical method to determine the ideal ratio of specific functional roles for a given set of tasks.
- What evidence would resolve it: A comprehensive ablation study that systematically varies the ratio of generation vs. understanding roles and correlates these ratios with structural consistency metrics and downstream performance across diverse domains.

### Open Question 2
- Question: Can the unified structural schema be theoretically grounded or automated to maximize transfer without manual engineering?
- Basis in paper: Section 3.4.1 shows that performance varies significantly between Natural Language, XML, and the proposed schema, suggesting that schema design is a critical, non-trivial factor.
- Why unresolved: The "unified structural schema" appears to be a specific design choice rather than a learned or theoretically optimal representation, leaving the upper bounds of schema efficiency unexplored.
- What evidence would resolve it: Experiments comparing the hand-crafted schema against dynamically learned or induced schemas to see if structural reuse can be improved by optimizing the representation format itself.

### Open Question 3
- Question: Does the performance benefit of the graph substrate degrade or exhibit negative transfer when scaling to a significantly larger number of heterogeneous tasks?
- Basis in paper: The "Limitations and Future Work" section notes the authors "do not explicitly study how the composition of modalities... influences representation formation," and analysis notes gains are "not uniform."
- Why unresolved: The study is limited to four domains; it is unclear if the "structural alignment" mechanism is sufficient to prevent negative interference in a large-scale setting where conflicting structural constraints might overwhelm the unified state space.
- What evidence would resolve it: A scaling curve analysis showing G-Substrate performance as the number of tasks increases, specifically monitoring for performance drops in individual tasks compared to isolated training.

## Limitations

- **Cross-domain boundary conditions unknown:** Performance cliffs may exist beyond tested distributions when relation vocabularies or graph densities differ significantly.
- **Schema granularity tradeoff ambiguity:** Optimal level of relation typing granularity is not systematically explored, risking either loss of domain-specific constraints or prevention of meaningful transfer.
- **Architecture dependence uncertainty:** Results are based on Qwen3-VL-2B-Instruct; robustness across fundamentally different architectures (e.g., pure language models) is not established.

## Confidence

**High confidence** in mechanism 1 (Unified Structural Schema) and mechanism 3 (Recurring Local Motifs): These are directly observable from unified schema construction and motif frequency analysis.

**Medium confidence** in mechanism 2 (Interleaved Role-Based Training): Clear effects shown in ablation, but optimal ratio appears sensitive and theoretical justification remains unclear.

**Medium confidence** in overall performance claims: Consistent improvements across four diverse tasks are compelling, but magnitude varies significantly by task and metric.

## Next Checks

1. **Cross-domain transfer boundary test:** Systematically vary relation vocabulary size and graph density between source and target domains to identify performance degradation thresholds.

2. **Schema granularity ablation:** Train G-Substrate with progressively coarser and finer relation typing schemes to identify optimal granularity for balancing transfer and specificity.

3. **Architecture transfer test:** Implement G-Substrate on a pure language model backbone (e.g., Llama-3) and compare performance to vision-language model results to assess architectural dependence.