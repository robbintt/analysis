---
ver: rpa2
title: 'VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation
  for Multimodal RL'
arxiv_id: '2511.18902'
source_url: https://arxiv.org/abs/2511.18902
tags:
- training
- vade
- data
- gradient
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gradient vanishing problem in group-based
  reinforcement learning for multimodal models, where identical rewards within groups
  cause advantage estimates to collapse and eliminate training signals. The authors
  propose VADE, a variance-aware dynamic sampling framework that integrates online
  sample-level difficulty estimation using Beta distributions, Thompson sampling for
  information gain maximization, and a two-scale prior decay mechanism.
---

# VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL

## Quick Facts
- **arXiv ID**: 2511.18902
- **Source URL**: https://arxiv.org/abs/2511.18902
- **Reference count**: 37
- **Primary result**: VADE achieves 99.5% of DAPO's peak performance while using nearly three times fewer rollout inferences on multimodal reasoning benchmarks

## Executive Summary
VADE addresses gradient vanishing in group-based reinforcement learning where identical rewards within groups cause advantage estimates to collapse. The framework introduces a variance-aware dynamic sampling approach that proactively selects informative samples before rollout generation using online sample-level difficulty estimation. By integrating Thompson sampling for information gain maximization and a two-scale prior decay mechanism, VADE maintains higher effective gradient ratios throughout training compared to baseline methods like vanilla GRPO/GSPO and DAPO.

## Method Summary
VADE operates as a plug-in to group-based RL methods (GRPO/GSPO) by maintaining per-sample Beta distributions over correctness probabilities. During training, it uses Thompson sampling to select the top-B samples with highest information gain I(θ) = p(1-p)², where p represents the estimated correctness probability. Selected samples undergo rapid decay (λ₁=0.2) while unselected samples decay slowly (λ₂=0.999), allowing the system to adapt to policy evolution. The framework is initialized with G=8 rollouts per sample and trains with batch size B=512 over 15 epochs using Qwen2.5-VL-7B-Instruct with Qwen2.5-72B-Instruct as judge.

## Key Results
- VADE achieves 99.5% of DAPO's peak performance while using nearly three times fewer rollout inferences
- Maintains significantly higher effective gradient ratios throughout training (consistently above 50% vs. <20% for baselines)
- Outperforms vanilla GRPO/GSPO and DAPO on MathVista, MathVerse, MathVision, ScienceQA, and ChartQA benchmarks
- Ablation studies show asymmetric information gain and Thompson sampling each contribute ~1-1.5% performance improvements

## Why This Works (Mechanism)

### Mechanism 1: Information Gain Objective via Intermediate Difficulty
Samples with correctness probability in intermediate ranges yield higher effective gradients than trivially easy (p≈1) or impossibly hard (p≈0) samples. The asymmetric information gain function I(θ) ≈ p(1-p)² peaks for challenging samples, prioritizing data where the model has partial competence and avoiding groups where all rollouts receive identical rewards.

### Mechanism 2: Thompson Sampling for Balanced Exploration-Exploitation
Sampling from Beta posteriors rather than selecting by mean estimates maintains diversity in selected batches while prioritizing high-information samples. This stochastic sampling gives uncertain samples occasional selection chances, preventing premature convergence to a fixed curriculum.

### Mechanism 3: Two-Scale Prior Decay for Non-Stationarity
Different decay rates for selected vs. unselected samples maintain accurate difficulty estimates as the policy evolves. Selected samples undergo rapid decay (λ₁=0.2) with new evidence added, while unselected samples undergo slow decay (λ₂=0.999), accounting for gradual generalization effects without new rollout data.

## Foundational Learning

- **Concept**: Group Relative Policy Optimization (GRPO/GSPO)
  - Why needed here: VADE operates as a plug-in to group-based methods; understanding advantage computation from group rollouts is prerequisite.
  - Quick check: Can you explain why advantage estimates collapse to zero when all 8 rollouts for a sample receive reward=1?

- **Concept**: Beta Distribution and Conjugate Priors
  - Why needed here: Core to VADE's online difficulty estimation; α and β parameters track pseudo-counts of correct/incorrect responses.
  - Quick check: Given α=3, β=7 (10 pseudo-trials), what is the expected value and how does adding 2 correct responses update the parameters?

- **Concept**: Thompson Sampling in Multi-Armed Bandits
  - Why needed here: VADE formulates sample selection as a non-stationary MAB problem solved via posterior sampling.
  - Quick check: Why does Thompson sampling naturally balance exploration and exploitation without explicit epsilon schedules?

## Architecture Onboarding

- **Component map**: Difficulty Estimator -> Thompson Sampler -> GRPO/GSPO Core -> Prior Decay Module
- **Critical path**: 1) Initialize: Generate G rollouts per sample, set (αi, βi) from initial results 2) Per-step: Thompson sample → select top-B samples → rollout → compute advantages → update policy → decay/update (α, β)
- **Design tradeoffs**: λ₁ too large → stale estimates; λ₁ too small → high variance. λ₂ too small → unselected samples lose all information; λ₂=1.0 → no decay, accumulation artifacts.
- **Failure signatures**: Effective gradient ratio drops despite VADE → check if all samples have converged to extreme α/(α+β) values. No improvement over vanilla → verify Thompson sampling is actually stochastic. Worse than vanilla → check for data leakage or hyperparameter misconfiguration.
- **First 3 experiments**: 1) Sanity check: Replicate GRPO baseline with random sampling on small subset; verify effective gradient ratio declines as shown in Figure 1 2) VADE core: Add Thompson sampling with asymmetric I=p(1-p)² and λ₁=0.2, λ₂=0.999; plot effective gradient ratio over training 3) Ablation sweep: Test symmetric p(1-p), greedy selection instead of Thompson, last-update instead of dual decay

## Open Questions the Paper Calls Out
- Future work will extend VADE to broader applications including mathematical reasoning and code generation, and explore integration with other policy optimization frameworks.
- How sensitive is VADE's performance to the specific choices of the two-scale prior decay hyperparameters (λ₁ = 0.2, λ₂ = 0.999)?
- How would VADE need to be modified to handle non-binary reward signals or continuous reward functions?
- Does VADE scale efficiently to training datasets with orders of magnitude more samples than the 9,471 used in experiments?

## Limitations
- Effectiveness of asymmetric information gain function assumes specific correctness probability distributions that may not generalize to all task types
- Two-scale decay mechanism assumes policy evolution affects selected samples more than unselected samples, which may vary across training dynamics
- Framework is currently limited to binary reward settings and multimodal reasoning tasks

## Confidence
- **High confidence**: Core problem identification and basic mechanism of using Beta distributions for difficulty estimation are well-established
- **Medium confidence**: Specific formulation of information gain function and two-scale decay rates are based on empirical tuning
- **Low confidence**: Assumption that Beta distribution accurately approximates correctness probability throughout significant policy evolution

## Next Checks
1. Derive optimal information gain function for multi-step case beyond Knapsack RL framework's single-step analysis
2. Test VADE on non-multimodal reasoning tasks (text-only reasoning, code generation) for cross-domain robustness
3. Systematically vary λ₁ and λ₂ decay rates across wider range to identify more robust decay schedules for different training regimes