---
ver: rpa2
title: 'Scaling with Collapse: Efficient and Predictable Training of LLM Families'
arxiv_id: '2509.25087'
source_url: https://arxiv.org/abs/2509.25087
tags:
- training
- loss
- arxiv
- preprint
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that when training large language models under\
  \ maximal update parameterization, the shapes of training loss curves (TLCs) across\
  \ model sizes can collapse onto a universal trajectory\u2014provided the AdamW timescale\
  \ (\u03C4), tokens-per-parameter (TPP), and learning rate schedule are held constant\
  \ or optimally scaled together. This collapse acts as a signature of compute-efficient\
  \ training."
---

# Scaling with Collapse: Efficient and Predictable Training of LLM Families
## Quick Facts
- arXiv ID: 2509.25087
- Source URL: https://arxiv.org/abs/2509.25087
- Reference count: 40
- When training large language models under maximal update parameterization, training loss curves across model sizes can collapse onto a universal trajectory when AdamW timescale, tokens-per-parameter, and learning rate schedule are held constant or optimally scaled together.

## Executive Summary
This paper demonstrates that when training large language models under maximal update parameterization, training loss curves across different model sizes can collapse onto a universal trajectory when key hyperparameters are held constant or optimally scaled. This collapse serves as a signature of compute-efficient training and enables two practical applications: early diagnosis of training issues through deviation from the expected trajectory, and early stopping in hyperparameter tuning by predicting final loss from partial training runs. The authors introduce Celerity, a competitive LLM family trained under collapse conditions, and show that fixed tokens-per-parameter bands with optimal AdamW timescales improve training efficiency and reliability.

## Method Summary
The authors analyze training loss curves (TLCs) across model sizes from 1B to 22B parameters, identifying conditions under which these curves collapse onto a universal trajectory. They demonstrate that when the AdamW timescale (τ), tokens-per-parameter (TPP), and learning rate schedule are held constant or optimally scaled together, the TLCs exhibit collapse behavior. They develop a parametric model of TLC shape that enables accurate predictions of final loss within 10-30% of training, allowing for early stopping in hyperparameter tuning. The Celerity family of models is introduced as a practical application of these findings, trained under collapse conditions with fixed-TPP bands to achieve competitive performance while improving efficiency and reliability.

## Key Results
- Training loss curves across model sizes collapse onto a universal trajectory when AdamW timescale, tokens-per-parameter, and learning rate schedule are held constant or optimally scaled
- Deviation from collapse serves as an early diagnostic for training issues, enabling early stopping in hyperparameter tuning
- A simple parametric model predicts final loss within 10-30% of training, saving significant tuning compute
- Fixed-TPP bands with optimal AdamW timescales improve training efficiency and reliability

## Why This Works (Mechanism)
The collapse phenomenon occurs because when key hyperparameters are properly scaled, the optimization dynamics become scale-invariant across model sizes. Under maximal update parameterization, the relative learning rates and weight decay schedules create similar optimization landscapes regardless of model size. When AdamW timescale (τ) and tokens-per-parameter (TPP) are held constant, the effective optimization trajectory becomes proportional across scales, causing the training loss curves to follow the same shape. This scale-invariance means that deviations from the expected trajectory can signal training issues, while the predictable shape enables early stopping based on partial runs.

## Foundational Learning
- **Maximal Update Parameterization**: Why needed - enables scale-invariant optimization; Quick check - verify parameter scaling follows μP rules
- **AdamW Hyperparameter Scaling**: Why needed - controls optimization timescale across model sizes; Quick check - confirm τ scales appropriately with model width
- **Tokens-per-Parameter Ratio**: Why needed - determines effective batch size and optimization stability; Quick check - verify TPP consistency across experiments
- **Training Loss Curve Analysis**: Why needed - provides diagnostic signal for training health; Quick check - confirm TLC shape matches parametric model predictions
- **Early Stopping Criteria**: Why needed - reduces hyperparameter tuning compute; Quick check - validate predicted final loss accuracy
- **Scale-Invariance in Optimization**: Why needed - explains why collapse occurs; Quick check - test if scaling relationships hold across different model architectures

## Architecture Onboarding
**Component Map:** Model Architecture -> Hyperparameter Configuration -> Training Process -> TLC Monitoring -> Early Stopping Decision
**Critical Path:** Model initialization with μP scaling → Training with fixed τ and TPP → TLC tracking → Parametric model fitting → Early stopping prediction
**Design Tradeoffs:** Fixed TPP improves efficiency but may limit flexibility; Early stopping saves compute but risks premature termination; Collapse enables diagnostics but requires strict hyperparameter control
**Failure Signatures:** TLC deviation from predicted trajectory → Training instability; Poor parametric model fit → Hyperparameter misconfiguration; No collapse across scales → Non-optimal scaling relationships
**First Experiments:** 1) Train 1B and 10B models with identical τ and TPP to verify collapse; 2) Apply early stopping on partial TLCs and compare predicted vs actual final loss; 3) Systematically vary τ and TPP to find optimal fixed bands

## Open Questions the Paper Calls Out
The paper leaves several open questions regarding the generalizability of collapse phenomena. It's unclear whether these findings extend to encoder-decoder architectures, smaller models (sub-1B parameters), or alternative architectures like Mamba. The relationship between collapse and compute efficiency could benefit from more rigorous theoretical grounding. The parametric model's sensitivity to different initial conditions and training regimes hasn't been fully explored. Additionally, the computational overhead of searching for optimal TPP values versus the efficiency gains achieved hasn't been comprehensively evaluated.

## Limitations
- Analysis focuses primarily on decoder-only Transformers in the 1-22B parameter range
- Relationship between collapse and compute efficiency could benefit from more rigorous theoretical grounding
- Claims about deviation from collapse as diagnostic tool lack extensive validation of false positive/negative rates

## Confidence
- Training loss curve collapse under specific conditions: High
- Collapse as signature of compute-efficient training: Medium
- Early stopping based on partial TLCs: Medium
- Fixed-TPP bands improve efficiency: High

## Next Checks
1. Test TLC collapse and prediction accuracy across different architectures (encoder-decoder, state-space models) and parameter ranges (sub-1B and multi-hundred-billion models)
2. Systematically evaluate false positive/negative rates for using TLC deviation as a training diagnostic across diverse failure modes
3. Conduct ablation studies on the parametric TLC model's sensitivity to initialization and training hyperparameters to establish robustness bounds