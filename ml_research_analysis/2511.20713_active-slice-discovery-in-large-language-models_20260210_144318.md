---
ver: rpa2
title: Active Slice Discovery in Large Language Models
arxiv_id: '2511.20713'
source_url: https://arxiv.org/abs/2511.20713
tags:
- slice
- active
- learning
- discovery
- slices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work formalizes and explores active slice discovery, an active
  learning approach to identifying coherent groups of model errors, called slices,
  in large language models. Given a pre-trained classifier and a small set of examples
  with known slice memberships, the method iteratively queries an annotator to label
  additional examples most likely to belong to the same slice, using uncertainty-based
  active learning strategies.
---

# Active Slice Discovery in Large Language Models

## Quick Facts
- arXiv ID: 2511.20713
- Source URL: https://arxiv.org/abs/2511.20713
- Reference count: 19
- Primary result: Uncertainty-based active learning with SAE representations achieves 85.8% slice detection accuracy using only 2-10% of labels.

## Executive Summary
This work formalizes and explores active slice discovery, an active learning approach to identifying coherent groups of model errors, called slices, in large language models. Given a pre-trained classifier and a small set of examples with known slice memberships, the method iteratively queries an annotator to label additional examples most likely to belong to the same slice, using uncertainty-based active learning strategies. Experiments on toxicity classification with Llama 3.1 show that uncertainty-based query strategies combined with sparse auto-encoder (SAE) representations achieve strong slice detection accuracy—up to 85.8%—using as few as 2% of available labels, significantly outperforming random sampling baselines. SAE features also stabilize training and improve performance on more complex slices.

## Method Summary
Active slice discovery leverages a small set of seed examples with known slice memberships to iteratively query an oracle for additional annotations. The method trains a slice classifier on LLM representations (either raw embeddings or SAE activations) and uses uncertainty-based query strategies (Least Confidence, Prediction Entropy, Breaking Ties) to select the most informative examples for annotation. The process repeats until a budget of K labels is exhausted. The approach is evaluated on toxicity classification with Llama 3.1, comparing uncertainty-based and diversity-based query strategies across different representation types.

## Key Results
- Uncertainty-based query strategies achieve 85.8% slice detection accuracy using only 2-10% of available labels.
- SAE features stabilize training and improve performance on complex, sentiment-based slices.
- Identity-based slices (e.g., "female," "christian") are detected more efficiently than sentiment-based slices (e.g., "disagree," "sad").

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty-based query strategies identify slice members more efficiently than random or diversity-based sampling.
- Mechanism: The active learner selects examples where the slice classifier has lowest confidence (highest uncertainty), which correlate with decision boundaries near the target slice. By querying these uncertain points, the model receives supervision precisely where it most needs to disambiguate slice membership.
- Core assumption: Uncertain predictions are more likely to lie near the true slice boundary than confident predictions.
- Evidence anchors:
  - [abstract] "uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information"
  - [Section 4.2] "uncertainty based query strategies yield higher accuracy with fewer labels across both embeddings and SAE based inputs. This finding aligns with prior work in text classification active learning"
  - [corpus] Weak direct support; neighbor papers focus on unsupervised slice discovery rather than active learning query strategies.
- Break condition: If uncertain examples cluster away from slice boundaries (e.g., due to label noise or feature ambiguity), uncertainty sampling may query uninformative points.

### Mechanism 2
- Claim: Sparse Auto-Encoder (SAE) representations improve slice detection stability and performance on complex slices compared to raw LLM embeddings.
- Mechanism: SAEs decompose dense LLM hidden states into sparse, interpretable feature activations. This sparsity may reduce feature interference and provide more disentangled representations that better separate semantic slice concepts.
- Core assumption: Sparse features correspond more directly to human-interpretable slice concepts (e.g., demographic identity, sentiment) than dense embeddings.
- Evidence anchors:
  - [abstract] "SAE features also stabilize training and improve performance on more complex slices"
  - [Section 4.1] "the detection rate for the disagree slice improves significantly from 0.8 with layer embeddings to 0.83 with SAE representations"
  - [Section 4.2] "The use of SAE features further improves the stability of the active learning training process, yielding smoother training curves"
  - [corpus] No direct validation of SAE-for-slice-discovery in neighbors; SAE interpretability work cited (Cunningham et al., Templeton et al.) but not empirically tested for this application.
- Break condition: If slices are defined by features not captured by the SAE's learned dictionary, sparse representations may discard relevant information.

### Mechanism 3
- Claim: Slice type (identity-based vs. sentiment-based) determines sample efficiency of active discovery.
- Mechanism: Identity-based slices (e.g., "female," "christian") have more consistent lexical cues, enabling the classifier to rapidly learn discriminative patterns. Sentiment-based slices (e.g., "disagree," "sad") exhibit higher lexical diversity and contextual variation, requiring more labeled examples.
- Core assumption: Slice coherence correlates with lexical/semantic consistency in the input space.
- Evidence anchors:
  - [Section 4.1] "identity based slices like female, christian can be detected with high accuracy using only a few hundred annotations, whereas some reaction based slices like disagree and sad fail to significantly improve with under 1000 labeled samples"
  - [Section 4.1] "slices with similar lexical cues can be easier to identify compared to heterogeneous and sentiment based slices"
  - [corpus] Neighbor paper "Error Slice Discovery via Manifold Compactness" similarly notes slice interpretability depends on semantic coherence, supporting this as a general property.
- Break condition: If an identity-based slice uses varied or obfuscated language (e.g., coded speech), it may behave like complex sentiment slices.

## Foundational Learning

- Concept: **Active Learning (Pool-Based)**
  - Why needed here: The core framework—selectively querying labels from an oracle to maximize model improvement per annotation—is the foundation of active slice discovery.
  - Quick check question: Can you explain why uncertainty sampling might outperform diversity sampling when the target concept is narrow (a specific error slice)?

- Concept: **Sparse Auto-Encoders for Interpretability**
  - Why needed here: Understanding how SAEs decompose activations into sparse features is necessary to interpret why they might help slice discovery.
  - Quick check question: What property of SAE reconstructions might make slice boundaries easier to learn—sparsity, reconstruction fidelity, or both?

- Concept: **Slice Discovery vs. Standard Classification**
  - Why needed here: This work targets *slice membership* (s), not task labels (y). The evaluation metric and problem structure differ from typical active learning.
  - Quick check question: In standard active learning, you query labels y. In active slice discovery, you query slice memberships s. How does this change the oracle's task?

## Architecture Onboarding

- Component map: Llama-3.1-8B -> (Raw Embeddings or SAE Features) -> Slice Classifier (MLP/SVM) -> Query Strategy -> Oracle Annotation -> Retrain Classifier

- Critical path:
  1. Extract representations from LLM/SAE for all pool examples
  2. Initialize with small labeled seed set (known slice memberships)
  3. Train slice classifier on current labeled set
  4. Query strategy selects unlabeled example with highest utility
  5. Oracle provides slice membership label
  6. Add to labeled set, retrain classifier
  7. Repeat until budget K exhausted

- Design tradeoffs:
  - **MLP vs. SVM classifier**: MLP achieves highest accuracy (85.8%) but requires hyperparameter tuning; SVM with SAE features is simpler (83.0%) with minimal tuning.
  - **Raw embeddings vs. SAE**: SAE provides stability and smoother learning curves; raw embeddings may achieve higher peak accuracy with careful tuning.
  - **Uncertainty vs. diversity queries**: Uncertainty better for focused slice discovery; diversity may help when slices are broadly distributed.

- Failure signatures:
  - **Flat learning curves**: Query strategy selecting uninformative points; try switching from diversity to uncertainty methods.
  - **High variance across runs**: Classifier unstable on small labeled sets; switch to SAE features or simpler model.
  - **Slow improvement on complex slices**: Expected behavior per Section 4.1; allocate larger budget for sentiment-based slices.

- First 3 experiments:
  1. **Baseline replication**: Run active slice discovery on the "disagree" slice with SAE features + SVM + Least Confidence. Verify you can reach ~80% accuracy with <1,000 labels.
  2. **Ablate representation**: Compare raw embeddings vs. SAE on the same slice. Measure accuracy at fixed label budgets (100, 500, 1000).
  3. **Query strategy sweep**: Test all three uncertainty methods (Least Confidence, Prediction Entropy, Breaking Ties) on an identity-based slice (e.g., "female"). Confirm all outperform random sampling with <500 labels.

## Open Questions the Paper Calls Out

- **Question**: How does active slice discovery perform when identifying multiple, potentially overlapping slices simultaneously?
  - Basis in paper: [explicit] The introduction states the "problem can become more complicated as we deal with... multiple slices," though the experiments focus on detecting individual slices in isolation.
  - Why unresolved: The current evaluation protocol detects specific slices (e.g., "disagree", "female") independently using a One-vs-Rest strategy, leaving the pipeline's ability to disentangle concurrent, distinct error modes untested.
  - What evidence would resolve it: Experiments on datasets with ground-truth overlapping slice memberships, evaluating the model's capacity to retrieve multiple distinct slices concurrently without performance degradation.

- **Question**: Can uncertainty-based query strategies effectively identify highly subtle or semantically complex error patterns?
  - Basis in paper: [explicit] The introduction highlights that the problem complicates with "more subtle error patterns," and results show sentiment-based slices (e.g., "disagree") require significantly more labels than identity-based slices.
  - Why unresolved: It is unclear if the high sample efficiency (2% labels) achieved on lexical or identity slices transfers to slices where the model's uncertainty signal is weak or indistinguishable from noise.
  - What evidence would resolve it: Evaluation on datasets containing nuanced, non-lexical error patterns (e.g., specific logical fallacies) to compare uncertainty-based sampling against diversity-based baselines.

- **Question**: Do the benefits of Sparse Auto-Encoder (SAE) representations for slice discovery generalize across different model architectures and tasks?
  - Basis in paper: [inferred] The study is limited to Llama 3.1 8B and the Jigsaw toxicity classification task.
  - Why unresolved: The superior performance of SAE features over raw embeddings might be specific to the Llama 3.1 architecture or the lexical nature of toxicity, rather than a general property of SAE representations in LLMs.
  - What evidence would resolve it: Ablation studies applying the same active slice discovery pipeline to different model families (e.g., Mistral, Gemma) and domains (e.g., medical text, QA).

## Limitations

- Results are based on a single model (Llama 3.1-8B) and dataset (Jigsaw Toxicity), limiting generalizability.
- The mechanism by which SAE representations improve slice discovery is not experimentally isolated (sparsity vs. interpretability vs. reduced interference).
- Claims about uncertainty sampling effectiveness assume uncertain points cluster near slice boundaries, which is not directly validated.

## Confidence

- **High confidence**: Slice detection accuracy numbers and relative ranking of query strategies on Jigsaw with Llama 3.1.
- **Medium confidence**: SAE benefits for stability and complex slice performance.
- **Low confidence**: Generalization of uncertainty sampling assumptions to other domains and models.

## Next Checks

1. **Validate uncertainty sampling assumptions**: Visualize the spatial distribution of uncertain examples in the representation space for multiple slices. Confirm that high-uncertainty points are indeed located near slice boundaries, and that low-uncertainty points are consistently inside or outside the slice.

2. **Ablate SAE vs. raw embeddings on diverse slices**: Test both representation types on a broader set of slices, including those with known linguistic diversity or adversarial patterns. Compare not only accuracy but also training stability and sensitivity to seed set size.

3. **Benchmark against unsupervised slice discovery**: Compare active slice discovery (with uncertainty sampling and SAE) to state-of-the-art unsupervised slice discovery methods (e.g., HiBug2, Error Slice Discovery via Manifold Compactness) on the same task and data. Measure the trade-off between annotation cost and detection performance.