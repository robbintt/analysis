---
ver: rpa2
title: Group Distributionally Robust Optimization with Flexible Sample Queries
arxiv_id: '2505.15212'
source_url: https://arxiv.org/abs/2505.15212
tags:
- lemma
- algorithm
- where
- probability
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates group distributionally robust optimization
  (GDRO) in scenarios where the number of samples per iteration can vary arbitrarily
  and dynamically. Existing GDRO algorithms are limited to processing either one or
  m samples per round, which is impractical in real-world settings where resources
  fluctuate or where processing multiple samples simultaneously can accelerate convergence.
---

# Group Distributionally Robust Optimization with Flexible Sample Queries

## Quick Facts
- **arXiv ID:** 2505.15212
- **Source URL:** https://arxiv.org/abs/2505.15212
- **Reference count:** 40
- **Primary result:** Achieves optimization error bound of O(1/t · √(∑_{j=1}^t (m/r_j)log m)) for GDRO with flexible sample sizes

## Executive Summary
This paper addresses the limitation in existing Group Distributionally Robust Optimization (GDRO) algorithms that can only process either one or m samples per iteration. The authors formulate GDRO with flexible sample queries as a two-player game and propose a unified algorithm that handles arbitrary sample sizes per round. The key innovation is a novel Prediction with Limited Advice (PLA) strategy that constructs appropriate loss estimators based on whether the sample size is 1 (using Implicit-eXploration) or ≥2 (using unbiased estimators). This approach achieves the first high-probability regret bound for non-oblivious PLA and demonstrates that optimization error decreases as sample size increases, while maintaining a consistent sample complexity of O(m log(m)/ε²) for any fixed sample size.

## Method Summary
The method formulates GDRO as a two-player game where one player solves a non-oblivious online convex optimization problem while the other tackles a Prediction with Limited Advice (PLA) problem supporting varying sample sizes. The key algorithm (Algorithm 1) uses Follow-the-Regularized-Leader (FTRL) with a unified strategy that combines Implicit-eXploration for single-sample rounds and unbiased estimators for multiple-sample rounds. The loss estimator (Eq. 14) normalizes observed losses based on sampling probabilities, with a critical IX term γ_t preventing explosion when sampling probabilities become small. The w-player uses FTRL with averaged outputs, enabling anytime guarantees without requiring knowledge of the total number of iterations.

## Key Results
- First high-probability regret bound for non-oblivious PLA with flexible sample queries
- Optimization error bound of O(1/t · √(∑_{j=1}^t (m/r_j)log m)) where r_t denotes sample size at round t
- Consistent sample complexity of O(m log(m)/ε²) for any fixed sample size r ∈ [m]
- Experimental validation showing faster convergence with more samples per round on both synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
The optimization error decreases as the number of samples per round (r_t) increases, allowing convergence acceleration without changing the fundamental sample complexity. The paper models GDRO as a two-player game where the q-player uses a novel PLA strategy that constructs specific loss estimators based on whether r_t=1 (using Implicit-eXploration) or r_t≥2 (using unbiased estimators). This allows the regret bound to scale with ∑(m/r_j) rather than just mt, meaning more samples per round directly reduce per-iteration variance. Core assumption: the sequence of sample sizes {r_t} can be arbitrary and varying, but the underlying distributions P_i are fixed, and the loss functions are convex and bounded. Evidence anchors: [abstract] constructing appropriate loss estimators for cases where the sample size is either 1 or not; [section 3.3] Eq. (14) defines the specific loss estimator. Break condition: If sample sizes r_t are not provided by the environment or if the loss estimators are not correctly normalized by q_{t,i} and r_t, the regret bounds likely fail to hold.

### Mechanism 2
Using Follow-the-Regularized-Leader (FTRL) with averaged outputs allows the algorithm to be "anytime" (no prior knowledge of total iterations needed) and improves error bounds by an O(log t) factor compared to Stochastic Mirror Descent (SMD). Standard SMD relies on step-size-weighted averages which introduce O(log t) factors due to accumulating {1/j} terms and require known time horizons. FTRL uses a simple uniform average (Eq. 8) and a specific regularizer, decoupling the step size from the output format. Core assumption: the specific regularizer ν_q (negative entropy) and ν_w (strong convexity) provide sufficient stability for the FTRL analysis in a non-oblivious setting. Evidence anchors: [abstract] updating the decision using follow-the-regularized-leader; [section 3.2] explicitly contrasts FTRL with SMD. Break condition: If the problem is non-convex, the FTRL guarantees derived from convex analysis may not apply.

### Mechanism 3
The "Unified Strategy" (Algorithm 1) integrates the exploration required for single-sample rounds with the exploitation possible in multi-sample rounds into a single update process. Instead of running two separate sub-algorithms, the unified approach aggregates losses into a single cumulative loss vector L_t. It uses a modified sampling step via DepRound to select r_t distributions without replacement, ensuring unbiased selection probabilities even when r_t varies. Core assumption: the DepRound algorithm correctly implements dependent rounding to ensure Pr[i ∈ C_t] matches the required probability mass exactly. Evidence anchors: [abstract] establish the first high-probability regret bound for non-oblivious PLA; [section 3.3] Algorithm 1 description and Eq. (16) for the unified update. Break condition: If r_t > m or if DepRound is replaced with simple independent sampling with replacement, the loss estimator bias correction becomes invalid.

## Foundational Learning

- **Concept: Group Distributionally Robust Optimization (GDRO)**
  - Why needed here: This is the core objective function (Eq. 1) the architecture solves. You must understand that it minimizes the worst-case risk across m distinct groups (distributions), not the average risk.
  - Quick check question: How does the q-player in this paper enforce the "worst-case" constraint dynamically?

- **Concept: Online Convex Optimization (OCO) & Regret**
  - Why needed here: The paper frames the optimization as a game where players minimize "regret" (the difference between incurred loss and the best fixed strategy in hindsight).
  - Quick check question: Why is the "non-oblivious" nature of the adversary (where the environment depends on past actions) a critical challenge for the theoretical bounds?

- **Concept: Prediction with Limited Advice (PLA)**
  - Why needed here: The q-player solves a PLA problem—a middle ground between Multi-Armed Bandits (observing 1 loss) and Expert Advice (observing all losses). The algorithm adapts based on how many "arms" (groups) are observed (r_t).
  - Quick check question: In PLA, if r_t=m, what does the q-player observe, and how does the loss estimator change compared to r_t=1?

## Architecture Onboarding

- **Component map:**
  Environment Oracle (E) -> q-player (Algorithm 1) -> w-player (Algorithm 2) -> Output
  Environment Oracle provides r_t and returns gradients/losses; q-player maintains cumulative loss L_t and uses DepRound for sampling; w-player maintains cumulative gradient F_t and updates model w_t.

- **Critical path:**
  1. Receive r_t from environment
  2. q-player computes distribution q_t and samples set C_t of size r_t
  3. Environment returns losses for i ∈ C_t
  4. Construct biased (IX) or unbiased loss estimators
  5. Update L_t (for q) and F_t (for w) and compute averages w̄_t, q̄_t

- **Design tradeoffs:**
  - Hybrid vs. Unified: The paper proposes a "Unified" algorithm over the "Hybrid" for elegance and efficiency, though both achieve the same bound. Unified is preferred for implementation.
  - FTRL vs. SMD: FTRL is chosen for "anytime" capability and better log-factors, but requires storing cumulative sums (F_t, L_t) rather than just current parameters.

- **Failure signatures:**
  - Explosion of Loss Estimates: If q_{t,i} becomes extremely small for a selected group i, the loss estimator can explode. The Implicit-eXploration (IX) term γ_t is critical to prevent this for r_t=1.
  - Stagnation: If r_t is consistently low (e.g., 1), convergence relies entirely on the Exp3-IX component and will be slow (O(√(m log m / t))).

- **First 3 experiments:**
  1. Dynamic Resource Validation: Implement the environment to vary r_t randomly in [1, m-1]. Plot "Max Risk vs Iterations" to verify if the proposed UNI/HYB methods outperform baselines like Online(1) or Online(1)' (Fig 2).
  2. Fixed Resource Ablation: Fix r at different values (1, 5, 10, m). Verify that the convergence rate improves proportionally to √(r) as predicted by the bound O(√(m log m / r t)) (Fig 4).
  3. Running Time vs. Samples: Plot "Max Risk vs Running Time" (Fig 3) to ensure that the overhead of processing multiple samples per round actually results in faster convergence in wall-clock time, not just iteration count.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the flexible sample query framework be extended to support Minimax Regret Optimization (MRO) algorithms?
  - Basis in paper: [explicit] The conclusion states future work could "extend support... to other optimization algorithms, such as the MRO algorithm".
  - Why unresolved: The current analysis focuses on GDRO, whereas MRO replaces the vanilla risk with excess risk, requiring different theoretical tools.
  - Evidence would resolve it: Theoretical regret bounds or optimization error guarantees for an MRO algorithm with varying sample sizes.

- **Open Question 2:** Can time-uniform optimization error bounds be established when the sample size sequence {r_t} is non-oblivious (adaptive)?
  - Basis in paper: [inferred] Theorem 2 provides a time-uniform guarantee but explicitly assumes the sequence {r_t} is "oblivious," which excludes settings where sample availability depends on past algorithmic states.
  - Why unresolved: The current time-uniform proof techniques rely on the sequence being fixed or independent of the algorithm's history.
  - Evidence would resolve it: A proof of Theorem 2 that holds simultaneously for all t without assuming the sequence {r_t} is oblivious.

- **Open Question 3:** How does the proposed method perform empirically and theoretically when applied to large language models (LLMs) with dynamic data access?
  - Basis in paper: [explicit] The conclusion suggests applying the approach to "practical applications with dynamic sampling, including... large language models".
  - Why unresolved: The theoretical analysis relies on convexity (Assumption 4) and linear models, while LLMs are non-convex and high-dimensional.
  - Evidence would resolve it: Convergence analysis for non-convex functions or empirical validation showing improved robustness in LLM fine-tuning tasks.

## Limitations

- The regret bounds assume convex losses and bounded gradients, which may not hold in deep learning settings where GDRO is commonly applied.
- The FTRL analysis depends critically on the specific choice of regularizers, and the regret bounds may not generalize to other regularizer choices.
- The paper focuses on the "non-oblivious" setting where sample sizes can vary, but does not address the more challenging "adversarial" setting where both sample sizes and losses could be chosen to maximize regret.

## Confidence

- **High Confidence:** The mechanism connecting flexible sample sizes to improved optimization error (Mechanism 1) is well-supported by the regret bound analysis and experimental validation.
- **Medium Confidence:** The FTRL vs SMD comparison (Mechanism 2) is theoretically sound but the practical significance of the log-factor improvement is less clear from the experiments.
- **Medium Confidence:** The unified algorithm's theoretical guarantees (Mechanism 3) appear correct, but the practical benefit over the hybrid approach is not extensively validated.

## Next Checks

1. **Robustness to Distribution Mismatch:** Test the algorithm when the group distributions P_i are not fixed but drift over time, to assess the practical limits of the non-oblivious assumption.
2. **Scalability to High Dimensions:** Evaluate performance on high-dimensional datasets (e.g., image classification with many groups) to verify the algorithm scales well beyond the 500-dimensional synthetic data used.
3. **Comparison with Deep GDRO:** Implement a deep learning version of the algorithm and compare against existing GDRO methods (e.g., GDRO with ERM) on standard fairness benchmarks like CelebA or Waterbirds.