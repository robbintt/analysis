---
ver: rpa2
title: Active Few-Shot Learning for Text Classification
arxiv_id: '2502.18782'
source_url: https://arxiv.org/abs/2502.18782
tags:
- flan-t5-rep
- support
- sample
- sampling
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an active learning-based few-shot text classification
  method that improves performance by iteratively selecting informative samples from
  an unlabeled pool. The method uses embedding-based representations (either encoder
  hidden states or model output scores) combined with sampling strategies including
  uncertainty, representativeness, and clustering to select samples for annotation.
---

# Active Few-Shot Learning for Text Classification

## Quick Facts
- **arXiv ID**: 2502.18782
- **Source URL**: https://arxiv.org/abs/2502.18782
- **Reference count**: 21
- **Primary result**: Iterative active learning with uncertainty and representativeness sampling improves few-shot text classification performance compared to random sampling

## Executive Summary
This paper introduces an active learning framework for few-shot text classification that iteratively selects informative samples from an unlabeled pool. The method combines uncertainty-based sampling (entropy of model predictions) with representativeness-based sampling (K-Means clustering) to identify the most informative samples for annotation. Experiments across five classification tasks demonstrate that the iterative approach consistently outperforms random sampling and achieves state-of-the-art results, particularly in low-resource settings with only 10-100 labeled samples available.

## Method Summary
The approach extracts embeddings from an unlabeled pool using either encoder hidden states (mean-pooled) or model output scores (softmax probabilities). It then applies sampling strategies including Random, Representative (K-Means clustering), Uncertainty (entropy-based), UnRep (uncertainty filtering + clustering), and ClUn (clustering + uncertainty) to select M samples for annotation. These samples are added to the support set and used to fine-tune a fresh model instance. The process repeats iteratively, with each iteration using the fine-tuned model from the previous round to extract better-informed embeddings. First iteration must use non-uncertainty methods (Random or Rep) since no prior model exists.

## Key Results
- Iterative approaches consistently outperform random sampling across all five classification tasks
- FLAN-T5-Rep(En)-ClUn(En) achieves 1.6% improvement over best non-iterative methods at K=100
- Combining uncertainty and representativeness sampling yields better performance than either alone
- Method works with different LLMs and shows significant gains in low-resource settings (K=10-100)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Active Learning Sample Selection
- **Claim**: Iteratively selecting informative samples from an unlabeled pool improves few-shot learning performance compared to random sampling.
- **Mechanism**: The approach cycles through embedding extraction, sampling, annotation, and fine-tuning. Each iteration uses the fine-tuned model from the previous round to extract better-informed embeddings, progressively improving sample quality in the support set.
- **Core assumption**: Sample quality matters more than quantity in FSL; systematic selection outperforms random sampling.
- **Evidence anchors**:
  - [abstract]: "the performance of FSL suffers when unsuitable support samples are chosen"
  - [section 5.1]: "iterative approaches outperform most non-iterative methods when K ≥ 20"
  - [corpus]: Paper 112250 (medical image analysis) supports combining FSL with active learning for sample selection
- **Break condition**: Performance gains diminish as K approaches full dataset size (Figure 3 shows plateau after K=96).

### Mechanism 2: Combined Uncertainty and Representativeness Sampling
- **Claim**: Combining uncertainty-based and representativeness-based sampling yields better performance than either alone.
- **Mechanism**: 
  - **ClUn**: Clusters data via K-Means, then selects the most uncertain sample from each cluster
  - **UnRep**: First identifies α×M uncertain samples, then applies representative sampling within that subset
  - This balances exploring diverse regions of the embedding space while focusing on confusing examples.
- **Core assumption**: Diversity and model uncertainty provide complementary signals for identifying informative samples.
- **Evidence anchors**:
  - [section 3.2]: Formal definitions of UnRep (α×M filtering + clustering) and ClUn (cluster-first + uncertainty)
  - [section 5.1]: "FLAN-T5-Rep(En)-ClUn(En) beats the best non-iterative methods on average by 1.6% points at K=100"
  - [corpus]: Paper 12829 supports importance of sample relations in few-shot classification
- **Break condition**: Uncertainty methods require a previously fine-tuned model; first iteration must use Rep or Random.

### Mechanism 3: Dual Embedding Extraction for Sample Representation
- **Claim**: Using both encoder hidden states and model output scores captures complementary information for sample selection.
- **Mechanism**: 
  - **En embeddings**: Mean-pooled encoder hidden states capture semantic similarity
  - **Sc embeddings**: Softmax probabilities over labels capture model confidence/uncertainty
- **Core assumption**: The model's internal representations encode task-relevant structure usable for sample selection.
- **Evidence anchors**:
  - [section 3.1]: "En provides a rich feature space that encodes the input sequence; Sc focuses on the model's confidence"
  - [section 5.5]: Both embeddings can be extracted in a single model pass, enabling efficiency
  - [corpus]: Weak direct evidence; corpus focuses on contrastive learning features (paper 12829)
- **Break condition**: Requires LLMs that expose encoder states and label logits; token-level processing needed for multi-token labels.

## Foundational Learning

- **Concept: Few-Shot Learning (FSL)**
  - Why needed here: The entire method operates in the regime where K=10-100 samples. Understanding why FSL is sensitive to sample quality explains why active selection matters.
  - Quick check question: Why might adding more randomly-selected samples hurt performance in FSL?

- **Concept: Active Learning (AL) Sampling Strategies**
  - Why needed here: The method combines three classic AL strategies—uncertainty (entropy), representativeness (clustering), and diversity. Each serves a distinct purpose.
  - Quick check question: What problem does each strategy address, and when might each fail?

- **Concept: Encoder-Decoder LLM Architecture**
  - Why needed here: The embedding extraction requires accessing both encoder hidden states (for semantic similarity) and decoder output logits (for uncertainty). Understanding this architecture is prerequisite for implementation.
  - Quick check question: How do you extract encoder hidden states vs. decoder logits from a BART/FLAN-T5 model?

## Architecture Onboarding

- **Component map**: Pre-trained Model -> Embedding Extractor (En, Sc) -> Sampler (Random, Rep, Un, UnRep, ClUn) -> Support Set -> Unlabeled Pool

- **Critical path**:
  1. Initialize: Empty support set, full unlabeled pool, pre-trained model
  2. Extract embeddings from unlabeled pool using current model
  3. Select M samples (M=10 default; M=5 showed better results at same K)
  4. Annotate (oracle), add to support set
  5. Fine-tune fresh model instance on augmented support set
  6. Repeat from step 2 using fine-tuned model
  7. First iteration must use Rep or Random (no prior model for uncertainty)

- **Design tradeoffs**:
  - Iterative vs. non-iterative: ~2 hours vs. ~10 minutes (Table 5); significant performance gain at low K, diminishing returns at high K
  - M selection size: Smaller M (5) outperformed M=10 at same K, but requires more iterations
  - α parameter in UnRep: Controls uncertainty filtering; experiments suggest α=10 is reasonable (Table 8 shows stability across α∈{1,50})

- **Failure signatures**:
  - ICL approaches show stagnant or declining performance as K increases (cannot leverage larger support sets)
  - High variance in ICL results vs. FT (Table 2 shows larger standard deviations)
  - MPQA Intensity: Only iterative methods beat majority baseline; non-iterative methods fail completely

- **First 3 experiments**:
  1. Reproduce FLAN-T5-Rep(En)-ClUn(En) on MPQA Polarity with M=10, K∈{10,50,100} to validate pipeline
  2. Ablation: Compare Rep vs. ClUn vs. UnRep to isolate contribution of uncertainty vs. clustering
  3. Test generalization on a new dataset with different characteristics (e.g., more classes, different domain) to assess robustness

## Open Questions the Paper Calls Out

- **Open Question 1**: How does integrating semi-supervised learning methods into the pipeline affect the performance of the proposed active few-shot learning approach?
  - Basis in paper: [explicit] The conclusion states that future work "will explore the effect of semi-supervised learning methods on top of our approach in a pipeline, making use of the rest of the unlabeled data to improve performance."
  - Why unresolved: The current study strictly uses the unlabeled pool for sampling candidates to be annotated, ignoring the remaining unlabeled data rather than utilizing it for training.
  - What evidence would resolve it: Experiments comparing the current baseline against a modified pipeline that applies semi-supervised techniques (e.g., consistency regularization) to the unselected data.

- **Open Question 2**: Can the proposed embedding-based sampling strategies be effectively adapted for few-shot text generation tasks?
  - Basis in paper: [explicit] The authors list "other types of NLP tasks, such as text generation" as a specific direction for future work to expand beyond classification.
  - Why unresolved: The current methodology relies on classification-specific metrics (logit probabilities for uncertainty and encoder hidden states), which do not directly transfer to open-ended generation.
  - What evidence would resolve it: A study defining new uncertainty or representativeness metrics suitable for sequence generation and evaluating them on generation benchmarks.

- **Open Question 3**: Does the active sampling framework generalize effectively to non-English languages and alternative datasets?
  - Basis in paper: [explicit] The Limitations section notes, "In the current study, we have centered our attention on English. In the future, we plan to focus on other natural languages."
  - Why unresolved: The empirical validation was restricted to English datasets (MPQA, AG News, Amazon Reviews), leaving cross-lingual robustness untested.
  - What evidence would resolve it: Applying the same active learning strategies to multilingual corpora to determine if "Cluster Uncertainty" and other sampling methods retain their performance edge.

## Limitations

- The iterative approach requires multiple fine-tuning runs (2 hours vs. 10 minutes for non-iterative), creating computational overhead that may limit practical applicability
- Method shows diminishing returns as K approaches full dataset size, suggesting limited scalability beyond the few-shot regime
- Approach depends on accessing both encoder hidden states and decoder output probabilities, restricting applicability to encoder-decoder architectures like BART and FLAN-T5

## Confidence

- **High Confidence**: The core finding that iterative active learning outperforms random sampling and achieves state-of-the-art results is well-supported by extensive experiments across five datasets and multiple sampling strategies
- **Medium Confidence**: The claim that combining uncertainty and representativeness sampling yields superior performance is supported by experimental evidence, but optimal parameter settings are somewhat heuristic
- **Low Confidence**: The assertion that both encoder embeddings (En) and score embeddings (Sc) capture complementary information for sample selection lacks strong empirical validation

## Next Checks

1. **Reproducibility Validation**: Replicate the FLAN-T5-Rep(En)-ClUn(En) pipeline on MPQA Polarity with M=10, K∈{10,50,100} using the exact data splits and label mappings to verify the core performance claims. Report mean and standard deviation across 5 seeds to assess result stability.

2. **Embedding Type Ablation**: Systematically compare Rep(En) vs. Rep(Sc) vs. ClUn(En) vs. ClUn(Sc) across all five tasks to isolate the contribution of encoder vs. score embeddings to sample selection quality. This would validate whether both embedding types are truly necessary or if one dominates.

3. **Cross-Domain Generalization**: Test the approach on a new dataset with different characteristics (e.g., more classes, different domain such as biomedical text or legal documents) to assess robustness and identify potential failure modes beyond the evaluated tasks. This would help determine whether the method generalizes beyond the specific dataset characteristics of the current evaluation.