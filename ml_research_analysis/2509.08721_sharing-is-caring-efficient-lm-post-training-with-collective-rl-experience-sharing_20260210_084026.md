---
ver: rpa2
title: 'Sharing is Caring: Efficient LM Post-Training with Collective RL Experience
  Sharing'
arxiv_id: '2509.08721'
source_url: https://arxiv.org/abs/2509.08721
tags:
- rollouts
- sharing
- swarm
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Swarm sAmpling Policy Optimization (SAPO),
  a fully decentralized and asynchronous reinforcement learning algorithm designed
  for heterogeneous compute nodes to collaboratively post-train language models. The
  core innovation is allowing each node to train its own policy while sharing decoded
  rollouts, enabling lightweight experience exchange without synchronization overhead.
---

# Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing

## Quick Facts
- arXiv ID: 2509.08721
- Source URL: https://arxiv.org/abs/2509.08721
- Reference count: 14
- One-line primary result: Swarm Sampling Policy Optimization (SAPO) improves RL post-training efficiency by up to 94% through decentralized rollout sharing across heterogeneous nodes.

## Executive Summary
This paper introduces Swarm Sampling Policy Optimization (SAPO), a decentralized reinforcement learning algorithm for post-training language models. SAPO enables each compute node to train its own policy while asynchronously sharing decoded rollouts, eliminating synchronization bottlenecks and supporting heterogeneous architectures. In experiments with 0.5B parameter models on the ReasoningGYM dataset, SAPO achieved up to 94% cumulative reward improvement compared to standard RL fine-tuning, with optimal performance at a 4:4 local-to-external rollout ratio. The approach scales to thousands of nodes and reduces financial costs by enabling collaborative learning without centralized coordination.

## Method Summary
SAPO is a fully decentralized RL algorithm where each node independently trains its policy using GRPO while broadcasting decoded rollouts to peers. Nodes sample questions locally, generate multiple completions, and share a subset as decoded text with metadata. External rollouts are filtered (removing zero-advantage samples) and uniformly sampled alongside local rollouts for policy updates. The system operates asynchronously without requiring weight synchronization or model homogeneity, allowing nodes to function in isolation if needed. GRPO updates use asymmetric clipping thresholds and KL penalties, with cumulative verifiable rewards as the optimization objective.

## Key Results
- SAPO achieved 94% cumulative reward improvement over baseline (8 local/0 external) with 4 local/4 external rollout ratio
- Performance degraded with high external ratios (2 local/6 external) due to oscillations and forgetting
- Large-scale testing with thousands of heterogeneous nodes showed consistent performance gains for mid-capacity models

## Why This Works (Mechanism)

### Mechanism 1
Sharing decoded rollouts across a swarm enables faster learning by propagating successful reasoning patterns ("Aha moments"). When one node discovers a successful reasoning path, sharing it as decoded text allows other nodes to re-encode and learn from that experience via their own policy gradient calculations, bootstrapping exploration. Core assumption: reasoning strategies discovered by one model can be usefully absorbed by other models regardless of architectural differences.

### Mechanism 2
A balanced ratio of local to external rollouts (4:4) maximizes learning, while over-reliance on external samples causes instability. Local rollouts maintain policy coherence; external rollouts increase diversity. Too many external samples dilute local signal and expose high-performing nodes to lower-quality experiences, causing oscillation and forgetting. Core assumption: nodes contribute comparable-quality rollouts; the swarm has sufficient density of useful experiences.

### Mechanism 3
Fully decentralized, asynchronous sharing avoids synchronization bottlenecks and enables heterogeneous participation. Nodes broadcast rollouts in decoded format without requiring weight synchronization or model homogeneity. Each node independently filters, samples, and updates its policy using any compatible RL algorithm. Core assumption: network latency and node churn do not catastrophically disrupt rollout availability; local reward computation remains reliable.

## Foundational Learning

- **Policy Gradient Methods (PPO/GRPO)**: SAPO uses GRPO for local policy updates; understanding advantage estimation and clipping is essential for debugging training dynamics. Quick check: Can you explain why asymmetric clipping thresholds (ϵ_low=0.2, ϵ_high=0.28) might reduce instability?
- **RL with Verifiable Rewards (RLVR)**: The paper uses rule-based verifiers (ReasoningGYM) to assign binary rewards; understanding how verifiable rewards differ from learned reward models clarifies the experimental setup. Quick check: How does a programmatic verifier differ from a learned reward model in RLHF?
- **Decentralized/Peer-to-Peer Communication**: SAPO's architecture assumes gossip-style rollout sharing; basic understanding of eventual consistency and node discovery helps with implementation. Quick check: What happens to shared rollouts if a node joins or leaves the swarm mid-round?

## Architecture Onboarding

- **Component map**: Node -> Rollout generator -> Broadcaster -> Sampler -> Policy updater
- **Critical path**: 1. Sample batch B_n from local questions 2. Generate rollouts R_n(q) for each q ∈ B_n 3. Broadcast C_n(q) = (q, y_q, R_n(q), M_n) for subset S_n 4. Receive external rollouts from peers; filter zero-advantage samples 5. Assemble T_n with I local + J external samples 6. Compute rewards via ρ_n; update π_n via GRPO 7. Repeat for N rounds
- **Design tradeoffs**: Local/External ratio: Higher external increases diversity but risks instability (4:4 optimal in experiments; 2:6 oscillates); Filtering strategy: Discarding zero-advantage rollouts improves sample efficiency but requires local re-encoding overhead; Synchronization vs. asynchrony: Fully async scales better but rollout staleness may slow convergence
- **Failure signatures**: Oscillating rewards with high external ratio: Indicates over-reliance on lower-quality swarm rollouts; reduce J_n; No improvement over baseline: Check if external rollouts are being filtered too aggressively or if swarm density is too low; Node dropout stalling training: Expected in async setup; ensure graceful degradation and local fallback
- **First 3 experiments**: 1. Baseline validation: Run J=0 (no sharing) to establish local RL performance; verify reward model and GRPO implementation 2. Ratio sweep: Test I:J configurations (8:0, 6:2, 4:4, 2:6) on a single task type; plot reward trajectories and identify oscillation onset 3. Heterogeneity test: Mix two different model sizes (e.g., 0.5B and 0.6B Qwen) in the same swarm; compare whether benefits transfer across model capacities

## Open Questions the Paper Calls Out
1. Can hybrid approaches, such as reward-guided sharing or generative verifiers, mitigate the learning instability observed when nodes rely heavily on external rollouts?
2. Can advanced sampling strategies enable stronger, higher-capacity models to benefit from swarm participation?
3. Does increased heterogeneity in model architectures and specializations amplify the performance gains of SAPO?

## Limitations
- Limited empirical scope to 0.5B models and programmatic verification, leaving scalability to larger models and real-world reward models untested
- Unclear experimental design details (batch sizes, advantage estimation) create reproduction challenges
- Theoretical gaps in explaining why reasoning patterns transfer across heterogeneous models

## Confidence
- **High confidence**: SAPO's architectural feasibility and basic implementation are well-specified and technically sound
- **Medium confidence**: The 94% performance improvement claim is credible within controlled conditions but may not generalize to larger models or complex reward landscapes
- **Low confidence**: The scalability claims for "thousands of heterogeneous nodes" are not empirically validated and remain speculative

## Next Checks
1. Deploy SAPO across multiple model sizes (0.5B, 1.5B, 3B) and architectures (Qwen, Llama, Mistral) to verify cross-model reasoning transfer claims
2. Replace programmatic verifiers with learned reward models (e.g., from RLHF pipelines) to test whether sharing degrades with more complex, probabilistic rewards
3. Implement network latency and node churn in simulation to verify decentralized operation remains robust under realistic failure modes