---
ver: rpa2
title: 'Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge
  Graphs through Human-Inspired Reasoning'
arxiv_id: '2507.16971'
source_url: https://arxiv.org/abs/2507.16971
tags:
- mkgqagent
- step
- query
- plan
- sparql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mKGQAgent, a multilingual knowledge graph
  question answering system that employs a human-inspired LLM agent architecture to
  convert natural language questions into SPARQL queries. The system decomposes the
  complex task into modular subtasks (planning, entity linking, query refinement)
  using coordinated agent workflows and an experience pool for in-context learning.
---

# Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning

## Quick Facts
- arXiv ID: 2507.16971
- Source URL: https://arxiv.org/abs/2507.16971
- Reference count: 40
- Primary result: Achieved 54.83% F1 on English QALD-9-plus, winning 2025 Text2SPARQL challenge

## Executive Summary
This paper introduces mKGQAgent, a multilingual knowledge graph question answering system that converts natural language questions into SPARQL queries across 10 languages. The system employs a human-inspired LLM agent architecture that decomposes the complex task into modular subtasks including planning, entity linking, and query refinement. By using coordinated agent workflows and an experience pool for in-context learning, mKGQAgent achieves state-of-the-art performance on the QALD-9-plus benchmark and took first place in the 2025 Text2SPARQL challenge. The approach demonstrates particular strength in handling low-resource languages compared to existing systems, though performance varies significantly across language families.

## Method Summary
mKGQAgent implements a modular LLM agent architecture that breaks down the text-to-SPARQL conversion into specialized subtasks. The system uses a coordination mechanism where multiple agents work in parallel on planning, entity recognition, and query refinement phases. A key innovation is the experience pool that stores successful query patterns for in-context learning during inference. The system processes questions in their native languages while leveraging machine translation as a fallback strategy. The architecture is evaluated on the QALD-9-plus benchmark covering 10 languages, demonstrating superior multilingual capability compared to monolingual baselines and traditional end-to-end approaches.

## Key Results
- Achieved 54.83% F1 score on English questions in QALD-9-plus benchmark
- First place winner in the 2025 Text2SPARQL challenge
- Demonstrated superior performance on low-resource languages compared to translation-based baselines
- Showed effective balance between query quality and computational efficiency across multilingual queries

## Why This Works (Mechanism)
The system's effectiveness stems from decomposing a complex end-to-end task into manageable subtasks that mirror human reasoning patterns. By breaking down query generation into planning, entity linking, and refinement phases, each component can focus on specific challenges while benefiting from specialized training. The experience pool enables in-context learning by providing successful query patterns, reducing the need for extensive fine-tuning. The modular design allows parallel processing of different aspects of the query generation pipeline, while the coordination mechanism ensures consistency across subtasks. This human-inspired decomposition proves particularly effective for multilingual scenarios where different languages present unique entity recognition and semantic mapping challenges.

## Foundational Learning
- **Modular decomposition**: Breaking complex tasks into specialized subtasks allows focused optimization and parallel processing. Quick check: Verify each module can operate independently and produce intermediate outputs that can be validated.
- **In-context learning**: Using experience pools provides relevant examples without additional training. Quick check: Measure performance improvement when experience pool size increases and test with novel query patterns.
- **Multilingual entity linking**: Mapping entities across languages requires understanding of linguistic variations and cultural context. Quick check: Test entity recognition accuracy across different language families and script systems.
- **Query refinement iteration**: Multiple refinement passes improve accuracy by correcting errors from earlier stages. Quick check: Compare performance with different numbers of refinement iterations and measure diminishing returns.
- **Human-inspired reasoning**: Mimicking human problem-solving approaches through systematic decomposition and validation. Quick check: Analyze error patterns to identify whether they align with typical human reasoning mistakes.

## Architecture Onboarding

**Component Map**: User Question -> Planning Agent -> Entity Linking Agent -> Query Generation Agent -> Refinement Agent -> SPARQL Output

**Critical Path**: The planning agent initiates the workflow by analyzing question structure and identifying required entities and relationships. Entity linking follows to map natural language references to knowledge graph entities. Query generation creates initial SPARQL candidates, which refinement agents then iteratively improve based on semantic validation and syntax checking.

**Design Tradeoffs**: The modular approach increases computational overhead due to multiple LLM calls but enables specialized optimization for each subtask. Using native language processing preserves semantic nuances but requires multilingual capability. The experience pool reduces inference costs through in-context learning but may introduce bias toward previously seen patterns. Machine translation fallback provides coverage for unsupported languages but sacrifices accuracy.

**Failure Signatures**: Common failure modes include incorrect entity linking due to ambiguous references, planning errors when questions contain implicit assumptions, and refinement loops that fail to converge on valid SPARQL. Low-resource languages often exhibit higher error rates due to limited training data and entity coverage in knowledge graphs.

**First 3 Experiments**:
1. Run ablation test comparing full mKGQAgent against versions with individual modules disabled to quantify each component's contribution
2. Measure performance degradation when removing the experience pool to assess in-context learning benefits
3. Compare native language processing versus machine-translated English queries across all 10 languages to evaluate true multilingual advantage

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance variability between high-resource and low-resource languages, with unclear consistency of human-inspired reasoning benefits across all language families
- Limited benchmark representativeness, as QALD-9-plus may not capture real-world complexity and the 2025 Text2SPARQL challenge lacks comparative baselines
- High computational overhead from modular agent architecture without quantitative analysis of inference costs or production feasibility

## Confidence
- **High confidence**: Modular decomposition approach and experience pool for in-context learning are well-established with demonstrated effectiveness in results
- **Medium confidence**: "Human-inspired" reasoning claims are supported by architectural choices but lack empirical validation comparing agent behavior to human patterns
- **Medium confidence**: Machine translation viability for non-English queries is supported by results but needs further validation with language-specific translation quality assessments

## Next Checks
1. Conduct ablation studies comparing mKGQAgent's performance across languages when using translated queries versus native language processing to quantify true multilingual advantage
2. Perform detailed error analysis categorizing failures by query complexity, entity type, and language family to identify systematic weaknesses in agent architecture
3. Measure end-to-end computational costs (token counts, latency) across different query complexities and languages to validate claimed balance between quality and efficiency