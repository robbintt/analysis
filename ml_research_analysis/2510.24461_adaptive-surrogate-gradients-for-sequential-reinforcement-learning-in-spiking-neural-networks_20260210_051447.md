---
ver: rpa2
title: Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking
  Neural Networks
arxiv_id: '2510.24461'
source_url: https://arxiv.org/abs/2510.24461
tags:
- gradient
- training
- learning
- spiking
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training spiking neural networks
  (SNNs) for reinforcement learning in continuous control tasks, particularly drone
  navigation, where non-differentiable spiking neurons and the need for sequence-based
  training create significant obstacles. The authors analyze how surrogate gradient
  slope settings affect learning, showing that shallower slopes improve exploration
  in RL by increasing gradient magnitude but reducing alignment with true gradients.
---

# Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks

## Quick Facts
- arXiv ID: 2510.24461
- Source URL: https://arxiv.org/abs/2510.24461
- Reference count: 40
- Primary result: 4.5× training efficiency improvement using adaptive surrogate gradients in drone navigation

## Executive Summary
This paper addresses the challenge of training spiking neural networks (SNNs) for reinforcement learning in continuous control tasks, particularly drone navigation, where non-differentiable spiking neurons and the need for sequence-based training create significant obstacles. The authors analyze how surrogate gradient slope settings affect learning, showing that shallower slopes improve exploration in RL by increasing gradient magnitude but reducing alignment with true gradients. They propose an adaptive slope scheduling method that improves training efficiency by 4.5×. To address the warm-up period challenge in sequence-based training, they introduce a novel jump-start framework (TD3BC+JSRL) that uses a privileged guiding policy to bootstrap learning while leveraging online environment interactions. Combining this with adaptive surrogate gradients, they achieve an average return of 400 points in drone position control—substantially outperforming prior methods (BC, TD3BC, TD3) which achieve at most -200 points. Their approach demonstrates competitive performance to ANNs while maintaining energy efficiency benefits for neuromorphic deployment.

## Method Summary
The authors develop TD3BC+JSRL, a framework for training spiking neural networks in reinforcement learning tasks that combines adaptive surrogate gradient scheduling with a jump-start mechanism. The method uses a pre-trained privileged ANN guiding policy to control initial timesteps, allowing the SNN to warm up its internal states before taking over control. The SNN actor uses Leaky Integrate-and-Fire neurons with a fast sigmoid surrogate gradient whose slope is adaptively scheduled based on reward performance. The critic is a standard ANN that receives privileged action history information. Training uses asymmetric actor-critic TD3BC with behavioral cloning loss that decays over time. The adaptive slope scheduler adjusts the surrogate gradient slope between 1 and 100 based on a 9-timestep reward window, enabling better exploration in RL compared to fixed steep slopes.

## Key Results
- Achieves 400-point average return in drone position control, outperforming prior methods (BC, TD3BC, TD3) which achieve at most -200 points
- Demonstrates 4.5× training efficiency improvement with adaptive surrogate gradient slope scheduling versus fixed steep slopes
- Successfully bridges the SNN warm-up period using jump-start framework, enabling sequence-based training in risky environments
- Shows shallow surrogate gradients improve RL exploration through gradient noise injection while maintaining competitive performance to ANNs

## Why This Works (Mechanism)

### Mechanism 1
Shallow surrogate gradient slopes facilitate exploration in deep reinforcement learning by acting as a noise injection mechanism, whereas steep slopes constrain gradient flow. The surrogate gradient slope $k$ controls the width of the gradient approximation. Shallow slopes (low $k$) allow non-zero gradients for a wider range of inputs, maintaining gradient magnitude in deep layers but introducing directional noise (low cosine similarity with the "true" steep gradient). This noise functions similarly to parameter space noise, encouraging exploration in RL.

### Mechanism 2
Sequence-based training fails in robotic control without a mechanism to bridge the SNN "warm-up" period because early policies crash before the network state stabilizes. SNNs are stateful; they require a specific number of timesteps (warm-up) to stabilize hidden states before useful gradients can be computed. In risky environments (e.g., drone flight), untrained policies crash quickly (e.g., < 100 steps), resulting in replay buffers filled with short, unusable sequences that never bridge the required warm-up (e.g., 50 steps).

### Mechanism 3
A "Jump-Start" framework (TD3BC+JSRL) enables stable SNN training by decoupling data collection (using a privileged guide) from policy optimization. A pre-trained, privileged (non-spiking) guide policy controls the initial timesteps of an episode. This guarantees long trajectories, allowing the spiking policy to "observe" and warm up its hidden states safely. The spiking policy takes over for the remaining steps, and the BC loss term decays exponentially to shift from imitation to reward optimization.

## Foundational Learning

### Concept: Surrogate Gradients
Why needed here: SNNs use non-differentiable binary spikes (Heaviside function). To apply backpropagation, one must substitute a differentiable approximation (e.g., fast sigmoid) during the backward pass.
Quick check question: How does the slope hyperparameter $k$ in the surrogate function affect the trade-off between gradient vanishing (steep slope) and gradient noise (shallow slope)?

### Concept: SNN Warm-up Period
Why needed here: SNNs have internal memory (membrane potential). At $t=0$, states are zero-initialized. They need a burn-in period to reach a representative steady state before the output is reliable or gradients are meaningful.
Quick check question: Why does frame-stacking (used in standard RL) waste this property in SNNs, and why does early termination break it?

### Concept: Asymmetric Actor-Critic (TD3BC)
Why needed here: The system uses a Spiking Actor (for efficiency) but a standard ANN Critic (for training stability).
Quick check question: Why is it advantageous for the Critic to receive "privileged information" (action history) that the Spiking Actor does not receive at inference time?

## Architecture Onboarding

### Component map:
State (18) -> SNN Actor (18→256→128→4) -> Actions -> Environment -> Reward
State + Action History (32) -> ANN Critic -> Q-value
Guide Policy (ANN) -> Early timesteps -> Warm-up period

### Critical path:
1. Train Guide: Train the privileged ANN actor until it can maintain stable hover
2. Hybrid Rollout: Run episodes where the Guide controls steps 0→t_warmup, and the SNN controls steps t_warmup→T
3. Sequence Update: Store full sequences in buffer. Sample, run backprop through time (BPTT), and update SNN weights using TD3BC loss

### Design tradeoffs:
- Fixed vs. Adaptive Slope: Fixed slopes require sweeping; Adaptive slopes automate exploration/exploitation but add hyperparameters (scheduler sensitivity)
- BC Decay Rate: If λ decays too fast, the agent might crash before learning stabilizers. If too slow, it mimics the suboptimal guide

### Failure signatures:
- Silent Weights / No Spiking: Slope k is too steep, causing vanishing gradients in early training
- Random Behavior: Slope k is too shallow (alignment → 0), causing weight updates to be effectively random
- Immediate Crash: Warm-up period is insufficient or Guide policy is failing
- Oscillation at Hover: The SNN converges but exhibits higher frequency oscillation than the ANN counterpart

### First 3 experiments:
1. Slope Validation (Supervised): Train a shallow SNN on a static dataset (BC) with varying fixed slopes (k=1, 10, 100) to confirm the paper's finding that performance is largely slope-invariant in this setting
2. Ablation on Warm-up: Attempt to train the SNN from scratch without the Guide policy (vanilla TD3) in the drone sim. Verify if the agent fails to gather sequences longer than the warm-up period
3. Full Integration: Implement the Adaptive Slope Scheduler + JSRL. Measure epochs to reach Reward=100. Verify the "4.5x speedup" claim by comparing against a fixed steep slope (k=100) baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can the requirement for a pre-trained guiding policy be relaxed or replaced with a more automated initialization procedure for domains where stable baseline policies are difficult to obtain? The authors state in the conclusion: "Our method depends on the availability of a guiding policy... it must still produce stable behavior early in training, which might not always be trivial to obtain." This remains unresolved as no analysis is provided for scenarios where such a policy is unavailable or expensive to train.

### Open Question 2
What mechanisms can reduce the oscillatory behavior observed in deployed SNN controllers while preserving the temporal processing benefits? The paper reports: "When deployed on the Crazyflie, the spiking actor exhibits oscillatory behavior" and suggests future work could "incorporate angular velocity penalties in the reward function, use throttle deviation outputs rather than absolute throttle settings, train for longer, or increased control frequency." Multiple potential remedies are proposed but none are evaluated.

### Open Question 3
Do the findings on surrogate gradient slope scheduling generalize to deeper SNN architectures and different neuron models? The analysis uses a 4-layer SNN with 64 neurons per layer (Section 3.3) and the deployed controller uses only 2 hidden layers. The theoretical analysis in Section 8.1 derives bias accumulation for sigmoid networks but acknowledges "we believe the qualitative findings hold for spiking neurons as well." Whether slope scheduling remains beneficial in architectures with more layers or different spiking neuron models (e.g., Izhikevich, Hodgkin-Huxley) is untested.

## Limitations

- Adaptive slope scheduling mechanism lacks explicit details on reward normalization and gradient computation over the 9-timestep window
- Warm-up period duration (50 timesteps) appears arbitrary without ablation studies showing sensitivity to this hyperparameter
- Behavioral cloning decay rate (0.99/epoch) and curriculum progression frequency are not fully specified

## Confidence

- High Confidence: The core mechanism that shallow surrogate gradients improve RL exploration (Mechanism 1) is well-supported by empirical results showing poor performance with steep slopes in RL tasks. The failure of vanilla TD3 without jump-starting (Mechanism 2) is convincingly demonstrated through ablation.
- Medium Confidence: The jump-start framework's effectiveness (Mechanism 3) is strongly supported, but the assumption that training a privileged ANN guide is "easier" than direct SNN training is asserted rather than empirically validated through comparative complexity analysis.
- Low Confidence: The exact implementation details of the adaptive slope scheduler and curriculum learning progression are underspecified, making precise replication challenging.

## Next Checks

1. Implement and validate the adaptive slope scheduler by reproducing Figure 1c's cosine similarity measurements with varying k values
2. Conduct ablation studies on warm-up period duration (test 20, 50, 100 timesteps) to determine sensitivity and optimal settings
3. Compare training efficiency of adaptive vs. fixed shallow slopes in a controlled supervised learning setting to isolate the exploration benefit from other effects