---
ver: rpa2
title: Large VLM-based Stylized Sports Captioning
arxiv_id: '2508.19295'
source_url: https://arxiv.org/abs/2508.19295
tags:
- sports
- lvlm
- caption
- images
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-level fine-tuned LVLM pipeline for
  generating stylized sports captions from football images in real-time. The pipeline
  addresses the challenge of accurate entity identification (players, coaches, celebrities)
  and natural language description in a specific caption format, overcoming limitations
  of existing models that lack domain-specific sports jargon.
---

# Large VLM-based Stylized Sports Captioning

## Quick Facts
- arXiv ID: 2508.19295
- Source URL: https://arxiv.org/abs/2508.19295
- Authors: Sauptik Dhar; Nicholas Buoncristiani; Joe Anakata; Haoyu Zhang; Michelle Munson
- Reference count: 16
- One-line primary result: Two-level LVLM pipeline generates real-time stylized sports captions with 91.2% BERT score and 8-10% entity F1 improvement

## Executive Summary
This paper presents a two-level Large Vision-Language Model pipeline for generating stylized sports captions from football images in real-time. The system addresses the challenge of accurate entity identification (players, coaches, celebrities) and natural language description in a specific caption format. By decomposing the task into entity recognition and caption generation, the approach overcomes limitations of existing models that lack domain-specific sports jargon. The pipeline achieves 91.2% BERT score while processing 6 images every 3-5 seconds during live games.

## Method Summary
The method employs a two-level Supervised Fine-Tuning pipeline using Llama3.2-11B with 4-bit quantization. Level 1 focuses on player recognition, outputting team and jersey number with confidence labels (HIGH/LOW) to reduce hallucination. These predictions are deterministically mapped to player names via roster lookup. Level 2 generates captions using outputs from traditional vision models (celebrity faces, logos, OCR, generic image description) alongside Level 1 results. The approach uses confidence-based labeling during SFT to boost True Positive rates for in-focus players while reducing False Positives for out-of-focus subjects. The system processes over 1000 captions during Super Bowl LIX while maintaining operational efficiency.

## Key Results
- Entity F1 score improved by 8-10% through confidence-based labeling in Level 1
- Achieved 91.2% BERT score for caption quality
- Processes 6 images every 3-5 seconds during live games
- Generates over 1000 captions during Super Bowl LIX
- Uses 4-bit quantization for >10× VRAM savings while maintaining low latency

## Why This Works (Mechanism)

### Mechanism 1
Two-level decomposition isolates entity identification from caption generation, reducing compound errors. Level 1 predicts only team + jersey number (not names), which are deterministically mapped to player names via roster lookup. This constrained output vocabulary reduces hallucination probability. Level 2 receives already-grounded entities, focusing its capacity on action association and style.

### Mechanism 2
Confidence-based labels (HIGH/LOW) during SFT teach the model to weight attention toward in-focus players. Training data includes confidence labels that signal which entities are visually salient. The model learns to associate confidence with focal attention, improving TP rate for prominent subjects while suppressing FP for background figures.

### Mechanism 3
Multi-source context fusion (traditional vision outputs + metadata + roster) grounds LVLM generation in verifiable signals. Level 2 receives structured inputs—celebrity face detection, logo recognition, OCR, generic image description—alongside Level 1 outputs. These act as external memory, reducing reliance on LVLM parametric knowledge for factual claims.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) templates for LVLMs**: Both levels use SFT with structured templates. Understanding prompt-response formatting is essential for data preparation and debugging.
  - Quick check question: Can you write an SFT template that accepts an image + roster text and outputs only "Team Name (Jersey Number) | Confidence"?

- **Quantization (4-bit) for inference efficiency**: The production system relies on 4-bit quantization for >10× VRAM reduction. Engineers must understand accuracy-latency-memory tradeoffs.
  - Quick check question: What is the expected VRAM reduction when switching from FP16 to 4-bit for an 11B parameter model?

- **Entity grounding via external knowledge bases**: The roster lookup decouples visual recognition from name generation. This pattern generalizes to other domains requiring factual grounding.
  - Quick check question: Given jersey #15 on the Kansas City Chiefs roster, what is the failure mode if the roster is outdated?

## Architecture Onboarding

- Component map: Image + Metadata → Level 1 LVLM (Player Model) → Traditional Vision Models → Level 2 LVLM (Caption Model) → Stylized Caption
- Critical path: Level 1 entity prediction → roster lookup → Level 2 caption generation. A failure at Level 1 propagates; roster staleness breaks grounding.
- Design tradeoffs: Two-level vs. direct fine-tuning yields +8% entity F1 but adds orchestration complexity. Decoder vs. encoder for Level 1 allows post-training prompt optimization for OOD. 4-bit quantization provides >10× VRAM savings vs. potential accuracy drop.
- Failure signatures: Hallucinated player names (if Level 1 generates names directly), wrong entity-action pairing (Level 2 may misattribute actions), style violations (if SFT data doesn't cover format), timeout under load (if throughput drops below 6 images per 3-5 seconds).
- First 3 experiments:
  1. Baseline replication: Run zero-shot Llama-3.2-11B-V on 50 football images with metadata; measure entity F1 and BERTScore against ground truth.
  2. Ablation on confidence labels: Train Level 1 with and without HIGH/LOW confidence annotation on same data; compare TP/FP rates on held-out set.
  3. End-to-end latency test: Deploy full two-level pipeline with 4-bit quantization; measure images/second and VRAM usage under simulated live load (100 images in 60 seconds).

## Open Questions the Paper Calls Out

### Open Question 1
What are the comparative advantages of decoder-based versus encoder-based architectures for visual/video LLMs in sports entity recognition tasks? The paper observed empirical benefits from decoder-based approaches for out-of-distribution images but did not conduct systematic comparison against encoder-based baselines.

### Open Question 2
How can scalable and robust hyperparameter tuning methods be developed for LVLMs under supervised fine-tuning or preference optimization settings? Current tuning approaches produced inconsistent results across runs, suggesting sensitivity to initialization, learning rates, or data ordering.

### Open Question 3
Why did ORPO preference optimization fail to improve caption quality in the two-level pipeline? Preference alignment methods typically improve output quality; understanding the failure mode could reveal whether the issue lies in preference data construction, the two-level architecture, or sports-domain specifics.

### Open Question 4
How does the pipeline generalize to other sports beyond American football, and what sport-specific adaptations are required? Different sports have distinct jargon, equipment, entity types, and action vocabularies that may require architecture or training modifications.

## Limitations

- SFT hyperparameter sensitivity: Learning rates, batch sizes, and training epochs are not reported, making reproducibility uncertain and potentially attributing some improvement to hyperparameter tuning rather than confidence-labeling mechanism.
- Traditional vision model integration opacity: Specific model choices, accuracy metrics, and failure handling for conflicting outputs are not detailed, limiting understanding of robustness.
- Quantization accuracy trade-offs: While >10× VRAM savings is claimed, the accuracy drop is not quantified, leaving real-world impact on caption quality unclear.

## Confidence

- **High Confidence**: Two-level architecture (entity grounding → caption generation) is logically sound and well-supported by the evidence.
- **Medium Confidence**: Confidence-labeling mechanism is plausible but relies on subjective human annotation without ablation studies.
- **Low Confidence**: Exact contribution of each traditional vision model and handling of their potential conflicts are unclear without implementation details.

## Next Checks

1. Ablation Study on Confidence Labels: Retrain Level 1 with and without HIGH/LOW confidence annotations on the same data. Measure entity F1, TP/FP rates, and hallucination frequency on a held-out test set.

2. Latency and Memory Profiling Under Load: Deploy the full two-level pipeline with 4-bit quantization. Measure images/second throughput and VRAM usage under simulated live load (e.g., 100 images in 60 seconds).

3. Out-of-Distribution Testing: Evaluate the system on football images with jersey color changes, non-standard uniforms, and roster updates. Measure entity F1 and hallucination rates to assess robustness.