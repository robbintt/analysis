---
ver: rpa2
title: 'Generate-Then-Validate: A Novel Question Generation Approach Using Small Language
  Models'
arxiv_id: '2512.10110'
source_url: https://arxiv.org/abs/2512.10110
tags:
- answer
- question
- questions
- human
- phi-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that small language models (SLMs) can generate
  high-quality multiple-choice questions (MCQs) when guided by a "generate-then-validate"
  pipeline. The approach uses Phi-2 to first generate an abundance of candidate questions
  and then applies novel probabilistic reasoning-based validation to filter low-quality
  questions.
---

# Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models

## Quick Facts
- arXiv ID: 2512.10110
- Source URL: https://arxiv.org/abs/2512.10110
- Authors: Yumou Wei; John Stamper; Paulo F. Carvalho
- Reference count: 34
- Small language models (Phi-2) can generate high-quality MCQs when guided by generate-then-validate pipeline

## Executive Summary
This study presents a novel approach for generating high-quality multiple-choice questions (MCQs) using small language models (SLMs). The "generate-then-validate" pipeline employs Phi-2 to first create an abundance of candidate questions, then applies probabilistic reasoning-based validation to filter low-quality outputs. The approach demonstrates that SLMs can achieve substantial agreement with human experts on question correctness (Cohen's kappa = 0.76) and learning-objective alignment (Cohen's kappa = 0.62), challenging the assumption that only large language models can produce educational content of sufficient quality.

## Method Summary
The pipeline operates in two stages: expansive generation and selective validation. During generation, Phi-2 creates 200 MCQs per learning objective using nucleus sampling (temperature=1.0, top_p=0.95) through a five-step incremental prompt (seed → stem → choices → answer → explanation). The validation stage applies three filters: syntactic rules (excluding malformed questions), answer confidence evaluation (keeping questions where the model's probability distribution strongly favors a correct answer with p > 0.9), and learning-objective alignment checking (using log-probability differences to verify semantic relevance). The entire process yields approximately 16 high-quality questions per learning objective from an initial pool of 200 candidates.

## Key Results
- Human experts achieved substantial inter-rater agreement on MCQ correctness (Fleiss' kappa = 0.70)
- Phi-2's answers aligned strongly with human judgments (Cohen's kappa = 0.76)
- When evaluated for learning-objective alignment, human experts showed moderate agreement (Fleiss' kappa = 0.58)
- Phi-2 achieved similar alignment to human experts (Cohen's kappa = 0.62)
- Gemini-2.5-Pro validated results as surrogate judge with high consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating a surplus of candidates increases the likelihood of discovering high-quality questions that an SLM would otherwise miss in a single attempt.
- Mechanism: The pipeline employs an "abundance mindset" (targeting 200 questions per learning objective). By using nucleus sampling (temperature 1.0, top_p 0.95), the system explores a wider solution space. The law of large numbers suggests that while individual SLM outputs may be noisy, a large pool contains valid signal recoverable via filtering.
- Core assumption: The SLM's generation distribution covers the valid solution space sufficiently, meaning high-quality questions are possible but low-probability.
- Evidence anchors: [abstract] "pipeline first performs expansive generation to create an abundance of candidate questions"; [section] Page 2: "law of large numbers will guarantee that good outcomes will emerge from an abundance of trials."; [corpus] Related work "CCQA" supports the viability of inference-time reasoning strategies for SLMs.
- Break condition: If the generator falls into mode collapse, producing semantically identical or low-quality stems regardless of sampling parameters, volume will not yield quality.

### Mechanism 2
- Claim: An SLM's native confidence (token probabilities) serves as an effective proxy for question clarity and answerability.
- Mechanism: The "Answer Confidence Evaluation" adds a "none of the above" option and extracts next-token probabilities (softmax) for all choices. If the probability distribution is flat (low entropy), the question is ambiguous. If the probability peaks on a valid choice (p > 0.9), the question is retained. High confidence correlates with human agreement.
- Core assumption: The model's internal probability aligns with semantic correctness; i.e., the model is not "confidently wrong."
- Evidence anchors: [abstract] "probabilistic reasoning-based validation to filter low-quality questions"; [section] Page 10: "average Cohen's kappa between Phi-2 and Gemini improved from 0.67 at the baseline to 0.91 at the 95% threshold."; [corpus] "Empowering Small Language Models with Factual Hallucination-Aware Reasoning" suggests SLMs can be calibrated for reasoning despite hallucination risks.
- Break condition: If the model displays miscalibration (e.g., high confidence on incorrect, hallucinated facts), this filter passes flawed questions.

### Mechanism 3
- Claim: Comparing the log-probability of a question conditioned on a Learning Objective (LO) versus its unconditional probability isolates semantic alignment.
- Mechanism: The LO Alignment Check computes $Relevance(L, Q) = \log P(Q|L) - \log P(Q)$. If the presence of the LO significantly increases the probability of the question text, the LO is deemed relevant. This repurposes the SLM as a zero-shot classifier.
- Core assumption: The semantic relationship between the LO and question is captured by the shift in log-probability, rather than just keyword overlap.
- Evidence anchors: [section] Page 5: "Relevance(L, Q)... difference in the log-probability... when L is present vs absent."; [section] Page 10: "LO alignment check enhanced the relevance... from 59%... to 61%."; [corpus] No direct corpus evidence refutes this; standard practice often uses embeddings, making this generative approach novel.
- Break condition: If questions and LOs share specific lexical markers (keywords) that artificially inflate $P(Q|L)$ without conceptual depth, the alignment check will yield false positives.

## Foundational Learning

- Concept: **Next-Token Probability & Softmax**
  - Why needed here: The entire validation logic rests on reading the raw logits of the model for specific tokens (a, b, c, d) and converting them to a normalized distribution.
  - Quick check question: If a model assigns logits of [2.0, 1.0, 0.1] to options A, B, and C, does the softmax probability of A exceed the 0.9 threshold suggested in the paper?

- Concept: **Nucleus Sampling (Top-p) vs. Greedy Decoding**
  - Why needed here: The architecture uses different decoding strategies for different goals: nucleus sampling for divergent creation (questions) and greedy decoding for convergent selection (answers/explanations).
  - Quick check question: Why would setting temperature to 0 during the generation of question stems defeat the "Expansive Generation" strategy?

- Concept: **Inter-Rater Reliability (Cohen's Kappa)**
  - Why needed here: The paper validates the SLM not by accuracy alone, but by how closely it mimics human expert agreement rates.
  - Quick check question: The paper reports Phi-2 achieved a Cohen's Kappa of 0.76 with humans. Does this represent "slight," "moderate," or "substantial" agreement?

## Architecture Onboarding

- Component map: Input LO -> Generator (Phi-2: seed→stem→choices→answer→explanation) -> Validator (Syntactic→Confidence→LO Alignment) -> Evaluator (Gemini/Human)
- Critical path: 1. Generation: Batch process 200 questions per LO using `top_p=0.95`. 2. Confidence Filtering: For every generated MCQ, shuffle options, add "none," and calculate softmax probs. Drop if max_prob < 0.9. 3. Alignment Filtering: Calculate Relevance Score (LogProb delta) for the question against *all* LOs. Keep only if Top-1 LO matches the source LO.
- Design tradeoffs:
  - **Compute vs. Precision**: You must generate ~200 questions to yield ~78 final high-quality questions (approx. 39% yield). This requires significant inference overhead compared to one-shot generation.
  - **Model Choice**: The paper uses Phi-2 for *both* generation and validation. A hybrid approach (SLM for gen, LLM for validation) is suggested as future work but not implemented here.
- Failure signatures:
  - **Low Retention Rate**: If the validator rejects >90% of questions, check the generation temperature or the quality of the input LOs (vague LOs lead to ambiguous questions).
  - **"None of the above" Dominance**: If the confidence check frequently assigns high probability to "none," the generation step is failing to produce plausible distractors or correct answers.
- First 3 experiments:
  1. **Overgenerate-then-Filter Test**: Run the pipeline on 3 distinct LOs. Compare the quality of the final 10 questions against 10 questions generated via standard greedy decoding (no filtering).
  2. **Threshold Sweep**: Replicate the ablation study (Fig 5) on your specific domain data. Plot the "Answer Confidence Threshold" vs. "Human Agreement" to find the optimal cut-off (paper uses 0.9, but domains may vary).
  3. **Probability Distribution Inspection**: Visualize the softmax outputs for a batch of rejected questions vs. accepted ones to ensure the model isn't just uniformly confused (flat distribution) vs. confidently wrong (peaked on wrong answer).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid pipeline utilizing an SLM for generation and an LLM for validation achieve superior cost-efficiency and output quality compared to systems relying solely on large models?
- Basis in paper: [explicit] The conclusion invites future work to "explore a synergy between an SLM for generation and an LLM for validation" where the LLM provides accurate feedback.
- Why unresolved: The current study only used the LLM (Gemini) to simulate human evaluation results post-hoc, not as an active component within the validation loop to filter candidates.
- What evidence would resolve it: A comparative experiment measuring latency, token cost, and human-rated quality of MCQs generated by a hybrid pipeline versus an LLM-only baseline.

### Open Question 2
- Question: Does the pipeline's high "discard rate" (generating 200 questions to retain roughly 16) result in lower overall computational efficiency compared to direct generation using LLMs?
- Basis in paper: [inferred]