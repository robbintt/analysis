---
ver: rpa2
title: Scaling Synthetic Task Generation for Agents via Exploration
arxiv_id: '2509.25047'
source_url: https://arxiv.org/abs/2509.25047
tags:
- task
- tasks
- agent
- action
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoPlay addresses the challenge of scaling task generation for
  post-training multimodal large language models (MLLMs) to build interactive agents
  across domains like computer use, web navigation, and robotics. The key bottleneck
  is the lack of high-quality downstream agentic task datasets that are diverse, feasible,
  and verifiable.
---

# Scaling Synthetic Task Generation for Agents via Exploration

## Quick Facts
- arXiv ID: 2509.25047
- Source URL: https://arxiv.org/abs/2509.25047
- Authors: Ram Ramrakhya, Andrew Szot, Omar Attia, Yuhao Yang, Anh Nguyen, Bogdan Mazoure, Zhe Gan, Harsh Agrawal, Alexander Toshev
- Reference count: 40
- One-line primary result: AutoPlay generates 20k tasks across 20 Android apps and 10k tasks across 13 Ubuntu apps, improving MLLM UI agent success rates by up to 20.0% on mobile and 10.9% on computer use.

## Executive Summary
AutoPlay addresses the bottleneck of scaling task generation for multimodal large language models (MLLMs) to build interactive agents across domains like computer use, web navigation, and robotics. The key challenge is the lack of high-quality downstream agentic task datasets that are diverse, feasible, and verifiable. AutoPlay introduces a two-stage pipeline: environment exploration where an MLLM explorer agent systematically uncovers novel environment states and functionalities, followed by task generation where a task generator leverages exploration trajectories and task guideline prompts to synthesize diverse, executable, and verifiable tasks. This approach generates 20k tasks across 20 Android apps and 10k tasks across 13 Ubuntu apps, improving MLLM-based UI agent success rates by up to 20.0% on mobile-use and 10.9% on computer-use scenarios.

## Method Summary
AutoPlay operates in two stages: exploration and task generation. First, an MLLM explorer agent interacts with the environment to produce exploration trajectories that uncover novel states and functionalities. These trajectories are summarized into episodic memory. Second, a task generator samples from this memory along with task guideline prompts to synthesize diverse, executable, and verifiable tasks grounded in the specific app state. The generated tasks are then executed by an executor (planner + grounding model) and verified by an MLLM verifier for success. The system uses these verified tasks for supervised fine-tuning or reinforcement learning training of UI agents, with verifier-based rewards enabling scalable RL without human annotation.

## Key Results
- Generated 20k tasks across 20 Android apps and 10k tasks across 13 Ubuntu apps
- Improved MLLM UI agent success rates by up to 20.0% on mobile-use scenarios
- Achieved 10.9% improvement on computer-use scenarios with an additional 5.7% gain from RL training
- Outperformed baselines that rely on static descriptions or no exploration context

## Why This Works (Mechanism)

### Mechanism 1: Exploration-Grounded Task Feasibility
Tasks generated using explicit environment exploration are significantly more feasible to execute than those generated from static descriptions. An MLLM Explorer agent interacts with the environment to produce a trajectory, which serves as the "Environment Context" for the Task Generator. By grounding task generation in observed states rather than parametric knowledge, the generator creates tasks with parameters that exist in the current environment state, reducing hallucination.

### Mechanism 2: Verifier-Based Reinforcement Learning Scaling
MLLM-based verifiers provide a sufficient reward signal to scale Reinforcement Learning for UI agents without human annotation. The system treats the task verifier as a reward model, assessing trajectory success against the instruction and assigning rewards of 1 or 0. This enables GRPO optimization of the policy using verifier feedback.

### Mechanism 3: Guideline-Driven Diversity
Explicit "Task Guideline Prompts" are necessary to enforce diversity and functional coverage in the generated dataset. The Task Generator samples from predefined guidelines (e.g., "Feature-Use", "Information Retrieval", "Feature-Composition") alongside exploration context, forcing synthesis of tasks requiring distinct cognitive strategies rather than clustering around simple interactions.

## Foundational Learning

- **Concept: Goal-Conditioned Policy (POMDP)**
  - Why needed here: The paper formalizes the UI agent problem as a POMDP where the agent must map observation and goal to action. Understanding this frame is required to grasp why "tasks" are the bottleneck for training.
  - Quick check question: How does the policy π(a|o, g) differ from a standard policy π(a|o) in the context of the AndroidWorld benchmark?

- **Concept: Hindsight Relabeling**
  - Why needed here: The paper explicitly contrasts its "Exploration-then-Generation" approach with "Iterative Exploration" methods that use hindsight relabeling. Understanding this contrast clarifies why AutoPlay generates diverse tasks before execution rather than labeling whatever happened.
  - Quick check question: Why might a task generated via hindsight relabeling of a failed sub-goal trajectory be less useful for training than a task generated from a successful exploration trajectory?

- **Concept: Grounding in UI Agents**
  - Why needed here: The core contribution is "grounding" task generation. One must understand that grounding connects the semantic instruction to the visual-spatial pixel space.
  - Quick check question: In the AutoPlay architecture, which specific component (Planner vs. Grounding Model) is responsible for translating the natural language plan into pixel coordinates?

## Architecture Onboarding

- **Component map:** Environment -> Explorer Agent -> Summarizer -> Memory -> Task Generator -> Task Instruction -> Executor (Planner + Grounding Model) -> Trajectory -> Verifier -> Reward

- **Critical path:**
  1. Exploration: Run Explorer → Summarize Trajectory → Store in Memory. Repeat for M turns.
  2. Generation: Sample Context + Guideline → Generate Tasks.
  3. Data Synthesis: Execute Tasks → Verify Success.
  4. Training: Filter for successes (SFT) or use Verifier score (RL).

- **Design tradeoffs:**
  - Proprietary vs. Open-Source Verifiers: GPT-4o is stronger but Qwen-2.5-VL is viable for RL to manage cost.
  - Exploration Depth vs. Compute: Increasing exploration turns improves context but linearly increases inference cost.

- **Failure signatures:**
  - Hallucinated Tasks: "No Exploration" baseline failed 78.7% due to tasks referring to non-existent entities.
  - Stuck Loops: Explorer or Executor may repeat actions if reflection trace is insufficient.

- **First 3 experiments:**
  1. Ablation on Context Source: Compare task generation success rates with "No Context" vs. "Static Description" vs. "Exploration Context".
  2. Verifier Sensitivity: Swap Verifier model (GPT-4o vs. Qwen-2.5-72B) on fixed trajectories to measure reward signal noise.
  3. Guideline Coverage: Visualize task distribution with and without "Task Guidelines" prompts to confirm they prevent mode collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the manual creation of "task guidelines" be automated or dynamically adapted to improve coverage?
- Basis in paper: The authors attribute gaps in computer-use task coverage to a "lack of sufficiently diverse task guidelines prompts".
- Why unresolved: The current method relies on manually defined prompts, which limits scalability to domains with vast, unstructured functionality.
- What evidence would resolve it: A method that autonomously generates or refines task guidelines based on exploration results without human intervention.

### Open Question 2
- Question: How can the exploration phase be adapted to reliably cover complex UI interactions and cross-app workflows?
- Basis in paper: The authors note the executor struggles with "fine-grained text editing, complex UI interactions, and cross app navigation".
- Why unresolved: The current exploration policy may not systematically trigger complex multi-step interactions required for those categories.
- What evidence would resolve it: A modified exploration strategy that successfully generates executable tasks involving cross-app dependencies or complex input widgets.

### Open Question 3
- Question: To what extent does the performance of the exploration executor cap the quality of the generated synthetic dataset?
- Basis in paper: The authors state "improving the task executor while keeping the task generator the same would lead to better quality synthetic dataset".
- Why unresolved: AutoPlay relies on the executor's trajectories to ground tasks; if the executor fails to reach a state, the generator cannot propose tasks for it.
- What evidence would resolve it: Analysis showing that replacing the MLLM executor with a more capable executor significantly increases the diversity or success rate of generated tasks.

## Limitations
- Proprietary reliance on GPT-4o raises reproducibility and cost concerns despite open-source alternatives for RL
- Exploration effectiveness depends heavily on Explorer agent's ability to systematically uncover diverse functionalities with limited analysis of failure modes
- Task diversity gains from guidelines lack deeper theoretical justification for why specific guideline categories map to downstream skill improvements

## Confidence
- **High Confidence:** Exploration-grounded task generation produces more feasible tasks than static description-based generation
- **Medium Confidence:** MLLM verifiers enable scalable RL without human annotation, though sensitive to verifier quality
- **Medium Confidence:** Task guidelines are necessary for diversity, but specific mechanism mapping to downstream skills needs more rigorous establishment

## Next Checks
1. Implement the AutoPlay pipeline using only open-source models throughout all stages to verify the approach works without proprietary models
2. Systematically measure the fraction of app functionalities discovered by the Explorer agent versus the total available, and analyze failure cases
3. Conduct a controlled experiment where tasks are generated with individual guideline categories in isolation to measure which types contribute most to different downstream skill improvements