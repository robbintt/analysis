---
ver: rpa2
title: 'M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning
  in Medical Instructional Video Understanding'
arxiv_id: '2507.04289'
source_url: https://arxiv.org/abs/2507.04289
tags:
- video
- questions
- medical
- reasoning
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents M3-Med, the first benchmark for Multi-lingual,
  Multi-modal, and Multi-hop reasoning in Medical instructional video understanding.
  The benchmark addresses limitations in existing medical video datasets by introducing
  multilingual support (Chinese and English) and complex questions requiring deep
  cross-modal reasoning across video and text.
---

# M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding

## Quick Facts
- arXiv ID: 2507.04289
- Source URL: https://arxiv.org/abs/2507.04289
- Authors: Shenxi Liu; Kan Li; Mingyang Zhao; Yuhang Tian; Bin Li; Shoujun Zhou; Hongliang Li; Fuxia Yang
- Reference count: 40
- Primary result: First benchmark for multilingual, multi-modal, and multi-hop reasoning in medical instructional video understanding

## Executive Summary
This paper introduces M3-Med, the first benchmark specifically designed to evaluate Multi-lingual, Multi-modal, and Multi-hop reasoning capabilities in medical instructional video understanding. The benchmark addresses critical limitations in existing medical video datasets by introducing multilingual support (Chinese and English) and complex questions requiring deep cross-modal reasoning across video and text. The dataset contains 3,748 videos with 12,747 question-answer pairs, annotated by medical experts. A key innovation is the two-tier question design: "simple" questions for direct retrieval and "complex" questions requiring multi-hop reasoning through a constructed knowledge graph. Experiments with state-of-the-art models and LLMs show significant performance gaps compared to human experts, especially on complex questions.

## Method Summary
The M3-Med benchmark consists of two tasks: Temporal Answer Grounding in Single Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). The dataset includes 3,748 videos (1,628 Chinese, 2,120 English) with 12,747 QA pairs (6,704 simple, 6,043 complex), annotated by medical professionals. Each video includes SRT subtitles generated via Whisper and manually-annotated knowledge graphs. Questions are designed with two tiers: simple questions for direct retrieval and complex questions requiring multi-hop reasoning across text and visual modalities. The benchmark evaluates models across four input settings: Full (video+subs+KG), Video+Subs, Subs+KG, and Subs-only. Evaluation uses IoU metrics for temporal grounding with human performance serving as the ceiling.

## Key Results
- Significant performance gaps exist between human experts and state-of-the-art models, particularly on complex multi-hop questions
- Complex questions with low lexical overlap but high semantic relevance effectively expose models' inability to perform genuine cross-modal reasoning
- Ground-truth Knowledge Graphs provide only moderate performance gains, suggesting current models struggle to synthesize symbolic and perceptual information
- (M)LLMs show strong zero-shot reasoning capabilities but suffer from format compliance issues and spurious refusals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-hop question design with low lexical overlap but high semantic relevance forces models to perform genuine cross-modal reasoning rather than text-based shortcut learning.
- Mechanism: Complex questions require traversing multiple reasoning hops (text entity → visual evidence → synthesis) while minimizing keyword overlap with subtitles, breaking shortcut learning patterns.
- Core assumption: Models that cannot integrate visual and textual modalities will fail on questions where linguistic cues are insufficient.
- Evidence anchors: Section 1, Section 5.3, and corpus references confirm this design successfully challenges deep cross-modal understanding.

### Mechanism 2
- Claim: Ground-truth Knowledge Graphs provide structured reasoning scaffolds but cannot fully replace genuine visual reasoning.
- Mechanism: KGs encode entities and relationships as reasoning paths, but models must still synthesize symbolic and perceptual information.
- Core assumption: The gap between "Subtitles + KG" and "Full Input" performance reflects the irreducible visual reasoning component.
- Evidence anchors: Section 3.3, Section 6.1, and corpus references show moderate KG benefits but persistent visual reasoning gaps.

### Mechanism 3
- Claim: Two-tier question design enables precise measurement of reasoning depth rather than just retrieval capability.
- Mechanism: Simple questions establish baseline retrieval performance; complex questions reveal multi-hop cross-modal reasoning challenges.
- Core assumption: Simple and complex questions are matched in content relevance and differ primarily in reasoning requirements.
- Evidence anchors: Section 3.4, Section 5.2, Section 5.3, and corpus references validate the tier comparison approach.

## Foundational Learning

- Concept: **Video Temporal Grounding**
  - Why needed here: Both TAGSV and TAGVC tasks require precise localization of answer-relevant video segments with start/end timestamps.
  - Quick check question: Given a video and question, can you identify the timestamp range where the answer appears?

- Concept: **Multi-hop Reasoning**
  - Why needed here: Complex questions require chaining multiple inference steps across modalities (e.g., identifying an entity in text → locating it visually → synthesizing with other evidence).
  - Quick check question: Can you trace the reasoning chain required to answer: "How to give first aid when a choking patient becomes unconscious?"

- Concept: **Cross-modal Representation Alignment**
  - Why needed here: Models must map between textual entities/subtitles and visual frames to perform the required synthesis.
  - Quick check question: How would you represent "CPR chest compression" in both text and visual feature spaces so they can be matched?

## Architecture Onboarding

- Component map:
  Video Encoder -> Text Encoder -> Knowledge Graph Module -> Temporal Grounding Head
  Video Retrieval Module (for TAGVC only)

- Critical path:
  1. Question parsing → entity extraction
  2. Cross-modal evidence retrieval (text KG lookup + visual grounding)
  3. Evidence synthesis via multi-hop traversal
  4. Temporal boundary prediction (IoU with ground truth)

- Design tradeoffs:
  - Specialized VTG models vs. General MLLMs: Specialized models are task-optimized but less flexible; MLLMs show stronger zero-shot reasoning but suffer from format adherence issues
  - Annotation quality vs. scalability: Human-verified KGs ensure quality but limit scale; automation risks noise
  - KG as shortcut vs. scaffold: Providing ground-truth KGs helps reasoning but may mask visual reasoning deficiencies

- Failure signatures:
  - Spurious refusals: MLLMs output "Null" even when answer exists
  - Format violations: Non-standard timestamp formats, missing boundaries, extra text
  - Sharp performance drop on complex questions: Indicates reliance on text shortcuts
  - Near-zero performance in "Subtitles Only" condition: Expected for properly designed complex questions

- First 3 experiments:
  1. Baseline ablation: Run all four input settings on both simple and complex questions to establish modality contribution and verify complex questions require visual reasoning.
  2. MLLM format robustness test: Evaluate zero-shot MLLM performance with explicit format constraints and post-processing to isolate reasoning failures from output formatting issues.
  3. Cross-lingual transfer: Train on Chinese questions, test on English (and vice versa) to assess whether multi-hop reasoning patterns transfer across languages within the same model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can cross-modal relation extraction be fully automated to construct high-quality knowledge graphs for medical videos without reliance on manual curation?
- Basis in paper: Section 7.2 and Section 8 state that achieving full, high-quality automation for complex annotations is a key limitation and important future direction, noting current automated methods yielded unsatisfactory results.
- Why unresolved: The paper relies on semi-automated pipeline with human-in-the-loop curation because existing automated methods failed to meet quality standards for complex relationship linking.
- What evidence would resolve it: An automated pipeline that generates knowledge graphs yielding comparable model performance to human-curated graphs on the M3-Med benchmark.

### Open Question 2
- Question: What specific mechanisms cause general-purpose (M)LLMs to exhibit "spurious refusals" in specialized temporal grounding tasks, and how can this behavior be corrected?
- Basis in paper: Section 6.2 notes models frequently output "Null" or "Cannot answer" even when correct answers exist, warranting further investigation for their reliability in structured tasks.
- Why unresolved: This behavior differs from traditional fine-tuned models and suggests different internal mechanisms for handling uncertainty in general-purpose models.
- What evidence would resolve it: An analysis of LLM attention mechanisms during failure cases or a fine-tuning strategy that significantly reduces spurious refusals without lowering accuracy.

### Open Question 3
- Question: How can model architectures be improved to better synthesize explicit symbolic knowledge (Knowledge Graphs) with implicit perceptual video features?
- Basis in paper: Section 6.1 shows ground-truth KGs provided only moderate performance gains, suggesting current models lack architectural capacity to fuse distinct modalities effectively for multi-hop reasoning.
- Why unresolved: Authors expected structured knowledge to serve as stronger signal, but results suggest models struggle with symbolic-perceptual synthesis.
- What evidence would resolve it: A model architecture that achieves significantly higher performance on "Subtitles + KG" or "Full Input" settings, narrowing the gap between simple and complex multi-hop reasoning tasks.

## Limitations

- Knowledge graph construction relies heavily on human curation, limiting scalability and automation potential
- (M)LLM format compliance issues (spurious refusals, malformed outputs) may obscure true reasoning capabilities
- Subjective nature of "complex" vs. "simple" question labeling could introduce variability in difficulty assessment

## Confidence

- **High confidence**: Benchmark construction methodology and dataset statistics are well-documented and verifiable
- **Medium confidence**: Complex questions successfully prevent text-based shortcut learning, though strengthened with ablation studies
- **Medium confidence**: Performance gap between specialized models and MLLMs reflects genuine reasoning differences, though format compliance issues complicate interpretation
- **Low confidence**: Ground-truth knowledge graphs cannot serve as complete shortcuts requires more rigorous ablation testing

## Next Checks

1. **Format compliance validation**: Systematically evaluate MLLM outputs for timestamp format adherence across different prompt variations to isolate reasoning failures from output formatting issues.

2. **Knowledge graph ablation study**: Test model performance with ground-truth KGs versus automatically generated KGs versus no KGs to quantify the true value of structured knowledge scaffolding.

3. **Cross-lingual reasoning transfer**: Train models on one language (Chinese) and test on the other (English) to determine whether multi-hop reasoning patterns generalize across languages within the same model architecture.