---
ver: rpa2
title: 'Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning'
arxiv_id: '2510.13322'
source_url: https://arxiv.org/abs/2510.13322
tags:
- backdoor
- uni00000013
- unlearning
- attack
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first paradigm of revocable backdoor
  attacks, where backdoors can be proactively and thoroughly removed via machine unlearning
  after achieving attack objectives. The key innovation is formulating trigger optimization
  as a bilevel problem that balances high attack success rate with effective backdoor
  revocation.
---

# Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning

## Quick Facts
- **arXiv ID:** 2510.13322
- **Source URL:** https://arxiv.org/abs/2510.13322
- **Reference count:** 40
- **Primary result:** Achieves 0.11% ASR-U on ImageNet-10 after unlearning, outperforming traditional backdoor attacks in both stealth and revocability.

## Executive Summary
This paper introduces the first paradigm of revocable backdoor attacks, where backdoors can be proactively and thoroughly removed via machine unlearning after achieving attack objectives. The key innovation is formulating trigger optimization as a bilevel problem that balances high attack success rate with effective backdoor revocation. By employing deterministic sample partitioning and Projected Conflicting Gradient (PCGrad) techniques, the method achieves attack success rates comparable to state-of-the-art backdoor attacks while enabling significant backdoor weakening or removal after unlearning.

## Method Summary
The method trains a trigger generator using bilevel optimization, where the outer level optimizes the trigger and the inner levels simulate both backdoor injection and unlearning processes. The trigger generator (U-Net) creates poisoned data for a surrogate model (ResNet-18), which is then subjected to simulated unlearning on a fixed subset of poisoned samples. The optimization balances maximizing attack success rate while minimizing post-unlearning attack success, using PCGrad to resolve conflicting gradients. The approach employs deterministic partitioning of poisoning and unlearning samples to reduce variance during optimization.

## Key Results
- Achieves ASR-U as low as 0.11% on ImageNet-10 under UnrollSGD unlearning
- Outperforms traditional backdoor attacks in both stealth and revocability
- Maintains attack success rates comparable to state-of-the-art methods (98% ASR) while enabling revocation
- Demonstrates effectiveness across CIFAR-10 and ImageNet-10 datasets

## Why This Works (Mechanism)

### Mechanism 1: Bilevel Optimization for Lifecycle-Aware Triggers
The system trains a trigger generator against two inner-level processes: standard backdoor injection and machine unlearning. By minimizing the Attack Success Rate (ASR) of the unlearned model during trigger generation, the trigger learns features that standard unlearning algorithms can easily strip away. This works when the unlearning algorithm used during optimization closely matches the target's method.

### Mechanism 2: Gradient Conflict Resolution via PCGrad
The objectives of maximizing attack effectiveness and maximizing revocability produce conflicting gradients. PCGrad resolves this by projecting the unlearning gradient onto the normal plane of the attack gradient when their inner product is negative, preventing the two objectives from canceling each other out.

### Mechanism 3: Deterministic Sample Partitioning
Fixing specific samples for poisoning and unlearning reduces variance and stabilizes the bilevel optimization compared to random sampling. This ensures the trigger generator receives consistent feedback regarding which specific data points will be "forgotten" to erase the backdoor.

## Foundational Learning

- **Concept: Machine Unlearning Algorithms (First-Order & UnrollSGD)**
  - Why needed here: The attack explicitly relies on understanding how models "forget" data to design triggers that are easily removable
  - Quick check question: How does a first-order approximation of unlearning differ from simply retraining the model from scratch?

- **Concept: Bilevel Optimization**
  - Why needed here: The trigger generator is the "outer" model, and the victim/surrogate network is the "inner" model; nested gradients are required to comprehend how the trigger adapts to the victim's learning dynamics
  - Quick check question: In this setup, does the trigger generator update based on the weights of the backdoored model or the unlearned model?

- **Concept: Gradient Surgery (PCGrad)**
  - Why needed here: The core innovation balances two opposing goals; understanding multi-task learning conflicts explains why optimization doesn't collapse
  - Quick check question: If two loss functions have a cosine similarity of -1, what happens to standard gradient descent, and how does projecting one gradient onto the normal plane of the other fix it?

## Architecture Onboarding

- **Component map:** Trigger Generator (U-Net) -> Surrogate Model (ResNet-18) -> Poison Dataset -> Backdoored Model -> Unlearning Simulation -> Unlearned Model
- **Critical path:** The partitioning of dataset D into Poison set P and Unlearn set U. If U is not a subset of P, or if the sizes are misaligned with the target's unlearning capacity, the trigger will either persist or fail to inject.
- **Design tradeoffs:**
  - Poisoning Rate: Higher rate → Higher ASR but harder to unlearn
  - Trigger Strength: Higher strength → Higher ASR but lower revocability and stealth
  - Unlearning Method: UnrollSGD generally revokes better than First-Order
- **Failure signatures:**
  - High ASR-U (>50%) after unlearning: Trigger was too robust or simulated unlearning was too weak
  - Low BA (<80%) after unlearning: Over-forgetting occurred
  - Optimization Instability: Oscillating losses suggest PCGrad parameters need tuning
- **First 3 experiments:**
  1. Plot cosine similarity of attack and unlearning gradients over epochs with and without PCGrad
  2. Vary the forgetting rate to find minimum deletion request required to achieve 0% ASR
  3. Train trigger generator on ResNet-18 and test on VGG-16 to verify revocability transferability

## Open Questions the Paper Calls Out
None

## Limitations
- Bilevel optimization scalability creates significant computational overhead that isn't discussed
- Dependence on poisoning set composition assumes attackers can predict which samples will be requested for deletion
- Generalizability across unlearning algorithms is unclear beyond First-Order and UnrollSGD

## Confidence

- **High confidence:** The PCGrad mechanism for resolving gradient conflicts is technically sound and well-supported by literature
- **Medium confidence:** Empirical results demonstrating revocability on CIFAR-10 and ImageNet-10 are convincing but limited to specific unlearning algorithms
- **Medium confidence:** The claim of being the "first" revocable backdoor attack is reasonable given the literature search

## Next Checks

1. Test whether triggers optimized for First-Order unlearning maintain revocability when the target model uses UnrollSGD (and vice versa)
2. Evaluate the attack's effectiveness against certified unlearning methods
3. Simulate unlearning requests following realistic patterns (random samples vs. specific poisoned samples) to assess robustness to unpredictable deletion requests