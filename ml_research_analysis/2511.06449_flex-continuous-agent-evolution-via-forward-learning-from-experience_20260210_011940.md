---
ver: rpa2
title: 'FLEX: Continuous Agent Evolution via Forward Learning from Experience'
arxiv_id: '2511.06449'
source_url: https://arxiv.org/abs/2511.06449
tags:
- experience
- arxiv
- learning
- library
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FLEX, a gradient-free learning paradigm enabling
  LLM agents to evolve continuously through accumulated experience rather than parameter
  tuning. Instead of backpropagation, FLEX constructs an evolving experience library
  via forward exploration and reflection, organizing knowledge hierarchically for
  reuse.
---

# FLEX: Continuous Agent Evolution via Forward Learning from Experience

## Quick Facts
- arXiv ID: 2511.06449
- Source URL: https://arxiv.org/abs/2511.06449
- Authors: Zhicheng Cai; Xinyuan Guo; Yu Pei; Jiangtao Feng; Jinsong Su; Jiangjie Chen; Ya-Qin Zhang; Wei-Ying Ma; Mingxuan Wang; Hao Zhou
- Reference count: 40
- Primary result: Introduces FLEX, a gradient-free learning paradigm enabling LLM agents to evolve continuously through accumulated experience rather than parameter tuning

## Executive Summary
FLEX presents a novel gradient-free learning paradigm that enables continuous agent evolution through experience accumulation rather than traditional parameter optimization. The approach constructs an evolving experience library via forward exploration and reflection, organizing knowledge hierarchically for reuse. Unlike conventional backpropagation-based methods, FLEX uses an updater agent to provide semantic feedback on failures, allowing the actor agent to learn from experience without gradient descent.

The framework demonstrates substantial performance improvements across diverse domains including mathematical reasoning, chemical retrosynthesis, and protein fitness prediction. By treating the learning process as an optimization problem solved through experience accumulation rather than parameter tuning, FLEX offers a pathway toward open-ended, interpretable agent evolution that could potentially overcome limitations of traditional gradient-based learning.

## Method Summary
FLEX operates through a forward learning mechanism where an actor agent generates outputs, a critic identifies failures, and an updater agent provides semantic feedback to build an experience library. The process begins with an initial agent and library, then iteratively explores, reflects on failures, and accumulates experience. Each experience entry contains the original input, the agent's output, and the updater's feedback on why the output was incorrect.

The experience library is organized hierarchically into "warning zones" (common failure patterns) and "golden zones" (successful strategies). When facing new problems, the actor agent queries the library for relevant experiences and incorporates the feedback to generate improved responses. This process continues iteratively, with the library evolving as more experiences are accumulated. The framework treats learning as an optimization problem where the objective is to maximize correctness by accumulating useful experiences rather than adjusting parameters through gradient descent.

## Key Results
- Achieves up to 23% accuracy improvement on AIME25 mathematical reasoning problems
- Demonstrates 10% performance gain on USPTO50k chemical retrosynthesis tasks
- Shows 14% Spearman correlation improvement on ProteinGym protein fitness prediction
- Identifies a scaling law for experience accumulation effectiveness
- Validates cross-agent experience inheritance capabilities

## Why This Works (Mechanism)
FLEX works by decoupling knowledge acquisition from parameter optimization. Instead of adjusting millions of parameters through backpropagation, the system accumulates discrete experiences that capture successful strategies and common failure patterns. The updater agent acts as a domain expert, providing semantic explanations for why certain approaches fail, which are then stored in the experience library. This allows the actor agent to leverage accumulated wisdom without requiring gradient computation.

The hierarchical organization of experiences enables efficient retrieval and application of relevant knowledge. Warning zones help the agent avoid common pitfalls by recognizing patterns that previously led to failure, while golden zones provide templates for successful approaches. This structure mimics human learning where we build upon past experiences and learned heuristics rather than starting from scratch each time. The forward learning approach also provides interpretability, as the experience library offers a transparent record of what the agent has learned and how it applies that knowledge.

## Foundational Learning
- **Experience replay**: Storing and reusing past experiences to improve learning efficiency - needed because it allows agents to learn from limited data without requiring continuous parameter updates
- **Semantic feedback generation**: Creating human-understandable explanations for failures - needed because it transforms raw errors into actionable knowledge that can be systematically organized
- **Hierarchical knowledge organization**: Structuring experiences into categories based on their utility and type - needed because it enables efficient retrieval and application of relevant knowledge patterns
- **Forward optimization**: Solving learning problems through experience accumulation rather than gradient descent - needed because it provides an alternative path for continuous learning without parameter tuning
- **Cross-agent transfer**: Sharing experience libraries between different agents - needed because it enables collective learning and accelerates the evolution of multiple agents simultaneously

## Architecture Onboarding
**Component Map**: Actor agent → Experience library → Updater agent → Critic → Actor agent (feedback loop)

**Critical Path**: The core learning loop involves the actor generating outputs, the critic identifying failures, the updater creating feedback, and the experience library being updated. This cycle repeats continuously, with each iteration potentially improving the actor's performance through better-informed decision-making based on accumulated experiences.

**Design Tradeoffs**: FLEX trades computational efficiency during inference (querying large experience libraries) for the ability to learn without parameter updates. The approach requires maintaining and querying potentially large libraries, which could become computationally expensive. However, this is offset by the ability to continuously improve without retraining and the interpretability benefits of having explicit knowledge records.

**Failure Signatures**: The system may fail when the updater agent provides poor or hallucinated feedback, when the experience library becomes too large and unwieldy to search efficiently, or when experiences become outdated or contradictory. Additionally, the approach may struggle in domains where failures are subtle or where ground truth is subjective and difficult to verify.

**First Experiments**: 
1. Test basic experience accumulation on a simple classification task with synthetic data to verify the forward learning loop functions correctly
2. Evaluate the impact of experience library size on performance to validate the scaling law empirically
3. Compare performance against traditional experience replay methods on a standard benchmark to establish baseline improvements

## Open Questions the Paper Calls Out
**Open Question 1**: Can experience libraries from heterogeneous agents be merged to form a single, universal library without semantic conflict or catastrophic interference? While the paper demonstrates one-way inheritance, it does not test bidirectional merging of libraries trained on distinct tasks or by diverse agents, which risks introducing contradictory heuristics. Experiments merging libraries from distinct domains would resolve this.

**Open Question 2**: How does FLEX perform in domains lacking definitive ground truth, such as creative writing or open-ended dialogue? The methodology relies on identifying "incorrect conclusions" which requires verifiable answers. It's unclear how the "warning zone" or "golden zone" would function with subjective rewards. Applying FLEX to preference-based tasks would provide answers.

**Open Question 3**: Is the evolution of the experience library fundamentally bottlenecked by the reasoning capability of the updater agent μ? The paper treats the updater as a reliable optimizer, but if its reflection is shallow or hallucinates causes for failure, the library could accumulate noise. Ablation studies varying the updater's capability would reveal this dependency.

## Limitations
- Lacks comparison against established online learning and meta-learning baselines, making it unclear whether improvements stem from forward learning or experience replay benefits
- Experimental validation uses relatively small-scale datasets (AIME25 with 25 problems, USPTO50k with 50k reactions) that may not capture real-world complexity
- Hierarchical experience organization is conceptually described but lacks detailed analysis of how different organizational strategies affect performance

## Confidence
**High**: The forward learning mechanism is clearly described and the experimental methodology is sound for the proposed approach

**Medium**: Performance improvements over baseline models, though limited by lack of comprehensive baseline comparisons

**Low**: Claims about cross-agent experience inheritance and hierarchical organization benefits, which lack rigorous quantitative validation

## Next Checks
1. Conduct ablation studies comparing FLEX against established online learning methods (e.g., experience replay with gradient-based updates, meta-learning approaches) on identical tasks

2. Test the experience inheritance mechanism across agents with significantly different architectures or training distributions to measure transfer efficiency and identify potential negative transfer

3. Evaluate the hierarchical organization strategy against flat experience storage approaches while systematically varying experience library size to validate the claimed scaling law beyond the reported parameter ranges