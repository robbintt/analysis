---
ver: rpa2
title: 'Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved
  Multi-Image Model'
arxiv_id: '2506.11737'
source_url: https://arxiv.org/abs/2506.11737
tags:
- llav
- vision
- visual
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates LLaVA-NeXT-Interleave on 22 datasets across
  three tasks: Multi-Image Reasoning, Document and Knowledge-Based Understanding,
  and Interactive Multi-Modal Communication. A Dense Channel Integration (DCI) connector
  is added to enhance visual feature fusion.'
---

# Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model

## Quick Facts
- arXiv ID: 2506.11737
- Source URL: https://arxiv.org/abs/2506.11737
- Reference count: 22
- Achieved 87.85% accuracy on Multi-Image Reasoning and 80.52% on Document and Knowledge-Based Understanding in Track A

## Executive Summary
This paper presents an evaluation of LLaVA-NeXT-Interleave on 22 datasets across three multi-image tasks: Multi-Image Reasoning, Document and Knowledge-Based Understanding, and Interactive Multi-Modal Communication. The authors introduce a Dense Channel Integration (DCI) connector that aggregates visual features from all vision encoder layers to enhance semantic coherence. The standard fine-tuned model achieves the highest overall accuracy, particularly excelling in vision-heavy tasks (88.55% on VISION), while the DCI-enhanced version shows particular strength on datasets requiring deeper semantic coherence such as MIT-States PropertyCoherence (94.75%). The study reveals task-dependent trade-offs between architectural complexity and performance optimization.

## Method Summary
The evaluation employs LLaVA-NeXT-Interleave with Qwen-7B backbone and SigLIP vision encoder, fine-tuned on 22 datasets with a 9:1 train/validation split. The Dense Channel Integration (DCI) connector optionally aggregates features from all L vision encoder layers by partitioning them into G groups, averaging within groups, and concatenating with the final layer output before MLP projection. Training uses Adam optimizer with learning rate 2×10⁻⁵ for 1 epoch on a single A100-80GB GPU, requiring approximately 22 hours per epoch. The DCI connector adds less than 2% trainable parameters to the base architecture.

## Key Results
- Standard model achieves highest overall accuracy (88.55% on VISION, 86.93% on NLVR2)
- DCI-enhanced version excels on semantic coherence tasks (94.75% on MIT-States PropertyCoherence, 80.00% on SlideVQA)
- On test set: 87.85% accuracy on Multi-Image Reasoning, 80.52% on Document and Knowledge-Based Understanding
- Generative tasks show low performance (ROUGE-L scores under 36.00 on Spot-the-Diff, IEdit, Birds-to-Words)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating visual features from all vision encoder layers improves performance on semantic coherence tasks.
- Mechanism: DCI partitions L layers into G groups, averages adjacent features within each group, then concatenates all fused representations with the final layer's output.
- Core assumption: Earlier encoder layers contain complementary spatial/detective features that, when fused with final-layer semantic features, improve coherence tasks.
- Evidence anchors: MIT-States PropertyCoherence (94.75%) and SlideVQA (80.00%) performance with DCI; formula GLi = (1/M) Σ Vi where features are grouped and averaged.
- Break condition: If task depends primarily on fine-grained visual discrimination without requiring cross-image semantic alignment.

### Mechanism 2
- Claim: Standard fine-tuning outperforms DCI on vision-heavy discriminative tasks.
- Mechanism: Standard model passes only final-layer visual features through MLP projection, reducing architectural complexity for tasks where final-layer features are sufficient.
- Core assumption: Vision-heavy tasks rely more on final-layer abstracted features than mid-layer spatial details.
- Evidence anchors: VISION accuracy: 88.55% (FT) vs 88.90% (DCI); NLVR2: 86.93% (FT) vs 81.84% (DCI).
- Break condition: When vision tasks require understanding property changes across states rather than pure visual discrimination.

### Mechanism 3
- Claim: DCI accelerates early-stage training convergence but introduces optimization instability in later training phases.
- Mechanism: Dense connectivity provides richer gradient signals early in training, but increased parameter interactions create higher loss variance as optimization progresses.
- Core assumption: Trade-off between faster early learning and later-stage stability is task-dependent.
- Evidence anchors: DCI model converges faster up to step 60 with lower loss values, but exhibits higher variance after step 100.
- Break condition: If training resources are limited and stability is prioritized over rapid early adaptation.

## Foundational Learning

- Concept: **Vision Transformer (ViT) Layer Hierarchy**
  - Why needed here: DCI operates on the principle that different ViT layers encode different levels of abstraction—early layers capture edges/textures, later layers capture semantic concepts.
  - Quick check question: Can you explain why averaging features from layers 1-8 might capture different information than using only layer 24's output?

- Concept: **Interleaved Multi-Modal Contexts**
  - Why needed here: The model processes sequences where images and text alternate (not just single image + caption), requiring positional and relational understanding across modalities.
  - Quick check question: How does handling "image-text-image-question" differ from "image-question" in terms of attention mechanisms?

- Concept: **Plug-and-Play Connector Design**
  - Why needed here: DCI is designed to be architecture-agnostic, inserting between any vision encoder and LLM without requiring full retraining.
  - Quick check question: What constraints must a connector satisfy to be truly "plug-and-play" (under 2% parameters, single-stage training)?

## Architecture Onboarding

- Component map: Input Images → SigLIP Vision Encoder (L layers) → [DCI Connector - Optional] → MLP Projection (2-layer) → Qwen-7B LLM → Output

- Critical path: The vision embedding block (encoder + optional DCI + projection) determines how visual information reaches the LLM. DCI modification occurs before the MLP, affecting all downstream reasoning.

- Design tradeoffs:
  - **Standard FT vs DCI**: Standard is more balanced across tasks; DCI specializes in semantic coherence at potential cost to vision-heavy accuracy
  - **Training efficiency**: DCI adds <2% parameters but requires careful monitoring of late-stage variance
  - **Task selection**: Choose DCI for property coherence, change detection, document QA; choose standard for spatial reasoning, object comparison

- Failure signatures:
  - Low ROUGE-L scores on free-form generation (all under 36.00 on Spot-the-Diff, IEdit, Birds-to-Words)
  - Training loss variance increase after step 100 with DCI
  - Performance drop on vision-heavy benchmarks when using DCI (e.g., NLVR2: 81.84% vs 86.93%)

- First 3 experiments:
  1. **Baseline validation**: Run standard LLaVA-NeXT-Interleave FT on validation split to establish task-specific baselines per category (vision-heavy vs semantic coherence).
  2. **DCI ablation by task type**: Enable DCI and compare performance delta on MIT-States PropertyCoherence vs VISION to quantify the task-dependent benefit.
  3. **Training stability test**: Monitor loss curves with and without DCI to step 150, noting convergence point and variance threshold where DCI becomes unstable.

## Open Questions the Paper Calls Out

- Can a hybrid fine-tuning approach that combines standard strategies with DCI enhancements outperform individual methods across all three competition tasks?
  - Basis: Authors state "These findings suggest a promising future direction in combining both strategies for hybrid fine-tuning approaches."
  - Why unresolved: Current setup treats strategies as separate, revealing trade-offs where standard excels in vision tasks and DCI excels in semantic coherence.

- Does increasing the model scale beyond 7B parameters amplify the specific benefits of the Dense Channel Integration (DCI) connector?
  - Basis: Authors note as a limitation: "we only pretrain with the 7B-parameter version, that we believe it's still more space to surpass our results."
  - Why unresolved: Unclear if observed trade-offs are intrinsic to connector design or byproduct of limited model capacity.

- What specific architectural or loss-function modifications are required to improve free-form answer generation?
  - Basis: High accuracy on classification tasks but "performances are low" on generative tasks (ROUGE-L scores < 36).
  - Why unresolved: Paper identifies failure mode but doesn't investigate whether this is due to connector, base LLM, or instruction-tuning data.

## Limitations

- DCI connector lacks specification of critical parameters (G for grouping, L for total layers), making exact reproduction challenging
- Performance claims on semantic coherence tasks are supported by only 2-3 datasets while showing clear inferiority on vision-heavy tasks
- Significant performance gap on free-form answer generation (ROUGE-L scores consistently under 36.00) suggests fundamental limitations in generative capabilities

## Confidence

- High confidence: Standard model performance on vision-heavy tasks (88.55% VISION accuracy, 86.93% NLVR2)
- Medium confidence: DCI's task-dependent benefits (supported by test results but limited dataset coverage)
- Low confidence: Claims about training dynamics and early-stage convergence acceleration (limited empirical evidence beyond single training run)

## Next Checks

1. **DCI parameter validation**: Verify the G and L values used in the implementation by examining model configuration files or contacting authors directly to ensure correct feature grouping.

2. **Task-type ablation study**: Run controlled experiments isolating DCI's impact on semantic coherence tasks (MIT-States, SlideVQA) versus vision-heavy tasks (NLVR2, VISION) to quantify the task-dependent performance tradeoff.

3. **Training stability monitoring**: Extend training to 150+ steps with both variants, tracking loss variance to confirm the reported DCI instability after step 100 and determine optimal early stopping points.