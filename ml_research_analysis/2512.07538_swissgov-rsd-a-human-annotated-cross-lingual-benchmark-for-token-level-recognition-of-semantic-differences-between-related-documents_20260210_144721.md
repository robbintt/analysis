---
ver: rpa2
title: 'SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition
  of Semantic Differences Between Related Documents'
arxiv_id: '2512.07538'
source_url: https://arxiv.org/abs/2512.07538
tags:
- language
- text
- pair
- pairs
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SwissGov-RSD, the first human-annotated, document-level,
  cross-lingual dataset for recognizing semantic differences across related texts.
  It consists of 224 multi-parallel documents in English-German, English-French, and
  English-Italian with token-level difference annotations.
---

# SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents

## Quick Facts
- arXiv ID: 2512.07538
- Source URL: https://arxiv.org/abs/2512.07538
- Authors: Michelle Wastl; Jannis Vamvas; Rico Sennrich
- Reference count: 40
- Key outcome: First human-annotated, document-level, cross-lingual dataset for recognizing semantic differences across related texts

## Executive Summary
This paper introduces SwissGov-RSD, the first human-annotated, cross-lingual benchmark for token-level recognition of semantic differences between related documents. The dataset comprises 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations. The authors evaluate various systems including unsupervised methods, few-shot prompted LLMs, and fine-tuned encoder and LLM models, revealing that all systems perform significantly worse on this naturalistic benchmark compared to synthetic alternatives.

The study addresses a critical gap in semantic difference recognition research by providing realistic evaluation data that better reflects real-world scenarios. Results show substantial performance drops across all evaluated systems, with Spearman correlations decreasing by up to 78% when moving from synthetic to naturalistic data. The findings suggest that current approaches may be overfitting to synthetic data patterns and highlight the need for more robust methods and realistic evaluation settings in cross-lingual semantic difference recognition.

## Method Summary
The authors created SwissGov-RSD by collecting 224 multi-parallel documents from Swiss federal sources across three language pairs (English-German, English-French, English-Italian). Human annotators performed token-level difference annotations between document versions. The evaluation framework compares system performance against this human-annotated gold standard using Spearman correlation metrics. Multiple system types were evaluated: unsupervised methods, few-shot prompted large language models, and fine-tuned encoder models (including both traditional encoders and LLMs in fine-tuning settings).

## Key Results
- All systems show significantly worse performance on SwissGov-RSD compared to synthetic benchmarks
- Spearman correlations drop by up to 78% when evaluated on naturalistic vs synthetic data
- Encoder models perform competitively, especially in unsupervised settings
- Fine-tuned LLMs show the largest performance drops, suggesting potential overfitting to synthetic data patterns

## Why This Works (Mechanism)
The mechanism behind SwissGov-RSD's effectiveness lies in its human-annotated, naturalistic data that captures real-world semantic differences between related documents. Unlike synthetic benchmarks that often use artificially generated differences, SwissGov-RSD reflects actual variations found in Swiss federal documents across language pairs. This realistic evaluation setting exposes the limitations of current approaches and provides a more accurate measure of system capabilities for practical applications.

## Foundational Learning
- **Cross-lingual semantic difference recognition**: Understanding how meaning changes across languages is crucial for multilingual applications and requires handling linguistic variations, translation effects, and cultural nuances. Quick check: Can the system identify differences in meaning despite translation variations?
- **Token-level annotation**: Fine-grained difference marking enables precise evaluation of system capabilities at the word/phrase level. Quick check: Does the system correctly identify which specific tokens differ between versions?
- **Naturalistic vs synthetic data**: Real-world documents contain complex, context-dependent differences that synthetic data often fails to capture. Quick check: How does performance degrade when moving from synthetic to real-world evaluation?
- **Unsupervised vs fine-tuned approaches**: Comparing these paradigms reveals overfitting tendencies and generalization capabilities. Quick check: Which approach maintains performance better on new domains?
- **Multilingual evaluation**: Testing across language pairs reveals cross-lingual transfer limitations and language-specific challenges. Quick check: Does performance vary significantly across different language pairs?
- **Document-level vs sentence-level analysis**: Document-level evaluation captures broader context and discourse-level differences. Quick check: Can the system handle differences spanning multiple sentences?

## Architecture Onboarding

Component map: Document collection -> Human annotation -> System evaluation -> Performance analysis

Critical path: The critical path involves collecting authentic Swiss federal documents, performing human token-level difference annotations, and evaluating multiple system types against this gold standard. The process requires careful quality control in annotation and systematic comparison across diverse system architectures.

Design tradeoffs: The dataset size (224 documents) represents a tradeoff between annotation quality and coverage. Human annotation ensures high-quality gold standards but limits scalability. The focus on Swiss federal documents provides domain specificity but may reduce generalizability to other domains.

Failure signatures: Systems failing on SwissGov-RSD typically show over-reliance on synthetic data patterns, poor cross-lingual transfer, and inability to handle naturalistic variation. Performance drops of 70-80% indicate fundamental limitations in current approaches.

Three first experiments:
1. Evaluate system performance across different document length ranges to identify where differences become most challenging
2. Analyze false positive rates to understand if systems are over-predicting differences in naturalistic settings
3. Compare performance on different types of semantic differences (lexical, syntactic, pragmatic) to identify specific weakness patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (224 documents across three language pairs) may limit statistical robustness and generalizability
- Focus on Swiss federal documents may not represent broader domains or document types
- Evaluation primarily compares against synthetic benchmarks rather than real-world deployment scenarios
- Cross-lingual differences may be influenced by translation quality and cultural/linguistic variations specific to the document types studied

## Confidence

High confidence: The dataset provides the first human-annotated, cross-lingual benchmark for semantic difference recognition at the token level. The finding that all systems perform significantly worse on SwissGov-RSD compared to synthetic benchmarks is well-supported by the reported results.

Medium confidence: Claims about encoder models performing competitively and fine-tuned LLMs showing largest performance drops suggest potential overfitting to synthetic data patterns. While the results support these observations, the small dataset size and limited number of language pairs introduce uncertainty about broader applicability.

## Next Checks

1. Expand evaluation to additional language pairs and document domains beyond Swiss federal texts to assess generalizability
2. Conduct ablation studies on document length and difference complexity to understand performance bounds
3. Implement blind human evaluation studies to validate system performance relative to human agreement rates and establish meaningful performance thresholds