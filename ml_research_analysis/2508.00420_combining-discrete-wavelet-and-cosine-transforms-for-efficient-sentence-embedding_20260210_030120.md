---
ver: rpa2
title: Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding
arxiv_id: '2508.00420'
source_url: https://arxiv.org/abs/2508.00420
tags:
- embeddings
- embedding
- word
- sentence
- coefficients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of Discrete Wavelet Transforms
  (DWT) to word and sentence embeddings in natural language processing. The authors
  propose using DWT to selectively compress word embeddings while retaining important
  semantic information, and then combining DWT with Discrete Cosine Transform (DCT)
  to encode variable-length sentences into fixed-size vectors without increasing dimensionality.
---

# Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding

## Quick Facts
- arXiv ID: 2508.00420
- Source URL: https://arxiv.org/abs/2508.00420
- Reference count: 40
- Primary result: DWT-DCT sentence embedding model outperforms or matches state-of-the-art non-parametric models across multiple NLP tasks while reducing dimensionality by 50-75%

## Executive Summary
This paper introduces a novel approach to sentence embedding using Discrete Wavelet Transforms (DWT) combined with Discrete Cosine Transform (DCT). The method addresses the challenge of representing variable-length sentences as fixed-size vectors without increasing dimensionality. By applying DWT to compress word embeddings while preserving semantic information, and then using DCT to encode sentence-level representations, the approach achieves competitive performance on multiple NLP tasks including sentiment analysis, inference, paraphrase detection, and question classification.

## Method Summary
The proposed method applies Discrete Wavelet Transforms to word embeddings for selective compression, retaining important semantic information while reducing dimensionality by 50-75%. The compressed word embeddings are then processed using Discrete Cosine Transform to encode variable-length sentences into fixed-size vectors. This two-stage spectral approach enables efficient sentence representation without increasing the overall dimensionality of the embeddings. The method is evaluated both intrinsically through semantic similarity and concept categorization tasks, and extrinsically on downstream NLP applications.

## Key Results
- DWT embeddings maintain or improve performance compared to original embeddings while reducing dimensionality by 50-75%
- The combined DWT-DCT sentence embedding model outperforms or matches state-of-the-art non-parametric models across multiple tasks
- The approach demonstrates effectiveness for efficient sentence representation in natural language processing applications

## Why This Works (Mechanism)
The method leverages the multi-resolution analysis capability of wavelet transforms to selectively compress word embeddings while preserving important semantic features. DWT's ability to decompose signals into approximation and detail coefficients allows the model to retain critical information at different scales. When combined with DCT, which is particularly effective at energy compaction for correlated data, the approach efficiently encodes variable-length sentences into fixed-size representations. This spectral approach takes advantage of the inherent structure in language data to achieve compression without significant loss of semantic information.

## Foundational Learning
- **Discrete Wavelet Transform (DWT)**: Multi-resolution signal decomposition technique that separates signals into approximation and detail coefficients at different frequency scales. Needed for selective compression of word embeddings while preserving semantic information. Quick check: Verify that DWT coefficients capture both coarse-grained and fine-grained features of embeddings.

- **Discrete Cosine Transform (DCT)**: Energy compaction transform that concentrates signal energy into fewer coefficients, particularly effective for correlated data. Needed to encode variable-length sentences into fixed-size vectors. Quick check: Confirm that DCT coefficients provide compact representation with minimal information loss.

- **Spectral Analysis in NLP**: Application of frequency domain transforms to language data for feature extraction and compression. Needed to understand how wavelet and cosine transforms can capture linguistic patterns. Quick check: Validate that spectral features correlate with semantic properties of text.

- **Dimensionality Reduction Trade-offs**: The balance between compression ratio and information preservation in embedding spaces. Needed to optimize the wavelet decomposition parameters for maximum efficiency. Quick check: Measure performance degradation as a function of compression ratio.

## Architecture Onboarding

Component Map: Word Embeddings -> DWT Compression -> DCT Encoding -> Sentence Vectors

Critical Path: The most performance-critical path is the DWT compression stage, as it directly affects both the quality of the compressed embeddings and the computational efficiency of the overall system. The choice of wavelet family and decomposition level significantly impacts downstream performance.

Design Tradeoffs: The primary tradeoff involves selecting between higher compression ratios (better efficiency) and semantic information preservation (better accuracy). Different wavelet families (Haar, Daubechies, etc.) offer different characteristics in terms of smoothness and localization, affecting the quality of the compressed embeddings. The decomposition level determines the granularity of information retained versus discarded.

Failure Signatures: Performance degradation typically manifests as reduced accuracy on semantic similarity tasks first, followed by downstream task performance decline. Over-compression leads to loss of fine-grained semantic distinctions, while under-compression fails to achieve efficiency gains. Incorrect wavelet parameter selection may result in poor energy compaction and suboptimal compression.

First Experiments:
1. Baseline comparison: Evaluate original embeddings against DWT-compressed embeddings on semantic similarity benchmarks to establish compression effectiveness
2. Wavelet parameter sweep: Systematically vary wavelet family and decomposition level to identify optimal configuration for semantic preservation
3. Cross-task validation: Test the approach across multiple NLP tasks to assess generalizability and identify task-specific strengths and weaknesses

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though several areas remain unexplored including multilingual applications, theoretical analysis of information preservation, and comparison with alternative compression methods.

## Limitations
- The evaluation focuses primarily on English-language tasks with standard benchmark datasets, leaving generalizability to other languages unclear
- The study does not address potential information loss during compression beyond task performance metrics
- The comparison framework is somewhat narrow, focusing on non-parametric models without extensive ablation studies on different wavelet families

## Confidence

**Major Claim Clusters and Confidence:**
- DWT compression effectiveness (Medium): While results show maintained performance with reduced dimensionality, the lack of ablation studies on wavelet parameters and limited comparison with alternative compression methods reduces confidence.
- DWT-DCT sentence embedding superiority (Medium): The model performs competitively with state-of-the-art approaches, but the evaluation set is limited and does not include parametric transformer-based models for direct comparison.
- Generalization across tasks (Low-Medium): Performance varies across tasks, with some showing stronger gains than others, suggesting the approach may be more suitable for certain NLP tasks than others.

## Next Checks

1. Conduct ablation studies varying wavelet families, decomposition levels, and compression ratios to identify optimal configurations and their impact on semantic preservation
2. Evaluate performance on multilingual datasets and specialized domains (e.g., biomedical, legal) to assess cross-domain generalization
3. Perform controlled experiments comparing DWT-based compression against other dimensionality reduction techniques (PCA, autoencoders) on identical tasks to establish relative advantages and disadvantages