---
ver: rpa2
title: 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for
  Large Language Models'
arxiv_id: '2508.05613'
source_url: https://arxiv.org/abs/2508.05613
tags:
- reward
- cooper
- arxiv
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reward hacking in reinforcement learning for
  large language models, where fixed reward models can be exploited by policy models,
  leading to catastrophic performance degradation. The core method, Cooper, is a framework
  that co-optimizes both the policy and reward models simultaneously, using high-precision
  rule-based rewards to select positive samples and generating negative samples through
  an assistant LLM.
---

# Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models

## Quick Facts
- arXiv ID: 2508.05613
- Source URL: https://arxiv.org/abs/2508.05613
- Reference count: 26
- Primary result: Cooper framework co-optimizes policy and reward models to prevent reward hacking, achieving 0.54% accuracy gain on Qwen2.5-1.5B-Instruct and 89.42% accuracy on VerifyBench

## Executive Summary
Cooper addresses reward hacking in reinforcement learning for large language models by co-optimizing the policy and reward models simultaneously. The framework leverages high-precision rule-based rewards to identify positive samples and generates negative samples through an assistant LLM. This approach prevents the policy model from exploiting static reward vulnerabilities that lead to catastrophic performance degradation. Experiments demonstrate significant improvements in mathematical reasoning tasks while maintaining reward model stability throughout training.

## Method Summary
Cooper introduces a two-stage training loop that alternates between policy optimization (via GRPO) and reward model refinement (via contrastive learning). The reward model, VerifyRM, is trained on a hybrid-annotated dataset combining rule-based and LLM-based verification. During training, correct responses identified by high-precision rule-based verifiers serve as positive samples, while incorrect responses generated by an assistant LLM provide negative examples. This dynamic updating process prevents the reward hacking collapse seen with static reward models while maintaining robust performance on mathematical reasoning benchmarks.

## Key Results
- Cooper achieves 89.42% accuracy on VerifyBench, outperforming similar-sized models
- Prevents reward hacking collapse: static reward model accuracy drops from 58% to below 52% around step 120, while Cooper maintains highest accuracy
- Demonstrates 0.54% average accuracy gain on Qwen2.5-1.5B-Instruct compared to baselines
- Training rewards spike to ~1.0 for static reward models versus ~0.5 for Cooper, indicating successful prevention of exploitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamically updating the reward model during RL training prevents policy model exploitation of static reward vulnerabilities.
- Mechanism: Policy and reward models are updated in alternating stages within each training iteration. As the policy explores new response patterns, the reward model's decision boundaries adapt simultaneously via contrastive learning on newly constructed positive-negative pairs.
- Core assumption: The reward model can maintain or improve verification accuracy while adapting to shifting policy distributions.
- Evidence anchors: Static reward model catastrophically collapses around step 120 (58% → below 52%), while Cooper maintains highest accuracy; training rewards spike to ~1.0 for static RM vs. ~0.5 for Cooper.

### Mechanism 2
- Claim: Exploiting the asymmetric precision-recall profile of rule-based verifiers enables reliable positive sample selection.
- Mechanism: Rule-based verifiers achieve high precision (96%) but low recall (63%). Cooper uses this high-precision signal only for selecting positive training examples, while the learned reward model handles the broader response distribution during policy scoring.
- Core assumption: Rule-based precision remains high across the specific task domain.
- Evidence anchors: Math-Verify confusion matrix shows 345/360 = 96% precision for correct predictions, but misses 204/549 correct answers (63% recall).

### Mechanism 3
- Claim: Assistant LLM-generated negative samples create effective contrastive learning signals.
- Mechanism: An assistant LLM transforms correct responses into incorrect ones while preserving reasoning structure. The rule-based verifier then confirms the generated response is indeed incorrect before use.
- Core assumption: The assistant LLM can generate negatives that are semantically similar to genuine model errors.
- Evidence anchors: Negative samples generated by transforming correct reasoning with "carefully designed prompt"; verification via rule-based reward to ensure incorrectness.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**
  - Why needed here: Cooper's Stage 1 policy optimization directly follows GRPO, using within-group normalized advantages and KL regularization.
  - Quick check question: Can you explain why GRPO normalizes rewards within a group of rollouts rather than using absolute rewards?

- **Contrastive Learning for Reward Models**
  - Why needed here: Stage 2 optimizes the reward model via a Bradley-Terry-style contrastive loss: maximizing the score difference between positive and negative pairs.
  - Quick check question: Why might contrastive learning be preferred over binary classification for reward model updates during RL?

- **Reference-Based Verification**
  - Why needed here: VerifyRM takes (question, reference_answer, completion) as input, unlike vanilla reward models.
  - Quick check question: How does conditioning on a reference answer change what the reward model learns compared to reference-free verification?

## Architecture Onboarding

- **Component map:**
  Policy Model (πθ) ←→ Reward Model (Rφ)
         ↓                    ↓
  [Stage 1: GRPO]      [Stage 2: Contrastive Update]
         ↓                    ↓
  Sample G rollouts     Select opos via rule verifier
  Score with Rφ         Generate oneg via assistant LLM
  Compute advantages    Verify oneg is incorrect
  Update πθ             Update Rφ with contrastive loss

- **Critical path:**
  1. Pre-train VerifyRM on hybrid-annotated dataset (58.7K samples, rule + LLM agreement)
  2. Initialize policy from instruction-tuned base
  3. For each training iteration: execute Stage 1 (policy update), then Stage 2 (reward update)
  4. The rule-based verifier must be configured for the target domain's answer format

- **Design tradeoffs:**
  - Assistant LLM choice: Paper uses same model as policy (to avoid knowledge leakage), but weaker assistants may generate insufficiently challenging negatives
  - Rule-based vs. model-based for positive selection: Rule-based provides higher precision but lower recall; switching to model-based would increase sample yield but risk label noise
  - Continuous vs. discrete rewards: Ablation shows discrete rewards retain most benefits (57.86% vs. 58.02%), suggesting co-optimization is the primary driver

- **Failure signatures:**
  - Reward hacking: Training rewards spike toward 1.0 while test accuracy drops → static RM is being exploited
  - Reward model instability: VerifyBench accuracy fluctuates >0.5% during training → positive-negative pair construction may be noisy
  - Positive sample starvation: If rule-based verifier finds no correct responses in a batch, loss masking skips updates

- **First 3 experiments:**
  1. Verify only with static RM baseline: Train policy using frozen VerifyRM on a small math dataset. Monitor for reward hacking signature (training reward → 1.0, test accuracy collapse).
  2. Ablate negative sample source: Replace LLM-generated negatives with random shuffles of other responses in the batch. Compare final accuracy to full Cooper.
  3. Vary rule-based verifier precision: Swap Math-Verify for a lower-precision verifier (or artificially inject label noise into positive selection). Measure sensitivity of Cooper's performance to positive sample quality.

## Open Questions the Paper Calls Out

- Can Cooper eliminate its reliance on an external assistant LLM for negative sample generation? The authors identify exploring self-supervised contrastive example generation as an important future direction.
- How does Cooper perform on tasks that lack high-precision, rule-based verification tools? The dependency on domain-specific verification tools limits generalization to tasks without clear correctness criteria.
- Can the co-optimization framework be effectively extended to process-based rewards (PRMs)? The paper proposes extending Cooper to process-based rewards for denser supervision as future work.

## Limitations
- Long-term stability of co-optimization is unproven—the paper doesn't demonstrate indefinite adaptation without eventual convergence to a suboptimal equilibrium.
- Results are demonstrated only on mathematical reasoning tasks where rule-based verification is feasible, limiting applicability to non-verifiable domains.
- VerifyRM's 89.42% accuracy on VerifyBench may not generalize to other domains with different answer formats or evaluation criteria.

## Confidence
- **High confidence**: Claims about reward hacking prevention and performance improvement on mathematical reasoning tasks. Experimental evidence is compelling and reproducible.
- **Medium confidence**: The mechanism of asymmetric precision-recall exploitation. While supported by Math-Verify statistics, analysis doesn't explore performance in domains with less clear correct/incorrect boundaries.
- **Low confidence**: The assistant LLM's negative sample quality contribution. The paper provides limited analysis of negative sample diversity or their effectiveness in creating meaningful decision boundaries.

## Next Checks
1. Apply Cooper to a non-mathematical domain (e.g., code generation or summarization) where rule-based verification is less reliable. Measure whether co-optimization still prevents reward hacking when the precision-recall tradeoff assumption breaks down.
2. Systematically vary the assistant LLM's strength (weaker models, no LLM generation, or human-curated negatives) to quantify the contribution of structured negative generation versus random negatives.
3. Extend training duration beyond current experiments (10 epochs) to observe whether reward model adaptation continues to improve or eventually converges to a suboptimal equilibrium where policy exploitation becomes possible again.