---
ver: rpa2
title: 'Be.FM: Open Foundation Models for Human Behavior'
arxiv_id: '2505.23058'
source_url: https://arxiv.org/abs/2505.23058
tags:
- behavioral
- data
- behavior
- sharing
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Be.FM is a family of foundation models designed to model human
  behavior by fine-tuning large language models on diverse behavioral datasets. It
  aims to predict and simulate behaviors, infer subject characteristics, generate
  contextual insights, and apply behavioral science knowledge.
---

# Be.FM: Open Foundation Models for Human Behavior

## Quick Facts
- arXiv ID: 2505.23058
- Source URL: https://arxiv.org/abs/2505.23058
- Authors: Yutong Xie; Zhuoheng Li; Xiyuan Wang; Yijun Pan; Qijia Liu; Xingzhi Cui; Kuang-Yu Lo; Ruoyi Gao; Xingjian Zhang; Jin Huang; Walter Yuan; Matthew O. Jackson; Qiaozhu Mei
- Reference count: 40
- Primary result: Be.FM is a family of foundation models designed to model human behavior by fine-tuning large language models on diverse behavioral datasets.

## Executive Summary
Be.FM is a family of foundation models designed to model human behavior by fine-tuning large language models on diverse behavioral datasets. It aims to predict and simulate behaviors, infer subject characteristics, generate contextual insights, and apply behavioral science knowledge. Evaluated on tasks like economic game behavior prediction, Big Five personality trait inference, demographic prediction, context inference in experiments, and complex problem-solving (IEO), Be.FM outperforms general-purpose LLMs and commercial models on most benchmarks. It shows improved alignment with human behavior distributions and better individual-level predictions, demonstrating the potential of behavioral foundation models to advance behavioral science research and applications.

## Method Summary
Be.FM fine-tunes Llama-3.1-8B/70B-Instruct models using LoRA on a heterogeneous corpus including 2,703 AER publications, MobLab experimental game records (82,057 observations), and Big Five personality survey data. The training follows a y = F(K, x, c) framework where behavior is modeled as a function of knowledge, subject characteristics, and context. Data is formatted in Alpaca template style and trained for 3 epochs with SFT only. The model is evaluated on distributional alignment (Wasserstein distance), individual predictions (MAE, Spearman), and complex reasoning tasks (IEO accuracy).

## Key Results
- Be.FM achieves lower Wasserstein distances than base Llama and GPT-4o across 6/7 game scenarios
- Personality prediction improves individual-level accuracy with better MAE and Spearman correlation
- IEO complex problem-solving accuracy increases from 48.4% → 51.3% (8B) and 68.8% → 73.3% (70B)
- Research workflow prediction shows Be.FM outperforms baselines on 3/4 metrics

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning general-purpose LLMs on heterogeneous behavioral data improves alignment with human behavioral distributions across economic games. The model learns implicit associations between game instructions and empirically observed action distributions via supervised fine-tuning on 82,057 game-play records, adjusting LoRA weights to shift priors from "rational agent" toward "empirically observed human" patterns. Core assumption: Behavioral regularities in experimental games reflect generalizable decision-making heuristics that transfer across game structures. Evidence: Table 3 shows Be.FM achieves lower Wasserstein distances than base Llama and GPT-4o across 6/7 game scenarios.

### Mechanism 2
Structuring training data around the y = F(K, x, c) decomposition enables cross-task generalization to untrained tasks. By training on literature data, experimental records {x, c, y}, and survey responses, the model develops compositional representations where context inference can emerge from inverting learned forward mappings. Core assumption: The functional decomposition is approximately correct—behavior can be modeled as a function of knowledge, subject characteristics, and context. Evidence: Context inference task shows Be.FM generates treatments aligned with meta-analysis literature despite no direct training on intervention prediction.

### Mechanism 3
Behavioral knowledge extraction from literature (titles, abstracts → research workflows) enhances domain-specific reasoning without direct task training. Structured extraction of context-idea-method-outcome-impact from 2,703 AER publications creates a reasoning scaffold; the model learns to associate research contexts with valid experimental approaches. Core assumption: Scientific literature encodes causal and procedural knowledge that can be repurposed for novel reasoning tasks. Evidence: IEO accuracy improves from 48.4% → 51.3% (8B) and 68.8% → 73.3% (70B) despite no IEO training data.

## Foundational Learning

- **Wasserstein Distance**
  - Why needed here: Primary metric for evaluating distributional alignment between predicted and human behavioral distributions
  - Quick check question: Can you explain why Wasserstein distance is preferred over KL divergence for comparing discrete action distributions with potential non-overlapping support?

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: Enables efficient fine-tuning of 70B model with limited compute while preserving base capabilities
  - Quick check question: If LoRA rank is set too low, what type of behavioral patterns might the model fail to capture?

- **Behavioral Economics Paradigms (Dictator, Ultimatum, Trust Games)**
  - Why needed here: Core evaluation benchmarks; understanding these games is essential for interpreting distributional results
  - Quick check question: In the Dictator Game, why might a model trained on "rational" behavior over-predict $0 transfers compared to human data?

## Architecture Onboarding

- **Component map**: Literature data → Experimental data → Survey data → Llama-3.1 backbone → LoRA fine-tuning → Be.FM model
- **Critical path**: 1. Data formatting per category (research workflows for literature, game records for experimental, demographic→score pairs for survey) 2. LoRA configuration (rank, alpha, dropout not specified—needs verification) 3. Training: 3 epochs, lr=1e-4, cosine scheduler, warmup 0.1, bf16 4. Evaluation: Distribution-level (Wasserstein) and individual-level (MAE, Spearman)
- **Design tradeoffs**: 8B vs 70B: 8B faster/cheaper; 70B better at complex reasoning (IEO: 51.3% vs 73.3%). SFT-only vs RL: Authors deliberately avoided RL to demonstrate generalization; future versions plan RL for reasoning. Quantization (8-bit) reduces memory but may affect precision on numerical predictions
- **Failure signatures**: High Wasserstein distance on specific games indicates domain mismatch. Low Spearman correlation with acceptable MAE suggests distribution looks right but individual rankings are wrong. Context inference outputs that contradict established literature suggest overfitting to training distribution
- **First 3 experiments**: 1. Ablation by data category: Train three models (literature-only, experimental-only, survey-only) and evaluate on all benchmarks to isolate contribution of each data type 2. LoRA rank sensitivity: Test ranks {4, 8, 16, 32} on personality prediction task; measure if behavioral nuance requires higher rank 3. Cross-population generalization: Train on MobLab data (student subjects), evaluate on a demographically different held-out population to test break condition #1

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning (RL) training significantly enhance Be.FM's reasoning abilities and complex problem-solving performance compared to supervised fine-tuning alone? Basis: The conclusion notes that the initial versions used only supervised fine-tuning (SFT) and that "RL-based reasoning will be equipped in future versions" because it is "critical to improve the reasoning abilities of foundation models." Unresolved because current architecture relies exclusively on SFT; the marginal gains or potential trade-offs of introducing RL for behavioral tasks are untested.

### Open Question 2
How does the inclusion of observational data impact the model's robustness and generalizability to real-world scenarios compared to controlled experimental data? Basis: Section 2.1 states that observational data captures "spontaneous, less controlled behaviors" and "will be included in future versions," but was not used in the initial training. Unresolved because the current model is trained on controlled experimental and survey data; its ability to handle the noise and variability of naturalistic, real-world behavioral data remains unverified.

### Open Question 3
What standardized, quantitative evaluation procedures can effectively measure the success of context inference and intervention design? Basis: Section 3.5 states, "A quantitative evaluation procedure is needed to measure the success of context inference," noting that current assessments are qualitative. Unresolved because there is currently no metric to numerically score how well a model suggests valid experimental treatments or infers situational factors.

## Limitations
- MobLab experimental data requires proprietary access, creating a barrier to full replication
- Training corpus draws heavily from student populations and specific behavioral economics paradigms, raising questions about generalizability
- Absence of RL fine-tuning means the current model may not achieve optimal reasoning capabilities for complex behavioral prediction tasks

## Confidence
- **High confidence**: Distributional alignment results (Wasserstein distances) - directly measurable and show consistent improvements over baselines
- **Medium confidence**: Individual-level predictions (MAE, Spearman) - depend heavily on quality and representativeness of training data
- **Low confidence**: Cross-task generalization claims - particularly IEO improvements without task-specific training; literature-based knowledge extraction lacks strong empirical validation

## Next Checks
1. Cross-population generalization test: Train Be.FM on MobLab data from one demographic group and evaluate on held-out data from a significantly different population to assess WEIRD population biases
2. Ablation study on data contributions: Systematically train separate models on each data category and evaluate all benchmarks to quantify each category's contribution to observed improvements
3. Literature bias validation: Perform systematic review of AER publications to identify methodological biases or temporal trends that could be learned and propagated by the model, then test whether these biases affect downstream predictions