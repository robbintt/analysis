---
ver: rpa2
title: 'NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization'
arxiv_id: '2505.24575'
source_url: https://arxiv.org/abs/2505.24575
tags:
- nexus
- marianne
- elinor
- summarization
- narrative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NEXUS SUM, a hierarchical multi-agent LLM
  framework that transforms long-form narrative summarization by converting dialogue
  to descriptive prose and applying iterative compression. The method improves coherence,
  controls summary length, and preserves key narrative details across diverse storytelling
  domains.
---

# NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization

## Quick Facts
- arXiv ID: 2505.24575
- Source URL: https://arxiv.org/abs/2505.24575
- Reference count: 40
- Primary result: Hierarchical multi-agent LLM framework achieving up to 30.0% improvement in BERTScore (F1) over state-of-the-art methods for long-form narrative summarization

## Executive Summary
NEXUS SUM introduces a hierarchical multi-agent LLM framework that transforms long-form narrative summarization by converting dialogue to descriptive prose and applying iterative compression. The method improves coherence, controls summary length, and preserves key narrative details across diverse storytelling domains. Evaluated on four benchmarks (BookSum, MovieSum, MENSA, SummScreenFD), NEXUS SUM achieves up to a 30.0% improvement in BERTScore (F1) over state-of-the-art methods, demonstrating superior performance in processing extended narratives while maintaining factual accuracy and length adherence.

## Method Summary
NEXUS SUM is a 3-stage hierarchical multi-agent pipeline using Mistral-Large-Instruct-2407 (123B) on 4x A100 GPUs. The Preprocessor (P) converts dialogue to narrative prose using 8-scene chunks. The Narrative Summarizer (S) generates initial summaries from preprocessed text. The Compressor (C) iteratively compresses summaries via sentence-based chunking until meeting target length θ (max 10 iterations). All stages use chunk-and-concat approach with temperature=0.3, top_p=1.0, seed=42. The method achieves controllable summary length while preserving narrative coherence across diverse domains.

## Key Results
- Up to 30.0% improvement in BERTScore (F1) over CachED on BookSum
- Length Adherence Rate (LAR) of 0.88-0.99 across target lengths vs. 0.24-0.60 for Zero-Shot
- Dialogue-to-prose preprocessing improves BERTScore by +2.45 on MENSA benchmark
- Maintains factual accuracy while processing narratives up to 158K tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting dialogue into narrative prose improves downstream summarization coherence.
- Mechanism: The Preprocessor agent (P) transforms multi-speaker dialogues and stage directions into unified third-person prose, reducing fragmentation and normalizing mixed-format narrative structures before summarization.
- Core assumption: Narrative summarizers perform better on homogeneous prose than on interleaved dialogue/description formats.
- Evidence anchors:
  - [abstract] "A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence."
  - [section 5.2] Ablation shows P alone improves BERTScore from 54.81 to 57.26 (+2.45).
  - [corpus] Weak corpus signal; no direct neighbor papers validate dialogue-to-description as a pre-processing step for summarization.
- Break condition: If input narratives contain minimal dialogue or already use consistent prose formatting, the transformation may yield negligible gains.

### Mechanism 2
- Claim: Hierarchical chunk-and-concat processing enables summarization of arbitrarily long narratives without context window truncation.
- Mechanism: Input text is segmented into scene-based chunks (k = total scenes/8), each chunk is processed independently by the appropriate agent, and outputs are concatenated. This decouples input length from model context limits.
- Core assumption: Scene boundaries are semantically meaningful units that preserve narrative coherence when processed independently.
- Evidence anchors:
  - [section 3.1] "P segments the input text into scene-based chunks... k is dynamically computed based on a fixed scene-based chunk size."
  - [section 5.1] On BookSum (158K avg tokens), NEXUS SUM achieves +30.0% BERTScore over CachED, which uses static chunking.
  - [corpus] Neighbor paper "STAGE" corroborates that screenplay understanding benefits from scene-level structuring.
- Break condition: If scene segmentation is inaccurate or documents lack clear scene markers, chunk boundaries may disrupt narrative dependencies.

### Mechanism 3
- Claim: Iterative sentence-level compression provides controllable summary length while retaining key content.
- Mechanism: The Compressor agent (C) applies hierarchical compression across multiple iterations, with sentence-based chunking at granularity δ. Compression continues until output falls within target word count θ (max 10 iterations).
- Core assumption: Progressive compression at finer granularity preserves more salient content than single-pass summarization.
- Evidence anchors:
  - [section 3.3] "Si = Ci(si−1,1) ⊕ Ci(si−1,2) ⊕ ... ⊕ Ci(si−1,li−1)" — defines iterative compression formula.
  - [section 5.3] Table 4 shows LAR (Length Adherence Rate) of 0.88–0.99 vs. 0.24–0.60 for Zero-Shot across target lengths.
  - [appendix C] Table 13 demonstrates inverse relationship between chunk size δ and compression ratio.
  - [corpus] No direct corpus validation for iterative compression in narrative summarization.
- Break condition: If compression ratio is too aggressive per iteration, critical plot points may be lost; if θ is set too low, iterations will fail to converge.

## Foundational Learning

- Concept: Hierarchical vs. flat document processing
  - Why needed here: NEXUS SUM relies on multi-stage decomposition; understanding when hierarchical processing outperforms single-pass is essential for architectural decisions.
  - Quick check question: Given a 100K-token document, what factors determine whether chunked hierarchical processing will outperform a long-context model?

- Concept: Compression ratio and granularity trade-offs
  - Why needed here: The δ (chunk size) parameter directly controls how aggressively content is compressed per iteration.
  - Quick check question: If you observe 90% compression with δ=5000 tokens but need higher detail retention, should you increase or decrease δ?

- Concept: Prompt engineering for multi-agent orchestration
  - Why needed here: NEXUS SUM is fine-tuning-free; all task adaptation occurs via prompts. Understanding CoT and Few-Shot integration is critical for domain transfer.
  - Quick check question: How would you modify the Summarizer agent prompt to handle a new domain (e.g., technical documentation vs. screenplays)?

## Architecture Onboarding

- Component map: Preprocessor (P) -> Narrative Summarizer (S) -> Compressor (C) -> Final Output
- Critical path: P → S → C (n iterations until θ satisfied). All stages use chunk-and-concat; failure at any stage propagates downstream.
- Design tradeoffs:
  - Smaller δ → lower compression ratio, more detail retained, but more iterations and compute
  - Larger δ → higher compression, faster convergence, but potential information loss
  - Scene-based vs. token-based chunking: Scene preserves narrative coherence; token-based is domain-agnostic but may split mid-scene
- Failure signatures:
  - Output length far exceeds or falls short of θ: Check if θ is realistic for input length; verify δ configuration
  - Incoherent summaries with missing character arcs: Inspect P outputs for incomplete dialogue transformation
  - Excessive iterations (>10) without convergence: Compression ratio may be too conservative; reduce δ
- First 3 experiments:
  1. Ablate each agent (P-only, S-only, C-only) on a held-out screenplay to quantify individual contributions using BERTScore and LAR
  2. Sweep δ values (300, 500, 1000, 2000, 4000) on BookSum and measure compression ratio vs. BERTScore trade-off to validate Appendix C findings on your infrastructure
  3. Compare Zero-Shot vs. NEXUS SUM on a custom dataset (e.g., technical manuals) to test domain transfer assumptions and identify prompt engineering needs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can narrative summarization systems better align automated evaluation metrics (e.g., BERTScore) with human preferences for readability and fluency?
- Basis in paper: [explicit] The authors note in Section 7 (Limitations) that "higher automated scores do not necessarily align with human preference"—experts rated Zero-Shot outputs as more readable (4.17 vs. 2.17) despite NEXUS SUM achieving higher BERTScore.
- Why unresolved: The discrepancy reveals that semantic similarity metrics fail to capture critical human-valued qualities like natural phrasing and narrative flow.
- What evidence would resolve it: Development and validation of evaluation metrics that correlate strongly with human judgments across readability, coherence, and user preference dimensions.

### Open Question 2
- Question: What post-processing or fluency-enhancing techniques can preserve NEXUS SUM's factual accuracy while improving readability to match human expectations?
- Basis in paper: [explicit] Section 6 states: "Future work should explore a fluency-enhancing summarization framework while preserving factual consistency." NEXUS SUMR showed preliminary promise (+1.5 readability points) but this remains an open challenge.
- Why unresolved: The tension between dense, fact-preserving summaries and naturally flowing prose has not been resolved; the reflection-based NEXUS SUMR was only tested on a limited K-Drama sample.
- What evidence would resolve it: Systematic evaluation of fluency-enhancement modules across diverse narrative genres with both automated metrics and human evaluation.

### Open Question 3
- Question: How does the dialogue-to-description transformation affect narrative coherence differently across genres (novels vs. scripts vs. TV shows)?
- Basis in paper: [inferred] RQ2 asks about dialogue-to-description's impact, and Table 3 shows +2.45 BERTScore improvement on MENSA, but Table 2 shows variable gains across benchmarks (+30.0% BookSum, +7.1% MovieSum, +1.7% MENSA, 0.0% SummScreenFD), suggesting genre-dependent effects that were not systematically analyzed.
- Why unresolved: The paper demonstrates overall effectiveness but does not isolate how the preprocessing stage interacts with different narrative structures (novel prose vs. screenplay format vs. TV dialogue-heavy scripts).
- What evidence would resolve it: Controlled ablation studies isolating the preprocessing component across each dataset with fine-grained analysis of dialogue density and narrative style.

## Limitations
- Evaluation relies on automatic metrics (BERTScore, ROUGE, LAR) rather than human judgment, which may not fully capture narrative coherence or factual consistency
- Preprocessing step's effectiveness depends on the assumption that dialogue-to-prose conversion universally improves summarization, but this may not hold for narratives with minimal dialogue
- Scene-based chunking assumes clear semantic boundaries exist in all narrative types, which may not be true for certain literary styles or non-Western narrative structures

## Confidence
- High Confidence: The hierarchical architecture and chunk-and-concat approach for bypassing context window limits is technically sound and well-validated through ablation studies
- Medium Confidence: The dialogue-to-prose preprocessing contribution (+2.45 BERTScore) is validated through ablation but lacks direct corpus support for the underlying mechanism
- Low Confidence: Claims about maintaining factual accuracy across diverse domains rely solely on automatic metrics without human evaluation

## Next Checks
1. **Human Evaluation Study:** Conduct a small-scale human evaluation comparing NEXUS SUM summaries against baseline methods on BookSum and MovieSum, focusing on coherence, factual accuracy, and narrative completeness
2. **Cross-Domain Transfer Test:** Apply NEXUS SUM to a non-narrative domain (e.g., technical documentation or scientific papers) using the same prompt engineering approach
3. **Error Analysis on Scene Segmentation:** Systematically evaluate the scene segmentation heuristic on a diverse corpus including novels without clear scene markers, dialogue-heavy plays, and descriptive prose