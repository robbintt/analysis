---
ver: rpa2
title: 'VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via
  Structured Tool Integration'
arxiv_id: '2601.14440'
source_url: https://arxiv.org/abs/2601.14440
tags:
- reasoning
- image
- modality
- text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VisTIRA introduces a tool-integrated reasoning framework for visual
  mathematical problem solving by decomposing image-based math questions into natural
  language rationales and executable Python programs. It generates high-quality tool-use
  trajectories from real-world homework-style images and converts text-only math corpora
  into visually typeset counterparts via LaTeX rendering.
---

# VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration

## Quick Facts
- **arXiv ID:** 2601.14440
- **Source URL:** https://arxiv.org/abs/2601.14440
- **Reference count:** 40
- **Primary result:** Tool-integrated reasoning improves visual math accuracy; OCR grounding benefits smaller VLMs but not larger ones.

## Executive Summary
VisTIRA introduces a framework that decomposes visual math problems into natural language rationales and executable Python programs, enabling accurate mathematical reasoning through external tool execution. The approach generates high-quality tool-use trajectories from real-world homework-style images and converts text-only math corpora into visually typeset counterparts via LaTeX rendering. By fine-tuning vision-language models on these trajectories, VisTIRA demonstrates improved image-based reasoning accuracy while revealing a scale-dependent modality gap where larger models require less external scaffolding. The framework achieves state-of-the-art results on visual mathematical reasoning benchmarks while providing insights into the complementary roles of structured reasoning and OCR-based grounding.

## Method Summary
VisTIRA trains vision-language models to solve mathematical problems presented as images by generating natural language rationales followed by executable Python programs, with tool execution outputs fed back iteratively until a final answer is produced. The framework uses two key datasets: SnapAsk (real-world homework-style images converted to tool-integrated trajectories) and NuminaMath (text-only math problems rendered as images via LaTeX). During fine-tuning, the model learns to generate reasoning paths that include rationale (ρᵢ), Python program (Aᵢ), and execution output (Oᵢ) in a loop until producing a final answer marked by \boxed{}. The approach also explores OCR grounding by concatenating extracted text with image inputs, finding this helps smaller models but provides diminishing returns for larger ones.

## Key Results
- Tool-integrated reasoning improves accuracy on visual math benchmarks compared to standard CoT approaches
- OCR grounding narrows the modality gap for smaller VLMs (2B-7B) but shows diminishing returns at larger scales
- The severity of the image-text modality gap inversely correlates with model size, with larger models showing stronger intrinsic visual parsing
- VisTIRA achieves state-of-the-art results on both SnapAsk and NuminaMath visual math reasoning benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Tool-Integrated Reasoning Loop
Structured decomposition of visual math problems into rationales and executable Python programs improves accuracy by deferring computation to symbolic engines. The model iteratively generates natural language rationale (ρᵢ), followed by a Python program (Aᵢ), which is executed externally. The output (Oᵢ) is fed back, enabling correction of hallucinated arithmetic through verifiable intermediate signals. Core assumption: The visual encoder correctly parses layout-level structure so symbolic reasoning proceeds from accurate transcriptions.

### Mechanism 2: OCR-Based Textual Grounding
Augmenting image inputs with OCR-extracted text narrows the modality gap for smaller VLMs by providing explicit textual cues that compensate for weaker visual encoders. A state-of-the-art OCR system (DeepSeek-OCR) extracts text from the math image, and the OCR output is concatenated with the original image as input to the VLM, giving the model a textual scaffold for interpretation. Core assumption: The OCR transcription is sufficiently accurate; for smaller models, the benefit of explicit text outweighs potential noise.

### Mechanism 3: Scale-Dependent Modality Gap
The severity of the image-text modality gap inversely correlates with model size; larger VLMs have stronger intrinsic visual parsing, reducing reliance on external scaffolding. Smaller models (2B–7B parameters) have limited visual encoder capacity, causing more frequent misreads of dense formulas and layouts. Structured reasoning and OCR grounding compensate for this weakness, while larger models internalize more robust visual representations, diminishing marginal returns from external aids.

## Foundational Learning

- **Concept: Program-of-Thoughts (PoT) / Tool-Augmented Reasoning**
  - Why needed here: VisTIRA builds on PoT, where LLMs generate executable code rather than free-form reasoning. Without this background, the rationale-code-output loop may seem opaque.
  - Quick check question: Can you explain why deferring arithmetic to a Python interpreter reduces hallucination compared to direct chain-of-thought reasoning?

- **Concept: Vision-Language Model (VLM) Architecture**
  - Why needed here: VisTIRA fine-tunes a VLM (Qwen2.5-VL-7B). Understanding how visual encoders map images to token space helps diagnose where parsing failures originate.
  - Quick check question: In a standard VLM, does the visual encoder output interact with the language model at the token level, embedding level, or both?

- **Concept: Modality Gap in Contrastive Learning**
  - Why needed here: The paper characterizes a performance gap between text and image modalities. Prior work shows this gap arises from representational separation in embedding space.
  - Quick check question: Why might semantically identical content (text vs. rendered image) yield different embeddings in a vision-language model?

## Architecture Onboarding

- **Component map:** VLM backbone (Qwen2.5-VL-7B) → Tool execution environment (external Python interpreter) → OCR module (DeepSeek-OCR) → LaTeX rendering pipeline → Trajectory dataset (147,948 VisTIRA trajectories)

- **Critical path:**
  1. Image input → VLM generates rationale (ρᵢ)
  2. VLM generates Python program (Aᵢ)
  3. External tool executes Aᵢ → produces output (Oᵢ)
  4. Oᵢ fed back to VLM → iterate or terminate with \boxed{} answer
  5. (Optional) OCR extraction concatenated with image at inference time

- **Design tradeoffs:**
  - OCR grounding: Helps smaller models (+5.12% for Qwen), but may add noise for larger models (GPT-5 shows mixed results)
  - Trajectory filtering: Retaining only trajectories where CoT matches final answer improves quality but may reduce solution diversity
  - LoRA vs. full fine-tuning: Paper uses LoRA (r=32, α=64) for efficiency; full fine-tuning may yield higher gains at greater compute cost

- **Failure signatures:**
  - Cascading parsing errors: Misread superscript/subscript leads to wrong Python code → incorrect final answer
  - OCR noise at scale: For large models, redundant OCR text may introduce conflicting signals
  - Stopping failure: Model may not emit \boxed{} within maximum steps k, truncating trajectory

- **First 3 experiments:**
  1. Baseline modality gap measurement: Evaluate Qwen2.5-VL-7B on NuminaMath text vs. LaTeX-rendered images to quantify the gap on your infrastructure.
  2. OCR grounding ablation: Compare image-only, image+OCR, and OCR-only inputs for a smaller model (e.g., 3B) to confirm scale-dependent benefits.
  3. Tool integration vs. CoT-only: Fine-tune a small VLM on VisTIRA trajectories and compare against a CoT-only baseline on SnapAsk holdout set to isolate the contribution of tool-execution supervision.

## Open Questions the Paper Calls Out

### Open Question 1
Does the VisTIRA framework generalize to handwritten or low-quality photographed mathematical content? The authors explicitly state that synthetically rendered images do not fully capture the noise, distortions, and stylistic diversity of real-world handwritten or photographed mathematical content, leaving the model's robustness to true handwriting or camera capture unverified.

### Open Question 2
Can adaptive grounding mechanisms be developed to mitigate the negative effects of OCR redundancy for larger models? The authors note that more adaptive grounding mechanisms are needed because simple concatenation introduces redundancy or noise for larger models like GPT-5, even though it aids smaller models.

### Open Question 3
Does training on synthetic trajectories from strong teacher models bias student models toward specific, potentially sub-optimal reasoning styles? The authors warn that tool-integrated trajectories may introduce bias toward specific reasoning styles and reduce diversity in solution strategies.

## Limitations
- Proprietary SnapAsk dataset cannot be independently verified or reproduced, limiting assessment of training data distribution
- Limited model size comparison (only 7B vs. frontier) weakens generalization claims about scale-dependent modality gap
- Lack of proper ablation studies prevents isolation of individual component contributions (tool integration vs. OCR vs. fine-tuning)

## Confidence
- **High confidence:** The fundamental mechanism of tool-integrated reasoning (PoT-style decomposition into rationale, code, and execution) is well-established and the paper's implementation follows standard practices
- **Medium confidence:** The scale-dependent modality gap finding is plausible but the limited model size comparison weakens generalization claims
- **Low confidence:** The exact contribution of each component cannot be isolated due to lack of proper ablation studies

## Next Checks
1. Ablation study on tool components: Evaluate VisTIRA-trained models using only CoT reasoning, only tool execution, and the full tool-integrated approach to isolate component contributions
2. Cross-scale modality gap measurement: Extend analysis to include additional model sizes (3B, 13B, 34B) to verify inverse correlation with model scale
3. OCR grounding robustness test: Systematically vary OCR quality and measure accuracy degradation across model sizes to determine whether diminishing returns are due to inherent robustness or noise tolerance