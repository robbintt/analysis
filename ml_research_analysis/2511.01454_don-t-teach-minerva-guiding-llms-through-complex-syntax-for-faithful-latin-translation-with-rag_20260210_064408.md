---
ver: rpa2
title: '"Don''t Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin
  Translation with RAG'
arxiv_id: '2511.01454'
source_url: https://arxiv.org/abs/2511.01454
tags:
- latin
- draft
- translation
- pipeline
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a reproducible draft-based refinement pipeline
  for Latin-to-English translation, combining a fine-tuned NLLB-1.3B model with zero-shot
  LLM refinement (Llama-3.3 or Qwen3), augmented by retrieval of similar examples.
  The method addresses challenges of translating morphology-rich, low-resource Latin
  while preserving philological fidelity.
---

# "Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG

## Quick Facts
- arXiv ID: 2511.01454
- Source URL: https://arxiv.org/abs/2511.01454
- Authors: Sergio Torres Aguilar
- Reference count: 21
- Key outcome: Draft-based refinement pipeline achieves statistically comparable performance to GPT-5 benchmark on Latin-to-English translation

## Executive Summary
This paper introduces a reproducible pipeline for Latin-to-English translation that combines a fine-tuned NLLB-1.3B model with zero-shot LLM refinement (Llama-3.3 or Qwen3), augmented by retrieval of similar examples. The method addresses challenges of translating morphology-rich, low-resource Latin while preserving philological fidelity. The pipeline achieves statistically comparable performance to the GPT-5 benchmark on both in-domain and out-of-domain test sets, with a peak COMET score of 75.7