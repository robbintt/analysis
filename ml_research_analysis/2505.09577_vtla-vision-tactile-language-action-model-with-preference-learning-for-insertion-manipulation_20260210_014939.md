---
ver: rpa2
title: 'VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion
  Manipulation'
arxiv_id: '2505.09577'
source_url: https://arxiv.org/abs/2505.09577
tags:
- vtla
- arxiv
- tactile
- manipulation
- insertion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VTLA, a vision-tactile-language-action model
  designed for contact-rich robotic manipulation tasks like peg-in-hole insertion.
  The authors address the challenge of integrating visual and tactile sensing in language-conditioned
  robotic control, where traditional vision-language models struggle due to lack of
  tactile feedback and limited temporal reasoning.
---

# VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation

## Quick Facts
- **arXiv ID**: 2505.09577
- **Source URL**: https://arxiv.org/abs/2505.09577
- **Reference count**: 40
- **Primary result**: VTLA achieves >90% success rates on unseen peg shapes, outperforming traditional imitation learning methods and multi-modal baselines in contact-rich robotic insertion tasks.

## Executive Summary
This paper presents VTLA, a vision-tactile-language-action model designed for contact-rich robotic manipulation tasks like peg-in-hole insertion. The authors address the challenge of integrating visual and tactile sensing in language-conditioned robotic control, where traditional vision-language models struggle due to lack of tactile feedback and limited temporal reasoning. The core method introduces Vision-Guided Temporally Enhanced Tokens (VGTE) that prioritize visual information and enhance temporal fusion before tokenization, improving cross-modal temporal reasoning. Additionally, Direct Preference Optimization (DPO) is applied to provide regression-like supervision, bridging the gap between classification-based next token prediction and continuous robotic control tasks. The model is trained on a synthetic dataset of 28,000 vision-tactile-action-instruction pairs collected in simulation, with domain randomization applied to improve real-world transfer.

## Method Summary
VTLA uses a two-stage training approach on Qwen2-VL 7B: first fine-tuning with next token prediction loss on vision-tactile-language-action data, then applying Direct Preference Optimization using preference pairs generated from action distance rankings. The architecture employs a frozen ViT encoder for tactile temporal encoding, modality adapters to project tokens to LLM space, and Vision-Guided Token Ordering that places visual tokens after tactile tokens to leverage LLM recency bias. Training uses Isaac Gym with TacFlex for data collection, applying extensive domain randomization for Sim2Real transfer. The action space is continuous 3D (Δx, Δy, Δrz) represented as token sequences.

## Key Results
- VTLA achieves 47.3% GCR on ID geometric shapes versus 15.3% for TLA baseline
- Real-world insertion experiments show 95%+ success rates across various peg-hole clearances
- Under poor lighting conditions, VTLA maintains performance while vision-only methods fail
- DPO provides 16% relative improvement in OOD generalization (31.4% vs 27.0% GCR)

## Why This Works (Mechanism)

### Mechanism 1: Vision-Guided Token Ordering for Recency Bias Mitigation
- Claim: Positioning visual tokens closer to action predictions than tactile tokens improves temporal reasoning and action generation accuracy.
- Mechanism: VTLA exploits the recency bias inherent in LLMs by placing wrist camera images after tactile image sequences in the token stream. This ordering ensures visual information—which prior work shows is critical in early manipulation phases—remains salient during action prediction, while tactile sequences benefit from explicit temporal encoding via ViT before tokenization.
- Core assumption: LLMs weight later tokens more heavily in context; visual priors are more informative for initializing contact-rich manipulation than tactile feedback alone.
- Evidence anchors:
  - [Section 3.2]: "VTLA positions visual inputs after tactile inputs, bringing vision closer to action prediction. This strategy emphasizes the importance of visual information during initial manipulation phases."
  - [Table 1]: VTLA achieves 47.3% GCR (ID) vs. TLA's 15.3%, demonstrating vision's contribution.
  - [corpus]: Weak—no direct corpus papers validate token ordering for recency bias in robotic control.
- Break condition: If visual observations are degraded (e.g., poor lighting) and tactile signals alone drive success, token ordering may be less critical; see Appendix C where VTLA still succeeds but relies more on tactile.

### Mechanism 2: Direct Preference Optimization as Regression-Like Supervision
- Claim: DPO bridges the mismatch between discrete next-token prediction loss and continuous robotic control by providing gradient signals that penalize actions distant from ground truth.
- Mechanism: After SFT, VTLA generates multiple action candidates per input. Actions are ranked by L1 distance to ground truth; closer actions become "chosen," farther actions become "rejected." DPO loss then optimizes the policy to increase likelihood of chosen actions relative to rejected ones, effectively injecting regression-style supervision into the language model's discrete output space.
- Core assumption: The continuous action space can be meaningfully discretized into token sequences where preference ranking correlates with task success.
- Evidence anchors:
  - [Section 3.3]: "We reformulate the VTLA prediction task as a multi-label problem, enabling richer supervision through multi-label optimization."
  - [Table 4]: VTLA-DPO-1k achieves 31.4% GCR (OOD) vs. 27.0% without DPO—a 16% relative improvement.
  - [corpus]: No direct corpus evidence on DPO for robotic action prediction; this appears novel to VTLA.
- Break condition: If preference data diversity is insufficient (only 1k-2.4k samples tested), DPO may overfit to specific action patterns; Table 4 shows DPO-2k provides no additional gains over DPO-1k.

### Mechanism 3: Tactile Temporal Encoding for Contact-State Reasoning
- Claim: Encoding tactile image sequences through ViT before LLM tokenization provides temporally-aware representations that improve contact-rich manipulation where VLMs struggle with fine-grained temporal dependencies.
- Mechanism: Tactile observations (4-frame sequences per fingertip) are arranged in 2×2 grids and processed through a frozen ViT encoder. This pre-fuses temporal information into spatial token representations before LLM input, bypassing VLMs' limitations in modeling short-duration, low-level temporal dynamics inherent in tactile sensing.
- Core assumption: ViT's spatial attention can capture temporal patterns when frames are arranged spatially; the pre-trained vision encoder generalizes to tactile modality without modality-specific pre-training.
- Evidence anchors:
  - [Section 3.2]: "We encode tactile observations into image-like representations and extract temporally-aware tactile tokens using a Vision Transformer (ViT)."
  - [Appendix C, Figure 6-7]: VTLA completes insertion under poor lighting in 3 steps; VLA fails after 15 steps, demonstrating tactile's role when vision degrades.
  - [corpus]: TranTac (arXiv:2509.16550) similarly emphasizes transient tactile signals for contact-rich tasks, supporting tactile's importance but not the specific ViT encoding approach.
- Break condition: If tactile sensor characteristics differ significantly from training (e.g., different sensor type or degraded silicone layer), ViT representations may fail to transfer; domain randomization in Table 8 attempts to address this but remains unvalidated across sensor types.

## Foundational Learning

- Concept: **Next Token Prediction (NTP) for Continuous Actions**
  - Why needed here: VTLA formulates robotic control as NTP, requiring understanding how discrete tokens represent continuous action vectors [Δx, Δy, Δrz].
  - Quick check question: Can you explain how the action `[-0.9, 0.4, 0.013]` is tokenized and predicted by an LLM?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: DPO is core to VTLA's Stage 2 training; understanding the loss function in Eq. (2) is essential for debugging preference learning.
  - Quick check question: Given two action outputs with L1 distances 0.2 and 0.8 from ground truth, which becomes "chosen" and how does DPO loss update the model?

- Concept: **Domain Randomization for Sim2Real**
  - Why needed here: VTLA trains purely in simulation (28k samples) but achieves 95%+ real-world success; understanding randomization strategies (Table 8) is critical for reproducing transfer.
  - Quick check question: Which three parameter categories are randomized in VTLA's simulation, and why is color jittering applied to tactile images?

## Architecture Onboarding

- Component map:
  Input Layer (Wrist camera + Tactile sensors) -> Vision Encoder (Pre-trained ViT, frozen) -> Modality Adapter (Projects to LLM space, frozen) -> LLM Backbone (Qwen2-VL 7B, trainable) -> Output (Action tokens decoded as [Δx, Δy, Δrz])

- Critical path:
  1. Data collection in Isaac Gym with TacFlex (28k samples, 5 peg shapes, 0.6-2.0mm clearances)
  2. Format as instruction dialogues with special tokens (`<|vision_start|>`, `