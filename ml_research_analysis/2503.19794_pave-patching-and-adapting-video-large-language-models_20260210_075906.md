---
ver: rpa2
title: 'PAVE: Patching and Adapting Video Large Language Models'
arxiv_id: '2503.19794'
source_url: https://arxiv.org/abs/2503.19794
tags:
- video
- tokens
- llav
- visual
- side-channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAVE, a flexible framework for adapting pre-trained
  Video Large Language Models (Video LLMs) to tasks with side-channel signals such
  as audio, 3D cues, multi-view videos, or high frame rate inputs. PAVE adds lightweight
  adapters ("patches") with a small number of parameters and operations, avoiding
  modifications to the base model's architecture or pre-trained weights.
---

# PAVE: Patching and Adapting Video Large Language Models

## Quick Facts
- arXiv ID: 2503.19794
- Source URL: https://arxiv.org/abs/2503.19794
- Reference count: 40
- One-line primary result: Flexible framework adding lightweight adapters to Video LLMs for side-channel signals, achieving 2-6% relative improvements across four tasks while adding only ~0.1% parameters and FLOPs.

## Executive Summary
PAVE introduces a flexible framework for adapting pre-trained Video Large Language Models to tasks involving side-channel signals such as audio, 3D cues, multi-view videos, or high frame rate inputs. The framework adds lightweight adapters ("patches") with minimal parameters that use cross-attention to align and fuse video tokens with side-channel signal tokens, then update video tokens through residual connections. Experiments across four task categories show PAVE consistently outperforms task-specific models while maintaining efficiency, with particular success on 3D QA (6% improvement) and audio-visual QA (2% improvement).

## Method Summary
PAVE operates by extracting video tokens from a frozen visual encoder and side-channel tokens from respective modality encoders, then applying temporal-aligned cross-attention where video tokens serve as queries and side-channel tokens as keys/values within local temporal neighborhoods. The fusion results update video tokens via residual addition while preserving the original token count for the LLM. The framework combines these patches with LoRA modules for parameter-efficient adaptation, training with AdamW optimizer, linear warmup, and cosine annealing. The design adds only ~0.1% additional parameters and FLOPs compared to the base model while maintaining competitive performance across diverse video understanding tasks.

## Key Results
- 2% relative boost on Audio-Visual Question Answering (AVQA)
- 6% improvement on 3D Question Answering (SQA3D)
- 1-5% performance gains when integrating high frame rate videos
- 1% improvement on multi-view recognition (Ego-Exo4D)

## Why This Works (Mechanism)

### Mechanism 1: Temporal-Aligned Cross-Attention Fusion
Side-channel signals are aligned and fused with video tokens through localized cross-attention that respects temporal structure. Video tokens from key frames serve as queries while side-channel tokens form keys and values, with cross-attention restricted to temporal neighborhoods. This prevents misalignment between modalities with different sampling rates.

### Mechanism 2: Residual Token Update Preserving Input Structure
Fused information updates video tokens via summation without changing the token count passed to the LLM. The fusion output has identical shape to original video tokens, allowing side-channel information injection while maintaining fixed input dimensions.

### Mechanism 3: Minimal Parameter Overhead Through Constrained Patch Design
Patches achieve parameter efficiency through small cross-attention modules combined with LoRA, avoiding modification of pre-trained weights. The base Video LLM's reasoning capability is leveraged while only small adaptations are needed for side-channel integration.

## Foundational Learning

- **Cross-Attention in Transformers**
  - Why needed here: PAVE's core fusion mechanism relies on understanding how queries attend to different key-value pairs
  - Quick check question: Can you explain why video tokens should be queries and side-channel tokens keys/values, rather than the reverse?

- **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: PAVE combines patches with LoRA; understanding low-rank adaptation explains why only ~0.1% parameters suffice
  - Quick check question: Why does LoRA add Δθ as low-rank decomposition rather than fine-tuning full weight matrices?

- **Video LLM Architecture (Visual Encoder → Projection → LLM)**
  - Why needed here: PAVE operates between visual encoding and LLM processing; knowing where tokens come from is essential
  - Quick check question: At what point in the Video LLM pipeline does PAVE intervene, and why can't it modify the visual encoder?

## Architecture Onboarding

- **Component map:** Base Video LLM (frozen) → Visual encoder → Video tokens → PAVE Patch → LLM with LoRA modules
- **Critical path:** 1) Extract video tokens from SigLIP encoder (196 tokens/frame × 32 frames) 2) Extract side-channel tokens (audio ~120 tokens/2min, 3D ~18K tokens) 3) Apply cross-attention with temporal neighborhoods 4) Residual update: ˆzv = zv + zv|s 5) Pass ˆzv to LLM with LoRA adaptations
- **Design tradeoffs:** Local vs. global attention (local reduces compute but may miss long-range dependencies), zero initialization (safe but may slow learning), fixed token count (preserves efficiency but limits information capacity)
- **Failure signatures:** Audio-visual conflicts (Music-AVQA shows model prioritizes visual cues), domain mismatch (high frame rate patches underperform on egocentric benchmarks), temporal misalignment (performance drops without synchronization)
- **First 3 experiments:**
  1. Ablation on query design: Compare visual tokens as queries vs. learnable tokens on ScanQA
  2. Token count analysis: Profile FLOPs with varying side-channel token lengths
  3. Cross-model transfer: Train patch on LLaVA-OneVision-7B, test on Video-LLaVA-7B

## Open Questions the Paper Calls Out

- **Open Question 1:** How can PAVE be extended to support simultaneous multi-task joint training and flexible composition of patches? The current implementation trains patches independently or uses stage-wise learning, but doesn't demonstrate mechanisms for dynamically combining patches trained separately.

- **Open Question 2:** How can the fusion mechanism be improved to prevent the model from prioritizing visual priors when side-channel signals contradict visual content? On Music-AVQA, PAVE lags on conflicting audio-visual splits where the Video LLM prioritizes visual cues.

- **Open Question 3:** Why does adding high frame rate side-channel signals yield only marginal improvements for long-form videos compared to short videos? PAVE provides major boosts on short/medium videos but only minor gains (0.1-0.3%) on videos 30 minutes to 1 hour long.

## Limitations

- The temporal neighborhood size N(k) for local cross-attention is not specified, making it unclear how the model handles different sampling rates between video and side-channel signals
- Performance trade-offs aren't characterized - the paper doesn't explain when side-channel signals are actually informative versus redundant
- Limited testing of cross-domain transfer between video styles (egocentric vs. third-person)

## Confidence

**High Confidence:**
- PAVE's parameter efficiency claims (~0.1% additional parameters/FLOPs)
- Core architecture design: residual token updates preserving input structure
- Performance gains across all four task categories (2-6% relative improvements)

**Medium Confidence:**
- Claims about generalization across different Video LLM architectures
- The necessity of temporal-aligned cross-attention for alignment quality
- Multi-task learning benefits with shared patches

**Low Confidence:**
- Robustness to temporal misalignment between video and side-channel signals
- Performance on truly out-of-distribution video styles (egocentric vs. third-person)
- Computational efficiency at scale with very large side-channel token counts

## Next Checks

1. **Temporal Neighborhood Sensitivity:** Systematically vary N(k) (e.g., 5, 10, 20 tokens) on AVQA benchmark to determine optimal temporal alignment range and identify break points where performance degrades.

2. **Cross-Domain Transfer Experiment:** Train PAVE patches on third-person video datasets (AVSD/ScanQA) and evaluate on egocentric benchmarks (EgoSchema, Ego-Exo4D) to test true generalization beyond in-domain styles.

3. **Side-Channel Redundancy Analysis:** Create controlled experiments on AVQA where audio is either perfectly aligned with visual information or contains conflicting information. Measure whether PAVE learns to weight side-channel inputs appropriately or defaults to visual dominance.