---
ver: rpa2
title: 'Magentic-UI: Towards Human-in-the-loop Agentic Systems'
arxiv_id: '2507.22358'
source_url: https://arxiv.org/abs/2507.22358
tags:
- magentic-ui
- agent
- user
- plan
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Magentic-UI is an open-source interface for human-in-the-loop AI
  agents that enables collaboration through co-planning, co-tasking, and verification
  mechanisms. The system uses a multi-agent architecture with a lead orchestrator
  that manages specialized agents for web browsing, coding, and file manipulation.
---

# Magentic-UI: Towards Human-in-the-loop Agentic Systems

## Quick Facts
- **arXiv ID**: 2507.22358
- **Source URL**: https://arxiv.org/abs/2507.22358
- **Reference count**: 40
- **Primary result**: 72.2% task completion on WebVoyager with human-in-the-loop assistance

## Executive Summary
Magentic-UI presents an open-source interface for human-in-the-loop AI agents that enables collaborative task completion through co-planning, co-tasking, and verification mechanisms. The system employs a multi-agent architecture with specialized agents for web browsing, coding, and file manipulation, orchestrated by a lead agent. Evaluations demonstrate significant performance improvements when human oversight is incorporated, with task completion rates increasing by up to 71% compared to fully autonomous approaches.

## Method Summary
The system uses a multi-agent architecture where a lead orchestrator manages specialized agents for different domains including web browsing, coding, and file manipulation. The interface supports action guards to prevent harmful actions, memory mechanisms to reuse learned plans, and multi-tasking capabilities to handle concurrent sessions. Human-in-the-loop functionality is achieved through verification mechanisms and co-planning interfaces that allow users to guide agent actions. The system was evaluated on benchmark tasks including WebVoyager (72.2% completion) and GAIA (42.5% completion), with additional qualitative studies involving 12 users who found the interface easy to use with a 74.6 SUS score.

## Key Results
- Achieved 72.2% task completion on WebVoyager benchmark
- Reached 42.5% task completion on GAIA benchmark
- Human-in-the-loop approaches improved performance by up to 71%
- 74.6 SUS score indicating ease of use among 12 study participants

## Why This Works (Mechanism)
The system leverages a multi-agent architecture where specialized agents handle specific domains while a lead orchestrator coordinates overall task execution. Human-in-the-loop capabilities are enabled through verification mechanisms that allow users to approve or modify agent actions before execution. The system maintains memory of learned plans to improve efficiency on repeated tasks and employs action guards and sandboxing to prevent harmful actions. The interface design facilitates seamless collaboration between humans and agents through co-planning and co-tasking interfaces.

## Foundational Learning

**Multi-agent orchestration**: Why needed - to handle complex tasks requiring diverse capabilities across domains. Quick check - verify that lead orchestrator properly delegates tasks to specialized agents.

**Action guards and sandboxing**: Why needed - to prevent harmful or unauthorized actions during autonomous execution. Quick check - test safety mechanisms against adversarial inputs.

**Memory and plan reuse**: Why needed - to improve efficiency by learning from previous task executions. Quick check - measure performance improvement on repeated task types.

**Human-in-the-loop verification**: Why needed - to combine agent autonomy with human judgment for better outcomes. Quick check - compare performance with and without human verification steps.

## Architecture Onboarding

**Component map**: User Interface -> Lead Orchestrator -> (Web Agent, Code Agent, File Agent) -> Execution Engine

**Critical path**: User request → Lead orchestrator → Task planning → Specialized agent execution → Verification → User feedback loop

**Design tradeoffs**: Autonomy vs. human oversight balance, latency vs. thoroughness in verification, specialized agent complexity vs. generalization

**Failure signatures**: Prompt injection attacks, social engineering vulnerabilities, latency-induced task abandonment, excessive verbosity in communication

**3 first experiments**:
1. Baseline task completion without human-in-the-loop on WebVoyager
2. Safety mechanism effectiveness against adversarial prompt injection
3. User experience testing with SUS score measurement on simple information gathering tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Small user study sample size (12 participants) limits generalizability of SUS score
- Simulated user testing methodology may not reflect real-world usage patterns
- Scalability of multi-agent architecture to more complex tasks remains unverified
- No objective measurements of latency and verbosity issues reported

## Confidence
High: Multi-agent architecture design and implementation
Medium: Task completion rates on benchmarks (72.2% WebVoyager, 42.5% GAIA)
Low: User experience metrics from small sample (74.6 SUS from 12 users)

## Next Checks
1. Conduct longitudinal study to assess performance degradation over extended use periods
2. Implement and test additional adversarial scenarios including novel prompt injection techniques
3. Perform comparative study with different human-in-the-loop frequencies to optimize autonomy-human oversight balance