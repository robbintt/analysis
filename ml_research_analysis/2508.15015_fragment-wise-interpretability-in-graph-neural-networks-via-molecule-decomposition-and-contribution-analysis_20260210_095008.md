---
ver: rpa2
title: Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition
  and Contribution Analysis
arxiv_id: '2508.15015'
source_url: https://arxiv.org/abs/2508.15015
tags:
- seal
- explanation
- positive
- atoms
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the interpretability challenge in Graph Neural
  Networks (GNNs) for molecular property prediction, where the black-box nature of
  GNNs limits trust in applications like drug discovery. The proposed method, SEAL
  (Substructure Explanation via Attribution Learning), introduces an interpretable
  GNN that decomposes molecular graphs into chemically meaningful fragments and estimates
  their causal influence on model predictions.
---

# Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition and Contribution Analysis

## Quick Facts
- arXiv ID: 2508.15015
- Source URL: https://arxiv.org/abs/2508.15015
- Reference count: 9
- Primary result: SEAL achieves SE scores up to 0.98 on synthetic benchmarks while maintaining AUROC up to 1.00

## Executive Summary
This paper addresses the interpretability challenge in Graph Neural Networks (GNNs) for molecular property prediction by proposing SEAL (Substructure Explanation via Attribution Learning). The method decomposes molecular graphs into chemically meaningful fragments using BRICS rules and estimates their causal influence on model predictions through controlled message passing. By separating intrafragment and interfragment weights and applying regularization, SEAL achieves both strong predictive performance and interpretable fragment-level explanations that align with chemical intuition.

## Method Summary
SEAL introduces an interpretable GNN architecture that fragments molecular graphs into chemically meaningful substructures using an extended BRICS decomposition. The model maintains separate weights for intrafragment and interfragment message passing, with a regularization parameter λ controlling information leakage between fragments. This design enables causal attribution analysis where each fragment's contribution to the final prediction can be isolated and quantified. The method is evaluated on synthetic benchmarks (B-XAIC) for explanation quality and real-world datasets (hERG, CYP2C9, Solubility) for both predictive accuracy and fidelity of explanations.

## Key Results
- Achieved SE scores up to 0.98 on synthetic B-XAIC benchmarks, outperforming existing explainers
- Maintained competitive predictive performance with AUROC up to 1.00 on synthetic tasks
- Strong fidelity scores on real-world datasets, with Fidelity 10+ reaching 0.63
- Expert chemists preferred SEAL explanations over baseline methods in user study

## Why This Works (Mechanism)
The method works by explicitly controlling information flow between chemically meaningful fragments, allowing the model to attribute predictions to specific molecular substructures rather than treating the entire molecule as a monolithic graph. By separating message passing into intrafragment and interfragment components with adjustable regularization, the model can balance the need for local fragment isolation (for interpretability) with the requirement for some inter-fragment communication (for predictive accuracy).

## Foundational Learning
1. **Graph Neural Networks (GNNs)**: Neural networks operating on graph-structured data, aggregating information from neighboring nodes iteratively.
   - Why needed: Core architecture for processing molecular structures
   - Quick check: Can you explain message passing in GNNs?

2. **BRICS Fragmentation Rules**: Chemistry-based rules for decomposing molecules into synthetically accessible fragments.
   - Why needed: Provides chemically meaningful decomposition for interpretation
   - Quick check: Do you understand how chemical bonds are classified for fragmentation?

3. **Explainability Methods**: Techniques for attributing model predictions to input features or components.
   - Why needed: Enables interpretation of black-box models
   - Quick check: Can you name common explainability methods for GNNs?

## Architecture Onboarding

**Component Map**: Input Graph -> BRICS Decomposition -> Fragment Separation -> Intra/Inter-fragment Message Passing -> Attributed Contributions -> Prediction

**Critical Path**: The core innovation lies in the dual-weight message passing system where G^{(l)}_{intra} and G^{(l)}_{inter} matrices control information flow within and between fragments respectively, combined with the λ regularization term.

**Design Tradeoffs**: The method balances interpretability (high λ, strong fragment isolation) against predictive accuracy (lower λ, more information sharing). This creates a tunable system but requires task-specific optimization of λ.

**Failure Signatures**: Poor performance on properties requiring complex inter-fragment interactions, failure to isolate relevant substructures when important atoms are embedded in larger fragments, and reduced accuracy when λ is too high.

**First Experiments**:
1. Run baseline GNN without fragmentation on B-XAIC benchmark to establish performance ceiling
2. Test different λ values on CYP2C9 dataset to observe trade-off between fidelity and predictive accuracy
3. Compare fragment-level attributions against ground truth in B-XAIC to validate SE scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative fragmentation strategies overcome the limitations of the current BRICS-based decomposition to better isolate chemically meaningful substructures in complex motifs?
- Basis in paper: The authors note a "significant decline in performance appears in the boron (B) task due to its frequent appearance in complex substructures that our extended BRICS decomposition cannot efficiently segment," and explicitly suggest "Future work could explore different fragmentation strategies."
- Why unresolved: The current fixed decomposition rules (a variant of BRICS) fail to isolate specific relevant atoms when they are embedded within larger, synthetically defined fragments, forcing the model to attribute importance to the entire group rather than the specific active site.
- What evidence would resolve it: Demonstrating improved Subgraph Explanation (SE) scores on the Boron or PAINS tasks using a learned or adaptive decomposition method that successfully isolates smaller, chemically relevant subunits without sacrificing overall prediction accuracy.

### Open Question 2
- Question: How does the restriction of inter-fragment message passing affect the model's ability to learn properties that depend on complex, non-local interactions or 3D spatial conformations?
- Basis in paper: The paper hypothesizes that "many chemical prediction tasks inherently depend more on the presence and identity of specific fragments than on complex interactions between them," and results show that for tasks like PAINS detection, optimal explanation performance requires λ=0 (no regularization), implying the method struggles to interpret properties requiring global information flow.
- Why unresolved: The architectural design prioritizes local fragment isolation, which may fundamentally conflict with modeling phenomena like stereochemistry or allosteric effects where the spatial relationship between distant fragments defines the property.
- What evidence would resolve it: A comparative study on datasets explicitly labeled for non-local properties (e.g., protein-ligand binding affinities or stereo-specific reactions) showing that SEAL maintains competitive AUROC while providing meaningful fragment contributions without defaulting to zero regularization.

### Open Question 3
- Question: Can an adaptive or learnable regularization parameter (λ) be developed to replace the current reliance on task-specific cross-validation?
- Basis in paper: The methodology describes selecting λ via cross-validation using the Wilcoxon signed-rank test, noting that "optimal value of λ depends on the specific task" and varies widely from 0 to 2 across different datasets.
- Why unresolved: The current approach requires re-running validation for every new dataset to find the balance between interpretability (high λ) and necessary information flow (low λ), creating a computational bottleneck for applying the model to new domains.
- What evidence would resolve it: The implementation of a differentiable or reinforcement-learning-based mechanism that learns λ per layer or per graph during training, achieving comparable fidelity and explanation metrics without the need for an external grid search.

## Limitations
- Method effectiveness depends heavily on quality of molecular fragmentation, which may not generalize to highly complex or atypical molecular structures
- Current implementation focuses on pairwise inter-fragment interactions, potentially limiting ability to capture complex multi-fragment relationships in larger molecules
- Performance on properties requiring complex inter-fragment interactions or 3D spatial conformations may be limited

## Confidence
- **High Confidence**: Predictive performance metrics (AUROC up to 1.00) on synthetic benchmarks, basic fidelity scores on real-world datasets, and core methodological framework
- **Medium Confidence**: Expert chemist preferences in user study, SE scores (up to 0.98), and generalization to complex molecular structures
- **Low Confidence**: Real-world applicability in diverse drug discovery scenarios, scalability to very large molecular graphs, and performance on edge cases or unusual chemical structures

## Next Checks
1. **Cross-Domain Testing**: Evaluate SEAL on diverse molecular datasets including proteins, polymers, and materials science applications to assess generalizability beyond pharmaceutical compounds
2. **Scalability Analysis**: Test performance on larger molecular graphs (>100 atoms) and assess computational efficiency with increasing molecular complexity
3. **Ablation Studies**: Conduct systematic removal of regularization terms and decomposition strategies to quantify their individual contributions to model performance and interpretability