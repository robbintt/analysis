---
ver: rpa2
title: 'Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study
  of NeuroSPICE'
arxiv_id: '2512.23624'
source_url: https://arxiv.org/abs/2512.23624
tags:
- neurospice
- circuit
- time
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeuroSPICE is a physics-informed neural network (PINN) framework
  for device and circuit simulation that solves differential-algebraic equations by
  minimizing residuals through backpropagation. Unlike conventional SPICE simulators
  that rely on time-discretized numerical solvers, NeuroSPICE models device and circuit
  waveforms as continuous analytical functions of time, enabling exact temporal derivatives
  via automatic differentiation.
---

# Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE

## Quick Facts
- **arXiv ID:** 2512.23624
- **Source URL:** https://arxiv.org/abs/2512.23624
- **Reference count:** 25
- **Primary result:** PINN framework NeuroSPICE solves circuit DAEs via residual minimization, offering differentiable surrogate models and emerging device prototyping despite longer training times than conventional SPICE

## Executive Summary
NeuroSPICE introduces a physics-informed neural network framework that solves circuit differential-algebraic equations by minimizing residuals through backpropagation. Unlike conventional SPICE simulators that rely on time-discretized numerical solvers, NeuroSPICE models device and circuit waveforms as continuous analytical functions of time, enabling exact temporal derivatives via automatic differentiation. While training takes longer than commercial SPICE (ranging from 4 to 7 minutes depending on circuit complexity), NeuroSPICE offers unique advantages including flexible prototyping of emerging devices like ferroelectric memories, Multiphysics coupling, and differentiable circuit surrogate models for design optimization.

## Method Summary
NeuroSPICE implements a 4-layer fully connected neural network with Tanh activation that takes time as input and outputs node voltages as continuous analytical functions. The framework uses automatic differentiation to compute exact temporal derivatives (dV/dt, dQ/dt) rather than finite-difference approximations. Device physics is implemented directly as loss functions in Python, allowing rapid prototyping of emerging technologies. The network is trained to minimize the residual of circuit differential-algebraic equations across the time domain using the Adam optimizer, with training times ranging from 4-7 minutes depending on circuit complexity.

## Key Results
- Successfully simulates transistor amplifiers, ring oscillators, and FeRAM cells with waveforms matching conventional SPICE results
- Enables exact temporal derivatives via automatic differentiation, eliminating numerical errors from finite-difference approximations
- Provides differentiable circuit surrogate models with inference speeds of approximately 200 μs per simulation, enabling gradient-based design optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing time-discretized numerical integration with continuous analytical function representation allows the solver to find circuit waveforms without iterative time-stepping.
- **Mechanism:** A neural network takes time $t$ as input and outputs node voltages $V(t)$ as continuous functions. Instead of solving $V(t+\Delta t)$ based on $V(t)$ (as in standard SPICE), the network defines the entire waveform at once. The solution is found by minimizing the residual of the governing Differential-Algebraic Equations (DAEs) across the time domain.
- **Core assumption:** The circuit waveforms can be approximated sufficiently well by the compositional functions of the neural network (e.g., Tanh layers) within the given training epochs.
- **Evidence anchors:** Abstract states NeuroSPICE "solves circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation" and section II explains replacing "numerical time stepping with a neural network that directly represents the entire waveform as an analytical function of time."
- **Break condition:** If the circuit exhibits chaotic behavior or discontinuities that cannot be smoothed by the activation functions, the single continuous function approximation may fail to converge or may smooth over critical rapid transitions.

### Mechanism 2
- **Claim:** Exact temporal derivatives via automatic differentiation (autograd) eliminate the numerical errors associated with finite-difference approximations used in traditional solvers.
- **Mechanism:** Standard SPICE uses numerical integration schemes (like Backward Euler) to approximate $dV/dt$. NeuroSPICE utilizes the autograd feature of frameworks like PyTorch. Since $V(t)$ is an analytical function defined by the network weights, $dV/dt$ is computed exactly via the chain rule during backpropagation.
- **Core assumption:** The machine learning framework's automatic differentiation engine provides stable and accurate gradients for the specific nonlinear device equations implemented (e.g., MOSFET currents).
- **Evidence anchors:** Abstract states NeuroSPICE enables "exact temporal derivatives via automatic differentiation" and section II explains these "temporal derivatives such as dV/dt and dQ/dt... are computed exactly from the network's analytical representation rather than by finite-difference approximations."
- **Break condition:** If the loss landscape becomes too rugged due to highly nonlinear device physics, gradient-based optimization may struggle to find a global minimum, causing the "exact" derivative to be mathematically correct but optimizing towards a poor solution.

### Mechanism 3
- **Claim:** Implementing device physics directly as loss functions in Python (rather than compiled Verilog-A) facilitates rapid prototyping of emerging devices and Multiphysics coupling.
- **Mechanism:** Conventional SPICE requires rigid compact models (C code/Verilog-A). NeuroSPICE defines device behavior (e.g., Landau-Khalatnikov equation for ferroelectrics) directly in the loss function. This allows arbitrary algebraic constraints to be added without modifying the underlying solver engine.
- **Core assumption:** The user can formulate the device physics as a differentiable algebraic equation that fits within the standard autograd framework.
- **Evidence anchors:** Abstract mentions "flexible prototyping of emerging devices like ferroelectric memories, Multiphysics coupling" and section III shows implementation of the FeRAM model in DAE form.
- **Break condition:** If the device model is non-differentiable or contains discrete state transitions that do not have gradient information, the backpropagation mechanism fails to update the network weights effectively.

## Foundational Learning

- **Concept: Differential-Algebraic Equations (DAEs)**
  - **Why needed here:** Unlike purely differential systems (ODEs), circuits often have constraints (like Kirchhoff's Voltage Law creating algebraic loops) that require solving DAEs. Understanding the difference between differential variables (inductor currents, capacitor voltages) and algebraic variables (node voltages) is crucial for constructing the loss function.
  - **Quick check question:** Can you explain why a circuit simulator cannot solve capacitor voltage without simultaneously solving the algebraic constraints imposed by resistors and sources connected to it?

- **Concept: Automatic Differentiation (Autograd)**
  - **Why needed here:** This is the computational engine of NeuroSPICE. One must understand that this is not symbolic differentiation (like Mathematica) nor numerical differentiation (finite differences), but a programmatic application of the chain rule to compute exact gradients of the network output with respect to its inputs.
  - **Quick check question:** If a neural network represents $y = f(t)$, how does autograd help in calculating $dy/dt$ compared to estimating it using $(f(t+\epsilon) - f(t))/\epsilon$?

- **Concept: Residual Minimization (Physics-Informed Loss)**
  - **Why needed here:** Standard neural networks minimize error against labeled data (supervised learning). NeuroSPICE is "unsupervised" regarding data; it minimizes the "physics residual" (the error in satisfying the governing equation). The goal is to drive the output of the physical equation to zero.
  - **Quick check question:** In a simple RC circuit equation $C \frac{dV}{dt} + \frac{V}{R} = 0$, what is the "residual" that the neural network tries to minimize for a given guess of $V(t)$?

## Architecture Onboarding

- **Component map:** Input Layer (Time t) -> 4-layer FCN with Tanh activation (50 neurons each) -> Output Layer (Node Voltages) -> Physics Engine (Device models) -> Loss Aggregator (DAE residuals + IC loss)

- **Critical path:**
  1. Define time domain sampling (collocation points)
  2. Initialize network weights
  3. Forward pass: Predict $V(t)$
  4. Physics pass: Compute $I(t)$ and $dV/dt$ (using autograd) and evaluate KCL residuals
  5. Loss calculation: Sum of squared residuals + squared IC error
  6. Backward pass: Update weights via Adam optimizer

- **Design tradeoffs:**
  - **Speed vs. Flexibility:** NeuroSPICE training (4-7 mins) is significantly slower than SPICE transient analysis (milliseconds/seconds), but allows inverse design and exotic device modeling without Verilog-A compilation
  - **Hyperparameter Sensitivity:** The paper notes highly nonlinear devices (FeRAM) require 60,000 epochs and lower learning rates vs. 20,000 for oscillators. Convergence is not guaranteed "out of the box"
  - **Representation:** The output is a smooth analytical approximation; it may miss high-frequency noise or jitter that a discrete transient solver would capture

- **Failure signatures:**
  - **Physics Violation:** Loss plateaus above zero; KCL is not satisfied (network hallucinates waveforms that don't satisfy Ohm's law)
  - **Stiffness Failure:** Network fails to capture rapid transitions (e.g., switching edges in FeRAM), smoothing them out unphysically
  - **Initialization Drift:** Network finds a valid physical solution that doesn't match the intended initial conditions (IC loss weight too low)

- **First 3 experiments:**
  1. **RC Transient:** Simulate a simple RC circuit. Verify that the network learns the exponential decay curve and check if the learned time constant matches $R \times C$ exactly
  2. **Amplifier Bias:** Replicate the Transistor Amplifier case. Check if the DC operating point (before the pulse) stabilizes correctly, as PINNs often struggle with steady-state initialization
  3. **Inference Speed Test:** Train a model for a Ring Oscillator, save the weights, and run 10,000 inference passes. Compare the wall-clock time against a single HSPICE run to validate the "surrogate model" speed advantage claim (approximately 200 μs cited)

## Open Questions the Paper Calls Out

- **Can NeuroSPICE effectively scale to simulate large-scale circuits with thousands of nodes?**
  - Basis in paper: The conclusion explicitly states, "Further work should evaluate scalability"
  - Why unresolved: The paper only demonstrates results on simple circuits (amplifiers, ring oscillators) with small node counts
  - What evidence would resolve it: Successful simulation of complex industrial circuits with quantitative metrics on memory usage and training time relative to circuit size

- **How robust is the convergence of NeuroSPICE when applied to highly stiff or complex nonlinear systems?**
  - Basis in paper: The conclusion identifies "convergence" as a specific area requiring further evaluation
  - Why unresolved: The authors note that the highly nonlinear FeRAM model required significantly more epochs and a smaller learning rate, suggesting potential convergence difficulties
  - What evidence would resolve it: A systematic analysis of convergence rates and failure modes across a benchmark suite of stiff circuit equations

- **Can NeuroSPICE be utilized effectively as a differentiable surrogate model for gradient-based circuit design optimization?**
  - Basis in paper: The conclusion recommends evaluating "its use in gradient-based design optimization"
  - Why unresolved: While the paper highlights the potential for inverse design due to exact derivatives, it provides no experimental results demonstrating an actual optimization workflow
  - What evidence would resolve it: Case studies where NeuroSPICE is used to optimize circuit parameters (e.g., transistor widths) via backpropagation to meet specific performance targets

## Limitations

- **Convergence robustness:** Success heavily depends on hyperparameter tuning that varies significantly between circuit types, with highly nonlinear devices requiring substantially more epochs and lower learning rates
- **Physical fidelity verification:** Limited quantitative analysis of error metrics or discussion of potential physical artifacts introduced by smooth analytical approximations that may miss high-frequency effects and discontinuities
- **Scalability concerns:** 4-7 minute training time raises questions about scaling to larger, more complex designs, with no analysis of computational complexity scaling or memory requirements for larger netlists

## Confidence

**High confidence** (well-supported by evidence):
- The basic PINN framework can solve circuit DAEs and produce waveforms matching conventional SPICE results for the tested cases
- The differentiable nature enables inverse design capabilities that conventional SPICE lacks
- The continuous time representation provides exact temporal derivatives via autograd

**Medium confidence** (partially supported):
- The speed advantage as a surrogate model (200 μs inference vs SPICE) is demonstrated but not thoroughly benchmarked across diverse circuit types
- The flexibility for emerging device prototyping is theoretically sound but only practically demonstrated for FeRAM

**Low confidence** (weakly supported or speculative):
- Claims about Multiphysics coupling capabilities are mentioned but not demonstrated
- Long-term stability and convergence guarantees for arbitrary circuit topologies are not established

## Next Checks

1. **Convergence robustness test:** Systematically vary circuit complexity (add stages, change topologies) and document the required hyperparameter adjustments. Measure how often the framework fails to converge without manual intervention.

2. **Physical artifact detection:** Design test cases with sharp transitions (step inputs, switching circuits) and compare high-frequency content between NeuroSPICE and SPICE using FFT analysis. Identify frequency ranges where the smooth approximation introduces errors.

3. **Scalability benchmark:** Implement progressively larger circuits (10x, 100x netlist size) and measure training time, memory usage, and inference speed. Compare against commercial SPICE scaling behavior to identify practical limits.