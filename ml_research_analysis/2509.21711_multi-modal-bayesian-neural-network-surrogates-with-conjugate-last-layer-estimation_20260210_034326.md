---
ver: rpa2
title: Multi-modal Bayesian Neural Network Surrogates with Conjugate Last-Layer Estimation
arxiv_id: '2509.21711'
source_url: https://arxiv.org/abs/2509.21711
tags:
- data
- multi-modal
- modalities
- surrogate
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two Bayesian neural network-based surrogate
  models designed to handle multi-modal data for applications like optimization and
  sensitivity analysis. The core innovation is leveraging conditionally conjugate
  distributions in the last layer of the network, enabling efficient variational inference
  even with partially missing observations.
---

# Multi-modal Bayesian Neural Network Surrogates with Conjugate Last-Layer Estimation
## Quick Facts
- arXiv ID: 2509.21711
- Source URL: https://arxiv.org/abs/2509.21711
- Reference count: 40
- One-line primary result: Multi-modal Bayesian neural network surrogates improve accuracy and uncertainty quantification, especially using a layered approach that conditions predictions on auxiliary data.

## Executive Summary
This paper introduces two Bayesian neural network-based surrogate models designed to handle multi-modal data for applications like optimization and sensitivity analysis. The core innovation is leveraging conditionally conjugate distributions in the last layer of the network, enabling efficient variational inference even with partially missing observations. The authors propose a joint model that outputs all modalities together and a layered model that uses auxiliary modalities as inputs to predict the main quantity of interest. Both models are implemented using Pyro and tested on diverse datasets including scalar functions, time series, and wind data. Results show that the layered model consistently reduces prediction bias, especially for out-of-sample points, while maintaining well-calibrated uncertainty estimates. The canonical correlation between modalities was found to be a useful indicator of potential performance gains. Overall, the multi-modal surrogate models demonstrate improved accuracy and uncertainty quantification compared to standard uni-modal BNNs, with the layered approach showing particular promise for handling complex, heterogeneous data sources.

## Method Summary
The paper proposes two Bayesian neural network architectures for surrogate modeling of multi-modal data. The first is a joint model that simultaneously learns to predict all modalities from shared inputs. The second is a layered model that uses auxiliary modalities as additional inputs to improve predictions of the main quantity of interest. Both models exploit conditionally conjugate priors in the last layer, enabling efficient variational inference via Pyro. This approach allows the models to handle partially missing observations by conditioning on available modalities during training and prediction. The layered model, in particular, can leverage strong correlations between modalities to reduce prediction bias and improve uncertainty quantification, especially for out-of-sample points.

## Key Results
- The layered model consistently reduces prediction bias compared to standard uni-modal BNNs, especially for out-of-sample points.
- Canonical correlation between modalities is a useful indicator of potential performance gains from the layered approach.
- Both models maintain well-calibrated uncertainty estimates, with the layered model showing particular promise for handling complex, heterogeneous data sources.

## Why This Works (Mechanism)
The models work by leveraging the statistical dependencies between different data modalities. The joint model captures these dependencies by learning shared representations across all modalities, while the layered model explicitly conditions predictions of the main quantity of interest on auxiliary modalities. The use of conditionally conjugate priors in the last layer enables efficient variational inference, which is crucial for scaling to larger networks and datasets. By incorporating multiple sources of information, the models can improve prediction accuracy and uncertainty quantification, especially when there are strong correlations between modalities. The layered approach is particularly effective when there is a high canonical correlation between the auxiliary and main modalities, as it can leverage this relationship to reduce bias and improve out-of-sample predictions.

## Foundational Learning
- **Bayesian Neural Networks**: Neural networks with probabilistic weights to quantify uncertainty; needed for robust surrogate modeling under uncertainty.
  - Quick check: Verify that the model outputs are distributions rather than point estimates.
- **Variational Inference**: Approximate Bayesian inference method that optimizes a lower bound on the marginal likelihood; enables scalable training of BNNs.
  - Quick check: Confirm that the ELBO (Evidence Lower Bound) is being optimized during training.
- **Conditionally Conjugate Priors**: Priors where the posterior distribution is in the same family as the prior given the likelihood; enables closed-form updates and efficient inference.
  - Quick check: Ensure that the last layer's weights and biases have conjugate priors.
- **Multi-modal Data**: Data consisting of multiple related but distinct sources or types; common in scientific and engineering applications.
  - Quick check: Verify that the dataset contains multiple related outputs or modalities.
- **Canonical Correlation**: Measures the linear relationship between two sets of variables; useful for identifying useful auxiliary modalities.
  - Quick check: Compute the canonical correlation between modalities to assess potential benefits.

## Architecture Onboarding
- **Component Map**: Input Data -> Neural Network (Shared Layers) -> Last Layer (Conditionally Conjugate) -> Output Distribution; Auxiliary Modalities -> Last Layer (Conditional) -> Main Output Distribution
- **Critical Path**: Input -> Shared Layers -> Last Layer (Conditional) -> Output; the last layer's conjugate structure is critical for efficient inference.
- **Design Tradeoffs**: Joint model captures all dependencies but may be harder to train; layered model is more modular and can leverage strong correlations but requires identifying useful auxiliary modalities.
- **Failure Signatures**: Poor performance if modalities are weakly correlated; sensitivity to architectural choices (e.g., number of hidden layers); potential overfitting if the model is too complex relative to the data.
- **First Experiments**: 1) Compare joint vs. layered model on a synthetic dataset with known correlations; 2) Test sensitivity to the number of hidden layers and activation functions; 3) Evaluate performance on a real-world dataset with multiple modalities and missing observations.

## Open Questions the Paper Calls Out
None

## Limitations
- The reported improvements rely heavily on surrogate datasets and a single real-world case (wind data), limiting generalizability to other domains or larger-scale problems.
- The assumption of conditionally conjugate priors in the last layer restricts architectural flexibility, particularly for non-Gaussian or highly non-linear relationships between modalities.
- Performance gains from the layered approach are tied to the strength of canonical correlation between modalities, but the threshold for "useful" correlation is not quantified.
- The inference pipeline depends on Pyro-specific implementations, raising questions about portability to other probabilistic programming frameworks.

## Confidence
- **High**: Variational inference via conditionally conjugate last-layer estimation is mathematically sound and computationally efficient.
- **Medium**: Layered model consistently reduces bias and improves uncertainty calibration in tested scenarios.
- **Low**: Claims about domain transferability and performance gains tied to canonical correlation are not robustly validated.

## Next Checks
1. Test the layered model on a broader set of real-world datasets with varying correlation structures and missing data patterns.
2. Benchmark performance against non-conjugate alternatives (e.g., mean-field VI or MCMC) on small networks to assess scalability trade-offs.
3. Evaluate sensitivity to architectural choices (e.g., number of hidden layers, activation functions) and quantify impact on predictive accuracy.