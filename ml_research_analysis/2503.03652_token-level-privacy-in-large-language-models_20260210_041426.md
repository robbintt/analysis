---
ver: rpa2
title: Token-Level Privacy in Large Language Models
arxiv_id: '2503.03652'
source_url: https://arxiv.org/abs/2503.03652
tags:
- privacy
- stencil
- token
- mechanism
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preserving privacy when using
  large language models (LLMs) as remote services, where users must transmit sensitive
  data to external providers. Existing privacy-preserving NLP methods primarily focus
  on semantic similarity and neglect contextual information, which is crucial for
  maintaining utility.
---

# Token-Level Privacy in Large Language Models

## Quick Facts
- **arXiv ID:** 2503.03652
- **Source URL:** https://arxiv.org/abs/2503.03652
- **Reference count:** 20
- **Primary result:** dχ-STENCIL achieves 2ε-dχ-privacy with comparable/better utility-privacy tradeoffs vs baselines on 4 datasets

## Executive Summary
This paper addresses token-level privacy preservation when using LLMs as remote services, where users must transmit sensitive data to external providers. The authors introduce dχ-STENCIL, a novel mechanism that combines contextual and semantic information while ensuring strong privacy guarantees under the dχ differential privacy framework. By encoding embedding vectors of neighboring tokens to capture context, adding calibrated Laplacian noise, and selecting the closest token to the perturbed vector, dχ-STENCIL achieves 2ε-dχ-privacy while maintaining utility. Experimental results on diverse datasets show that incorporating contextual information enhances privacy preservation while maintaining utility, outperforming existing methods like CUSTEXT+ and NOISE.

## Method Summary
dχ-STENCIL operates by first retrieving embeddings of neighboring tokens for each target token, applying Gaussian weighting to capture contextual information, and computing a weighted sum of these embeddings. Laplacian noise is then added to the weighted sum, where the noise magnitude is calibrated based on the embedding vector's norm and a privacy parameter. The perturbed vector is mapped back to the nearest vocabulary token using cosine similarity. The method achieves 2ε-dχ-privacy by ensuring that the probability of reconstructing the original token remains bounded across neighboring datasets. Stopwords are excluded from privatization to preserve readability, and the approach is evaluated across multiple datasets (SST2, QNLI, SWAG, MMLU) using state-of-the-art LLMs (FLAN-T5, QWEN 2.5).

## Key Results
- dχ-STENCIL achieves 2ε-dχ-privacy with low reconstruction rates (Pr@5) across all tested datasets
- The method outperforms existing privacy-preserving techniques (CUSTEXT+, NOISE) in utility-privacy tradeoff
- Incorporating contextual information through neighboring token embeddings significantly improves privacy preservation while maintaining comparable accuracy
- Even window sizes show inherent accuracy ceilings due to perturbed tokens not matching originals at high privacy levels

## Why This Works (Mechanism)
The effectiveness of dχ-STENCIL stems from its dual approach of contextual weighting and calibrated noise addition. By incorporating neighboring token embeddings with Gaussian weighting, the method captures local semantic context that helps maintain utility while making reconstruction more difficult. The Laplacian noise sampling, calibrated to the embedding norm, provides strong differential privacy guarantees by ensuring that small changes in the input lead to bounded changes in the output distribution. The nearest-neighbor mapping to vocabulary tokens preserves semantic coherence while adding uncertainty. The exclusion of stopwords prevents over-privatization of function words that carry little semantic content, maintaining readability while focusing privacy efforts on content-bearing tokens.

## Foundational Learning

**dχ Differential Privacy**
- *Why needed:* Provides the theoretical foundation for privacy guarantees beyond traditional (ε,δ)-DP
- *Quick check:* Verify 2ε-dχ-privacy claim holds by testing reconstruction rate bounds

**Contextual Token Embeddings**
- *Why needed:* Captures semantic context that improves both utility preservation and privacy
- *Quick check:* Compare performance with/without contextual weighting

**Laplacian Noise Calibration**
- *Why needed:* Ensures proper noise magnitude for differential privacy guarantees
- *Quick check:* Validate noise sampling matches theoretical distribution

**Cosine Similarity Token Mapping**
- *Why needed:* Maps perturbed vectors back to meaningful tokens while preserving semantics
- *Quick check:* Measure semantic drift between original and mapped tokens

## Architecture Onboarding

**Component Map:** Input text -> GloVe embedding lookup -> Contextual weighting (Gaussian) -> Laplacian noise addition -> Nearest neighbor mapping -> Privatized output

**Critical Path:** Token → Neighbor embedding retrieval → Gaussian weighting → Weighted sum → Noise addition → Nearest neighbor search → Output token

**Design Tradeoffs:** Context window size (L) vs computational cost, privacy parameter (η) vs reconstruction rate, stopword exclusion vs information preservation

**Failure Signatures:** Even L shows accuracy ceiling, noise magnitude affects reconstruction rate, stopword handling impacts output coherence

**3 First Experiments:**
1. Baseline: Run FLAN-T5/QWEN on original test sets to establish accuracy
2. Noise-only: Apply dχ-STENCIL with L=1 (no context) to measure impact of contextual weighting
3. Full dχ-STENCIL: Test with L∈{4,5,8,9}, σ∈{0.5,0.75,1.0}, η∈{80,120,160,200,240}

## Open Questions the Paper Calls Out
None

## Limitations
- Privacy evaluation limited to reconstruction rate Pr@5 without testing against membership inference or other attacks
- dχ-privacy framework relationship to conventional DP definitions not fully clarified
- Stopword exclusion could create identifiable patterns in output despite formal privacy guarantees
- Evaluation limited to 4 datasets and 2 model architectures, restricting generalizability

## Confidence

**High confidence:** Core algorithm specification and implementation details are well-documented with clear mathematical formulation
**Medium confidence:** Empirical results show consistent improvements but limited dataset and model diversity
**Low confidence:** Privacy guarantees rely on dχ-framework which requires careful interpretation beyond reconstruction rate

## Next Checks

1. **Privacy metric expansion:** Implement additional privacy evaluation metrics including membership inference tests and semantic similarity-based reconstruction attempts to validate robustness of dχ-privacy guarantees beyond Pr@5

2. **Parameter sensitivity analysis:** Systematically vary L, σ, and η across broader ranges to identify optimal configurations and assess stability of performance improvements, particularly focusing on context window size interactions with privacy-utility tradeoffs

3. **Cross-domain generalization:** Evaluate dχ-STENCIL on additional diverse datasets including specialized domains (medical, financial) where privacy preservation is critical to assess real-world applicability and identify potential failure modes