---
ver: rpa2
title: 'Don''t Forget the Nonlinearity: Unlocking Activation Functions in Efficient
  Fine-Tuning'
arxiv_id: '2509.13240'
source_url: https://arxiv.org/abs/2509.13240
tags:
- nora
- activation
- functions
- rational
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NoRA, the first PEFT framework that directly
  adapts nonlinear activation functions in pretrained transformer-based models. It
  replaces fixed activations with learnable rational functions and applies structured
  low-rank updates to numerator and denominator coefficients, with a group-wise design
  that localizes adaptation and improves stability at minimal cost.
---

# Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning

## Quick Facts
- **arXiv ID:** 2509.13240
- **Source URL:** https://arxiv.org/abs/2509.13240
- **Reference count:** 31
- **Primary result:** NoRA matches or exceeds full fine-tuning with only 0.4% of parameters on vision tasks, and improves MMLU by +0.3%–0.8% on LLMs.

## Executive Summary
This paper introduces NoRA, a novel PEFT framework that adapts nonlinear activation functions rather than just attention/feedforward weights in pretrained transformers. By replacing fixed activations (e.g., GELU) with learnable rational functions and applying structured low-rank updates, NoRA achieves competitive or superior performance to full fine-tuning while updating only 0.4% of parameters on vision tasks. On LLMs, NoRA++ (combined with LoRA) improves generation quality and MMLU scores across multiple instruction-tuning datasets.

## Method Summary
NoRA adapts nonlinear activation functions in pretrained transformers by replacing fixed activations with learnable rational functions (polynomial numerator/denominator). Structured low-rank updates are applied to both numerator and denominator coefficients in a group-wise manner, localizing adaptation and improving stability. This allows adaptation with minimal trainable parameters while preserving the pretrained backbone. The framework is validated on Vision Transformers (CIFAR-10/100) and LLMs (LLaMA3-8B), showing accuracy gains and efficiency improvements over existing PEFT methods.

## Key Results
- On CIFAR-100, NoRA achieves +0.27% accuracy over full fine-tuning with only 0.02M trainable parameters (0.4% of total).
- NoRA++ (NoRA + LoRA) outperforms LoRA and DoRA under matched training budgets with fewer parameters.
- On LLaMA3-8B instruction tuning, NoRA++ yields MMLU gains of +0.3%–0.8%, including +1.6% on STEM (Alpaca) and +1.3% on OpenOrca.

## Why This Works (Mechanism)
NoRA unlocks previously frozen nonlinearities by parameterizing activation functions as rational polynomials and adapting them with low-rank updates. This preserves the backbone's learned representations while allowing fine-grained, localized control over activation behavior. The group-wise design localizes adaptation to specific feature groups, improving stability and efficiency. By initializing the rational function to match the original activation (e.g., GELU), NoRA starts as the pretrained model and only perturbs nonlinearities as needed.

## Foundational Learning
- **Rational function parameterization**: Representing activations as P(x)/Q(x) provides flexibility beyond fixed nonlinearities; needed for expressive adaptation without changing backbone weights; quick check: verify P(x)/Q(x) ≈ GELU at initialization.
- **Low-rank adaptation**: Applying structured updates to polynomial coefficients limits trainable parameters; needed for PEFT efficiency; quick check: count trainable parameters vs. full fine-tuning.
- **Group-wise adaptation**: Partitioning coefficients into groups localizes changes and stabilizes training; needed to avoid global instability; quick check: compare group count ablation results.
- **Denominator stabilization**: Using |b₀ + ...| in Q(x) prevents division by zero; needed for numerical stability; quick check: monitor denominator values during training.
- **Initialization matching**: Setting base coefficients to match original activation ensures start-from-baseline behavior; needed for controlled adaptation; quick check: plot P(x)/Q(x) vs. GELU at step 0.

## Architecture Onboarding
- **Component map**: Vision backbone (ViT-Tiny) → Group-wise Rational Activation module (replaces FFN activations) → Classification head
- **Critical path**: Forward pass: frozen backbone → rational activation (P(x)+ΔP(x))/(Q(x)+ΔQ(x)) → classifier output
- **Design tradeoffs**: Group count g vs. parameter efficiency and adaptation capacity; rank r vs. expressiveness and overfitting risk
- **Failure signatures**: Numerical instability (denominator → 0), accuracy collapse (improper initialization or rank too low)
- **First experiments**: 1) Verify GELU matching at initialization; 2) Train on CIFAR-10 with g=8, r=2; 3) Combine with LoRA and test on CIFAR-100

## Open Questions the Paper Calls Out
- Can adaptive strategies be developed to automatically select the optimal group count (g) and subspace rank (r) during training?
- Does NoRA transfer effectively to generative diffusion models and graph neural networks, or is it specific to Transformer architectures?
- Do spline-based or Fourier-inspired activation functions offer better expressiveness or stability than the rational functions used in NoRA?

## Limitations
- Implementation details for base coefficient initialization and denominator stabilization are underspecified, risking reproducibility issues.
- LLM experiment hyperparameters (learning rate, rank) are incomplete, particularly for Table 8 where learning rate is omitted.
- Results are limited to classification and instruction tuning; generalization to generative or graph-based models is unproven.

## Confidence
- **High Confidence**: Core methodology and vision results are clearly described and validated; reproduction is feasible given training details.
- **Medium Confidence**: Theoretical advantages and design choices are supported but not fully justified; some ablation studies would strengthen claims.
- **Low Confidence**: LLM instruction tuning lacks sufficient detail for full reproduction; reported gains are promising but not robustly validated.

## Next Checks
1. **Numerical Stability Test**: Implement denominator stabilization (Eq 6) and verify stable gradients during early training.
2. **GELU Matching Verification**: Reconstruct base polynomial coefficients to ensure exact GELU matching at initialization.
3. **LLM Hyperparameter Sweep**: Systematically vary learning rate and rank on LLaMA3-8B to validate MMLU improvements across all instruction datasets.