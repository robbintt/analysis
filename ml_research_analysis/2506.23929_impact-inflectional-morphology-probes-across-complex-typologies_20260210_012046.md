---
ver: rpa2
title: 'IMPACT: Inflectional Morphology Probes Across Complex Typologies'
arxiv_id: '2506.23929'
source_url: https://arxiv.org/abs/2506.23929
tags:
- llms
- plurality
- verb
- table
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces IMPACT, a synthetically generated evaluation
  framework for assessing Large Language Models'' (LLMs) understanding of inflectional
  morphology across five morphologically rich languages: Arabic, Russian, Finnish,
  Turkish, and Hebrew. The framework includes unit-test-style cases covering shared
  and language-specific phenomena, from basic verb inflections to unique features
  like Arabic''s reverse gender agreement and vowel harmony in Finnish and Turkish.'
---

# IMPACT: Inflectional Morphology Probes Across Complex Typologies

## Quick Facts
- arXiv ID: 2506.23929
- Source URL: https://arxiv.org/abs/2506.23929
- Reference count: 20
- Key outcome: Template-based synthetic evaluation reveals LLMs struggle with morphological agreement and grammaticality judgment, especially for ungrammatical examples and language-specific phenomena across five morphologically rich languages.

## Executive Summary
This paper introduces IMPACT, a synthetic evaluation framework for assessing Large Language Models' understanding of inflectional morphology across five morphologically rich languages: Arabic, Russian, Finnish, Turkish, and Hebrew. The framework includes unit-test-style cases covering shared and language-specific phenomena, from basic verb inflections to unique features like Arabic's reverse gender agreement and vowel harmony in Finnish and Turkish. The authors evaluate eight multilingual LLMs across two scenarios: Generation (predicting correct inflections) and Judgement (assessing grammaticality of utterances). Results show that while LLMs perform well on English and grammatical sentences, they struggle significantly with ungrammatical examples and complex morphological patterns, especially language-specific features. Chain-of-Thought and Thinking Modes do not consistently improve performance and sometimes degrade it. The study reveals persistent gaps in LLMs' handling of linguistic complexity, with performance varying widely across languages and tasks. The IMPACT framework is publicly released to support further research in multilingual morphological evaluation.

## Method Summary
IMPACT uses template-based synthetic evaluation to assess LLMs on inflectional morphology. Templates define placeholder structures (e.g., `[NAME] [VERB]`) instantiated with morphologically annotated wordlists from UniMorph. Language-specific agreement logic is applied to create evaluation units testing specific feature combinations. Two scenarios are evaluated: Generation (fill-in-the-blank inflection prediction) and Judgement (grammaticality assessment of well-formed and perturbed utterances). Models are evaluated zero-shot with CoT reasoning, and scores are aggregated using harmonic mean to penalize weak units.

## Key Results
- LLMs perform significantly worse on ungrammatical examples than grammatical ones across all languages and models
- Chain-of-Thought and Thinking Modes do not consistently improve performance and sometimes degrade it, especially on negative judgments
- Language-specific morphological features (Arabic reverse gender agreement, Finnish lexical case, Turkish vowel harmony) show near-zero performance on some models
- Performance varies widely across languages, with English showing near-ceiling performance while target languages show substantial gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-based synthetic evaluation isolates specific morphological phenomena for controlled probing.
- Mechanism: Templates define placeholder structures (e.g., `[NAME] [VERB]`) that are instantiated with morphologically annotated wordlists from UniMorph. Agreement logic is applied per language—for example, Arabic requires gender/plurality/dual agreement, while Finnish only requires plurality agreement. This creates "morphological unit tests" that target specific feature combinations (e.g., feminine dual noun + masculine plural verb).
- Core assumption: Template coverage captures linguistically meaningful inflectional patterns; morphological generalization is testable via controlled synthetic utterances rather than natural text.
- Evidence anchors:
  - [abstract] "IMPACT includes unit-test-style cases covering both shared and language-specific phenomena, from basic verb inflections... to unique features like Arabic's reverse gender agreement and vowel harmony in Finnish and Turkish."
  - [Section 3.1] "Each unique placeholder combination, called an 'Evaluation Unit', is the smallest evaluated element."
  - [corpus] Related work on morphological probing (M2C, wug tests) supports template-based behavioral testing, but corpus evidence for this specific 5-language template design is limited—no direct predecessor uses this exact approach.
- Break condition: If templates fail to capture productive morphological patterns (e.g., irregular forms, lexical exceptions not in UniMorph), the evaluation may underestimate model capabilities. The authors acknowledge this limitation: "template creation demands considerable linguistic expertise and relies on the UniMorph dataset, which is not fully human-verified."

### Mechanism 2
- Claim: LLMs exhibit asymmetric competence: better at recognizing grammatical forms than rejecting ungrammatical ones, with generation in between.
- Mechanism: Models may rely on surface fluency patterns rather than explicit grammatical rule representations. Grammatical examples align with high-probability patterns from training data, while ungrammatical examples require detecting violations—a capability not reinforced during standard language modeling training.
- Core assumption: Grammaticality judgment depends on learned probability distributions; detecting violations requires additional abstraction not guaranteed by standard pretraining objectives.
- Evidence anchors:
  - [Section 4.2] "LLMs perform well on English and grammatical sentences, they struggle significantly with ungrammatical examples... with performance varying widely across languages and tasks."
  - [Table 1] JN (judging negative/ungrammatical) scores consistently lag behind JY (judging positive/grammatical) across all models and languages (e.g., GFL2 on Arabic ara-com-1: JY=0.972, JN=0.915; EuroLLM on same: JY=0.925, JN=0.821).
  - [corpus] The "Generative AI Paradox" (West et al., 2024, cited in paper) suggests models may produce fluent outputs without genuine understanding.
- Break condition: If models were to develop explicit grammatical representations (e.g., through targeted training), JN performance should improve toward JY levels. The persistent gap suggests such representations are not currently emergent.

### Mechanism 3
- Claim: Chain-of-Thought reasoning can degrade morphological performance by introducing reasoning errors or misapplying rules.
- Mechanism: Longer reasoning chains provide more opportunities for error propagation. Models may correctly identify morphological rules in reasoning but fail at verification or application steps. For vowel harmony tasks, models sometimes assume input forms are correct during verification.
- Core assumption: CoT benefits derive from task decomposition; morphology may not benefit from—or may be harmed by—verbalized reasoning when the task requires pattern recognition rather than multi-step logic.
- Evidence anchors:
  - [abstract] "Chain-of-Thought and Thinking Modes do not consistently improve performance and sometimes degrade it."
  - [Section 4.3] "CoT boosted the JY score of Qwen2.5 on tur-2, but simultaneously reduced the JN score for the same template... longer reasoning chains introduce more potential points of failure—either by applying incorrect grammatical rules or generating incorrect inflections."
  - [Section 4.4] "For fin-2, GF2 generally reasons well about ungrammatical sentences but struggles during the final verification step... incorrectly produces the correct inflection by assuming it is part of the input."
  - [corpus] Sprague et al. (2025) find CoT helps mainly on math/symbolic reasoning, not all tasks—consistent with these findings.
- Break condition: If models are trained specifically to verify intermediate reasoning steps against morphological constraints, CoT-related degradation should decrease.

## Foundational Learning

- Concept: **Inflectional vs. Derivational Morphology**
  - Why needed here: IMPACT targets inflectional morphology (word forms expressing grammatical features like tense, case, number) but excludes derivational morphology (creating new words via affixation). Understanding this distinction is necessary to interpret what the benchmark tests versus what it doesn't.
  - Quick check question: Given "unhappiness" (derive noun from adjective) vs. "walked" (inflect verb for past tense), which is inflectional?

- Concept: **Morphological Typology: Fusional vs. Agglutinative**
  - Why needed here: The paper tests both fusional languages (Arabic, Hebrew, Russian—multiple grammatical functions in single affixes) and agglutinative languages (Finnish, Turkish—one function per affix). Different error patterns may emerge by type.
  - Quick check question: In Turkish "yürüyorum" (I am walking), can you identify distinct morphemes for [walk] + [progressive] + [1st person singular]? How does this compare to Russian "иду" (I go/walk)?

- Concept: **LLM-as-a-Judge Evaluation Paradigm**
  - Why needed here: The Judgement scenario uses LLMs to evaluate grammaticality, but prior work shows judges can be unreliable and over-positive. Understanding this paradigm's limitations contextualizes why IMPACT tests the judges themselves.
  - Quick check question: If an LLM judge rates a sentence as grammatical with high confidence, what factors might cause this rating to be unreliable?

## Architecture Onboarding

- Component map:
  - Template Layer -> Morphological Logic Layer -> Wordlist Layer -> Evaluation Unit -> Prompt Construction Layer -> Scoring Layer

- Critical path:
  1. Identify morphological phenomena → 2. Design template with placeholders → 3. Define agreement logic per language → 4. Generate utterances (correct) and perturbations (ungrammatical) → 5. Construct prompts → 6. Evaluate LLM responses → 7. Aggregate scores via harmonic mean

- Design tradeoffs:
  - **Synthetic vs. natural**: Templates ensure controlled coverage but may miss naturalistic complexity; authors acknowledge "utterances rather than fully composed semantic sentences"
  - **Harmonic vs. arithmetic mean**: Harmonic mean penalizes outliers more severely, exposing weaknesses; arithmetic mean used for language-level aggregation to avoid zero-value issues
  - **Sampling**: 50 prompts per evaluation unit; downsampling when combinations exceed 100 (e.g., Russian com-1 has 311 combinations)
  - **Zero-shot evaluation**: No few-shot examples used to assess "inherent capabilities," but this may underrepresent models' in-context learning potential

- Failure signatures:
  - **Consistent JN underperformance**: Models fail to reject ungrammatical forms across all languages—indicates fluency bias
  - **Language-specific template failures**: Arabic reverse gender agreement, Finnish lexical case (fin-1), Turkish vowel harmony (tur-1) show near-zero performance on some models—indicates training data gaps
  - **CoT-induced errors**: Models identify rules correctly in reasoning but fail verification (e.g., GF2 on fin-2)
  - **Reasoning-output mismatch**: Model reasoning chain mentions correct rule but final answer is wrong (Table 17 examples)

- First 3 experiments:
  1. **Baseline diagnostic**: Run a single model (e.g., GFL2) on all English templates in both Generation and Judgement modes to verify pipeline correctness—English should show near-ceiling performance (abstract confirms "LLMs perform well on English").
  2. **Language-complexity probe**: Compare performance on com-1 (name-verb agreement) across all five target languages to identify which language-specific agreement features cause the steepest performance drops relative to English.
  3. **CoT ablation**: For one language (e.g., Arabic), run both Direct and CoT prompting on templates where CoT showed mixed effects (ara-com-1, ara-1) and manually inspect reasoning chains to identify where correct rule identification fails to translate to correct outputs (using Table 17 failure patterns as a guide).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance degradation observed in inflectional morphology tasks extend to derivational morphology across morphologically rich languages?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that they "focus mainly on inflectional morphology, leaving other crucial phenomena—such as derivational morphology... unaddressed."
- Why unresolved: The IMPACT framework currently synthesizes tests only for inflectional features (e.g., tense, case) and does not generate evaluation units for derivational processes.
- What evidence would resolve it: An extension of the IMPACT framework to include derivational templates (e.g., nominalization) and subsequent evaluation of the eight LLMs.

### Open Question 2
- Question: Can novel prompting strategies or training interventions be developed to consistently improve morphological reasoning without the performance degradation observed with Chain-of-Thought (CoT)?
- Basis in paper: [explicit] The Conclusion notes that current reasoning strategies like CoT and Thinking Modes are "inconsistent" and "may introduce new errors," highlighting the need for "more robust reasoning strategies."
- Why unresolved: The study found that increasing reasoning steps via CoT often hurts performance on negative judgments (ungrammatical examples) and generation tasks.
- What evidence would resolve it: Identification of specific prompting heuristics that yield positive delta scores in the Judgement of Negative (JN) instances across all five languages.

### Open Question 3
- Question: Can targeted fine-tuning on negative instances bridge the significant performance gap between judging grammatical and ungrammatical morphological constructions?
- Basis in paper: [explicit] The authors conclude that "LLMs excel at recognizing grammatical sentences, struggle more with ungrammatical ones," and explicitly point to "clear room for improvement" in this area.
- Why unresolved: The study reveals a systemic asymmetry where models achieve high accuracy on positive judgments (JY) but consistently fail on negative judgments (JN), but does not propose a fix.
- What evidence would resolve it: Pre-training or fine-tuning experiments showing a reduction in the performance delta between JY and JN scores on language-specific templates.

## Limitations
- Template-based synthetic evaluation may not capture full complexity of natural language production where morphological features interact with syntax, semantics, and discourse context
- Reliance on UniMorph data introduces potential coverage gaps, as acknowledged regarding human verification limitations
- Exclusive focus on inflectional morphology omits derivational processes and compounding, limiting generalizability to broader morphological competence

## Confidence
- **High Confidence**: The core finding that LLMs show asymmetric performance (strong on grammatical examples, weak on ungrammatical ones) is consistently replicated across languages, models, and evaluation scenarios. The synthetic template methodology provides controlled conditions that minimize confounding variables.
- **Medium Confidence**: The claim that Chain-of-Thought reasoning sometimes degrades performance is well-supported but shows mixed patterns across languages and models. The mechanism—reasoning errors propagating through longer chains—is plausible but requires more granular analysis to confirm.
- **Medium Confidence**: The identification of specific language-pairing failures (Arabic reverse gender agreement, Finnish lexical case, Turkish vowel harmony) is based on systematic testing, but the severity rankings may shift with different template designs or evaluation conditions.

## Next Checks
1. **Natural Data Validation**: Replicate key findings using naturally occurring ungrammatical sentences from corpus data to determine whether template-based evaluation accurately reflects real-world morphological error detection capabilities.
2. **Cross-linguistic Generalization Test**: Extend evaluation to languages outside the current typology (e.g., isolating, polysynthetic) to assess whether identified patterns (asymmetric grammaticality judgment, CoT limitations) generalize across morphological diversity.
3. **Mechanism Decomposition Study**: Conduct targeted experiments to isolate whether JN underperformance stems from distributional bias (insufficient negative examples in training), architectural constraints (attention patterns), or evaluation artifacts (prompt formatting effects).