---
ver: rpa2
title: Understanding the Effects of Domain Finetuning on LLMs
arxiv_id: '2510.09359'
source_url: https://arxiv.org/abs/2510.09359
tags:
- medical
- vectors
- tuning
- qwen2
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically investigates how domain-specific fine-tuning
  reshapes large language models, particularly in the medical domain. It introduces
  tuning vectors, a framework capturing the directional parameter shifts induced by
  fine-tuning.
---

# Understanding the Effects of Domain Finetuning on LLMs

## Quick Facts
- arXiv ID: 2510.09359
- Source URL: https://arxiv.org/abs/2510.09359
- Reference count: 40
- Primary result: Domain fine-tuning modifies only a small subset of LLM representational subspace while preserving most pretrained activations.

## Executive Summary
This paper systematically investigates how domain-specific fine-tuning reshapes large language models, introducing tuning vectors as a framework to capture directional parameter shifts. The analysis reveals that fine-tuning modifies only a small subset of the representational subspace, with neural activations remaining largely consistent between pretrained and fine-tuned models. Tuning vectors are shown to be critical for improving instruction-following, benchmark performance, and generation quality, and can be combined across domains for more generalizable models.

## Method Summary
The study computes tuning vectors as the arithmetic difference between fine-tuned and pretrained weights (T = θ_ft - θ_pre). It analyzes neural activation patterns by extracting binary activation vectors from GLU gates, measuring normalized edit distances between pretrained and fine-tuned models. Subspace Alignment (SSA) is calculated via SVD decomposition to quantify how tuning vectors align with pretrained subspaces. The approach includes negation experiments (removing tuning vectors) and addition experiments (combining vectors from different domains).

## Key Results
- Fine-tuning modifies only a small subset of the representational subspace, preserving most pretrained activations
- Tuning vectors capture causal improvements in instruction-following, benchmark performance, and generation quality
- MLP layers primarily write new directional information while attention heads amplify existing directions
- Cross-domain tuning vector addition shows mixed but partially successful generalization

## Why This Works (Mechanism)

### Mechanism 1: Minimal Subspace Modification Hypothesis
Fine-tuning operates within a constrained parameter subspace rather than globally restructuring weights. The pretrained model's representations remain largely reusable, with modifications confined to task-specific directions.

### Mechanism 2: Tuning Vectors as Functional Operators
Tuning vectors capture the causal parameter shifts responsible for improved capabilities. When removed, they systematically degrade model performance across multiple axes simultaneously.

### Mechanism 3: Layer-Type Specific Encoding Strategy
MLP layers serve as the primary locus for injecting novel domain knowledge through orthogonal directions, while attention layers refine existing relational patterns.

## Foundational Learning

- **Singular Value Decomposition (SVD) for Subspace Analysis**: Used to decompose weight matrices and measure tuning vector alignment with pretrained subspaces. Quick check: Can you explain what the top-k singular vectors represent and how SSA uses them to quantify overlap?
- **Feed-Forward Networks as Key-Value Memories**: FFN layers write information into residual streams. Quick check: In FFN(x) = Act(W_up x) W_down, which matrices determine which features are activated versus what information is written?
- **Task Vectors and Model Arithmetic**: Tuning vectors extend task vector concepts to multi-objective domain adaptation. Quick check: How does tuning vector addition differ from negation, and what does each reveal about encoded information?

## Architecture Onboarding

- **Component map**:
Pretrained Model (θ_pre) -> Attention Layers (W_K/W_V: High SSA, W_Q/W_O: Lower SSA) -> MLP Layers (W_gate/W_up: Low SSA, W_down: High SSA) -> Tuning Vector (T = θ_ft - θ_pre) -> Combined Model (θ_pre + Σ T_domain)

- **Critical path**:
1. Extract tuning vector: T = θ_ft - θ_pre
2. Compute SSA per component using SVD of pretrained weights
3. Negate T from fine-tuned model to verify causal role
4. Add T from multiple domains to test generalization

- **Design tradeoffs**:
  - Vector addition vs. full fine-tuning: Addition is cheaper but may not capture all domain interactions
  - SSA threshold selection: Paper uses ε=0.05 approximation error to select k
  - Cross-domain orthogonality: Medical vectors are orthogonal to math/code (cosine similarity ~0)

- **Failure signatures**:
  - Minimal performance change when negating tuning vectors: Vector extraction failed or model was already well-aligned
  - Combined vectors degrade performance: Vectors may conflict in overlapping subspaces
  - Uniformly high SSA scores: Fine-tuning may not have effectively added new domain knowledge

- **First 3 experiments**:
  1. Reproduce negation results: Subtract tuning vector from fine-tuned model and verify performance drops
  2. SSA decomposition: Compute layer-wise SSA scores for a new domain model
  3. Cross-domain addition: Combine tuning vectors from two domains and evaluate transfer

## Open Questions the Paper Calls Out

### Open Question 1
Would accounting for permutation equivariance in feed-forward layers reveal greater alignment between pretrained and fine-tuned model representations than current neural activation analyses suggest?

### Open Question 2
Can tuning vectors be integrated into parameter-efficient fine-tuning methods (e.g., LoRA) to constrain optimization along domain-relevant directions, improving training efficiency?

### Open Question 3
Would more sophisticated vector composition strategies that reduce directional conflicts yield better cross-domain generalization than simple addition?

### Open Question 4
Why do tuning vectors from models fine-tuned from instruct checkpoints show different robustness properties to negation compared to those fine-tuned from base models?

## Limitations

- The study assumes linear mode connectivity in fine-tuning trajectories without empirical validation
- SSA analysis depends on the choice of k singular vectors via ε=0.05 threshold
- Cross-domain vector addition results show mixed outcomes, suggesting composition strategies may be domain-dependent
- The paper focuses exclusively on medical domain fine-tuning, limiting generalizability claims

## Confidence

- **High confidence**: Minimal subspace modification hypothesis (supported by weight similarity >99.9% and low edit distances)
- **High confidence**: Tuning vectors capturing causal improvements (verified through negation experiments)
- **Medium confidence**: Layer-type specific encoding strategy (SSA scores suggest MLP vs attention differences)
- **Medium confidence**: Cross-domain generalization via vector addition (limited positive results)

## Next Checks

1. Test linear mode connectivity assumption by measuring weight-space distance along fine-tuning trajectory
2. Vary the ε threshold in SSA analysis (0.01, 0.05, 0.10) to assess sensitivity of alignment interpretations
3. Apply tuning vector composition to non-medical domains (legal, financial) to validate cross-domain generalization patterns