---
ver: rpa2
title: JPEG Compliant Compression for Both Human and Machine, A Report
arxiv_id: '2503.10912'
source_url: https://arxiv.org/abs/2503.10912
tags:
- compression
- image
- images
- jpeg
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of lossy image compression degrading
  DNN accuracy by formulating a multi-objective optimization problem balancing compression
  rate, human visual quality, and DNN accuracy. The authors propose Human and Machine-Oriented
  Error (HMOE) as a new distortion measure and develop the Human and Machine Oriented
  Soft Decision Quantization (HMOSDQ) algorithm based on it.
---

# JPEG Compliant Compression for Both Human and Machine, A Report

## Quick Facts
- arXiv ID: 2503.10912
- Source URL: https://arxiv.org/abs/2503.10912
- Authors: Linfeng Ye
- Reference count: 40
- One-line primary result: HMOSDQ improves AlexNet accuracy by 0.81% at 0.61 BPP or reduces compression rate by 9.6× while maintaining accuracy

## Executive Summary
This paper addresses the challenge of lossy image compression degrading DNN accuracy by formulating a multi-objective optimization problem balancing compression rate, human visual quality, and DNN accuracy. The authors propose Human and Machine-Oriented Error (HMOE) as a new distortion measure and develop the Human and Machine Oriented Soft Decision Quantization (HMOSDQ) algorithm based on it. HMOSDQ is fully JPEG-compliant and jointly optimizes quantization tables, run-length coding, and Huffman coding. Experimental results on AlexNet and VGG-16 models using ImageNet subsets show that HMOSDQ outperforms default JPEG compression. Specifically, HMOSDQ improves AlexNet's validation accuracy by over 0.81% at 0.61 BPP or reduces the compression rate by 9.6× while maintaining the same accuracy. The method also achieves better rate-distortion performance, with up to 2.1 dB PSNR improvement at 2.69 BPP for VGG-16. An ablation study confirms the effectiveness of the proposed HMOE distortion measure.

## Method Summary
The paper formulates a multi-objective optimization problem for JPEG compression that jointly optimizes compression rate, human visual quality (PSNR), and DNN classification accuracy. The key innovation is the Human and Machine-Oriented Error (HMOE) distortion measure, which weights DCT coefficient distortions by their sensitivity to DNN accuracy loss. The method estimates sensitivity from gradients of the classification loss with respect to DCT coefficients, then uses Adaptive Sensitivity Mapping (ASM) to generalize sensitivity estimates across resolutions. HMOSDQ extends Soft Decision Quantization by replacing MSE with HMOE in the distortion term and jointly optimizes quantization tables, run-length coding, and Huffman coding through iterative optimization. The approach is fully JPEG-compliant and validated on AlexNet and VGG-16 models using ImageNet subsets.

## Key Results
- HMOSDQ improves AlexNet validation accuracy by over 0.81% at 0.61 BPP compared to default JPEG
- HMOSDQ achieves 9.6× compression rate reduction while maintaining the same accuracy as default JPEG
- HMOSDQ provides up to 2.1 dB PSNR improvement at 2.69 BPP for VGG-16 in rate-distortion performance

## Why This Works (Mechanism)

### Mechanism 1: Sensitivity-Based Loss Upper-Bounding
The paper claims that quantization-induced accuracy degradation can be mitigated by weighting DCT coefficient distortions according to their sensitivity—defined as the squared gradient of the surrogate loss with respect to each coefficient. Through Taylor expansion and Cauchy-Schwarz inequality, the authors derive that the squared change in surrogate loss (ΔL²) can be upper-bounded by a weighted sum of squared distortions on DCT coefficients, where weights are the sensitivity values Ŝᵢ = Σⱼ(∂L/∂x^f_{i,j})². By minimizing this weighted distortion (HMOE), the algorithm implicitly minimizes the upper bound on loss degradation. The derivation assumes gradients on pixels are i.i.d. from some distribution and that gradient random variables are zero-mean. The paper uses t-tests to validate zero-mean assumption, finding only 2 of 192 coefficient gradients reject the null hypothesis.

### Mechanism 2: Adaptive Sensitivity Mapping (ASM) for Resolution Generalization
Sensitivity estimated at one resolution can accurately approximate sensitivity at different resolutions via linear transformation. Since resizing operations (padding, interpolation, DCT) are linear, their composition has a Jacobian matrix J independent of input. The estimated sensitivity at target resolution Ŝ' relates to source resolution Ŝ via: Ŝ'ᵢ = Σₖ J²_{i,k} Ŝₖ. This enables offline estimation on 224×224 images, then mapping to arbitrary resolutions. The method requires assumptions (i) and (ii) to hold at both source and target resolutions, and that the linear transformation captures all relevant spatial transformations in the preprocessing pipeline.

### Mechanism 3: Iterative Joint Optimization via Trellis Quantization
Jointly optimizing quantization tables, run-length coding, and Huffman coding with HMOE (instead of MSE) achieves superior rate-accuracy-distortion tradeoffs while maintaining JPEG compliance. HMOSDQ extends Soft Decision Quantization by replacing MSE with HMOE in the distortion term. It alternates between: (1) finding optimal (r,s,a) sequences via trellis quantization with fixed Q and P_{r,s}, and (2) updating Q and P_{r,s} with fixed (r,s,a). For chroma channels, both Cb and Cr are optimized simultaneously since they share Q and Huffman tables. The iterative optimization converges to a local minimum that meaningfully improves over JPEG baseline, with HMOE accurately representing the joint human-machine distortion objective.

## Foundational Learning

- **Concept: DCT-Based Image Compression** - The entire method operates on 8×8 DCT coefficient blocks; understanding quantization's effect on coefficients is essential to grasp how sensitivity-weighting changes the compression behavior. *Quick check: Why does quantizing high-frequency DCT coefficients more aggressively typically have less visual impact than quantizing low-frequency coefficients?*

- **Concept: Taylor Expansion for Sensitivity Analysis** - The paper's core theoretical contribution uses first-order Taylor expansion to bound loss changes; readers must understand why this provides a local approximation and when higher-order terms matter. *Quick check: If the quantization distortion ∥Δx^f∥ is large, why might the Taylor-based upper bound become unreliable?*

- **Concept: Multi-Objective Optimization via Lagrangian Relaxation** - The paper formulates rate-distortion-accuracy tradeoffs as constrained optimization, solved via Lagrangian multiplier λ; understanding this framing is critical for interpreting results and tuning λ. *Quick check: What happens to the human-machine tradeoff as λ → 0 versus λ → ∞?*

## Architecture Onboarding

- **Component map:**
  Input Image (RGB) -> Color Space Conversion (RGB → YCbCr) -> 8×8 Block Partitioning & DCT -> [HMOSDQ Core Loop] -> Entropy Coding (Run-Length + Huffman) -> JPEG-Compliant Bitstream

- **Critical path:**
  1. **Offline phase:** Estimate Ŝ on representative images at base resolution (paper uses 10K ImageNet samples at 224×224)
  2. **Per-image encoding:** Compute Jacobian J for target resolution -> apply ASM -> run HMOSDQ iterations until convergence
  3. **λ selection:** Choose tradeoff point; paper shows λ=10¹¹ favors accuracy, λ=10¹³ favors distortion

- **Design tradeoffs:**
  - **λ value:** Higher λ → better accuracy preservation, potentially worse compression/PSNR. Paper shows 9.6× compression reduction at constant accuracy (λ=10¹¹) vs. 2.1 dB PSNR gain (λ=10¹³).
  - **Resolution handling:** Direct estimation per-resolution is accurate but expensive; ASM is cheap but relies on linear preprocessing assumptions.
  - **Model specificity:** Ŝ must be re-estimated per DNN architecture; paper shows different sensitivities for AlexNet vs. VGG-16.

- **Failure signatures:**
  - Accuracy gains disappear: Likely λ too small, or target DNN differs significantly from sensitivity-estimation model.
  - PSNR severely degraded: λ too large; HMOE over-prioritizes machine distortion.
  - Compression ratio unchanged: HMOSDQ not converging; check initial Q table (paper uses OPs method from [37]).
  - ASM errors on non-standard resolutions: Verify preprocessing pipeline matches assumptions; non-linear operations break Jacobian computation.

- **First 3 experiments:**
  1. **Reproduce ablation:** Run SDQ with λ=0 (pure MSE) vs. HMOSDQ with λ=10¹² on 100 ImageNet validation images with AlexNet; confirm SDQ underperforms JPEG on accuracy while HMOSDQ outperforms.
  2. **Validate ASM approximation:** Estimate Ŝ directly at 512 resolution (expensive) vs. ASM-mapped from 224; compute correlation coefficient across 64 DCT coefficients for Y channel.
  3. **Test model transfer:** Apply AlexNet-estimated sensitivity to VGG-16 compression; quantify accuracy degradation vs. model-specific sensitivity to bound cross-model applicability.

## Open Questions the Paper Calls Out

### Open Question 1
Can the HMOSDQ framework be effectively adapted for dense prediction tasks like object localization or semantic segmentation? The authors state in the conclusion, "we plan to investigate the performance of HMOSDQ on more computer vision tasks like object localization or semantic segmentation." This remains unresolved because the current validation is restricted to image classification (AlexNet, VGG-16), and it is unclear if the sensitivity mapping derived for classification loss translates effectively to the localization loss functions used in detection or segmentation. Experimental results showing the rate-accuracy-distortion tradeoff of HMOSDQ on standard object detection (e.g., COCO) or segmentation benchmarks compared to standard JPEG would resolve this.

### Open Question 2
Can the proposed Human and Machine-Oriented Error (HMOE) be extended to video (e.g., H.264) or audio (e.g., MP3) compression standards? The conclusion identifies "extend[ing] the HMOE for other lossy coding formats of different contents like MP3, and H.264" as a promising direction. This remains unresolved because the mathematical formulation of HMOE relies on the Discrete Cosine Transform (DCT) structure of JPEG and spatial-domain sensitivity; video codecs involve motion estimation and temporal redundancy which are not addressed by the current 2D spatial model. A derivation of sensitivity mapping for H.264 macroblocks or MP3 frames, followed by experiments demonstrating reduced machine accuracy loss at equivalent compression rates for video or audio classification tasks, would resolve this.

### Open Question 3
Does the first-order Taylor expansion approximation of the surrogate loss change remain valid under high-distortion (low BPP) quantization? Section IV-A upper bounds the loss change using a first-order Taylor expansion, which assumes small perturbations, yet the method is applied at low BPPs where quantization noise is significant. This remains unresolved because at high compression rates, the higher-order terms (o(∥Δx_f∥)) omitted in the derivation may become dominant, potentially causing the "sensitivity" metric to no longer correlate with the actual degradation in DNN accuracy. An analysis comparing the predicted upper bound of the loss change against the actual empirical loss change across a wide range of BPP values (e.g., < 0.5 BPP) to verify the linearity assumption would resolve this.

## Limitations

- The method relies on i.i.d. gradient assumptions and zero-mean gradient distributions, which may not hold for all DNN architectures
- Adaptive Sensitivity Mapping assumes linear preprocessing pipelines, breaking with non-linear operations like gamma correction
- No automated procedure exists for selecting the critical λ hyperparameter that controls the human-machine tradeoff

## Confidence

**High Confidence**: The core mechanism of weighting distortions by sensitivity (HMOE) to improve DNN accuracy under compression is well-supported by the derivation and ablation study. The experimental results showing accuracy improvements at specific BPP levels are reproducible and clearly presented.

**Medium Confidence**: The Adaptive Sensitivity Mapping approach for resolution generalization is theoretically sound but only validated for standard ImageNet preprocessing. The cross-resolution approximation quality depends on the linearity of the preprocessing pipeline, which isn't guaranteed for arbitrary use cases.

**Low Confidence**: The paper doesn't provide a principled method for selecting λ, instead testing three fixed values. The sensitivity estimation assumes the model architecture remains fixed, creating a practical limitation for compressing images for unknown or varying DNNs.

## Next Checks

1. **Architecture Generalization Test**: Apply sensitivity estimates from AlexNet to compress images for ResNet-50 and MobileNet-v2; measure accuracy degradation compared to architecture-specific sensitivity estimation to quantify cross-model applicability.

2. **Preprocessing Robustness Test**: Modify the resizing pipeline to include gamma correction or non-linear interpolation; evaluate ASM accuracy degradation and determine operational boundaries for the linear assumption.

3. **λ Sensitivity Analysis**: Implement a small validation loop that searches λ ∈ [10⁹, 10¹³] for a given DNN and dataset; measure accuracy-BPP curves to identify optimal λ values and characterize sensitivity to this hyperparameter.