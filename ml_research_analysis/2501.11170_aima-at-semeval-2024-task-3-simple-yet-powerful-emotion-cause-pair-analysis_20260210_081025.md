---
ver: rpa2
title: 'AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis'
arxiv_id: '2501.11170'
source_url: https://arxiv.org/abs/2501.11170
tags:
- emotion
- extraction
- cause
- task
- pair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents AIMA''s approach to SemEval-2024 Task 3, focusing
  on emotion-cause pair extraction in conversational contexts. The team developed
  a three-component model: embedding extraction using EmoBERTa, cause-pair extraction
  with emotion classification, and cause extraction via question-answering post-pair
  detection.'
---

# AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis

## Quick Facts
- arXiv ID: 2501.11170
- Source URL: https://arxiv.org/abs/2501.11170
- Reference count: 4
- Placed 10th out of 23 teams in Subtask 1

## Executive Summary
This paper presents AIMA's approach to SemEval-2024 Task 3, focusing on emotion-cause pair extraction in conversational contexts. The team developed a three-component model: embedding extraction using EmoBERTa, cause-pair extraction with emotion classification, and cause extraction via question-answering post-pair detection. Despite supporting multimodal inputs, the model focused on textual data for Subtask 1. The approach achieved 10th place out of 23 teams in Subtask 1 and 6th in Subtask 2. Key performance metrics included F1 scores across six emotion categories, with weighted averages accounting for class imbalances. The model demonstrated strong emotion classification capabilities while identifying opportunities for refining causal factor identification within conversations.

## Method Summary
AIMA's approach to emotion-cause pair extraction employed a three-component architecture. The first component utilized EmoBERTa for embedding extraction from conversational text. The second component performed cause-pair extraction while simultaneously classifying emotions. The third component implemented cause extraction through a question-answering framework after pair detection. While the model supported multimodal inputs, the team focused exclusively on textual data for Subtask 1 evaluation. This design choice allowed for deeper optimization of text-based emotion and cause detection mechanisms.

## Key Results
- Achieved 10th place out of 23 teams in Subtask 1
- Achieved 6th place in Subtask 2
- Demonstrated strong emotion classification capabilities with weighted F1 scores across six emotion categories
- Identified opportunities for refining causal factor identification within conversations

## Why This Works (Mechanism)
The three-component architecture effectively decomposes the complex emotion-cause pair extraction task into manageable subtasks. EmoBERTa provides strong contextual embeddings that capture emotional nuances in conversational text. The cause-pair extraction component leverages these embeddings to identify relationships between emotions and their causes while simultaneously classifying emotional states. The question-answering framework for cause extraction allows the model to reason about causal relationships in a more interpretable manner. This modular approach enables focused optimization of each component while maintaining overall system coherence.

## Foundational Learning
- **EmoBERTa**: A domain-specific variant of BERT pretrained on emotion-rich conversational data. Why needed: Standard BERT models may not capture subtle emotional expressions in conversations. Quick check: Compare performance with standard BERT and other emotion-specific models.
- **Question-Answering Framework**: Adapts causal factor identification to a QA format. Why needed: Transforms a complex relationship extraction task into a more tractable format. Quick check: Evaluate QA performance on causal questions versus direct causal extraction.
- **Weighted F1 Scoring**: Accounts for class imbalance in emotion categories. Why needed: Prevents model from biasing toward majority emotion classes. Quick check: Compare weighted vs unweighted F1 scores across all emotion categories.
- **Multimodal Support**: Framework designed to incorporate non-textual inputs. Why needed: Conversations often contain visual or auditory emotional cues. Quick check: Test multimodal performance on available datasets with visual or audio components.
- **Three-Component Decomposition**: Breaks complex task into embedding, pair extraction, and cause extraction. Why needed: Simplifies optimization and enables modular improvements. Quick check: Perform ablation studies on each component.
- **Conversational Context Modeling**: Handles multi-turn dialogue structure. Why needed: Emotions and causes often depend on conversation history. Quick check: Evaluate performance with and without conversation history context.

## Architecture Onboarding

**Component Map**: Embedding Extraction -> Cause-Pair Extraction with Emotion Classification -> Cause Extraction via QA

**Critical Path**: EmoBERTa embeddings → Emotion classification → Causal relationship identification → Final cause extraction

**Design Tradeoffs**: The team prioritized simplicity and interpretability by using a three-component architecture over more complex end-to-end models. This choice favored explainability and modular optimization at the potential cost of some performance. The decision to focus solely on textual data, despite supporting multimodal inputs, allowed for deeper optimization of text-based features but may have missed valuable contextual information from other modalities.

**Failure Signatures**: 
- Poor performance on minority emotion classes despite weighted averaging
- Difficulty identifying causal relationships in complex multi-turn conversations
- Potential over-reliance on textual cues when multimodal inputs are available
- Challenges in distinguishing between similar emotion categories

**First Experiments**:
1. Evaluate per-class F1 scores to identify systematic weaknesses in emotion classification
2. Implement and test a multimodal version using available visual or audio data
3. Compare performance against an end-to-end model to assess the three-component architecture's effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive focus on textual data for Subtask 1 despite multimodal support
- Relatively low F1 score (10th place out of 23 teams) suggesting room for improvement
- Weighted F1 averages obscure per-class performance, making it difficult to assess model capabilities across all emotion categories
- Reliance on a single pretrained model (EmoBERTa) may limit the ability to capture nuanced emotional expressions across diverse conversational contexts

## Confidence

**High Confidence**: The three-component model architecture and its implementation details are well-documented and verifiable from the description provided.

**Medium Confidence**: The ranking and F1 scores are stated clearly, but the lack of detailed per-class performance metrics introduces uncertainty about the model's true capabilities across all emotion categories.

**Low Confidence**: The impact of the model's decision to use only textual data despite multimodal support is unclear, as is the potential benefit of incorporating additional modalities.

## Next Checks
1. Conduct a detailed error analysis comparing false positives and false negatives across all six emotion categories to identify systematic weaknesses in the model's emotion classification and causal factor identification.
2. Implement and evaluate an augmented version of the model that incorporates multimodal inputs (particularly visual cues in conversations) to assess the impact on performance for Subtask 1.
3. Perform ablation studies on the three-component architecture to determine the relative contribution of each component to overall performance, particularly examining whether the question-answering approach for cause extraction is optimal or if alternative methods might yield better results.