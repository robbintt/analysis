---
ver: rpa2
title: 'Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized
  Smoothing with Differentially Private Guided Denoising Diffusion'
arxiv_id: '2507.08163'
source_url: https://arxiv.org/abs/2507.08163
tags:
- denoising
- adds
- diffusion
- noise
- certified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Adaptive Diffusion Denoised Smoothing (ADDS),
  a method for certifying adversarial robustness of vision models by adaptively guiding
  a denoising diffusion process. The core idea is to reinterpret guided diffusion
  as a sequence of adaptive Gaussian Differential Privacy (GDP) mechanisms, enabling
  end-to-end analysis using privacy filters.
---

# Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion

## Quick Facts
- arXiv ID: 2507.08163
- Source URL: https://arxiv.org/abs/2507.08163
- Authors: Frederick Shpilevskiy; Saiyue Lyu; Krishnamurthy Dj Dvijotham; Mathias Lécuyer; Pierre-André Noël
- Reference count: 15
- Key outcome: ADDS achieves competitive certified accuracy at r=0 (58.8-60.4%) on ImageNet using adaptive GDP-guided diffusion denoising

## Executive Summary
This paper proposes Adaptive Diffusion Denoised Smoothing (ADDS), a method for certifying adversarial robustness of vision models by adaptively guiding a denoising diffusion process. The core idea is to reinterpret guided diffusion as a sequence of adaptive Gaussian Differential Privacy (GDP) mechanisms, enabling end-to-end analysis using privacy filters. This approach extends adaptive randomized smoothing to long sequences of steps while maintaining provable certification guarantees.

Experiments on ImageNet with three noise levels (σ ∈ {1.0, 1.5, 2.0}) show that ADDS achieves competitive certified accuracy at r=0 (58.8-60.4%) compared to baselines like Carlini et al. (2023b) and DensePure (Xiao et al., 2023). The method trades some image fidelity for improved robustness, particularly at higher noise levels, and demonstrates that adaptive guidance can tighten certification bounds for diffusion-based defenses.

## Method Summary
ADDS adapts guided denoising diffusion for certified robustness by treating each denoising step as an adaptive Gaussian Differential Privacy mechanism. The method uses a privacy filter to track per-pixel privacy budgets, dynamically adjusting guidance to ensure end-to-end certification. The algorithm iteratively denoises an image, applying guidance toward the original input when privacy budget allows, and samples the next noisy state using the diffusion model's predicted covariance. Certification is computed via Monte Carlo sampling using the composed privacy guarantee.

## Key Results
- ADDS achieves 58.8-60.4% certified accuracy at r=0 on ImageNet across σ ∈ {1.0, 1.5, 2.0}
- Competitive with Carlini et al. (2023b) and DensePure baselines at low radii
- Guidance improves robustness at higher noise levels but trades image fidelity
- 5-vote majority voting improves certified accuracy but can reduce clean accuracy on ambiguous images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guided denoising steps can be composed as GDP mechanisms to yield end-to-end certification.
- Mechanism: Each guided step Mt : x → At(x) + z is modeled as a Gaussian mechanism with input-dependent sensitivity. The privacy budget μ²_t,i accumulates per-pixel as Σ_t (ΔA²_t,i / σ²_t,i), where the sensitivity ΔAt,i ≤ ri · s · √(ᾱₜ₋₁(1−αₜ)²)/(1−ᾱₜ)² depends on guidance scale s and diffusion coefficients. GDP composition then yields an rₓ/σ-GDP guarantee for the full sequence.
- Core assumption: The denoiser's predicted clean image x̂₀ᵗ and covariance Σθ(xt, t) remain sufficiently stable under small input perturbations.
- Evidence anchors:
  - [abstract] "reinterpret a guided denoising diffusion model as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms"
  - [Page 3, Proposition 3.1–3.2] Derives per-step μ² budget and composes via GDP filter
  - [corpus] Related work (Robustifying Diffusion-Denoised Smoothing) extends DDS frameworks, confirming the denoised smoothing paradigm is actively developed
- Break condition: If guidance scale s or diffusion coefficients cause ΔA²_t,i to exceed allocated budget before denoising completes, the filter halts guidance prematurely.

### Mechanism 2
- Claim: Privacy filters enable adaptive variance composition without pre-fixing noise schedules.
- Mechanism: Algorithm 1 (PrivacyFilter) tracks a per-pixel budget Λ, deducting s² · ᾱₜ₋₁(1−αₜ)²/((1−ᾱₜ)²σ²ₜ) at each guided step. If Λ′ ≤ 0, guidance is skipped for remaining steps. This ensures Σ_t μ²_t,i ≤ μ² holds dynamically.
- Core assumption: The diagonal covariance Σθ(xt, t) accurately reflects per-pixel noise variance; misestimation would violate GDP bounds.
- Evidence anchors:
  - [Page 3, Algorithm 1] Explicit filter logic with budget deduction formula
  - [Page 3, Proposition 3.2] "Algorithm 1 ensures that over any run of Algorithm 2 we have Σ_t 1/σ²_i · s² · ... ≤ μ²"
  - [corpus] No direct corpus papers implement GDP filters for diffusion; the approach appears novel to this work
- Break condition: If Σθ underestimates true noise variance, the privacy guarantee weakens; certification becomes unsound.

### Mechanism 3
- Claim: Adaptive guidance trades image fidelity for tighter certification bounds at higher noise levels.
- Mechanism: Guidance shifts x̂₀ᵗ toward x via x̂₀ᵗ ← x̂₀ᵗ + s(x − x̂₀ᵗ), creating a convex combination of model prediction and original image. This reduces accumulated noise variance but introduces semantic drift—e.g., "a blurry dog might become a detailed cat" (Page 4).
- Core assumption: The base classifier g tolerates semantically shifted reconstructions without systematic misclassification.
- Evidence anchors:
  - [Page 4, Table 2] ADDS without unguided denoising achieves highest clean accuracy (70.0%, 60.0%, 48.0%) across σ values
  - [Page 4, paragraph 3] "ADDS makes a trade-off between original image fidelity and noise"
  - [corpus] DensePure (Xiao et al., 2023) shows multi-hop denoising improves certified accuracy, supporting multi-step refinement
- Break condition: At low noise (σ = 1.0), one-shot denoising already performs well; guidance adds variance without benefit, degrading certified accuracy.

## Foundational Learning

- Concept: **Gaussian Differential Privacy (GDP)**
  - Why needed here: GDP provides the composition theorem allowing per-step privacy budgets to sum to an end-to-end guarantee; without this, adaptive guidance would void certification.
  - Quick check question: Given two mechanisms with μ₁ = 0.3 and μ₂ = 0.4, what is the composed GDP parameter? (Answer: μ = √(0.3² + 0.4²) ≈ 0.5)

- Concept: **Randomized Smoothing (RS) certification**
  - Why needed here: RS defines the base robustness framework; the certificate radius rₓ = σ/2 · (Φ⁻¹(p₊) − Φ⁻¹(p₋)) extends to ADDS via GDP composition.
  - Quick check question: If the top-class probability p₊ = 0.7 and runner-up p₋ = 0.2, does the certified radius increase or decrease with higher noise σ? (Answer: Increases linearly, but classification accuracy typically drops)

- Concept: **DDPM reverse process and covariance prediction**
  - Why needed here: ADDS relies on the learned Σθ(xt, t) for per-pixel noise variance; misunderstanding this would lead to incorrect budget calculations.
  - Quick check question: In Equation (2), what happens if Σθ is underestimated by 2×? (Answer: The GDP budget μ²_t,i is overestimated by 4×, causing premature filter termination)

## Architecture Onboarding

- Component map:
  Input: Image x, noise level σ, guidance scale s
  -> Initialization: xT ~ N(0, I), per-pixel budget Λ = μ·1_d
  -> Loop (t = T → 1): Denoiser predicts x̂₀ᵗ -> PrivacyFilter checks Λ -> Apply guidance if budget remains -> Sample xt−1 ~ N(·, Σθ)
  -> Post-processing: Optionally continue unguided denoising after budget exhaustion
  -> Output: x₀ -> Classifier g -> Smoothed prediction via majority vote

- Critical path:
  1. Privacy filter correctness (must not exceed budget)
  2. Covariance estimator accuracy (Σθ must reflect true noise)
  3. Guidance scale tuning (s controls fidelity/robustness trade-off)

- Design tradeoffs:
  - Higher s → stronger guidance → less noise variance but more semantic drift
  - Unguided post-processing → higher clean accuracy variance → worse certified accuracy at low σ
  - 5-vote majority → concentrates predictions, improves certification on "easy" images but can misclassify ambiguous ones

- Failure signatures:
  - Budget exhausted early (Λ ≤ 0 at t > 1) → guidance stops prematurely, output resembles standard DDPM
  - Certified accuracy drops at σ = 1.0 → guidance adds unnecessary variance; switch to one-shot denoising
  - Clean accuracy > certified accuracy gap widens → voting biasing predictions toward incorrect labels on hard images

- First 3 experiments:
  1. Replicate Table 1 (σ = 1.0) with your DDPM checkpoint; verify certified accuracy ~58–61% at r = 0 matches reported range
  2. Ablate guidance scale s ∈ {0.0, 0.5, 0.8, 1.0}; plot certified accuracy vs. s at σ = 1.5 to validate the trade-off claim
  3. Compare per-pixel budget exhaustion timestep distribution; if >50% exhaust before t = 500, reduce s or increase σ budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-off between noise reduction and semantic fidelity be optimized to prevent the reconstructed image from deviating significantly in content from the original (e.g., changing object identity)?
- Basis in paper: [explicit] Section 4 states that the method trades fidelity for noise, leading to cases where "a blurry dog might become a detailed cat."
- Why unresolved: The current mechanism mixes the denoiser's prediction with the original input, inherently balancing detail recovery against maintaining the original signal's semantics.
- What evidence would resolve it: A modified guidance strategy that maintains higher semantic consistency scores (e.g., CLIP score consistency) while achieving comparable certified accuracy.

### Open Question 2
- Question: How can the variance introduced by unguided denoising steps be reduced to prevent the degradation of clean accuracy?
- Basis in paper: [explicit] Section 4 observes that "unguided denoising... increases variance... [and] systematically degrades clean accuracy" compared to one-shot methods.
- Why unresolved: The paper identifies the variance increase as a key drawback of the multi-step diffusion process used in baselines like DensePure, but ADDS primarily addresses the certification rather than the variance of the unguided phase.
- What evidence would resolve it: A method that selectively halts the process or modifies the unguided sampling distribution to minimize variance while preserving the robustness certification.

### Open Question 3
- Question: Can the GDP composition analysis be extended to certify diffusion models using guidance signals other than reconstruction, such as classifier guidance?
- Basis in paper: [inferred] The theoretical analysis in Section 3 relies specifically on "Clean Image Guided Denoising" where sensitivity is calculated based on the distance to the input $x$.
- Why unresolved: The current proof bounds the sensitivity of the guidance term ($s \cdot (x - \hat{x}_0)$); other guidance types may have unbounded or complex sensitivity profiles that do not fit the current GDP filter.
- What evidence would resolve it: A theoretical derivation of sensitivity bounds for classifier-based guidance within the GDP filter framework, or an empirical study showing certification is maintained.

## Limitations
- Guidance introduces semantic drift, potentially changing object identity in reconstructions
- Multi-step denoising increases variance, degrading clean accuracy compared to one-shot methods
- Certification performance trails Carlini et al. (2023b) at higher radii (r=1.5)

## Confidence

- **High**: GDP filter composition theorem, baseline comparison methodology, core trade-off between fidelity and robustness
- **Medium**: Per-step sensitivity derivation, guidance scale selection rationale
- **Low**: Generalization to alternative diffusion models, stability under distribution shift

## Next Checks

1. Replicate Table 1 with your own pretrained DDPM and BEiT classifier; verify certified accuracy at r=0 matches 58-61% range for σ=1.0.
2. Test guidance scale sensitivity by sweeping s ∈ {0.5, 0.8, 1.0} at σ=1.5; plot certified accuracy to confirm optimal trade-off point.
3. Measure per-pixel budget exhaustion distribution; if >50% pixels exhaust budget before t=500, adjust s or total steps to ensure sufficient denoising.