---
ver: rpa2
title: 'TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on
  Vision-Language Models'
arxiv_id: '2512.16523'
source_url: https://arxiv.org/abs/2512.16523
tags:
- adversarial
- padding
- clean
- accuracy
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of Vision-Language Models
  (VLMs) like CLIP to adversarial attacks, which compromise their performance in safety-critical
  applications. Existing defenses rely on costly retraining or uniform test-time adaptation,
  both of which fail to optimize clean accuracy and adversarial robustness simultaneously.
---

# TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models

## Quick Facts
- **arXiv ID**: 2512.16523
- **Source URL**: https://arxiv.org/abs/2512.16523
- **Reference count**: 40
- **Primary result**: TTP achieves superior adversarial robustness on CLIP without compromising clean accuracy by detecting adversarial inputs via padding-induced embedding shifts and adapting with trainable padding and similarity-aware ensemble.

## Executive Summary
TTP addresses the vulnerability of Vision-Language Models like CLIP to adversarial attacks by introducing a lightweight detection-then-adaptation framework. The method identifies adversarial inputs through the cosine similarity shift between CLIP feature embeddings before and after spatial padding, using a universal threshold for reliable detection across architectures and datasets. For detected adversarial examples, TTP applies robust adaptation using trainable padding and a similarity-aware ensemble strategy, while preserving original predictions for clean inputs. Extensive experiments demonstrate that TTP consistently outperforms state-of-the-art test-time defenses, achieving superior adversarial robustness without compromising clean accuracy.

## Method Summary
TTP operates through a two-stage process: detection and adaptation. During detection, it computes the cosine similarity between CLIP embeddings of the input and its padded version, classifying inputs as adversarial if similarity falls below a threshold (0.8). For detected adversarial examples, TTP generates 64 AugMix augmented views, selects low-entropy subsets, and optimizes instance-specific padding parameters via single-step entropy minimization. A similarity-aware ensemble then aggregates predictions, weighting augmented views based on their proximity to the padded adversarial embedding while maintaining distance from the unpadded version. The method is evaluated across eight fine-grained datasets and three CLIP backbones under various attack types.

## Key Results
- TTP achieves consistent adversarial robustness improvements across ViT-B/32, ViT-B/16, and ViT-L/14 backbones
- The detection mechanism shows clear separability between clean (cosine similarity ~0.93) and adversarial samples (~0.54-0.62) with padding size 32
- Trainable padding and similarity-aware ensemble provide incremental but consistent robustness gains (1-2% improvements)
- Clean accuracy is preserved across all datasets while robustness improves substantially

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adversarial examples exhibit significantly larger cosine similarity shifts between original and padded embeddings than clean samples, enabling reliable detection with a universal threshold.
- **Mechanism**: Spatial padding partially restores attention patterns disrupted by adversarial perturbations, causing adversarial embeddings to shift more than clean embeddings under the same transformation.
- **Core assumption**: Adversarial perturbations disrupt attention in a manner that is partially recoverable through spatial transformations like padding, while clean samples are more invariant to such transformations.
- **Evidence anchors**:
  - [abstract] "TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets."
  - [section 3.2] "Clean samples exhibit minimal shift, whereas adversarial examples show pronounced change."
  - [section 4.3, Figure 4a] Shows clean samples maintain ~0.93 cosine similarity while adversarial examples drop to ~0.54-0.62 depending on padding size.

### Mechanism 2
- **Claim**: Trainable padding parameters optimized via entropy minimization on confident augmented views restore attention more effectively than random padding.
- **Mechanism**: Instance-specific padding parameters θ are updated via single-step gradient descent on entropy loss over low-entropy views.
- **Core assumption**: Low-entropy predictions on adversarial views correlate with correct classifications; entropy minimization guides padding toward configurations that counteract adversarial disruption.
- **Evidence anchors**:
  - [section 3.2] "Unlike prior test-time defenses that adapt textual prompt embeddings, our proposed method optimizes instance-specific image padding parameters at inference, for restoring clean attention patterns disrupted by adversarial perturbations."
  - [Table 7] Ablation shows trainable padding (EntMin) improves robustness from 37.5% to 39.0% on ViT-B/32.

### Mechanism 3
- **Claim**: Similarity-aware ensemble weighting prioritizes augmented views that are close to the padded adversarial embedding but distant from the unpadded adversarial embedding.
- **Mechanism**: Weight w_i ∝ exp(α_i - β_i) where α_i = cos(z_pad_i, z_pad_adv) and β_i = cos(z_pad_i, z_adv).
- **Core assumption**: Padded adversarial embeddings approximate clean embeddings; views similar to padded versions are more likely correct.
- **Evidence anchors**:
  - [section 3.2] "Because adversarial perturbations substantially distort image features, we prefer padded views that are farther from the original adversarial embedding (smaller β_i)."
  - [Table 7] Adding Sim-Aware improves ViT-B/32 robustness from 39.0% to 39.7%.

## Foundational Learning

- **CLIP dual-tower architecture and zero-shot classification**: Understanding F(·), G(·), and cosine similarity computation is prerequisite for TTP's operation on CLIP embeddings.
  - *Quick check*: Given image encoder F and text encoder G, how does CLIP compute class probabilities for a C-way classification problem?

- **Adversarial attacks (PGD, FGSM, CW) and perturbation bounds**: TTP evaluates under PGD with ε=4.0; understanding attack strength informs detection threshold sensitivity.
  - *Quick check*: Why does the paper use ε=4/255 as the perturbation bound, and how does attack strength affect detection separability?

- **Entropy minimization in test-time adaptation**: TTP's trainable padding uses marginal entropy minimization; understanding this objective explains why low-entropy views are selected.
  - *Quick check*: What is the assumption underlying entropy minimization as a self-supervised adaptation signal, and when might it fail?

## Architecture Onboarding

- **Component map**: Input → Detection module (fixed padding → dual encoding → cosine similarity → threshold) → Clean path (return original prediction) or Adversarial path (AugMix augmentation → entropy ranking → trainable padding → similarity-aware ensemble → weighted aggregation)

- **Critical path**: Input x → encode z and z_pad → compute similarity s → if s > 0.8: return p_c(x) else: generate augmented views → update padding parameters → compute weights → aggregate predictions

- **Design tradeoffs**:
  - Padding size (16-128): Larger padding increases detection separability but eventually degrades structural integrity
  - Threshold τ: 0.8 is dataset-agnostic; lowering increases false positive rate, raising increases false negative rate
  - Single-step vs. multi-step optimization: Paper uses single-step for efficiency; multi-step could improve robustness but increases latency

- **Failure signatures**:
  - Low detection accuracy on new backbones/datasets: Check if similarity distributions overlap; may need threshold recalibration
  - Robustness gains not matching paper: Verify attack strength (ε=4.0), padding initialization range [0,10], and AugMix configuration
  - Clean accuracy degradation: Detection false positives causing unnecessary adaptation; check threshold against clean similarity distribution

- **First 3 experiments**:
  1. **Detection calibration**: Run TTP detection on held-out clean/adversarial samples from a dataset not in Table 8; plot similarity distributions and verify τ=0.8 separates them.
  2. **Padding size sweep**: On DTD dataset with PGD attack, sweep padding size [16, 32, 64, 128] and plot both detection accuracy and robustness to reproduce Figure 4c.
  3. **Component ablation**: Disable Sim-Aware ensemble (use uniform weighting) and measure robustness drop; compare against Table 7 to validate implementation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can adaptive adversarial attacks circumvent TTP's detection mechanism by manipulating the cosine similarity shift relative to the fixed threshold?
- **Basis in paper**: [inferred] TTP relies on a fixed threshold ($\tau=0.8$) for detection (Methodology). Adaptive attacks could potentially optimize perturbations to maintain high similarity with padded versions, thus evading detection.
- **Why unresolved**: The evaluation focuses on standard attacks (PGD, CW, FGSM) without considering attackers who have knowledge of the defense strategy.
- **What evidence would resolve it**: Evaluation against adaptive attacks specifically designed to minimize the similarity shift used for detection.

### Open Question 2
- **Question**: How does TTP perform against targeted adversarial attacks that generate high-confidence (low-entropy) incorrect predictions?
- **Basis in paper**: [inferred] The adaptation phase relies on entropy minimization (Eq. 7), which assumes that minimizing prediction entropy moves the model toward the correct class.
- **Why unresolved**: Targeted attacks often result in low-entropy outputs for the target class; minimizing entropy in these cases might reinforce the adversarial prediction rather than correct it.
- **What evidence would resolve it**: Experiments analyzing TTP's defense performance on targeted adversarial benchmarks (e.g., Targeted PGD).

### Open Question 3
- **Question**: Is the computational overhead of the trainable padding optimization negligible for real-time or edge-device applications?
- **Basis in paper**: [inferred] While described as "lightweight," TTP requires generating $N$ augmented views and performing a backward pass for gradient updates on adversarial inputs (Algorithm 1).
- **Why unresolved**: The paper provides accuracy metrics but lacks a systematic analysis of latency or FLOPs compared to standard inference or other test-time defenses.
- **What evidence would resolve it**: Detailed latency analysis (ms per image) across different hardware setups and batch sizes.

## Limitations
- The exact architecture of the trainable padding module P_θ(·) is underspecified, leaving ambiguity about its implementation details
- The method relies on a fixed detection threshold that may not generalize optimally to all domains without recalibration
- Incremental robustness gains (1-2%) suggest the approach may be approaching practical limits for this defense paradigm

## Confidence
- **High confidence**: Detection mechanism based on cosine similarity shifts is well-supported by empirical evidence showing clear separation between clean and adversarial distributions
- **Medium confidence**: Trainable padding via entropy minimization shows consistent but modest improvements (1-2% robustness gains)
- **Medium confidence**: Similarity-aware ensemble strategy demonstrates incremental benefits, but the improvement (0.3-0.7% robustness) is relatively small

## Next Checks
1. **Detection calibration on unseen dataset**: Apply TTP detection to a fine-grained dataset not in Table 8 (e.g., Food101 or StanfordCars) and verify that cosine similarity distributions for clean vs. adversarial samples show clear separation at τ=0.8, confirming the universality claim.
2. **Component sensitivity analysis**: Systematically vary the detection threshold τ from 0.7 to 0.9 on a held-out dataset and measure the trade-off between clean accuracy preservation and adversarial detection accuracy to identify optimal operating points.
3. **Padding size optimization sweep**: On DTD with PGD attack, sweep padding size from 16 to 128 pixels and measure both detection accuracy and robustness to reproduce the peak at 32-64 shown in Figure 4c, validating the sweet spot claim.