---
ver: rpa2
title: 'WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking'
arxiv_id: '2511.07863'
source_url: https://arxiv.org/abs/2511.07863
tags:
- watermarking
- watermod
- entropy
- token
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WaterMod, a modular token-rank partitioning
  method for probability-balanced LLM watermarking. The approach addresses limitations
  in existing watermarking techniques that rely on random or semantically clustered
  token splits, which can degrade fluency by excluding high-probability tokens.
---

# WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking

## Quick Facts
- **arXiv ID**: 2511.07863
- **Source URL**: https://arxiv.org/abs/2511.07863
- **Reference count**: 16
- **Primary result**: Achieves strong watermark detection (AUROC up to 100%) while preserving fluency (perplexity as low as 12.58) and accuracy on natural language, math reasoning, and code generation tasks

## Executive Summary
WaterMod introduces a modular token-rank partitioning method for probability-balanced LLM watermarking that addresses key limitations in existing approaches. Unlike random or semantically clustered token splits that degrade fluency by excluding high-probability tokens, WaterMod sorts the vocabulary by model probability and partitions token ranks using modular arithmetic (rank mod k). This ensures high-probability tokens are distributed across classes while maintaining generation quality. The approach supports both zero-bit watermarking with entropy-adaptive green-list selection and multi-bit watermarking with payload digit-based class selection, achieving superior performance across natural language, mathematical reasoning, and code generation tasks.

## Method Summary
WaterMod partitions the vocabulary by sorting tokens by descending model probability and assigning them to classes via rank mod k. For zero-bit watermarking (k=2), an entropy-adaptive gate selects the green list based on Shannon entropy; for multi-bit watermarking (k>2), a payload digit determines the color class. The method adds a logit bias to guide sampling toward the selected class without excluding alternatives. Experiments demonstrate consistent watermark detection performance (AUROC up to 100%) while preserving fluency (perplexity as low as 12.58) and task accuracy across diverse domains.

## Key Results
- Achieves AUROC up to 100% on watermark detection while maintaining perplexity as low as 12.58
- Outperforms state-of-the-art baselines in both zero-bit (SeqMark) and multi-bit (MPAC, EKA) settings
- Preserves task accuracy on GSM8K (math reasoning) and MBPP+ (code generation) while embedding watermarks
- Demonstrates robust performance across natural language, mathematical reasoning, and code generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular partitioning of probability-ranked tokens preserves fluency by ensuring high-probability tokens remain available in every color class.
- Mechanism: Tokens are sorted by descending model probability (argsort), then assigned to classes via `rank mod k`. Since adjacent ranks fall into different residue classes (e.g., rank 0 and 1 have different parities), the top-2 tokens are guaranteed to span both classes. A small logit bias δ guides sampling toward the selected class without excluding alternatives.
- Core assumption: Adjacent tokens in probability rank order are contextually interchangeable (near-synonyms) per the model's own distribution.
- Evidence anchors:
  - [abstract] "the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent—and therefore semantically similar—tokens across different classes"
  - [section 3.1] "Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling"
  - [corpus] Related work (Guo et al., Hou et al.) confirms semantic-aware partitioning improves fluency, but WaterMod achieves this without external resources.

### Mechanism 2
- Claim: Entropy-adaptive green-list selection balances watermark strength against fluency by conditioning the parity choice on distribution uncertainty.
- Mechanism: Compute Shannon entropy `Ht = -Σ pi log pi`. Map to a Bernoulli parameter `podd = (Ht/Hmax)^Hscale`. When entropy is low (sharp distribution), `podd ≈ 0` and even ranks become green, preserving the top-1 token. When entropy is high, odd ranks become more likely, embedding more signal while relying on the top-2 guarantee.
- Core assumption: Low-entropy contexts are fluency-critical and should prefer even-ranked (top-1) tokens; high-entropy contexts tolerate signal embedding via odd ranks.
- Evidence anchors:
  - [abstract] "In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list"
  - [section 3.1, Eq. 4] Podd formula with Hscale controlling steepness
  - [corpus] "Semantic Differentiation" paper (SeqMark) similarly addresses low-entropy challenges in constrained generation, confirming this is a known problem.

### Mechanism 3
- Claim: Multi-bit watermarking extends the modular rule to k>2 classes, embedding `log2(k)` bits per position while maintaining near-uniform class distribution.
- Mechanism: Payload message `m` is represented in base-k. At each step, a pseudorandom position `p` is selected, and the digit `d = m[p]` determines which residue class (`rank mod k = d`) receives the logit bias. Detection uses majority voting over color tallies per position.
- Core assumption: Repeated observations of each digit position provide natural error correction, enabling recovery even from short passages.
- Evidence anchors:
  - [abstract] "In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d"
  - [section 3.2] "each token rank belongs to exactly one residue class... majority voting over the observed color counts recovers every digit"
  - [corpus] MPAC baseline comparison shows WaterMod achieves higher AUROC (98.29 vs 48.40) on MBPP+ code generation.

## Foundational Learning

- Concept: Logit-based watermarking fundamentals
  - Why needed here: WaterMod modifies logits before sampling; understanding the green/red list paradigm is prerequisite.
  - Quick check question: Given logits `[2.0, 1.0, 0.5]` and green list `{0, 2}` with δ=1.0, what are the modified logits? (Answer: `[3.0, 1.0, 1.5]`)

- Concept: Shannon entropy of token distributions
  - Why needed here: The entropy gate determines green-list selection; you must compute `Ht` and interpret its meaning.
  - Quick check question: A uniform distribution over V tokens has entropy `log2(V)`. What is `Ht/Hmax` in this case? (Answer: 1.0)

- Concept: Modular arithmetic and residue classes
  - Why needed here: The core partitioning rule is `rank mod k`; you need to understand collision-free deterministic mapping.
  - Quick check question: For k=4, which residue class does rank 7 belong to? (Answer: `7 mod 4 = 3`, class C3)

## Architecture Onboarding

- Component map:
  - **Ranker**: `argsort(logits, descending=True)` → permutation π
  - **EntropyGate**: Computes Ht, derives podd, samples parity choice g
  - **ModularPartitioner**: Maps each rank to class via `r mod k`
  - **LogitBiaser**: Adds δ to tokens in selected class
  - **PayloadEncoder** (multi-bit): Selects digit position p and class d from message m
  - **Detector**: Reconstructs classes, counts green hits, computes z-score

- Critical path:
  1. Receive logits `ℓt` and previous token `xt-1`
  2. Compute entropy Ht and determine green parity/class
  3. Sort vocabulary by probability → ranks
  4. Partition ranks by `mod k`, identify target class
  5. Add bias δ to target class logits
  6. Sample/greedy decode next token

- Design tradeoffs:
  - Higher k → more payload bits but smaller target class (requires higher δ)
  - Higher Hscale → steeper entropy gate → more conservative in low-entropy (better fluency, weaker signal)
  - δ=1.0 (zero-bit) vs δ=2.5 (multi-bit): multi-bit needs stronger bias due to smaller class fraction

- Failure signatures:
  - Perplexity spikes: entropy gate too aggressive (Hscale misconfigured)
  - Low AUROC: δ too small or passage too short for payload recovery
  - Code/math accuracy drops: low-entropy contexts being forced to odd ranks

- First 3 experiments:
  1. Reproduce zero-bit C4 results: Set k=2, δ=1.0, Hscale=1.2; verify perplexity ~12.58 and AUROC ~87%
  2. Ablate entropy gate: Force even-only vs odd-only parity selection; measure fluency degradation
  3. Multi-bit capacity test: Embed 16-bit payload with k=4 on GSM8K; verify accuracy ~40% and AUROC ~97%

## Open Questions the Paper Calls Out
None

## Limitations
- **Key Uncertainty 1**: Entropy gate robustness across domains - the entropy-adaptive selection was evaluated primarily on high-entropy C4, and its contribution in low-entropy domains like code generation may be minimal.
- **Key Uncertainty 2**: Scalability to larger models and vocabularies - all experiments use Qwen-2.5-1.5B with 6,400 tokens; scaling to larger models (7B-70B) and vocabularies (50K-200K) remains unexplored.
- **Key Uncertainty 3**: Detection threshold selection and calibration - the paper reports AUROC but does not specify the z-score threshold τ for binary classification or discuss threshold calibration requirements for operational use.

## Confidence

**High Confidence Claims**
- Modular token-rank partitioning distributes high-probability tokens across classes while maintaining semantic similarity within classes
- WaterMod outperforms state-of-the-art baselines in both zero-bit and multi-bit settings on detection metrics
- The modular partitioning preserves fluency better than random or semantically-clustered partitioning

**Medium Confidence Claims**
- The entropy-adaptive gate meaningfully improves fluency in low-entropy contexts
- Multi-bit watermarking with k>2 classes achieves payload embedding with near-uniform class distribution

**Low Confidence Claims**
- WaterMod is universally applicable across all LLM generation tasks
- The PRF and Hash2Uniform implementations provide sufficient cryptographic security

## Next Checks

**Validation Check 1**: Ablation study on entropy gate contribution
Re-run the GSM8K and MBPP+ experiments with fixed parity selection (always even, always odd) in addition to the adaptive gate. Compare perplexity and accuracy to quantify the actual contribution of entropy adaptation in low-entropy domains.

**Validation Check 2**: Parameter sensitivity analysis for model scaling
Evaluate WaterMod on at least two additional model scales (e.g., 7B and 13B parameters) using the same tasks and datasets. Systematically vary δ values to identify optimal bias strengths for each scale, and analyze how detection AUROC and perplexity vary with model size.

**Validation Check 3**: Real-world deployment simulation
Simulate a practical watermarking deployment by: (1) calibrating z-score thresholds on held-out validation data to achieve 95% detection at 5% false positive rate, (2) testing detection on watermarked text that has undergone common transformations (paraphrasing, translation, compression), and (3) evaluating the impact of variable-length passages on multi-bit payload recovery rates.