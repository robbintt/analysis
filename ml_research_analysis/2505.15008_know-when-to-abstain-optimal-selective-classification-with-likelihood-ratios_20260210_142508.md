---
ver: rpa2
title: 'Know When to Abstain: Optimal Selective Classification with Likelihood Ratios'
arxiv_id: '2505.15008'
source_url: https://arxiv.org/abs/2505.15008
tags:
- selective
- classification
- optimal
- scores
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selective classification under
  covariate shift, where the input distribution changes between training and test
  time. The authors propose a new framework based on the Neyman-Pearson lemma, showing
  that optimal selection functions can be derived from likelihood ratios of correct
  versus incorrect predictions.
---

# Keep When to Abstain: Optimal Selective Classification with Likelihood Ratios

## Quick Facts
- arXiv ID: 2505.15008
- Source URL: https://arxiv.org/abs/2505.15008
- Authors: Alvin Heng; Harold Soh
- Reference count: 40
- One-line primary result: Framework provides principled way to design effective selectors, reducing AURC by up to 50% on vision-language models like CLIP

## Executive Summary
This paper addresses selective classification under covariate shift, where input distributions change between training and test time. The authors propose a framework based on the Neyman-Pearson lemma, showing that optimal selection functions can be derived from likelihood ratios of correct versus incorrect predictions. They introduce two novel distance-based scores, ∆-MDS and ∆-KNN, and demonstrate that linear combinations with logit-based scores yield strong performance. Experiments across vision and language tasks show consistent improvements over existing baselines.

## Method Summary
The framework partitions training features into correctly and incorrectly classified sets, then scores test points by their relative proximity to each set. Two methods are introduced: ∆-MDS computes class-conditional Gaussian statistics and uses Mahalanobis distance, while ∆-KNN uses k-nearest neighbors in each partition. Both methods can be linearly combined with logit-based scores like RLog. The approach is theoretically justified by the Neyman-Pearson lemma, which characterizes the optimal rejection rule as a likelihood ratio test. Implementation requires feature extraction, training set partitioning, statistic estimation or KNN index building, and score combination at inference.

## Key Results
- ∆-MDS and ∆-KNN reduce average AURC by ~50% versus standard MDS/KNN on CLIP
- ∆-KNN-RLog achieves best average NAURC (0.163 on CLIP, 0.166 on EVA) across methods
- Framework performs well on powerful vision-language models like CLIP under covariate shift
- Consistent improvements over existing baselines across vision and language tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The optimal selection function for selective classification is a monotonic transformation of the likelihood ratio between correct and incorrect prediction densities.
- **Mechanism:** Frame selective classification as binary hypothesis testing: H₀ (correct prediction) vs H₁ (incorrect). By the Neyman-Pearson lemma, thresholding the likelihood ratio p_c(x)/p_w(x) minimizes false acceptances for any fixed false rejection rate.
- **Core assumption:** Densities p_c and p_w exist and are strictly positive on shared support; the selector score is a monotonic transform of the true likelihood ratio.
- **Evidence anchors:** Abstract mentions "likelihood ratio test" optimality; Lemma 1 and Corollary 1 in section 3 establish monotonic transform preservation; limited direct corpus validation exists.
- **Break condition:** When p_c/p_w cannot be reliably estimated (insufficient training samples in one partition, or feature distributions violate continuity assumptions).

### Mechanism 2
- **Claim:** Separating training features into correctly and incorrectly classified sets, then scoring test points by their relative proximity to each set, approximates the NP-optimal likelihood ratio.
- **Mechanism:** For ∆-MDS: compute class-conditional Gaussian statistics for correct samples and wrong samples; score = difference in Mahalanobis distances. For ∆-KNN: compute average log-distances to k-nearest neighbors in each set; score = difference.
- **Core assumption:** ∆-MDS: features are class-conditionally Gaussian given correct/incorrect prediction. ∆-KNN: asymptotic consistency of k-NN density estimation (k→∞, k/N→0).
- **Evidence anchors:** Theorem 2 proves ∆-MDS optimality under Gaussian assumptions; Theorem 3 proves ∆-KNN asymptotic optimality; table 1 shows ~50% AURC reduction; no direct corpus corroboration.
- **Break condition:** ∆-MDS fails when features are non-Gaussian (paper notes CLIP's contrastive features favor ∆-KNN); ∆-KNN underperforms with insufficient samples in A_w.

### Mechanism 3
- **Claim:** Linear combinations of NP-optimal scores remain NP-optimal under a tilted likelihood model and empirically outperform individual scores.
- **Mechanism:** Combine distance-based and logit-based scores: s_combined = s_distance + λ·s_logit. Distance-based methods capture feature-space geometry; logit-based methods capture classifier boundary confidence.
- **Core assumption:** The joint density factorizes as a multiplicative (tilted) product: p_c^(1)(p_c^(2))^λ / Z_c.
- **Evidence anchors:** Lemma 2 proves linear combinations preserve NP-optimality under tilted likelihoods; table 1, 2 show ∆-KNN-RLog achieves best average NAURC; corpus does not provide independent validation.
- **Break condition:** When λ is poorly tuned causing magnitude imbalance, or when both component scores are miscalibrated in the same direction.

## Foundational Learning

- **Concept: Neyman-Pearson Lemma**
  - **Why needed here:** Central theoretical justification; without it, the likelihood-ratio framing lacks optimality guarantees.
  - **Quick check question:** Given two hypotheses with fixed Type I error tolerance, what test minimizes Type II error?

- **Concept: Mahalanobis Distance**
  - **Why needed here:** ∆-MDS uses this to measure distance from class-conditional Gaussian distributions; standard Euclidean distance ignores covariance structure.
  - **Quick check question:** If Σ is diagonal with entries [4, 1], does point (2, 2) have equal Mahalanobis distance contribution from both dimensions?

- **Concept: Covariate Shift**
  - **Why needed here:** The entire evaluation framework targets this scenario; understanding p(x) change vs. p(y|x) change is critical for interpreting results.
  - **Quick check question:** In covariate shift, which of these changes between train and test: p(x), p(y|x), p(y)?

## Architecture Onboarding

- **Component map:** Feature extractor → Training partitioner → Statistics module (∆-MDS) or KNN index (∆-KNN) → Score combiner

- **Critical path:**
  1. Run classifier on training set → obtain predictions
  2. Partition features by prediction correctness (requires ground-truth labels)
  3. For ∆-MDS: estimate per-class means and shared covariance for each partition
  4. For ∆-KNN: build FAISS/annoy indices on A_c and A_w
  5. At inference: extract feature → compute both distance scores → combine with logit score

- **Design tradeoffs:**
  - ∆-MDS vs. ∆-KNN: MDS requires Gaussian assumption (better for supervised softmax classifiers); KNN is non-parametric (better for contrastive models like CLIP)
  - k selection: Paper uses k=25 for ∆-KNN (ablation shows averaging top-k beats single k-th distance)
  - λ selection: Requires magnitude balancing; paper uses grid search (λ=10 for CLIP ∆-KNN-RLog, λ=0.5 for EVA)
  - Storage: ∆-MDS stores O(K·d) parameters; ∆-KNN stores all training features O(N·d)

- **Failure signatures:**
  - Very high base accuracy → A_w too small → unreliable p_w estimation → ∆-scores degrade
  - Non-Gaussian features with ∆-MDS → CLIP underperforms vs. ∆-KNN (confirmed in paper)
  - Imbalanced λ → one score dominates → combination no better than single score
  - Missing ground-truth labels at training → cannot partition correctly → method inapplicable

- **First 3 experiments:**
  1. **Reproduce ∆-KNN vs. KNN gap:** On ImageNet-1K with a pretrained ResNet, compare standard KNN score vs. ∆-KNN; expect ~50% AURC reduction as reported
  2. **Validate Gaussian assumption:** Visualize t-SNE of A_c vs. A_w features for a supervised model; assess whether Gaussian assumption is plausible for your architecture
  3. **Lambda sensitivity sweep:** Fix ∆-KNN-RLog pipeline, sweep λ ∈ {0.1, 1, 10, 100} on validation set; verify paper's claim that magnitude balancing is sufficient without extensive tuning

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the Neyman-Pearson likelihood ratio framework be adapted for selective generation in Large Language Models (LLMs) to mitigate hallucinations?
  - **Basis in paper:** The "Limitations and Future Work" section states that enabling LLMs to recognize uncertainty and selectively abstain ("selective generation") is an "exciting avenue for future work."
  - **Why unresolved:** The current theoretical derivation assumes a fixed label space Y and discriminative classification, whereas LLMs operate over open-ended generative token sequences.
  - **What evidence would resolve it:** An extension of the likelihood ratio test to sequence probabilities and empirical benchmarks showing reduced hallucination rates when LLMs utilize the proposed selective abstention mechanism.

- **Open Question 2:** Can the proposed Δ-MDS and Δ-KNN selectors be effectively generalized to structured prediction tasks like semantic segmentation or regression?
  - **Basis in paper:** The authors explicitly identify "regression, semantic segmentation, or time series forecasting" as "promising future directions" in the conclusion.
  - **Why unresolved:** The current proofs rely on a binary "correct vs. incorrect" hypothesis test, which maps cleanly to classification but is ambiguous for continuous or structured outputs where "correctness" is not discrete.
  - **What evidence would resolve it:** A reformulation of the likelihood ratio p_c/p_w for continuous error metrics (e.g., IoU for segmentation) and experimental validation on standard regression/segmentation benchmarks under covariate shift.

- **Open Question 3:** Does post-hoc calibration (e.g., temperature scaling) restore the Neyman-Pearson optimality of Maximum Softmax Probability (MSP) in modern deep networks?
  - **Basis in paper:** Theorem 1 proves MSP is NP-optimal only assuming a calibrated classifier. The text notes that modern networks are poorly calibrated but leaves the interaction between calibration interventions and NP optimality "beyond the scope of this work."
  - **Why unresolved:** While calibration aligns confidence with accuracy, it is unclear if the transformation preserves the monotonic relationship between MSP and the likelihood ratio p_c/p_w required by Corollary 1.
  - **What evidence would resolve it:** Theoretical analysis of how temperature scaling affects the monotonicity of the likelihood ratio, accompanied by experiments comparing selective classification performance on raw versus calibrated logits.

## Limitations
- Theoretical claims hinge on existence of strictly positive densities p_c and p_w, which may not hold in high-dimensional spaces where features become sparse
- Empirical validation is robust within studied setting (CLIP, EVA, ImageNet) but generalization to domains with highly non-Gaussian features or severe class imbalance remains untested
- Assumption of covariate shift (p(x) changes, p(y|x) fixed) may not capture all real-world deployment scenarios

## Confidence
- **High Confidence:** The Neyman-Pearson lemma framing and the monotonic transformation property of likelihood ratios (Mechanism 1)
- **Medium Confidence:** The optimality proofs for ∆-MDS and ∆-KNN under their respective assumptions (Mechanism 2)
- **Medium Confidence:** The empirical advantage of linear combinations over individual scores (Mechanism 3)

## Next Checks
1. **Test Gaussian Assumption:** Generate synthetic data where A_c features follow Gaussian distributions and A_w features follow heavy-tailed distributions. Measure whether ∆-MDS performance degrades relative to ∆-KNN as predicted by the theory.
2. **Extreme Accuracy Regime:** Evaluate the framework on a model with 99%+ accuracy where A_w contains only ~1% of training samples. Quantify the variance in AURC estimates and assess whether the partition becomes statistically unreliable.
3. **Covariate Shift Violation:** Design a test set where p(y|x) also changes (e.g., domain shift with label distribution drift). Compare performance of likelihood-ratio based selectors against methods designed for open-set recognition to see if the theoretical framing breaks down.