---
ver: rpa2
title: 'XAM: Interactive Explainability for Authorship Attribution Models'
arxiv_id: '2512.06924'
source_url: https://arxiv.org/abs/2512.06924
tags:
- text
- style
- features
- authors
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IXAM, an interactive tool for explaining
  authorship attribution models. The tool visualizes a model's latent space, allowing
  users to explore clusters of authors and analyze their shared writing style features.
---

# XAM: Interactive Explainability for Authorship Attribution Models

## Quick Facts
- arXiv ID: 2512.06924
- Source URL: https://arxiv.org/abs/2512.06924
- Authors: Milad Alshomary; Anisha Bhatnagar; Peter Zeng; Smaranda Muresan; Owen Rambow; Kathleen McKeown
- Reference count: 4
- Primary result: Interactive explainability tool improves user understanding of authorship attribution model predictions through latent space exploration and style feature analysis

## Executive Summary
XAM is an interactive tool that explains authorship attribution (AA) model predictions by visualizing the model's latent space and extracting writing style features. The system uses t-SNE to project high-dimensional embeddings into 2D space, allowing users to explore clusters of authors and identify shared stylistic patterns. IXAM combines LLM-generated high-level style descriptions with Gram2Vec's traceable linguistic features, highlighting relevant text spans to enable verification. A user study with three participants showed IXAM produced more helpful and interpretable explanations compared to static baselines, with users demonstrating higher agreement on style features and better confidence calibration.

## Method Summary
IXAM operates by first embedding documents using any HuggingFace Sentence Transformer model, then projecting these embeddings into 2D space using t-SNE for visualization. Users interactively explore this space, selecting regions to analyze. The system extracts style features using two complementary methods: GPT-4o generates context-relevant high-level style descriptions, while Gram2Vec extracts normalized relative frequencies of grammatical features (POS n-grams, morphology tags, dependency labels). Both feature types are accompanied by text span highlighting for verification. The tool is implemented as a Gradio web app with caching at three levels (projections, author subsets, features/spans) to reduce latency.

## Key Results
- Users showed 38% overlap in selected style features using IXAM versus 3% for static baseline explanations
- All three participants rated the highlighting feature usefulness at 4 or 5 on a 5-point scale
- IXAM explanations led to clearer confidence differences between correct and incorrect predictions compared to baseline
- Interactive exploration enabled users to construct broader, more plausible explanations than single predefined explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive exploration of latent space enables users to construct broader, more plausible explanations than static precomputed ones.
- Mechanism: Users navigate a 2D t-SNE projection of the AA model's embedding space, zoom into clusters of interest, and inspect common writing style features for that region. This replaces a single predefined explanation with user-driven hypothesis generation.
- Core assumption: The 2D projection preserves meaningful neighborhood structure from the high-dimensional latent space, and cluster proximity corresponds to stylistic similarity.
- Evidence anchors:
  - [abstract] "our tool enables users to interactively explore the model's embedding space and construct an explanation...as a set of writing style features at different levels of granularity"
  - [Section 3.2] "this interactivity allows users to construct a wider set of different hypotheses that explain the model's prediction"
  - [Section 4 Results] "38% overlap in selected style features using IXAM vs. 3% for baseline" indicating higher explanation agreement
- Break condition: t-SNE projections can introduce noise; authors note "might generate noise" (Section 6). If cluster structure is not faithful to original embeddings, explanations may be misaligned.

### Mechanism 2
- Claim: Combining LLM-generated high-level style descriptions with Gram2Vec's traceable linguistic features provides complementary explanatory signals.
- Mechanism: LLMs (GPT-4o) induce context-relevant style features without predefined taxonomies; Gram2Vec extracts normalized relative frequencies of grammatical features directly traceable to documents. Users can inspect both types.
- Core assumption: LLM-generated style descriptions are relevant and accurate for the selected texts, and Gram2Vec features capture meaningful stylistic variation.
- Evidence anchors:
  - [Section 3.2] "LLMs...obviate the need for a predefined set of stylistic features and are able to induce context-relevant style features"
  - [Section 3.2] "Gram2Vec creates an interpretable linguistic representation...extracts normalized relative frequencies of grammatical features"
  - [corpus] Weak—no direct corpus evidence comparing LLM vs. Gram2Vec efficacy; corpus neighbors focus on AA applications, not explainability mechanisms
- Break condition: LLM explanations may hallucinate features not grounded in text; Gram2Vec features may miss higher-level rhetorical patterns. Authors note post-hoc explanations "might not be faithful to the model's prediction" (Section 6).

### Mechanism 3
- Claim: Span highlighting enables users to verify explanations against concrete text evidence, improving interpretability and confidence calibration.
- Mechanism: Given a style feature (LLM or Gram2Vec), the system highlights all text spans in task authors' documents that exhibit that feature. Users can visually confirm feature presence.
- Core assumption: Users can accurately judge whether highlighted spans genuinely reflect the claimed feature.
- Evidence anchors:
  - [Section 3.2] "This highlighting allows the user to verify the exact part of the text that was claimed to have the specific feature"
  - [Section 4 Results] "clear decrease in confidence when the system makes a wrong prediction" with IXAM, suggesting explanations are interpretable enough for calibration
  - [Section 4, Figure 4] All three participants rated highlighting usefulness at 4 or 5
- Break condition: If LLM span extraction is inaccurate or Gram2Vec library fails to identify relevant spans, verification step provides false confidence.

## Foundational Learning

- Concept: Embedding-based authorship attribution models
  - Why needed here: The entire system explains latent space predictions; understanding how embedding models work (contrastive learning, style vs. content encoding) is prerequisite.
  - Quick check question: Given two texts from the same author, would you expect their embeddings to be closer or farther in the learned space? Why?

- Concept: Dimensionality reduction (t-SNE)
  - Why needed here: Users interact with 2D projections of high-dimensional embeddings; knowing t-SNE preserves local neighborhoods but distorts global distances helps interpret visualizations correctly.
  - Quick check question: If two points appear close in a t-SNE plot, can you conclude their original embeddings are similar? What caveats apply?

- Concept: Stylometric feature extraction
  - Why needed here: Gram2Vec extracts POS n-grams, morphology tags, dependency labels; understanding these as quantifiable style signals grounds interpretation.
  - Quick check question: What is the difference between a "POS bigram" feature and an LLM-generated feature like "use of personal anecdotes"?

## Architecture Onboarding

- Component map: Embedding layer -> Projection layer (t-SNE) -> Style extraction layer (LLM + Gram2Vec) -> Span extraction layer -> Interface layer (Gradio)
- Critical path: User selects region → System retrieves authors in region → GPT-4o + Gram2Vec extract features → Features displayed → User clicks feature → System highlights spans in task texts
- Design tradeoffs:
  - Post-hoc vs. faithful: Explanations are plausible but not causally guaranteed to reflect model internals (explicitly noted in Limitations)
  - Flexibility vs. latency: Caching reduces latency but precomputing limits ad-hoc exploration of arbitrary regions
  - LLM vs. rule-based features: LLM provides breadth but may hallucinate; Gram2Vec is precise but narrow in scope
- Failure signatures:
  - Low agreement between users on selected features (baseline showed 3% overlap)
  - No confidence difference between correct/wrong predictions (baseline behavior)
  - Projection shows scattered points with no clear cluster structure
  - Highlighted spans don't visibly match claimed feature (LLM extraction failure)
- First 3 experiments:
  1. **Domain shift test**: Run IXAM with background corpus from a different domain than the AA model's training data; observe whether clusters remain interpretable (authors note this is a potential failure mode).
  2. **Ablation study**: Disable LLM features, keep only Gram2Vec; measure user confidence and task accuracy to quantify contribution of each feature type.
  3. **Faithfulness probe**: Edit highlighted spans in mystery text (remove or modify feature-bearing text), re-run AA model, observe prediction change—testing whether highlighted features causally influence model output (suggested in Limitations).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the style features identified by IXAM be causally linked to the AA model's predictions?
- Basis in paper: [explicit] The authors state in the Limitations section that due to the post-hoc nature of the tool, explanations "might not be faithful to the model's prediction" and suggest analyzing causality via text edits.
- Why unresolved: The current evaluation focuses on user preference and plausibility (agreement on features) rather than verifying if the model relies on these specific features.
- What evidence would resolve it: A counterfactual study where identified style features are removed or altered in the input text to test if the model's prediction changes.

### Open Question 2
- Question: Does interactive explainability improve user decision accuracy, or does it primarily increase confidence without reliable calibration?
- Basis in paper: [inferred] While participants reported higher confidence when the model was correct, the results section notes that accuracy in identifying correct predictions was "not conclusive," varying wildly per user (e.g., dropping for U1).
- Why unresolved: The sample size (N=3) is insufficient to determine if the tool helps users verify predictions or if the interactivity creates a false sense of understanding.
- What evidence would resolve it: A larger-scale user study that specifically isolates the impact of the tool on decision accuracy versus confidence calibration.

### Open Question 3
- Question: How does domain shift between the background corpus and the AA model's training data affect the validity of the visualized clusters?
- Basis in paper: [explicit] The authors note that using a Reddit background corpus works for the specific model tested, but acknowledge that in scenarios without training data access, "domain shift... would maybe lead to misalignment."
- Why unresolved: The tool relies on projecting the background corpus to visualize structure; if this does not match the model's internal logic, the visualization becomes misleading.
- What evidence would resolve it: Experiments applying IXAM to black-box models trained on distinct domains (e.g., formal literary texts) using a mismatched background corpus to measure projection fidelity.

## Limitations
- The system provides post-hoc explanations that may not faithfully reflect the model's actual prediction mechanisms
- Small user study sample size (N=3) limits generalizability of findings about user experience improvements
- t-SNE projections can introduce noise and potentially distort cluster relationships, affecting explanation validity

## Confidence

- **High confidence**: The interactive visualization interface functions as described and enables users to explore latent space clusters
- **Medium confidence**: LLM and Gram2Vec feature extraction produces relevant style features, though hallucination risks exist for LLM-generated features
- **Medium confidence**: User experience improvements (higher agreement, confidence calibration) are demonstrated in the small study
- **Low confidence**: Claims about explanation faithfulness to model internals due to post-hoc nature

## Next Checks
1. **Faithfulness validation**: Conduct causal intervention experiments by editing highlighted spans in mystery texts and measuring prediction changes to verify whether identified features actually influence model outputs
2. **Ablation study**: Compare IXAM performance with LLM features disabled versus full system to quantify the contribution of each feature extraction method to user understanding
3. **Domain robustness test**: Evaluate IXAM's cluster interpretability when background corpus comes from different domains than the AA model's training data to assess generalization limits