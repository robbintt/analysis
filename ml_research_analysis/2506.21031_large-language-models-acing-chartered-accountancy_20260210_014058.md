---
ver: rpa2
title: Large Language Models Acing Chartered Accountancy
arxiv_id: '2506.21031'
source_url: https://arxiv.org/abs/2506.21031
tags:
- financial
- language
- large
- llama
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CA-Ben, a comprehensive benchmark for evaluating
  large language models (LLMs) on financial, legal, and quantitative reasoning tasks
  specific to the Chartered Accountancy curriculum in India. CA-Ben consists of structured
  question-answer datasets derived from ICAI examinations across three difficulty
  levels.
---

# Large Language Models Acing Chartered Accountancy

## Quick Facts
- **arXiv ID:** 2506.21031
- **Source URL:** https://arxiv.org/abs/2506.21031
- **Authors:** Jatin Gupta; Akhil Sharma; Saransh Singhania; Mohammad Adnan; Sakshi Deo; Ali Imam Abidi; Keshav Gupta
- **Reference count:** 22
- **Primary result:** Claude 3.5 Sonnet achieved 56.31% accuracy on CA-Ben, outperforming other models on financial, legal, and quantitative reasoning tasks from ICAI exams.

## Executive Summary
This paper introduces CA-Ben, a comprehensive benchmark for evaluating large language models (LLMs) on Chartered Accountancy curriculum tasks from India. The benchmark consists of structured question-answer datasets derived from ICAI examinations across three difficulty levels. Six prominent LLMs were evaluated using standardized prompts and protocols. Claude 3.5 Sonnet achieved the highest overall accuracy at 56.31%, followed by GPT-4o at 54.29%. The results demonstrate strong performance in conceptual subjects like Auditing & Ethics but reveal notable challenges in numerical computations and legal interpretation, suggesting opportunities for improvement through hybrid reasoning and retrieval-augmented generation techniques.

## Method Summary
The authors created CA-Ben by extracting 14 question-answer datasets from ICAI exam papers across different subjects and difficulty levels. Six LLMs (GPT-4o, Claude 3.5 Sonnet, and four others) were evaluated using a standardized system prompt requiring answers in the format "Result: [A-D]" with temperature set to 0.75. A Regex parser extracted answers from model outputs, which were compared against ground truth to calculate accuracy. The evaluation protocol aimed to ensure fair comparison across diverse LLM architectures by enforcing consistent output formatting and controlled generation randomness.

## Key Results
- Claude 3.5 Sonnet achieved the highest overall accuracy at 56.31%, significantly outperforming other models
- GPT-4o followed closely with 54.29% accuracy
- Models showed exceptional performance in conceptual subjects like Auditing & Ethics (up to 93.33%) but struggled with numerical computations and legal interpretations
- Performance degraded substantially in advanced subjects, with some models scoring as low as 6.67% on complex topics like Indirect Tax Laws

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A standardized evaluation protocol, consisting of a uniform system prompt and a fixed temperature setting, enables fair comparative performance measurement across diverse LLM architectures.
- Mechanism: The authors enforce a consistent output format (e.g., "Result: [A-D]") via a system prompt. This structure allows a simple Regex parser to reliably extract answers from each model's response. The fixed temperature of 0.75 controls the stochasticity of the generation process, aiming to strike a balance that allows models to converge on an answer without being overly constrained.
- Core assumption: The system prompt successfully constrains all models to output answers in the exact parseable format, and the chosen temperature provides an equitable trade-off between randomness and determinism for all tested models.
- Evidence anchors:
  - [abstract] "Six prominent LLMs... were evaluated using standardized protocols."
  - [section 4.2] "A temperature setting of 0.75 was selected to introduce a balanced level of randomness... Lowering the temperature any further restricted the models' creative flexibility and it increased the likelihood of them not selecting any of the provided answer choices."
  - [corpus] Related benchmarks like "NitiBench" for Thai legal QA also rely on structured evaluation frameworks, supporting this methodological choice.
- Break condition: If a model consistently ignores the formatting instructions (e.g., provides a reasoning chain without a final "Result:" line), the Regex extraction fails, and the accuracy metric becomes invalid for that model.

### Mechanism 2
- Claim: Performance disparities across subjects reflect the degree to which conceptual, legal, and quantitative reasoning patterns are encoded in the models' pre-trained weights.
- Mechanism: Higher accuracy in conceptual subjects like Auditing & Ethics suggests these domains are well-represented in the training data and can be solved via pattern recognition or robust conceptual embeddings. Conversely, poor performance in areas requiring precise, multi-step numerical computations or detailed statutory interpretation indicates these are weak points in the models' learned reasoning capabilities.
- Core assumption: Success on the benchmark requires generalizable domain reasoning, not just the retrieval of memorized exam questions.
- Evidence anchors:
  - [abstract] "Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and legal reasoning. Notable challenges emerged in numerical computations and legal interpretations."
  - [section 5.3.1] Claude 3.5 Sonnet showed "exceptional results particularly in Auditing and Ethics (93.33%)... despite relatively weaker performance in Taxation (20%)."
  - [corpus] Corpus signals on this specific mechanism are weak; however, papers like "TermGPT" address terminology adaptation in legal/financial domains, implying general models may lack precise domain-specific embeddings.
- Break condition: The mechanism assumes failures are due to reasoning limits. If failures are instead caused by ambiguous questions in the dataset, performance reflects benchmark quality, not model capability.

### Mechanism 3
- Claim: Performance degradation in advanced subjects is driven by error propagation in multi-step reasoning chains.
- Mechanism: Advanced CA subjects (Final level) require integrating multiple concepts (e.g., combining tax laws with financial reporting). LLMs, which generate tokens sequentially, are susceptible to compounding errors; a mistake in an early reasoning step leads to an incorrect final answer. This explains the sharp drop in accuracy for subjects like Indirect Tax Laws and Integrated Business Solutions.
- Core assumption: The decline in accuracy from Foundation to Final level is primarily due to increased reasoning complexity, not merely a lack of training data for advanced topics.
- Evidence anchors:
  - [abstract] "Models struggled most with numerical computations and legal interpretations, indicating room for improvement..."
  - [section 5.2.3] Shows performance declining in Final-level subjects, with models like LLAMA 3.3 70B scoring as low as 6.67% on Indirect Tax Laws.
  - [corpus] The paper "Reasoning Models Ace the CFA Exams" contrasts this by suggesting newer reasoning-focused models may handle such complexity better, reinforcing that standard LLMs struggle here.
- Break condition: If models are failing because of context window limitations (truncating long questions) rather than reasoning failures, then the mechanism is context management, not reasoning complexity.

## Foundational Learning

- **Zero-Shot Benchmarking**
  - Why needed here: CA-Ben evaluates models on unseen professional exam questions. Understanding this is critical to interpret results as a measure of generalization, not recall from fine-tuning.
  - Quick check question: If a model was fine-tuned on past ICAI exam papers, would its CA-Ben score be a fair measure of its general financial reasoning?

- **LLM Temperature Parameter**
  - Why needed here: The authors selected T=0.75. Engineers must understand that this hyperparameter controls the trade-off between a model's adherence to the most likely tokens (low T) and its exploration of less probable ones (high T), which impacts answer selection reliability.
  - Quick check question: Why would setting the temperature too low (e.g., 0.1) cause a model to fail to select one of the multiple-choice options provided?

- **Regular Expressions (Regex) for Output Parsing**
  - Why needed here: The evaluation pipeline's objectivity hinges on a Regex pattern (`(?:Result|Answer):\s*(?([A-D]))?`) extracting the answer. Understanding this brittle dependency is key for debugging why a model might score 0% despite providing correct reasoning.
  - Quick check question: What happens to the accuracy score if a model responds with "The correct option is B" instead of the required format "Result: B"?

## Architecture Onboarding

- **Component map:** Data Source -> Prompt Assembler -> Inference Engine -> Extraction Layer -> Scoring Module
- **Critical path:** Prompt Assembly -> LLM Inference -> Regex Extraction. If the Regex fails to find a match, the evaluation for that query fails. The data source and scoring module are secondary to this core interactive loop.
- **Design tradeoffs:** The authors traded nuanced evaluation for automation. Using Regex to score multiple-choice answers is scalable but ignores the quality of the model's reasoning. They also traded determinism for flexibility by choosing T=0.75; a lower temperature might have improved format adherence but risked models getting "stuck" and failing to select an answer.
- **Failure signatures:**
  - Format Mismatch: The model outputs a valid explanation but omits the "Result:" keyword, causing the Regex to fail.
  - Hallucinated Citations: In legal tasks, the model may reference non-existent sections of law.
  - Calculation Drift: In quantitative problems, the model performs the correct logical steps but makes an arithmetic error in an intermediate step.
- **First 3 experiments:**
  1. **Ablation on Temperature:** Re-run the evaluation on a subset of questions with T=0.0, 0.5, and 1.0 to quantify the trade-off between format adherence (successful Regex extraction) and answer accuracy.
  2. **Parser Upgrade:** Replace the Regex parser with a secondary LLM (e.g., a smaller, cheaper model) instructed to extract the answer from the raw text. Compare the "answer extraction rate" to identify how many correct answers were missed by the Regex.
  3. **Error Taxonomy Analysis:** Take a random sample of 50 incorrectly answered questions from the "Taxation" subject and categorize the failure mode (e.g., legal misinterpretation, calculation error, format error) to validate the authors' qualitative claims.

## Open Questions the Paper Calls Out

None

## Limitations

- **Evaluation Pipeline Fragility:** The accuracy metric depends entirely on the Regex parser successfully extracting a final answer from the model's response. If a model provides correct reasoning but omits the required "Result:" format, it receives a score of 0% for that question, even if the answer is implicit in its explanation.
- **Ground Truth Ambiguity:** The dataset quality is unverified. Without independent validation of the "ground truth" answers, performance scores could reflect errors in the benchmark itself rather than model limitations.
- **Generalization Claims:** The authors assert that performance differences reflect encoding of reasoning patterns in pre-trained weights, but this conflates inability to perform multi-step reasoning with lack of exposure to advanced CA topics during pre-training.

## Confidence

- **High Confidence:** The standardized evaluation protocol successfully enables fair comparative measurement across models.
- **Medium Confidence:** Performance disparities across subjects reflect genuine differences in domain reasoning capabilities.
- **Low Confidence:** The claim that performance degradation in advanced subjects is primarily driven by error propagation in multi-step reasoning chains.

## Next Checks

1. **Temperature Ablation Study:** Systematically evaluate a subset of questions with temperatures T=0.0, 0.5, and 1.0 to quantify the trade-off between format adherence and answer accuracy.
2. **Parser Robustness Enhancement:** Replace the Regex parser with a secondary LLM instructed to extract the answer from the raw text and compare extraction rates.
3. **Error Mode Taxonomy Analysis:** Manually categorize failure modes for a random sample of 50 incorrectly answered questions from the "Taxation" subject to validate qualitative claims about reasoning limitations.