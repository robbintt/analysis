---
ver: rpa2
title: How Scientists Use Large Language Models to Program
arxiv_id: '2502.17348'
source_url: https://arxiv.org/abs/2502.17348
tags:
- code
- programming
- https
- llms
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scientists often lack training in software development practices,
  yet programming is critical for data analysis, simulations, and experiments across
  disciplines. While large language models (LLMs) that generate code have become widely
  available, little is known about how scientists use them or verify their outputs.
---

# How Scientists Use Large Language Models to Program

## Quick Facts
- **arXiv ID**: 2502.17348
- **Source URL**: https://arxiv.org/abs/2502.17348
- **Reference count**: 40
- **Primary result**: Scientists primarily use LLMs for information lookup rather than complex code generation, relying on visual inspection for verification

## Executive Summary
This study investigates how scientists at a research university adopt and use code-generating large language models (LLMs). Through surveys of 199 researchers and interviews with 14 users, the research reveals that most scientists access LLMs through browser-based chat interfaces rather than integrated development environment (IDE) tools. Many are multilingual programmers working in languages where they lack fluency, and they primarily use LLMs to look up functions and syntax rather than generate complex code. The findings suggest that scientists may need better tools for understanding generated code and clearer interfaces that help distinguish information retrieval from code generation.

## Method Summary
The study employed a mixed-methods approach combining surveys and interviews with scientists at a research university. Researchers surveyed 199 participants about their LLM usage patterns, preferences, and verification strategies. They conducted in-depth interviews with 14 users to understand their workflows and decision-making processes. The study also analyzed user logs to identify instances where errors were introduced and not detected. However, the study did not include detailed demographic data about participants' programming experience levels, and the user log analysis appears limited in scope.

## Key Results
- Most scientists access LLMs through browser-based chat interfaces rather than IDE-integrated tools
- Multilingual programmers use LLMs to bridge gaps in language fluency
- Verification strategies rely heavily on visual inspection of outputs and reading code line-by-line
- User logs revealed instances where errors were introduced and not detected

## Why This Works (Mechanism)
Scientists use LLMs as assistive tools to overcome knowledge gaps and language barriers in programming. The mechanism works because LLMs can provide immediate access to syntax and function information that scientists might otherwise need to search for manually. Browser-based interfaces offer low-friction access compared to IDE integration, making them more attractive for quick queries. The line-by-line inspection approach works for simple code generation tasks but may fail to catch more subtle errors that require deeper understanding of program logic or scientific domain knowledge.

## Foundational Learning
- **Browser-based LLM interfaces**: Scientists prefer these over IDE tools for low-friction access; quick check: verify interface usage statistics across different scientific communities
- **Multilingual programming challenges**: Scientists work across languages they may not fully master; quick check: assess language proficiency levels and their correlation with LLM usage patterns
- **Visual inspection verification**: Primary method for checking LLM-generated code; quick check: compare error detection rates between visual inspection and automated testing approaches
- **Information retrieval vs. code generation**: Scientists often seek syntax/function information rather than full code implementations; quick check: categorize LLM queries by complexity and purpose
- **Error detection limitations**: Visual inspection may miss subtle errors; quick check: analyze types of undetected errors in user logs
- **IDE integration gap**: Limited adoption of in-IDE LLM tools; quick check: survey scientists on barriers to IDE integration adoption

## Architecture Onboarding
**Component Map**: User -> Browser Chat Interface -> LLM API -> Generated Code -> Visual Inspection
**Critical Path**: Query formulation → LLM response generation → Code output → Verification through visual inspection
**Design Tradeoffs**: Browser accessibility vs. IDE integration capabilities; simple syntax lookup vs. complex code generation; visual inspection vs. automated testing
**Failure Signatures**: Undetected syntax errors, logical inconsistencies, domain-specific mistakes that pass visual inspection
**First Experiments**:
1. Compare error detection rates between visual inspection and automated testing for LLM-generated code
2. Measure productivity differences between browser-based and IDE-integrated LLM usage
3. Test the effectiveness of LLM-generated explanations in helping scientists understand and verify code

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Small sample size (199 survey respondents, 14 interviews) from single institution limits generalizability
- Lack of demographic data on programming experience levels creates uncertainty about findings' applicability
- Self-reported usage patterns and verification strategies may be subject to recall bias or social desirability effects
- User log analysis appears limited in scope and may not capture full range of error types

## Confidence
**High confidence**: Scientists primarily access LLMs through browser interfaces rather than IDE integration; multilingual programmers use LLMs to bridge language fluency gaps
**Medium confidence**: Visual inspection and line-by-line reading are the dominant verification strategies; interviewees focus on simple code generation tasks rather than complex implementations
**Medium confidence**: User logs demonstrate actual instances of undetected errors being introduced into scientific code

## Next Checks
1. Conduct a follow-up study with stratified sampling across multiple institutions to assess generalizability and control for variables like programming experience, discipline, and career stage
2. Implement automated code analysis tools to independently verify self-reported error rates and detection capabilities, comparing scientist assessments with static analysis and test coverage metrics
3. Design experimental studies where scientists complete identical programming tasks with and without LLM assistance, measuring both productivity gains and error introduction rates under controlled conditions