---
ver: rpa2
title: Complete Chess Games Enable LLM Become A Chess Master
arxiv_id: '2501.17186'
source_url: https://arxiv.org/abs/2501.17186
tags:
- chess
- data
- move
- language
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ChessLLM, a large language model capable of
  playing complete chess games by transforming the game into a textual format with
  Forsyth-Edwards Notation (FEN) representing board states. The model is trained using
  supervised fine-tuning on a large dataset of chess games collected from open-source
  platforms, with best moves generated using Stockfish evaluations.
---

# Complete Chess Games Enable LLM Become A Chess Master

## Quick Facts
- arXiv ID: 2501.17186
- Source URL: https://arxiv.org/abs/2501.17186
- Reference count: 9
- Primary result: ChessLLM achieves 1788 Elo rating by training on FEN-formatted chess positions with Stockfish-generated best moves

## Executive Summary
This paper presents ChessLLM, a transformer-based language model that plays complete chess games by treating board states as text sequences in Forsyth-Edwards Notation (FEN). The model is trained via supervised fine-tuning on a large dataset of FEN-Move pairs, where moves are generated using Stockfish evaluation at various search depths. The key finding is that data quality, specifically the depth of search used to generate training labels, significantly impacts performance - long-round data supervision provides a 350 Elo rating improvement over short-round data. ChessLLM achieves professional-level performance with a 1788 Elo rating and outperforms other LLMs in chess gameplay.

## Method Summary
ChessLLM uses supervised fine-tuning of open-llama-3B on a 20B+ token dataset of FEN-Best move pairs. The training data combines short-round games from Chessdb (Stockfish depth 12-50) and long-round self-play endgame data (Stockfish depth 50-200). The model learns to predict the best move given a FEN board state through causal language modeling. During inference, the model samples up to 10 times to ensure legal move generation, using temperature=0.7, top_p=0.7, and top_k=50. Performance is evaluated through legal move accuracy, best move accuracy, Elo ratings against Stockfish at various skill levels, and win rates in direct competition.

## Key Results
- Achieves 1788 Elo rating, demonstrating professional-level chess performance
- Wins 61% of games against Stockfish Level 0, 56% at Level 1, and 30% at Level 2 with 10-sample inference
- Long-round data supervision provides 350 Elo rating improvement over short-round data
- Achieves 89.8% win rate against Stockfish Level 0, outperforming ChessGPT-Base (61.3%) and ChessGPT-Chat (59.8%)

## Why This Works (Mechanism)

### Mechanism 1
Converting chess board states into FEN allows the model to treat strategic gameplay as conditional text generation rather than visual or logic-based search. The FEN string provides a complete, serialized description of the game state, enabling the LLM to learn a policy map Ï€(a|s) where state s is represented as text tokens.

### Mechanism 2
Supervision quality, specifically the depth of search used to generate training labels, is the primary driver of Elo rating improvements. Deep search (depth 50-200) generates labels containing aggregated evaluation of millions of future game tree nodes, which the model internalizes without performing search itself during inference.

### Mechanism 3
Inference-time sampling (pass@k) acts as a stochastic search mechanism, significantly improving game outcomes by filtering for move legality and probability. By sampling 10 times and selecting a legal move, the system effectively ensembles multiple policy paths, mitigating the risk of single hallucinated illegal moves.

## Foundational Learning

- **Concept: Forsyth-Edwards Notation (FEN)**
  - Why needed: This is the input modality - a snapshot of the "now" rather than history of moves
  - Quick check: Given a FEN string, can you identify which side is to move and if castling is available?

- **Concept: Behavior Cloning (Imitation Learning)**
  - Why needed: The paper uses Supervised Fine-Tuning (SFT), which is effectively Behavior Cloning
  - Quick check: If the expert makes a mistake, what does the student learn? (Answer: The mistake)

- **Concept: Elo Rating System**
  - Why needed: This is the metric of success - a jump from ~1400 to 1788 represents transition from "knowing rules" to "competent club player"
  - Quick check: If a player with Elo 1788 plays against Elo 1350, who is the heavy favorite? (Answer: The 1788 player)

## Architecture Onboarding

- **Component map:** Input (FEN String) -> Backbone (GPT-like Transformer) -> Head (Language Modeling Head) -> Oracle (Stockfish for training labels)

- **Critical path:** Data curation is the bottleneck - generating long-round dataset requires running Stockfish at depth 50-200, which is computationally expensive

- **Design tradeoffs:** FEN vs PGN - FEN keeps context length constant but loses temporal history; Synthetic vs Human Data - Stockfish ensures high-quality labels but may lack human-like error distributions

- **Failure signatures:** Hallucination (generating illegal moves), Distribution Shift (degraded performance on unseen board states)

- **First 3 experiments:**
  1. Legal Move Baseline: Fine-tune 3B model on short-round data only; verify legal move accuracy reaches 99.11%
  2. Ablation on Depth: Train models on depth-12 vs depth-50 labels; play head-to-head to verify 350 Elo improvement
  3. Temperature Sweep: Run inference with varying temperatures (0.1, 0.7, 1.0) to find optimal balance between exploration and reliability

## Open Questions the Paper Calls Out
None

## Limitations
- Data provenance and scale uncertainty - exact breakdown between short and long-round data not specified
- Evaluation methodology gaps - relies on Stockfish matches rather than human play, creating potential Elo inflation
- Generalization concerns - may overfit to Stockfish's evaluation patterns rather than developing general chess understanding

## Confidence

**High confidence:** The fundamental approach of using FEN-based supervised fine-tuning is sound and technically well-explained

**Medium confidence:** Reported Elo rating of 1788 and win rates against Stockfish are plausible but depend on exact implementation details not fully specified

**Low confidence:** Claims about being first to achieve "professional-level" performance are difficult to verify without standardized benchmarks

## Next Checks
1. **Elo consistency check:** Reproduce model at 0.5B tokens and verify stable performance against Stockfish Level 0 across 100 games
2. **Data depth ablation:** Train models with depth-12 vs depth-50+ labels and play head-to-head to empirically measure the 350 Elo difference
3. **Sampling sensitivity analysis:** Systematically vary sampling parameter k (1, 3, 5, 10, 20) and measure legal move rate and win rate against Stockfish