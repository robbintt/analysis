---
ver: rpa2
title: 'Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation
  in Ad Auctions'
arxiv_id: '2512.03354'
source_url: https://arxiv.org/abs/2512.03354
tags:
- policy
- evaluation
- score
- price
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of Off-Policy Evaluation (OPE)
  in deterministic ad auctions where only the highest-bidding ad is shown, making
  standard OPE methods inapplicable due to zero propensities for non-winning ads.
  The authors propose DPM-OPE, a novel framework that repurposes the Discrete Price
  Model (DPM) from bid landscape forecasting to estimate a market price distribution
  and derive approximate propensity scores for counterfactual evaluation.
---

# Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions

## Quick Facts
- **arXiv ID**: 2512.03354
- **Source URL**: https://arxiv.org/abs/2512.03354
- **Reference count**: 26
- **Primary result**: DPM-OPE achieves 92% Mean Directional Accuracy (MDA) in CTR prediction by repurposing the Discrete Price Model (DPM) from bid landscape forecasting to estimate market price distributions and derive Approximate Propensity Scores (APS) for stable Off-Policy Evaluation (OPE) in deterministic ad auctions.

## Executive Summary
This paper tackles the challenge of Off-Policy Evaluation (OPE) in deterministic ad auctions where only the highest-bidding ad is shown, making standard OPE methods inapplicable due to zero propensities for non-winning ads. The authors propose DPM-OPE, a novel framework that repurposes the Discrete Price Model (DPM) from bid landscape forecasting to estimate a market price distribution and derive approximate propensity scores for counterfactual evaluation. By modeling the auction environment rather than the deterministic policy, they enable use of stable OPE estimators like SNIPS. Evaluated on both the AuctionNet benchmark and a 2-week industrial A/B test, DPM-OPE achieves 92% Mean Directional Accuracy (MDA) in CTR prediction and closely tracks online performance trends, significantly outperforming parametric baselines and offering a validated, scalable alternative to costly online experiments in deterministic auction environments.

## Method Summary
DPM-OPE addresses OPE in deterministic auctions by modeling the stochastic market price distribution rather than the deterministic policy choice. The framework uses the Discrete Price Model (DPM) to estimate the distribution of the highest competing score (market price) per auction segment, then computes Approximate Propensity Scores (APS) as the conditional winning probability for each logged action. These APS scores are used in place of true propensities in a Capped Self-Normalized IPS (SNIPS) estimator, with weights truncated at the 99th percentile to reduce variance. Adaptive quantile binning determines the discretization level per segment based on statistical confidence bounds, balancing bias and variance without manual tuning.

## Key Results
- Achieves 92% Mean Directional Accuracy (MDA) in CTR prediction compared to ground truth on industrial A/B tests
- Capped SNIPS with APS reduces RMSE by 40% compared to parametric baselines (Pareto/Normal distribution assumptions)
- Validated across both AuctionNet benchmark simulations and 2-week industrial A/B test data, with consistent performance trends
- Outperforms standard OPE methods that fail due to zero propensities in deterministic winner-takes-all auctions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling market price distribution enables approximate propensity scores in deterministic auctions where standard IPS fails.
- Mechanism: Instead of directly modeling the deterministic policy choice π(a|x) ∈ {0,1}, DPM-OPE models the competitive environment. The market price z (highest competing score) is stochastic even when the policy is deterministic. By estimating P(z|x) via Discrete Price Model, the framework computes winning probability W(s) = P(s > z) as a continuous propensity proxy, converting "did this ad win?" (binary) into "what's the probability this score wins?" (continuous).
- Core assumption: The market price distribution P(z|x) can be accurately estimated from historical auction data, and winning probability correlates with the logging policy's effective propensity.
- Evidence anchors:
  - [abstract] "repurposing the bid landscape model to approximate the propensity score"
  - [section 4.2] "The winning condition for ad aᵢ is simply sᵢ > z. Since the set of competitors and their scores vary for each auction, the market price z is a random variable."
  - [corpus] Weak direct evidence—neighbor papers address OPE in recommender systems and matching markets with different stochasticity sources; none validate market-price-based propensity approximation.
- Break condition: If market price distribution is highly volatile across segments or poorly estimated, APS introduces systematic bias into downstream OPE.

### Mechanism 2
- Claim: Self-normalization with weight capping stabilizes variance without catastrophic bias.
- Mechanism: Standard IPS suffers from extreme variance when importance weights w = π/π₀ are large (here, when APS is small). SNIPS normalizes by Σw, canceling scale. Capping at 99th percentile further truncates outlier weights that would otherwise dominate the estimate. The trade is bounded bias for substantially reduced variance.
- Core assumption: Capped weights still preserve relative ranking of policies; outliers are noise, not signal.
- Evidence anchors:
  - [section 4.4] "we employ a common technique of capping the importance weights at the 99th percentile of their distribution"
  - [section 6.3.2] "Capped SNIPS significantly reduces the variance by clipping outlier importance weights... lowest RMSE by a large margin"
  - [corpus] No direct validation of capping in neighbor papers; standard OPE variance reduction techniques (e.g., doubly robust) are discussed but not auction-specific.
- Break condition: If true high-value actions systematically have low APS (model mismatch), capping discards real signal and biases estimates downward.

### Mechanism 3
- Claim: Adaptive binning based on statistical confidence bounds minimizes discretization bias.
- Mechanism: DPM discretizes continuous scores into L bins. Too few bins → quantization bias (heterogeneous scores mixed). Too many → high variance (sparse bins). The adaptive rule L★ = ⌊(n/3.84)^(1/3)⌋ ensures each bin's winning-rate estimate meets a target confidence interval width, balancing bias-variance without manual tuning.
- Core assumption: Score distribution is relatively stable within segments (e.g., time periods); quantile bins capture the CDF adequately.
- Evidence anchors:
  - [section 4.2.3] "selecting L = L★ minimizes discretization bias under the variance bound"
  - [section 6.3.1] "adaptive binning achieves the lowest RMSE... validates our choice for robust performance without manual hyperparameter tuning"
  - [corpus] No comparable adaptive discretization methods in neighbor papers; binning strategies for OPE are not discussed.
- Break condition: If score distributions shift rapidly across segments (concept drift), per-segment binning may be insufficiently responsive.

## Foundational Learning

- Concept: Inverse Propensity Scoring (IPS) and importance weighting
  - Why needed here: DPM-OPE builds on IPS but addresses its failure mode in deterministic settings. Understanding why IPS requires non-zero propensity (common support) is prerequisite to appreciating the APS innovation.
  - Quick check question: Given a logged action with propensity 0.1 and reward 1, what's the IPS contribution? (Answer: 1/0.1 = 10, re-weighting to correct under-representation.)

- Concept: Bid landscape forecasting and censored data
  - Why needed here: The paper repurposes DPM from bid optimization. Understanding that losing bids only reveal "market price > my bid" (censoring) clarifies why distribution modeling is non-trivial.
  - Quick check question: In a second-price auction, if you bid 5 and lose, what do you learn about the market price? (Answer: z ≥ 5, exact value unknown → right-censored observation.)

- Concept: Bias-variance tradeoff in OPE estimators
  - Why needed here: The choice of SNIPS + capping is fundamentally a bias-variance decision. Understanding this tradeoff helps diagnose when the method might fail.
  - Quick check question: Why does self-normalization reduce variance but introduce bias? (Answer: Normalizing by random Σw changes expectation; IPS is unbiased, SNIPS is biased but lower variance.)

## Architecture Onboarding

- Component map: Logging Policy -> Historical logs (x, a, r, score) -> Market Price Model (DPM) -> Per-segment P(z) estimation via quantile bins -> APS Calculator -> π_APS(a|x) = h_l where score(a,x) ∈ V_l -> SNIPS Estimator -> Capped importance weighting -> Validation -> Compare OPE estimates vs. online A/B ground truth (MDA, RMSE, Pearson)

- Critical path:
  1. Segment historical data by chosen feature (e.g., time period, publisher)
  2. Compute adaptive L★ per segment using Eq. 12
  3. Fit DPM: estimate winning rates per bin from (score, win/lose) pairs
  4. For each evaluation policy, compute APS for logged actions
  5. Apply Capped SNIPS (Eq. 15) to estimate V(π)
  6. Validate against held-out A/B results before production use

- Design tradeoffs:
  - More segments → better distribution fit but higher compute and sparser bins
  - Higher confidence target (smaller ε) → more bins L → lower bias but higher variance
  - Lower cap percentile → more variance reduction but more bias (risk of discarding signal)
  - Parametric vs. DPM: parametric is faster but assumes unimodal distributions; DPM is flexible but requires more data

- Failure signatures:
  - MDA < 80%: APS model poorly captures true winning probability; check segment heterogeneity
  - Extremely high RMSE with low MDA: weight explosion despite capping; inspect APS distribution for near-zero values
  - Negative Pearson correlation: systematic bias in market price estimation; check for censoring handling errors
  - Disagreement between simulation and real-world results: overfitting to AuctionNet characteristics; validate on multiple data sources

- First 3 experiments:
  1. Sanity check on logging policy: OPE estimate of π₀ should closely match empirical CTR (self-evaluation test).
  2. Ablation on binning: Compare static bins (L=100, 1000) vs. adaptive on held-out validation set; verify adaptive achieves lowest RMSE.
  3. Stress test with synthetic score perturbation: Add noise to evaluation policy scores and verify MDA degrades gracefully (robustness to model uncertainty).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can feature-aware market price modeling (e.g., using Deep Landscape Forecasting) improve OPE accuracy compared to the current score-only Discrete Price Model?
- Basis in paper: [explicit] The authors state the current DPM ignores contextual features and suggest advanced models like DLF or ADM could improve personalization and accuracy.
- Why unresolved: The current implementation relies on aggregated scores, potentially missing fine-grained context that could refine the Approximate Propensity Score.
- What evidence would resolve it: A comparative study integrating context-aware distribution models into the DPM-OPE framework on the AuctionNet benchmark.

### Open Question 2
- Question: Can combining Approximate Propensity Scores with model-based or Doubly Robust estimators significantly reduce variance compared to the Self-Normalized IPS approach?
- Basis in paper: [explicit] The authors note their focus is propensity correction and suggest future work could combine APS with model-based or doubly robust estimators.
- Why unresolved: While Capped SNIPS reduces variance, model-based approaches might offer further stability improvements by incorporating reward estimation.
- What evidence would resolve it: Implementing a Doubly Robust estimator using the derived APS and comparing variance and error metrics against the DPM-SNIPS baseline.

### Open Question 3
- Question: How can the theoretical error propagation from market price distribution misestimation to the final OPE estimate be rigorously quantified?
- Basis in paper: [explicit] The authors acknowledge that accuracy hinges on distribution estimation and call for theoretical study to quantify and control error propagation.
- Why unresolved: The current work relies on empirical validation (A/B tests), but the theoretical bounds of the bias introduced by approximation errors remain undefined.
- What evidence would resolve it: Deriving upper bounds for the estimation error of the DPM-OPE value function relative to the error of the underlying market price model.

## Limitations
- Performance critically depends on accurate market price distribution estimation, with effectiveness under rapid concept drift or extreme score heterogeneity untested
- 99th percentile weight capping lacks theoretical justification specific to deterministic auctions, raising concerns about potential bias introduction
- Framework's behavior under severe data sparsity, highly non-stationary environments, or when the logging policy has systematic coverage gaps is not explored

## Confidence
- **High confidence**: The core mechanism of using market price distribution to derive APS scores is well-grounded in the DPM literature and validated through both simulation (AuctionNet) and real-world A/B test results. The MDA of 92% and close tracking of online performance trends provide strong empirical support.
- **Medium confidence**: The variance reduction through weight capping and the adaptive binning approach are supported by empirical results but lack rigorous theoretical guarantees specific to the deterministic auction setting. The trade-offs between bias and variance are not fully characterized.
- **Low confidence**: The framework's behavior under severe data sparsity, highly non-stationary environments, or when the logging policy has systematic coverage gaps is not explored.

## Next Checks
1. **Concept drift stress test**: Simulate rapid shifts in market price distributions (e.g., through synthetic score perturbations) and measure DPM-OPE's degradation in MDA and RMSE compared to baselines.
2. **Coverage analysis**: Systematically remove or downweight segments of the action space to test how DPM-OPE handles situations where the logging policy has poor coverage of the evaluation policy's action distribution.
3. **Theoretical bounds**: Derive formal bounds on the bias introduced by weight capping and the variance reduction achieved by SNIPS in the context of APS estimation, comparing them to the observed empirical trade-offs.