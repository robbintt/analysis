---
ver: rpa2
title: 'Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot
  and Fine-Tuned Transformers'
arxiv_id: '2512.21709'
source_url: https://arxiv.org/abs/2512.21709
tags:
- text
- bengali
- detection
- ai-generated
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates transformer-based models for detecting
  AI-generated Bengali paraphrases. Zero-shot evaluation showed near-chance performance
  (around 50% accuracy), while fine-tuning significantly improved results.
---

# Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers

## Quick Facts
- **arXiv ID:** 2512.21709
- **Source URL:** https://arxiv.org/abs/2512.21709
- **Reference count:** 23
- **Primary result:** Fine-tuning transformers achieves ~91% accuracy in detecting AI-generated Bengali paraphrases, outperforming zero-shot approaches (~50% accuracy)

## Executive Summary
This study investigates transformer-based models for detecting AI-generated Bengali paraphrases, comparing zero-shot and fine-tuned approaches. Zero-shot evaluation shows all models perform near chance levels (around 50% accuracy), highlighting the need for task-specific fine-tuning. Fine-tuning significantly improves classification performance, with XLM-RoBERTa-Large, mDeBERTaV3-Base, and MultilingualBERT-Base achieving approximately 91% accuracy and F1-score, while IndicBERT shows comparatively weaker performance. The research demonstrates that model capacity and multilingual pre-training outweigh language-specific optimization for this task, and emphasizes the importance of supervised adaptation for effective AI-generated text detection in Bengali.

## Method Summary
The study evaluates five transformer models (XLM-RoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base, MultilingualBERT-Base) on the BanglaTextDistinguish dataset containing 6,640 samples (3,322 human-written + 3,322 GPT-3.5 paraphrased). Models are fine-tuned using Hugging Face Trainer API with cross-entropy loss, AdamW optimizer, and mixed precision (fp16). Training uses early stopping with validation monitoring, and evaluation metrics include accuracy, F1-score, AUROC, and Brier Score. The research compares zero-shot performance against fine-tuned results to assess the impact of supervised adaptation on detection capability.

## Key Results
- Zero-shot models achieve near-chance performance (~50% accuracy), confirming the need for fine-tuning
- Fine-tuned XLM-RoBERTa-Large achieves highest accuracy of 91.50% with AUROC of 96.87%
- mDeBERTaV3-Base closely follows with 91.35% accuracy, demonstrating strong cross-lingual transfer
- IndicBERT shows weakest performance at 74.09% F1-macro score, indicating limited effectiveness of language-specific optimization
- Calibration analysis reveals varying Brier Scores (7.06% for MultilingualBERT to 20.77% for IndicBERT), highlighting reliability differences

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Fine-Tuning Enables Discriminative Feature Learning
Pre-trained transformers learn general linguistic representations but lack discrimination for AI-generated text boundaries. Fine-tuning with labeled examples adjusts classification heads and attention weights to recognize patterns in AI paraphrases, such as stylistic regularities and perplexity-related cues. This mechanism assumes AI-generated paraphrases exhibit learnable patterns that persist across training and test distributions.

### Mechanism 2: Cross-Lingual Transfer from Multilingual Pre-Training
Models with extensive multilingual pre-training (XLM-RoBERTa-Large with 100 languages, mDeBERTaV3-Base, MultilingualBERT-Base with 104 languages) transfer language-agnostic representations to Bengali. These models learn shared semantic-syntactic structures across languages during pre-training, enabling strong performance for the classification task by encoding universal properties of human vs. machine text patterns.

### Mechanism 3: Model Capacity and Pre-Training Scale Outweigh Language-Specific Optimization
IndicBERT's weaker performance despite Bengali-specific training suggests that model scale and diverse pre-training data matter more than language specialization. The performance gap stems from architecture and training differences rather than hyperparameter settings, as IndicBERT's smaller capacity and different pre-training objectives limit its ability to capture subtle statistical differences in generated text compared to larger multilingual models.

## Foundational Learning

- **Transformer Fine-Tuning Paradigm**
  - Why needed here: The methodology depends on understanding how pre-trained models adapt to downstream tasks via supervised learning
  - Quick check question: Can you explain why a model pre-trained on masked language modeling cannot directly classify text without a task-specific head and fine-tuning?

- **Zero-Shot vs. Fine-Tuned Classification**
  - Why needed here: The central finding hinges on the dramatic gap between zero-shot (~50%) and fine-tuned (~91%) performance
  - Quick check question: What does "zero-shot" mean in the context of using XLM-RoBERTa-Large-xnli for Bengali text classification?

- **Evaluation Metrics for Binary Classification (AUROC, F1, Brier Score)**
  - Why needed here: The paper reports multiple metrics to capture discrimination, balance, and calibration quality
  - Quick check question: Why would a model with high AUROC (96.87%) still show poor calibration in its reliability curve?

## Architecture Onboarding

- **Component map:** BanglaTextDistinguish dataset → Model-specific tokenization → Classification head (2 labels) → Fine-tuning with validation monitoring → Evaluation metrics computation
- **Critical path:** Data preprocessing → Model-specific tokenization → Fine-tuning with validation monitoring → Evaluation on held-out test set → Metrics computation (Acc, F1, AUROC, Brier)
- **Design tradeoffs:** Larger models (XLM-RoBERTa-Large) achieve best performance but require more compute; language-specific models (BanglaBERT) underperform large multilingual models; precision-recall tradeoff varies by model
- **Failure signatures:** Zero-shot models with ~50% accuracy and high recall (>90%) → predicting single class; low recall with moderate accuracy → conservative predictions missing positives; high Brier Score → poorly calibrated probabilities
- **First 3 experiments:** 1) Baseline replication: Fine-tune XLM-RoBERTa-Large to verify ~91% accuracy; 2) Ablation on model size: Compare XLM-RoBERTa-Base vs. Large to isolate scale effects; 3) Generalization test: Evaluate on AI-generated text from different sources (GPT-4, Claude, open-source LLMs)

## Open Questions the Paper Calls Out
The paper identifies several future research directions including improving robustness to unseen generation methods, exploring cross-lingual generalization capabilities, and developing more effective calibration techniques for probability estimates.

## Limitations
- Performance evaluated only on GPT-3.5-generated paraphrases, limiting generalization claims to other LLM sources
- Limited analysis of why IndicBERT underperforms despite Bengali-specific optimization
- Poor probability calibration observed in fine-tuned models without proposed remediation strategies

## Confidence

- **High Confidence:** The fundamental finding that fine-tuning dramatically outperforms zero-shot approaches (50% → 91% accuracy) is well-supported by experimental results
- **Medium Confidence:** The ranking of model performances is reliable for tested conditions, though IndicBERT's performance may be influenced by hyperparameter choices
- **Low Confidence:** Claims about cross-lingual transfer mechanisms and superiority of model scale over language-specific training lack direct corpus evidence

## Next Checks

1. **Cross-Model Generalization Test:** Evaluate fine-tuned models on AI-generated Bengali text from multiple sources (GPT-4, Claude, Llama/BLOOM) to quantify out-of-distribution robustness

2. **Controlled Architecture Ablation:** Systematically compare XLM-RoBERTa-Base vs. XLM-RoBERTa-Large with identical hyperparameters to isolate whether performance gains stem from model capacity or multilingual pre-training coverage

3. **Calibration Curve Analysis:** Generate reliability diagrams for all models to visualize probability calibration and identify systematic overconfidence or underconfidence patterns, particularly for IndicBERT's high Brier Score