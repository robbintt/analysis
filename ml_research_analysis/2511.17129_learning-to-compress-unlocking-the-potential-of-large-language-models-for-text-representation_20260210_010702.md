---
ver: rpa2
title: 'Learning to Compress: Unlocking the Potential of Large Language Models for
  Text Representation'
arxiv_id: '2511.17129'
source_url: https://arxiv.org/abs/2511.17129
tags:
- learning
- training
- tasks
- text
- llm2comp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates context compression as a pretext task for
  adapting large language models (LLMs) to text representation tasks. It proposes
  a continuation task with knowledge distillation (CTKD) objective, which outperforms
  other compression-based pretext tasks and is less prone to dimensional collapse.
---

# Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation

## Quick Facts
- arXiv ID: 2511.17129
- Source URL: https://arxiv.org/abs/2511.17129
- Authors: Yeqin Zhang; Yizheng Zhao; Chen Hu; Binxing Jiao; Daxin Jiang; Ruihang Miao; Cam-Tu Nguyen
- Reference count: 25
- One-line primary result: State-of-the-art performance on MTEB across 14 tasks using only 0.36M supervised samples

## Executive Summary
This paper proposes a novel approach to adapt large language models (LLMs) into text representation models by leveraging context compression as a pretext task. The key innovation is the Continuation Task with Knowledge Distillation (CTKD) objective, which trains the model to compress global semantic information into memory tokens that preserve next-token prediction distributions. This approach not only achieves state-of-the-art performance on the MTEB benchmark across 14 diverse tasks but also requires significantly less training data (up to 60% less) than competing methods. The paper demonstrates that CTKD pretraining provides a superior initialization for contrastive learning, leading to better alignment and higher effective dimensionality in the learned representations.

## Method Summary
The method adapts LLMs into text encoders through a three-stage pipeline: (1) CTKD pretraining that compresses context into memory tokens for continuation prediction via KL divergence loss, (2) unsupervised contrastive learning (UCL) with dropout augmentation, and (3) supervised contrastive learning (SCL) with hard negatives. The approach converts causal LLMs to bidirectional attention, adds special memory tokens to the vocabulary, and applies LoRA adapters to key layers. The model learns to generate compact memory tokens that substitute the whole context for downstream sequence prediction, achieving superior performance while mitigating dimensional collapse issues common in other compression-based approaches.

## Key Results
- Achieves 66.78 average score on MTEB benchmark across 14 tasks
- Requires only 0.36M supervised samples versus 1.16M for previous best
- Outperforms all competing methods including LLM2Vec, EBAE, and MNTP
- Demonstrates 60% reduction in supervised training data requirements

## Why This Works (Mechanism)

### Mechanism 1: Context Compression via Knowledge Distillation (CTKD)
The Continuation Task with Knowledge Distillation (CTKD) objective creates superior text representations by training the model to compress global semantic information into memory tokens that preserve next-token prediction distributions. During pre-training, the encoder generates k memory tokens from context, which are then used to predict continuation tokens. The KL divergence loss aligns the prediction distribution from compressed context with that from the full context, forcing the memory tokens to encode sufficient semantic information for continuation. This approach shows 52.49 average score versus 46.22 for NLL-based methods.

### Mechanism 2: Dimensional Collapse Mitigation via Distributional Regularization
CTKD reduces dimensional collapse compared to NLL-based continuation training by acting as a regularizer that preserves information from less frequent tokens. Dimensional collapse manifests when pooled embeddings have low effective rank and memory tokens become highly correlated. The KL divergence loss in CTKD implicitly regularizes the embedding space by requiring distributional matching rather than pointwise token prediction, preserving information across more dimensions. SVD analysis shows CTKD maintains ~100 effective dimensions versus ~10 for NLL methods.

### Mechanism 3: Sample-Efficient Contrastive Learning Synergy
CTKD pre-training provides a stronger initialization for contrastive learning, enabling convergence with ~60% less supervised data than token-level pretext tasks. CTKD implicitly achieves good alignment (semantic similarity), allowing contrastive learning to rapidly improve effective dimensionality by pushing negative samples apart. The synergy arises because CTKD already provides semantically meaningful representations, allowing contrastive learning to focus on dispersion rather than learning alignment from scratch. Models achieve 66.78 average with 0.36M samples versus 66.11 with 1.16M samples for previous methods.

## Foundational Learning

- **Concept: Pretext Tasks in Self-Supervised Learning**
  - Why needed here: Understanding why compression is a valid pretext task requires knowing that pretext tasks create supervision signals from data structure itself. Traditional approaches (MNTP, EBAE) use token-level prediction; this work proposes sequence-level compression.
  - Quick check question: Can you explain why predicting masked tokens might preserve less global semantic information than compressing an entire sequence for continuation prediction?

- **Concept: Dimensional Collapse in Representation Learning**
  - Why needed here: The paper's key insight is that compression objectives can collapse embeddings into low-dimensional subspaces. Understanding this phenomenon (via SVD analysis, rank deficiency, uniformity) is essential for interpreting the experimental results.
  - Quick check question: If an embedding matrix has 4096 dimensions but only 10 non-zero singular values after SVD, what does this imply about the representation capacity?

- **Concept: Alignment vs. Uniformity in Contrastive Learning**
  - Why needed here: The paper frames representation quality through two properties: alignment (similar texts → similar embeddings) and high effective dimensionality (uniform distribution across embedding space). Contrastive learning trades off these objectives.
  - Quick check question: Why might optimizing only alignment (making similar texts close) lead to a collapsed representation where all embeddings become nearly identical?

## Architecture Onboarding

- **Component map:**
  Input Text → Bidirectional Encoder (f_θ with LoRA adapters) → Memory Tokens (k=8 default) → Mean Pooling over memory tokens → Text Embedding z

- **Critical path:**
  1. Convert causal LLM attention to bidirectional (modifying attention mask)
  2. Add k special memory tokens to vocabulary (embedding layer must be fully trainable)
  3. Train encoder with CTKD: encoder produces memory tokens, frozen original LLM provides KL target
  4. Extract embeddings via mean pooling over memory tokens only (not input tokens)
  5. Apply LoRA to q_proj, v_proj, o_proj, and FFN layers (rank=16, alpha=32)

- **Design tradeoffs:**
  - Memory token count (k): k=1-8 stable for most tasks; k>8 harms retrieval. Default k=8 balances information capacity vs. redundancy.
  - Reconstruction vs. Continuation: Reconstruction task provides marginal gains; continuation captures forward semantic coherence better.
  - NLL vs. KL loss: NLL (CT-NLL) is unstable (std=5.32); KL (CTKD) is stable (std=1.37) but requires frozen teacher model.
  - Bidirectional vs. Causal: Bidirectional better for retrieval; Causal better for STS/classification. Trade-off depends on target application.
  - Training data volume: 32K sufficient for CTKD; more data helps CT-NLL more than CTKD.

- **Failure signatures:**
  - High token correlation: Memory tokens become nearly identical (check correlation matrix; if diagonal ≈ 1, tokens redundant). Solution: increase training data or switch to CTKD.
  - Low effective dimension: SVD analysis shows rapid singular value decay. Solution: verify CTKD loss is correctly computing KL divergence, not NLL.
  - Retrieval performance decline during SCL: Over-training beyond 200 steps. Solution: early stopping or dynamic negative sampling.
  - Unstable training: High variance across runs. CT-NLL shows 42.95-51.85 range; CTKD more stable.
  - Reconstruction task fails: LLM2CompRC performs near last-token baseline. Continuation task is required.

- **First 3 experiments:**
  1. Validate CTKD implementation: Train with k=8 memory tokens on 32K Wikipedia samples. Compare LLM2CompKL vs LLM2CompNLL on 2-3 MTEB tasks (e.g., STS17, SciFact). Expect CTKD to match or exceed NLL with lower variance.
  2. Measure dimensional collapse: Extract embeddings from trained model on held-out data (e.g., 1K SciDocsRR samples). Compute SVD of embedding covariance matrix. Plot singular values; expect CTKD to show slower decay than CT-NLL.
  3. Ablate memory token count: Train CTKD variants with k ∈ {1, 4, 8, 16}. Evaluate on retrieval (ArguAna) and classification (Banking77). Expect k=8 optimal, k=16 to hurt retrieval, k=1 to underperform across all tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to different LLM architectures remains unvalidated beyond Llama-2-7b
- Dimensional collapse metric reliability depends on SVD analysis which may not fully capture representation quality
- Static negative sampling in SCL may limit scalability compared to dynamic approaches
- Evaluation is limited to MTEB benchmark which may not represent all real-world embedding use cases

## Confidence
- **High Confidence:** CTKD pretraining outperforms other compression pretext tasks (Table 1) and shows lower variance across runs (std=1.37 vs 5.32 for CT-NLL)
- **Medium Confidence:** Sample efficiency claim (60% less supervised data) is demonstrated on MTEB but relies on specific E5 dataset subset
- **Low Confidence:** The assertion that CTKD "unlocks the potential" of LLMs is aspirational without exploring fundamental limits or comparing against non-LLM state-of-the-art

## Next Checks
1. **Architecture Transfer Test:** Reproduce CTKD pretraining and contrastive learning pipeline on a different LLM architecture (e.g., OPT-1.3b or BLOOM-7b) to validate generalizability. Compare effective dimensionality and MTEB performance against the Llama-2 baseline.

2. **Dynamic Negative Sampling Ablation:** Replace the static hard-negative sampling in the SCL stage with a dynamic strategy (e.g., momentum contrast or learned sampling). Measure the impact on retrieval performance and training stability, particularly for over-trained models.

3. **Domain-Specific Performance Evaluation:** Evaluate LLM2Comp embeddings on a specialized dataset outside MTEB (e.g., biomedical text similarity or legal document clustering). Compare performance against a non-LLM baseline trained on domain-specific data to assess real-world utility beyond general benchmarks.