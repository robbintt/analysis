---
ver: rpa2
title: Semantic Label Drift in Cross-Cultural Translation
arxiv_id: '2510.25967'
source_url: https://arxiv.org/abs/2510.25967
tags:
- cultural
- translation
- label
- greek
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates semantic label drift in cross-cultural
  machine translation, focusing on mental health and irony domains. The core method
  involves translating English datasets into Bengali and Greek using both traditional
  MT tools and modern LLMs, with labels annotated via a Human-LLM collaboration scheme.
---

# Semantic Label Drift in Cross-Cultural Translation

## Quick Facts
- arXiv ID: 2510.25967
- Source URL: https://arxiv.org/abs/2510.25967
- Reference count: 13
- Primary result: Cross-cultural translation induces label drift, especially in nuanced mental health and irony categories, with literal prompting outperforming cultural prompting in label preservation.

## Executive Summary
This study investigates semantic label drift when translating mental health and irony datasets across cultural boundaries. The research demonstrates that while extreme classes (severe depression, non-ironic) are well-preserved, nuanced categories (mild depression, situational irony) frequently drift during translation. The authors introduce a Human-LLM collaborative annotation scheme and find that literal prompting consistently outperforms cultural prompting for label preservation. Cultural similarity between source and target languages mitigates drift in specific domains, particularly irony, but has negligible effects on mental health translations.

## Method Summary
The study translates English mental health (DEPTWEET) and irony (SemEval-2018 Task 3) datasets into Bengali and Greek using six translation systems (Google Translate, NLLB-1.3B, GPT-4.1 Mini, Claude Sonnet 4, Llama-3.3, DeepSeek V3) with both literal and anthropological prompting strategies. Annotations are performed via a Human-LLM collaboration scheme using majority voting across three LLM annotators (GPT-4.1 Mini, Claude Haiku 3.5, DeepSeek V3), with human validation for ambiguous cases. Evaluation metrics include Label Preservation Rate, KL Divergence, Matthews Correlation Coefficient (MCC), and Inter-Annotator Agreement (Fleiss' Kappa, Cohen's Kappa).

## Key Results
- Extreme classes (severe depression, non-ironic) show >80% preservation across all systems
- Nuanced categories (mild depression, situational irony) show severe drift with MCC near zero
- Literal prompting outperforms cultural prompting with mean ΔMCC = +0.0085 (p < 0.01)
- Cultural similarity reduces drift in irony domain (ΔMCC = +0.039, p < 0.05) but not mental health
- Translation refusal rates of 5-10% cluster in minority classes (severe depression ~60%, situational irony ~37%)

## Why This Works (Mechanism)

### Mechanism 1: Culturally Sensitive Domains Induce Label Drift via Semantic Feature Alteration
Translation systems alter semantic labels more frequently in culturally sensitive domains (mental health, irony) than in neutral domains, with nuanced categories drifting more than extreme classes. MT systems preserve surface-level meaning but fail to maintain affective and stylistic cues that carry label-determinative information in sensitive domains.

### Mechanism 2: Cultural Prompting Amplifies Drift by Altering Label-Critical Features
Explicitly instructing LLMs to adopt culturally-situated perspectives increases label drift compared to literal prompting. Anthropological prompting causes LLMs to adapt content for cultural naturalness, which unintentionally modifies the textual features that determine classification labels.

### Mechanism 3: Cultural Similarity Provides Domain-Conditional Drift Mitigation
Cultural proximity between source and target languages reduces label drift, but effects are domain-dependent (stronger for irony, negligible for mental health). For conceptually aligned domains, direct translation preserves ironic intent, while sensitive domains requiring cultural adaptation may introduce additional shift.

## Foundational Learning

- **Label Drift vs. Sentiment Preservation**: The paper distinguishes between prior findings (2-3% degradation in binary sentiment) and their findings (severe drift in nuanced multi-class labels). Understanding this gap is essential for interpreting results.
  - Quick check: Would you expect "I hate this product" to translate with higher label preservation than "I feel a bit off lately"? Why?

- **Anthropological Prompting Strategy**: This is the core intervention tested for cultural alignment. Understanding its structure (6 demographic dimensions) is necessary to replicate or modify experiments.
  - Quick check: If you set Region: Country-Specific but Social Class: Working Class (instead of Middle), would you expect different translation outputs? What label categories might shift?

- **Human-LLM Collaborative Annotation**: The paper's methodology relies on majority voting across 3 LLMs + human validation for ambiguous cases. Understanding inter-annotator agreement metrics is critical for evaluating result reliability.
  - Quick check: If Fleiss' Kappa among LLMs is 0.6 (moderate) but Cohen's Kappa vs. humans is 0.3 (fair), what does this suggest about the annotation pipeline?

## Architecture Onboarding

- **Component map**: Source Dataset (DEPTWEET / SemEval-2018 T3) → Translation Layer [Google Translate | NLLB-1.3B | GPT-4.1 Mini | Claude Sonnet 4 | Llama-3.3 | DeepSeek V3] → Translated Text (Bengali / Greek) → Annotation Layer [GPT-4.1 Mini + Claude Haiku 3.5 + DeepSeek V3] → Majority Vote → Human Validation: Native speakers → Evaluation: Label Preservation Rate, KL Divergence, MCC

- **Critical path**: Translation model selection → Prompting strategy → Annotation agreement threshold. If annotation Fleiss' Kappa < 0.4, escalate to human review before proceeding.

- **Design tradeoffs**: Literal vs. Cultural prompting (Literal better for label preservation; Cultural better for naturalness/misalignment avoidance); Model selection (Traditional MT lacks cultural knowledge but predictable; LLMs encode culture but may refuse sensitive content); Language pair selection (Cultural similarity reduces drift but only in specific domains).

- **Failure signatures**: Translation refusal (content involving sexually explicit material, slang and aggression, gore, and drug-related themes); Cultural misalignment (slang translated as profanity, cultural references lost); Severity inflation (mild/moderate depression systematically shifted toward severe categories); Situational irony collapse (MCC near zero).

- **First 3 experiments**:
  1. Baseline replication: Translate 100 samples from DEPTWEET into a target language using Google Translate (literal) + GPT-4.1 Mini (literal). Compute label preservation rate and MCC. Expect: Severe >80%, Mild/Moderate <60%.
  2. Prompting ablation: Translate same 100 samples with GPT-4.1 Mini using anthropological prompting vs. simplified cultural prompting. Measure ΔMCC. Expect: Full anthropological may show slightly higher drift.
  3. Domain transfer test: Apply same pipeline to culturally neutral dataset (product reviews with 3-class sentiment). Expect: >85% preservation for positive/negative, neutral ~45-50% due to polarity shift.

## Open Questions the Paper Calls Out

- **Can culturally adaptive translation strategies effectively balance semantic label fidelity with cultural appropriateness in sensitive domains?** The paper identifies the trade-off between literal and cultural prompting but does not implement or evaluate any unified adaptive approach.

- **Does the pattern of label drift generalize to other culturally sensitive domains beyond mental health and irony, such as humor, sarcasm, or political discourse?** The study deliberately restricted scope to two domains due to resource constraints.

- **How does translation refusal systematically impact downstream model performance when training on filtered translated datasets?** The paper characterizes refused content but does not quantify the downstream consequences of losing these minority class samples in training data.

## Limitations

- Annotation uncertainty: Inter-annotator agreement (Fleiss' Kappa 0.3-0.6) raises questions about whether observed drift reflects true semantic shifts or inconsistent interpretation of culturally-adapted text.
- Selection bias: Translation refusal rates (5-10%) cluster in minority classes, potentially excluding the most sensitive cases from analysis.
- Cultural mapping oversimplification: The anthropological prompting strategy's six demographic dimensions may not fully capture linguistic expression patterns across domains.

## Confidence

- **High confidence**: Extreme class preservation (severe depression, non-ironic) across all systems and languages; translation refusal patterns in sensitive content; overall advantage of literal over cultural prompting
- **Medium confidence**: Domain-dependent cultural similarity effects (stronger for irony than mental health); nuanced category drift patterns (mild depression, situational irony MCC near zero); severity inflation systematic shift
- **Low confidence**: Attribution of drift to specific linguistic features vs. annotation inconsistency; generalizability to other language pairs beyond Bengali/Greek; effectiveness of anthropological prompting dimensions

## Next Checks

1. **Annotation consistency validation**: Re-annotate 100 randomly selected translated samples using two independent human annotators (blind to source labels). Calculate Cohen's Kappa and compare against LLM inter-annotator agreement.

2. **Cross-linguistic generalization**: Apply identical methodology to Bengali→English and Greek→English translation pairs. Measure whether drift patterns reverse or persist.

3. **Feature importance analysis**: Perform ablation studies removing hypothesized drift-inducing features (emotional adverbs, cultural references, idiomatic expressions) and measure impact on label preservation.