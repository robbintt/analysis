---
ver: rpa2
title: 'The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic
  Networks'
arxiv_id: '2505.17958'
source_url: https://arxiv.org/abs/2505.17958
tags:
- where
- learning
- case
- error
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the high-dimensional asymptotics of empirical\
  \ risk minimization in over-parameterized two-layer neural networks with quadratic\
  \ activations, focusing on the regime with quadratically many samples. The authors\
  \ map the \u21132-regularized learning problem to a convex matrix sensing task with\
  \ nuclear norm penalization, revealing that capacity control emerges from a low-rank\
  \ structure in the learned feature maps."
---

# The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks

## Quick Facts
- arXiv ID: 2505.17958
- Source URL: https://arxiv.org/abs/2505.17958
- Reference count: 40
- This paper analyzes ERM in over-parameterized two-layer quadratic networks, revealing that capacity control emerges from nuclear norm penalization and low-rank feature maps.

## Executive Summary
This paper establishes sharp asymptotic formulas for training and test errors in empirical risk minimization for over-parameterized two-layer neural networks with quadratic activations. By mapping the learning problem to a convex matrix sensing task with nuclear norm regularization, the authors demonstrate that generalization emerges from a low-rank structure in the learned feature maps rather than explicit capacity control. The analysis reveals how the width of the target function governs learnability and identifies the interpolation threshold's location as dependent on the target's rank structure.

## Method Summary
The authors analyze empirical risk minimization on synthetic Gaussian data using gradient descent with random initialization. The setup involves a teacher-student model where the target function is generated by a two-layer network with quadratic activations. The key methodological advance is the equivalence between the ERM problem and a convex matrix sensing problem with nuclear norm penalization. The analysis uses high-dimensional asymptotics where the sample size n, student width m, and teacher width m* scale with input dimension d such that α = n/d², κ* = m*/d, and κ = m/d remain O(1). The main results are derived by solving state evolution equations that capture the behavior at global minima.

## Key Results
- Theorem 1 provides closed-form expressions for training and test errors at global minima, showing how target rank governs learnability
- The interpolation threshold location depends on the target's rank structure rather than just data-to-parameter ratios
- Zero label noise can still yield generalization, revealing that noise-free interpolation doesn't necessarily lead to overfitting
- Low-rank target functions exhibit a fundamental limit on achievable performance

## Why This Works (Mechanism)
The mechanism relies on the equivalence between ERM in quadratic networks and nuclear norm regularized matrix sensing. When quadratic activations are used, the network's output can be expressed as a quadratic form in the input, leading to a matrix sensing interpretation where the learned weights form a low-rank matrix. The nuclear norm regularization emerges naturally from the ℓ2 regularization on the original weights, creating an implicit bias toward low-rank solutions. This structure enables sharp asymptotic analysis using tools from random matrix theory and statistical physics.

## Foundational Learning
- **Random Matrix Theory**: Needed for analyzing spectral properties of large random matrices; check by verifying Marchenko-Pastur law predictions for synthetic data spectra
- **Matrix Sensing**: Understanding how to recover low-rank matrices from linear measurements; check by confirming equivalence between network output and matrix sensing formulation
- **Spin Glass Theory**: Provides mathematical framework for analyzing high-dimensional random systems; check by verifying state evolution equations match predictions from replica analysis
- **Convex Optimization**: Essential for understanding the equivalent convex problem; check by verifying that the nuclear norm penalized formulation is indeed convex

## Architecture Onboarding

**Component Map**: Synthetic data generation -> Teacher network (W*) -> Student network (W) -> Gradient descent optimization -> Test error evaluation -> Fixed-point equation solver

**Critical Path**: W* initialization → Data generation → Student W initialization → GD training → Error computation → Theory comparison

**Design Tradeoffs**: 
- Choice between explicit nuclear norm regularization vs. implicit regularization through ℓ2 penalty
- Balance between student width (generalization vs. computational cost)
- Decision between exact vs. approximate solutions to fixed-point equations

**Failure Signatures**: 
- Non-convergence when κ < 1 or rank constraint is active
- Numerical instability in solving J(a,b) integrals for low-noise regimes
- Discrepancy between theoretical predictions and empirical results when assumptions are violated

**First Experiments**:
1. Generate synthetic data with varying teacher widths and verify Marchenko-Pastur spectral density
2. Train student networks with different widths and compare test errors against Theorem 1 predictions
3. Verify low-rank structure emergence by examining singular value spectra of learned weight matrices

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Analysis is restricted to the high-dimensional limit (α, κ*, κ → ∞ with α, κ*, κ = O(1)), potentially missing finite-sample behaviors
- Results rely on Gaussian initialization and data, limiting applicability to non-Gaussian settings
- The analysis is specific to quadratic activations and may not generalize to other activation functions
- Focuses on ℓ2-regularized case without direct extension to unregularized or differently regularized settings

## Confidence

**Major Claims Confidence**:
- Theorem 1 predictions for test error: High
- Interpolation threshold location: High
- Generalization without label noise: Medium
- Low-rank target function limit: High

## Next Checks
1. Implement the fixed-point equation solver and compare against synthetic experiments with varying α, κ, and noise levels to verify reproduction
2. Test robustness by running experiments with non-Gaussian initialization and data distributions to check universality of results
3. Extend the analysis to ReLU activations and verify whether similar nuclear-norm regularization emerges from the equivalent convex formulation