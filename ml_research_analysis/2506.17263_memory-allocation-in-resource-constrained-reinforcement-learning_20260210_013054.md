---
ver: rpa2
title: Memory Allocation in Resource-Constrained Reinforcement Learning
arxiv_id: '2506.17263'
source_url: https://arxiv.org/abs/2506.17263
tags:
- agent
- memory
- learning
- plan
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines how memory-constrained agents must allocate\
  \ limited memory between building a world model and planning with it. The authors\
  \ formalize this as a trade-off between the memory for transition model estimation\
  \ (Np) and the memory for planning (N\u03C0), where Np + N\u03C0 = N."
---

# Memory Allocation in Resource-Constrained Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.17263
- Source URL: https://arxiv.org/abs/2506.17263
- Reference count: 1
- This paper examines how memory-constrained agents must allocate limited memory between building a world model and planning with it, finding optimal performance when model and planning memory are approximately balanced.

## Executive Summary
This paper investigates the fundamental trade-off between memory allocation for transition model estimation and planning in resource-constrained reinforcement learning agents. The authors formalize this as a memory budget N that must be split between N_p (model memory) and N_π (planning memory). Through experiments with MCTS-based agents in MiniGrid environments and PT-DQN agents in continual learning settings, they demonstrate that optimal performance occurs when these allocations are balanced, with some asymmetry favoring planning. When data quality is poor, agents require more planning resources to compensate for unreliable models. The results show that careful memory allocation can recover near-optimal performance even under severe memory constraints.

## Method Summary
The paper formalizes memory-constrained RL as a budget allocation problem where total memory N must be split between transition model estimation (N_p) and planning (N_π). For MCTS experiments, agents collect streaming transitions D and randomly sample N_p transitions to build an MLE transition model, then run MCTS with N_π nodes. The total memory budget is fixed at N=500. In PT-DQN experiments, agents use a permanent-transient value function decomposition where Q = Q(P) + Q(T), with varying splits between permanent (P) and transient (T) components under the same memory constraint. Experiments are conducted in CorridorEnv (MiniGrid) and Jelly Bean World environments with 20-30 seeds per configuration.

## Key Results
- In episodic MCTS settings, performance peaks when model and planning resources are balanced (N_π ≈ N_p ≈ N/2), with returns dropping when either component dominates
- Poor quality training data shifts the optimal allocation toward more planning capacity to handle noisy transitions
- In continual learning with PT-DQN, a 10-90 split (permanent vs transient value functions) outperformed the proposed 50-50 split under memory constraints
- With N=500 constraint, optimal MCTS allocation achieves ~0.65 return compared to ~0.25 for poor splits, while optimal PT-DQN split achieves ~0.3 reward

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance in memory-constrained MCTS follows an inverted-U curve, peaking when model and planning memory are approximately balanced.
- Mechanism: With fixed memory budget N, allocating too much to model estimation (N_p) leaves insufficient capacity for planning depth (N_π), while excessive planning allocation starves the model of transition data needed for accurate simulation. The optimal split occurs when both components have sufficient resources to contribute meaningfully.
- Core assumption: The environment requires both reasonable model accuracy and non-trivial planning depth to achieve good returns.
- Evidence anchors:
  - [abstract]: "In episodic MCTS settings, performance peaks when model and planning resources are balanced, with returns dropping when either component dominates."
  - [section]: "An interesting trend is that all curves in the two plots (except for the yellow line) showed an inverse U shape. Performance roughly peaks when N_π ≈ 250 ≈ N_p."
  - [corpus]: Weak direct evidence; related work focuses on multi-agent resource allocation rather than internal memory trade-offs.
- Break condition: When N is large relative to environment complexity, the trade-off disappears; both components receive adequate resources regardless of split.

### Mechanism 2
- Claim: Noisy or low-quality training data shifts the optimal allocation toward more planning capacity.
- Mechanism: Random or irrelevant transitions in the dataset create "dead branches" in the MCTS tree that planning must explore and prune. With limited memory, the agent cannot store enough diverse transitions to smooth noise, requiring more planning resources to navigate uncertainty.
- Core assumption: The agent cannot collect additional data after the initial allocation; the plan is computed once.
- Evidence anchors:
  - [section]: "The curves are not symmetric, hinting that the plan may need more resources than the model estimate... If D contains a lot of random transitions, the agent may need more planning capacity to account for 'dead branches' in the MCTS tree."
  - [section]: "The agent cannot recover from data which does not contain at least one full (sub-)optimal trajectory, as the plan is only computed once at the beginning."
  - [corpus]: No directly comparable mechanism found in neighbor papers.
- Break condition: If the dataset contains no trajectory to any goal, no allocation strategy succeeds (demonstrated by yellow line achieving minimum returns).

### Mechanism 3
- Claim: In continual learning with PT-DQN under memory constraints, asymmetric allocation favoring transient value functions (10% permanent, 90% transient) outperforms symmetric 50-50 splits.
- Mechanism: The permanent value function captures cross-task knowledge while the transient function adapts to current task specifics. Under tight memory budgets, over-allocating to permanent storage reduces capacity for task-specific adaptation, which is critical when environments change (reward swaps every 150k steps).
- Core assumption: Task-specific adaptation requires more representational capacity than cross-task knowledge retention in the tested environment.
- Evidence anchors:
  - [abstract]: "In continual learning with DQN-based PT-DQN, a 10-90 split (permanent vs transient value functions) outperformed the proposed 50-50 split."
  - [section]: "The 50–50 PT-split proposed by Anand and Precup, for example, only reaches 0.2 per-step reward, while the best split (yellow line; 10% of the hidden units for the permanent value function) reaches 0.3 reward."
  - [corpus]: Weak evidence; neighbor papers do not address PT-DQN or value function decomposition.
- Break condition: When memory is not meaningfully constrained (N >> 500, original network), all PT-splits achieve similar performance (~0.3 reward), eliminating allocation effects.

## Foundational Learning

- Concept: **Model-Based RL and Transition Models**
  - Why needed here: The paper assumes agents estimate transition dynamics p̂ from data to simulate future states during planning. Without this foundation, the model-vs-planning trade-off is unintelligible.
  - Quick check question: Can you explain why a model-based agent needs both a transition model and a policy?

- Concept: **Monte Carlo Tree Search (MCTS)**
  - Why needed here: MCTS is the primary algorithm tested, where tree nodes consume planning memory (N_π) and stored transitions consume model memory (N_p).
  - Quick check question: What are the four phases of MCTS, and which phase uses the transition model?

- Concept: **Plasticity-Stability Dilemma in Continual Learning**
  - Why needed here: PT-DQN's permanent/transient split addresses this dilemma; understanding why both components are needed clarifies the allocation problem.
  - Quick check question: Why can't a single neural network easily handle both retaining old knowledge and adapting to new tasks?

## Architecture Onboarding

- Component map: Total Memory Budget (N) → [Model Component (N_p)] + [Planning Component (N_π)] → MLE Transition Model from D → MCTS Tree / Q-Network from p̂
  For PT-DQN: N → [Permanent Q(P)] + [Transient Q(T)] where Q = Q(P) + Q(T)

- Critical path:
  1. Define memory budget N based on hardware constraints
  2. Select allocation ratio based on problem characteristics (episodic vs continual, data quality)
  3. Build model component from available data
  4. Compute plan/policy using remaining resources
  5. Evaluate and iterate on allocation if possible

- Design tradeoffs:
  - **High N_p / Low N_π**: Better model accuracy, shallow planning; suited for noisy environments requiring accurate simulation
  - **Low N_p / High N_π**: Deep planning, poor model; suited for clean data with long horizons
  - **Symmetric split**: Reasonable default for MCTS in episodic settings (observed peak ~250/250)
  - **Asymmetric PT-split**: Favored transient (10/90) for continual learning with changing rewards

- Failure signatures:
  - Returns near zero with low N_π: Agent cannot plan far enough to reach any goal
  - Returns drop at high N_π: Model memory too small to store useful transitions
  - Flat minimum returns across all splits: Dataset contains no successful trajectories
  - 50-50 PT-split underperforms in continual learning: Memory constraint is binding

- First 3 experiments:
  1. Replicate the MCTS memory sweep on CorridorEnv with N=500, varying N_π from 0-500. Plot returns to confirm the inverted-U shape and identify the peak allocation.
  2. Test data quality sensitivity: Train on datasets with varying noise ratios (clean trajectories vs random transitions added) and observe how the optimal N_π shifts.
  3. Replicate PT-DQN experiment with N=500 constraint on Jelly Bean World, comparing 10-90, 50-50, and 90-10 splits. Verify that asymmetric splits outperform symmetric under meaningful constraint.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an agent autonomously discover the optimal memory allocation between model learning and planning without external tuning?
- Basis in paper: [explicit] Future directions state: "discovering how the agent can autonomously find the best memory allocation."
- Why unresolved: Current experiments manually sweep all N_p and N_π combinations; no adaptive mechanism was proposed or tested.
- What evidence would resolve it: An algorithm that dynamically adjusts allocations during learning and achieves near-optimal performance across environments.

### Open Question 2
- Question: What constitutes a "unit" of memory across different architectures and representations?
- Basis in paper: [explicit] Future directions call for "defining the nuances of a unit," noting the formalism abstracts from precise definitions.
- Why unresolved: The paper treats tree nodes and transitions as equivalent units without justification; neural network units may not be directly comparable.
- What evidence would resolve it: A formalized unit definition that enables meaningful cross-algorithm comparisons and predicts transfer of optimal allocations.

### Open Question 3
- Question: How does memory allocation interact with partial observability in POMDP settings?
- Basis in paper: [explicit] The introduction states the problem "can easily be extended to POMDPs" but this extension was not explored.
- Why unresolved: POMDPs require additional memory for belief states, potentially creating a three-way allocation trade-off.
- What evidence would resolve it: Experiments showing how optimal N_p, N_π, and belief-state allocations vary in partially observable environments.

### Open Question 4
- Question: Can principled transition selection strategies outperform random sampling when |D| > N_p?
- Basis in paper: [inferred] The agent randomly selects N_p transitions when data exceeds memory; results show noisy datasets degrade performance significantly.
- Why unresolved: Random sampling may discard useful transitions; intelligent selection could preserve goal-relevant information.
- What evidence would resolve it: Comparison of selection strategies (e.g., uncertainty-based, reward-weighted) showing improved returns over random sampling.

## Limitations

- The paper's empirical scope is limited to specific architectures (MCTS, PT-DQN) in controlled MiniGrid and Jelly Bean World environments
- The "memory unit" definition remains ambiguous across components - whether it refers to parameter count, activation memory, or another metric is unclear
- The transition model's estimation procedure (MLE specifics) and MCTS simulation parameters (UCB constant, rollout policy) are underspecified
- The data collection protocols for generating clean vs noisy datasets are not detailed, making reproduction of the data quality experiments uncertain

## Confidence

- **High confidence**: The inverted-U relationship between model and planning allocation in episodic MCTS is well-supported by the empirical results, particularly the peak at N_π ≈ N_p ≈ 250 when N=500
- **Medium confidence**: The mechanism linking data quality to optimal allocation asymmetry (favoring planning) is plausible but relies on assumptions about dead branch pruning that aren't explicitly validated
- **Low confidence**: The PT-DQN results showing 10-90 permanent/transient split superiority are promising but depend on specific implementation details of the value function decomposition that aren't fully specified

## Next Checks

1. Implement resource-constrained MCTS on CorridorEnv and verify the inverted-U curve with peak performance at balanced allocation.
2. Generate datasets with controlled noise levels and measure how the optimal N_π shifts under different data quality conditions.
3. Replicate the PT-DQN experiment with varying permanent/transient splits to confirm that asymmetric allocation outperforms symmetric under memory constraints.