---
ver: rpa2
title: An agentic system with reinforcement-learned subsystem improvements for parsing
  form-like documents
arxiv_id: '2505.13504'
source_url: https://arxiv.org/abs/2505.13504
tags:
- extraction
- document
- documents
- page
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an agentic AI system for extracting alphanumeric
  data from form-like documents such as invoices and purchase orders. The approach
  combines multiple LLM agents and reinforcement learning to improve accuracy over
  monolithic LLM-based systems.
---

# An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents

## Quick Facts
- arXiv ID: 2505.13504
- Source URL: https://arxiv.org/abs/2505.13504
- Authors: Ayesha Amjad; Saurav Sthapit; Tahir Qasim Syed
- Reference count: 39
- One-line primary result: Agentic multi-LLM framework with RL achieves 85%+ exact match on invoices and 96%+ on financial documents, outperforming monolithic LLM baselines.

## Executive Summary
This paper introduces an agentic AI system that uses multiple collaborating LLM agents and reinforcement learning to extract structured alphanumeric data from form-like documents. The approach addresses limitations of monolithic LLM-based extraction by implementing a modular pipeline with iterative learning from errors. The system processes invoices, purchase orders, and financial documents with significant accuracy improvements over traditional single-prompt methods.

## Method Summary
The method employs a multi-agent framework consisting of a document classifier, schema builder, data extractor, evaluator, and meta-prompting agent, supported by two Gymnasium RL agents. The system uses GPT-4o-mini or LLaMA 3.3-70B, with OCR via PaddleOCR and RL training through Vowpal Wabbit contextual bandit. Key innovations include iterative prompt refinement through meta-prompting strategies, parallel processing of multi-page documents, and learning from past extraction errors. The framework achieves dynamic schema generation while maintaining high extraction accuracy across diverse document types and quality levels.

## Key Results
- Exact match accuracy increased from ~43% to over 85% on invoice extraction tasks
- Financial document extraction improved from 30% to 96% exact match accuracy
- Near-perfect performance achieved on high-quality, single-page documents

## Why This Works (Mechanism)
The system's effectiveness stems from its agentic architecture that mimics human-like problem-solving through specialized sub-agents. Each agent handles a specific task - classification, schema building, extraction, and evaluation - allowing for focused optimization. The reinforcement learning component enables the system to learn from extraction errors and refine prompts iteratively, similar to how humans improve through feedback. The meta-prompting agent explores different prompting strategies to find optimal extraction approaches for each document type.

## Foundational Learning
- **Document classification with confidence scoring**: Uses LLM log probabilities to determine document type and confidence levels, critical for routing documents to appropriate extraction strategies
  - Quick check: Verify classification accuracy >95% on benchmark datasets
- **Schema building with perplexity optimization**: Constructs JSON schemas based on document structure while minimizing parsing complexity through RL rewards
  - Quick check: Confirm generated schemas match ground truth field structures
- **Iterative extraction with exploration-exploitation balance**: Employs ε-greedy policy (ε(t) = ε₀ × e^(-δt), δ=0.2) to balance between trying new prompt strategies and refining proven ones
  - Quick check: Monitor reward convergence curves during training

## Architecture Onboarding

**Component map**: Document Reader -> Classifier -> Schema Builder -> Data Extractor -> Evaluator -> Meta-Prompting Agent

**Critical path**: Document Reader → Classifier → Schema Builder → Data Extractor → Evaluator → (back to Data Extractor via Meta-Prompting)

**Design tradeoffs**: Multi-agent modularity provides flexibility and specialization but increases complexity; RL-driven prompt refinement improves accuracy but requires training time; parallel page processing speeds multi-page documents but risks missing cross-page dependencies.

**Failure signatures**: 
- Low OCR confidence scores (<0.8) cascade into extraction failures
- Schema mismatches cause zero exact match scores despite correct data
- Unknown document classifications halt automated processing

**3 first experiments**:
1. Implement baseline single-prompt extraction and measure exact match on SOIRE dataset
2. Add document classifier and verify 95%+ accuracy on known document types
3. Test parallel page processing on multi-page invoices and confirm result concatenation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can multimodal LLMs that process images directly (bypassing OCR) improve extraction accuracy for low-quality scanned documents (≤100 dpi)?
- Basis in paper: The paper states "A potential solution is to bypass OCR and use multimodal LLMs for direct image-based data extraction" and lists this in future work.
- Why unresolved: Current framework relies on paddle OCR, which struggles with low-resolution utility bills (42.5% exact match). Multimodal approaches were not tested.
- What evidence would resolve it: A comparative experiment running the agentic framework with a multimodal LLM (e.g., GPT-4o vision) on the low-quality utility bill subset, measuring exact match and semantic match scores against the OCR-dependent baseline.

### Open Question 2
- Question: Would expanding the RL action space beyond five discrete meta-prompting strategies yield better prompt optimization?
- Basis in paper: Section 6.1 states: "Future studies may explore... adding more learnable parameters for RL policy."
- Why unresolved: The current action space A ∈ {0,1,2,3,4} only includes five strategies (Best Practice, Clarity, Few-Shot, Feedback-Refine, Preservation), which may not cover all optimization pathways.
- What evidence would resolve it: An ablation study with additional meta-prompting strategies (e.g., chain-of-thought decomposition, role-playing) showing statistically significant improvements in cumulative reward and extraction accuracy.

### Open Question 3
- Question: How can the framework automate handling of documents classified as "unknown" without human intervention?
- Basis in paper: Section 6.1 states: "there is no automated way of handling documents classified as 'unknown'" and these are currently flagged for manual review.
- Why unresolved: The classification agent can only categorize into 7 predefined classes; documents outside these categories break the automated pipeline promise.
- What evidence would resolve it: A mechanism (e.g., dynamic schema generation for unknown types, or clustering-based discovery of new document categories) tested on a held-out set of non-standard documents, reporting classification accuracy and extraction quality.

### Open Question 4
- Question: Can schema variability across similar documents be reduced while preserving adaptive extraction benefits for downstream ETL processes?
- Basis in paper: Section 5.2 notes: "Documents with similar layouts but different content may produce different metadata, which is acceptable for extraction but problematic for downstream ETL processes that require uniform structures."
- Why unresolved: Dynamic schema generation improves extraction accuracy but creates inconsistent output structures that complicate database ingestion and analytics pipelines.
- What evidence would resolve it: A schema normalization layer or constrained schema generation approach evaluated on extraction accuracy (maintaining >85% exact match) while achieving >95% structural consistency across similar document types.

## Limitations
- Private evaluation data prevents independent validation of proprietary dataset results
- Schema standardization ambiguity may artificially deflate exact match scores
- OCR dependency creates unrecoverable errors for low-quality scans (<100 dpi)

## Confidence
- **High confidence**: Multi-agent framework architecture and reinforcement learning approach are technically sound and well-described
- **Medium confidence**: Benchmark results on publicly available datasets (SOIRE, CORD) showing 85%+ exact match accuracy
- **Low confidence**: Proprietary dataset results (>96% exact match on financial documents) cannot be independently verified

## Next Checks
1. Replicate benchmark evaluation on SOIRE and CORD datasets using exact match, cosine similarity, and semantic match metrics
2. Test system's robustness to schema variations by running extraction on documents with known ground truth but varied JSON schemas
3. Systematically evaluate extraction accuracy across document quality gradient (100-600 dpi) to quantify performance degradation