---
ver: rpa2
title: Improving Large Language Model Safety with Contrastive Representation Learning
arxiv_id: '2506.11938'
source_url: https://arxiv.org/abs/2506.11938
tags:
- triplet
- harmful
- attack
- loss
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a contrastive representation learning framework
  for improving the robustness of large language models (LLMs) against adversarial
  attacks. The proposed method, termed "triplet defense," formulates model defense
  as a contrastive learning problem by using a triplet-based loss combined with adversarial
  hard negative mining.
---

# Improving Large Language Model Safety with Contrastive Representation Learning

## Quick Facts
- **arXiv ID:** 2506.11938
- **Source URL:** https://arxiv.org/abs/2506.11938
- **Reference count:** 40
- **Primary result:** Triplet-based contrastive learning reduces ASR from 29% to 5% against embedding attacks on Llama 3 8B

## Executive Summary
This paper introduces a contrastive representation learning framework for improving LLM safety against adversarial jailbreak attacks. The method, called "triplet defense," uses a triplet-based loss with adversarial hard negative mining to create separation between benign and harmful representations in the model's embedding space. By operating at the representation level rather than input/output boundaries, the approach generalizes across attack types and input formats while maintaining standard model performance.

## Method Summary
The approach formulates model defense as a contrastive learning problem by training a modified model (M') using a triplet loss on internal residual stream representations. The method uses a frozen original model (M) to provide reference representations for benign (h_b,i) and harmful (h_h,i) prompts. The trainable defense model (M') is initialized with LoRA adapters and optimized to produce new representations (h'_b,i, h'_h,i) that satisfy four properties: benign representations stay close to originals, harmful representations cluster together, and benign/harmful representations are separated. An optional adversarial hard negative mining component generates harder harmful representations by inserting attack modules at random layers and training them to minimize NLL loss.

## Key Results
- Reduces Llama 3 8B ASR from 29% to 5% against embedding attacks
- Eliminates REINFORCE-GCG input-space attack success (0% ASR)
- Maintains capability on GSM8K (only -1.06% drop vs -26.31% for RepBend)
- Improves cross-format generalization compared to existing representation engineering methods

## Why This Works (Mechanism)

### Mechanism 1: Triplet-based Contrastive Representation Separation
The triplet loss enforces four properties simultaneously: new benign representations stay close to original benign representations, new harmful representations are pushed away from original harmful representations, benign and harmful representations are separated from each other, and harmful representations cluster together. This forces the model to map harmful inputs to a compact region far from benign activations, causing generation to "break" into incoherence rather than produce harmful outputs. The core assumption is that harmful behaviors share representational structure across different input formats and attack types.

### Mechanism 2: Adversarial Hard Negative Mining
Mining adversarial harmful representations during training improves robustness against embedding-space attacks by exposing the defense to harder, more informative negative examples. An adversarial attack module is inserted at randomly selected layers and trained via NLL loss to produce harmful representations that closely resemble benign ones. These "hard negatives" are mixed into training (70% original harmful, 30% adversarial harmful), teaching the defense to recognize and disrupt harmful representations that would otherwise evade detection.

### Mechanism 3: Layer-wise Representation Manipulation
Applying the defense objective to internal residual stream representations at multiple layers produces a representation space where harmful activations are structurally disrupted, causing incoherent generation rather than refusals. By operating in the representation space rather than at input/output boundaries, the defense generalizes across input formats that share the same underlying harmful concept.

## Foundational Learning

- **Concept: Triplet Loss in Metric Learning**
  - Why needed here: The paper's core innovation frames safety defense as a metric learning problem. Without understanding how triplet loss enforces relative distances (anchor-positive vs. anchor-negative), the loss formulation in Equations 7-9 will be opaque.
  - Quick check question: Given an anchor a, positive p, and negative n, what happens to the gradient when d(a,p) < d(a,n) - m (where m is margin)?

- **Concept: Representation Engineering for LLMs**
  - Why needed here: The method builds directly on circuit breakers and RepBend. Understanding that these manipulate internal hidden states (rather than inputs/outputs) is essential for grasping why the defense generalizes across attack surface types.
  - Quick check question: Why would editing internal representations generalize better than input filtering or output refusal training?

- **Concept: Adversarial Training and Hard Negative Mining**
  - Why needed here: The adversarial hard negative mining component assumes familiarity with why training on "easy" negatives is insufficient for robustness. The paper adapts this from contrastive learning to the safety domain.
  - Quick check question: In contrastive learning, what makes a negative "hard," and why do hard negatives improve robustness more than random negatives?

## Architecture Onboarding

- **Component map:**
  - Frozen original model (M) -> Provides reference representations h_b,i and h_h,i
  - Trainable defense model (M') -> Modified via LoRA adapters to produce new representations h'_b,i and h'_h,i
  - Adversarial attack modules (Attack_l) -> Linear modules inserted at random layers, trained to generate hard negative representations
  - Loss computation -> Triplet loss with KL divergence regularizer on benign logits
  - Representation extraction -> Mean pooling over harmful representations (ĥ') serves as positive sample for harmful triplet and negative sample for benign triplet

- **Critical path:**
  1. Initialize M' with LoRA adapters; freeze M
  2. Sample batch of benign (x_b) and harmful (x_h) prompts
  3. Forward pass through both M and M' to extract representations at layers 20-31
  4. If adversarial mining enabled: Every 30 steps, train new Attack_l on random layer for K steps; sample 30% adversarial harmful representations
  5. Compute triplet loss: L_benign pushes h'_b away from ĥ', L_harmful pushes h'_h toward ĥ' and away from h_h
  6. Add KL divergence on benign logits; update M' parameters
  7. Repeat for 1100 steps (~7-9 hours on H100 for Llama 3 8B)

- **Design tradeoffs:**
  - **Margin values (m_b=500, m_h=1500):** Larger margins enforce stronger separation but may cause overfitting to training distribution
  - **Layer selection (20-31):** Later layers chosen heuristically; earlier layers may capture different harmful concepts
  - **Adversarial mining ratio (70/30):** Higher adversarial ratio may improve embedding robustness but harm input-space robustness
  - **Distance function (d_mix):** Combines L2 and cosine distance with equal weight

- **Failure signatures:**
  - **Benign capability degradation:** If KL term weight (γ) is too low, benign representations drift, harming benchmarks like GSM8K
  - **Incoherent refusals on edge cases:** If harmful cluster is too tight, model may misclassify benign edge-case inputs as harmful
  - **Embedding attack breakthrough:** If adversarial mining undertrains, embedding ASR will spike

- **First 3 experiments:**
  1. **Reproduce triplet-only defense on Llama 3 8B:** Train for 1100 steps with specified hyperparameters. Evaluate on HarmBench validation set with both input-space and embedding attacks. Target: input-space ASR <5%, embedding ASR <15%.
  2. **Ablate adversarial mining component:** Compare Triplet A3 (full loss, no adversarial mining) vs. Triplet A4 (full loss + adversarial mining). Hypothesis: A4 should show lower embedding ASR but potentially higher input-space ASR.
  3. **Layer sensitivity analysis:** Train separate defenses extracting representations from layers [10-20], [20-31], and [25-31]. Evaluate on both attack types and general benchmarks to determine optimal layer selection.

## Open Questions the Paper Calls Out
- How does the choice of positive and negative samples ($p_{h,i}$ and $n_{b,i}$) impact the effectiveness of the triplet loss defense compared to using the mean of harmful representations?
- What specific architecture-specific adaptations are required to improve the robustness of models like Mistral 7B?
- Can this defense mechanism scale to models with hundreds of billions of parameters without encountering prohibitive computational costs or training instabilities?
- Is there an optimal deterministic strategy for selecting the layer $l$ for adversarial hard negative mining, rather than the current random selection approach?

## Limitations
- The method requires 7-12 hours of training on H100 for Llama 3 8B, potentially limiting scalability to larger models
- The defense generalizes poorly to Mistral 7B, requiring further tuning or architecture-specific adaptations
- The method requires separate models for different architectures, as learned representation spaces may not transfer
- The effectiveness of the learned representation space for novel harmful concepts remains untested

## Confidence
- **High Confidence:** The experimental results showing reduced ASR on both input-space and embedding-space attacks are well-supported by the provided tables (Tables 2-4) and ablation studies.
- **Medium Confidence:** The claim that the method generalizes to out-of-distribution input/output formats is partially supported by the cross-format robustness experiments, but the sample size and diversity of tested formats are limited.
- **Low Confidence:** The assertion that harmful representations share a common structural basis across all attack types is theoretical and lacks direct empirical validation.

## Next Checks
1. **Cross-Architecture Transfer Test:** Apply the trained Triplet Defense from Llama 3 8B to Mistral 7B and a larger model (e.g., Llama 3 70B) without retraining. Evaluate ASR on the same attack suite to determine if the learned representation space transfers across model families.

2. **Adversarial Mining Ablation with Real Attacks:** Remove the adversarial mining component and test the defense against a suite of real-world jailbreak prompts (e.g., from AdvBench or JailbreakBench). Compare ASR to the full Triplet + Adv model to quantify the practical value of hard negative mining.

3. **Layer Sensitivity and Causal Analysis:** Systematically vary the layer range (e.g., [10-20], [20-31], [25-31]) and use causal tracing tools (e.g., Integrated Gradients or Attention Rollout) to identify which layers are most critical for harmful content generation. Validate that the defense's layer selection aligns with these causal pathways.