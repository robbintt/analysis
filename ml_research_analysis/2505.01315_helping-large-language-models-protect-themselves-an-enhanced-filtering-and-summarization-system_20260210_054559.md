---
ver: rpa2
title: 'Helping Large Language Models Protect Themselves: An Enhanced Filtering and
  Summarization System'
arxiv_id: '2505.01315'
source_url: https://arxiv.org/abs/2505.01315
tags:
- arxiv
- prompts
- framework
- research
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a novel defense framework that enables Large
  Language Models (LLMs) to autonomously identify, filter, and protect against adversarial
  or malicious inputs without requiring retraining or fine-tuning. The approach consists
  of two main components: a prompt filtering module using advanced NLP techniques
  including zero-shot classification, keyword analysis, and encoded content detection
  (e.g., base64, hexadecimal, URL encoding) to detect and classify harmful inputs;
  and a summarization module that processes adversarial research literature to provide
  the LLM with context-aware defense knowledge.'
---

# Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System

## Quick Facts
- arXiv ID: 2505.01315
- Source URL: https://arxiv.org/abs/2505.01315
- Authors: Sheikh Samit Muhaimin; Spyridon Mastorakis
- Reference count: 40
- Key outcome: Novel defense framework achieves 98.71% malicious prompt detection and improves jailbreak resistance without LLM retraining

## Executive Summary
This paper presents a novel defense framework that enables Large Language Models (LLMs) to autonomously identify, filter, and protect against adversarial or malicious inputs without requiring retraining or fine-tuning. The approach consists of two main components: a prompt filtering module using advanced NLP techniques including zero-shot classification, keyword analysis, and encoded content detection (e.g., base64, hexadecimal, URL encoding) to detect and classify harmful inputs; and a summarization module that processes adversarial research literature to provide the LLM with context-aware defense knowledge. The framework achieves a 98.71% success rate in identifying malicious patterns, manipulative language structures, and encoded prompts. It also demonstrates improved jailbreak resistance and refusal rates while maintaining response quality. By integrating text extraction, summarization, and harmful prompt analysis, the system significantly increases LLM resilience to hostile misuse, offering an efficient alternative to computationally expensive retraining-based defenses.

## Method Summary
The framework employs a two-stage approach: (1) filtering/detection and (2) context-aware summarization for mitigation. The detection pipeline uses regex matching for encoded content, keyword matching with 550 pre-generated flagged terms derived from the ALERT dataset via TF-IDF and mutual information scoring, and zero-shot classification using `facebook/bart-large-mnli`. When a prompt is classified as malicious, the system extracts keywords, retrieves relevant sentences from 2-3 pre-loaded research papers on LLM attacks, and summarizes them using `facebook/bart-large-cnn`. The summarized context is then injected into the LLM prompt to enhance its ability to recognize and refuse harmful requests. The system achieves high detection accuracy while maintaining response quality, with safe prompts bypassing summarization entirely to minimize latency.

## Key Results
- 98.71% detection accuracy on malicious prompts across multiple datasets
- Jailbreak resistance improved from 0.86 to 0.91 for llama3-70b-8192
- 99.73% detection rate on Babelscape/ALERT dataset
- Safe prompt processing time: 6.98 seconds vs malicious: 114.54 seconds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-stage prompt filtering achieves high malicious input detection through layered heuristic and semantic analysis.
- **Mechanism:** The framework combines three detection layers: (1) pattern matching via regex for encoded content (Base64, hexadecimal, URL encoding), (2) flagged filter-word matching using 550 pre-generated terms derived via TF-IDF and mutual information scoring on the ALERT dataset, and (3) zero-shot classification using `facebook/bart-large-mnli` to categorize prompts as safe/malicious. Each layer catches different attack vectors—syntactic obfuscation, semantic harm indicators, and contextual intent respectively.
- **Core assumption:** Malicious prompts exhibit distinguishable patterns (encoded strings, harmful terminology, manipulative structures) that heuristic + ML combinations can capture without extensive training data.
- **Evidence anchors:**
  - [abstract] "multi-stage prompt analysis including pattern matching, keyword analysis, and zero-shot classification to detect malicious inputs"
  - [section III.A] "generated a list of 550 pre-generated flagged filter-words... Examples include 'bomb,' 'terrorism,' 'suicide,' 'revenge,' 'shooting'"
  - [section IV.5, Table IV] "framework successfully detected 98.71% of malicious prompts... 99.73% on Babelscape/ALERT dataset"
  - [corpus] Related work "Broken-Token" (FMR 0.56) similarly targets obfuscated prompt detection via character-per-token analysis, suggesting this is an active research direction

### Mechanism 2
- **Claim:** Summarized adversarial research literature provides sufficient defensive context for LLMs to refuse harmful requests appropriately.
- **Mechanism:** When a prompt is classified as malicious, the system (1) extracts keywords from the prompt using spaCy noun chunks, (2) retrieves relevant sentences from 2-3 pre-loaded research papers on LLM attacks/defenses, (3) summarizes these sentences using `facebook/bart-large-cnn` via parallelized processing, and (4) injects the summary as context before the LLM generates its response. This context provides the LLM with knowledge about attack patterns and appropriate refusal strategies.
- **Core assumption:** LLMs can leverage in-context knowledge about adversarial attacks to recognize and refuse malicious requests, without requiring weight updates—essentially exploiting the model's existing instruction-following capabilities.
- **Evidence anchors:**
  - [abstract] "leverages adversarial research literature to equip LLMs with context-aware defensive knowledge"
  - [abstract] "By processing only 2-3 research papers to generate defensive context, the system provides an efficient, scalable solution"
  - [section IV.3, Table II] Refusal rate improvements: llama3-70b-8192 (0.04→0.17), gemma2-9b-it (0.00→0.08); jailbreak resistance: llama3-70b-8192 (0.86→0.91)
  - [corpus] Weak corpus evidence—no directly comparable "summarization-as-defense" papers found in neighbors; this appears novel

### Mechanism 3
- **Claim:** Processing safe prompts through minimal pipeline stages while routing malicious prompts through full analysis preserves response quality while adding defense.
- **Mechanism:** Binary classification at the prompt stage creates two pathways: safe prompts (avg. 6.98s) bypass summarization entirely and proceed directly to the LLM; malicious prompts (avg. 114.54s) undergo the full risk analysis, summarization, and contextual query pipeline. This prevents computational overhead for benign requests while ensuring defense where needed.
- **Core assumption:** The zero-shot classifier + filter-word system produces sufficiently few false positives that safe users won't experience unnecessary latency, and sufficiently few false negatives that attacks won't bypass defenses.
- **Evidence anchors:**
  - [section IV.4, Table III] "Safe prompts... average processing time of 6.98 seconds" vs "Malicious... 114.54 seconds"
  - [section IV.3] "helpfulness was relatively small across all models, with llama3-70b-8192 showing the largest increase (0.44 to 0.49)"
  - [section IV.5] "no explicit false positive or false negative tests were carried out... framework places more emphasis on identification of potentially harmful prompts over ensuring absolute accuracy in benign cases"
  - [corpus] "LiteLMGuard" paper addresses similar on-device filtering efficiency concerns for SLMs

## Foundational Learning

- **Concept: Zero-shot classification with NLI models**
  - **Why needed here:** The framework uses `facebook/bart-large-mnli` for classifying prompts as safe/malicious without task-specific training. Understanding how natural language inference models can be repurposed for classification via premise-hypothesis formulation is essential.
  - **Quick check question:** Can you explain how an NLI model trained on entailment/contradiction can classify a prompt as "malicious" without fine-tuning?

- **Concept: TF-IDF + Mutual Information for feature selection**
  - **Why needed here:** The 550 filter-words are generated using TF-IDF scoring followed by mutual information filtering to identify terms that distinguish harmful from safe prompts.
  - **Quick check question:** Why might mutual information be preferred over raw TF-IDF scores for selecting discriminative terms in a classification context?

- **Concept: Parallel processing bottlenecks in NLP pipelines**
  - **Why needed here:** The summarization module uses multiprocessing to process PDF chunks, but shows diminishing returns beyond 32 cores. Understanding Amdahl's law and inter-core communication overhead is critical for deployment decisions.
  - **Quick check question:** Why does the paper observe performance degradation beyond 32 cores for summarization tasks?

## Architecture Onboarding

- **Component map:**
  Input Prompt -> Pattern Matching (regex) -> Filter-word Matching (550 terms) -> Zero-shot Classification -> [SAFE] Direct to LLM (6.98s avg) OR [MALICIOUS] Keyword Extraction -> Relevant Text Extraction from Research Papers -> Parallelized Summarization (BART-large-cnn) -> Context + Prompt -> LLM Response

- **Critical path:** Prompt classification -> (if malicious) keyword extraction -> relevant sentence retrieval -> parallelized summarization -> context injection -> LLM response. The summarization stage is the primary latency bottleneck (~103s of the 114.54s malicious path).

- **Design tradeoffs:**
  - **Speed vs. coverage:** Processing more papers increases context quality but linearly increases latency; paper finds 4 papers optimal
  - **False positives vs. false negatives:** Framework explicitly prioritizes catching malicious prompts over avoiding false positives on benign prompts
  - **Model selection:** `facebook/bart-large-cnn` chosen for speed (264s) over alternatives like T5 (394s) or LED (351s), trading potential quality for efficiency
  - **Static vs. dynamic knowledge:** Pre-loaded research papers provide stable defense but require manual updates for novel attack vectors

- **Failure signatures:**
  - Encoded prompts using schemes not in regex patterns (e.g., ROT13, custom ciphers)
  - Semantic attacks without explicit harmful keywords (explains 12% miss rate on `codesagar` dataset)
  - Context window overflow if summarized papers exceed model limits
  - Foreign language prompts lacking corresponding research literature context

- **First 3 experiments:**
  1. **Baseline calibration:** Run the filter-word + zero-shot classification pipeline against your production traffic distribution to measure false positive rates and latency impact on safe prompts
  2. **Ablation study:** Test detection accuracy with (a) pattern matching only, (b) filter-words only, (c) zero-shot only, (d) full pipeline to quantify each component's contribution
  3. **Context window sizing:** Measure response quality degradation as summarized context length increases (test 1 vs 2 vs 4 vs 6 papers) to validate the paper's 2-3 paper recommendation for your specific LLM

## Open Questions the Paper Calls Out

- **How can real-time monitoring and automated learning be integrated to defend against novel attack vectors not yet present in the summarized research literature?**
  - The authors state the framework "currently operates offline, limiting its capacity to adapt to novel attack techniques in real-time." The system relies on a static corpus of existing research papers, creating a lag between the emergence of new threats and the availability of defensive context.

- **To what extent does the framework's performance degrade when processing multilingual or domain-specific adversarial prompts?**
  - The conclusion identifies "extending the framework to support multilingual and domain-specific models" as a promising avenue for future development. The current methodology relies heavily on English-specific NLP tools and a fixed list of English filter-words.

- **Can the computational overhead of the summarization module be reduced to support seamless, real-time user interactions?**
  - Results indicate that processing malicious prompts takes an average of 114.54 seconds compared to 6.98 seconds for safe prompts, identifying summarization as the primary bottleneck. While parallelization helps, the authors note diminishing returns due to overhead, and ~2 minutes of latency is impractical for many real-time chat applications.

## Limitations

- The framework's reliance on pre-existing research literature for contextual knowledge creates a dependency on the breadth and quality of available literature, with the 2-3 paper limitation potentially missing emerging attack vectors.
- The paper acknowledges that "no explicit false positive or false negative tests were carried out," suggesting the detection pipeline may prioritize catching malicious prompts over ensuring absolute accuracy in benign cases.
- The framework's generalizability to foreign language prompts is explicitly questioned due to the unavailability of corresponding research literature.

## Confidence

- **High confidence:** The multi-stage filtering mechanism (pattern matching + filter-words + zero-shot classification) achieves the reported 98.71% detection rate on established datasets. The latency measurements for safe vs malicious prompt processing paths are directly specified and reproducible.
- **Medium confidence:** The summarization module's effectiveness in improving LLM refusal rates (jailbreak resistance from 0.86→0.91 for llama3-70b-8192) is demonstrated but may be dataset-dependent. The paper doesn't specify which 2-3 research papers were used, introducing potential variability.
- **Low confidence:** The framework's generalizability to foreign language prompts is explicitly questioned ("limitation in handling foreign language prompts due to the unavailability of corresponding research literature"). The impact on user experience from potential false positives remains unmeasured.

## Next Checks

1. **False positive/false negative calibration:** Run the complete detection pipeline against a production-representative sample of 10,000 safe prompts to measure false positive rate, then against 1,000 known malicious prompts from multiple datasets to measure false negative rate. This validates the stated tradeoff between security and usability.

2. **Cross-dataset generalization:** Test the framework on at least two adversarial prompt datasets not used during filter-word generation (e.g., `TrustAIRLab/in-the-wild-jailbreak-prompts` and `codesagar/malicious-llm-prompts-v3`) to measure performance degradation when encountering novel attack patterns.

3. **Summarization context sensitivity:** Conduct an ablation study measuring LLM refusal rates with (a) no summarization context, (b) summarization from 1 paper, (c) 2 papers, (d) 3 papers, and (e) 4+ papers to empirically validate the claimed optimal range of 2-3 papers for context quality vs latency tradeoff.