---
ver: rpa2
title: 'The Majority is not always right: RL training for solution aggregation'
arxiv_id: '2509.06870'
source_url: https://arxiv.org/abs/2509.06870
tags:
- solutions
- solution
- majority
- aggregation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AggLM, a reinforcement learning method that
  trains an LLM to aggregate multiple candidate solutions into a better final answer.
  Unlike standard majority voting or reward model ranking, AggLM learns to reconcile
  and combine solutions using RL from verifiable rewards.
---

# The Majority is not always right: RL training for solution aggregation

## Quick Facts
- **arXiv ID:** 2509.06870
- **Source URL:** https://arxiv.org/abs/2509.06870
- **Reference count:** 26
- **Primary result:** RL-trained AggLM-1.7B achieves 50% accuracy on AIME25 by aggregating 8 solutions, outperforming majority voting (45%)

## Executive Summary
This paper introduces AggLM, a reinforcement learning method that trains an LLM to aggregate multiple candidate solutions into a better final answer. Unlike standard majority voting or reward model ranking, AggLM learns to reconcile and combine solutions using RL from verifiable rewards. On four challenging math competition datasets (AIME24/25, HMMT24/25), AggLM-1.7B achieves 50% accuracy on AIME25 when aggregating eight solutions from Qwen3-1.7B, outperforming majority voting (45%) and reward-model baselines. It generalizes to stronger models like Qwen3-8B and non-thinking modes, and is more token-efficient than generating more solutions. A key insight is that balancing easy and hard examples during training is crucial for strong performance.

## Method Summary
The paper trains AggLM-1.7B using GRPO to aggregate multiple candidate solutions from Qwen3-1.7B into a final answer. The training data consists of DeepScaler (~40K math problems), with 128 solutions sampled per problem using thinking mode at temp=1.5. These are grouped into 16 sets of 8 solutions each. The training mixture includes all "hard" examples (majority answer wrong) plus 50% of "easy" examples (majority correct), yielding 446,220 examples. The model is trained for 1 epoch with binary rewards (1 if aggregated solution matches ground truth via math_verify, else 0). Evaluation is performed on AIME24/25 and HMMT24/25 datasets, where the model generates 4 aggregated solutions per set of 8 candidates, and Pass@1 is computed per set and averaged.

## Key Results
- AggLM-1.7B achieves 50% accuracy on AIME25, outperforming majority voting (45%) and reward model baselines
- Optimal performance requires balancing easy and hard examples during training (5-50% easy ratio)
- The model generalizes to stronger base models (Qwen3-8B) and non-thinking modes
- AggLM is more token-efficient than generating additional solutions

## Why This Works (Mechanism)
The paper does not provide explicit mechanism analysis in the provided text.

## Foundational Learning
- **Reinforcement Learning from Verifiable Rewards (RLVR)**: Learning to generate outputs that maximize a binary reward based on correctness verification, rather than traditional language modeling objectives
  - *Why needed*: Mathematical problems have verifiable ground truth answers, making binary rewards possible
  - *Quick check*: Can you implement a binary reward function using math_verify for a simple math problem?
- **Solution Aggregation**: The task of combining multiple candidate solutions into a single, potentially better answer
  - *Why needed*: Individual solutions may be incomplete or partially correct, but combining them can yield better results
  - *Quick check*: Can you write a prompt that takes multiple solutions and asks for a synthesis?
- **GRPO (Group Relative Policy Optimization)**: A variant of PPO adapted for language models that uses group-based advantage estimation
  - *Why needed*: Enables efficient RL training with multiple candidate solutions per example
  - *Quick check*: Can you explain how group relative advantages differ from individual advantages?

## Architecture Onboarding

### Component Map
DeepScaler dataset -> Solution Sampling (128 per problem) -> Grouping (16 sets of 8) -> Training Data Creation (balanced hard/easy) -> GRPO Training -> AggLM-1.7B -> Evaluation (AIME24/25, HMMT24/25)

### Critical Path
1. Sample 128 solutions per problem from Qwen3-1.7B thinking mode
2. Group into 16 sets of 8 solutions
3. Classify as hard/easy based on majority correctness
4. Create balanced training mixture (all hard + 50% easy)
5. Train AggLM-1.7B with GRPO using binary rewards
6. Evaluate on held-out math competition datasets

### Design Tradeoffs
- **Solution sampling temperature (1.5)**: Higher temperature increases diversity but may reduce individual solution quality
- **Training data balance (hard/easy)**: Too many easy examples reduces challenge; too few makes learning harder
- **Reward granularity**: Binary reward simplifies training but loses partial credit information

### Failure Signatures
- Performance degrades if training uses only hard examples or too many easy examples (4-6 point drop)
- Poor reward computation (answer extraction) prevents learning
- Incorrect grouping or sampling invalidates the aggregation task

### 3 First Experiments
1. Test the binary reward computation on a small set of problems with manual verification
2. Verify the easy/hard classification works correctly on a sample of the training data
3. Check that the aggregation prompt produces reasonable outputs when given sample solutions

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can aggregation skills learned by AggLM be distilled back into the solution model to improve its standalone reasoning capabilities?
- **Basis in paper:** [explicit] The conclusion states "Future work could explore further uses of our aggregator beyond improved final performance, for example for distilling better reasoning skills back into the original solutions."
- **Why unresolved:** The paper only evaluates aggregation performance, not whether the learned synthesis and correction behaviors transfer back to improve base model generation.
- **What evidence would resolve it:** Train a solution model using distilled data from AggLM's outputs and measure pass@1 improvement on held-out reasoning benchmarks.

### Open Question 2
- **Question:** How does the optimal easy/hard training mixture ratio depend on solution model capability and candidate set size?
- **Basis in paper:** [inferred] Table 4 shows 5-50% easy examples work best, but the paper does not explain why this specific range is optimal or whether it varies with model strength or number of solutions k.
- **Why unresolved:** The ablation tests a single model (1.7B) at a single set size (k=8), leaving unclear whether harder models or larger sets require different balancing strategies.
- **What evidence would resolve it:** Sweep the easy/hard ratio across different solution model sizes and k values to identify systematic patterns in the optimal ratio.

### Open Question 3
- **Question:** Does AggLM transfer to domains beyond mathematical reasoning where verifiable rewards exist (e.g., code generation, formal theorem proving)?
- **Basis in paper:** [inferred] All experiments use math competition datasets; the paper does not test whether the learned aggregation skill generalizes across reasoning domains.
- **Why unresolved:** Mathematical reasoning may have unique structure (single numeric answers, clear verification) that facilitates aggregation learningâ€”other domains may present different challenges.
- **What evidence would resolve it:** Train AggLM on code problems with unit test rewards or formal proofs with proof-checker rewards and evaluate cross-domain transfer.

## Limitations
- Missing implementation details for faithful reproduction, particularly thinking mode configuration and answer extraction
- Binary reward signal may be too coarse to capture partial credit or reasoning quality
- Improvement on AIME24 (2.6%) is notably smaller than AIME25 (5%), suggesting sensitivity to dataset difficulty

## Confidence
- **High confidence**: The core finding that AggLM outperforms majority voting and reward model baselines on AIME25, and the importance of balanced training data
- **Medium confidence**: The claimed token efficiency benefits and generalization to stronger models, as these were evaluated on fewer datasets
- **Low confidence**: The specific GRPO implementation details and how answer extraction is handled, which are critical for reproduction

## Next Checks
1. Replicate the easy/hard training split analysis on a held-out validation set to confirm that 5-50% easy examples provides optimal performance across different dataset splits
2. Test whether the binary reward signal is too coarse by implementing a partial credit system based on math_verify's confidence scores or intermediate step correctness
3. Evaluate whether the improvement on AIME25 holds when using a different base model (e.g., Qwen3-8B) with the same aggregation prompt format to test true generalization