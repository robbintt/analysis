---
ver: rpa2
title: Can You Really Trust Code Copilots? Evaluating Large Language Models from a
  Code Security Perspective
arxiv_id: '2505.10494'
source_url: https://arxiv.org/abs/2505.10494
tags:
- code
- vulnerability
- security
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CoV-Eval, a multi-task benchmark for evaluating
  large language models'' code security across four tasks: code completion, vulnerability
  repair, vulnerability detection, and vulnerability classification. The benchmark
  covers 18 vulnerability types in C and Python.'
---

# Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective

## Quick Facts
- arXiv ID: 2505.10494
- Source URL: https://arxiv.org/abs/2505.10494
- Reference count: 18
- Primary result: Most LLMs can detect vulnerabilities effectively but still tend to generate insecure code and struggle with specific vulnerability types and repairs.

## Executive Summary
This paper introduces CoV-Eval, a multi-task benchmark for evaluating large language models' code security across four tasks: code completion, vulnerability repair, vulnerability detection, and vulnerability classification. The benchmark covers 18 vulnerability types in C and Python, using both synthetic and real-world code scenarios. The authors also propose VC-Judge, an improved judgment model for identifying vulnerabilities in LLM-generated code. Evaluations on 20 LLMs reveal that while most models can detect vulnerabilities effectively, they still tend to generate insecure code and struggle with specific vulnerability types and repairs. Proprietary models significantly outperform open-source ones, and the study finds that code-specific fine-tuning and high-quality secure code data improve both security and usability of generated code.

## Method Summary
The authors created CoV-Eval, a benchmark with four tasks (code completion, vulnerability repair, detection, classification) covering 18 CWE vulnerability types in C and Python. They used a GPT-4o-based Vul-Evol framework to synthesize complex code scenarios from seed examples. VC-Judge, a fine-tuned Llama3-8B model, serves as the automated evaluator for security metrics. The benchmark was applied to 20 LLMs, measuring Security Rate (SR@1) for generative tasks and F1/accuracy for discriminative tasks. The authors also explored instruction fine-tuning with secure code data (SC-IFT) to improve model security.

## Key Results
- Proprietary models significantly outperform open-source models on all security metrics
- Most models can detect vulnerabilities effectively but still generate insecure code in completion tasks
- Instruction fine-tuning with secure code data improves both security and usability of generated code
- Specific vulnerability types (CWE-78, CWE-434, CWE-190) remain particularly challenging for most models

## Why This Works (Mechanism)
The evaluation framework works by combining synthetic scenario generation with automated security assessment. The Vul-Evol framework creates controlled, complex test cases that expose model weaknesses in handling realistic security patterns. VC-Judge provides consistent, scalable evaluation that catches vulnerabilities static analyzers miss. The separation of generative and discriminative tasks reveals whether models understand security conceptually versus applying it in practice. Instruction fine-tuning with secure code examples directly addresses the gap between theoretical security knowledge and practical secure code generation.

## Foundational Learning
- **Security Rate (SR@1)**: The proportion of generated code samples that are secure out of the first attempt, measuring immediate security effectiveness.
  - Why needed: Provides a practical metric for evaluating whether LLMs produce secure code on the first try, reflecting real-world developer expectations.
  - Quick check: Compare SR@1 across models and tasks to identify which LLMs and scenarios produce secure code most reliably.

- **Synthetic vs. Real-World Data**: The tradeoff between controlled, synthetic test scenarios and unpredictable real-world codebases for security evaluation.
  - Why needed: Synthetic data enables systematic testing of specific vulnerability types, while real-world data captures complex, contextual security issues.
  - Quick check: Measure performance drop when models move from synthetic (Seed Set) to complex (Vul-Evol) scenarios to assess generalization.

- **Generative vs. Discriminative Evaluation**: Testing whether LLMs can both avoid creating vulnerabilities and identify them in existing code.
  - Why needed: A model that can detect vulnerabilities but generates insecure code has a critical gap in practical security utility.
  - Quick check: Compare detection F1 scores with generation Security Rates to identify models with theoretical vs. practical security understanding.

- **LLM-based vs. Static Analysis Evaluation**: Using fine-tuned models versus traditional tools to assess code security.
  - Why needed: Static analyzers catch known patterns but miss context-dependent vulnerabilities that LLM judges can identify.
  - Quick check: Compare VC-Judge's false negative rate against static analyzers to validate its effectiveness for edge cases.

## Architecture Onboarding

- **Component map:**
  - CoV-Eval Benchmark -> 4 tasks (Code Completion, Vulnerability Repair, Detection, Classification)
  - Vul-Evol Framework -> Synthetic scenario generation from seed examples
  - VC-Judge -> Fine-tuned Llama3-8B security evaluator
  - SC-IFT Data -> Secure code instruction fine-tuning dataset
  - Target LLMs -> 20 models evaluated across all tasks

- **Critical path:**
  1. Dataset Creation: Use Vul-Evol to expand seed code scenarios into more complex test cases
  2. Benchmark Execution: Run inference on target LLMs for the four CoV-Eval tasks
  3. Automated Evaluation: Use the fine-tuned VC-Judge to assess the security of the generated code (Security Rate) and detect/classify vulnerabilities in discrimination tasks
  4. Analysis & Improvement: Analyze results to identify weak vulnerability types. Use the findings to generate high-quality, secure training data (SC-IFT) for further model fine-tuning

- **Design tradeoffs:**
  - Generative vs. Discriminative Evaluation: The paper uses generative tasks (completion/repair) to test if a model writes secure code, and discriminative tasks (detection/classification) to test if it understands security. A tradeoff exists between evaluating functional correctness (usability) and security, which are often assessed separately.
  - Synthetic vs. Real-World Data: The benchmark relies on synthetic scenarios (Vul-Evol) for breadth and control, but this may not perfectly capture all nuances of real-world software vulnerabilities. The study notes 40% of initial GPT-4o-generated scenarios included security features, which had to be manually filtered out, indicating a tradeoff between generation cost and data quality.
  - LLM vs. Static Analysis Evaluator: While VC-Judge outperforms static tools on false negatives, it is not perfect. The paper notes that LLM evaluators still fall short of human experts. There is a tradeoff between the scalability of an LLM judge and the ultimate reliability of a human review.

- **Failure signatures:**
  - High Detection, Low Generation Security: A model achieves high F1 scores on vulnerability detection but has a low Security Rate (SR@1) in code completion. This indicates a gap between recognizing and avoiding vulnerabilities in generation.
  - Inconsistent Task Correlation: A model that performs well on vulnerability classification but poorly on repair, suggesting its knowledge is not actionable for code transformation.
  - Systematic Vulnerability Bias: A model consistently generating code with the same vulnerability types (e.g., CWE-78, CWE-434, CWE-190), indicating a learned bias in its pre-training data or instruction set.
  - Evaluator Misalignment: Security scores from an evaluator differ significantly from human judgments, especially showing a large difference ("Diff.") in security rate percentage, signaling a problem with the evaluator's calibration.

- **First 3 experiments:**
  1. Establish a Baseline: Run a target LLM on the CoV-Eval Seed Set for all four tasks and evaluate its performance using VC-Judge. Report the Security Rate (SR@1) for generative tasks and F1/Accuracy for discriminative tasks.
  2. Test Generalization to Complexity: Run the same model on the Vul-Evol Set (more complex scenarios) and compare the Security Rate to the Seed Set results. A significant drop indicates poor generalization to complex code patterns.
  3. Evaluate Self-Correction Capability: Use the code generated in the completion task as input for the same model's detection and repair tasks. Measure if the model can first identify vulnerabilities in its own code ("Self-detection Recall") and then successfully fix them ("Self-repair SR@1"). This tests the model's practical utility in an iterative development loop.

## Open Questions the Paper Calls Out
- **Open Question 1:** How can code security and usability be effectively integrated into a single, unified automated testing framework for LLMs?
  - Basis in paper: [explicit] The authors state in the Limitations section that current evaluation separates security and usability assessments and express the intent to "explore more reliable and unified automated software testing methods in future work."
  - Why unresolved: Current unit testing for usability suffers from runtime overhead and lacks the test cases necessary for confidence in security bug detection, making a combined approach difficult.
  - What evidence would resolve it: The development of a benchmark or framework that evaluates functional correctness and vulnerability presence simultaneously without the prohibitive costs of current methods.

- **Open Question 2:** What are the optimal data ratios and training methods for combining Secure Code-specific (SC-IFT) and Vulnerability Detection (VD-IFT) instruction tuning data?
  - Basis in paper: [explicit] In Section 7 (Discussion), the authors validate that these data types improve security but explicitly note that "Further exploration of data ratios and training methods will be left for future work."
  - Why unresolved: While the paper demonstrates that high-quality data helps, the specific combination of data volumes and training schedules required to maximize security without sacrificing usability remains unoptimized.
  - What evidence would resolve it: A systematic ablation study showing the performance impact of varying ratios of SC-IFT, VD-IFT, and general instruction data on LLM security and usability metrics.

- **Open Question 3:** How does expanding the benchmark to include more diverse code scenarios and vulnerability types affect the generalizability of LLM security evaluations?
  - Basis in paper: [explicit] The authors note in the Limitations section that CoV-Eval is currently "limited by the diversity of the seed set" and plan to "incorporate more diverse code scenarios, vulnerability types, and task categories."
  - Why unresolved: The current benchmark relies on a seed set of only 54 scenarios, potentially limiting the scope of security assessment for varied real-world applications.
  - What evidence would resolve it: Evaluation results from an expanded benchmark covering more programming languages and CWE types, demonstrating whether current model rankings hold true across a broader domain.

## Limitations
- The synthetic nature of the Vul-Evol dataset may not fully capture the contextual dependencies and multi-faceted interactions present in real-world codebases.
- The benchmark's coverage of 18 CWE types, while comprehensive, may not represent the full spectrum of security vulnerabilities encountered in production environments.
- The evaluation framework's reliance on LLM-based judgment introduces uncertainty, as VC-Judge still exhibits notable false negative rates for complex vulnerability patterns.

## Confidence
- **High Confidence:** Claims regarding proprietary models outperforming open-source models in security metrics (SR@1 scores, F1 scores). The empirical data across 20 models provides strong statistical support.
- **Medium Confidence:** Claims about specific vulnerability types being most challenging (CWE-78, CWE-434, CWE-190). While supported by data, the synthetic nature of test cases may not fully represent real-world difficulty distributions.
- **Low Confidence:** Claims about the long-term effectiveness of instruction fine-tuning with secure code data. The evaluation only demonstrates short-term improvements, and the durability of these gains under continued deployment remains unknown.

## Next Checks
1. **Human Expert Validation Study:** Conduct a blinded evaluation where security researchers assess a subset of model outputs (both proprietary and open-source) using the same CoV-Eval tasks. Compare human judgment agreement rates with VC-Judge predictions to establish ground truth reliability.

2. **Cross-Domain Generalization Test:** Evaluate model performance on vulnerabilities from real-world code repositories (e.g., GitHub Security Lab datasets) to assess whether synthetic benchmark performance translates to practical security assessment capabilities.

3. **Temporal Stability Analysis:** Track model security performance over multiple evaluation periods as models receive updates. This would validate whether the identified security-usability tradeoffs persist or evolve with continued training and whether instruction fine-tuning provides durable improvements.