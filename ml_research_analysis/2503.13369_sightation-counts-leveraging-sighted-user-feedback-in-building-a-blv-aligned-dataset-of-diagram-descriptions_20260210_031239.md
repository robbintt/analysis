---
ver: rpa2
title: 'Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned
  Dataset of Diagram Descriptions'
arxiv_id: '2503.13369'
source_url: https://arxiv.org/abs/2503.13369
tags:
- diagram
- text
- sighted
- dataset
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SIGHTATION, a large-scale dataset for generating
  diagram descriptions that are aligned with the needs of blind and low-vision (BLV)
  users. The authors address the challenge of creating accessible diagram descriptions
  by using a two-pass guided generation approach with vision-language models (VLMs),
  where the first pass generates question-answer pairs that guide the second pass
  to produce more useful descriptions.
---

# Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions

## Quick Facts
- arXiv ID: 2503.13369
- Source URL: https://arxiv.org/abs/2503.13369
- Authors: Wan Ju Kang; Eunki Kim; Na Min An; Sangryul Kim; Haemin Choi; Ki Hoon Kwak; James Thorne
- Reference count: 40
- Introduces SIGHTATION, a large-scale dataset for generating diagram descriptions aligned with BLV user needs

## Executive Summary
SIGHTATION addresses the challenge of creating accessible diagram descriptions for blind and low-vision (BLV) users by leveraging sighted user feedback rather than direct BLV annotation. The dataset employs a two-pass guided generation approach using vision-language models, where initial question-answer pairs guide the generation of more useful descriptions. To reduce costs and avoid annotator bias, sighted users assess descriptions across multiple dimensions including factuality, informativeness, succinctness, and diversity, with BLV educators providing final validation for educational contexts.

The resulting dataset contains 5,000 diagrams and 137,000 samples covering various tasks including completion, preference alignment, retrieval, question answering, and reasoning. Fine-tuning experiments demonstrate significant improvements in BLV-relevant aspects, with a 2B model tuned on SIGHTATION outperforming larger models fine-tuned on other datasets. The retrieval tuning on this dataset achieves 65% higher precision compared to COCO-tuned baselines, making it a valuable resource for advancing accessible visual content generation.

## Method Summary
The paper introduces a novel two-pass guided generation approach using vision-language models to create diagram descriptions aligned with BLV user needs. The first pass generates question-answer pairs that serve as a guided context for the second pass, which produces the final descriptions. To avoid the high costs and potential biases of direct BLV annotation, the authors employ sighted users to assess descriptions across multiple dimensions (factuality, informativeness, succinctness, diversity) rather than generating them. BLV educators then validate the descriptions for educational usefulness. This approach distributes the assessment workload while attempting to capture BLV preferences through multi-dimensional evaluation criteria.

## Key Results
- 2B model tuned on SIGHTATION outperforms 3B model fine-tuned on chart comprehension in 8 out of 11 automatic metrics
- Retrieval tuning on SIGHTATION improves precision by 65% compared to COCO-tuned baseline
- Dataset includes 5,000 diagrams and 137,000 samples covering multiple task types
- BLV teaching professionals validate the dataset's usefulness for educational contexts

## Why This Works (Mechanism)
The approach works by leveraging sighted user assessments to approximate BLV preferences while reducing annotation costs. The two-pass guided generation creates a structured feedback loop where question-answer pairs inform more targeted description generation. The multi-dimensional assessment framework (factuality, informativeness, succinctness, diversity) captures different aspects of description quality that are important for BLV users. By validating with BLV educators, the method grounds the sighted assessments in actual educational needs, creating a scalable pipeline for building BLV-aligned datasets without requiring extensive direct BLV participation.

## Foundational Learning
**Vision-Language Models (VLMs)**: AI models that can process both visual and textual information, essential for understanding diagrams and generating descriptions. Needed because they can bridge the gap between visual content and textual descriptions required by BLV users. Quick check: Can the model correctly identify key elements in a diagram and relate them to each other?

**Two-pass Guided Generation**: A generation strategy where initial outputs inform subsequent generation passes. Needed to create structured, contextually relevant descriptions rather than generic captions. Quick check: Does the second pass incorporate insights from the first pass's question-answer pairs?

**Multi-dimensional Assessment Framework**: Evaluating outputs across multiple quality dimensions rather than a single metric. Needed to capture the complex requirements of BLV-accessible content. Quick check: Are all assessment dimensions (factuality, informativeness, succinctness, diversity) being applied consistently?

**Indirect Preference Elicitation**: Using proxy users to assess preferences when direct user feedback is costly or difficult to obtain. Needed to scale dataset creation while maintaining alignment with target user needs. Quick check: Do sighted assessments correlate with BLV user preferences when validated?

## Architecture Onboarding

Component map: VLMs -> Question-Answer Generation -> Description Generation -> Sighted Assessment -> BLV Educator Validation -> Dataset

Critical path: The core workflow flows from VLMs generating initial Q&A pairs, which guide the description generation, followed by multi-dimensional assessment by sighted users, and final validation by BLV educators. This pipeline ensures that descriptions are both technically accurate and practically useful.

Design tradeoffs: The main tradeoff is between direct BLV user involvement (more accurate but expensive and difficult to scale) versus sighted user assessments (cheaper but potentially less aligned). The paper mitigates this by using multi-dimensional assessment and BLV educator validation. Another tradeoff is between dataset size and quality control - the approach enables large-scale data collection while maintaining quality through structured assessment.

Failure signatures: Potential failures include sighted assessors missing nuances important to BLV users, over-reliance on automatic metrics that may not capture true usability, and descriptions that are technically correct but not pedagogically useful. The approach may also struggle with highly complex diagrams where question-answer generation becomes insufficient for guiding description quality.

First experiments to run:
1. Test the two-pass generation approach on a small set of diagrams to verify that question-answer pairs meaningfully improve description quality
2. Conduct a pilot assessment with sighted users to validate the multi-dimensional rubric and ensure consistent scoring
3. Compare descriptions generated with and without BLV educator validation to measure the impact on educational usefulness

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on sighted annotators may introduce subtle biases despite multi-dimensional evaluation framework
- Dataset focuses specifically on diagrams and may not generalize to other visual content types
- Validation by BLV teaching professionals represents a relatively small sample size

## Confidence

High confidence in the technical methodology and dataset construction approach
Medium confidence in the alignment with BLV user needs due to reliance on sighted assessments
Medium confidence in the evaluation results due to the use of automatic metrics

## Next Checks

1. Conduct direct user studies with diverse BLV learners to evaluate the real-world utility of the generated descriptions in educational settings
2. Test the approach across different diagram types and complexity levels to assess generalizability
3. Compare the sighted-based assessment methodology with direct BLV user feedback to validate the effectiveness of the indirect assessment approach