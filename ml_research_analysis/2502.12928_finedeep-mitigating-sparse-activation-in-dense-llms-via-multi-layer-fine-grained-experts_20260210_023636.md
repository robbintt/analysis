---
ver: rpa2
title: 'Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained
  Experts'
arxiv_id: '2502.12928'
source_url: https://arxiv.org/abs/2502.12928
tags:
- experts
- expert
- finedeep
- activation
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sparse activation in dense large language models,
  where many activation values are close to zero, leading to inefficient use of model
  capacity. The proposed Finedeep method partitions feed-forward network layers into
  fine-grained experts arranged across multiple sub-layers, using a novel routing
  mechanism based on expert outputs with sigmoid normalization instead of softmax
  to avoid competition among experts.
---

# Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts

## Quick Facts
- arXiv ID: 2502.12928
- Source URL: https://arxiv.org/abs/2502.12928
- Reference count: 29
- Primary result: Finedeep achieves significant perplexity improvements and downstream benchmark gains while mitigating sparse activation in dense LLMs

## Executive Summary
Finedeep addresses the fundamental problem of sparse activation in dense large language models, where many activation values are close to zero, leading to inefficient use of model capacity. The proposed method partitions feed-forward network layers into fine-grained experts arranged across multiple sub-layers, using a novel routing mechanism based on expert outputs with sigmoid normalization instead of softmax to avoid competition among experts. Extensive experiments demonstrate Finedeep significantly outperforms traditional dense architectures in perplexity and downstream benchmarks while maintaining comparable parameters and FLOPs.

## Method Summary
Finedeep mitigates sparse activation in dense LLMs by partitioning standard FFN layers into fine-grained experts arranged across multiple sub-layers. The method uses a novel output-guided sigmoid routing mechanism where routing scores are computed from expert outputs rather than inputs, avoiding the competition that softmax-based routing creates. The architecture maintains comparable parameters and FLOPs to dense models while significantly improving performance. Training uses a 100B token mixture from various sources with a LLaMA 3 tokenizer, and optimal performance is achieved with specific depth-width balances (M=2/K=8 or M=4/K=4).

## Key Results
- Finedeep significantly outperforms traditional dense architectures in perplexity and downstream benchmarks
- The approach effectively mitigates sparse activation while maintaining comparable parameters and FLOPs
- Optimal performance achieved when balancing depth and width of expert arrangement (M=2/K=8 or M=4/K=4)

## Why This Works (Mechanism)
The key innovation is the output-guided sigmoid routing mechanism that computes routing scores from expert outputs rather than inputs. This approach avoids the competition among experts that softmax-based routing creates, leading to more diverse and efficient expert utilization. By partitioning FFN layers into fine-grained experts across multiple sub-layers, Finedeep creates a more granular and flexible computation pattern that better utilizes model capacity. The sigmoid normalization ensures that routing scores are bounded and non-competitive, allowing multiple experts to contribute meaningfully to the final output.

## Foundational Learning
- **Sparse activation in dense LLMs**: Many activation values in dense models are close to zero, wasting model capacity - check by measuring NSAR metrics on baseline dense models
- **Expert partitioning**: Dividing FFN layers into smaller expert units - verify by comparing per-expert computation with standard FFN
- **Output-guided routing**: Computing routing scores from expert outputs rather than inputs - implement and compare routing behavior with input-guided approaches
- **Sigmoid vs softmax routing**: Sigmoid avoids competition among experts - test by measuring expert utilization patterns under both mechanisms
- **Multi-layer expert arrangement**: Distributing experts across multiple sub-layers - experiment with different M and K configurations to find optimal balance
- **RMSNorm and residual connections**: Stabilizing training and preserving information flow - monitor training stability and gradient norms

## Architecture Onboarding
**Component map**: Input -> RMSNorm -> Expert partitioning -> Multi-layer sub-layers -> Sigmoid routing -> Output combination -> Residual connections
**Critical path**: Token embeddings → Expert processing → Routing mechanism → Output combination → Final prediction
**Design tradeoffs**: Depth (M) vs width (K) of expert arrangement - deeper stacks need sufficient experts per layer; sigmoid routing vs softmax routing - sigmoid avoids competition but requires careful implementation
**Failure signatures**: Excessive depth degrades performance due to insufficient experts per sub-layer; softmax routing causes increased sparsity and competition among experts
**3 first experiments**:
1. Implement and test sigmoid-based output-guided routing mechanism separately, comparing NSAR metrics with softmax routing
2. Conduct ablation studies varying M and K parameters to identify optimal depth-width balance
3. Measure wall-clock training times and memory usage to validate computational efficiency claims

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of complete training details (batch size, training steps, learning rate scheduling) critical for faithful reproduction
- Practical deployment implications of multi-layer expert arrangement not fully explored
- Computational efficiency claims based on FLOPs equivalence, but wall-clock times and memory overhead not fully characterized

## Confidence
- **High confidence** in architectural design and routing mechanism effectiveness based on clear mathematical formulations and empirical evidence
- **Medium confidence** in sparsity mitigation claims due to unclear comparison methodology and baseline definitions
- **Medium confidence** in computational efficiency claims as FLOPs are maintained but practical deployment aspects are not fully characterized

## Next Checks
1. Implement and test the sigmoid-based output-guided routing mechanism separately to verify it produces different sparsity patterns compared to softmax routing, measuring NSAR_0.1 metrics for both approaches
2. Conduct ablation studies varying the depth-width balance (M and K parameters) to identify the optimal configuration and verify the claimed sweet spot of M=2/K=8 or M=4/K=4
3. Measure and compare wall-clock training times and memory usage between Finedeep and baseline dense models to validate the computational efficiency claims beyond FLOPs equivalence