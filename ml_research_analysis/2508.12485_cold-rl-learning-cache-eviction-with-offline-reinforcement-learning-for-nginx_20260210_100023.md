---
ver: rpa2
title: 'Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX'
arxiv_id: '2508.12485'
source_url: https://arxiv.org/abs/2508.12485
tags:
- eviction
- cache
- cold-rl
- learning
- nginx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cold-RL replaces NGINX's LRU eviction policy with a learned approach
  using a dueling Deep Q-Network (DQN) served by an ONNX sidecar. The system samples
  the K coldest objects from the LRU tail, extracts six lightweight features (age,
  size, hit count, inter-arrival time, TTL remaining, and last origin RTT), and requests
  an eviction bitmask from the policy within a strict 500-microsecond timeout.
---

# Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX

## Quick Facts
- arXiv ID: 2508.12485
- Source URL: https://arxiv.org/abs/2508.12485
- Authors: Aayush Gupta; Arpit Bhayani
- Reference count: 12
- Primary result: 146% hit ratio improvement over LRU under high pressure (25 MB cache)

## Executive Summary
Cold-RL replaces NGINX's LRU cache eviction policy with a learned dueling Deep Q-Network (DQN) that operates within a strict 500-microsecond latency budget. The system samples the K coldest objects from the LRU tail, extracts six lightweight features, and uses an ONNX sidecar to select eviction victims via bitmask. Trained offline on replayed access logs with a +1 reward for retained hits before TTL expiry, Cold-RL achieves significant hit ratio improvements across varying cache pressure levels while maintaining microsecond-level latency and sub-2% CPU overhead.

## Method Summary
Cold-RL implements a dueling DQN architecture trained offline on NGINX access logs replayed through a cache simulator. The system samples K candidates from the LRU tail, extracts six features (age, size, hit count, inter-arrival time, TTL remaining, origin RTT), and requests an eviction bitmask from the DQN via synchronous IPC to an ONNX sidecar with a 500µs timeout. On timeout or error, the system falls back to native LRU. Policies are hot-swappable and use int8 quantization for speed. The reward signal is +1 when a retained object receives a hit before TTL expiry.

## Key Results
- 146% improvement in hit ratio over best classical policy under high pressure (25 MB cache)
- 15% improvement at medium pressure (100 MB cache)
- 95th percentile eviction latency at 498 µs meets 500 µs SLO
- Less than 2% CPU overhead from ML inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Examining only the K coldest objects from the LRU tail captures nearly all viable eviction candidates while bounding computation.
- Mechanism: Instead of evaluating the entire cache, sample K=8-32 objects from the tail, extract features, and let the DQN select victims via bitmask.
- Core assumption: Useful victims predominantly reside in the LRU tail; better candidates rarely exist elsewhere.
- Evidence anchors:
  - [abstract]: "samples the K coldest objects from the LRU tail"
  - [section 4]: "victims almost always lurk in the shadows of the LRU tail. By examining only the coldest K objects (typically 8-32), we achieve bounded O(K) complexity"
  - [corpus]: Limited direct validation in neighboring papers; this is an internal design choice without external corroboration.
- Break condition: If workload exhibits frequent re-access of long-dormant objects (multi-hour gaps), K-tail may miss viable keep decisions.

### Mechanism 2
- Claim: Six lightweight features provide sufficient signal for predicting future object utility within a TTL window.
- Mechanism: For each candidate, compute age, size, hit count, inter-arrival time, TTL remaining, and last origin RTT; feed K×6 matrix to dueling DQN.
- Core assumption: Historical access patterns (frequency, timing) and object metadata predict future hits before TTL expiry.
- Evidence anchors:
  - [abstract]: "extracts six lightweight features (age, size, hit count, inter-arrival time, TTL remaining, and last origin RTT)"
  - [section 6.3]: Ablation shows feature importance—without size (-31% hit ratio), without inter-arrival (-18%), without TTL (-12%)
  - [corpus]: Neighboring papers on KV caching (EVICPRESS, ForesightKV) use related feature sets for LLM contexts, but do not validate this specific six-feature set for HTTP caches.
- Break condition: If origin RTT is unavailable or TTL headers are absent, feature quality degrades; the paper notes BART could handle missing features, but current DQN does not address this explicitly.

### Mechanism 3
- Claim: Offline reinforcement learning on replayed access logs can produce policies that generalize to live traffic.
- Mechanism: Replay NGINX access logs through a high-fidelity cache simulator; assign +1 reward when a retained object is hit again before TTL expiry; train dueling DQN on trajectories.
- Core assumption: Workload patterns in historical logs are representative of future traffic distribution.
- Evidence anchors:
  - [abstract]: "Policies are trained offline by replaying NGINX access logs through a cache simulator"
  - [section 5.1]: "Training Pipeline transforms raw logs into trained policies through ETL, high-fidelity simulation, trajectory generation, dueling DQN training"
  - [corpus]: Related work (LRB, HALP) uses similar offline/simulation approaches at millisecond scales; Cold-RL extends to microsecond budgets.
- Break condition: Sudden workload distribution shifts (e.g., new content types, attack patterns) require retraining; cold-start needs 24-48 hours of logs.

### Mechanism 4
- Claim: A hard timeout with deterministic fallback ensures ML inference never violates latency SLOs.
- Mechanism: 500 µs timeout on synchronous IPC to ONNX sidecar; on timeout or error, immediately fall back to native LRU.
- Core assumption: The fallback policy (LRU) provides acceptable baseline performance and is always available.
- Evidence anchors:
  - [abstract]: "strict 500-microsecond timeout triggers immediate fallback to native LRU"
  - [section 6.2]: "Fallback Rate 0.02% (primarily during spikes)"; p95 total eviction latency at 498 µs meets SLO
  - [corpus]: Related learned caching systems (HR-Cache, HALP) operate at millisecond scales without strict microsecond fallback guarantees.
- Break condition: If fallback triggers frequently during sustained high load, the system effectively reverts to LRU behavior.

## Foundational Learning

- Concept: Dueling Deep Q-Networks (DQN)
  - Why needed here: The eviction decision is discrete (keep/evict per candidate); dueling architecture separates state value from action advantages, stabilizing learning when many actions are similarly poor under cache pressure.
  - Quick check question: Can you explain why V(s) + (A(s,a) - mean(A)) is more stable than raw Q(s,a) when most eviction candidates are equally bad?

- Concept: Cache Eviction Policies (LRU, LFU, ARC)
  - Why needed here: Understanding baseline failure modes—LRU is size-blind and periodicity-blind; LFU over-weights stale popularity; ARC adapts via fixed rules—clarifies what the learned policy must overcome.
  - Quick check question: Under what workload pattern does LRU "catastrophically thrash," and why would a learned policy handle it better?

- Concept: Offline Reinforcement Learning / Log Replay
  - Why needed here: Training happens on historical logs, not live traffic; requires reconstructing state-action-reward trajectories from access logs via simulation.
  - Quick check question: How would you construct a reward signal from NGINX access logs that reflects hit-ratio optimization?

- Concept: Production ML Safety Patterns
  - Why needed here: The system includes hard timeouts, circuit breakers, shadow mode, and kill switches—standard for ML-in-infrastructure but critical to understand before deployment.
  - Quick check question: What should happen if the ONNX sidecar crashes mid-inference request?

## Architecture Onboarding

- Component map: NGINX Worker Process -> Cold-RL Module -> Unix Domain Socket -> ONNX Sidecar
- Critical path:
  1. Cache full -> NGINX calls forced_expire hook
  2. Module samples K-tail candidates (typically K=16)
  3. Extract 6 features per candidate -> K×6 matrix (192 bytes for K=8)
  4. Serialize and send synchronous IPC with 500 µs deadline
  5. ONNX inference (p50: 127 µs, p95: 342 µs) -> returns eviction bitmask
  6. Apply bitmask OR fall back to LRU on timeout/error

- Design tradeoffs:
  - K=16 optimal: 86.8% hit ratio at 234 µs inference vs K=32 at 87.0% at 456 µs
  - 10K parameters: fits L2 cache, prioritizes speed over capacity
  - Offline-only training: eliminates online risk but requires retraining cycles for workload shifts

- Failure signatures:
  - Inference timeout (>500 µs) -> automatic LRU fallback
  - Invalid/null features -> handled by null checks in module
  - Consecutive failures -> circuit breaker disables learned policy
  - Sidecar crash -> module falls back immediately; kill switch for full disable

- First 3 experiments:
  1. Shadow mode deployment: Log counterfactual decisions without applying them; compare would-be hit ratios against actual LRU.
  2. Offline training on 24-48 hours of production access logs: Run simulator, train dueling DQN, export int8 ONNX, validate on held-out log segment.
  3. Canary rollout with 5% traffic: Monitor p95 eviction latency, fallback rate, and hit-ratio delta; escalate percentage if SLOs hold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can gradient boosting models achieve sub-50µs inference latency while offering better interpretability than the current dueling DQN?
- Basis in paper: [explicit] Section 7.2 suggests gradient boosting as a "compelling alternative" that could unlock sub-50µs inference and enforce monotonicity constraints.
- Why unresolved: The current implementation uses DQN; shifting to gradient boosting requires reformulating the sparse reward signal into dense supervision for regression.
- What evidence would resolve it: Benchmarks comparing LightGBM/XGBoost inference speed and hit ratios against the DQN baseline under the 500µs SLO.

### Open Question 2
- Question: Can federated learning across edge locations mitigate cold-start issues and improve resilience to sudden workload shifts?
- Basis in paper: [explicit] Section 7.4 proposes exploring "federated learning across edge locations" to handle the limitation that cold-starts require 24–48 hours of local logs.
- Why unresolved: The paper notes sudden workload shifts currently require retraining cycles; it is unknown if federated weight updates can occur fast enough to be effective.
- What evidence would resolve it: A federated training loop that converges on new traffic patterns faster than the current offline retraining pipeline.

### Open Question 3
- Question: Does the Cold-RL decision framework generalize to other microsecond-scale systems tasks like memory allocation or packet scheduling?
- Basis in paper: [explicit] Section 7.4 claims the techniques could "generalize beyond caching to any system requiring microsecond decisions."
- Why unresolved: The architecture is specialized for cache eviction features (e.g., TTL, hit count); different tasks require different state representations and reward signals.
- What evidence would resolve it: Successful integration of the ONNX sidecar architecture into a memory allocator or network scheduler with maintained latency bounds.

## Limitations

- Offline-only training requires 24-48 hours of logs for cold-start and retraining when workload distributions shift
- Feature quality degrades if origin RTT is unavailable or TTL headers are absent
- Performance under diverse workloads beyond NASA access logs is not validated

## Confidence

- High confidence: The microsecond latency constraints and fallback mechanism design
- Medium confidence: The 146% hit ratio improvement under high pressure
- Low confidence: Generalization across diverse workloads

## Next Checks

1. **Distribution shift sensitivity**: Conduct experiments where the model trained on one workload type (e.g., NASA logs) is evaluated on a different workload type (e.g., media streaming or API request logs) to quantify performance degradation.

2. **Fallback rate under sustained load**: Measure how frequently the 500µs timeout triggers during prolonged high-pressure scenarios (multi-minute sustained cache full states) to validate the circuit breaker behavior.

3. **Feature robustness testing**: Systematically remove or corrupt each feature (TTL, RTT, inter-arrival time) in a controlled test environment to verify the stated ablation results (-31% without size, -18% without inter-arrival, -12% without TTL) and assess graceful degradation.