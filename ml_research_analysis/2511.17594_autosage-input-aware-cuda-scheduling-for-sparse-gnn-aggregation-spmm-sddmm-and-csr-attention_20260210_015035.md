---
ver: rpa2
title: 'AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM)
  and CSR Attention'
arxiv_id: '2511.17594'
source_url: https://arxiv.org/abs/2511.17594
tags:
- baseline
- autosage
- spmm
- sddmm
- guardrail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoSAGE addresses the performance variability of sparse GNN aggregations
  (CSR SpMM/SDDMM) on GPUs due to degree skew, feature width, and micro-architecture
  differences. The method introduces an input-aware CUDA scheduler that selects optimal
  tiling and mapping strategies per input using lightweight estimates refined by on-device
  micro-probes, with a guardrail to safely fall back to vendor kernels and persistent
  caching for deterministic replay.
---

# AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention

## Quick Facts
- arXiv ID: 2511.17594
- Source URL: https://arxiv.org/abs/2511.17594
- Reference count: 26
- AutoSAGE achieves up to 4.7× kernel-level speedups on synthetic stress tests with degree skew or extreme sparsity

## Executive Summary
AutoSAGE introduces input-aware CUDA scheduling for sparse GNN aggregations (CSR SpMM/SDDMM) and CSR attention, addressing performance variability caused by degree skew, feature width, and micro-architecture differences. The system uses a hybrid approach combining lightweight estimates, on-device micro-probes on small subgraphs, and a guardrail to safely fall back to vendor kernels. A persistent cache enables deterministic replay across runs. On real datasets Reddit and OGBN-Products, AutoSAGE matches vendor baselines at bandwidth-bound feature widths and achieves gains at smaller widths; on synthetic stress tests, it achieves up to 4.7× kernel-level speedups.

## Method Summary
AutoSAGE schedules sparse GNN operations (SpMM and SDDMM) by first extracting graph features (nonzeros, degree quantiles, feature width), then using a roofline-style estimate to shortlist candidate kernels. Top candidates are timed on an induced subgraph (2-3% of rows, minimum 512) to predict full-graph performance. A guardrail accepts only kernels that are faster than the baseline (cuSPARSE/gather-dot) by a threshold (α=0.95). Hub-aware splits assign heavy rows to dedicated CTAs to improve load balance under degree skew. Decisions are cached persistently for deterministic replay across runs.

## Key Results
- Matches vendor baselines at bandwidth-bound feature widths on Reddit and OGBN-Products
- Achieves 4.7× kernel-level speedups on synthetic stress tests with degree skew or extreme sparsity
- Covers SpMM and SDDMM operations and composes into a CSR attention pipeline

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Estimate→Micro-probe→Guardrail Selection
- Claim: Input-aware kernel selection outperforms static kernels when sparse structure and feature width vary.
- Mechanism: Extract graph features, shortlist candidates via roofline-style estimate, then time top-k candidates on an induced subgraph. Accept only if `t_star ≤ α × t_baseline`.
- Core assumption: Subgraph timing correlates with full-graph relative performance; baseline is well-optimized for bandwidth-bound regimes.
- Evidence anchors: [abstract] "chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes"; [Section 4.2] describes full decision pipeline; [corpus] Related work confirms sparse operator performance varies with input structure.
- Break condition: When subgraph is unrepresentative or probe noise exceeds α margin, guardrail forces fallback.

### Mechanism 2: Hub-aware CTA-per-hub Split for Skewed Degrees
- Claim: Assigning one CTA per heavy row improves load balance under degree skew.
- Mechanism: Detect rows exceeding `hubT` threshold; dispatch these to a dedicated "hub" kernel with CTA-per-row mapping, while normal rows use warp-per-row.
- Core assumption: Degree distribution has identifiable "hub" rows; overhead of split dispatch is amortized.
- Evidence anchors: [abstract] "hub-aware splits" mentioned as key contribution; [Table 10] shows 2.2×–3.4× speedups with split vs. baseline; [corpus] Sparsity-Aware Communication notes SpMM is sensitive to load imbalance.
- Break condition: Uniform degree distributions yield no benefit; split overhead may slightly regress.

### Mechanism 3: Persistent Op-aware Cache with Deterministic Replay
- Claim: Caching kernel decisions eliminates probe overhead after warm-up.
- Mechanism: Key cache by `(device_signature, graph_signature, F, op)`. On cache hit, replay chosen kernel directly; on miss, run probe and store result.
- Core assumption: Same (device, graph, F, op) tuple produces stable optimal choice across runs.
- Evidence anchors: [abstract] "persistent cache for deterministic replay"; [Section 8.6] "probe overhead is ~6–9% of a full-graph iteration" uncached; "steady-state replay is near-zero overhead."
- Break condition: Driver/CUDA updates or hardware changes invalidate cache; schema encodes version minors to mitigate.

## Foundational Learning

- Concept: **CSR (Compressed Sparse Row) Format**
  - Why needed here: All kernels operate on CSR matrices; understanding `rowptr`, `colind`, `val` is prerequisite for reading kernel templates.
  - Quick check question: Given `rowptr = [0, 2, 5]` and `colind = [1, 3, 0, 2, 4]`, how many nonzeros are in row 1? (Answer: 3)

- Concept: **SpMM vs. SDDMM Semantics**
  - Why needed here: AutoSAGE covers both; SpMM computes `C = A × B` (sparse × dense), SDDMM computes attention scores at sparsity pattern of A.
  - Quick check question: Which operation would you use to compute attention weights for a sparse graph? (Answer: SDDMM)

- Concept: **GPU Execution Hierarchy (Warp, CTA, SM)**
  - Why needed here: Kernel variants differ by mapping (warp-per-row vs. CTA-per-hub); understanding thread granularity is essential.
  - Quick check question: If a row has 10,000 neighbors, why might warp-per-row cause load imbalance? (Answer: Warps must synchronize, creating tail latency)

## Architecture Onboarding

- Component map: Python bindings → Scheduler module → Kernel selection → CUDA kernels (spmm_rows.cu, spmm_hub.cu, sddmm_csr.cu) → GPU execution

- Critical path:
  1. Input arrives (CSR + dense B, feature width F)
  2. Check cache for `(device, graph_hash, F, op)`
  3. If miss: extract features → estimate → probe top-k on subgraph → guardrail check → cache decision
  4. Dispatch chosen kernel or fallback to cuSPARSE/gather-dot

- Design tradeoffs:
  - Larger probe fraction → more accurate but higher overhead; default 2–3% balances cost
  - Lower α (e.g., 0.90) accepts more speculative wins; higher α (0.98) is conservative
  - vec4 requires `F % 4 == 0` and 16B alignment; helps on ER synthetics (+4–20%) but hurt on Reddit@64

- Failure signatures:
  - Probe variance causes guardrail to reject valid wins → reduce `AUTOSAGE_PROBE_FRAC` or tighten cap
  - vec4 regressions on misaligned data → check alignment before enabling
  - Cache invalidation across CUDA updates → verify `.meta.json` sidecar matches runtime

- First 3 experiments:
  1. Reproduce Tables 2–3 (Reddit/OGBN-Products) to validate guardrail behavior at F∈{64,128,256}
  2. Run synthetic ER stress test (N=200k, p=2×10⁻⁵) to confirm ~4.7× speedup claim
  3. Ablate `AUTOSAGE_HUB_T` on hub-skewed synthetic to find optimal split threshold for your graph distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Subgraph representativeness: Micro-probe relies on small subgraphs (2-3% of rows) that may be structurally unrepresentative of full graph's sparsity pattern
- Cache invalidation: Persistent cache assumes deterministic behavior across runs; CUDA/driver updates could silently invalidate decisions
- Baseline specification gaps: SDDMM baseline described as "gather-dot" without full implementation details affecting measured speedups

## Confidence
- **High confidence**: Guardrail mechanism effectiveness (Section 4.2), hub-aware split benefits on skewed datasets (Table 10), and cache overhead reduction (Section 8.6)
- **Medium confidence**: Input-aware scheduling superiority claims (Tables 2-3) depend on exact baseline implementation details
- **Low confidence**: Long-term cache stability across CUDA/driver updates and generalization to graphs with localized community structure

## Next Checks
1. **Guardrail sensitivity analysis**: Vary `AUTOSAGE_PROBE_FRAC` (0.01, 0.02, 0.05) and `α` (0.90, 0.95, 0.98) on Reddit @ F=64 to quantify trade-off between probe overhead and rejection rate
2. **Subgraph representativeness test**: Run micro-probes on multiple random subgraphs (2-3% each) of same graph and measure variance in predicted optimal kernel
3. **Cache invalidation scenario**: Test cache replay after upgrading CUDA version or on different GPU models with identical compute capability to verify schema versioning