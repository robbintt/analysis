---
ver: rpa2
title: Challenges in interpretability of additive models
arxiv_id: '2504.10169'
source_url: https://arxiv.org/abs/2504.10169
tags:
- additive
- functions
- feature
- shape
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines the interpretability claims of generalized
  additive models (GAMs) and their neural variants (NAMs). While these models are
  often presented as "transparent" or "interpretable" alternatives to black-box methods,
  the authors demonstrate that nonidentifiability issues can undermine their interpretability.
---

# Challenges in interpretability of additive models

## Quick Facts
- arXiv ID: 2504.10169
- Source URL: https://arxiv.org/abs/2504.10169
- Authors: Xinyu Zhang; Julien Martinelli; ST John
- Reference count: 11
- Primary result: Nonidentifiability issues in GAMs/NAMs undermine interpretability claims

## Executive Summary
This paper critically examines the interpretability claims of generalized additive models (GAMs) and their neural variants (NAMs). While these models are often presented as "transparent" or "interpretable" alternatives to black-box methods, the authors demonstrate that nonidentifiability issues can undermine their interpretability. The paper identifies two main sources of nonidentifiability: the inability to observe individual shape functions separately, only their sum, and concurvity (nonlinear correlation) among covariates. These issues can lead to multiple models with equivalent predictive performance but different interpretations of feature effects.

## Method Summary
The paper uses synthetic experiments to demonstrate nonidentifiability issues in additive models. Two main synthetic datasets are employed: (1) a 7-feature concurvity example where the ground truth relationship is Y = 2X1² + X5³ + 2sin(X6) + noise, and (2) a 2D correlated example with f(x) = min(x1, x2) − 0.1x1 − 0.1x2. NAMs are trained with concurvity regularization R⊥ at varying λ values, and the performance is evaluated using test-set RMSE across different feature subsets. The learned shape functions are visualized and compared to understand how different feature combinations can produce equivalent predictive performance but different interpretations.

## Key Results
- Nonidentifiability issues arise from two sources: inability to observe individual shape functions separately, and concurvity among covariates
- Orthogonal constraints help with some nonidentifiability issues but fail to resolve concurvity problems
- Multiple models with equivalent predictive performance can have different interpretations of feature effects
- Interpretability in additive models requires careful consideration beyond identifiability, potentially needing ensemble approaches examining multiple plausible solutions

## Why This Works (Mechanism)
The mechanism of nonidentifiability in additive models stems from the fundamental structure where we only observe the sum of shape functions rather than individual components. When covariates are nonlinearly correlated (concurvity), different combinations of shape functions can produce similar overall predictions. This creates a many-to-one mapping from model parameters to predictions, making it impossible to uniquely determine the true underlying feature effects. The paper demonstrates that even with regularization techniques designed to decorrelate shape functions, nonlinear dependencies persist, preventing unique attribution of feature effects.

## Foundational Learning
- **Generalized Additive Models (GAMs)**: Additive models where each feature has its own shape function; why needed: foundation for understanding interpretable models; quick check: can you write the GAM equation
- **Neural Additive Models (NAMs)**: Neural network variants of GAMs with one network per feature; why needed: modern interpretable model class being evaluated; quick check: understand how NAMs extend GAMs
- **Concurvity**: Nonlinear correlation between covariates; why needed: primary source of nonidentifiability; quick check: can you distinguish linear correlation from concurvity
- **Orthogonal Constraints**: Regularization technique to encourage independence between shape functions; why needed: proposed solution to nonidentifiability; quick check: understand how orthogonality differs from decorrelation
- **Rashomon Set**: Set of models with similar predictive performance; why needed: proposed approach for handling nonidentifiability; quick check: can you explain why multiple models might be acceptable

## Architecture Onboarding

**Component Map**: Input data -> Feature networks (one per feature) -> Summation layer -> Output prediction

**Critical Path**: Feature extraction (shape functions) → Summation → Prediction; Interpretability depends on understanding individual shape functions

**Design Tradeoffs**: 
- Simpler architectures (GAMs) have more obvious nonidentifiability but are easier to analyze
- Neural variants (NAMs) can learn more complex shapes but have more subtle nonidentifiability issues
- Regularization helps but cannot fully resolve concurvity problems

**Failure Signatures**: 
- Multiple feature subsets achieving similar RMSE but showing different shape functions
- Shape functions that don't match expected ground truth despite good predictive performance
- High variance in shape functions across different random initializations

**3 First Experiments**:
1. Generate the 7-feature synthetic dataset from Equation 6 and verify the ground truth relationship
2. Train NAMs with different feature subsets and compare test RMSE values
3. Visualize shape functions for different regularization λ values to observe concurvity effects

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Synthetic experiments may not fully capture real-world dataset complexity
- Claims about practical implications for domain experts lack empirical validation from actual applications
- The proposed ensemble approaches are suggested but not empirically validated beyond synthetic examples

## Confidence
- **High Confidence**: Theoretical nonidentifiability issues are well-supported and clearly demonstrated
- **Medium Confidence**: Claims about inadequacy of orthogonal constraints and regularization are demonstrated but may depend on implementation
- **Low-Medium Confidence**: Practical recommendations for domain experts and ensemble approaches lack empirical validation

## Next Checks
1. Replicate synthetic experiments using specified NAM architecture and training hyperparameters to verify reported RMSE values and shape function visualizations
2. Test NAMs across wider range of concurvity regularization values (λ ∈ {0.0, 0.0001, 0.001, 0.01, 0.1, 1.0}) and sample sizes to determine robustness of findings
3. Apply NAM framework to a well-understood real-world dataset with known ground truth feature relationships to assess if synthetic nonidentifiability issues manifest similarly in practice