---
ver: rpa2
title: 'VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning'
arxiv_id: '2506.09049'
source_url: https://arxiv.org/abs/2506.09049
tags:
- task
- arxiv
- reasoning
- step
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VIKI-Bench, the first hierarchical benchmark
  for embodied multi-agent cooperation, comprising three levels: agent activation,
  task planning, and trajectory perception. To tackle this, the authors propose VIKI-R,
  a two-stage framework that first fine-tunes a vision-language model using Chain-of-Thought
  demonstrations, then applies reinforcement learning with hierarchical rewards.'
---

# VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2506.09049
- **Source URL:** https://arxiv.org/abs/2506.09049
- **Reference count:** 40
- **Primary result:** Two-stage SFT-RL framework achieving 93% agent activation, 69% task planning accuracy, and reduced trajectory perception errors on VIKI-Bench.

## Executive Summary
This paper introduces VIKI-Bench, the first hierarchical benchmark for embodied multi-agent cooperation, comprising three levels: agent activation, task planning, and trajectory perception. To address this, the authors propose VIKI-R, a two-stage framework that first fine-tunes a vision-language model using Chain-of-Thought demonstrations, then applies reinforcement learning with hierarchical rewards. Experiments show that VIKI-R significantly outperforms baselines across all three task levels, achieving agent activation accuracy of 93%, task planning accuracy of 69%, and trajectory perception with reduced error metrics. Reinforcement learning also enables the emergence of compositional cooperation patterns among heterogeneous agents.

## Method Summary
VIKI-R uses a two-stage approach: Stage 1 applies supervised fine-tuning (SFT) on 500 Chain-of-Thought-annotated samples to establish basic task competence, followed by Stage 2 reinforcement learning using Group Relative Policy Optimization (GRPO). The method employs a hierarchical reward structure combining format compliance (binary reward for correct answer tags) and task-specific accuracy metrics. The base model is Qwen2.5-VL-3B/7B, trained using the verl framework with batch size 256, actor learning rate 1e-6, and KL coefficient 0.01. Training runs for 5 epochs on L1, 15 on L2, and 2 on L3 using 8×A800 GPUs.

## Key Results
- Achieves 93% accuracy on agent activation (L1) compared to 89% for SFT-only and 2% for RL-only baselines
- Reaches 69% task planning accuracy (L2) with hierarchical rewards outperforming single-reward approaches
- Reduces trajectory perception errors with RMSE, Hausdorff Distance, and Fréchet Distance metrics showing significant improvement
- Demonstrates emergence of compositional cooperation patterns through RL training

## Why This Works (Mechanism)
The two-stage SFT-RL pipeline enables progressive skill acquisition: SFT provides stable initial task competence through supervised learning on annotated demonstrations, while RL fine-tunes for optimal performance using hierarchical rewards that balance format compliance and task accuracy. The Chain-of-Thought reasoning in SFT helps the model learn structured problem-solving approaches that transfer to RL optimization.

## Foundational Learning
- **Chain-of-Thought reasoning**: Structured reasoning in < > tags with answers in <answer> tags, needed for teaching compositional task decomposition; quick check: verify all cold-start samples follow this format.
- **Group Relative Policy Optimization (GRPO)**: Policy gradient method that compares returns within a group rather than against a fixed baseline, needed for stable RL in multi-task settings; quick check: monitor KL divergence to prevent policy collapse.
- **Hierarchical reward engineering**: Combination of format compliance (binary) and task-specific accuracy metrics, needed to balance structural correctness with functional performance; quick check: ensure format reward saturates before accuracy reward increases.

## Architecture Onboarding
- **Component map**: Qwen2.5-VL -> SFT (cold-start) -> GRPO (hierarchical rewards) -> VIKI-Bench evaluation
- **Critical path**: SFT warmup on CoT data → RL fine-tuning with format + accuracy rewards → benchmark evaluation across L1/L2/L3
- **Design tradeoffs**: SFT provides stability but may limit exploration; RL enables optimization but requires careful reward shaping to prevent degenerate solutions
- **Failure signatures**: RL-only fails on L2 planning (reward stuck at ~0.01), indicating base model cannot generate coherent rollouts without SFT initialization
- **First experiments**: 1) Run SFT on cold-start data and verify accuracy improvement over base model; 2) Apply GRPO with format-only reward and observe initial response length reduction; 3) Add accuracy reward and verify compositional pattern emergence in plan generation.

## Open Questions the Paper Calls Out
None

## Limitations
- VIKI-Bench dataset and CoT annotations are not publicly available beyond summary statistics
- World Simulator and Action Checker modules for plan feasibility verification are described but not specified in sufficient detail
- Automated data generation pipeline using GPT-4o is incompletely documented with partial prompt templates

## Confidence
**High Confidence**: The core architectural contribution of a two-stage SFT-RL pipeline for hierarchical multi-agent cooperation is well-documented and technically sound.

**Medium Confidence**: The quantitative results on VIKI-Bench are internally consistent and show clear improvement over baselines, but cannot be fully verified without access to the benchmark data and evaluation infrastructure.

**Low Confidence**: Claims about cross-task generalization and the specific contributions of each reward component to final performance are difficult to validate without ablation studies on the actual benchmark data.

## Next Checks
1. **Dataset Reconstruction Verification**: Attempt to reconstruct the VIKI-Bench splits from RoboCasa/ManiSkill3 following the stated statistics and verify that the agent activation accuracy of 93% can be reproduced using the described SFT procedure on the reconstructed cold-start data.

2. **World Simulator Implementation**: Implement a basic version of the World Simulator and Action Checker based on the descriptions in Appendix C.2 and validate that it correctly rejects infeasible plans while accepting feasible ones, then verify that the L2 reward computation produces values consistent with the training curves shown in Figure 6.

3. **Compositional Pattern Analysis**: Analyze the qualitative examples of emergent cooperation patterns using the provided rollout examples to determine whether the observed behaviors represent true compositional generalization or simply memorized action sequences from the training data.