---
ver: rpa2
title: On the Relation of State Space Models and Hidden Markov Models
arxiv_id: '2601.13357'
source_url: https://arxiv.org/abs/2601.13357
tags:
- state
- ssms
- latent
- space
- kalman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unified comparison of Hidden Markov Models
  (HMMs) and State Space Models (SSMs) through a probabilistic graphical modeling
  lens. While HMMs model discrete latent states with Markov chain dynamics, classical
  SSMs employ continuous-valued latent states with Gaussian noise, and modern NLP
  SSMs like S4 and Mamba use deterministic state updates trained via backpropagation.
---

# On the Relation of State Space Models and Hidden Markov Models
## Quick Facts
- arXiv ID: 2601.13357
- Source URL: https://arxiv.org/abs/2601.13357
- Reference count: 6
- Key outcome: Unified probabilistic graphical modeling comparison of HMMs, SSMs, and modern NLP SSMs, clarifying fundamental differences in uncertainty representation, inference, and training

## Executive Summary
This paper provides a unified comparison of Hidden Markov Models (HMMs) and State Space Models (SSMs) through a probabilistic graphical modeling lens. While HMMs model discrete latent states with Markov chain dynamics, classical SSMs employ continuous-valued latent states with Gaussian noise, and modern NLP SSMs like S4 and Mamba use deterministic state updates trained via backpropagation. The authors clarify that despite identical temporal dependency structures, these models differ fundamentally in their probabilistic assumptions, inference procedures (forward-backward vs. Kalman filtering vs. deterministic scan), and training paradigms (EM vs. backpropagation). The analysis bridges control theory, probabilistic modeling, and deep learning, revealing that HMMs and SSMs represent uncertainty over state identity and trajectories respectively, while NLP SSMs do not represent latent uncertainty at all. The work demystifies the relationships between classical probabilistic models and modern sequence architectures.

## Method Summary
The paper conducts a theoretical comparison through mathematical formulations of HMMs, Linear Gaussian SSMs, and deterministic NLP SSMs. The analysis derives joint likelihood equations (12-13), EM algorithms for HMM (14-16) and SSM (17), and compares inference procedures including forward-backward, Kalman filtering/smoothing, and deterministic scans. The methodology establishes structural similarities via probabilistic graphical models while highlighting critical differences in probabilistic assumptions, uncertainty quantification, and learning procedures. The comparison reveals that HMMs and SSMs are inherently probabilistic models with distinct types of uncertainty representation, whereas modern NLP SSMs are deterministic sequence models trained via backpropagation without probabilistic semantics.

## Key Results
- HMMs and SSMs share identical temporal dependency structures but differ fundamentally in probabilistic assumptions: discrete vs. continuous latent states with Gaussian noise
- Modern NLP SSMs (S4, Mamba) are deterministic models that abandon probabilistic semantics entirely, using backpropagation instead of EM
- HMMs represent uncertainty over latent state identity, while SSMs represent uncertainty over latent state trajectories, and NLP SSMs represent neither
- Despite structural similarities in temporal dependencies, the three model classes employ fundamentally different inference procedures and training paradigms

## Why This Works (Mechanism)
The paper works by establishing a unified mathematical framework that expresses HMMs, SSMs, and NLP SSMs within the same probabilistic graphical modeling formalism. By deriving the joint likelihood equations and inference procedures for each model class, the authors reveal that the apparent similarities in temporal structure mask profound differences in how uncertainty is represented and processed. The forward-backward algorithm for HMMs computes posterior probabilities over discrete state sequences, Kalman filtering/smoothing for SSMs computes Gaussian distributions over continuous state trajectories, and deterministic NLP SSMs simply compute point estimates without any probabilistic interpretation. This unified treatment clarifies why these models, despite their structural similarities, belong to fundamentally different mathematical and computational paradigms.

## Foundational Learning
**Probabilistic Graphical Models**: Essential for understanding the unified treatment of HMMs, SSMs, and NLP SSMs as graphical models with different node types and dependency structures. Quick check: Can you draw the factor graph representation of each model type?
**Forward-Backward Algorithm**: Core inference procedure for HMMs that computes posterior state probabilities given observations. Why needed: Fundamental to HMM inference and EM learning. Quick check: Implement forward-backward on a simple two-state HMM and verify probability sums to 1.
**Kalman Filtering and Smoothing**: Sequential Bayesian inference for Linear Gaussian SSMs that maintains Gaussian posteriors over continuous states. Why needed: Contrasts with HMM inference by operating on continuous distributions. Quick check: Derive Kalman gain formula and verify it weights prediction vs. observation appropriately.
**Expectation-Maximization Algorithm**: Iterative learning procedure that alternates between computing expected sufficient statistics (E-step) and maximizing likelihood (M-step). Why needed: Standard training method for probabilistic models vs. backpropagation for deterministic ones. Quick check: Implement EM for a simple HMM and verify parameter convergence.
**Deterministic State Updates**: Modern NLP SSMs use fixed, learnable state transition matrices without stochastic noise. Why needed: Key distinction from classical probabilistic SSMs. Quick check: Verify that NLP SSM forward pass is differentiable and has no random components.

## Architecture Onboarding
**Component Map**: Observations (y) -> Emission/Observation Model (C) -> Latent States (h) -> Transition Model (A) -> Next Latent States; HMM adds discrete state space, SSM adds Gaussian noise, NLP SSM removes noise
**Critical Path**: Forward pass through temporal dependencies; inference computes posterior over latent states given observations; learning updates model parameters
**Design Tradeoffs**: HMMs handle discrete state spaces well but suffer from state space explosion; SSMs naturally handle continuous dynamics and Gaussian noise but assume linearity and Gaussianity; NLP SSMs are computationally efficient and scalable but lose probabilistic interpretability
**Failure Signatures**: HMMs fail when state space is too large or continuous dynamics are needed; SSMs fail when non-Gaussian noise or nonlinear dynamics dominate; NLP SSMs fail when uncertainty quantification or principled handling of missing data is required
**First Experiments**: 1) Implement HMM forward-backward and verify on synthetic categorical data; 2) Implement Linear Gaussian SSM with Kalman filter/smoothing and verify EM convergence; 3) Implement deterministic NLP SSM and compare inference speed vs. probabilistic counterparts

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Lack of empirical validation leaves practical implications of theoretical distinctions untested
- Analysis assumes stationarity and Gaussianity in classical models without exploring robustness to assumption violations
- No quantitative comparison of computational complexity trade-offs between inference algorithms in practice
- Theoretical claims about modern NLP SSMs being non-probabilistic are not verified on actual implementations

## Confidence
- **High Confidence**: Mathematical derivations of HMM forward-backward and Kalman filtering algorithms are correct and represent established results
- **Medium Confidence**: Characterization of modern NLP SSMs as deterministic and non-probabilistic is theoretically justified but lacks empirical verification
- **Medium Confidence**: Claims about different types of uncertainty representation are conceptually compelling but would benefit from quantitative validation

## Next Checks
1. **Empirical Implementation Test**: Implement the three model classes (HMM, Linear Gaussian SSM, deterministic NLP SSM) on a synthetic sequence prediction task and measure whether the deterministic SSMs indeed lack the uncertainty quantification properties of their probabilistic counterparts
2. **Noise Sensitivity Analysis**: Systematically vary observation and transition noise levels in HMMs and Linear Gaussian SSMs to quantify how the uncertainty representations differ in practice, particularly when model assumptions are violated
3. **Computational Complexity Benchmark**: Measure wall-clock inference times for forward-backward (HMM), Kalman filtering/smoothing (SSM), and deterministic scan (NLP SSM) across varying sequence lengths to empirically validate the claimed computational trade-offs