---
ver: rpa2
title: 'OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer
  in Industrial Recommender'
arxiv_id: '2510.26104'
source_url: https://arxiv.org/abs/2510.26104
tags:
- sequence
- arxiv
- unified
- transformer
- onetrans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes OneTrans, a unified Transformer backbone for
  industrial recommendation systems that jointly performs user-behavior sequence modeling
  and feature interaction within a single model. Unlike conventional approaches that
  separate these tasks, OneTrans employs a unified tokenizer to convert both sequential
  and non-sequential features into a single token sequence, and uses mixed parameterization
  to share weights for similar sequential tokens while assigning token-specific parameters
  to non-sequential tokens.
---

# OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender

## Quick Facts
- **arXiv ID:** 2510.26104
- **Source URL:** https://arxiv.org/abs/2510.26104
- **Reference count:** 40
- **Primary result:** OneTrans achieves 5.68% lift in per-user GMV in online A/B tests by unifying sequential and non-sequential feature modeling in a single Transformer

## Executive Summary
OneTrans introduces a unified Transformer architecture for industrial recommendation systems that simultaneously models user behavior sequences and feature interactions within a single model. Unlike conventional approaches that separate these tasks into distinct modules, OneTrans employs a unified tokenizer to convert both sequential and non-sequential features into a single token sequence, and uses mixed parameterization to share weights for similar sequential tokens while assigning token-specific parameters to non-sequential tokens. The model incorporates causal attention and cross-request KV caching to reduce computational costs during training and inference. Experiments on industrial-scale datasets demonstrate that OneTrans scales efficiently with increasing parameters, consistently outperforms strong baselines, and achieves significant business impact in online A/B tests.

## Method Summary
OneTrans unifies user-behavior sequence modeling and feature interaction in a single Transformer backbone. It uses a unified tokenizer (Auto-Split) to convert multi-behavior sequences and non-sequential features into a single token sequence. The model employs mixed parameterization where sequential tokens share Q/K/V and FFN weights, while each non-sequential token type has dedicated parameters. Causal attention allows non-sequential tokens to attend to the entire history of sequential tokens. A pyramid stacking strategy progressively prunes sequential query tokens during deeper layers, and cross-request KV caching reuses sequential computations across different candidate items in the same request. The model is trained with dual optimizers (Adagrad for sparse, RMSProp for dense) and achieves substantial efficiency gains while maintaining or improving recommendation quality.

## Key Results
- Achieves 5.68% lift in per-user GMV in online A/B tests
- Consistently outperforms strong baselines (DIEN, M6, NextItNet) with up to 0.15% AUC improvement
- Reduces TFLOPs by 62.4% and runtime by 28.4% through pyramid stacking and KV caching
- Scales efficiently with parameters, showing log-linear performance improvement

## Why This Works (Mechanism)

### Mechanism 1: Unified Token Interaction Space
If sequential (S) and non-sequential (NS) features are modeled in separate modules, bidirectional information flow is restricted; unifying them in a single sequence allows context to shape sequence representations dynamically. OneTrans concatenates user behavior tokens (S) and feature tokens (NS) into one sequence. Using causal attention, NS tokens (target features) attend to the entire history of S tokens, while S tokens attend to preceding S tokens. This removes the "encode-then-interaction" barrier. Core assumption: The semantic gap between sparse ID features and dense behavior embeddings can be bridged by a unified Transformer without collapsing feature-specific nuances. Evidence anchors: [abstract] "...unified Transformer backbone that simultaneously performs user-behavior sequence modeling and feature interaction." [section 3.1] "NS-tokens are permitted to attend over the entire history of S-tokens, thereby enabling comprehensive cross-token interaction." Break condition: If the sequence length of S-tokens dominates the context window excessively, the gradient signals from sparse NS-tokens may dilute, failing to update context representations effectively.

### Mechanism 2: Mixed Parameterization for Heterogeneity
Sharing parameters across all feature types degrades performance; assigning token-specific parameters to heterogeneous non-sequential features preserves semantic distinctiveness. Within the unified stack, S-tokens (homogeneous events) share a single set of Q/K/V and FFN weights. Each NS-token (e.g., User ID, Item Category) receives a dedicated set of projection weights. This allows the model to learn specialized representations for sparse features while maintaining parameter efficiency for dense sequences. Core assumption: Non-sequential features have distributions and semantic densities distinct enough to warrant isolated projection matrices, unlike the uniform nature of sequential events. Evidence anchors: [section 3.3.1] "...sharing weights across sequential tokens, while assigning separate parameters to non-sequential tokens..." [table 3] "Shared parameters" variant shows a -0.15% drop in CTR AUC, supporting the need for token-specific parameters. Break condition: If the number of non-sequential tokens ($L_{NS}$) scales into the thousands, the parameter count for token-specific heads could explode, leading to overfitting or memory exhaustion.

### Mechanism 3: Pyramid Stacking & KV Caching
A full-sequence Transformer is too expensive for industrial serving; causal masking allows for progressive pruning (pyramid) and computation reuse (caching). The model uses a "pyramid schedule" where the number of query tokens (S-tokens) shrinks linearly with depth (pruning old history queries), while KV caches are retained. Furthermore, Cross-Request KV Caching reuses the S-side computation (user history) across different candidate items in the same request. Core assumption: Older sequential tokens have less impact on the final prediction than recent ones, justifying the pruning of their queries in deeper layers. Evidence anchors: [abstract] "...enables precomputation and caching of intermediate representations, significantly reducing computational costs..." [table 4] Shows Pyramid Stack reduces runtime by ~28% and Cross-Request KV Caching by ~30%. Break condition: If user preference shifts are subtle and buried deep in history, aggressive pyramid pruning may discard critical long-range signals.

## Foundational Learning

- **Concept: Causal vs. Bidirectional Attention**
  - **Why needed here:** OneTrans strictly uses causal masking (token $t$ attends only to $t-k$). This is distinct from BERT-style models and is the prerequisite for the KV Caching efficiency mechanism.
  - **Quick check question:** Can a non-sequential token representing the "current item" attend to a sequential token representing a "future click"? (Answer: No, the paper specifies NS tokens attend to *preceding* S-tokens).

- **Concept: Feature Heterogeneity in RecSys**
  - **Why needed here:** Understanding why standard NLP Transformers fail here. NLP tokens share a vocabulary; RecSys tokens are a mix of IDs (User), categories (Item), and floats (Price).
  - **Quick check question:** Why can't we use a single `nn.Linear` layer to project both "User ID" and "Item Price" into the attention space effectively? (Answer: Their semantic spaces and distribution sparsity differ, requiring the "Mixed Parameterization" described).

- **Concept: KV Caching**
  - **Why needed here:** This is the primary driver for the efficiency claims (O(C) to O(1)).
  - **Quick check question:** In a batch of 100 candidate items for one user, how many times is the user's historical sequence encoded? (Answer: Once; the K/V vectors are computed once and cached/reused).

## Architecture Onboarding

- **Component map:** Raw Features -> Tokenizer (Auto-Split/Concat) -> S/NS Token Sequence -> [Layer 1..N] (Attention + FFN + Pruning) -> Final NS Embeddings -> Prediction
- **Critical path:** Raw Features -> Tokenizer (Auto-Split/Concat) -> S/NS Token Sequence -> [Layer 1..N] (Attention + FFN + Pruning) -> Final NS Embeddings -> Prediction
- **Design tradeoffs:**
  - *Depth vs. Width:* Paper finds increasing **depth** (more layers) yields higher-order interactions better than just increasing width, but adds serialization latency.
  - *Tokenizer:* `Auto-Split` (single MLP + split) is preferred over `Group-wise` (multiple MLPs) for kernel efficiency, though `Group-wise` may offer more manual control.
  - *Fusion:* `Timestamp-aware` ordering of sequences outperforms `Timestamp-agnostic` (intent-based) ordering when timestamps are available.
- **Failure signatures:**
  - *Attention Collapse:* If using Post-Norm instead of Pre-Norm (RMSNorm) with mixed token types, training may destabilize due to scale discrepancies between S and NS tokens.
  - *Latency Spikes:* If Pyramid schedule is disabled (`w/o pyramid stack`), TFLOPs jump significantly (2.64T -> 8.08T), likely breaching latency SLAs.
- **First 3 experiments:**
  1. **Ablate Parameterization:** Replace Mixed Parameterization (token-specific NS params) with fully shared params. Expect ~0.15% drop in AUC (per Table 3).
  2. **Efficiency Profiling:** Measure p99 latency with and without Cross-Request KV Caching. Expect ~30% reduction (per Table 4).
  3. **Scaling Law Check:** Train a small (6-layer) vs. large (8-layer) model. Verify that performance gain follows a log-linear trend relative to TFLOPs (per Fig 3b).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific system–model co-optimizations are required to scale OneTrans substantially beyond the current OneTrans L regime without violating strict production latency constraints?
- **Basis in paper:** [explicit] Section 4.5 states that scaling "substantially beyond this regime remains constrained by online efficiency" and explicitly leaves "further system–model co-optimizations to future work."
- **Why unresolved:** While current optimizations (KV caching, FlashAttention) enable OneTrans L, deeper models increase serial computation and wider models increase memory pressure, creating a bottleneck for future scaling.
- **What evidence would resolve it:** Novel architectures or training techniques that improve Model FLOPs Utilization (MFU) or reduce inference latency while supporting parameter counts significantly higher than 330M.

### Open Question 2
- **Question:** Can a learnable or adaptive token pruning strategy outperform the fixed linear heuristic used in the pyramid stack?
- **Basis in paper:** [inferred] Section 4.1.4 describes the pyramid schedule as a "heuristic" that linearly shrinks sequential query tokens, and Section 3.4 discusses the need to focus capacity on "informative events."
- **Why unresolved:** A fixed linear schedule may prematurely discard informative historical tokens or retain redundant ones, depending on the specific user context, whereas a data-dependent approach could optimize this dynamically.
- **What evidence would resolve it:** Ablation studies comparing the fixed linear schedule against dynamic halting mechanisms or attention-based token importance scoring in terms of AUC retention vs. TFLOPs reduction.

### Open Question 3
- **Question:** Can candidate-specific sequences be integrated natively into the unified stack without the lossy pre-aggregation step currently required?
- **Basis in paper:** [inferred] Section 3.5.1 notes that candidate-specific sequences (e.g., SIM) are "pre-aggregated into NS-tokens via pooling, as they cannot reuse the shared S-side cache."
- **Why unresolved:** Pre-aggregating these sequences into non-sequential tokens via pooling collapses sequential information, potentially losing nuanced interactions between the candidate and specific historical behaviors.
- **What evidence would resolve it:** A modified caching or attention mechanism that handles candidate-dependent sequence keys/values efficiently without increasing computational complexity to O(C) per request.

## Limitations

- **Sequence Length Heterogeneity:** The method's behavior under highly variable sequence lengths (e.g., some users with 10 actions, others with 10,000) remains untested. The pyramid pruning strategy assumes linear decay in token importance, which may not hold for all user behavior patterns.
- **Generalization Across Domains:** All datasets originate from the same e-commerce platform. The unified modeling approach's effectiveness across different recommendation domains (media streaming, social feeds, search) with different interaction patterns is not established.
- **Cold-start Performance:** The paper focuses on existing user scenarios with rich behavior history. The handling of new users with minimal interaction history or new items with few interactions is not discussed.

## Confidence

- **High Confidence** - The unified modeling approach (Mechanism 1) and its efficiency gains (Mechanism 3) are well-supported by both theoretical reasoning and empirical results. The causal attention + KV caching mechanism is clearly specified and validated.
- **Medium Confidence** - The necessity of mixed parameterization (Mechanism 2) is supported by ablation studies, but the specific design choice (shared Q/K/V for S, specific for NS) could potentially be optimized further. Alternative parameterization schemes may yield similar or better results.
- **Medium Confidence** - The pyramid pruning strategy is justified empirically but lacks theoretical grounding for why linear decay is optimal. Alternative decay schedules might perform differently.

## Next Checks

1. **Cross-domain validation** - Apply OneTrans to a different recommendation domain (e.g., news recommendation or video streaming) with distinct interaction patterns and measure whether the 5.68% GMV lift transfers or degrades.

2. **Cold-start stress test** - Create controlled experiments with varying amounts of user behavior history (0-5 actions vs. 100+ actions) to quantify performance degradation and identify breaking points for the unified modeling approach.

3. **Parameter scaling analysis** - Systematically vary L_NS (number of non-sequential token types) and measure the impact on both model quality and inference latency to identify practical limits of the mixed parameterization scheme.