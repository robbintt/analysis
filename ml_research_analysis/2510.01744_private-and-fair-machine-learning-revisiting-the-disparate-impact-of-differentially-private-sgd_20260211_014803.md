---
ver: rpa2
title: 'Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially
  Private SGD'
arxiv_id: '2510.01744'
source_url: https://arxiv.org/abs/2510.01744
tags:
- dpsgd
- difference
- accuracy
- hyperparameter
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Differential privacy training impacts model performance and fairness
  across metrics. This work shows that a disparate impact on one metric (e.g., accuracy)
  does not necessarily imply disparate impact on others (e.g., AUC-ROC, precision).
---

# Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD

## Quick Facts
- arXiv ID: 2510.01744
- Source URL: https://arxiv.org/abs/2510.01744
- Reference count: 17
- Key outcome: Differential privacy training impacts model performance and fairness across metrics

## Executive Summary
This work examines how differentially private stochastic gradient descent (DPSGD) affects both model performance and fairness across different metrics. The authors demonstrate that disparate impact on one performance metric (such as accuracy) does not necessarily imply disparate impact on other metrics like AUC-ROC or precision. They explore whether performance-based hyperparameter tuning on differentially private models can improve fairness compared to reusing hyperparameters from non-private models, and evaluate a variant called DPSGD-Global-Adapt against standard DPSGD.

## Method Summary
The study evaluates the fairness implications of differentially private training by comparing performance across various metrics (accuracy, AUC-ROC, precision) when applying DPSGD. The authors investigate whether hyperparameter tuning specifically for differentially private models can mitigate fairness disparities, and compare standard DPSGD with a variant called DPSGD-Global-Adapt. The experimental setup involves training models on datasets with demographic attributes to assess disparate impact across different subgroups.

## Key Results
- Disparate impact on one performance metric (e.g., accuracy) does not necessarily imply disparate impact on other metrics (e.g., AUC-ROC, precision)
- Performance-based hyperparameter tuning on differentially private models can improve fairness compared to reusing non-private model hyperparameters, but does not reliably eliminate disparate impact
- DPSGD-Global-Adapt often improves or does not worsen performance and fairness compared to standard DPSGD, but is not consistently superior

## Why This Works (Mechanism)
Differential privacy introduces noise during training to protect individual data points, which affects the optimization landscape differently across subgroups. This noise can disproportionately impact certain metrics while leaving others relatively unaffected. The mechanism of DPSGD-Global-Adapt likely modifies how noise is applied globally versus locally, potentially reducing variance in the impact across subgroups. Performance-based hyperparameter tuning allows the model to adapt to the specific characteristics of the differentially private training process, potentially mitigating some fairness disparities that arise from using hyperparameters optimized for non-private training.

## Foundational Learning

**Differential Privacy**: Mathematical framework ensuring individual data points cannot be distinguished in the output model
- Why needed: Provides theoretical guarantees for privacy preservation
- Quick check: Verify epsilon-delta values satisfy desired privacy bounds

**Fairness Metrics**: Multiple measurements (accuracy, AUC-ROC, precision, etc.) needed to comprehensively assess algorithmic fairness
- Why needed: Single metric evaluation may mask disparate impacts on different subgroups
- Quick check: Compare metric variations across demographic groups

**Hyperparameter Optimization**: Process of finding optimal training parameters for specific model configurations
- Why needed: Different training regimes (private vs non-private) may require distinct optimal parameters
- Quick check: Validate that tuned hyperparameters improve target metrics

## Architecture Onboarding

**Component Map**: Data Preprocessing -> DPSGD Training -> Metric Evaluation -> Fairness Analysis

**Critical Path**: The sequence from privacy parameter selection through training to fairness metric computation determines the overall impact on model behavior

**Design Tradeoffs**: Privacy-utility tradeoff vs fairness-utility tradeoff - increasing privacy noise may improve fairness for some metrics while degrading it for others

**Failure Signatures**: 
- Over-pruning leading to model collapse
- Disparate noise sensitivity across subgroups
- Hyperparameter mismatch between private and non-private training regimes

**First Experiments**:
1. Train identical models with varying epsilon values to observe privacy-fairness tradeoffs
2. Compare metric disparities across demographic groups for baseline vs DPSGD-Global-Adapt
3. Perform hyperparameter tuning specifically for differentially private training and measure resulting fairness impacts

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Findings may not generalize across different model architectures beyond those tested
- Study focuses on specific DPSGD configurations and may not capture behavior of alternative privacy-preserving methods
- Employed fairness metrics, while comprehensive, may not capture all relevant aspects of real-world algorithmic fairness

## Confidence

High confidence: Disparate impact on one performance metric does not necessarily imply disparate impact on others

Medium confidence: Performance-based hyperparameter tuning can improve fairness but does not reliably eliminate disparate impact

Medium confidence: Comparative analysis of DPSGD variants given specific experimental setup

## Next Checks

1. Replicate the study using diverse model architectures (e.g., transformers, graph neural networks) to assess architectural sensitivity

2. Conduct experiments on additional datasets with varying demographic distributions to test robustness across different fairness contexts

3. Implement and evaluate alternative privacy-preserving training methods (e.g., PATE, DP-SVRG) to compare their fairness implications with DPSGD variants