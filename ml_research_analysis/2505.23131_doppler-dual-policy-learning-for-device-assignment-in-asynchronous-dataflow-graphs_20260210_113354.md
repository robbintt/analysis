---
ver: rpa2
title: 'DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow
  Graphs'
arxiv_id: '2505.23131'
source_url: https://arxiv.org/abs/2505.23131
tags:
- doppler
- time
- learning
- assignment
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DOPPLER addresses device assignment in asynchronous dataflow graphs
  to minimize execution time. It uses a dual-policy framework with a node-selection
  policy and a device-placement policy, trained through three stages: imitation learning,
  simulation-based reinforcement learning, and real-system reinforcement learning.'
---

# DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs

## Quick Facts
- arXiv ID: 2505.23131
- Source URL: https://arxiv.org/abs/2505.23131
- Reference count: 40
- Primary result: 62.5% lower execution times than strong baselines

## Executive Summary
DOPPLER introduces a dual-policy framework for device assignment in asynchronous dataflow graphs, using separate node-selection and device-placement policies to minimize execution time. The method employs a three-stage training approach: imitation learning from a CRITICAL PATH heuristic, simulation-based reinforcement learning, and real-system fine-tuning. DOPPLER achieves up to 62.5% lower execution times compared to baselines and outperforms strong enumerative optimizers by 13.8%, demonstrating efficient sampling and effective load balancing across both policies.

## Method Summary
DOPPLER addresses device assignment in work-conserving asynchronous dataflow graphs through a dual-policy GNN architecture. The SEL policy selects which node to assign next, while the PLC policy chooses which device to place it on. Training follows a three-stage curriculum: (1) imitation learning from the CRITICAL PATH heuristic, (2) simulation-based reinforcement learning using a custom WC simulator, and (3) real-system fine-tuning on actual hardware. The approach uses efficient message passing by computing GNN embeddings once per episode rather than per MDP step, dramatically reducing training time while maintaining solution quality.

## Key Results
- Achieves 62.5% lower execution times compared to strong baselines
- Outperforms a strong enumerative optimizer by 13.8%
- Demonstrates efficient sampling with both policies contributing to performance gains
- Ablation studies confirm effectiveness of dual-policy design and pre-training benefits

## Why This Works (Mechanism)

### Mechanism 1
Decomposing device assignment into separate node-selection and device-placement policies improves learning efficiency by allowing each policy to specialize. SEL learns traversal order matching non-deterministic WC execution flow, while PLC focuses on load-balancing and communication minimization for single nodes.

### Mechanism 2
Three-stage training accelerates convergence by bootstrapping from heuristics, refining via simulator, and fine-tuning on real hardware. This curriculum avoids cold-start problems and uses cheap simulator rewards before deploying on actual systems.

### Mechanism 3
Performing GNN message-passing once per episode maintains solution quality while reducing training time by 3049%. Dynamic assignment state is encoded in device features rather than re-computing embeddings at each step.

## Foundational Learning

- **Work-conserving vs. bulk-synchronous scheduling**: DOPPLER targets WC systems where operations execute as soon as inputs arrive, creating non-deterministic execution orders. *Quick check*: If GPU A finishes 10ms before GPU B, does a bulk-synchronous system proceed immediately or wait?

- **List scheduling heuristics (CRITICAL PATH)**: DOPPLER's dual policy is a neural generalization of list scheduling. *Quick check*: In list scheduling, what determines which ready task gets scheduled next?

- **Policy gradient methods for combinatorial optimization**: Stages II-III use REINFORCE-style updates where ∇θ(log Πθ(a|s))R(s,a) shapes the policy. *Quick check*: Why is a baseline reward subtracted from the episodic return?

## Architecture Onboarding

- **Component map**: Computation Graph G → GNN encoder (once/episode) → node embeddings H[v] → At each step h: SELθ(H, features, candidate set C) → node vh → PLCθ(H, vh, device features XD) → device dh → Assignment A complete → ExecTime(A) → reward rH

- **Critical path**: Stage I pre-training → Stage II simulation RL (4k-8k episodes) → Stage III real-system fine-tuning. Simulation stage handles most exploration; real-system stage is fine-tuning only.

- **Design tradeoffs**: Single message-passing (faster training, slightly reduced expressiveness) vs. dual policy vs. monolithic (more modular, requires training two networks) vs. simulator vs. real-only (enables safe exploration vs. risks user-facing latency spikes).

- **Failure signatures**: High variance across random seeds (simulator-reality gap or insufficient episodes), PLC assigns all nodes to one device (SEL not providing useful context), Stage III degrades performance (overfitting to simulator quirks).

- **First 3 experiments**: (1) Run DOPPLER-SIM on CHAIN MM with 100, 500, 1000 episodes; plot simulator reward vs. real execution time. (2) Ablate SEL and PLC separately to isolate each policy's contribution. (3) Test zero-shot transfer: train on FFNN, evaluate on LLAMA-BLOCK without fine-tuning.

## Open Questions the Paper Calls Out

- **Transfer capability**: Can DOPPLER effectively transfer learned policies across distinct hardware configurations without requiring extensive retraining? The authors note future work could explore transfer across dataflow graphs with different hardware configurations, but current experiments were restricted to same hardware setups.

- **Scalability to massive graphs**: How does training efficiency scale for massive dataflow graphs lacking repeating structural blocks? The authors note training time grows with graph size and currently relies on repeating graph structures to handle large models.

- **Simulation refinement**: Can the simulation-based reward model be refined to minimize systematic overestimation of execution time relative to the real system? The simulator tends to overestimate running time compared to real system, showing moderate correlation that reduces sample efficiency.

## Limitations
- Simulator-reality gap uncertainty with moderate correlation (Pearson 0.79, Spearman 0.69) between simulator and real execution times
- Work-conserving system specificity to Turnip/EinDecomp runtime, may not generalize to different asynchronous frameworks
- Computational cost assumptions with 0.7% runtime degradation from single-message-passing optimization

## Confidence
- **Dual-policy framework effectiveness**: High - Strong ablation results show both policies contribute meaningfully
- **Three-stage training curriculum**: Medium - Stage I and III results are robust, but Stage II simulator correlation shows room for improvement
- **Single-message-passing optimization**: High - Quantitative comparison clearly demonstrates dramatic training speedup with minimal performance impact

## Next Checks
1. **Simulator-reality transfer validation**: Train DOPPLER-SIM on novel graph types and measure execution time correlation degradation to quantify simulator bias effects on real-system performance.

2. **Runtime framework generalization**: Implement DOPPLER on a different WC framework (e.g., PyTorch Distributed) and compare performance to document architectural assumptions that break.

3. **Graph structure sensitivity**: Systematically vary graph connectivity patterns and measure how the 0.7% runtime degradation from single-message-passing scales to identify when per-step message-passing becomes necessary.