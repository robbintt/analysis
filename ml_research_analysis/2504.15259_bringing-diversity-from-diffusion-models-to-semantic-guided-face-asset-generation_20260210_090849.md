---
ver: rpa2
title: Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation
arxiv_id: '2504.15259'
source_url: https://arxiv.org/abs/2504.15259
tags:
- face
- texture
- generation
- diffusion
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for generating diverse, high-quality
  3D face assets with semantic control by leveraging pre-trained diffusion models
  and adversarial training. The approach addresses the challenge of limited diversity
  in existing 3D face datasets by synthesizing a large dataset of 44,000 3D face models
  with associated attributes (age, gender, ethnicity) using a pre-trained diffusion
  model.
---

# Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation

## Quick Facts
- arXiv ID: 2504.15259
- Source URL: https://arxiv.org/abs/2504.15259
- Authors: Yunxuan Cai; Sitao Xiang; Zongjian Li; Haiwei Chen; Yajie Zhao
- Reference count: 33
- Key outcome: This paper presents a method for generating diverse, high-quality 3D face assets with semantic control by leveraging pre-trained diffusion models and adversarial training. The approach addresses the challenge of limited diversity in existing 3D face datasets by synthesizing a large dataset of 44,000 3D face models with associated attributes (age, gender, ethnicity) using a pre-trained diffusion model. A texture normalization method converts these synthetic images into clean albedo maps, and a disentangled GAN-based generator maps semantic attributes to UV-space geometry and albedo, enabling both direct sampling and continuous latent editing while preserving identity. The system produces PBR-ready assets including geometry, albedo, specular, and displacement maps, along with secondary assets like eyeballs and teeth. The generated assets can be rendered and animated in standard pipelines, and the system achieves real-time generation (0.014s per face) with high attribute control accuracy (race: 75.88%, gender: 91.93%, age: 49.90%) and superior identity preservation during editing compared to baselines.

## Executive Summary
This paper addresses the challenge of generating diverse, high-quality 3D face assets with semantic control by synthesizing a large dataset of 44,000 3D face models with associated attributes (age, gender, ethnicity) using a pre-trained diffusion model. A texture normalization method converts these synthetic images into clean albedo maps, and a disentangled GAN-based generator maps semantic attributes to UV-space geometry and albedo, enabling both direct sampling and continuous latent editing while preserving identity. The system produces PBR-ready assets including geometry, albedo, specular, and displacement maps, along with secondary assets like eyeballs and teeth. The generated assets can be rendered and animated in standard pipelines, and the system achieves real-time generation (0.014s per face) with high attribute control accuracy (race: 75.88%, gender: 91.93%, age: 49.90%) and superior identity preservation during editing compared to baselines.

## Method Summary
The method involves a two-stage pipeline: first, generating diverse 2D portraits using Stable Diffusion v1.5 with ControlNet, reconstructing 3D geometry with ReFA, and normalizing synthetic textures to clean albedo maps using a patch-based parameter estimation network trained with adversarial losses. Second, training a two-step disentangled GAN (StyleGAN2 architecture) where an encoder learns identity codes through adversarial training to discard labeled attributes, and a conditional generator maps semantic attributes to UV-space geometry and albedo while preserving identity through spatial conditioning. The system produces PBR-ready assets including geometry, albedo, specular, and displacement maps, along with secondary assets like eyeballs and teeth, achieving real-time generation (0.014s per face) with high attribute control accuracy.

## Key Results
- Generates 44,000 diverse 3D face models with accurate demographic labels (age, gender, ethnicity)
- Achieves real-time generation (0.014s per face) compared to 40s for diffusion models
- High attribute control accuracy: race (75.88%), gender (91.93%), age (49.90%)
- Superior identity preservation during editing (0.9594 for gender edits vs 0.8703 for random pairs)
- CLIP score of 0.316±0.042 and texture normalization PSNR of 24.67

## Why This Works (Mechanism)

### Mechanism 1: Diffusion-Guided Data Synthesis with Normalized Albedo Extraction
Converting diffusion-generated portraits into clean albedo textures enables training of high-quality 3D face generators without expensive capture setups, conditional on effective lighting removal. The pipeline uses Stable Diffusion v1.5 with ControlNet (conditioned on normal maps) to generate diverse 2D portraits. A texture normalization module treats lighting removal as a domain transfer problem—mapping 44K synthetically-lit textures to a small target domain of 200 scanned albedos using a patch-based parameter estimation network with spatially-varying factors (θ) and an MLP translation network trained with adversarial losses (L_N-adv, L_color). Core assumption: Lighting affects facial appearance approximately uniformly within local patches, allowing per-patch parameterization to generalize across diverse illumination conditions. Also assumes the 200-scan target domain is representative of clean albedo space despite its limited size.

### Mechanism 2: Two-Step Disentanglement Training for Identity-Preserving Attribute Control
Separating labeled attributes (age, gender, ethnicity) from unlabeled identity information via a two-step adversarial autoencoder + conditional GAN enables smooth semantic editing while preserving identity, conditional on successful latent space disentanglement. Step 1 trains encoder E to discard labeled information using a conditional code discriminator D_E (trained to distinguish codes from prior N(0,1) given labels), forcing E(x) to contain only identity-relevant information. Step 2 trains conditional generator G₂(z, labels) with discriminator D_G that receives the identity code E(x) as spatial conditioning (via G₁ reconstruction), ensuring generated samples match the unlabeled identity features. Core assumption: The conditional code discriminator can effectively enforce that p(E(x)|labels) ≈ N(0,1) for all label values, and that the reconstruction loss (L_E-rec) preserves all identity-relevant information without leakage of labeled attributes.

### Mechanism 3: Knowledge Distillation from Diffusion to GAN for Real-Time Generation
Training a GAN on diffusion-generated data achieves 0.014s vs. 40s inference while maintaining quality, conditional on the synthetic training distribution covering the target generation space. The 44K normalized 3D face dataset bridges the diversity-quality gap: diffusion provides demographic diversity (14 ethnicities, 13 age groups), while normalization aligns textures with scanned albedo quality. The GAN (StyleGAN2 architecture) learns this combined distribution, enabling efficient sampling and latent space interpolation. Core assumption: The synthetic dataset's demographic sampling (uniform ethnicity, 45%/45%/10% male/female/unisex) produces a balanced generative model. Also assumes artifacts in raw diffusion outputs can be filtered via sanity checks (44K retained from 65K).

## Foundational Learning

- Concept: 3D Morphable Models (3DMM) and UV Parameterization
  - Why needed here: All generated faces share a template topology, enabling texture transfer, blendshape animation, and consistent UV-space representation across the 44K dataset.
  - Quick check question: Can you explain how a 3DMM represents both geometry and texture in a shared UV space, and why topology consistency matters for GAN training?

- Concept: Adversarial Training and Latent Space Disentanglement
  - Why needed here: The two-step training uses adversarial losses (L_E-adv, L_G-adv) to enforce disentanglement; understanding the min-max game dynamics is essential for debugging training instability.
  - Quick check question: What happens to the encoder E if discriminator D_E overpowers the adversarial loss during training?

- Concept: Physically-Based Rendering (PBR) and Albedo Maps
  - Why needed here: The normalization module specifically targets albedo extraction (reflectance without illumination), critical for downstream PBR pipelines requiring relightable assets.
  - Quick check question: Why can't baked-lighting textures be used directly in PBR rendering, and how does the paper's normalization differ from simple histogram matching?

## Architecture Onboarding

- Component map: Stable Diffusion v1.5 + ControlNet → 2D portraits → ReFA reconstruction → Texture completion (nearest-neighbor blending) → Normalization (patch CNN + MLP) → 44K albedo/geometry pairs → Two-step GAN (E, G₁, D_E) learns identity codes → Conditional generator G₂(z, labels) with discriminator D_G conditioned on identity codes → Super-resolution (1K→4K albedo) → Translation networks (albedo→specular/displacement) → Secondary assets (eyes, teeth)

- Critical path: The normalization module (Section 3.1.3) is the highest-risk component—its failure cascades to GAN training quality. Verify albedo extraction on held-out scans before full dataset generation.

- Design tradeoffs:
  - Dataset size vs. quality: 65K→44K filtering trades quantity for reliability; consider loosening sanity checks if demographic coverage is sparse.
  - Scan domain size: 200 scans enable clean albedo targets but limit skin-tone fidelity for rare appearances; consider augmenting with publicly available albedo datasets if demographic gaps emerge.
  - Discriminator conditioning: Using G₁ output as spatial conditioning for D_G helps but adds frozen-network dependency; if G₁ is poorly trained, D_G receives noisy signals.

- Failure signatures:
  - Normalization failure: Residual lighting in albedo (check cheek highlights); uneven skin tone across UV boundaries
  - Disentanglement failure: Identity drift when changing age/gender (visualize latent interpolations)
  - Training instability: D_E loss collapses to zero (encoder no longer challenged); common in adversarial setups
  - Demographic bias: GAN generates stereotyped features for certain ethnicity/age combinations (audit via classifier on generated samples)

- First 3 experiments:
  1. Validate normalization on scans: Render scanned data under synthetic lighting, normalize, compute PSNR vs. ground-truth albedo. Target: >24 PSNR (paper reports 24.67 vs. FFHQ-UV's 17.64).
  2. Ablate discriminator conditioning: Train G₂ with/without G₁ spatial conditioning; measure identity preservation (cosine similarity) on held-out identities under attribute edits.
  3. Demographic coverage audit: Train attribute classifier on normalized data; generate 1000 samples per demographic; verify classifier accuracy distribution matches test set (paper shows +15.8% race accuracy improvement, suggesting potential overfitting to training distribution).

## Open Questions the Paper Calls Out

- Can multi-view consistent texture synthesis methods replace the current nearest-neighbor retrieval approach for completing invisible UV regions (e.g., ears)? The authors state in Section 5, "Therefore, we aim to explore a multi-view consistent texture synthesis method that can improve our texture completion design." This remains unresolved because the current pipeline relies on blending projected frontal portraits with scanned textures retrieved via nearest-neighbor search, which fails if the synthetic portrait lacks a similar counterpart in the scanned database.

- Does incorporating general-purpose intrinsic decomposition as additional supervision improve the accuracy of texture normalization for rare appearances? The authors suggest in Section 5 that "With richer scanned-albedo coverage, leveraging general-purpose intrinsic decomposition as additional de-lighting supervision is a potential way to improve normalization further." This is unresolved because the current normalization model is constrained by a small target domain of only 200 scanned subjects, limiting its ability to faithfully normalize rare skin tones or appearances without manual skin color estimation.

- How can the pipeline be advanced to generate high-quality, diverse 3D face geometries without relying on the limited diversity of scanned datasets? Section 5 notes, "There is currently no controllable high-quality large-scale generative model for face geometry," and "the diversity of the initial geometries is still limited by the scanned dataset." This remains unresolved because while the pipeline successfully generates diverse textures from diffusion models, the geometry reconstruction remains anchored to a refined version of a scanned mesh, capping the geometric variety available to the generator.

## Limitations

- The texture normalization module's effectiveness hinges on the assumption that 200 scans provide sufficient albedo diversity, which is acknowledged as a potential limitation
- The demographic filtering (14 ethnicities, 45%/45%/10% gender split) may introduce synthetic bias despite aiming for balance
- The custom identity similarity metric lacks validation against established face embedding methods, creating uncertainty about the claimed preservation quality during edits

## Confidence

- High confidence: Runtime efficiency claims (0.014s vs 40s), PBR asset pipeline functionality, and dataset scale (44K models) are directly measurable and well-documented
- Medium confidence: Attribute control accuracy (race: 75.88%, gender: 91.93%) and normalization PSNR (24.67) are validated against reasonable baselines but depend on training stability
- Low confidence: Identity preservation during semantic editing and the two-step disentanglement mechanism's robustness require more rigorous validation with standard face recognition metrics

## Next Checks

1. Validate albedo normalization on held-out scans: Test the texture normalization module on scan data not used in training to verify PSNR claims and check for lighting artifacts in reconstructed albedos

2. Benchmark identity preservation with standard metrics: Compare the paper's identity similarity metric against ArcFace or FaceNet embeddings on same-subject attribute edits to validate the claimed 0.9594 gender edit similarity

3. Audit demographic bias in generated samples: Generate 1000 samples per demographic category and measure attribute classifier accuracy distribution to verify the model doesn't overfit to training demographics or amplify biases from the diffusion model source