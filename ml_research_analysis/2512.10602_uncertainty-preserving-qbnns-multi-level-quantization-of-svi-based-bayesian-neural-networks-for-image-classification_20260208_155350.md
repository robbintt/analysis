---
ver: rpa2
title: 'Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian
  Neural Networks for Image Classification'
arxiv_id: '2512.10602'
source_url: https://arxiv.org/abs/2512.10602
tags:
- quantization
- uncertainty
- bayesian
- bnns
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a multi-level quantization framework for\
  \ Stochastic Variational Inference (SVI)-based Bayesian Neural Networks (BNNs) that\
  \ addresses the computational and memory overhead of probabilistic models. The authors\
  \ develop three quantization strategies\u2014Variational Parameter Quantization\
  \ (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ)\u2014\
  and introduce logarithmic quantization for variance parameters and specialized activation\
  \ functions to preserve uncertainty estimation quality."
---

# Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification

## Quick Facts
- arXiv ID: 2512.10602
- Source URL: https://arxiv.org/abs/2512.10602
- Reference count: 40
- This paper introduces a multi-level quantization framework for SVI-based BNNs that enables 4-bit quantization while preserving both classification accuracy and uncertainty estimation quality.

## Executive Summary
This paper addresses the computational and memory overhead of Bayesian Neural Networks (BNNs) by introducing a multi-level quantization framework for Stochastic Variational Inference (SVI)-based models. The authors develop three quantization strategies—Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ)—along with logarithmic quantization for variance parameters and specialized activation functions. Through comprehensive experiments on Dirty-MNIST, they demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. The framework preserves the main benefit of BNNs—reliable uncertainty quantification—while enabling deployment on resource-constrained edge devices.

## Method Summary
The framework quantizes BNNs at three key locations: variational parameters (µ, σ) in the Guide (VPQ), sampled weights before forward pass (SPQ), and jointly (JQ). VPQ uses uniform quantization for means and logarithmic quantization for variances to prevent relative errors on small σ values. SPQ applies uniform quantization to sampled weights. JQ combines both for maximum memory reduction. The model uses SoftPlus activations to preserve uncertainty disentanglement, and weights are clipped to [-1,1] for bias-free layers. Training follows a two-stage approach: deterministic pretraining (1500 epochs) followed by quantized BNN training (1000 epochs) with KL-annealing.

## Key Results
- BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement
- Logarithmic quantization for variance parameters is essential for calibrated uncertainty estimation
- Smooth activation functions like SoftPlus are required to maintain uncertainty disentanglement under quantization
- Joint Quantization achieves up to 8× memory reduction compared to floating-point implementations with minimal degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-level quantization strategy with specialized encoding for variance preserves BNN uncertainty.
- Mechanism: The framework quantizes at three locations: variational parameters (VPQ), sampled weights (SPQ), and jointly (JQ). Crucially, it uses a logarithmic quantizer for variance parameters (σ), which are naturally represented in log-space. This prevents the massive relative errors that uniform quantization would inflict on the small values of σ, thereby preserving the core probabilistic structure needed for uncertainty estimation.
- Core assumption: The distributional semantics of the variational posterior can be preserved if parameters are quantized in a space that respects their mathematical role (e.g., log-space for variance).
- Evidence anchors:
  - [abstract] "Our logarithmic quantization for variance parameters... are essential for calibrated uncertainty estimation."
  - [section V & Fig. 5] Figure 5 illustrates how uniform quantization error becomes orders of magnitude larger than logarithmic quantization error for small σ values, which is critical due to the 1/σ² term in backpropagation.
  - [corpus] Corpus evidence for this specific multi-level, logarithmic approach is weak or missing; related work primarily focuses on single-level Post-Training Quantization.
- Break condition: Aggressive quantization (e.g., 2-bit) causes total collapse, or non-logarithmic quantization is used for variance.

### Mechanism 2
- Claim: Smooth activation functions like SoftPlus are required to maintain uncertainty disentanglement under quantization.
- Mechanism: The paper empirically shows that standard ReLU activations cause aleatoric and epistemic uncertainty to become entangled under quantization, whereas the smooth SoftPlus function maintains their separation. This suggests that the sharp discontinuities of ReLU interact poorly with low-precision arithmetic, disrupting the gradient flow necessary for the model to distinguish data noise from model ignorance.
- Core assumption: Assumption: The smoothness of the activation function is a causal factor in preserving the disentanglement of uncertainty components under quantization noise.
- Evidence anchors:
  - [abstract] Not explicitly mentioned in abstract.
  - [section V.A & Fig. 4] Figure 4 shows that at 7-bit SPQ, ReLU causes Ambiguous-MNIST and Fashion-MNIST uncertainties to cluster together, while SoftPlus keeps them distinct along the Softmax Entropy and Mutual Information axes, respectively.
  - [corpus] Corpus evidence is weak or missing.
- Break condition: Model uses non-smooth activations like ReLU.

### Mechanism 3
- Claim: A 4-bit precision threshold exists below which uncertainty fidelity degrades sharply.
- Mechanism: Empirical results on Dirty-MNIST demonstrate a non-linear degradation pattern. From 32-bit down to 4-bit, classification accuracy and uncertainty disentanglement are remarkably stable. However, the transition from 4-bit to 3-bit causes a significant degradation in uncertainty separation, and a further drop to 2-bit results in complete model collapse. This points to 4 bits as a critical information bottleneck for this architecture.
- Core assumption: The observed 4-bit threshold is a generalizable property of SVI-based BNNs for this class of problems.
- Evidence anchors:
  - [abstract] "BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement."
  - [section VI & Table I] Table I and Figure 6 show that at 4-bit JQ, accuracy (97.34%) and AUROC scores remain high. At 3-bit, uncertainty disentanglement visually deteriorates in scatter plots, and at 2-bit, accuracy drops to ~10%.
  - [corpus] Related work (Ferianc et al.) supports BNN robustness to low precision but does not identify this specific 4-bit inflection point.
- Break condition: Bit-width is reduced below 4 bits.

## Foundational Learning

- Concept: **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper's core contribution is preserving these two distinct uncertainty types. Aleatoric uncertainty (Softmax Entropy) is irreducible noise in the data (e.g., a blurry digit), while epistemic uncertainty (Mutual Information) is reducible model ignorance (e.g., an out-of-distribution image). The entire evaluation methodology depends on understanding this difference.
  - Quick check question: For an image of a shirt in a digit classifier, which uncertainty component should be high?

- Concept: **Stochastic Variational Inference (SVI)**
  - Why needed here: This is the probabilistic framework being quantized. SVI approximates the intractable true posterior by learning the parameters of a simpler distribution (e.g., Gaussian µ, σ) over each weight. The quantization strategies (VPQ, SPQ) directly target this learned distribution.
  - Quick check question: In SVI with a Gaussian posterior, does each weight have a single learned value or two learned values?

- Concept: **Quantization-Aware Training (QAT) vs. Post-Training Quantization (PTQ)**
  - Why needed here: This paper uses QAT (simulating quantization during training), while prior art uses PTQ. This is a critical distinction because QAT allows the model to adapt its weights to minimize the distortion caused by low precision, which is key to the authors' success at aggressive 4-bit levels.
  - Quick check question: Does the model learn to compensate for quantization noise during its training phase or only after it is trained?

## Architecture Onboarding

- **Component map:** Guide (holds variational parameters µ, σ) -> Weight Sampling -> SPQ (Uniform Quantizer A) -> Model (layers, SoftPlus activations) -> Output

- **Critical path:** The JQ forward pass: 1) Sample float weights from Guide, 2) Apply SPQ (Uniform Quantizer A), 3) Feed quantized weights and inputs through Model layers, 4) Output prediction. The backward pass uses straight-through estimators to update the Guide's parameters through the VPQ quantizers (Uniform B1 for µ, Logarithmic B2 for σ).

- **Design tradeoffs:**
  - **VPQ vs. JQ:** VPQ reduces model memory but not compute (samples are float). JQ reduces both, enabling integer arithmetic, but is more brittle.
  - **Logarithmic vs. Uniform Quantization:** Logarithmic for σ is more complex but essential for calibration; uniform for σ risks high error.
  - **SoftPlus vs. ReLU:** SoftPlus preserves uncertainty disentanglement but may have different latency/area characteristics on hardware compared to ReLU.

- **Failure signatures:**
  - **Accuracy Collapse:** Random-guess accuracy (~10% on MNIST) indicates total information loss (e.g., 2-bit).
  - **Uncertainty Entanglement:** In scatter plots, Ambiguous-MNIST and Fashion-MNIST samples form a single confused cluster instead of separating along the Softmax Entropy (x) and Mutual Information (y) axes.
  - **OOD Detection Failure:** A significant drop in AUROC for Fashion-MNIST (epistemic) against MNIST.

- **First 3 experiments:**
  1. **Replicate Baseline Plots:** Train a full-precision BNN on Dirty-MNIST and generate the Mutual Information vs. Softmax Entropy scatter plot. Verify that MNIST, Ambiguous-MNIST, and Fashion-MNIST form distinct clusters.
  2. **Ablate Quantizer for Variance:** Train two 4-bit JQ models. Use uniform quantization for σ in one and the recommended logarithmic quantization in the other. Compare their AUROC scores and scatter plots to validate the mechanism.
  3. **Find the Collapse Point:** Starting from 8-bit, iteratively reduce bit-width (7, 6, 5, 4, 3, 2) for a JQ model. Plot accuracy and both AUROC metrics to confirm the 4-bit threshold on your target architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the 4-bit quantization threshold and uncertainty-preserving findings generalize to convolutional and transformer-based BNN architectures?
- Basis in paper: [explicit] "Future work includes extending the study to convolutional and transformer-based BNNs"
- Why unresolved: This work only evaluated MLP architectures with two hidden layers of 100 neurons on Dirty-MNIST. CNNs and transformers have different parameter distributions, layer types, and uncertainty propagation characteristics that may interact differently with quantization.
- What evidence would resolve it: Replicating the multi-level quantization framework (VPQ, SPQ, JQ) on CNN-based BNNs (e.g., Bayesian ResNet) and transformer-based BNNs across standard vision benchmarks, measuring accuracy and uncertainty disentanglement at varying bit-widths.

### Open Question 2
- Question: How does activation quantization interact with the multi-level BNN quantization framework, and can full integer inference be achieved without degrading uncertainty estimates?
- Basis in paper: [explicit] "Future work includes... integrating activation quantization and full-integer inference" and [inferred] "Notably we do not quantize activations, as we are primarily interested in the extension of quantization to BNNs"
- Why unresolved: The current study deliberately excluded activation quantization to isolate the effects of weight distribution quantization. Activations may introduce additional quantization noise that compounds across layers and could disrupt the stochastic sampling process essential for epistemic uncertainty estimation.
- What evidence would resolve it: Experiments systematically adding activation quantization at different bit-widths (4-8 bits) to the existing framework, measuring the impact on aleatoric/epistemic uncertainty separation and calibration.

### Open Question 3
- Question: Can hardware-aware automated bit-width allocation methods be developed that jointly optimize for efficiency, accuracy, and uncertainty fidelity?
- Basis in paper: [explicit] "Future work includes... ideally coupled with automated, hardware-aware bit-width allocation, forming a promising path toward truly resource-efficient, uncertainty-aware learning"
- Why unresolved: Current approaches like HAQ optimize for accuracy and hardware constraints but do not incorporate uncertainty metrics. The three-level quantization structure (VPQ, SPQ, JQ) creates a complex design space where different components may require different precisions to preserve uncertainty.
- What evidence would resolve it: Development of a multi-objective optimization framework that takes accuracy, uncertainty calibration metrics (AUROC for aleatoric/epistemic), and hardware constraints as inputs and outputs layer-wise and component-wise bit-width assignments.

### Open Question 4
- Question: Do the Dirty-MNIST findings transfer to more complex, real-world datasets and higher-resolution imagery?
- Basis in paper: [inferred] "While these experiments were done only on Dirty-MNIST, they show a general trend and guideline, for initial experiments on more complex datasets"
- Why unresolved: Dirty-MNIST uses 28×28 grayscale images with simple digit patterns. Higher-resolution images with more complex features may have different uncertainty distributions and could exhibit different sensitivity to quantization-induced noise in the weight sampling process.
- What evidence would resolve it: Evaluation on standard BNN benchmarks with higher complexity (CIFAR-10/100, ImageNet subsets) and domain-specific datasets (medical imaging, autonomous driving) with corresponding OOD test sets.

## Limitations

- Limited dataset scope: Experiments conducted only on Dirty-MNIST (28×28 grayscale digits), raising questions about generalization to more complex datasets and higher-resolution imagery
- No statistical validation: Results lack confidence intervals or significance testing across multiple random seeds
- Unclear implementation details: The logarithmic quantizer for variance parameters is described as "custom" without mathematical specification, making exact reproduction difficult

## Confidence

- **High Confidence:** The observation that logarithmic quantization for variance parameters (σ) is superior to uniform quantization is well-supported by quantitative error analysis in Figure 5. The mechanism relating σ quantization to uncertainty calibration is mathematically sound given the 1/σ² term in backpropagation.
- **Medium Confidence:** The empirical claim that 4-bit quantization represents a threshold for maintaining both accuracy and uncertainty quality is supported by the presented data, though limited to a single architecture and dataset. The comparative advantage of the multi-level quantization framework over simpler approaches is reasonably demonstrated within the tested regime.
- **Low Confidence:** The claim that SoftPlus activations are required to maintain uncertainty disentanglement under quantization lacks rigorous validation. The paper does not compare against other smooth alternatives (e.g., GELU, ELU) or provide a theoretical explanation for why ReLU specifically causes failure.

## Next Checks

1. **Statistical Validation:** Repeat all experiments across 5 different random seeds and report mean ± standard deviation for all metrics (accuracy, AUROC, MI/SE scatter plot statistics). Test for statistically significant differences between quantization levels and quantization strategies.

2. **Architecture Generalization:** Apply the framework to a convolutional Bayesian neural network on CIFAR-10 or Fashion-MNIST classification. Validate whether the 4-bit threshold and the superiority of logarithmic quantization for variance parameters hold across different model architectures.

3. **Theoretical Analysis:** Derive the gradient distortion introduced by uniform vs. logarithmic quantization of variance parameters. Formalize the relationship between activation function smoothness and uncertainty disentanglement preservation under quantization noise, and validate predictions against additional activation functions beyond ReLU and SoftPlus.