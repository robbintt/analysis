---
ver: rpa2
title: Bayesian Social Deduction with Graph-Informed Language Models
arxiv_id: '2506.17788'
source_url: https://arxiv.org/abs/2506.17788
tags:
- reasoning
- players
- game
- evil
- grail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of social reasoning in large
  language models (LLMs), specifically their ability to infer unobservable beliefs
  and intentions in multi-agent settings. The authors evaluate LLMs in the social
  deduction game Avalon and find that while large reasoning models (LRMs) demonstrate
  strong performance, they require extensive test-time inference and degrade sharply
  when distilled to smaller, real-time-capable variants.
---

# Bayesian Social Deduction with Graph-Informed Language Models

## Quick Facts
- **arXiv ID**: 2506.17788
- **Source URL**: https://arxiv.org/abs/2506.17788
- **Reference count**: 40
- **Primary result**: GRAIL achieves 67% win rate against human players in Avalon, outperforming larger reasoning models while receiving higher qualitative ratings.

## Executive Summary
This paper addresses the challenge of social reasoning in large language models, specifically their ability to infer unobservable beliefs and intentions in multi-agent settings. The authors evaluate LLMs in the social deduction game Avalon and find that while large reasoning models demonstrate strong performance, they require extensive test-time inference and degrade sharply when distilled to smaller, real-time-capable variants. To overcome this limitation, the authors propose a hybrid reasoning framework called GRAIL that combines an LLM with a structured probabilistic graphical model for belief inference. GRAIL uses the LLM for language understanding and interaction while the factor graph tracks latent roles and beliefs by analyzing observed game events and social interactions.

## Method Summary
The GRAIL framework uses a bipartite factor graph to encode role variables and game-state variables, with max-product belief propagation computing approximate MAP assignments for hidden roles. A shared neural network approximates conditional probabilities p(role | game state) from historical game data, with temperature scaling applied for calibration. The LLM provides qualitative belief adjustments (higher/lower/same) rather than numeric probabilities, which are mapped to priors via tunable β values. The system integrates language understanding, probabilistic inference, and rule-based action selection to play as a Good agent in Avalon, achieving competitive performance with much larger models while maintaining real-time capability.

## Key Results
- GRAIL achieves 67% win rate against human players, the first language agent to defeat human players in social deduction games
- In agent-agent play, GRAIL matches or exceeds performance of much larger reasoning models (405B Llama reasoning agent)
- Human evaluation shows GRAIL receives higher qualitative ratings than both reasoning baselines and human teammates on contribution and helpfulness metrics

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Graphical Model Externalization
Decoupling belief inference from the LLM via a factor graph provides a stable reasoning backbone that is robust to model scale. A bipartite factor graph encodes role variables and game-state variables with hard constraints and soft factors, using max-product belief propagation to compute MAP assignments without autoregressive token generation.

### Mechanism 2: Language Prior Injection via Qualitative LLM Judgments
Using the LLM to provide qualitative belief adjustments (higher/lower/same) rather than numeric probabilities improves robustness while incorporating dialogue signals. After each dialogue round, the LLM outputs δⱼ ∈ {higher, lower, same}, which maps to a prior p(rⱼ) via tunable βₜ that increases across rounds.

### Mechanism 3: Neural Factor Function Approximation with Calibration
Approximating conditional probabilities p(rⱼ | game state) with a trained neural network, then applying temperature scaling, yields well-calibrated factor functions suitable for belief propagation. Each factor uses a shared feedforward network trained on 100K+ games, with post-hoc temperature scaling improving confidence-accuracy alignment.

## Foundational Learning

- **Factor Graphs and Belief Propagation**: Understanding how variables and factors compose, and how max-product message passing computes MAP beliefs. Quick check: Given a factor graph with a hard constraint that exactly k variables are positive, what happens if sum-product (vs. max-product) is used?

- **Neural Network Calibration (Temperature Scaling)**: Interpreting neural network outputs as probabilities requires calibration; uncalibrated outputs mislead belief propagation. Quick check: If a classifier outputs 0.8 confidence but only achieves 0.6 accuracy at that confidence, is it overconfident or underconfident?

- **Qualitative vs. Quantitative LLM Prompting**: The paper deliberately avoids asking LLMs for numeric probabilities, relying on ordinal judgments instead. Quick check: Why might asking an LLM "What is the probability X is Evil?" yield unreliable estimates compared to "Should your belief about X increase, decrease, or stay the same?"

## Architecture Onboarding

- **Component map**: LLM (GPT-4.1 / Llama 3.1) -> Factor Graph Engine (Pomegranate) -> Neural Factor Functions (PyTorch) -> Action Heuristic -> Game Output

- **Critical path**: Game state -> Factor graph update -> Belief propagation -> Beliefs -> LLM (prior adjustment + message generation) -> Action heuristic -> Propose/vote

- **Design tradeoffs**: Shared vs. per-node factor networks (shared mitigates positional bias but reduces expressiveness); qualitative vs. numeric priors (qualitative improves reliability but introduces βₜ sensitivity); max-product vs. sum-product (max-product targets MAP but provides less distributional information)

- **Failure signatures**: Beliefs fail to converge (KL divergence never drops below ε); agent hallucinates game events (message contradicts ground-truth state); sycophancy (large models agreeing with Evil players' requests); repetitive communication (temperature adjustments don't diversify outputs)

- **First 3 experiments**:
  1. Ablation on belief source: Run GRAIL in "Graph Only" mode (β = 0) vs. "LLM Only" mode across model sizes to confirm factor graph provides performance floor
  2. Convergence stress test: Construct adversarial game states and monitor belief convergence rate and KL divergence trajectory
  3. Calibration drift detection: Compare factor network calibration on held-out agent-agent games vs. human-agent games to quantify domain shift

## Open Questions the Paper Calls Out

- **Open Question 1**: Can GRAIL's factor graph approach be extended to construct effective deceptive agents (Evil players) that maintain cover while strategically sabotaging quests? The paper only evaluates GRAIL playing as Good agents; Evil agent performance remains poor despite attempts.

- **Open Question 2**: How does GRAIL's performance scale to experienced or expert Avalon players who employ more sophisticated strategies? The human study involved only "novice human players" (n=44), with no analysis of skill level as a moderating variable.

- **Open Question 3**: Can the Bayesian graphical approach transfer effectively to domains beyond social deduction games, such as embodied social reasoning, strategic negotiation, or multi-agent planning? The factor graph structure and conditional probability networks are domain-specific.

## Limitations

- Domain shift risk: Neural factor functions trained on agent-agent data but evaluated on human-agent games, with potential calibration drift
- Hyperparameter sensitivity: β schedule for qualitative prior injection is only qualitatively described without precise values
- Generalization beyond Avalon: Factor graph structure exploits Avalon-specific constraints requiring significant adaptation for other games

## Confidence

- **High confidence**: GRAIL's architectural design and its ability to achieve competitive performance with smaller models (win rates and belief F1 scores in agent-agent experiments)
- **Medium confidence**: Claim that GRAIL is "first language agent to defeat human players" (supported by 67% win rate but small human-agent sample size)
- **Medium confidence**: Mechanism claims about qualitative vs. quantitative LLM prompting improving robustness (supported by calibration improvements but lacks direct comparative experiments)
- **Medium confidence**: Neural factor function approximation approach (supported by calibration results but untested on truly out-of-distribution human play patterns)

## Next Checks

1. **Domain shift validation**: Evaluate GRAIL's factor functions on held-out human-agent game data or systematically compare calibration statistics between agent-agent and human-agent domains to quantify potential distribution shift

2. **β schedule sensitivity analysis**: Systematically vary the β schedule parameters (initial value, growth rate, final value) across a grid search to identify optimal settings and quantify performance sensitivity to this critical hyperparameter

3. **Prompting style ablation**: Conduct a controlled experiment comparing GRAIL's qualitative prompting approach against direct numeric probability estimation from the LLM, measuring both performance and calibration quality to validate claimed benefits of qualitative judgments