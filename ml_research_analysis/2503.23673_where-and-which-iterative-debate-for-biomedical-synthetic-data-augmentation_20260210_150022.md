---
ver: rpa2
title: 'WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation'
arxiv_id: '2503.23673'
source_url: https://arxiv.org/abs/2503.23673
tags:
- data
- biomedical
- sentence
- tasks
- augmented
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses data scarcity in biomedical NLP by proposing
  BioRDA, a rationale-based synthetic data augmentation method. BioRDA combines lexicon
  diversity and biomedical relation restriction to identify optimal replacement positions,
  followed by a multi-agent reflection system to ensure accurate word selection.
---

# WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation

## Quick Facts
- **arXiv ID**: 2503.23673
- **Source URL**: https://arxiv.org/abs/2503.23673
- **Reference count**: 36
- **Primary result**: BioRDA achieves 2.98% average performance improvement across biomedical NLP tasks using iterative debate and attribution-based augmentation

## Executive Summary
This paper addresses data scarcity in biomedical NLP by proposing BioRDA, a rationale-based synthetic data augmentation method. BioRDA combines lexicon diversity and biomedical relation restriction to identify optimal replacement positions, followed by a multi-agent reflection system to ensure accurate word selection. The approach improves model performance across four biomedical tasks (NER, RE, QA, TC) on nine datasets from BLURB and BigBIO benchmarks. BioRDA achieves an average performance improvement of 2.98% over baseline models, with the most notable gains in NER (99.71% F1) and RE (98.68% F1) when using BioLinkBERT-large.

## Method Summary
BioRDA operates through two phases: WHERE (Attribution Selector) and WHICH (Multi-Agent Debate). The Attribution Selector computes token contribution scores using Leave-One-Out for lexicon diversity and bio-relation similarity, selecting top-n common keywords for replacement. A T5-based generator creates augmented candidates while preserving biomedical syntax through a syntax re-fix module. The Multi-Agent Debate system employs n LLM agents in an iterative Advise-Reflect-Revise loop to validate replacements against word definitions, similarity, syntax, and usage examples before acceptance. The method uses small PLMs (BioLinkBERT, PubMedBERT) for attribution and large LLMs (LLaMA2, ChatGLM) for debate, with LLaMA2 showing superior performance for the debate phase.

## Key Results
- BioRDA achieves 2.98% average performance improvement over baseline models across nine biomedical datasets
- Notable gains in NER (99.71% F1) and RE (98.68% F1) when using BioLinkBERT-large
- Ablation study shows WHERE information contributes 1.1% F1 improvement on average
- Multi-agent debate system effectively filters counterfactual augmentations compared to single-pass generation

## Why This Works (Mechanism)

### Mechanism 1: Attribution-based Position Selection
Combining lexicon-level attribution with bio-relation restriction identifies replacement positions that preserve semantic integrity while introducing diversity. The Attribution Selector computes contribution scores using Leave-One-Out (LOO) for lexicon diversity (attr(e ← wi)) and bio-relation description similarity (attr(rbio ← wi)), then selects top-n common keywords with highest scores from both maps. Tokens contributing highly to both entity prediction AND relation preservation can be safely replaced without generating counterfactual instances.

### Mechanism 2: Multi-Agent Iterative Debate
Multi-agent iterative debate reduces mis-replacement by enforcing explicit reasoning about word definitions, similarities, syntax, and usage examples. n LLM agents alternate as judge; one proposes replacements, others review across four criteria, judge revises, then all score. Iterations continue until acceptance threshold σ is met. Distributed critique across multiple LLM instances catches domain-specific errors that single-pass generation misses.

### Mechanism 3: Relation-aware Generation
Incorporating bio-relation descriptions into both the attribution selector and decoder improves biomedical logical coherence of augmented instances. Relation definitions from datasets (e.g., DDI's "mechanism" type definition) are injected into contribution scoring and decoder generation (p(yi|y<i, H, rbio, s̄)), plus syntax re-fix module extracts key structures from similar sentences. Explicit relation descriptions provide semantic anchors that constrain generation to medically plausible outputs.

## Foundational Learning

- **Leave-One-Out (LOO) Attribution**: Core technique for computing token contribution scores; must understand how removing each token affects model predictions.
  - Quick check: Given sentence "Drug A inhibits Protein B," if removing "inhibits" changes relation prediction from "inhibition" to "no_relation," what is the contribution score for "inhibits"?

- **Counterfactual Data Augmentation**: Paper's central problem—naive augmentation produces semantically invalid instances that poison models. Must recognize why "dose"→"concentration" substitution is problematic.
  - Quick check: Why might replacing "did not confirm" with "denied" in a clinical context constitute a counterfactual augmentation?

- **Multi-Agent Debate Systems**: Implements the WHICH selection phase; understanding Advise-Reflect-Revise paradigm and acceptance thresholds is essential.
  - Quick check: If 3 agents score an augmented sentence as [6/10, 5/10, 7/10] and threshold σ=6.5, does the sentence pass? What happens next?

## Architecture Onboarding

- **Component map**: Attribution Selector -> Pseudo Data Generator -> Multi-Agent Debate System
- **Critical path**: Input sentence → Attribution Selector → masked template with entity markers → Masked template + bio-relation description + sampled key structure → T5 decoder → augmented candidate → Original + augmented → Multi-agent debate → score ≥ threshold → accept; else iterate
- **Design tradeoffs**: Small PLM vs Large LLM split (BioLinkBERT/PubMedBERT for WHERE, LLMs for WHICH); Number of agents n (more agents = more diversity but higher cost); Threshold σ (higher = higher precision but more iterations)
- **Failure signatures**: Low overlap between attrlogits and attrbio (few common keywords → sparse augmentation); Agent consensus on wrong answer (all miss domain nuance → accepted counterfactual data); Excessive iterations (threshold too high or poor agent alignment → timeout/resource exhaustion)
- **First 3 experiments**:
  1. Ablation on WHERE proportions: Vary WHERE information from 0% to 100% on held-out subset (500 instances) across NER/RE tasks to confirm contribution of attribution masking
  2. Single-agent vs multi-agent comparison: Run WHICH phase with 1 agent vs 3 agents vs 5 agents on 200 sentences, measuring acceptance rate, iteration count, and downstream task F1
  3. Cross-domain relation description test: Replace dataset-specific relation descriptions with generic definitions on DDI dataset; if performance drops >1% F1, confirms mechanism 3 (relation descriptions matter)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the Attribution Selector maintain semantic consistency in datasets that lack explicit textual definitions for bio-relations?
  - Basis: Method relies on "bio-relation description" (rbio) as supervised signal, noting BLURB datasets provide specific definitions
  - Evidence: Testing BioRDA on biomedical datasets lacking textual relation descriptions and measuring semantic drift vs description-rich datasets

- **Open Question 2**: Does multi-task training in models like BioLinkBERT introduce specific interpretative biases that degrade performance in Question Answering tasks?
  - Basis: Section V.C notes BioLinkBERT-large underperforms PubMedBERT in QA, suggesting "multitask training can also introduce interpretative biases"
  - Evidence: Ablation study comparing models pre-trained with/without multi-task objectives specifically on QA benchmarks

- **Open Question 3**: What is the computational efficiency trade-off of the iterative multi-agent debate system compared to single-pass generation methods?
  - Basis: Algorithm 1 describes "while" loop where multiple LLM agents iterate until acceptance score threshold is met
  - Evidence: Reporting average number of debate rounds and total inference time per augmented instance against simpler baselines like EDA or ChatGPT-one-shot

## Limitations
- Attribution selector assumes significant overlap between lexicon and bio-relation contribution maps, which may be minimal in domains with sparse entity-relation alignment
- Multi-agent configuration parameters (number of agents n, acceptance threshold σ) are unspecified, requiring empirical tuning
- Results validated only on BLURB and BigBIO benchmarks; generalization to clinical text, other languages, or non-biomedical domains remains untested

## Confidence

- **High confidence**: 2.98% average performance improvement and specific task gains (NER 99.71% F1, RE 98.68% F1) are well-supported by Table II results across nine datasets
- **Medium confidence**: Multi-agent debate mechanism's effectiveness relies on agent configuration that isn't fully specified, requiring empirical tuning for different biomedical domains
- **Low confidence**: Cross-domain generalization claims are unsupported; performance on clinical narratives, medical imaging reports, or biomedical data in other languages cannot be inferred from current results

## Next Checks

1. Cross-domain robustness test: Apply BioRDA to clinical text datasets (e.g., MIMIC-III clinical notes) and measure performance degradation relative to BLURB/BigBIO benchmarks. Document changes in attribution overlap and agent acceptance rates.

2. Ablation on relation description specificity: Systematically replace dataset-specific relation descriptions with generic medical definitions on the DDI dataset. Measure performance changes to quantify the contribution of domain-specific relation supervision.

3. Hyperparameter sensitivity analysis: Conduct grid search over agent count (1, 3, 5, 7) and acceptance thresholds (7.0, 8.0, 9.0/10) on a subset of datasets. Report trade-offs between acceptance rate, iteration count, and downstream task performance to establish optimal configurations.