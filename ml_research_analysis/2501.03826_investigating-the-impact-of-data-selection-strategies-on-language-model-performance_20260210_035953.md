---
ver: rpa2
title: Investigating the Impact of Data Selection Strategies on Language Model Performance
arxiv_id: '2501.03826'
source_url: https://arxiv.org/abs/2501.03826
tags:
- data
- selection
- dataset
- distribution
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates data selection strategies for language
  model pretraining, focusing on aligning training data with target distributions.
  The authors propose a hybrid importance resampling (HIR) method that combines n-gram
  features and neural embedding-based features to compute sample importance weights
  for data selection.
---

# Investigating the Impact of Data Selection Strategies on Language Model Performance

## Quick Facts
- **arXiv ID**: 2501.03826
- **Source URL**: https://arxiv.org/abs/2501.03826
- **Reference count**: 2
- **Primary result**: DSIR (n-gram-based data selection) outperforms HIR (neural embedding-based) and random selection on 5/6 GLUE tasks

## Executive Summary
This paper investigates data selection strategies for language model pretraining, focusing on aligning training data with target distributions. The authors propose a hybrid importance resampling (HIR) method that combines n-gram features and neural embedding-based features to compute sample importance weights for data selection. They compare HIR with random selection and DSIR (a n-gram-based method) using the Pile dataset and evaluate the selected data through language model pretraining and fine-tuning on GLUE benchmark tasks. Results show that DSIR consistently outperforms HIR and random selection across most GLUE tasks, demonstrating the effectiveness of n-gram-based token-level modeling for language model pretraining.

## Method Summary
The study compares three data selection methods: random selection, DSIR (distribution-sensitive importance resampling using n-gram features), and HIR (hybrid importance resampling using both n-gram and neural embedding features). For DSIR, samples are encoded using hashed n-gram features (10K dimensions), and importance weights are computed based on the ratio of target and raw data distributions. HIR uses SentenceTransformer embeddings (384 dimensions) with GMM density estimation to compute importance weights. The selected subsets are then used for BERT-style MLM pretraining followed by fine-tuning on GLUE benchmark tasks.

## Key Results
- DSIR achieves best results on 5/6 GLUE tasks (COLA: 28.42, MRPC: 82.84, QNLI: 85.76, RTE: 61.25, SST2: 87.35)
- HIR (neural-only) underperforms both DSIR and random selection
- Token-level n-gram features align better with MLM pretraining objectives than sentence-level semantic features
- Importance resampling with p(x)/q(x) weighting effectively aligns training data with target distributions

## Why This Works (Mechanism)

### Mechanism 1: Importance Resampling via Distribution Ratio
Selecting training samples with importance weights proportional to p(x)/q(x) aligns the raw dataset distribution with the target distribution, improving downstream task performance. Given a target distribution p and raw distribution q, the importance weight ω(x) = p(x)/q(x) upweights samples that are underrepresented in q relative to p. Resampling with probabilities proportional to ω yields a subset whose empirical distribution approximates p. Core assumption: The feature space used to estimate p̂ and q̂ sufficiently captures the properties relevant to downstream tasks.

### Mechanism 2: Hashed N-gram Features Capture Token-Level Pretraining Signals
N-gram features (unigrams and bigrams) capture local token-level statistics that align well with masked language modeling objectives, leading to better downstream performance than neural embeddings. DSIR hashes n-gram counts into a fixed-dimensional vector, estimates multinomial distributions over these features for both target and raw data, and computes importance weights. This captures word co-occurrence patterns directly relevant to next-token prediction. Core assumption: Token-level statistics are more aligned with language modeling pretraining objectives than sentence-level semantic features.

### Mechanism 3: Neural Embedding Features Capture Global Semantic Alignment
Sentence-level neural embeddings capture semantic and syntactic richness at a macro level, enabling distribution alignment when target and raw data differ in contextual properties. HIR uses SentenceTransformer (all-MiniLM-L6-v2) to embed samples in 384-dimensional space, fits GMMs to estimate p_nn and q_nn, and computes importance weights. This captures sentence-level semantic similarity beyond surface n-grams. Core assumption: GMM with diagonal covariance can accurately estimate densities in high-dimensional embedding space.

## Foundational Learning

- **Concept: Importance Sampling / Resampling**
  - Why needed here: Core mathematical framework for all selection methods; understanding why p(x)/q(x) produces a target-aligned sample is essential.
  - Quick check question: If q(x) = 0.01 for a sample but p(x) = 0.05, what is the importance weight and how does it affect selection probability?

- **Concept: Gaussian Mixture Models (GMM) for Density Estimation**
  - Why needed here: HIR relies on GMM to estimate p_nn and q_nn from embeddings; poor density estimation directly degrades importance weights.
  - Quick check question: Why might a diagonal covariance GMM fail to capture the true density of sentence embeddings compared to a full covariance GMM?

- **Concept: Masked Language Modeling (MLM) Pretraining Objective**
  - Why needed here: The paper's explanation for DSIR's superiority hinges on alignment between n-gram features and the token-prediction nature of MLM.
  - Quick check question: How does predicting masked tokens from local context relate to why token-level n-gram features might outperform sentence-level embeddings for pretraining data selection?

## Architecture Onboarding

- **Component map**: Target Data -> Feature Extractors (n-gram + neural) -> Distribution Estimators (multinomial + GMM) -> Importance Weight Calculator -> Resampler -> Pretrainer -> Evaluator

- **Critical path**: Feature extraction → Distribution estimation (fit multinomial/GMM) → Compute importance weights for all raw samples → Sort by ω → Select top-k → Pretrain model → Fine-tune and evaluate

- **Design tradeoffs**:
  - DSIR (n-gram) vs. HIR (neural): DSIR is computationally cheaper and captures token-level signal aligned with MLM; HIR captures semantic richness but requires GMM fitting in high-dimensional space and may misestimate densities.
  - GMM component count: 1000 for raw (high diversity) vs. 50 for target (focused); under/over-specifying affects density estimation quality.
  - α parameter: Set to 0 (pure neural) in this study; optimal mixing between n-gram and neural features unexplored.

- **Failure signatures**:
  - DSIR underperforms random: Target distribution may not differ meaningfully from raw at token level; check vocabulary overlap statistics.
  - HIR performs worse than DSIR: GMM density estimates unreliable (check log-likelihood convergence); embeddings may not separate target/raw distributions.
  - High variance across seeds: Selected subset too small or importance weights too peaked, causing low effective sample size.

- **First 3 experiments**:
  1. Reproduce DSIR baseline: Select 1.7M samples from Pile using n-gram DSIR targeting Gutenberg+Wikipedia; pretrain BERT-base; evaluate on GLUE. Verify SST2 ≈ 87.35%, RTE ≈ 61.25%.
  2. Ablate target distribution: Run DSIR with only Wikipedia vs. only Gutenberg as target to disentangle which target properties drive GLUE gains.
  3. Diagnose HIR density estimation: Plot log-likelihood curves during GMM fitting; visualize embedding space with t-SNE colored by target/raw membership to assess separability; check if importance weights are concentrated on few samples (low effective sample size).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal value for the weighting parameter $\alpha$ that balances n-gram and neural embedding contributions in the hybrid distribution?
- **Basis in paper**: The authors state that exploring the impact and strategies for selecting the weight parameter $\alpha$ is left as "avenue for future work" and that testing only $\alpha=0$ was a deliberate limitation due to complexity.
- **Why unresolved**: The study restricted its experiments to $\alpha=0$ (neural only) and compared it against DSIR (n-gram only), skipping the potentially superior intermediate combinations of the hybrid distribution $\hat{p}_{hybrid}(x)$.
- **What evidence would resolve it**: A sweep of $\alpha$ values (e.g., 0.1 to 0.9) during the data selection phase, followed by an evaluation of downstream GLUE performance to find a performance maximum.

### Open Question 2
- **Question**: Do the performance gains from n-gram and neural data selection methods generalize to full-scale datasets (e.g., the full 800GB Pile dataset)?
- **Basis in paper**: The authors acknowledge in the Limitations section that they used a "proportionally smaller raw dataset" and a smaller subset of instances due to computational constraints, noting this "might not generalize well to larger-scale datasets."
- **Why unresolved**: The observed trade-offs between token-level (DSIR) and contextual (HIR) features were observed on a reduced data regime; scaling laws may shift the effectiveness of these feature types.
- **What evidence would resolve it**: Replicating the DSIR and HIR selection pipelines on the complete Pile dataset and comparing the fine-tuned results against the current baseline.

### Open Question 3
- **Question**: Can advanced density estimation techniques improve the performance of neural-based data selection over the diagonal Gaussian Mixture Models (GMM) used in this study?
- **Basis in paper**: The Discussion notes that "applying a diagonal GMM to high-dimensional embedding spaces may not yield accurate probability estimates," suggesting this modeling choice is a bottleneck for the HIR method.
- **Why unresolved**: The inferior performance of HIR relative to DSIR may stem from the diagonal covariance assumption in the GMM failing to capture complex manifold structures in the embedding space, rather than a failure of neural features themselves.
- **What evidence would resolve it**: Implementing HIR with full-covariance GMMs or non-parametric density estimators (e.g., KDE or normalizing flows) to determine if neural features can outperform n-grams when modeled more accurately.

## Limitations

- Model architecture ambiguity: BERT variant specifications (layers, hidden size, attention heads) remain unspecified
- Feature representation quality concerns: Hashed n-grams may lose discriminative signal; GMM density estimation in high dimensions may be unreliable
- Target distribution composition uncertainty: Relative weighting and domain characteristics of Gutenberg vs Wikipedia targets are not analyzed

## Confidence

**High Confidence** - The experimental methodology for comparing selection strategies is well-specified and the relative performance ordering (DSIR > HIR > Random) appears robust across multiple GLUE tasks.

**Medium Confidence** - The theoretical justification for importance resampling (p(x)/q(x) weighting) is sound, but the empirical validation of whether selected subsets actually match the target distribution is limited.

**Low Confidence** - The HIR method's density estimation quality is questionable. The paper explicitly notes that diagonal GMM may not yield accurate probability estimates in high-dimensional embedding spaces, yet proceeds with this approach without alternative validation or comparison to full covariance models.

## Next Checks

1. **Distribution Alignment Validation** - Before pretraining, compute and visualize the empirical distribution of n-gram and embedding features in selected subsets versus target and raw datasets. Use KL divergence or Wasserstein distance to quantify alignment, verifying that importance weighting actually produces target-similar distributions rather than just upweighting individual samples.

2. **Effective Sample Size Analysis** - Calculate the effective sample size (ESS = (Σω_i)²/Σω_i²) for each selection method to determine whether importance weights are too peaked, reducing the diversity of the selected subset. Low ESS indicates high variance and potential pretraining instability.

3. **Cross-Validation of Target Properties** - Run ablation studies with different target distributions (Wikipedia-only, Gutenberg-only, combined in different ratios) to isolate which target characteristics drive GLUE performance improvements. This validates whether the selection method captures meaningful task-relevant signal versus domain-specific artifacts.