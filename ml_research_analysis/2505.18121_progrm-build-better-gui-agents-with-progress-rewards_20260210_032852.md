---
ver: rpa2
title: 'ProgRM: Build Better GUI Agents with Progress Rewards'
arxiv_id: '2505.18121'
source_url: https://arxiv.org/abs/2505.18121
tags:
- progress
- steps
- training
- trajectories
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PROGRM, a progress-based reward model designed
  to improve GUI agents by providing dense intermediate rewards during online reinforcement
  learning. Unlike traditional outcome rewards, which only signal success or failure
  at the end of a task, PROGRM estimates task progress at each step, enabling more
  informative and efficient training.
---

# ProgRM: Build Better GUI Agents with Progress Rewards

## Quick Facts
- arXiv ID: 2505.18121
- Source URL: https://arxiv.org/abs/2505.18121
- Reference count: 40
- Primary result: ProgRM achieves 62.00% success rate on WikiHow benchmark, outperforming proprietary models

## Executive Summary
This paper introduces PROGRM, a progress-based reward model designed to improve GUI agents through dense intermediate rewards during online reinforcement learning. Unlike traditional outcome rewards that only signal success or failure at task completion, PROGRM estimates task progress at each step, providing more informative feedback for training. The authors address the challenge of progress label annotation through an LCS-based self-annotation algorithm that automatically identifies key steps in trajectories without human effort. Evaluated on the WikiHow benchmark, GUI agents trained with PROGRM achieve state-of-the-art performance, surpassing proprietary models like Claude-3.7-Sonnet.

## Method Summary
PROGRM provides dense progress rewards by predicting task completion progress at each step during online RL training. The method uses an LCS-based self-annotation algorithm to discover key steps in successful trajectories and assign progress labels automatically. A Qwen2.5-7B model with MLP head and sigmoid output is trained on these self-labeled data using BCE loss. The resulting progress reward model is then used in online RL with REINFORCE++, achieving 62% success rate on WikiHow tasks. The approach addresses the exploration efficiency problem of sparse rewards by providing intermediate feedback throughout the trajectory.

## Key Results
- Achieves 62.00% success rate on WikiHow benchmark, outperforming Claude-3.7-Sonnet (56.00%) and ORM-based approaches
- Progress model false positive rate: 7.33% (LCS-based) vs 22.00% (outcome-based), showing better discrimination of partial progress
- History length k=1 performs best; longer histories degrade performance for typical GUI tasks
- Cross-Page tasks show highest improvement (65.67% SR) compared to In-Page (55.33%) and QA (62.67%) tasks

## Why This Works (Mechanism)

### Mechanism 1
Dense progress rewards enable more efficient RL training than sparse outcome rewards. Instead of binary success/failure at trajectory end, ProgRM computes r(p)t = Prog(st; g) - Prog(st-k; g), rewarding cumulative progress gain over k steps. This provides intermediate feedback that guides exploration. Core assumption: Progress toward goals can be meaningfully quantified at intermediate steps, not just at completion. Evidence: SSL paper confirms binary rewards "fail to capture quality differences among trajectories achieving identical outcomes."

### Mechanism 2
LCS-based recipe extraction identifies key steps without human annotation. Successful trajectories for the same task are grouped by similarity (threshold θL=0.6), then their Longest Common Subsequence is computed as a "recipe." Steps matching the recipe are key steps; others inherit preceding key step progress. Core assumption: Successful trajectories share common behavior patterns representing task milestones. Evidence: ProgRM false positive rate: 7.33% (LCS) vs 22.00% (ORM).

### Mechanism 3
Progress labels trained via BCE loss produce lower false positive rates than outcome-based models. The progress model (LLM + MLP + sigmoid) outputs [0,1] progress values, trained with binary cross-entropy on self-annotated labels. This learns graded estimation rather than binary classification. Core assumption: The self-annotated progress labels approximate true task progress sufficiently for training. Evidence: ORM tends to assign scores close to either 0 or 1 while ProgRM assigns moderate credit to partial progress.

## Foundational Learning

- **Reinforcement Learning (sparse vs dense rewards)**
  - Why needed here: ProgRM's core contribution is converting sparse trajectory-level feedback to dense step-level rewards.
  - Quick check question: Can you explain why sparse rewards cause exploration efficiency problems in long-horizon tasks?

- **Longest Common Subsequence (LCS) algorithm**
  - Why needed here: Recipe extraction relies on computing LCS across trajectory groups with soft matching.
  - Quick check question: How would standard LCS differ from "soft" LCS when matching actions with natural language arguments?

- **Binary Cross-Entropy for regression-style prediction**
  - Why needed here: Progress values [0,1] are trained with BCE despite being continuous, not discrete.
  - Quick check question: Why might BCE be preferred over MSE for normalized progress prediction?

## Architecture Onboarding

- Component map:
  Recipe Library -> Progress Reward Model -> RL Trainer
  (LCS patterns per task) (Qwen2.5-7B + MLP + sigmoid) (REINFORCE++ with progress rewards)

- Critical path:
  1. Collect successful trajectories per task using base LLM
  2. Group by SoftLCS similarity, extract recipes
  3. Match new trajectories to recipes, identify key steps
  4. Train ProgRM on self-labeled progress data
  5. Use ProgRM rewards in online RL with REINFORCE++

- Design tradeoffs:
  - History length k=1 vs k>1: Paper finds k>1 degrades performance for typical GUI tasks
  - Environment-reward vs LCS-based labels: Env-based achieves 62% SR vs 59.33% for LCS-based, but requires milestone annotations
  - Soft match function design: Currently uses SBERT for text actions; domain-specific tuning may help

- Failure signatures:
  - High "useless repetition" errors → progress model not penalizing non-productive actions
  - "Article not found" errors → exploration phase issues, not reward model
  - High false positive rate → ORM-like behavior, check if progress model collapsed to binary

- First 3 experiments:
  1. Validate labeling quality: Compare LCS-based key steps against ground-truth milestone rewards using completion ratio metrics
  2. Ablate history length k: Test k∈{1,3,5} on held-out tasks to confirm paper's finding that k=1 is optimal
  3. Cross-environment transfer: Train ProgRM on one app, evaluate labeling quality on another Android app to test generalization

## Open Questions the Paper Calls Out

- Can PROGRM maintain its superiority in diverse GUI environments beyond the WikiHow benchmark?
- How can the LCS-based self-annotation algorithm be optimized to close the performance gap with environment-reward-based labeling?
- Can extended RL training cycles fully restore the general language capabilities lost during the supervised fine-tuning phase?
- Can PROGRM be effectively integrated with other reinforcement learning algorithms besides REINFORCE++?

## Limitations

- Self-annotation reliability depends on trajectory similarity and shared solution patterns, but validation of key step identification accuracy is limited
- Cross-task generalization performance remains unclear without evaluation on diverse GUI environments
- Environment dependency suggests results may be benchmark-specific rather than algorithmically general

## Confidence

- High confidence: Dense reward mechanism well-supported by RL theory and empirical false positive rate comparison
- Medium confidence: LCS-based labeling algorithm logically sound but lacks validation of key step identification accuracy
- Low confidence: Training efficiency improvements asserted but not quantified with direct sample efficiency comparisons

## Next Checks

1. Ground-truth validation of key steps: Compare LCS-identified key steps against environment-provided milestone rewards to validate self-annotation accuracy
2. History length ablation across task categories: Systematically test k∈{1,3,5} on all test categories to confirm optimal history length
3. Cross-environment transfer study: Train PROGRM on WikiHow, evaluate progress labeling quality on different GUI environment to assess generalization limits