---
ver: rpa2
title: 'Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data
  Categorization'
arxiv_id: '2510.13885'
source_url: https://arxiv.org/abs/2510.13885
tags:
- categorization
- text
- llms
- taxonomy
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates ten leading large language models (LLMs) on
  unstructured text categorization using the IAB 2.2 taxonomy, a hierarchical industry
  standard with 690 categories. Each model was tested on 8,660 human-annotated documents
  using a uniform zero-shot prompting strategy, and performance was measured with
  both classic metrics (accuracy, precision, recall, F1-score) and LLM-specific indicators
  (hallucination ratio, inflation ratio, categorization cost).
---

# Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization

## Quick Facts
- **arXiv ID:** 2510.13885
- **Source URL:** https://arxiv.org/abs/2510.13885
- **Reference count:** 9
- **Primary result:** Despite advances in LLM capabilities, classic performance remains modest with average accuracy of 34%, precision of 42%, recall of 45%, and F1-score of 41% on unstructured text categorization using IAB 2.2 taxonomy.

## Executive Summary
This paper evaluates ten leading large language models on unstructured text categorization using the IAB 2.2 taxonomy, a hierarchical industry standard with 690 categories. Each model was tested on 8,660 human-annotated documents using a uniform zero-shot prompting strategy. The study reveals that raw model scale does not guarantee better categorization performance, as the task requires disciplined semantic compression into a sparse taxonomy. The authors demonstrate that ensemble-based approaches, where multiple LLMs act as independent experts, substantially improve accuracy, completely eliminate hallucinations, and reduce category inflation, suggesting that coordinated orchestration of models may be more effective than scale alone for achieving human-expert performance in large-scale text categorization.

## Method Summary
The study uses 8,660 human-annotated text samples from open news corpora, applying a zero-shot prompting strategy with iterative hierarchy-aware classification (Tier-1→Tier-2→Tier-3→Tier-4). Ten models were evaluated including Claude 3.5, Gemini 1.5/2.0 Flash, LLaMA 3 8B/3.3 70B, Mistral, Grok, DeepSeek, and GPT 20B/120B. Performance was measured using classic metrics (accuracy, precision, recall, F1-score) plus LLM-specific metrics (hallucination ratio, inflation ratio, categorization cost). The Parent Exclusion Rule was applied to handle hierarchical redundancy. Exact prompt templates were provided, though specific hyperparameters were not fully specified in the paper.

## Key Results
- All models showed modest classic performance with average accuracy of 34%, precision of 42%, recall of 45%, and F1-score of 41%
- Models consistently overproduced categories compared to human annotations, with inflation ratios averaging 209%
- Hallucination ratios ranged from 0.7% to 5.9%, with GPT 120B achieving the lowest rate at 0.7%
- Ensemble methods completely eliminated hallucinations and reduced category inflation while improving accuracy
- Cost analysis identified Gemini 1.5/2.0 Flash, GPT 20B/120B, and DeepSeek as most cost-efficient options

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Orchestration Eliminates Hallucinations Through Consensus
- Claim: Coordinating multiple LLMs as independent experts can eliminate hallucinations and reduce category inflation.
- Mechanism: Each model independently categorizes text; consensus/aggregation filters out spurious predictions (categories outside the taxonomy or overly specific labels that lack multi-model support). The intersection of valid predictions converges on reliable categories.
- Core assumption: Errors are uncorrelated across models; hallucinations and over-generation patterns differ by architecture/training.
- Evidence anchors: [abstract] "The ensemble method... substantially improved accuracy, reduced inflation, and completely eliminated hallucinations." [section 6] "The ensemble framework... completely eliminated hallucinations and reduced category inflation."

### Mechanism 2: Taxonomy Compression Requires Structured World Models, Not Just Scale
- Claim: Raw model scale does not guarantee better categorization because the task demands disciplined semantic compression into a sparse, predefined label space.
- Mechanism: Categorization forces high-dimensional unstructured text into 690 discrete IAB categories—a many-to-few mapping. Next-token prediction pretraining optimizes for fluency, not for consistent taxonomy-aware retrieval. Without explicit schema internalization, models default to over-generation (inflation) or invent labels (hallucination).
- Core assumption: The IAB taxonomy's 690 categories are insufficient to reflect real-world semantic diversity, creating inherent compression loss.
- Evidence anchors: [section 6] "Categorization requires compressing arbitrary, unstructured texts into a sparse, predefined label space... Standard next-token prediction pretraining does not guarantee this structured compression ability."

### Mechanism 3: Hallucination Rate Predicts Practical Reliability More Than F1 Alone
- Claim: Low hallucination ratio (not just F1) is the critical production metric for taxonomy-constrained tasks.
- Mechanism: Classic metrics reward recall but don't penalize invalid predictions. Hallucination ratio directly measures constraint violations. GPT 120B achieved the lowest hallucination (0.7%) while maintaining competitive F1 (0.53), making it safer for automated pipelines where invalid labels require manual review.
- Core assumption: Downstream systems cannot tolerate out-of-taxonomy outputs without human intervention.
- Evidence anchors: [section 5, Table 3] GPT 120B: 0.7% hallucination rate vs. 1.0–5.9% for others; F1 = 0.53.

## Foundational Learning

- **Hierarchical Taxonomy Navigation**
  - Why needed here: The IAB 2.2 taxonomy is 4-tier deep with 690 categories. Models must traverse parent-child relationships correctly; errors cascade across levels.
  - Quick check question: Can you explain why predicting a Tier-4 category without its Tier-1 parent might be invalid in some taxonomies but acceptable in others?

- **Zero-Shot Classification Constraints**
  - Why needed here: The study uses zero-shot prompts with no task-specific training. Performance ceiling is bounded by pretraining exposure to the IAB schema and generalization capacity.
  - Quick check question: What is the difference between zero-shot classification with an explicit label list vs. open-ended generation? Which is more constrained?

- **LLM-Specific Evaluation Metrics (Hallucination & Inflation)**
  - Why needed here: Standard F1/accuracy miss critical failure modes unique to generative models—fabricating labels or over-assigning categories.
  - Quick check question: If a model assigns 8 categories where humans assigned 2, and 6 are valid but redundant, does this hurt precision, recall, inflation ratio, or all three?

## Architecture Onboarding

- **Component map:**
  - Input layer: Unstructured text samples (8,660-sample benchmark) → Preprocessing/normalization
  - Prompt layer: Hierarchy-aware prompts with embedded taxonomy constraints (Tier-1 → Tier-4 iterative refinement)
  - Model layer: Single LLM (baseline) or Ensemble of multiple LLMs (orchestrated)
  - Output layer: Normalized IAB labels → Hallucination filter → Inflation correction (Parent Exclusion Rule)
  - Evaluation layer: Classic metrics (accuracy, precision, recall, F1) + LLM-specific (hallucination ratio, inflation ratio, cost)

- **Critical path:**
  1. Define taxonomy constraints in prompt (allowable children, canonical IDs, formatting rules).
  2. Execute iterative hierarchical classification (Tier-1 first, then refine).
  3. Normalize outputs to canonical IAB labels.
  4. Apply hallucination filter (reject out-of-taxonomy predictions).
  5. If ensemble: aggregate across models via consensus/voting.
  6. Compute all metrics; log cost per 1M tokens.

- **Design tradeoffs:**
  - Single model vs. ensemble: Ensemble adds latency and cost (3–5× inference calls) but eliminates hallucinations and reduces inflation.
  - Cost-performance: Gemini 1.5/2.0 Flash and GPT 20B/120B offer best price/performance; Mistral ($8/1M tokens) is expensive for high-volume tasks.
  - Temperature/top-k tuning: Paper found negligible impact on categorization quality for short, schema-constrained outputs—use defaults.

- **Failure signatures:**
  - High hallucination (>5%): Indicates poor schema grounding; likely in smaller or less instruction-tuned models (LLaMA 3 8B: 5.4%, LLaMA 3.3 70B: 5.9%).
  - High inflation (>2.0 ratio): Model over-generates; may need prompt tightening or ensemble pruning.
  - Low accuracy + high recall: Model is "trigger-happy"—assigning many categories to maximize coverage at precision cost.

- **First 3 experiments:**
  1. **Baseline single-model benchmark**: Run GPT 120B on a 500-sample subset; measure all 7 metrics. Confirm hallucination <1% and inflation <1.5×.
  2. **Ensemble MVP**: Combine 3 models (e.g., GPT 120B, Gemini 2.0 Flash, Claude 3.5) with majority-vote aggregation. Verify hallucination drops to zero.
  3. **Cost-ablation test**: Compare per-category cost across models. Identify if cheaper models (Gemini Flash) can replace expensive ones (Claude) in the ensemble without accuracy loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific ensemble design (voting mechanism, model selection, aggregation strategy) optimally balances categorization accuracy against computational cost?
- Basis in paper: [explicit] "While a detailed analysis of ensemble design lies beyond the scope of this paper, these findings suggest that orchestration—rather than scale alone—offers a promising path"
- Why unresolved: The paper reports ensemble results but defers architectural details to a separate study.
- What evidence would resolve it: Systematic comparison of ensemble configurations (majority voting vs. weighted averaging vs. cascaded verification) with cost-accuracy trade-off curves.

### Open Question 2
- Question: Does LLM categorization performance plateau at a specific parameter scale for hierarchical taxonomy tasks?
- Basis in paper: [explicit] "We hypothesize that for categorization tasks, performance improvements diminish beyond a certain scale threshold"
- Why unresolved: The hypothesis is stated but not empirically tested across a controlled range of model sizes.
- What evidence would resolve it: Controlled experiments with model families at multiple scale points (e.g., 7B, 13B, 70B, 120B) on identical categorization tasks.

### Open Question 3
- Question: Would few-shot prompting or taxonomy-specific fine-tuning substantially improve zero-shot categorization performance?
- Basis in paper: [inferred] The study uses only zero-shot prompting; prior work (Edwards and Camacho-Collados, 2024) shows in-context learning results vary with taxonomy complexity.
- Why unresolved: No comparison between zero-shot, few-shot, and fine-tuned approaches is provided.
- What evidence would resolve it: Experiments comparing zero-shot, few-shot (3–10 examples), and LoRA-based fine-tuning on the same IAB 2.2 benchmark.

## Limitations
- The ensemble method's effectiveness depends critically on error independence across models—if models share systematic biases or taxonomy exposure, hallucination elimination may not scale
- The taxonomy compression argument, while compelling, is primarily inferential with limited direct empirical evidence linking model pretraining objectives to categorization failures
- Cost analysis is based on public pricing without accounting for potential enterprise discounts or API-specific optimizations

## Confidence
- **High confidence**: Classic performance metrics (accuracy, precision, recall, F1) and their relative rankings across models; hallucination and inflation ratios as defined metrics; cost-efficiency rankings based on public pricing
- **Medium confidence**: Claims about ensemble hallucination elimination (dependent on independent error assumptions); taxonomy compression bottleneck argument (primarily inferential)
- **Low confidence**: Direct causal link between pretraining objectives and categorization failures; generalizability of ensemble approach beyond IAB taxonomy

## Next Checks
1. Conduct error correlation analysis across ensemble models to verify independence assumptions; if correlation >0.5 for common errors, ensemble benefits may diminish
2. Test taxonomy-specific fine-tuning on a subset of models to quantify the impact of schema internalization vs. scale alone
3. Implement A/B testing comparing ensemble performance against single high-performing models (GPT 120B) on out-of-domain taxonomies to assess generalization