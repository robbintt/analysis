---
ver: rpa2
title: PTMs-TSCIL Pre-Trained Models Based Class-Incremental Learning
arxiv_id: '2503.07153'
source_url: https://arxiv.org/abs/2503.07153
tags:
- learning
- feature
- drift
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first exploration of pre-trained models
  (PTMs) for time series class-incremental learning (TSCIL), addressing the challenge
  of catastrophic forgetting when learning new classes sequentially. The proposed
  method leverages frozen PTM backbones with shared adapters for parameter-efficient
  tuning, combined with knowledge distillation and a two-stage Feature Drift Compensation
  Network (DCN) to correct feature drift across tasks.
---

# PTMs-TSCIL Pre-Trained Models Based Class-Incremental Learning

## Quick Facts
- arXiv ID: 2503.07153
- Source URL: https://arxiv.org/abs/2503.07153
- Reference count: 40
- Achieves 1.4%-6.1% accuracy gains over existing PTM-based approaches

## Executive Summary
This paper introduces the first exploration of pre-trained models (PTMs) for time series class-incremental learning (TSCIL). The method addresses catastrophic forgetting by leveraging frozen PTM backbones with shared adapters for parameter-efficient tuning. A two-stage Feature Drift Compensation Network (DCN) is employed to correct feature drift across tasks by learning feature space transformations that accurately project old class prototypes into new feature spaces.

## Method Summary
The proposed approach utilizes frozen PTM backbones with shared adapters for parameter-efficient tuning, combined with knowledge distillation to preserve previous task knowledge. The key innovation is the two-stage Feature Drift Compensation Network (DCN) that learns transformations to correct feature drift when new classes are introduced. This enables accurate classifier retraining by projecting old class prototypes into the new feature space.

## Key Results
- Achieves 1.4%-6.1% accuracy gains compared to existing PTM-based approaches
- Demonstrates state-of-the-art performance across five real-world datasets
- Effectively addresses catastrophic forgetting in time series class-incremental learning scenarios

## Why This Works (Mechanism)
The method works by freezing pre-trained model backbones to capture temporal dynamics while using shared adapters for efficient parameter tuning. Knowledge distillation preserves previous task knowledge during new class learning. The two-stage Feature Drift Compensation Network learns transformations to align feature spaces across tasks, preventing the drift that typically causes catastrophic forgetting in incremental learning scenarios.

## Foundational Learning
- **Time Series Class-Incremental Learning (TSCIL)**: Sequential learning of new classes while preserving old class performance - needed to handle streaming time series data; quick check: verify temporal dependencies are maintained across incremental steps
- **Catastrophic Forgetting**: Model degradation on previous tasks when learning new tasks - fundamental challenge in incremental learning; quick check: measure performance drop on old classes after new class introduction
- **Feature Drift Compensation**: Correcting feature space misalignment across incremental tasks - critical for maintaining classifier accuracy; quick check: validate prototype projection accuracy between feature spaces
- **Parameter-Efficient Tuning**: Using adapters instead of full fine-tuning - reduces computational overhead and overfitting risk; quick check: compare parameter count and performance against full fine-tuning
- **Knowledge Distillation**: Transferring knowledge from old to new model versions - preserves learned representations; quick check: measure knowledge retention through distillation loss
- **Prototype-based Classification**: Using class prototypes for decision boundaries - enables efficient classifier updates; quick check: verify prototype stability across incremental steps

## Architecture Onboarding
**Component Map**: Frozen PTM -> Shared Adapters -> Knowledge Distillation -> DCN (Stage 1) -> DCN (Stage 2) -> Classifier Update

**Critical Path**: The DCN's two-stage transformation learning process is the critical path, as accurate feature space alignment directly determines classifier retraining success and final accuracy.

**Design Tradeoffs**: Frozen PTMs provide parameter efficiency but may limit adaptation to task-specific temporal patterns. The shared adapter approach balances efficiency with flexibility. DCN adds computational overhead but is essential for feature alignment.

**Failure Signatures**: Poor feature drift compensation leading to classifier degradation, adapter overfitting to new tasks, knowledge distillation failure causing catastrophic forgetting, or DCN transformation errors causing prototype misalignment.

**First Experiments**:
1. Validate frozen PTM effectiveness vs. fine-tuned alternatives on small-scale incremental tasks
2. Test DCN performance with synthetic feature drift scenarios
3. Measure parameter efficiency gains from shared adapters vs. task-specific adapters

## Open Questions the Paper Calls Out
None

## Limitations
- DCN scalability and performance on larger-scale, more diverse time series data remains unverified
- Computational overhead of DCN may be prohibitive for real-time or resource-constrained applications
- Knowledge distillation approach may face challenges with significantly more classes or tasks
- Frozen PTM assumption for capturing temporal dynamics lacks comprehensive ablation validation

## Confidence
- **High**: Core framework effectiveness and accuracy improvements
- **Medium**: Feature Drift Compensation Network scalability
- **Medium**: Parameter efficiency claims for shared adapters

## Next Checks
1. Conduct extensive ablation studies comparing frozen PTMs against fine-tuned alternatives across varying time series domains
2. Evaluate DCN performance and computational efficiency on larger-scale datasets with 20+ classes and tasks
3. Test the framework's robustness to varying incremental learning scenarios, including class-imbalanced and noisy time series data