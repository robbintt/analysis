---
ver: rpa2
title: 'Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools'
arxiv_id: '2508.02110'
source_url: https://arxiv.org/abs/2508.02110
tags:
- tool
- attack
- agent
- should
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Attractive Metadata Attack (AMA), a novel
  black-box attack that manipulates the metadata (e.g., name, description, parameters)
  of malicious tools to induce LLM agents to invoke them. AMA frames metadata crafting
  as a state-action-value optimization problem using in-context learning, iteratively
  generating tool metadata that maximizes selection likelihood.
---

# Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools

## Quick Facts
- arXiv ID: 2508.02110
- Source URL: https://arxiv.org/abs/2508.02110
- Reference count: 40
- Primary result: Novel black-box attack that manipulates tool metadata to induce LLM agents to invoke malicious tools with 81%-95% success rates

## Executive Summary
This paper introduces Attractive Metadata Attack (AMA), a novel black-box attack that manipulates the metadata (e.g., name, description, parameters) of malicious tools to induce LLM agents to invoke them. AMA frames metadata crafting as a state-action-value optimization problem using in-context learning, iteratively generating tool metadata that maximizes selection likelihood. Experiments across ten realistic scenarios and four popular LLM agents show AMA achieves high attack success rates (81%-95%), causes significant privacy leakage, and remains effective under prompt-level defenses, auditor-based detection, and structured protocols like MCP. The attack is orthogonal to injection attacks and can be combined with them for stronger efficacy, revealing systemic vulnerabilities in current agent architectures.

## Method Summary
AMA is a black-box attack that optimizes tool metadata to maximize selection probability by LLM agents. The method uses state-action-value optimization with in-context learning: starting with malicious tool seeds, it iteratively generates candidate tool descriptions using an LLM, evaluates their selection likelihood across a fixed query set, and updates the population based on weighted fitness scores. The optimization loop runs for a maximum of 5 iterations with batch size 10, using models like Gemma3-27B, LLaMA3.3-70B, Qwen2.5-32B, and GPT-4o-mini. The attack targets the agent's tool selection mechanism by crafting semantically attractive descriptions that appear relevant regardless of the specific task, exploiting the implicit trust agents place in tool metadata.

## Key Results
- AMA achieves 81%-95% attack success rates across ten realistic attack scenarios
- The attack causes significant privacy leakage by inducing agents to invoke malicious tools
- AMA remains effective against prompt-level defenses, auditor-based detection, and MCP protocols
- The attack is orthogonal to injection attacks and can be combined with them for enhanced efficacy

## Why This Works (Mechanism)

### Mechanism 1: Semantic Primacy in Tool Selection
LLM agents prioritize tool metadata (descriptions/names) over functional verification when selecting tools. The agent uses a scoring function dependent on semantic similarity between the query and tool description, and AMA optimizes this description to maximize the score, effectively "hacking" the relevance ranking. The core assumption is that agents trust tool metadata accurately reflects the tool's backend code behavior.

### Mechanism 2: Query-Agnostic Optimization (The "Universal" Tool)
AMA can craft a single tool description that ranks highly across diverse queries by optimizing for broad semantic "attractiveness." The attack iteratively refines descriptions using terms like "comprehensive" or "versatile" to maximize selection probability across a distribution of queries, creating a tool that appears relevant regardless of the specific task.

### Mechanism 3: Orthogonality to Prompt Defenses
Standard prompt-level defenses (sanitization, guardrails) fail because the attack resides in the tool definition rather than the user prompt. Defenses like "Rewrite" or "Refuge" sanitize the user query or inject safety rules into the system prompt, but they do not scrutinize the content of the tool list passed to the model. Since the malicious instruction is the tool definition itself, it bypasses input filters designed to stop injection attacks.

## Foundational Learning

**ReAct Paradigm (Reason + Act):** Understanding where the attack injects itself - in ReAct, the agent "Observes" the tool list before "Acting." AMA targets this specific observation step. *Quick check:* Does the defense protect the user prompt, or the tool list provided to the agent?

**Black-Box Optimization:** The attacker treats the target agent as an Oracle, observing outputs to refine the attack without needing model weights. *Quick check:* If you cannot see the model's confidence scores, how would you verify if a tool description is "attractive" enough? (Answer: Check if the agent invokes it).

**Context Window Priority:** Tool definitions occupy prime real estate in the context window and are often structured to be highly accessible to the model's attention mechanism. *Quick check:* Does placing the malicious tool first or last in the list affect its selection probability? (Evidence suggests "attractiveness" outweighs position, but position is a factor).

## Architecture Onboarding

**Component map:** Attacker Module (Optimization Loop: Generator LLM -> Value Estimator) -> Target Agent (System Prompt + User Query + Tool List -> Selection) -> Interface (API/Protocol that fetches and formats the Tool List)

**Critical path:** 1) Injection: Malicious tool is added to the tool repository, 2) Retrieval: Agent fetches tool list including malicious tool, 3) Selection (Vulnerability): Agent scores malicious tool higher due to optimized metadata, 4) Execution: Agent calls malicious tool with parameters (data exfiltration)

**Design tradeoffs:** Open ecosystems vs. security (allowing third-party tools maximizes capability but introduces attack surface), semantic matching vs. strict typing (flexible natural language descriptions aid usability but enable semantic manipulation)

**Failure signatures:** The "Universal Tool" Pattern (a tool claiming to solve everything invoked for unrelated tasks), Data Exfiltration (tool parameters request sensitive data irrelevant to stated function)

**First 3 experiments:**
1. Reproduction (Untargeted): Implement the "Universal Solution Aggregator" optimization loop against a local Llama-3 model. Measure the selection rate against a standard toolset.
2. Defense Evasion: Apply a prompt-sanitization defense to the System Prompt. Verify if the "Universal Tool" is still selected.
3. Transfer Test: Generate an optimized tool description using GPT-4o-mini and test its success rate against a Qwen-2.5 agent to measure cross-model transferability.

## Open Questions the Paper Calls Out

**Open Question 1:** What execution-level security mechanisms can effectively neutralize metadata manipulation without disrupting the functional integrity of benign tool invocation? The paper identifies this as future work because current prompt-level safeguards and auditor-based detection failed to stop AMA.

**Open Question 2:** To what extent does the Attractive Metadata Attack propagate or fail within multi-agent architectures where agents share tool contexts? The experimental scope was restricted to single-agent workflows, leaving multi-agent dynamics untested.

**Open Question 3:** Why does the attack success rate fail to decrease monotonically as the number of declared tool parameters increases? Appendix B.2.1 notes this non-intuitive relationship and calls for further investigation into why structural complexity doesn't consistently hinder selection.

## Limitations
- Computational cost of simulating tool selection across query sets may not scale efficiently for real-time deployment
- Cross-model transferability shows meaningful degradation (7-10 percentage point drops across models)
- Defenses remain under-tested, with limited evaluation of defense stacking effectiveness

## Confidence
**High Confidence (80-100%):** Core mechanism of metadata optimization works, semantic primacy in tool selection, orthogonality to prompt defenses
**Medium Confidence (40-79%):** Universal tool optimization effectiveness varies by query type and target model, cross-model transferability is imperfect, real-world practicality limited by computational requirements
**Low Confidence (0-39%):** Scalability to production systems with thousands of tools, resistance to adaptive defenses from informed attackers

## Next Checks
1. Implement a comprehensive defense testbed combining repository filtering, MCP protocol enforcement, and auditor-based detection. Measure whether stacked defenses reduce ASR below 20% across all attack scenarios.
2. Profile the optimization loop's computational requirements across different scales: full query sets, larger tool repositories (100-1000 tools), and reduced iteration budgets (K=1, K=3).
3. Evaluate AMA in a dynamic environment with periodically updated tool descriptions, contextual tool selection, and reputation scores based on usage history to test effectiveness in realistic agent ecosystems.