---
ver: rpa2
title: Convexified Message-Passing Graph Neural Networks
arxiv_id: '2505.18289'
source_url: https://arxiv.org/abs/2505.18289
tags:
- graph
- layer
- neural
- convex
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Convexified Message-Passing Graph Neural
  Networks (CGNNs), a novel framework that transforms training of message-passing
  GNNs into a convex optimization problem. By mapping nonlinear filters into a reproducing
  kernel Hilbert space, CGNNs achieve global optimality and computational efficiency
  without relying on over-parameterization.
---

# Convexified Message-Passing Graph Neural Networks

## Quick Facts
- arXiv ID: 2505.18289
- Source URL: https://arxiv.org/abs/2505.18289
- Authors: Saar Cohen; Noa Agmon; Uri Shaham
- Reference count: 40
- Primary result: Convexified GNNs achieve 10-40% higher accuracy than non-convex baselines while providing global optimality guarantees

## Executive Summary
This paper introduces Convexified Message-Passing Graph Neural Networks (CGNNs), a novel framework that transforms training of message-passing GNNs into a convex optimization problem. By mapping nonlinear filters into a reproducing kernel Hilbert space, CGNNs achieve global optimality and computational efficiency without relying on over-parameterization. The authors provide rigorous generalization guarantees, showing that the expected error of two-layer CGNNs converges to that of the optimal GNN as sample size increases. Empirical results on benchmark datasets demonstrate that CGNNs significantly outperform leading non-convex GNN models, achieving 10-40% higher accuracy in most cases.

## Method Summary
CGNNs reformulate message-passing GNNs by mapping nonlinear activation functions into reproducing kernel Hilbert spaces, transforming the non-convex learning problem into a convex one. The method uses kernelization of nonlinear filters, convex relaxation of low-rank constraints via nuclear norm regularization, and projected gradient descent for optimization. The framework employs Nyström approximation to scale kernel matrix computations and provides theoretical guarantees through Rademacher complexity analysis.

## Key Results
- CGNNs outperform leading non-convex GNN models by 10-40% accuracy on benchmark datasets
- Shallow convex models surpass over-parameterized non-convex ones in both accuracy and model compactness
- Theoretical generalization bounds show expected error convergence to optimal GNN performance as sample size increases

## Why This Works (Mechanism)

### Mechanism 1: Kernelization of Nonlinear Filters
Mapping nonlinear graph filters into RKHS transforms non-convex learning into linear problems. The paper uses Mercer's Theorem to reparameterize nonlinear filters as linear combinations of kernel products, enabling convex optimization. Core assumption: activation functions are sufficiently smooth for RKHS inclusion. Break condition: non-smooth activations (like standard ReLU) violate RKHS assumptions.

### Mechanism 2: Convex Relaxation of Low-Rank Constraints
Replacing non-convex rank constraints with convex nuclear norm constraints ensures globally optimal solutions. Standard GCN filters have low rank due to parameter sharing, which is relaxed to nuclear norm balls. Core assumption: optimal filter complexity can be controlled by nuclear norm bounds. Break condition: excessively large or small bounds cause ill-conditioning or exclude optimal solutions.

### Mechanism 3: Generalization via Rademacher Complexity
CGNNs provide theoretical guarantees through convex function classes with bounded Rademacher complexity. The framework proves generalization gap decays polynomially with sample size. Core assumption: loss function is M-Lipschitz and model is restricted to two layers. Break condition: guarantees are limited to two-layer models; deeper architectures lack unified theoretical bounds.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS)**: Mathematical framework enabling nonlinear activation functions to be treated linearly. Without understanding kernel inner products, the transition from nonlinear to linear operations is opaque. Quick check: How does the "kernel trick" compute dot products of infinite-dimensional vectors without explicit calculation?

- **Nuclear Norm**: Convex relaxation for matrix rank minimization. Since rank minimization is NP-hard, nuclear norm (sum of singular values) encourages low-rank solutions while maintaining convexity. Quick check: For singular values [3, 2, 0.1], what is the nuclear norm and how does it relate to rank?

- **Projected Gradient Descent**: Solver for convex problems with constraint sets. Since constraints are nuclear norm balls (not simple box constraints), gradient updates require specific SVD-based projections. Quick check: What mathematical operation brings parameters back into the constraint set when a gradient step violates bounds?

## Architecture Onboarding

- **Component map**: Input X & S -> Aggregator Z^k_ℓ -> Kernel Layer Q^k_ℓ -> Convex Layer Â^ℓ -> Output (with nuclear norm constraint)

- **Critical path**: Nyström Approximation. The system relies on this approximation to scale. Poor sampling degrades performance.

- **Design tradeoffs**: Accuracy vs. Speed (exact kernel methods O(m³n³) vs. Nyström approximation); Smoothness vs. Expressivity (requires smooth activations, excluding standard ReLU).

- **Failure signatures**: Oversmoothing (if kernel width γ is too small or P too low); Projection failure (if learning rate too high relative to nuclear norm bound); Incompatibility (using ReLU with Gaussian RBF kernel).

- **First 3 experiments**:
  1. Implement 2-layer CGCN on MUTAG; verify accuracy exceeds 80%
  2. Vary Nyström dimension P on PROTEINS; plot accuracy vs. training time
  3. Swap "smoothed hinge loss" for "erf" and "sinusoid" on synthetic dataset; observe convergence impact

## Open Questions the Paper Calls Out

1. **Computational complexity reduction**: Can kernel matrix factorization complexity be reduced to scale CGNNs to large graphs without unstable random approximations? (Explicit, Section 4.4/6 - requires O(P²nm) time; poor sampling degrades performance)

2. **Global generalization bounds**: Can global generalization bounds be established for multi-layer CGNNs rather than layer-wise analysis? (Explicit, Section 4.5 - Theorem 4.7 covers only two-layer models)

3. **Non-smooth activation extension**: Is it possible to extend convexification to support non-smooth activation functions like ReLU? (Inferred, Appendix C.3 - requires sufficiently smooth functions; ReLU violates RKHS assumptions)

## Limitations

- Theoretical guarantees apply only to two-layer models; deeper architectures use layer-wise training without unified bounds
- Kernel width parameter γ=0.2 is fixed; sensitivity analysis is not explored
- Claims of bridging graph learning and graph kernels lack extensive empirical comparison with explicit kernel methods

## Confidence

- **High**: CGNNs outperform non-convex baselines on benchmark datasets (10-40% accuracy gains)
- **Medium**: Theoretical generalization guarantees for two-layer models; layer-wise training extensions are less rigorously supported
- **Medium**: Computational efficiency claims rely on Nyström approximation, trading exact convexity for scalability

## Next Checks

1. **Scalability validation**: Implement CGNN on graphs with >1000 nodes to verify memory/compute efficiency and Nyström approximation quality
2. **Hyperparameter sensitivity**: Systematically vary kernel width γ and Nyström dimension P to quantify accuracy vs. training time tradeoff
3. **Robustness to activation choice**: Replace "smoothed hinge loss" with ReLU and observe convergence degradation, validating RKHS smoothness assumptions