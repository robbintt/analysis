---
ver: rpa2
title: Towards Anomaly-Aware Pre-Training and Fine-Tuning for Graph Anomaly Detection
arxiv_id: '2504.14250'
source_url: https://arxiv.org/abs/2504.14250
tags:
- graph
- nodes
- anomaly
- homophily
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses label scarcity and local homophily disparity
  in graph anomaly detection by proposing Anomaly-Aware Pre-Training and Fine-Tuning
  (APF). The method leverages a label-free Rayleigh Quotient metric to guide the extraction
  of node-specific subgraphs for anomaly-aware pre-training, employing dual spectral
  polynomial filters to capture both general semantics and anomaly cues.
---

# Towards Anomaly-Aware Pre-Training and Fine-Tuning for Graph Anomaly Detection

## Quick Facts
- **arXiv ID**: 2504.14250
- **Source URL**: https://arxiv.org/abs/2504.14250
- **Reference count**: 40
- **Primary result**: APF achieves 49.6% average AUPRC on 10 datasets, outperforming state-of-the-art baselines.

## Executive Summary
This paper addresses label scarcity and local homophily disparity in graph anomaly detection by proposing Anomaly-Aware Pre-Training and Fine-Tuning (APF). The method leverages a label-free Rayleigh Quotient metric to guide the extraction of node-specific subgraphs for anomaly-aware pre-training, employing dual spectral polynomial filters to capture both general semantics and anomaly cues. During fine-tuning, a gated fusion network adaptively combines these representations at node and dimension levels, supported by an anomaly-aware regularization loss. Theoretical analysis shows that under mild conditions, APF achieves linear separability across nodes. Experiments on 10 datasets demonstrate APF's superior performance, effectively mitigating local homophily disparity.

## Method Summary
APF is a two-stage framework that first pre-trains dual spectral filters (low-pass and high-pass) using Rayleigh Quotient-based subgraph sampling and DGI-style contrastive learning, then fine-tunes with a gated fusion network and anomaly-aware regularization. The pre-training phase learns representations by maximizing mutual information between node embeddings and summary vectors from anomaly-cue subgraphs, while the fine-tuning phase combines these representations adaptively based on node characteristics. The method employs a Rayleigh Quotient sampler to select subgraphs that highlight spectral energy inconsistencies, which are strong indicators of anomalies, and uses Chebyshev polynomial constraints to enforce filter distinctness.

## Key Results
- APF achieves an average AUPRC of 49.6% across 10 benchmark datasets, outperforming state-of-the-art baselines.
- The method effectively mitigates local homophily disparity, particularly on datasets with high structural heterogeneity.
- Theoretical analysis demonstrates linear separability under mild conditions, providing a strong foundation for the approach's effectiveness.

## Why This Works (Mechanism)

### Mechanism 1: Rayleigh Quotient as a Label-Free Anomaly Proxy
- **Claim:** Effective pre-training for Graph Anomaly Detection (GAD) can be achieved by targeting subgraphs that maximize spectral energy inconsistency (high-frequency signals) without ground-truth labels.
- **Mechanism:** The framework uses the Rayleigh Quotient (RQ) as a proxy for anomaly degree. By sampling subgraphs with high RQ values, the model forces the encoder to focus on "right-shift" spectral energy distributions (heterophily/feature inconsistency), which are strong indicators of anomalies.
- **Core assumption:** Anomalies in attributed graphs manifest as high-frequency signals where connected nodes have dissimilar features ("right-shift" phenomenon).

### Mechanism 2: Disentangled Dual-Spectral Encoding
- **Claim:** Representing nodes via independent low-pass and high-pass filters preserves both semantic context and subtle anomaly cues better than monolithic encoders.
- **Mechanism:** Standard GCNs act as low-pass filters that smooth features, potentially erasing anomaly signals. APF enforces a dual-branch architecture using Chebyshev polynomial constraints ($g_L$ and $g_H$) to explicitly preserve the high-frequency components (anomaly cues) alongside low-frequency components (semantics).

### Mechanism 3: Node-Adaptive Gated Fusion
- **Claim:** Theoretical linear separability is best approximated by a data-driven, node-specific fusion of low- and high-pass representations rather than a global filter.
- **Mechanism:** Instead of fixing a global filter, a Gated Fusion Network (GFN) learns coefficients $C$ to mix $Z_L$ and $Z_H$ per node and dimension. A regularization loss ($L_{reg}$) explicitly biases abnormal nodes to rely more on high-pass representations ($Z_H$) and normal nodes on low-pass ($Z_L$), addressing homophily disparity.

## Foundational Learning

- **Concept: Graph Spectral Filtering & Laplacian**
  - **Why needed here:** The entire APF architecture relies on manipulating the graph spectrum (eigenvalues). You must understand how the Laplacian $L$ relates to "smoothness" and how polynomial filters manipulate frequency responses (low-pass vs. high-pass).
  - **Quick check question:** Why does a standard GCN act as a low-pass filter, and why might that be detrimental for detecting camouflaged fraudsters?

- **Concept: Rayleigh Quotient**
  - **Why needed here:** This is the core unsupervised signal used for subgraph selection. You need to understand that $RQ(x) = \frac{x^T L x}{x^T x}$ measures the alignment between the signal $x$ and the graph structure.
  - **Quick check question:** Does a high Rayleigh Quotient indicate a node's features are similar to its neighbors or different?

- **Concept: Mutual Information (MI) Maximization (DGI)**
  - **Why needed here:** The pre-training objective is built on DGI. Understanding how contrastive learning (maximizing MI between node and summary representations) works is essential to grasp how APF transfers knowledge without labels.
  - **Quick check question:** In the context of APF, what does maximizing MI between a node $Z_i^H$ and its RQ-subgraph summary $s_i^H$ achieve?

## Architecture Onboarding

- **Component map:** MRQSampler -> Dual Encoder ($g_L$, $g_H$) -> Pre-train Head (DGI) -> Fine-tune Head (GFN + MLP) -> Loss ($L_{ft} = L_{bce} + L_{reg}$)

- **Critical path:** The RQ sampling must correctly identify high-frequency subgraphs. If the subgraphs are trivial (e.g., just random neighbors), the pre-training loses its anomaly-awareness. The constraints on Chebyshev coefficients ($\gamma^L$ vs $\gamma^H$) must be enforced to maintain filter distinctness.

- **Design tradeoffs:** APF introduces higher complexity (dual encoders + sampling) compared to vanilla GNNs. The "two-stage" approach (pre-train then freeze) is chosen over joint learning to prevent the classification loss from degrading the unsupervised anomaly features (justified in Appendix I.9).

- **Failure signatures:**
  - **Low AUPRC on high homophily datasets:** The model might be over-emphasizing the high-pass filter. Check the fusion coefficients $C$â€”normal nodes should have high $C$ (using $Z_L$).
  - **Slow convergence:** RQ sampling is expensive. Ensure it is parallelized or cached (O(n log n)).
  - **Performance collapse on Tabular data:** As noted in the paper, tree-based models (XGBoost) might still win if node features dominate structure. APF relies on structure-feature interplay.

- **First 3 experiments:**
  1. **Sanity Check (RQ vs Random):** Train APF with random subgraph sampling vs. RQ-sampling. Verify if RQ provides a tangible lift in AUPRC.
  2. **Ablation (Fusion):** Replace the Node- and Dimension-adaptive GFN with a simple concatenation or mean-pooling. Look for performance degradation, especially on datasets with high homophily disparity (like T-Finance).
  3. **Homophily Stratification:** Evaluate AUPRC specifically on Q4 (lowest homophily quartile) nodes. Confirm APF outperforms baselines like BWGNN or GHRN specifically in this difficult region.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the APF framework be hybridized with tree-based models to better handle datasets where node features are highly heterogeneous or tabular-dominant? (Section D mentions future work on hybrid architectures)
- **Open Question 2:** Can uncertainty quantification mechanisms be effectively integrated into the APF framework to mitigate the risks of false positives in sensitive real-world applications? (Section D lists this as future research)
- **Open Question 3:** To what extent does the theoretical guarantee of linear separability hold when the strict assumptions of the Anomalous Stochastic Block Model (ASBM) are violated by non-Gaussian features? (Appendix J acknowledges reliance on Gaussian features for analytical tractability)

## Limitations

- The method's reliance on Rayleigh Quotient assumes anomalies manifest primarily as heterophily, which may not hold for all real-world scenarios where anomalies can be homophilic or purely structural.
- The two-stage training procedure adds complexity and may not adapt well to dynamic graph environments where node relationships change over time.
- Computational overhead from MRQSampler and dual encoding could limit scalability to massive graphs with millions of nodes.

## Confidence

- **High:** The theoretical foundation for linear separability under mild conditions and the experimental superiority over baselines (49.6% average AUPRC) are well-supported by the methodology and results.
- **Medium:** The claim that RQ effectively captures anomaly cues relies on the assumption that high-frequency signals consistently indicate anomalies, which may vary across domains.
- **Medium:** The effectiveness of the node-adaptive gated fusion assumes a strong correlation between homophily levels and node normality, which may not always hold in practice.

## Next Checks

1. **Domain Generalization:** Test APF on datasets where anomalies are primarily homophilic or structural to evaluate robustness beyond heterophilic anomaly patterns.
2. **Scalability Analysis:** Measure the runtime and memory usage of MRQSampler and dual encoding on large-scale graphs to assess practical applicability.
3. **Label Efficiency:** Experiment with fewer than 100 labeled nodes to quantify the minimum supervision required for APF to outperform unsupervised methods like FreeGAD.