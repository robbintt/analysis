---
ver: rpa2
title: Reinforced Context Order Recovery for Adaptive Reasoning and Planning
arxiv_id: '2508.13070'
source_url: https://arxiv.org/abs/2508.13070
tags:
- recor
- order
- token
- training
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that standard causal and diffusion language
  models struggle with complex reasoning tasks because they generate tokens in a fixed
  (left-to-right) or random order, rather than an adaptive order that would make the
  problem tractable. To address this, the authors propose Reinforced Context Order
  Recovery (ReCOR), a reinforcement learning framework that learns to adaptively select
  the next token to generate based on the current context.
---

# Reinforced Context Order Recovery for Adaptive Reasoning and Planning

## Quick Facts
- **arXiv ID:** 2508.13070
- **Source URL:** https://arxiv.org/abs/2508.13070
- **Authors:** Long Ma; Fangwei Zhong; Yizhou Wang
- **Reference count:** 40
- **Primary result:** ReCOR learns adaptive token generation orders via RL, outperforming standard left-to-right models and even oracle methods on arithmetic, Sudoku, and Zebra puzzles.

## Executive Summary
The paper identifies that standard causal and diffusion language models struggle with complex reasoning tasks because they generate tokens in a fixed (left-to-right) or random order, rather than an adaptive order that would make the problem tractable. To address this, the authors propose Reinforced Context Order Recovery (ReCOR), a reinforcement learning framework that learns to adaptively select the next token to generate based on the current context. ReCOR uses self-supervised rewards derived from token prediction difficulty to guide the order selection during both training and inference, avoiding the distribution shift issues of inference-only adaptive methods. Experiments on arithmetic, Sudoku, and Zebra puzzle datasets show that ReCOR consistently outperforms strong baselines, including adaptive masked diffusion models and even oracle models supervised with ground-truth orders. The method scales well with additional compute and demonstrates the importance of adaptive ordering during training, not just inference.

## Method Summary
ReCOR is a reinforcement learning framework that learns an adaptive token generation order for complex reasoning tasks. It consists of a token predictor and an order policy, both implemented as transformers. The order policy selects the next token position to generate based on the current context, while the token predictor fills in that position. The system is trained end-to-end using a self-supervised reward signal derived from the token predictor's success rate. This on-policy training approach ensures the model learns to solve tractable sub-problems during training, avoiding the distribution shift issues that plague inference-only adaptive methods.

## Key Results
- ReCOR outperforms standard causal and diffusion models on arithmetic, Sudoku, and Zebra puzzle datasets
- The method achieves performance that sometimes exceeds oracle models trained with ground-truth orders
- ReCOR demonstrates scalability, with performance improving as more token and order queries are used
- The on-policy training approach is crucial, as models trained with random masking and adaptive inference perform significantly worse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A learned, data-dependent token generation order can improve a model's tractability on problems with dense inter-token dependencies, which fixed left-to-right or random orders fail to address.
- **Mechanism:** The framework replaces standard sequential or random generation with a reinforcement learning (RL) policy $\pi_\theta$. This policy functions as a "navigator," estimating the hardness of predicting each unfilled token given the current context and selecting the next position to generate. By prioritizing tokens with high predictive V-information, the model builds a solution iteratively, using the "easy" newly-solved parts as context to make subsequent "harder" predictions tractable.
- **Core assumption:** The reasoning task's difficulty is non-uniform across tokens, and an "easy-first" policy provides a viable solution path.
- **Evidence anchors:**
  - [abstract] "...observed that current causal and diffusion models encounter difficulties in problems that require adaptive token generation orders..."
  - [section 4.1] "Intuitively, we would like to start with the easy parts... and work our way to the harder parts... We formalize this intuition using the framework of predictive V-information"
  - [corpus] Weak direct evidence. Neighbor papers like `C$^2$DLM` and `Parallelism and Generation Order` explore diffusion and order but do not explicitly validate the V-information hardness hypothesis.
- **Break condition:** If a problem's dependencies are uniformly dense (no token is easier given others), this mechanism degrades to random selection.

### Mechanism 2
- **Claim:** The "distribution shift" between training and inference is a primary cause of failure in inference-only adaptive methods (like standard adaptive masked diffusion), and it can be mitigated by using the same adaptive order during both phases.
- **Mechanism:** ReCOR uses on-policy reinforcement learning. The token predictor $p_\psi$ is trained on orders sampled from the *current* policy $\pi_\theta$. This ensures $p_\psi$ is only evaluated on sub-problems it has learned to solve and avoids being exposed to the intractable sub-problems that arise from random masking. This contrasts with methods like AdaMDM, which train on random masks but attempt adaptive inference at test time, leading to a mismatch.
- **Core assumption:** Random masking during training creates a scarcity of tractable sub-problems, leaving the model ill-prepared for structured inference.
- **Evidence anchors:**
  - [abstract] "...self-supervised by token prediction statistics, ReCOR estimates the hardness... during both training and inference. Experiments... demonstrate the superior performance... sometimes outperforming oracle models..."
  - [section 5.4] "...AdaMDM is trained under random masking while ReCOR follows the recovered order... MDMs rarely see any long, complete contexts during training... ReCOR automatically recognizes and frequently visits these good sub-problems..."
  - [corpus] `Train for the worst, plan for the best` (cited in paper) supports the problem of intractable sub-problems from random masking.
- **Break condition:** If the training data lacks sufficient diversity or the RL policy fails to converge, the on-policy training loop could collapse, yielding a non-functional token predictor.

### Mechanism 3
- **Claim:** Self-supervision via token prediction statistics can serve as a reliable reward signal for learning a generation order, removing the need for human annotations of "ideal" reasoning steps.
- **Mechanism:** The system uses a reward signal derived from the log-probability or a thresholded binary indicator of the token predictor's success. This creates a tight feedback loop: the policy $\pi_\theta$ selects a position, and the token predictor $p_\psi$'s ability to correctly fill it provides the scalar reward. The two components are optimized jointly, allowing the system to discover latent orders from plain text data.
- **Core assumption:** A token's predictability is a valid proxy for its position in a logical solution sequence.
- **Evidence anchors:**
  - [abstract] "Self-supervised by token prediction statistics, ReCOR estimates the hardness of predicting every unfilled token..."
  - [section 4.3] "We set the reward function to the (negated) perplexity... Empirically, we found the following thresholded, sparse reward with a better performance..."
  - [corpus] `Reinforcing the Diffusion Chain of Lateral Thought` explores RL in diffusion models, providing indirect support for the viability of RL-based reasoning frameworks.
- **Break condition:** If the token predictor is too weak or overfitted, its probability estimates become meaningless, providing a noisy reward that prevents the policy from learning.

## Foundational Learning

- **Concept: V-information (Predictive Information)**
  - **Why needed here:** The paper uses this to formalize "token hardness" under computational constraints. It justifies why some tokens are intractable without prior context, moving beyond standard entropy.
  - **Quick check question:** Given a partially solved Sudoku, how does the V-information of a specific empty cell change after a related cell is filled?

- **Concept: Reinforcement Learning (RL) as Search**
  - **Why needed here:** The paper formulates order recovery as a Markov Decision Process (MDP). Understanding the components—state (current puzzle), action (pick a position), reward (prediction success)—is crucial to grasp how the model "searches" for a good order.
  - **Quick check question:** In ReCOR's MDP, what is the action space and what does the reward function directly measure?

- **Concept: Distribution Shift**
  - **Why needed here:** A central argument is that the mismatch between random-mask training and adaptive inference harms performance. Understanding this concept is key to appreciating the value of ReCOR's joint training approach.
  - **Quick check question:** Why would a masked language model, trained on random subsets of tokens, fail when asked to generate tokens in a specific "easiest-first" order at inference time?

## Architecture Onboarding

- **Component map:** Prompt and partial response -> Main stream -> Order policy and token query stream -> Position selection -> Token prediction -> Reward generation

- **Critical path:**
  1. The prompt $x$ and partial response $y_{\rho_{<t}}$ are fed into the main stream.
  2. The order policy $\pi_\theta$ uses the main stream output (or a dedicated order stream) to assign a Q-value to all unfilled positions.
  3. An action $a_t$ (a position) is sampled based on Q-values.
  4. The token predictor $p_\psi$ uses the token query stream to predict the token at position $a_t$.
  5. This prediction's success generates a reward, which updates the policy $\pi_\theta$ via Soft Q-learning, while the predictor $p_\psi$ is updated via the standard language modeling loss.

- **Design tradeoffs:**
  - **Query Streams vs. Main Stream Only:** Using separate query streams (XLNet-style) increases compute and parameters but ensures queries don't interfere. The paper notes a simpler "main stream only" option is a viable lightweight alternative.
  - **Sparse vs. Dense Reward:** The paper found a thresholded sparse reward to be more effective than dense perplexity-based rewards, likely because it's more robust to noise from inherently hard tokens.
  - **On-Policy Training:** Requires generating new rollouts for each training batch, which is computationally more expensive than off-policy methods but essential to avoid distribution shift.

- **Failure signatures:**
  - **Policy Collapse:** The order policy always selects the same position (e.g., always left-to-right), failing to explore. This is mitigated by entropy regularization in the RL objective.
  - **Reward Hacking:** The policy might learn to select tokens it can predict *incorrectly* but with high (overconfident) probability. This is mitigated by using thresholded sparse rewards.
  - **Slow Convergence:** The RL component requires many samples to learn a good order. The paper uses a large number of gradient steps.

- **First 3 experiments:**
  1. **Baseline Validation on Arithmetic:** Train ReCOR and a standard causal language model (CLM) on a synthetic autoregression task where tokens depend on future tokens. ReCOR should significantly outperform CLM, demonstrating its ability to recover a non-left-to-right order.
  2. **Ablation of Adaptive Training:** Compare ReCOR against a model trained with random masking but using adaptive inference (AdaMDM). The performance gap, especially on tasks requiring long contexts (like the "Autoregression" dataset), will highlight the importance of adaptive order during training.
  3. **Compute Scaling Analysis:** Run ReCOR on Sudoku, varying the number of token queries ($K$) and order queries ($C$). Plotting validation accuracy against compute should show monotonically improving performance, validating the scalability of the design.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to long sequences (hundreds or thousands of tokens) is unclear due to linear increase in compute with token and order queries
- Generalization to open-domain tasks with ambiguous "correct" orders is uncertain, as evaluation focuses on structured synthetic datasets
- Data efficiency is a concern due to computationally expensive on-policy training requiring new rollouts for each batch

## Confidence
**High Confidence (8/10)**
- **Claim Cluster 1:** ReCOR's on-policy training mitigates distribution shift between training and inference, leading to superior performance on tasks requiring adaptive ordering. Strong evidence from ablation study and AdaMDM comparison.

**Medium Confidence (6/10)**
- **Claim Cluster 2:** Self-supervised rewards derived from token prediction statistics are sufficient to guide learning of effective generation orders without human annotations. Works on synthetic tasks but concerns about reward hacking and predictor quality remain.

**Low Confidence (4/10)**
- **Claim Cluster 3:** ReCOR will significantly improve performance on open-ended, real-world reasoning tasks (e.g., complex code generation, open-domain QA). Current evidence limited to structured synthetic datasets with clear logical structures.

## Next Checks
1. **Long Sequence Scalability Test:** Evaluate ReCOR on a synthetic task with sequences of 500+ tokens (e.g., complex mathematical proof or long-form story generation task with logical dependencies). Measure both performance and compute cost as K and C scale to validate scalability to longer, more complex sequences.

2. **Open-Domain Generalization Test:** Apply ReCOR to an open-ended task like code generation (e.g., generating Python function from description) or multi-hop reasoning (e.g., answering complex questions from multiple documents). Compare against strong non-adaptive baselines (e.g., chain-of-thought prompting, supervised fine-tuning) to test transfer to tasks with ambiguous or context-dependent "correct" orders.

3. **Robustness to Noise and Ambiguity:** Create a noisy version of Sudoku or Zebra puzzle dataset where some cells have ambiguous or incorrect clues. Evaluate ReCOR's performance under these conditions and compare to standard causal language model to test robustness of reward signal and policy's ability to handle uncertainty.