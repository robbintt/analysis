---
ver: rpa2
title: 'WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular
  Datasets'
arxiv_id: '2506.06455'
source_url: https://arxiv.org/abs/2506.06455
tags:
- consensus
- wisca
- features
- datasets
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WISCA, a consensus-based interpretability
  approach for tabular datasets that addresses the challenge of conflicting explanations
  generated by different interpretability algorithms. The method combines class probabilities
  and normalized attributions through a weighted scaling mechanism, differentiating
  between global and local explanations while accounting for model accuracy.
---

# WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets

## Quick Facts
- **arXiv ID**: 2506.06455
- **Source URL**: https://arxiv.org/abs/2506.06455
- **Reference count**: 11
- **Primary result**: WISCA combines class probabilities and normalized attributions through weighted scaling to produce superior consensus-based interpretability across tabular datasets

## Executive Summary
This paper introduces WISCA, a consensus-based interpretability approach designed to address the challenge of conflicting explanations generated by different interpretability algorithms on tabular datasets. The method combines class probabilities and normalized attributions through a weighted scaling mechanism, differentiating between global and local explanations while accounting for model accuracy. Tested on six synthetic datasets with known ground truths across classification and regression tasks, WISCA consistently outperformed five established consensus functions by better identifying the most relevant features. The approach achieved superior hit rates compared to both traditional consensus methods and individual interpretability algorithms, demonstrating its effectiveness in producing more reliable and consistent explanations for machine learning models.

## Method Summary
WISCA addresses interpretability conflicts by combining class probabilities and normalized attributions through a weighted scaling mechanism. The approach distinguishes between global explanations (dataset-wide feature importance) and local explanations (instance-specific attributions) while incorporating model accuracy into the weighting process. The method operates by first computing attributions from multiple interpretability algorithms, normalizing these attributions, and then combining them with class probabilities using a weighted scaling approach that accounts for each algorithm's reliability. This consensus mechanism produces harmonized explanations that aim to identify the most relevant features more accurately than individual methods or traditional consensus approaches.

## Key Results
- WISCA outperformed five established consensus functions on six synthetic datasets with known ground truths
- The method achieved superior hit rates compared to both traditional consensus methods and individual interpretability algorithms
- WISCA demonstrated effectiveness across both classification and regression tasks in identifying the most relevant features

## Why This Works (Mechanism)
WISCA works by addressing the fundamental problem of conflicting explanations from different interpretability algorithms through a consensus mechanism that leverages both class probabilities and normalized attributions. The weighted scaling approach allows the method to account for the reliability of different interpretability algorithms by incorporating their accuracy, while the normalization process ensures fair comparison across different attribution scales. By distinguishing between global and local explanations, WISCA can provide both dataset-wide insights and instance-specific interpretations, making it more versatile than approaches that only focus on one type of explanation.

## Foundational Learning

**Class Probabilities**: The predicted probabilities for each class in classification tasks or the predicted values in regression tasks.
*Why needed*: Serve as a foundation for weighting the importance of different interpretability algorithms based on their predictive performance.
*Quick check*: Verify that predicted probabilities sum to 1 for classification tasks and that regression predictions are within reasonable ranges.

**Normalized Attributions**: The feature importance scores from interpretability algorithms scaled to a common range.
*Why needed*: Allows fair comparison and combination of attributions from different algorithms that may use different scales.
*Quick check*: Confirm that normalized attributions sum to 1 or fall within [0,1] depending on the normalization method used.

**Weighted Scaling Mechanism**: A mathematical approach that combines class probabilities with normalized attributions using weights derived from model accuracy.
*Why needed*: Enables the incorporation of model reliability into the consensus process, giving more weight to accurate models.
*Quick check*: Verify that weights are positive and sum to 1, and that the scaling mechanism produces reasonable combined scores.

**Global vs. Local Explanations**: The distinction between dataset-wide feature importance and instance-specific interpretations.
*Why needed*: Allows WISCA to provide both macro-level insights about feature importance across the dataset and micro-level interpretations for individual predictions.
*Quick check*: Confirm that global explanations remain consistent across similar instances while local explanations capture instance-specific variations.

## Architecture Onboarding

**Component Map**: Input Data -> Multiple Interpretability Algorithms -> Normalized Attributions -> Weighted Scaling with Class Probabilities -> Consensus Explanations

**Critical Path**: The most computationally intensive part is the generation of attributions from multiple interpretability algorithms, which WISCA must then normalize and combine. The weighted scaling mechanism is relatively lightweight in comparison.

**Design Tradeoffs**: WISCA trades computational complexity (running multiple interpretability algorithms) for improved explanation quality and consistency. The method assumes that model accuracy correlates with attribution quality, which may not always hold true. The approach also requires synthetic datasets with known ground truths for validation, limiting its applicability to real-world datasets without modification.

**Failure Signatures**: WISCA may fail when interpretability algorithms produce highly divergent attributions with no clear consensus, when model accuracy does not correlate with attribution quality, or when the synthetic ground truth assumptions do not match the true data generating process. The method may also struggle with high-dimensional datasets where feature interactions are complex.

**3 First Experiments**:
1. Apply WISCA to a simple synthetic dataset with 3-5 features and a clear ground truth to verify basic functionality
2. Compare WISCA's explanations against individual interpretability algorithms on a dataset with known feature importance
3. Test WISCA on a binary classification problem to validate the distinction between global and local explanations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation was conducted exclusively on synthetic datasets with known ground truths, which may not capture real-world complexity
- Performance on high-dimensional datasets with hundreds or thousands of features remains untested
- The weighted scaling mechanism assumes model accuracy directly correlates with attribution quality, which may not hold in all scenarios

## Confidence

**High Confidence**: The mathematical framework for combining class probabilities and normalized attributions is sound and well-defined. The distinction between global and local explanations is clearly articulated and implemented.

**Medium Confidence**: The superiority of WISCA over established consensus functions is demonstrated, but this claim is based on synthetic datasets only. The generalizability to real-world datasets requires further validation.

**Medium Confidence**: The assertion that WISCA produces more reliable and consistent explanations is supported by hit rate comparisons, but alternative metrics for interpretability quality could provide additional validation.

## Next Checks

1. **Real-World Dataset Testing**: Apply WISCA to benchmark tabular datasets from domains like healthcare (e.g., MIMIC-III) or finance to assess performance on noisy, high-dimensional data with complex feature interactions.

2. **Scalability Analysis**: Evaluate WISCA's computational efficiency and attribution quality on datasets with 1000+ features to determine practical limitations and potential optimizations for large-scale applications.

3. **Alternative Ground Truth Generation**: Compare WISCA's explanations against multiple types of ground truths (e.g., domain expert knowledge, causal inference methods) to validate robustness beyond synthetic data scenarios.