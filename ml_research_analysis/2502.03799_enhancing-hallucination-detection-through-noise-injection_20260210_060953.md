---
ver: rpa2
title: Enhancing Hallucination Detection through Noise Injection
arxiv_id: '2502.03799'
source_url: https://arxiv.org/abs/2502.03799
tags:
- noise
- uncertainty
- detection
- hallucination
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting hallucinations
  in Large Language Models (LLMs), where models generate plausible but incorrect responses.
  The authors propose a method that combines epistemic uncertainty (model uncertainty)
  with aleatoric uncertainty (data uncertainty) to improve hallucination detection.
---

# Enhancing Hallucination Detection through Noise Injection

## Quick Facts
- arXiv ID: 2502.03799
- Source URL: https://arxiv.org/abs/2502.03799
- Reference count: 17
- Primary result: Noise injection into intermediate layers improves hallucination detection AUROC by 4-7% compared to prediction-layer sampling

## Executive Summary
This paper addresses the challenge of detecting hallucinations in Large Language Models (LLMs) where models generate plausible but incorrect responses. The authors propose a method that combines epistemic uncertainty (model uncertainty) with aleatoric uncertainty (data uncertainty) to improve hallucination detection. They introduce a noise injection technique that perturbs hidden unit activations in intermediate layers of the model, effectively sampling from a distribution of plausible models. The method is evaluated across multiple datasets (GSM8K, CSQA, TriviaQA) and model architectures (Gemma-2B-it, Phi-3-mini-4k-instruct, Mistral-7B-Instruct, Llama-2-7B-chat, Llama-2-13B-chat), showing consistent improvements in hallucination detection without accuracy degradation.

## Method Summary
The method involves training-free noise injection into MLP activations of upper layers during sampling to approximate Bayesian posterior sampling and capture epistemic uncertainty. For each question, the model generates K=10 samples with temperature T=0.5, with uniform noise U(0, α) added to MLP activations at layers L₁-L₂ (e.g., Llama-2-7B: layers 20-32). The approach combines this epistemic uncertainty with aleatoric uncertainty from temperature-based sampling. Answer entropy H_ans(Y) = -Σp(a_j)log(p(a_j)) is computed over the K samples, and a threshold on this entropy classifies hallucination. Noise magnitude α is selected per model and dataset based on validation results.

## Key Results
- Noise injection consistently improves hallucination detection AUROC by 4-7% compared to prediction-layer sampling alone
- The method enhances model accuracy without degradation across all tested configurations
- Upper-layer MLP activations are effective perturbation targets, with perturbations in roughly the top third of layers showing optimal results
- Combining epistemic and aleatoric uncertainty provides complementary detection signals (Pearson correlation 0.58)

## Why This Works (Mechanism)

### Mechanism 1
Perturbing intermediate-layer activations approximates sampling from a Bayesian posterior over plausible models, capturing epistemic uncertainty that prediction-layer sampling alone misses. Noise injection into MLP activations (equivalent to perturbing bias terms) creates a surrogate distribution q(ω) centered at pre-trained weights. When the model generates responses under perturbation, hallucinated outputs show greater variance than truthful ones, enabling detection via entropy metrics. The paper reports robustness across α ∈ [0.01, 0.11] but notes optimal values vary by model and dataset.

### Mechanism 2
Epistemic and aleatoric uncertainty are complementary signals; combining both improves hallucination detection over either alone. Aleatoric uncertainty (from temperature-based token sampling) captures data-inherent randomness; epistemic uncertainty (from noise injection) captures model-parameter uncertainty. Their Pearson correlation of 0.58 indicates partial overlap but distinct information. Table 4 shows T=1.0 without noise (72.34 AUROC) underperforms T=0.8 with noise (79.03 AUROC), demonstrating the value of combining both uncertainty types.

### Mechanism 3
Upper-layer MLP activations are effective perturbation targets because they encode abstract representations; lower layers require smaller noise magnitudes due to error propagation sensitivity. The paper injects noise into roughly the top third of layers (e.g., layers 20-32 for 32-layer models). Upper layers tolerate larger perturbations (α up to 0.07) while lower layers require smaller magnitudes (α ~ 0.01). Table 5 shows noise injection at lower, middle, and upper layers all improve detection over baseline, with upper layers achieving 69.10 AUROC vs 67.55 baseline on CSQA.

## Foundational Learning

- **Epistemic vs Aleatoric Uncertainty**: The entire method hinges on distinguishing model uncertainty (epistemic—reducible with more data) from data uncertainty (aleatoric—irreducible). Misunderstanding this distinction leads to incorrect interpretation of results. Quick check: If you double the training data, which uncertainty type should decrease?

- **Variational Inference / Bayesian Neural Networks**: The paper frames noise injection as a lightweight approximation to Bayesian posterior sampling. Understanding what full Bayesian treatment would require helps contextualize why this shortcut is meaningful. Quick check: Why is computing p(ω|D) intractable for billion-parameter LLMs?

- **Entropy-based Uncertainty Metrics**: The detection signal comes from answer entropy computed over K samples. Understanding entropy as a dispersion measure is essential for interpreting AUROC results. Quick check: If all K samples yield identical answers, what is the answer entropy?

## Architecture Onboarding

- **Component map**: Input prompt x → Noise Injection Module (adds U(0, α) to MLP activations) → Sampling Module (temperature-scaled categorical sampling) → Answer extraction → Answer entropy computation H_ans(Y) → Threshold comparison

- **Critical path**: 1. Forward pass with perturbed MLP activations → 2. Token sampling with temperature → 3. Answer extraction → 4. Entropy computation → 5. Threshold comparison

- **Design tradeoffs**:
  - Higher K: Better entropy estimation but linear compute cost. Paper shows gains plateau around K=10-15.
  - Larger α: Better epistemic capture but risk of accuracy degradation. Paper uses validation-set tuning; reports α ∈ [0.01, 0.11].
  - Layer depth: Upper layers tolerate larger noise; lower layers require smaller α. All layers work, but upper layers are default.

- **Failure signatures**:
  - Accuracy drops: α too large for chosen layers; reduce magnitude or shift to higher layers.
  - No AUROC improvement: α too small (insufficient epistemic signal) or baseline already saturated (e.g., TriviaQA with Phi-3).
  - High variance across runs: K too small; increase sample count.

- **First 3 experiments**:
  1. Baseline replication: Run Llama-2-7B-chat on GSM8K with T=0.5, K=10, no noise. Verify AUROC ~71.56 and ACC ~23.64% match Table 2.
  2. Noise injection ablation: Add uniform noise U(0, 0.07) to layers 20-32 MLP activations. Expect AUROC improvement to ~76.14 without accuracy loss.
  3. Magnitude sweep: Test α ∈ {0.01, 0.03, 0.05, 0.07, 0.09} on validation split to identify optimal magnitude before test evaluation.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored. The choice of Uniform distribution for the surrogate model q(ω) versus other distributions (e.g., Gaussian) is not investigated. The method's efficacy in long-form generation or agentic workflows where errors propagate over significantly more tokens than in QA tasks is untested. Additionally, the theoretical derivation of optimal noise magnitude α versus empirical validation-set tuning remains an open question.

## Limitations
- The method requires careful hyperparameter tuning of noise magnitude α per model and dataset, limiting practical deployment
- The study focuses on QA tasks with relatively short answers (≤20 tokens), leaving open whether the method generalizes to longer-form generation
- The perturbation mechanism is applied to MLP activations rather than attention parameters, which could be a limitation if attention uncertainty is more predictive of hallucinations

## Confidence
- **High Confidence**: The core claim that noise injection improves AUROC (4-7% gains) is well-supported by results across multiple datasets and models
- **Medium Confidence**: The claim that combining epistemic and aleatoric uncertainty is superior to either alone is supported but the complementarity mechanism could benefit from more extensive analysis
- **Medium Confidence**: The claim that upper-layer perturbations are optimal is supported by ablation studies, but the evidence is less comprehensive than for the main detection results

## Next Checks
1. **Robustness to Temperature Settings**: Replicate the experiments across a wider temperature range (T ∈ {0.1, 0.3, 0.7, 0.9, 1.0}) to verify the complementarity claim between epistemic and aleatoric uncertainty holds across all conditions

2. **Cross-Dataset Generalization**: Apply the method to datasets with longer answers (e.g., NarrativeQA or long-form generation tasks) to test whether the entropy-based detection signal remains effective when answers exceed 20 tokens

3. **Attention vs MLP Perturbation**: Implement an ablation comparing noise injection into attention parameters versus MLP activations to determine whether the choice of perturbation target significantly impacts detection performance