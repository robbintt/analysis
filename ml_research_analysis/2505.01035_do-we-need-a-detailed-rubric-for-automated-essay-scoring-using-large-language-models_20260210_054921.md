---
ver: rpa2
title: Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language
  Models?
arxiv_id: '2505.01035'
source_url: https://arxiv.org/abs/2505.01035
tags:
- rubric
- scoring
- essay
- language
- rubrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether detailed rubrics are necessary for
  automated essay scoring (AES) using large language models (LLMs). The research compares
  scoring accuracy across three rubric conditions (detailed, simplified, and none)
  using four different LLMs on the TOEFL11 dataset.
---

# Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?

## Quick Facts
- arXiv ID: 2505.01035
- Source URL: https://arxiv.org/abs/2505.01035
- Authors: Lui Yoshida
- Reference count: 39
- Key outcome: Simplified rubrics maintain scoring accuracy while reducing token usage by ~40% across three of four tested LLMs

## Executive Summary
This study examines whether detailed rubrics are necessary for automated essay scoring using large language models. The research compares scoring accuracy across three rubric conditions (detailed, simplified, and none) using four different LLMs on the TOEFL11 dataset. Results show that three out of four models achieved similar scoring accuracy with simplified rubrics compared to detailed ones, while reducing token usage by approximately 40%. However, one model (Gemini 1.5 Flash) performed worse with more detailed rubrics, suggesting model-specific sensitivity to prompt length.

## Method Summary
The study evaluated four LLMs (Claude 3.5 Haiku, GPT-4o-mini, Gemini 1.5 Flash, Llama 3 70B Instruct) on the TOEFL11 dataset containing 12,100 essays across 8 prompts. Three rubric conditions were tested: full rubric (375 words), simplified rubric (78 words), and no rubric. Each model generated 5-point scores converted to 3-point scales (>3→high, =3→medium, <3→low). Quadratic Weighted Kappa (QWK) measured human-LLM scoring agreement, with statistical significance assessed via 1,000 bootstrap iterations and Holm's correction. Token counts were tracked to evaluate efficiency gains.

## Key Results
- Three of four models (Claude, GPT-4o-mini, Llama) maintained scoring accuracy with simplified rubrics while reducing token usage by ~40%
- Gemini 1.5 Flash showed decreased performance with more detailed rubrics, exhibiting a tendency toward score inflation
- Simplified rubrics (78 words) achieved comparable QWK scores to full rubrics (375 words) for most models
- Using any rubric significantly improved human-LLM alignment compared to no rubric condition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Condensed rubrics maintain scoring alignment with human evaluators while reducing token overhead.
- Mechanism: LLMs extract sufficient evaluation signal from high-level trait descriptors without requiring granular criteria definitions.
- Core assumption: Models have internalized sufficient essay quality representations during pre-training to operationalize brief descriptors.
- Evidence anchors: Three models maintained similar QWK with simplified vs. full rubrics; simplified rubric was 78 words vs. full rubric's 375 words.
- Break condition: When essay domains diverge significantly from pre-training distribution, brief rubrics may lack domain-specific signal.

### Mechanism 2
- Claim: Some models exhibit inverse sensitivity to prompt length—more detailed instructions degrade performance.
- Mechanism: Longer prompts may cause attention dispersion or instruction-following degradation in certain model architectures.
- Core assumption: The phenomenon relates to context-processing architecture rather than rubric content quality.
- Evidence anchors: Gemini 1.5 Flash showed decreased QWK as rubric detail increased; known length-sensitivity phenomena documented in related work.
- Break condition: Larger or differently-tuned variants within the same model family may not exhibit this pattern.

### Mechanism 3
- Claim: Presence of explicit evaluation criteria significantly improves human-LLM scoring alignment over implicit judgment.
- Mechanism: Providing any structured rubric anchors model output to human evaluation frameworks.
- Core assumption: Rubric content aligns with latent qualities human experts evaluate.
- Evidence anchors: Three models showed significantly higher QWK with rubrics than without; using simplified rubric is advantageous in effort and cost.
- Break condition: If rubric criteria conflict with model's pre-training priors about "good writing," alignment may not improve.

## Foundational Learning

- Concept: **Quadratic Weighted Kappa (QWK)**
  - Why needed here: Primary metric for human-machine scoring agreement; interprets whether LLM outputs match expert judgment patterns.
  - Quick check question: If QWK = 0.6 and human-human QWK = 0.85, what fraction of scoring variance does the LLM fail to capture that humans agree on?

- Concept: **Prompt Token Budget vs. Performance Trade-off**
  - Why needed here: Paper's core practical contribution—40% token reduction with maintained accuracy has direct cost and latency implications for production systems.
  - Quick check question: Given 12,100 essays, what annual cost difference would 400 fewer tokens per request represent at typical API pricing?

- Concept: **Confusion Matrix Interpretation for Scoring Bias**
  - Why needed here: Paper uses confusion matrices to diagnose Gemini's score inflation tendency with detailed rubrics.
  - Quick check question: What would a left-shifted diagonal in a confusion matrix indicate about model scoring behavior?

## Architecture Onboarding

- Component map:
  - **Prompt Template**: Instruction + Essay Prompt + Response + Assessment Method + Output Format
  - **Rubric Module**: Pluggable evaluation criteria (Full/Simplified/None variants)
  - **Scoring Pipeline**: LLM API → Raw score extraction → 5→3 point mapping → QWK calculation
  - **Evaluation Layer**: Bootstrap resampling (1000 iterations) + Holm-corrected significance testing

- Critical path:
  1. Select model → validate rubric behavior (test Full vs. Simplified vs. None)
  2. If model shows length-sensitivity (like Gemini Flash), prefer simplified rubric
  3. Calculate QWK against expert labels; if <0.75, consider fine-tuning rather than prompt optimization
  4. Monitor confusion matrices for systematic bias before deployment

- Design tradeoffs:
  - Full rubric: Higher development effort, ~2x token cost, may trigger length-sensitivity degradation
  - Simplified rubric: Lower effort, ~1.2x token cost, comparable performance for most models
  - No rubric: Minimal tokens, significantly worse accuracy—not recommended

- Failure signatures:
  - Score inflation with detailed prompts (Gemini pattern): LLM assigns systematically higher ratings as rubric length increases
  - QWK plateau ~0.6: Current prompting-only approaches hitting ceiling; signals need for fine-tuning
  - Cross-prompt inconsistency: Model performance varies across the 8 TOEFL prompts—signals prompt-specific overfitting risk

- First 3 experiments:
  1. Replicate the three rubric conditions on your target model with a 500-essay subset; confirm which condition achieves best QWK-to-token ratio
  2. Test your specific essay domain: Compare TOEFL-style rubric against domain-adapted rubric to validate generalization claim
  3. Run confusion matrix analysis: If model shows directional bias (over- or under-scoring specific quality levels), adjust rubric wording before production use

## Open Questions the Paper Calls Out

- **Open Question 1**: Do simplified rubrics maintain scoring accuracy across diverse essay datasets such as ASAP or CLC-FCE?
  - Basis: The author explicitly states that verifying findings with other datasets like ASAP or CLC-FCE is necessary to generalize the insights.
  - Why unresolved: The study was restricted exclusively to the TOEFL11 corpus.
  - Evidence: Replicating the three rubric conditions on the ASAP or CLC-FCE datasets and comparing QWK scores.

- **Open Question 2**: How do recently released reasoning models respond to varying levels of rubric detail?
  - Basis: The paper notes that reasoning models (e.g., OpenAI o1) may exhibit different behaviors than standard LLMs, suggesting a specific need for further investigation.
  - Why unresolved: The experiments utilized standard instruction-tuned models rather than models optimized for chain-of-thought reasoning.
  - Evidence: Evaluating reasoning models using the same rubric conditions to determine if they show improved adherence to detailed prompts.

- **Open Question 3**: Does the negative correlation between rubric detail and performance observed in Gemini 1.5 Flash persist in larger model variants?
  - Basis: The author speculates that "different results might be obtained with higher-performing models" within the same series, given the anomaly found in Gemini Flash.
  - Why unresolved: The study tested specific model versions (mostly "mini" or "flash" variants) and found one outlier, but did not test the larger counterparts.
  - Evidence: Conducting comparative tests between standard and pro/large versions of the same model families using detailed rubrics.

## Limitations
- Domain transferability uncertain—findings based on TOEFL11 academic essays may not generalize to other essay types
- Full rubric text not provided, limiting reproducibility and comparison of simplification techniques
- Model-specific results suggest findings may not apply uniformly across all LLM variants

## Confidence
- High Confidence: Token efficiency gains (40% reduction with simplified rubrics)
- Medium Confidence: Three-of-four models maintaining scoring accuracy with simplified rubrics
- Medium Confidence: Presence of any rubric improves human-LLM alignment over no rubric
- Low Confidence: Mechanism 1's assumption about pre-trained representations

## Next Checks
1. **Cross-Domain Replication**: Test the simplified vs. detailed rubric comparison on non-academic essay datasets (creative writing, technical documentation) to validate domain transferability claims
2. **Model Family Expansion**: Evaluate additional models within the same families (Claude 3.5 Sonnet, GPT-4, Llama 3 8B) to determine whether the Gemini 1.5 Flash sensitivity pattern represents a broader architectural phenomenon
3. **Rubric Content Variation**: Create alternative simplified rubrics with different structural approaches (trait-based vs. holistic) while maintaining similar token counts to isolate whether the simplification technique or specific wording drives the observed patterns