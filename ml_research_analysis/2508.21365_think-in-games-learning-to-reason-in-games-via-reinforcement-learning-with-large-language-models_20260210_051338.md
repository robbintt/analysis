---
ver: rpa2
title: 'Think in Games: Learning to Reason in Games via Reinforcement Learning with
  Large Language Models'
arxiv_id: '2508.21365'
source_url: https://arxiv.org/abs/2508.21365
tags:
- game
- language
- learning
- games
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large language models
  (LLMs) to develop procedural knowledge for interactive tasks by bridging the gap
  between declarative knowledge and procedural understanding. The proposed Think in
  Games (TiG) framework reformulates reinforcement learning (RL) as a language modeling
  task, allowing LLMs to generate language-guided policies that are refined through
  direct interaction with game environments.
---

# Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models

## Quick Facts
- arXiv ID: 2508.21365
- Source URL: https://arxiv.org/abs/2508.21365
- Reference count: 40
- Enables LLMs to acquire procedural knowledge through language-guided RL in interactive environments

## Executive Summary
This paper introduces the Think in Games (TiG) framework, which bridges the gap between LLMs' declarative knowledge and procedural understanding in interactive tasks. TiG reformulates reinforcement learning as a language modeling task, allowing LLMs to generate language-guided policies that are refined through direct interaction with game environments. The approach maintains LLMs' reasoning and explanatory capabilities while grounding them in experiential learning, demonstrating competitive performance in the Honor of Kings game with lower data and computational requirements than conventional RL methods.

## Method Summary
The TiG framework integrates reinforcement learning with language modeling by treating policy generation as a text generation task. LLMs generate language-guided policies that are then refined through interaction with game environments, creating a feedback loop between reasoning and experiential learning. This dual approach leverages the LLM's natural language understanding while incorporating procedural knowledge gained through gameplay. The framework is evaluated in the Honor of Kings game environment, where it demonstrates the ability to generate step-by-step natural language explanations for decisions while achieving competitive performance metrics.

## Key Results
- Qwen-3-14B trained with TiG achieves 90.91% accuracy, outperforming Deepseek-R1 (86.67%)
- TiG achieves competitive performance with significantly lower data and computational demands than conventional RL methods
- The framework provides interpretable, step-by-step natural language explanations for decision-making

## Why This Works (Mechanism)
TiG works by reformulating the reinforcement learning problem as a language modeling task, allowing LLMs to leverage their strong reasoning capabilities while learning procedural knowledge through interaction. By generating policies as language sequences rather than raw action vectors, the framework maintains interpretability and allows the model to reason about its decisions in natural language. The direct interaction with game environments provides experiential learning that grounds the LLM's declarative knowledge, creating a synergistic relationship between reasoning and experience. This approach enables smaller models to achieve competitive performance while maintaining transparency through natural language explanations.

## Foundational Learning
- **Reinforcement Learning Basics**: Understanding of RL concepts like policies, rewards, and value functions - needed to appreciate how TiG reformulates RL as language modeling; quick check: can you explain the difference between model-free and model-based RL?
- **Language Model Capabilities**: Familiarity with LLM strengths in reasoning and text generation - needed to understand how TiG leverages these capabilities for policy generation; quick check: what are the limitations of using LLMs for sequential decision-making?
- **Game Environment Interaction**: Knowledge of how agents interact with game states and receive feedback - needed to grasp the experiential learning component of TiG; quick check: how does environment feedback differ between supervised learning and reinforcement learning?

## Architecture Onboarding

**Component Map**
LLM Policy Generator -> Game Environment -> Experience Replay -> Policy Refinement -> LLM Policy Generator

**Critical Path**
The critical path involves the LLM generating a policy based on the current state, the policy being executed in the game environment, receiving feedback (rewards/states), storing experiences in replay memory, and using these experiences to refine the LLM's policy generation capabilities.

**Design Tradeoffs**
- Language modeling reformulation trades raw action efficiency for interpretability and reasoning capability
- Direct environment interaction provides experiential learning but requires more computational resources than pure imitation learning
- Smaller model sizes reduce computational demands but may limit the complexity of reasoning patterns

**Failure Signatures**
- Poor policy generation due to insufficient language understanding of game states
- Overfitting to specific game scenarios without generalization
- Ineffective experience replay leading to slow policy refinement
- Generation of nonsensical or unsafe actions despite natural language explanations

**3 First Experiments**
1. Test basic policy generation in a simple grid-world environment before moving to complex games
2. Evaluate the quality of natural language explanations against ground truth reasoning
3. Compare learning curves between TiG and conventional RL approaches on a benchmark task

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to a single game environment (Honor of Kings), raising questions about generalizability across different game genres and interactive task domains
- The claim of "significantly lower data and computational demands" lacks direct quantitative comparisons with baseline RL approaches
- The evaluation does not provide detailed ablations or analyses of how specific architectural choices contribute to performance differences

## Confidence
- **High confidence**: The core methodology of reformulating RL as a language modeling task and the basic framework architecture are well-described and theoretically sound
- **Medium confidence**: The reported performance improvements and computational efficiency gains, as these claims would benefit from more rigorous benchmarking against established RL baselines
- **Medium confidence**: The transparency and interpretability benefits, since while step-by-step explanations are generated, the quality and utility of these explanations for end-users has not been thoroughly evaluated

## Next Checks
1. Conduct systematic comparisons with established RL baselines (e.g., PPO, DQN) across multiple game environments to verify computational efficiency and performance claims
2. Perform ablation studies to isolate the contribution of individual components in the TiG framework (language modeling reformulation, experience replay, etc.) to overall performance
3. Evaluate the quality and practical utility of the generated natural language explanations through human studies or automated metrics that assess explanation coherence and decision justification