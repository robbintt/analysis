---
ver: rpa2
title: 'Camera Movement Classification in Historical Footage: A Comparative Study
  of Deep Video Models'
arxiv_id: '2510.14713'
source_url: https://arxiv.org/abs/2510.14713
tags:
- video
- motion
- movement
- camera
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the adaptation of modern video classification\
  \ models to the task of camera movement classification (CMC) in historical footage,\
  \ where visual degradation and lack of training data pose significant challenges.\
  \ Five standard video classification architectures\u2014C3D, R(2+1)D, TSN, I3D,\
  \ and Video Swin Transformer\u2014are evaluated on the HISTORIAN dataset, an expert-annotated\
  \ collection of World War II archival film material."
---

# Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models

## Quick Facts
- arXiv ID: 2510.14713
- Source URL: https://arxiv.org/abs/2510.14713
- Reference count: 16
- Primary result: Video Swin Transformer achieves 80.25% top-1 accuracy on 6-class camera movement classification in degraded WWII archival footage

## Executive Summary
This paper investigates how modern video classification architectures perform on the challenging task of camera movement classification in historical footage. Five standard video models—C3D, R(2+1)D, TSN, I3D, and Video Swin Transformer—are evaluated on the HISTORIAN dataset, an expert-annotated collection of World War II archival film material. The study finds that spatiotemporal attention mechanisms in Video Swin Transformer outperform simpler approaches, achieving 80.25% accuracy despite severe visual degradation and limited training data. The results demonstrate that general-purpose video architectures can effectively handle CMC tasks in historical footage without requiring specialized handcrafted features.

## Method Summary
The study evaluates five deep video classification architectures on the HISTORIAN dataset using RGB-only inputs. All models are initialized with pretrained weights (Sports1M for C3D, ImageNet for others) to facilitate convergence on the small dataset of 767 annotated movement segments. The evaluation uses shot-level data partitioning (9:1 train-validation split) to prevent data leakage, and class-balanced sampling to mitigate class imbalance. The task involves classifying six camera movements (pan, tilt, track, truck, dolly, pan_tilt) while excluding zoom and pedestal due to insufficient samples. Performance is measured using top-1 accuracy, top-2 accuracy, and weighted F1 score.

## Key Results
- Video Swin Transformer achieves highest performance with 80.25% top-1 accuracy
- All architectures show strong convergence despite limited training data (767 samples)
- Temporal modeling capacity strongly correlates with performance: Video Swin > I3D > C3D > R(2+1)D > TSN
- Top-2 accuracy consistently higher than top-1, indicating confusion between semantically similar movements

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical spatiotemporal attention captures subtle, globally coherent camera movement patterns more effectively than sparse sampling or factorized convolutions. Video Swin Transformer applies local window-based attention across both spatial and temporal dimensions, preserving fine-grained motion continuity needed to distinguish similar movements (pan vs. tilt vs. track) that differ primarily in motion direction.

### Mechanism 2
Pretrained spatiotemporal features from action recognition transfer effectively to camera movement classification because both tasks require modeling temporal dynamics. Models initialized with weights from large-scale datasets encode general motion and appearance representations that accelerate convergence on the small HISTORIAN dataset.

### Mechanism 3
Shot-level partitioning prevents data leakage and enables more reliable evaluation in small-data regimes. By grouping all segments from the same shot into either training or validation, the design prevents models from learning shot-specific artifacts that would artificially inflate validation metrics.

## Foundational Learning

- **Concept: 3D Convolutions vs. Temporal Attention** - Understanding how 3D convolutions learn local spatiotemporal filters versus how attention captures variable-range dependencies explains performance differences between architectures.
  - Quick check: Given a 16-frame clip where frames 1-8 show a pan left and frames 9-16 show a pan right, how would a 3x3x3 3D convolution versus a temporal attention mechanism differently represent this motion sequence?

- **Concept: Transfer Learning Under Domain Shift** - The paper applies models trained on modern video to historical footage with noise, blur, and irregular motion. Understanding what transfers and what degrades is essential for interpreting the 80.25% ceiling.
  - Quick check: A model pretrained on YouTube videos achieves 80% on archival footage. List three hypotheses for the 20% performance gap and propose one experiment to distinguish between them.

- **Concept: Optical Flow and Motion Representation** - Related work uses optical flow explicitly, while this paper uses RGB-only learning. Understanding trade-offs between explicit motion representation and implicit learning informs future multimodal extensions.
  - Quick check: Historical footage has flicker and scratches. How would these artifacts affect dense optical flow estimation, and why might RGB-only learning be more robust?

## Architecture Onboarding

- **Component map:** RGB frames → fixed-length clips (model-specific resolution/stride) → class-balanced sampling → Backbone (C3D/R(2+1)D/TSN/I3D/Video Swin) → Cross-entropy loss → 6-class softmax output

- **Critical path:** Load HISTORIAN annotations → filter to 6 classes → create shot-grouped 9:1 splits → convert segments to model-specific input format → initialize backbone with pretrained weights → train with class-balanced sampler → evaluate using top-1/top-2 accuracy and weighted F1

- **Design tradeoffs:**
  - RGB-only vs. multimodal: Simpler pipeline but may miss explicit motion cues; optical flow could improve robustness but requires reliable flow estimation on degraded video
  - Temporal modeling capacity vs. computational cost: Video Swin achieves best results but is heaviest; TSN is efficient but underperforms
  - Validation set size: 9:1 split maximizes training data but yields small validation; authors plan cross-validation for future work

- **Failure signatures:**
  - High top-2 but low top-1 (R(2+1)D: 48.15% → 64.20%): Model confuses semantically similar movements
  - Accuracy >> F1 score (C3D: 64.20% vs. 59.16%): Per-class performance variance indicates class imbalance effects
  - Training loss plateaus early with high validation loss: Overfitting on limited data; increase regularization or data augmentation
  - Predictions collapse to majority class: Class-balanced sampler not functioning; check dataloader configuration

- **First 3 experiments:**
  1. Baseline reproduction: Train Video Swin Transformer with ImageNet pretrained weights, shot-level partitioning, and class-balanced sampling
  2. Temporal modeling ablation: Compare I3D vs. TSN on identical data splits to isolate contribution of continuous temporal processing
  3. Error analysis on confused pairs: Extract (pan, pan_tilt) and (track, truck) confusions; visualize optical flow to determine if annotation boundaries or motion subtlety drives errors

## Open Questions the Paper Calls Out

### Open Question 1
Does incorporating optical flow or motion vector inputs improve CMC robustness against heavy visual degradation in historical footage? The authors suggest extending input modalities beyond RGB but leave multimodal performance untested.

### Open Question 2
Can pre-training models on large-scale modern CMC datasets (e.g., MovieShots) enhance generalization when fine-tuned on historical archival footage? Current experiments use action recognition pretraining rather than domain-specific CMC datasets.

### Open Question 3
How do state-of-the-art deep learning models compare against traditional handcrafted methods on identical historical data partitions? The baseline comparison used different data subsets, preventing fair evaluation.

## Limitations

- Small dataset size (767 segments) creates high variance in validation metrics, particularly weighted F1 score
- 9:1 shot-level split maximizes training data but results in small validation set that may not reliably estimate true performance
- Exclusion of zoom and pedestal classes (4 and 1 examples respectively) means results don't reflect the full 8-class CMC task
- RGB-only approach may underperform compared to multimodal methods that incorporate explicit motion representations

## Confidence

- **High confidence:** Video Swin Transformer achieves best performance (80.25% top-1 accuracy) due to superior spatiotemporal modeling
- **Medium confidence:** Pretrained features transfer effectively from action recognition to CMC tasks
- **Medium confidence:** Shot-level partitioning prevents data leakage and enables reliable evaluation

## Next Checks

1. Implement cross-validation with multiple random shot-level splits to better estimate performance variance and confidence intervals
2. Compare RGB-only Video Swin performance against a multimodal baseline incorporating optical flow to quantify the cost of simplified input representation
3. Test model generalization by training on HISTORIAN and evaluating on an independent historical footage dataset to assess domain adaptation capabilities