---
ver: rpa2
title: 'Improving Diversity in Language Models: When Temperature Fails, Change the
  Loss'
arxiv_id: '2508.09654'
source_url: https://arxiv.org/abs/2508.09654
tags:
- recall
- temperature
- precision
- loss
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the impact of temperature scaling on Precision-Recall
  trade-offs in language models. While lowering temperature improves Precision, increasing
  it often fails to boost Recall, contrary to common intuition.
---

# Improving Diversity in Language Models: When Temperature Fails, Change the Loss

## Quick Facts
- **arXiv ID**: 2508.09654
- **Source URL**: https://arxiv.org/abs/2508.09654
- **Reference count**: 40
- **Primary result**: Temperature scaling fails to improve Recall in language models due to sparsity; training with Recall-oriented losses enables better Precision-Recall trade-offs.

## Executive Summary
This paper demonstrates that standard temperature scaling often fails to improve diversity (Recall) in language models, contrary to common intuition. Through theoretical analysis and empirical validation, the authors show that this limitation stems from the sparsity of natural language distributions. They prove that models must be explicitly trained to prioritize Recall through modified loss functions—specifically TruncR, c-Div, and λ-PR—to make temperature scaling an effective tool for balancing quality (Precision) and diversity. The proposed methods achieve superior Recall levels while maintaining comparable Precision, making temperature tuning more effective for controlling the Precision-Recall trade-off at inference.

## Method Summary
The core approach involves replacing standard NLL loss with three novel loss functions designed to optimize Recall. These losses—TruncR, c-Div, and λ-PR—are implemented as weighted NLL objectives where sample weights are computed using a detached model distribution. The c-Div loss uses Tsallis α-divergence (α > 1), TruncR focuses on low-likelihood samples, and λ-PR targets specific Precision-Recall curve points. All methods use the same training framework: fine-tuning pre-trained models (Olmo-1B, Llama variants) with Adam optimizer, learning rate 1e-6, batch size 8, for 3 epochs. The key innovation is that these losses shift optimization from Precision-favoring to Recall-favoring, enabling temperature scaling to effectively navigate the Precision-Recall trade-off during inference.

## Key Results
- Temperature scaling alone cannot improve Recall due to sparsity constraints in natural language distributions
- Models trained with Recall-oriented losses (c-Div, λ-PR, TruncR) achieve significantly higher Recall while maintaining comparable Precision
- Temperature tuning becomes an effective Precision-Recall trade-off mechanism only when models are trained with Recall-focused objectives
- The c-Div loss with α ≈ 1.4 provides the best overall trade-off across multiple tasks and model sizes

## Why This Works (Mechanism)

### Mechanism 1: The Sparsity-Temperature Interaction Bound
Temperature scaling's effectiveness for improving diversity is fundamentally limited by the sparsity of the target distribution. Theorem 4.2 establishes that both Precision and Recall are upper-bounded by the target distribution's sparsity, scaled by $e^{ZL/t}$. As temperature increases, the tempered model distribution approaches uniform, allocating probability mass to invalid tokens and causing both metrics to degrade toward values determined by the sparsity ratio.

### Mechanism 2: Loss Functions as Precision/Recall Optimizers
Standard losses like NLL implicitly favor Precision by focusing on high-likelihood tokens. The proposed losses—TruncR (low-likelihood focus), c-Div (conditional α-divergence with α > 1), and λ-PR (targeted PR optimization)—re-balance gradients to encourage probability mass across plausible tokens, improving Recall through sample re-weighting and divergence modification.

### Mechanism 3: Training for Recall Enables Effective Temperature Tuning
Models must be explicitly trained for Recall to make temperature scaling effective. NLL-trained models are Precision-locked, where raising temperature mainly degrades quality. Recall-oriented training teaches the model a diverse distribution where valid tokens carry significant mass, creating "headroom" for temperature tuning to recover Precision without fully sacrificing Recall.

## Foundational Learning

- **Concept: Precision-Recall Framework for Generative Models**
  - **Why needed here:** The paper frames quality and diversity entirely through these metrics
  - **Quick check question:** Does high Recall mean generated samples are likely under the reference distribution, or that the reference distribution is covered by generated samples?

- **Concept: Temperature Scaling (t) in Sampling**
  - **Why needed here:** The central problem is that this common diversity knob fails to improve Recall as expected
  - **Quick check question:** As temperature t → ∞, does the output distribution become more peaked or closer to uniform?

- **Concept: f-divergences and Loss Functions in LM Training**
  - **Why needed here:** The solution involves changing the loss from NLL (KL divergence) to alternatives that prioritize Recall
  - **Quick check question:** Minimizing NLL is equivalent to minimizing which f-divergence between P and Q_θ?

## Architecture Onboarding

- **Component map:** Sparsity Bound → Precision-favored Standard Losses → Weighted NLL Implementation → Recall-oriented Training → Tunable Trade-off
- **Critical path:** Understand sparsity bound → recognize Precision-favored standard losses → implement weighted NLL → train with c-Div (α ≈ 1.4) → tune temperature at inference
- **Design tradeoffs:**
  - c-Div: Best P&R trade-off, stable, requires α tuning
  - λ-PR: Precise PR targeting, less stable
  - TruncR: Simple but costly (rolling buffer for quantiles)
  - General: Recall-oriented losses can be less stable than NLL
- **Failure signatures:**
  - NLL models cannot gain Recall via higher temperature
  - Recall losses may show training instability
  - Lower baseline Precision from Recall training must be recovered via lower temperature
- **First 3 experiments:**
  1. Reproduce Sparsity Bound (Figure 5): Train on integer multiplication; plot P&R vs. temperature to observe Recall rise then fall
  2. Compare Losses (Table 1/Figure 7): Fine-tune Olmo-1B on WritingPrompts with NLL, Trunc, c-Div, λ-PR; compare P&R
  3. Validate Tunable Trade-off (Figure 6): Sweep temperature at inference for the best model; verify superior PR curve vs. NLL baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical bounds rely on assumptions about sparsity that may not fully capture real-world language distributions
- Empirical validation focuses on specific tasks and model architectures, limiting generalizability
- Proposed methods introduce hyperparameters requiring careful tuning without systematic sensitivity analysis

## Confidence
- **High Confidence:** NLL training leads to Precision-locked models where temperature scaling fails to improve Recall
- **Medium Confidence:** Theoretical bounds linking sparsity to temperature scaling limits, though assumptions may not fully capture complexity
- **Medium Confidence:** Effectiveness of proposed loss functions in improving Precision-Recall trade-off, requiring further validation across diverse tasks

## Next Checks
1. Validate Sparsity Bound Across Tasks: Reproduce sparsity analysis on diverse language tasks to confirm generality of sparsity assumption
2. Systematic Hyperparameter Sweep: Conduct sweep of α in c-Div, λ in λ-PR across multiple tasks to identify optimal settings and assess sensitivity
3. Generalization to Larger Models: Fine-tune larger models (Llama-2-7B, Llama-2-13B) on same tasks using proposed losses to assess scalability and benefits