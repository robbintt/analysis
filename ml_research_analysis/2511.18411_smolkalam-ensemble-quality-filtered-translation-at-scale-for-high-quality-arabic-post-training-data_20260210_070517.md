---
ver: rpa2
title: 'SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality
  Arabic Post-Training Data'
arxiv_id: '2511.18411'
source_url: https://arxiv.org/abs/2511.18411
tags:
- think
- arabic
- translation
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of large-scale, multi-turn Arabic
  datasets that include reasoning and tool calling for post-training of Arabic language
  models. The authors introduce SmolKalam, a quality-filtered Arabic translation of
  Smoltalk2, using a multi-model ensemble translation pipeline.
---

# SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data

## Quick Facts
- arXiv ID: 2511.18411
- Source URL: https://arxiv.org/abs/2511.18411
- Authors: Sultan Alrashed; Chadi Helwe; Francesco Orabona
- Reference count: 8
- Introduces SmolKalam, a 1.5-1.8M example Arabic SFT dataset via multi-model ensemble translation

## Executive Summary
This paper addresses the lack of large-scale, multi-turn Arabic datasets that include reasoning and tool calling for post-training of Arabic language models. The authors introduce SmolKalam, a quality-filtered Arabic translation of Smoltalk2, using a multi-model ensemble translation pipeline. They combine local translation using Seed-X 7B with API-based translation using Gemma 3-27B, then rank candidates using a reward model and intrinsic quality metrics (Language Ratio and Script Purity). The dataset contains approximately 1.5-1.8 million examples (2.8-3.3B tokens) with improved quality metrics: mean LR of 0.796-0.808 and mean SCR of 0.925-0.928.

## Method Summary
The method involves filtering Smoltalk2 for multi-turn dialogues/reasoning, removing multilingual splits, and stratifying subsets 1:1:2 code:science:math with 50K reasoning + 180K no-think samples. Dual translation is performed - SeedX 7B locally (~490 token chunks, sentence boundary priority, V100 async workers) and Gemma 3-27B via API, generating ≥2 candidates per sample. A Qwen 2.5 1.5B reward model is trained on S1K translation preferences ranked by Arabic MMLU, and LR/SCR intrinsic metrics are computed for all candidates. The final dataset is selected through ranking and filtering based on these quality signals.

## Key Results
- SmolKalam contains 1.5-1.8 million examples (2.8-3.3B tokens) with mean LR of 0.796-0.808 and mean SCR of 0.925-0.928
- Translation quality and downstream Arabic MMLU performance are sensitive to translator choice, sequence length, and sampling parameters
- The dataset represents the first multi-model parallel corpus of this scale and diversity for Arabic instruction tuning
- Ablation studies demonstrate the importance of ensemble approach for quality filtering

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Translation Diversifies Error Modes
Combining translations from multiple models (Seed-X 7B local + Gemma 3-27B API) produces higher-quality candidates than single-model translation by offering alternatives for ranking. Different translation models make different errors; ensemble generation creates N≥2 candidates per sample, allowing downstream selection to pick the best via reward model and intrinsic signals. At least one model in the ensemble will produce a faithful, fluent translation for any given sample.

### Mechanism 2: Reward Model Captures Downstream-Useful Quality Signals
A lightweight reward model (Qwen 2.5 1.5B) trained on preference data ranked by Arabic MMLU performance selects translations that better support downstream fine-tuning. Preference pairs are constructed from translations whose downstream MMLU performance differs; the reward model learns to prefer translations that correlate with better reasoning sensitivity.

### Mechanism 3: Intrinsic Metrics Filter Systematic Translation Failures
Language Ratio (LR) and Script Purity (SCR) filter out translations with length anomalies or script contamination before they reach training. LR penalizes translations that are too short/long relative to source; SCR penalizes non-Arabic script after whitelisting code/URLs. High LR/SCR correlate with translation faithfulness and downstream utility.

## Foundational Learning

- **Concept: Bradley-Terry Preference Modeling**
  - Why needed here: The reward model uses Bradley-Terry objective to learn pairwise preferences from translation rankings
  - Quick check question: Can you explain how Bradley-Terry converts pairwise comparisons into a scalar score?

- **Concept: Translation Quality Estimation (QE)**
  - Why needed here: LR and SCR are QE-style features; understanding QE helps interpret why these metrics work
  - Quick check question: What are common QE features beyond length and script purity?

- **Concept: Multi-turn Instruction Tuning Data**
  - Why needed here: SmolKalam targets multi-turn dialogues with reasoning traces and tool-calling; this differs from single-turn SFT
  - Quick check question: How does message-level chunking preserve conversation structure during translation?

## Architecture Onboarding

- **Component map:** Source Selection (SmolTalk2 subset) → Chunking (≈490 tokens, sentence-aware) → [Local Seed-X 7B | API Gemma 3-27B] → Candidate Pool → Reward Model Ranking + LR/SCR Filtering → SmolKalam Output
- **Critical path:** Chunking quality (sentence boundary detection directly affects reconstruction fidelity) → Candidate generation (both pipelines must produce translations for all samples) → Reward model inference (bottleneck if not batched/parallelized) → LR/SCR threshold selection (too strict = data loss; too loose = quality degradation)
- **Design tradeoffs:** Local (Seed-X 7B) vs API (Gemma 3-27B): Seed-X offers lower latency and cost; Gemma 3 produces higher LR/SCR on long-context reasoning. Chunk size (25 vs 50 vs 100 lines): Table 5 shows 25-line chunks score best on LR/SCR; smaller chunks increase API calls but improve quality. Temperature (0.0–0.7): Table 6 shows moderate temperature (0.7) slightly improves LR/SCR; deterministic (0.0) may under-explore translation alternatives.
- **Failure signatures:** Low SCR on code-heavy splits (e.g., xlam_traces_no_think SCR=0.01–0.05): Expected; do not filter these out if tool-calling is required. Reconstruction errors after chunking: Check sentence boundary logic and metadata consistency. Reward model collapses to trivial preferences (e.g., always preferring shorter): Inspect preference dataset balance.
- **First 3 experiments:** Validate ensemble benefit: Translate held-out subset with Seed-X only, Gemma 3 only, and ensemble+ranking; compare Arabic MMLU after fine-tuning Qwen3-4B on each. LR/SCR threshold sweep: Vary LR threshold (0.6–0.9) and SCR threshold (0.8–0.95); measure retention rate and downstream MMLU to find operating point. Chunk size ablation: Translate s1k-1.1_think with 25/50/100/500-line chunks; report LR, SCR, and reconstruction error rate to confirm 25-line optimality claim.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction scalability: The multi-model ensemble approach requires significant computational resources (600 GPU-hours for Seed-X 7B alone plus API costs); scalability to 10M+ examples remains unproven
- Reward model generalization: The Qwen 2.5 1.5B reward model is trained on S1K translation preferences ranked by Arabic MMLU performance; generalization beyond Arabic MMLU to other downstream tasks is uncertain
- Code-heavy split handling: Several splits show very low SCR (0.01-0.05) due to extensive code content; filtering approach requires careful handling to avoid removing valid tool-calling examples

## Confidence
- **High confidence:** The ensemble translation approach with quality filtering is technically sound and well-implemented; reported LR (0.796-0.808) and SCR (0.925-0.928) metrics indicate successful quality filtering
- **Medium confidence:** Downstream Arabic MMLU performance improvements are demonstrated, but exact contribution of each component is not fully isolated; preference dataset construction details are partially specified
- **Low confidence:** Generalization of reward model beyond Arabic MMLU tasks and approach's effectiveness on substantially larger datasets (>10M examples) remain unproven

## Next Checks
1. **Downstream task generalization:** Fine-tune Arabic models on SmolKalam and evaluate on a diverse set of Arabic benchmarks beyond MMLU (including code generation, reasoning, and conversation tasks) to validate the reward model's preference learning generalizes
2. **Component ablation study:** Systematically remove each quality filtering component (reward model, LR filtering, SCR filtering) and measure their individual contributions to both translation quality metrics and downstream performance to isolate their effects
3. **Scalability stress test:** Attempt to scale the ensemble approach to 5M+ examples using the same computational budget, measuring quality degradation points and identifying bottlenecks in the pipeline (chunking, translation, ranking, filtering)