---
ver: rpa2
title: Optimal Regret of Bernoulli Bandits under Global Differential Privacy
arxiv_id: '2505.05613'
source_url: https://arxiv.org/abs/2505.05613
tags:
- regret
- bound
- bandits
- basu
- azize
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies regret minimization in stochastic bandits under\
  \ global differential privacy (DP), focusing on Bernoulli bandits. The main contributions\
  \ are: (1) a tighter regret lower bound that introduces a new information-theoretic\
  \ quantity d\u03F5, which smoothly interpolates between Kullback-Leibler divergence\
  \ and Total Variation distance depending on the privacy budget; (2) a new concentration\
  \ inequality for sums of Bernoulli variables under Laplace mechanism, which is a\
  \ DP version of the Chernoff bound; and (3) two DP algorithms, DP-KLUCB and DP-IMED,\
  \ that asymptotically match the lower bound up to a constant arbitrarily close to\
  \ 1."
---

# Optimal Regret of Bernoulli Bandits under Global Differential Privacy

## Quick Facts
- arXiv ID: 2505.05613
- Source URL: https://arxiv.org/abs/2505.05613
- Reference count: 40
- Main result: Tighter regret lower bound for DP Bernoulli bandits using $d_\epsilon$, new Chernoff-style concentration, and DP-KLUCB/DP-IMED algorithms matching the bound up to constant $\alpha>1$

## Executive Summary
This paper establishes tight regret bounds for stochastic bandits with Bernoulli rewards under global differential privacy. The key innovation is a new information-theoretic quantity $d_\epsilon$ that interpolates between KL divergence and Total Variation distance, providing a precise measure of the privacy-utility tradeoff. The paper also introduces a novel concentration inequality coupling Laplace noise with Bernoulli rewards, and presents two algorithms (DP-KLUCB and DP-IMED) that asymptotically achieve the lower bound without requiring reward-forgetting.

## Method Summary
The method centers on three technical contributions: (1) a hardness quantity $d_\epsilon(x,y) = \inf_z \{\epsilon|z-x| + \text{kl}(z,y)\}$ that characterizes the fundamental limit of private bandit learning, (2) a coupled concentration inequality for sums of Bernoulli rewards and Laplace noise (Proposition 7) that avoids sub-optimal separate bounds, and (3) two algorithms using arm-dependent geometric batching that accumulate noisy sums cumulatively rather than forgetting past rewards. The algorithms solve index computations involving $d_\epsilon$ to balance exploration and privacy.

## Key Results
- Introduces $d_\epsilon$ quantity interpolating between KL and TV distances for privacy-utility characterization
- Proves regret lower bound of $\Omega\left(\sum_{i:\Delta_i>0} \frac{\log T}{d_\epsilon(\mu_i, \mu^*)}\right)$ under global DP
- Establishes new concentration inequality for Bernoulli + Laplace sums with optimal exponent
- DP-KLUCB and DP-IMED algorithms match lower bound up to factor $\alpha>1$ determined by batching schedule

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Hardness Quantification ($d_\epsilon$)
The difficulty of distinguishing optimal arms under differential privacy is characterized by $d_\epsilon$, which finds an intermediate distribution minimizing the cost of transporting probability mass while accounting for privacy constraints. This interpolation property is unique to the Bernoulli case and enables precise regret lower bounds.

### Mechanism 2: Coupled Concentration Inequality
Instead of bounding reward sums and noise sums separately, the paper derives a joint Chernoff-style bound where the dominant exponent depends on $d_\epsilon$ as if only one noise term were present, provided the number of noise instances is sublinear in samples. This coupling yields tighter regret bounds than traditional approaches.

### Mechanism 3: Non-Forgetting Batching Strategy
The algorithms maintain cumulative noisy sums by adding fresh Laplace noise to reward increments in each batch, rather than resetting between phases. This stateful approach refutes the conjecture that forgetting is necessary for optimal regret under DP, though it requires careful batch size control to ensure sublinear noise addition.

## Foundational Learning

- **Concept: Regret Minimization (Stochastic Bandits)** - Why needed: The paper's entire analysis revolves around minimizing regret (difference between optimal and actual rewards). Quick check: In KL-UCB, does the index use variance or KL-divergence? (KL-divergence for tighter bounds).

- **Concept: Global Differential Privacy ($\epsilon$-DP)** - Why needed: The algorithms must ensure that the sequence of actions doesn't reveal information about individual rewards. Quick check: In Global DP, is noise added on user's device (Local DP) or by trusted server? (Trusted server).

- **Concept: Laplace Mechanism** - Why needed: The fundamental tool for achieving $\epsilon$-DP is adding Laplace noise scaled to query sensitivity. Quick check: For sum of rewards in $[0,1]$, what is sensitivity? (1, so noise ~ Lap(1/$\epsilon$)).

## Architecture Onboarding

- **Component map:** Environment -> Batching Scheduler -> Private Aggregator -> Index Solver -> Action Selector
- **Critical path:** The Index Solver (Eq 12/13) implementation, requiring numerical optimization to compute the supremum or $d_\epsilon$ function efficiently within the main loop.
- **Design tradeoffs:** Batching ratio $\alpha$ close to 1 matches theory but increases phases and numerical instability; cumulative sums are more efficient than forgetting but require state management.
- **Failure signatures:** Poor $d_\epsilon$ optimization approximation causes regret spikes in high-privacy regimes; large Laplace noise from small $\epsilon$ can overwhelm early signals.
- **First 3 experiments:**
  1. Replicate Figure 1 regret trajectories for $\mu_1$, $\mu_2$ environments with $\epsilon=0.25$ vs. AdaP-KLUCB
  2. Implement Appendix G sweep varying $\epsilon$ from 0.01 to 1.0 to verify smooth transition between TV/KL regimes
  3. Modify Algorithm 1 to "forget" sums between phases and compare regret to cumulative version

## Open Questions the Paper Calls Out

- **Generalization to other distributions:** Can the concentration inequality and regret bounds extend to sub-Gaussian or exponential families? This requires re-deriving $d_\epsilon$ and the coupled bound for non-Bernoulli cases.

- **Approximate DP equivalence:** Does the equivalence between Table DP, View DP, and Adaptive Continual Release hold for $(\epsilon,\delta)$-DP? The current proof relies on atomic events sufficient for pure $\epsilon$-DP but may break for composite events needed in approximate DP.

- **Optimal constant achievement:** Is it possible to achieve the asymptotic lower bound with constant exactly 1 rather than factor $\alpha>1$ determined by batching? This questions whether $\alpha$ is an artifact of analysis or fundamental to the no-forgetting approach.

## Limitations

- The theoretical analysis relies on closed-form $d_\epsilon$ for Bernoulli distributions; extension to other reward families requires significant new derivation
- Optimal batching ratio $\alpha$ must be arbitrarily close to 1 theoretically but this is impractical due to numerical stability concerns
- Experimental validation is limited to only two specific 5-arm environments, potentially missing broader problem characteristics

## Confidence

- **High Confidence:** Regret lower bound (Theorem 5) and its connection to $d_\epsilon$ are rigorously proven; non-forgetting batching strategy is theoretically sound given Proposition 7
- **Medium Confidence:** DP-KLUCB and DP-IMED algorithms correctly implement the blueprint; matching the lower bound requires setting $\alpha$ extremely close to 1 which may be impractical
- **Low Confidence:** Experimental comparison with baseline algorithms lacks implementation details; exact replication is uncertain without access to their code or hyperparameters

## Next Checks

1. **Regret Scaling Verification:** Implement DP-KLUCB and DP-IMED on $\mu_1$ and $\mu_2$ environments; plot cumulative regret vs. $T$ up to $10^6$ to verify $O(\log T)$ scaling with constant close to theoretical lower bound

2. **Privacy Regime Sweep:** Run experiments varying $\epsilon$ from 0.01 to 1.0; confirm regret transitions smoothly between TV-dominated (high privacy) and KL-dominated (low privacy) regimes as predicted by $d_\epsilon$ interpolation

3. **Ablation on Forgetting:** Modify DP-KLUCB to reset cumulative sum at each phase; compare regret against standard version to empirically validate theoretical claim that forgetting is suboptimal