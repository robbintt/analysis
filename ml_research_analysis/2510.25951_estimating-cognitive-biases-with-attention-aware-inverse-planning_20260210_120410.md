---
ver: rpa2
title: Estimating cognitive biases with attention-aware inverse planning
arxiv_id: '2510.25951'
source_url: https://arxiv.org/abs/2510.25951
tags:
- inverse
- behavior
- construal
- planning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating human cognitive
  biases in attention-limited decision-making by extending value-guided construal
  theory to incorporate heuristic attentional biases. The core method idea involves
  formalizing attention-aware inverse planning as maximum likelihood inference over
  heuristic bias parameters that govern construal selection.
---

# Estimating cognitive biases with attention-aware inverse planning

## Quick Facts
- arXiv ID: 2510.25951
- Source URL: https://arxiv.org/abs/2510.25951
- Reference count: 28
- One-line primary result: Attention-aware inverse planning successfully recovers cognitive biases from observed behavior in both tabular and complex driving domains.

## Executive Summary
This paper addresses the problem of estimating human cognitive biases in attention-limited decision-making by extending value-guided construal theory to incorporate heuristic attentional biases. The core method formalizes attention-aware inverse planning as maximum likelihood inference over heuristic bias parameters that govern construal selection. The approach combines deep reinforcement learning with computational cognitive modeling to estimate biases from observed behavior, demonstrating successful recovery of underlying heuristic biases from synthetic data in both tabular DrivingWorld scenarios and complex real-world driving scenarios from the Waymo Open Dataset.

## Method Summary
The method extends value-guided construal theory by adding parameterized bias functions to the construal selection policy, which governs how agents simplify mental representations of their environment. For maximum likelihood estimation, the likelihood of observed trajectories is computed by marginalizing over all possible construals, with construal probabilities weighted by both utility and learned attentional heuristics. To scale to complex continuous domains like driving, the computational cost of planning for every construal is avoided by masking inputs to a pre-trained deep RL generalist policy. The recovery process involves inferring bias parameters that maximize the likelihood of observed behavior under this attention-aware decision-making model.

## Key Results
- Successfully recovered heuristic biases from synthetic behavior in tabular DrivingWorld scenarios with R² = 0.77-0.83 for less consequential biases
- Maximum likelihood estimation recovered heuristic parameters from 80 trajectories (12 minutes of driving data) across 10 scenarios with 215 agents in Waymo Open Dataset
- Standard inverse reinforcement learning fails to capture attention-limited behavior in certain scenarios, while the proposed attention-aware approach systematically differs by incorporating attentional biases

## Why This Works (Mechanism)

### Mechanism 1: Value-Guided Construal with Heuristic Bias
Agents do not perceive the full state but rather a construed state containing a subset of objects. The selection of this subset is probabilistic, governed by a softmax over the sum of Value of Representation (VOR) and a Heuristic Bias function. VOR incentivizes including task-relevant objects while Hλ introduces systematic biases (e.g., ignoring "ice" if driving in a warm climate). Agents use a static construal throughout an episode rather than dynamic switching. The static model will fail if agents dynamically switch attention or if the object-oriented representation fails to capture critical features.

### Mechanism 2: Inversion via Maximum Likelihood Estimation
Hidden cognitive biases are recovered by finding parameters λ that maximize the likelihood of the observed trajectory. The problem is framed as MLE where likelihood is computed by marginalizing over all possible construals, summing the probability of taking observed actions under each construal weighted by the probability of selecting that construal given bias parameters. The optimizer may attribute behavior to "value" rather than "bias" when the bias is indistinguishable from the value signal, causing identifiability issues.

### Mechanism 3: Policy Amortization for Scalability
To scale to complex continuous domains like driving, the computational cost of planning for every construal is avoided by masking inputs to a pre-trained deep RL generalist policy. Instead of solving a new MDP for every subset of objects, the method uses a pre-trained PPO agent and masks out objects not in the construal before passing to the policy. This approximates the construed policy. The generalist policy may fail if it relies on implicit background context from masked objects or crashes immediately when valid objects are masked.

## Foundational Learning

- **Concept: Value of Representation (VOR)**
  - Why needed here: This is the core objective function agents optimize, explaining why agents pay attention to some things (high utility) and ignore others (high complexity cost).
  - Quick check question: If an agent ignores a "parked car" but pays attention to a "car merging," which term in VOR (Utility vs. Cost) likely drives the difference?

- **Concept: Inverse Reinforcement Learning (IRL) vs. Inverse Planning**
  - Why needed here: The paper positions itself against standard IRL. You must understand that IRL assumes agents optimize an unknown reward with full attention, whereas this method assumes known rewards but limited/biased attention.
  - Quick check question: If a driver ignores a stop sign, would standard IRL infer that stop signs have a low penalty, or that the driver failed to attend to it?

- **Concept: Object-Oriented MDPs**
  - Why needed here: The mechanism relies on "construals" being subsets of objects. Understanding that states are collections of objects (cars, cones, ice) rather than monolithic vectors is essential for the masking operation.
  - Quick check question: In a grid world, what is the difference between masking a "state feature" (e.g., specific coordinate) and masking an "object" (e.g., a specific obstacle)?

## Architecture Onboarding

- **Component map:**
  1. Simulator (GPUDrive) -> 2. Construal Generator -> 3. Masking Layer -> 4. Generalist Policy (Oracle) -> 5. Inference Engine

- **Critical path:**
  1. Start with a dataset of trajectories
  2. For a candidate λ, compute the probability of selecting various construals C
  3. Pass masked states s[C] to the Generalist Policy to get action probabilities
  4. Aggregate probabilities to get P(ζ|λ)
  5. Update λ to maximize this likelihood

- **Design tradeoffs:**
  - Exact vs. Amortized Inference: Using a pre-trained policy is fast (amortized) but may introduce approximation errors compared to exact planning in the tabular setting
  - Identifiability: Less consequential biases are easier to recover (R² ≈ 0.83) than critical ones (e.g., ignoring parked cars), because value signals can drown out bias signals

- **Failure signatures:**
  - IRL Confounding: If you feed this data to standard IRL, it will output "noisy" rewards or fail to converge, misattributing attention lapses to reward preferences
  - Policy Generalization Gap: If the generalist policy crashes immediately when valid objects are masked, the inversion will fail

- **First 3 experiments:**
  1. Implement the "DrivingWorld" grid and verify that an agent with λIce = -10 drives over ice while λIce = 10 avoids it
  2. Run standard IRL on trajectories generated by a "bias-ignoring" agent and confirm that IRL infers incorrect rewards rather than identifying the inattention
  3. Select one Waymo scenario, generate 80 synthetic trajectories using a known λ, and attempt to recover λ using the GPUDrive integration to verify the 12-minute data requirement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the model be extended to handle dynamic construal switching within a single episode?
- **Basis in paper:** [explicit] The authors state they "do not model dynamic construals or switching construals within an episode" but identify this as an "important direction for future work."
- **Why unresolved:** The current formulation assumes a decision-maker selects a single fixed construal at the beginning of an episode and maintains it, ignoring how attention shifts in response to new objects or changing task contexts.
- **What evidence would resolve it:** A modified model that successfully updates task construals in response to environmental events and accurately predicts time-varying attentional behaviors.

### Open Question 2
- **Question:** Is it feasible to perform joint inference of an agent's reward function and their attentional biases simultaneously?
- **Basis in paper:** [explicit] Section 5 notes the work assumes a known reward model and suggests "future work will need to... extend our approach to... joint inference about rewards and attentional biases."
- **Why unresolved:** The current MLE framework fixes the reward function R and only infers the heuristic bias parameters λ; it is unclear if both sets of parameters can be identified independently from behavior alone.
- **What evidence would resolve it:** A study demonstrating that the algorithm can recover distinct reward and bias parameters from the same dataset without encountering identifiability issues or convergence errors.

### Open Question 3
- **Question:** Can the approach accurately estimate heuristic biases when applied to real human behavior rather than synthetic agents?
- **Basis in paper:** [explicit] The authors note they "estimate heuristics from synthetic data" and that "validation of our algorithm on human behavior in more tasks is an important next step."
- **Why unresolved:** The validation relies on synthetic trajectories generated by RL agents with known ground-truth biases. It is unconfirmed whether the model can disentangle biases from the noise and variability inherent in actual human decision-making.
- **What evidence would resolve it:** Successful application of the method to human driving data where inferred bias parameters correlate with independent validation metrics, such as eye-tracking data.

## Limitations
- Static construal assumption - agents maintain fixed subset of attended objects throughout an episode, which may not capture dynamic attention shifts
- Identifiability problem - biases affecting critical safety objects show poor recovery because their effects are easily attributed to reward preferences
- Requires knowing the true reward function a priori, which may not hold in real applications
- Limited validation - Waymo dataset analysis is limited to 10 scenarios and 215 agents

## Confidence
**High Confidence**: The fundamental framework of value-guided construal theory, the MLE formulation for bias recovery, and the systematic failure of standard IRL to capture attention-limited behavior.

**Medium Confidence**: The scalability claims for the GPUDrive implementation and the 12-minute data requirement for accurate recovery. The masking approximation's fidelity to true construal-based planning is reasonable but not proven equivalent.

**Low Confidence**: The generalization of bias recovery patterns from synthetic to real human data, the robustness of the method when reward functions are misspecified, and the performance on diverse driving scenarios beyond the 10 tested.

## Next Checks
1. **Dynamic Construal Validation**: Modify the DrivingWorld implementation to allow agents to recompute construals at each timestep. Compare bias recovery performance between static and dynamic attention models on the same synthetic data.

2. **Reward Misspecification Stress Test**: Run the GPUDrive pipeline with a deliberately incorrect reward function (e.g., wrong penalty for collisions). Measure how this affects bias recovery accuracy and whether the method can distinguish between reward and attention errors.

3. **Cross-Scenario Generalizability**: Take the 10 Waymo scenarios and train the recovery model on 7, then test bias recovery on the held-out 3. Quantify performance degradation and analyze which scenario features correlate with recovery success/failure.