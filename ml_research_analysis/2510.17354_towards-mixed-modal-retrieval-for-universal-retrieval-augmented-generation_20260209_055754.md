---
ver: rpa2
title: Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation
arxiv_id: '2510.17354'
source_url: https://arxiv.org/abs/2510.17354
tags:
- retrieval
- zhang
- generation
- text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of Universal Retrieval-Augmented
  Generation (URAG), where both queries and documents contain mixed modalities such
  as text and images. The authors propose Nyx, a unified retriever designed for mixed-modal-to-mixed-modal
  retrieval, and construct NyxQA, a large-scale dataset of mixed-modal question-answer
  pairs reflecting real-world scenarios.
---

# Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2510.17354
- Source URL: https://arxiv.org/abs/2510.17354
- Authors: Chenghao Zhang; Guanting Dong; Xinyu Yang; Zhicheng Dou
- Reference count: 40
- One-line primary result: Nyx achieves up to 7-point accuracy gains on mixed-modal retrieval tasks through VLM-guided alignment.

## Executive Summary
This paper addresses the challenge of Universal Retrieval-Augmented Generation (URAG), where both queries and documents contain mixed modalities such as text and images. The authors propose Nyx, a unified retriever designed for mixed-modal-to-mixed-modal retrieval, and construct NyxQA, a large-scale dataset of mixed-modal question-answer pairs reflecting real-world scenarios. The method employs a two-stage training framework: pretraining on both public and synthetic data using contrastive learning with Matryoshka Representation Learning, followed by fine-tuning with feedback from vision-language models to align retrieval outputs with generative preferences. Experimental results show that Nyx achieves significant improvements over state-of-the-art baselines, with accuracy gains of up to 7 points on NyxQA and strong performance across text-only and multimodal RAG benchmarks.

## Method Summary
Nyx employs a two-stage training framework on a Qwen-2.5-VL-3B-Instruct backbone. Stage 1 uses contrastive pretraining with Matryoshka Representation Learning on mixed public and synthetic data, encoding arbitrarily interleaved text-image sequences into unified embeddings via the VLM's <EOS> token. Stage 2 fine-tunes using VLM-guided feedback alignment, constructing a preference dataset where sliding-window contexts to VLMs identify documents supporting correct generation. The model processes mixed-modal inputs with a 400×28×28 pixel cap on visual resolution and uses LoRA rank 8 with DeepSpeed ZeRO-2 for efficient training.

## Key Results
- Nyx achieves 7-point accuracy gains on NyxQA compared to state-of-the-art baselines
- Performance degrades gracefully with MRL truncation: 0.8183 (2048D) → 0.7467 (256D)
- VLM feedback fine-tuning improves MMQA F1 from 35.97% to 44.50%
- Strong performance across text-only and multimodal RAG benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Unified Mixed-Modal Embedding Space
Encoding arbitrarily interleaved text-image sequences into a single embedding enables retrieval across modality combinations that "divide-and-conquer" approaches miss. The model treats mixed-modal content as an ordered sequence x ∈ {a₁a₂...aₙ | aᵢ ∈ {T,I}}, using the VLM's final <EOS> token hidden representation as the global retrieval embedding, preserving spatial/logical relationships between text and images within documents.

### Mechanism 2: VLM-Guided Feedback Alignment
Fine-tuning retrievers using downstream VLM preferences improves generation quality beyond gold-document labels. Stage-2 constructs a preference dataset: for each query, retrieve top-K candidates using Stage-1 model, apply sliding-window contexts to VLM, mark the first document in windows yielding correct/quality-threshold answers as positive, others as negatives. This aligns retrieval with actual generative utility.

### Mechanism 3: Matryoshka Representation Learning (MRL) for Flexible Retrieval
Training embeddings to remain informative at multiple truncated dimensions enables scalable retrieval with graceful performance trade-offs. Aggregate InfoNCE losses at dimensions {2048, 1024, 512, 256} with weights {1.0, 1.0, 0.2, 0.2}, forcing prefix dimensions to preserve semantic structure.

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss)**: Why needed here - The training objective explicitly uses InfoNCE with hard negatives; understanding why this pulls positives together and pushes negatives apart is essential for debugging retrieval quality. Quick check question: Given query q, positive d+, and negatives {d⁻}, can you write the InfoNCE loss and explain why temperature τ matters?

- **Vision-Language Models (VLMs)**: Why needed here - The backbone (Qwen-2.5-VL-3B) processes interleaved text-image inputs; understanding tokenization, image resolution constraints (400×28×28 cap), and <EOS> extraction is critical for implementation. Quick check question: How does a VLM represent an image within a text sequence, and what happens if you exceed the resolution budget during training?

- **Retrieval-Augmented Generation (RAG)**: Why needed here - The entire framework extends RAG to mixed-modal settings; knowing the baseline (retrieve → generate) helps contextualize why alignment matters. Quick check question: In standard text RAG, what's the trade-off between retrieving more documents vs. generation quality? How might this differ in multimodal settings?

## Architecture Onboarding

- **Component map**: OBELICS web documents → chunking (≤200 tokens) → VLM QA generation → post-processing (error filtering, refinement, option generation) → hard negative mining (mmE5 retrieval) → Qwen-2.5-VL-3B-Instruct backbone → <EOS> embedding extraction → MRL projection → InfoNCE training → VLM feedback collection → final Nyx

- **Critical path**: Build NyxQA corpus (46,741 chunks, 10K for QA) → Train Nyx-pretrained with InfoNCE+MRL → Collect VLM feedback (sliding windows, quality thresholds) → Fine-tune to obtain final Nyx

- **Design tradeoffs**: 3B model vs. 11B (mmE5) - smaller backbone but better alignment through targeted feedback; Dimension flexibility (MRL) vs. raw performance at max dimension; Training coverage (text-only + interleaved) vs. specialization risk

- **Failure signatures**: Memory overflow on multi-image inputs (mitigated by 400×28×28 resolution cap); Non-monotonic performance vs. generator size (alignment varies across VLMs); ~33% of answers incorrect even with golden documents (VLM noise, grounding gaps)

- **First 3 experiments**: 1) Baseline comparison: Run Nyx-pretrained vs. mmE5 on NyxQA with Qwen2.5-VL-7B generator - verify reported accuracy gap (74.83% vs 66.83%); 2) Ablation: Remove Stage-2 feedback - Compare Nyx-pretrained to final Nyx on MMQA and NyxQA - quantify alignment contribution (e.g., F1 delta 35.97→44.50); 3) MRL dimension sweep - Measure retrieval accuracy and latency at 256/512/1024/2048 dims on NyxQA - validate graceful degradation claim

## Open Questions the Paper Calls Out

### Open Question 1
How can retrieval models be optimized to leverage non-golden evidence that VLMs prefer over human-annotated "golden" documents? The authors state in Section 5.5 that "Future improvements may arise from modelling VLM preferences on non-golden evidence, which can sometimes diverge from human intuition." This remains unresolved because the analysis reveals that nearly half of the answers are correct even with non-golden documents, and documents preferred by VLMs may differ from labeled positives, yet current training still relies on "golden" labels.

### Open Question 2
Why does RAG performance not scale monotonically with the size of the generator VLM, and how can retrievers be aligned to specific generator architectures? Section 5.3 notes that "performance does not increase monotonically with model size, this suggests that generator size is not a reliable predictor of RAG pipeline performance. Effective alignment is crucial..." While Nyx shows gains across InternVL3 variants, the performance varies non-linearly (e.g., drops at 8B and 38B), indicating unexplained alignment gaps between the fixed retriever and varying generator architectures.

### Open Question 3
Does the reliance on synthetic data generation by a specific VLM (InternVL3-78B) introduce bias regarding the types of mixed-modal reasoning Nyx can perform? The NyxQA pipeline relies entirely on InternVL3-78B to generate questions and answers. While filtering is applied, the paper does not evaluate if the retriever inherits the limitations or hallucinations of the teacher model used for synthesis. If the synthetic data contains systematic errors or specific reasoning patterns unique to the teacher model, Nyx might overfit to these synthetic distributions rather than generalizing to human-generated mixed-modal queries.

## Limitations
- Reliance on single VLM (Qwen-2.5-VL-3B-Instruct) for both encoding and feedback generation with no ablation on alternative backbones
- 400×28×28 pixel cap on visual inputs may discard fine-grained visual details critical for certain retrieval tasks
- Feedback alignment mechanism depends on unspecified quality thresholds, making robustness assessment difficult

## Confidence
- **High Confidence**: The unified embedding space approach is well-grounded in the paper's architecture and evaluation shows consistent improvements across benchmarks
- **Medium Confidence**: The VLM-guided feedback alignment shows strong empirical gains but depends heavily on the quality and generalizability of the feedback collection process
- **Low Confidence**: The synthetic data generation pipeline's quality control mechanisms are described but not validated for potential biases or domain-specific failures

## Next Checks
1. **Feedback Threshold Sensitivity**: Systematically vary the EM/F1 quality thresholds used in VLM feedback collection and measure their impact on retrieval accuracy and alignment generalization across different VLM generators

2. **Backbone Generalization**: Replace the Qwen-2.5-VL-3B backbone with alternative VLMs (e.g., GPT-4V, Gemini Pro Vision) while keeping the same training procedure to assess the method's dependency on specific architectural choices

3. **Visual Resolution Impact**: Conduct controlled experiments varying the maximum visual resolution cap (e.g., 200×200, 400×400, 800×800) on retrieval accuracy for visually-rich document queries to quantify the trade-off between computational efficiency and visual detail preservation