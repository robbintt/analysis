---
ver: rpa2
title: 'Counting trees: A treebank-driven exploration of syntactic variation in speech
  and writing across languages'
arxiv_id: '2505.22774'
source_url: https://arxiv.org/abs/2505.22774
tags:
- syntactic
- spoken
- structures
- corpora
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel treebank-driven method to analyze
  syntactic variation in speech and writing across languages. By extracting delexicalized
  dependency trees from Universal Dependencies corpora in English and Slovenian, it
  identifies syntactic structures without imposing prior assumptions.
---

# Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages

## Quick Facts
- **arXiv ID**: 2505.22774
- **Source URL**: https://arxiv.org/abs/2505.22774
- **Reference count**: 13
- **Primary result**: Spoken corpora contain fewer and less diverse syntactic structures than written ones, with minimal overlap between modalities.

## Executive Summary
This study introduces a novel treebank-driven method to analyze syntactic variation in speech and writing across languages. By extracting delexicalized dependency trees from Universal Dependencies corpora in English and Slovenian, it identifies syntactic structures without imposing prior assumptions. Results show that spoken corpora contain fewer and less diverse syntactic structures than written ones, with minimal overlap between modalities. Speech-specific structures—such as short elliptical clauses and formulaic expressions—reflect interactivity and economy of expression. This scalable, language-independent framework enables systematic corpus-driven syntactic comparison, offering new insights into modality-specific grammatical preferences.

## Method Summary
The method extracts all dependency (sub)trees from Universal Dependencies treebanks, treating each word as the root of its own syntactic structure. Delexicalized trees are created by replacing lexical items with Universal POS tags, then compared across spoken and written corpora using segmented Type-Token Ratio (STTR) for diversity, overlap analysis for shared structures, and %DIFF keyness for modality-specific patterns. The STARK tool is configured with labeled, delexicalized trees while preserving word order. Punctuation is removed, and disfluencies are optionally excluded to isolate core syntax.

## Key Results
- Spoken corpora show consistently lower STTR values than written corpora (p < 0.001), indicating less syntactic diversity.
- Structural overlap between spoken and written modalities is minimal, suggesting distinct syntactic inventories.
- Speech-specific structures include short elliptical clauses, paratactic constructions, and formulaic expressions reflecting interactive and economical communication.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Delexicalized dependency tree extraction enables cross-linguistic, cross-modal syntactic comparison without prior assumptions about which structures matter.
- Mechanism: Each word in a corpus is treated as the root of its own dependency (sub)tree, then lexical items are replaced with Universal POS tags (e.g., "the" → DET). This creates abstract structural patterns that can be counted and compared regardless of vocabulary differences.
- Core assumption: Delexicalized trees capture syntactically meaningful generalizations; removing lexical content preserves structural regularities while enabling comparability.
- Evidence anchors:
  - [abstract] "we define syntactic structures as delexicalized dependency (sub)trees and extract them from spoken and written Universal Dependencies (UD) treebanks"
  - [section 3.1] "we consider each word in the corpus as the head (root) of its own syntactic structure... comprising the word itself along with all its direct and indirect dependents"
  - [corpus] No corpus evidence available for whether delexicalization preserves all linguistically relevant distinctions; this is a methodological assumption.
- Break condition: If research questions require lexico-grammatical patterns (e.g., "let's see" as a formulaic unit), pure delexicalization will miss these; the paper notes configurability to add lexical information (Section 5).

### Mechanism 2
- Claim: Segmented Type-Token Ratio (STTR) provides size-independent comparison of syntactic diversity across corpora of different lengths.
- Mechanism: Rather than computing one TTR for an entire corpus (which conflates corpus size with diversity), STTR averages TTR across fixed 1,000-token segments. Higher STTR indicates more structural variety per unit of text.
- Core assumption: The structural diversity captured by STTR reflects meaningful differences in syntactic usage patterns, not just artifacts of genre or transcription conventions.
- Evidence anchors:
  - [section 3.3] "we computed the Type-Token Ratio (TTR)... used Segmented TTR (STTR), which averages TTR over fixed-size segments (1,000 tokens) rather than full corpora, enabling size-independent comparison"
  - [section 4.1] "STTR values are consistently lower in speech than in writing for both languages (p < 0.001)"
  - [corpus] Related work on syntactic complexity (De Clercq & Housen 2017, cited in references) supports structural diversity metrics, but no direct corpus comparison of STARK-extracted trees exists.
- Break condition: If corpora have highly variable segment quality (e.g., transcription boundaries don't align with syntactic units), STTR may introduce noise.

### Mechanism 3
- Claim: The %DIFF keyness measure identifies structures that are disproportionately characteristic of one modality, surfacing modality-specific syntactic patterns.
- Mechanism: %DIFF calculates the proportional difference in normalized frequencies between two corpora. Structures absent from the comparison corpus receive very high values, effectively highlighting unique constructions.
- Core assumption: Structures with high %DIFF values are genuinely characteristic of the target modality, not just low-frequency noise or corpus artifacts.
- Evidence anchors:
  - [section 3.3] "%DIFF keyness measure... a commonly used effect-size metric in corpus linguistics to measure the proportion of the difference between the normalized frequencies"
  - [section 4.3] Tables 5 and 6 show top-ranked speech-specific structures; the paper notes these "reflect the interactive, context-grounding, and economical dimensions of speech"
  - [corpus] Limited external validation; the interpretations of speech-specific structures align with prior spoken grammar literature (Biber et al. 1999) but the specific structures identified are novel to this method.
- Break condition: When an item is absent from the comparison corpus, the paper uses a proxy value (1×10⁻¹⁵); this creates extremely large %DIFF values that may over-emphasize rare structures.

## Foundational Learning

- **Concept: Universal Dependencies (UD) annotation scheme**
  - Why needed here: The entire extraction method depends on consistently annotated dependency trees. UD provides 17 POS tags, 37 dependency relations, and cross-linguistic guidelines.
  - Quick check question: Given the sentence "She stayed," what is the dependency relation between "She" and "stayed"? (Answer: nsubj—nominal subject)

- **Concept: Dependency grammar tree structure**
  - Why needed here: The method extracts trees rooted at every word; understanding head-dependent relationships is essential for interpreting results.
  - Quick check question: In a dependency tree, does each word have exactly one head (except the root)? (Answer: Yes, by definition of dependency grammar.)

- **Concept: Type-Token Ratio (TTR) and its segmentation**
  - Why needed here: The paper uses STTR to compare syntactic diversity; understanding why raw TTR is problematic for comparing unequal corpus sizes is critical.
  - Quick check question: Why would a 10,000-word corpus likely have a lower TTR than a 1,000-word corpus even if both draw from the same distribution? (Answer: More tokens provide more opportunities for types to repeat, lowering the ratio.)

## Architecture Onboarding

- **Component map**: UD treebank files (CoNLL-U format) -> STARK extractor (configured for labeled, delexicalized trees, fixed word order) -> Structure lists (one row per tree type, with frequency counts) -> Comparison scripts (TTR/STTR, overlap analysis, %DIFF keyness)
- **Critical path**: 
  1. Obtain or parse corpora with UD annotation (existing treebanks or run Stanza/Trankit parsers)
  2. Create normalized corpus versions (remove punctuation; optionally remove disfluencies)
  3. Configure STARK: `node_type=upos`, `labelled=yes`, `label_subtypes=no`, `fixed=yes`
  4. Extract structure lists for each corpus
  5. Compute STTR (1,000-token segments), overlap counts, and %DIFF keyness
- **Design tradeoffs**:
  - Delexicalized vs. lexicalized: Delexicalization enables cross-linguistic comparison but loses formulaic expressions; the paper notes this is configurable.
  - Include vs. exclude disfluencies: Including reparandum/discourse captures full spoken syntax; excluding isolates "core" syntax. The paper found nearly identical results on both versions, suggesting findings are robust.
  - Word order sensitivity: The paper preserves word order (`fixed=yes`), treating pre- vs. post-modification as distinct structures; this increases type counts but captures typologically relevant variation.
- **Failure signatures**:
  - Extremely high %DIFF values with low absolute frequencies indicate likely corpus artifacts or noise rather than meaningful patterns.
  - Very low overlap between corpora may reflect preprocessing inconsistencies (e.g., different tokenization or annotation practices) rather than genuine structural differences.
  - If STTR confidence intervals overlap substantially, observed differences may not be statistically meaningful.
- **First 3 experiments**:
  1. Reproduce paper results: Download the English GUM and Slovenian SSJ/SST treebanks from UD v2.15, run STARK with the paper's configuration, and verify STTR and overlap statistics match reported values.
  2. Add a third language: Apply the same pipeline to a language with different typology (e.g., Japanese or Arabic from UD) to test cross-linguistic robustness of the speech-writing contrast.
  3. Vary extraction parameters: Run STARK with `fixed=no` (ignoring word order) to test how much of the speech-writing difference is driven by word-order flexibility vs. structural configuration alone.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the observed cross-modal syntactic differences generalize to a wider range of typologically diverse languages beyond English and Slovenian?
  - Basis in paper: [explicit] The Discussion states this study is "only an initial exploration" and notes that "over 300 treebanks now available in the Universal Dependencies framework, covering more than 180 languages" could be analyzed using this method.
  - Why unresolved: Only two languages were studied; generalizability to other language families and typological profiles remains untested.
  - What evidence would resolve it: Applying the same extraction and comparison methodology to spoken/written UD treebanks across multiple language families (e.g., agglutinative, polysynthetic, tone languages).

- **Open Question 2**: How would integrating morphosyntactic features (e.g., case, tense, aspect) into the structural representations alter the inventory comparisons and modality-specific patterns?
  - Basis in paper: [explicit] The Discussion mentions configurability opens the door to "integrating morphosyntactic features for finer distinctions" as a potential extension.
  - Why unresolved: Current analysis uses only delexicalized UPOS tags; morphological information encoded in UD annotations was not leveraged.
  - What evidence would resolve it: Re-extracting structures with morphosyntactic feature attributes included and comparing resulting inventories and keyness profiles.

- **Open Question 3**: To what extent are the observed differences attributable to modality (speech vs. writing) versus genre composition differences within each corpus?
  - Basis in paper: [inferred] The paper aggregates multiple genres per modality (e.g., interviews, conversations, speeches vs. news, academic, fiction) without controlling for genre effects.
  - Why unresolved: Modality and genre are confounded; written corpora contain more edited, formal texts while spoken corpora capture informal, interactive settings.
  - What evidence would resolve it: Controlling for comparable communicative purposes (e.g., comparing academic lectures to academic writing, casual conversation to casual correspondence).

## Limitations

- The primary methodological assumption that delexicalized dependency trees adequately capture syntactically meaningful variation across modalities systematically excludes lexico-grammatical patterns like formulaic expressions.
- The %DIFF metric's extreme sensitivity to absent structures may over-emphasize rare constructions as modality-specific when they might be corpus artifacts.
- The approach assumes UD annotation quality is sufficient for cross-linguistic comparison, though annotation guidelines and consistency can vary between treebanks.

## Confidence

- **High confidence**: The finding that spoken corpora contain fewer and less diverse syntactic structures than written ones is robust, supported by significant STTR differences (p < 0.001) and consistent across languages.
- **Medium confidence**: The claim that minimal structural overlap reflects genuine modality-specific grammar is plausible but could also reflect transcription practices or corpus composition differences.
- **Medium confidence**: The interpretation of speech-specific structures (short elliptical clauses, formulaic expressions) aligns with prior spoken grammar literature but requires external validation beyond the UD corpora examined.

## Next Checks

1. **External validation**: Apply the same pipeline to spoken-written pairs in a third, typologically distinct language (e.g., Japanese or Arabic from UD) to test whether the speech-writing contrast replicates beyond English and Slovenian.
2. **Parameter sensitivity**: Run STARK with `fixed=no` (ignoring word order) to determine whether the observed differences stem primarily from word-order flexibility or deeper structural distinctions.
3. **Lexicalized comparison**: Re-run the analysis with lexicalized trees to quantify what proportion of modality-specific patterns are captured by lexical vs. structural features alone.