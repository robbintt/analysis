---
ver: rpa2
title: 'Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated
  Reinforcement Learning'
arxiv_id: '2601.13642'
source_url: https://arxiv.org/abs/2601.13642
tags:
- learning
- q-learning
- sample
- federated
- average-reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies Q-learning algorithms for average-reward reinforcement\
  \ learning (RL) in both single-agent and federated settings. The key contribution\
  \ is establishing sample complexity guarantees that improve upon existing Q-learning\
  \ analyses by a factor of eO(\u2225h\u22C6\u2225\xB2/\u03B5\xB2) in the single-agent\
  \ case, achieving eO(SA\u2225h\u22C6\u2225\xB3/\u03B5\xB3), and demonstrating linear\
  \ speedup in the federated setting where M agents achieve eO(SA\u2225h\u22C6\u2225\
  \xB3/M\u03B5\xB3) per-agent sample complexity."
---

# Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.13642
- **Source URL:** https://arxiv.org/abs/2601.13642
- **Reference count:** 40
- **Primary result:** Achieves eO(SA\|h\⋆\|³/ε³) sample complexity for average-reward Q-learning, improving upon prior work by a factor of eO(\|h\⋆\|²/ε²), with linear speedup in federated setting

## Executive Summary
This paper establishes sample complexity bounds for average-reward Q-learning in both single-agent and federated settings. The key innovation is a stage-wise algorithm that dynamically adjusts discount factors and learning rates to balance bias and variance, achieving improved sample complexity of eO(SA\|h\⋆\|³/ε³) for weakly communicating MDPs. In the federated setting, the algorithm demonstrates linear speedup with only eO(\|h\⋆\|_{sp}/ε) communication rounds, independent of the number of agents.

## Method Summary
The method employs epoch-based Q-learning with dynamic scheduling of discount factors γ_k and learning rates η_{k,t}. For single-agent learning, epochs grow exponentially (N_k = c_N 2^k) while γ_k approaches 1 polynomially. The algorithm maintains two error components - horizon mismatch and optimality gap - which are balanced through careful parameter tuning. In the federated setting, M agents perform local updates and synchronize via averaging at scheduled intervals, achieving per-agent sample complexity of eO(SA\|h\⋆\|³/Mε³).

## Key Results
- Achieves eO(SA\|h\⋆\|³/ε³) sample complexity for single-agent average-reward Q-learning
- Demonstrates linear speedup in federated setting: per-agent complexity of eO(SA\|h\⋆\|³/Mε³)
- Requires only eO(\|h\⋆\|_{sp}/ε) communication rounds, independent of agent count
- Improves upon prior Q-learning analyses by eO(\|h\⋆\|²/ε²) factor
- Applicable to weakly communicating MDPs without mixing time assumptions

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Discount Factor Scheduling
- **Claim:** Balancing horizon mismatch and statistical error through principled scheduling of discount factors and learning rates enables convergence in average-reward settings
- **Mechanism:** Stage-wise structure with dynamically adjusted γ_k ensures both bias (distance between discounted optimum and average optimum) and variance (estimation error) decay synchronously
- **Core assumption:** Weakly communicating MDPs with finite span norm \|h\⋆\|_{sp}
- **Evidence anchors:** Abstract mentions careful design of learning rates and discount factors; Section B.1 provides explicit error decomposition
- **Break condition:** Improper scheduling causes either horizon bias or statistical variance to dominate

### Mechanism 2: Federated Linear Speedup
- **Claim:** M agents achieve linear speedup with communication rounds independent of M
- **Mechanism:** Variance reduction through averaging Q-tables, with communication schedule preventing staleness accumulation
- **Core assumption:** Synchronous generative model access across agents
- **Evidence anchors:** Abstract highlights O(\|h\⋆\|_{sp}/ε) communication rounds; Algorithm 2 details aggregation process
- **Break condition:** Excessive communication intervals cause bias accumulation from stale V-values

### Mechanism 3: Span-Norm Complexity
- **Claim:** Sample complexity scales with bias function span rather than mixing time
- **Mechanism:** Leverages Bellman equation for bias function to bound errors relative to h\⋆ instead of t_{mix}
- **Core assumption:** Weakly communicating MDPs ensuring state-independent optimal average reward
- **Evidence anchors:** Abstract emphasizes improvement by \|h\⋆\|_{sp}²/ε² factor; Section 2.1 defines weakly communicating MDPs
- **Break condition:** Multichain MDPs break scalar approximation target

## Foundational Learning

- **Concept: Bias Function (h\⋆) and Span Norm**
  - **Why needed here:** Replaces discount factor for measuring long-term value in average-reward RL
  - **Quick check:** Compute \|h\⋆\|_{sp} for simple MDPs like RiverSwim to understand scale

- **Concept: Weakly Communicating MDPs**
  - **Why needed here:** Allows analysis without uniform mixing assumptions, essential for realistic RL settings
  - **Quick check:** Verify MDP satisfies weakly communicating property (finite span, connected communicating classes)

- **Concept: Generative Model vs Markovian Trajectory**
  - **Why needed here:** Enables independent sampling for variance reduction, simplifying analysis
  - **Quick check:** Distinguish between simulator access (generative) and online interaction (Markovian)

## Architecture Onboarding

- **Component Map:** Q-table initialization → Epoch-based updates with γ_k, η_{k,t} scheduling → Error decomposition (bias + variance) → Convergence verification
- **Critical Path:** Parameter scheduling (γ_k, η_{k,t}) → Epoch execution → Error monitoring → Federated synchronization (if applicable)
- **Design Tradeoffs:** Balance between convergence speed and communication cost in federated setting; choice between polynomial vs exponential parameter schedules
- **Failure Signatures:** Unbounded Q-value growth indicates improper parameter scaling; lack of linear speedup suggests synchronization issues
- **First Experiments:** 1) Implement RiverSwim with generative model to verify theoretical bounds 2) Test parameter sensitivity with varying c_N 3) Evaluate federated performance with heterogeneous agent distributions

## Open Questions the Paper Calls Out

### Open Question 1: Optimal Rate Achievability
Can vanilla Q-learning achieve the minimax optimal sample complexity rate of O(1/ε²) for average-reward MDPs without variance reduction techniques? The current O(1/ε³) rate falls short of optimal, presenting a theoretical challenge for closing this gap.

### Open Question 2: Asynchronous Setting Extension
Can sample complexity guarantees be extended to asynchronous settings with Markovian trajectories rather than independent generative samples? Current analysis relies on sample independence that breaks in temporally correlated settings.

### Open Question 3: Variance Reduction Integration
How does incorporating variance reduction techniques affect communication complexity in federated average-reward settings? While variance reduction improves sample efficiency, it may require different communication schedules affecting the current O(\|h\⋆\|_{sp}/ε) bound.

## Limitations
- Analysis restricted to weakly communicating MDPs with finite span norm
- Federated analysis assumes synchronous generative models, stronger than typical federated learning assumptions
- Specific parameter schedules require careful tuning without empirical validation
- Generative model access assumption limits direct applicability to online RL settings

## Confidence

- **High Confidence:** Core theoretical framework and error decomposition methodology
- **Medium Confidence:** Linear speedup claim under synchronous generative model assumption
- **Low Confidence:** Specific numerical improvement factors dependent on parameter choices

## Next Checks

1. **Empirical Validation:** Implement RiverSwim environment and verify O(SA\|h\⋆|³/ε³) sample complexity scaling
2. **Parameter Sensitivity:** Systematically vary c_N and communication intervals g_k to identify robust parameter ranges
3. **Generalization Test:** Evaluate federated algorithm performance with heterogeneous agent data distributions