---
ver: rpa2
title: 'When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation
  Biology'
arxiv_id: '2511.13825'
source_url: https://arxiv.org/abs/2511.13825
tags:
- random
- gene
- kosmos
- 'null'
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "KOSMOS, an autonomous AI scientist, was evaluated on three radiobiology\
  \ hypotheses using empirical null models based on random gene sets. In Hypothesis\
  \ 1, DDR capacity did not predict p53 response (Spearman \u03C1 = -0.40, p = 0.756),\
  \ performing no better than random five-gene sets."
---

# When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology

## Quick Facts
- arXiv ID: 2511.13825
- Source URL: https://arxiv.org/abs/2511.13825
- Reference count: 0
- Primary result: AI-generated hypotheses require rigorous statistical validation against empirical null models before acceptance

## Executive Summary
KOSMOS, an autonomous AI scientist, was evaluated on three radiobiology hypotheses using empirical null models based on random gene sets. The analysis produced one clear discovery (CDO1), one ambiguous result (prostate signature), and one false hypothesis (DDR-p53), demonstrating that AI-generated hypotheses require rigorous statistical validation against appropriate null models before acceptance. Without proper auditing, LLM fluency can mask statistical weakness, leading to acceptance of biologically plausible but statistically unsupported claims.

## Method Summary
The study evaluated three AI-generated hypotheses using empirical null models. Hypothesis 1 tested DDR capacity's prediction of p53 response using 4 cell lines, computing Spearman correlation and comparing to 10,000 random 5-gene sets. Hypothesis 2 assessed CDO1 and OGT expression's prediction of radiation-induced module strength using 16 breast cancer cell lines, computing Pearson correlations against 10,000 random genes. Hypothesis 3 evaluated a 12-gene prostate cancer signature's prognostic power using 248 patients, comparing c-index and hazard ratio uniqueness against 5,000 random signatures.

## Key Results
- DDR capacity did not predict p53 response (Spearman ρ = -0.40, p = 0.756), performing no better than random five-gene sets
- CDO1 expression strongly predicted radiation-induced module strength (r = 0.70, empirical p = 0.0039)
- 12-gene signature showed modest prognostic power (c-index = 0.61, p = 0.017) but lacked unique hazard ratio (p = 0.37)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-generated hypotheses require empirical null models for validation before acceptance
- Mechanism: Apply Popper's falsification principle to AI outputs by treating them as preliminary hypotheses. For each claim, construct domain-appropriate null distributions (e.g., random gene sets), compute empirical p-values, and classify as supported, refuted, or inconclusive based on position relative to null
- Core assumption: Random gene sets or signatures provide a meaningful baseline for "chance-level" performance in the specific domain
- Evidence anchors:
  - [abstract]: "AI-generated hypotheses require rigorous statistical validation against appropriate null models before acceptance"
  - [section]: "We propose a falsification-based auditing methodology based on Popper's principle of falsification. Each AI claim must be testable and falsifiable."
  - [corpus]: "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems" notes that internal workflows of AI scientist systems "have not been closely examined" and lack of scrutiny permits errors
- Break condition: If null model assumptions are violated (e.g., genes within pathways are not independent), empirical p-values may be miscalibrated

### Mechanism 2
- Claim: LLM fluency can mask underlying statistical weakness; quantitative auditing separates valid insights from hallucinations
- Mechanism: KOSMOS generates articulate mechanistic explanations (e.g., DDR-p53 link using known biology). Auditing ignores narrative coherence and tests raw statistical claims against null distributions. The DDR-p53 hypothesis appeared biologically plausible but showed ρ = -0.40, p = 0.756—indistinguishable from random
- Core assumption: Scientific validity is independent of narrative quality; only quantitative evidence matters
- Evidence anchors:
  - [abstract]: "Without rigorous evaluation, fluency can seem like thoughtful reasoning"
  - [section]: "KOSMOS likely rationalized the DDR–p53 link using known biology... and assembled a convincing mechanistic story. Our auditing approach treats AI outputs as hypotheses to falsify."
- Break condition: If the statistical test itself is inappropriate or underpowered (n=4 cell lines in Hypothesis 1), even correct auditing may yield inconclusive results

### Mechanism 3
- Claim: Prognostic signatures require validation on both discrimination and effect-size uniqueness
- Mechanism: For the 12-gene signature, compute both c-index (discrimination) and |logHR| (effect size). Compare each against null distributions from random signatures. A valid discovery should be unusual in both metrics. The signature achieved c-index = 0.613 (p = 0.017) but |logHR| = 0.899 (p = 0.37)—modest discrimination but non-unique effect size
- Core assumption: A genuinely novel prognostic signature should be an outlier in both discrimination and effect magnitude, not just one
- Evidence anchors:
  - [abstract]: "12-gene signature showed modest prognostic power (c-index = 0.61, p = 0.017) but lacked a unique hazard ratio (p = 0.37)"
  - [section]: "The proposed gene combination did not produce an unusually large separation in survival risk between patient groups... many random gene sets with enough proliferative genes would be expected to similarly predict outcome"
- Break condition: If the null distribution doesn't preserve relevant data structure (expression correlation, pathway membership), uniqueness claims may be misleading

## Foundational Learning

- Concept: Empirical p-values via permutation/randomization
  - Why needed here: All three hypotheses use empirical p-values computed against simulated null distributions rather than parametric tests
  - Quick check question: If you run 10,000 random gene sets and 39 produce |r| ≥ 0.70, what is the empirical p-value?

- Concept: Concordance index (c-index) in survival analysis
  - Why needed here: Hypothesis 3 evaluates prognostic signatures using c-index as the discrimination metric
  - Quick check question: A c-index of 0.5 indicates what level of predictive power?

- Concept: Gene exchangeability assumption in null models
  - Why needed here: The paper's null models assume genes are exchangeable, which the authors acknowledge is "an oversimplification since genes within the same pathway are not independent"
  - Quick check question: Why might a null model that randomly samples genes overestimate the "uniqueness" of a pathway-specific signature?

## Architecture Onboarding

- Component map: Input -> Statistical test module -> Null distribution generator -> Empirical p-value calculator -> Classifier
- Critical path: Proper null distribution construction is the bottleneck. Must match gene set size, use same expression matrix, and preserve data structure where possible
- Design tradeoffs:
  - Simple nulls (random genes) vs. structured nulls (preserve expression distribution, network degree)
  - Single metric validation vs. multi-metric (c-index + |logHR|)
  - Retrospective validation (same dataset) vs. independent validation (new dataset)
- Failure signatures:
  - High fluency + weak statistics: DDR-p53 hypothesis (ρ = -0.40, p = 0.756)
  - Modest discrimination + non-unique effect: 12-gene signature (likely capturing proliferation signal)
  - Small sample size + high variance: n=4 cell lines limits statistical power
- First 3 experiments:
  1. Replicate the CDO1 validation on GSE59732: extract baseline CDO1 expression, compute Cluster 3 induction scores, correlate, and generate null distribution from 10,000 random genes. Verify r ≈ 0.70, empirical p ≈ 0.0039
  2. Test the DDR-p53 hypothesis with expanded gene sets: Use larger DDR gene sets (10, 20 genes) to assess whether the null result persists or was driven by arbitrary gene selection
  3. Implement structured null models: Create null distributions that preserve gene co-expression structure (sample genes from matching expression quantiles) and compare p-values to simple random sampling

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample sizes (n=4 cell lines for Hypothesis 1) limit statistical power and increase uncertainty in effect estimates
- Null model assumption of gene exchangeability is acknowledged as an oversimplification that may affect p-value calibration
- Specific gene lists used for hypotheses are not provided in the paper text, requiring retrieval from external repositories

## Confidence
- High: CDO1 expression strongly predicts radiation-induced module strength (r = 0.70, empirical p = 0.0039)
- Medium: 12-gene signature shows modest prognostic power (c-index = 0.61, p = 0.017) but lacks unique hazard ratio (p = 0.37)
- High: DDR capacity does not predict p53 response (Spearman ρ = -0.40, p = 0.756), indistinguishable from random

## Next Checks
1. Replicate CDO1 validation on GSE59732: extract baseline CDO1 expression, compute Cluster 3 induction scores, correlate, and generate null distribution from 10,000 random genes to verify r ≈ 0.70, empirical p ≈ 0.0039
2. Test DDR-p53 hypothesis with expanded gene sets: use larger DDR gene sets (10, 20 genes) to assess whether null result persists or was driven by arbitrary gene selection
3. Implement structured null models: create null distributions that preserve gene co-expression structure (sample genes from matching expression quantiles) and compare p-values to simple random sampling