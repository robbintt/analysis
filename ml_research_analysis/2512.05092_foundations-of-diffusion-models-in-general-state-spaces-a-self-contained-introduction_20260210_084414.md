---
ver: rpa2
title: 'Foundations of Diffusion Models in General State Spaces: A Self-Contained
  Introduction'
arxiv_id: '2512.05092'
source_url: https://arxiv.org/abs/2512.05092
tags:
- diffusion
- process
- reverse
- discrete
- forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified theoretical framework for diffusion
  models across continuous and discrete state spaces, bridging the gap between stochastic
  differential equations (SDEs) in continuous domains and continuous-time Markov chains
  (CTMCs) in discrete domains. The authors develop a self-contained introduction that
  unifies these settings through the infinitesimal generator formalism, showing how
  both SDEs and CTMCs emerge as special cases of general Markov processes.
---

# Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction

## Quick Facts
- **arXiv ID**: 2512.05092
- **Source URL**: https://arxiv.org/abs/2512.05092
- **Reference count**: 40
- **Primary result**: Unified theoretical framework showing SDEs and CTMCs as special cases of general Markov processes via infinitesimal generators, with derived ELBO yielding DSM for continuous and DSE for discrete spaces

## Executive Summary
This paper presents a comprehensive theoretical foundation for diffusion models that unifies continuous and discrete state spaces under a single framework. The authors show how both stochastic differential equations (SDEs) in continuous domains and continuous-time Markov chains (CTMCs) in discrete domains emerge as special cases of general Markov processes characterized by infinitesimal generators. This unification enables a single theoretical treatment that bridges denoising score matching for continuous data with denoising score entropy for discrete data, providing a complete roadmap for understanding and implementing diffusion models across different data types.

## Method Summary
The paper develops a three-step recipe for implementing diffusion models: (1) define a fixed forward Markov process (SDE or CTMC) with tractable marginals, (2) parameterize the reverse process via a neural network predicting scores, clean data, or reverse rates, and (3) optimize an ELBO-derived loss. The forward process can be continuous (Gaussian SDE) or discrete (CTMC with absorbing or uniform transitions), with noise schedules controlling the noising process. The reverse process is parameterized to match the forward process family, enabling closed-form sampling when marginals are tractable. For discrete spaces, the paper introduces a stochasticity parameter γ that controls the balance between reversibility and efficiency in the reverse process.

## Key Results
- Both continuous diffusion (SDEs) and discrete diffusion (CTMCs) emerge as special cases of general Markov processes characterized by infinitesimal generators
- The evidence lower bound (ELBO) derived from the generator perspective specializes to denoising score matching for continuous spaces and denoising score entropy for discrete spaces
- Multiple parameterizations (score, x0-prediction, ϵ-prediction for continuous; posterior, concrete score, direct rate for discrete) are unified through linear relationships
- Latent diffusion enables application of continuous techniques to discrete data through encoder-decoder architectures
- The stochasticity parameter γ in discrete reverse processes controls the trade-off between reversibility and sampling efficiency

## Why This Works (Mechanism)

### Mechanism 1: Infinitesimal Generator Unification
- Claim: Both continuous diffusion (SDEs) and discrete diffusion (CTMCs) emerge as special cases of a general Markov process characterized by an infinitesimal generator.
- Mechanism: The generator Lt acts on test functions to capture instantaneous local dynamics. Its adjoint (Lt)* governs marginal evolution via the Kolmogorov forward equation. SDEs yield generators with drift and Laplacian terms; CTMCs yield generators expressible through rate matrices. This operator-theoretic view subsumes both under one formalism.
- Core assumption: The process is Markov, and the generator domain includes relevant test functions (smooth with compact support for continuous; bounded for discrete).
- Evidence anchors:
  - [abstract]: "unifying continuous domains and discrete/categorical structures under one lens... showing how both SDEs and CTMCs emerge as special cases of general Markov processes"
  - [Section 7]: "The generator formalism in Section 7 offers the most general framework, subsuming both SDEs and CTMCs as special cases"
  - [corpus]: Weak direct evidence; neighbors focus on convergence or specific model variants rather than the generator unification itself.

### Mechanism 2: Time Reversal via Marginal Tilt
- Claim: The reverse-time generative process has a generator that depends on the forward generator and the marginal density ratios, enabling closed-form reverse dynamics when marginals are tractable.
- Mechanism: Given forward generator Lt and marginal qt, the reverse generator is ˆLsϕ(x) = (1/q1−s(x))[(L1−s)*(q1−sϕ)(x) − ϕ(x)(L1−s)*q1−s(x)]. In continuous spaces, this adds a score-dependent drift correction; in discrete spaces, it yields reverse rates tilted by qt(y)/qt(x). This structure is what makes learning tractable.
- Core assumption: Marginals qt are accessible (analytically or via tractable conditionals q(xt|x0)); the density ratio is well-defined and finite.
- Evidence anchors:
  - [abstract]: "derive the evidence lower bound (ELBO) directly from the generator perspective"
  - [Section 5.2]: Reverse SDE and reverse CTMC formulas derived via time reversal of Kolmogorov equations
  - [corpus]: Neighbor "Almost Linear Convergence under Minimal Score Assumptions" addresses score-based dynamics but not the general reversal mechanism.

### Mechanism 3: ELBO Decomposition into Local Matching Terms
- Claim: The negative log-likelihood is bounded by a path-space KL that decomposes into per-timestep matching terms, which specialize to denoising score matching (continuous) or denoising score entropy (discrete).
- Mechanism: The ELBO arises from KL(q(x1:T|x0) || pθ(x1:T)). In the continuous-time limit, this becomes an integral of generator-based discrepancies. For continuous spaces, this yields squared score error weighted by g²t. For discrete spaces, it yields a sum over transitions of rate-matched terms involving learned reverse rates and posterior ratios.
- Core assumption: Forward process has tractable q(xt|x0) and q(xt−1|xt, x0); the KL decomposition holds under sufficient regularity.
- Evidence anchors:
  - [Section 6]: Full ELBO derivation with discrete-time decomposition and continuous-time limit
  - [Section 7.3]: "ELBO via the generator... specialises to the denoising score matching objective for continuous spaces and the denoising score entropy objective for discrete spaces"
  - [corpus]: "Non-Asymptotic Convergence of Discrete Diffusion Models" discusses discrete diffusion but not the unified ELBO structure.

## Foundational Learning

- Concept: Variational inference and the ELBO
  - Why needed here: The training objective for diffusion models is derived from maximizing a variational lower bound on log-likelihood; understanding Jensen's inequality, KL divergence, and the role of the variational posterior is essential.
  - Quick check question: Can you explain why adding a variational distribution q(z|x) yields a tractable bound on log p(x)?

- Concept: Markov processes and their generators
  - Why needed here: The unified framework relies on the infinitesimal generator to describe local dynamics; familiarity with how Lt characterizes a process and how (Lt)* governs marginal evolution is prerequisite.
  - Quick check question: For a CTMC with rate matrix R, what is the generator Lt and how does it relate to the master equation?

- Concept: Time reversal of stochastic processes
  - Why needed here: Generative modeling requires running the noising process backward; understanding Anderson (1982) style reversals and the role of marginals/scores in the reverse drift or rate is central.
  - Quick check question: In the reverse SDE, what term corrects the drift and why does it involve the score ∇log qt?

## Architecture Onboarding

- Component map:
  - Forward noising process -> Fixed Markov process (SDE for continuous, CTMC for discrete) with tractable marginals q(xt|x0)
  - Reverse generative process -> Parameterized by neural network predicting either scores (continuous) or reverse rates/posteriors (discrete)
  - Training objective -> ELBO-derived loss; DSM for continuous (score matching) or DSE for discrete (rate/posterior matching)
  - Noise schedule -> Controls signal/noise balance; must satisfy α0=1, σ0=0 and αT≈0, σT≈1 for continuous
  - Latent diffusion (optional) -> Encoder-decoder compresses data to latent space where diffusion operates

- Critical path:
  1. Define state space (continuous R^d or discrete {1,...,K}^d) and forward process type (Gaussian SDE or categorical CTMC)
  2. Choose noise schedule and verify tractable marginals q(xt|x0)
  3. Derive or look up conditional reverse kernel q(xt−1|xt, x0) in closed form
  4. Select parameterization (score, x0-prediction, ϵ-prediction for continuous; posterior, concrete score, or direct rate for discrete)
  5. Implement training loop sampling xt|x0 and minimizing the ELBO-derived loss
  6. For sampling, integrate reverse SDE or simulate reverse CTMC using learned approximations

- Design tradeoffs:
  - Discrete vs. continuous diffusion: Discrete preserves categorical structure but has more complex rate matrix design; continuous enables gradient-based guidance but requires embedding discrete data
  - Parameterization choice: x0-prediction is stable at low noise; ϵ-prediction is standard in DDPM; v-prediction balances across noise levels (Karras et al., 2022)
  - Stochasticity level (γ in discrete reverse): γ=0 minimizes jumps; higher γ adds reversible transitions, increasing sample diversity but potentially efficiency
  - Latent vs. data-space diffusion: Latent reduces dimensionality and improves perceptual quality but requires training an autoencoder first

- Failure signatures:
  - Mode collapse or blurry samples: Often indicates insufficient noise schedule (αT not near 0) or poor score/rate approximation
  - Training instability with discrete diffusion: May stem from non-column-stochastic Q matrices or rate matrices not satisfying divergence-free property (1^T R = 0^T)
  - Intractable reverse rates in discrete setting: Occurs if forward process does not have closed-form q(xt|x0) or if posterior parameterization is misspecified
  - Likelihood not improving: Check that the ELBO loss is correctly implemented and that reconstruction term (L0) is not dominating inappropriately

- First 3 experiments:
  1. Implement Gaussian diffusion on 2D toy data (e.g., swiss roll) with a simple MLP denoiser; verify that forward marginals match q(xt|x0) and reverse samples recover the data distribution
  2. Implement discrete absorbing diffusion on a small categorical dataset (e.g., binarized MNIST or short text sequences); confirm that the ELBO reduces to a masked language modeling objective as claimed in Eq. (140)
  3. Compare parameterizations (x0 vs. ϵ vs. v) on the same continuous diffusion task; measure sample quality (FID or visual inspection) and training stability to validate claims about noise-level tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to couple continuous and discrete diffusion for discrete data generation (e.g., text), and what theoretical guarantees can be established for hybrid approaches like CADD or CCDD?
- Basis in paper: [explicit] Section 8.3 states "A growing line of work seeks to explicitly couple continuous and discrete diffusion for discrete data... Together, these hybrids suggest a promising path to better likelihood training, guidance, and sampling efficiency in discrete generative modeling."
- Why unresolved: The paper describes recent methods (Duo, CADD, CCDD) as promising but does not provide theoretical analysis of when/why such couplings improve over pure discrete or continuous approaches.
- What evidence would resolve it: Comparative empirical benchmarks across discrete domains plus theoretical analysis establishing conditions under which coupled methods outperform uncoupled baselines.

### Open Question 2
- Question: How should the projection function for discrete-to-continuous latent diffusion be designed to optimally balance approximation fidelity to the discrete space against representational expressivity?
- Basis in paper: [explicit] Section 8.2 states "The difficulty lies in balancing the trade-off between maintaining a continuous representation that closely approximates the discrete data space and employing a more complex, expressive representation that may deviate further away from the original discrete space."
- Why unresolved: The paper surveys approaches (analog bits, simplex diffusion, pretrained embeddings) but provides no principled framework for choosing among them or measuring the trade-off.
- What evidence would resolve it: Systematic study comparing projection methods with metrics for both discrete-space fidelity and generative quality, potentially yielding design principles.

### Open Question 3
- Question: What is the theoretically optimal stochasticity level (γ parameter) for discrete diffusion reverse processes, and can task-optimal values be determined analytically?
- Basis in paper: [explicit] Section 5.2 notes that the reverse rate matrix is not unique due to detailed balance, introducing a free parameter γ≥0, and states "While an optimal stochasticity level may be task-dependent, γ = 0 is often preferred in practice."
- Why unresolved: The paper presents the mathematical freedom but offers no theoretical guidance on selecting γ or understanding its impact on sample quality and sampling efficiency.
- What evidence would resolve it: Theoretical analysis linking γ to convergence properties, sample diversity, and likelihood, complemented by empirical studies across tasks.

## Limitations

- The paper does not provide explicit experimental validation across both continuous and discrete settings, leaving practical performance to be demonstrated by future work
- Neural network architectures, optimization hyperparameters, and specific noise schedules are left to the practitioner, which could significantly impact practical performance
- The framework assumes tractable forward marginals and conditional distributions, which may not hold for all forward process designs

## Confidence

- **High**: The generator unification theory and ELBO derivations (Sections 7 and 6)
- **Medium**: Parameterization connections (Section 5) and practical implementation guidance
- **Low**: Direct empirical validation across both discrete and continuous settings

## Next Checks

1. Implement discrete absorbing diffusion on a simple text task and verify the ELBO reduces to the MLM objective as claimed
2. Compare multiple parameterizations (x0, ϵ, v) on a continuous diffusion task to validate the noise-level tradeoffs
3. Test reverse sampling stability with different γ values in discrete diffusion to quantify the diversity-efficiency tradeoff