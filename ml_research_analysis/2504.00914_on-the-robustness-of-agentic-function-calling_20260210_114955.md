---
ver: rpa2
title: On the Robustness of Agentic Function Calling
arxiv_id: '2504.00914'
source_url: https://arxiv.org/abs/2504.00914
tags:
- function
- arxiv
- toolkit
- tool
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating the robustness
  of function calling (FC) in large language model (LLM) agents. While prior research
  focused on improving FC accuracy, this work examines how agents handle input perturbations
  and toolkit expansions.
---

# On the Robustness of Agentic Function Calling

## Quick Facts
- arXiv ID: 2504.00914
- Source URL: https://arxiv.org/abs/2504.00914
- Authors: Ella Rabinovich; Ateret Anaby-Tavor
- Reference count: 9
- Key outcome: Existing function calling evaluation methodologies show critical brittleness when handling input perturbations and toolkit expansions, with most failures stemming from exact-match parameter validation rather than true model weaknesses.

## Executive Summary
This paper introduces a benchmark for evaluating the robustness of function calling (FC) in large language model (LLM) agents, focusing on how agents handle input perturbations and toolkit expansions. While prior research concentrated on improving FC accuracy, this work reveals critical weaknesses in existing evaluation methodologies, particularly in handling parameter value variations. The authors generate meaning-preserving paraphrases of user requests and expand toolkits with semantically related tools, testing several top-performing FC models. The results demonstrate significant performance degradation under both interventions, with most paraphrase failures attributed to parameter value mismatches in exact-match AST evaluation.

## Method Summary
The benchmark generates meaning-preserving paraphrases of user requests using Llama3.1-70B while preserving parameter values, and expands toolkits by creating semantically related tools via CodeLlama-13B with duplicate filtering using cosine similarity (0.8 threshold). Testing several top-performing FC models on this carefully constructed dataset reveals critical weaknesses in existing evaluation methodologies. The approach focuses on AST-level evaluation (phase 1 of BFCL's two-phase method) rather than execution simulation, isolating construction-stage errors. The methodology includes manual review of 50 paraphrase examples to ensure semantic preservation and uses embedding-based filtering to maintain tool quality during expansion.

## Key Results
- Expanding toolkits with related functions causes performance degradation across all models, with failures spanning incorrect function selection (23–56%), wrong parameter assignments (23–71%), and wrong number of functions (up to 46% for Llama3.3-70B).
- 70–90% of paraphrase-related failures stem from parameter value mismatches in exact-match AST evaluation, revealing evaluation methodology brittleness rather than true model weaknesses.
- Performance drops significantly when semantically related tools are added, indicating models struggle to discriminate among proximate options in expanded toolkits.

## Why This Works (Mechanism)

### Mechanism 1: Query Paraphrase Robustness Testing
- Claim: Meaning-preserving rephrasings expose evaluation brittleness more than agent failure.
- Mechanism: The benchmark generates paraphrased queries via Llama3.1-70B while strictly preserving parameter values. When evaluated, failures cluster around parameter value mismatches (e.g., "Miami,FL" vs. predefined "Miami, Florida"), revealing that current AST-based exact-match evaluation cannot handle equivalent entity surface forms.
- Core assumption: The paraphrases preserve both semantics and slot values accurately; manual review of 50 examples found no semantic drift.
- Evidence anchors:
  - [abstract] "critical weaknesses in existing evaluation methodologies, particularly in handling parameter value variations"
  - [section 3.2] "70–90% of errors indeed stem from mis-match in parameter value assignment"
  - [corpus] No direct corpus support for this specific evaluation weakness; neighbors focus on multilingual/tool retrieval.
- Break condition: If paraphrases inadvertently alter slot-fillable values, observed drops conflate paraphrase quality with evaluation rigidity.

### Mechanism 2: Toolkit Expansion Stress Testing
- Claim: Adding semantically related tools degrades function selection and parameter assignment accuracy.
- Mechanism: The benchmark expands toolkits from ~2.7 to ~5.6 tools on average by generating related-but-distinct request variants and their corresponding tool definitions (via CodeLlama-13B), then filtering near-duplicates via embedding similarity. Models must now discriminate among more semantically proximate options, increasing confusion in function selection and slot filling.
- Core assumption: Generated tools are realistic, style-consistent, and not functionally equivalent to originals; cosine similarity threshold (0.8) adequately filters duplicates.
- Evidence anchors:
  - [abstract] "Expanding toolkits with related functions causes performance degradation across models"
  - [section 3.2] Error breakdown shows wrong function (23–56%), wrong parameter assignment (23–71%), and occasionally wrong number of functions (up to 46% for Llama3.3-70B).
  - [corpus] "Dynamic Tool Dependency Retrieval" (arXiv:2512.17052) supports the premise that retrieval/shortlisting impacts FC performance under larger toolsets.
- Break condition: If generated tools are unrealistically similar or poorly formatted, degradation may reflect data quality rather than genuine discrimination difficulty.

### Mechanism 3: Two-Phase Evaluation Separation
- Claim: Isolating AST-level evaluation from execution simulation reveals construction-stage errors before runtime.
- Mechanism: BFCL's approach separates (1) AST tree-matching for call correctness from (2) simulated execution. This paper focuses only on phase 1, attributing most paraphrase failures to evaluation methodology rather than model capability.
- Core assumption: AST matching is sufficient for robustness assessment; execution-phase robustness may differ.
- Evidence anchors:
  - [section 3.1] "our focus in this study is the evaluation of FC construction provided interventions in its input; we, therefore, adhere to the first evaluation phase – namely, AST"
  - [corpus] No direct corpus corroboration for this methodological choice.
- Break condition: If execution-phase robustness diverges significantly from AST-level robustness, conclusions may not generalize to deployed systems.

## Foundational Learning

- Concept: Function Calling (FC) / Tool Use
  - Why needed here: The entire benchmark evaluates FC robustness; without understanding that agents select tools and fill slots from JSON descriptions, the degradation signals are unintelligible.
  - Quick check question: Given a user query and a JSON tool schema, can you manually produce the correct function call with parameters?

- Concept: AST-based Evaluation
  - Why needed here: The paper attributes many failures to AST exact-match limitations; understanding tree-matching clarifies why "Miami,FL" ≠ "Miami, Florida" in evaluation.
  - Quick check question: Why might an AST match reject a semantically correct function call?

- Concept: Semantic Similarity for Filtering
  - Why needed here: Toolkit expansion uses embedding-based cosine similarity to avoid generating duplicate tools; this is critical to the benchmark's validity.
  - Quick check question: If two tools have cosine similarity 0.85, what might happen if you set the filter threshold at 0.8?

## Architecture Onboarding

- Component map:
  1. Query Paraphraser (Llama3.1-70B) → generates meaning-preserving rephrasings
  2. Request Variant Generator (Llama3.1-70B) → creates related-but-different requests
  3. Tool Definition Generator (CodeLlama-13B) → produces JSON tool schemas for variants
  4. Duplicate Filter (sentence-transformers embeddings + cosine threshold 0.8) → removes near-identical tools
  5. FC Agent Under Test → receives (query, toolkit) and outputs function call
  6. AST Evaluator → validates call structure and parameter exact match

- Critical path: Query → Paraphraser → (Original Toolkit OR Expanded Toolkit) → FC Agent → AST Evaluator → Compare to Baseline

- Design tradeoffs:
  - AST-only evaluation isolates construction errors but ignores execution robustness.
  - Exact-match parameter evaluation ensures determinism but fails on entity surface-form equivalence.
  - LLM-generated tools scale dataset creation but may introduce style inconsistencies or subtle duplicates.

- Failure signatures:
  - Parameter value mismatch under paraphrase (e.g., "Miami,FL" vs. predefined list).
  - Wrong function selected when semantically related tools are present.
  - Over-generation (two functions instead of one) under toolkit expansion.
  - Hallucinated parameters when context grows.

- First 3 experiments:
  1. Reproduce baseline vs. paraphrase drop on 50 examples: quantify how many failures are evaluation-artifact vs. true model errors by manually inspecting parameter value equivalence.
  2. Incrementally expand toolkit (1→3→5→7 related tools) on a fixed query set to measure degradation curve per model.
  3. Replace AST exact-match with semantic similarity (e.g., embeddings or LLM-as-judge) for parameter validation and compare error attribution.

## Open Questions the Paper Calls Out
None

## Limitations
- The AST-based exact-match evaluation methodology itself introduces brittleness that may be conflated with true model robustness failures, particularly for entity surface-form variations.
- Toolkit expansion depends on LLM-generated tool definitions that may introduce subtle style inconsistencies or near-duplicates despite filtering, potentially confounding degradation with data quality issues.
- The benchmark focuses exclusively on construction-phase robustness (AST matching) without evaluating execution-phase robustness, which may diverge significantly in real-world deployments.

## Confidence

- **High Confidence**: The observed performance degradation under toolkit expansion is well-supported by error breakdowns showing increased wrong function selection (23–56%) and wrong parameter assignment (23–71%).
- **Medium Confidence**: The attribution of paraphrase failures primarily to evaluation methodology (AST exact-match limitations) is plausible but requires broader manual validation across more examples to confirm.
- **Medium Confidence**: The toolkit expansion mechanism produces realistic stress scenarios, though the quality of generated tools remains a potential confounder.

## Next Checks
1. Expand manual validation of paraphrase examples from 50 to 200, specifically checking parameter value preservation and semantic equivalence, to quantify the true rate of evaluation-artifact failures.
2. Implement semantic similarity-based parameter validation (using embeddings or LLM-as-judge) alongside AST exact-match to compare error attribution and determine if evaluation methodology drives most failures.
3. Conduct incremental toolkit expansion experiments (1→3→5→7 tools) on fixed query sets to establish degradation curves and identify threshold points where models consistently fail.