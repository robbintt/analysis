---
ver: rpa2
title: 'R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language
  Models via Share-GRPO'
arxiv_id: '2505.16673'
source_url: https://arxiv.org/abs/2505.16673
tags:
- reasoning
- arxiv
- question
- share-grpo
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Share-GRPO, a novel reinforcement learning
  method for multimodal large language models (MLLMs) that addresses sparse reward
  and advantage vanishing issues in reasoning tasks. The method expands the question
  space via semantically consistent transformations and shares diverse reasoning trajectories
  across question variants during training.
---

# R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO

## Quick Facts
- arXiv ID: 2505.16673
- Source URL: https://arxiv.org/abs/2505.16673
- Reference count: 40
- Achieves state-of-the-art performance on six reasoning benchmarks without requiring supervised fine-tuning

## Executive Summary
R1-ShareVL introduces Share-GRPO, a reinforcement learning method that significantly enhances the reasoning capabilities of multimodal large language models (MLLMs). The approach addresses key challenges in RL training for reasoning tasks: sparse reward signals and advantage vanishing. By expanding the question space through semantically consistent transformations and sharing reasoning trajectories across question variants, Share-GRPO enables more effective exploration of diverse reasoning paths. The method employs hierarchical advantage estimation that combines global and local normalization levels for more accurate and stable policy optimization. Extensive experiments demonstrate substantial improvements across mathematical and general reasoning benchmarks, achieving state-of-the-art performance without the need for supervised fine-tuning.

## Method Summary
Share-GRPO enhances MLLM reasoning through three core innovations: (1) expanding the question space using semantically consistent transformations (SCT), where offline textual rewrites via GPT-4o and online visual transformations generate multiple semantically equivalent question variants; (2) sharing reasoning trajectories across these variants during policy optimization to maintain response diversity and mitigate advantage vanishing; and (3) employing hierarchical advantage estimation that combines global rewards (across all variants) with local rewards (within each variant) for more stable and informative gradient signals. The method trains on 52K multimodal samples from the MM-Eureka dataset using the EasyR1 codebase, achieving significant performance gains on six reasoning benchmarks without requiring supervised fine-tuning.

## Key Results
- Achieves state-of-the-art performance on six reasoning benchmarks (MathVista, MMStar, MMMU, MathVerse, MathVision, AI2D)
- Improves reward diversity and mitigates training instability through question space expansion and trajectory sharing
- Demonstrates +0.6% improvement from hierarchical advantage estimation (global + local) over global-only estimation
- Successfully enhances reasoning capabilities without requiring supervised fine-tuning, using only RL training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding the question space through semantically consistent transformations (SCT) may increase the probability of generating at least one successful reasoning trajectory, thereby densifying reward signals during RL training.
- Mechanism: The method generates multiple semantically equivalent question variants through (1) offline textual rewrites via GPT-4o that alter syntax while preserving meaning, and (2) online visual transformations (rotation, noise injection) with compensatory textual prompts. Each variant produces n candidate responses, expanding the total response pool from n to m×n for a given seed question. This structural expansion increases the chance that at least one response receives a positive reward.
- Core assumption: The transformations preserve solution semantics—i.e., correct reasoning on a variant implies correct reasoning on the original. This requires visual transformations to retain critical reasoning cues and textual rewrites to maintain logical equivalence.
- Evidence anchors:
  - [abstract] "Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space"
  - [section 3.2.1, p.5-6] "With the expanded question space Q = {Q1, Q2, ..., Qm}, Share-GRPO enables to explore diverse reasoning trajectories in an enlarged solution space for each given question."
  - [corpus] Vision-R1 (arXiv:2503.06749) addresses MLLM reasoning via RL but does not employ question-space expansion; corpus evidence for SCT specifically is absent.

### Mechanism 2
- Claim: Sharing reasoning trajectories across question variants may mitigate advantage vanishing by maintaining response diversity even when the policy becomes homogeneous on individual questions.
- Mechanism: Instead of computing advantages solely within responses from a single question (which collapse toward zero when all responses are correct or all incorrect), Share-GRPO aggregates responses from all m variants during policy optimization. Equation 8 computes πθ(oᵢQj | Qk) for j,k ∈ {1,...,m}, meaning responses from variant Qj are evaluated against all variant contexts Qk. This cross-variant sharing preserves diversity in the advantage pool.
- Core assumption: Responses generated from semantically equivalent variants remain meaningfully comparable for policy learning—that a correct trajectory from Q₁ provides a useful learning signal for optimizing responses to Q₂.
- Evidence anchors:
  - [abstract] "shares the discovered reasoning trajectories across the expanded questions during RL"
  - [section 3.2.3, p.6] "Share-GRPO enables to explore and share diverse reasoning trajectories and allows more accurate advantage estimation for each given question. Then, we optimize policy model πθ by sharing diverse reasoning trajectories O = {{o₁^Q1, ..., oₙ^Q1}, ..., {o₁^Qm, ...oₙ^Qm}} across question variants"
  - [corpus] No corpus papers validate trajectory sharing across question variants; this appears novel to Share-GRPO.

### Mechanism 3
- Claim: Hierarchical advantage estimation combining global and local levels may provide more stable and informative gradient signals than single-level estimation alone.
- Mechanism: Advantages are computed at two levels: (1) Global (Eq. 5) normalizes rewards against the full m×n response pool across all variants; (2) Local (Eq. 6) normalizes within each variant's n responses. The final advantage (Eq. 7) combines both when j=k (same-variant responses), enabling intra-variant discrimination while leveraging cross-variant diversity. When j≠k, only global advantage is used for cross-variant sharing.
- Core assumption: Both global and local normalization scales are informative—that combining them captures complementary signal rather than introducing optimization conflicts.
- Evidence anchors:
  - [abstract] "estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training"
  - [section 3.2.2, p.5-6] Equations 5-7 define the hierarchical estimation; Table 2 (p.8) shows +0.6% improvement from adding local to global advantage estimation.
  - [corpus] DeepSeek-R1 (arXiv:2501.12948) uses GRPO with group-relative advantages but does not employ hierarchical multi-level estimation.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Share-GRPO extends GRPO; understanding the baseline advantage computation (Eq. 1) and clipped objective (Eq. 2) is prerequisite to comprehending the hierarchical modifications.
  - Quick check question: Given rewards {R₁=1.0, R₂=0.5, R₃=0.0}, compute the normalized advantage Â₂ using Eq. 1.

- Concept: **Advantage Estimation in RL**
  - Why needed here: The core contribution involves redesigning advantage estimation; readers must understand that advantages measure relative quality of actions for policy gradient methods.
  - Quick check question: If all responses in a group receive identical rewards, what happens to GRPO advantages and why is this problematic?

- Concept: **Semantic Equivalence in Data Augmentation**
  - Why needed here: The method depends on transformations preserving solution semantics; understanding what constitutes valid augmentation vs. semantic drift is critical.
  - Quick check question: For a geometry problem with a right triangle, would horizontal flipping preserve or break the problem's solvability? What about color inversion?

## Architecture Onboarding

- Component map:
  Input: (I_ori, T_ori)
       ↓
  [SCT Module] → Offline: GPT-4o textual rewrites (ϕ)
                → Online: Visual transforms (ψ) + compensatory prompts (τ)
       ↓
  Expanded Questions: Q = {Q₁, Q₂, ..., Q_m}
       ↓
  [Policy Rollout] → Generate n responses per variant → O (m×n total)
       ↓
  [Reward Computation] → Accuracy + Format rewards → R
       ↓
  [Hierarchical Advantage] → Global (Eq. 5) + Local (Eq. 6) → Â_hier (Eq. 7)
       ↓
  [Shared Policy Optimization] → Cross-variant loss (Eq. 8) → Update π_θ

- Critical path: The SCT transformations → expanded question space → increased response diversity → hierarchical advantage computation. If transformations fail to preserve semantics, downstream advantages become misaligned.

- Design tradeoffs:
  - **m (variant count)**: Higher m increases diversity but computational cost. Table 4 shows improvement from m=2 (75.4%) to m=4 (75.9%), but diminishing returns.
  - **n (samples per variant)**: Paper fixes n=6; Table 5 shows GRPO plateaus at N=24 (73.0%), while Share-GRPO achieves 75.4% with m=2, n=6.
  - **Online transform probability p**: Set to 0.3; higher p risks semantic drift from aggressive visual changes.

- Failure signatures:
  - **Reward curves flatlining**: May indicate transformations introduce semantic drift; check variant-answer alignment manually.
  - **Advantage variance → 0**: Policy may have collapsed to homogeneous responses; verify SCT is producing meaningfully different variants.
  - **Performance degradation on general benchmarks**: Overfitting to mathematical reasoning; the paper reports +3.1% on MMStar, suggesting this is mitigated but monitorable.

- First 3 experiments:
  1. **Sanity check SCT semantics**: For 50 seed questions, manually verify that answers remain consistent across offline textual variants. Report consistency rate.
  2. **Ablate hierarchical estimation**: Compare (global-only) vs. (global+local) advantage on a held-out validation set during training; plot validation accuracy over steps to isolate the stability contribution.
  3. **Scale m with fixed compute**: Train with m∈{1,2,4} while holding total responses (m×n) constant at 12 to isolate the effect of question diversity vs. raw sample count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reliance on proprietary models like GPT-4o for offline textual Semantically Consistent Transformations (SCT) be effectively replaced by smaller, open-source models without compromising the quality of reasoning trajectory exploration?
- Basis in paper: [inferred] Section 3.2.1 states, "we prompt GPT-4o to generate m semantically consistent variants," creating a dependency on a high-cost proprietary model for the question expansion phase.
- Why unresolved: The paper does not provide an ablation study comparing GPT-4o against weaker or open-source models for the textual rewriting task, nor does it analyze the sensitivity of the final RL policy to the quality of these textual variants.
- What evidence would resolve it: A comparison experiment where the SCT module uses a smaller open-source LLM (e.g., Llama-3-8B) to generate variants, measuring the resulting performance delta on the MathVista or MathVerse benchmarks.

### Open Question 2
- Question: Does the heuristic exclusion of visual transformations like cropping and color distortion limit the generalizability of Share-GRPO to domains where such features are non-critical or where the model must learn invariance to them?
- Basis in paper: [inferred] Section 3.2.1 notes, "we avoid transformations (e.g., cropping, color distortion) that may disrupt key information," assuming these are universally harmful for reasoning tasks.
- Why unresolved: While safe for math diagrams, avoiding these transformations might limit the method's effectiveness in real-world scenarios where models must learn to ignore visual noise or where spatial cropping is a valid data augmentation strategy.
- What evidence would resolve it: An analysis of Share-GRPO performance on general vision benchmarks (e.g., COCO or visual grounding tasks) when the "avoided" transformations are included in the online multimodal SCT pipeline.

### Open Question 3
- Question: Does the hierarchical advantage estimation introduce a bias where policy updates are dominated by "easy" question variants that yield higher rewards, effectively under-utilizing harder variants?
- Basis in paper: [inferred] Section 3.2.2 describes aggregating rewards across variants with different difficulty levels (via global advantage estimation), but does not analyze the variance in learning signals between the variants.
- Why unresolved: It is unclear if the global aggregation smooths out the specific learning signals required for difficult variants, or if the optimization process converges primarily by optimizing the policy on the easiest transformation of the seed question.
- What evidence would resolve it: A breakdown of the gradient norms or policy improvements attributed specifically to high-reward (easy) variants versus low-reward (hard) variants during the training steps.

## Limitations
- The paper's claimed advantage of not requiring supervised fine-tuning may be overstated, as the base models already undergo extensive SFT before RL training
- The method relies on GPT-4o for offline textual transformations, creating a dependency on a proprietary model without exploring open-source alternatives
- The paper doesn't fully investigate edge cases where local and global advantages have opposing signs, which could introduce optimization conflicts

## Confidence
- **High confidence**: The core mechanism of question space expansion through semantically consistent transformations works as described, with clear empirical support showing improved reasoning performance across multiple benchmarks
- **Medium confidence**: The hierarchical advantage estimation provides stable and informative gradient signals, though the exact conditions under which local+global combination outperforms either alone remain underspecified
- **Medium confidence**: The cross-variant trajectory sharing effectively mitigates advantage vanishing, though the paper doesn't fully explore edge cases where variants have substantially different difficulty levels

## Next Checks
1. Conduct a controlled ablation study comparing global-only vs. hierarchical advantage estimation during training, measuring both final performance and training stability (validation curves)
2. Perform manual verification of semantic consistency across 100 transformed question pairs to quantify the rate of semantic drift and its impact on reward alignment
3. Test the method's robustness by intentionally introducing semantic inconsistencies in 20% of transformations to measure degradation in reasoning performance