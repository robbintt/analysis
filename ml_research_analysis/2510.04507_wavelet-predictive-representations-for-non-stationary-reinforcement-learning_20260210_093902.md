---
ver: rpa2
title: Wavelet Predictive Representations for Non-Stationary Reinforcement Learning
arxiv_id: '2510.04507'
source_url: https://arxiv.org/abs/2510.04507
tags:
- wavelet
- task
- non-stationary
- tasks
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Wavelet Predictive Representations for Non-Stationary Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2510.04507
- **Source URL:** https://arxiv.org/abs/2510.04507
- **Reference count:** 34
- **Primary result:** WISDOM outperforms baselines in non-stationary RL tasks by using wavelet-based temporal analysis.

## Executive Summary
This paper addresses non-stationary reinforcement learning by introducing WISDOM, a method that uses wavelet transforms to capture multi-scale temporal evolution of task representations. The approach decomposes context representations into approximation and detail coefficients, enabling the agent to track global trends while filtering high-frequency noise. WISDOM combines this with a wavelet temporal difference update operator and demonstrates improved performance across Meta-World, MuJoCo, and Type-1 Diabetes environments compared to standard meta-RL approaches.

## Method Summary
WISDOM builds upon context-based meta-RL by processing trajectory histories through a Wavelet Representation Network. This network applies discrete wavelet transforms using dilated convolutions to separate low-frequency global trends from high-frequency local variations. The wavelet coefficients are then reconstructed into task representations that are fed to standard SAC policy and critic networks. Training involves optimizing both the wavelet reconstruction (via TD and auto-regressive losses) and the SAC objectives simultaneously. The method specifically targets environments where MDP dynamics change stochastically over time.

## Key Results
- WISDOM outperforms PEARL and SAC baselines in Meta-World ML1 tasks with stochastic parameter changes
- The method shows improved robustness to Gaussian observation noise compared to non-wavelet approaches
- Maintains high success rates in Type-1 Diabetes environment while adapting to changing patient dynamics

## Why This Works (Mechanism)

### Mechanism 1: Multi-Scale Temporal Evolution Tracking
Transforming task representation sequences into the wavelet domain enables the agent to decouple global task trends from local, high-frequency variations. The Wavelet Representation Network applies a Discrete Wavelet Transform via recursive dilated convolutions, separating input sequences into Approximation Coefficients (capturing low-frequency global trends) and Detail Coefficients (capturing high-frequency local changes). This is critical for adapting to irregular non-stationary patterns.

### Mechanism 2: Wavelet Temporal Difference (TD) Update
A Bellman-style update operator applied to wavelet domain features stabilizes learning of task representations and mitigates error propagation common in standard auto-regressive models. The operator $F_W(z_t) = z_t + \Gamma W(z_{t+1})$ is proven to be a contraction mapping, ensuring representation updates converge and track task changes rather than relying solely on implicit gradient updates.

### Mechanism 3: Signal-to-Noise Ratio Enhancement
Reconstructing task representations from filtered wavelet coefficients improves policy robustness against state-noise and observation jitter. The architecture selectively downsamples/preserves detail coefficients and transforms them back to time domain, acting as a built-in filter that removes task-irrelevant high-frequency noise while retaining structural evolution of the MDP.

## Foundational Learning

- **Concept: Discrete Wavelet Transform (DWT)**
  - Why needed: Fundamental mathematical tool WISDOM uses to analyze task history
  - Quick check: Can you explain why DWT is preferred over Fourier Transform for non-stationary signals? (Answer: FT loses time locality; DWT preserves both time and frequency info)

- **Concept: Context-Based Meta-RL**
  - Why needed: WISDOM builds upon the "Module A" paradigm where a context encoder infers latent variable from history
  - Quick check: How does the agent infer current task if environment changes mid-episode? (Answer: By encoding recent trajectory history into latent context)

- **Concept: Contraction Mapping**
  - Why needed: Paper claims theoretical convergence of Wavelet TD operator based on this property
  - Quick check: What mathematical property ensures iterative update process converges to fixed point? (Answer: Contraction mapping / γ < 1)

## Architecture Onboarding

- **Component map:** Trajectory Batch -> Context Encoder -> Wavelet Network (Conv -> Filter -> Inverse) -> Wavelet TD Loss & AR Loss
- **Critical path:** Raw transitions processed through encoder to create z, then through wavelet network for multi-scale decomposition, with reconstructed $\hat{z}$ used for policy decisions
- **Design tradeoffs:** Decomposition Level (M) affects trend capture vs. detail preservation; filter coefficients learned from Haar initialization; Wavelet TD coefficient $\alpha_Y$ balances tracking vs. stability
- **Failure signatures:** Over-smoothing from aggressive decomposition; representation collapse from unbalanced losses; slow convergence from mismatched M to task frequency
- **First 3 experiments:** 1) Stationarity check comparing WISDOM to vanilla PEARL/SAC on stationary task, 2) Frequency analysis visualizing approximation vs detail coefficients on known periodic changes, 3) Noise robustness test injecting Gaussian noise to verify detail filtering removes noise without degrading policy

## Open Questions the Paper Calls Out

- **Open Question 1:** How can adaptive mechanisms be developed to dynamically adjust the number of decomposition levels based on specific environmental characteristics?
  - Basis: Explicitly listed as primary future direction in Conclusion
  - Why unresolved: Current fixed decomposition levels require manual tuning for specific environments
  - Evidence needed: Training regime where M is learned automatically resulting in equal or better performance

- **Open Question 2:** How can the effective selection of detail coefficients be optimized to enhance rapid adaptation in highly non-stationary environments?
  - Basis: Explicitly stated as future work item
  - Why unresolved: Current downsampling approach doesn't explore selective filtering or gating mechanisms
  - Evidence needed: New selection mechanism improving sample efficiency or performance in high-frequency task evolution

- **Open Question 3:** What structural components are required to successfully apply Variable-Wise Encoding to high-dimensional task representations without disrupting cross-variate dependencies?
  - Basis: Inferred from ablation study noting VWE performs poorly
  - Why unresolved: Current implementation relies on feature concatenation limiting processing of dimensions with differing frequencies
  - Evidence needed: Architecture with inter-variate interaction module enabling VWE to outperform feature concatenation baseline

## Limitations

- Dilated convolution configuration (exact dilation schedule and kernel sizes) is not specified, potentially impacting multi-scale temporal pattern capture
- Detail coefficient filtering logic is not precisely defined, which is critical for the noise reduction mechanism
- Hyperparameter dependencies suggest potential brittleness, as optimal decomposition levels and Wavelet TD coefficients appear environment-specific

## Confidence

- **Multi-Scale Temporal Evolution:** High - Theoretical foundation of DWT for non-stationary signals is well-established
- **Wavelet TD Update:** Medium - Contraction mapping proof provides theoretical support but lacks direct empirical validation
- **Signal-to-Noise Enhancement:** Medium - Wavelet-based noise filtering is documented but selective downsampling approach requires further validation

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary decomposition levels (M) and Wavelet TD coefficients (α_Y) across all benchmark environments to quantify robustness to hyperparameter changes

2. **Frequency Domain Visualization:** Generate wavelet coefficient spectra for tasks with known periodic changes to verify the network is correctly separating global trends from local variations

3. **Break Case Testing:** Design environments with purely high-frequency task changes (instantaneous switches) to test limits of noise filtering mechanism and identify when it may discard legitimate task information