---
ver: rpa2
title: Taxonomy of User Needs and Actions
arxiv_id: '2510.06124'
source_url: https://arxiv.org/abs/2510.06124
tags:
- user
- information
- system
- request
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Taxonomy of User Needs and Actions (TUNA),
  an empirically grounded framework for classifying user behavior in conversational
  AI systems. Developed through qualitative analysis of 1193 human-AI conversations,
  TUNA organizes user actions into a three-level hierarchy of 57 request types across
  six interaction modes: Information Seeking, Information Processing & Synthesis,
  Procedural Guidance & Execution, Content Creation & Transformation, Social Interaction,
  and Meta-Conversation.'
---

# Taxonomy of User Needs and Actions

## Quick Facts
- arXiv ID: 2510.06124
- Source URL: https://arxiv.org/abs/2510.06124
- Reference count: 40
- Primary result: Introduces TUNA, a three-level taxonomy with 57 request types across 6 interaction modes for classifying user behavior in conversational AI

## Executive Summary
This paper introduces the Taxonomy of User Needs and Actions (TUNA), an empirically grounded framework for classifying user behavior in conversational AI systems. Developed through qualitative analysis of 1193 human-AI conversations, TUNA organizes user actions into a three-level hierarchy across six interaction modes: Information Seeking, Information Processing & Synthesis, Procedural Guidance & Execution, Content Creation & Transformation, Social Interaction, and Meta-Conversation. The taxonomy addresses limitations in existing frameworks by capturing both instrumental goals and the situated, conversational strategies users employ to achieve them, enabling multi-scale evaluation and serving as a backbone for domain-specific taxonomies.

## Method Summary
The taxonomy was developed through iterative qualitative analysis of 1193 human-AI conversations from WildChat and ShareGPT datasets, sampled uniformly at random. The development followed a three-phase approach: (1) Empirical-to-conceptual with 200 dialogues (50 overlapping, 150 divided), (2) Expanded empirical with additional 200 + 499 dialogues, and (3) Conceptual-to-empirical with narrative literature review. The framework achieved theoretical saturation and was validated on a held-out set of 294 dialogues with 1,247 user turns, ensuring all turns were classifiable without requiring new modes or strategies. Three annotators performed consensus coding through weekly team discussions.

## Key Results
- TUNA successfully classifies 1,193 human-AI conversations into a three-level hierarchy (6 modes → 14 strategies → 57 request types)
- The taxonomy captures both instrumental goals and situated conversational strategies, including previously "invisible work" like social interaction and meta-communication
- Validation achieved theoretical saturation with no new high-level dimensions emerging from additional data

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition of User Intent
- **Claim:** Structuring user actions into a three-level hierarchy resolves the analytical gap between high-level instrumental goals and low-level conversational mechanics
- **Mechanism:** Separates what users are trying to achieve (Instrumental Modes 1–4) from relational work (Social Mode 5) and control mechanisms (Meta Mode 6), allowing mapping of specific utterances to broader strategies and high-level modes
- **Core assumption:** User intent can be discretely categorized into a nested hierarchy without losing conversational fluidity
- **Break condition:** Compound requests conflating multiple distinct request types may struggle to assign a single primary label

### Mechanism 2: Operationalizing "Invisible Work" via Dedicated Modes
- **Claim:** Explicitly categorizing Social Interaction and Meta-Conversation makes visible the "invisible work" users perform to manage the AI relationship
- **Mechanism:** Defines specific request types for "Social Etiquette," "Persona Directives," and "System Performance Feedback" as distinct behaviors rather than noise
- **Core assumption:** Social and meta-communicative acts are distinct enough from instrumental tasks to warrant separate categorization
- **Break condition:** If AI systems interpret Social Interaction inputs purely as task context rather than a distinct mode requiring relational response

### Mechanism 3: Multi-Scale Evaluation Backbone
- **Claim:** A unified, empirically grounded vocabulary serves as a "backbone" enabling consistent policy harmonization and disaggregated evaluation across diverse products
- **Mechanism:** Provides a shared lexicon allowing different stakeholders to align on user behavior definitions and standardize definitions of failure or success across domains
- **Core assumption:** A task-agnostic taxonomy can remain robust enough to be meaningfully overlaid with domain-specific taxonomies
- **Break condition:** If Instrumental modes are too broad for specialized domains, failing to provide actionable signal without heavy extension

## Foundational Learning

- **Concept: Situated Action & Articulation Work**
  - **Why needed here:** To understand why TUNA separates "conversational strategies" from "goals." Users improvise (articulation work) to fix errors or clarify ambiguity
  - **Quick check question:** Can you explain why a "broken" query might be classified as a valid "Communicative Status" action rather than just noise?

- **Concept: Conversational Grounding**
  - **Why needed here:** Essential for understanding Mode 5 (Social Interaction) and the "Shared Understanding" strategy. Users exert effort to establish "common ground" before or during instrumental tasks
  - **Quick check question:** How does TUNA classify a user saying "I meant the book, not the movie"—is it a new task or a repair mechanism?

- **Concept: Speech Act Theory (high-level)**
  - **Why needed here:** TUNA contrasts itself with linguistic frameworks that classify utterance function. You need to know the difference between a linguistic form (a question) and the user's instrumental goal
  - **Quick check question:** Why would TUNA classify "Write me a poem" as "Content Creation" rather than just a "Command" or "Question"?

## Architecture Onboarding

- **Component map:** 6 Interaction Modes (Modes 1-4: Instrumental; Mode 5: Social; Mode 6: Meta) → 14 Strategies → 57 Request Types
- **Critical path:**
  1. Ingest conversation logs
  2. Map user turns to TUNA Request Types (multiple labels permitted)
  3. Roll up Request Types to Strategies and Modes for multi-scale analysis
  4. Identify sequences (e.g., Background Info → Feasibility Assessment)
- **Design tradeoffs:** Specificity vs. Ambiguity (boundary cases require context-dependent interpretation); Compound Requests (must support multi-label classification)
- **Failure signatures:** Mode Collapse (everything classified as Information Seeking); Context Blindness (ignoring Mode 5/6 signals)
- **First 3 experiments:**
  1. Ambiguity Stress Test: Test human annotators on 50 compound user turns containing both instrumental and meta directives
  2. Sequential Harm Detection: Filter for [System Performance Feedback] → [Regeneration Request] sequences to identify system failures
  3. Safety Alignment Check: Compare TUNA classifications of benign vs. adversarial prompts to validate utility for safety policy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can automated classifiers accurately apply TUNA to large-scale interaction logs, and what is the inter-rater reliability compared to human coders?
- **Basis in paper:** The authors state that "building a reliable, automated classifier to apply TUNA at scale is a separate technical challenge"
- **Why unresolved:** The methodology relied on manual, iterative qualitative analysis and consensus coding
- **What evidence would resolve it:** Development and validation of an LLM-based classifier evaluated against human-annotated gold standard

### Open Question 2
- **Question:** Is TUNA sufficiently comprehensive and exhaustive when applied to non-English linguistic contexts and diverse cultural norms?
- **Basis in paper:** The paper notes "Further research is needed to validate its applicability and exhaustiveness in other linguistic contexts" as a limitation
- **Why unresolved:** The development corpus was majority English (56%), and validation included 20 other languages only as preliminary robustness check
- **What evidence would resolve it:** Validation study focusing on non-English corpora to identify if new interaction modes or strategies are required

### Open Question 3
- **Question:** To what extent do specific sequences of TUNA request types predict user satisfaction or task completion?
- **Basis in paper:** The paper suggests TUNA supports "sequential pattern mining to analyze how sequences of actions predict key outcomes, such as satisfaction" but does not present these findings
- **Why unresolved:** While the taxonomy provides the vocabulary for sequences, the correlational data linking specific behavioral patterns to outcomes is not yet established
- **What evidence would resolve it:** Large-scale log analysis correlating TUNA-annotated dialogue sequences with explicit satisfaction ratings

### Open Question 4
- **Question:** How stable are the TUNA strategies as AI capabilities and user appropriation practices evolve over time?
- **Basis in paper:** The authors posit that "it is possible user actions will evolve over time... TUNA should be considered a first, rather than a final iteration"
- **Why unresolved:** The taxonomy is based on a snapshot of current interactions; longitudinal stability is unknown
- **What evidence would resolve it:** Longitudinal studies analyzing shifts in the distribution of the 57 request types across different model versions and time periods

## Limitations

- Major uncertainties remain about inter-rater reliability during development phases, as IRR was only assessed at the end rather than throughout iterative coding
- The validation relied on a single annotator for most turns in the validation set, limiting confidence in the IRR metric's robustness
- The taxonomy may struggle with compound requests that contain multiple legitimate labels requiring context-dependent interpretation

## Confidence

- **High Confidence:** The taxonomy's empirical grounding through iterative development with 1,193 conversations and achievement of theoretical saturation; the three-level hierarchical structure is clearly specified and reproducible
- **Medium Confidence:** The claim that TUNA captures "invisible work" through dedicated Social and Meta modes, as this relies on qualitative interpretation rather than quantitative validation
- **Low Confidence:** The assertion that TUNA serves as an effective "backbone" for multi-scale evaluation and policy harmonization, as this remains a theoretical benefit without demonstrated case studies

## Next Checks

1. **IRR During Development:** Reconstruct the annotation process to assess inter-rater reliability at each iteration (200, 200, and 499 dialogues) to verify consistency throughout taxonomy development
2. **Compound Request Resolution:** Test the taxonomy's handling of compound requests by having multiple annotators independently label 100 multi-intent turns and measuring agreement rates
3. **Domain Extension Test:** Apply TUNA to a specialized domain (e.g., software engineering or healthcare) and evaluate whether the backbone structure meaningfully reduces the annotation burden compared to building a domain-specific taxonomy from scratch