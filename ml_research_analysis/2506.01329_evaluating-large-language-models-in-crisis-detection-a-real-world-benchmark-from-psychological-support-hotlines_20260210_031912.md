---
ver: rpa2
title: 'Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark
  from Psychological Support Hotlines'
arxiv_id: '2506.01329'
source_url: https://arxiv.org/abs/2506.01329
tags:
- performance
- suicidal
- risk
- were
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PsyCrisisBench benchmarks 64 large language models on 540 real-world
  hotline transcripts across four crisis assessment tasks. LLMs achieved strong performance
  on suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),
  and risk assessment (F1=0.907), with fine-tuning and few-shot prompting providing
  notable improvements.
---

# Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines

## Quick Facts
- arXiv ID: 2506.01329
- Source URL: https://arxiv.org/abs/2506.01329
- Reference count: 40
- PsyCrisisBench benchmarks 64 LLMs on 540 real-world hotline transcripts, achieving F1=0.880 for suicidal ideation detection

## Executive Summary
This study introduces PsyCrisisBench, the first comprehensive benchmark evaluating large language models (LLMs) on real-world crisis hotline transcripts across four critical tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. Testing 64 LLMs including both open-source and proprietary models, the research demonstrates that LLMs can achieve strong performance in crisis detection tasks, with fine-tuned and few-shot prompted models showing particular promise. The benchmark reveals that while LLMs perform comparably to trained human operators in many crisis assessment tasks, mood status recognition remains challenging due to the loss of crucial non-verbal cues in text-only transcripts.

## Method Summary
The researchers compiled 540 real-world crisis hotline transcripts from 2020-2024, covering diverse caller demographics and crisis types. They employed a rigorous three-stage annotation process with 80% inter-rater agreement to label transcripts for four assessment tasks. The benchmark evaluated 64 LLMs using zero-shot, few-shot, and fine-tuned approaches across multiple prompting strategies including reasoning and Chain-of-Thought methods. Performance was measured using task-specific metrics including F1-score, precision, and recall. The study also conducted cross-lingual experiments translating Chinese transcripts to English to assess model generalizability across languages.

## Key Results
- LLMs achieved F1=0.880 for suicidal ideation detection and F1=0.779 for suicide plan identification
- Fine-tuned 1.5B-parameter Qwen2.5-1.5B model outperformed larger models on mood and suicidal ideation tasks
- Risk assessment task showed highest performance with F1=0.907, while mood status recognition had lowest performance at F1=0.709
- Few-shot prompting (1-8 examples) significantly improved model performance across all tasks

## Why This Works (Mechanism)
The benchmark demonstrates that LLMs can effectively process crisis hotline transcripts by leveraging their natural language understanding capabilities to identify patterns in textual expressions of psychological distress. The models' performance on structured assessment tasks suggests they can capture semantic indicators of crisis risk from conversational text, particularly when provided with appropriate prompting strategies or fine-tuning on domain-specific data. The superior performance of smaller fine-tuned models indicates that task-specific optimization may be more valuable than model scale for this application domain.

## Foundational Learning
- **Zero-shot prompting**: Why needed - establishes baseline LLM capabilities without task-specific training; Quick check - compare performance across all models without examples
- **Few-shot learning**: Why needed - demonstrates how minimal examples improve performance; Quick check - test with 1, 2, 4, and 8 examples per task
- **Fine-tuning**: Why needed - reveals whether task-specific training improves over prompting; Quick check - compare fine-tuned vs. prompted models on same tasks
- **Reasoning mode vs. Chain-of-Thought**: Why needed - identifies optimal prompting strategy for clinical reasoning; Quick check - measure output verbosity vs. performance trade-offs
- **Cross-lingual evaluation**: Why needed - assesses model generalizability across languages; Quick check - compare performance on Chinese vs. English translations
- **Multimodal processing**: Why needed - identifies limitations of text-only analysis; Quick check - measure performance gap between text and audio-visual inputs

## Architecture Onboarding

**Component Map:** Data Collection -> Annotation Pipeline -> Model Evaluation -> Cross-lingual Testing -> Analysis

**Critical Path:** High-quality transcript collection → Expert annotation → LLM benchmarking → Performance analysis → Clinical implications

**Design Tradeoffs:** Text-only analysis provides scalability and privacy but loses non-verbal cues; few-shot prompting balances efficiency with performance; fine-tuning improves accuracy but requires annotated data

**Failure Signatures:** Low mood recognition performance indicates missing vocal/prosodic features; inconsistent cross-lingual results suggest cultural bias; verbosity in reasoning mode may hinder clinical utility

**First Experiments:**
1. Replicate benchmark using crisis transcripts from multiple cultural contexts
2. Test multimodal LLMs with audio inputs on same transcript set
3. Conduct A/B testing of reasoning mode outputs with human operators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating prosodic and paralinguistic features (e.g., tone, pace, hesitation) into multimodal LLM architectures significantly improve mood status recognition performance beyond the current F1 ceiling of 0.709?
- Basis in paper: The authors state that "crucial nonverbal cues such as prosody, pace, and hesitation are lost" in transcription, likely imposing a performance ceiling, and explicitly call for "multimodal integration incorporating prosodic and paralinguistic features" as a future research priority.
- Why unresolved: The current benchmark relies solely on ASR-generated text transcripts, which strip away acoustic information that may carry critical emotional signals for mood inference—particularly relevant given mood status recognition showed the lowest performance across all tasks.
- What evidence would resolve it: A comparative study evaluating the same LLMs on audio-enhanced vs. text-only inputs, with statistical analysis showing whether multimodal features yield significant F1 improvements in mood status recognition.

### Open Question 2
- Question: Does the PsyCrisisBench evaluation framework maintain validity and comparable model performance rankings when applied to authentic multilingual crisis hotline datasets from non-Chinese linguistic and cultural contexts?
- Basis in paper: The authors note their cross-lingual evaluation on English translations "represents a simplified approximation of true cross-cultural evaluation" and explicitly state "future work should evaluate the framework on authentic multilingual datasets."
- Why unresolved: The current cross-lingual experiment used culturally-adapted translations of Chinese transcripts rather than naturally-occurring multilingual crisis communications, which may not capture culturally-specific expressions of distress or suicide risk.
- What evidence would resolve it: Replication of the benchmark on native-language crisis hotline data from diverse cultural settings (e.g., English, Spanish, Arabic), comparing model performance patterns and identifying culturally-dependent performance gaps.

### Open Question 3
- Question: Does reasoning mode's 4.5-fold increase in output length yield clinically meaningful improvements in crisis assessment transparency and operator decision-making support, or does it introduce unnecessary cognitive burden?
- Basis in paper: The authors observe that reasoning mode produces "nearly 4.5-fold increase" in output words and question "whether such verbosity benefits clinical practice remains uncertain," noting that "extended outputs may increase operator burden rather than support decision-making."
- Why unresolved: While reasoning mode showed performance gains in mood classification, the trade-off between output verbosity and clinical utility in time-sensitive crisis settings has not been empirically evaluated with human operators.
- What evidence would resolve it: A user study with trained hotline operators assessing whether extended reasoning outputs improve their confidence, accuracy, or efficiency in crisis triage decisions compared to concise outputs.

## Limitations
- Benchmark relies on anonymized Chinese hotline transcripts from single organization, limiting cultural generalizability
- Text-only analysis excludes crucial non-verbal vocal cues that human operators use for mood assessment
- Cross-lingual evaluation used translations rather than authentic multilingual crisis communications
- Ethical implications of deploying models in clinical settings remain unexplored

## Confidence
- Crisis detection performance (F1 scores): High confidence
- Cross-cultural generalizability: Medium confidence
- Impact of missing vocal cues: Medium confidence
- Clinical utility of reasoning mode: Medium confidence

## Next Checks
1. Replicate benchmark results using crisis transcripts from multiple cultural contexts and languages to assess cross-cultural generalizability and identify potential bias patterns.
2. Conduct a comparative analysis incorporating human assessors with access to both transcript text and audio recordings to quantify the impact of missing vocal cues on model performance versus human judgment.
3. Implement controlled deployment studies in clinical settings with diverse patient populations to evaluate model performance under real-world conditions, including interaction with human operators and actual clinical decision-making processes.