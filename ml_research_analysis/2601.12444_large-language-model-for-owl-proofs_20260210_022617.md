---
ver: rpa2
title: Large Language Model for OWL Proofs
arxiv_id: '2601.12444'
source_url: https://arxiv.org/abs/2601.12444
tags:
- axioms
- axiom
- simp
- language
- overall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates Large Language Models (LLMs) for generating
  proofs in OWL ontologies through three tasks: extraction, simplification, and explanation.
  A dataset construction framework automatically selects subsumptions and justifications
  from real ontologies, then adds noisy axioms based on semantic distances.'
---

# Large Language Model for OWL Proofs

## Quick Facts
- arXiv ID: 2601.12444
- Source URL: https://arxiv.org/abs/2601.12444
- Reference count: 40
- LLMs struggle with OWL proof generation when faced with noise or incomplete premises, with performance drops up to 47% with noise and 38% with incomplete axioms.

## Executive Summary
This paper evaluates Large Language Models for generating proofs in OWL ontologies through three tasks: extraction, simplification, and explanation. The authors construct a dataset framework that automatically selects subsumptions and justifications from real ontologies, adding controlled noise based on semantic distances. Seven LLMs are tested across standard and complex settings, revealing that logical complexity most affects performance rather than representation form. GPT-o4-mini and Qwen3-32B achieve the best overall performance but still struggle significantly with complex derivations and noisy conditions.

## Method Summary
The authors develop an automatic dataset construction framework that selects subsumptions and justifications from real OWL ontologies. The framework injects controlled noise into the axioms based on semantic distance metrics. Seven LLMs are evaluated on three proof-related tasks: extraction (identifying relevant axioms), simplification (reducing proof complexity), and explanation (generating human-readable proofs). Testing occurs under both standard conditions and complex scenarios involving noise injection and incomplete premises.

## Key Results
- GPT-o4-mini and Qwen3-32B achieve the best overall performance across all tasks
- Logical complexity, not representation form, most significantly impacts LLM performance
- Performance drops up to 47% with noise injection and 38% with incomplete premises
- All tested LLMs struggle with complex derivations despite showing competence on simpler proofs

## Why This Works (Mechanism)
The effectiveness of LLMs for OWL proof generation depends on their ability to capture logical relationships between axioms and apply inference rules consistently. The models' performance correlates with their capacity to handle increasing logical complexity rather than the specific OWL syntax used. However, the introduction of noise and incomplete information significantly disrupts the models' reasoning chains, suggesting they rely heavily on complete and consistent input structures.

## Foundational Learning

1. **OWL Ontology Structure** - Understanding how OWL represents knowledge through classes, properties, and axioms
   *Why needed*: Forms the basis for understanding what constitutes valid proofs in this domain
   *Quick check*: Can identify class hierarchy and property relationships in a simple ontology

2. **Semantic Distance Metrics** - Measures of how logically related different axioms are
   *Why needed*: Critical for understanding how noise injection affects proof validity
   *Quick check*: Can explain why certain axiom modifications create more logical distance than others

3. **Proof Justification Extraction** - The process of identifying minimal sets of axioms that justify a subsumption
   *Why needed*: Core task for evaluating LLM performance in proof generation
   *Quick check*: Can trace which axioms are necessary versus sufficient for a given subsumption

## Architecture Onboarding

**Component Map**: Dataset Construction -> LLM Evaluation -> Performance Analysis -> Complexity Assessment

**Critical Path**: Ontology → Justification Selection → Noise Injection → Proof Task → LLM Output → Performance Metric

**Design Tradeoffs**: Automatic dataset construction enables scalability but may introduce sampling bias; controlled noise injection provides reproducibility but may not capture all real-world reasoning errors.

**Failure Signatures**: Performance degradation with noise follows predictable patterns based on semantic distance; incomplete premises cause cascading failures in proof chains; complex derivations overwhelm all tested models regardless of architecture.

**3 First Experiments**:
1. Test GPT-o4-mini on a simple subsumption with no noise to establish baseline performance
2. Introduce semantic distance-based noise at increasing levels to measure degradation
3. Remove one axiom from a complete proof to assess sensitivity to incomplete premises

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Dataset construction framework may introduce sampling bias through automatic selection methods
- Noise injection mechanism may not capture all types of logical inconsistencies encountered in practice
- Evaluation focuses on three specific tasks and may not generalize to other proof-related applications
- Performance findings are based on controlled conditions that may not reflect real-world complexity

## Confidence
- High confidence in relative LLM performance ranking across the three core tasks
- Medium confidence in absolute performance metrics due to potential dataset construction artifacts
- Medium confidence in noise sensitivity findings as the noise model represents one specific perturbation type
- Medium confidence in logical complexity being the primary performance factor, as other confounding variables may exist

## Next Checks
1. Test the same LLMs on manually curated datasets with diverse ontology structures to validate the automatic dataset construction framework's representativeness
2. Evaluate performance with alternative noise models, including structural and semantic noise types not covered in the current study
3. Assess whether fine-tuning LLMs specifically on OWL proof tasks improves performance, particularly for complex derivations and noisy conditions