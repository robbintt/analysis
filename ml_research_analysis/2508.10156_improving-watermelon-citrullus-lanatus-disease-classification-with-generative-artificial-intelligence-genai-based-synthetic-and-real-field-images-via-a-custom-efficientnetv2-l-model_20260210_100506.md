---
ver: rpa2
title: Improving watermelon (Citrullus lanatus) disease classification with generative
  artificial intelligence (GenAI)-based synthetic and real-field images via a custom
  EfficientNetV2-L model
arxiv_id: '2508.10156'
source_url: https://arxiv.org/abs/2508.10156
tags:
- images
- synthetic
- real
- were
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated whether combining real and synthetic images
  improves watermelon disease classification performance. Five training treatments
  were evaluated using an EfficientNetV2-L model: only real images, only synthetic
  images, 1:1 real-to-synthetic ratio, 1:10 real-to-synthetic ratio, and 1:10 real-to-synthetic
  with an unknown class.'
---

# Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model

## Quick Facts
- **arXiv ID:** 2508.10156
- **Source URL:** https://arxiv.org/abs/2508.10156
- **Reference count:** 8
- **Primary result:** Hybrid training with 1:10 real-to-synthetic ratio achieved Weighted F1-score of 1.00, outperforming real-only (0.65) and synthetic-only (0.74) approaches.

## Executive Summary
This study demonstrates that combining real and synthetic images significantly improves watermelon disease classification performance compared to using either data source alone. The research team evaluated five training treatments using an EfficientNetV2-L model: real images only, synthetic images only, and three hybrid ratios (1:1, 1:10, and 1:10 with unknown class). The hybrid approach with a 1:10 real-to-synthetic ratio achieved perfect classification performance (Weighted F1-score of 1.00), while synthetic images alone could not substitute for real images. The findings validate that even a small number of real images significantly enhances model generalizability and that a hybrid approach maximizes classification accuracy for crop disease diagnosis.

## Method Summary
The study employed a custom EfficientNetV2-L model with a modified classifier head consisting of three dense layers with Swish activation, dropout (0.5, 0.5, 0.4), and batch normalization. Five training treatments were evaluated using real images (2048×1536, resized to 480×480) and Stable Diffusion 3.5M synthetic images. The training strategy used transfer learning with two phases: first training the custom head with frozen backbone, then fine-tuning deeper backbone layers. AdamW optimizer was used with early stopping and learning rate reduction callbacks. The hybrid approach (1:10 real-to-synthetic ratio) achieved perfect classification performance (Weighted F1-score of 1.00) compared to real-only (0.65) and synthetic-only (0.74) approaches.

## Key Results
- Hybrid training with 1:10 real-to-synthetic ratio achieved Weighted F1-score of 1.00
- Real-only training achieved Weighted F1-score of 0.65
- Synthetic-only training achieved Weighted F1-score of 0.74, failing to generalize to real images
- The unknown class treatment (H4) maintained perfect performance while adding robustness to novel samples

## Why This Works (Mechanism)
The hybrid approach works because synthetic images provide data augmentation and diversity that helps the model learn disease-specific features, while the small number of real images anchors the model to actual field conditions and noise characteristics. The 1:10 ratio appears optimal because it provides sufficient real-world grounding without sacrificing the diversity benefits of synthetic data. The synthetic images alone fail because they lack the complex noise patterns and subtle variations present in real field conditions, despite appearing visually similar.

## Foundational Learning
- **Transfer Learning**: Why needed - Leverages pre-trained ImageNet weights for faster convergence and better feature extraction; Quick check - Verify model loads ImageNet weights before training
- **Data Augmentation through Synthesis**: Why needed - Overcomes data scarcity while maintaining disease-specific visual characteristics; Quick check - Compare synthetic image quality to real images using visual inspection
- **Hybrid Training Strategy**: Why needed - Balances the diversity benefits of synthetic data with the real-world grounding of actual images; Quick check - Test different real-to-synthetic ratios to find optimal balance

## Architecture Onboarding
**Component Map:** Input images → EfficientNetV2-L backbone → Custom Dense Head (3 blocks) → Output classification
**Critical Path:** Image preprocessing → Backbone feature extraction → Dense head classification → Evaluation metrics
**Design Tradeoffs:** Large EfficientNetV2-L provides strong feature extraction but requires 180GB VRAM; Custom head adds complexity but enables fine-tuning for specific disease classes
**Failure Signatures:** Synthetic-only training shows high training accuracy but poor real-world generalization (F1 ~0.74); Real-only training shows overfitting with low recall for rare classes
**First Experiments:** 1) Train synthetic-only model and test on real data to confirm generalization gap; 2) Train hybrid 1:10 model and compare performance to real-only baseline; 3) Add unknown class and verify robustness to novel samples

## Open Questions the Paper Calls Out
- **Open Question 1:** What is the optimal intermediate ratio of real-to-synthetic images (e.g., between 1:2 and 1:9) for training, rather than the extreme 1:1 or 1:10 ratios tested?
- **Open Question 2:** Can the hybrid training approach maintain high classification accuracy when deployed across diverse geographic regions, seasons, and commercial farm management practices?
- **Open Question 3:** Can generative AI models be refined to accurately synthesize watermelon leaves exhibiting multiple simultaneous disease symptoms (co-infections)?

## Limitations
- Missing critical hyperparameters (learning rates, fine-tuning depth, batch size) that significantly impact reproducibility
- Data generation pipeline ambiguity regarding SD 3.5M fine-tuning methodology and synthetic image quality validation
- Evaluation dataset transparency issues with unclear class stratification in the 112-image test set

## Confidence
- **High Confidence**: Hybrid real-synthetic training (1:10 ratio) outperforms both pure real and pure synthetic approaches
- **Medium Confidence**: Specific performance metrics due to limited transparency in evaluation methodology and hyperparameter selection
- **Low Confidence**: Claims about computational efficiency and scalability given lack of optimization strategy details

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates (1e-4, 1e-5, 1e-6) and fine-tuning depths to establish robustness of performance gains
2. **Synthetic Image Quality Validation**: Implement quantitative metrics (FID, IS) and human evaluation to verify synthetic image quality matches original study
3. **Cross-Domain Generalization Test**: Evaluate trained models on independent real datasets from different growing conditions to confirm generalizability beyond training data