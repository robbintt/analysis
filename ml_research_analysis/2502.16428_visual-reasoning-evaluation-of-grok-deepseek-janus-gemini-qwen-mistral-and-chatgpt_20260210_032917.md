---
ver: rpa2
title: Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral,
  and ChatGPT
arxiv_id: '2502.16428'
source_url: https://arxiv.org/abs/2502.16428
tags:
- reasoning
- janus
- multimodal
- accuracy
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses limitations in multimodal LLM evaluation by
  introducing a benchmark that assesses multi-image reasoning, rejection-based evaluation,
  and reasoning consistency. We employ entropy as a novel metric to quantify stability
  across reordered answer variants, revealing models that rely on positional heuristics
  versus genuine comprehension.
---

# Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT

## Quick Facts
- arXiv ID: 2502.16428
- Source URL: https://arxiv.org/abs/2502.16428
- Authors: Nidhal Jegham; Marwan Abdelatti; Abdeltawab Hendawi
- Reference count: 40
- Primary result: Introduced entropy-based evaluation showing ChatGPT-o1 achieves 82.5% accuracy while Janus models show high entropy (0.787-0.839) indicating positional bias

## Executive Summary
This study evaluates nine multimodal large language models across eight visual reasoning tasks using a benchmark that emphasizes reasoning stability, rejection-based assessment, and multi-image integration. The benchmark introduces entropy as a novel metric to detect positional bias and reasoning inconsistencies across reordered answer variants. Results show ChatGPT-o1 achieving highest overall performance (82.5% accuracy, 70.0% rejection accuracy) while Janus models exhibit poor consistency with high entropy scores, demonstrating that model size alone does not guarantee superior reasoning capabilities.

## Method Summary
The evaluation uses 120 questions from MUIRBench with 376 images, featuring 40 unanswerable questions and reordered answer variants to assess reasoning consistency. Models are evaluated using four metrics: accuracy, rejection accuracy, abstention rate, and mean entropy across reordered variants. The entropy metric quantifies answer distribution variability to detect positional bias. Models are tested zero-shot with temperature=1.0 for web APIs and local deployment for open-source variants. Answer parsing extracts letter choices regardless of explanation quality.

## Key Results
- ChatGPT-o1 achieves highest overall accuracy (82.5%) and rejection accuracy (70.0%)
- Janus models show poor consistency with high entropy scores (Janus 7B: 0.8392, Janus 1B: 0.787)
- QVQ-72B-Preview achieves highest rejection accuracy (85.0%) but with excessive abstention (42.5%)
- Model size does not guarantee performance: Qwen2.5-VL-72B (82.5B) underperforms smaller Gemini 2.0 Flash

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Reasoning Consistency Detection
Entropy quantifies reasoning stability by measuring answer distribution variability across reordered variants. For each question group, the frequency of answer selections forms a probability distribution where low entropy indicates consistent reasoning regardless of answer position. This exposes models that rely on positional heuristics versus genuine content understanding.

### Mechanism 2: Rejection Accuracy for Uncertainty Calibration
Rejection accuracy measures whether models correctly abstain when no valid answer exists, serving as a proxy for uncertainty calibration. With 40 unanswerable questions (33% of total), models should select "None of the provided options." This metric reveals whether models know their limitations or hallucinate answers.

### Mechanism 3: Multi-Image Contextual Reasoning Integration
Multi-image tasks require integrating information across 2-8 images with temporal, spatial, or comparative relationships. Questions demand cross-referencing images rather than independent processing, exposing models that succeed at single-image pattern matching but fail at relational reasoning.

## Foundational Learning

- **Entropy in Information Theory**
  - Why needed here: Shannon entropy measures answer distribution uncertainty; high entropy = instability across reorderings
  - Quick check question: If a model selects options A, B, C, D equally (0.25 each) across reorderings, what is the entropy?

- **Uncertainty Calibration in ML**
  - Why needed here: Rejection accuracy measures whether models know when evidence is insufficient
  - Quick check question: A model with 70% rejection accuracy but 42.5% abstention rateâ€”is it over-confident or over-cautious?

- **Positional Bias in Multiple-Choice Evaluation**
  - Why needed here: LLMs may prefer certain answer positions; reordering detects this shortcut exploitation
  - Quick check question: If a model answers correctly when the answer is in position A but incorrectly when the same answer is in position C, what does this reveal?

## Architecture Onboarding

- **Component map:**
  MUIRBench subset (120 Q, 376 images) -> Question Variants Generator -> Model Inference -> Evaluation Metrics

- **Critical path:** Entropy calculation depends on correctly grouping original + reordered variants. Answer parsing must normalize variations (e.g., "C) First image" vs "C").

- **Design tradeoffs:**
  - 120 questions balances coverage vs. cost; may miss edge cases
  - Temperature=1.0 encourages diversity but increases randomness
  - Binary correctness (letter match) ignores explanation quality

- **Failure signatures:**
  - High entropy + low accuracy: Model guessing randomly
  - High rejection accuracy + high abstention (>0.33): Over-conservative model
  - Low rejection accuracy + low abstention: Overconfident model (hallucination risk)
  - Perfect accuracy on original but fails on reordered: Positional bias detected

- **First 3 experiments:**
  1. Run Janus-7B locally with temperature sweep (0.0, 0.5, 1.0) to isolate temperature effects on entropy scores
  2. Create adversarial reorderings where the same semantic answer appears in all four positions across variants to verify entropy measures stability
  3. Ablate rejection questions to test whether inclusion changes model behavior on answerable questions (priming effects)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed performance gaps and reasoning instability patterns persist when scaling evaluation to the full MUIRBench dataset?
- Basis in paper: [inferred] The authors limited evaluation to 120 questions from MUIRBench's 2,600 questions for computational feasibility
- Why unresolved: It remains unclear if high entropy scores and positional biases are artifacts of specific sample selection or systemic issues
- What evidence would resolve it: Replication using complete MUIRBench dataset to verify statistical significance of entropy and accuracy correlations

### Open Question 2
- Question: To what extent will Grok 3's reasoning stability and rejection calibration improve as the model transitions from beta to finalized release?
- Basis in paper: [explicit] Grok 3 is currently in "Beta version" and showed "unmet expectations" with overly conservative abstention rate (0.375)
- Why unresolved: The paper evaluates a snapshot of a model in active development, making it difficult to determine if underperformance is temporary
- What evidence would resolve it: Re-evaluating official, non-beta release of Grok 3 using entropy and rejection accuracy metrics

### Open Question 3
- Question: What specific architectural or training modifications are required to reduce high entropy and positional bias in lightweight open-source models?
- Basis in paper: [inferred] While the paper identifies Janus models exhibit high entropy due to "insufficient exposure to rejection-based reasoning," it does not propose specific solutions
- Why unresolved: Results demonstrate current training regimes lead to unstable reasoning, but exact mechanism for transferring stability from large proprietary models remains undefined
- What evidence would resolve it: Ablation study applying interventions like rejection-tuning or ordered-answer augmentation to Janus models

## Limitations
- Corpus validation gap: Limited validation against established consistency measures in multimodal evaluation literature
- Question curation ambiguity: Exact selection criteria for 120 MUIRBench questions remain unspecified beyond task distribution
- Positional bias isolation: Entropy metric assumes consistency across reorderings indicates genuine comprehension, but models could learn benchmark-specific heuristics

## Confidence
- **High Confidence**: Rejection accuracy findings are directly measurable and align with observable model behavior differences
- **Medium Confidence**: Entropy-based consistency metric shows clear numerical differences but requires additional validation for interpreting as "genuine comprehension"
- **Medium Confidence**: Multi-image reasoning evaluation demonstrates measurable performance gaps, but assumptions about relational reasoning need further investigation

## Next Checks
1. **Entropy Mechanism Validation**: Create controlled experiments with synthetic models exhibiting known reasoning patterns (consistent, position-biased, random) and verify entropy scores correctly distinguish behaviors across reordered variants.

2. **Rejection Calibration Assessment**: Conduct comprehensive analysis of 42.5% abstention rate across all models to determine if this represents appropriate calibration or systematic over-conservatism. Compare against human performance on same unanswerable questions.

3. **Cross-Benchmark Generalization**: Evaluate same models on independent multi-image reasoning benchmarks (e.g., MMMU, NLVR2) to determine whether Janus models' entropy scores reflect fundamental reasoning instability or benchmark-specific artifacts.