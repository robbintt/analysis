---
ver: rpa2
title: 'Think-J: Learning to Think for Generative LLM-as-a-Judge'
arxiv_id: '2505.14268'
source_url: https://arxiv.org/abs/2505.14268
tags:
- thinking
- judgment
- data
- trace
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes Think-J, a method to improve generative LLM-as-a-judge
  models by learning how to think. The approach involves two steps: (1) initializing
  judgment thinking capability using a small amount of high-quality curated data annotated
  by Deepseek-R1, and (2) optimizing the thinking traces via reinforcement learning
  (RL) using either critic-guided offline learning or rule-based online learning.'
---

# Think-J: Learning to Think for Generative LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2505.14268
- Source URL: https://arxiv.org/abs/2505.14268
- Reference count: 23
- Key outcome: Think-J improves generative LLM-as-a-judge models via learning to think, achieving higher accuracy on RewardBench and RMBench without extra human annotations.

## Executive Summary
Think-J introduces a method to enhance generative LLM-as-a-judge models by learning how to think. The approach involves two key steps: initializing judgment thinking capability using a small, high-quality dataset annotated by Deepseek-R1, and optimizing thinking traces via reinforcement learning using either critic-guided offline learning or rule-based online learning. Experiments on models like Qwen-2.5-32B and Llama-3-8B show significant performance gains over existing generative and classifier-based LLM judges. The method demonstrates that enhanced thinking traces lead to more accurate and interpretable judgments, and the model is more robust to noisy or lower-quality training data compared to classifier-based approaches.

## Method Summary
Think-J enhances generative LLM-as-a-judge models by learning to think. It starts with a small, high-quality dataset (LIMJ707) annotated by Deepseek-R1 to initialize judgment thinking capabilities via supervised fine-tuning. Thinking traces are clipped to focus on decisive reasoning. The method then optimizes thinking traces using two RL approaches: (1) critic-guided offline learning with contrastive preference optimization (DPO) and (2) rule-based online learning using GRPO with accuracy, format, and strength rewards. This dual approach improves both the quality and interpretability of judgments without requiring additional human annotations.

## Key Results
- Think-J significantly outperforms existing generative and classifier-based LLM judges on RewardBench and RMBench benchmarks.
- Enhanced thinking traces lead to more accurate and interpretable judgments.
- The model is more robust to noisy or lower-quality training data compared to classifier-based approaches.
- Performance gains are achieved without requiring extra human annotations.

## Why This Works (Mechanism)

### Mechanism 1: High-Quality Thinking Trace Initialization
- Claim: A small set of carefully curated, high-quality thinking traces can activate latent reasoning capabilities for judgment tasks.
- Mechanism: 707 samples filtered by accuracy, difficulty, and diversity are annotated by Deepseek-R1 with reasoning traces, then used for SFT. Trace clipping removes verbose reasoning to focus on decisive factors.
- Core assumption: LLMs inherently possess chain-of-thought reasoning that can be elicited with minimal high-quality examples; longer thinking does not inherently improve judgment accuracy.
- Evidence anchors:
  - [abstract] "We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities."
  - [section 3.1] "Recent studies have shown that LLMs inherently possess long chain-of-thought (CoT) reasoning capabilities, which can be activated with a small amount of data."
  - [section 5.1] "using thinking trace annotated with Deepseek-V3, or removing the trace from the data would results in a substantial decline in performance."
- Break condition: If base model lacks sufficient reasoning capacity, or if initialization data contains systematic biases, the elicited thinking patterns may be flawed or fail to generalize.

### Mechanism 2: Critic-Guided Contrastive Learning for Thinking Optimization
- Claim: A critic model conditioned on judgment outcomes can construct informative positive/negative thinking trace pairs for offline preference optimization.
- Mechanism: A critic model is trained to generate thinking traces given the correct judgment, while the judge generates traces given only inputs. When judge is correct, critic generates incorrect trace as negative; when judge is incorrect, critic generates correct trace as positive. DPO optimizes the judge using these pairs.
- Core assumption: Critic can reliably generate plausible but incorrect reasoning traces; contrastive learning signal from correct vs. incorrect traces improves reasoning quality beyond SFT alone.
- Evidence anchors:
  - [abstract] "The offline RL requires training a critic model to construct positive and negative examples for learning."
  - [section 3.2] "Based on the positive and negative samples, optimize the judgment thinking ability with offline learning objective."
  - [Table 13] Critic-based DPO achieves 83.2 overall vs. 81.2 for sampling-based DPO on Llama-3-8B.
- Break condition: If critic generates implausible negative traces, or if positive/negative traces differ only superficially, the contrastive signal degrades.

### Mechanism 3: Rule-Based Online RL with Multi-Component Rewards
- Claim: GRPO with discrete rule-based rewards (accuracy, format, strength) can optimize thinking traces without a learned value function.
- Mechanism: Group sampling generates multiple responses per prompt; rewards combine accuracy (binary correct/incorrect), format (structure compliance), and strength (preference degree). GRPO optimizes using group-relative advantages without value model overhead.
- Core assumption: Simple rule-based rewards provide sufficient gradient signal; value models may destabilize NLG-style reasoning tasks.
- Evidence anchors:
  - [abstract] "The online method defines rule-based reward as feedback for optimization."
  - [section 3.2] "PPO significantly underperforms compared to GRPO and Reinforce++... incorporating a value model for thinking optimization would decrease training stability."
  - [Table 7] Format reward removal causes degradation (84.1→79.4 on Qwen-7B); strength reward helps 32B but may confuse 7B.
- Break condition: If rewards are gamed (e.g., extreme score manipulation) or if format constraints are too rigid, optimization may reward surface features over genuine reasoning.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Offline learning branch uses DPO to optimize thinking traces from constructed preference pairs without training a separate reward model.
  - Quick check question: Can you explain how DPO reparameterizes the reward function in terms of the policy and reference model?

- Concept: GRPO (Group Relative Policy Optimization)
  - Why needed here: Online learning branch uses GRPO as the primary algorithm; understanding group-based advantage estimation is critical.
  - Quick check question: How does GRPO compute advantages differently from PPO, and why does this eliminate the need for a value model?

- Concept: Chain-of-Thought Reasoning Elicitation
  - Why needed here: The initialization phase assumes latent reasoning can be activated; understanding few-shot CoT activation is prerequisite.
  - Quick check question: What evidence exists that CoT capabilities are emergent vs. require explicit training?

## Architecture Onboarding

- Component map:
  - Judge Model: Takes (instruction, response_a, response_b) → generates thinking trace → judgment
  - Critic Model: Takes (instruction, response_a, response_b, correct_judgment) → generates thinking trace
  - Reward Functions: accuracy (binary), format (template compliance), strength (preference degree)
  - Training Pipelines: SFT on LIMJ707 → Offline DPO or Online GRPO

- Critical path:
  1. Curate LIMJ707: filter Skywork-Preference for accuracy → difficulty → diversity
  2. Annotate with Deepseek-R1, apply trace clipping
  3. SFT both judge and critic on clipped traces
  4. Offline: Judge samples → critic constructs opposite traces → DPO
  5. Online: GRPO with rule rewards, temperature 0.0 for inference

- Design tradeoffs:
  - Offline vs. Online: Offline more stable with limited data; online (GRPO) achieves higher peak performance but requires more compute
  - Trace clipping: Reduces inference time 3.7x but assumes concise reasoning suffices; may lose nuance for complex judgments
  - Strength reward: Benefits larger models (32B) but can confuse smaller models (7B)
  - Critic vs. sampling: Critic yields 8942 pairs vs. 5111 for sampling at n=16

- Failure signatures:
  - Reward hacking: Model assigns extreme scores (0/100) to maximize margin reward (Table 9)
  - Length collapse: PPO shows unstable response length; GRPO maintains gradual increase
  - Thinking-quality mismatch: 50%+ trace errors even with correct judgments on noisy data (Table 16)
  - Sampling inefficiency: Nearly half of data yields no negative sample at n=16

- First 3 experiments:
  1. Reproduce LIMJ707 curation on a held-out preference subset: verify that accuracy → difficulty → diversity filtering yields improvement over random sampling.
  2. Ablate trace clipping: train with full vs. clipped R1 traces, measure both accuracy and inference latency.
  3. Compare GRPO vs. PPO on same initialization: confirm value model destabilization hypothesis with learning curves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can margin-based reward designs for fine-grained scoring be stabilized against reward hacking without requiring absolute-score annotated preference data?
- Basis in paper: [explicit] Appendix A.1 states: "We believe that mitigating this reward hacking for r_margin necessitates preference data annotated with absolute scores, which we leave for future investigation."
- Why unresolved: The authors tested margin-based rewards but observed consistent reward hacking where scores were driven to extreme values. They could not find an effective formulation to prevent this behavior.
- What evidence would resolve it: A modified reward formulation or training technique that maintains stable score distributions without collapsing to extremes, validated on benchmarks requiring fine-grained discrimination.

### Open Question 2
- Question: Why does the strength reward component benefit larger models (32B) but harm smaller models (7B), and what does this imply about the relationship between model capacity and multi-objective reward learning?
- Basis in paper: [inferred] Section 5.3 reports that removing r_strength improves performance for Qwen-2.5-7B but provides slight enhancement for the 32B model, with the hypothesis that stronger models can better learn correlations.
- Why unresolved: The paper only hypothesizes that the 32B model's stronger abilities enable it to handle the additional learning objective, but does not investigate the underlying mechanism or test intermediate model sizes.
- What evidence would resolve it: A controlled study across multiple model sizes with ablation of the strength reward, combined with analysis of learned representations to identify where smaller models fail to integrate preference strength signals.

### Open Question 3
- Question: To what extent can incorrect or imperfect thinking traces still yield correct judgments, and does this phenomenon undermine the reliability of thinking-based evaluation in safety-critical domains?
- Basis in paper: [explicit] Appendix A.6 states: "For a binary classification task like judgment, an imperfect thinking trace can still lead to a correct judgment in many cases, provided the critical point with decisive influence is identified."
- Why unresolved: The analysis revealed over 50% of thinking traces contained identifiable errors in models trained on noised data, yet accuracy remained high. This disconnect between process quality and outcome correctness is not fully investigated.
- What evidence would resolve it: Systematic analysis correlating thinking trace error types and locations with judgment outcomes, particularly for safety-critical scenarios where flawed reasoning could propagate undetected.

## Limitations

- Initialization data quality dependence: The method's performance hinges on the assumption that Deepseek-R1 annotations are consistently high-quality and free of systematic bias, which is not independently validated.
- Generalizability of critic-guided contrastive learning: The offline RL branch depends on the critic's ability to generate plausible yet incorrect thinking traces, but the paper does not benchmark the critic's generated traces for plausibility or diversity.
- Rule-based reward stability and bias: The online GRPO approach relies on simple, discrete rewards, which may be susceptible to reward hacking and may not capture nuanced aspects of reasoning quality.

## Confidence

- High: Performance improvements on RewardBench and RMBench (relative to baselines), the existence of the LIMJ707 dataset and its curation process, the basic architecture of judge/critic models and GRPO training pipeline.
- Medium: Claims about the efficacy of trace clipping for efficiency, the superiority of critic-guided DPO over sampling-based methods, the necessity of the strength reward for larger models.
- Low: The robustness of the method to noisy or low-quality training data, the general applicability of the approach beyond the tested model families, the long-term stability of rule-based rewards.

## Next Checks

1. **Independent validation of initialization data**: Have a separate annotator (or model) evaluate a subset of the Deepseek-R1 thinking traces for correctness, reasoning coherence, and potential biases. Compare agreement rates and identify systematic errors.
2. **Stress test critic-guided contrastive learning**: Systematically evaluate the critic's generated negative traces for plausibility, diversity, and alignment with human judgments. Test whether replacing critic-generated negatives with human-written or model-generated plausible alternatives affects performance.
3. **Reward function robustness analysis**: Perform a reward hacking analysis by probing whether the model can exploit the discrete reward structure (e.g., via targeted prompts or adversarial examples). Test whether alternative reward formulations (e.g., smoother scales, auxiliary penalties) mitigate exploitation and improve reasoning quality.