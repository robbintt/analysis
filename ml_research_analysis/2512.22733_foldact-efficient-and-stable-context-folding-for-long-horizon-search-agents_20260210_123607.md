---
ver: rpa2
title: 'FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents'
arxiv_id: '2512.22733'
source_url: https://arxiv.org/abs/2512.22733
tags:
- context
- training
- summary
- loss
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FoldAct addresses the fundamental challenge of training long-horizon\
  \ RL agents with context folding, where summaries create policy-dependent, non-stationary\
  \ observation distributions that violate core RL assumptions. The framework introduces\
  \ three key innovations: (1) separated loss computation for independent gradient\
  \ signals on summary and action tokens to prevent gradient dilution, (2) full context\
  \ consistency loss to minimize KL divergence between compressed and full contexts,\
  \ reducing distribution shift from self-conditioning, and (3) selective segment\
  \ training that samples a subset of trajectory turns, achieving 5.19\xD7 speedup."
---

# FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents

## Quick Facts
- arXiv ID: 2512.22733
- Source URL: https://arxiv.org/abs/2512.22733
- Authors: Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo
- Reference count: 8
- Primary result: Achieves 38.5 F1/29.5 EM on HotpotQA and 32.9 F1/29.0 EM on PopQA

## Executive Summary
FoldAct addresses the fundamental challenge of training long-horizon reinforcement learning agents with context folding, where summaries create policy-dependent, non-stationary observation distributions that violate core RL assumptions. The framework introduces three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to minimize KL divergence between compressed and full contexts, and selective segment training that samples a subset of trajectory turns. On local RAG benchmarks, FoldAct achieves state-of-the-art performance while reducing computational cost by 80.7% and preventing training collapse by reducing response lengths from over 5000 to 1200-1400 tokens.

## Method Summary
FoldAct tackles the non-stationary observation problem in context folding by implementing a three-pronged approach. First, it separates loss computation between summary and action tokens to prevent gradient dilution and ensure both components receive independent learning signals. Second, it introduces a consistency loss that minimizes KL divergence between the compressed summary context and the original full context, reducing distribution shift from self-conditioning. Third, it implements selective segment training that samples a subset of trajectory turns, achieving 5.19Ã— speedup while maintaining performance. These innovations work together to stabilize training while maintaining efficiency, enabling effective long-horizon RL training with context folding.

## Key Results
- Achieves 38.5 F1/29.5 EM on HotpotQA and 32.9 F1/29.0 EM on PopQA
- Reduces computational cost by 80.7% through selective segment training
- Stabilizes training by reducing response lengths from over 5000 to 1200-1400 tokens

## Why This Works (Mechanism)
FoldAct works by addressing the fundamental mismatch between context folding and RL requirements. In traditional RL, observations are assumed to be independent of the policy, but context folding creates policy-dependent summaries that violate this assumption. By separating loss computation, FoldAct ensures that gradients flow appropriately to both summary and action tokens without dilution. The consistency loss directly targets the distribution shift problem by enforcing similarity between compressed and full contexts. Selective segment training reduces computational burden while maintaining learning effectiveness by focusing on representative trajectory segments rather than full trajectories.

## Foundational Learning

**Context Folding**: The process of compressing trajectory information into summaries to manage context length in long-horizon tasks. Needed because standard transformer attention mechanisms scale quadratically with context length. Quick check: Verify that folding maintains essential information while reducing token count.

**Non-stationary Observations in RL**: When the observation distribution depends on the policy itself, violating the Markov Decision Process assumption. Needed to understand why standard RL algorithms fail with context folding. Quick check: Monitor observation distribution stability across training iterations.

**KL Divergence Consistency Loss**: Measures distributional similarity between full context and its compressed summary. Needed to quantify and minimize information loss during folding. Quick check: Compare KL divergence values before and after consistency loss training.

**Selective Segment Training**: Training on a subset of trajectory segments rather than full trajectories. Needed to reduce computational complexity in long-horizon tasks. Quick check: Verify performance retention when varying the percentage of segments used.

## Architecture Onboarding

Component Map: Input Trajectory -> Context Folding Module -> Summary Generation -> Separated Loss Computation -> RL Policy Network -> Action Output

Critical Path: The essential sequence flows from input trajectory through context folding to summary generation, then through separated loss computation to update both summary and action components of the policy.

Design Tradeoffs: The framework trades computational efficiency (via selective segment training) against potential loss of long-range dependencies, and parameter efficiency (via shared encoders) against potential interference between summary and action learning.

Failure Signatures: Training collapse manifests as rapidly increasing response lengths (>5000 tokens), unstable KL divergence values, or degraded performance on downstream tasks. Computational inefficiency appears as training times exceeding baseline without performance gains.

First Experiments:
1. Verify separated loss computation prevents gradient dilution by comparing gradient norms with and without separation
2. Test consistency loss effectiveness by measuring KL divergence reduction between full and compressed contexts
3. Validate selective segment training efficiency by comparing training time and performance across different sampling rates

## Open Questions the Paper Calls Out
None

## Limitations
- KL divergence minimization may not fully capture semantic fidelity for complex multi-hop reasoning tasks
- Selective segment training could miss important long-range dependencies across full trajectory contexts
- Generalization claims to other domains beyond specific RAG benchmarks are not well-established

## Confidence

High Confidence:
- Empirical results demonstrating effectiveness in reducing response lengths and preventing training collapse
- Computational speedup claims from selective segment training are directly measurable

Medium Confidence:
- Separated loss computation preventing gradient dilution needs more granular ablation studies
- Consistency loss effectiveness in maintaining summary quality lacks detailed qualitative analysis

Low Confidence:
- Generalization to other domains beyond specific RAG benchmarks
- Performance on tasks requiring different types of context manipulation

## Next Checks

1. Conduct systematic ablation studies isolating the contribution of each innovation (separated loss, consistency loss, selective training)

2. Evaluate summary quality through human assessment or automated semantic similarity metrics

3. Test the framework on diverse long-horizon tasks beyond RAG, including multi-agent coordination and planning-intensive problems