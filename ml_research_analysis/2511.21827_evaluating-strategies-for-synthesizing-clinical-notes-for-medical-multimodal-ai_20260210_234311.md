---
ver: rpa2
title: Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal
  AI
arxiv_id: '2511.21827'
source_url: https://arxiv.org/abs/2511.21827
tags:
- clinical
- notes
- images
- metadata
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the impact of synthetic clinical notes on
  multimodal (MM) learning in dermatology, where paired image-text data are scarce.
  Clinical notes are generated using metadata-driven templates and large language
  models (LLMs), with strategies varying in prompt structure and metadata inclusion.
---

# Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI

## Quick Facts
- arXiv ID: 2511.21827
- Source URL: https://arxiv.org/abs/2511.21827
- Reference count: 4
- This work shows that synthetic clinical notes improve multimodal skin lesion classification, especially under domain shift, with metadata-guided prompts enabling robust cross-modal retrieval.

## Executive Summary
This paper addresses the scarcity of paired image-text data in dermatology by generating synthetic clinical notes using metadata-driven templates and large language models (LLMs). Four strategies are tested: metadata-only, metadata with structured LLM prompts, metadata with free-form LLM prompts, and free-form LLM only. Experiments on eleven heterogeneous datasets demonstrate that multimodal models trained with synthetic notes outperform unimodal baselines in skin lesion classification, particularly under domain shift. The study finds that metadata-guided prompts enable emergent cross-modal retrieval capabilities, while unstructured LLM outputs introduce hallucinations that weaken alignment.

## Method Summary
The authors generate synthetic clinical notes for dermatology images using four strategies: metadata-only templates (M), metadata with structured LLM prompts (M + P1, M + P2), and free-form LLM prompts (P3). They train multimodal models combining DenseNet121 for images and PubMedBERT for text, with a shared classifier and multiple alignment losses (L1, cosine similarity, NT-Xent). The models are evaluated on eleven heterogeneous dermatology datasets across classification and cross-modal retrieval tasks.

## Key Results
- Multimodal models with synthetic notes improve skin lesion classification over unimodal baselines, especially under domain shift
- Metadata-guided prompts (M + P1, M + P2) enable robust cross-modal retrieval without explicit retrieval optimization
- Unstructured LLM outputs (P3) improve classification but degrade cross-modal alignment due to hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metadata-guided synthetic clinical notes create semantically aligned cross-modal embeddings that enable emergent retrieval capabilities, even when retrieval is not explicitly optimized.
- Mechanism: Structured prompts constrain LLM output vocabulary to clinically relevant attributes (lesion type, border, color, dermoscopic structures), reducing hallucination variance and producing text embeddings that consistently map to corresponding image features in the shared latent space. The alignment losses (L1, cosine similarity, NT-Xent) then enforce sample-wise and class-level correspondence between modalities.
- Core assumption: The metadata (class labels, subclass annotations) contains sufficient diagnostic signal to ground the LLM's descriptions in clinically meaningful content.
- Evidence anchors:
  - [abstract] "Cross-modal retrieval, not explicitly optimized, emerges as a robust capability with metadata-guided prompts."
  - [section: Results, Table 3] M + P1 and M + P2 achieve mAP 0.787 and 0.782 on BCN20000 retrieval, vs. 0.510 for P3.
  - [corpus] Related work on clinical text encoders (Clinical ModernBERT, arXiv:2504.03964) demonstrates that domain-specific pretraining improves biomedical embedding quality, supporting the need for structured medical vocabulary.
- Break condition: If metadata is sparse, incorrect, or fails to capture visually salient features, the synthetic text will lack discriminative content and alignment gains will diminish.

### Mechanism 2
- Claim: Multimodal training with synthetic text acts as implicit regularization on the image encoder, improving generalization under domain shift.
- Mechanism: The text modality provides compact, semantically structured supervision that complements pixel-level visual features. Even synthetic notes derived from metadata inject class-relevant semantic context, encouraging the image encoder to learn representations that generalize better to out-of-distribution data.
- Core assumption: The synthetic text captures diagnostic semantics that are relevant across different acquisition protocols and patient populations.
- Evidence anchors:
  - [abstract] "MM models trained with synthetic notes improve skin lesion classification performance over unimodal baselines, particularly under domain shift."
  - [section: Results, Table 2] External partition: SKINL2 improves from 0.686 (unimodal) to 0.743 (M + P2); Fitzpatrick17k improves from 0.418 to 0.456 (P3).
  - [corpus] MoMA (arXiv:2508.05492) similarly finds that multimodal EHR integration improves clinical prediction, supporting the regularization hypothesis.
- Break condition: If the synthetic text contains systematic errors or hallucinations that are consistent across training samples, the regularization effect may introduce bias rather than reduce overfitting.

### Mechanism 3
- Claim: Free-form LLM-generated notes (without metadata grounding) can improve classification but degrade cross-modal alignment due to hallucination noise.
- Mechanism: Classification tasks rely primarily on label supervision, which can override noisy text embeddings. However, retrieval tasks require precise embedding-level correspondence, which is disrupted when hallucinated content creates spurious associations between text and image features.
- Core assumption: The LLM's internal knowledge includes some dermatologically relevant patterns, but without grounding constraints, it generates inconsistent descriptions across similar cases.
- Evidence anchors:
  - [abstract] "Unstructured LLM outputs introduce noise that weakens alignment."
  - [section: Discussion] "P3 underperforms in retrieval tasks, suggesting that free-form LLM-generated notes introduce hallucinations or irrelevant details that weaken the alignment between text and images."
  - [section: Results, Table 5] Cosine similarity for P3 ranges from 0.204–0.312 across datasets, vs. 0.689–0.797 for M + P2.
  - [corpus] Open-source evaluation tools (arXiv:2503.16504) address AI-generated note quality assessment, indicating community concern about hallucination in clinical text.
- Break condition: If the LLM is specifically fine-tuned on dermatology corpora, hallucination rates may decrease and the trade-off between classification and alignment could shift.

## Foundational Learning

- Concept: Contrastive cross-modal alignment (NT-Xent loss)
  - Why needed here: The paper uses NT-Xent alongside L1 and cosine losses to align image and text embeddings. Understanding how contrastive learning pulls paired samples together while pushing non-paired samples apart is essential for debugging retrieval failures.
  - Quick check question: Given a batch of 4 image-text pairs, which samples would the NT-Xent loss treat as positive vs. negative pairs?

- Concept: Mean Average Precision (mAP) for retrieval evaluation
  - Why needed here: Tables 3 and 4 report mAP for cross-modal retrieval. Without understanding precision-recall trade-offs and how mAP aggregates across queries, you cannot assess whether retrieval performance is clinically meaningful.
  - Quick check question: If a model retrieves 5 notes for an image query, with the correct note at position 2, what is the average precision for this single query?

- Concept: LLM hallucination in structured generation
  - Why needed here: The paper's central tension is that LLMs can synthesize useful text but also hallucinate. Understanding why constrained decoding (predefined options for lesion attributes) reduces hallucinations helps explain the M + P1 vs. P3 performance gap.
  - Quick check question: Why does providing a constrained vocabulary for "lesion color" reduce hallucination risk compared to open-ended generation?

## Architecture Onboarding

- Component map: Images -> DenseNet121 -> Image embeddings -> Projection -> Shared latent space -> Shared classifier; Text -> PubMedBERT -> Text embeddings -> Projection -> Shared latent space -> Shared classifier

- Critical path:
  1. Preprocess images (Otsu thresholding for dermoscopy; manual bounding boxes for clinical photos) -> 224×224
  2. Generate synthetic notes via M, M + P1, M + P2, or P3 strategy
  3. Tokenize text (BERT tokenizer, max 512 tokens)
  4. Forward pass through modality-specific encoders
  5. Project to shared space, compute alignment losses
  6. Classify via shared head, compute cross-entropy

- Design tradeoffs:
  - CNN (DenseNet121) vs. Vision Transformer: Authors chose CNN due to limited dataset size and overfitting concerns with ViT.
  - Simple template (M) vs. LLM-augmented prompts (M + P1/P2): Templates are reliable but low-information; LLM adds richness but introduces hallucination risk.
  - Multiple alignment losses vs. single contrastive loss: Combination provides both sample-wise (L1, cosine) and global structure (NT-Xent), but increases optimization complexity.

- Failure signatures:
  - High classification accuracy but low retrieval mAP -> likely P3-style ungrounded text causing misalignment.
  - Low cosine similarity between paired embeddings -> check for systematic metadata errors or prompt design flaws.
  - Large performance gap between internal and external partitions -> model overfitting to source-specific artifacts; consider more aggressive data augmentation or metadata enrichment.

- First 3 experiments:
  1. Establish unimodal image baseline (Img column in Table 2) on all test partitions to quantify the MM improvement margin.
  2. Compare M (metadata-only) vs. M + P1 vs. M + P2 on a held-out validation set, focusing on retrieval mAP to identify the prompt strategy with best alignment properties before full training.
  3. Ablate alignment losses: train with only NT-Xent, only L1+cosine, and the full combination, measuring both classification (κ-score) and retrieval (mAP) to verify that the multi-loss design is necessary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would clinical notes generated by medically-trained LLMs reduce hallucinations and improve cross-modal alignment compared to general-purpose LLMs?
- Basis in paper: [explicit] "Further directions include... evaluating notes generated by medically trained LLMs" and "this limitation arises in part because the LLM is not trained on medical corpora, which could reduce noise."
- Why unresolved: Only gpt-4o-mini was tested, and P3 (no metadata) showed poor alignment, suggesting domain-specific training may help.
- What evidence would resolve it: Comparison experiments using medically-trained LLMs (e.g., Meditron, BioMedLM) with identical prompts and evaluation metrics.

### Open Question 2
- Question: Can combining multiple prompt strategies reduce LLM-induced noise while maintaining strong classification and retrieval performance?
- Basis in paper: [explicit] "Further directions include combining prompts to reduce LLM-induced noise."
- Why unresolved: Current results show trade-offs—P3 excels at classification but underperforms on retrieval due to hallucinations; structured prompts show opposite trends.
- What evidence would resolve it: Systematic evaluation of hybrid prompt strategies that blend metadata-guided and free-form generation across both tasks.

### Open Question 3
- Question: Do the findings generalize to other medical imaging domains beyond dermatology?
- Basis in paper: [inferred] The paper focuses exclusively on dermatology, and notes that "MM algorithms require paired samples corresponding to the same clinical case" across domains.
- Why unresolved: Dermatology has specific visual-semantic relationships; unclear if metadata-driven synthesis works equally well for radiology, pathology, or ophthalmology.
- What evidence would resolve it: Replication of the synthetic note generation and MM training pipeline on datasets from other medical specialties with similar metadata availability.

## Limitations

- The study relies on the assumption that metadata and template structures adequately capture clinically relevant features for dermatology, but metadata quality varies across heterogeneous datasets
- Clinical utility is not validated—the study focuses on technical performance metrics without assessing whether synthetic notes would be accepted by practicing dermatologists
- Results depend heavily on prompt engineering quality, but the paper does not provide systematic guidelines for developing effective prompts across different medical domains

## Confidence

- High confidence: Multimodal models with synthetic notes improve classification performance over unimodal baselines
- Medium confidence: Metadata-guided prompts enable emergent cross-modal retrieval capabilities
- Low confidence: Free-form LLM outputs "introduce noise that weakens alignment" (mechanistic explanation lacking)

## Next Checks

1. **Metadata quality audit**: Systematically evaluate the completeness and accuracy of metadata across all datasets to quantify how metadata sparsity or errors correlate with synthetic note quality and model performance degradation.

2. **Cross-specialty transferability**: Apply the synthetic note generation framework to a different medical domain (e.g., radiology reports or pathology findings) to test whether the observed benefits generalize beyond dermatology and whether domain-specific prompt engineering is required.

3. **Human evaluation study**: Conduct a blinded comparison where practicing dermatologists assess the clinical validity and potential diagnostic utility of synthetic notes versus real clinical documentation, measuring inter-rater agreement and identification of hallucinated content.