---
ver: rpa2
title: Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification
arxiv_id: '2506.23247'
source_url: https://arxiv.org/abs/2506.23247
tags:
- sats
- saliency
- images
- segment
- segments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Segment Attribution Tables (SATs), a method
  to aggregate local saliency explanations into interpretable, semi-global insights
  for image classification models. SATs leverage segmentation maps to group pixel-level
  saliency values into meaningful image segments (e.g., "eyes" or "watermark"), then
  compute and aggregate mean attributions across multiple images to reveal patterns
  the model relies on.
---

# Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification

## Quick Facts
- arXiv ID: 2506.23247
- Source URL: https://arxiv.org/abs/2506.23247
- Reference count: 16
- Primary result: SATs detect shortcut learning earlier than accuracy metrics by aggregating local saliency explanations into segment-level summaries.

## Executive Summary
This paper introduces Segment Attribution Tables (SATs), a method to aggregate local saliency explanations into interpretable, semi-global insights for image classification models. SATs leverage segmentation maps to group pixel-level saliency values into meaningful image segments (e.g., "eyes" or "watermark"), then compute and aggregate mean attributions across multiple images to reveal patterns the model relies on. The approach addresses the gap between overly detailed local explanations and oversimplified global summaries. Experiments on synthetic ImageNet data and the BAR dataset show SATs can detect shortcut features like watermarks, even when they appear in only a small fraction of images and across varying positions.

## Method Summary
SATs aggregate pixel-level saliency maps into segment-level summaries by first computing mean attribution values within each semantic segment of an image, then ranking segments by absolute attribution within each image, and finally aggregating these ranks across images using either relative or absolute methods. The method uses segmentation models to partition images into named regions, applies any saliency method to generate attribution maps, and computes mean attribution per segment. Relative aggregation normalizes by ranking segments within each image, while absolute aggregation pads missing segments with zero values. The approach enables detection of recurring influential features and shortcut learning patterns that may be position-invariant or appear at low prevalence.

## Key Results
- On synthetic watermark dataset, SATs detected shortcut reliance at 15% watermark prevalence while accuracy metrics showed little change until 25%.
- In Chihuahua experiment, "eyes" and "nose" consistently ranked highest across saliency methods, demonstrating method robustness.
- For BAR dataset, SATs revealed the model relied more on person segments than activity-specific segments, confirming environmental bias detection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating saliency by semantic segment names (rather than pixel positions) enables detection of recurring patterns even when features appear at varying locations across images.
- Mechanism: Each image's saliency map is masked by segmentation regions; mean attribution is computed per named segment (e.g., "watermark"). These named values are then aggregated across images, making the method position-invariant at the global level while preserving local semantics.
- Core assumption: The segmentation model provides meaningful, consistent labels across images; saliency methods produce directional signals that survive averaging.
- Evidence anchors:
  - [abstract] "SATs take image segments (such as 'eyes' in Chihuahuas) and leverage saliency maps to quantify their influence."
  - [Page 2] "This enables the identification of recurring influential features, such as facial features in specific dog breeds or dataset artefacts like watermarks in horse images even when these appear infrequently and at varying locations."
  - [Page 3] "SpRAy... prioritises the position of influential areas, rather than their semantic meaning... if a watermark shortcut appeared in different positions across images, SpRAy would likely return multiple separate clusters."
- Break condition: Segment labels are inconsistent, too granular, or mislabeled; the paper notes this limitation [Page 7]: "Issues such as incorrect labels, undetected objects, or inappropriate granularity... can reduce effectiveness."

### Mechanism 2
- Claim: Ranking segments by mean attribution within each image before aggregation normalizes across varying saliency scales and improves comparability.
- Mechanism: For each SAT, segments are ranked by absolute mean attribution (rank 1 = highest). Aggregated ranks (rather than raw values) are then compared across images, reducing dependence on saliency method normalization.
- Core assumption: Within-image rank is more stable than absolute attribution values across different images and saliency methods.
- Evidence anchors:
  - [Page 4] "We further define the local rank of the mean attribution within a SAT, with rank 1 corresponding to the segment with the highest mean attribution. This facilitates fairer comparisons across SATs and reduces the need for explicit normalisation of saliency maps."
  - [Page 5] "We compare six different saliency methods on the Chihuahua class and observe a large amount of agreement between them."
  - [corpus] No direct corpus comparison on ranking vs. raw aggregation; evidence limited to this paper.
- Break condition: When segment sets differ substantially across images, ranks become incomparable; the paper addresses this with "Absolute aggregation" that pads missing segments with zero attribution [Page 4].

### Mechanism 3
- Claim: SATs can detect shortcut learning before accuracy metrics show degradation, because the model may learn genuine features and shortcuts simultaneously.
- Mechanism: As shortcut prevalence increases, the shortcut segment's aggregated attribution rank rises (indicating model reliance) even while accuracy remains stable—the model is using both valid features and the shortcut.
- Core assumption: Models can learn multiple strategies in parallel; shortcut reliance is detectable through attribution patterns before it dominates predictions.
- Evidence anchors:
  - [Page 6, Figure 5] "Test accuracy remains relatively stable until after 25% of zebra images are watermarked. However, SATs begin to highlight the watermark shortcut much earlier, with a clear signal emerging from around 15% watermarking."
  - [Page 6] "This reflects the earlier point that the model continues to learn the true classification task, making use of meaningful segments such as eyes, while also learning the shortcut in parallel."
  - [corpus] No corpus papers validate this specific early-detection claim.
- Break condition: If the shortcut is highly predictive (>50% prevalence in the binary case), accuracy drops sharply and the shortcut becomes obvious through standard metrics; SATs' advantage is for subtle, low-prevalence shortcuts.

## Foundational Learning

- Concept: Saliency/attribution maps (e.g., LRP, Integrated Gradients, Grad-CAM)
  - Why needed here: SATs require a pre-computed saliency map as input; understanding what these maps represent (pixel-level influence on prediction) is essential.
  - Quick check question: Given a classification output, can you explain what a saliency map's high-value pixels indicate about the model's decision?

- Concept: Semantic segmentation with labeled regions
  - Why needed here: SATs aggregate saliency over named segments; you must understand how segmentation models partition images and assign labels.
  - Quick check question: If a segmentation model labels a region "watermark" in some images but "text" in others, how would this affect SAT aggregation?

- Concept: Shortcut learning (spurious correlations)
  - Why needed here: The primary use case demonstrated is detecting shortcuts; you need to recognize why high accuracy can coexist with undesirable model behavior.
  - Quick check question: A medical imaging model achieves 95% accuracy but relies on hospital-specific watermarks. Would you detect this with standard test metrics? Why or why not?

## Architecture Onboarding

- Component map:
  1. Input: Image + trained classifier
  2. Saliency generator: Any attribution method (LRP-Z, Deep Taylor, SHAP, etc.) → per-pixel attribution map A
  3. Segmentation model: Open-set model (e.g., DinoX) or prompted segmentation → set of (name, mask) tuples
  4. SAT generator: For each segment, compute mean attribution over masked region → single-row entry
  5. Aggregator: Combine SATs across images using Relative or Absolute aggregation → summary tables/visualizations
  6. Output: Critical difference diagrams, ranked segment tables, or filtered views

- Critical path: Saliency map quality → Segmentation label consistency → Aggregation method choice. Errors in segmentation labels propagate directly and cannot be corrected downstream.

- Design tradeoffs:
  - Relative vs. Absolute aggregation: Relative ignores segment frequency (can over-weight rare segments); Absolute pads missing segments (dilutes signals but enables fair comparison).
  - Segmentation granularity: Too fine → noisy attributions; too coarse → loses specificity. Paper uses 12 prompted segments per class.
  - Saliency method: Paper shows agreement across methods but notes each has different noise characteristics; LRP-Z used for shortcut experiments.

- Failure signatures:
  - Watermark detected only in some positions → segmentation labels are inconsistent (e.g., "watermark-left" vs. "watermark-right").
  - Person segment ranks below background in action recognition → shortcut learning present (confirmed expected behavior).
  - Empty or near-zero attributions for all segments → saliency method may be incompatible with model architecture or images misprocessed.

- First 3 experiments:
  1. Replicate the Chihuahua experiment on 100 images: Generate saliency maps (Deep Taylor), segment with DinoX (12 prompted segments), aggregate SATs, and verify "eyes" and "nose" rank highly. Confirms pipeline correctness.
  2. Controlled watermark test: Train a CNN on a binary dataset with watermarks added to 20% of one class. Compute SATs on held-out watermarked images. Verify watermark segment rank rises compared to a model trained on clean data.
  3. Segmentation sensitivity check: Run SATs on the same images using different segmentation models (e.g., DinoX vs. Grounded SAM) and compare aggregated ranks. Identifies how label consistency affects results.

## Open Questions the Paper Calls Out
None

## Limitations
- **Segmentation consistency**: SATs depend heavily on segmentation model reliability and label consistency. Inconsistent segment names (e.g., "watermark-left" vs. "watermark-right") would fragment signals. The paper acknowledges this limitation but doesn't quantify its impact [Page 7].
- **Saliency method sensitivity**: While the paper shows agreement across methods on synthetic data, real-world saliency noise could obscure subtle shortcut patterns. No systematic sensitivity analysis is provided.
- **Early detection boundary**: The 15% watermark prevalence detection claim is compelling but based on a synthetic dataset with controlled conditions. Real-world shortcut prevalence and detectability may differ substantially.

## Confidence

- **High confidence**: The core SAT aggregation mechanism (rank-based, segment-level summarization) is well-described and validated on synthetic data. The mathematical formulation is sound.
- **Medium confidence**: The shortcut detection capability on BAR dataset is demonstrated, but the exact environmental bias quantification and segment naming consistency are not fully specified.
- **Low confidence**: The claim that SATs detect shortcuts earlier than accuracy metrics in general practice is supported only by the synthetic watermark experiment. Real-world validation is needed.

## Next Checks

1. **Segmentation robustness test**: Run SATs on the same images using multiple segmentation models (DinoX, Grounded SAM, and manual segmentation) and measure how much the aggregated ranks vary. This quantifies the impact of label consistency.

2. **Saliency noise tolerance**: Add controlled Gaussian noise to saliency maps at varying levels and measure how detection accuracy for known shortcuts degrades. This establishes method sensitivity.

3. **Real-world shortcut case study**: Apply SATs to a medical imaging dataset known to have acquisition bias (e.g., hospital-specific protocols) and validate whether SATs detect the bias before accuracy metrics show degradation.