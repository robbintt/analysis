---
ver: rpa2
title: 'AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning'
arxiv_id: '2601.18631'
source_url: https://arxiv.org/abs/2601.18631
tags:
- tool
- reasoning
- tools
- image
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaReasoner, a multimodal model family that
  learns adaptive tool use as a general reasoning skill rather than memorizing tool-specific
  patterns. The approach combines a scalable data curation pipeline for long-horizon
  multi-step tool interactions, Tool-GRPO reinforcement learning for optimizing tool
  selection and sequencing, and an adaptive learning mechanism that randomizes tool
  definitions to enhance generalization.
---

# AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning

## Quick Facts
- **arXiv ID**: 2601.18631
- **Source URL**: https://arxiv.org/abs/2601.18631
- **Authors**: Mingyang Song; Haoyu Sun; Jiawei Gu; Linjie Li; Luxin Xu; Ranjay Krishna; Yu Cheng
- **Reference count**: 40
- **One-line result**: 7B AdaReasoner improves by +24.9% on average and surpasses proprietary systems like GPT-5 on multiple tasks.

## Executive Summary
AdaReasoner introduces a multimodal model family that learns adaptive tool use as a general reasoning skill rather than memorizing tool-specific patterns. The approach combines a scalable data curation pipeline for long-horizon multi-step tool interactions, Tool-GRPO reinforcement learning for optimizing tool selection and sequencing, and an adaptive learning mechanism that randomizes tool definitions to enhance generalization. AdaReasoner can autonomously adopt beneficial tools, suppress irrelevant ones, and adjust usage frequency based on task context. Evaluated on diverse benchmarks, the 7B model improves by +24.9% on average and surpasses proprietary systems like GPT-5 on multiple tasks, including VSP and Jigsaw, while maintaining strong generalization to unseen tools and tasks.

## Method Summary
AdaReasoner employs a three-stage training pipeline: Tool Cold Start (TC) for pre-training on curated multi-step tool trajectories, Tool-GRPO (TG) for reinforcement learning to optimize tool selection, and Adaptive Learning (ADL) for generalization through tool definition randomization. The model operates on a Thought-Action-Observation loop, using Qwen2.5-VL as the policy network and an external tool server for execution. The approach emphasizes learning to recover from tool failures and suppress irrelevant tools through asymmetric reward structures and semantic decoupling via randomization.

## Key Results
- 7B AdaReasoner improves by +24.9% on average across benchmarks compared to baseline models
- Surpasses proprietary systems like GPT-5 on VSP and Jigsaw tasks
- Achieves strong generalization to unseen tools, with VSPO performance increasing from ~24% to ~70% with randomization
- Demonstrates adaptive tool usage, with A* invocation frequency decreasing for irrelevant tasks during RL training

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Trajectory Pre-training (Tool Cold Start)
Exposing the model to high-fidelity, multi-step trajectories containing explicit failures and backtracking enables robust error recovery, rather than just mimicking successful paths. The data curation pipeline synthesizes trajectories that include "reflection and backtracking" and "explicit tool failure," teaching the policy to handle the TAO loop when observations are suboptimal or null.

### Mechanism 2: Long-Horizon Reward Optimization (Tool-GRPO)
Decoupling tool usage from mandatory task completion via an asymmetric reward structure encourages treating tools as fallback mechanisms, optimizing for utility rather than frequency. The Tool-GRPO stage uses adaptive rewards where incorrect predictions with high-quality tool usage receive partial credit, while correct predictions without tools receive full reward, modulating tool frequency based on task context.

### Mechanism 3: Semantic Decoupling via Randomization (Adaptive Learning)
Randomizing tool identifiers and paraphrasing descriptions during training forces the model to ground tool selection in semantic functional understanding rather than surface-level lexical matching. By replacing meaningful tool names with random strings and rephrasing descriptions, the Adaptive Learning method acts as regularization, compelling the model to infer utility from docstrings and context.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Standard RL often requires a complex critic model. GRPO simplifies this by comparing groups of outputs to estimate advantage, crucial for stabilizing training in the complex multi-tool action space.
  - **Quick check question:** Can you explain how GRPO estimates the advantage $A_i$ differently than PPO? (Hint: It uses group statistics rather than a value function).

- **Concept: The TAO Loop (Thought-Action-Observation)**
  - **Why needed here:** AdaReasoner operates on this loop (Section 2.1, Eq. 1). Understanding state transitions $s_t \xrightarrow{a_t} s_{t+1}$ based on observation $o_t$ is essential for debugging why a model might get stuck in a reasoning loop.
  - **Quick check question:** If $o_t$ returns a "Tool Failure" error, what should the ideal transition $s_t \to s_{t+1}$ look like in the model's reasoning trace?

- **Concept: Tool-Augmented Trajectories**
  - **Why needed here:** The "Tool Cold Start" isn't just SFT on text; it's SFT on structured trajectories involving JSON calls and visual outputs. You must understand how to serialize these interactions into training tokens.
  - **Quick check question:** How does the model distinguish between a text generation and a tool call during inference? (Hint: Look for special tokens like `<tool_call>`).

## Architecture Onboarding

- **Component map:** Base Model (Qwen2.5-VL) -> Policy $\pi_\theta$ -> Tool Server (MCP-like service) -> Curator (synthetic trajectories) -> RL Trainer (AdaTG with GRPO)

- **Critical path:** The most fragile component is the **Data Curation Pipeline (Section 2.2)**. The quality of the "reflection" data determines if the model learns to self-correct or simply hallucinates corrections. If the curated trajectories lack diversity in failure modes, the RL stage will struggle to explore effective strategies.

- **Design tradeoffs:**
  - SFT vs. RL: Section 3.2 notes that while SFT (TC) provides stability, RL (TG) is strictly required for the model to *learn to discard* irrelevant tools. SFT alone leads to rigid, "always-call" behaviors.
  - Generalization vs. Performance: The randomization strategy (Section 2.4) improves generalization to new tools but may slightly lower peak performance on the specific training tools compared to overfitting to their names.

- **Failure signatures:**
  - **Syntactic Locking:** The model generates perfect JSON tool calls but for the wrong tool, indicating it learned the Format Reward but failed at semantic reasoning.
  - **Tool loops:** The model calls a tool, receives an observation, and immediately calls the same tool again with identical parameters, suggesting the reflection mechanism failed.
  - **Premature Answering:** The model outputs `<response>` before gathering sufficient visual evidence, often seen when the accuracy reward is weighted too high relative to the tool reward.

- **First 3 experiments:**
  1. **Validation of TC Data:** Train a model using *only* the Tool Cold Start data on the VSP task. Verify that the model can execute the "Perception -> Planning -> Verification" logic without crashing.
  2. **RL Frequency Modulation Test:** Run the TG phase on VSP with A* enabled. Plot the tool invocation frequency (as in Figure 3). Confirm that the "Navigation" subtask increases A* usage while "Verification" decreases it.
  3. **Generalization Stress Test:** Train with Adaptive Learning (randomized tools) and evaluate on a completely unseen tool (e.g., a new "Edge Detection" tool) to confirm the model infers usage from the description alone.

## Open Questions the Paper Calls Out

None

## Limitations

- The paper's primary claims rest on the assumption that the data curation pipeline can generate sufficiently diverse and realistic failure scenarios at scale, but scalability remains uncertain.
- The effectiveness of the randomization strategy for generalization is promising but may have limitations, as it's unclear whether generalization extends to tools with substantially different modalities or APIs.
- The approach scales effectively to larger model sizes beyond 7B, but the paper only reports results for 3B and 7B models, leaving questions about performance at 70B+ scales.

## Confidence

**High Confidence:**
- Tool-GRPO reinforcement learning stage effectively modulates tool usage frequency based on task context
- Adaptive learning mechanism improves generalization to unseen tools
- 7B AdaReasoner model demonstrates state-of-the-art performance on VSP and Jigsaw benchmarks

**Medium Confidence:**
- Data curation pipeline generates sufficient diversity in failure scenarios to teach robust error recovery
- Asymmetric reward structure successfully encourages treating tools as fallback mechanisms
- Semantic decoupling via randomization forces functional understanding over surface-level matching

**Low Confidence:**
- The approach scales effectively to larger model sizes beyond 7B
- The reflection mechanism reliably prevents tool loops across all task types

## Next Checks

1. **Trajectory Diversity Validation**: Analyze the distribution of failure modes in the curated TC data. Quantify the variety of tool failures, reflection patterns, and backtracking scenarios to ensure the model is exposed to a representative sample of real-world reasoning challenges rather than synthetic patterns.

2. **Zero-Shot Generalization Stress Test**: Evaluate AdaReasoner on a benchmark containing 10-15 completely unseen tools with varying modalities (vision, text, numerical computation) and APIs. Measure both success rate and the model's ability to correctly infer tool usage from descriptions without any fine-tuning.

3. **Reward Weight Sensitivity Analysis**: Conduct an ablation study varying the relative weights of tool usage rewards versus accuracy rewards in the TG stage. Identify the optimal balance that maximizes performance while minimizing tool overuse or premature answering behaviors.