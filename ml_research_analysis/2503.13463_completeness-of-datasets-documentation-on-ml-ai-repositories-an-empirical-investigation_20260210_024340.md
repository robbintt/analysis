---
ver: rpa2
title: 'Completeness of Datasets Documentation on ML/AI repositories: an Empirical
  Investigation'
arxiv_id: '2503.13463'
source_url: https://arxiv.org/abs/2503.13463
tags:
- documentation
- dataset
- datasets
- data
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the completeness of dataset documentation
  across popular ML/AI repositories. We created the Documentation Test Sheet (DTS),
  a schema based on literature recommendations, to measure documentation quality across
  100 datasets from Hugging Face, Kaggle, OpenML, and UCI.
---

# Completeness of Datasets Documentation on ML/AI repositories: an Empirical Investigation

## Quick Facts
- arXiv ID: 2503.13463
- Source URL: https://arxiv.org/abs/2503.13463
- Reference count: 39
- Primary result: Severe documentation gaps found in ML/AI datasets, particularly for collection processes and data processing procedures

## Executive Summary
This study investigates the completeness of dataset documentation across popular ML/AI repositories. The researchers created the Documentation Test Sheet (DTS), a schema based on literature recommendations, to measure documentation quality across 100 datasets from Hugging Face, Kaggle, OpenML, and UCI. Results show that only a few datasets exceeded 50% completeness, with average scores ranging from 0.21 to 0.30 across repositories. Critical information like funding sources, ethical review processes, and compensation for data workers was rarely documented, while basic metadata and usage information were more commonly present.

The findings highlight the need for improved documentation practices in the ML/AI community to ensure transparency and accountability. The study demonstrates that current dataset documentation practices are insufficient for ensuring reproducibility, fairness, and ethical compliance in ML/AI research and applications. The researchers argue that comprehensive documentation is essential for understanding dataset limitations, potential biases, and appropriate use cases.

## Method Summary
The researchers developed the Documentation Test Sheet (DTS) schema based on recommendations from the Machine Learning and Data Mining community literature. They evaluated 100 datasets (25 from each of four repositories: Hugging Face, Kaggle, OpenML, and UCI) using this schema. The evaluation measured documentation completeness across various categories including basic metadata, collection processes, data processing procedures, ethical considerations, and usage information. Each dataset was scored based on the presence of required documentation elements, with scores normalized to produce completeness percentages.

## Key Results
- Average documentation completeness ranged from 0.21 to 0.30 across repositories
- Only a few datasets exceeded 50% completeness
- Critical information about funding sources, ethical review processes, and data worker compensation was rarely documented
- Basic metadata and usage information were more commonly present than collection processes and data processing procedures

## Why This Works (Mechanism)
The DTS schema provides a systematic framework for evaluating dataset documentation completeness by mapping documentation elements to established best practices in ML/AI research. By using a standardized scoring system across multiple repositories, the study can identify consistent patterns in documentation gaps. The schema's comprehensiveness ensures that all critical aspects of dataset documentation are evaluated, from basic metadata to ethical considerations.

## Foundational Learning
- Documentation schema design: Understanding how to create effective evaluation frameworks for dataset documentation (why needed: to systematically assess quality across diverse repositories; quick check: verify schema coverage of all essential documentation elements)
- Documentation completeness metrics: Learning how to quantify and compare documentation quality (why needed: to identify gaps and measure improvement; quick check: validate scoring methodology with domain experts)
- Repository documentation practices: Understanding current state of dataset documentation across ML/AI platforms (why needed: to identify industry-wide gaps and improvement opportunities; quick check: compare documentation completeness across different repository types)

## Architecture Onboarding
Component map: DTS Schema -> Repository Sampling -> Dataset Evaluation -> Completeness Scoring -> Analysis
Critical path: Schema Development -> Dataset Selection -> Documentation Assessment -> Result Analysis
Design tradeoffs: Comprehensive vs. practical evaluation framework; subjective vs. objective scoring criteria
Failure signatures: Incomplete schema coverage, sampling bias, inconsistent evaluation criteria
First experiments:
1. Apply DTS schema to 10 additional datasets from underrepresented domains
2. Conduct inter-rater reliability test with multiple evaluators
3. Test schema applicability on non-traditional ML datasets (e.g., audio, video)

## Open Questions the Paper Calls Out
None

## Limitations
- DTS schema completeness and applicability across diverse dataset types remains uncertain
- Selection of 100 datasets may introduce sampling bias
- Subjective nature of documentation completeness assessment could introduce variability

## Confidence
Medium: While the DTS provides a systematic approach, schema completeness is uncertain. The 100 dataset sample may not represent full repository diversity. Subjective assessment nature could introduce variability in results.

## Next Checks
1. Validate the DTS schema against a broader range of dataset types (e.g., text, audio, video) to ensure its generalizability and identify any missing critical elements.
2. Conduct a follow-up study with a larger sample size (e.g., 200-300 datasets) to confirm the consistency of the findings and assess potential repository-specific variations.
3. Implement a blind re-evaluation of a subset of datasets by multiple researchers to quantify inter-rater reliability and assess the consistency of the documentation assessment process.