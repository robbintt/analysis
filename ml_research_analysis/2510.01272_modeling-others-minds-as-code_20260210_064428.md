---
ver: rpa2
title: Modeling Others' Minds as Code
arxiv_id: '2510.01272'
source_url: https://arxiv.org/abs/2510.01272
tags:
- agent
- rote
- behavior
- human
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ROTE is a novel algorithm that models human behavior as executable
  programs rather than through complex mental state inference. It uses LLMs to generate
  candidate behavioral programs from observed trajectories, then applies Bayesian
  inference to select the most likely ones.
---

# Modeling Others' Minds as Code

## Quick Facts
- **arXiv ID:** 2510.01272
- **Source URL:** https://arxiv.org/abs/2510.01272
- **Reference count:** 40
- **Primary result:** ROTE models human behavior as executable programs, outperforming behavior cloning and inverse planning baselines by up to 50% in action prediction accuracy across gridworld and robotics environments.

## Executive Summary
ROTE is a novel algorithm that models human behavior as executable programs rather than through complex mental state inference. It uses LLMs to generate candidate behavioral programs from observed trajectories, then applies Bayesian inference to select the most likely ones. ROTE outperforms behavior cloning and inverse planning baselines by up to 50% in action prediction accuracy across gridworld and embodied robotics environments. The method achieves human-level accuracy in predicting human behavior and demonstrates superior zero-shot generalization to novel environments. Code-based representations enable efficient multi-step predictions and provide interpretable decision-making processes.

## Method Summary
ROTE synthesizes Python programs that implement agent decision logic from observed trajectories. Given history h₀:t−1, an LLM generates N candidate programs λ ∈ Λ, each representing a hypothesis about the agent's decision logic. The LLM's prior over tokens provides an implicit program prior p(λ). Bayesian inference computes posterior p(λ|h₀:t−1) ∝ p(h₀:t−1|λ) · p_prior(λ), where likelihood p(h₀:t−1|λ) measures how well each program explains observed actions. Sequential Monte Carlo with rejuvenation refines the program distribution, and weighted voting over top-k programs predicts actions. The approach assumes agents follow predictable "scripts" or conventions rather than complex goal-directed planning.

## Key Results
- ROTE outperforms behavior cloning and inverse planning baselines by up to 50% in action prediction accuracy
- Achieves human-level accuracy in predicting human behavior across multiple environments
- Demonstrates superior zero-shot generalization to novel environments compared to baselines
- Efficient multi-step predictions with constant time after initial synthesis (vs. per-step regeneration for LLM baselines)

## Why This Works (Mechanism)

### Mechanism 1: Script-Based Behavioral Representation
Representing agent behavior as executable programs captures routine decision-making more efficiently than goal/belief inference. LLMs synthesize Python programs implementing state-transition logic (e.g., `if found_toy(obs): move_to_chair(obs)`), which are then executed directly to predict actions. This bypasses expensive online reasoning over goals. Core assumption: many everyday behaviors follow predictable "scripts" or conventions rather than complex goal-directed planning. Evidence: paper shows ROTE excels in environments with routine behaviors. Break condition: if agents exhibit highly adaptive, non-routine planning that cannot be compressed into finite-state logic, program synthesis will underfit.

### Mechanism 2: LLM-Guided Hypothesis Space Generation
LLMs can synthesize a distribution of plausible behavioral programs from sparse trajectory observations. Given history h₀:t−1, the LLM generates N candidate Python programs λ ∈ Λ, each representing a hypothesis about the agent's decision logic. The LLM's prior over tokens provides an implicit program prior p(λ). Core assumption: LLMs pre-trained on code contain sufficient inductive bias to propose structurally valid behavioral programs from limited examples. Evidence: ablation shows LLM choice matters—DeepSeek-Coder outperforms general-purpose models. Break condition: if observation modality cannot be converted to text (e.g., raw pixels without scene graphs), LLM synthesis fails without VLM integration.

### Mechanism 3: Sequential Monte Carlo for Program Posterior Refinement
Bayesian inference over program hypotheses enables robust uncertainty handling and out-of-distribution generalization. Compute posterior p(λ|h₀:t−1) ∝ p(h₀:t−1|λ) · p_prior(λ), where likelihood p(h₀:t−1|λ) measures how well each program explains observed actions. SMC with rejuvenation replaces low-probability programs with new samples. Core assumption: the true behavior is representable within the generated hypothesis space; noise model (parameter ϵ) accounts for stochasticity. Evidence: ROTE's multi-step prediction time scales efficiently vs. LLM baselines that regenerate per step. Break condition: if hypothesis space is too narrow or priors are misspecified, posterior will concentrate on wrong programs; SMC rejuvenation helps but cannot recover if LLM cannot generate correct hypotheses.

## Foundational Learning

- **Concept: Bayesian Inference and Sequential Monte Carlo**
  - Why needed here: ROTE relies on computing posteriors over program hypotheses; SMC enables online updating as new observations arrive.
  - Quick check question: Can you explain how p(λ|data) ∝ p(data|λ)·p(λ) is computed incrementally as new action-observation pairs arrive?

- **Concept: Program Synthesis from Examples**
  - Why needed here: The core innovation is using LLMs to generate executable code that explains observed trajectories.
  - Quick check question: Given input-output pairs [(obs₁, a₁), (obs₂, a₂)], what constraints must a synthesized program satisfy?

- **Concept: Finite State Machines as Agent Models**
  - Why needed here: ROTE assumes agents can be modeled as FSMs with internal states and deterministic transitions.
  - Quick check question: Draw an FSM for "patrol clockwise": what states and transition conditions are needed?

## Architecture Onboarding

- **Component map:** Observation Parser → LLM Program Synthesizer → Program Executor → SMC Inference Engine → Action Predictor
- **Critical path:** obs_history → LLM synthesis (N programs) → compile & filter → SMC posterior update → top-k selection → execute programs → weighted action. First prediction incurs synthesis cost (~seconds); subsequent predictions execute cached programs (~milliseconds).
- **Design tradeoffs:**
  - Structure enforcement (Light/Moderate/Severe): More structure aids FSM-like behaviors but harms goal-directed agents in partially observable settings
  - Hypothesis count N: Higher N improves accuracy but increases latency; paper uses N=30
  - Two-stage observation parsing: Helps Construction (abstracts grid details), hurts Partnr (scene graphs lose critical info)
- **Failure signatures:**
  - Non-compiling programs: LLM generates invalid syntax; handled by resampling (not revision)
  - Hallucinated states: "Severe" condition invents impossible states (e.g., "CHARGE" in Listing 6)
  - Deterministic assumption violated: If ground-truth agent is highly stochastic, noise model ϵ may be insufficient
  - Out-of-hypothesis behavior: If true behavior isn't in Λ, posterior converges to best approximation, not truth
- **First 3 experiments:**
  1. Reproduce Construction single-step prediction: Implement ROTE with DeepSeek-Coder-V2-Lite, N=30 hypotheses, Light structure. Compare accuracy vs. BC baseline on held-out FSM trajectories.
  2. Ablate observation parsing: Run with and without two-stage parsing on Partnr validation set. Expect ~10-15% accuracy drop with parsing enabled.
  3. Test generalization: Train on one Construction environment, predict in novel layout without updating program weights. Measure accuracy decay vs. AutoToM.

## Open Questions the Paper Calls Out

- Can ROTE be effectively adapted for high-dimensional, continuous control settings in real-world robotics? The current evaluation is restricted to discrete action spaces (grid movements) or high-level tool selection, avoiding the complexity of continuous motor control required for physical robotic assistance. What evidence would resolve it: Successful integration of ROTE with neural controllers and VLMs to predict and execute continuous manipulation tasks from pixel-based inputs.

- Does the structure of ROTE's inferred code align with how humans intuitively reason about others' mental states? The study benchmarks ROTE against human prediction accuracy but does not analyze the cognitive interpretability or structural similarity of the generated code compared to human explanations. What evidence would resolve it: Cognitive studies comparing the structure of ROTE's synthesized programs with human-generated explanations of the same observed behaviors.

- How can an agent dynamically select the appropriate level of structural constraint (e.g., strict FSM vs. open-ended code) for a given scenario? Currently, the level of structural enforcement is a fixed hyperparameter chosen per dataset, preventing the model from adapting its representational bias to individual agents online. What evidence would resolve it: A meta-learning framework that successfully adapts the code generation prompt strategy based on observed trajectory characteristics.

## Limitations

- Evaluation only on synthetic environments (gridworlds and simplified robotics tasks) limits generalizability to real-world human behavior complexity
- Heavy dependence on LLM capability, with DeepSeek-Coder outperforming general models, suggesting architectural dependence
- Lacks robustness analysis against adversarial or out-of-distribution behaviors that violate script-based assumptions
- Assumes behavioral scripts can be efficiently captured as finite-state programs, which may not hold for highly adaptive agents

## Confidence

- **High confidence**: SMC-based inference mechanism and program execution framework are well-established
- **Medium confidence**: Multi-step prediction accuracy claims, particularly for out-of-distribution environments
- **Medium confidence**: Zero-shot generalization performance without fine-tuning
- **Low confidence**: Real-world applicability to complex human behaviors beyond routine scripts

## Next Checks

1. **Adversarial testing**: Evaluate ROTE against agents designed to produce non-routine, unpredictable behaviors that violate script-based assumptions
2. **Cross-LLM validation**: Test ROTE with multiple different code-specialized LLMs to assess dependence on specific model architecture
3. **Real-world pilot**: Apply ROTE to human interaction datasets (e.g., household robotics demonstrations) to validate claims beyond synthetic environments