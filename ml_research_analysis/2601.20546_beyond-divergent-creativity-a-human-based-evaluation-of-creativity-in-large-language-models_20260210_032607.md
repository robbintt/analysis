---
ver: rpa2
title: 'Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large
  Language Models'
arxiv_id: '2601.20546'
source_url: https://arxiv.org/abs/2601.20546
tags:
- creativity
- novelty
- appropriateness
- cdat
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study critically evaluates whether the widely-used Divergent\
  \ Association Task (DAT) validly measures creativity in large language models (LLMs).\
  \ While DAT captures novelty through semantic distance, it ignores appropriateness\u2014\
  a key component of human creativity\u2014leading to inflated scores from random\
  \ or task-agnostic outputs."
---

# Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models

## Quick Facts
- arXiv ID: 2601.20546
- Source URL: https://arxiv.org/abs/2601.20546
- Reference count: 32
- Key outcome: This study critically evaluates whether the widely-used Divergent Association Task (DAT) validly measures creativity in large language models (LLMs). While DAT captures novelty through semantic distance, it ignores appropriateness—a key component of human creativity—leading to inflated scores from random or task-agnostic outputs. To address this, the authors introduce Conditional Divergent Association Task (CDAT), which conditions novelty on contextual appropriateness using a cue word. CDAT gates novelty evaluation on a minimal appropriateness criterion and computes creativity as mean semantic distance under this constraint. Results show that all models pass the gate, validating novelty as a creativity signal when appropriately contextualized. Notably, smaller model families generally achieve higher CDAT scores (more novelty with less appropriateness), while advanced families favor appropriateness at lower novelty. This systematic trade-off suggests that scaling, instruction tuning, and alignment shift models along the novelty-appropriateness frontier toward more conservative, less creative outputs. The findings underscore the importance of context-aware evaluation in creativity research for LLMs.

## Executive Summary
This paper challenges the validity of the Divergent Association Task (DAT) as a measure of creativity in large language models. DAT captures novelty through semantic distance but ignores appropriateness, leading to inflated scores from random or task-agnostic outputs. The authors introduce CDAT, which conditions novelty on contextual appropriateness using a cue word. By gating novelty evaluation on a minimal appropriateness criterion, CDAT provides a more rigorous assessment of creative capability. The study reveals a systematic trade-off between novelty and appropriateness across model families, suggesting that scaling and alignment shift models toward more conservative, less creative outputs.

## Method Summary
The Conditional Divergent Association Task (CDAT) evaluates LLM creativity by generating 10 nouns that are semantically diverse yet related to a cue word. Using 539 cue nouns derived from the Brown corpus, models generate responses that are filtered for validity (singular common nouns only). Novelty is computed as average pairwise semantic distance among generated words, while appropriateness measures average cosine similarity to the cue word. A statistical gate (Welch's t-test vs. Random baseline, FDR-corrected α=0.001) ensures responses meet a minimal appropriateness threshold before novelty is scored. The study tests multiple model families across three temperature settings (0.5, 1.0, 1.5) to assess temperature sensitivity.

## Key Results
- All tested models pass the appropriateness gate, validating novelty as a creativity signal when appropriately contextualized
- Smaller model families generally achieve higher CDAT scores, showing more novelty with less appropriateness
- Advanced families favor appropriateness at lower novelty, suggesting alignment shifts models toward conservative outputs
- Temperature insensitivity in highly aligned models indicates potential mode collapse from alignment-induced suppression

## Why This Works (Mechanism)

### Mechanism 1: Gated Scoring for Filtering Random Outputs
CDAT's appropriateness gate statistically separates meaningful semantic divergence from random noise before novelty is evaluated. Models must produce words with mean semantic similarity to the cue that significantly exceeds a task-agnostic random baseline (Welch's t-test, FDR-corrected α=0.001). Only models passing this gate receive a novelty score, preventing high-divergence random outputs from being misclassified as creative.

### Mechanism 2: Pareto Frontier of Novelty-Appropriateness Trade-offs
LLM families occupy characteristic positions along a novelty-appropriateness frontier, reflecting architectural and training differences rather than temperature-induced noise. Larger, instruction-tuned models exhibit higher probability mass concentrated around cue-associated semantic clusters, yielding high appropriateness but reduced between-cluster exploration. Smaller models with less alignment-induced concentration retain broader sampling distributions, enabling higher pairwise divergence but weaker cue adherence.

### Mechanism 3: Temperature-Resistant Mode Collapse in Aligned Models
Highly aligned models show flat novelty curves across temperature settings, suggesting that reduced novelty stems from constrained output distributions rather than conservative decoding. Post-training alignment (RLHF, safety fine-tuning) may induce mode collapse by suppressing low-probability semantic paths. Increasing temperature amplifies probability mass but cannot recover suppressed modes.

## Foundational Learning

- **Concept: Divergent Thinking (Guilford, 1967)**
  - **Why needed here:** CDAT operationalizes human creativity theory; without understanding that creativity requires both novelty AND appropriateness, the gate mechanism seems arbitrary.
  - **Quick check question:** Why wouldn't "generating 10 random words" be considered creative under human creativity theory?

- **Concept: Pareto Frontiers and Multi-Objective Optimization**
  - **Why needed here:** The paper frames creativity as a trade-off; understanding Pareto dominance is essential for interpreting why no model maximizes both dimensions.
  - **Quick check question:** If Model A has higher novelty but lower appropriateness than Model B, and both lie on the Pareto front, which model is "more creative"?

- **Concept: Semantic Embeddings (SBERT vs. GloVe)**
  - **Why needed here:** CDAT scores depend on embedding quality; the paper's switch from GloVe to SBERT affects what counts as "semantically distant."
  - **Quick check question:** Why might GloVe's frequency artifacts inflate novelty scores for lists containing rare words?

## Architecture Onboarding

- **Component map:** Cue selection pipeline (Brown corpus → lemmatization → GPT-4.1 nano filtering → 539 cue nouns) -> Response generation (uniform prompt, 10 nouns per cue, validity filtering) -> Embedding layer (SBERT) -> Scoring module (appropriateness gate → conditional novelty) -> Visualization (2D novelty-appropriateness plane + Pareto front)

- **Critical path:** 1) Generate responses with consistent prompting across model families and temperatures 2) Filter to first 7 valid nouns per response (POS tagging, WordNet validation) 3) Compute appropriateness; apply statistical gate 4) For passing models, compute mean novelty as CDAT score 5) Plot 2D landscape; extract Pareto front for trade-off analysis

- **Design tradeoffs:** Single-word cue constraint maximizes control and objectivity but limits ecological validity; SBERT over GloVe reduces frequency artifacts but introduces transformer-specific embedding biases; gate threshold (α=0.001) is very conservative but may be too permissive for future gaming strategies

- **Failure signatures:** High DAT but low CDAT (model produces divergent words irrelevant to cue); flat novelty across temperatures (potential mode collapse from alignment); very high appropriateness with minimal novelty (over-conservative alignment)

- **First 3 experiments:** 1) Base vs. Instruct comparison on same model family to isolate alignment effects 2) Prompt sensitivity probe ("be creative" vs. neutral instructions) 3) Cross-lingual replication in morphologically rich language (Finnish, Turkish)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific alignment objectives (e.g., safety, instruction tuning) causally induce the observed shift toward appropriateness at the expense of novelty?
- Basis in paper: The authors hypothesize that alignment shifts models along the novelty-appropriateness frontier but explicitly state this is correlational, calling for "targeted ablations on... alignment objectives" to establish causal mechanisms.
- Why unresolved: The current study compares existing model families with different sizes and training regimes, making it impossible to isolate alignment as the specific cause of reduced novelty.
- What evidence would resolve it: Controlled ablation studies on open-weight models (e.g., Llama) comparing base checkpoints against instruction-tuned or RLHF-aligned variants under matched decoding settings.

### Open Question 2
- Question: What is the optimal method for defining the "minimal appropriateness criterion" to balance noise filtering and creative freedom?
- Basis in paper: The authors state that defining a minimal level of appropriateness "remains an open question" regarding how to best flag responses for rejection.
- Why unresolved: The paper uses a strict statistical threshold (α=0.001) compared to a random baseline, but this arbitrary cutoff may exclude valid creative associations that differ significantly from the random distribution.
- What evidence would resolve it: An analysis comparing various gating strategies (e.g., human semantic checks, varying confidence intervals) against external measures of creative utility.

### Open Question 3
- Question: Do the observed trade-offs between novelty and appropriateness persist when evaluating relational or compositional cues rather than single nouns?
- Basis in paper: In "Scope Extensions," the authors suggest moving "beyond single nouns to relational or compositional cues" to test if models can diversify around structured contexts.
- Why unresolved: The current CDAT setup relies on single-word cues, which may not capture the complexity of "richer context" found in real-world creative applications.
- What evidence would resolve it: Extending the CDAT framework to prompt generation based on two-word cues or typed relations and observing if the Pareto front pattern shifts.

## Limitations
- CDAT's use of isolated nouns as cues limits ecological validity for real-world creative tasks that involve richer contexts and multi-modal inputs
- The observed trade-off may reflect embedding-space dependencies rather than true model capability differences
- The strict statistical gate may exclude valid creative associations that differ significantly from the random distribution

## Confidence
- **High Confidence:** The statistical gating mechanism effectively filters random outputs and the Pareto frontier analysis correctly identifies trade-offs between novelty and appropriateness across model families
- **Medium Confidence:** The temperature-insensitivity finding for aligned models indicates mode collapse, though the exact causal link to alignment versus other architectural factors remains uncertain
- **Low Confidence:** The claim that smaller models are "more creative" assumes novelty-appropriateness trade-offs are monotonic and universal, which may not hold for tasks requiring high appropriateness

## Next Checks
1. **Base vs. Instruct ablation:** Run CDAT on base and instruction-tuned checkpoints of the same model family to isolate alignment effects on the novelty-appropriateness frontier
2. **Prompt sensitivity probe:** Test whether prompting models to "be creative" vs. neutral instructions shifts positions along the frontier, diagnosing whether observed trade-offs reflect decoding or capability limits
3. **Cross-lingual replication:** Adapt CDAT to a morphologically rich language (e.g., Finnish, Turkish) to test whether the novelty-appropriateness trade-off is universal or embedding-space-dependent