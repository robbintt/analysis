---
ver: rpa2
title: Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion
  LLMs
arxiv_id: '2510.09885'
source_url: https://arxiv.org/abs/2510.09885
tags:
- masked
- fine-tuning
- paraphrases
- training
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares how autoregressive and diffusion language models
  learn new facts from unstructured text during fine-tuning. While autoregressive
  models require paraphrase augmentation to generalize knowledge into question-answering
  and still fail on backward questions due to the reversal curse, diffusion models
  learn both forward and backward QA effectively without paraphrases.
---

# Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs

## Quick Facts
- arXiv ID: 2510.09885
- Source URL: https://arxiv.org/abs/2510.09885
- Reference count: 40
- This paper proposes masked fine-tuning for autoregressive models to close the knowledge injection gap with diffusion models, enabling effective learning of both forward and backward question-answering pairs without paraphrase augmentation.

## Executive Summary
This paper addresses the data-efficiency gap between autoregressive language models (arLLMs) and diffusion language models (dLLMs) in knowledge injection tasks. While dLLMs naturally learn to answer both forward and backward questions from unstructured text, arLLMs struggle with backward questions due to the reversal curse and require paraphrase augmentation for effective learning. The authors propose masked fine-tuning for arLLMs, which frames knowledge injection as a document reconstruction task where random tokens are masked and the model must recover them. This approach enables arLLMs to learn both forward and backward QA pairs effectively, achieving comparable or better performance than dLLMs while maintaining the decoder-only architecture.

## Method Summary
The core contribution is masked fine-tuning for autoregressive models during knowledge injection. For each document, random tokens are masked at ratios sampled from U(0.05,0.95), and the model is prompted to reconstruct the masked passage. The loss is computed only on original unmasked tokens in the assistant response. This differs from standard supervised fine-tuning where the model simply learns to predict the next token in a sequence. The method uses FSDP2, bf16 mixed precision, batch size 64, Adam optimizer with learning rates between 3e-6 to 5e-6, and weight decay of 0.1. Inference uses max_new_tokens=128 and temperature=0.

## Key Results
- Masked fine-tuning enables autoregressive models to learn both forward and backward QA pairs effectively without requiring paraphrase augmentation
- The method mitigates the reversal curse, allowing backward question answering without explicit backward training pairs
- Masked fine-tuning achieves comparable or better accuracy than diffusion models on knowledge injection tasks while maintaining decoder-only architecture
- The approach shows improved math task performance (GSM8K) over standard supervised fine-tuning

## Why This Works (Mechanism)
The masked fine-tuning approach works by forcing the autoregressive model to reconstruct the full document context rather than simply predicting the next token in a sequence. By masking random tokens and requiring reconstruction, the model must understand the document holistically and cannot rely on memorizing token order. This holistic understanding enables the model to answer both forward and backward questions, as it has learned the bidirectional relationships within the text. The method effectively transforms the knowledge injection task from a sequence prediction problem into a denoising reconstruction problem, which better captures the bidirectional nature of factual knowledge.

## Foundational Learning
- **Knowledge Injection**: Teaching LLMs new facts from unstructured text; needed because base models have limited knowledge; quick check: test forward/backward QA accuracy before/after fine-tuning
- **Reversal Curse**: Autoregressive models struggle to answer backward questions (Q: "What is X's description?" after learning "X is Y") due to unidirectional training; needed to explain arLLM limitations; quick check: compare forward vs backward accuracy on symmetric QA pairs
- **Paraphrase Augmentation**: Generating multiple phrasings of the same fact to improve generalization; needed for arLLMs but not dLLMs; quick check: measure accuracy improvement with vs without paraphrases
- **ROUGE-1 Score**: Measures n-gram overlap between generated and reference answers; needed for quantitative evaluation; quick check: verify tokenization and normalization consistency
- **Diffusion Language Models**: Generate text by iteratively denoising corrupted versions; needed as comparison baseline; quick check: compare arLLM vs dLLM performance on same tasks
- **Autoregressive Language Models**: Generate text by predicting next token conditioned on previous tokens; needed as primary model class; quick check: verify unidirectional training effects

## Architecture Onboarding

**Component Map**: Dataset (NameDescription/Biography/Wiki) -> Masked Fine-tuning (random mask sampling, reconstruction prompt) -> Autoregressive Loss (on original tokens) -> Model Updates -> Evaluation (ROUGE-1, forward/backward accuracy)

**Critical Path**: The critical path is from mask sampling through reconstruction prompt to autoregressive loss computation. The random mask sampling U(0.05,0.95) is essential as it prevents the model from learning trivial patterns. The reconstruction prompt must clearly indicate which tokens are masked and request their recovery. The loss computation must exclude prompt tokens and only supervise original unmasked positions.

**Design Tradeoffs**: Masked fine-tuning trades computational efficiency (requires mask sampling and reconstruction objectives) for improved bidirectional learning capability. Standard fine-tuning is faster but requires paraphrase augmentation and still fails on backward questions. The mask ratio sampling adds stochasticity but improves robustness compared to fixed ratios.

**Failure Signatures**: 
- If backward accuracy remains near zero while training loss drops, the model is memorizing sequence order rather than learning bidirectional relationships
- If performance collapses at t=0 (no masking), the masking logic is correctly applied but insufficiently challenging
- If loss includes prompt tokens, the model learns to copy rather than reconstruct

**First Experiments**:
1. Test masked fine-tuning with fixed mask ratios (0.5, 0.75) to verify the U(0.05,0.95) sampling isn't critical
2. Compare standard fine-tuning with and without paraphrases on backward QA to confirm the reversal curse
3. Implement masked fine-tuning with debug logging to verify mask sampling, prompt construction, and loss masking are correct

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on factual knowledge injection into instruct-tuned decoder-only models, with unexplored effectiveness for other fine-tuning objectives
- The computational overhead of masked fine-tuning versus standard fine-tuning is not explicitly quantified
- The comparison with dLLMs is limited to a single model (LLaDA-8B-Instruct), making generalization claims about dLLM superiority tentative
- Evaluation relies heavily on ROUGE-1 overlap, which may not fully capture semantic correctness in all cases

## Confidence

**High confidence**: The effectiveness of masked fine-tuning for improving backward question answering and reducing paraphrase dependence in autoregressive models. The experimental methodology is clearly described and the results are consistent across multiple datasets.

**Medium confidence**: The claim that masked fine-tuning achieves comparable performance to dLLMs. While the results are promising, the comparison is limited to one dLLM model and the evaluation protocol differs slightly between model types.

**Medium confidence**: The assertion that masked fine-tuning improves math task performance. This is demonstrated on a single math dataset (GSM8K) and the improvement, while consistent, is relatively modest.

## Next Checks
1. Test masked fine-tuning on additional downstream tasks beyond knowledge injection, including instruction following and reasoning benchmarks, to assess generalizability of the approach
2. Conduct a controlled study comparing the wall-clock time and memory overhead of masked fine-tuning versus standard fine-tuning across different model sizes to quantify computational costs
3. Validate the backward question answering improvements using alternative evaluation metrics (e.g., exact match, semantic similarity measures) beyond ROUGE-1 to ensure the gains reflect true understanding rather than surface-level text matching