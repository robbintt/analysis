---
ver: rpa2
title: 'Safe, Untrusted, "Proof-Carrying" AI Agents: toward the agentic lakehouse'
arxiv_id: '2510.09567'
source_url: https://arxiv.org/abs/2510.09567
tags:
- data
- lakehouse
- agents
- code
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of safely deploying AI agents
  to manage data pipelines in cloud-based data lakehouses, where concerns around trust,
  correctness, and governance are paramount. The authors propose using a programmable,
  API-first lakehouse architecture that exposes the entire data lifecycle through
  code abstractions, enabling safe-by-design agentic workflows.
---

# Safe, Untrusted, "Proof-Carrying" AI Agents: toward the agentic lakehouse

## Quick Facts
- arXiv ID: 2510.09567
- Source URL: https://arxiv.org/abs/2510.09567
- Reference count: 25
- One-line primary result: AI agents can safely repair broken data pipelines in cloud lakehouses using proof-carrying verification and branch-then-merge isolation

## Executive Summary
This paper addresses the challenge of safely deploying AI agents to manage data pipelines in cloud-based data lakehouses, where concerns around trust, correctness, and governance are paramount. The authors propose using a programmable, API-first lakehouse architecture that exposes the entire data lifecycle through code abstractions, enabling safe-by-design agentic workflows. Their core method involves implementing transactional pipeline execution with Git-for-Data concepts, declarative environments, and a "proof-carrying" safety mechanism where agents must pass deterministic correctness checks before merging changes to production.

## Method Summary
The method combines Git-for-Data transactional semantics with proof-carrying verification to enable untrusted AI agents to safely repair broken data pipelines. The approach uses a programmable lakehouse (Bauplan) with a MCP server exposing tools for observability, data exploration, and execution. Agents operate through a ReAct loop, creating isolated debug branches from production data, proposing and testing fixes, then running a deterministic verifier function before any changes can merge to production. The system uses typed code APIs as the lingua franca for agent-tool interactions, maintaining safety through branch isolation and human-defined verification criteria.

## Key Results
- Agents successfully repaired broken pipelines (NumPy 2.0 / pandas mismatch) without corrupting production data
- The branch-then-merge isolation prevented partial or failed writes from polluting downstream consumers
- Proof-carrying verification enabled "untrusted" agents to produce trusted results through deterministic correctness checks

## Why This Works (Mechanism)

### Mechanism 1: Branch-Then-Merge Isolation
Sandboxing agent writes in isolated data branches prevents production corruption even when agents fail. Copy-on-write branching creates isolated execution environments; changes materialize atomically to production only upon successful merge, preventing partial or failed writes from polluting downstream consumers. Core assumption: The lakehouse maintains transactional consistency across the branch-merge lifecycle and downstream systems read only from designated production branches.

### Mechanism 2: Proof-Carrying Verification
Deterministic verifier functions (`Branch → bool`) act as safety gates that validate agent outputs before production merge, enabling "untrusted" agents to produce trusted results. A human-defined verifier inspects the agent's materialized branch outputs against business correctness constraints; only branches passing verification can merge, creating a human-in-the-loop safety layer without requiring trust in the agent itself. Core assumption: Verifiers can capture the essential correctness properties for the business context and are themselves correct and tamper-proof.

### Mechanism 3: Code-as-Lingua-Franca Abstraction
Exposing the entire data lifecycle through typed, documented code APIs enables reliable agent tool-use across pipeline definition, execution, and observability. Programmable lakehouses unify data, infrastructure, and runtime observability behind consistent code abstractions (decorators, functions, CLI); agents interact via typed MCP-wrapped tool calls, eliminating context-switching and reducing the attack surface to a single, auditable interface. Core assumption: LLMs can reliably generate correct code against the provided API signatures and that the API surface is both complete enough for agentic tasks and constrained enough to prevent dangerous operations.

## Foundational Learning

- **Concept: Git-for-Data (MVCC for Lakehouses)**
  - Why needed here: Understanding how database-style transactional semantics map to data lakehouse architectures via branching and merging.
  - Quick check question: Can you explain why a failed pipeline run should never leave partial tables visible to downstream consumers?

- **Concept: Proof-Carrying Code (PCC)**
  - Why needed here: The paper adapts this formal verification concept—originally for untrusted code—to validate AI agent outputs against business rules.
  - Quick check question: What's the difference between "trusting an agent" and "trusting the verification of an agent's output"?

- **Concept: ReAct Agent Loop**
  - Why needed here: The prototype uses this reasoning-and-acting pattern; understanding it clarifies how agents iteratively debug pipelines.
  - Quick check question: In a ReAct loop, what triggers the next tool call after an observation?

## Architecture Onboarding

- **Component map:**
  - LLM Backend (LiteLLM) -> MCP Server -> Programmable Lakehouse (Bauplan) -> ReAct Framework (smolagents) -> Verifier Function

- **Critical path:**
  1. Agent detects failed pipeline → retrieves logs via MCP tools
  2. Agent creates debug branch from production data
  3. Agent proposes code fix and runs pipeline in sandbox
  4. Verifier checks branch outputs; if passing, merge to production

- **Design tradeoffs:**
  - Safety vs. autonomy: Removing merge permissions from agents increases safety but requires human approval latency
  - Expressiveness vs. attack surface: More API tools enable richer agent behavior but expand potential misuse vectors
  - Verifier strictness: Tighter constraints catch more errors but may reject valid edge-case solutions

- **Failure signatures:**
  - Agent infinite loops (same fix attempted repeatedly)
  - Verifier false positives/negatives (accepting bad outputs or rejecting good ones)
  - Branch proliferation (many abandoned debug branches consuming storage)
  - Model-specific failures (e.g., "GPT-5-mini" had lower success rates per the paper)

- **First 3 experiments:**
  1. Reproduce the demo failure (NumPy 2.0 / pandas mismatch) and observe agent repair loop
  2. Implement a custom verifier with intentional edge cases to test verification robustness
  3. Swap LLM backends to compare success rates and token usage on identical repair tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed architecture maintain performance and safety guarantees under massive parallelism during concurrent multi-agent operations?
- Basis in paper: [explicit] The conclusion identifies "massive parallelism" as "outside the scope of our prototype" but "arguably the primary challenge" for future agentic OLAP systems.
- Why unresolved: The prototype focused on single-agent pipeline repair feasibility rather than high-concurrency stress testing.
- What evidence would resolve it: Benchmarks of throughput and latency under high concurrent agent load.

### Open Question 2
- Question: What are the quantitative performance differences (success rates, token usage) between frontier models in the proposed self-repairing loop?
- Basis in paper: [explicit] Section IV.B notes that "frontier model performance vary greatly" and that a "thorough exploration of this experimental space is beyond scope."
- Why unresolved: The paper serves as a proof-of-concept for safety abstractions, not a comparative model benchmark.
- What evidence would resolve it: Standardized benchmarks across different LLMs on the pipeline repair task.

### Open Question 3
- Question: How robust is the system when the human-defined "verifier" functions are incomplete or brittle against edge cases?
- Basis in paper: [inferred] The safety mechanism relies on a `Branch -> bool` verifier specified by humans; the paper assumes this function correctly captures all safety requirements.
- Why unresolved: The system guarantees the agent passes the check, but does not verify if the check itself is sufficient to prevent subtle bugs.
- What evidence would resolve it: Analysis of failure modes where agents satisfy the verifier while introducing semantic errors into the data.

## Limitations

- The paper demonstrates a proof-of-concept but does not provide quantitative safety metrics or systematic evaluation across diverse failure modes
- The verification mechanism's completeness is unproven—the verifier function shown appears simple and may not capture complex business rules or edge cases
- The agent's reasoning capabilities depend heavily on prompt engineering and tool quality, neither of which are fully specified

## Confidence

- **High confidence**: The branch-then-merge isolation mechanism works as described (fundamental MVCC principle applied to lakehouses)
- **Medium confidence**: The proof-carrying verification concept is valid and creates meaningful safety boundaries, though verifier completeness is unverified
- **Medium confidence**: Code-as-lingua-franca abstraction enables agent tool use, though performance across diverse pipeline complexities is unknown
- **Low confidence**: The agent's success rate and robustness across varied failure scenarios (beyond the single pandas/NumPy example)

## Next Checks

1. Implement and test a comprehensive verifier with edge cases, invalid schemas, and subtle correctness violations to evaluate verification robustness
2. Run the agentic repair system across 10+ diverse pipeline failure scenarios (dependency conflicts, logic errors, data quality issues) to measure success rate and identify failure patterns
3. Benchmark the performance and storage overhead of the branch-then-merge approach with realistic workloads and multiple concurrent agent operations