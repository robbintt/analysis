---
ver: rpa2
title: 'PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language
  Models'
arxiv_id: '2506.14808'
source_url: https://arxiv.org/abs/2506.14808
tags:
- prompt
- variations
- which
- reliability
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARC is a framework for analyzing prompt sensitivity in vision-language
  models (VLMs) by systematically varying both language and vision components of prompts.
  It introduces 11 realistic prompt variations including reformulations (same expected
  answer) and semantic changes (changed expected answer), a novel reliability score
  that combines accuracy and confidence with explicit guarantees, and a principled
  calibration step for cross-dataset comparisons.
---

# PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models

## Quick Facts
- **arXiv ID:** 2506.14808
- **Source URL:** https://arxiv.org/abs/2506.14808
- **Reference count:** 40
- **Primary result:** PARC is a framework for analyzing prompt sensitivity in vision-language models (VLMs) by systematically varying both language and vision components of prompts.

## Executive Summary
PARC introduces a quantitative framework to systematically analyze prompt sensitivity in vision-language models by varying both language and vision components. The framework defines 11 realistic prompt variations spanning reformulations and semantic changes, introduces a novel reliability score that combines accuracy and confidence with explicit guarantees, and implements principled calibration for cross-dataset comparisons. Evaluating 22 VLMs across 7 datasets, PARC reveals that VLMs inherit language sensitivity in the vision domain and are most vulnerable to semantic variations like negations, with model family being a stronger predictor of robustness than size.

## Method Summary
PARC evaluates VLMs on multiple-choice visual question answering tasks by systematically generating prompt variations using LLaMA3-70B for language changes and standard image processing for vision changes. The framework computes calibrated accuracy, certainty via conformal prediction, consistency across prompt pairs, and a reliability score that provides explicit performance guarantees. All metrics are normalized against random baseline performance to enable cross-dataset comparison. The method requires white-box access to model logits and is currently limited to MC-VQA format.

## Key Results
- VLMs are more sensitive to semantic changes (which alter expected answers) than to reformulations (which preserve answers)
- Model family predicts robustness better than model size, with InternVL2 models showing highest reliability
- Negations and other semantic variations are most destructive to VLM performance
- Training data quality appears crucial for reducing prompt sensitivity

## Why This Works (Mechanism)

### Mechanism 1: Baseline-Relative Calibration
If evaluation metrics are not calibrated against expected random performance, comparisons across datasets with different option counts are misleading. The framework rescales raw scores relative to $s_{rand}$ using the formula $s_{calib} = \frac{s-s_{rand}}{1-s_{rand}}$ (for $s \ge s_{rand}$). This normalizes performance relative to a random baseline, ensuring that a 70% accuracy on a binary task isn't conflated with 70% on a task with more options.

### Mechanism 2: Reliability Score Decomposition
Combining accuracy and certainty into a single "reliability" score with signed values allows for the rapid identification of "confidently wrong" predictions. The score $rel = (2 \cdot acc - 1) \cdot cert$ maps performance to $[-1, 1]$. A positive score guarantees a minimum accuracy improvement over random with minimum certainty, while a negative score flags confident errors (high certainty, low accuracy).

### Mechanism 3: Semantic vs. Reformulation Perturbation
VLMs exhibit higher sensitivity to semantic changes (which alter the expected answer) than to reformulations (which preserve the answer), indicating a reliance on surface-level correlations rather than grounded logic. By separating variations into those that change the "ground truth" vs. those that don't, the framework isolates logical brittleness from sensory noise robustness.

## Foundational Learning

- **Concept: Conformal Prediction**
  - **Why needed here:** Used to calculate the "certainty" score (size of the prediction set) which feeds into the reliability metric.
  - **Quick check question:** Can you explain how a conformal prediction set size of 1 differs from a size of 4 in terms of model confidence?

- **Concept: Multiple-Choice Visual Question Answering (MC-VQA)**
  - **Why needed here:** The entire framework relies on discrete answer sets to define "expected random performance" and calculate accuracy.
  - **Quick check question:** Why can't this specific calibration method be directly applied to open-ended generative tasks without an additional LLM scorer?

- **Concept: Model Robustness vs. Accuracy**
  - **Why needed here:** The paper argues that high accuracy on standard benchmarks does not imply robustness to prompt variations (reliability).
  - **Quick check question:** If a model has 90% accuracy but flips its answer when "not" is added to the prompt, is it "reliable" in the PARC definition?

## Architecture Onboarding

- **Component map:** VLMs (logits access) + Standard VQA Datasets (cleaned) -> LLaMA-3-70B (Text Variations) + Image Processing (Vision Variations) -> Conformal Prediction (Certainty) + Logit Analysis (Accuracy) -> Calibration Module -> Reliability Score

- **Critical path:** The **Calibration Step** is the bottleneck for comparability; without it, cross-dataset analysis fails (as seen in the MMBench vs. NYU-Depth example).

- **Design tradeoffs:**
  - **White-box access:** Requires logit access for certainty; cannot run on black-box APIs without modification.
  - **Dataset Limitation:** Strictly requires MC-VQA format; generative tasks require an additional LLM judge (injected noise).

- **Failure signatures:**
  - **Negative Reliability:** Model is confidently wrong (hallucinating or ignoring negations).
  - **High Accuracy / Low Consistency:** Model succeeds on the original prompt but fails on reformulations (surface-level overfitting).

- **First 3 experiments:**
  1. **Reproduce Calibration Effect:** Run a VLM on MMBench with and without calibration to verify the "flip" in performance order between Original and Negated prompts shown in Figure 3.
  2. **Semantic Negation Stress Test:** Apply only the LS-N (Language Semantic Negation) variation to your target VLM to check for the "negation blindness" observed in the paper.
  3. **Family vs. Size Ablation:** Compare reliability scores of two models from the same family (e.g., InternVL2-8B vs. 40B) to validate if family is indeed a stronger predictor of robustness than size.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the quality of training data curation causally reduce prompt sensitivity, independent of model architecture?
- **Basis in paper:** [explicit] The authors find "indications that prompt sensitivity is linked to training data" and identify "training data protocols as path to alleviate excessive sensitivity," noting that InternVL2's robustness may stem from diverse, curated training data.
- **Why unresolved:** The current study is observational and comparative across existing model families; it cannot isolate training data effects from architectural differences (e.g., attention mechanisms, resolution).
- **What evidence would resolve it:** A controlled ablation study training identical architectures on datasets differing only in curation levels (e.g., raw web-crawled vs. curated) and measuring the resulting PARC reliability scores.

### Open Question 2
- **Question:** How can prompt sensitivity be reliably measured in black-box VLMs where internal confidence scores are unavailable?
- **Basis in paper:** [explicit] The limitations section states the framework is "aimed at white box VLMs for which we can calculate certainty," noting that applying similar methods to black-box models is "prohibitively expensive" or requires different metrics.
- **Why unresolved:** PARC's reliability score depends on certainty derived from conformal prediction on softmax scores, which are inaccessible in black-box API settings.
- **What evidence would resolve it:** The development of a proxy for certainty (e.g., verbalized confidence or consistency-based metrics) that correlates strongly with the white-box PARC reliability score on the same models.

### Open Question 3
- **Question:** How can the PARC framework be adapted for open-ended generative tasks without introducing the noise and bias of external LLM evaluators?
- **Basis in paper:** [explicit] The authors restrict the framework to Multiple-Choice VQA because generative tasks "require additional LLMs to validate the tested model's responses, injecting more noise and bias."
- **Why unresolved:** Semantic variations in PARC require tracking discrete changes in the "expected answer," which is trivial in multiple-choice but ambiguous in free-form generation.
- **What evidence would resolve it:** A calibration protocol or metric for generative models that handles semantic prompt variations (e.g., negations) by defining "correctness" relative to the prompt's meaning without an external judge.

## Limitations
- The framework requires white-box access to VLMs for confidence score computation, limiting applicability to black-box APIs.
- Current implementation is restricted to MC-VQA tasks and cannot directly handle open-ended generative responses without additional LLM scoring.
- LLaMA-3-70B-generated variations may not capture all possible prompt sensitivities that human-generated variations would reveal.

## Confidence
- **High confidence** in the calibration mechanism and reliability score formulation, as these are mathematically rigorous and well-justified.
- **Medium confidence** in the generalizability of specific sensitivity rankings (e.g., "negations are most destructive") across different model families and domains.
- **Medium confidence** in the claim that model family predicts robustness better than size, as this requires extensive cross-family comparisons.

## Next Checks
1. **Cross-Domain Robustness Test:** Apply PARC to a non-VQA VLM (e.g., a captioning or grounding model) to verify if sensitivity patterns hold across vision-language tasks beyond multiple-choice question answering.

2. **Human Variation Benchmark:** Compare LLaMA-3-70B-generated variations against human-authored prompt perturbations on a subset of data to quantify potential gaps in variation coverage and assess whether the automated approach misses critical sensitivities.

3. **Architectural Ablation Study:** Systematically disable components of the framework (e.g., calibration, certainty scoring) on a single model-dataset pair to isolate their individual contributions to the reliability score and validate the additive value of each mechanism.