---
ver: rpa2
title: 'Drop-Muon: Update Less, Converge Faster'
arxiv_id: '2510.02239'
source_url: https://arxiv.org/abs/2510.02239
tags:
- cost
- page
- update
- faster
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Drop-Muon introduces a new approach to deep learning optimization
  by selectively updating only a subset of layers at each training step, rather than
  the entire network as is standard practice. The method uses randomized layer sampling
  with non-Euclidean updates, combining the efficiency of progressive training with
  layer-specific optimization.
---

# Drop-Muon: Update Less, Converge Faster

## Quick Facts
- arXiv ID: 2510.02239
- Source URL: https://arxiv.org/abs/2510.02239
- Reference count: 40
- Key outcome: Drop-Muon achieves up to 1.4× wall-clock speedup over full-network Muon on 3-layer CNNs for MNIST, Fashion-MNIST, and CIFAR-10 by selectively updating subsets of layers per iteration

## Executive Summary
Drop-Muon introduces a novel optimization approach for deep learning that selectively updates only a subset of network layers at each training step, rather than the entire network as standard practice. The method combines randomized layer sampling with non-Euclidean updates, providing both theoretical convergence guarantees and practical speedups. By exploiting heterogeneous layer-wise smoothness properties, Drop-Muon achieves faster wall-clock convergence through reduced per-iteration cost while maintaining the same accuracy as full-network updates. The approach challenges conventional wisdom about always updating all layers and offers a theoretically grounded alternative for more efficient large-scale model training.

## Method Summary
Drop-Muon extends Muon-style non-Euclidean optimization to selectively update subsets of neural network layers per iteration. The method uses Randomized Progressive Training (RPT) to sample a minimal active layer index, then updates all layers from that index through the output. This exploits backpropagation mechanics where gradients are computed sequentially from the last layer backward, allowing gradients for deeper layers to be computed "for free" when truncating at the first active layer. Each layer's update uses a sharp operator computed with respect to a layer-specific norm (spectral norm for hidden layers), performing steepest descent in the geometry defined by that norm rather than treating all gradient components equally. The approach maintains rigorous convergence guarantees under both layer-wise smoothness and layer-wise (L0, L1)-smoothness assumptions.

## Key Results
- Drop-Muon consistently achieves 1.2-1.4× wall-clock speedup over full-network Muon on 3-layer CNNs for MNIST, Fashion-MNIST, and CIFAR-10
- The method maintains the same accuracy as full-network updates while using fewer parameter updates per iteration
- Cost analysis reveals full-network updates are only optimal under a very specific condition on layer smoothness constants that is unlikely to hold in practice
- Randomized Progressive Training (RPT) aligns layer sampling with backpropagation mechanics to avoid redundant gradient computation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selective layer updates can achieve faster wall-clock convergence than full-network updates under realistic layer smoothness conditions.
- **Mechanism:** Drop-Muon exploits heterogeneous layer-wise smoothness properties. Since different layers have different curvature characteristics, updating all layers with a uniform step size is constrained by the most sensitive layer. By sampling and updating only a subset of layers per iteration (particularly deeper layers in progressive training), each update uses larger effective stepsizes for the active layers, trading more iterations for lower per-iteration cost (partial backpropagation).
- **Core assumption:** Layer smoothness constants vary across layers and across layer subsets; specifically, the condition L₁,{1,...,b} = max_{i∈[b]} L_i,{1,...,b} (where the first layer has the maximum smoothness) is unlikely to hold in practice.
- **Evidence anchors:**
  - [abstract] "Our cost analysis further reveals that full-network updates are not optimal unless a very specific relationship between layer smoothness constants holds."
  - [Section 4.1.1, Theorem 4.3] "The cost (8) is minimized by (p₁, p₂, ..., p_b) = (1, 0, ..., 0) if and only if L₁,{1,...,b} = max_{i∈[b]} L_i,{1,...,b}."
  - [Section 6] Experiments show 1.2-1.4× wall-clock speedup on MNIST, Fashion-MNIST, CIFAR-10 CNNs.
- **Break condition:** If all layers have similar smoothness constants or if the first layer consistently has the highest smoothness (contrary to empirical observation), selective updates offer no advantage.

### Mechanism 2
- **Claim:** Randomized Progressive Training (RPT) aligns layer sampling with backpropagation mechanics to avoid redundant gradient computation.
- **Mechanism:** Backpropagation computes gradients sequentially from the last layer backward. If the minimum sampled layer index is s_k, gradients for layers [s_k, ..., b] are computed "for free" during backprop to layer s_k. RPT samples a starting layer s_k from distribution {p_i} and updates all layers from s_k onward, exploiting this dependency rather than computing gradients for arbitrary disconnected subsets (which would require multiple forward passes or inefficient gradient masking).
- **Core assumption:** Backpropagation cost dominates forward pass and activation caching overhead; the computational benefit of truncated backprop outweighs any increased variance from sampling.
- **Evidence anchors:**
  - [Section 3] "Since layers 1, ..., s_k−1 are frozen, no gradients are computed for them, effectively truncating the gradient flow at the first active layer."
  - [Section 4] "computing the gradient ∇_{s_k} f(X^k) requires backpropagating from the last layer b up to layer s_k, which automatically produces all gradient components [∇_{s_k} f(X^k), ..., ∇_b f(X^k)]."
  - [corpus] Limited direct corpus support; conceptually related to "Dynamic Gradient Sparse Update for Edge Training" (sparse gradient computation) but specific RPT-backprop alignment is novel to this work.
- **Break condition:** If gradient computation cost is not the bottleneck (e.g., dominated by activation storage or forward passes), or if architecture prevents caching activations between iterations, efficiency gains diminish.

### Mechanism 3
- **Claim:** Non-Euclidean updates (spectral norm steepest descent) per layer improve convergence on anisotropic loss landscapes compared to standard SGD-style updates.
- **Mechanism:** Each layer's update uses the "sharp operator" M^♯ = argmax_X {⟨M, X⟩ − ½‖X‖²} computed with respect to a layer-specific norm (spectral norm ‖·‖₂→₂ for hidden layers). This performs steepest descent in the geometry defined by that norm, aligning updates with the dominant singular direction of the momentum matrix rather than treating all gradient components equally. For spectral norm, this is equivalent to orthogonalizing the update direction.
- **Core assumption:** Loss landscape anisotropy across layers is better captured by layer-specific norms than a global Euclidean metric; the Newton-Schulz approximation for spectral orthogonalization is sufficiently accurate.
- **Evidence anchors:**
  - [Section 1] "enabling better alignment with the highly anisotropic loss landscapes of neural networks"
  - [Section A.1] "Muon moves in the direction of steepest descent measured in the spectral norm... the update is obtained by solving the constrained optimization problem min_{ΔX_i} ⟨G_i, ΔX_i⟩ subject to ‖ΔX_i‖₂→₂ ≤ t_i"
  - [corpus] Related to broader LMO-based optimization literature (Muon, Scion, Gluon) but layer-wise selective application is the novel contribution.
- **Break condition:** If loss landscapes are approximately isotropic, or if Newton-Schulz iterations introduce significant approximation error, non-Euclidean updates may not outperform standard Euclidean SGD.

## Foundational Learning

- **Concept:** Randomized Block Coordinate Descent (RBCD) theory
  - **Why needed here:** Drop-Muon extends RBCD from coordinate blocks to neural network layers with non-Euclidean geometry and stochastic gradients. Understanding RBCD convergence rates (O(1/K) for smooth deterministic, O(1/√K) for stochastic) provides intuition for why selective updates can converge despite updating fewer parameters.
  - **Quick check question:** In standard RBCD with serial sampling, how does the stepsize for block i relate to its block-wise smoothness constant L_i, and why can this be larger than the full-gradient descent stepsize 1/L?

- **Concept:** (L₀, L₁)-smoothness (generalized smoothness)
  - **Why needed here:** The paper's strongest theoretical results (Theorem 4.2, 4.4) assume layer-wise (L₀, L₁)-smoothness, where local curvature depends on gradient magnitude. This is more realistic for deep learning than classical L-smoothness and explains why adaptive stepsizes are critical.
  - **Quick check question:** What does the condition "‖∇f(X+Γ) − ∇f(X)‖ ≤ (L₀ + L₁‖∇f(X)‖)‖Γ‖" imply about gradient behavior near steep cliffs versus flat regions?

- **Concept:** Linear Minimization Oracle (LMO) and steepest descent in arbitrary norms
  - **Why needed here:** Drop-Muon's core update rule uses LMO over norm balls (Equation 1). Understanding that LMO_B(X,t)(M) finds the point minimizing ⟨M, Z⟩ subject to ‖Z−X‖ ≤ t is essential for grasping why Muon-style updates differ from SGD.
  - **Quick check question:** For the spectral norm ball B₂→₂(0, t), what is the solution to min_Z ⟨G, Z⟩ subject to ‖Z‖₂→₂ ≤ t, and how does it relate to the SVD of G?

## Architecture Onboarding

- **Component map:**
  - Sampling module -> Backprop controller -> Momentum buffer -> Sharp operator/LMO solver -> Stepsize scheduler

- **Critical path:**
  1. Sample S_k → 2. Forward pass (cache activations) → 3. Truncated backprop for gradients in S_k → 4. Update momentum for active layers → 5. Compute sharp operators → 6. Apply parameter updates → 7. Repeat

- **Design tradeoffs:**
  - **Sampling distribution:** Uniform is simplest; epoch-shift dynamically adjusts from shallow to deep layers over training (Section G.2). Aggressive deep-layer bias speeds later-stage convergence but may hurt early feature learning.
  - **τ in τ-submodel:** Larger τ means more layers updated per step (higher per-iteration cost, lower variance). τ=1 is most aggressive subsampling; τ=b recovers full Muon.
  - **Newton-Schulz iterations:** More iterations = more accurate orthogonalization but higher cost. Paper uses 5; fewer may suffice for small layers.
  - **Learning rate scaling:** Theory suggests Drop-Muon can use larger stepsizes than full-network Muon due to layer-specific smoothness (Theorem 4.1, 4.2), but experiments used identical LR for fair comparison.

- **Failure signatures:**
  - **Slow convergence per epoch but not wall-clock speedup:** Sampling distribution too aggressive (updating too few layers), or smoothness constants poorly estimated.
  - **High variance across seeds:** Sampling instability; consider reducing sampling variance (increase τ) or use more stable distributions like epoch-shift.
  - **No benefit on small models:** Overhead from sampling/conditional logic dominates; gradient computation not the bottleneck.
  - **Divergence:** Stepsizes too large for layer-specific smoothness; need better L₀, L₁ estimates or adaptive LR.

- **First 3 experiments:**
  1. **Baseline reproduction:** Replicate MNIST/Fashion-MNIST experiments (3-layer CNN, uniform sampling, batch 8192, LR 0.1) to validate 1.2-1.4× speedup. Measure both per-epoch accuracy and wall-clock time.
  2. **Ablation on sampling distribution:** Compare uniform vs. epoch-shift vs. τ-submodel (τ∈{1,2,3}) on same architecture. Plot wall-clock time to 90% train accuracy to identify optimal strategy for your hardware/model.
  3. **Sensitivity to model depth/width:** Test on deeper CNNs (6-12 layers) or larger channel counts to verify if speedup scales with model size (where backprop cost increasingly dominates). Monitor for variance increase and adjust sampling accordingly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the theoretical condition for optimal full-network updates (first layer having the largest smoothness constant) hold across diverse architectures and datasets?
- **Basis in paper:** [explicit] "While broader validation is needed, there is no reason to expect this phenomenon to hold broadly" (Section 4.1.1, page 7).
- **Why unresolved:** Authors validated this only on NanoGPT experiments; systematic empirical verification across broader model types is lacking.
- **What evidence would resolve it:** Empirical measurements of layer-wise smoothness constants across diverse architectures (CNNs, Transformers, MLPs) and datasets to verify whether the condition L1,{1,...,b} = max_i L1,i,{1,...,b} ever naturally occurs.

### Open Question 2
- **Question:** Can adaptive sampling distributions that learn online during training outperform fixed sampling strategies?
- **Basis in paper:** [explicit] "A promising future direction is to learn the layer sampling distribution online, so that it automatically adapts to the training dynamics of a given dataset and architecture" (Section G.7, page 65).
- **Why unresolved:** Current paper only evaluates uniform and epoch-shift distributions; no adaptive/learned sampling strategies were investigated.
- **What evidence would resolve it:** Experiments comparing fixed distributions against gradient-based or reinforcement learning methods that dynamically adjust layer sampling probabilities during training.

### Open Question 3
- **Question:** How does Drop-Muon scale to large language models and modern transformer architectures used in practice?
- **Basis in paper:** [inferred] Experiments limited to 3-layer CNNs on MNIST/Fashion-MNIST/CIFAR-10, despite paper motivation being "large-scale model training" and theoretical focus on LLM optimizers (Muon, Scion, Gluon).
- **Why unresolved:** No empirical validation on architectures where the method would have greatest practical impact.
- **What evidence would resolve it:** Training experiments on transformer-based models (e.g., GPT-style architectures, ViTs) with comparison to baseline Muon on perplexity/accuracy metrics and wall-clock time.

### Open Question 4
- **Question:** How significant are the potential speedups from using Drop-Muon-specific learning rate schedules versus sharing hyperparameters with baseline Muon?
- **Basis in paper:** [explicit] "The observed speed-up over Muon could be even greater with dedicated tuning... theory suggests that Drop-Muon can safely employ larger stepsizes than the full-network Muon baseline" (Section 6, page 9).
- **Why unresolved:** Authors deliberately used identical learning rates to isolate the effect of partial updates; method-specific tuning remains unexplored.
- **What evidence would resolve it:** Ablation studies comparing shared vs. independently tuned learning rates, including adaptive schedules that exploit layer-wise stepsize bounds from Theorems 4.1, 4.2, and 4.4.

## Limitations
- Drop-Muon's wall-clock speedup relies on the assumption that backprop gradient computation dominates forward pass and activation caching overhead; if activation storage becomes the bottleneck, efficiency gains diminish.
- The theoretical results assume access to accurate layer-wise smoothness constants, which may be difficult to estimate in practice.
- Experiments are limited to small 3-layer CNNs on MNIST, Fashion-MNIST, and CIFAR-10; benefits may not extend to transformers, recurrent networks, or tasks requiring very deep feature hierarchies.

## Confidence
- **High confidence:** The theoretical analysis of cost-minimizing layer sampling distributions (Theorem 4.3) and the convergence guarantees under layer-wise smoothness assumptions (Theorems 4.1-4.4). These results are mathematically rigorous and the proofs appear sound.
- **Medium confidence:** The empirical speedup claims (1.2-1.4×) based on MNIST, Fashion-MNIST, and CIFAR-10 experiments. While the methodology is reasonable, the experiments are limited to small CNNs and the exact implementation details (seeds, initialization, data augmentation) are not fully specified.
- **Low confidence:** The generality of the speedup across diverse architectures, optimizers, and tasks. The paper focuses on a specific 3-layer CNN architecture, and the benefits may not extend to transformers, recurrent networks, or tasks requiring very deep feature hierarchies.

## Next Checks
1. **Scaling analysis:** Test Drop-Muon on progressively larger CNNs (6-12 layers) and ResNets to verify if the 1.2-1.4× speedup scales with model size, particularly when backprop cost increasingly dominates forward pass overhead.
2. **Robustness to initialization and data augmentation:** Replicate the CIFAR-10 experiments with standard data augmentation (random crops, flips) and common initialization schemes (Kaiming He) to verify the speedup is not sensitive to these choices.
3. **Comparison with alternative layer-sampling strategies:** Implement and compare against simpler layer-sampling baselines (e.g., fixed layer update schedules, random layer masking without backprop alignment) to isolate the benefit of RPT's backprop-truncation mechanism versus the general principle of selective layer updates.