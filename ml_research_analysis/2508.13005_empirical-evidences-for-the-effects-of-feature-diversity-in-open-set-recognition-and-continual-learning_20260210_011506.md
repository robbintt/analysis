---
ver: rpa2
title: Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition
  and Continual Learning
arxiv_id: '2508.13005'
source_url: https://arxiv.org/abs/2508.13005
tags:
- learning
- data
- recognition
- continual
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides empirical evidence that enhancing feature diversity
  improves both open set recognition (OSR) and continual learning. The authors conducted
  controlled experiments on a synthetic dataset where models learned combinations
  of colors and shapes.
---

# Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning

## Quick Facts
- **arXiv ID**: 2508.13005
- **Source URL**: https://arxiv.org/abs/2508.13005
- **Reference count**: 40
- **Primary result**: Enhancing feature diversity improves both open set recognition and continual learning, with better feature retention and forward transfer when tasks share feature dimensions.

## Executive Summary
This paper provides empirical evidence that feature diversity significantly impacts both open set recognition (OSR) and continual learning performance. Through controlled experiments on synthetic data with colored shapes, the authors demonstrate that models learning more diverse features (both color and shape) achieve better OSR performance through larger Mahalanobis distance separation between known and unknown classes. For continual learning, models with diverse feature representations show reduced feature forgetting and better forward transfer when subsequent tasks share feature dimensions with prior tasks.

## Method Summary
The study uses synthetic 64×64×3 images of colored shapes (circle, rectangle, ellipse in blue, red, green, pink) to create controlled experimental conditions. A 5-layer CNN architecture is trained on different class configurations to manipulate feature diversity. For OSR, models are evaluated on their ability to separate close-set from open-set samples using Mahalanobis distance histograms. For continual learning, pre-trained models are extended with new output nodes and fine-tuned on new tasks without regularization, measuring feature forgetting via CKA similarity and forward transfer via linear probing accuracy.

## Key Results
- Models learning diverse features (E2) achieved larger Mahalanobis distance histograms between close-set and open-set samples compared to color-only models (E1), indicating better OSR performance.
- In continual learning, models learning diverse features better retained previously learned data when subsequent tasks shared feature dimensions, as shown by higher CKA similarity scores.
- Linear probing accuracy demonstrated better forward transfer when base models learned diverse features, particularly when new tasks overlapped with existing feature dimensions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning more diverse features improves open set recognition by increasing the separability between known and unknown class distributions.
- Mechanism: When models encode multiple feature dimensions (e.g., both color and shape), open-set samples that differ along any encoded dimension produce larger Mahalanobis distances from close-set class centers, making them easier to detect as outliers.
- Core assumption: The open-set samples differ from training data along at least one feature dimension that the model has learned to encode.
- Evidence anchors:
  - [abstract] "enhancing feature diversity improves the recognition of open set samples"
  - [section 3.2] "the Dhist E2 are larger or at least equal to those in E1, indicating better OSR performances"
  - [corpus] "Know Yourself Better: Diverse Object-Related Features Improve Open Set Recognition" supports feature diversity for OSR, though empirical validation is limited
- Break condition: If open-set samples differ only along feature dimensions the model has NOT learned (e.g., a model trained only on color encountering novel textures), this mechanism provides no benefit.

### Mechanism 2
- Claim: Feature diversity reduces feature forgetting in continual learning when subsequent tasks share feature dimensions with prior tasks.
- Mechanism: When features are reused across tasks, gradients from the new task reinforce rather than overwrite the existing feature representations. CKA similarity remains higher because the feature subspace remains aligned.
- Core assumption: Features learned in Task 0 are relevant for Task 1 (feature overlap exists).
- Evidence anchors:
  - [abstract] "models learning more diverse features better retained previously learned data... when feature overlap existed"
  - [section 3.3.1] "When comparing E3 & E4 with E5 & E6, more features are forgotten in the later... In E3 & E4, the color features can be reused"
  - [corpus] Limited direct corpus support for this specific mechanism; most related work focuses on architectural or replay-based approaches
- Break condition: If Task 1 requires entirely disjoint features from Task 0, feature diversity in the base model provides no forgetting protection for the original task.

### Mechanism 3
- Claim: Models with diverse feature representations show better forward transfer to new tasks that share feature dimensions.
- Mechanism: Frozen feature extractors that encode diverse attributes provide richer representations for linear probing, enabling the new classifier head to leverage pre-existing feature dimensions rather than learning them from scratch.
- Core assumption: The pre-trained features are at least partially disentangled, allowing the new classifier to selectively use relevant dimensions.
- Evidence anchors:
  - [section 3.3.2] "the accuracy in E6 is much higher than in E5 because the base model E2 has better learned the shapes"
  - [section 3.3.2] "learning more features from the observed data is helpful for learning new data... when there are overlaps"
  - [corpus] "Informed Mixing" paper suggests attribution-based feature relevance, but direct evidence for forward transfer via diversity is limited
- Break condition: If features are entangled (not separable), transfer degrades—the paper notes E7/E8 showed no transfer benefit for genuinely novel shapes, suggesting entangled representations limit this mechanism.

## Foundational Learning

- Concept: **Mahalanobis Distance for OOD Detection**
  - Why needed here: Core metric used to quantify how well models separate close-set from open-set samples; assumes Gaussian class-conditional distributions.
  - Quick check question: Can you explain why Mahalanobis distance accounts for feature covariance while Euclidean distance does not?

- Concept: **Centered Kernel Alignment (CKA)**
  - Why needed here: Measures representational similarity between feature maps before and after training on new tasks; higher CKA = less forgetting.
  - Quick check question: If CKA between Task 0 and Task 1 features is 0.95, what does this tell you about feature stability?

- Concept: **Linear Probing for Transfer Assessment**
  - Why needed here: Freezes the feature extractor and trains only a new classifier head; isolates the quality of learned representations from optimization dynamics.
  - Quick check question: Why is linear probing a better measure of representation quality than fine-tuning all layers?

## Architecture Onboarding

- Component map:
  - Input (64×64×3 synthetic images) -> Conv2D -> AvgPool -> Flatten -> Linear -> Linear -> Linear (Classifier head)

- Critical path:
  1. Train base model (Task 0) with class configurations that force diverse feature learning (E2: requires both color + shape)
  2. Extract features from penultimate layer for all samples
  3. Compute class-conditional statistics (μc, Σc) for Mahalanobis scoring
  4. For continual learning: extend output layer, train on Task 1 without replay/regularization
  5. Evaluate via CKA (feature forgetting) and linear probing (forward transfer)

- Design tradeoffs:
  - E1 vs. E2 class configuration: E1 (binary, color-only) trains faster but learns less diverse features; E2 (3-class with shape conflict) forces shape learning at cost of slower convergence
  - Synthetic vs. real data: Controlled features enable causal attribution, but results may not transfer to complex natural images
  - No replay/regularization: Isolates effect of feature diversity but understates performance in practical settings

- Failure signatures:
  - Dhist ≈ 0: Model cannot distinguish open-sets; likely learned only one feature type
  - CKA drops sharply when Task 1 has feature overlap: Features are being overwritten rather than reused
  - Linear probing accuracy ≈ random: Frozen features lack relevant dimensions for new task

- First 3 experiments:
  1. Replicate E1 vs. E2 comparison on synthetic data; verify Dhist divergence correlates with number of learned feature types (confusion matrix should show E2 struggling on shape-discriminated classes longer)
  2. Extend to real data (e.g., colored MNIST or CIFAR with controlled augmentations) to test whether the mechanism generalizes beyond synthetic shapes
  3. Add feature disentanglement loss (e.g., β-VAE regularization) to test the authors' hypothesis that disentanglement improves forward transfer for novel feature combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the benefits of feature diversity for Open Set Recognition (OSR) and Continual Learning observed in synthetic settings generalize to complex, real-world datasets?
- Basis in paper: [explicit] The authors explicitly state in Section 4 that their experiments are based on small-volume synthetic datasets and call for future work to design experiments for "real and complex data."
- Why unresolved: The controlled nature of the synthetic dataset (distinct colors and shapes) lacks the noise, high dimensionality, and semantic complexity of natural images, potentially limiting the applicability of the findings to practical scenarios.
- What evidence would resolve it: Replicating the Mahalanobis distance and CKA similarity experiments on standard benchmarks (e.g., CIFAR-100 or ImageNet) to verify if diverse features still yield better OSR separation and reduced forgetting.

### Open Question 2
- Question: Does feature disentanglement provide distinct advantages over feature diversity alone for forward transfer in continual learning?
- Basis in paper: [explicit] The conclusion hypothesizes that "disentangling features can be positive for both OSR and continual learning" and identifies it as future work. This follows the observation in Section 3.3.2 regarding the failure to transfer features for blue ellipses.
- Why unresolved: The authors noted in Section 3.3.2 that models struggled to transfer knowledge when features were not disentangled (e.g., confusing blue ellipses vs. circles), suggesting diversity without separation may be insufficient for specific task overlaps.
- What evidence would resolve it: Ablation studies comparing models trained with disentanglement regularizers against those with just diversity promotion, measuring forward transfer accuracy on tasks requiring specific feature reuse.

### Open Question 3
- Question: What effective training mechanisms can enforce feature diversity in standard supervised learning without relying on specific task configurations?
- Basis in paper: [inferred] The study induced diversity manually by manipulating class combinations (e.g., adding red circles to force shape learning in E2), a method that is not scalable or applicable to fixed, real-world datasets.
- Why unresolved: While the paper establishes that diversity is beneficial, it does not propose a generalized loss function or architectural modification to ensure a model learns diverse features when the dataset structure does not force it to do so.
- What evidence would resolve it: Development and validation of a diversity-promoting regularization term that forces networks to utilize a broader feature space on standard datasets where shortcut learning (like color bias) is common.

## Limitations

- **Synthetic data dependency**: All results derive from controlled synthetic shapes/colors, limiting generalizability to real-world datasets with complex feature interactions.
- **No regularization comparison**: The continual learning experiments use naive fine-tuning without replay or regularization techniques, potentially overstating the benefit of feature diversity.
- **Limited open-set validation**: Dhist comparisons only show larger Mahalanobis distances for diverse models but don't validate actual open-set detection performance (e.g., AUROC).

## Confidence

- **High**: Feature diversity improves OSR when measured by Mahalanobis distance separation (controlled synthetic experiments, direct metric comparison).
- **Medium**: Feature diversity reduces forgetting in continual learning when feature overlap exists (CKA evidence, but limited to synthetic task pairs).
- **Low**: Feature diversity improves forward transfer (linear probing results show correlation but lack ablation on feature disentanglement).

## Next Checks

1. **Real data validation**: Replicate the E1/E2 comparison on colored MNIST or CIFAR with controlled augmentations to test whether diversity benefits generalize beyond synthetic shapes.

2. **Entanglement ablation**: Add feature disentanglement regularization (β-VAE style) to test whether explicit separation of color/shape features improves forward transfer for novel combinations.

3. **CL comparison baseline**: Compare feature diversity approach against standard CL techniques (EWC, replay buffers) to quantify practical benefits in realistic settings.