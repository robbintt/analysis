---
ver: rpa2
title: 'LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring
  Preferences'
arxiv_id: '2508.06060'
source_url: https://arxiv.org/abs/2508.06060
tags:
- project
- projects
- budget
- cost
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores whether Large Language Models (LLMs) can act\
  \ as effective social planners in participatory budgeting (PB), a mechanism design\
  \ framework for allocating resources based on community preferences. Three prompting\
  \ strategies\u2014greedy selection, direct optimization, and a hill-climbing-inspired\
  \ refinement\u2014are used to task LLMs with selecting project subsets under budget\
  \ constraints."
---

# LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences

## Quick Facts
- arXiv ID: 2508.06060
- Source URL: https://arxiv.org/abs/2508.06060
- Reference count: 40
- Primary result: LLMs, particularly with optimization and hill-climbing strategies, produce competitive participatory budgeting allocations, sometimes outperforming structured-input variants.

## Executive Summary
This paper investigates whether Large Language Models can serve as effective social planners in participatory budgeting, a mechanism design framework for allocating resources based on community preferences. The authors evaluate three prompting strategies—greedy selection, direct optimization, and hill-climbing-inspired refinement—across multiple LLMs on real-world PB instances from the PABULIB library. Results demonstrate that LLMs can generate competitive allocations compared to a utility-maximizing oracle, with notable success in inferring preferences from natural language or metadata. The study highlights the critical role of prompt design and shows that LLMs hold promise for real-world resource allocation tasks where explicit preference data may be unavailable.

## Method Summary
The study uses 24 English PB instances from PABULIB Mechanical Turk, featuring 10-20 projects, ~75 voters, and $400K-500K budgets. Three instance types are evaluated: PPI (structured votes), VRPI (votes removed, only metadata), and NLVPI (natural language votes). The authors implement three prompting strategies—Greedy (mimicking the Utilitarian Greedy algorithm), Optimization (knapsack-style), and Hill Climb (generating diverse candidate subsets). Few-shot prompting (k=2 exemplars) is used with models including LLaMA-3.1-70B, LLaMA-3.1-70B-NV, and Qwen2.5-72B. The evaluation compares normalized average utility against the oracle baseline and measures instruction-following through JSON format and budget feasibility compliance.

## Key Results
- LLMs using optimization and hill-climbing strategies produce competitive allocations, with Qwen2.5-72B achieving 0.88 normalized utility on structured instances
- VRPI and NLVPI variants often outperform PPI, suggesting LLMs can effectively infer preferences from metadata and natural language
- Hill-climb strategy significantly improves performance by mitigating early stopping and leaving budget unutilized
- Chain-of-thought instructions improve instruction-following but don't eliminate faithfulness errors between reasoning and final output

## Why This Works (Mechanism)

### Mechanism 1: Inference-Time Search via Decomposition (Hill Climb)
LLMs produce higher-utility allocations when prompted to decompose problems into diverse candidate subsets rather than executing linear algorithms. This hill-climbing approach creates a lookahead buffer, allowing the model to explore solution space before committing, reducing early stopping failures where significant budget remains unspent. The method assumes the model's context window can effectively hold and process multiple diverse subsets.

### Mechanism 2: Semantic-to-Utility Mapping (Preference Inference)
LLMs approximate utility functions from unstructured data by leveraging pre-trained semantic associations between demographic features and project categories. Without explicit votes, the model acts as a proxy voter, predicting preferences based on correlations learned during pre-training. This assumes demographic-spending preference correlations in training data align with specific community characteristics.

### Mechanism 3: Constraint Grounding via Chain-of-Thought
Explicit Chain-of-Thought instructions combined with strict JSON output formatting improve instruction-following by forcing the model to externalize arithmetic checks before finalizing decisions. This reduces "internal but not external faithfulness" errors where reasoning contradicts final output, though it doesn't eliminate such failures.

## Foundational Learning

- **Concept: Participatory Budgeting (PB) as Combinatorial Optimization**
  - Why needed: Understanding the trade-off between project cost and voter utility is essential for interpreting normalized average utility results
  - Quick check: If a project has high utility but extremely high cost, would the Utilitarian Greedy algorithm select it early or late? (Answer: Late, because it maximizes the utility-to-cost ratio)

- **Concept: Theory of Mind (ToM) in Agents**
  - Why needed: The paper frames preference inference from metadata as a ToM task, distinguishing between the model guessing votes versus processing explicit votes
  - Quick check: In VRPI setup, does the model see ground-truth votes during inference or must it hallucinate them based on voter age/sex? (Answer: Must infer/hallucinate; ground truth only used for evaluation)

- **Concept: Faithfulness in Chain-of-Thought**
  - Why needed: Understanding the distinction between internal and external faithfulness is critical for debugging instruction-following failures
  - Quick check: If an LLM says "I removed Project X to save money" in CoT but includes Project X in final JSON, is this internal or external faithfulness failure? (Answer: Internally faithful but externally unfaithful)

## Architecture Onboarding

- **Component map:** PABULIB Parser -> Instance Generator -> Strategy Module -> System Prompt -> Target LLM -> JSON Output + CoT -> Parser -> Constraint Checker & Utility Calculator
- **Critical path:** The VRPI pipeline is most sensitive, relying on successful preference prediction from limited metadata
- **Design tradeoffs:** Greedy is token-cheap but prone to early stopping; Hill Climb is token-expensive but significantly improves utility optimization
- **Failure signatures:** "Loose Greedy" execution leaving budget unspent; format violations crashing evaluator; conflict hallucination selecting mutually exclusive projects
- **First 3 experiments:** 1) Run Greedy prompt on 10-project instance to check for early stopping, 2) Run VRPI instance to compare inferred vs ground-truth allocations, 3) Introduce conflicting project pair to test Optimization prompt's conflict avoidance

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does fine-grained voter metadata improve LLM preference inference compared to coarse demographic data? The study's limited metadata (age, sex, education) constrains the model's ability to leverage rich contextual cues, suggesting more detailed voter behavioral history could improve inference accuracy.

### Open Question 2
How do different decoding strategies impact the trade-off between instruction-following consistency and utility maximization? The study used default greedy decoding, but it's unclear if stochastic methods would improve allocation quality or exacerbate constraint violations.

### Open Question 3
Can LLMs generalize performance to other complex mechanism design frameworks beyond Participatory Budgeting? While successful in PB, it remains unknown if prompting strategies transfer to domains with different constraint structures like combinatorial auctions.

## Limitations

- The semantic-to-utility mapping mechanism assumes demographic correlations in LLM training data align with specific PABULIB communities, which isn't directly tested across diverse populations
- Hill-climbing strategy's effectiveness depends on the model's context window handling multiple diverse subsets without redundancy, which isn't analyzed
- The study focuses on a single mechanism (PB) and may not capture the full complexity of decision-making systems in other domains

## Confidence

- **High confidence:** LLMs can produce competitive PB allocations when properly prompted, supported by direct utility comparisons against oracle baseline
- **Medium confidence:** Hill-climbing superiority over greedy selection relies on assumptions about auto-regressive decoding behavior
- **Low confidence:** Generalizability of preference inference from metadata depends on untested demographic-spending preference correlations in training corpus

## Next Checks

1. Test hill-climbing strategy across models with varying context windows to confirm subset diversity quality directly impacts performance
2. Cross-validate preference inference by running VRPI experiments on PB instances from different geographic/cultural regions to assess demographic correlation assumptions
3. Implement ablation study comparing few-shot exemplar count (k=1 vs k=2 vs k=5) to quantify trade-off between prompt context cost and allocation quality