---
ver: rpa2
title: 'Global Dynamics of Heavy-Tailed SGDs in Nonconvex Loss Landscape: Characterization
  and Control'
arxiv_id: '2510.20905'
source_url: https://arxiv.org/abs/2510.20905
tags:
- heavy-tailed
- proof
- learning
- minima
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes sharp characterizations of the global dynamics
  of heavy-tailed stochastic gradient descent (SGD) in non-convex loss landscapes.
  The key insight is that when injecting and then truncating heavy-tailed noise during
  training, SGD can almost completely avoid sharp minima and achieve better generalization
  performance.
---

# Global Dynamics of Heavy-Tailed SGDs in Nonconvex Loss Landscape: Characterization and Control

## Quick Facts
- arXiv ID: 2510.20905
- Source URL: https://arxiv.org/abs/2510.20905
- Authors: Xingyu Wang; Chang-Han Rhee
- Reference count: 40
- Primary result: Heavy-tailed SGD with truncation converges to a Markov jump process that visits only the widest minima, avoiding sharp minima and improving generalization

## Executive Summary
This paper establishes sharp characterizations of the global dynamics of heavy-tailed stochastic gradient descent (SGD) in non-convex loss landscapes. The key insight is that when injecting and then truncating heavy-tailed noise during training, SGD can almost completely avoid sharp minima and achieve better generalization performance. The theoretical analysis reveals that truncated heavy-tailed SGD converges to a Markov jump process that only visits the widest minima over the loss landscape. This is confirmed through simulation and deep learning experiments, which show that the proposed optimization algorithm consistently finds local minima with flatter geometry and improved generalization performance compared to vanilla SGD methods.

## Method Summary
The method involves injecting heavy-tailed noise into the gradient updates and then truncating this noise to prevent excessive variance. This truncated heavy-tailed SGD converges to a Markov jump process in the infinite-dimensional limit, which has the property of only visiting the widest minima in the loss landscape. The theoretical framework uses stochastic differential equation analysis and large deviation theory to characterize this behavior. The noise injection and truncation mechanism creates a controlled exploration-exploitation tradeoff that biases the optimization dynamics toward flatter, more generalizable minima while avoiding sharp, poorly generalizing regions.

## Key Results
- Truncated heavy-tailed SGD converges to a Markov jump process that exclusively visits the widest minima
- The method consistently finds local minima with flatter geometry compared to vanilla SGD
- Empirical validation shows improved generalization performance on both synthetic and deep learning experiments

## Why This Works (Mechanism)
The mechanism relies on the unique properties of heavy-tailed noise injection combined with truncation. When heavy-tailed noise is added to gradient updates, it creates large jumps in parameter space that can help escape sharp minima. However, without truncation, this noise can also destabilize training. By truncating the heavy-tailed noise, the method maintains the beneficial exploration properties while controlling variance. The resulting Markov jump process has an inherent bias toward wide minima due to the noise structure and the landscape geometry. This creates a self-reinforcing dynamic where the optimization process naturally avoids sharp minima and converges to regions of the loss landscape that generalize better.

## Foundational Learning
1. **Stochastic Differential Equations** - Why needed: To model the continuous-time limit of SGD with noise injection. Quick check: Verify understanding of ItÃ´ calculus and stochastic integrals.
2. **Large Deviation Theory** - Why needed: To characterize the rare events that dominate the dynamics of heavy-tailed processes. Quick check: Confirm grasp of rate functions and their role in SGD dynamics.
3. **Markov Jump Processes** - Why needed: To describe the limiting behavior of truncated heavy-tailed SGD. Quick check: Validate understanding of jump intensity and stationary distributions.
4. **Loss Landscape Geometry** - Why needed: To understand how sharp vs. flat minima affect generalization. Quick check: Assess knowledge of sharpness measures and their empirical validation.
5. **Heavy-Tailed Distributions** - Why needed: To characterize the noise injection mechanism. Quick check: Confirm understanding of stability properties and truncation effects.
6. **Optimization Dynamics** - Why needed: To connect theoretical analysis with practical training behavior. Quick check: Validate comprehension of convergence guarantees and generalization bounds.

## Architecture Onboarding
Component map: Noise injection -> Gradient update -> Truncation -> Loss evaluation -> Parameter update

Critical path: The noise injection and truncation steps are critical, as they determine the exploration-exploitation tradeoff and the final convergence properties. The Markov jump process characterization depends entirely on these components.

Design tradeoffs: The main tradeoff is between exploration (achieved through heavy-tailed noise) and stability (maintained through truncation). Too much noise can destabilize training, while too little may not provide sufficient escape from sharp minima. The truncation threshold is a key hyperparameter that must be tuned for each problem.

Failure signatures: If the truncation is too aggressive, the method may revert to standard SGD behavior and lose the benefits of heavy-tailed exploration. If the noise is too heavy-tailed without sufficient truncation, training may become unstable or diverge. Poor hyperparameter choices can lead to suboptimal convergence to flat but high-loss regions.

First experiments:
1. Validate the Markov jump process characterization on a simple 2D non-convex landscape with known minima
2. Test the sensitivity of the method to truncation threshold on a small neural network
3. Compare generalization performance against vanilla SGD and other flat-minima seeking methods on a standard benchmark dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis assumes idealized conditions including infinite-dimensional limits and Markovian dynamics that may not fully capture practical optimization scenarios
- The sharp minima avoidance claim is demonstrated primarily through synthetic experiments, with fewer empirical validations on diverse real-world deep learning tasks
- The truncation mechanism for heavy-tailed noise requires careful hyperparameter tuning, which may limit practical applicability

## Confidence
Theoretical framework confidence: Medium - The analysis relies on asymptotic approximations and idealized noise distributions
Empirical validation confidence: Medium-High - Supported by both simulation studies and deep learning experiments, though with room for broader testing across different architectures and datasets

## Next Checks
1. Extend empirical validation to diverse real-world deep learning tasks across different architectures and datasets to verify the claimed benefits of truncated heavy-tailed SGD
2. Investigate the sensitivity of the method to hyperparameters, particularly the truncation threshold and noise injection parameters
3. Compare the generalization performance against other state-of-the-art optimization methods that also target flat minima, such as Sharpness-Aware Minimization (SAM)