---
ver: rpa2
title: 'Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers,
  and Clean Evaluation Benchmarks'
arxiv_id: '2509.20209'
source_url: https://arxiv.org/abs/2509.20209
tags:
- translation
- tigrinya
- languages
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of machine translation for\
  \ low-resource languages, specifically Tigrinya, by proposing a method that leverages\
  \ multilingual pretrained models combined with language-specific tokenization and\
  \ domain-adaptive fine-tuning. The approach uses a custom morpheme-aware tokenizer\
  \ tailored for Tigrinya\u2019s Geez script and integrates it into the MarianMT architecture,\
  \ which is fine-tuned on a curated English-Tigrinya parallel corpus."
---

# Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks

## Quick Facts
- arXiv ID: 2509.20209
- Source URL: https://arxiv.org/abs/2509.20209
- Reference count: 40
- This study proposes a method for English-Tigrinya machine translation using custom morpheme-aware tokenization and multilingual fine-tuning, achieving BLEU scores of 21 and chrF scores of 19.50 for English-to-Tigrinya.

## Executive Summary
This study addresses the challenge of machine translation for low-resource languages, specifically Tigrinya, by proposing a method that leverages multilingual pretrained models combined with language-specific tokenization and domain-adaptive fine-tuning. The approach uses a custom morpheme-aware tokenizer tailored for Tigrinya's Geez script and integrates it into the MarianMT architecture, which is fine-tuned on a curated English-Tigrinya parallel corpus. Experimental results demonstrate that this method significantly outperforms zero-shot baselines, achieving BLEU scores of 21 and chrF scores of 19.50 for English-to-Tigrinya, and BLEU scores of 18 and chrF scores of 16.20 for Tigrinya-to-English. The study also introduces a high-quality, manually aligned evaluation dataset spanning diverse domains, validated through statistical significance testing using Bonferroni correction.

## Method Summary
The method combines transfer learning from multilingual pretrained models with custom language-specific components for low-resource English-Tigrinya translation. A custom SentencePiece tokenizer is trained on Tigrinya text to improve subword segmentation quality, addressing the limitations of generic multilingual tokenizers for morphologically rich languages. This tokenizer is integrated into the MarianMT encoder-decoder Transformer architecture, which is then fine-tuned on a curated parallel corpus with script normalization, alignment verification, and noise removal. The fine-tuning process uses specific hyperparameters including a batch size of 16, max sequence length of 128, AdamW optimizer with weight decay 0.01, learning rate 1.44e-07, and 3 epochs. Evaluation is performed on a manually aligned, multi-domain benchmark dataset using BLEU and chrF metrics, with Bonferroni correction applied for statistical significance testing.

## Key Results
- Custom tokenizer with fine-tuned MarianMT achieves BLEU 21/18 and chrF 19.50/16.20 for English→Tigrinya and Tigrinya→English respectively
- Ablation shows custom tokenizer improves chrF from 10.49 to 19.50 for English→Tigrinya
- Multi-domain evaluation dataset covers religious, news, health, and education domains with 4,000 sentences
- Statistical significance testing confirms performance improvements using Bonferroni correction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A custom morpheme-aware tokenizer for Tigrinya's Ge'ez script improves subword segmentation quality, which in turn improves translation output quality.
- Mechanism: Generic multilingual tokenizers underrepresent low-resource languages, yielding fragmented subwords and high OOV rates. A SentencePiece tokenizer trained on Tigrinya text extends BPE with script normalization and morphology-aligned segmentation, producing more meaningful tokens. This reduces the sequence length and semantic fragmentation seen by the encoder-decoder.
- Core assumption: Tigrinya's morphological patterns can be captured sufficiently via data-driven subword segmentation trained on the available Tigrinya monolingual or parallel side.
- Evidence anchors:
  - [abstract] "...integrates language-specific tokenization, informed embedding initialization, and domain-adaptive fine-tuning."
  - [section IV.C] "We trained a language-specific SentencePiece tokenizer...to ensure accurate subword segmentation, which is critical for low-resource languages with complex morphology."
  - [corpus] Related work "Multilingual Tokenization through the Lens of Indian Languages" confirms tokenizer skew toward high-resource languages and challenges for morphologically rich languages.
- Break condition: If training data for the tokenizer is too small, noisy, or unrepresentative, segmentation quality may degrade, and OOV/fragmentation issues can persist or worsen.

### Mechanism 2
- Claim: Fine-tuning a multilingual pretrained model (MarianMT) with language-specific components substantially outperforms zero-shot baselines for English–Tigrinya translation.
- Mechanism: Pretrained multilingual models encode cross-lingual representations but lack language-specific adaptation. Fine-tuning on curated parallel data transfers representations while adapting to Tigrinya's morphology and vocabulary. Informed embedding initialization and tokenizer adaptation enable better alignment between input representations and model expectations.
- Core assumption: The pretrained model has at least partial exposure or transferable structure for related languages or scripts; sufficient clean parallel data exists for effective adaptation.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that transfer learning with a custom tokenizer substantially outperforms zero-shot baselines..."
  - [section V] Fine-tuned MarianMT with custom tokenizer achieves BLEU 21/18 and chrF 19.50/16.20 vs baseline chrF 10.49/9.39.
  - [corpus] "Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer" highlights that semantically equivalent words across languages need better alignment for effective cross-lingual transfer—tokenizer adaptation helps.
- Break condition: If pretrained weights lack transferable structure for the target language/script, or if embedding initialization is poor, gains may be minimal or unstable.

### Mechanism 3
- Claim: A clean, manually aligned, multi-domain evaluation benchmark enables more reliable assessment and reduces noise-driven conclusions.
- Mechanism: OPUS-derived test sets can contain alignment noise and cross-language interference (e.g., confusing Tigrinya with Amharic). A curated benchmark with human alignment across domains (religious, news, health, education) and statistical rigor (Bonferroni correction) mitigates false-positive conclusions.
- Core assumption: Automatic metrics (BLEU, chrF) correlate sufficiently with translation adequacy/fluency to guide conclusions; human-aligned data is cleaner than automatically aligned alternatives.
- Evidence anchors:
  - [section III] "...we developed a carefully curated, high-quality parallel benchmark dataset...spanning four domains."
  - [section V.B] "Bonferroni correction is applied to ensure statistical significance across configurations."
  - [corpus] No direct corpus evidence on evaluation benchmarks for Tigrinya specifically; neighboring papers do not address evaluation data quality for this pair.
- Break condition: If evaluation domains diverge significantly from training domains without explicit domain adaptation, metrics may not reflect generalization.

## Foundational Learning

- Concept: Subword tokenization (BPE/WordPiece/SentencePiece)
  - Why needed here: The paper relies on a custom SentencePiece tokenizer; understanding subword methods is prerequisite to adapting or debugging tokenization.
  - Quick check question: Can you explain why BPE reduces OOV for morphologically rich languages, and how vocabulary size trades off between granularity and sequence length?

- Concept: Transfer learning in sequence-to-sequence models
  - Why needed here: The approach fine-tunes MarianMT (a multilingual Transformer) rather than training from scratch.
  - Quick check question: What is the difference between zero-shot cross-lingual transfer and fine-tuning, and why does fine-tuning often outperform zero-shot for low-resource pairs?

- Concept: Evaluation metrics for MT (BLEU, chrF)
  - Why needed here: The study reports BLEU and chrF; chrF is emphasized for morphologically rich languages.
  - Quick check question: Why might chrF be more informative than BLEU for morphologically complex languages like Tigrinya?

## Architecture Onboarding

- Component map: NLLB English-Tigrinya parallel corpus -> Custom SentencePiece tokenizer (trained on Tigrinya text) -> MarianMT encoder-decoder Transformer -> Fine-tuned model -> Multi-domain evaluation benchmark

- Critical path:
  1. Curate and normalize the parallel corpus (script normalization, alignment verification, noise removal)
  2. Train or integrate the Tigrinya-specific SentencePiece tokenizer
  3. Fine-tune MarianMT with the custom tokenizer on the curated parallel data
  4. Evaluate on the clean benchmark using BLEU and chrF; apply statistical correction

- Design tradeoffs:
  - Tokenizer vocabulary size: Larger vocab reduces OOV but may increase data sparsity; smaller vocab fragments words, increasing sequence length.
  - Fine-tuning scope: Full-model fine-tuning can overfit with limited data; freezing lower layers may reduce overfitting but limit adaptation.
  - Evaluation domain coverage: Multi-domain benchmarks improve robustness assessment but require more curation effort.

- Failure signatures:
  - Cross-language interference: Model confuses Tigrinya with Amharic due to shared Ge'ez script (observed in OPUS data).
  - High OOV or fragmented tokens: Indicates tokenizer vocabulary is insufficient or training data is too small/noisy.
  - Large train-eval domain gap: Metrics collapse on out-of-domain test sets without domain-adaptive fine-tuning.

- First 3 experiments:
  1. Establish baselines: Run zero-shot MarianMT with default tokenizer on the curated benchmark; record BLEU/chrF.
  2. Tokenizer ablation: Compare default multilingual tokenizer vs custom Tigrinya SentencePiece tokenizer (same model, same data); isolate tokenizer impact.
  3. Fine-tuning with vs without domain curation: Fine-tune on raw NLLB data vs cleaned/filtered subset; assess sensitivity to data quality.

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown methodological details: Exact MarianMT checkpoint identifier, SentencePiece vocabulary size, and embedding initialization procedure are not specified, critical for exact reproduction
- Cross-language ambiguity: Tigrinya and Amharic share Ge'ez script, creating cross-lingual interference risk without quantitative analysis of language confusion
- Evaluation reliability: Manually aligned benchmark lacks inter-annotator agreement statistics and domain representation validation

## Confidence

**High confidence**: The core hypothesis that custom tokenization improves subword segmentation quality for morphologically rich low-resource languages is well-supported by the ablation experiment (chrF 19.50 vs 10.49) and consistent with prior work on multilingual tokenization challenges.

**Medium confidence**: The claim that fine-tuning multilingual pretrained models substantially outperforms zero-shot baselines is supported by the reported BLEU/chrF gains, but the lack of a detailed zero-shot baseline experiment description and the unknown pretrained model variant introduce uncertainty.

**Low confidence**: The claim that the manually curated evaluation benchmark significantly reduces noise compared to OPUS-derived sets is plausible but not empirically validated—no quantitative comparison of alignment quality or cross-lingual interference before/after curation is provided.

## Next Checks

1. **Tokenizer ablation with zero-shot baseline**: Run the MarianMT zero-shot baseline on the custom evaluation set using both the default multilingual tokenizer and the custom Tigrinya SentencePiece tokenizer, holding all else equal. Report BLEU and chrF to isolate tokenizer impact from fine-tuning.

2. **Cross-lingual interference quantification**: Analyze model outputs for Amharic-specific vocabulary or morphology in the evaluation set. Compute the proportion of sentences containing clear Amharic interference to quantify the script normalization and domain filtering effectiveness.

3. **Domain generalization test**: Evaluate the fine-tuned model on an out-of-domain test set (e.g., social media or conversational text) to assess robustness beyond the four curated domains. Compare performance drop to baseline to measure domain adaptation effectiveness.