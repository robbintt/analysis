---
ver: rpa2
title: Learning Time-Varying Convexifications of Multiple Fairness Measures
arxiv_id: '2508.14311'
source_url: https://arxiv.org/abs/2508.14311
tags:
- fairness
- action
- feedback
- vertices
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning time-varying convexifications
  of multiple fairness measures in sequential decision-making, where the relative
  weights of fairness regularizers are unknown and may change over time. The authors
  propose a framework that incorporates graph-structured feedback, where actions and
  fairness measures are represented as vertices in a graph, and edges indicate relationships
  between them.
---

# Learning Time-Varying Convexifications of Multiple Fairness Measures

## Quick Facts
- arXiv ID: 2508.14311
- Source URL: https://arxiv.org/abs/2508.14311
- Authors: Quan Zhou; Jakub Marecek; Robert Shorten
- Reference count: 40
- Primary result: Regret bound of O(√T log T) for time-varying case with graph-structured feedback

## Executive Summary
This paper addresses the challenge of learning time-varying convexifications of multiple fairness measures in sequential decision-making problems where relative weights of fairness regularizers are unknown and may change over time. The authors propose a framework that incorporates graph-structured feedback, where actions and fairness measures are represented as vertices in a graph, and edges indicate relationships between them. The framework allows for limited feedback, where only the rewards associated with certain actions are observed based on the graph structure. The core method uses exponentially-weighted algorithms with linear programming to make decisions in each round while updating weights based on observed rewards.

## Method Summary
The framework operates in a sequential decision-making setting where actions (ad allocations) and fairness measures (regularizers) form vertices in a compatibility graph. The algorithm maintains exponential weights over actions and uses importance sampling to estimate rewards when only partial feedback is available. A linear program ensures minimum exploration probability across the graph structure, preventing starvation of under-explored actions. Two algorithms are presented: one for time-invariant graphs (solving LP once) and one for time-varying graphs (solving LP each round). The approach combines revenue objectives with weighted fairness regularizers through convexification, updating weights based on observed rewards via importance-sampled estimators.

## Key Results
- Achieves regret bound of O(√T log T) for time-varying graph-structured feedback case
- Provides theoretical guarantees showing sublinear regret in both time-invariant and time-varying cases
- Demonstrates effectiveness on political advertising dataset with 100,000 impressions across 3 parties and 3 regularizers
- Shows that regret scales with maximal acyclic subgraph size rather than total number of actions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph-structured feedback enables learning under partial observability by revealing rewards for chosen actions and their neighbors.
- **Mechanism:** When action vertex $a_i$ is selected, the algorithm observes not only its reward $r(a_i, t)$ but also rewards for all out-neighbors $n \in N^{out}_t(a_i)$. This expands the effective information gain per round without requiring full feedback.
- **Core assumption:** The compatibility graph accurately captures which regularizers are affected by which actions, and which action rewards are co-disclosed.
- **Evidence anchors:** [abstract] "feedback is limited to actions and their neighbors in a (possibly time-varying) compatibility graph"; [section 3, equation 1] Formal definition: $N^{out}_t(a_i) := \{a_n | a_n \subseteq V_a, (a_i \rightarrow a_n) \in E_t\}$; [corpus] Limited corpus support for graph-structured fairness learning; neighbor papers focus on group/individual fairness trade-offs rather than feedback structure.
- **Break condition:** If the graph is mis-specified (edges missing or spurious), observed rewards will not reflect true regularizer dependencies, causing biased weight updates.

### Mechanism 2
- **Claim:** Linear programming over the graph structure ensures minimum exploration probability, preventing starvation of under-explored actions.
- **Mechanism:** The LP (equation 10) solves $\max_{\xi \in \Delta^I} \min_i \sum_{n \in N^{out}(a_i)} \xi(n)$, finding a distribution that maximizes the minimum probability of observing any action's reward. This $\xi$ is mixed with exploitation weights via egalitarian factor $\gamma_t$.
- **Core assumption:** The graph is strongly or weakly observable (every action is either self-observing or observed by some neighbor); unobservable graphs yield vacuous bounds.
- **Evidence anchors:** [section 5.1] "ξ(i) represent the desire to pick an action vertex uniformly from each clique among action vertices"; [algorithm 1, line 2] $p(i,t) := (1-\gamma_t)\phi(i,t)/\Phi_t + \gamma_t \xi(i)$; [corpus] No direct corpus precedent; LP-based exploration mixes are from graph bandit literature (Alon et al. 2017, cited in table 1).
- **Break condition:** If the LP solution has zero minimum probability for some action (unobservable graph), that action's reward is never learned.

### Mechanism 3
- **Claim:** Regret scales with the maximal acyclic subgraph size $\text{mas}(G_t)$, not the number of actions, exploiting graph structure for tighter bounds.
- **Mechanism:** The importance sampling estimator $\hat{r}(n,t) = r(n,t)\mathbb{I}\{n \in N^{out}(a_i)\}/q(n,t) + \beta/q(n,t)$ uses observation probabilities $q(n,t) = \sum_{i \in N(n)} p(i,t)$. Variance of the estimator depends on graph connectivity; acyclic subgraphs bound the effective independence number.
- **Core assumption:** Assumption: Reward functions are bounded and adversarial but fixed per round (non-stochastic setting).
- **Evidence anchors:** [theorem 1] "Algorithm 1 achieves weak regret of $\tilde{O}(\sqrt{\log(I/\delta) \sum_{t \in [T]} \text{mas}(G_t)})$"; [appendix 8.3] Proof follows Theorem 9 from Alon et al. 2017, with regret decomposition using Freedman's inequality; [corpus] Corpus papers on fairness do not provide regret analysis; mechanism is inherited from graph bandit theory.
- **Break condition:** If $\text{mas}(G_t)$ approaches $I$ (graph is nearly independent), regret approaches standard bandit bounds $\tilde{O}(\sqrt{IT})$, losing structural advantage.

## Foundational Learning

- **Concept: Online Convex Optimization with Bandit Feedback**
  - **Why needed here:** The algorithm operates in a sequential decision setting where only partial reward information is revealed. Understanding regret definitions (weak vs. dynamic) and the role of learning rates is essential.
  - **Quick check question:** Can you distinguish between weak regret (best single action in hindsight) and dynamic regret (best sequence in hindsight)?

- **Concept: Graph-Structured Bandits**
  - **Why needed here:** The compatibility graph $G_t$ determines what information is observable. Key parameters include the maximal acyclic subgraph, independence number, and weak domination number.
  - **Quick check question:** Given a directed graph, can you compute the size of its maximal acyclic subgraph or identify whether it is strongly observable?

- **Concept: Multi-Objective Optimization and Convexification**
  - **Why needed here:** The paper convexifies trade-offs between revenue and multiple fairness regularizers with unknown, time-varying weights $w(j,t)$. Understanding scalarization via weighted sums and the limitations of convex approximations is critical.
  - **Quick check question:** Why might convexification fail to capture Pareto-optimal trade-offs when fairness measures are non-convex or conflicting?

## Architecture Onboarding

- **Component map:** Action vertices $V_a$ (discrete decisions) -> Regularizer vertices $V_f$ (fairness constraints) -> Compatibility graph $G_t$ (directed edges encode affect relation $a_i \rightarrow f_j$ and co-disclosure $a_{i1} \rightarrow a_{i2}$)

- **Critical path:**
  1. Observe current graph $G_t$ (and neighbor sets $N^{out}_t$, $N^{in}_t$).
  2. Solve LP for $\xi$; compute action probabilities $p(i,t)$.
  3. Sample action $a_i$; observe rewards for $N^{out}_t(a_i)$.
  4. Compute importance-sampled estimators $\hat{r}(n,t)$.
  5. Update weights $\phi(i, t+1) = \phi(i,t) \exp(\eta \hat{r}(n,t))$.

- **Design tradeoffs:**
  - Time-invariant vs. time-varying graphs: Algorithm 1 solves LP once; Algorithm 2 re-solves each round (higher compute, handles evolving relations).
  - Exploration rate $\gamma_t$: Higher $\gamma_t$ increases exploration but slows convergence to optimal actions.
  - Learning rate $\eta$: Must satisfy $\eta \leq 1/(3I)$; smaller $\eta$ yields stable but slow learning.

- **Failure signatures:**
  - Regret not decreasing: Check if $\text{mas}(G_t)$ is large (sparse feedback) or if learning rate is too small.
  - LP infeasible: Graph may have unobservable nodes; verify strong/weak observability.
  - Weight collapse: If $\beta$ is too small, importance sampling variance causes unstable updates.

- **First 3 experiments:**
  1. **Synthetic graph test:** Construct a 10-action graph with known $\text{mas}(G) = 3$; verify regret scales as $\tilde{O}(\sqrt{3T})$ against a fixed optimal action.
  2. **Time-varying graph stress test:** Randomly perturb 20% of edges per round; compare Algorithm 2 vs. Algorithm 1 (mismatched assumption) on regret trajectory.
  3. **Political advertising replication:** Reproduce Section 6 experiment with $I=3$ parties, $J=3$ regularizers, and the 100,000-impression revenue matrix; confirm objective value $R(A)$ tracks $OPT_W$ within expected regret bound.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes the graph structure accurately reflects which actions affect which fairness measures and which rewards are co-disclosed. Mis-specification leads to biased weight updates.
- The convexification approach may fail to capture non-convex Pareto-optimal trade-offs between fairness measures.
- Theoretical guarantees assume adversarial but bounded rewards, which may not hold in practice with stochastic reward distributions.

## Confidence
- **High confidence**: Regret bounds scaling with $\text{mas}(G_t)$ - directly derived from established graph bandit theory with formal proofs.
- **Medium confidence**: Graph-structured feedback effectiveness - theoretically sound but limited corpus precedent for fairness applications.
- **Medium confidence**: Linear programming exploration - technically correct but requires careful graph observability verification in practice.

## Next Checks
1. **Graph Mis-specification Test**: Deliberately corrupt 20% of edges in the compatibility graph and measure degradation in weight tracking accuracy and regret performance.
2. **Pareto Frontier Validation**: For a synthetic problem with known non-convex Pareto front, compare the convexified solution path against true Pareto-optimal solutions.
3. **Stochastic Reward Robustness**: Replace adversarial rewards with stochastic rewards following known distributions and evaluate whether regret bounds still hold or require adjustment.