---
ver: rpa2
title: Effective Reinforcement Learning for Reasoning in Language Models
arxiv_id: '2505.17218'
source_url: https://arxiv.org/abs/2505.17218
tags:
- dash
- gradient
- reasoning
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies reinforcement learning (RL) algorithms
  for improving the reasoning capabilities of small language models (LMs), focusing
  on accuracy and computational efficiency. It compares supervised fine-tuning (SFT)
  and on-policy RL, finding that on-policy RL significantly outperforms SFT for the
  small models studied.
---

# Effective Reinforcement Learning for Reasoning in Language Models

## Quick Facts
- **arXiv ID:** 2505.17218
- **Source URL:** https://arxiv.org/abs/2505.17218
- **Reference count:** 27
- **Primary result:** DASH RL algorithm achieves 83% training time reduction without accuracy loss for small LMs

## Executive Summary
This paper systematically evaluates reinforcement learning approaches for enhancing reasoning capabilities in small language models, with a focus on balancing accuracy and computational efficiency. The authors compare supervised fine-tuning (SFT) with on-policy RL algorithms and discover that sampling efficiency is a critical bottleneck, particularly due to mismatched optimal batch sizes for inference versus backpropagation. To address this, they introduce DASH, which employs preemptive sampling (large inference batches, small gradient batches) and gradient filtering to drop samples with minimal advantage estimates. The approach achieves significant speedups while maintaining accuracy and improving stability compared to PPO-based methods.

## Method Summary
The study investigates RL algorithms for improving reasoning in small language models by first establishing that on-policy RL significantly outperforms SFT for the target model sizes. The key insight is that sampling efficiency becomes a bottleneck because optimal batch sizes differ between inference (where larger batches are more efficient) and backpropagation (where smaller batches work better). DASH addresses this through preemptive sampling, which runs large-batch inference upfront and stores results for subsequent small-batch gradient updates. Additionally, DASH implements gradient filtering to discard samples with small advantage estimates, further improving efficiency. The algorithm removes KL divergence regularization, resulting in more concise outputs while maintaining or improving accuracy.

## Key Results
- DASH reduces training time by 83% compared to GRPO without sacrificing accuracy
- On-policy RL significantly outperforms SFT for small models studied
- Removing KL divergence regularization leads to more concise generations and higher accuracy
- DASH shows improved stability compared to PPO-based approaches

## Why This Works (Mechanism)
The effectiveness of DASH stems from addressing the fundamental mismatch between optimal batch sizes for inference and gradient computation. By decoupling these operations through preemptive sampling, the algorithm maximizes GPU utilization during the expensive inference phase while maintaining the benefits of small-batch gradient updates for stable learning. The gradient filtering mechanism further enhances efficiency by focusing computation on samples that provide meaningful learning signals, as determined by their advantage estimates. This selective approach prevents wasteful gradient updates on samples that would contribute minimally to policy improvement.

## Foundational Learning
- **On-policy vs Off-policy RL:** On-policy methods use current policy samples for training, providing more stable updates but requiring fresh data collection; needed because small LMs benefit from policy-specific feedback loops, check by verifying policy improvement per epoch
- **Advantage Estimation:** Measures how much better an action is compared to the average; crucial for determining which samples to prioritize in gradient filtering, verify by examining advantage distribution before/after filtering
- **Preemptive Sampling:** Decouples data collection from training by generating large batches upfront; essential for exploiting GPU efficiency in inference, validate by measuring throughput with/without preemption
- **Gradient Filtering:** Selects only high-impact samples based on advantage magnitude; reduces computational waste while preserving learning quality, test by ablating filtering and measuring convergence speed
- **KL Divergence Regularization:** Constrains policy updates to prevent large deviations from reference behavior; removing it improves conciseness but may reduce stability, assess by measuring output length and variance

## Architecture Onboarding

**Component Map:** Preemptive Sampler -> Advantage Estimator -> Gradient Filter -> Optimizer -> Model

**Critical Path:** Data generation → advantage computation → filtering → gradient update → parameter synchronization

**Design Tradeoffs:** Large inference batches maximize throughput but increase memory pressure; aggressive filtering speeds training but risks discarding useful rare samples; no KL regularization improves conciseness but may reduce output diversity

**Failure Signatures:** Degraded accuracy when filtering threshold is too high; training instability when batch sizes are mismatched; memory overflow during preemptive sampling phase

**First Experiments:**
1. Baseline GRPO with standard batch sizes on GSM8K to establish performance floor
2. DASH with varying filtering thresholds to identify optimal balance point
3. Ablation study removing preemptive sampling to quantify its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to small LMs; performance on larger models remains untested
- Evaluation focused on mathematical reasoning datasets (GSM8K, AQuA), limiting generalizability to other reasoning domains
- Gradient filtering lacks theoretical justification and may introduce selection bias
- Removal of KL regularization could reduce output diversity in applications requiring varied responses

## Confidence
- **High confidence:** DASH's computational efficiency improvements (83% training time reduction)
- **Medium confidence:** Accuracy improvements due to limited dataset diversity
- **Medium confidence:** Stability claims based on specific PPO implementation comparisons
- **Low confidence:** Generalization across model sizes and reasoning domains

## Next Checks
1. Evaluate DASH on larger language models (7B+ parameters) to verify if preemptive sampling remains beneficial under different GPU memory constraints
2. Test the algorithm across diverse reasoning tasks including commonsense reasoning, code generation, and logical inference to assess domain generalization
3. Conduct ablation studies specifically isolating the impact of gradient filtering to determine whether it introduces selection bias or affects long-tail reasoning capabilities