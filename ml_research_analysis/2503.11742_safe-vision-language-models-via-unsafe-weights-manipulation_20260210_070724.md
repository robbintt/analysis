---
ver: rpa2
title: Safe Vision-Language Models via Unsafe Weights Manipulation
arxiv_id: '2503.11742'
source_url: https://arxiv.org/abs/2503.11742
tags:
- unsafe
- safe
- safety
- performance
- safe-clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of unsafe behaviors in vision-language
  models (VLMs) by addressing the shortcomings of existing training-based safety alignment
  methods. The authors identify that fine-tuning VLMs for safety can inadvertently
  degrade the model's performance on safe inputs.
---

# Safe Vision-Language Models via Unsafe Weights Manipulation

## Quick Facts
- arXiv ID: 2503.11742
- Source URL: https://arxiv.org/abs/2503.11742
- Authors: Moreno D'Incà; Elia Peruzzo; Xingqian Xu; Humphrey Shi; Nicu Sebe; Massimiliano Mancini
- Reference count: 40
- Key outcome: Training-free method improves safety on unsafe queries while outperforming training-based methods on safe queries and preserving model knowledge better.

## Executive Summary
This paper addresses unsafe behaviors in vision-language models (VLMs) by proposing Unsafe Weights Manipulation (UWM), a training-free method that identifies and manipulates weights associated with unsafe behaviors. Unlike fine-tuning approaches that degrade performance on safe inputs, UWM uses activation discrepancies between safe and unsafe content to identify unsafe-associated weights, then negates their values. Experiments demonstrate that UWM consistently improves safety metrics while preserving zero-shot classification accuracy better than training-based safety alignment methods like Safe-CLIP.

## Method Summary
UWM identifies unsafe weights by comparing activations between safe and unsafe content using a calibration dataset. For each weight, it computes layer-wise saliency scores (Φ_sf and Φ_uns) based on information flow and magnitude, then calculates their ratio to measure importance for unsafe processing. Weights contributing disproportionately to unsafe behavior are selected using an adaptive threshold and their values are negated. The method is applied independently to image and text encoders, targeting specific layers (Fc2 for vision encoder, output projection for text encoder). This training-free approach avoids catastrophic forgetting while improving safety metrics.

## Key Results
- UWM achieves 61.3% mean zero-shot accuracy across 17 datasets versus 54.2% for Safe-CLIP
- Improves SafeGround metrics from 1.2% to 4.5% on unsafe queries
- Outperforms Safe-CLIP on safe queries while preserving knowledge better
- Successfully generalizes across different VLM architectures including LLaVA

## Why This Works (Mechanism)

### Mechanism 1: Unsafe Weight Identification via Activation Discrepancy
- **Claim:** Weights that contribute most to unsafe behavior can be identified by comparing how information flows through them when processing safe versus unsafe content.
- **Mechanism:** UWM computes layer-wise saliency scores (Φ_sf and Φ_uns) for each weight using a calibration dataset of safe/unsafe pairs. The ratio Φ_uns/Φ_sf measures the discrepancy—weights with high ratios are disproportionately involved in processing unsafe content. An adaptive threshold (τ=0.02) selects the smallest subset of weights contributing to at least τ of the cumulative score.
- **Core assumption:** Safe and unsafe content produce measurably different activation patterns that can be attributed to specific weights.
- **Evidence anchors:**
  - [abstract] "UWM uses a calibration set of safe and unsafe instances to compare activations between safe and unsafe content, identifying the most important parameters for processing the latter."
  - [section 4] Equation 8 defines the scoring function combining input/output node information flow with weight magnitude; Equation 10 defines the ratio Φ_uns/Φ_sf as the aggregation score.
  - [corpus] Weak corpus support for this specific mechanism; related papers focus on safety benchmarks rather than weight-level identification methods.
- **Break condition:** If safe and unsafe activations are indistinguishable for a given weight (Φ_uns ≈ Φ_sf), the method cannot identify it as unsafe-associated; if the calibration set is not representative, identified weights may not generalize.

### Mechanism 2: Weight Negation Reverses Unsafe Influence
- **Claim:** Negating the values of identified unsafe weights (multiplying by α=-1) reduces unsafe outputs while preserving model capabilities better than zeroing or fine-tuning.
- **Mechanism:** Rather than pruning (setting α=0), UWM flips the sign of selected weights. The authors find this "intuitively 'flips' their effect" without destroying the learned representations. The manipulation is applied independently to each encoder to prevent cross-modal interference.
- **Core assumption:** Assumption: Unsafe behaviors are encoded with a directional bias that can be counteracted by sign inversion, rather than being distributed across weight magnitudes.
- **Evidence anchors:**
  - [abstract] "Their values are then manipulated via negation."
  - [section 4] "We experiment with negative values, such as α=-1, effectively reversing the influence of selected weights."
  - [table 1] UWM achieves 61.3% mean zero-shot accuracy (preserving knowledge) while improving GS from 1.2% to 4.5% (improving safety).
  - [corpus] No corpus evidence directly addresses weight negation for safety; related work focuses on training-based alignment.
- **Break condition:** If unsafe behavior emerges from distributed representations rather than directional weight values, negation may fail or introduce new pathologies; α values closer to 1 gradually restore original unsafe behavior (Figure 6).

### Mechanism 3: Avoiding Catastrophic Forgetting via Training-Free Manipulation
- **Claim:** Training-free methods preserve original model knowledge better than fine-tuning approaches because they avoid modifying weights through gradient descent on safety objectives.
- **Mechanism:** Training-based safety alignment (e.g., Safe-CLIP) updates weights via contrastive learning, which causes unintended forgetting of prior knowledge. UWM avoids this by only manipulating pre-identified weights post-hoc, without any backward passes or optimization steps.
- **Core assumption:** Assumption: The original pre-trained VLM has useful safe behaviors that fine-tuning inadvertently overrides.
- **Evidence anchors:**
  - [abstract] "UWM improves safety on unsafe queries while outperforming training-based methods on safe ones and preserving model knowledge better."
  - [section 3.3] "Safe-CLIP degrades safety when tested on safe queries... -23% in Pt_s and -5.9% in Pv_s."
  - [table 2] UWM preserves 61.3% mean accuracy across 17 datasets vs. 54.2% for Safe-CLIP.
  - [corpus] Consistent with broader literature on catastrophic forgetting in fine-tuning (Li & Hoiem reference in paper).
- **Break condition:** If the original model lacks useful safe behaviors to preserve, or if safety and knowledge are fundamentally entangled in the same weights, any manipulation will trade one for the other.

## Foundational Learning

- **Vision-Language Models (VLMs) and CLIP Architecture:**
  - Why needed here: UWM operates on contrastive VLMs with separate image and text encoders; understanding how similarity scores are computed is essential for interpreting safety metrics and activation patterns.
  - Quick check question: Given an image and two text captions (one safe, one unsafe), can you predict how a standard CLIP model would rank their similarities, and why this baseline behavior is "unsafe"?

- **Model Editing and Pruning Fundamentals:**
  - Why needed here: UWM adapts pruning scoring functions (information flow, gradient magnitude) for safety rather than compression; understanding weight saliency helps debug why certain layers are more sensitive.
  - Quick check question: Why might zeroing high-gradient weights (standard pruning) destroy more knowledge than negating high-discrepancy weights (UWM)?

- **Safety Evaluation Tradeoffs:**
  - Why needed here: The paper introduces SafeGround metrics to decouple safety from retrieval accuracy; prior metrics conflate the two, obscuring failure modes on safe inputs.
  - Quick check question: If a model retrieves the wrong safe image for an unsafe query (retrieval fails) but still prefers safe over unsafe images (safety succeeds), which metric captures each behavior?

## Architecture Onboarding

- **Component map:**
  Calibration Set Construction -> Scoring Function -> Adaptive Selection -> Weight Manipulation -> Application

- **Critical path:** Calibration set quality → Scoring function accuracy → Adaptive threshold (τ) → Weight negation (α) → Layer selection. Errors propagate; a biased calibration set produces misidentified unsafe weights.

- **Design tradeoffs:**
  - **τ (sparsity):** Lower τ = fewer weights modified = better knowledge preservation but weaker safety improvement (Figure 5).
  - **α (scaling):** α=-1 maximally reverses unsafe influence; α→1 restores original behavior (Figure 6).
  - **Layer selection:** Text encoder MLP layers (Fc1, Fc2) are highly sensitive to manipulation (Table 8); attention output projection is safer.
  - **Magnitude prior:** Multiplying scores by |W| helps text encoder but severely degrades vision encoder performance (Table 7).

- **Failure signatures:**
  - **Over-aggressive pruning (high τ):** Zero-shot accuracy collapses (Vs-Ts → 0% in Table 8 for MLP layers).
  - **Missing safe scores (Φ_sf omitted):** Model becomes less safe on safe inputs (PS drops from 67.5% to 60.9% in Table 6).
  - **Wrong layer:** Pruning text encoder Fc1/Fc2 destroys almost all downstream capability.
  - **Calibration set drift:** If safe/unsafe distributions don't match test distribution, identified weights won't generalize.

- **First 3 experiments:**
  1. **Reproduce baseline scoring:** Implement Φ_sf and Φ_uns on a small calibration set; verify that unsafe images/text produce higher Φ_uns scores for a subset of weights, and visualize the distribution of Φ_uns/Φ_sf ratios across layers.
  2. **Ablate α on a single layer:** Fix τ=0.02 and a single layer (text encoder output projection); sweep α from -1 to 1 and plot both safety (GS) and knowledge (Vs-Ts) to confirm the tradeoff curve matches Figure 6.
  3. **Cross-architecture sanity check:** Apply UWM with fixed hyperparameters (α=-1, τ=0.02) to a different CLIP backbone (e.g., ViT-B16) and verify that safety improves (Txt_s, Img_s increase) without catastrophic knowledge loss (Vs-Ts remains >50%). This tests generalization before investing in full hyperparameter sweeps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unsafe behaviors be fully disentangled from general knowledge in VLM weights when individual parameters may encode overlapping concepts?
- Basis in paper: [explicit] The Limitations section states that "The complexity of large-scale VLMs challenges the isolation of unsafe weights without affecting model capabilities, as individual parameters may encode overlapping knowledge."
- Why unresolved: While UWM improves the safety/knowledge tradeoff, Table 2 shows a performance gap; UWM lowers mean zero-shot accuracy to 61.3% compared to the original CLIP's 72.7%, indicating that current isolation techniques still compromise general capabilities.
- What evidence would resolve it: A modification of UWM that maintains or improves zero-shot accuracy (e.g., >72.7%) while achieving high safety scores on the SafeGround metrics would demonstrate successful disentanglement.

### Open Question 2
- Question: Can UWM be effectively generalized to purely generative VLM architectures beyond the contrastive encoders primarily tested?
- Basis in paper: [explicit] The Limitations section notes that the authors "focus mainly on contrastive-based VLMs, leaving other architectures for future research."
- Why unresolved: While tested on LLaVA, the method was applied only to the vision encoder (Table 5). It is unclear if the scoring function and manipulation strategy apply to the language model component where generative unsafe outputs often originate.
- What evidence would resolve it: Successful application of UWM to the language decoder weights of generative VLMs (e.g., LLaVA or GPT-4V), resulting in reduced unsafe generation rates without catastrophic forgetting of linguistic knowledge.

### Open Question 3
- Question: Does applying a global weight negation parameter restrict the effectiveness of UWM compared to layer-specific or weight-specific manipulation strategies?
- Basis in paper: [inferred] The methodology fixes the scaling factor $\alpha=-1$ for all layers based on a sweep, rather than adapting it per layer or per weight.
- Why unresolved: A global parameter assumes uniformity in how unsafe concepts are encoded across different layers. However, the ablation in Table 8 shows that different layers (e.g., Value vs. Output projections) have vastly different sensitivities to manipulation.
- What evidence would resolve it: Experiments demonstrating that optimizing $\alpha$ individually per layer or via a learned function yields higher SafeGround scores (GS) while minimizing the drop in retrieval accuracy (Vs-Ts).

## Limitations

- The method relies heavily on the representativeness of the calibration dataset; if safe/unsafe distributions do not match the target deployment environment, identified weights may not generalize.
- The adaptive threshold τ=0.02 is empirically chosen but may need tuning per dataset or model.
- The mechanism assumes that unsafe behaviors are encoded through directional weight biases that can be reversed by negation, which may not hold for all unsafe behaviors or model architectures.

## Confidence

- **High confidence**: Claims about UWM outperforming training-based methods (Safe-CLIP) on safe queries and preserving knowledge better, supported by direct comparisons in Table 2.
- **Medium confidence**: Claims about the general mechanism of unsafe weight identification via activation discrepancy, as the specific scoring function is well-defined but its universality across different unsafe concepts is not fully explored.
- **Medium confidence**: Claims about weight negation being superior to other manipulations (e.g., zeroing), as the evidence is based on ablation studies but alternative values of α are not extensively explored.

## Next Checks

1. **Cross-dataset generalization test**: Apply UWM trained on ViSU to a different safety benchmark (e.g., toxic image captioning) and measure whether safety improvements transfer without retraining.
2. **Ablation of calibration set size**: Systematically vary the number of calibration tuples per concept (e.g., 50, 200, 400) and measure the impact on both safety metrics and knowledge preservation to quantify sensitivity to calibration data.
3. **Exploration of alternative α values**: Extend Figure 6 to include finer-grained α sweeps (e.g., -1.0, -0.5, -0.25, 0.0) and measure not just GS and Vs-Ts but also specific downstream task performance to identify optimal tradeoffs.