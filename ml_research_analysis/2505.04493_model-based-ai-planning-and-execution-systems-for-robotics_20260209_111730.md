---
ver: rpa2
title: Model-Based AI planning and Execution Systems for Robotics
arxiv_id: '2505.04493'
source_url: https://arxiv.org/abs/2505.04493
tags:
- state
- skill
- robot
- system
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews model-based planning and execution systems for
  robotics (MPERs), which use AI planning algorithms to automatically combine basic
  robot skills to solve diverse tasks. The authors examine various MPERs including
  ROSPlan, CLIPS Executive, Plansys2, SkiROS2, Skill-Based Architecture, and AOS,
  comparing their modeling approaches, decision-making mechanisms, state representations,
  integration methods, and impacts.
---

# Model-Based AI planning and Execution Systems for Robotics

## Quick Facts
- **arXiv ID:** 2505.04493
- **Source URL:** https://arxiv.org/abs/2505.04493
- **Reference count:** 40
- **Primary result:** MPERs combine AI planning with robot skills, offering transparency and explainability but face challenges in modeling complexity and integration.

## Executive Summary
This paper surveys model-based planning and execution systems (MPERs) for robotics, which integrate AI planning algorithms with robot skill code to autonomously solve tasks. The authors analyze several prominent MPER frameworks including ROSPlan, Plansys2, SkiROS2, and AOS, comparing their modeling approaches, decision-making mechanisms, and integration methods. They identify key advantages such as transparency, explainability, and accessibility for autonomous robot design, while highlighting challenges in handling real-world complexities and integrating with skill code. The paper proposes future research directions including multi-level hybrid architectures, improved sensing and monitoring, automated planning model simplification, and integration of anomaly detection and LLM technologies.

## Method Summary
The paper provides a comprehensive survey of MPERs by examining their fundamental components: modeling approaches (PDDL, SDL, Probabilistic Programming Languages), decision-making mechanisms (classical planning, MDP solvers, deep RL), state representations (fluents, execution states), and integration methods with robot skill code. The authors analyze how these systems combine to create autonomous robot architectures that automatically schedule and execute basic robot skills to achieve complex tasks. The survey methodology involves reviewing technical documentation, academic papers, and implementation guides for major MPER frameworks, with a focus on comparing their architectural trade-offs and practical implications for real-world deployment.

## Key Results
- MPERs provide transparency and explainability by making robot decisions traceable through planning models and execution logs
- Integration challenges remain significant, requiring careful mapping between planner expectations and skill code interfaces
- Future research opportunities include multi-level hybrid architectures, automated model simplification, and LLM integration for natural language task specification

## Why This Works (Mechanism)
MPERs work by creating a formal bridge between high-level task specifications and low-level robot capabilities. The planner generates sequences of actions based on a model of the environment and available skills, while execution monitoring ensures the robot adapts to real-world deviations. This separation of concerns allows engineers to focus on skill development independently from task planning, while maintaining centralized control over autonomous decision-making.

## Foundational Learning

**PDDL (Planning Domain Definition Language):** A standardized language for describing planning problems, including actions, preconditions, and effects. *Why needed:* Provides a common interface between planners and robot skills. *Quick check:* Can you write a simple PDDL file describing a "move" action between locations?

**Action Interface:** The code wrapper that connects planner commands to actual robot skill execution. *Why needed:* Translates abstract planner actions into concrete robot commands. *Quick check:* Does your skill interface correctly update the planner's knowledge base after execution?

**Replanning Loop:** The mechanism that monitors execution and generates new plans when deviations occur. *Why needed:* Handles real-world uncertainties and unexpected outcomes. *Quick check:* Does your system automatically replan when a skill fails or takes longer than expected?

**Knowledge Base (KB):** The shared state representation used by both planner and execution components. *Why needed:* Maintains consistency between planning assumptions and execution reality. *Quick check:* Are state updates synchronized between skill execution and planner monitoring?

## Architecture Onboarding

**Component Map:** Task Input → PDDL Model → Planner → Action Interface → Robot Skill Code → Environment → State Monitoring → Knowledge Base → Replanner

**Critical Path:** Model creation → Planner configuration → Action interface development → State monitoring setup → Task execution

**Design Tradeoffs:** Deterministic vs. probabilistic modeling (speed vs. robustness), centralized vs. distributed execution (control vs. scalability), PDDL vs. custom languages (standardization vs. expressiveness)

**Failure Signatures:** Model-reality gaps causing repeated failures, state synchronization errors leading to inconsistent plans, integration mismatches between planner expectations and skill capabilities

**First Experiments:**
1. Implement a simple "move to location" task using Plansys2 with a simulated robot
2. Test replanning by introducing deliberate skill failures and measuring recovery time
3. Compare execution logs between ROSPlan and Plansys2 for the same task to evaluate transparency differences

## Open Questions the Paper Calls Out

**Open Question 1:** How can MPER architectures enable planners to invoke other planners or policies as skills?
*Basis in paper:* Section V.A suggests hybrid multi-level architectures where both solvers and predefined policies are treated as skills.
*Why unresolved:* Current systems typically rigidly separate deliberative planning layers and reactive control layers.
*Evidence:* A demonstration of a planner successfully scheduling and invoking a sub-planner as a generic action.

**Open Question 2:** How can expressive planning models be automatically simplified to improve planning efficiency?
*Basis in paper:* Section V.C calls for algorithms to automatically simplify the model or detect properties like determinism.
*Why unresolved:* Expressive models (e.g., POMDPs) are computationally heavy, often hindering real-time performance.
*Evidence:* An algorithm that detects deterministic sub-components within a complex model and solves them via faster classical planners.

**Open Question 3:** Can LLMs be integrated to allow real-time task specification in natural language?
*Basis in paper:* Section V.E suggests LLM Integration to specify new tasks in real-time.
*Why unresolved:* LLMs may produce syntactically incorrect or logically inconsistent formal models, limiting reliability.
*Evidence:* A system that consistently translates natural language commands into valid PDDL constraints during execution without failure.

## Limitations
- The paper is a survey rather than an experimental study, so claims about MPER advantages are based on literature review rather than controlled comparisons
- Implementation details for newer frameworks like AOS are described conceptually but lack concrete code examples
- Real-world deployment challenges are discussed theoretically without empirical case studies demonstrating practical limitations

## Confidence
**High:** Comparative analysis of existing frameworks and their technical characteristics
**Medium:** Future research directions as informed projections rather than empirical validations

## Next Checks
1. Implement a simple task (e.g., pick-and-place) using at least two different MPER frameworks (ROSPlan and Plansys2) to empirically compare modeling effort and execution reliability
2. Test state synchronization mechanisms by introducing deliberate delays or failures in skill execution and measuring replanning effectiveness
3. Evaluate the explainability claims by conducting user studies where participants debug plan failures using different MPER frameworks' monitoring interfaces