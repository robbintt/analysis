---
ver: rpa2
title: 'Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World
  API Complexity'
arxiv_id: '2601.00268'
source_url: https://arxiv.org/abs/2601.00268
tags:
- user
- complexity
- agent
- error
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce WildAGTEval, a benchmark that evaluates LLM agents
  under real-world API complexity by incorporating specification-level constraints
  (e.g., ad-hoc formatting) and execution-level noise (e.g., irrelevant information).
  Using an assign-and-inject mechanism guided by real-world usage patterns, we construct
  60 complexity scenarios yielding ~32K test configurations.
---

# Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity

## Quick Facts
- **arXiv ID:** 2601.00268
- **Source URL:** https://arxiv.org/abs/2601.00268
- **Reference count:** 40
- **Primary result:** All complexity types degrade LLM agent performance; irrelevant information causes 27.3% average drop, cumulative complexity reduces performance by up to 63.2%

## Executive Summary
This paper introduces WildAGTEval, a benchmark that evaluates LLM agents under realistic API complexities rather than idealized conditions. The authors systematically inject specification-level constraints (like ad-hoc formatting requirements) and execution-level noise (such as irrelevant information) into an 86-function multi-domain API system using an assign-and-inject mechanism guided by real-world usage patterns. Their evaluation reveals significant performance degradation across all complexity types, with agents frequently distorting user intent when tasks become infeasible, highlighting critical gaps in current reasoning and error-handling mechanisms.

## Method Summary
WildAGTEval uses an 86-function multi-domain API system with 300 multi-turn conversations (3,525 API calls, ~4.7 turns, ~2.5 calls/turn). The benchmark employs an assign-and-inject mechanism to create 60 complexity scenarios yielding ~32K test configurations. Agents operate in zero-shot ReAct-style inference (Thought + Action) with max 15 steps per turn. Two evaluation modes are used: isolated complexity (gold history) and cumulative complexity (agent predictions fed forward). Performance is measured through API call accuracy (predicted core APIs must match gold core APIs) and error-handling accuracy via LLM-Judge (1–5 scale).

## Key Results
- All complexity types degrade agent performance, with irrelevant information causing the greatest average drop of 27.3%
- Cumulative complexity reduces performance by up to 63.2% compared to baseline
- Agents frequently distort user intent when tasks are infeasible, rather than gracefully failing
- Specification-level constraints and execution-level noise both significantly impact accuracy

## Why This Works (Mechanism)
The benchmark works by systematically introducing real-world API complexities that agents encounter in production environments. The assign-and-inject mechanism ensures that complexities are applied consistently across scenarios while maintaining ecological validity. By using both isolated and cumulative evaluation modes, the benchmark captures both individual complexity impacts and their compounding effects over multi-turn interactions.

## Foundational Learning
- **Assign-and-inject mechanism**: Systematically applies real-world API complexities to benchmark scenarios; needed to create reproducible test conditions that mirror production environments; quick check: verify complexity types align with reported 8 categories
- **ReAct-style zero-shot inference**: Thought + Action framework for function calling without fine-tuning; needed to evaluate out-of-the-box agent performance; quick check: confirm 15-step limit per turn
- **LLM-Judge evaluation**: Automated assessment of error-handling quality on 1–5 scale; needed to scale evaluation beyond manual inspection; quick check: validate judge prompt templates match reported criteria

## Architecture Onboarding
- **Component map**: Conversation Generator -> Assign-and-Inject Mechanism -> Agent Executor -> LLM-Judge Evaluator
- **Critical path**: Input conversation → complexity injection → agent prediction → accuracy/error handling assessment → results aggregation
- **Design tradeoffs**: Zero-shot evaluation vs. fine-tuned performance, comprehensive complexity coverage vs. benchmark complexity, automated judging vs. human evaluation quality
- **Failure signatures**: Omission of functional dependencies, confusion of irrelevant data with valid results, intent distortion when tasks are infeasible
- **First experiments**: 1) Run isolated evaluation on stratified 50-conversation subset, 2) Test cumulative evaluation with feature error complexity, 3) Measure performance drop from irrelevant information injection

## Open Questions the Paper Calls Out
None

## Limitations
- Dependencies on unknown implementation details in conversation generation pipeline and validation prompts create medium confidence in specific performance metrics
- Reliance on a specific 86-function API system and 300 curated conversations may limit generalizability to all production environments
- Database schemas and initial states required for consistent execution are not fully documented

## Confidence
- **High**: Overall degradation trends across complexity types (comprehensive 32K test configurations, sound methodology)
- **Medium**: Specific magnitude of performance drops (27.3% irrelevant information drop) due to partial specification of conversation generation and validation details
- **Low**: Generalizability to production systems (specific API system and conversation dataset may not capture all real-world variations)

## Next Checks
1. Reconstruct the conversation generation pipeline using intent primitive templates to verify the 300 conversation dataset structure and 3,525 API call distribution
2. Implement a minimal database schema matching the 86 functions and validate execution consistency across all complexity types
3. Replicate the assign-and-inject mechanism to generate a subset of 60 complexity scenarios and measure performance degradation patterns against reported metrics