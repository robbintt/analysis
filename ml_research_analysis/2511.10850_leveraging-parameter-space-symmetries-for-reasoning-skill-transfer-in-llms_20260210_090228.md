---
ver: rpa2
title: Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs
arxiv_id: '2511.10850'
source_url: https://arxiv.org/abs/2511.10850
tags:
- reasoning
- tulu3
- wang
- zhang
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of transferring specialized reasoning
  skills between independently trained Large Language Models (LLMs) without incurring
  negative interference. The authors propose aligning the parameter spaces of the
  source and target models using the inherent symmetries in Transformer architectures,
  specifically adapting rotation and scaling symmetries for modern Grouped-Query Attention
  (GQA) layers and permutation symmetries for SwiGLU layers.
---

# Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs

## Quick Facts
- arXiv ID: 2511.10850
- Source URL: https://arxiv.org/abs/2511.10850
- Reference count: 30
- Key outcome: Parameter space alignment significantly improves reasoning skill transfer between diverged LLMs, with aligned models outperforming standard task arithmetic and even surpassing source reasoning models on some benchmarks

## Executive Summary
This paper addresses the challenge of transferring specialized reasoning skills between independently trained Large Language Models without causing negative interference. The authors propose aligning parameter spaces using inherent symmetries in Transformer architectures—specifically rotation and scaling symmetries for Grouped-Query Attention layers and permutation symmetries for SwiGLU layers—before performing task arithmetic. By first transforming the target model's weights to align with the source model's coordinate system, they achieve substantial improvements in reasoning performance on challenging benchmarks like AIME24 and AMC23. The Tulu3(w.align)+R model even surpasses the source Nemotron-Nano reasoning model on AIME24, demonstrating the effectiveness of this alignment-first strategy.

## Method Summary
The method involves extracting a reasoning skill vector from a source model (Nemotron-Nano - Llama-3.1-Instruct), then aligning a target model's (Tulu3) parameter space to match the reference model's geometry before adding the skill vector. The alignment process uses the Hungarian algorithm to solve permutation alignment for SwiGLU feed-forward layers, and Orthogonal Procrustes (SVD) combined with quartic equation solving for rotation and scaling alignment of Grouped-Query Attention layers. After alignment, the skill vector is added to the transformed target weights to produce the final model with transferred reasoning capabilities.

## Key Results
- Aligned models consistently outperform standard task arithmetic on reasoning benchmarks (AIME24, AMC23)
- Tulu3(w.align)+R model surpasses source Nemotron-Nano on AIME24 benchmark
- Rotation alignment provides the largest accuracy gains among alignment components
- Despite improvements, negative interference still degrades general capabilities (IFEval/MMLU scores drop)

## Why This Works (Mechanism)

### Mechanism 1: Permutation Alignment in SwiGLU Feed-Forward Layers
- **Claim:** Independently trained models may implement identical functions using different neuron orderings; resolving this via permutation improves weight correlation
- **Mechanism:** Uses Hungarian algorithm to solve linear assignment problem minimizing L2 distance between Gate, Up, and Down projection matrices, reordering intermediate neurons to align functionally
- **Core assumption:** Intermediate neurons in SwiGLU layers are functionally permutable with one-to-one mapping between source and target models
- **Break condition:** Fails when models have diverged to point where neurons are polysemantic or have specialized into non-corresponding roles

### Mechanism 2: Rotation and Scale Alignment in Grouped-Query Attention (GQA)
- **Claim:** Internal basis of Query/Key subspaces can rotate and scale without affecting output; resolving these symmetries aligns representational geometry
- **Mechanism:** Applies Orthogonal Procrustes (SVD) to find optimal rotation matrix and solves quartic equation to find scaling factor for grouped query heads
- **Core assumption:** Dimensionality of subspaces remains consistent with primarily linear (rotation/scale) rather than non-linear warping relationships
- **Break condition:** Insufficient when GQA groups have structured themselves fundamentally differently across training runs

### Mechanism 3: Mitigation of Negative Interference via Geometric Alignment
- **Claim:** Standard task arithmetic fails because vector addition operates on misaligned coordinates; alignment transforms skill vector into target model's native coordinate system
- **Mechanism:** Transforms target model weights to align with reference model before adding skill vector, ensuring added weights update correct functional directions
- **Core assumption:** Skill vector is transferable if destination coordinate system is rotated/scaled to match source
- **Break condition:** Cannot transfer skills relying on specific knowledge embeddings that don't exist in target model

## Foundational Learning

- **Concept: Task Arithmetic**
  - **Why needed here:** Baseline operation the paper seeks to improve; skill vector defined as θ_fine-tuned - θ_base
  - **Quick check question:** If you add a reasoning vector to a model, are you adding new knowledge or re-weighting existing connections? (Answer: Ideally re-weighting existing connections to elicit reasoning behavior)

- **Concept: Grouped-Query Attention (GQA)**
  - **Why needed here:** Alignment math is specific to GQA where multiple Query heads share one Key head
  - **Quick check question:** Why does the paper solve for one rotation matrix per group rather than per head? (Answer: Because Key head is shared by group, so Query heads must rotate in sync with their shared Key)

- **Concept: Symmetries in Neural Networks**
  - **Why needed here:** Core thesis that weight matrices are not unique coordinates; can change (permute/rotate) without changing function
  - **Quick check question:** Why does SGD result in misaligned models? (Answer: Random initialization and data ordering cause SGD to converge to different "rotated" versions of equivalent functions)

## Architecture Onboarding

- **Component map:** Nemotron-Nano -> Llama-3.1-Instruct -> Tulu3 (model receiving skill)
- **Critical path:** Extract Skill Vector → Compute Symmetries → Transform Target → Transfer Skill
- **Design tradeoffs:** Weight-based alignment is faster but activation-based aligns based on functional output; rotation offers major gains while scaling provides diminishing returns
- **Failure signatures:** Performance drop on general tasks (IFEval/MMLU), identity permutations indicating insufficient model divergence
- **First 3 experiments:**
  1. Baseline Reproduction: Implement standard Task Arithmetic to quantify negative interference gap
  2. Rotation Ablation: Implement only rotation alignment for GQA layers as biggest contributor to accuracy gains
  3. Interference Check: Compare aligned transfer model against baseline on general benchmarks to verify alignment doesn't amplify catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does parameter space alignment effectively transfer non-reasoning skills or work on model architectures outside the Llama ecosystem?
- **Basis in paper:** [explicit] The conclusion states: "For future work, we plan to extend this analysis to other model families and skills beyond reasoning."
- **Why unresolved:** Current study restricts experiments to Llama-3.1-8B family and focuses exclusively on mathematical and logical reasoning capabilities
- **What evidence would resolve it:** Demonstrating improved performance on tasks like code generation or multilingual translation when transferring skills between distinct architectures (e.g., Gemma or Mistral)

### Open Question 2
- **Question:** To what extent does training divergence necessitate permutation alignment compared to rotation and scaling?
- **Basis in paper:** [inferred] Ablation study notes optimal permutations were "always the identity," leading authors to hypothesize models "haven't diverged enough" to require re-ordering neurons
- **Why unresolved:** Experiment used models with shared ancestor; unknown if permutation becomes dominant factor for aligning models trained from different initializations or with severe drift
- **What evidence would resolve it:** Applying alignment method to models with significantly divergent training trajectories and observing if non-identity permutations yield larger performance gains

### Open Question 3
- **Question:** Can alignment methods be adapted to mitigate negative interference that degrades target model's general capabilities?
- **Basis in paper:** [inferred] Table 3 shows target model's instruction-following accuracy drops significantly (from 83.5 to ~60) after skill transfer, and alignment fails to recover this performance
- **Why unresolved:** Paper successfully transfers reasoning skills but doesn't address destruction of target model's original utility, known issue with task arithmetic
- **What evidence would resolve it:** Transfer mechanism that maintains target model's baseline scores on general benchmarks while adding new reasoning capability

## Limitations

- **Identity Permutations:** Optimal permutations found were always identity, suggesting models haven't diverged enough to require hard re-ordering, questioning practical necessity of this component
- **Marginal Scaling Benefits:** Quartic equation solving for scaling factor shows only minor improvements over rotation alone, suggesting scaling may offer diminishing returns
- **Incomplete Negative Interference Mitigation:** Despite alignment improvements, general capabilities (IFEval/MMLU) still degrade after skill transfer

## Confidence

**High Confidence:**
- Mathematical framework for rotation alignment using Orthogonal Procrustes is well-established and correctly implemented
- Existence of parameter space symmetries (rotations, permutations, scaling) that preserve model function is theoretically sound
- Empirical observation that aligned models outperform non-aligned task arithmetic on reasoning benchmarks is reproducible

**Medium Confidence:**
- Practical necessity of permutation alignment given identity permutations in experiments
- Marginal benefit of scaling alignment versus rotation alone
- Generalizability of findings across different model families and divergence levels

**Low Confidence:**
- Whether skill vector truly captures reasoning capability versus other correlated factors
- How method scales to larger model differences or different architectural choices
- Long-term stability of transferred skills under continued training

## Next Checks

1. **Cross-Divergence Validation:** Test alignment method on models with deliberately increased architectural divergence (different layer widths, attention patterns) to determine breaking points where permutation and rotation alignments fail

2. **Activation-Based vs. Weight-Based Comparison:** Implement both alignment variants and conduct systematic comparison across multiple skill transfer tasks to quantify when activation-based alignment's better preservation of general capabilities outweighs weight-based alignment's simplicity

3. **Knowledge Injection Experiment:** Create synthetic reasoning skills by injecting known knowledge patterns into base model, then attempt transfer using both aligned and non-aligned methods to isolate whether failures are due to alignment gaps versus missing knowledge in target model