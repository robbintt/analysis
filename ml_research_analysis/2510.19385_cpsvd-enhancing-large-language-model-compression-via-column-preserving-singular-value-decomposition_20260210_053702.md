---
ver: rpa2
title: 'CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular
  Value Decomposition'
arxiv_id: '2510.19385'
source_url: https://arxiv.org/abs/2510.19385
tags:
- compression
- ratio
- duo-svd
- language
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Duo-SVD, a novel training-free framework
  for compressing large language models (LLMs) using Singular Value Decomposition
  (SVD). The key insight is that SVD approximation errors vary significantly across
  different columns and modules of a weight matrix, leading to suboptimal compression
  with uniform approaches.
---

# CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular Value Decomposition

## Quick Facts
- arXiv ID: 2510.19385
- Source URL: https://arxiv.org/abs/2510.19385
- Reference count: 40
- SVD-based compression method that preserves high-sensitivity columns while applying rank reduction to low-sensitivity ones, outperforming state-of-the-art methods on LLaMA and Mistral models.

## Executive Summary
This paper introduces Duo-SVD, a novel training-free framework for compressing large language models (LLMs) using Singular Value Decomposition (SVD). The key insight is that SVD approximation errors vary significantly across different columns and modules of a weight matrix, leading to suboptimal compression with uniform approaches. Duo-SVD addresses this through a dual-level optimization strategy: at the column level, it preserves high-sensitivity columns exactly while applying SVD only to low-sensitivity ones, using a greedy selection method and ternary search to find the optimal number of preserved columns; at the module level, it formulates compression ratio allocation as a global constrained optimization problem based on perturbation-induced model deviation, allowing adaptive allocation across modules while adhering to a target compression ratio. Extensive experiments on LLaMA-2, Mistral, and LLaMA-3.1 models demonstrate that Duo-SVD consistently outperforms state-of-the-art SVD-based compression methods, achieving lower perplexity (e.g., 5.12 vs. 6.32-7.84 for baselines on LLaMA-2 13B) and higher accuracy across downstream tasks. The method also shows compatibility with quantization and provides practical inference speedups.

## Method Summary
Duo-SVD is a training-free compression framework that leverages Singular Value Decomposition with column-preserving capabilities. The method operates through two complementary optimization levels: column-level and module-level. At the column level, it identifies and preserves high-sensitivity columns (those whose perturbation causes significant performance degradation) while applying SVD to compress low-sensitivity columns. This is achieved through a greedy selection algorithm combined with ternary search to determine the optimal number of columns to preserve. At the module level, Duo-SVD formulates compression ratio allocation as a constrained optimization problem that minimizes total perturbation-induced model deviation while satisfying a target compression ratio. The method treats each weight matrix module independently but allocates compression resources globally based on their relative sensitivities. The approach is compatible with existing quantization methods and can be applied to various LLM architectures including LLaMA, Mistral, and their variants.

## Key Results
- Achieves 5.12 perplexity on LLaMA-2 13B, outperforming baselines (6.32-7.84) while maintaining the same compression ratio
- Demonstrates consistent superiority across multiple model families including LLaMA-2, Mistral, and LLaMA-3.1
- Shows compatibility with quantization and provides practical inference speedups without sacrificing compression quality

## Why This Works (Mechanism)
The paper identifies that traditional SVD compression methods suffer from uniform approximation errors across weight matrix columns, leading to suboptimal compression. High-sensitivity columns contribute more to model performance, and their approximation errors cause disproportionate performance degradation. By preserving these columns exactly while applying SVD only to low-sensitivity columns, Duo-SVD minimizes the impact on model quality. The dual-level optimization ensures that compression resources are allocated where they matter most, both within individual matrices (column-level) and across the entire model (module-level).

## Foundational Learning

**Singular Value Decomposition (SVD)**
- *Why needed*: Core mathematical technique for matrix approximation and dimensionality reduction
- *Quick check*: Verify understanding of how SVD decomposes a matrix into U, Î£, V^T components and how truncation affects reconstruction error

**Model Sensitivity Analysis**
- *Why needed*: Identifies which columns/modules are most critical to preserve for maintaining model performance
- *Quick check*: Confirm understanding of how perturbation-based sensitivity measures are calculated and interpreted

**Constrained Optimization**
- *Why needed*: Enables optimal allocation of compression resources across model modules while meeting target compression ratios
- *Quick check*: Validate comprehension of how global optimization balances local module sensitivities with overall compression constraints

## Architecture Onboarding

**Component Map**
Input Model -> Sensitivity Analysis -> Column Selection -> SVD Compression -> Module-Level Optimization -> Compressed Model

**Critical Path**
The critical path involves sensitivity analysis to identify high-impact columns, followed by column preservation and SVD application to remaining columns, with module-level optimization ensuring global compression ratio targets are met.

**Design Tradeoffs**
The method trades computational overhead of sensitivity analysis against compression quality gains. The greedy column selection provides efficiency but may not find globally optimal column sets. Binary preservation vs. compression approach is simpler than continuous rank reduction but may be suboptimal in some cases.

**Failure Signatures**
- Poor sensitivity measure leads to wrong columns being preserved, causing unnecessary performance degradation
- Suboptimal compression ratio allocation across modules results in some modules being over-compressed while others could handle more
- Greedy column selection may miss better combinations of columns to preserve

**First Experiments**
1. Run sensitivity analysis on a single weight matrix to visualize column sensitivity distribution
2. Apply column-preserving SVD to a small module and compare perplexity with standard SVD compression
3. Test module-level compression ratio allocation on a subset of layers to validate the optimization framework

## Open Questions the Paper Calls Out
None

## Limitations
- Column sensitivity definition may not be universally applicable across all model architectures and tasks
- Greedy column selection may not find globally optimal column sets for preservation
- Binary preservation vs. compression approach may be suboptimal compared to more nuanced rank reduction strategies

## Confidence

- **High Confidence**: Claims about consistent superiority over baseline SVD methods across tested models and tasks
- **Medium Confidence**: Claims about compatibility with quantization and inference speedups
- **Medium Confidence**: Claims about the effectiveness of the greedy column selection method

## Next Checks

1. Test Duo-SVD on additional LLM architectures beyond LLaMA and Mistral families, including encoder-decoder models and multimodal models, to verify generalizability

2. Conduct ablation studies varying the sensitivity calculation method and perturbation magnitude to assess how sensitive the final compression quality is to these hyperparameters

3. Evaluate the compressed models on practical downstream applications (e.g., code generation, mathematical reasoning) rather than just standard benchmarks to assess real-world utility