---
ver: rpa2
title: Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?
arxiv_id: '2506.19467'
source_url: https://arxiv.org/abs/2506.19467
tags:
- llms
- human
- reasoning
- disagreement
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether reasoning capabilities in large
  language models (LLMs) improve their ability to model human annotator disagreement.
  The authors conduct a comprehensive evaluation across three reasoning settings (RLHF
  with/without Chain-of-Thought and RLVR), five model sizes (8B-671B parameters),
  two distribution expression methods, and two steering approaches, resulting in 60
  experimental setups across three tasks.
---

# Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?

## Quick Facts
- **arXiv ID**: 2506.19467
- **Source URL**: https://arxiv.org/abs/2506.19467
- **Reference count**: 36
- **Primary result**: RLVR-style reasoning degrades LLM performance in modeling human annotator disagreement when variance is high

## Executive Summary
This paper investigates whether reasoning capabilities in large language models improve their ability to capture human annotator disagreement. Through comprehensive experiments across three reasoning settings, five model sizes, two distribution expression methods, and two steering approaches, the authors find that RLVR-style reasoning actually harms disagreement modeling in high-variance scenarios while naive Chain-of-Thought reasoning can improve performance. The study reveals that RLVR optimization on deterministic tasks may systematically impair models' ability to capture multiple valid perspectives, suggesting caution when using such models for tasks requiring understanding of human disagreement.

## Method Summary
The authors conduct a systematic evaluation across three reasoning settings (RLHF with/without Chain-of-Thought and RLVR), five model sizes ranging from 8B to 671B parameters, two methods for expressing distributions, and two steering approaches. This creates 60 experimental setups across three tasks. The evaluation framework systematically compares how different reasoning approaches affect models' ability to capture human annotator disagreement versus simply predicting majority labels. The study uses both verbalized distribution methods and sampling-based approaches to test distribution expression, while also examining the impact of scaling model size on disagreement modeling capabilities.

## Key Results
- RLVR-style reasoning degrades performance in disagreement modeling when human annotation variance is high
- Naive Chain-of-Thought reasoning improves performance for RLHF models in disagreement modeling
- Verbalized distribution methods outperform sampling-based approaches for capturing disagreement
- Scaling model size more effectively boosts majority label prediction than disagreement modeling

## Why This Works (Mechanism)
The degradation of RLVR models in disagreement modeling appears to stem from their optimization on deterministic, verifiable tasks that may not require capturing multiple perspectives. RLVR training likely reinforces single-answer reasoning patterns that conflict with the need to model diverse human viewpoints. Chain-of-Thought reasoning, even when naive, may help RLHF models by encouraging more deliberative processing that considers alternative interpretations. The verbalized distribution methods may work better because they explicitly force models to consider and articulate multiple possibilities rather than relying on implicit sampling mechanisms.

## Foundational Learning

**Large Language Model Reasoning** - Understanding different reasoning approaches (RLHF, RLVR, CoT) is essential because they fundamentally shape how models process and generate responses. Quick check: Compare outputs from RLHF vs RLVR models on ambiguous prompts to observe reasoning differences.

**Human Annotator Disagreement** - Recognizing that human disagreement is task-dependent and often reflects legitimate multiple valid perspectives rather than errors. Quick check: Analyze annotation distributions on a subjective task to identify variance patterns.

**Distribution Expression Methods** - Knowing how models can represent uncertainty (verbalized vs sampling) is crucial for tasks requiring probabilistic reasoning. Quick check: Compare verbalized vs sampled outputs on a multi-choice task with known answer distributions.

## Architecture Onboarding

**Component Map**: Input Task -> Reasoning Module (RLHF/RLVR/CoT) -> Distribution Expression (Verbalized/Sampling) -> Output Prediction

**Critical Path**: The reasoning module selection critically determines downstream disagreement modeling capability, with RLVR showing systematic degradation in high-variance scenarios.

**Design Tradeoffs**: RLVR offers strong performance on deterministic tasks but sacrifices the ability to model disagreement; CoT adds computational overhead but can improve nuanced understanding; verbalized distributions require explicit reasoning but better capture multiple perspectives.

**Failure Signatures**: RLVR models consistently converge to single answers even when human annotations show high variance; sampling-based methods produce inconsistent outputs that don't match observed disagreement patterns.

**Three First Experiments**:
1. Test RLVR model on a task with known human disagreement to observe convergence patterns
2. Compare CoT vs non-CoT outputs from RLHF model on ambiguous prompts
3. Evaluate verbalized vs sampled distribution outputs on multi-choice tasks

## Open Questions the Paper Calls Out

The paper highlights several major uncertainties: generalizability of findings to tasks and domains beyond the three evaluated; lack of causal explanations for why RLVR degrades disagreement modeling; and whether these patterns hold for other types of subjective or ambiguous tasks. The analysis is based on correlations rather than mechanistic understanding, leaving open questions about whether degradation stems from optimization processes, training data characteristics, or their interaction.

## Limitations

- Findings may not generalize beyond the three specific tasks evaluated in the study
- Analysis is correlational rather than causal, lacking mechanistic explanations for observed behaviors
- Focus on tasks with known high disagreement may not represent broader application scenarios

## Confidence

- **High confidence**: Empirical finding that RLVR degrades disagreement modeling in high-variance scenarios is well-supported
- **Medium confidence**: Verbalized distribution methods outperforming sampling approaches is supported but implementation-dependent
- **Medium confidence**: Scaling improving majority prediction more than disagreement modeling is observed but needs broader validation

## Next Checks

1. Test reasoning approaches on additional subjective tasks from different domains (creative writing evaluation, ethical dilemmas) to verify RLVR degradation pattern persistence

2. Conduct ablation studies on RLVR training objectives to isolate whether degradation stems from verification-focused loss function, reasoning patterns in training data, or optimization process

3. Evaluate whether fine-tuning RLVR models with disagreement-aware objectives can recover ability to model multiple perspectives through multi-task training