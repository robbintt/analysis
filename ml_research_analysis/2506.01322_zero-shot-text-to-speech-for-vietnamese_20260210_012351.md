---
ver: rpa2
title: Zero-Shot Text-to-Speech for Vietnamese
arxiv_id: '2506.01322'
source_url: https://arxiv.org/abs/2506.01322
tags:
- audio
- phoaudiobook
- speech
- dataset
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhoAudiobook, a 941-hour high-quality Vietnamese
  text-to-speech dataset derived from audiobooks. The authors train three zero-shot
  TTS models (VALL-E, VoiceCraft, XTTS-v2) on this dataset and evaluate them against
  a baseline model (viXTTS) trained on the viVoice dataset.
---

# Zero-Shot Text-to-Speech for Vietnamese

## Quick Facts
- arXiv ID: 2506.01322
- Source URL: https://arxiv.org/abs/2506.01322
- Authors: Thi Vu; Linh The Nguyen; Dat Quoc Nguyen
- Reference count: 13
- This paper introduces PhoAudiobook, a 941-hour high-quality Vietnamese text-to-speech dataset derived from audiobooks. The authors train three zero-shot TTS models (VALL-E, VoiceCraft, XTTS-v2) on this dataset and evaluate them against a baseline model (viXTTS) trained on the viVoice dataset. The models are tested on both seen and unseen speakers as well as out-of-distribution data. Results show that XTTS-v2 trained on PhoAudiobook consistently outperforms the baseline across all metrics including WER, MCD, RMSE F0, MOS, and SMOS. VALL-E and VoiceCraft demonstrate particular strength in synthesizing short sentences. The PhoAudiobook dataset is publicly released to support further Vietnamese TTS research.

## Executive Summary
This paper addresses the need for high-quality Vietnamese text-to-speech synthesis by introducing PhoAudiobook, a large-scale dataset of 941 hours from 735 speakers. The authors evaluate three zero-shot TTS models (VALL-E, VoiceCraft, XTTS-v2) trained on this dataset against a baseline model trained on viVoice. The models are tested across multiple scenarios including seen and unseen speakers, as well as out-of-distribution data. Results demonstrate consistent performance improvements across all evaluation metrics, with XTTS-v2 showing the most comprehensive gains and VALL-E/VoiceCraft excelling at short-sentence synthesis. The paper also highlights architectural limitations of XTTS-v2 in handling short text inputs.

## Method Summary
The authors created PhoAudiobook from 2,400 Vietnamese audiobooks, filtering for single-speaker segments of 10-20 seconds and normalizing text. They trained three zero-shot TTS models on this dataset: VALL-E (neural codec language model), VoiceCraft (token-infilling approach), and XTTS-v2 (diffusion-based encoder-decoder). Each model was evaluated on four test sets: seen speakers (PAB-S), unseen speakers (PAB-U), VIVOS (short sentences), and viVoice (out-of-distribution). The baseline viXTTS was trained on the viVoice dataset. All models were fine-tuned from public checkpoints with specific hyperparameters, and performance was measured using WER, MCD, RMSE F0, MOS, and SMOS metrics.

## Key Results
- XTTS-v2 trained on PhoAudiobook (XTTS-v2PAB) consistently outperforms viXTTS baseline across all metrics including WER, MCD, RMSE F0, MOS, and SMOS
- VALL-E and VoiceCraft demonstrate superior performance on short sentences compared to XTTS-v2, which tends to generate redundant speech at the end of outputs
- XTTS-v2PAB achieves substantially higher speaker similarity (SMOS) than baseline, indicating better voice cloning for both seen and unseen speakers
- PhoAudiobook's longer audio segments (10-20 seconds) compared to typical datasets (<10 seconds) contribute to improved model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-form audio training data (10-20 seconds) improves zero-shot TTS generalization for Vietnamese.
- Mechanism: Longer audio segments provide richer prosodic and speaker identity information during training, enabling better voice cloning for unseen speakers. The extended context allows models to capture speaking style patterns that short clips (<10 seconds) cannot convey.
- Core assumption: The performance improvement stems from audio duration rather than coincidental data quality differences between PhoAudiobook and viVoice.
- Evidence anchors:
  - [abstract] "PhoAudiobook consistently enhances model performance across various metrics"
  - [section 2.2] "previous datasets primarily consist of audio segments shorter than 10 seconds. PhoAudiobook addresses this limitation by providing audio samples ranging from 10 to 20 seconds"
  - [corpus] Weak direct evidence—no corpus neighbors isolate duration as a causal variable
- Break condition: If ablation studies showed short-form training on PhoAudiobook matched long-form performance, duration would not be the causal factor.

### Mechanism 2
- Claim: Neural codec language modeling (VALL-E, VoiceCraft) handles short-sentence synthesis more robustly than encoder-decoder approaches (XTTS-v2).
- Mechanism: Token-infilling and autoregressive codec modeling directly predict discrete audio tokens conditioned on text, avoiding cascaded pipeline error accumulation. For short inputs, this reduces architectural tendency to generate extraneous speech.
- Core assumption: The VIVOS performance gap reflects architectural differences, not training data distribution.
- Evidence anchors:
  - [section 4] "VALL-EPAB and VoiceCraftPAB are more adept at handling short sentences... XTTS-v2-based models often generate redundant or rambling speech at the end of the output"
  - [section 3.1] "VoiceCraft employs a Transformer decoder architecture with a novel token rearrangement procedure"
  - [corpus] Pseudo-Autoregressive Neural Codec Language Models paper addresses similar AR vs NAR tradeoffs, supporting the mechanism
- Break condition: If XTTS-v2 with identical codec-based training matched VALL-E on short sentences, the architecture would not be causal.

### Mechanism 3
- Claim: Speaker-verified training data with explicit speaker IDs improves voice cloning fidelity.
- Mechanism: Accurate speaker labels enable models to learn speaker embeddings without contamination from multi-speaker samples, improving speaker similarity metrics (SMOS) for both seen and unseen speakers.
- Core assumption: The wav2vec2-bartpho filtering effectively removed multi-speaker contamination.
- Evidence anchors:
  - [section 2.1] "we use the wav2vec2-bartpho model to identify and filter out short audio samples containing multiple speakers"
  - [section 4] "XTTS-v2PAB also produces substantially higher SMOS... indicating that the speech it generates more closely resembles the reference speaker"
  - [corpus] VoiceCraft-X paper emphasizes voice cloning quality, consistent with speaker verification importance
- Break condition: If unfiltered data with speaker IDs matched filtered performance, the multi-speaker filtering would be unnecessary.

## Foundational Learning

- Concept: Neural audio codecs (e.g., EnCodec)
  - Why needed here: VALL-E and VoiceCraft operate on discrete codec tokens rather than mel-spectrograms. Understanding codec quantization is essential for debugging token prediction issues.
  - Quick check question: Can you explain why codec tokens enable language modeling approaches for speech?

- Concept: Zero-shot voice cloning via speaker embeddings
  - Why needed here: All three models condition on reference audio to clone voices. You must understand how speaker encoders (e.g., d-vector, x-vector) extract identity features.
  - Quick check question: How does zero-shot TTS differ from speaker adaptation fine-tuning?

- Concept: Autoregressive vs. non-autoregressive TTS tradeoffs
  - Why needed here: The paper shows VALL-E/VoiceCraft (AR) excel at short inputs while XTTS-v2 (modified encoder-decoder) struggles. Understanding inference speed vs. quality tradeoffs is critical.
  - Quick check question: Why might autoregressive models generate less redundant speech for short inputs?

## Architecture Onboarding

- Component map:
  - VALL-E: Text → phonemes → phoneme encoder || Audio → EnCodec tokens → transformer decoder (token prediction)
  - VoiceCraft: Similar to VALL-E with token rearrangement for infilling
  - XTTS-v2: Text → BPE tokenizer → GPT-style decoder || Audio → VAE encoder → diffusion decoder

- Critical path: Data preparation (phonemization with dialect detection, codec tokenization) → training (select checkpoint on validation loss) → inference (prompt audio + text input)

- Design tradeoffs:
  - AR models (VALL-E/VoiceCraft): Better short-sentence handling, slower inference due to sequential token generation
  - XTTS-v2: Faster inference via diffusion, but generates redundant speech on short inputs
  - 10-20s training clips: Better prosody capture but requires more GPU memory

- Failure signatures:
  - High WER on short inputs with XTTS-v2 → check for trailing redundant speech (architectural issue noted in paper)
  - Low SMOS → verify speaker ID consistency; check for multi-speaker contamination in training data
  - Dialect mismatch → verify phonemizer uses correct Vietnamese dialect (paper fine-tuned dialect classifier)

- First 3 experiments:
  1. Reproduce XTTS-v2_PAB training on a 50-hour subset of PhoAudiobook; validate SMOS improvement over viXTTS baseline
  2. Ablate audio duration: train separate models on clips <10s vs. 10-20s; compare WER/SMOS on unseen speakers
  3. Test VALL-E_PAB on short vs. long text inputs; quantify the "redundant speech" issue relative to XTTS-v2

## Open Questions the Paper Calls Out
- Can models trained on PhoAudiobook handle code-switched input combining Vietnamese and English effectively?
- What architectural modifications would enable XTTS-v2 to synthesize short sentences without generating redundant or rambling speech?
- How well do PhoAudiobook-trained models generalize to noisy, uncontrolled acoustic environments?

## Limitations
- Lack of ablation studies isolating which factors drive the performance improvements
- No statistical significance testing for MOS/SMOS subjective evaluation results
- Training of VALL-E and VoiceCraft requires Vietnamese dialect-labeled data that was not released

## Confidence
- **High confidence**: XTTS-v2_PAB outperforms viXTTS baseline on PhoAudiobook test sets (WER, MCD, RMSE F0, MOS, SMOS)
- **Medium confidence**: PhoAudiobook's 10-20 second audio duration is the primary driver of improved zero-shot TTS performance
- **Medium confidence**: VALL-E and VoiceCraft architectures handle short sentences better than XTTS-v2
- **Low confidence**: The wav2vec2-bartpho filtering effectively removed multi-speaker contamination

## Next Checks
1. Conduct an ablation study training XTTS-v2 on PhoAudiobook subsets with different audio durations (<10s vs 10-20s) to isolate whether duration is the primary performance driver.
2. Perform statistical significance testing on MOS and SMOS scores across all models and test sets to validate that observed differences are not due to random variation in subjective evaluations.
3. Systematically analyze XTTS-v2's "redundant speech" problem on short inputs by measuring the ratio of generated vs reference audio length across different sentence lengths, and test whether architectural modifications (e.g., attention masking) can mitigate this issue.