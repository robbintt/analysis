---
ver: rpa2
title: A Large-Scale Benchmark for Vietnamese Sentence Paraphrases
arxiv_id: '2502.07188'
source_url: https://arxiv.org/abs/2502.07188
tags:
- paraphrase
- sentence
- dataset
- paraphrases
- nguyen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViSP, a large-scale Vietnamese paraphrase
  dataset with 1.2M original-paraphrase pairs. It was constructed using a hybrid approach
  combining automatic paraphrase generation with manual evaluation to ensure high
  quality.
---

# A Large-Scale Benchmark for Vietnamese Sentence Paraphrases

## Quick Facts
- **arXiv ID**: 2502.07188
- **Source URL**: https://arxiv.org/abs/2502.07188
- **Reference count**: 29
- **Primary result**: Introduced ViSP, a large-scale Vietnamese paraphrase dataset with 1.2M pairs, where BARTpho-wordlarge achieved best performance with BLEU-4 of 72.06 and ROUGE-2 of 76.06

## Executive Summary
This paper introduces ViSP, a large-scale Vietnamese paraphrase dataset containing 1.2 million original-paraphrase pairs. The dataset was constructed using a hybrid approach that combined automatic paraphrase generation with manual evaluation to ensure high quality. It was built from existing Vietnamese corpora and manually verified by annotators. The authors evaluated traditional methods like EDA and back-translation, along with baseline models (BART, T5) and large language models (GPT-4o, Gemini-1.5, etc.). The dataset and findings serve as a valuable foundation for Vietnamese paraphrase research and applications.

## Method Summary
The ViSP dataset was constructed using a hybrid approach combining automatic paraphrase generation with manual evaluation. The authors started with existing Vietnamese corpora and applied automatic generation techniques, followed by manual verification by annotators to ensure quality. The evaluation framework included traditional methods like EDA and back-translation, as well as neural models including BART and T5 variants, and large language models accessed through APIs. The paper reports comprehensive results across multiple evaluation metrics including BLEU-4 and ROUGE scores.

## Key Results
- ViSP dataset contains 1.2M Vietnamese paraphrase pairs
- BARTpho-wordlarge achieved the best performance with BLEU-4 of 72.06 and ROUGE-2 of 76.06 on the validation set
- Large language models (GPT-4o, Gemini-1.5) were evaluated through third-party APIs

## Why This Works (Mechanism)
The hybrid approach of combining automatic generation with manual verification ensures high-quality paraphrases while scaling to a large dataset. The use of multiple evaluation methods (traditional, neural, and LLM-based) provides comprehensive benchmarking. The dataset construction from existing corpora leverages established resources while expanding their utility for paraphrase tasks.

## Foundational Learning
- **Paraphrase quality metrics**: BLEU-4 and ROUGE scores are standard for evaluating text generation quality, but may not fully capture semantic equivalence in paraphrases. Quick check: Compare these metrics against human judgments of paraphrase quality.
- **Cross-lingual model evaluation**: Using APIs to evaluate large language models across languages requires careful consideration of model limitations and evaluation consistency. Quick check: Verify API evaluation consistency across different model versions.
- **Dataset construction methodology**: Hybrid approaches combining automatic generation with manual verification balance scale and quality. Quick check: Analyze the ratio of automatic to manual processing in the pipeline.

## Architecture Onboarding

**Component map**: Existing Vietnamese corpora -> Automatic generation -> Manual verification -> Model training -> Evaluation (BLEU-4, ROUGE)

**Critical path**: Data preparation (existing corpora) → Automatic generation → Manual verification → Model training → Evaluation

**Design tradeoffs**: The hybrid approach balances dataset size (1.2M pairs) with quality through manual verification, but this process may introduce subjectivity and scalability challenges.

**Failure signatures**: 
- Low inter-annotator agreement indicates subjective quality assessment
- BLEU/ROUGE scores may not capture semantic equivalence
- API-based LLM evaluation may have consistency issues

**First experiments**:
1. Train BARTpho-wordlarge on ViSP and evaluate on held-out test set
2. Compare EDA and back-translation approaches on small sample
3. Evaluate GPT-4o and Gemini-1.5 on paraphrase generation task

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Manual verification process may introduce subjectivity in quality assessment without clear inter-annotator agreement metrics
- BLEU-4 and ROUGE metrics may not optimally capture paraphrase quality, being designed for machine translation
- Dataset construction relied on existing corpora, potentially limiting domain diversity
- LLM evaluation through APIs lacked detailed hyperparameter tuning and ablation studies

## Confidence
- **Dataset quality**: High - substantial size (1.2M pairs) with hybrid construction approach
- **Benchmark results**: Medium - sound methodology but metric choice may not optimally capture paraphrase quality
- **Cross-lingual comparisons**: Medium-Low - API-based evaluation without detailed experimental controls

## Next Checks
1. Conduct human evaluation studies with multiple annotators to establish inter-annotator agreement and validate quality metrics
2. Perform domain analysis to assess coverage and potential biases in topic distribution
3. Implement alternative evaluation metrics specifically designed for paraphrase quality (e.g., semantic similarity measures, diversity metrics) to complement BLEU and ROUGE scores