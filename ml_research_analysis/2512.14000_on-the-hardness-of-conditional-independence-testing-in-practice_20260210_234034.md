---
ver: rpa2
title: On the Hardness of Conditional Independence Testing In Practice
arxiv_id: '2512.14000'
source_url: https://arxiv.org/abs/2512.14000
tags:
- conditional
- test
- dkcin
- kernel
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the practical hardness of conditional independence
  testing, focusing on Kernel-based Conditional Independence (KCI) tests. The authors
  show that the main difficulty arises from errors in estimating conditional mean
  embeddings, which inflate Type-I error.
---

# On the Hardness of Conditional Independence Testing In Practice

## Quick Facts
- **arXiv ID:** 2512.14000
- **Source URL:** https://arxiv.org/abs/2512.14000
- **Reference count:** 40
- **Primary result:** KCI tests suffer Type-I error inflation primarily due to imperfect conditional mean embedding estimation, with kernel selection optimizing power but amplifying false rejections.

## Executive Summary
This paper investigates why Kernel-based Conditional Independence (KCI) tests frequently fail in practice, demonstrating that the primary source of Type-I error inflation is imperfect estimation of conditional mean embeddings (CMEs) rather than sample noise. The authors show that regression errors induce spurious dependence that the KCI statistic interprets as conditional dependence, violating test validity. Through theoretical analysis and synthetic experiments, they establish formal bounds linking regression error to test calibration failure and reveal a fundamental tension: while kernel selection can improve test power, it simultaneously increases false rejections when regressors are poorly trained.

## Method Summary
The method involves splitting data into training and test sets, using Kernel Ridge Regression on the training set to estimate conditional mean embeddings $\hat{\mu}_{A|C}$ and $\hat{\mu}_{B|C}$, then optimizing the conditioning kernel $k_C$ by maximizing empirical Signal-to-Noise Ratio on the training set. The KCI statistic is computed using an unbiased HSIC-like estimator on the test set, with null distribution calibration via Wild Bootstrap. The approach explicitly analyzes how regression error affects test validity and provides bounds for when the test remains valid.

## Key Results
- Regression error in CME estimation induces spurious dependence that inflates Type-I error
- SNR maximization for kernel selection increases both power and Type-I error when regressors are imperfect
- GCM is a special case of KCI with linear kernels, lacking flexibility to detect localized dependence
- Theoretical bounds establish conditions under which regression error invalidates test calibration

## Why This Works (Mechanism)

### Mechanism 1: Regression Error Induces Spurious Dependence
The primary driver of Type-I error is estimation error in conditional mean embeddings. When regression models are imperfect, residuals retain dependence on conditioning variable $C$. The KCI statistic interprets correlation of these error terms across samples as conditional dependence. If regression error variance decays slower than $O(1/n)$, the test statistic's variance decays at $O(1/\sqrt{n})$ rather than $O(1/n)$, causing significant exceedance of null thresholds calibrated for perfect estimation.

### Mechanism 2: Power Maximization Amplifies Noise
Optimizing the conditioning kernel $k_C$ to maximize test power increases sensitivity but amplifies Type-I error when regressors are poorly trained. The optimization identifies lengthscales that maximize spurious correlation between regression errors rather than true signal. This creates a fundamental tension where power maximization strategies inadvertently fit to regression noise artifacts.

### Mechanism 3: GCM as a Linear Kernel Limit of KCI
GCM corresponds to KCI with linear kernels on $A$ and $B$ and a global kernel on $C$ (equivalent to $\ell_C = \infty$). Because GCM averages conditional covariance over all $C$, it fails to detect dependence that cancels out globally (e.g., oscillating signs), whereas KCI with tuned Gaussian kernel can localize to specific $C$ regions.

## Foundational Learning

- **Concept: Conditional Mean Embeddings (CMEs)**
  - **Why needed here:** The paper centers on imperfect estimation of CMEs as the root cause of Type-I error inflation. Understanding CMEs as kernel representations of conditional distributions is required to diagnose error bounds.
  - **Quick check question:** How does the error $\Delta = \mu_{true} - \hat{\mu}$ affect the expectation of the KCI statistic under the null?

- **Concept: U-Statistics and Variance Decay**
  - **Why needed here:** The validity of KCI test relies on variance decaying fast enough relative to sample size ($n$). The paper proves regression errors change decay rate from $O(1/n)$ to $O(1/\sqrt{n})$, breaking standard calibration.
  - **Quick check question:** In a degenerate U-statistic (null hypothesis), what is the expected rate of variance decay, and how does regression error disturb this?

- **Concept: Hilbert-Schmidt Independence Criterion (HSIC)**
  - **Why needed here:** KCI is an extension of HSIC to conditional setting. Understanding HSIC as a measure of dependence in RKHS is necessary to interpret KCI operator and test statistic.
  - **Quick check question:** KCI aggregates conditional covariances. How does choice of kernel $k_C$ weight these covariances?

## Architecture Onboarding

- **Component map:** Data -> Train/Test Split -> Regression Module -> Kernel Selection Module -> Test Statistic Computer -> Calibration Engine
- **Critical path:**
  1. Train regressors on independent data (sample splitting mandatory)
  2. Select $k_C$ (if optimizing) on training set
  3. Compute residuals on test set
  4. Run Wild Bootstrap for threshold
- **Design tradeoffs:**
  - Flexible kernels (Gaussian) vs. Linear: Flexible kernels increase power but are more sensitive to regression error bias
  - Sample Splitting: Prevents overfitting but doesn't eliminate bias from imperfect regression
- **Failure signatures:**
  - High Type-I Error with Large $n$: Indicates regression error variance is non-negligible
  - Power Maximization Instability: Gradient optimization fails to converge or selects extreme lengthscales
  - Sensitivity to Training Size: Type-I error drops significantly when increasing training set size
- **First 3 experiments:**
  1. Baseline Null Check: Run KCI on data where $A \perp B | C$ with small vs. large training sets
  2. Kernel Lengthscale Sweep: On synthetic example, sweep $\ell_C^2$ and plot Type-I/II error
  3. GCM Comparison: Compare KCI (optimal $\ell_C$) against GCM ($\ell_C = \infty$) on oscillating covariance example

## Open Questions the Paper Calls Out

### Open Question 1
How can conditional independence tests be effectively calibrated to mitigate spurious residual dependence caused by imperfect regression, given that sample splitting alone is insufficient? The paper states users must carefully consider how to mitigate spurious residual dependence, something that sample splitting alone does not resolve.

### Open Question 2
Can a conditioning kernel selection strategy be developed that maximizes test power while rigorously controlling Type-I error inflation in the presence of regression errors? The paper highlights a fundamental tension where maximizing SNR tends to select kernels that capture spurious dependence from regression errors.

### Open Question 3
What are the practical estimation rates required for conditional mean embeddings (CME) to satisfy the theoretical validity bound $\nu_1 = o(1/n)$ in complex, non-parametric settings? The analysis establishes that for wild bootstrap to be valid, the variance decay must satisfy $\nu_1 = o(1/n)$.

## Limitations
- Analysis assumes smooth dependence and quadratic loss for regression, but real-world data may exhibit discontinuities
- SNR-based kernel selection heuristic lacks formal guarantees for non-convex optimization landscapes
- Theoretical bounds depend on specific smoothness and boundedness assumptions that may not hold in practice
- Paper doesn't fully characterize generalization to non-Gaussian kernels or alternative regression frameworks

## Confidence

- **High confidence:** The mechanism linking regression error to Type-I inflation is theoretically proven and experimentally validated
- **Medium confidence:** SNR maximization procedure's effectiveness is demonstrated empirically but relies on heuristics without rigorous convergence guarantees
- **Low confidence:** Paper doesn't fully address generalization to non-Gaussian kernels or alternative regression frameworks beyond KRR

## Next Checks

1. Test the regression error hypothesis on real-world CI testing benchmarks where ground truth independence is known, comparing Type-I error across different regression model complexities
2. Implement the SNR maximization procedure with multiple random initializations to verify it consistently finds the same lengthscale optimum
3. Evaluate whether alternative regression losses (e.g., Huber, quantile) reduce Type-I error inflation compared to quadratic loss for non-smooth conditional distributions