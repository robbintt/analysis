---
ver: rpa2
title: Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed
  Bandit
arxiv_id: '2402.06388'
source_url: https://arxiv.org/abs/2402.06388
tags:
- gradient
- policy
- convergence
- when
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the convergence of a policy gradient algorithm
  for Multi Armed Bandit problems, specifically focusing on the softmax parameterized
  policy gradient with L2 regularization. The authors prove convergence under appropriate
  technical hypotheses and test the algorithm numerically.
---

# Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed Bandit

## Quick Facts
- **arXiv ID:** 2402.06388
- **Source URL:** https://arxiv.org/abs/2402.06388
- **Reference count:** 33
- **Primary result:** Convergence of softmax policy gradient with L2 regularization is proven under technical conditions, with numerical experiments showing improved performance when initial policy is far from optimal.

## Executive Summary
This paper investigates the convergence of policy gradient algorithms for Multi Armed Bandit problems with L2 regularization. The authors prove that softmax parameterized policy gradient with L2 regularization converges under appropriate technical hypotheses. They demonstrate numerically that a time-dependent regularized procedure can improve performance over the canonical approach, especially when the initial guess is far from the optimal solution. The work bridges theoretical understanding of policy gradient methods in MAB settings with practical insights into regularization benefits.

## Method Summary
The method employs softmax policy gradient with L2 regularization to solve the MAB problem. The algorithm maintains a preference vector H that parameterizes a softmax policy. At each step, an action is sampled according to the policy, a reward is observed, and the preference vector is updated using a stochastic gradient estimate. The key innovation is the addition of an L2 regularization term that prevents parameter explosion and ensures existence of a unique optimum. The authors also explore time-dependent regularization where the coefficient decays over time, allowing escape from poor initializations while maintaining convergence guarantees.

## Key Results
- Convergence of regularized softmax policy gradient is proven under technical conditions
- Time-dependent regularization improves performance when initial policy is far from optimal
- Regularization prevents parameter explosion and ensures unique optimum existence
- Numerical experiments demonstrate effectiveness across various scenarios

## Why This Works (Mechanism)

### Mechanism 1
L2 regularization ensures existence of a unique optimum and prevents parameter explosion in softmax policy gradient MAB. The regularization term $-\gamma\|H\|^2$ makes the objective function $\mathcal{L}_\gamma(H)$ coercive at infinity when $\gamma > c^*$, where $c^* = \max_a q^*(a) - \min_a q^*(a)$. This transforms an ill-posed optimization (where optimal $H$ may have infinite components when $\gamma=0$) into a well-conditioned problem with a unique maximum $H^*$.

### Mechanism 2
The stochastic update (eq. 5) is an unbiased estimator of the true gradient $\nabla_H \mathcal{L}(H)$. The baseline $\bar{R}_t$ cancels in expectation (eq. 14), and the indicator-softmax difference $(1_{a=A_t} - \Pi_{H_t}(a))$ matches the softmax derivative structure (eq. 16). This fits the Robbins-Monro stochastic approximation framework.

### Mechanism 3
Time-dependent regularization $\gamma_t = \gamma_0/(1+\eta t)$ combined with decaying learning rate improves convergence from poor initializations. Initial large $\gamma_0$ biases the policy away from sub-optimal critical points (Dirac masses are critical points in unregularized MAB). Decaying $\gamma_t$ then allows convergence to near-optimal solutions without the permanent bias of fixed regularization.

## Foundational Learning

- **Stochastic Gradient Descent (SGD) and Robbins-Monro conditions**: The entire convergence proof rests on SGD theory requiring $\rho_t \to 0$ and $\sum \rho_t = \infty$. Quick check: Can you explain why a learning rate sequence must satisfy both conditions for convergence?

- **Softmax function and its gradient structure**: The policy parameterization $\Pi_H(A) = e^{H(A)}/\sum_a e^{H(a)}$ and its Jacobian (eq. 16) determine the gradient estimator form. Quick check: Derive $\partial \Pi_H(a)/\partial H(b)$ and show why the indicator-softmax difference appears in the gradient.

- **Multi-Armed Bandit formulation and exploration-exploitation**: The MAB reward structure $q^*(a)$ and the policy $\Pi_H$ define the objective; regularization affects exploration implicitly. Quick check: What happens to exploration when $\gamma$ is very large? When $\gamma \to 0$?

## Architecture Onboarding

- **Component map:** Preference vector H -> Softmax policy $\Pi_H$ -> Gradient estimator $g_t$ -> Learning rate scheduler $\rho_t$ -> (Optional) Regularization coefficient $\gamma_t$

- **Critical path:** 1. Initialize $H_0$ (uniform: all zeros; or biased: non-uniform) 2. Sample action $A_t \sim \Pi_{H_t}$ 3. Observe reward $R_t \sim \mathcal{R}(A_t)$ 4. Update baseline $\bar{R}_t$ 5. Compute gradient estimate $g_t$ (eq. 11) 6. Update $H_{t+1} = H_t + \rho_t g_t$ 7. (Optional) decay $\gamma_t$

- **Design tradeoffs:** Large fixed $\gamma$: Guaranteed convergence but biased solution; Small $\gamma$: Near-optimal solution but slower convergence; Decaying $\gamma_t$: Best of both worlds but requires tuning

- **Failure signatures:** Parameters diverge ($\|H_t\| \to \infty$): Likely $\gamma$ too small or learning rate too large; Convergence to sub-optimal arm: Initial $H_0$ biased toward wrong arm with insufficient regularization; Oscillation: Learning rate $\rho$ too large

- **First 3 experiments:** 1. Baseline validation with $k=10$, uniform $H_0$, $\rho_t = 0.05$ constant, $\gamma = 0$ 2. Regularization sweep with biased $H_0 = (5, 0, ..., 0)$, test $\gamma \in \{0, 0.01, 1.0, 10.0\}$ 3. Decay schedule test with biased $H_0$, test $\gamma_t = \gamma_0/(1+0.2t)$ with $\gamma_0 \in \{0, 10\}$

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal decay schedule for the time-dependent regularization parameter $\gamma_t$? The precise optimal decay schedule for $\gamma_t$ is not known and will be left for future works.

### Open Question 2
Can convergence be guaranteed for fixed small regularization coefficients where $\gamma < c^*$? Theorem 1 requires $\mu > 0$ (i.e., $\gamma > c^*$), but Section 4 shows numerically that small $\gamma$ (e.g., 0.01) works well despite $c^* \approx 3.08$.

### Open Question 3
Can the convergence of this regularized policy gradient method be extended to general Markov Decision Processes (MDPs)? The paper focuses exclusively on the Multi-Armed Bandit (MAB) setting, but the literature review discusses policy gradients in the context of general MDPs.

## Limitations

- Theoretical framework assumes bounded reward variance and appropriate learning rate decay schedules
- Optimal parameter choices for hyperparameters are not proven
- Claim that time-dependent regularization universally improves performance remains empirical without theoretical guarantees

## Confidence

- **Convergence guarantees under technical conditions**: High confidence - Proof structure follows established stochastic approximation theory with clear assumptions
- **Effectiveness of regularization in practice**: Medium confidence - Numerical results are compelling but depend on parameter tuning
- **Generalization beyond MAB setting**: Low confidence - Results are specific to bandit structure and may not extend to full RL

## Next Checks

1. **Theoretical bound validation**: Prove a convergence rate bound for the regularized algorithm that explicitly quantifies the bias-variance tradeoff as a function of $\gamma$

2. **Robustness testing**: Systematically test algorithm performance across different reward distributions (e.g., heavy-tailed) and arm counts ($k > 10$)

3. **Alternative regularization schemes**: Compare L2 regularization against other regularization forms (e.g., entropy regularization) under identical experimental conditions