---
ver: rpa2
title: 'PARK: Personalized academic retrieval with knowledge-graphs'
arxiv_id: '2507.13910'
source_url: https://arxiv.org/abs/2507.13910
tags:
- user
- retrieval
- graph
- search
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PARK, a personalized academic retrieval system
  that leverages knowledge graph embeddings to enhance user modeling in academic search.
  The proposed two-stage approach first trains a neural language model for retrieval
  and then converts the academic citation graph into a knowledge graph, embedding
  it into the same semantic space using translational embedding techniques.
---

# PARK: Personalized academic retrieval with knowledge-graphs

## Quick Facts
- arXiv ID: 2507.13910
- Source URL: https://arxiv.org/abs/2507.13910
- Authors: Pranav Kasela; Gabriella Pasi; Raffaele Perego
- Reference count: 40
- Primary result: PARK achieves up to 10% improvement in MAP@100 over second-best model across four academic domains

## Executive Summary
PARK introduces a personalized academic retrieval system that combines neural language models with knowledge graph embeddings to enhance user modeling in academic search. The two-stage approach first trains a neural language model for retrieval, then converts the academic citation graph into a knowledge graph embedded in the same semantic space using translational embedding techniques. This allows user models to capture both explicit relationships and hidden structures in citation graphs and paper content. Experiments across four academic domains demonstrate significant performance improvements, with affiliation nodes contributing most substantially to gains.

## Method Summary
PARK uses a two-stage pipeline: BM25 first-stage retrieval followed by MiniLM bi-encoder re-ranking, with final scores computed via convex combination of BM25, dense similarity, and user similarity. The knowledge graph is constructed from citation data with 4 node types (users, documents, venues, affiliations) and 5 relation types. Document embeddings from MiniLM are frozen during KG training, allowing user/venue/affiliation nodes to learn positions relative to fixed document semantics. TransE/TransH embeddings are trained to position entities in a shared space where relations approximate vector translations. The final ranking combines normalized scores from all three retrieval signals.

## Key Results
- PARK-H achieves best MAP@100 in 3/4 domains, with up to 10% improvement over second-best model
- Affiliation nodes add 2.7-2.9% NDCG@10 improvement across domains, more than venue nodes
- Fixed document embeddings preserve semantic content while allowing KG structure learning
- Computer Science domain shows POP baseline outperforming PARK, suggesting domain-specific considerations

## Why This Works (Mechanism)

### Mechanism 1
Fixing document embeddings from the neural language model and training only non-document nodes in the KG preserves semantic content while infusing structural relationships. The neural LM produces document embeddings trained via triplet margin loss. These are frozen when training TransE/TransH, so user/venue/affiliation nodes learn positions relative to fixed document semantics. This anchors the KG embedding space to meaningful text representations without requiring joint training. Core assumption: Document embeddings from the neural LM are sufficiently representative that they need not be updated to accommodate graph structure.

### Mechanism 2
Affiliation nodes capture institutional and collaborative context that improves retrieval beyond author-paper relationships alone. Adding affiliation nodes and their relations creates indirect pathways between users who share institutions, enabling the translational model to position such users closer in embedding space even without direct co-authorship. Core assumption: Researchers at the same institution share relevant topical interests that inform personalized retrieval.

### Mechanism 3
Three-score convex combination (BM25, dense similarity, user similarity) outperforms any single signal by balancing lexical precision, semantic understanding, and personalization. BM25 provides high-recall lexical candidates. The neural re-ranker rescores for semantic relevance. User similarity (cosine between user embedding and document author embeddings) reweights based on research profile alignment. Normalized scores are combined via learned weights (λ1, λ2, λ3). Core assumption: The three signals provide complementary information and their optimal combination generalizes across queries and users.

## Foundational Learning

- **TransE/TransH translational embeddings**
  - Why needed here: PARK uses these algorithms to position entities in a shared space where relations approximate vector translations (h + r ≈ t). Understanding the margin ranking loss and how these methods handle one-to-many relations (TransH's hyperplane projection) is essential for debugging embedding quality.
  - Quick check question: Given a triplet (paper_A, cited, paper_B), what should the embedding of paper_A + the "cited" relation vector approximate?

- **Triplet margin loss for bi-encoder training**
  - Why needed here: The neural re-ranker is trained to minimize distance between query and relevant documents while maximizing distance to in-batch negatives. This loss formulation directly impacts the quality of frozen document embeddings fed into the KG.
  - Quick check question: What happens to the loss if the margin γ is set too small relative to typical embedding distances?

- **Closed World Assumption (CWA) for knowledge graphs**
  - Why needed here: PARK adopts CWA, treating missing triples as false. This affects how the KG embedding model interprets sparse citation networks and may influence generalization to new authors/papers.
  - Quick check question: If a user has authored 3 papers but the graph only records 2, what does CWA imply about the missing "wrote" relationship?

## Architecture Onboarding

- **Component map**: BM25 -> MiniLM bi-encoder -> TransE/TransH KG embedding -> Score fusion
- **Critical path**: 1) Train MiniLM on retrieval dataset (triplet margin loss, 10 epochs) 2) Extract and freeze document embeddings from trained MiniLM 3) Construct KG from citation graph with metadata 4) Train TransE/TransH with frozen document embeddings (100 epochs) 5) At inference: BM25 retrieves → MiniLM rescores → user similarity computed → fuse scores
- **Design tradeoffs**: TransE vs. TransH: TransE is simpler and faster but struggles with complex relations; TransH handles many-to-many via hyperplane projection but adds parameters. Fixed vs. learnable document embeddings: Fixing preserves pretrained semantics and speeds training but may limit graph-document alignment. Venue nodes included despite minimal empirical gain; may be removable for simplification.
- **Failure signatures**: User embeddings cluster in a small region → check embedding dimension alignment and learning rate. Performance degradation in one domain (e.g., CS where POP wins) → citation count/popularity may dominate over structural signals. Venue node adds no improvement → venue relations may be redundant with author-topic signals.
- **First 3 experiments**: 1) Reproduce baseline comparison: Run PARK-E and PARK-H on the four domain datasets with provided hyperparameters; verify MAP@100 falls within reported ranges. 2) Ablation by node type: Train KG with only user nodes, then add venue, then add affiliation; confirm NDCG@10 improvements match table 2 (affiliation adds ~2-3%). 3) Lambda sensitivity analysis: Vary λ1, λ2, λ3 on validation set to understand how score fusion weights impact per-domain performance; check if optimal weights differ by domain.

## Open Questions the Paper Calls Out
1. Can a hybrid model combining PARK's knowledge graph embeddings with explicit popularity features close the performance gap observed in the Computer Science domain? The authors note that the "POP" baseline outperforms PARK in Computer Science and suggest "exploring hybrid models that integrate popularity-based features with knowledge graph embeddings" as a future direction.
2. Does relaxing the constraint of fixed document node embeddings during knowledge graph training improve the semantic alignment between structural relationships and text content? The conclusion states that "the decision to fix document nodes during training may constrain the model's flexibility" and suggests exploring "soft constraints in the loss function."
3. How robust is the PARK model to data sparsity and noise in citation graphs when the Closed World Assumption (CWA) is violated? The authors list the "Closed World Assumption" as a limitation and note that "incomplete or biased citation records can adversely impact embedding quality."

## Limitations
- The optimal weighting of the three-score fusion (λ1, λ2, λ3) was determined via validation but not reported, limiting reproducibility
- The fixed-document-embedding strategy assumes neural LM captures sufficient citation structure, which may not hold in all domains
- Venue nodes contribute minimal improvement despite added complexity, suggesting potential over-engineering

## Confidence
- **High confidence**: The two-stage architecture design and overall experimental methodology are well-specified and reproducible
- **Medium confidence**: The mechanism of fixed document embeddings preserving semantic content while allowing KG structure learning is theoretically sound but lacks direct empirical validation
- **Medium confidence**: Affiliation nodes' contribution is empirically supported, but the causal mechanism (institutional context improving personalization) is assumed rather than proven

## Next Checks
1. **Score Fusion Sensitivity**: Systematically vary λ1, λ2, λ3 on validation sets to identify optimal weights and determine if they vary significantly across domains
2. **Embedding Alignment Analysis**: Measure cosine similarity distributions between document embeddings from MiniLM and their corresponding KG positions to quantify semantic-graph alignment
3. **Domain-Specific Ablation**: Remove venue nodes entirely and retrain to confirm their negligible contribution, potentially simplifying the model architecture without performance loss