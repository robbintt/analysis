---
ver: rpa2
title: 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning'
arxiv_id: '2510.25992'
source_url: https://arxiv.org/abs/2510.25992
tags:
- reasoning
- arxiv
- learning
- expert
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Supervised Reinforcement Learning (SRL),
  a framework that decomposes complex problem-solving into sequential decision-making
  steps to enable effective learning on challenging reasoning tasks. SRL provides
  step-wise rewards based on the similarity between model-generated actions and expert
  actions extracted from training data, offering richer learning signals than traditional
  methods.
---

# Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning

## Quick Facts
- arXiv ID: 2510.25992
- Source URL: https://arxiv.org/abs/2510.25992
- Reference count: 20
- This paper introduces Supervised Reinforcement Learning (SRL), a framework that decomposes complex problem-solving into sequential decision-making steps to enable effective learning on challenging reasoning tasks.

## Executive Summary
This paper introduces Supervised Reinforcement Learning (SRL), a framework that decomposes complex problem-solving into sequential decision-making steps to enable effective learning on challenging reasoning tasks. SRL provides step-wise rewards based on the similarity between model-generated actions and expert actions extracted from training data, offering richer learning signals than traditional methods. Experiments show SRL significantly outperforms SFT and RLVR baselines on math reasoning benchmarks, with improvements of 3.0% average accuracy and 3.7% when combined with RLVR. SRL also generalizes effectively to software engineering tasks, achieving a 74% relative improvement over strong SFT baselines.

## Method Summary
SRL trains models on step-by-step reasoning by decomposing expert trajectories into partial trajectory instances. The model generates both internal monologue (thinking) and external actions, with rewards computed only on action similarity to expert demonstrations using sequence matching. The framework uses GRPO for policy optimization with dynamic sampling to filter uninformative samples, and can be combined with RLVR for further refinement. The method is validated on math benchmarks (AMC23, AIME24/25, Minerva) and software engineering tasks (SWE-Bench-Verified).

## Key Results
- SRL achieves 3.0% average accuracy improvement over SFT and RLVR baselines on math reasoning benchmarks
- SRL+RLVR pipeline achieves 3.7% improvement over RLVR alone (28.3% vs 24.5% Minerva accuracy)
- On SWE-Bench-Verified, SRL achieves 74% relative improvement over SFT baselines

## Why This Works (Mechanism)

### Mechanism 1: Dense Step-wise Reward Signal
- **Claim:** SRL provides informative gradients even when final answers are incorrect by rewarding action-level similarity to expert demonstrations.
- **Mechanism:** Expert trajectories are decomposed into N-1 partial trajectory instances. At each step k, the model generates action y'_step_k and receives reward R = 2M/T where M = sum of matched elements in non-overlapping blocks, T = total elements. This dense signal replaces sparse binary outcome rewards.
- **Core assumption:** The student model can achieve non-trivial similarity scores on decomposed sub-tasks even when full problem solutions are unreachable.
- **Evidence anchors:**
  - [Section 4.2]: "SRL provides a reward based on the similarity between the model's predicted action and the corresponding expert action, thereby providing fine-grained, efficiently computable supervision."
  - [Section 5.2, Table 3]: Multi-step SRL outperforms one-step sequence similarity (27.6 vs 25.9 average), confirming granularity matters.
  - [Corpus]: Related work (Reasoning Pattern Matters, arxiv 2510.12643) confirms SFT focuses on imitation while outcome-based RL struggles without verifiable rewards—SRL bridges this gap.
- **Break condition:** If step-wise rewards have near-zero variance across rollouts (std < ε), dynamic sampling filters the sample, preventing uninformative updates.

### Mechanism 2: Monologue-Action Decoupling
- **Claim:** Separating internal reasoning (monologue) from external actions enables flexible reasoning styles while enforcing action alignment with experts.
- **Mechanism:** Model generates y'_think enclosed in "..." tags before producing action y'_step_k. Reward is computed ONLY on the action, not monologue. This grants freedom in reasoning process while constraining behavioral output.
- **Core assumption:** The model possesses sufficient instruction-following capability to produce structured outputs with distinct thinking and action components.
- **Evidence anchors:**
  - [Section 4.2]: "Notably, our reward is computed only on the logical action, not the internal monologue. This grants the model flexibility to develop its own internal reasoning style."
  - [Section 5.2, Example 1]: Model demonstrates emergent verification behavior ("We can verify this by substituting x=2...") not explicitly trained.
  - [Corpus]: No direct corpus validation of monologue-action decoupling mechanism.
- **Break condition:** If model fails to follow format (missing tags), reward = -1 regardless of content quality.

### Mechanism 3: Curriculum via SRL→RLVR Pipeline
- **Claim:** SRL establishes actionable intermediate behaviors that enable subsequent RLVR to discover correct solutions it couldn't reach from base initialization.
- **Mechanism:** SRL trains on D_hard problems where pass@k ≈ 0, providing dense guidance. This creates a "warm start" policy with reasonable action priors. RLVR then refines with outcome-based rewards, now sampling correct trajectories with non-zero probability.
- **Core assumption:** SRL-trained policy lands in a region of policy space where correct rollouts become discoverable within sampling budget.
- **Evidence anchors:**
  - [Abstract]: "initializing training with SRL before refining with RLVR yields the strongest overall performance"
  - [Table 1]: SRL→RLVR achieves 28.3% Minerva accuracy vs 27.6% SRL alone (+0.7%) and 24.5% RLVR alone (+3.8%)
  - [Corpus]: RL Squeezes, SFT Expands (arxiv 2509.21128) finds RL refines reasoning paths while SFT expands capacity—consistent with sequential pipeline benefits.
- **Break condition:** If SRL overfits to expert action style without learning transferable reasoning, RLVR refinement provides no benefit (not observed in data).

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: SRL uses GRPO as its RL objective; understanding advantage computation (group-normalized rewards) and clipping is essential.
  - Quick check question: Given 8 rollouts with rewards [0.3, 0.5, 0.4, 0.6, 0.2, 0.5, 0.4, 0.3], compute the advantage for the rollout with reward 0.6.

- **Concept: Sequence Matching (difflib.SequenceMatcher)**
  - Why needed here: Core reward function; understanding how matching blocks are identified and ratio computed is critical for debugging reward signals.
  - Quick check question: For sequences S1="abcde" and S2="abfde", what matching blocks exist and what is the similarity ratio?

- **Concept: Dynamic Sampling for Policy Gradient**
  - Why needed here: Filtering samples with zero-variance rewards prevents wasted computation and training instability.
  - Quick check question: Why does uniform reward across rollouts (e.g., all r=0.5) produce uninformative gradients in GRPO?

## Architecture Onboarding

- **Component map:**
  - Data processor: Expert trajectory → N-1 partial trajectory instances (x_step_k, y_step_k pairs)
  - Rollout generator: Policy p_θ samples G responses with temperature 1.0
  - Reward computer: difflib.SequenceMatcher on action component only
  - Dynamic filter: Retain samples where reward_std > ε
  - Policy updater: GRPO objective with advantage normalization

- **Critical path:**
  1. Parse expert solution into numbered steps (must preserve step boundaries)
  2. Construct partial contexts x_step_k = [query, steps 1..k-1]
  3. Sample G rollouts per instance with monologue + action format
  4. Extract actions, compute similarity rewards
  5. Filter by reward variance threshold
  6. Update policy via GRPO

- **Design tradeoffs:**
  - Batch size: SRL uses 512 vs GRPO's 128 (higher filter rate in RLVR due to sparse rewards)
  - Step granularity: Finer steps = denser rewards but more data processing overhead
  - KL coefficient: Set to 0 in experiments—trust reward signal over reference policy anchoring

- **Failure signatures:**
  - Reward variance collapses → dynamic filter rejects all samples → no updates
  - Format violations spike → majority of rewards = -1 → training degrades
  - Monologue quality degrades while actions remain similar → model "gaming" reward

- **First 3 experiments:**
  1. **Sanity check:** On 10 samples, manually verify reward computation matches expected similarity scores for near-match, partial-match, and no-match action pairs.
  2. **Ablation:** Train with and without dynamic sampling on validation set; confirm Table 2 result (DS improves average from 24.7 to 27.6 on Minerva).
  3. **Data efficiency:** Train SRL on 100, 500, 1000 samples from s1K to establish learning curve and minimum viable dataset size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SRL effectiveness scale with model size beyond the 7B parameter range tested in this paper?
- Basis in paper: [inferred] All experiments use Qwen2.5-7B-Instruct and Qwen2.5-Coder-7B-Instruct; the paper states SRL is designed for "small-scale open-source models" but does not test larger models.
- Why unresolved: Larger models may already learn effectively from SFT or RLVR, reducing the relative benefit of SRL's dense step-wise rewards.
- What evidence would resolve it: Experiments applying SRL to 13B, 70B, and larger models on the same benchmarks, comparing gains against SFT and RLVR baselines.

### Open Question 2
- Question: Can SRL handle expert trajectories without pre-segmented step structure (e.g., unstructured CoT from arbitrary teachers)?
- Basis in paper: [explicit] The method relies on "solutions formatted with structured, numbered steps" from DeepSeek R1; Section 5.4 notes effectiveness depends on "quality of the step-wise data."
- Why unresolved: Automatic step segmentation from unstructured reasoning traces could introduce noise into the similarity reward, degrading training.
- What evidence would resolve it: Experiments applying SRL to datasets with unstructured expert solutions, comparing performance when steps are automatically vs. manually segmented.

### Open Question 3
- Question: Would semantic similarity metrics outperform the difflib.SequenceMatcher token-based similarity for SRL rewards?
- Basis in paper: [inferred] The reward uses only surface-level sequence matching; the paper does not ablate alternative similarity measures.
- Why unresolved: Token-level similarity may penalize semantically equivalent actions with different wording, potentially limiting the reward signal's quality.
- What evidence would resolve it: Ablation experiments comparing difflib similarity against embedding-based semantic similarity (e.g., cosine similarity of action embeddings) on the same training setup.

## Limitations

- **Unknown dynamic sampling threshold**: The paper doesn't specify the exact ε threshold for filtering samples, which critically affects training stability.
- **Unknown step boundary parsing**: The exact mechanism for extracting step boundaries from DeepSeek R1's reasoning traces is not detailed.
- **Unknown training duration**: The number of epochs or iterations required for SRL to converge is unspecified.

## Confidence

- **High Confidence**: The core mechanism of step-wise dense rewards and the SRL→RLVR pipeline effectiveness are well-supported by experimental results across multiple benchmarks.
- **Medium Confidence**: The monologue-action decoupling mechanism is theoretically sound but has limited empirical validation.
- **Low Confidence**: The generalizability of SRL to domains beyond math and software engineering is untested.

## Next Checks

1. **Reward Computation Verification**: Implement the exact sequence similarity reward function and validate against ground truth on a small set of manually labeled action pairs with varying similarity levels.
2. **Dynamic Sampling Sensitivity**: Conduct controlled experiments varying the ε threshold to identify the optimal range that maximizes reward signal while maintaining adequate sample throughput.
3. **Step Granularity Impact**: Systematically test different levels of step decomposition (coarse vs. fine granularity) to determine the optimal tradeoff between reward density and computational overhead.