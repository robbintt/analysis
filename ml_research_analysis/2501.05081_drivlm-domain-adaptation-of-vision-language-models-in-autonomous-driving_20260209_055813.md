---
ver: rpa2
title: 'DriVLM: Domain Adaptation of Vision-Language Models in Autonomous Driving'
arxiv_id: '2501.05081'
source_url: https://arxiv.org/abs/2501.05081
tags:
- language
- visual
- large
- tasks
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large vision-language
  models to specialized domains like autonomous driving, focusing on computational
  efficiency and performance. The proposed method, DriVLM, uses a small-scale multimodal
  model (Mini-InternVL) with a compact vision encoder (InternViT-300M) and knowledge
  distillation from larger teacher models.
---

# DriVLM: Domain Adaptation of Vision-Language Models in Autonomous Driving

## Quick Facts
- arXiv ID: 2501.05081
- Source URL: https://arxiv.org/abs/2501.05081
- Reference count: 19
- Mini-InternVL-2B achieves 0.191 CIDEr score and 76.3% accuracy on DriveLM-nuScenes

## Executive Summary
This paper introduces DriVLM, a domain adaptation method for vision-language models (VLMs) in autonomous driving that balances computational efficiency with strong performance. The approach uses knowledge distillation to train a compact vision encoder (InternViT-300M) from a larger teacher (InternViT-6B), then employs a two-stage training process to align the vision encoder with a small-scale LLM. By reformating driving tasks into VQA format and fine-tuning with balanced general and domain-specific data, the method achieves state-of-the-art results on the DriveLM-nuScenes dataset while maintaining significantly fewer parameters than larger models.

## Method Summary
DriVLM employs a two-stage training approach for domain adaptation of small-scale multimodal models to autonomous driving. First, a compact vision encoder (InternViT-300M) is trained via knowledge distillation from a larger teacher model (InternViT-6B). In the alignment stage, the visual encoder is frozen while an MLP projector is trained to map visual features to the LLM's embedding space. During domain adaptation, all parameters are fine-tuned on reformulated VQA tasks using a balanced mix of general multimodal data and domain-specific driving data. The model processes multi-view images from 6 cameras with dynamic resolution input and pixel fixing to manage computational efficiency.

## Key Results
- Mini-InternVL-2B achieves 0.191 CIDEr score and 76.3% accuracy on DriveLM-nuScenes
- Model maintains significantly fewer parameters than larger VLMs while achieving competitive performance
- Demonstrates strong performance on perception, prediction, and planning tasks in autonomous driving
- Shows slight deficiency in object center point prediction affecting BLEU-1 scores

## Why This Works (Mechanism)

### Mechanism 1: Distillation-Driven Visual Compression
- Claim: A compact vision encoder can approximate larger models' feature extraction for driving domains when distilled from a teacher model.
- Mechanism: Small encoder learns to replicate output distributions of larger teacher, retaining semantic richness while reducing latency and memory footprint.
- Core assumption: Teacher model's visual knowledge is transferable to a smaller student without catastrophic loss of critical spatial features.
- Evidence anchors: Abstract mentions InternViT-300M with knowledge distillation from InternViT-6B; LEO corpus confirms trend of mixture of vision encoders.

### Mechanism 2: Decoupled Alignment and Full Fine-Tuning
- Claim: Separating training into alignment and domain adaptation phases maximizes modality integration while preventing degradation of pre-trained knowledge.
- Mechanism: Stage 1 freezes vision encoder while training MLP projector; Stage 2 performs full-parameter fine-tuning on driving tasks.
- Core assumption: Pre-trained vision encoder is robust enough to not need early updates to align with LLM.
- Evidence anchors: Section 3.1 describes the two-stage process; H-MBA corpus supports specialized adaptation strategies.

### Mechanism 3: Unified VQA Reformatting with Data Mixing
- Claim: Reformating diverse driving tasks into VQA format allows single generative model to handle multi-task reasoning without specialized heads.
- Mechanism: Treats object detection and motion forecasting as text generation problems, leveraging LLM's reasoning capabilities.
- Core assumption: LLM possesses sufficient world knowledge to handle spatial and temporal logic when prompted correctly.
- Evidence anchors: Section 3.2 describes VQA reformatting; NuPlanQA corpus validates VQA approach for driving scenes.

## Foundational Learning

**Knowledge Distillation**: Why needed here? The paper relies on InternViT-6B teaching InternViT-300M. Quick check: Can you explain why a small model learns better from a large model's probabilities than from hard labels alone?

**Projectors / Adapters in MLLMs**: Why needed here? The MLP projector is the critical bridge in Stage 1. Quick check: Why might a 2-layer MLP projector preserve spatial information better than a simple linear layer when mapping image patches to LLM tokens?

**Catastrophic Forgetting**: Why needed here? The paper explicitly mentions mixing general data with domain data during fine-tuning. Quick check: What happens to a pre-trained model's general reasoning abilities if you fine-tune it exclusively on a specific dataset without mixing in general data?

## Architecture Onboarding

**Component map**: Multi-view images -> InternViT-300M -> MLP Projector -> InternLM2-1.8B -> Text descriptions/coordinates/planning decisions

**Critical path**: *Pixel Fixing/Dynamic Resolution* -> *InternViT-300M* -> *MLP Projector* -> *LLM*

**Design tradeoffs**: Resolution vs. Tokens (dynamic resolution improves detail but increases token count); General vs. Specific (high domain data improves driving scores but risks forgetting general abilities)

**Failure signatures**: Object Hallucination (model invents vehicles or traffic signs); Spatial Confusion (misidentifying camera views); Syntax Drift (outputs valid logic but in unparsable format)

**First 3 experiments**: 1) Data Ratio Sensitivity: Vary general-to-domain data ratios to find inflection point between CIDEr and general instruction following. 2) Projector Ablation: Compare Linear Layer vs. MLP Projector in Stage 1. 3) Dynamic Resolution Stress Test: Evaluate on distant objects with resolution ON vs. OFF.

## Open Questions the Paper Calls Out

**Open Question 1**: How can the model's precision in predicting object center points be improved to address the observed deficiency in Bleu 1 scores? [explicit] The results section notes the model "performed slightly lower on the Bleu 1 metric, which might be due to its limitations in predicting object center points."

**Open Question 2**: To what extent do current evaluation metrics constrain the performance potential of larger-scale models on autonomous driving tasks? [explicit] The paper states that "the constraints of the training data and evaluation metrics could impact the potential of larger-scale models."

**Open Question 3**: Does the proposed transfer learning framework maintain general multimodal capabilities when applied to safety-critical domains with imbalanced data distributions? [inferred] The paper claims the method advances application in "real-world scenarios" but does not test performance on out-of-distribution driving scenarios or general benchmarks post-adaptation.

## Limitations

**Architectural Transparency**: Paper does not specify exact MLP projector architecture or knowledge distillation methodology, making replication difficult.

**Generalization Boundaries**: Evaluation lacks testing on out-of-distribution driving scenarios or non-driving VQA tasks to quantify catastrophic forgetting.

**Scalability Concerns**: Approach relies on distilling from 6B vision encoder, which may not be feasible for practitioners without large computational resources.

## Confidence

**High Confidence**: Efficacy of knowledge distillation for compressing vision encoders (supported by established literature); Benefit of reformatting tasks into VQA format for multi-task handling.

**Medium Confidence**: Specific two-stage training procedure is theoretically sound but lacks empirical ablation; Dynamic resolution mechanism is described but not rigorously evaluated for edge cases.

**Low Confidence**: Claim that balanced data mixing prevents forgetting is stated but not empirically validated with forgetting metrics or generalization tests.

## Next Checks

1. **Data Ratio Sensitivity Test**: Systematically vary the ratio of general-to-domain data during Stage 2 fine-tuning and measure both driving performance and general instruction-following ability to identify optimal trade-off point.

2. **Cross-Dataset Generalization**: Evaluate fine-tuned Mini-InternVL on a held-out driving dataset not seen during training to quantify out-of-distribution robustness and test for overfitting.

3. **Teacher Model Ablation**: Repeat distillation process using progressively smaller vision encoders to determine minimum teacher size required for acceptable driving performance, assessing true computational efficiency gains.