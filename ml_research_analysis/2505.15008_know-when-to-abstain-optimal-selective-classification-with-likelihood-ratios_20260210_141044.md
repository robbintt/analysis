---
ver: rpa2
title: 'Know When to Abstain: Optimal Selective Classification with Likelihood Ratios'
arxiv_id: '2505.15008'
source_url: https://arxiv.org/abs/2505.15008
tags:
- selective
- classification
- optimal
- scores
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selective classification under
  covariate shift, where the input distribution changes between training and test
  time. The authors propose a new framework based on the Neyman-Pearson lemma, showing
  that optimal selection functions can be derived from likelihood ratios of correct
  versus incorrect predictions.
---

# Keep When to Abstain: Optimal Selective Classification with Likelihood Ratios

## Quick Facts
- arXiv ID: 2505.15008
- Source URL: https://arxiv.org/abs/2505.15008
- Reference count: 40
- Primary result: Optimal selective classification under covariate shift using likelihood ratio tests; achieves up to 50% AURC reduction with distance-based scores

## Executive Summary
This paper addresses selective classification under covariate shift by deriving optimal selection functions from likelihood ratios of correct versus incorrect predictions. The authors introduce two novel distance-based scores (∆-MDS and ∆-KNN) that approximate the Neyman-Pearson optimal test by measuring relative proximity to correctly versus incorrectly classified training samples. They demonstrate that combining these with logit-based scores yields strong performance across vision and language tasks, achieving significant AURC improvements on powerful models like CLIP.

## Method Summary
The method partitions training features into correctly and incorrectly classified sets (A_c and A_w), then computes scores based on relative distances to these partitions. ∆-MDS uses Mahalanobis distances under Gaussian assumptions, while ∆-KNN uses k-nearest neighbor distances. The scores are combined with logit-based confidence measures (RLog) using a weighted sum. At test time, the combined score is thresholded to decide whether to accept predictions or abstain, with thresholds chosen to optimize the AURC metric.

## Key Results
- ∆-MDS and ∆-KNN reduce average AURC by ~50% compared to standard MDS/KNN on CLIP
- ∆-KNN-RLog achieves best average NAURC (0.163 on CLIP, 0.166 on EVA) across all methods
- Linear combinations of distance-based and logit-based scores outperform individual components
- Consistent improvements across ImageNet variants and Amazon Reviews language tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The optimal selection function for selective classification is a monotonic transformation of the likelihood ratio between correct and incorrect prediction densities.
- **Mechanism:** Frame selective classification as binary hypothesis testing: H₀ (correct prediction) vs H₁ (incorrect). By the Neyman-Pearson lemma, thresholding the likelihood ratio p_c(x)/p_w(x) minimizes false acceptances for any fixed false rejection rate.
- **Core assumption:** Densities p_c and p_w exist and are strictly positive on shared support; the selector score is a monotonic transform of the true likelihood ratio.
- **Evidence anchors:** [abstract] "a classical result in statistics that characterizes the optimal rejection rule as a likelihood ratio test"; [section 3] Lemma 1 and Corollary 1 establish that monotonic transforms of likelihood ratios preserve NP-optimality
- **Break condition:** When p_c/p_w cannot be reliably estimated (e.g., insufficient training samples in one partition, or feature distributions violate continuity assumptions).

### Mechanism 2
- **Claim:** Separating training features into correctly and incorrectly classified sets, then scoring test points by their relative proximity to each set, approximates the NP-optimal likelihood ratio.
- **Mechanism:** For ∆-MDS: compute class-conditional Gaussian statistics (µ_c, Σ_c) for correct samples and (µ_w, Σ_w) for wrong samples; score = D_MDS(x; µ_c, Σ_c) − D_MDS(x; µ_w, Σ_w). For ∆-KNN: compute average log-distances to k-nearest neighbors in each set; score = D_KNN(x; A_c) − D_KNN(x; A_w).
- **Core assumption:** ∆-MDS: features are class-conditionally Gaussian given correct/incorrect prediction. ∆-KNN: asymptotic consistency of k-NN density estimation (k→∞, k/N→0).
- **Evidence anchors:** [section 3.2] Theorem 2 proves ∆-MDS optimality under Gaussian assumptions; Theorem 3 proves ∆-KNN asymptotic optimality; [table 1] ∆-MDS and ∆-KNN reduce average AURC by ~50% vs. standard MDS/KNN on CLIP
- **Break condition:** ∆-MDS fails when features are non-Gaussian (paper notes CLIP's contrastive features favor ∆-KNN); ∆-KNN underperforms with insufficient samples in A_w (the incorrect set may be small for accurate models).

### Mechanism 3
- **Claim:** Linear combinations of NP-optimal scores remain NP-optimal under a tilted likelihood model and empirically outperform individual scores.
- **Mechanism:** Combine distance-based and logit-based scores: s_combined = s_distance + λ·s_logit. Distance-based methods capture feature-space geometry; logit-based methods capture classifier boundary confidence. The combination compensates for violated assumptions in either method.
- **Core assumption:** The joint density factorizes as a multiplicative (tilted) product: p_c^(1)(p_c^(2))^λ / Z_c.
- **Evidence anchors:** [section 3.3] Lemma 2 proves linear combinations preserve NP-optimality under tilted likelihoods; [table 1, 2] ∆-KNN-RLog achieves best average NAURC (0.163 CLIP, 0.166 EVA) across methods
- **Break condition:** When λ is poorly tuned causing magnitude imbalance, or when both component scores are miscalibrated in the same direction.

## Foundational Learning

- **Concept: Neyman-Pearson Lemma**
  - **Why needed here:** Central theoretical justification; without it, the likelihood-ratio framing lacks optimality guarantees.
  - **Quick check question:** Given two hypotheses with fixed Type I error tolerance, what test minimizes Type II error?

- **Concept: Mahalanobis Distance**
  - **Why needed here:** ∆-MDS uses this to measure distance from class-conditional Gaussian distributions; standard Euclidean distance ignores covariance structure.
  - **Quick check question:** If Σ is diagonal with entries [4, 1], does point (2, 2) have equal Mahalanobis distance contribution from both dimensions?

- **Concept: Covariate Shift**
  - **Why needed here:** The entire evaluation framework targets this scenario; understanding p(x) change vs. p(y|x) change is critical for interpreting results.
  - **Quick check question:** In covariate shift, which of these changes between train and test: p(x), p(y|x), p(y)?

## Architecture Onboarding

- **Component map:** Feature extractor → Training partitioner → Statistics module/ KNN index → Score combiner → Selector threshold
- **Critical path:**
  1. Run classifier on training set → obtain predictions
  2. Partition features by prediction correctness (requires ground-truth labels)
  3. For ∆-MDS: estimate per-class means and shared covariance for each partition
  4. For ∆-KNN: build FAISS/annoy indices on A_c and A_w
  5. At inference: extract feature → compute both distance scores → combine with logit score
- **Design tradeoffs:**
  - ∆-MDS vs. ∆-KNN: MDS requires Gaussian assumption (better for supervised softmax classifiers); KNN is non-parametric (better for contrastive models like CLIP)
  - k selection: Paper uses k=25 for ∆-KNN (ablation shows averaging top-k beats single k-th distance)
  - λ selection: Requires magnitude balancing; paper uses grid search (λ=10 for CLIP ∆-KNN-RLog, λ=0.5 for EVA)
  - Storage: ∆-MDS stores O(K·d) parameters; ∆-KNN stores all training features O(N·d)
- **Failure signatures:**
  - Very high base accuracy → A_w too small → unreliable p_w estimation → ∆-scores degrade
  - Non-Gaussian features with ∆-MDS → CLIP underperforms vs. ∆-KNN (confirmed in paper)
  - Imbalanced λ → one score dominates → combination no better than single score
  - Missing ground-truth labels at training → cannot partition correctly → method inapplicable
- **First 3 experiments:**
  1. Reproduce ∆-KNN vs. KNN gap: On ImageNet-1K with a pretrained ResNet, compare standard KNN score vs. ∆-KNN; expect ~50% AURC reduction as reported
  2. Validate Gaussian assumption: Visualize t-SNE of A_c vs. A_w features for a supervised model; assess whether Gaussian assumption is plausible for your architecture
  3. Lambda sensitivity sweep: Fix ∆-KNN-RLog pipeline, sweep λ ∈ {0.1, 1, 10, 100} on validation set; verify paper's claim that magnitude balancing is sufficient without extensive tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Neyman-Pearson likelihood ratio framework be adapted for selective generation in Large Language Models (LLMs) to mitigate hallucinations?
- Basis in paper: [explicit] The "Limitations and Future Work" section states that enabling LLMs to recognize uncertainty and selectively abstain ("selective generation") is an "exciting avenue for future work."
- Why unresolved: The current theoretical derivation assumes a fixed label space $Y$ and discriminative classification, whereas LLMs operate over open-ended generative token sequences.
- What evidence would resolve it: An extension of the likelihood ratio test to sequence probabilities and empirical benchmarks showing reduced hallucination rates when LLMs utilize the proposed selective abstention mechanism.

### Open Question 2
- Question: Can the proposed $\Delta$-MDS and $\Delta$-KNN selectors be effectively generalized to structured prediction tasks like semantic segmentation or regression?
- Basis in paper: [explicit] The authors explicitly identify "regression, semantic segmentation, or time series forecasting" as "promising future directions" in the conclusion.
- Why unresolved: The current proofs rely on a binary "correct vs. incorrect" hypothesis test, which maps cleanly to classification but is ambiguous for continuous or structured outputs where "correctness" is not discrete.
- What evidence would resolve it: A reformulation of the likelihood ratio $p_c/p_w$ for continuous error metrics (e.g., IoU for segmentation) and experimental validation on standard regression/segmentation benchmarks under covariate shift.

### Open Question 3
- Question: Does post-hoc calibration (e.g., temperature scaling) restore the Neyman-Pearson optimality of Maximum Softmax Probability (MSP) in modern deep networks?
- Basis in paper: [inferred] Theorem 1 proves MSP is NP-optimal only assuming a calibrated classifier. The text notes that modern networks are poorly calibrated but leaves the interaction between calibration interventions and NP optimality "beyond the scope of this work."
- Why unresolved: While calibration aligns confidence with accuracy, it is unclear if the transformation preserves the monotonic relationship between MSP and the likelihood ratio $p_c/p_w$ required by Corollary 1.
- What evidence would resolve it: Theoretical analysis of how temperature scaling affects the monotonicity of the likelihood ratio, accompanied by experiments comparing selective classification performance on raw versus calibrated logits.

## Limitations
- Theoretical framework assumes well-defined densities that may be poorly estimated when incorrect set A_w is small or features are non-continuous
- All experiments focus on covariate shift (p(x) change) while holding p(y|x) fixed; performance under concept drift remains untested
- Method requires ground-truth labels during training for partitioning, limiting applicability to online/streaming settings

## Confidence
- **High**: NP-optimality of likelihood ratio framework (Theorem 1, Corollary 1); empirical superiority of combined scores (Table 1, 2)
- **Medium**: Asymptotic optimality of ∆-KNN (Theorem 3 relies on k→∞ assumptions that may not hold with finite samples)
- **Medium**: Claim that magnitude balancing suffices for λ tuning without extensive search (based on single grid search in experiments)

## Next Checks
1. Test on a low-accuracy model (e.g., ResNet-18 at ~70% accuracy) to verify ∆-scores degrade when A_w is insufficient, confirming the identified break condition
2. Evaluate on a synthetic dataset with known p(x) shift but controlled p(y|x) drift to test robustness beyond covariate shift
3. Perform ablation on k values for ∆-KNN across different feature dimensionalities to verify the claimed averaging-top-k advantage over single k-th distance