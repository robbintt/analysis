---
ver: rpa2
title: 'TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal
  Distance'
arxiv_id: '2509.26627'
source_url: https://arxiv.org/abs/2509.26627
tags:
- reward
- learning
- frame
- progress
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TimeRewarder, a method that learns dense rewards
  from passive videos by modeling frame-wise temporal distances between video frames.
  The core idea is to predict how far apart two frames are in terms of task progression,
  using this temporal distance as a proxy reward for reinforcement learning.
---

# TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance

## Quick Facts
- arXiv ID: 2509.26627
- Source URL: https://arxiv.org/abs/2509.26627
- Reference count: 40
- Primary result: Achieves nearly perfect success on 9/10 Meta-World tasks with 200K interactions, outperforming baselines and manual dense rewards

## Executive Summary
TimeRewarder is a method for learning dense rewards from passive videos by modeling frame-wise temporal distances between video frames. The approach predicts how far apart two frames are in terms of task progression, using this temporal distance as a proxy reward for reinforcement learning. By treating temporal ordering as an implicit measure of progress and employing techniques like implicit negative sampling, weighted frame pair selection, and discretization, TimeRewarder generates step-wise rewards that guide agents toward task completion. Experiments on ten Meta-World manipulation tasks show that TimeRewarder achieves nearly perfect success on 9 out of 10 tasks with only 200,000 environment interactions, outperforming all baselines and even manual dense reward designs in both success rate and sample efficiency. Additionally, it generalizes well to cross-domain settings, leveraging human demonstration videos alongside robot data.

## Method Summary
TimeRewarder learns dense rewards from passive videos by predicting the temporal distance between frames as a proxy for task progress. The method employs implicit negative sampling to generate challenging negative pairs, uses weighted frame pair selection to focus on informative transitions, and discretizes temporal distances into bins for stable learning. A temporal distance predictor is trained to estimate how far apart frames are in the task sequence, and this prediction is used as a dense reward signal for reinforcement learning. The approach enables efficient learning from videos without requiring explicit reward annotations or active interaction during training.

## Key Results
- Achieves nearly perfect success on 9 out of 10 Meta-World manipulation tasks
- Requires only 200,000 environment interactions to reach optimal performance
- Outperforms all baseline methods and manual dense reward designs in both success rate and sample efficiency
- Successfully generalizes to cross-domain settings using human demonstration videos

## Why This Works (Mechanism)
TimeRewarder works by leveraging the natural temporal structure of task demonstrations to infer dense reward signals. The core insight is that the relative ordering of frames in a video implicitly encodes task progress, and by predicting the temporal distance between frames, the model can generate continuous reward signals that guide the agent through the task sequence. The method's effectiveness stems from its ability to extract rich, task-relevant information from passive video data without requiring explicit reward annotations or active interaction during training.

## Foundational Learning

### Temporal Distance Prediction
- **Why needed**: To quantify task progress and generate dense rewards from video frames
- **Quick check**: Validate that predicted distances correlate with actual task completion steps

### Implicit Negative Sampling
- **Why needed**: To create challenging training examples that improve temporal distance prediction
- **Quick check**: Compare performance with and without negative sampling on a subset of tasks

### Weighted Frame Pair Selection
- **Why needed**: To focus learning on informative transitions and improve sample efficiency
- **Quick check**: Analyze the impact of different weighting schemes on reward quality and learning speed

## Architecture Onboarding

### Component Map
Passive Video -> Frame Extraction -> Temporal Distance Predictor -> Reward Signal -> RL Agent -> Task Execution

### Critical Path
The critical path is: Video frames → Temporal distance prediction → Reward signal → RL learning → Task completion. The temporal distance predictor is the key component that transforms passive video data into actionable reward signals.

### Design Tradeoffs
- **Discretization vs. Continuous Prediction**: Discretization provides stability but may lose fine-grained information
- **Negative Sampling Strategy**: Implicit sampling is more efficient than explicit but requires careful implementation
- **Weight Function Design**: More complex weight functions can improve learning but add hyperparameters

### Failure Signatures
- Poor temporal distance predictions lead to noisy or misleading reward signals
- Inadequate negative sampling results in reward hacking or suboptimal policies
- Domain mismatch between demonstration videos and target environment causes poor generalization

### First Experiments
1. Validate temporal distance predictor accuracy on held-out video sequences
2. Test reward signal quality by evaluating policy performance with oracle temporal distances
3. Conduct ablation study on the impact of discretization granularity on learning efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes monotonic task progression and visual observability of task-relevant information
- Performance depends on video quality, frame rate, and domain alignment with target environment
- Additional hyperparameters (e.g., number of temporal bins, weight function) may affect robustness across settings

## Confidence
- Claims of dense reward learning and improved sample efficiency on Meta-World tasks: High
- Generalization to cross-domain settings: Medium
- Scalability to highly dynamic or partially observable environments: Low

## Next Checks
1. Test TimeRewarder on a broader range of robotic manipulation tasks, including those with non-monotonic or branching task structures
2. Evaluate performance with varying video qualities, frame rates, and domain gaps between demonstrations and target environments
3. Conduct ablation studies to quantify the impact of hyperparameters such as the number of temporal bins and the weight function for frame pair selection on both reward quality and learning efficiency