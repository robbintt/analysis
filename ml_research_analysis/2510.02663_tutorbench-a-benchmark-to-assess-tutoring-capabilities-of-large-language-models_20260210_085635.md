---
ver: rpa2
title: 'TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language
  Models'
arxiv_id: '2510.02663'
source_url: https://arxiv.org/abs/2510.02663
tags:
- student
- tutoring
- response
- students
- rubric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TutorBench is a benchmark for evaluating the tutoring capabilities
  of large language models (LLMs) across three core use cases: adaptive explanation
  generation, assessment and feedback, and active learning support. The dataset comprises
  1,490 high-school and AP-level STEM samples, including both text-only and multimodal
  questions.'
---

# TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models

## Quick Facts
- arXiv ID: 2510.02663
- Source URL: https://arxiv.org/abs/2510.02663
- Reference count: 40
- 16 frontier LLMs evaluated with none scoring above 56% on tutoring capabilities

## Executive Summary
TutorBench is a comprehensive benchmark designed to evaluate the tutoring capabilities of large language models across three core educational use cases: adaptive explanation generation, assessment and feedback, and active learning support. The benchmark includes 1,490 high-school and AP-level STEM questions with sample-specific rubrics for standardized evaluation. All 16 evaluated frontier models scored below 56%, with the highest-performing model (Gemini 2.5 Pro) achieving 55.65%. Models struggled most with generating adaptive explanations and providing examples/analogies, while performing relatively better at identifying correct/incorrect student steps.

## Method Summary
The benchmark employs a two-phase evaluation approach using an LLM-judge to assess responses based on sample-specific rubrics. Each rubric contains binary criteria that are weighted and averaged to produce final scores. The evaluation framework tests models across three use cases: adaptive explanation generation (requiring personalized responses based on student background and preferences), assessment and feedback (involving step-by-step problem analysis and targeted corrections), and active learning support (focusing on Socratic questioning and learner agency). The benchmark includes both text-only and multimodal question types to test model capabilities across different input formats.

## Key Results
- No evaluated LLM achieved a score above 56% on the overall tutoring benchmark
- Adaptive explanation generation was the most challenging task at 47.16% average score
- Gemini 2.5 Pro achieved the highest overall score of 55.65%
- Models performed relatively better at identifying correct student steps (53.7%) compared to other tasks

## Why This Works (Mechanism)
The benchmark works by establishing standardized evaluation criteria through sample-specific rubrics that define what constitutes effective tutoring responses. The two-phase evaluation approach ensures consistency by first generating rubrics for each question based on learning objectives, then using an LLM-judge to systematically evaluate model responses against these criteria. This mechanism allows for objective comparison across diverse tutoring scenarios while maintaining pedagogical relevance through expert-crafted rubrics.

## Foundational Learning
- Adaptive explanation generation: Why needed - To personalize tutoring based on student background; Quick check - Compare explanations for different student profiles
- Step-by-step assessment: Why needed - To identify specific errors in student reasoning; Quick check - Verify correct identification of incorrect steps
- Active learning support: Why needed - To promote learner agency and deeper understanding; Quick check - Evaluate quality of Socratic questions
- Multimodal reasoning: Why needed - To handle visual STEM problems; Quick check - Test performance on diagram-based questions
- Rubric-based evaluation: Why needed - To standardize assessment across diverse responses; Quick check - Compare LLM-judge scores with human judgments

## Architecture Onboarding
- Component map: Question pool -> Rubric generation -> Model response -> LLM-judge evaluation -> Weighted scoring
- Critical path: Input question → Model response generation → Rubric-based evaluation → Score calculation
- Design tradeoffs: Binary rubric scoring (simplicity vs. nuance loss) vs. partial credit systems; Automated LLM-judge (scalability vs. potential bias) vs. human evaluation
- Failure signatures: Low scores on adaptive explanations indicate poor personalization; Poor performance on examples/analogies suggests limited pedagogical knowledge
- First experiments: 1) Test rubric consistency by having multiple LLM-judges evaluate same responses; 2) Compare scores across different student profile variations; 3) Evaluate model performance on questions with and without visual components

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on high-school and AP-level STEM content may not generalize to other educational levels
- LLM-judge evaluation introduces potential circularity and bias in scoring
- Binary rubric scoring system may oversimplify complex tutoring interactions
- Relatively small sample size of 1,490 questions may not cover full diversity of tutoring scenarios

## Confidence
- Confidence in core finding (LLMs struggle with tutoring): High
- Confidence in specific performance rankings: Medium
- Confidence in generalizability to broader contexts: Low

## Next Checks
1. Re-evaluate the same models using human expert judges to validate LLM-judge scores
2. Expand benchmark to include lower-grade content and non-STEM subjects
3. Implement granular scoring system capturing partial credit and nuanced tutoring quality aspects