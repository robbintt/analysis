---
ver: rpa2
title: Exploring Cultural Variations in Moral Judgments with Large Language Models
arxiv_id: '2506.12433'
source_url: https://arxiv.org/abs/2506.12433
tags:
- moral
- llms
- cultural
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether Large Language Models (LLMs) reflect
  cross-cultural moral norms by comparing their predictions to the World Values Survey
  (WVS) and Pew Research Global Attitudes Survey (PEW). Using log-probability-based
  moral justifiability scores, we assessed 20 models ranging from GPT-2 to GPT-4o.
---

# Exploring Cultural Variations in Moral Judgments with Large Language Models

## Quick Facts
- arXiv ID: 2506.12433
- Source URL: https://arxiv.org/abs/2506.12433
- Reference count: 5
- Primary result: Large Language Models reflect cross-cultural moral norms with significant W.E.I.R.D. bias, showing higher alignment with Western cultures

## Executive Summary
This study evaluates whether Large Language Models (LLMs) reflect cross-cultural moral norms by comparing their predictions to the World Values Survey (WVS) and Pew Research Global Attitudes Survey (PEW). Using log-probability-based moral justifiability scores, the research assesses 20 models ranging from GPT-2 to GPT-4o. Results show that instruction-tuned models like GPT-4o and Gemma-2-9B-IT achieve substantially higher correlations with survey data (r ≥ 0.4) compared to smaller or base models that often produce near-zero or negative correlations.

Regional analysis reveals strong W.E.I.R.D. bias, with models aligning best with Western Europe and North America but performing poorly for Sub-Saharan Africa and MENA regions. Topic difficulty varies, with issues like political violence showing high error rates across all models. While larger models and instruction tuning improve cultural alignment, significant gaps remain, highlighting the need for more diverse training data and bias mitigation strategies.

## Method Summary
The study employs log-probability-based moral justifiability scores to evaluate LLMs against survey data from WVS and PEW. Twenty models were tested across different regional groupings, with correlations computed between model predictions and survey responses. The methodology compares base models versus instruction-tuned variants, examines regional performance patterns, and analyzes topic-specific variations in moral judgment accuracy.

## Key Results
- Instruction-tuned models (GPT-4o, Gemma-2-9B-IT) achieve correlations r ≥ 0.4 with survey data, outperforming smaller or base models
- Strong W.E.I.R.D. bias observed: models align best with Western Europe/North America, poorly with Sub-Saharan Africa and MENA regions
- Topic difficulty varies significantly, with political violence showing consistently high error rates across all models

## Why This Works (Mechanism)
The study demonstrates that model architecture and training approach directly impact cultural alignment. Larger models with more parameters show improved ability to capture complex moral reasoning patterns. Instruction tuning appears to enhance model sensitivity to moral judgment nuances by incorporating explicit guidance during training. The log-probability scoring mechanism effectively captures subtle variations in model responses, allowing for more precise measurement of cultural alignment than binary classification approaches.

## Foundational Learning
- Log-probability scoring: Used as proxy for moral judgment strength; needed to quantify model alignment with survey data. Assumption: Higher log-probabilities indicate stronger moral justification.
- W.E.I.R.D. bias: Western, Educated, Industrialized, Rich, Democratic cultural bias; needed to explain regional performance gaps. Unknown: Whether this bias stems from training data composition or architectural limitations.
- Correlation analysis: Statistical method to measure model-survey alignment; needed to validate cultural representation. Assumption: Pearson correlation adequately captures relationship between model predictions and human survey responses.

## Architecture Onboarding
- Component map: LLMs -> Log-probability scoring -> Correlation analysis -> Regional/Topic evaluation
- Critical path: Survey data ingestion -> Model prediction generation -> Probability calculation -> Statistical validation
- Design tradeoffs: Log-probability vs. alternative scoring methods; regional aggregation vs. fine-grained analysis
- Failure signatures: Near-zero/negative correlations indicate cultural misalignment; high error rates signal topic difficulty
- First experiments: 1) Test alternative scoring metrics; 2) Expand regional groupings; 3) Compare with newer model generations

## Open Questions the Paper Calls Out
The paper raises several important open questions: How can we develop training methodologies that explicitly address cultural bias in LLMs? What are the implications of W.E.I.R.D. bias for global deployment of AI systems? Can we create evaluation frameworks that better capture cultural context beyond survey responses? How do we balance model performance with cultural representation when these goals conflict?

## Limitations
- Exclusive reliance on log-probability scores may not capture full nuances of human moral reasoning
- Fixed survey question set may miss cultural context and evolving moral perspectives
- Regional groupings oversimplify cultural distinctions, particularly for diverse regions like Sub-Saharan Africa
- The study does not account for potential temporal changes in moral attitudes across different survey waves
- Limited exploration of how model size and architecture interact with cultural alignment

## Confidence
- High confidence: Instruction-tuned models show substantially higher correlations with survey data
- Medium confidence: W.E.I.R.D. bias pattern is clear but underlying causes need further investigation
- Low confidence: Topic-specific performance variations may reflect question phrasing rather than moral reasoning limits

## Next Checks
1. Conduct human evaluation studies comparing LLM moral judgments with culturally diverse expert panels to validate log-probability-based scores
2. Expand regional analysis to include finer-grained cultural distinctions and validate findings across multiple survey waves over time
3. Test model performance on novel moral scenarios not present in training data to assess generalization beyond survey questions