---
ver: rpa2
title: Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks
arxiv_id: '2510.06695'
source_url: https://arxiv.org/abs/2510.06695
tags:
- llms
- tasks
- rewriting
- input
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for improving Large Language
  Model (LLM) performance on downstream tasks by rewriting the input component of
  prompts rather than the instruction component. The approach addresses the limitation
  that existing prompt engineering methods primarily focus on optimizing instructions,
  which is less effective for tasks like machine translation where the input component
  plays a critical role.
---

# Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks

## Quick Facts
- arXiv ID: 2510.06695
- Source URL: https://arxiv.org/abs/2510.06695
- Authors: Qinhao Zhou; Xiang Xiang; Kun He; John E. Hopcroft
- Reference count: 10
- Key outcome: Significant BLEU improvements (2.8-3.3 points) on machine translation tasks by rewriting input components rather than instructions

## Executive Summary
This paper introduces ROI (Rewrite the Input), a method for improving Large Language Model (LLM) performance on downstream tasks by optimizing the input component of prompts rather than the instruction component. The approach addresses the limitation that existing prompt engineering methods primarily focus on optimizing instructions, which is less effective for tasks like machine translation where the input component plays a critical role. The method employs a small-parameter rewriting model trained using a back-translation-based strategy, where the original training data is rewritten and used to train the model to learn the LLM's preferences for input data.

## Method Summary
The ROI framework constructs rewriting training data by having an LLM back-translate target outputs to create synthetic source inputs, which are paired with original inputs. A small rewriting model (mBart/mT5) is fine-tuned on these pairs. At inference, new inputs are rewritten through this model before being passed to the frozen LLM. For NLG tasks with longer inputs, a dedicated small model is used; for NLU tasks with shorter inputs, the LLM itself rewrites inputs using task-specific prompts. A filtering mechanism using similarity metrics (BLEU, ROUGE-L, edit distance) removes hallucinations and maintains data quality.

## Key Results
- Medical domain translation: +2.87 BLEU (Alpaca), +2.81 BLEU (Vicuna), +3.27 BLEU (Baize) improvements
- Translation Edit Rate (TER) improvements: -3.41 (Alpaca), -3.43 (Vicuna), -3.72 (Baize)
- Summarization (XSum): +1.5 RougeL improvement across all three LLMs
- GLUE benchmark: Accuracy and F1 improvements across tasks, with +1.5 F1 on SST-2 and +0.9 accuracy on CoLA

## Why This Works (Mechanism)

### Mechanism 1: Input Alignment via Preference Learning
The rewriting model learns to transform inputs into forms that LLMs process more effectively by training on back-translated pairs. The core assumption is that LLMs exhibit consistent preferences for certain input phrasings while preserving semantic content. This breaks if LLMs don't have stable input preferences or if preferences are instance-specific rather than learnable.

### Mechanism 2: Quality Filtering via Similarity Thresholding
Rewrites are scored against originals using BLEU, ROUGE-L, or edit distance, with low-scoring candidates discarded to prevent hallucinations. The assumption is that low similarity correlates with problematic rewrites while high similarity indicates safe transformations. This breaks if similarity metrics don't reliably distinguish beneficial from harmful rewrites.

### Mechanism 3: Task-Adaptive Rewriting Strategy
Different task types benefit from different approaches—small models for NLG tasks with substantial input text, and LLM self-rewriting for NLU tasks with shorter inputs. The assumption is that longer inputs require specialized training while shorter inputs can be handled by zero-shot LLM capabilities. This breaks if the short/long boundary is unclear or if LLM self-rewriting introduces bias.

## Foundational Learning

- **Back-translation (machine translation)**
  - Why needed here: The rewriting model training pipeline directly uses back-translation to create synthetic training pairs
  - Quick check question: Can you explain how back-translation creates parallel training data from monolingual corpora?

- **Prompt structure: instruction vs. input components**
  - Why needed here: ROI specifically targets the input component; understanding this distinction is essential for knowing when to apply this method
  - Quick check question: For a sentiment classification task, which part of "Classify sentiment: This movie was terrible" is the instruction versus the input?

- **Text similarity metrics (BLEU, ROUGE-L, edit distance)**
  - Why needed here: The filtering mechanism relies on these metrics to assess rewrite quality and semantic preservation
  - Quick check question: Why might BLEU be more suitable than ROUGE-L for filtering in translation tasks, according to the paper's ablation results?

## Architecture Onboarding

- **Component map:** Original dataset → LLM generates rewritten candidates → Similarity filtering → Filtered pairs train small rewriting model (mBART/mT5) → Inference: New input → Trained rewriting model → Filtered rewrite (or original if rejected) → Frozen LLM → Final output

- **Critical path:**
  1. Construct rewriting training data via back-translation (for NLG) or LLM self-rewriting (for NLU)
  2. Apply filtering with chosen metric and threshold (BLEU recommended for translation; threshold 0.3–0.8 tested)
  3. Fine-tune small rewriting model on filtered pairs
  4. At inference, rewrite inputs and pass to LLM

- **Design tradeoffs:**
  - Rewriting model size: mBART (406M) outperforms smaller models (Tiny-mBART 60M) but increases overhead
  - Filtering threshold: Higher thresholds (0.8) favor conservative rewrites; lower thresholds (0.3) allow more aggressive changes but risk noise
  - Metric choice: BLEU stable across thresholds for translation; ROUGE-L sensitive but best at 0.5 for NLU

- **Failure signatures:**
  - Hallucinated rewrites (low similarity to original, semantic drift)
  - No improvement or degradation on domains with difficult or already-optimal data
  - Excessive filtering (most inputs reverted to original, no benefit)

- **First 3 experiments:**
  1. Replicate Medical translation with mBART rewriting model, BLEU filtering at threshold 0.5—verify ~2.9 BLEU improvement
  2. Ablate filtering: run with no filter, BLEU filter, and edit distance filter—compare noise retention vs. over-filtering
  3. Test generalization: apply same trained rewriting model to a held-out domain (e.g., Law if trained on Medical)—measure cross-domain transfer

## Open Questions the Paper Calls Out

- **Cross-LLM scalability**: The authors explicitly state they have not yet conducted experiments on larger-scale LLMs like GPT-3.5, leaving transferability to massive commercial models unknown.

- **Multi-turn conversational tasks**: The method is primarily limited to single-turn question-answering language tasks, lacking a mechanism for handling context-dependent turns without hallucinating or losing context.

- **Automated filtering**: The ablation study shows filtering effectiveness varies significantly by task and metric, requiring manual selection of thresholds (0.3 vs 0.8), suggesting the process is brittle and requires manual adjustment.

## Limitations

- **Input vs. instruction optimization tradeoff**: Does not directly compare against instruction optimization methods (RLPrompt, APO) on the same tasks, making it unclear whether input rewriting is universally superior or task-dependent.

- **Cross-domain generalization gaps**: Method shows domain-specific improvements but limited cross-domain transfer, with Medical domain showing consistent gains while IT and Koran show mixed or negative results.

- **Small-data scaling limitations**: Performance plateaus when training data exceeds ~10K samples, indicating potential overfitting or quality degradation with larger datasets.

## Confidence

- **High confidence**: The core mechanism of input rewriting via back-translation is well-established through ablation studies and consistent performance improvements across multiple domains and LLMs.

- **Medium confidence**: The task-adaptive strategy (small model for NLG, LLM self-rewriting for NLU) is supported by experimental results but lacks extensive ablation across all task types.

- **Medium confidence**: The filtering mechanism using similarity metrics is theoretically sound and shows positive results, but optimal threshold selection appears dataset-dependent.

## Next Checks

1. **Cross-method comparison**: Implement and compare ROI against instruction-focused prompt optimization methods (APO, RLPrompt) on the same machine translation tasks to determine relative effectiveness.

2. **Metric ablation study**: Test additional similarity metrics (BERTScore, MoverScore) alongside BLEU/ROUGE-L/edit distance to determine if alternative metrics provide better filtering for specific task types.

3. **Domain generalization experiment**: Train the rewriting model on a diverse multi-domain dataset (combining Medical, IT, Law) and evaluate performance across all held-out domains to assess whether domain-specific training can be replaced with broader generalization.