---
ver: rpa2
title: Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects
arxiv_id: '2511.02132'
source_url: https://arxiv.org/abs/2511.02132
tags:
- attention
- block
- cache
- head
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of non-uniform memory access (NUMA)
  effects in large-scale attention workloads on disaggregated AI GPUs. The core method,
  Swizzled Head-first Mapping, is a spatially-aware scheduling strategy that aligns
  attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse.
---

# Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects

## Quick Facts
- arXiv ID: 2511.02132
- Source URL: https://arxiv.org/abs/2511.02132
- Reference count: 9
- Primary result: Swizzled Head-first Mapping achieves up to 50% higher performance and 80-97% L2 cache hit rates on AMD MI300X by mitigating NUMA effects in attention workloads

## Executive Summary
This paper addresses the problem of non-uniform memory access (NUMA) effects in large-scale attention workloads on disaggregated AI GPUs, specifically AMD's MI300X architecture. The core method, Swizzled Head-first Mapping, is a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. By co-locating all workgroups of a single attention head onto one physical chiplet, the approach prevents cache fragmentation and significantly improves L2 cache hit rates, achieving up to 50% performance gains over state-of-the-art attention algorithms using conventional scheduling techniques.

## Method Summary
The method implements a swizzling-based workgroup scheduling strategy for FlashAttention2 kernels on disaggregated GPU architectures. The approach remaps linear workgroup IDs to ensure all query blocks of the same attention head execute on the same Accelerator Complex Die (XCD), maximizing L2 cache reuse for shared Key/Value tensors. The implementation uses spatial swizzling of workgroup IDs to override the hardware's default round-robin scheduler, with block sizes of BLOCK_M=128 and BLOCK_N=64. The technique is evaluated across Multi-Head Attention (MHA) and Grouped Query Attention (GQA) workloads with varying context lengths (2K-128K), batch sizes (1-8), and attention heads (8-128).

## Key Results
- Achieves up to 50% higher performance compared to state-of-the-art attention algorithms using conventional scheduling
- Maintains consistently high L2 cache hit rates of 80-97% on AMD MI300X architecture
- Shows widening performance gap as context length increases from 8K to 128K tokens
- Demonstrates effectiveness across both Multi-Head Attention and DeepSeekV3 prefill workloads

## Why This Works (Mechanism)

### Mechanism 1: Cache Locality through Head Co-location
Co-locating all workgroups of a single attention head onto one physical chiplet (XCD) maximizes L2 cache hit rates by concentrating shared data access. The "Swizzled Head-first Mapping" reorders the execution grid so that all query blocks sharing the same Key (K) and Value (V) tensors execute on the same Accelerator Complex Die (XCD), preventing cache splitting across dies.

### Mechanism 2: Eliminating Cache Fragmentation
Default round-robin scheduling causes "cache fragmentation," significantly lowering effective bandwidth on disaggregated architectures. Standard schedulers distribute workgroups in a chunked round-robin fashion, scattering WGs processing the same data across different physical dies, forcing redundant fetches from HBM for shared K/V data.

### Mechanism 3: Scaling with Context Length
Performance improvements scale with context length and head count due to increased data reuse opportunity. As sequence length grows, the amount of shared K/V data per head increases, making the reduction in cross-die traffic and HBM accesses more pronounced.

## Foundational Learning

- **Non-Uniform Memory Access (NUMA) on GPUs**: Why needed here - Unlike traditional monolithic GPUs, chiplet-based designs have memory controllers and caches distributed across dies. Quick check - Does the L2 cache of XCD0 cache data requested by a Compute Unit (CU) on XCD1?

- **FlashAttention Tiling Strategy**: Why needed here - The optimization relies on the fact that within a single head, multiple query blocks all need to read the entire Key and Value matrices. Quick check - In FlashAttention, do different workgroups processing different query rows share access to the same Key (K) matrix?

- **Workgroup Swizzling**: Why needed here - This is the implementation technique used to override the hardware's default scheduler. Quick check - If you have 8 XCDs and 128 blocks per head, how do you calculate the target XCD ID for a specific linear workgroup ID to ensure all blocks of head 0 land on XCD 0?

## Architecture Onboarding

- **Component map**: MI300X -> 8 XCDs -> Each XCD contains 38 CUs and private 4 MB L2 cache -> 192 GB HBM3 distributed across memory controllers linked to XCDs
- **Critical path**: 1) Load K/V blocks into L2, 2) Compute QK^T, 3) Softmax, 4) Compute PV. Optimization Focus: Step 1 - ensure Step 1 hits the L2 cache for all subsequent blocks of the same head.
- **Design tradeoffs**: Load Balance vs. Locality - strictly binding heads to XCDs maximizes locality but could theoretically hurt load balance; Code Complexity - swizzling logic adds complexity to kernel launch code.
- **Failure signatures**: L2 Hit Rate Drop - if L2 hit rate drops below 80% at high sequence lengths, scheduling is fragmenting the ACC across XCDs; Stagnant Performance - if speedups do not appear at 128K+ tokens, swizzling logic may be incorrect.
- **First 3 experiments**:
  1. Baseline Profiling: Run standard FlashAttention kernel on MI300X with 128K sequence length and measure L2 hit rate using ROCProfiler v3 hardware counters.
  2. Swizzling Implementation: Implement the ID remapping logic from Figure 11. Run same workload and verify all blocks of Head 0 are dispatched to XCD 0.
  3. Scaling Validation: Compare throughput and L2 hit rates of Naive vs. Swizzled Head-first kernels across 8K, 32K, and 128K context lengths.

## Open Questions the Paper Calls Out

- **Backward Pass Optimization**: Can further optimizations close the performance gap in the FlashAttention2 backward pass, where Swizzled Head-first achieves only ~10% improvement versus ~50% in the forward pass? The paper notes that "further gains may be constrained by emerging bottlenecks" and states "we leave further optimization to future work."

- **Architecture Generalization**: How effectively does Swizzled Head-first Mapping generalize to architectures like NVIDIA Blackwell that abstract NUMA effects through hardware cache coherency? All evaluation is on MI300X, while NVIDIA Blackwell maintains full cache coherency between dies.

- **Integration with Temporal Optimizations**: How does Swizzled Head-first interact with orthogonal FlashAttention optimizations such as asynchronous pipelining (FlashAttention-3)? The paper optimizes spatial locality independently but does not combine with temporal optimizations like async operations.

## Limitations

- Effectiveness depends critically on L2 cache capacity (4 MB per XCD) being sufficient for KV blocks - may break down for very long sequences or wide head dimensions
- Implementation complexity introduces maintenance overhead and potential for subtle bugs
- Performance evaluation focuses on forward pass kernel performance rather than end-to-end training impact

## Confidence

**High Confidence Claims:**
- The fundamental NUMA problem exists on disaggregated GPU architectures like MI300X
- L2 cache hit rate measurements (80-97%) for Swizzled Head-first vs. 1-60% for Naive approaches are reliable

**Medium Confidence Claims:**
- The 50% performance improvement figure represents an upper bound observed at specific configurations
- Performance improvements scale with context length

**Low Confidence Claims:**
- Generalization to architectures beyond MI300X without additional validation
- End-to-end training performance impact

## Next Checks

1. **Cross-Architecture Validation**: Implement and benchmark the Swizzled Head-first Mapping on NVIDIA Grace-Hopper or other disaggregated GPU systems to verify the NUMA optimization principle generalizes beyond MI300X.

2. **Scaling Boundary Analysis**: Systematically test the approach at sequence lengths and head dimensions that approach/exceed the 4 MB L2 cache capacity per XCD to identify the precise breaking point where cache thrashing negates NUMA benefits.

3. **End-to-End Training Impact**: Measure the effect of this scheduling optimization on full training runs (including backward pass and optimizer steps) rather than just forward pass kernel performance.