---
ver: rpa2
title: Evaluating The Impact of Stimulus Quality in Investigations of LLM Language
  Performance
arxiv_id: '2510.06018'
source_url: https://arxiv.org/abs/2510.06018
tags:
- stimuli
- filler
- gpt-2
- about
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-evaluates Large Language Models' (LLMs) performance
  on parasitic gap (PG) constructions, building on prior work by Lan et al. (2024).
---

# Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance

## Quick Facts
- **arXiv ID**: 2510.06018
- **Source URL**: https://arxiv.org/abs/2510.06018
- **Reference count**: 15
- **Primary result**: Refined stimuli with controlled confounds improved GPT-2's performance on parasitic gap constructions from ~6% to ~60% accuracy

## Executive Summary
This paper re-evaluates Large Language Models' (LLMs) performance on parasitic gap (PG) constructions, building on prior work by Lan et al. (2024). The authors identify potential confounds in the original stimuli, such as lexical ambiguities (e.g., "John's" meaning "John is" vs. possessive) and structural complexity in the noun phrases hosting parasitic gaps. To address these issues, they generate a new, controlled dataset using Gemini 2.5 Pro Preview, guided by linguistically-informed templates designed to mitigate these confounds. They then compare GPT-2's performance on the original Lan et al. stimuli, a filtered subset, and their new refined stimuli. The results show a significant improvement in GPT-2's performance on the refined stimuli, with accuracy increasing from approximately 6% to 60% for the direct preference criterion (Δ+filler > 0) and from approximately 69% to 80% for the Difference-in-Differences (DiD) criterion. These findings suggest that stimulus quality significantly influences outcomes in surprisal-based evaluations of LLM syntactic competency, and that conclusions about LLM failures to acquire linguistic phenomena may be premature if based on stimuli with significant potential confounds.

## Method Summary
The authors generated a refined stimulus dataset for evaluating parasitic gap constructions using Gemini 2.5 Pro Preview. They applied linguistically-informed templates to systematically control for lexical ambiguities and structural complexity in the noun phrases hosting parasitic gaps. The refined stimuli were then evaluated using GPT-2, comparing performance against the original Lan et al. stimuli and a filtered subset. Performance was measured using two criteria: direct preference (Δ+filler > 0) and Difference-in-Differences (DiD), showing substantial improvements in accuracy when using the refined stimuli.

## Key Results
- GPT-2's performance on refined stimuli improved from ~6% to ~60% for the direct preference criterion
- GPT-2's performance on refined stimuli improved from ~69% to ~80% for the Difference-in-Differences criterion
- The refined stimuli systematically controlled for lexical ambiguities and structural complexity that may have confounded earlier evaluations

## Why This Works (Mechanism)
The refined stimuli eliminate lexical ambiguities (e.g., possessive vs. contraction forms like "John's") and structural complexity in noun phrases hosting parasitic gaps. By using linguistically-informed templates and controlled vocabulary, the new stimuli provide clearer evidence about whether LLMs can acquire parasitic gap constructions without being confounded by superficial linguistic features.

## Foundational Learning
- **Parasitic gap constructions**: A syntactic phenomenon where a gap (missing element) appears in a subordinate clause that is licensed by another gap in the main clause - needed to understand what's being evaluated
- **Surprisal-based evaluation**: Measuring model performance through likelihood scores or preferences between grammatical and ungrammatical variants - needed to understand the evaluation methodology
- **Difference-in-Differences (DiD)**: A statistical approach that compares the difference in model preferences between grammatical and ungrammatical constructions across different conditions - needed to understand the analytical framework
- **Stimulus confounds**: Extraneous linguistic features in test items that may influence model behavior independently of the target phenomenon - needed to understand why stimulus quality matters
- **Lexical ambiguity**: Words or phrases with multiple interpretations (e.g., "John's" as possessive vs. contraction) - needed to understand the specific confounds addressed
- **Structural complexity**: The degree of embeddedness and hierarchical organization in syntactic structures - needed to understand how complexity affects model performance

## Architecture Onboarding
- **Component map**: GPT-2 (decoder-only transformer) -> Stimuli generation (Gemini 2.5 Pro Preview) -> Evaluation metrics (Δ+filler, DiD) -> Performance comparison
- **Critical path**: Stimuli generation → Model evaluation → Performance measurement → Result interpretation
- **Design tradeoffs**: Controlled stimuli improve measurement validity but may reduce ecological validity; smaller models show clearer confound effects but may not generalize to larger models
- **Failure signatures**: Poor performance on refined stimuli would suggest fundamental inability to learn parasitic gaps; unchanged performance would suggest confounds weren't driving earlier failures
- **First 3 experiments**: 1) Evaluate GPT-4 on both original and refined stimuli, 2) Systematically vary one confound dimension at a time, 3) Test additional syntactic phenomena with similar methodology

## Open Questions the Paper Calls Out
None

## Limitations
- Cannot definitively separate stimulus quality effects from potential LLM generalization effects
- Results based on GPT-2 may not generalize to larger models
- Only evaluates one syntactic phenomenon (parasitic gaps)
- Refined stimuli introduce new vocabulary and structures that may affect model performance differently

## Confidence
- **High confidence**: Methodological improvements for controlling lexical ambiguity and structural complexity are sound, and observed performance differences are statistically robust for GPT-2
- **Medium confidence**: Conclusion about stimulus quality influencing surprisal-based evaluations applies specifically to the evaluated phenomenon and model
- **Low confidence**: Claims about LLM "failures to acquire linguistic phenomena" remain speculative without evidence that refined stimuli represent truly unambiguous input

## Next Checks
1. Replicate the refined stimulus generation and evaluation protocol with multiple LLM families (including GPT-4 and Claude) to assess whether smaller models are uniquely sensitive to stimulus confounds
2. Conduct ablation studies systematically varying one confound dimension at a time (e.g., lexical ambiguity vs. structural complexity) to isolate their individual effects on model performance
3. Extend the methodology to evaluate at least two additional syntactic phenomena with different acquisition profiles to determine whether stimulus quality effects are phenomenon-general or specific to parasitic gaps