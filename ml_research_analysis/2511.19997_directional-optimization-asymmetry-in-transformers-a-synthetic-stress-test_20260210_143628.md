---
ver: rpa2
title: 'Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test'
arxiv_id: '2511.19997'
source_url: https://arxiv.org/abs/2511.19997
tags:
- directional
- loss
- inverse
- excess
- forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic benchmark to isolate directional
  learning asymmetries in Transformers, separating architectural effects from data
  confounds. Using random string mappings with tunable branching factor K, the benchmark
  controls entropy and ensures direction neutrality.
---

# Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test

## Quick Facts
- arXiv ID: 2511.19997
- Source URL: https://arxiv.org/abs/2511.19997
- Reference count: 13
- Primary result: Scratch-trained causal Transformers exhibit significantly larger excess loss on inverse tasks compared to forward tasks, even when data is direction-neutral.

## Executive Summary
This paper introduces a synthetic benchmark to isolate directional learning asymmetries in Transformers by separating architectural effects from data confounds. Using random string mappings with tunable branching factor K, the benchmark controls entropy and ensures direction neutrality. Forward tasks have zero conditional entropy while inverse tasks have analytically determined entropy floors (log K). The "excess loss" metric isolates optimization inefficiency beyond the theoretical minimum. Experiments compare scratch-trained GPT-2 against MLP and pre-trained/GPT-2 with LoRA, revealing that scratch Transformers exhibit a large directional gap (e.g., 1.16 nats at K=5) exceeding that of MLPs, indicating architectural bias. Pre-training reduces forward efficiency but not directional asymmetry; LoRA struggles on high-entropy inverse mappings due to capacity limits. The benchmark isolates a semantics-free signature of directional friction intrinsic to causal Transformer training.

## Method Summary
The method uses synthetic string mappings where forward (A→B) is deterministic (H=0) and inverse (B→A) is probabilistic with entropy floor log(K). GPT-2 Small is trained on prompts like "x: A y: B" with causal LM objective, masking prompt tokens. Excess loss is computed as observed loss minus entropy floor. Directional gap Δ_dir = L_inverse_excess - L_forward_excess. The setup includes MLP baseline and LoRA fine-tuning at ranks {8,64,256}, with hyperparameters specified for each configuration.

## Key Results
- Scratch GPT-2 shows directional gap of 1.16 nats at K=5 vs MLP gap of 0.22 nats on identical data
- Pre-training reduces forward efficiency but preserves directional asymmetry
- LoRA inverse excess loss = 3.15 nats at K=5, r=256 vs scratch = 2.07, full FT = 1.47
- Directional gap vanishes at K=1 (bijective case) for both architectures

## Why This Works (Mechanism)

### Mechanism 1: Causal Factorization Creates Directional Friction
Scratch-trained causal Transformers exhibit larger excess loss on inverse (B→A) mappings than forward (A→B), even when data is direction-neutral. Autoregressive left-to-right factorization couples gradient signals sequentially, creating optimization path asymmetry when inverse tasks require recovering multiple pre-images. The directional gap reflects training dynamics, not representational incapacity—models can represent inverse distributions but optimize less efficiently. Confirmed by scratch vs MLP comparison at K=5 (gap: 1.16 vs 0.22 nats).

### Mechanism 2: Entropy Floor Bounds Inverse Task Difficulty
Inverse task difficulty is analytically bounded by H(A|B) = log(K), separating irreducible entropy from optimization inefficiency. For branching factor K, each B has K valid pre-images uniformly distributed; optimal policy achieves cross-entropy = log(K). Excess loss above this floor measures architectural inefficiency alone. The uniform distribution over pre-images is learnable given sufficient model capacity and optimization.

### Mechanism 3: LoRA Capacity Wall on High-Entropy Inverses
Low-rank adaptation fails to efficiently memorize high-entropy inverse mappings even at rank r=256. LoRA constrains weight updates to low-rank subspaces; high-entropy inverse mappings require representing K-way uniform distributions per target, demanding higher effective rank than forward (deterministic) mappings. The bottleneck is expressivity, not optimization hyperparameters. Confirmed by LoRA inverse excess loss (3.15 nats at K=5, r=256) exceeding both scratch (2.07) and full FT (1.47).

## Foundational Learning

- Concept: Conditional Entropy H(Y|X)
  - Why needed here: Distinguishes irreducible uncertainty (entropy floor) from optimization inefficiency; forward H(B|A)=0, inverse H(A|B)=log(K)
  - Quick check question: If K=8, what is the theoretical minimum cross-entropy for the inverse task? (Answer: log(8) ≈ 2.08 nats)

- Concept: Excess Loss Metric
  - Why needed here: Normalizes comparison across tasks with different inherent difficulty; isolates architectural effects from information-theoretic bounds
  - Quick check question: If observed inverse loss = 3.5 nats at K=5, what is excess loss? (Answer: 3.5 - log(5) ≈ 3.5 - 1.61 = 1.89 nats)

- Concept: Causal vs. Bidirectional Attention
  - Why needed here: Explains why MLP (non-causal) shows smaller directional gap than causal Transformer
  - Quick check question: Why does causal masking create directional asymmetry when data is symmetric? (Answer: Left-to-right factorization couples gradients sequentially, creating optimization path differences between forward deterministic and inverse probabilistic mappings)

## Architecture Onboarding

- Component map: Data generator -> Prompt formatter -> Loss masker -> Loss calculator -> Excess loss calculator -> Directional gap calculator

- Critical path: 1. Generate pairs with branching factor K → 2. Format prompts symmetrically → 3. Train with causal LM objective → 4. Compute excess loss vs. entropy floor → 5. Compare directional gap (Δ_dir = L_excess^inverse - L_excess^forward)

- Design tradeoffs: Synthetic simplicity vs. real-world relevance: Strips semantic priors to isolate architecture, but cannot directly explain natural-language reversal failures. GPT-2 Small scale vs. modern LLMs: 124M params sufficient to detect gap; scale effects unknown. BPE tokenizer on random strings: Creates fragmented tokens but affects both directions symmetrically.

- Failure signatures: K=1 asymmetry: If gap appears at K=1, data construction is biased (should not happen with correct implementation). LoRA converging on forward but plateauing on inverse: Capacity wall confirmed. Pre-trained worse than scratch on forward: "Plasticity tax" from pre-trained weight rigidity.

- First 3 experiments: 1. **Sanity check**: Run K=1 bijective case—confirm zero directional gap for both Transformer and MLP (Table 1 baseline). 2. **Reproduce main finding**: Train scratch GPT-2 on K=5 with 40K pairs, measure directional gap (target: ~1.1-1.2 nats). 3. **Ablate architecture**: Compare causal Transformer vs. MLP on K=8—confirm MLP gap remains small (~0.1 nats) while Transformer gap stays large (~0.9 nats).

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanistic factors within the causal Transformer architecture drive the observed directional optimization friction? The paper concludes by motivating "deeper mechanistic study of why inversion remains fundamentally harder" and explicitly states the benchmark "does not diagnose the ultimate cause of asymmetry," isolating only that the friction is "intrinsic to causal Transformer training." The results demonstrate the existence of a gap (1.16 nats at K=5) compared to MLPs, but the analysis does not determine if the root cause is the causal masking, the autoregressive factorization, or the specific optimization dynamics of self-attention.

### Open Question 2
Does increasing model scale mitigate or exacerbate the directional optimization asymmetry? The "Limitations" section notes that experiments were restricted to GPT-2 Small (≈124M parameters) and explicitly states, "We do not claim scale laws for directional asymmetry," leaving the behavior of larger models unknown. It is unclear if the "excess loss" gap is a fundamental property of the architecture that persists with scale, or if larger models possess sufficient capacity to close the efficiency gap between forward and inverse tasks.

### Open Question 3
Is the directional asymmetry caused primarily by the causal attention mask or the autoregressive training objective? The paper contrasts causal GPT-2 with a non-causal MLP, finding the MLP has a significantly smaller gap. However, the paper does not isolate the specific architectural component responsible, noting only that the gap is "tied to the causal Transformer architecture or its optimization dynamics." The comparison between a full Transformer and a simple MLP conflates architectural differences (attention vs. fully connected layers) with the directional constraints of causal masking.

## Limitations

- Synthetic domain transfer: The benchmark isolates architecture-specific directional friction but cannot directly explain why causal Transformers struggle with naturally occurring inverse tasks where semantic priors exist.
- Pre-training and post-training effects: While the paper shows pre-training reduces forward efficiency without eliminating directional asymmetry, the mechanisms of how real-world training dynamics affect these gaps are not explored.
- Mechanistic opacity: The benchmark demonstrates a directional gap but does not explain why causal factorization creates optimization path asymmetry—the observed gap could stem from gradient propagation delays, attention pattern asymmetry, or other factors.

## Confidence

- High confidence: The directional gap measurement methodology and results are reproducible given the synthetic setup. The information-theoretic floor derivation (H(A|B) = log K) is mathematically sound.
- Medium confidence: The claim that the gap reflects training dynamics (not representational incapacity) is supported by scratch vs. pre-trained comparisons, but direct mechanistic evidence is limited.
- Low confidence: Extrapolating from K=1 (bijective, zero gap) to K>1 (many-to-one, large gap) as evidence of architectural bias assumes the synthetic setup perfectly isolates causal effects—real data confounds could amplify the effect.

## Next Checks

1. **Scale dependence study**: Test whether the directional gap persists or diminishes at larger model scales (GPT-2 Medium/Large or modern LLMs) to determine if it's a fundamental architectural constraint or a small-model artifact.

2. **Attention mechanism ablation**: Replace causal attention with bidirectional attention in the Transformer while keeping other components identical; measure if the directional gap vanishes or shrinks, isolating the causal masking contribution.

3. **Gradient flow analysis**: Track gradient norms and effective learning rates for forward vs. inverse tasks during training; determine if the gap correlates with differential gradient propagation or vanishing gradients in the inverse direction.