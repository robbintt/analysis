---
ver: rpa2
title: Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning
arxiv_id: '2512.04359'
source_url: https://arxiv.org/abs/2512.04359
tags:
- entropy
- learning
- tokens
- reasoning
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles entropy collapse in reinforcement learning for
  LLM reasoning by introducing a dual-level framework that combines semantic entropy-guided
  curriculum learning with token-level optimization. The method organizes training
  data from low to high semantic entropy to scaffold learning complexity and applies
  KL regularization on low-entropy tokens, with stronger constraints on high-covariance
  portions, to maintain exploration.
---

# Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning

## Quick Facts
- arXiv ID: 2512.04359
- Source URL: https://arxiv.org/abs/2512.04359
- Reference count: 40
- Addresses entropy collapse in RLVR by combining semantic entropy curriculum with token-level KL regularization

## Executive Summary
This paper tackles entropy collapse in reinforcement learning for LLM reasoning by introducing a dual-level framework that combines semantic entropy-guided curriculum learning with token-level optimization. The method organizes training data from low to high semantic entropy to scaffold learning complexity and applies KL regularization on low-entropy tokens, with stronger constraints on high-covariance portions, to maintain exploration. Extensive experiments across 6 benchmarks with 3 base models (1.5B, 7B, 14B) show that this approach consistently outperforms entropy-based baselines, effectively mitigates entropy collapse, and enhances reasoning performance while sustaining exploration.

## Method Summary
The method operates in two stages: First, it computes semantic entropy for each query by sampling multiple responses, clustering them by answer equivalence, and measuring entropy over clusters. Training data is then sorted by ascending semantic entropy and split into curriculum stages. During RL training, the approach identifies low-entropy tokens and applies hierarchical KL regularization—stronger constraints on high-covariance tokens within the low-entropy subset. This dual-level optimization preserves exploration while maintaining reasoning performance, implemented within the GRPO framework with modified token-specific regularization coefficients.

## Key Results
- Consistently outperforms entropy-based baselines across 6 benchmarks (AIME 2024/2025, AMC 2023, MATH500, OlympiadBench, Minerva)
- Maintains stable entropy trajectories (0.2-0.4 range) while preventing collapse observed in standard GRPO
- Shows robust performance across model scales (1.5B, 7B, 14B parameters) with 2-stage curriculum optimal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Organizing training data by semantic entropy creates a curriculum that scaffolds learning complexity and stabilizes entropy dynamics.
- Mechanism: Semantic entropy clusters model responses by meaning equivalence (e.g., same final answer), then computes entropy over these clusters. Queries with low semantic entropy indicate consistent solutions (easier); high semantic entropy indicates diverse solution paths (harder). Training progresses from low to high semantic entropy, preventing abrupt difficulty jumps that trigger entropy collapse.
- Core assumption: Semantic entropy measured from initial policy correlates with intrinsic task difficulty and remains informative across training.
- Evidence anchors:
  - [abstract] "organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks"
  - [Section IV-A] "This curriculum design prevents the model from encountering overly challenging problems prematurely, which would cause aggressive policy updates and rapid entropy collapse."
  - [corpus] Related work "Rethinking Entropy Interventions in RLVR" confirms entropy collapse as a critical risk but does not test curriculum-based interventions, suggesting limited external validation of data-level approaches.
- Break condition: If semantic entropy estimates are noisy or task difficulty is orthogonal to solution diversity, curriculum provides no benefit over random ordering.

### Mechanism 2
- Claim: Low-entropy tokens are the primary drivers of entropy collapse and require targeted KL regularization to preserve exploration.
- Mechanism: Low-entropy tokens (where H_t < τ_H) represent near-deterministic decisions that constrain reasoning diversity. Applying KL regularization to these tokens (but not high-entropy tokens) slows entropy decline at vulnerable positions while allowing free optimization elsewhere.
- Core assumption: Low-entropy tokens identified by current policy meaningfully constrain exploration rather than being trivial linguistic tokens.
- Evidence anchors:
  - [abstract] "applies KL regularization on low-entropy tokens that critically impact policy exploration"
  - [Section IV-B] "approximately 80% of low-entropy tokens significantly influence the learning process for reasoning"
  - [corpus] "Beyond the 80/20 rule" (masked low-entropy tokens) supports the importance of low-entropy tokens but uses masking rather than KL regularization—different intervention strategy.
- Break condition: If low-entropy tokens are predominantly function words (as word cloud analysis suggests), regularization may constrain linguistically trivial tokens while missing reasoning-critical positions.

### Mechanism 3
- Claim: Within low-entropy tokens, high-covariance positions are most influential for entropy dynamics and require stronger constraints.
- Mechanism: Covariance (Cov_t) measures correlation between log-probability and advantage. High covariance indicates tokens where confident predictions receive strong learning signals—precisely where policy updates most aggressively reduce entropy. Applying β_high > β_low KL penalty to high-covariance subsets within low-entropy tokens provides fine-grained entropy preservation where it matters most.
- Core assumption: Covariance computed from current rollouts accurately identifies tokens that will drive entropy collapse in subsequent updates.
- Evidence anchors:
  - [Section IV-C, Theorem 2] "The second term represents the entropy-preserving effect of our KL regularization... this term has a positive contribution to entropy change, counteracting the entropy collapse from Term 1."
  - [Section V-D, Fig. 9] "jointly constraining high-covariance tokens with low-entropy tokens yields superior learning dynamics compared to constraining low-entropy tokens in isolation"
  - [corpus] "The entropy mechanism of reinforcement learning for reasoning language models" derives the covariance-entropy relationship but applies uniform clipping rather than hierarchical KL constraints.
- Break condition: If covariance estimates are unstable across batches or advantage estimates are noisy, high-covariance identification becomes unreliable.

## Foundational Learning

- Concept: **Policy entropy in language models**
  - Why needed here: The entire framework hinges on understanding entropy as a measure of exploration capacity, not just prediction uncertainty.
  - Quick check question: Can you explain why entropy collapse reduces reasoning capability even if accuracy temporarily improves?

- Concept: **KL divergence as a regularization tool**
  - Why needed here: The token-level intervention uses KL penalties to constrain policy updates—understanding why KL (not direct entropy maximization) is chosen is critical.
  - Quick check question: Why might maximizing entropy directly cause instability, while KL regularization provides controlled exploration?

- Concept: **Curriculum learning and difficulty estimation**
  - Why needed here: The semantic entropy curriculum assumes difficulty can be estimated from solution diversity—understanding this assumption helps diagnose when it will fail.
  - Quick check question: On what types of tasks might semantic entropy poorly correlate with actual difficulty?

## Architecture Onboarding

- Component map:
  Semantic Entropy Calculator -> Curriculum Organizer -> Token Entropy Identifier -> Covariance Analyzer -> Adaptive KL Applicator -> Modified GRPO Optimizer

- Critical path:
  1. Pre-training: Compute semantic entropy for all queries → sort → partition
  2. Per-step: Rollout responses → compute advantages → identify T_low → compute covariance → assign β_con → update with J_SENT

- Design tradeoffs:
  - Two-stage vs. three-stage curriculum: Paper finds two-stage optimal; more stages dilute signal.
  - Thresholds (τ_H, τ_cov): Default en=80%, cov=0.0002; paper claims low sensitivity but provides no sensitivity curves.
  - β_high vs. β_low: Default β_high=2, β_low=0.5; ratio matters more than absolute values.

- Failure signatures:
  - Entropy explosion (persistent high entropy): KL constraints too weak or curriculum stages too easy.
  - Continued entropy collapse despite intervention: Check if T_low identification is catching reasoning-critical tokens or just function words.
  - Instability after curriculum stage transition: Difficulty jump too steep—consider smoother curriculum or more stages.

- First 3 experiments:
  1. **Baseline replication**: Reproduce GRPO entropy collapse on a single benchmark (e.g., AIME24) to confirm your setup matches paper dynamics. Plot entropy and Pass@K curves.
  2. **Ablation by component**: Test curriculum-only (no token-level KL), token-level-only (no curriculum), and full SENT. Quantify contribution of each component to entropy maintenance and reasoning performance.
  3. **Threshold sensitivity sweep**: Vary τ_H (70%, 80%, 90%) and τ_cov (0.0001, 0.0002, 0.0025) on a held-out benchmark. Paper claims robustness—verify whether performance range is acceptable for your task domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can causally-informed data sequencing outperform semantic entropy-based curriculum learning for scaffolding reasoning complexity?
- Basis in paper: [explicit] The authors state a future intention to "explore causal reasoning frameworks to refine our curriculum design, moving beyond entropy-based difficulty estimation toward causally-informed data sequencing."
- Why unresolved: Semantic entropy measures uncertainty (difficulty) but may not capture the prerequisite structure or dependencies between reasoning concepts as effectively as a causal framework could.
- What evidence would resolve it: Experiments comparing model reasoning performance and convergence speed when training data is ordered by causal graphs versus the proposed semantic entropy sorting.

### Open Question 2
- Question: Can empowerment-based optimization objectives mitigate the policy instability occasionally observed during training more effectively than current KL constraints?
- Basis in paper: [explicit] The authors propose future work involving "incorporating empowerment-based optimization objectives... to mitigate the policy instability occasionally observed during training."
- Why unresolved: While SENT uses KL regularization to maintain exploration, the authors acknowledge that instability can still occur; empowerment provides a different, principled mechanism for controlled exploration that remains untested in this context.
- What evidence would resolve it: A comparative study analyzing training curves and variance in rewards between the current SENT implementation and a variant utilizing empowerment objectives.

### Open Question 3
- Question: Does analyzing entropy at the reasoning-step or sub-problem level yield more effective optimization signals than the current token-level approach?
- Basis in paper: [explicit] The Broader Impact section notes a limitation of the current focus on token-level entropy, suggesting potential to "explore more sophisticated granularities... such as reasoning step-level or sub-problem-level entropy patterns."
- Why unresolved: Token-level entropy might miss higher-order structural uncertainties or over-regularize individual tokens that are part of a stable reasoning step, potentially limiting nuanced exploration.
- What evidence would resolve it: Implementation of a hierarchical entropy mechanism that segments responses by reasoning steps, followed by benchmarking against the token-level SENT baseline.

## Limitations

- Semantic entropy may not reliably correlate with task difficulty across diverse reasoning domains, limiting curriculum effectiveness
- Low-entropy tokens often contain function words rather than reasoning-critical positions, potentially misdirecting regularization
- Covariance estimation stability is uncertain, with fixed thresholds appearing arbitrary without sensitivity analysis

## Confidence

**High confidence** in: The entropy collapse problem being well-documented in RLVR literature, the general framework architecture (semantic curriculum + token-level KL), and the experimental methodology (controlled ablation studies across multiple models and benchmarks).

**Medium confidence** in: The specific numerical thresholds used (τ_H=80%, τ_cov=0.0002, β_high=2, β_low=0.5), the assumption that semantic entropy reliably proxies task difficulty, and the claim that low-entropy tokens are predominantly reasoning-critical rather than linguistic artifacts.

**Low confidence** in: The long-term stability of the intervention beyond the 5-epoch training window shown, generalization to non-mathematical reasoning domains, and whether the curriculum ordering provides benefits beyond random shuffling when token-level KL regularization is already applied.

## Next Checks

1. **Cross-domain generalization test**: Apply SENT to non-mathematical reasoning tasks (e.g., commonsense reasoning, multi-hop QA) to verify whether semantic entropy curriculum remains meaningful when answers cannot be easily clustered by equivalence. Track both entropy trajectories and reasoning quality across task types.

2. **Token-level ablation with linguistic analysis**: Replace the current low-entropy token identification with a linguistically-informed approach that filters out function words and punctuation before applying KL regularization. Compare entropy maintenance and reasoning performance to quantify whether the intervention is targeting reasoning-critical positions.

3. **Covariance stability evaluation**: Implement bootstrap resampling of advantage estimates and compute coefficient of variation for covariance estimates across batches. If covariance estimates show high variance (>30%), implement exponential moving average smoothing and re-evaluate the hierarchical KL approach's effectiveness.