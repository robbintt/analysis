---
ver: rpa2
title: Reinforcement Learning with Stochastic Reward Machines
arxiv_id: '2510.14837'
source_url: https://arxiv.org/abs/2510.14837
tags:
- reward
- learning
- machines
- rewards
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning reward machines in
  reinforcement learning when the rewards are noisy and non-Markovian. It introduces
  Stochastic Reward Machines (SRMs) to handle noisy reward distributions and presents
  an algorithm, SRMI, that learns SRMs from agent experience using constraint solving.
---

# Reinforcement Learning with Stochastic Reward Machines

## Quick Facts
- arXiv ID: 2510.14837
- Source URL: https://arxiv.org/abs/2510.14837
- Reference count: 40
- Primary result: SRMI algorithm learns SRMs from noisy, non-Markovian rewards and converges faster to optimal policies than existing methods in noisy settings.

## Executive Summary
This paper addresses the challenge of learning reward machines when rewards are noisy and non-Markovian. It introduces Stochastic Reward Machines (SRMs) that map label sequences to probability distributions rather than single values, enabling robust learning in stochastic environments. The SRMI algorithm iteratively refines an SRM hypothesis using constraint solving, distinguishing between structural errors (requiring new automaton topology) and parametric errors (requiring output distribution updates). Experiments demonstrate that SRMI outperforms existing methods in noisy environments while maintaining baseline performance in deterministic settings.

## Method Summary
SRMI combines Q-learning for reward machines with a novel inference algorithm that learns SRMs from agent experience. The algorithm maintains a hypothesis SRM and a buffer of observed traces, checking each new trace for εc-consistency with the current hypothesis. When a counterexample is found, it's classified as either Type 1 (can be resolved by shifting output means using a midrange estimator) or Type 2 (requires structural changes solved via constraint programming). The algorithm enforces minimality by starting with the smallest possible SRM size and incrementally increasing it only when necessary, using an SMT solver to find consistent minimal structures.

## Key Results
- SRMI outperforms existing methods in noisy environments, converging faster to optimal policies
- In non-noisy scenarios, SRMI matches baseline performance while maintaining robustness
- The algorithm successfully learns SRMs in both mining and harvesting environments with varying levels of stochasticity
- Constraint-based structural inference enables learning of minimal SRM representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SRMs enable standard RL algorithms to handle noisy, non-Markovian rewards by generalizing output from scalars to probability distributions.
- **Mechanism**: Standard RMs map label sequences to single real numbers, which causes them to overfit or fail when rewards are stochastic. SRMs map label sequences to Cumulative Distribution Functions (CDFs). Q-learning for RMs (QRM) can then sample from these CDFs (or use expected values) to perform updates that are robust to variance, provided the agent distinguishes between structural differences in reward logic vs. random noise.
- **Core assumption**: The noise dispersion is bounded (parameter εc), and distinct reward distributions must have sufficiently separated means (Assumption 1).
- **Evidence anchors**:
  - [abstract] "...introduce a novel type of reward machines, called stochastic reward machines... can capture noisy, non-Markovian reward functions..."
  - [section 3] Definition 2 defines output alphabet O as a set of CDFs.
  - [corpus] *Physics-Informed Reward Machines* discusses generalizing RM structure, but specific stochastic inference mechanisms are unique to this paper.
- **Break condition**: If reward distributions are asymmetric and the agent uses the symmetric "midrange" estimator without correction (see Appendix B), or if εc intervals overlap significantly for distributions with different means (violating Assumption 1).

### Mechanism 2
- **Claim**: The SRMI algorithm isolates structural errors (logic) from parametric errors (noise) using a two-tier counterexample strategy.
- **Mechanism**: When a trace violates the current hypothesis, SRMI classifies it. If the existing machine structure can explain the reward by shifting output means (Type 1), it updates parameters. If no shift suffices (Type 2), it invokes a constraint solver (SMT) to build a new automaton structure that satisfies the constraints of all past counterexamples.
- **Core assumption**: The environment reward can be modeled by a finite-state machine, and the constraint problem is solvable within feasible time limits.
- **Evidence anchors**:
  - [section 4.2] "...categorized every counterexample as either Type 1 or Type 2... Type 2... requires solving the following task [constraint problem]."
  - [algorithm 1] Logic for distinguishing between shifting outputs (Line 9) vs. inferring new structure (Line 12).
  - [corpus] *Inferring Reward Machines...* discusses inference, but SRMI's specific use of SMT for stochastic constraints is distinct.
- **Break condition**: If the constraint solver times out on complex environments, or if the "minimality" assumption leads to frequent restructuring.

### Mechanism 3
- **Claim**: SRMI maintains consistency with noisy history by using a "midrange" estimator for output distributions rather than empirical averages.
- **Mechanism**: To prevent new high-variance samples from constantly invalidating the hypothesis (triggering unnecessary Type 2 restructuring), the algorithm estimates the mean of a distribution using the midrange of observed rewards (max + min)/2. This conservative estimate ensures the resulting distribution covers the support of observed noise, stabilizing the logical structure of the machine.
- **Core assumption**: Rewards follow symmetric, bounded distributions (Algorithm 2); or asymmetric handling is implemented (Algorithm 4).
- **Evidence anchors**:
  - [section 4.2] "Using the midrange estimator for outputs in H ensures it remains consistent with the counterexamples."
  - [algorithm 2] Explicit calculation μ' ← (max r(v, ℓ) + min r(v, ℓ))/2.
  - [corpus] No direct evidence in corpus neighbors regarding midrange estimation for RMs.
- **Break condition**: If rewards are unbounded or the "midrange" is skewed by extreme outliers not representative of the true distribution support.

## Foundational Learning

- **Concept: Reward Machines (RMs)**
  - **Why needed here**: This paper extends standard RMs. Without understanding that RMs are finite-state automata used to decompose sparse, non-Markovian rewards into Markovian sub-tasks, the contribution of adding "stochasticity" is unclear.
  - **Quick check question**: Can you explain how a standard Reward Machine uses a state register to convert a history-dependent reward function into a Markovian one?

- **Concept: Q-Learning for RMs (QRM)**
  - **Why needed here**: The paper assumes QRM as the baseline learning algorithm. Understanding that QRM maintains Q-functions for every state of the reward machine is necessary to see how SRMI integrates machine learning with policy learning.
  - **Quick check question**: In QRM, does the agent update one global Q-table or separate Q-functions based on the state of the reward machine?

- **Concept: SMT (Satisfiability Modulo Theories) / Constraint Solving**
  - **Why needed here**: The core inference engine of SRMI relies on encoding the machine learning problem into a logical formula (real arithmetic) and solving it.
  - **Quick check question**: Why would a constraint-based approach be preferred over a gradient-based approach for learning the *structure* (topology) of a machine?

## Architecture Onboarding

- **Component map**: Agent -> Hypothesis SRM -> Trace Buffer -> Counterexample Set -> Solver
- **Critical path**:
  1. Agent acts using QRM + Hypothesis H
  2. Step 4.2 check: Is the observed trace εc-consistent with H?
  3. If No: Classify counterexample (Type 1 vs Type 2)
  4. If Type 2: Call Solver with X to generate new minimal graph structure
  5. Update H parameters using `Estimates` (midrange) on buffer A

- **Design tradeoffs**:
  - **Minimality vs. Overhead**: The algorithm enforces minimal SRM size (start size 1, increment). This reduces policy complexity but requires multiple solver calls for complex tasks.
  - **Midrange vs. Mean**: Using midrange protects against re-identifying structure constantly, but is statistically less efficient than the mean for symmetric distributions unless sample sizes are high.

- **Failure signatures**:
  - **Runaway state explosion**: If εc is too small, the agent interprets noise as structural complexity, creating massive SRMs.
  - **Solver timeout**: If the counterexample set grows too large, the constraint problem becomes unsolvable in real-time.
  - **Stagnation**: If Assumption 1 is violated (indistinguishable distributions), the agent may flip-flop between hypotheses without convergence.

- **First 3 experiments**:
  1. **Hyperparameter Sweep (εc)**: Run SRMI on the "Mining" environment with noise. Vary εc (e.g., 0.1, 0.5, 1.0) to visualize the trade-off between overfitting (small εc) and underfitting/coarse-graining (large εc).
  2. **Ablation on "Minimality"**: Modify the algorithm to start with a large fixed machine size (e.g., size 10) instead of size 1. Compare convergence speed and constraint solving time against the minimal incremental approach.
  3. **Baseline Comparison**: Implement the "replay baseline" described in Section 4.1 and compare the sample complexity (number of steps) against SRMI in the "Harvest" environment where trajectories are unlikely to repeat.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the noise dispersion bound εc be inferred dynamically during the learning process, rather than being provided as a fixed hyperparameter?
- Basis in paper: [explicit] Section 4.2 states, "We assume that a bound on noise dispersion εc is known," and the Definition 4 of consistency relies explicitly on this parameter.
- Why unresolved: The algorithm relies on a user-provided tolerance (e.g., sensor error) to define the interval [μ-εc, μ+εc]. If this assumption is violated or unknown, the solver may fail to find a consistent machine or overfit the noise.
- What evidence would resolve it: An extension of SRMI where εc is a variable in the optimization problem, updated online based on the empirical variance of observed rewards.

### Open Question 2
- Question: How can the inference algorithm be adapted to relax Assumption 1 (distinguishability) to handle environments where distribution ranges overlap with unequal means?
- Basis in paper: [explicit] Section 4 states, "SRMI can only recover an SRM representation... that meets an additional requirement, which we formalize in Assumption 1," which mandates that distributions covered by the same εc-interval must have equal means.
- Why unresolved: The current constraint formulation cannot distinguish between two different reward distributions if their intervals [ai, bi] and [aj, bj] overlap within the noise bound εc.
- What evidence would resolve it: A modified constraint solving method that utilizes higher-order statistics or likelihood estimates rather than purely interval-based consistency to separate overlapping distributions.

### Open Question 3
- Question: Can the constraint-based approach be extended to handle unbounded reward distributions, such as Gaussian noise?
- Basis in paper: [inferred] Section 3 states, "We focus on continuous but bounded probability distributions," and the constraints in Section 4.2 rely on bounded intervals [a,b] to define the output alphabet and consistency checks.
- Why unresolved: The current formulation requires defining the output alphabet as a set of bounded distributions D([ai, bi]). The "midrange estimator" and consistency checks fail if the support is infinite or the min/max values are undefined.
- What evidence would resolve it: A reformulation of the constraint problem using probabilistic logic or confidence intervals that does not rely on hard bounds for the reward support.

## Limitations
- The algorithm requires a known bound on noise dispersion (εc), which may not be available in all environments
- Assumption 1 (distinguishability of reward distributions) is critical but not empirically validated across varying noise regimes
- Constraint solving introduces scalability concerns, particularly for environments with complex state spaces or long horizons

## Confidence
- **High Confidence**: The core mechanism of SRMs extending RMs to handle noisy rewards through CDF outputs is well-founded and theoretically sound. The general structure of the SRMI algorithm (QRM + counterexample-driven inference) is clearly specified.
- **Medium Confidence**: The effectiveness of the two-tier counterexample strategy and the midrange estimator for stabilizing learning is supported by experimental results but could benefit from deeper theoretical analysis or ablation studies. The constraint formulation appears correct but its scalability is unverified.
- **Low Confidence**: The paper does not provide sufficient detail on hyperparameter selection or provide theoretical guarantees for convergence in the presence of noise, making replication and generalization to new domains uncertain.

## Next Checks
1. **Noise Sensitivity Analysis**: Systematically vary the noise dispersion bound εc and the actual noise level in the Mining environment to determine the threshold at which Assumption 1 breaks down and SRMI performance degrades.
2. **Constraint Solver Scalability**: Measure constraint solving time as a function of the number of counterexamples and SRM size. Identify the point at which Z3 timeouts become prohibitive and explore alternative solvers or approximation methods.
3. **Midrange Estimator Ablation**: Replace the midrange estimator with the empirical mean in SRMI and compare performance on environments with both symmetric and asymmetric reward distributions to quantify the trade-off between stability and statistical efficiency.