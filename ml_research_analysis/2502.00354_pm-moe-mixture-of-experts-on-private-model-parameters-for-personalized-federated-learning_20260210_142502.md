---
ver: rpa2
title: 'PM-MOE: Mixture of Experts on Private Model Parameters for Personalized Federated
  Learning'
arxiv_id: '2502.00354'
source_url: https://arxiv.org/abs/2502.00354
tags:
- personalized
- learning
- federated
- client
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses statistical heterogeneity in federated learning
  by proposing PM-MOE, a framework that leverages Mixture of Experts (MoE) on private
  model parameters to enhance personalization. The core idea is to pretrain model-split-based
  personalized federated learning algorithms, then in a fine-tuning stage, dynamically
  weight and denoise personalized parameters from different clients using an energy-based
  method.
---

# PM-MOE: Mixture of Experts on Private Model Parameters for Personalized Federated Learning

## Quick Facts
- arXiv ID: 2502.00354
- Source URL: https://arxiv.org/abs/2502.00354
- Reference count: 40
- Key outcome: Improves personalized federated learning accuracy by up to 0.9033 on AGNews and 0.2% average across settings by dynamically weighting and denoising private parameters from other clients

## Executive Summary
PM-MOE addresses statistical heterogeneity in federated learning by leveraging Mixture of Experts (MoE) on private model parameters from other clients. The framework pretrains model-split PFL algorithms, then in a fine-tuning stage, dynamically weights and denoises personalized parameters from different clients using an energy-based method. This allows each client to selectively benefit from useful personalized knowledge from others while filtering out irrelevant or harmful parameters. Extensive experiments across nine PFL algorithms and six datasets show consistent performance improvements with minimal additional training time.

## Method Summary
PM-MOE operates in two stages: first, pre-train model-split PFL algorithms (like FedCP or DBE) to convergence, then collect the converged private parameters from all clients into a pool. In the fine-tuning stage, each client trains a lightweight gating network that assigns weights to external experts based on input data, constructing a mixed personalized module via weighted sum of top-k relevant parameters. The method includes energy-based denoising to filter out harmful experts before MoE training, and crucially decouples base model training from router training to prevent catastrophic forgetting. The framework is designed to work with any model-split PFL algorithm and requires minimal additional training time compared to baseline approaches.

## Key Results
- Achieves up to 0.9033 accuracy improvement on AGNews dataset
- Provides average improvement of 0.2% across all tested settings
- Consistently outperforms nine state-of-the-art personalized federated learning algorithms
- Requires minimal additional training time compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Cross-Client Parameter Recycling (Mixture of Personalized Modules)
Aggregating converged private parameters from other clients allows local clients to correct representation biases more effectively than using only their own parameters or global averages. The server collects personalized modules into a pool, and each client's gating network assigns weights to these external modules based on input data, constructing a mixed personalized module via weighted sum of top-k relevant parameters. This mechanism assumes private parameters capture transferable domain-specific nuances and that local input samples contain sufficient signal for the gating network to identify relevant external experts.

### Mechanism 2: Energy-Based Expert Filtering
The energy-based method calculates similarity between local and external feature representations to filter out harmful experts before disrupting gating network training. The method projects local data using local and external experts, calculating confidence scores based on negative Helmholtz free energy derived from vector similarity. Experts with high energy (low similarity) are removed from the pool by a dropout ratio. This assumes similarity in feature representation space is a reliable proxy for parameter transferability.

### Mechanism 3: Asynchronous Decoupling of Base and Router
Freezing base personalized models while training only the MoE router prevents catastrophic forgetting or interference common in synchronous FL updates. The framework separates training into two distinct phases: Phase 1 converges base models, then Phase 2 freezes global and personalized weights, treating them as a static library of experts while only optimizing routing weights. This assumes Phase 1 produces sufficient quality personalized models to act as static experts without further gradient updates.

## Foundational Learning

- **Model Splitting in Personalized FL:** PM-MOE relies on distinguishing between global parameters (shared) and private parameters (personalized). Understanding which layers of FedPer or FedRep models constitute "personalized parameters" is essential for implementing the Mixture of Personalized Modules.
  - Quick check: Can you identify which layers of a FedPer or FedRep model would go into the "Personalized Parameter Pool"?

- **Mixture of Experts (MoE) Gating:** The core engine is the gating network that decides which external client's knowledge to use. Understanding how Top-k gates differ from softmax attention mechanisms is crucial, particularly why Top-k might be preferred for filtering noise.
  - Quick check: How does a Top-k gate differ from a softmax attention mechanism, and why might Top-k be preferred for filtering noise?

- **Statistical Heterogeneity (Non-IID Data):** The entire motivation is that data distributions vary across clients. Without understanding Non-IID, the mechanism of "borrowing" parameters seems unnecessary since you would just use the global model.
  - Quick check: In a Dirichlet distribution setting with S=0 (high heterogeneity), why would sharing personalized parameters be more effective than averaging global gradients?

## Architecture Onboarding

- **Component map:** Server maintains Global Model (W_g), Personalized Parameter Pool (W_PP), and Personalized Expert Pool (W_PE). Client holds local data, frozen copy of local personalized parameters, copy of global model, and trainable Gating Network (θ). Router takes input x, computes energy scores (filtering), and calculates weights α for Top-k experts.

- **Critical path:** 1) Pre-training: Run standard PFL for E_g epochs until convergence. 2) Pooling: Server collects converged private weights from all M clients. 3) Init: Initialize local gating networks with Orthogonal initialization. 4) Fine-tune: For 50 epochs, train only the gating network parameters using higher learning rate (e.g., 0.5).

- **Design tradeoffs:** Top-k Selection - small k (k ≈ M/2 for high heterogeneity) protects against noise but limits knowledge transfer; large k (k ≈ M for lower heterogeneity) maximizes transfer but risks noise. Synchronous vs. Asynchronous - do not train MoE gate simultaneously with base model gradients (significant degradation); must decouple these stages.

- **Failure signatures:** Performance Drop vs. Base Model indicates Top-k value likely too large (introducing noise) or gating learning rate too low. Slow Convergence suggests energy-based denoising dropping local client's own expert (which serves as anchor).

- **First 3 experiments:** 1) Overfit Test: Run PM-MOE on single client - gate should learn 100% weight to its own local personalized parameter and 0% to others (if data identical) or verify it can identify useful peers (if data overlaps). 2) Ablation on Denoising: Compare accuracy with Energy-based Denoising ON vs. OFF - confirm EDM is specifically required when S=0 (high heterogeneity). 3) Hyperparameter Scan: Sweep Gating Network Learning Rate (try 0.05, 0.1, 0.5) - verify higher rates (0.5) work better for gate to escape local minima on your specific dataset.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several implicit research directions regarding privacy guarantees, scalability to large client populations, and robustness to concept drift in dynamic environments.

## Limitations
- Communication efficiency concerns when scaling to large client populations due to broadcasting M sets of parameters to every client
- Potential privacy vulnerabilities from sharing raw model weights, even personalization layers, despite claims of privacy preservation
- Lack of robustness analysis for concept drift scenarios where local data distributions shift after the pre-training phase

## Confidence
- **High Confidence:** The empirical improvement claims across nine PFL algorithms and six datasets are well-supported by experimental results
- **Medium Confidence:** The theoretical convergence analysis provides reasonable bounds but depends on assumptions about data heterogeneity that may not generalize
- **Medium Confidence:** The mechanism explanations are logically consistent but some components (like energy-based filtering) lack direct validation in the literature

## Next Checks
1. **Cross-Domain Transfer Test:** Evaluate PM-MOE when clients have completely disjoint label spaces (e.g., some clients have only digits 0-4 while others have 5-9 in MNIST) to test the filtering mechanism's effectiveness

2. **Ablation of Energy-Based Denoising:** Systematically disable the EDM component and measure performance degradation across different heterogeneity levels to quantify its exact contribution

3. **Gate Stability Analysis:** Track the evolution of gating weights across training epochs to verify that the router consistently identifies relevant experts rather than converging to random or static patterns