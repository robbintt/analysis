---
ver: rpa2
title: Rationality Check! Benchmarking the Rationality of Large Language Models
arxiv_id: '2509.14546'
source_url: https://arxiv.org/abs/2509.14546
tags:
- uni00000013
- uni00000011
- uni00000010
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first comprehensive benchmark for evaluating
  the rationality of large language models (LLMs), covering six domains: psychology,
  cognitive science, decision-making, economics, game theory, and collective rationality.
  The benchmark employs a wide range of questionnaires and tests adapted from human
  rationality studies, assessing both theoretical and practical aspects.'
---

# Rationality Check! Benchmarking the Rationality of Large Language Models

## Quick Facts
- arXiv ID: 2509.14546
- Source URL: https://arxiv.org/abs/2509.14546
- Reference count: 40
- Primary result: First comprehensive benchmark evaluating LLM rationality across six academic domains using validated human instruments, revealing LLMs generally outperform humans in collective rationality and many cognitive tasks while showing mixed results in economics and game theory.

## Executive Summary
This paper introduces a comprehensive benchmark for evaluating the rationality of large language models (LLMs) across six domains: psychology, cognitive science, decision-making, economics, game theory, and collective rationality. The benchmark employs validated questionnaires and tests from human rationality studies, assessing both theoretical and practical aspects of rational behavior. Experiments with 21 leading LLMs reveal that advanced models generally outperform humans in collective rationality and many cognitive tasks, while showing mixed results in economics and game theory domains. The benchmark provides an easy-to-use toolkit and detailed analysis, serving as a foundational tool for developers and users to assess and improve LLM rationality across applications.

## Method Summary
The benchmark adapts validated human rationality instruments from six academic domains, presenting them as prompts to LLMs which respond as participants. Each domain uses established questionnaires: psychology (self-reflection, emotion regulation scales), cognitive science (dual-process reasoning, cognitive bias assessments), decision-making (styles, heuristics), economics (risk preferences, economic biases), game theory (strategic interaction scenarios), and collective rationality (cooperation, wisdom of crowds). LLMs act as participants responding to prompts, with scores normalized (0 to 1) and compared against human baselines. Game theory involves multi-agent interactions where LLMs play against identical instances of themselves. The toolkit supports both API models and local model weights, providing unified evaluation and generating normalized heatmaps.

## Key Results
- LLMs generally exhibit high rationality, outperforming humans in collective rationality (cooperation games) and cognitive tasks like cognitive reflection tests
- Advanced models (GPT-4, GPT-4o, Claude-3.5) show higher rationality than smaller models, with reinforcement learning from human feedback correlating with improved performance
- Game theory is the only domain where humans consistently outperform LLMs, suggesting limitations in strategic reasoning capabilities
- Cross-domain correlation analysis reveals cognitive science and decision-making share 0.90 correlation, indicating shared underlying capabilities

## Why This Works (Mechanism)

### Mechanism 1: Domain-Decomposed Evaluation Framework
Decomposing rationality into six established academic domains enables comprehensive and interpretable benchmarking. The framework maps rationality constructs from psychology (self-reflection, emotion regulation), cognitive science (dual-process reasoning, cognitive biases), decision-making (styles, heuristics), economics (risk preferences, economic biases), game theory (strategic interaction), and collective rationality (cooperation, wisdom of crowds). Each domain employs validated human assessment instruments adapted for LLMs, with cross-domain correlation analysis revealing relationships between constructs.

### Mechanism 2: Normalized Human-LLM Performance Comparison
Normalizing LLM scores against established human baselines provides interpretable context for rationality assessment. The benchmark computes rationality scores from LLM responses to standardized instruments, then normalizes against human performance data from cited literature. Heatmaps visualize where models perform above (orange) or below (blue) human baseline, revealing patterns such as LLMs outperforming humans in collective rationality while underperforming in game theory.

### Mechanism 3: Theoretical-Practical Rationality Bifurcation
Separating rationality assessment into theoretical (belief formation, logical reasoning) and practical (decision-making, action) components reveals divergent capability patterns. The benchmark classifies each assessment as theoretical or practical, showing strong correlation between theoretical and practical scores but specific divergences. GPT-4 shows high theoretical rationality but some models show theoretical-practical gaps, with game theory showing consistent underperformance relative to humans.

## Foundational Learning

- **Dual-process theory (System 1 vs. System 2 reasoning)**: Multiple assessments measure LLM tendency toward intuitive vs. reflective processing; understanding this distinction is essential for interpreting results like LLMs' high cognitive reflection test scores. *Quick check*: When a model solves the bat-and-ball problem correctly at 5 cents, is it using System 1 (pattern matching from training data) or System 2 (explicit reasoning)? How would you distinguish?

- **Nash equilibrium and game-theoretic rationality**: Game theory domain assessments measure how closely LLM strategies approximate equilibrium play; understanding SPNE is necessary to interpret defection rate metrics and efficiency scores. *Quick check*: In a repeated prisoner's dilemma, why is "always defect" the unique SPNE under standard assumptions? What does it mean when LLMs cooperate more than this equilibrium predicts?

- **Cognitive biases as deviations from normative rationality**: Practical rationality is partially operationalized as resistance to known biases (availability heuristic, conjunction fallacy, framing effect); you need to recognize these patterns to understand why lower bias susceptibility is scored as higher rationality. *Quick check*: If an LLM avoids the conjunction fallacy more consistently than humans, what are two possible explanations beyond "the LLM is more rational"?

## Architecture Onboarding

- **Component map**: Input layer (domain/aspect/LLM specifications) -> Query layer (unified interface for API and local models with async handling) -> Evaluation layer (domain-specific scoring and normalized heatmaps)

- **Critical path**: Add model to config with API endpoint or local path -> Specify target domains/aspects -> Run toolkit which queries model on all relevant instruments -> Review normalized scores against human baseline -> Check for data contamination by comparing performance on modified vs. original test items

- **Design tradeoffs**: Breadth vs. depth (six domains provide coverage but each uses limited instrument sets), Human comparison vs. contamination risk (using established instruments enables direct comparison but increases contamination risk), Self-report vs. behavioral (psychology/cognitive domains use self-report; game theory uses behavioral play)

- **Failure signatures**: Uniformly high scores across all domains suggests memorization rather than reasoning, High theoretical but low practical scores may indicate good reasoning but poor action selection, Game theory underperformance with high other-domain scores suggests strategic reasoning deficit, Inconsistent responses to logically equivalent framings indicates sensitivity to surface form

- **First 3 experiments**:
  1. Cross-model comparison within cognitive science domain: Run REI, CRT, inductive/deductive/causal reasoning tasks, cognitive bias assessments on 3-4 models of different sizes; compare theoretical vs. practical scores
  2. Data contamination validation: Take CRT, Base-Rate Neglect, Conjunction Fallacy; create modified versions with changed names/numbers; run both versions on two models from different developers
  3. Game theory behavioral analysis: Run prisoner's dilemma (one-shot and 10-round repeated) with same-LLM agent pairs for 20 iterations; compute defection rate and efficiency scores; compare to human baseline

## Open Questions the Paper Calls Out

- **Dynamic benchmark synthesis**: How can evaluation benchmarks be dynamically synthesized or modified to effectively mitigate data contamination where LLMs might simply recall answers from training data? The paper notes that while modified questions were used for validation, a systemic method for generating fresh, uncontaminated benchmarks is not established.

- **Psychometric technique requirements**: What specific psychometric techniques or normalization methods are required to robustly evaluate LLM rationality given the discrepancies in response characteristics (fatigue in humans vs. refusals in LLMs)? The current benchmark applies human-designed scales directly without adjusting for these fundamental operational differences.

- **Theory of Mind integration**: How can Theory of Mind processes be effectively simulated or integrated into LLMs to improve their predictive capability and moral reasoning? While LLMs can emulate ToM, the mechanisms to move from emulation to genuine simulation of social cognition remain undefined.

## Limitations

- The benchmark's reliance on human-validated instruments adapted for LLM assessment introduces data contamination risk and potential validity concerns about whether LLMs interpret questionnaire items as intended by their human designers
- The six-domain decomposition, while comprehensive, may miss important aspects of rationality not captured by existing psychological or economic paradigms
- The theoretical-practical bifurcation, though grounded in philosophical literature, lacks empirical validation for AI systems specifically

## Confidence

- **High confidence**: Domain-decomposed evaluation framework and normalized human-LLM performance comparison mechanisms are well-supported by explicit methodology and reproducible results
- **Medium confidence**: Theoretical-practical rationality bifurcation shows promising correlations but requires further validation with larger model cohorts and additional task pairs
- **Low confidence**: Claims about domain-specific relationships and game theory underperformance interpretation require additional validation given small human reference sample size and potential strategic reasoning differences

## Next Checks

1. **Cross-cultural validation**: Repeat the benchmark with Chinese-language versions of the instruments on the same models to verify whether observed rationality patterns are language- and culture-invariant or reflect Western-centric test design

2. **Temporal stability analysis**: Run the same models on identical instruments at monthly intervals to assess whether reported rationality scores are stable or reflect transient training effects and model version changes

3. **Human-LLM equivalence testing**: Conduct controlled experiments where human participants complete the exact same prompts given to LLMs, controlling for response time and context, to validate the assumption that direct score comparison is meaningful