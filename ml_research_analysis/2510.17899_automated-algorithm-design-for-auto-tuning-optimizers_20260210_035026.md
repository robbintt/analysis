---
ver: rpa2
title: Automated Algorithm Design for Auto-Tuning Optimizers
arxiv_id: '2510.17899'
source_url: https://arxiv.org/abs/2510.17899
tags:
- algorithms
- optimization
- performance
- search
- auto-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to auto-tuning using large
  language models (LLMs) to automatically generate optimization algorithms. The authors
  introduce LLaMEA, a framework that combines LLMs with evolutionary algorithms to
  synthesize optimization strategies tailored for auto-tuning problems.
---

# Automated Algorithm Design for Auto-Tuning Optimizers

## Quick Facts
- **arXiv ID**: 2510.17899
- **Source URL**: https://arxiv.org/abs/2510.17899
- **Reference count**: 40
- **One-line primary result**: LLM-generated optimizers can rival and outperform existing human-designed algorithms for GPU kernel auto-tuning

## Executive Summary
This paper presents LLaMEA, a framework that uses large language models combined with evolutionary algorithms to automatically generate optimization algorithms for GPU kernel auto-tuning. The approach generates candidate algorithms as Python code, evaluates them using the Kernel Tuner framework, and iteratively refines them based on performance. The study demonstrates that LLM-generated optimizers can achieve an average improvement of 72.4% over state-of-the-art optimizers, with performance gains of 30.7% from application-specific and 14.6% from search space-specific information.

## Method Summary
The method employs an evolutionary loop where an LLM (GPT-4o-mini) generates optimization algorithm code strings based on high-performing parents and specific mutation prompts. These candidate algorithms are evaluated within the Kernel Tuner framework using a community-driven methodology for assessing optimization algorithms. The performance score $P$ serves as the fitness function, with poorly performing algorithms discarded. The framework supports mutation and selection strategies with different population sizes, and the evolutionary process guides the search toward high-performance optimization strategies tailored to auto-tuning problems.

## Key Results
- Generated algorithms achieved an average improvement of 72.4% over state-of-the-art optimizers
- Providing application-specific information improved performance by 30.7%
- Providing search space-specific information improved performance by 14.6%
- Generated algorithms (HybridVNDX, AdaptiveTabuGreyWolf) successfully combined known heuristics into novel configurations

## Why This Works (Mechanism)

### Mechanism 1: Evolutionary Refinement of LLM-Generated Heuristics
An evolutionary loop using an LLM as a mutation operator can synthesize high-performance optimization algorithms that outperform generic human-designed baselines. The system maintains a population of optimization algorithms (code strings), generates new candidates based on high-performing parents and mutation prompts, and evaluates them using Kernel Tuner performance scores as fitness. The LLM possesses sufficient coding knowledge to map strategy descriptions into syntactically correct, executable Python code.

### Mechanism 2: Context-Aware Prompting for Search Space Adaptation
Injecting specific domain knowledge (search space constraints, parameter types) into the LLM prompt significantly improves optimizer quality (14.6% improvement). The prompt template allows for optional search space specification, enabling the LLM to tailor algorithmic logic to the specific topology of the search space by choosing appropriate discrete steps or constraint-handling mechanisms.

### Mechanism 3: Performance-Relative Fitness Scoring ($P$-score)
Optimizers can be reliably compared using a time-normalized performance score that balances convergence speed against solution quality. Instead of rewarding only the final best result, the methodology computes a score based on the area under the convergence curve relative to a random search baseline, forcing generated algorithms to be efficient rather than just exhaustive.

## Foundational Learning

- **Concept: Auto-tuning (GPU Kernel Tuning)** - The target domain where configurations are compilation parameters for GPU kernels, not just vectors of floats. Quick check: Does the LLM generate GPU kernel code or the strategy to search the kernel's parameter space? (Answer: The search strategy)

- **Concept: Metaheuristics (Hybridization)** - The top-performing generated algorithms are hybrids of known methods (Tabu Search, Simulated Annealing, Grey Wolf Optimizer). Quick check: What role does the "Tabu list" play in HybridVNDX? (Answer: It prevents revisiting recently evaluated configurations)

- **Concept: LLaMEA Framework** - The specific instantiation of "LLM as a Programmer" used, treating code as the genotype. Quick check: Does the LLM evaluate the fitness of generated algorithms? (Answer: No, Kernel Tuner provides fitness scores)

## Architecture Onboarding

- **Component map**: LLaMEA (Controller) -> GPT-4o-mini API -> Kernel Tuner (Evaluator) -> BAT Benchmark Suite
- **Critical path**: 1) LLaMEA prompts LLM with OptAlg interface docs + optional search space info; 2) LLM returns Python code string; 3) Kernel Tuner executes code against kernel; 4) Calculate $P$-score; 5) LLaMEA selects best algorithms, feeds back with "Refine" prompts
- **Design tradeoffs**: Evaluation cost is high as it requires running optimizers many times on many kernels; generalization vs. specialization balance as training on GEMM might overfit to matrix multiplication patterns
- **Failure signatures**: Syntax/Runtime errors (LLM generates invalid code), infinite loops (algorithms don't respect budget), identity mappers (algorithms return immediately)
- **First 3 experiments**: 1) Run LLaMEA with minimal generations on Convolution kernel to verify basic functionality; 2) Ablate on context by running GEMM generation with/without search space specification to compare $P$-scores; 3) Test HybridVNDX algorithm from Dedispersion on Hotspot kernel to verify generalization

## Open Questions the Paper Calls Out

- **Question 1**: Can the generated optimization algorithms generalize effectively to auto-tuning domains outside of GPU kernel tuning? The authors note broader validation across additional architectures and domains is needed, as evaluation is restricted to four specific HPC kernels.

- **Question 2**: Can the algorithm generation process be accelerated to support online, problem-specific synthesis? The current generation-evaluation loop takes too long for direct inclusion in the auto-tuning process, requiring extensive computation.

- **Question 3**: To what extent does the choice of underlying Large Language Model impact the validity and performance of generated algorithms? The authors acknowledge generated algorithms depend on the choice of LLM, noting gemini-2.0-flash produced inferior results compared to o4-mini.

## Limitations
- Performance claims rely heavily on Kernel Tuner evaluation methodology, which may not generalize to other auto-tuning contexts
- Generated algorithms appear to be combinations of existing heuristics rather than truly innovative approaches
- Evolutionary approach requires extensive evaluation of generated algorithms, making it compute-expensive

## Confidence

- **High**: LLM can successfully generate syntactically valid Python code that interfaces with Kernel Tuner APIs
- **Medium**: Performance improvements from application- and search space-specific information are robust across benchmark suite
- **Low**: Claim that generated algorithms "rival and in many cases outperform" existing algorithms may be influenced by specific evaluation methodology

## Next Checks

1. Extract and test exact prompt templates from repository to verify "code format specification" and "minimum working code example" are sufficient for consistent LLM output

2. Evaluate best-performing generated algorithms (HybridVNDX, AdaptiveTabuGreyWolf) on hardware platforms not included in original benchmark suite to test true generalization

3. Re-run evaluation with different random seeds for random search baseline to confirm normalized performance score $P$ is stable and not sensitive to baseline estimates