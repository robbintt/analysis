---
ver: rpa2
title: Selective Sinkhorn Routing for Improved Sparse Mixture of Experts
arxiv_id: '2511.08972'
source_url: https://arxiv.org/abs/2511.08972
tags:
- noise
- smoe
- routing
- ssr-l
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Selective Sinkhorn Routing (SSR), a lightweight
  routing mechanism for Sparse Mixture-of-Experts models that replaces auxiliary balancing
  losses with minimal entropy-regularized optimal transport. The key idea is to reformulate
  token-to-expert assignment as a maximum-cost OT problem with balancing constraints,
  and derive routing weights directly from the transport plan.
---

# Selective Sinkhorn Routing for Improved Sparse Mixture of Experts

## Quick Facts
- arXiv ID: 2511.08972
- Source URL: https://arxiv.org/abs/2511.08972
- Reference count: 40
- Key outcome: SSR achieves 0.551 PPL improvement over vanilla SMoE on WikiText-103 while reducing training time overhead

## Executive Summary
This work introduces Selective Sinkhorn Routing (SSR), a lightweight routing mechanism for Sparse Mixture-of-Experts models that replaces auxiliary balancing losses with minimal entropy-regularized optimal transport. The key idea is to reformulate token-to-expert assignment as a maximum-cost OT problem with balancing constraints, and derive routing weights directly from the transport plan. By applying Sinkhorn-based routing to just 0.1%-1% of training steps and adding Gaussian noise to the cost matrix, SSR achieves faster training, higher accuracy, and greater robustness across both language modeling and image classification tasks compared to existing baselines.

## Method Summary
SSR reformulates MoE routing as an entropy-regularized optimal transport problem with balancing constraints. With probability p, it applies Sinkhorn routing (Algorithm 2) to compute transport plans; otherwise, it uses standard Softmax routing to maintain gradient flow. Gaussian noise is added to the cost matrix during training to ensure exploration. At inference, SSR uses only deterministic Softmax routing without Sinkhorn or noise. The method supports both linear (SSR-L) and row-wise softmax-normalized (SSR-S) cost matrices, with SSR-S being more numerically stable at small regularization values.

## Key Results
- SSR-L improves WikiText-103 test PPL by 0.551 points over vanilla SMoE (34.607 → 34.056)
- SSR-S improves ImageNet-1K Top-1 accuracy by 0.5 points over vanilla SMoE (66.6 → 67.1)
- SSR achieves 4.7% training speedup compared to auxiliary loss methods
- SSR shows better robustness to input noise attacks than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Deriving routing weights directly from the optimal transport plan achieves better load balancing while preserving token information. The entropy-regularized OT problem maximizes compatibility scores subject to constraints that each token fully routes its mass to experts and each expert receives equal expected load. For top-k selection, the transport plan values are renormalized, minimizing KL divergence between sparse weights and full transport distribution.

### Mechanism 2
Applying Sinkhorn routing to only 0.1%-1% of training steps is sufficient to achieve balanced expert utilization while avoiding computational overhead. The hybrid strategy uses Softmax routing for gradient propagation during most steps, with periodic Sinkhorn intervention providing balancing regularization.

### Mechanism 3
Gaussian noise injection into the cost matrix during training acts as implicit data augmentation, ensuring every expert has non-zero selection probability. This prevents routing collapse by making even poorly-matched experts selectable, with noise removed at inference for determinism.

## Foundational Learning

- **Sinkhorn-Knopp Algorithm**: Core numerical method for solving entropy-regularized OT; iteratively scales rows/columns of kernel matrix to match marginals. Quick check: Given cost matrix C and regularization ξ, what is the update rule for scaling vectors u and v?

- **Sparse Mixture of Experts (SMoE) Routing**: Standard top-k Softmax routing and its failure mode (routing collapse/expert underutilization) are the baselines SSR improves upon. Quick check: Why does vanilla Softmax routing with top-k selection lead to expert collapse?

- **Optimal Transport with Entropy Regularization**: Formulating routing as OT with balancing constraints provides the theoretical foundation; the entropy term ensures differentiability and efficient computation. Quick check: What do the three constraints (C1: Π > 0, C2: Π1_n = 1_m, C3: Π^T 1_m = m/n 1_n) each enforce?

## Architecture Onboarding

- **Component map**: X → S (gating scores) → C (cost matrix) → Sinkhorn → Π̂ (transport plan) → top-k → α (routing weights) → expert FFNs

- **Critical path**:
  1. Training: X → S → (with prob p) C̃ = C + noise → Sinkhorn(C̃) → Π̂ → top-k → α → expert outputs
  2. Training: X → S → (with prob 1-p) standard Softmax → α → expert outputs (maintains W_g gradients)
  3. Inference: X → S → standard Softmax only (no Sinkhorn, no noise)

- **Design tradeoffs**:
  - Linear vs Softmax cost: Softmax cost (SSR-S) bounds values for numerical stability; Linear cost (SSR-L) may overflow at small ξ but can work at larger ξ
  - Sinkhorn frequency p: Lower p reduces overhead but risks insufficient balancing; p=0.001 optimal
  - Noise scale α_noise: Higher values increase exploration but may harm convergence; optimal around 0.3-1.0
  - Regularization ξ: Smaller ξ = sharper transport (more deterministic); larger ξ = more entropic (smoother)

- **Failure signatures**:
  - NaN outputs: ξ too small with Linear cost; use Softmax cost or increase ξ ≥ 0.5
  - Training instability: z-loss baselines can spike (Table 1: 2-Head z-loss shows +4.009 PPL degradation)
  - Inference degradation: Using Sinkhorn or noise at inference increases perplexity (Table 2)
  - Expert underutilization: p too low (e.g., 0.00001) fails to prevent collapse

- **First 3 experiments**:
  1. Ablation on Sinkhorn frequency: Test p ∈ {0.0001, 0.001, 0.01, 0.1, 0.5} on WikiText-103 with fixed ξ=0.5, α_noise=1
  2. Noise scale sensitivity: Fix p=0.001, vary α_noise ∈ {0.1, 0.3, 1.0, 2.0, 4.0}; measure clean and attacked PPL
  3. Inference behavior validation: Compare four inference modes on held-out set; confirm "no balancing" is optimal

## Open Questions the Paper Calls Out

### Open Question 1
Does SSR maintain stability and performance at billion-parameter scales, or does removing auxiliary losses cause instability? The experiments use medium-scale models (216M-550M parameters), leaving billion-parameter behavior unverified.

### Open Question 2
Can the Sinkhorn application probability (p) or noise magnitude (α_noise) be formulated as learnable parameters or scheduled functions rather than fixed hyperparameters? The paper establishes a "sweet spot" but doesn't explore if optimal values change during training.

### Open Question 3
Is SSR effectiveness transferable to "Expert Choice" routing paradigms, or is it strictly limited to "Token Choice" architectures? The method is formulated for Token Choice setting, while Expert Choice routing inherently solves load balancing differently.

## Limitations
- Selective application frequency (0.1%-1%) lacks theoretical grounding for optimal range across different model sizes and tasks
- Noise injection introduces stochasticity that could interfere with other regularization techniques
- Inference-time restriction creates gap between training and inference behavior, limiting maximum achievable performance

## Confidence

**High Confidence**: Training-time improvements (PPL reductions of 0.353-0.551 on WikiText-103, 0.023-0.039 on Enwik-8) and computational overhead reduction are well-supported by ablation studies across multiple baselines.

**Medium Confidence**: Robustness claims to input noise attacks are supported by ImageNet experiments, but attack methodology details are not fully specified.

**Low Confidence**: Claims about optimal selective routing frequency (0.1%-1%) and noise scale (α_noise ~ 0.3-1.0) are based on limited ablation studies with unclear sensitivity to hyperparameters across different scales.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary p ∈ {0.0001, 0.001, 0.01, 0.1} and α_noise ∈ {0.1, 0.3, 1.0, 2.0} across different model sizes (2H, 4H, 6H, 8H experts) on WikiText-103 to establish guidelines for new settings.

2. **Training-Inference Consistency Evaluation**: Compare full SSR training with inference using: (a) standard Softmax only, (b) Sinkhorn routing at inference, (c) noise injection at inference. Quantify the PPL gap and analyze theoretical justification.

3. **Computational Overhead Breakdown**: Measure wall-clock time distribution across: (a) standard Softmax routing, (b) Sinkhorn routing (including iterations), (c) noise injection. Verify "lightweight" nature holds across different hardware configurations and batch sizes.