---
ver: rpa2
title: 'Position: Many generalization measures for deep learning are fragile'
arxiv_id: '2510.18934'
source_url: https://arxiv.org/abs/2510.18934
tags:
- learning
- arxiv
- measures
- generalization
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This position paper demonstrates that many post-mortem generalization\
  \ measures in deep learning are fragile: small training modifications\u2014such\
  \ as learning rate changes, optimizer swaps, or hyperparameter tweaks\u2014can cause\
  \ substantial shifts in the measure\u2019s value or trend, even when the underlying\
  \ network performance remains stable. Through systematic experiments on ResNet-50\
  \ and other architectures across FashionMNIST, CIFAR-10, and MNIST datasets, the\
  \ authors show that norms (path, spectral, Frobenius), margin-based metrics, and\
  \ PAC-Bayes surrogates often fail to track changes in data complexity or dataset\
  \ difficulty."
---

# Position: Many generalization measures for deep learning are fragile

## Quick Facts
- arXiv ID: 2510.18934
- Source URL: https://arxiv.org/abs/2510.18934
- Reference count: 40
- Many post-mortem generalization measures (path norm, spectral/Frobenius norms, margin-based metrics, PAC-Bayes surrogates) are fragile under small training modifications.

## Executive Summary
This position paper demonstrates that numerous post-mortem generalization measures in deep learning are fragile: small training modifications—such as learning rate changes, optimizer swaps, or hyperparameter tweaks—can cause substantial shifts in the measure's value or trend, even when the underlying network performance remains stable. Through systematic experiments on ResNet-50 and other architectures across FashionMNIST, CIFAR-10, and MNIST datasets, the authors show that norms (path, spectral, Frobenius), margin-based metrics, and PAC-Bayes surrogates often fail to track changes in data complexity or dataset difficulty. They contrast these fragile measures with the marginal-likelihood PAC-Bayes bound, which captures data complexity and learning-curve scaling robustly due to its function-space basis. A novel equivalence for scale-invariant networks reveals how parameter magnitudes can be inflated without changing the learned function, further exposing the fragility of magnitude-sensitive measures. The paper also introduces quantitative fragility scores (CMS, eCMS) to benchmark measure stability across training regimes. Overall, the work calls for rigorous fragility audits and encourages the development of more robust, data-aware generalization diagnostics.

## Method Summary
The authors systematically evaluate generalization measures by training ResNet-50 and other architectures on FashionMNIST, CIFAR-10, and MNIST datasets with varying hyperparameters (learning rates, optimizers, weight decay). They measure fragility by tracking how small training modifications affect measure values while test error remains stable. The experiments include training with SGD+momentum and Adam across 7 learning rates, different dataset sizes (n=100 to 50,000), and analyzing learning curves post-interpolation. Fragility is quantified using CMS and eCMS scores, which measure sensitivity to hyperparameter perturbations. The marginal-likelihood PAC-Bayes bound is evaluated using GP kernels to assess function-space complexity. The study also introduces scale-invariant networks through an Exp++ schedule to demonstrate parameter-space fragility.

## Key Results
- Path norm, spectral/Frobenius norms, margin-based metrics, and PAC-Bayes surrogates are fragile: small training modifications cause large measure fluctuations while test error remains stable.
- Marginal-likelihood PAC-Bayes bound captures data complexity and learning-curve scaling robustly due to its function-space basis.
- Scale-invariant networks demonstrate that parameter magnitudes can be inflated without changing the learned function, exposing fragility of magnitude-sensitive measures.
- Quantitative fragility scores (CMS, eCMS) reveal measure instability across training regimes and hyperparameter choices.

## Why This Works (Mechanism)
The paper's core mechanism is demonstrating that many generalization measures are parameter-space sensitive rather than function-space robust. When training modifications occur (optimizer changes, learning rate adjustments), the parameter values shift substantially while the actual learned function changes minimally. This creates a disconnect where measures that depend on parameter magnitudes (path norm, spectral norms) fluctuate wildly despite stable generalization performance. The marginal-likelihood PAC-Bes bound avoids this by operating in function space via GP kernels, making it invariant to parameter-space reparameterizations that don't affect the learned mapping.

## Foundational Learning
- **Path norm and spectral/Frobenius norms**: Parameter-space complexity measures that sum parameter magnitudes; fragile because optimizer choices change parameter scales without affecting function.
  - Why needed: These are common generalization measures that the paper demonstrates are unreliable.
  - Quick check: Verify that different optimizers (SGD vs Adam) produce vastly different path norms for same test accuracy.

- **Margin-based metrics**: Measure the separation between classes in feature space; can be sensitive to training dynamics.
  - Why needed: Another common generalization proxy that exhibits fragility.
  - Quick check: Compare margin distributions across different learning rates with similar test performance.

- **PAC-Bayes bounds**: Generalization bounds using Bayesian posteriors; standard forms are parameter-space sensitive.
  - Why needed: Shows even theoretically motivated bounds can be fragile.
  - Quick check: Compute PAC-Bayes bounds for same model with different optimizers.

- **Marginal-likelihood PAC-Bayes**: Function-space complexity measure using GP kernels; robust to parameter-space changes.
  - Why needed: Serves as the robust baseline that other measures should emulate.
  - Quick check: Verify ML-PACBayes correlates with dataset size while parameter-space measures do not.

- **Scale-invariant networks**: Architectures where parameter magnitudes can be scaled without changing the function (e.g., with BatchNorm).
  - Why needed: Demonstrates fundamental limitation of magnitude-based measures.
  - Quick check: Apply Exp++ schedule and verify path norm changes while test accuracy remains constant.

## Architecture Onboarding

**Component Map**
Data → Model (ResNet-50/DenseNet-121/NIN/FCN) → Optimizer (SGD/Adam) → Measure Computation (Path Norm/Spectral/Frobenius/ML-PACBayes) → Fragility Analysis

**Critical Path**
1. Train model to 100% training accuracy or cross-entropy < 0.01
2. Compute generalization measure (path norm, spectral norm, ML-PACBayes)
3. Repeat with small training modifications (learning rate, optimizer)
4. Analyze measure stability vs test error stability
5. Calculate fragility scores (CMS, eCMS)

**Design Tradeoffs**
- Parameter-space measures (path norm) are computationally cheap but fragile
- Function-space measures (ML-PACBayes) are robust but computationally intensive (GP kernel evaluations)
- Simple margin-based metrics are interpretable but sensitive to training dynamics
- The paper trades computational efficiency for robustness by advocating function-space approaches

**Failure Signatures**
- Measures that fluctuate by orders of magnitude under small training changes
- Positive learning curve slopes post-interpolation (measure increases while error plateaus)
- Poor correlation between measure values and dataset size or complexity
- CMS/eCMS scores above critical thresholds (0.01-0.05)

**First Experiments to Run**
1. Train ResNet-50 on FashionMNIST with SGD+momentum (LR=0.01) and vary training set sizes; plot path norm vs test error.
2. Repeat with Adam at LR=0.01 and LR=0.001; compare learning curve shapes (monotonic vs U-shaped).
3. Implement Exp++ schedule with weight decay λ=1.0; verify path norm changes while test accuracy remains constant.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can reparameterization-invariant, data-aware post-training diagnostics be designed that capture the robustness of the marginal-likelihood PAC-Bayes bound while remaining practical at scale?
- Basis in paper: The authors explicitly call for the development of "reparameterization-invariant, data-aware post-training diagnostics that borrow the Occam flavour of marginal likelihood while remaining practical at scale."
- Why unresolved: Current post-mortem measures are fragile, while the robust function-space bound is computationally intensive and not a post-training diagnostic.
- What evidence would resolve it: A new measure that passes the paper's fragility stress tests (e.g., CMS scores, data-complexity scaling) and correlates with generalization without relying on Gaussian Process approximations.

### Open Question 2
- Question: Do the observed fragility patterns and invariance principles extend to architectures outside of vision classification, such as sequence models or reinforcement learning?
- Basis in paper: The conclusion suggests researchers "extend evaluation beyond vision classification to sequence models, generative modelling, reinforcement learning, and explicit distribution shift."
- Why unresolved: The empirical evidence provided is limited to ResNet and Fully Connected Networks on image datasets (FashionMNIST, CIFAR-10, MNIST).
- What evidence would resolve it: Replicating the fragility audits (optimizer swaps, Exp++ schedules) on Transformers or RL agents to observe if norm-based measures remain unstable.

### Open Question 3
- Question: Can approximate function-space surrogates be developed to incorporate finite-width feature learning without reintroducing fragility?
- Basis in paper: The paper lists as a future direction the need to "develop approximate function-space surrogates (finite-width corrections...)" to make invariance-friendly predictors usable in routine training.
- Why unresolved: The robust ML-PACBayes bound relies on infinite-width GP limits and ignores the feature learning that occurs in finite-width networks.
- What evidence would resolve it: A bound that accurately reflects the performance difference between finite-width networks and their infinite-width GP limits while maintaining insensitivity to optimizer choice.

## Limitations
- Missing implementation details: Exact weight decay values λ for each experiment are unspecified, critical for reproducing Exp++ equivalence experiments.
- Dataset scope: Limited to vision classification tasks (FashionMNIST, CIFAR-10, MNIST); generalization to other domains untested.
- Computational constraints: ML-PACBayes bound requires GP kernel evaluations, making it impractical for large-scale models.

## Confidence
- **High Confidence**: Claims about SGD vs. Adam path norm behavior (monotonic vs. U-shaped) and the general fragility pattern of parameter-space measures across optimizer changes. These patterns are robust to moderate hyperparameter variations and are clearly observable in the publicly available code.
- **Medium Confidence**: Claims regarding dataset-size scaling effects on measures, as these require precise weight decay settings to reproduce the function-space invariance demonstrations.
- **Medium Confidence**: Fragility scores (CMS/eCMS) are sensitive to exact hyperparameter grids and stopping criteria; minor variations in implementation details can affect numerical values even when qualitative conclusions hold.

## Next Checks
1. Replicate the core path norm fragility experiment (Figure 2) using the GitHub repository with Adam LR=0.01 and LR=0.001 on FashionMNIST; verify U-shaped vs. monotonic learning curves while test error remains stable.
2. Recompute CMS fragility scores for the 7 learning rates × 2 optimizers × 2 stopping criteria configuration, ensuring pair construction uses absolute error tolerance δ=0.01.
3. Implement the Exp++ scale-invariant network construction with weight decay λ=1.0; verify that path norm changes while the learned function remains constant, confirming the function-space vs. parameter-space distinction.