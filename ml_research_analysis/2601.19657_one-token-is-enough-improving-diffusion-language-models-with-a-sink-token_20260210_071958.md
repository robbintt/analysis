---
ver: rpa2
title: 'One Token Is Enough: Improving Diffusion Language Models with a Sink Token'
arxiv_id: '2601.19657'
source_url: https://arxiv.org/abs/2601.19657
tags:
- sink
- token
- attention
- tokens
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion Language Models (DLMs) suffer from unstable attention
  sinks, where the sink token shifts unpredictably across diffusion steps due to bidirectional
  attention and dynamic masking. This phenomenon introduces inference instability.
---

# One Token Is Enough: Improving Diffusion Language Models with a Sink Token

## Quick Facts
- arXiv ID: 2601.19657
- Source URL: https://arxiv.org/abs/2601.19657
- Authors: Zihou Zhang; Zheyong Xie; Li Zhong; Haifeng Liu; Shaosheng Cao
- Reference count: 20
- One-line primary result: A single sink token improves DLM performance by 2.28-11.70 points on standard benchmarks.

## Executive Summary
Diffusion Language Models suffer from unstable attention sinks where the sink token shifts unpredictably across diffusion steps due to bidirectional attention and dynamic masking. This moving sink phenomenon introduces inference instability. We analyze this through the Transformer value space and find that sink tokens consistently exhibit low L2 norms, functioning as a protective mechanism to reduce information mixing. To stabilize this behavior, we propose a simple extra sink token added at the start of the sequence, constrained to attend only to itself while remaining globally visible. Experiments show consistent performance improvements across multiple scales and training paradigms.

## Method Summary
The method involves prepending a special sink token embedding to the input sequence, then modifying the attention mask to constrain this token to attend only to itself while remaining globally visible to all other tokens. This asymmetric visibility ensures the sink never aggregates semantic content but remains available for attention offloading. The approach works with both fine-tuned autoregressive models and from-scratch trained DLMs, showing consistent gains across different scales (0.5B and 1.5B parameters).

## Key Results
- 0.5B-scale DLMs improve by 10.77–11.70 points on ARC-e, ARC-c, HellaSwag, PIQA, RACE, and GSM8K
- 1.5B-scale models improve by 2.28–3.00 points on the same benchmarks
- From-scratch trained DLMs improve by 8.25–7.95 points
- Position of sink token (front vs. end) yields comparable results
- Zero-value sink tokens maintain similar gains, validating structural role

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sink tokens absorb redundant attention mass by maintaining low L2 norms in value space, minimizing interference with semantic information in the residual stream.
- Mechanism: The softmax operation requires attention weights to sum to one. When no strong semantic match exists, the model offloads excess attention to low-norm tokens. Attending to these tokens produces small attention outputs (by triangle inequality on convex combinations), approximating a "no-op" update where h^(ℓ+1) ≈ h^(ℓ).
- Core assumption: Low-norm value vectors contribute minimally to representation updates; this functions as implicit regularization against over-mixing.
- Evidence anchors:
  - [abstract] "sink tokens exhibit the lowest L2 norms in value space, confirming its role in absorbing redundant attention without semantic interference"
  - [section 3.2] "The root cause lies in the softmax operation which necessitates that attention weights sum to one... When a token lacks a strong semantic match within the context, the model is forced to assign redundant attention mass to globally visible tokens"
  - [corpus] Related paper "Attention Sinks in Diffusion Language Models" (arXiv:2510.15731) documents the same phenomenon; other corpus papers do not directly address this mechanism.
- Break condition: If sink token norms grow comparable to content tokens during training, the absorption mechanism degrades.

### Mechanism 2
- Claim: DLMs suffer from unstable "moving sinks" because bidirectional attention lacks a fixed anchor point, unlike autoregressive models where causal masking locks attention to the initial token.
- Mechanism: In DLMs, masked tokens (typically low-information) serve as implicit sinks. Since the set of masked positions changes stochastically across diffusion steps, the sink position shifts unpredictably. This introduces variance in attention patterns across timesteps, potentially degrading inference consistency.
- Core assumption: Positional stability of attention sinks correlates with inference robustness.
- Evidence anchors:
  - [abstract] "the moving sink phenomenon... their unpredictable positions across diffusion steps undermine inference robustness"
  - [section 1] "In the absence of a consistent start token and causal mask, the sink position in DLMs shifts erratically across diffusion timesteps and layers"
  - [corpus] Weak direct evidence; "Attention Sinks in Diffusion Language Models" mentions dynamic sinks but does not empirically link to robustness degradation.
- Break condition: If a DLM already develops a stable implicit sink (e.g., always attends to a specific token regardless of masking), the moving sink problem may not manifest.

### Mechanism 3
- Claim: A dedicated extra sink token with restricted self-attention provides a position-stable, semantically neutral anchor that outperforms learned gating mechanisms.
- Mechanism: Prepend or append a special token constrained by attention mask bias M_ij = -∞ when sink attends to non-sink positions. All other tokens can attend to it. This asymmetric visibility ensures the sink never aggregates semantic content but remains available for attention offloading. Effectiveness is position-invariant and enhanced when initialized as a zero vector.
- Core assumption: The sink's function is purely structural (attention offload target), not semantic.
- Evidence anchors:
  - [abstract] "a special token constrained to attend solely to itself, while remaining globally visible to all other tokens"
  - [section 4.3.1] Table 3 shows front vs. end placement yields comparable results (e.g., ARC-e: 55.47 vs. 56.14)
  - [section 4.4] Table 5 shows zero-value sink token maintains gains (e.g., PIQA: 63.44 → 64.09)
  - [corpus] No corpus papers propose this specific architectural intervention.
- Break condition: If the attention mask is incorrectly implemented (e.g., sink can attend to content), it may aggregate semantic information and lose neutrality.

## Foundational Learning

- Concept: **Diffusion Language Model (DLM) denoising process**
  - Why needed here: The moving sink phenomenon is specific to the iterative, bidirectional denoising of DLMs—understanding the forward/reverse process clarifies why sink positions shift.
  - Quick check question: Can you explain why masked token sets change across diffusion timesteps and how this differs from autoregressive left-to-right generation?

- Concept: **Attention sink phenomenon in Transformers**
  - Why needed here: The paper builds on prior work showing that LLMs concentrate attention on low-information tokens; this is the theoretical basis for the proposed intervention.
  - Quick check question: Why does the softmax normalization of attention weights force the model to allocate attention mass even when no strong semantic match exists?

- Concept: **Value-space L2 norm and its role in attention mixing**
  - Why needed here: The paper's core insight is that low-norm value vectors minimize information injection when attended to—this explains why sinks work as "no-ops."
  - Quick check question: Given the attention output o_i = Σ α_ij v_j, why does attending to low-norm v_j vectors produce smaller updates to hidden states?

## Architecture Onboarding

- Component map:
  Input augmentation: Prepend sink token embedding s ∈ R^d to sequence X → [s; X]
  Attention mask bias: M_ij = -∞ if i = sink_index AND j ≠ sink_index; else 0
  Visibility constraint: Sink attends only to itself; all other tokens can attend to sink
  Token initialization: Learnable embedding (default) or explicitly zero-initialized

- Critical path:
  1. Add sink token to vocabulary/embedding layer
  2. Modify attention mask generation to enforce self-attention-only constraint
  3. Ensure mask is applied consistently during both training (all diffusion timesteps) and inference
  4. Verify sink token position handling in downstream components (e.g., position embeddings, loss masking)

- Design tradeoffs:
  - Position (front vs. end): Front is conventional; end may simplify position embedding handling for fixed-length contexts. Paper shows both work.
  - Quantity (1 vs. multiple): Single token sufficient; additional tokens yield diminishing returns.
  - Initialization: Zero vector slightly improves some metrics but not universally; learnable is simpler.

- Failure signatures:
  - Sink token L2 norm grows large during training → suggests mask constraint is not being applied correctly
  - No performance gain → verify mask is applied at inference, not just training
  - Performance degradation in long-context settings → check if position embeddings interact unexpectedly with sink position

- First 3 experiments:
  1. **Mask verification test**: Log attention weights at sample layers; confirm sink receives high attention mass and attends only to itself.
  2. **Position ablation**: Train with sink at front and end; compare benchmark scores to verify position-invariance on your data.
  3. **Norm monitoring**: Track mean L2 norm of sink token vs. other tokens across layers and training steps; expect sink to maintain lowest norms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement from a dedicated sink token scale effectively to Diffusion Language Models significantly larger than 1.5B parameters?
- Basis in paper: [explicit] The authors explicitly state in the "Limitations" section: "We conducted extensive experiments on Diffusion Language Models at the 0.5B and 1.5B parameter scales. However, we did not scale our experiments to larger Diffusion Language Models."
- Why unresolved: While the method works on small scales, larger models have different attention dynamics and capacities. It is unknown if a single token provides sufficient "absorption" capacity for massive models or if the relative gains diminish.
- What evidence would resolve it: Training runs on DLM architectures at the 7B or 70B scale (e.g., modifying LLaDA or DiffuLLaMA) comparing the baseline against the sink-token augmented version on standard benchmarks.

### Open Question 2
- Question: Does the "moving sink" phenomenon exist in continuous diffusion language models, and can a discrete sink token resolve it?
- Basis in paper: [inferred] The paper focuses on discrete DLMs where tokens are explicitly replaced by a `[MASK]` token. The Related Work mentions continuous diffusion models (e.g., Gulrajani and Hashimoto, 2023), but the proposed mechanism relies on the discrete nature of token masking to define "low-norm" behavior.
- Why unresolved: Continuous models operate on latent embeddings rather than discrete token indices. The mechanism driving the sink (low semantic content of masked tokens) may not translate directly to continuous noise injection.
- What evidence would resolve it: An analysis of attention norms in continuous latent diffusion models, followed by experiments introducing a fixed "sink embedding" into the continuous sequence to see if similar stabilization occurs.

### Open Question 3
- Question: Can the stabilization provided by the sink token be leveraged to enable efficient KV-cache eviction or sliding window attention in DLMs?
- Basis in paper: [inferred] The Introduction identifies "inefficient KV caching" as a critical practical constraint of DLMs. The paper solves the *stability* issue (moving sinks), which is theoretically a prerequisite for the cache efficiency solutions (like StreamingLLM) used in autoregressive models.
- Why unresolved: The paper improves generation quality but does not benchmark inference speed or memory usage. It is unconfirmed if the "stable sink" allows DLMs to safely discard old KV pairs without performance collapse.
- What evidence would resolve it: Benchmarking inference latency and memory footprint using a sliding window attention mask on top of the sink token method, verifying that the model retains performance while using constant memory.

### Open Question 4
- Question: Is the standard learned embedding for the sink token optimal, or would a fixed zero-initialized embedding provide better regularization?
- Basis in paper: [explicit] In Section 4.4, the authors note that explicitly forcing the sink token's value states to zero vectors "yields consistent gains." However, the implementation (Eq. 5) uses a standard prepended embedding `s`.
- Why unresolved: While the value states are analyzed, the input embedding `s` is presumably learned like any other token. The analysis suggests "low-norm" is beneficial, raising the question of whether the input embedding itself should be constrained or fixed to zero to enforce the "no-op" behavior from the input layer.
- What evidence would resolve it: An ablation study comparing the current learned sink embedding against a fixed zero-embedding input (where gradients are not applied to `s`), measuring the impact on convergence speed and final benchmark accuracy.

## Limitations
- Limited to 0.5B and 1.5B parameter scales; scalability to larger models unknown
- Performance gains measured primarily on commonsense reasoning benchmarks, not generation quality
- Critical implementation details like exact diffusion formulation and noise schedules unspecified
- Potential regularization effects from extra token not distinguished from sink stabilization benefits

## Confidence
**High confidence** (supported by direct evidence and multiple validation experiments):
- The moving sink phenomenon exists and affects DLMs but not autoregressive models
- Sink tokens exhibit consistently lower L2 norms in value space
- The asymmetric attention mask implementation works as specified
- Position-invariance of the sink token's effectiveness
- Zero-initialized sink vectors maintain performance gains

**Medium confidence** (supported by primary evidence but with acknowledged limitations):
- The 10.77-11.70 point improvement range for 0.5B models on aggregate benchmarks
- The 8.25-7.95 point improvement for from-scratch trained DLMs
- The mechanism that low-norm value vectors minimize information mixing

**Low confidence** (claims with limited direct evidence or potential confounders):
- That sink stabilization alone accounts for the full performance improvement (vs. regularization effects)
- Generalization of improvements to generation tasks beyond classification benchmarks
- The proposed mechanism's dominance over other potential explanations for DLM performance

## Next Checks
1. **Mechanism isolation test**: Train a DLM with the proposed sink token but modify the attention mask to allow the sink to attend to other tokens (breaking the self-attention constraint). If performance degrades significantly, this validates that the asymmetric visibility—not just the presence of an extra token—drives improvements.

2. **Long-context scaling validation**: Evaluate the sink token's effectiveness on sequences longer than 2048 tokens (the reported context length). Monitor sink token attention patterns and value norms across layers to verify the low-norm property persists and that attention mass remains appropriately distributed in extended contexts.

3. **Generation quality assessment**: Beyond benchmark accuracy, measure generation diversity (self-BLEU, distinct n-grams) and coherence (MAUVE score, human evaluation) on open-ended generation tasks. This validates whether sink stabilization improves not just discriminative performance but also the quality of generated text sequences.