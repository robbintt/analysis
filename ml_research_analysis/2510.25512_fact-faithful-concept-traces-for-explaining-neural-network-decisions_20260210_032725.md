---
ver: rpa2
title: 'FaCT: Faithful Concept Traces for Explaining Neural Network Decisions'
arxiv_id: '2510.25512'
source_url: https://arxiv.org/abs/2510.25512
tags:
- concepts
- concept
- fact
- b-cos
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model with inherent concept-based explanations
  that are faithful to the model's decisions. Unlike prior work, FaCT does not make
  restrictive assumptions on concepts, such as being class-specific, small, or aligned
  with human expectations.
---

# FaCT: Faithful Concept Traces for Explaining Neural Network Decisions

## Quick Facts
- arXiv ID: 2510.25512
- Source URL: https://arxiv.org/abs/2510.25512
- Reference count: 40
- This paper proposes a model with inherent concept-based explanations that are faithful to the model's decisions.

## Executive Summary
FaCT introduces a neural network architecture that provides inherently interpretable explanations through concept-based attribution. Unlike prior work that relies on post-hoc explanations or restrictive concept assumptions, FaCT uses B-cos transforms and bias-free sparse autoencoders to extract shared, class-agnostic concepts that can be faithfully traced from any layer. The model achieves competitive ImageNet performance (<3% accuracy drop) while providing more consistent and interpretable concepts than baselines, validated through both quantitative metrics and user studies.

## Method Summary
FaCT combines B-cos transforms with bias-free TopK sparse autoencoders to create a model that inherently explains its decisions through concept attribution. The B-cos backbone uses row-normalized weights and cosine non-linearity to enable dynamic-linear decomposition, allowing faithful backpropagation of attributions. Bias-free SAEs extract sparse concept codes from intermediate features, with importance sampling ensuring representative training data. The model decomposes output logits into concept contributions and can visualize concepts at the input level. A novel C²-score metric uses DINOv2 features to evaluate concept consistency without human annotations.

## Key Results
- Competitive ImageNet accuracy with <3% drop across different backbone choices
- Higher concept consistency measured by C²-score compared to baselines
- User study confirms participants found FaCT's concepts more interpretable than baselines
- Faithful decomposition of output logits into concept contributions verified through deletion experiments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** B-cos transforms enable faithful decomposition of both logit-to-concept and concept-to-pixel contributions through dynamic-linearity.
- **Mechanism:** B-cos replaces standard ReLU blocks with row-normalized weights and cosine non-linearity, pushing weight-input alignment for higher activations. This creates a dynamic-linear transform $\tilde{W}(x)x$ that can be faithfully traced back to inputs at any layer.
- **Core assumption:** Weight-input alignment yields interpretable feature correspondences that humans can make sense of.
- **Evidence anchors:**
  - [abstract] "B-cos transforms... allow for dynamic-linear decompositions"
  - [Section 3.1, Eq. 2-3] Shows mathematical derivation of dynamic-linear property and summation to logits
  - [corpus] FACE paper addresses faithfulness but through different means; corpus lacks direct B-cos comparison
- **Break condition:** If downstream tasks require biases for performance, B-cos's bias-free design may limit accuracy gains.

### Mechanism 2
- **Claim:** Bias-free TopK Sparse Autoencoders create a shared concept basis where concept activations are strictly linear combinations of features, ensuring faithfulness.
- **Mechanism:** SAEs decompose intermediate features $F$ into sparse codes $U$ via a learned dictionary $V$. Removing biases ensures $U$ can be faithfully attributed back to inputs through the B-cos chain. TopK sparsity (k=8,16,32) provides interpretability while maintaining reconstruction.
- **Core assumption:** Sparse activations correspond to human-interpretable concepts that meaningfully explain model behavior.
- **Evidence anchors:**
  - [abstract] "sparse autoencoders (SAEs) to extract interpretable concepts at different layers"
  - [Section 3.1, Eq. 6-7] Shows how logits depend only on concept activations through reconstruction
  - [corpus] Corpus papers use SAEs but FaCT uniquely combines with B-cos for faithfulness guarantees
- **Break condition:** If concepts become too abstract or fail to align with any human-interpretable semantics, the method provides faithful but uninterpretable explanations.

### Mechanism 3
- **Claim:** C²-score uses DINOv2 features to measure concept consistency across images without requiring predefined annotations.
- **Mechanism:** For each concept, compute attribution-weighted embeddings $E_k(I)$ using DINOv2 features, then measure cosine similarity across images. Subtract random-baseline consistency to correct for dataset-level feature similarity. Higher scores indicate concepts activate on semantically similar regions.
- **Core assumption:** DINOv2 features capture semantic similarity well enough to serve as a ground-truth proxy for concept consistency.
- **Evidence anchors:**
  - [abstract] "A novel consistency metric, C²-score, is proposed to evaluate concept consistency without relying on predefined annotations"
  - [Section 4, Eq. 13-16] Full derivation of consistency computation
  - [corpus] Concept-Based XAI Metrics paper discusses concept evaluation; corpus lacks comparable annotation-free metrics
- **Break condition:** If DINOv2 features fail to capture the relevant semantic distinctions for a domain, C²-score may misrank concept quality.

## Foundational Learning

- **B-cos Transforms:**
  - Why needed here: Understanding how dynamic-linearity enables faithful backpropagation of attributions through arbitrary layer depths.
  - Quick check question: Given a 3-layer B-cos network, can you write the output as a single linear transform of the input?

- **Sparse Autoencoders (TopK variant):**
  - Why needed here: FaCT uses TopK-SAE to create sparse concept bases; understanding reconstruction-sparsity tradeoffs is essential for hyperparameter selection.
  - Quick check question: How does removing encoder/decoder biases affect the ability to attribute concept activations back to inputs?

- **Concept-Based Explainability:**
  - Why needed here: Distinguishing faithfulness (does the explanation reflect actual model computation?) from interpretability (can humans understand it?) clarifies what FaCT achieves.
  - Quick check question: If a concept perfectly predicts model output but humans cannot interpret its meaning, is it faithful, interpretable, both, or neither?

## Architecture Onboarding

- **Component map:**
  Input → B-cos Backbone (frozen, pretrained) → Intermediate Features F → TopK-SAE Encoder (ReLU(conv(W, F))) → Sparse Codes U → TopK-SAE Decoder (conv(V, U)) → Reconstructed Features F̂ → Remaining B-cos Layers → Logits

- **Critical path:**
  1. Select layer for concept extraction (earlier layers → simpler concepts, later layers → semantic concepts)
  2. Train bias-free TopK-SAE on importance-sampled features from ImageNet
  3. Verify reconstruction quality and check for "dead" or "always-active" latents before deployment

- **Design tradeoffs:**
  - **Sparsity vs. Accuracy:** Higher TopK improves reconstruction but reduces interpretability (more concepts per image). Paper uses TopK ∈ {8,16,32}.
  - **Layer depth vs. Concept semantics:** Earlier layers yield low-level features (curves, colors); later layers yield semantic concepts (parts, objects). Paper shows <3% accuracy drop across choices.
  - **Shared vs. class-specific basis:** Shared basis enables cross-class analysis but may dilute class-discriminative concepts.

- **Failure signatures:**
  - Dead latents: Concepts never activate → increase learning rate or reduce TopK
  - Always-active latents: Concepts activate on >60% of samples → model may have learned mean features; checkpoint selection filters these
  - Sharp accuracy drop on concept deletion: Small set of "always-on" concepts dominate → expected behavior, not a bug

- **First 3 experiments:**
  1. **Verify faithfulness:** Run concept deletion (Section 6) comparing FaCT's Eq. 9 contributions against Saliency and Sobol baselines on a 50-class subset. Expected: FaCT shows steeper logit drop.
  2. **Visualize concept diversity:** For a trained DenseNet-121 model, plot concept-label entropy vs. spatial extent (Appendix D.1) and manually inspect concepts from different entropy bins.
  3. **Validate C²-score alignment:** Compute C²-score for a set of concepts and compare rankings against human interpretability ratings using Spearman correlation (Appendix C.2). Expected: moderate-to-strong positive correlation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a model retain competitive performance if regularized to exclude uninterpretable concepts?
- Basis: [explicit] Section L states, "Whether a model without such concepts [uninterpretable ones] can be as performant as FaCT, is an open question."
- Why unresolved: The current unsupervised SAE approach yields a mix of high and low interpretability concepts; forcing interpretability may restrict the hypothesis space and reduce accuracy.
- Evidence: A comparative study measuring accuracy drops when penalizing low-interpretability concepts during training.

### Open Question 2
- Question: Would a single-stage training paradigm improve FaCT compared to the current two-stage approach?
- Basis: [explicit] Section L proposes "a single-stage training paradigm... regularized towards more-interpretable concepts, as opposed to the two-stage training."
- Why unresolved: Jointly optimizing the backbone and concept bottleneck might discover different, potentially more faithful or useful concepts than decoupled training.
- Evidence: Implementation of end-to-end training with interpretability constraints and comparison of concept consistency and task performance.

### Open Question 3
- Question: Are there foundation models superior to DINOv2 for evaluating concept consistency via the proposed C²-score?
- Basis: [explicit] Section L asks "whether there exist better alternatives [to DINOv2] is indeed still an open question."
- Why unresolved: DINOv2 may possess inherent biases or feature representations that fail to capture specific semantic nuances required for evaluating diverse concepts.
- Evidence: Correlating C²-scores derived from different foundation models against human user studies on concept consistency.

## Limitations
- Reliance on DINOv2 features for C²-score introduces potential domain dependence - if DINOv2 fails to capture relevant semantics for specific domains, concept quality evaluation may be compromised
- While the bias-free SAE design ensures faithfulness, it may limit reconstruction capacity, potentially affecting downstream accuracy
- The importance sampling strategy for SAE training is theoretically sound but computationally intensive and not fully specified in implementation details

## Confidence
- **High confidence:** Claims about B-cos transforms enabling dynamic-linear decomposition and the mechanism for logit-to-concept attribution (Section 3.1)
- **Medium confidence:** Claims about concept interpretability based on user study, as this relies on subjective human judgments
- **Medium confidence:** Claims about C²-score reliability as an annotation-free metric, pending validation across diverse domains

## Next Checks
1. **Cross-domain C²-score validation:** Evaluate C²-score on non-natural image datasets (e.g., medical imaging or satellite imagery) where DINOv2 features may have different semantic capture capabilities.
2. **Faithfulness verification under adversarial conditions:** Test whether concept deletions that should have minimal impact (based on FaCT's attribution) actually show reduced effect compared to Saliency/Sobol baselines when models face adversarial examples.
3. **Concept diversity analysis:** For each TopK setting (8, 16, 32), quantify the semantic diversity of extracted concepts using clustering metrics on their DINOv2 feature representations, validating that higher sparsity doesn't collapse to repetitive concepts.