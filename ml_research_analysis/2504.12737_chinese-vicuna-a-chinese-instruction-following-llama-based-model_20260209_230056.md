---
ver: rpa2
title: 'Chinese-Vicuna: A Chinese Instruction-following Llama-based Model'
arxiv_id: '2504.12737'
source_url: https://arxiv.org/abs/2504.12737
tags:
- chinese
- https
- training
- dataset
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chinese-Vicuna addresses the lack of efficient Chinese instruction-following
  language models by fine-tuning LLaMA with parameter-efficient LoRA and QLoRA methods.
  It leverages hybrid instruction datasets (BELLE and Guanaco) and supports 4-bit
  quantization, enabling training on consumer GPUs like RTX-2080Ti.
---

# Chinese-Vicuna: A Chinese Instruction-following Llama-based Model

## Quick Facts
- arXiv ID: 2504.12737
- Source URL: https://arxiv.org/abs/2504.12737
- Authors: Chenghao Fan; Zhenyi Lu; Jie Tian
- Reference count: 16
- Primary result: Fine-tuned LLaMA with LoRA/QLoRA for Chinese instruction-following, achieving competitive performance across dialogue, code generation, and domain-specific tasks

## Executive Summary
Chinese-Vicuna addresses the scarcity of efficient Chinese instruction-following language models by fine-tuning LLaMA using parameter-efficient LoRA and QLoRA methods. The model leverages a hybrid instruction dataset combining BELLE and Guanaco, supports 4-bit quantization for consumer GPU training, and includes domain adaptation for medical and legal tasks. Open-source tools for quantization, CPU inference, and multi-turn dialogue are provided, emphasizing accessibility and modularity for researchers and developers.

## Method Summary
Chinese-Vicuna fine-tunes LLaMA-7B/13B for Chinese instruction-following using parameter-efficient LoRA and QLoRA methods. The model trains on a merged dataset (~700K samples) combining BELLE (~500K Chinese pairs) and Guanaco (534K multilingual), with domain-specific datasets for medical (cMedQA2) and legal (Lawyer-LLaMA + CAIL) tasks. Training uses hyperparameters including BATCH_SIZE=128, EPOCHS=3, LR=3e-4, and LoRA configurations (R=8, alpha=16). The 7B model trains on 4×RTX-2080Ti with 8-bit LoRA in ~2.5 days, while the 13B uses 4-bit QLoRA in ~4 days. Domain adaptation employs continued fine-tuning from intermediate checkpoints to preserve general capabilities.

## Key Results
- Achieves competitive performance across multi-turn dialogue, translation, and code generation tasks
- Supports 4-bit quantization enabling training on consumer GPUs like RTX-2080Ti
- Demonstrates effective domain adaptation for medical and legal Q&A while retaining general instruction-following capabilities
- Provides open-source tools for quantization, CPU inference, and multi-turn dialogue generation

## Why This Works (Mechanism)
Chinese-Vicuna leverages parameter-efficient fine-tuning through LoRA and QLoRA, which modifies only a small subset of model parameters while preserving the pre-trained LLaMA weights. This approach enables effective instruction-following capability adaptation without the computational cost of full fine-tuning. The hybrid dataset combining BELLE and Guanaco provides diverse Chinese instruction patterns, while 4-bit quantization reduces memory requirements for consumer GPU deployment. Domain adaptation through continued fine-tuning from intermediate checkpoints prevents catastrophic forgetting, maintaining both general and specialized capabilities.

## Foundational Learning
- **Parameter-efficient fine-tuning (LoRA/QLoRA)**: Modifies only low-rank adapter weights instead of full model parameters, reducing memory and compute requirements while maintaining performance. Needed for practical fine-tuning on consumer hardware.
- **Instruction fine-tuning**: Trains models to follow natural language instructions through supervised learning from instruction-response pairs. Critical for transforming general-purpose LLMs into instruction-following systems.
- **Quantization (4-bit)**: Reduces model precision from 16/32-bit to 4-bit, dramatically decreasing memory footprint while preserving model quality. Essential for enabling LLaMA training on GPUs with limited VRAM.
- **Domain adaptation**: Continues fine-tuning on specialized datasets to improve performance on specific tasks while managing catastrophic forgetting. Required for building models effective in specialized domains like medical and legal.
- **Multi-turn dialogue handling**: Maintains conversation context across multiple exchanges through attention mechanisms and context window management. Necessary for interactive applications.

## Architecture Onboarding

**Component Map**: LLaMA base model -> LoRA adapters (q_proj, v_proj) -> 4-bit quantization (bitsandbytes) -> Chinese token extensions -> Instruction dataset fine-tuning

**Critical Path**: Base LLaMA → LoRA fine-tuning → 4-bit quantization → Instruction generation

**Design Tradeoffs**: Parameter efficiency (LoRA) vs. full fine-tuning performance; quantization quality vs. memory savings; general capability preservation vs. domain specialization; open-source accessibility vs. proprietary model advantages

**Failure Signatures**: Repetitive generation loops (low repetition penalty); catastrophic forgetting in domain adaptation; degraded Chinese character handling; memory overflow during 4-bit training

**First Experiments**:
1. Test basic Chinese generation quality with different repetition penalties (1.3-3.0) to avoid looping
2. Verify 4-bit quantization maintains generation quality while reducing memory usage
3. Compare domain-specific performance against general checkpoint to measure adaptation effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks quantitative benchmarks, relying on qualitative evaluation without standardized metrics
- Missing critical experimental details including optimizer configuration, weight decay, and warmup steps
- Does not explain the method for extending LLaMA's token embeddings to support Chinese characters
- Domain adaptation performance degradation not systematically quantified across tasks

## Confidence
- **High confidence**: Feasibility of LoRA/QLoRA fine-tuning approach on LLaMA
- **Medium confidence**: Reported training hyperparameters and hardware requirements
- **Low confidence**: Performance claims due to absence of quantitative benchmarks

## Next Checks
1. Evaluate Chinese-Vicuna on standardized Chinese instruction-following benchmarks (C-Eval, CMMLU) using consistent metrics
2. Implement training from scratch using provided hyperparameters and measure convergence, GPU memory usage, and generation quality
3. Systematically measure performance degradation during domain fine-tuning to quantify the trade-off between general and specialized capabilities