---
ver: rpa2
title: An Analysis of Large Language Models for Simulating User Responses in Surveys
arxiv_id: '2512.06874'
source_url: https://arxiv.org/abs/2512.06874
tags:
- demographic
- llms
- claims
- children
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle to accurately simulate user responses
  to survey questions across diverse demographic groups, even with advanced prompting
  strategies. Direct prompting and chain-of-thought prompting yield similar performance,
  while the proposed CLAIMSIM method improves response diversity by eliciting multiple
  viewpoints as contextual input.
---

# An Analysis of Large Language Models for Simulating User Responses in Surveys

## Quick Facts
- arXiv ID: 2512.06874
- Source URL: https://arxiv.org/abs/2512.06874
- Authors: Ziyun Yu; Yiru Zhou; Chen Zhao; Hongyi Wen
- Reference count: 40
- Large language models struggle to accurately simulate user responses to survey questions across diverse demographic groups, even with advanced prompting strategies.

## Executive Summary
This paper evaluates whether large language models can accurately simulate individual responses to survey questions given demographic profiles. Using the World Values Survey (WVS) dataset, the authors compare three prompting approaches—direct prompting, chain-of-thought (CoT), and their proposed CLAIMSIM method—across GPT-4O-MINI, LLAMA4 17B, and QWEN3 235B-A22B models. While CLAIMSIM improves response diversity by eliciting multiple viewpoints as contextual input, all methods perform only slightly above random accuracy. The study reveals fundamental limitations in LLMs' ability to adapt responses to specific user profiles due to viewpoint entrenchment and failure to reason over conflicting demographic claims.

## Method Summary
The study employs three prompting approaches on multiple LLMs to simulate survey responses: (1) Direct Prompting—single-step generation using demographic profiles and questions; (2) Chain-of-Thought—reasoning steps before answer generation; and (3) CLAIMSIM—a novel method that generates five diverse claims per demographic feature, summarizes them highlighting consistencies and contradictions, then predicts answers using CoT with these summaries as context. The evaluation uses 100 randomly sampled individuals from WVS Wave 7 across 16 questions in gender, politics, and religion domains, measuring exact match accuracy, binary accuracy, answer distribution diversity, and Wasserstein distance for distributional alignment.

## Key Results
- CLAIMSIM produces more diverse responses than direct prompting and CoT, but all methods achieve only slightly above random accuracy
- For 50% of survey questions, generated claims reflect single-perspective opinions regardless of demographic variation
- Models fail to reason over conflicting claims when demographic features suggest opposing viewpoints, limiting adaptive simulation

## Why This Works (Mechanism)

### Mechanism 1: CLAIMSIM Claim Diversification
- **Claim:** Eliciting multiple viewpoints from LLM parametric knowledge as contextual input increases response diversity, though not accuracy.
- **Mechanism:** For each demographic feature (e.g., sex, education, religion), the model samples five separate claims, summarizes them with explicit attention to consistencies and contradictions, then grounds the final answer prediction on these aggregated summaries alongside demographic information and the query.
- **Core assumption:** LLMs possess latent parametric knowledge about how different demographic groups might reason about survey questions, and surfacing this knowledge explicitly reduces default-response bias.
- **Evidence anchors:**
  - [abstract]: "while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users"
  - [section 2.4]: "We hypothesize that generating claims elicited from LLM parametric knowledge for each individual demographic feature as additional context could help mitigate model bias, as variations across features often lead to conflicting opinions"
  - [corpus]: Related work on patient perspective simulation (arXiv:2501.06964) shows similar role-playing approaches produce "cost-effective and efficient" alternatives to human subjects, though cross-cultural studies (arXiv:2506.23107) reveal persistent limitations in capturing nuanced decision-making behavior.
- **Break condition:** Diversity gains do not translate to accuracy gains when the underlying parametric knowledge itself reflects entrenchment (50% of questions yield single-perspective claims regardless of demographic variation).

### Mechanism 2: RLHF Viewpoint Entrenchment
- **Claim:** Reinforcement learning from human feedback (RLHF) reinforces dominant viewpoints, causing models to maintain fixed perspectives across varying demographic contexts.
- **Mechanism:** RLHF optimization pushes models toward widely accepted moral values and consensus opinions in training data; these reinforced patterns become resistant to conditioning on minority or dissenting perspectives, limiting simulation fidelity for underrepresented groups.
- **Core assumption:** The entrenchment effect operates at the parametric level rather than purely at inference-time, meaning prompt engineering alone cannot override learned viewpoint priors.
- **Evidence anchors:**
  - [abstract]: "RLHF reinforces dominant viewpoints and limits the models' ability to adapt responses to specific user profiles"
  - [section 3.4]: "for 50% of the survey questions, the claims generated by CLAIMSIM reflect a single-viewed opinion, regardless of demographic variation"
  - [corpus]: Evidence is limited in corpus neighbors; related work on fairness in LLM-generated surveys (arXiv:2501.15351) notes "unexplored biases across socio-demographic and geographic contexts" but does not directly confirm RLHF as the causal mechanism.
- **Break condition:** Entrenchment is not absolute—religion domain claims showed higher diversity (100% of questions) compared to politics (15%), suggesting domain-dependent vulnerability.

### Mechanism 3: Conflicting Claims Reasoning Failure
- **Claim:** LLMs fail to reason over nuanced differences among demographic features when presented with conflicting claims, limiting adaptive simulation.
- **Mechanism:** When claims generated from different demographic features contradict (e.g., education suggesting one view, religion another), LLMs lack reliable mechanisms to weight, prioritize, or integrate these tensions relative to a specific target profile.
- **Core assumption:** The failure reflects insufficient capability to model how demographic attributes interact in shaping opinions, not simply lack of factual knowledge.
- **Evidence anchors:**
  - [abstract]: "when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles"
  - [section 3.4, Figure 3]: Case study shows LLMs fail to reason over conflicting evidence, requiring them to "capture subtle relationships between demographic attributes"
  - [corpus]: Cross-cultural comparison work (arXiv:2506.21587) evaluates opinion simulation across Chinese and U.S. models, finding similar struggles with diverse societies, though specific mechanism evidence is weak.
- **Break condition:** Failure is more pronounced for finer-grained distinctions (exact accuracy ~30-42%) than binary agree/disagree categories (B-Acc ~52-73%), suggesting coarse reasoning succeeds where nuanced reasoning fails.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** CoT is the intermediate baseline between direct prompting and CLAIMSIM; understanding its failure mode (no significant improvement over direct prompting for this task) clarifies why task structure matters for reasoning approaches.
  - **Quick check question:** Why would CoT improve math problems but not survey simulation? (Hint: Does survey simulation have clear hierarchical reasoning steps?)

- **Concept: Wasserstein Distance**
  - **Why needed here:** The paper uses Wasserstein distance to quantify distributional alignment between predicted and ground-truth answer distributions across ordinal labels; this captures more nuance than accuracy alone.
  - **Quick check question:** If Model A achieves 40% accuracy and Model B achieves 38%, but Model B has lower Wasserstein distance, which better captures true response patterns?

- **Concept: Parametric Knowledge Elicitation**
  - **Why needed here:** CLAIMSIM's core innovation is extracting what LLMs "know" about demographic-viewpoint relationships; this differs from retrieval-augmented approaches that ground in external documents.
  - **Quick check question:** If parametric knowledge itself contains entrenched biases, what intervention strategies might help? (Consider: prompt optimization vs. fine-tuning vs. alternative training objectives.)

## Architecture Onboarding

- **Component map:**
  Survey Question + Demographic Profile (D) → [CLAIMSIM Step 1: Claims Generator] → For each D_i ∈ D: Sample 5 claims → Summarize with consistency/contradiction highlighting → [CLAIMSIM Step 2: Answer Generator] → Input: (question, D, aggregated summaries S) → Output: Simulated response a via CoT reasoning

- **Critical path:** The claims generation phase is the primary lever for diversity; the answer prediction phase integrates claims but remains bottlenecked by reasoning-over-conflicts limitations.

- **Design tradeoffs:**
  - Temperature = 0.7 balances diversity vs. consistency for claim sampling; lower values reduce diversity gains.
  - Five sampling rounds per demographic feature: more rounds may increase diversity but also risk reinforcing dominant patterns if underlying knowledge is biased.
  - CLAIMSIM adds inference cost (multiple generation calls per question) without proportional accuracy gains.

- **Failure signatures:**
  - Single-perspective claims: If all five sampled claims for a given demographic feature agree, the summarization step cannot introduce diversity.
  - Unintegrated demographic features: Case analysis shows CoT may enumerate demographic features but fail to weight them (e.g., ignoring sex in final reasoning).
  - Unified viewpoint across domains: Politics domain claims show lowest diversity (15%), suggesting entrenched political opinions resist demographic conditioning.

- **First 3 experiments:**
  1. **Domain sensitivity analysis:** Run CLAIMSIM across additional WVS domains (e.g., economics, environment) to test whether entrenchment is topic-specific or general.
  2. **Claim diversity intervention:** Before summarization, explicitly prompt for minority/dissenting perspectives or use contrastive decoding to suppress dominant responses; measure impact on Wasserstein distance.
  3. **Fine-grained demographic interaction:** Construct controlled profiles where two features strongly predict opposing views (e.g., highly religious + highly educated on certain questions); analyze whether models can articulate and resolve the tension, or default to one feature.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do advanced reasoning models (e.g., OpenAI O3, DeepSeek R1) improve simulation accuracy and claim diversity compared to standard LLMs?
- **Basis in paper:** [explicit] The authors note in the Limitations section: "we did not include reasoning models such as OpenAI O3, Deepseek R1... but may benefit both methods with more comprehensive claim generation and advanced reasoning capability."
- **Why unresolved:** These specific architectures were excluded from the evaluation due to API costs and inference time constraints, leaving their potential to resolve the identified reasoning failures untested.
- **What evidence would resolve it:** A comparative study evaluating reasoning models on the same World Values Survey (WVS) dataset to measure if they achieve higher than random accuracy and successfully reason over conflicting claims.

### Open Question 2
- **Question:** Can prompt optimization techniques overcome RLHF-induced viewpoint entrenchment to elicit truly diverse claims?
- **Basis in paper:** [explicit] The Discussion section states: "Future research shall look into effective ways for diverse claim elicitations, such as prompt optimizations (Pryzant et al., 2023)."
- **Why unresolved:** The current CLAIMSIM method relies on sampling multiple responses, which is still constrained by the model's parametric knowledge and inherent biases, failing to generate diverse claims for 50% of questions.
- **What evidence would resolve it:** Experiments applying automated prompt optimization (e.g., gradient-based methods) to maximize the Wasserstein distance of generated claims, demonstrating successful elicitation of non-dominant viewpoints.

### Open Question 3
- **Question:** To what extent do LLMs maintain consistency in individual-level opinions when simulating decision-making across multiple domains?
- **Basis in paper:** [explicit] The Limitations section explicitly states: "we did not study the consistency of individual-level opinions and simulation of decision-making across multiple domains, but we denote this as important perspective... shall be investigated in future work."
- **Why unresolved:** The current study evaluates responses to isolated questions but does not assess if a simulated persona maintains a coherent belief system (e.g., holding consistent views on gender and religion simultaneously).
- **What evidence would resolve it:** A longitudinal evaluation where a simulated user answers a battery of correlated survey questions, measuring cross-domain coherence against real human response patterns.

## Limitations
- CLAIMSIM improves response diversity but not accuracy, suggesting parametric knowledge itself contains entrenched biases
- Models maintain fixed viewpoints across demographic variations for 50% of questions, particularly in politics domain (15% diversity)
- LLMs fail to reason over conflicting claims from different demographic features, limiting adaptive simulation capabilities

## Confidence
- High confidence: CLAIMSIM improves response diversity (clear distributional differences in answer histograms)
- Medium confidence: RLHF entrenchment explains fixed viewpoints across demographics (mechanism plausible but not directly validated)
- Low confidence: Conflicting claims reasoning failure is the primary accuracy bottleneck (case study evidence exists but systematic analysis is limited)

## Next Checks
1. **Domain sensitivity analysis**: Test CLAIMSIM across additional WVS domains (economics, environment) to determine if entrenchment is topic-specific or general
2. **Claim diversity intervention**: Explicitly prompt for minority perspectives or use contrastive decoding before summarization; measure impact on Wasserstein distance
3. **Fine-grained demographic interaction**: Construct profiles where two features predict opposing views; analyze whether models can articulate and resolve tensions or default to single-feature reasoning