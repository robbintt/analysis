---
ver: rpa2
title: 'DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual
  Degradation'
arxiv_id: '2512.03992'
source_url: https://arxiv.org/abs/2512.03992
tags:
- degradation
- visual
- temporal
- hallucination
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIQ-H, the first benchmark designed to evaluate
  hallucination persistence in Vision-Language Models (VLMs) under temporal visual
  degradation. Existing benchmarks focus on static images and ignore how transient
  visual corruption leads to hallucinations that persist across subsequent frames.
---

# DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation

## Quick Facts
- arXiv ID: 2512.03992
- Source URL: https://arxiv.org/abs/2512.03992
- Reference count: 27
- First benchmark designed to evaluate hallucination persistence in Vision-Language Models under temporal visual degradation

## Executive Summary
This paper introduces DIQ-H, a novel benchmark that addresses the critical gap in evaluating Vision-Language Models' robustness to temporal visual degradation. While existing benchmarks focus on static images, DIQ-H specifically examines how transient visual corruption (motion blur, sensor noise, compression artifacts) leads to hallucinations that persist across subsequent frames. The benchmark employs a multi-turn question-answering framework to measure hallucination persistence, error recovery, and temporal consistency. A key innovation is the Uncertainty-Guided Iterative Refinement (UIR) method, which generates reliable pseudo-ground-truth annotations at scale, achieving 15.3% accuracy improvement over baseline approaches.

## Method Summary
DIQ-H introduces a comprehensive framework for evaluating VLM robustness under temporal degradation by applying physics-based degradations to video sequences and measuring hallucination persistence across multi-turn interactions. The core innovation is the Uncertainty-Guided Iterative Refinement (UIR) method, which leverages lightweight VLMs with uncertainty filtering to generate scalable pseudo-ground-truth annotations. The benchmark evaluates three key metrics: hallucination persistence (how long errors persist), error recovery (ability to correct mistakes), and temporal consistency (maintaining coherent understanding across frames). The methodology was tested across 16 state-of-the-art VLMs, revealing substantial performance gaps between proprietary and open-source models in handling degraded visual inputs over time.

## Key Results
- GPT-4o achieved only 78.5% recovery rate, indicating significant hallucination persistence under temporal degradation
- Open-source VLMs struggled with temporal consistency, scoring less than 60% on the benchmark
- UIR method improved annotation accuracy by 15.3% compared to baseline pseudo-ground-truth generation approaches

## Why This Works (Mechanism)
The benchmark works by creating controlled temporal degradation scenarios that force VLMs to process corrupted visual information over multiple frames, revealing how initial misinterpretations persist and propagate. The multi-turn question-answering format exposes the temporal dimension of hallucination persistence, where early visual errors compound over time. UIR's uncertainty filtering mechanism improves annotation quality by identifying and resolving ambiguous cases through iterative refinement, making large-scale evaluation feasible without extensive human annotation.

## Foundational Learning

**Vision-Language Model (VLM) Architecture**: Understanding the encoder-decoder structure and multimodal fusion mechanisms is essential for interpreting how visual corruption affects language generation. Quick check: Can you trace how visual features flow through the VLM and influence output tokens?

**Temporal Consistency in Sequential Processing**: Knowledge of how models maintain state across time steps explains why initial hallucinations persist. Quick check: What mechanisms do VLMs use to maintain context across multiple frames?

**Physics-Based Image Degradation**: Understanding motion blur, sensor noise, and compression artifacts is crucial for interpreting the benchmark's controlled degradation scenarios. Quick check: How do different degradation types affect low-level visual features differently?

## Architecture Onboarding

**Component Map**: Video Input -> Physics-Based Degradation -> Multi-Turn QA Interface -> VLM Under Test -> Response Generation -> UIR Pseudo-Annotation -> Metric Calculation (Persistence/Recovery/Consistency)

**Critical Path**: The degradation application and multi-turn QA sequence represents the critical evaluation path, as it directly exposes hallucination persistence behavior that the benchmark aims to measure.

**Design Tradeoffs**: The paper trades absolute realism for controlled reproducibility by using simulated degradations rather than natural video streams, enabling systematic comparison but potentially missing real-world complexities.

**Failure Signatures**: Models exhibiting high hallucination persistence show consistent error patterns across degradation types, while those with poor recovery fail to correct initial misinterpretations even when subsequent frames provide corrective visual information.

**First Experiments**:
1. Baseline evaluation without degradation to establish VLM performance ceilings
2. Single degradation type testing to isolate individual effect patterns  
3. Varying question complexity to assess how task difficulty influences hallucination persistence

## Open Questions the Paper Calls Out
None

## Limitations
- Pseudo-ground-truth generation reliability depends on lightweight VLM accuracy without extensive human validation
- Focus on specific degradation types excludes other real-world phenomena like lighting changes and occlusion patterns
- Temporal consistency metrics may be sensitive to question phrasing variations and task complexity

## Confidence

**Major claim clusters confidence:**
- Temporal hallucination persistence measurement methodology: **Medium** - Novel approach but validation depends on pseudo-ground-truth quality
- VLM robustness gap findings: **High** - Consistent across multiple model families, though absolute values uncertain
- UIR effectiveness: **Medium** - Quantitative improvement shown, but without independent verification

## Next Checks

1. Human validation study on 100+ samples to assess UIR pseudo-ground-truth accuracy and hallucination detection reliability
2. Cross-dataset generalization test using real-world video streams with natural temporal degradation beyond simulated physics-based effects
3. Ablation study varying question phrasing, task complexity, and degradation severity to establish robustness of temporal consistency metrics