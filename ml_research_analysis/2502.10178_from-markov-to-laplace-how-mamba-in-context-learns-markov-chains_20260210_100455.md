---
ver: rpa2
title: 'From Markov to Laplace: How Mamba In-Context Learns Markov Chains'
arxiv_id: '2502.10178'
source_url: https://arxiv.org/abs/2502.10178
tags:
- mamba
- markov
- estimator
- optimal
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the in-context learning (ICL) capabilities of
  Mamba, a selective state space model, on Markov chain prediction tasks. The authors
  propose a novel framework using random Markov chains to theoretically and empirically
  analyze Mamba's ICL performance.
---

# From Markov to Laplace: How Mamba In-Context Learns Markov Chains

## Quick Facts
- arXiv ID: 2502.10178
- Source URL: https://arxiv.org/abs/2502.10178
- Reference count: 40
- Key outcome: Single-layer Mamba efficiently learns optimal Laplacian smoothing estimator for Markov chains, unlike transformers which require multiple layers

## Executive Summary
This paper studies the in-context learning (ICL) capabilities of Mamba, a selective state space model, on Markov chain prediction tasks. The authors propose a novel framework using random Markov chains to theoretically and empirically analyze Mamba's ICL performance. They show that even a single-layer Mamba can efficiently learn the optimal Laplacian smoothing estimator for all Markovian orders, unlike transformers which require multiple layers. Theoretically, the authors characterize Mamba's representation capacity and demonstrate that convolution plays a fundamental role in enabling it to represent the optimal estimator. Empirically, they validate these findings on both Markovian and natural language data, showing that convolution significantly improves Mamba's performance on language modeling tasks. The paper provides the first formal connection between Mamba and optimal statistical estimators, offering insights into Mamba's learning mechanisms and suggesting promising research directions.

## Method Summary
The paper analyzes Mamba's in-context learning performance on random k-th order Markov chains by comparing it against transformers and optimal statistical estimators. The experimental setup involves training single-layer Mamba-2 and transformer models on binary sequences generated from random Markov chains, evaluating their ability to learn the optimal Laplacian smoothing estimator. The authors use cross-entropy loss as the primary metric, comparing model predictions against the theoretically optimal add-β estimator. They conduct ablation studies by removing convolutional layers to demonstrate their critical importance, and test Mamba's adaptability on switching Markov models to showcase its selectivity mechanism.

## Key Results
- Single-layer Mamba efficiently learns the optimal Laplacian smoothing estimator for first-order Markov chains
- Convolution plays a fundamental role in enabling Mamba to represent the optimal count-based estimation
- Mamba's selectivity mechanism via state-transition factor a_t allows it to handle non-Markovian processes by selectively resetting its state
- Theoretical lower bound shows that hidden dimension must scale exponentially with Markov order for any recurrent architecture to implement Laplacian smoothing precisely

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-layer Mamba can represent the optimal Laplacian smoothing estimator for first-order Markov chains.
- Mechanism: The state matrix H_t maintains running counts of state transitions. The convolution layer (window w ≥ k+1) captures the necessary local context (current token and length-k prefix). When a_t ≈ 1, the state integrates all past transition information without decay. The output y_t = H_t c_t then projects the accumulated counts to retrieve the relevant counts for the current context, which the final linear layer transforms into the add-β probability estimate (n_1 + β)/(n + 2β).
- Core assumption: The model parameters converge to a configuration where a_t ≈ 1 and specific orthogonality conditions (e.g., b^{(11)⊤c^{(0)} ≈ 0) are met.
- Evidence anchors:
  - [abstract] "...a single-layer Mamba efficiently learns the optimal Laplacian smoothing estimator... attributed to its convolution mechanism enabling optimal count-based estimation."
  - [section] Theorem 1 proves that for a simplified MambaZero model, parameters exist such that the prediction is arbitrarily close to the Laplacian estimator for order-1 Markov chains.
  - [corpus] A related paper on transformers (ArXiv:2508.07208, "What One Cannot, Two Can") confirms that single-layer transformers struggle with induction-head-like counting, supporting the architectural difference claim.
- Break condition: If the convolution window w < k+1, the model cannot distinguish sequences with identical prefix counts but different transition counts, breaking the Laplacian estimator representation.

### Mechanism 2
- Claim: The convolutional preprocessing layer is the critical architectural component for implementing Laplacian smoothing, more so than the selective gating or other non-linearities.
- Mechanism: Convolutions over the input sequence compute features that explicitly encode the local k-length context. These features generate the b_t and c_t vectors which index into the state matrix, allowing the model to selectively update and retrieve counts for the specific context required by the add-β estimator. Removing the convolution forces the model to rely on a single token, making it impossible to track transitions between specific prefix sequences.
- Core assumption: The recurrent state update, in isolation without the convolutional preprocessing, is insufficient to perform the necessary context-dependent counting.
- Evidence anchors:
  - [abstract] "...attributed to its convolution mechanism enabling optimal count-based estimation."
  - [section] Figure 3a and surrounding text: "convolution plays a fundamental role... the simplified Mamba with just the convolution succeeds..., while the full model without convolution fails."
  - [corpus] No direct corpus evidence on Mamba's convolutional role in ICL; this appears to be a novel contribution of the paper.
- Break condition: Ablating the convolutional layers leads to a dramatic increase in loss, as shown empirically in Figure 3a.

### Mechanism 3
- Claim: Mamba's selectivity mechanism, via the state-transition factor a_t, enables it to handle non-Markovian processes by selectively resetting its state.
- Mechanism: The a_t = exp(-Δ_t) term acts as a forget gate. In the switching Markov model, the optimal strategy is to reset transition counts upon encountering a switch token. The model learns to set a_t ≈ 0 when the input token is a switch, zeroing out the state H_t and discarding all past information. When the token is not a switch, a_t ≈ 1, allowing the state to accumulate counts as in the standard Markov case.
- Core assumption: The model can learn a function of the input that correctly identifies the "switch" condition and maps it to an appropriate a_t value.
- Evidence anchors:
  - [abstract] "...demonstrate Mamba's adaptability to non-Markovian processes via its selectivity mechanism..."
  - [section] Section 5.1, Figure 4b "highlights the selectivity process... every time a switch token appears, the model erases all information about the past by setting at = 0."
  - [corpus] No strong corpus evidence found for this specific mechanism in Mamba.
- Break condition: If a_t is constrained to be 1 (no forgetting), the model would conflate counts from different Markov regimes, leading to suboptimal prediction.

## Foundational Learning

- Concept: **Laplacian Smoothing (Add-β Estimator)**
  - Why needed here: This is the optimal statistical estimator for the problem posed. The paper's core claim is that Mamba learns to implement this specific statistical formula.
  - Quick check question: Given a sequence 0,1,1,0,1,1 of first-order Markov transitions from prefix 1, what is the add-1 estimate for the probability of the next token being 1?

- Concept: **Markov Chains of Order k**
  - Why needed here: The entire experimental and theoretical setup is based on data generated from random k-th order Markov chains. Understanding this dependency structure (next state depends on the previous k states) is crucial.
  - Quick check question: For a 2nd-order Markov chain, what is the set of possible contexts that determine the probability of the next token?

- Concept: **State Space Models (SSMs) / Selective SSMs**
  - Why needed here: Mamba is a Selective SSM. The mechanism explanations rely on the model's recurrent state update equations (H_t = a_t H_{t-1} + ...) and the role of the selectivity parameters (a_t, b_t, c_t).
  - Quick check question: In the Mamba update equation, what is the functional role of the scalar a_t in controlling information flow from the past state?

## Architecture Onboarding

- Component map: Tokens x_t ∈ {0,1} → Embedding e_t ∈ ℝ^d → Convolutional Preprocessing (1D convolutions over window w) → b_t, c_t vectors → State Update H_t = a_t H_{t-1} + e_{x_t} b_t^⊤ → State Projection y_t = H_t c_t → Output/MLP/Linear

- Critical path: The convolutional preprocessing layer is the critical component to understand. Unlike standard RNNs, Mamba processes a local window of input to generate the parameters (b_t, c_t) for the recurrent update, which grants it the representational power for context-specific counting.

- Design tradeoffs:
  - Convolution Window Size (w) vs. Markov Order (k): The paper establishes that w ≥ k+1 is required. A larger window increases computational cost but allows the model to handle higher-order dependencies.
  - Hidden State Dimension (d) vs. Markov Order (k): Theorem 2 proves a lower bound suggesting the required dimension d may scale exponentially with k for any recurrent architecture to implement Laplacian smoothing precisely. This is a fundamental tradeoff between model capacity and task complexity.

- Failure signatures:
  - Loss plateaus at suboptimal level: Check the convolution window size. If it's smaller than the Markov order k+1, the model will fail to learn the optimal estimator.
  - Model cannot adapt to distribution shifts (e.g., switching Markov): Examine the learned a_t values. If a_t does not approach 0 at switch points, the selectivity mechanism has not learned the correct forgetting behavior. This could be due to an insufficiently expressive network computing Δ_t.

- First 3 experiments:
  1. Reproduce the core ICL result: Train a single-layer Mamba and a single-layer transformer on first-order random Markov chains (as per Sec 2.1). Compare the test loss to the optimal Laplacian estimator's loss. The Mamba model should converge to the optimal loss; the transformer should not.
  2. Ablate the convolution: Retrain the single-layer Mamba with the convolutional preprocessing layer removed. The model should fail to learn the task (loss should remain high), empirically validating the central architectural claim.
  3. Test the selectivity mechanism: Train the model on the "Switching Markov" data from Section 5.1. Monitor the learned a_t values. A successful run will show a_t dropping to near zero at the time steps corresponding to the inserted switch tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical construction for order-1 Markov chains be formally extended to prove Conjecture 1 for arbitrary order k, specifically that MambaZero with dimensions d = N = 2^k+1 and window w = k + 1 can implement the optimal Laplacian smoothing estimator?
- Basis in paper: [explicit] Conjecture 1 states this claim but notes "the proof becomes intractable due to the difficulty in tracking the correlations between the transition counts as k increases."
- Why unresolved: The first-order proof leverages correlations between transition counts (n01 and n10 differ by at most one), but these correlations become complex for higher orders, making direct extension difficult.
- What evidence would resolve it: A constructive proof showing parameter choices that achieve ε-approximate Laplacian smoothing for arbitrary k, or a counterexample showing the conjectured dimensions are insufficient.

### Open Question 2
- Question: What is the role of depth in multi-layer Mamba models for in-context learning of Markov chains—does additional depth provide expressivity benefits beyond single-layer performance?
- Basis in paper: [explicit] The conclusion states "Extending our results to deeper Mamba models and understanding the role of depth are some interesting future directions."
- Why unresolved: All theoretical results and experiments focus on single-layer Mamba; the benefit of stacking layers for this ICL task remains unexplored.
- What evidence would resolve it: Experiments comparing single vs. multi-layer Mamba on higher-order Markov chains, and theoretical characterization of what additional layers enable.

### Open Question 3
- Question: How does the gating mechanism introduced in Mamba-2 interact with the convolution-based counting mechanism—does gating preserve, enhance, or interfere with Laplacian smoothing implementation?
- Basis in paper: [inferred] Related work states that gating in Mamba-2 "invalidates to a certain extent the convolutional view of the architecture," yet experiments show Mamba-2 successfully learns Laplacian smoothing.
- Why unresolved: The theoretical construction relies on the convolutional view without gating; the paper does not analyze how gating affects the mechanism.
- What evidence would resolve it: Ablation studies isolating gating's contribution, and theoretical analysis of how gated recurrence interacts with count-based estimation.

### Open Question 4
- Question: Can the lower bound of d·p ≥ 2^k(1-3ε)log(1/ε) be circumvented through architectural innovations beyond standard recurrent models, such as hybrid attention-SSM architectures?
- Basis in paper: [inferred] Theorem 2 establishes exponential scaling is necessary for "any recurrent architecture," but hybrid architectures combining recurrence with other mechanisms (like Jamba) are not covered.
- Why unresolved: The lower bound applies to pure recurrent models; hybrid approaches may achieve different scaling by leveraging non-recurrent components.
- What evidence would resolve it: Empirical evaluation of hybrid architectures on high-order Markov chains with bounded hidden dimensions, comparing scaling behavior to the theoretical bound.

## Limitations

- Theoretical results rely on simplified MambaZero models that abstract away architectural details like MLPs and layer normalization
- Exponential lower bound for hidden dimension scaling applies to recurrent architectures generally but not specifically proven for full Mamba implementation
- Analysis focuses on binary sequences; generalization to larger vocabularies remains unexplored
- Only single-layer models are theoretically characterized; role of depth in multi-layer Mamba remains open

## Confidence

Medium: The empirical evidence showing Mamba's superiority over transformers for ICL on Markov chains is compelling, and the ablation study clearly demonstrates the importance of the convolutional mechanism. However, the theoretical characterization, while rigorous, applies to simplified models. The paper acknowledges that the exact parameter configurations enabling optimal estimation in practice remain an open question.

## Next Checks

1. Extend the analysis to higher-order Markov chains (k>2) to verify if the exponential scaling of required hidden dimension holds empirically
2. Test Mamba's performance on non-binary alphabets and larger vocabularies to assess generalizability beyond the binary case studied here
3. Conduct a more comprehensive ablation study removing not just convolution but also other components (MLPs, layer norm) to isolate which architectural elements are truly essential for achieving the optimal estimator