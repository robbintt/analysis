---
ver: rpa2
title: Learning to Represent Individual Differences for Choice Decision Making
arxiv_id: '2503.21704'
source_url: https://arxiv.org/abs/2503.21704
tags:
- learning
- johujl
- user
- individual
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that using representation learning to model
  individual differences in behavioral decision-making tasks improves prediction accuracy
  compared to standard machine learning models. The Beh2vec architecture, inspired
  by Word2vec and Doc2vec, learns user embeddings from both structured (demographic)
  and unstructured (free-text responses to open-ended questions) data.
---

# Learning to Represent Individual Differences for Choice Decision Making

## Quick Facts
- arXiv ID: 2503.21704
- Source URL: https://arxiv.org/abs/2503.21704
- Reference count: 26
- Primary result: Beh2vec model with representation learning achieves 3.6-4.2% higher prediction accuracy than baseline ML models for individual choice prediction

## Executive Summary
This paper addresses the challenge of modeling individual differences in behavioral decision-making by introducing a representation learning framework called Beh2vec. The approach learns user embeddings from structured demographic data and unstructured free-text responses, then combines these with task-specific features to predict choices in economic gambles. The method achieves 3.6-4.2% improvement over standard ML baselines and matches the performance of a well-validated theory-based behavioral model, demonstrating that representation learning can effectively capture individual behavioral patterns from diverse data sources.

## Method Summary
The Beh2vec architecture uses representation learning to model individual differences in choice behavior. It learns user embeddings from both structured (demographic) and unstructured (free-text responses) data, then concatenates these with gamble features and feeds them into a multi-layer perceptron. The model is trained on data from 1,205 participants making 64 binary choices each across 8 different economic decision tasks. Text representations are generated using pre-trained Word2Vec vectors averaged per user, while demographic and user ID information are processed through embedding layers. The approach is compared against standard ML models and a theory-based behavioral model based on Prospect Theory.

## Key Results
- Text-based representation learning achieves highest accuracy at 75.7% versus 73.5% for user ID model
- Representation learning improves prediction accuracy by 3.6-4.2% over baseline MLP models
- Performance is competitive with a well-validated theory-based behavioral model
- The approach demonstrates flexibility in incorporating diverse data types (demographics, text, user IDs)

## Why This Works (Mechanism)
The Beh2vec architecture works by learning distributed representations that capture latent individual behavioral patterns from multiple data sources. Unlike traditional models that treat each user independently or rely on predefined theoretical constructs, the representation learning approach can automatically discover complex patterns in how individual characteristics relate to decision-making. The text-based embeddings appear particularly effective because open-ended responses likely contain rich contextual information about personality, values, and decision-making tendencies that aren't captured by demographics alone.

## Foundational Learning
- **Word2Vec embeddings**: Pre-trained semantic representations of text that capture word meaning in vector space - needed to convert free-text responses into numerical features for ML models; quick check: verify the Word2Vec model loads correctly and produces consistent vector outputs
- **User embedding layers**: Learnable dense representations for categorical user identifiers - needed to capture individual behavioral patterns beyond raw user IDs; quick check: ensure embedding layer outputs 200-dimensional vectors for user IDs
- **Prospect Theory foundations**: Behavioral economic theory modeling risk attitudes and loss aversion - needed as benchmark for comparison and theoretical grounding; quick check: verify the theory model implementation matches the described cumulative prospect theory formulation

## Architecture Onboarding

Component map: Text/Demographics/User ID -> Embedding Layer -> Concatenate with Gamble Features -> MLP -> Binary Prediction

Critical path: The representation layer (Branch A) is the most critical and novel part of the architecture. If this layer fails to produce a meaningful user embedding (e.g., due to insufficient data or irrelevant input features), the model's performance will drop to the baseline MLP level.

Design tradeoffs:
1. **User ID vs. Text vs. Demographics**: IDs are easiest but don't generalize to new users. Text generalizes better and is less privacy-sensitive but requires more processing. Demographics are a middle ground.
2. **Data-driven vs. Theory-driven**: The Beh2vec model is more flexible and can incorporate diverse data but is a "black box." The behavioral model is highly interpretable but rigid and domain-specific.
3. **Real vs. Synthetic Data**: CTGAN-generated data can help with cold-start but risks introducing artifacts not present in real human behavior.

Failure signatures:
1. **Overfitting to User IDs**: The model might memorize training user choices without learning generalizable individual differences, leading to poor performance on new users.
2. **Collapse of Text Embeddings**: If open-ended questions yield homogeneous responses, the centroid vectors will lack discriminative power.
3. **Discrepancy on Loss Trials**: If the model significantly underperforms the theory model on "loss" scenarios, it may have failed to implicitly capture loss aversion.

First 3 experiments:
1. **Reproduce the Baseline Comparison**: Train an MLP using only the 6 gamble features on the provided dataset. This establishes the performance floor without individual differences.
2. **Ablate User Representations**: Train three separate Beh2vec models using (a) user IDs, (b) demographics, and (c) text responses. Compare their test accuracy against the baseline MLP to quantify the contribution of each signal type.
3. **Compare to Theory Model**: Implement the Prospect Theory-based model described in Section 6 and compare its accuracy (overall and for gains vs. losses) against the best-performing Beh2vec model from the previous step.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on specific preprocessing choices (Word2Vec corpus, text preprocessing pipeline, regularization strategy) that are not fully specified
- Model represents a narrow application of representation learning to behavioral economics with untested generalizability to other decision-making domains
- CTGAN-based cold-start solution introduces potential artifacts from synthetic data that may not capture true human behavioral patterns

## Confidence

**High Confidence:**
- Core claim that representation learning improves prediction accuracy over baseline ML models is well-supported by reported results (3.6-4.2% improvement across models)

**Medium Confidence:**
- Superiority of text-based representations over ID and demographic representations is plausible but dependent on Word2Vec embedding quality and text preprocessing choices
- Competitive performance against theory-based model is demonstrated, though comparison methodology and behavioral model implementation details are not exhaustively detailed

## Next Checks

1. **Architecture Sensitivity Analysis**: Systematically vary the Word2Vec embedding dimension (e.g., 100, 300, 600) and FC layer sizes in the text branch to assess robustness of the ~75.7% accuracy claim.

2. **Cross-Domain Generalization Test**: Apply the Beh2vec architecture to a different behavioral decision-making dataset (e.g., consumer choice, medical decisions) to evaluate generalizability beyond the economic gambles domain.

3. **Interpretability Audit**: Use techniques like attention visualization or feature importance analysis to identify which input features (gamble characteristics, demographics, text patterns) most strongly drive the model's predictions, addressing the "black box" concern.