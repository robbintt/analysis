---
ver: rpa2
title: On the de-duplication of the Lakh MIDI dataset
arxiv_id: '2509.16662'
source_url: https://arxiv.org/abs/2509.16662
tags:
- music
- dataset
- midi
- retrieval
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of dataset duplication in the
  Lakh MIDI Dataset (LMD), a large-scale symbolic music dataset used for training
  generative models. The researchers investigate various methods to detect and remove
  duplicated MIDI files, including rule-based approaches, pre-trained symbolic music
  retrieval models, and a novel contrastive learning-based BERT model (CAugBERT) with
  MIDI augmentation.
---

# On the de-duplication of the Lakh MIDI dataset

## Quick Facts
- arXiv ID: 2509.16662
- Source URL: https://arxiv.org/abs/2509.16662
- Reference count: 0
- Primary result: Proposed method detects at least 38,134 duplicated files among 178,561 files in LMD-full with CAugBERT achieving precision > 0.9 among neural approaches

## Executive Summary
This study addresses dataset duplication in the Lakh MIDI Dataset (LMD), a large-scale symbolic music dataset used for training generative models. The researchers investigate various methods to detect and remove duplicated MIDI files, including rule-based approaches, pre-trained symbolic music retrieval models, and a novel contrastive learning-based BERT model (CAugBERT) with MIDI augmentation. They evaluate these methods using LMD-clean as a benchmark and propose three versions of filtered LMD lists to prevent data leakage in model training.

## Method Summary
The method involves training CAugBERT, a 4-layer transformer with contrastive learning and MIDI-specific augmentations, to detect duplicates in LMD. The approach uses LMD-clean as a ground truth benchmark, excludes it from training to create LMD-filtered, and then applies multiple detection methods (rule-based, CLaMP models, and CAugBERT) to LMD-full. Pairwise similarities are computed and clustered via graph algorithms to identify duplicate groups, with the file having the highest note count retained per cluster. Three filtering configurations are proposed based on different precision thresholds.

## Key Results
- CAugBERT achieved precision > 0.9, outperforming other neural approaches for duplicate detection
- Ensemble of CAugBERT and CLaMP-1024 achieved F1 = 0.548, improving performance over individual models
- Graph clustering identified 23,566 duplicate clusters containing 68,075 total files in the proposed configuration
- Conservative configuration (≥0.99 threshold) detected at least 38,134 duplicated files

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Learning with Domain-Specific Augmentations
Training a BERT model with contrastive learning using MIDI-specific augmentations improves duplicate detection by learning to recognize underlying musical identity despite surface-level differences. The augmentation set (pitch shifts, duration shifts, instrument mapping changes, track reordering) simulates real-world variations found in duplicates. CAugBERT achieved the best classification performance (precision > 0.9) among neural approaches.

### Mechanism 2: Ensemble Detection via Complementary Embedding Spaces
Combining multiple models (CLaMP-1024 + CAugBERT) yields higher F1 than either model alone because different pre-training objectives create embedding spaces that capture distinct aspects of musical similarity. CLaMP models are optimized for retrieval tasks while CAugBERT is optimized for duplicate detection via contrastive learning.

### Mechanism 3: Graph Clustering for Transitive Duplicate Grouping
Connected components in a pairwise similarity graph naturally identify complete duplicate clusters. Each file is a node and each detected duplicate pair creates an edge. Depth-first search finds connected components, each representing a duplicate cluster with the file having highest total note count retained as the canonical representative.

## Foundational Learning

- Concept: **Contrastive Learning (NT-Xent Loss)**
  - Why needed here: Core training objective for CAugBERT—pulls augmented versions of the same piece closer in embedding space while pushing different pieces apart
  - Quick check question: Given two MIDI files with identical melody but different instrument mappings, would a contrastive model trained without instrument augmentation classify them as similar?

- Concept: **Data Leakage in Train/Val/Test Splits**
  - Why needed here: The paper's primary motivation—duplicates across splits inflate validation metrics and undermine claimed generalization
  - Quick check question: If 10% of your test set appears in training as near-duplicates with minor metadata changes, how would this affect reported perplexity?

- Concept: **Hard vs. Soft Duplication**
  - Why needed here: The paper explicitly distinguishes these. Hard duplicates share identical arrangement sections with minor edits; soft duplicates share melody/harmony but differ in arrangement style
  - Quick check question: Two MIDI files of "Let It Be"—one piano-only, one full band arrangement—would this be hard or soft duplication under the paper's definition?

## Architecture Onboarding

- Component map: Octuple MIDI encoding via MidiTok → 1024-token segments → CAugBERT encoder → [CLS] embeddings → cosine similarity → threshold → graph construction → DFS clustering

- Critical path: Train CAugBERT on LMD-filtered with contrastive + MLM objectives → Extract embeddings for all LMD-full files → Compute pairwise cosine similarities → Apply precision-calibrated thresholds → Union predictions from both models → Build adjacency graph and run DFS to find duplicate clusters → Retain file with highest total note count per cluster

- Design tradeoffs:
  - Precision vs. recall: Higher thresholds reduce false positives but miss more duplicates
  - Aggressive vs. conservative filtering: Proposed config removes 68,075 files; conservative removes 38,134
  - Single model vs. ensemble: Ensemble adds computational cost but improves F1 by ~5 percentage points

- Failure signatures:
  - High false negatives on soft duplicates: Models struggle with different arrangements of the same song
  - High false positives on fugue-like or single-instrument pieces: 20.2% of retrieved items were irrelevant
  - Metadata errors in ground truth: LMD-clean contains incorrect song labels

- First 3 experiments:
  1. Run MIDI Encoding Hash on your LMD subset to establish rule-based duplicate counts before applying neural methods
  2. For your target precision level (e.g., 0.9), run CAugBERT and CLaMP-1024 on a manually annotated validation subset to find optimal similarity thresholds
  3. Train CAugBERT variants with subsets of augmentations (e.g., pitch-only, timing-only) to identify which augmentation types contribute most to duplicate detection performance

## Open Questions the Paper Calls Out

### Open Question 1
Does removing the identified duplicates from LMD measurably improve the generalization and validity of downstream symbolic music generation models? The study focused on detection methodology and providing filtered lists but did not re-train models to verify if the de-duplicated dataset leads to better convergence or lower overfitting.

### Open Question 2
Can neural methods for MIDI variation generation improve the detection of "soft duplicates" better than current rule-based augmentations? The current CAugBERT model uses simple rule-based augmentations which may not capture the complex stylistic variations inherent in soft duplicates.

### Open Question 3
How do metadata errors in the LMD-clean benchmark affect the accuracy of reported precision and recall scores? Section 6.3 notes that LMD-clean contains incorrect song labels, suggesting the ground truth contains noise that could affect evaluation metrics.

## Limitations

- Ground truth quality: LMD-clean benchmark contains errors in song labels, affecting validation accuracy
- Duplicate definition ambiguity: The boundary between hard and soft duplicates is subjective, with current methods struggling to detect soft duplicates
- Transitivity assumptions: Graph clustering assumes pairwise similarity is transitive, which may not hold in practice

## Confidence

**High Confidence**: 
- CAugBERT achieves superior performance among neural approaches with precision > 0.9
- LMD-clean contains errors that affect validation accuracy
- Ensemble of CAugBERT and CLaMP-1024 improves F1 by ~5 percentage points

**Medium Confidence**:
- 38,134 duplicated files detected in conservative configuration
- 68,075 files identified as duplicates in proposed configuration
- Graph clustering successfully identifies duplicate clusters

**Low Confidence**:
- Exact duplicate counts across all configurations (due to ground truth errors)
- Performance on soft duplicates (limited by augmentation coverage)
- Long-term stability of thresholds across different LMD subsets

## Next Checks

1. **Ground Truth Verification**: Manually audit a random sample of 100 detected duplicate pairs from the conservative configuration to verify actual duplication status, correcting for LMD-clean labeling errors and establishing true precision.

2. **Soft Duplicate Detection Test**: Create a controlled test set of 50 soft duplicate pairs (same melody, different arrangements) and evaluate CAugBERT's ability to detect these, measuring false negative rate and identifying augmentation types that would improve coverage.

3. **Threshold Stability Analysis**: Apply the conservative configuration (threshold ≥0.99) to multiple randomly sampled 10% subsets of LMD-full to measure variance in duplicate counts and verify stability of the 38,134 figure across different data partitions.