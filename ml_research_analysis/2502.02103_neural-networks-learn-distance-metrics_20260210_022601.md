---
ver: rpa2
title: Neural Networks Learn Distance Metrics
arxiv_id: '2502.02103'
source_url: https://arxiv.org/abs/2502.02103
tags:
- networks
- distance
- neural
- learning
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether neural networks have an inherent
  preference for distance-based or intensity-based representations. Through systematic
  experiments with six MNIST architectures, the authors demonstrate that networks
  exhibit strong biases in their representational choices.
---

# Neural Networks Learn Distance Metrics

## Quick Facts
- arXiv ID: 2502.02103
- Source URL: https://arxiv.org/abs/2502.02103
- Authors: Alan Oursland
- Reference count: 8
- One-line primary result: Neural networks exhibit strong biases toward distance-based representations, with architectures showing catastrophic failures when constrained to learn intensity-based representations.

## Executive Summary
This paper investigates whether neural networks have inherent preferences for distance-based or intensity-based representations. Through systematic experiments with six MNIST architectures, the authors demonstrate that networks exhibit strong biases in their representational choices. The study reveals that geometric interactions in feature space, rather than intrinsic properties of activation functions, fundamentally drive neural network representations.

The research introduces OffsetL2, a novel architecture that directly models Mahalanobis distance calculations. OffsetL2 achieved superior performance (97.61% accuracy) with remarkable stability (±0.07% standard deviation), outperforming all baseline models. These findings have significant implications for network design and interpretability, suggesting that understanding representational geometry is crucial for building effective neural networks.

## Method Summary
The authors conducted systematic experiments with six MNIST architectures, testing their performance under distance and intensity representation constraints. They introduced modified architectures (ReLU2-Neg and Abs2-Neg) to test hypotheses about representational biases. The study employed a novel OffsetL2 architecture that directly models Mahalanobis distance calculations. Performance was evaluated using standard MNIST classification accuracy, with careful attention to stability across multiple runs. The experimental design included both baseline and constrained architectures to isolate the effects of representational choices.

## Key Results
- ReLU2 architecture catastrophically failed when constrained to learn intensity representations
- Distance-constrained ReLU2-Neg recovered to baseline performance, supporting distance-based bias hypothesis
- OffsetL2 architecture achieved 97.61% accuracy with remarkable stability (±0.07% standard deviation), outperforming all baseline models

## Why This Works (Mechanism)
Neural networks learn distance metrics because their representational capacity is fundamentally shaped by geometric relationships in feature space. When networks are constrained to learn intensity-based representations, they fail catastrophically because this conflicts with their inherent geometric processing mechanisms. The success of distance-constrained architectures and the OffsetL2 model demonstrates that networks naturally optimize for spatial relationships between features rather than absolute intensity values.

## Foundational Learning
- Mahalanobis distance: A measure of distance that accounts for correlations between variables; needed to understand the OffsetL2 architecture's design and why it outperforms traditional approaches.
- Activation function constraints: How different activation functions interact with representational constraints; quick check: test ReLU, Abs, and other activations under both distance and intensity constraints.
- Geometric representations in neural networks: How networks encode spatial relationships between features; quick check: visualize feature space transformations under different constraints.

## Architecture Onboarding

**Component Map:** Input -> Convolutional Layers -> Distance/Intensity Processing -> Classification Head

**Critical Path:** The core computation path involves feature extraction through convolutional layers, followed by distance or intensity calculation, then classification. The critical decision point is whether the network processes geometric relationships or absolute intensities.

**Design Tradeoffs:** Distance-based representations offer better generalization and stability but may require more complex computations. Intensity-based approaches are simpler but show catastrophic failures under certain constraints, suggesting fundamental limitations.

**Failure Signatures:** Catastrophic performance drops (near-zero accuracy) when architectures are constrained to incompatible representational modes, particularly evident in ReLU2 under intensity constraints.

**First Experiments:**
1. Test baseline ReLU2 and Abs2 architectures on MNIST without constraints
2. Apply distance and intensity constraints to both architectures and measure performance degradation
3. Implement and evaluate OffsetL2 architecture on MNIST

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on MNIST dataset limits generalizability to more complex real-world applications
- Six architectures represent a narrow slice of possible network designs
- Mechanisms underlying representational biases remain incompletely characterized

## Confidence
High: Geometric representational bias in tested architectures and dataset
Medium: Universality of these biases across different network families
Low: Current theoretical frameworks explaining activation function-constraint interactions

## Next Checks
1. Replicate distance/intensity constraint experiments on CIFAR-10 and ImageNet to assess scalability and dataset dependency
2. Test additional activation functions (GELU, SiLU, Swish) under identical constraints to map the full landscape of representational biases
3. Implement and evaluate OffsetL2 on tabular and sequential data to determine if Mahalanobis-based architectures generalize beyond image classification