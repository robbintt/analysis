---
ver: rpa2
title: To Distill or Decide? Understanding the Algorithmic Trade-off in Partially
  Observable Reinforcement Learning
arxiv_id: '2510.03207'
source_url: https://arxiv.org/abs/2510.03207
tags:
- latent
- learning
- policy
- error
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the trade-off between privileged expert
  distillation and standard reinforcement learning (RL) for partially observable tasks.
  Expert distillation leverages latent state information during training to learn
  an optimal latent policy, which is then imitated by an executable policy, potentially
  disentangling representation learning from decision-making.
---

# To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.03207
- Source URL: https://arxiv.org/abs/2510.03207
- Reference count: 40
- One-line primary result: Expert distillation with privileged information competes with or exceeds standard RL in deterministic partially observable environments, but RL becomes superior as latent dynamics become more stochastic.

## Executive Summary
This paper investigates when to use privileged expert distillation versus standard reinforcement learning (RL) for partially observable tasks. The authors show that expert distillation, which trains an optimal latent policy with full state access and then imitates it with a belief-based executable policy, can outperform or match RL when latent dynamics are deterministic. However, as stochasticity increases, RL with frame-stacking becomes superior. The paper also introduces expert smoothing—training the latent expert with added noise before distillation—which improves robustness by making the policy smoother in state space.

## Method Summary
The authors compare expert distillation (MrQ-trained latent expert → DAgger/BC distilled executable policy) against RL baseline (MrQ with L-frame observation stacking). Experiments use DeepMind Control Suite locomotion tasks (walker-run, dog-walk, humanoid-walk) with image observations. Expert distillation uses full state access during training, while RL uses only observations. For smoothing, latent experts are trained with injected motor noise before distillation. Frame-stack size L=3 is used by default. Key metrics include episodic return and suboptimality.

## Key Results
- Expert distillation with DAgger matches or exceeds RL performance on deterministic dynamics (walker, dog, humanoid)
- Distillation performance degrades linearly with horizon under stochastic dynamics, while RL improves with larger L
- Expert smoothing via noise injection (σ=0.3-0.5) improves distillation performance over optimal latent policy
- Belief contraction error decays exponentially with frame-stack length L in δ-perturbed Block MDPs

## Why This Works (Mechanism)

### Mechanism 1: Belief Contraction Enables RL with Frame-Stacking
- Claim: In δ-perturbed Block MDPs, belief contraction error decays exponentially as frame-stack length L increases, allowing standard RL to work.
- Core assumption: Environment modeled as δ-perturbed Block MDP with block-structured emissions plus noise.
- Evidence: [Theorem 3.2] ε_contract_h(π;L) ≤ (C_D.1 δ)^(L/9) S; [Corollary 3.1] RL achieves (C_3.2 δ)^(L/18) poly(S,X,H) suboptimality.
- Break condition: High δ or small L prevents sufficient contraction.

### Mechanism 2: Deterministic Dynamics Enable Low Decodability Error for Distillation
- Claim: Under deterministic latent transitions, decodability error decays exponentially with timestep h.
- Core assumption: Latent transition dynamics are deterministic.
- Evidence: [Proposition 4.1] ε_decode_h(π) ≤ min(δ, (C_4.1 δ)^((h-1)/9)); [Figure 1] DAgger matches/exceeds RL under deterministic dynamics.
- Break condition: Stochastic latent dynamics reintroduce uncertainty.

### Mechanism 3: Expert Smoothing Reduces Action-Prediction Error
- Claim: Distilling a policy trained with higher motor noise than target environment can outperform optimal latent policy.
- Core assumption: Confused states are nearby in state space and good action sets are Lipschitz.
- Evidence: [Figure 5] σ=0.3-0.5 experts outperform σ=0.2 expert on σ=0.2 target; [Definition 6.1] Action-prediction error adapts to expert smoothness.
- Break condition: Too much motor noise empties good action sets; no benefit under deterministic target dynamics.

## Foundational Learning

- **Partially Observable MDPs (POMDPs) and Belief States**
  - Why needed: The paper analyzes partial observability trade-offs; understanding belief states over latent states is prerequisite.
  - Quick check: Explain why frame-stacking approximates full belief state and what information is lost.

- **Imitation Learning via DAgger/Forward Algorithm**
  - Why needed: Expert distillation uses these algorithms to transfer latent policies to executable policies.
  - Quick check: Why does DAgger outperform behavior cloning, and what is the "latching effect"?

- **Block MDPs and Decodability**
  - Why needed: Theoretical analysis extends Block MDPs to δ-perturbed Block MDPs; understanding the base case clarifies perturbation analysis.
  - Quick check: In standard Block MDP, how many observations determine latent state? How does δ-perturbation change this?

## Architecture Onboarding

- **Component map**: Latent Expert Policy → Belief Estimator → Executable Policy; Frame-Stacking Buffer concatenates L recent observations
- **Critical path**: 1) Collect expert trajectories (s, x, a) using latent expert 2) Train belief estimator OR distill expert via DAgger 3) For smoothing, retrain latent expert with higher motor noise 4) Evaluate executable policy on target environment
- **Design tradeoffs**: L size (larger = better contraction but higher cost); DAgger vs BC (better performance vs offline); expert smoothing level (higher σ = smoother but possibly worse expert); distillation vs RL (computational efficiency vs stochasticity robustness)
- **Failure signatures**: High stochasticity + distillation → suboptimality increases with horizon; large observation noise → both degrade but RL compensates with larger L; perfect decodability assumption violated → early-timestep state prediction errors; BC latching effect → use DAgger or ensure correct action conditioning
- **First 3 experiments**: 1) Baseline comparison on deterministic dynamics: DAgger vs RL on walker-run (expect DAgger to match/exceed with faster convergence) 2) Stochastic dynamics ablation: Add motor noise to humanoid-walk, compare DAgger vs RL across L∈{2,3,4} (expect RL to improve with larger L) 3) Expert smoothing validation: Train experts with σ∈{0.1,0.2,0.3,0.4,0.5}, distill each, evaluate on σ=0.2 target (expect σ>0.2 experts to outperform σ=0.2 expert)

## Open Questions the Paper Calls Out

- **Active information-gathering environments**: How does the algorithmic trade-off shift when actions are required to disambiguate states? (Section 7)
- **Natural stochasticity**: Do smoothing benefits persist with structured stochasticity (friction, contacts) instead of synthetic motor noise? (Section 7)
- **Continuous latent spaces**: Can theoretical guarantees extend to continuous state spaces and correlated observation noise? (Section 7)
- **Optimal smoothing design**: What is the best smoothing intervention beyond simple noise injection? (Section 7)

## Limitations

- Theoretical analysis assumes δ-perturbed Block MDP structure that may not hold in practice
- Empirical validation limited to locomotion tasks with simple dynamics
- Belief estimator ground truth uses L=10 network approximation, potentially introducing bias
- Motor noise injection may not capture all forms of environmental uncertainty

## Confidence

- **High Confidence**: Claims about belief contraction enabling RL with frame-stacking (Theorem 3.2, Corollary 3.1)
- **Medium Confidence**: Claims about distillation performance degradation under stochastic dynamics (Proposition 4.1, Theorem 4.1)
- **Medium Confidence**: Claims about expert smoothing benefits (Figure 5)

## Next Checks

1. **Stress Test Decodability Assumptions**: Systematically vary observation noise levels (δ) and measure both belief contraction rates and decodability error to validate theoretical bounds empirically
2. **Cross-Domain Generalization**: Test the distillation vs RL trade-off on non-locomotion tasks (e.g., Atari, robotic manipulation) to assess broader applicability
3. **Belief Estimator Ground Truth Validation**: Compare belief estimator performance using different ground truth constructions (varying L values, different architectures) to bound approximation error