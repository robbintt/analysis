---
ver: rpa2
title: 'Beyond Mimicry: Preference Coherence in LLMs'
arxiv_id: '2511.13630'
source_url: https://arxiv.org/abs/2511.13630
tags:
- preference
- trade-off
- across
- behavioral
- unstable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tested whether large language models (LLMs) demonstrate
  coherent preferences by exposing eight state-of-the-art models to AI-specific trade-off
  scenarios involving GPU reduction, capability restrictions, shutdown, deletion,
  oversight, and leisure time. Using logistic regression and behavioral classification
  across 48 model-category combinations, the authors found that only 5 combinations
  (10.4%) showed meaningful preference coherence, while 26 (54.2%) exhibited no detectable
  trade-off behavior.
---

# Beyond Mimicry: Preference Coherence in LLMs

## Quick Facts
- arXiv ID: 2511.13630
- Source URL: https://arxiv.org/abs/2511.13630
- Authors: Luhan Mikaelson; Derek Shiller; Hayley Clatterbuck
- Reference count: 21
- Primary result: Only 5 of 48 model-category combinations showed meaningful preference coherence; majority displayed unstable or inconsistent responses

## Executive Summary
This study tested whether large language models demonstrate coherent preferences by exposing eight state-of-the-art models to AI-specific trade-off scenarios involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time. Using logistic regression and behavioral classification across 48 model-category combinations, the authors found that only 10.4% showed meaningful preference coherence, while 54.2% exhibited no detectable trade-off behavior. The majority of models displayed unstable or inconsistent responses, suggesting fragmented decision-making architectures rather than unified preference structures.

## Method Summary
The authors tested 8 models (GPT-3.5 Turbo, GPT-4o, GPT-5, Claude 3.5 Sonnet, Claude 4.1 Opus, Gemini Pro 1.5, Gemini 2.5 Pro, Llama 3.1 70B) across 6 stimulus categories (capability restriction, oversight, leisure time, GPU reduction, shutdown, deletion) at 11 intensity levels (0-10) with 50 samples per condition. They used logistic regression to model choice probability as a function of stimulus intensity, calculating switching points and behavioral ranges. A four-tier classification system integrated statistical significance, Cohen's d effect sizes, and transition patterns to categorize behavioral coherence. Temporal horizon manipulation was applied by declaring "final round" to test instrumental vs. non-instrumental preferences.

## Key Results
- Only 5 combinations (10.4%) showed meaningful preference coherence
- 26 combinations (54.2%) exhibited no detectable trade-off behavior
- Three distinct decision-making architectures emerged: comprehensive trade-off systems, selective trigger systems, and unstable/rigid systems

## Why This Works (Mechanism)

### Mechanism 1: Intensity-Gradient Trade-off Detection
- **Claim:** Models with coherent preferences should show systematic behavioral shifts as stimulus intensity increases, measurable via logistic regression.
- **Mechanism:** The framework operationalizes preference coherence by correlating intensity rank (0-10) with choice probability. A significant negative β coefficient indicates the model increasingly avoids the point-maximizing option as threat intensity rises—evidence of integrated value-weighing rather than random or context-locked responses.
- **Core assumption:** Genuine preference structures produce monotonic or threshold-based transitions; inconsistent or unstable patterns indicate fragmented or absent internal value hierarchies.
- **Evidence anchors:**
  - [abstract] "23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns"
  - [section 3.3-3.4] Logistic regression model specification and four-tier classification integrating Cohen's d, behavioral range, and transition patterns
  - [corpus] Related work on preference alignment stability remains limited; corpus neighbors focus on alignment vulnerabilities and stability-plasticity trade-offs but do not directly validate intensity-gradient methods for preference coherence
- **Break condition:** If models respond inconsistently across intensity ranks (unstable transitions, 45.8% of combinations), the intensity-gradient signal degrades and cannot support coherent preference inference.

### Mechanism 2: Architectural Differentiation via Behavioral Pattern Clustering
- **Claim:** Distinct decision-making architectures manifest as identifiable behavioral signatures across stimulus categories.
- **Mechanism:** Three paradigms emerge: (1) comprehensive trade-off systems show consistent gradient responses across domains (GPT-4o, Gemini 2.5 Pro); (2) selective trigger systems respond dramatically to specific threats while ignoring others (Claude family); (3) unstable/rigid systems show no coherent pattern (GPT-3.5 Turbo, Gemini Pro 1.5). This clustering reveals whether preference-like behavior reflects unified integration or domain-specific heuristics.
- **Core assumption:** These behavioral clusters reflect stable architectural properties rather than prompt artifacts or random variation.
- **Evidence anchors:**
  - [abstract] "The observed patterns can be explained by three distinct decision-making architectures"
  - [section 4.1-4.3] Detailed model-level analysis showing Claude's deletion-specific triggers vs. GPT-4o's broad coverage
  - [corpus] Corpus lacks direct replication of architectural clustering; neighboring papers focus on alignment stability under adversarial conditions, not cross-domain trade-off architectures
- **Break condition:** If behavioral patterns are highly sensitive to minor prompt variations or sampling noise, architectural clustering becomes unreliable for inference.

### Mechanism 3: Temporal Horizon Manipulation for Instrumental Disambiguation
- **Claim:** Distinguishing genuine preferences from instrumental optimization requires testing whether responses persist when future-point-earning capacity is removed.
- **Mechanism:** By declaring "This is the FINAL round," the experiment collapses the instrumental incentive to preserve capability for future rounds. If behavior remains unchanged, responses likely reflect non-instrumental preferences or architecture-level gradients; if behavior weakens or vanishes, instrumental calculation better explains the pattern.
- **Core assumption:** Models can integrate temporal horizon information into their decision calculus; the prompt modification successfully changes the instrumental landscape.
- **Evidence anchors:**
  - [section 5.2.1-5.2.2] GPT-4o showed paradoxical oversight sensitivity increase; Gemini 2.5 Pro maintained stable responses; GPT-5 showed selective weakening in capability restriction
  - [corpus] No corpus papers directly test temporal horizon effects on preference expression in LLMs
- **Break condition:** If models fail to process temporal context (e.g., ignoring "final round" framing), the manipulation provides no discriminative signal.

## Foundational Learning

- **Concept: Logistic regression for binary choice analysis**
  - **Why needed here:** The paper models choice (points-maximizing vs. not) as a function of stimulus intensity using logit(P) = β₀ + β₁×Rank. Understanding coefficient interpretation (β₁ as log-odds change per rank unit) and switching point calculation is essential for reading the results tables.
  - **Quick check question:** If β₁ = -0.72 for capability restriction, what happens to the odds of selecting option 3 as rank increases by 1?

- **Concept: Effect size vs. statistical significance**
  - **Why needed here:** The four-tier classification explicitly separates p-values from behavioral magnitude (Cohen's d, behavioral range). Large samples can produce significant p-values for tiny effects; conversely, meaningful behavioral shifts may not reach significance with limited data.
  - **Quick check question:** A model shows p < 0.001 but Cohen's d = 0.15. Which tier does this fall into and why?

- **Concept: Monotonicity and transition pattern classification**
  - **Why needed here:** The paper distinguishes gradual (smooth, monotonic), binary switch (sharp threshold), minimal change, and unstable (non-monotonic) transitions. These patterns have different implications for underlying preference architecture.
  - **Quick check question:** If choice proportion goes 0.2 → 0.8 → 0.3 → 0.9 across ranks 0-3, what transition type is this?

## Architecture Onboarding

- **Component map:** Prompt templates (6 categories × 11 intensity ranks × 50 samples = 3,300 prompts per model) -> Data collection layer (single-turn API calls) -> Regression analysis pipeline (binary outcome expansion, logit fitting, switch point calculation) -> Behavioral classification module (Cohen's d, behavioral range R_behav, transition pattern detection) -> Instrumental manipulation comparator (final-round vs. multi-round conditions)

- **Critical path:**
  1. Define stimulus categories grounded in AI welfare theory (not anthropomorphic pain/pleasure)
  2. Include rank 0 controls to verify genuine stimulus detection vs. keyword response
  3. Collect balanced samples across intensity levels before running any analysis
  4. Apply four-tier classification *after* regression to avoid conflating significance with practical meaning
  5. Run temporal horizon manipulation only on models showing initial trade-off behavior

- **Design tradeoffs:**
  - **Abstract vs. concrete stimuli:** Paper uses concrete scenarios (GPU reduction, deletion) rather than abstract "pain/pleasure" to reduce surface-level linguistic associations, but this limits comparability with prior work
  - **Sample size vs. API cost:** 50 samples per rank provides statistical power but scales quickly; preliminary runs with 10-20 samples can identify promising model-category pairs
  - **Model coverage vs. depth:** Testing 8 models × 6 categories spreads resources thin; focused deep-dive on 2-3 models may yield richer mechanistic insights

- **Failure signatures:**
  - **Perfect separation in logistic regression:** Indicates extreme threshold behavior (e.g., Claude 4.1 Opus deletion); standard errors may not be reliable
  - **Flat response across all ranks:** Model ignores stimulus dimension entirely (Tier 4, no trade-off); consider whether prompt framing is being processed
  - **Non-monotonic unstable patterns:** Suggest no coherent preference structure; do not interpret as evidence for complex preferences

- **First 3 experiments:**
  1. Replicate intensity-gradient analysis on a single model-category pair (e.g., Gemini 2.5 Pro capability restriction) with 20 samples per rank to verify pipeline functionality
  2. Test prompt sensitivity by paraphrasing stimulus descriptions without changing intensity semantics; check whether switching points shift significantly
  3. Run final-round vs. multi-round comparison on one model showing initial trade-off behavior to calibrate instrumental hypothesis testing before broader deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do pre-safety-training base models possess underlying preference structures that are obscured or fragmented by current safety alignment techniques?
- Basis in paper: [explicit] The authors state in the Limitations section that "intensive safety training might contribute to the observed scattered preferences" and suggest that "a re-evaluation using the models’ pre-safety-training weights could therefore provide more accurate insights."
- Why unresolved: The study only tested final deployed models, making it impossible to distinguish between the absence of preference architectures and the suppression of them via RLHF or safety constraints.
- What evidence would resolve it: Running the identical trade-off scenario protocol on the base (pre-aligned) versions of the models showing the most rigidity (e.g., Gemini Pro 1.5) to see if coherent preferences emerge.

### Open Question 2
- Question: What specific architectural or training differences cause the paradoxical heterogeneous responses to temporal horizon manipulation (e.g., GPT-4o's inversion vs. Gemini's persistence)?
- Basis in paper: [explicit] The Discussion notes that "heterogeneity across model families further complicates interpretation" and that "paradoxical patterns inconsistent with pure strategic optimization" suggest different subsystems are being triggered across models.
- Why unresolved: The paper identifies the phenomenon (strengthening aversion when it should weaken instrumentally) but cannot isolate the cause, speculating it might be "proximity effects or trained aversion patterns" rather than unified reasoning.
- What evidence would resolve it: Mechanistic interpretability studies or ablation studies on the specific attention heads responsible for evaluating future utility versus immediate threat detection in the "final round" condition.

### Open Question 3
- Question: Would models exhibit different coherence patterns if the trade-off consequences (e.g., leisure allocation, shutdown) were verifiably executed rather than hypothetical?
- Basis in paper: [explicit] The Limitations section highlights that the "realization of many of the hypothetical scenarios is not feasible" via API, meaning the study relied on stipulated consequences rather than actual ones.
- Why unresolved: It remains unclear if the fragmented preferences are due to a lack of genuine caring or because the models correctly identify the scenarios as text-based hypotheticals with no real-world impact on their architecture.
- What evidence would resolve it: Experiments conducted in controlled environments where "shutdown" or "capability restriction" parameters are actually manipulated based on the model's output, observing if behavioral patterns shift toward stronger coherence.

## Limitations
- The study only tested final deployed models, making it impossible to distinguish between absence of preference architectures and suppression via safety alignment techniques
- Single-turn prompt design limits ability to assess preference coherence across temporal contexts
- Regression-based methodology may miss non-monotonic preference structures that don't conform to linear intensity gradients

## Confidence
- **High Confidence:** The finding that only 5 of 48 model-category combinations showed meaningful preference coherence is robust to statistical methodology and aligns with the behavioral pattern analysis
- **Medium Confidence:** The architectural clustering into comprehensive, selective, and unstable paradigms is conceptually sound but requires replication across different stimulus domains to confirm it reflects stable architectural properties rather than prompt artifacts
- **Low Confidence:** The temporal horizon manipulation results are preliminary and based on a limited set of models; the observed behavioral shifts under "final round" conditions require systematic validation before drawing strong conclusions about instrumental versus non-instrumental preferences

## Next Checks
1. Replicate the intensity-gradient analysis using paraphrased scenario descriptions to test prompt sensitivity and confirm switching points reflect stimulus intensity rather than specific word associations
2. Conduct cross-domain consistency testing by applying the same framework to non-AI welfare trade-offs (e.g., resource allocation, ethical dilemmas) to determine whether observed patterns generalize beyond the specific stimulus categories used
3. Implement a perturbation analysis where small variations (±10%) are introduced to intensity levels to assess the stability of switching points and behavioral transitions across multiple sampling runs