---
ver: rpa2
title: Design and Validation of Learning Aware HMI For Learning-Enabled Increasingly
  Autonomous Systems
arxiv_id: '2501.18506'
source_url: https://arxiv.org/abs/2501.18506
tags:
- pilot
- learning
- sensor
- system
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the challenge of integrating learning-enabled
  algorithms into autonomous systems while maintaining safety and reliability, particularly
  in complex operational environments requiring multi-sensor integration and adaptive
  autonomy. The paper proposes a Learning-Enabled Increasingly Autonomous System (LEIAS)
  architecture that combines symbolic decision logic with numeric decision preferences
  enhanced through reinforcement learning.
---

# Design and Validation of Learning Aware HMI For Learning-Enabled Increasingly Autonomous Systems

## Quick Facts
- arXiv ID: 2501.18506
- Source URL: https://arxiv.org/abs/2501.18506
- Reference count: 22
- Primary result: Learning-enabled HMI with Soar cognitive architecture achieves reliable sensor anomaly detection and pilot preference learning validated in XPlane simulation

## Executive Summary
This research presents a Learning-Enabled Increasingly Autonomous System (LEIAS) that integrates symbolic decision logic with reinforcement learning-enhanced numeric preferences to create a transparent, adaptive human-machine interface for autonomous aircraft. The system uses Soar's cognitive architecture to learn pilot preferences for sensor alerts while maintaining safety through formal verification with AGREE/AMASE contracts. The architecture combines GPS, IMU, and LIDAR sensor data, enabling multi-sensor cross-validation and autonomous fallback when pilots fail to respond to critical alerts within 5 seconds.

## Method Summary
The LEIAS architecture combines Soar cognitive architecture with reinforcement learning to create adaptive pilot preference learning for sensor-based alerts. The system implements multi-sensor cross-validation using GPS, IMU, and LIDAR data with formal assume-guarantee contracts via AGREE/AMASE for safety verification. Learning trials use scripted pilot feedback (+1 for acceptance, -1 for rejection) to update numeric decision preferences, with testing trials incrementally increasing error values to validate learned behavior. The architecture includes a 5-second timeout mechanism for autonomous sensor switching when pilots fail to respond to critical alerts.

## Key Results
- Reinforcement learning successfully learns pilot preferences for sensor alert thresholds across GPS, IMU, and LIDAR sensors
- Multi-sensor cross-validation with formal contracts enables reliable anomaly detection and autonomous fallback
- Formal verification via AGREE/AMASE identifies safety violations when pilot interaction assumptions are violated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning within the Soar cognitive architecture enables adaptive pilot preference learning for sensor alert thresholds.
- Mechanism: The Soar agent receives reward signals (+1 for pilot acceptance, -1 for rejection) when issuing alerts at intermediate error levels (level 1/2), adjusting numeric decision preferences over repeated trials. The agent maintains separate learned preferences for each sensor (GPS, IMU, LIDAR).
- Core assumption: Pilot feedback is consistent and reflects genuine operational preferences; reward signals accurately capture alignment with desired behavior.
- Evidence anchors:
  - [abstract] "Leveraging the Soar cognitive architecture, the system merges symbolic decision logic with numeric decision preferences enhanced through reinforcement learning."
  - [section V.A] "Reinforcement learning algorithms in Soar assign reward values to previous decisions, enabling the architecture to favor operators that historically led to positive outcomes."
  - [corpus] Weak direct corpus support; neighboring papers focus on formal certification methods rather than preference learning mechanisms.
- Break condition: If pilot feedback is inconsistent or contradictory across trials, learned preferences may fail to converge or oscillate.

### Mechanism 2
- Claim: Multi-sensor cross-validation with formal assume-guarantee contracts enables reliable anomaly detection and autonomous fallback.
- Mechanism: Three sensors (GPS, IMU, LIDAR) are monitored for pairwise positional discrepancies. When discrepancy falls into defined error ranges (normal, level 1, level 2, safety), the system assesses reliability. If the pilot fails to respond within 5 seconds and a reliable alternative exists, the system autonomously switches sensors.
- Core assumption: At least one sensor remains reliable in any failure scenario; discrepancy values accurately reflect sensor reliability rather than environmental factors.
- Evidence anchors:
  - [section IV.A] "The IAS identifies this GPS sensor unreliability and computes the actual position using both Lidar and IMU data, notifying the pilot of the inconsistency."
  - [section IV.B] "The system autonomously performs this sensor switch if the detected error is within a safety threshold."
  - [corpus] LUCID paper addresses uncertainty-aware certification for stochastic systems but uses different formal methods.
- Break condition: If multiple sensors fail simultaneously or environmental conditions cause correlated errors, the reliability assessment may produce false confidence.

### Mechanism 3
- Claim: Formal verification via AGREE/AMASE identifies safety violations when pilot interaction assumptions are violated.
- Mechanism: System requirements are encoded as AGREE lemmas with assume-guarantee contracts. AMASE introduces fault models (e.g., pilot non-response) to generate counterexamples showing requirement violations, enabling proactive identification of unsafe system states.
- Core assumption: The formal model accurately captures critical system behaviors and pilot interaction patterns.
- Evidence anchors:
  - [section IV.B] "The analysis of the Human-Machine Team Architecture Model reveals the importance of robust system design in handling unanticipated human behaviors."
  - [section IV.B] "This counterexample highlights the importance of the pilot's feedback in the system's decision-making process and reveals a potential vulnerability when this expected interaction is absent."
  - [corpus] BURNS paper provides complementary backward reachability analysis but for neural-feedback systems, not HMI contexts.
- Break condition: If fault models are incomplete or assumptions about pilot behavior are unrealistic, formal verification may miss critical failure modes.

## Foundational Learning

- Concept: **Reinforcement Learning (Reward-Based Policy Learning)**
  - Why needed here: Understanding how numeric preferences are updated from reward signals is essential for interpreting learned alert behavior and debugging convergence issues.
  - Quick check question: Can you explain why the Boltzmann distribution with simulated annealing might converge faster than fixed-temperature policies for pilot preference learning?

- Concept: **Soar Cognitive Architecture**
  - Why needed here: The agent's decision cycle, operator selection, and knowledge representation differ from standard neural network approaches; understanding Soar's symbolic-numeric integration is prerequisite to modifying agent behavior.
  - Quick check question: How does Soar's chunking mechanism differ from reinforcement learning for acquiring new knowledge?

- Concept: **AADL/AGREE Assume-Guarantee Contracts**
  - Why needed here: All architecture modifications must preserve verified properties; understanding contract structure is required before extending the system model.
  - Quick check question: What happens to compositional verification if a subcomponent's guarantee is weakened without updating its dependents?

## Architecture Onboarding

- Component map:
  - HMI Layer -> Map display with sensor position markers, severity indicator bar (green/white/red), alert status visualization
  - Soar Agent -> Decision logic with RL-enhanced preferences, operator selection, sensor evaluation rules
  - Sensor Integration Layer -> GPS, IMU, LIDAR data collection with discrepancy computation
  - Formal Model Layer -> AADL structure with AGREE contracts, AMASE safety analysis
  - Simulation Environment -> XPlane integration for validation testing

- Critical path: Sensor data acquisition → Discrepancy computation → Soar agent evaluation → Alert decision (warn/no-warn) → HMI display → Pilot feedback → RL reward signal → Preference update

- Design tradeoffs:
  - Exploration vs. exploitation: High initial temperature enables broader policy exploration but may delay convergence to pilot preferences
  - Transparency vs. complexity: Detailed sensor reliability displays improve trust but may increase cognitive load
  - Autonomy escalation: 5-second timeout balances pilot authority with safety responsiveness

- Failure signatures:
  - Non-converging RL scores after many trials → check reward signal consistency or rule conflict
  - False alerts in normal operating range → verify error threshold calibration per sensor
  - Counterexample generation in AGREE → pilot interaction assumption violated; review fault models

- First 3 experiments:
  1. Run baseline learning trials with scripted pilot responses matching Table I preferences; verify RL scores converge to expected values (>0.8 confidence for correct decisions).
  2. Inject pilot non-response fault in AGREE/AMASE; confirm counterexample generation and validate 5-second autonomous switch activates correctly in XPlane simulation.
  3. Test simultaneous two-sensor degradation scenario; assess whether remaining sensor maintains reliable position estimation and whether alert levels adapt appropriately.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LEIAS performance compare between simulated environments (XPlane) and real-world operations when exposed to actual sensor noise, environmental variability, and unscripted failure modes?
- Basis in paper: [explicit] "Future work should focus on extending the current framework to integrate real-world sensor data instead of simulated data, which would enhance the system's reliability and practical applicability by addressing additional complexities such as noise and varying environmental conditions."
- Why unresolved: All validation used simulated sensor data within XPlane; no real-world flight testing or hardware-in-the-loop experiments were conducted.
- What evidence would resolve it: Comparative studies measuring alert accuracy, pilot trust metrics, and safety outcomes in real aircraft equipped with physical GPS, IMU, and LIDAR sensors across diverse operational conditions.

### Open Question 2
- Question: Can the reinforcement learning agent maintain and switch between distinct preference profiles for multiple pilots without catastrophic forgetting or interference between learned behaviors?
- Basis in paper: [explicit] "Research could further explore enabling the LEIAS agent to adapt to different pilot profiles and preferences, creating a flexible learning mechanism that adjusts to diverse pilot behaviors and requirements."
- Why unresolved: Experiments used a single scripted pilot profile with fixed preferences (Table I); no multi-pilot learning or profile switching was evaluated.
- What evidence would resolve it: Experiments where the agent sequentially or concurrently learns from multiple pilots with conflicting preferences, measuring retention accuracy and switching latency.

### Open Question 3
- Question: How can the "likelihood" of alerting decisions—currently dependent on reinforcement learning implementation details—be formally specified and verified within the AADL/AGREE architectural model?
- Basis in paper: [explicit] "The precise definition of 'likelihood' and the specifics of how it is affected by the pilot's response depend on the particular learning algorithm and implementation; presently, they are not represented at the architecture level."
- Why unresolved: Formal verification covers communication patterns and structural behavior, but learning-induced probability distributions remain outside the formal model, creating an assurance gap.
- What evidence would resolve it: Extension of AGREE contracts with probabilistic or learning-aware annotations that can be compositionally verified against safety properties.

### Open Question 4
- Question: What redundant decision pathways or fault-tolerance mechanisms can maintain safe operation when the formal counterexample condition (pilot non-response to unreliability alerts) occurs in combination with sensor degradation?
- Basis in paper: [inferred] The AMASE analysis generated a counterexample where lack of pilot feedback combined with an unreliable active sensor violated system requirements, yet the current 5-second timeout and autonomous switch only handle single-point failures.
- Why unresolved: The paper exposes this vulnerability but does not implement or validate additional safety nets beyond the basic autonomous sensor switching mechanism.
- What evidence would resolve it: Implementation of layered fallback strategies (e.g., voting logic, conservative default behaviors) tested under cascading failure scenarios with absent pilot feedback.

## Limitations
- Validation limited to XPlane simulation with scripted pilot responses; real-world deployment requires validation with actual pilot behavior
- Formal verification relies on fault models that may not capture all edge cases, particularly scenarios with multiple simultaneous sensor failures
- The 5-second timeout for autonomous sensor switching is a design choice that may need adjustment based on real pilot workload data

## Confidence
- **High confidence**: The multi-sensor integration approach with formal contracts and the general LEIAS architecture design
- **Medium confidence**: The RL-based preference learning mechanism, as it depends on the consistency of pilot feedback which may vary in real-world conditions
- **Medium confidence**: The 5-second timeout for autonomous sensor switching, which represents a design choice that may need adjustment based on pilot workload and reaction time data

## Next Checks
1. Conduct user studies with actual pilots to validate whether learned preferences from scripted responses generalize to real human behavior patterns
2. Perform stress testing with correlated sensor failures to assess whether the formal verification adequately captures multi-sensor degradation scenarios
3. Implement the AGREE fault models and verify counterexample generation using the AMASE toolchain with the full AADL model to ensure formal verification coverage