---
ver: rpa2
title: Knowledge-Driven Federated Graph Learning on Model Heterogeneity
arxiv_id: '2501.12624'
source_url: https://arxiv.org/abs/2501.12624
tags:
- knowledge
- learning
- graph
- client
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedGKC tackles model-heterogeneity in federated graph learning
  by introducing a lightweight Copilot Model to bridge architectural gaps across clients.
  It uses Self-Mutual Knowledge Distillation to transfer knowledge bidirectionally
  between local and copilot models, while Server-side Knowledge-Aware Model Aggregation
  dynamically weights clients based on knowledge quality.
---

# Knowledge-Driven Federated Graph Learning on Model Heterogeneity

## Quick Facts
- **arXiv ID**: 2501.12624
- **Source URL**: https://arxiv.org/abs/2501.12624
- **Reference count**: 40
- **One-line primary result**: FedGKC achieves 3.88% average accuracy gain over baselines in heterogeneous federated graph learning, and outperforms homogeneous methods by 1.07% in standard settings.

## Executive Summary
FedGKC addresses the challenge of model heterogeneity in federated graph learning by introducing a lightweight Copilot Model that bridges architectural gaps across clients. The framework employs Self-Mutual Knowledge Distillation for bidirectional knowledge transfer between local and copilot models, enriched with neighborhood topology alignment and perturbation-based consistency. Server-side Knowledge-Aware Model Aggregation dynamically weights client contributions based on knowledge quality metrics, achieving superior performance in both heterogeneous and homogeneous federated learning scenarios while maintaining communication efficiency.

## Method Summary
FedGKC introduces a unified framework for federated graph learning across heterogeneous client architectures. Each client maintains a local GNN (potentially different architecture) and a lightweight shared Copilot Model. During local training, bidirectional knowledge distillation transfers structural and feature knowledge between models, with neighborhood alignment ensuring topology-aware transfer. The server aggregates only Copilot parameters using quality-weighted averaging based on client knowledge strength and clarity metrics. This approach enables standard parameter averaging despite architectural diversity, balancing communication efficiency with strong performance gains.

## Key Results
- FedGKC achieves 3.88% average accuracy improvement over state-of-the-art baselines in heterogeneous federated graph learning settings
- Outperforms specialized homogeneous methods by 1.07% in standard federated learning scenarios
- Demonstrates robustness across eight benchmark datasets including Cora, CiteSeer, and ogbn-arxiv

## Why This Works (Mechanism)

### Mechanism 1: Lightweight Copilot Model as Unified Knowledge Carrier
A shared lightweight Copilot Model across heterogeneous clients enables consistent knowledge exchange without requiring architectural homogeneity. Each client maintains two models: a heterogeneous local GNN and a uniform lightweight Copilot (e.g., 2-layer GCN). Only the homogeneous Copilot parameters are uploaded to the server for aggregation, allowing standard parameter averaging despite client architectural diversity. The aggregated global Copilot is redistributed to inject global knowledge into heterogeneous local models. The core assumption is that the Copilot is sufficiently expressive to capture essential structural and feature knowledge from diverse local GNNs without creating an information bottleneck.

### Mechanism 2: Self-Mutual Knowledge Distillation (SMKD) for Topology-Aware Transfer
Bidirectional knowledge distillation enriched with neighborhood alignment effectively transfers structural graph knowledge and improves local model robustness. SMKD operates bidirectionally: the local model distills its specific knowledge to the Copilot, while the Copilot injects global knowledge back into the local model. Crucially, the distillation loss aligns the embeddings of a node's neighbors, not just the node's own logits. A self-distillation component further regularizes the local model by enforcing consistency between strongly and weakly perturbed views of the same local graph, ensuring the knowledge distilled to the Copilot is robust.

### Mechanism 3: Knowledge-Aware Model Aggregation (KAMA) for Quality-Weighted Integration
Dynamically weighting client contributions based on the quality of learned knowledge (strength and clarity) improves global convergence over data-volume-only weighting. The server computes a knowledge score for each client based on the uploaded Copilot's embeddings. This score combines "Knowledge Strength" (prediction confidence) and "Knowledge Clarity" (distinctiveness of classes, penalizing over-smoothing). The final aggregation weight blends this quality score with data volume, giving well-informed, clear clients more influence in the global update.

## Foundational Learning

- **Graph Neural Networks (GNNs) and Message Passing**: The framework is built on GNNs, and the key innovation (SMKD) relies on aligning neighborhood representations. Understanding message passing is critical to see why topology-aware distillation is necessary. *Quick check*: How does a 2-layer GCN update a node's representation using its 2-hop neighborhood?

- **Knowledge Distillation (KD)**: The core of the client-side learning is bidirectional KD. Understanding "teacher-student" models and how matching soft probabilities transfers relational knowledge is essential. *Quick check*: How does a "soft" label (e.g., `[0.8, 0.1, 0.1]`) convey more information to a student model than a hard label (`[1, 0, 0]`)?

- **Federated Learning (FL) and Aggregation**: The server-side KAMA mechanism modifies standard FL aggregation. Understanding FedAvg (averaging parameters) is the baseline for evaluating KAMA's design. *Quick check*: In standard FedAvg, how does the server compute the new global model parameter `θ_g` based on client parameters `θ_k`?

## Architecture Onboarding

- **Component map**: Server -> Broadcasts global Copilot -> Clients (Local Model + Copilot Model) -> SMKD Trainer + Perturbation Module + Knowledge Evaluator -> Uploads Copilot parameters + Knowledge metrics -> KAMA Aggregator -> Server

- **Critical path**: 
  1. Server broadcasts global Copilot parameters to selected clients
  2. Clients jointly train Local and Copilot models using SMKD on private graphs
  3. Clients compute knowledge score for their updated Copilot
  4. Clients upload Copilot parameters and score to server
  5. Server aggregates Copilot parameters using KAMA
  6. Repeat until convergence

- **Design tradeoffs**:
  - **Copilot Capacity**: Simpler Copilot eases aggregation/communication but risks information bottleneck
  - **Bidirectional KD**: Ensures Copilot learns local data while Local learns global knowledge, but early Copilot errors could harm local training
  - **KAMA Complexity**: Quality-weighting improves fairness under heterogeneity but adds computational overhead for metric calculation

- **Failure signatures**:
  - **Representation Collapse**: Copilot embeddings converge to a single point due to excessive distillation pressure, making knowledge scores meaningless
  - **Training Instability**: KAMA weights fluctuate wildly, preventing the global model from converging
  - **Negative Transfer**: Self-distillation on noisy data causes the model to learn perturbations rather than robust features

- **First 3 experiments**:
  1. **Baseline Test (Homogeneous)**: Run FedGKC with all clients having the same architecture vs. FedAvg to check if SMKD/KAMA overhead is justified even without heterogeneity
  2. **Ablation: KAMA vs. FedAvg**: Replace server-side KAMA with standard FedAvg in a heterogeneous setting to isolate performance gain from quality-aware weighting
  3. **Ablation: Distillation Direction**: Compare (a) No distillation, (b) Local→Copilot only, and (c) Full bidirectional SMKD to validate the necessity of the complex bidirectional setup

## Open Questions the Paper Calls Out
- Refining mechanisms to further balance efficiency and accuracy in resource-constrained cross-device scenarios
- Extending the framework to handle more complex graph structures and larger-scale federated environments
- Investigating the theoretical convergence properties of the combined SMKD and KAMA optimization framework

## Limitations
- The Copilot Model's capacity to capture complex knowledge from diverse, larger local GNNs remains unproven without ablation studies on varying Copilot sizes
- Quality metrics (strength/clarity) may not reliably correlate with actual generalization performance, potentially amplifying errors from high-confidence wrong models
- Hyperparameter values for perturbation rates, hidden dimensions, and learning rates are assumed rather than specified, affecting reproducibility

## Confidence

- **High Confidence**: The framework's core architecture (Copilot + SMKD + KAMA) is internally consistent and the claimed 3.88% average accuracy improvement over baselines is supported by presented experiments
- **Medium Confidence**: The bidirectional knowledge distillation mechanism's effectiveness for graph topology transfer, as the neighborhood alignment component lacks direct corpus validation
- **Low Confidence**: The generalizability of quality metrics as proxies for knowledge value, and the framework's robustness to extreme heterogeneity scenarios not covered in experiments

## Next Checks

1. **Copilot Capacity Stress Test**: Systematically vary the Copilot Model size (1-4 layers) while keeping local models fixed to identify the information bottleneck threshold where performance degrades significantly

2. **Quality Metric Validation**: Create scenarios where a high-confidence but wrong model exists among clients, then measure if KAMA weights amplify this error versus standard FedAvg aggregation

3. **Architecture Transfer Robustness**: Test FedGKC when clients have highly dissimilar architectures (e.g., GCN vs. GAT vs. GIN with vastly different message-passing mechanisms) to validate the Copilot's ability to bridge fundamental architectural gaps