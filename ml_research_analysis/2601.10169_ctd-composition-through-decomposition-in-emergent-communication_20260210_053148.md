---
ver: rpa2
title: 'CtD: Composition through Decomposition in Emergent Communication'
arxiv_id: '2601.10169'
source_url: https://arxiv.org/abs/2601.10169
tags:
- communication
- codebook
- concepts
- compositionality
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Composition through Decomposition (CtD),
  a novel two-step training approach that enables artificial neural agents to acquire
  compositional generalization in emergent communication. The method first trains
  agents in a multi-target coordination game to decompose images into basic concepts
  using a discrete codebook, then leverages this learned codebook to compose descriptions
  of novel images by combining known concepts.
---

# CtD: Composition through Decomposition in Emergent Communication

## Quick Facts
- arXiv ID: 2601.10169
- Source URL: https://arxiv.org/abs/2601.10169
- Reference count: 40
- Primary result: Achieves perfect compositionality and accuracy across multiple datasets using a two-step decomposition-composition training approach

## Executive Summary
CtD (Composition through Decomposition) is a novel approach to emergent communication that enables artificial neural agents to acquire compositional generalization by learning to decompose complex concepts before composing them. The method trains agents in a multi-target coordination game to break down images into basic concepts using a discrete codebook, then leverages this learned codebook to compose descriptions of novel images by combining known concepts. The approach achieves perfect accuracy and compositionality scores on synthetic datasets (THING, SHAPE) and strong performance on MNIST (81% accuracy, 92% AMI, 95% CBM), significantly outperforming existing communication protocols while enabling zero-shot generalization in the composition phase.

## Method Summary
CtD employs a two-step training process where agents first learn to decompose complex objects into basic concepts through a multi-target coordination game, then use this learned decomposition to compose descriptions of novel images. The method uses a discrete latent codebook with Gumbel-softmax for differentiable quantization, combined with a multi-loss optimization framework that includes reconstruction, classification, and mutual information objectives. The training alternates between decomposition (identifying concepts) and composition (combining concepts), with a hyperparameter λ controlling the balance between these phases. The discrete codebook enables robust concept learning and compositionality, while the multi-target game structure forces agents to identify and represent individual concepts separately.

## Key Results
- Achieves 100% accuracy and CBM scores on THING dataset
- Achieves 99% accuracy, perfect CBM and CI scores on SHAPE dataset
- Achieves 81% accuracy, 92% AMI, and 95% CBM on MNIST dataset
- Demonstrates zero-shot generalization in composition phase without additional training
- Significantly outperforms Gumbel-softmax and Quantized communication protocols

## Why This Works (Mechanism)
The CtD approach succeeds because it addresses the fundamental challenge of emergent communication: agents must learn to represent and communicate complex concepts in a way that enables generalization to novel combinations. By first forcing agents to decompose objects into basic concepts through multi-target games, CtD establishes a shared vocabulary of atomic concepts that can then be reliably composed. The discrete codebook provides a stable, interpretable representation that supports systematic composition, while the multi-loss optimization ensures that both individual concept recognition and overall reconstruction quality are maintained. The two-phase training structure allows agents to first establish concept boundaries before learning how to combine them effectively.

## Foundational Learning
- **Compositional Generalization**: The ability to understand and produce novel combinations of known components - needed because neural networks typically struggle with systematic generalization beyond training data
- **Emergent Communication**: The process by which artificial agents develop their own communication protocols through interaction - needed to study how language-like systems can emerge without human supervision
- **Discrete Latent Representations**: Using discrete rather than continuous latent variables for concept encoding - needed to create interpretable, stable building blocks for composition
- **Multi-target Coordination Games**: Games where agents must coordinate on multiple targets simultaneously - needed to force decomposition of complex concepts into constituent parts
- **Gumbel-Softmax Relaxation**: A differentiable approximation of discrete sampling - needed to enable gradient-based training with discrete latent variables
- **Mutual Information Maximization**: Optimizing for statistical dependence between concepts and representations - needed to ensure concepts are properly captured in the codebook

## Architecture Onboarding

**Component Map**: Sender -> Encoder -> Discrete Codebook -> Gumbel-Softmax -> Communication Channel -> Receiver -> Decoder -> Reconstruction

**Critical Path**: The sender encodes images into discrete concepts, which are transmitted through a communication channel to the receiver, who reconstructs the image or identifies concepts. The discrete codebook and Gumbel-softmax form the core of concept representation and transmission.

**Design Tradeoffs**: Discrete codebooks provide interpretability and stability but require careful temperature scheduling in Gumbel-softmax; multi-target games improve decomposition but increase computational complexity; two-phase training enables systematic composition but requires hyperparameter tuning.

**Failure Signatures**: Poor decomposition in phase one leads to incorrect concept identification; insufficient codebook capacity results in concept collision; improper λ balance causes either over-decomposition or poor composition; communication channel noise degrades concept transmission.

**First Experiments**: 1) Train on single-concept images to verify basic concept learning; 2) Test decomposition accuracy on held-out concept combinations; 3) Evaluate zero-shot composition performance on novel concept combinations.

## Open Questions the Paper Calls Out
None

## Limitations
- Discrete codebooks may struggle with continuous or highly nuanced concepts beyond tested datasets
- Performance gap on MNIST (81% accuracy) suggests limitations with complex, real-world scenarios
- Reliance on pre-defined target sets constrains ability to discover truly novel concepts independently
- Evaluation metrics may not fully capture practical utility in real-world applications

## Confidence
- **High Confidence**: CtD achieves superior compositionality and accuracy compared to existing communication protocols
- **Medium Confidence**: Multi-target games are essential for concept decomposition
- **Medium Confidence**: Zero-shot generalization capability in composition phase

## Next Checks
1. Evaluate CtD on larger, more complex datasets with thousands of concepts and real-world images to assess scalability limits
2. Test system performance under adversarial conditions including noisy inputs, concept overlap, and ambiguous scenarios
3. Investigate transfer learning capabilities to measure concept retention and adaptation in cross-domain scenarios