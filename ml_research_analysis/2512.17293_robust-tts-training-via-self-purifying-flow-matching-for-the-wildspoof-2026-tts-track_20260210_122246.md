---
ver: rpa2
title: Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026
  TTS Track
arxiv_id: '2512.17293'
source_url: https://arxiv.org/abs/2512.17293
tags:
- flow
- matching
- spfm
- supertonic
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training robust text-to-speech
  (TTS) systems using noisy, in-the-wild speech data by introducing a self-purifying
  flow matching approach. The core method, Self-Purifying Flow Matching (SPFM), dynamically
  detects and mitigates label noise during training by comparing conditional and unconditional
  flow matching losses on each sample, routing unreliable text-speech pairs to unconditional
  training while preserving acoustic information.
---

# Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track

## Quick Facts
- arXiv ID: 2512.17293
- Source URL: https://arxiv.org/abs/2512.17293
- Reference count: 0
- Primary result: Achieved lowest WER among all teams (5.50% seen, 5.88% unseen speakers)

## Executive Summary
This paper introduces Self-Purifying Flow Matching (SPFM) to address the challenge of training robust text-to-speech systems on noisy, in-the-wild speech data. SPFM dynamically detects and mitigates label noise during training by comparing conditional and unconditional flow matching losses on each sample, routing unreliable text-speech pairs to unconditional training while preserving acoustic information. Built on the lightweight Supertonic TTS architecture, the system achieves the lowest Word Error Rate among all participating teams while maintaining competitive perceptual quality, demonstrating effective noise mitigation without compromising naturalness.

## Method Summary
The method fine-tunes a pre-trained Supertonic TTS model using Self-Purifying Flow Matching (SPFM) on WildSpoof Challenge datasets. SPFM compares conditional and unconditional flow matching losses per sample: when the conditional loss exceeds the unconditional loss, the sample is routed to unconditional training only, preserving acoustic information without corrupting text-speech alignment. The system uses a balanced mix of clean and noisy data (TITW-easy and TITW-hard subsets) with 1:1 batch sampling, activates SPFM after a 1,000-step warm-up period, and trains for 10,000 iterations with batch size 32.

## Key Results
- Achieved lowest WER among all teams: 5.50% (seen speakers), 5.88% (unseen speakers)
- Ranked second in perceptual quality metrics: UTMOS and DNSMOS
- Demonstrated effective noise mitigation without compromising naturalness
- Successfully handled challenging in-the-wild speech data with label noise

## Why This Works (Mechanism)

### Mechanism 1: Loss-Based Label Reliability Detection
SPFM compares conditional and unconditional flow matching losses to detect misaligned text-speech pairs. When `L_cond > L_uncond`, the conditioning signal (text) is degrading rather than helping prediction, indicating unreliable alignment. The sample is routed to unconditional training only, preventing corrupted text-speech mappings from being learned.

### Mechanism 2: Acoustic Preservation Through Unconditional Routing
Noisy samples flagged as unreliable are routed to unconditional training rather than discarded entirely. This allows the model to learn acoustic features (noise patterns, speaker characteristics, environmental conditions) without propagating incorrect text-to-acoustic mappings, maintaining acoustic diversity while preventing alignment corruption.

### Mechanism 3: Warm-Up Stabilization
SPFM activation is delayed until after 1,000 training steps to prevent premature filtering of valid samples. Early training produces unreliable loss ratios that don't reflect true label quality, so the warm-up period allows the model to develop reasonable flow field estimates before loss comparisons become meaningful.

## Foundational Learning

- **Conditional Flow Matching (CFM)**: Required to understand SPFM's operation within the CFM framework, including velocity fields, interpolation paths, and conditioning. Quick check: Can you explain why `v_θ(x_t, t, c)` predicts the direction from noise `x_0` to data `x_1` conditioned on text `c`?

- **Classifier-Free Guidance (CFG)**: Essential for understanding how SPFM leverages CFG's dual conditional/unconditional formulation, where ∅ (null conditioning) is used during training and inference. Quick check: During inference, how does CFG combine conditional and unconditional predictions to control generation fidelity?

- **Label Noise in Sequence Alignment**: Critical for understanding why TTS alignment is sensitive to text-speech mismatches (transcription errors, timing drift) and why misalignment corrupts duration and acoustic modeling. Quick check: What happens to a TTS model when the transcript says "hello" but the audio contains "hi there"?

## Architecture Onboarding

- **Component map**: Input Text → Duration Predictor → Flow Matching Generator → Speech Autoencoder Decoder → Audio; SPFM Router intercepts training loop, evaluating both losses per-sample and deciding routing.

- **Critical path**: Start from pre-trained Supertonic checkpoint → Load TITW-easy and TITW-hard with 1:1 batch sampling → For each training step after warm-up: compute `L_cond` and `L_uncond` per sample; route accordingly → Fine-tune for 10,000 iterations with batch size 32.

- **Design tradeoffs**: WER vs. Perceptual Quality (system prioritizes alignment correctness over acoustic refinement), Compactness vs. Capacity (lightweight architecture limits expressive power; SPFM compensates by preventing noise corruption), Routing Threshold (paper uses binary comparison; margin-based threshold could be more robust).

- **Failure signatures**: High WER despite low training loss (may indicate over-aggressive filtering or insufficient warm-up), Speaker identity drift (if unconditional training dominates), Unstable convergence (if interpolation time is poorly chosen).

- **First 3 experiments**: Ablate warm-up duration (compare activation at 0, 500, 1,000, 2,000 steps on validation WER), Routing threshold sweep (test margin-based thresholds), Dataset composition study (vary TITW-easy:TITW-hard ratios to understand noise prevalence effects).

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved based on the presented work and its limitations.

## Limitations
- Reproducibility challenges due to unspecified interpolation time $t'$ beyond "near midpoint"
- Limited generalizability demonstrated only on WildSpoof Challenge datasets
- Potential tradeoff between WER improvement and perceptual quality degradation
- No ablation studies on SPFM routing sensitivity or warm-up duration optimization

## Confidence

- **High confidence**: WER improvement claims are well-supported by official WildSpoof Challenge evaluation results; basic SPFM mechanism is clearly described and grounded in cited SPFM origin paper.
- **Medium confidence**: Acoustic preservation mechanism is theoretically sound but lacks direct ablation studies; claim about improving intelligibility without compromising naturalness is supported by secondary metrics but shows tradeoff between rankings.
- **Low confidence**: Warm-up duration choice appears arbitrary without empirical justification; routing mechanism's sensitivity to dataset noise characteristics is not characterized; paper does not explore alternative threshold strategies.

## Next Checks

1. **Ablate warm-up duration**: Compare SPFM activation at 0, 500, 1,000, 2,000 steps on validation WER to find minimum stable warm-up that prevents spurious filtering while maximizing noise mitigation.

2. **Routing threshold sweep**: Replace binary comparison with `L_cond > L_uncond + margin` for margin ∈ {-0.1, 0, 0.1, 0.2} to assess sensitivity and determine if margin-based thresholds outperform hard thresholds.

3. **Dataset composition study**: Vary TITW-easy:TITW-hard ratios (1:3, 1:1, 3:1) to understand how noise prevalence affects SPFM effectiveness and identify when unconditional routing becomes detrimental to text fidelity.