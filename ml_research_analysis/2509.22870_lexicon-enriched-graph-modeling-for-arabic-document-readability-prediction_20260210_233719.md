---
ver: rpa2
title: Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction
arxiv_id: '2509.22870'
source_url: https://arxiv.org/abs/2509.22870
tags:
- readability
- arabic
- graph
- sentence
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a lexicon-enriched graph modeling approach
  for Arabic document readability prediction, developed for the BAREC Shared Task
  2025. The method represents documents as heterogeneous sentence-lemma graphs enriched
  with SAMER lexicon features and contextual embeddings from a fine-tuned AraBERTv2
  model.
---

# Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction

## Quick Facts
- arXiv ID: 2509.22870
- Source URL: https://arxiv.org/abs/2509.22870
- Authors: Passant Elchafei; Mayar Osama; Mohamed Rageh; Mervat Abuelkheir
- Reference count: 4
- One-line primary result: Hybrid graph+transformer model achieves 76.9% QWK and 42.0% accuracy at document level, outperforming standalone models

## Executive Summary
This paper introduces a lexicon-enriched graph neural network approach for Arabic document readability prediction, developed for the BAREC Shared Task 2025. The method constructs heterogeneous sentence-lemma graphs enriched with SAMER lexicon features and contextual embeddings from a fine-tuned AraBERTv2 model. A graph neural network and transformer encoder are trained independently and combined via late fusion at inference, with document-level predictions obtained by max-pooling sentence-level outputs. Results demonstrate that the hybrid approach outperforms standalone models at the document level while the GNN-only model remains stronger for sentence-level prediction.

## Method Summary
The approach represents documents as heterogeneous graphs with sentence and lemma nodes connected by typed edges capturing lexical composition and co-occurrence patterns. Each lemma is matched to the SAMER lexicon to retrieve readability, frequency, and POS attributes as node features. The graph is processed through a 4-layer GraphSAGE network, while a separate AraBERTv2 transformer encodes contextual information. Both branches are trained independently and their predictions combined via weighted late fusion at inference. Document-level predictions are derived by max-pooling sentence-level outputs.

## Key Results
- Hybrid model achieves 76.9% Quadratic Weighted Kappa at document level, outperforming standalone GNN (75.6%) and transformer (74.3%) approaches
- Document-level accuracy reaches 42.0% versus 40.0% for GNN-only and 39.0% for transformer-only
- GNN-only model maintains superiority for sentence-level prediction (50.0% accuracy vs 41.4% for transformer-only)
- Late fusion improves document-level metrics while potentially diluting sentence-level discriminative power

## Why This Works (Mechanism)

### Mechanism 1
Heterogeneous graph encoding captures linguistic relationships that sequence models miss. The graph explicitly represents typed edges—HAS_LEMMA (sentence→lemma), OCCUR_WITH (lemma↔lemma), IN_CLASS (sentence→class), and IN_DOMAIN (sentence→domain)—enabling the GNN to propagate difficulty signals through structural relationships rather than treating text as a flat sequence. Core assumption: Lexical composition and co-occurrence patterns carry discriminative readability signal that contextual embeddings alone under-utilize. Break condition: If target sentences contain domain-agnostic vocabulary with weak co-occurrence patterns correlated to difficulty.

### Mechanism 2
Lexicon enrichment injects expert-curated difficulty knowledge orthogonal to learned embeddings. Each extracted lemma is matched to the SAMER lexicon to retrieve average readability, frequency, and POS tags; these statistical attributes are encoded directly as node features before GNN propagation. Core assumption: SAMER's graded vocabulary annotations correlate with actual text readability in the target domain. Break condition: If texts contain substantial vocabulary outside SAMER coverage, or if readability is driven by syntax rather than lexical difficulty.

### Mechanism 3
Late fusion of independently trained branches preserves specialization while capturing complementary signals. The GNN (structural/lexical) and transformer (contextual) branches train separately on the same classification objective; their softmax outputs are combined with a tunable weight at inference, avoiding gradient interference during optimization. Core assumption: GNN and transformer learn non-redundant features—structure vs. context—that synergize at prediction time. Break condition: If both branches converge to similar representations, or if systematic prediction conflicts emerge.

### Mechanism 4
Max-pooling over sentence predictions approximates document readability via the "hardest sentence" heuristic. Document-level labels are derived by selecting the maximum predicted difficulty across constituent sentences, based on the intuition that comprehension is bottlenecked by the most complex segment. Core assumption: Document readability equals its most difficult sentence, not an average across sentences. Break condition: If documents have heterogeneous difficulty distributions where average better reflects perceived readability.

## Foundational Learning

- Concept: Heterogeneous Graph Neural Networks
  - Why needed here: The model uses multiple node types (sentences, lemmas, classes, domains) with type-specific message passing; standard GNNs assume homogeneous graphs.
  - Quick check question: Can you explain why separate weight matrices per edge type are necessary in a heterogeneous graph?

- Concept: GraphSAGE (SAGE-Conv)
  - Why needed here: The architecture uses 4 SAGE-Conv layers with neighbor sampling and aggregation.
  - Quick check question: How does GraphSAGE's inductive learning differ from transductive GCN approaches for unseen node inference?

- Concept: Late Fusion vs. Early Fusion
  - Why needed here: The design deliberately trains branches independently, combining only predictions—not features.
  - Quick check question: What optimization risks does early fusion introduce that late fusion avoids?

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: Primary evaluation metric; accounts for ordinal structure across 19 readability levels.
  - Quick check question: Why is QWK preferable to raw accuracy for fine-grained ordinal classification?

## Architecture Onboarding

- Component map: Preprocessing (CAMeL Tools Morphology Analyzer → lemma extraction → SAMER matching) → Graph construction (4 node types, 4 directed edge types) → Feature projection (Linear layer) → GNN branch (4× SAGE-Conv layers + ReLU + LayerNorm + residual connections) → Transformer branch (AraBERTv2 fine-tuned) → Fusion (Weighted sum of softmax outputs) → Aggregation (Max-pooling over sentence predictions → document label)

- Critical path: Lemma extraction quality → SAMER alignment coverage → node feature richness → GNN message passing → fusion weight calibration

- Design tradeoffs:
  1. GNN-only stronger for sentence-level (50.0% vs 41.4%); fusion better for document-level (42.0% vs 40.0%)—optimize per task granularity.
  2. Max-pooling assumes bottleneck semantics; mean/weighted pooling may suit different document types.
  3. Independent training avoids cross-branch interference but foregoes joint feature learning.

- Failure signatures:
  1. Sentence accuracy >> document accuracy: Check max-pooling logic and sentence-document mapping.
  2. GNN-only consistently beats fusion: Fusion weight likely misconfigured; re-calibrate α.
  3. High OOV lemma rate: SAMER coverage insufficient; graph edges carry weak signal.
  4. High QWK / low accuracy: Model captures ordinal trends but fails exact classification.

- First 3 experiments:
  1. Establish baselines: Run GNN-only and transformer-only on both sentence and document tasks to quantify individual contributions.
  2. Fusion weight sweep: Grid search α ∈ {0.3, 0.4, 0.5, 0.6, 0.7} to find optimal GNN/transformer balance.
  3. Aggregation ablation: Compare max-pooling vs. mean-pooling vs. learned attention-weighted aggregation at document level.

## Open Questions the Paper Calls Out

### Open Question 1
Would alternative document aggregation strategies, such as attention-based pooling or mean pooling, outperform the max-pooling heuristic? Basis in paper: [explicit] The authors justify using max-pooling based on "the intuition that the most complex sentence may determine the document's comprehensibility floor" (Section 4), but provide no comparison to other aggregation methods. Why unresolved: The experimental setup tests only the max-pooling approach, leaving the possibility that a more nuanced aggregation of sentence difficulties could improve document-level metrics. What evidence would resolve it: An ablation study comparing document-level Quadratic Weighted Kappa scores using max, mean, and attention-weighted pooling strategies.

### Open Question 2
Can early or cross-attention fusion architectures prevent the dilution of discriminative power observed in sentence-level prediction? Basis in paper: [explicit] The authors note that while fusion helps document-level tasks, "the fusion approach may dilute some of the GNN's discriminative power for exact classification" at the sentence level (Section 4). Why unresolved: The study relies on late fusion of independently trained models, leaving unexplored whether deeper integration of the GNN and transformer branches could retain sentence-level accuracy. What evidence would resolve it: Comparative experiments using joint training or cross-attention mechanisms to see if they maintain high sentence-level accuracy (currently 50.0% for GNN-only) while matching document-level performance.

### Open Question 3
How robust is the graph model's performance when applied to documents with high rates of out-of-lexicon vocabulary? Basis in paper: [inferred] The system relies on matching lemmas to the SAMER lexicon to assign statistical attributes (Section 3), but does not analyze performance on texts where lemmas are missing from the lexicon. Why unresolved: It is unclear if the model fails gracefully or if the GNN's reliance on explicit lexical features creates brittleness when encountering rare or domain-specific words not present in SAMER. What evidence would resolve it: An error analysis plotting prediction accuracy against the percentage of out-of-lexicon words per document.

## Limitations

- Reliance on SAMER lexicon coverage without OOV analysis or ablation studies
- Max-pooling heuristic for document aggregation untested against alternatives
- Late fusion weight assumed optimal rather than systematically derived
- No comparison of document-level performance across different aggregation strategies

## Confidence

- **High Confidence**: The complementary nature of GNN and transformer performance at different granularities (sentence vs. document) is well-supported by the reported metrics.
- **Medium Confidence**: The lexicon enrichment's contribution is plausible given the mechanism described, but lacks direct ablation evidence in the paper.
- **Low Confidence**: The SAMER lexicon's coverage and the max-pooling heuristic's appropriateness across diverse document types are assumed but not empirically validated.

## Next Checks

1. **Lexicon Coverage Analysis**: Quantify the percentage of extracted lemmas matched to SAMER and assess performance degradation when excluding OOV lemmas from the graph.
2. **Document Aggregation Ablation**: Systematically compare max-pooling against mean-pooling and learned attention-based aggregation for document-level prediction.
3. **Fusion Weight Calibration**: Perform a rigorous grid search or Bayesian optimization over fusion weights to determine if the reported values are optimal or locally optimal.