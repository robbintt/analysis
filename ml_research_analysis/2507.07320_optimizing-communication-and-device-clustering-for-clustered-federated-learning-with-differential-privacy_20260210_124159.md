---
ver: rpa2
title: Optimizing Communication and Device Clustering for Clustered Federated Learning
  with Differential Privacy
arxiv_id: '2507.07320'
source_url: https://arxiv.org/abs/2507.07320
tags:
- users
- learning
- user
- task
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimizing communication and
  device clustering in clustered federated learning (CFL) systems that incorporate
  differential privacy (DP). The key challenges include joint optimization of user
  scheduling, resource block (RB) allocation, and DP noise levels to minimize training
  loss while ensuring privacy and meeting communication constraints.
---

# Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy

## Quick Facts
- **arXiv ID:** 2507.07320
- **Source URL:** https://arxiv.org/abs/2507.07320
- **Reference count:** 40
- **One-line primary result:** Dynamic Penalty Function Assisted Value Decomposition Multi-Agent Reinforcement Learning (DPVD-MARL) improves convergence rate by up to 20% and accumulated rewards by 15% compared to independent Q-learning.

## Executive Summary
This paper addresses the challenge of optimizing communication and device clustering in clustered federated learning (CFL) systems with differential privacy (DP). The authors propose a distributed reinforcement learning approach where base stations independently optimize user scheduling and resource allocation while jointly minimizing global training loss. A novel dynamic penalty function assigns penalties proportional to constraint violations, accelerating valid action discovery. The method combines gradient-direction similarity with loss values for task identity determination, achieving 96% clustering accuracy after 15 iterations while reducing communication overhead.

## Method Summary
The proposed DPVD-MARL framework enables distributed base stations to independently determine connected users, allocated resource blocks, and DP noise levels while jointly minimizing training loss. Each BS runs a local Q-network and shares Q-values with neighbors to approximate the global Q-function via value decomposition. Task identities are determined using a weighted combination of loss values and gradient similarity, then users add DP noise to gradients before transmitting local models. Resource block allocation uses the Hungarian algorithm while DP noise optimization employs convex programming. A dynamic penalty scheme assigns penalties proportional to the number of users violating communication constraints, guiding RL agents toward feasible regions and improving convergence speed.

## Key Results
- DPVD-MARL improves convergence rate by up to 20% and accumulated rewards by 15% compared to independent Q-learning
- The method achieves up to 35% faster convergence and 27% higher accumulated rewards compared to baselines
- Clustering accuracy reaches 96% after 15 iterations, reducing downlink communication overhead by allowing BSs to broadcast only subset of task models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributed base stations can independently optimize user scheduling and resource allocation while jointly minimizing global training loss across all tasks.
- Mechanism: Value decomposition approximates the global Q-function as a linear sum of local Q-functions: $Q_{tot}(s,a) = \sum_b Q_{\theta_b}(s_b, a_b)$. This allows each BS to train using only local observations plus aggregated Q-values from neighbors, rather than requiring full joint state-action information.
- Core assumption: The reward function is decomposable (team reward = sum of individual contributions), and local Q-function approximation error decreases during training.
- Evidence anchors:
  - [abstract] "DPVD-MARL enables distributed BSs to independently determine their connected users, RBs, and DP noise... while jointly minimize training loss across all BSs."
  - [Section 4.1] "Qtot(s(t), a(t)) = Σ_b Q_θb(sb(t), ab(t))... if one BS wants to obtain the global Q function, it only needs to know the local Q functions of other BSs."
  - [corpus] Related CFL works (DPMM-CFL, EBS-CFL) use centralized clustering; this distributed approach is differentiated but comparative validation against distributed baselines is limited.
- Break condition: If reward structure becomes non-additive (e.g., complex inter-BS interference patterns requiring global coordination beyond sum-decomposition), convergence guarantees may not hold.

### Mechanism 2
- Claim: Penalty scaling proportional to constraint violations accelerates valid action discovery compared to fixed large penalties.
- Mechanism: Invalid actions receive penalty $-M^2 \times 10^3$ where $M$ is the number of violating users, rather than a fixed penalty. This creates a gradient signal that guides exploration toward feasible regions rather than simply rejecting all invalid actions equally.
- Core assumption: RL agents can learn the relationship between action modifications and constraint satisfaction through the penalty magnitude signal.
- Evidence anchors:
  - [abstract] "penalty assignment scheme that assigns penalty depending on the number of devices that cannot meet communication constraints... guides the RL algorithm to quickly find valid actions."
  - [Fig. 5] "our scheme can improve convergence rate by up to 25%, and reduce the variance by at least 80% after 260 epochs" compared to fixed-penalty baseline.
  - [corpus] No direct comparison in related CFL literature found; this mechanism appears novel to this work.
- Break condition: If constraint satisfaction requires coordinated multi-BS actions that no single penalty signal can decompose, agents may oscillate near feasibility boundaries.

### Mechanism 3
- Claim: Combining gradient-direction similarity with loss values for task identity determination improves clustering accuracy over loss-only methods.
- Mechanism: Task identity $\epsilon_i(t) = \arg\max_k [\lambda S_{i,k}(t) + (1-\lambda)(-L_{i,k}^t)]$ where $S_{i,k}(t)$ measures cosine similarity between local gradient and global model update direction $\Delta w_k(t)$. This captures both current loss and alignment with optimization trajectory.
- Core assumption: Devices with similar data distributions exhibit similar gradient directions relative to model updates; gradient variance parameter $\zeta_2 < 1/4$ ensures convergence (Proposition 1).
- Evidence anchors:
  - [Section 2.1] "Different from prior works that use only loss values... we consider not only loss values but also the gradient descent direction."
  - [Fig. 7] "proposed method improves clustering accuracy by up to 5% compared to baseline that only uses training loss" and reaches 96% accuracy after 15 iterations.
  - [corpus] Standard CFL (Ghosh et al.) uses loss-based clustering; gradient-based clustering is also seen in some related works but without the combined weighted formulation.
- Break condition: If $\zeta_2 \geq 1/4$ (high local gradient variance), convergence guarantee fails; or if model updates become near-zero (late training), gradient similarity becomes uninformative.

## Foundational Learning

- Concept: Value Decomposition in Multi-Agent RL
  - Why needed here: Enables distributed BS decision-making while maintaining global coordination. Without understanding Q-function decomposition, the distributed optimization would appear incoherent.
  - Quick check question: Can you explain why summing local Q-functions approximates global value without requiring full joint state information?

- Concept: Differential Privacy (Gaussian mechanism, zCDP)
  - Why needed here: Privacy-utility tradeoff is central to problem formulation. Noise variance $\sigma_i$ directly affects convergence bound via $d^2\sigma_{max}^2 / (2L)$ term.
  - Quick check question: Given privacy budget $\rho_i = 2(M/(X_i\sigma_i))^2$, how does increasing dataset size $X_i$ affect the noise required for the same privacy level?

- Concept: Non-convex Optimization Convergence Analysis
  - Why needed here: Theorem 2 extends convergence bounds to non-convex loss functions common in deep learning, with additional $\eta(1-4\zeta_2)$ term capturing non-convexity effects.
  - Quick check question: What condition on $\zeta_2$ ensures the convergence gap doesn't explode, and what does $\zeta_2$ represent physically?

## Architecture Onboarding

- Component map: Users -> Local model update -> BS aggregation -> Global model broadcast -> Users
- Critical path:
  1. BSs broadcast current global models $w_k^G(t)$ for all tasks
  2. Each user computes losses and gradients for all K models, determines task identity via Eq. (4)
  3. Each BS observes state $s_b(t)$, selects user association action $a_b(t)$ via $\epsilon$-greedy
  4. Given actions, solve RB allocation (Hungarian) then DP noise (convex optimization)
  5. Compute reward $R(a|s)$, share local Q-values, update Q-networks via Eq. (35)
  6. Users upload models, BSs aggregate and share to form updated global models

- Design tradeoffs:
  - **Privacy vs. convergence**: Higher $\gamma$ prioritizes lower DP noise (better privacy) but increases training loss (Fig. 8 shows inflection around $\gamma = 10^7$)
  - **Action space reduction vs. optimality**: Decoupling user association from RB/DP decisions reduces action space but may miss jointly optimal solutions
  - **Clustering speed vs. accuracy**: Lower $\lambda$ (more loss-weighted) clusters faster but less accurately; $\lambda = 0.2$ outperforms $\lambda = 0.5$

- Failure signatures:
  - **Divergent Q-values**: If Q-network approximation error doesn't decrease, Lemma 1 convergence conditions fail; symptoms include unstable rewards after 200+ epochs
  - **Clustering oscillation**: If gradient directions become unstable (late training with small updates), task identities may flip; mitigation is to fix clustering after 15 iterations when accuracy reaches 96%
  - **Constraint deadlock**: All users repeatedly violating delay/rate constraints; indicates RB allocation insufficient for current user load—reduce connected users per BS

- First 3 experiments:
  1. **Validate convergence with synthetic convex loss**: Implement CFL with MSE loss, verify that training loss follows Theorem 1 upper bound; vary $\zeta_2$ to confirm $< 1/4$ requirement. This isolates RL from FL dynamics.
  2. **Ablate penalty scheme**: Compare fixed-penalty vs. dynamic-penalty versions on same network topology; measure epochs to first valid action, reward variance, and final convergence value. Reproduce Fig. 5.
  3. **Stress test user scaling**: Run DPVD-MARL with U = {6, 12, 24, 48} users, R = 4 RBs; plot CFL loss vs. users (extend Fig. 4); identify breaking point where Hungarian algorithm becomes bottleneck or clustering accuracy degrades.

## Open Questions the Paper Calls Out
- The authors suggest extending the proposed CFL framework for scenarios where user data distributions change over time
- Future work will study the use of analog transmission techniques such as over-the-air computation (AirComp) to further improve the FL model transmission efficiency

## Limitations
- The distributed Q-value decomposition assumes additive rewards, but inter-BS interference patterns may require more complex coordination mechanisms beyond simple summation
- The gradient-based task identification relies on the assumption that gradient directions remain informative throughout training, which may break down during late-stage convergence when model updates become small
- Privacy-utility tradeoff optimization assumes accurate knowledge of the zCDP parameter ζ₂, but this may be difficult to estimate in practice without extensive local data sampling

## Confidence
- **High confidence**: The convergence rate improvements (20% faster than IQL) and clustering accuracy (96% at 15 iterations) are well-supported by the presented results and the theoretical framework is sound.
- **Medium confidence**: The dynamic penalty mechanism's superiority over fixed penalties is demonstrated, but the comparison is limited to a single baseline without ablation studies varying penalty structures.
- **Medium confidence**: The gradient+loss task identification shows 5% improvement over loss-only methods, but the practical significance of this marginal gain needs further validation across different data distributions.

## Next Checks
1. **Ablation study on gradient similarity weighting**: Systematically vary λ in Eq. (4) from 0 to 1 to quantify the exact contribution of gradient direction vs. loss values to clustering accuracy, particularly for highly heterogeneous data distributions.
2. **Distributed baseline comparison**: Implement and compare against a fully distributed CFL approach without Q-value sharing to isolate the contribution of the value decomposition mechanism to the observed convergence improvements.
3. **Scalability stress test**: Evaluate the method with varying numbers of users (U ∈ {6, 12, 24, 48}) and tasks (K ∈ {2, 4, 8}) to identify breaking points where either the Hungarian algorithm becomes computationally prohibitive or clustering accuracy degrades significantly.