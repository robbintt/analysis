---
ver: rpa2
title: 'Read As Human: Compressing Context via Parallelizable Close Reading and Skimming'
arxiv_id: '2602.01840'
source_url: https://arxiv.org/abs/2602.01840
tags:
- compression
- context
- segment
- segments
- reading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficient context compression
  for long-input scenarios in Large Language Models (LLMs). The proposed method, RAM
  (Read As HuMan), mimics human reading behavior by partitioning context into segments
  and processing them in parallel with the input query.
---

# Read As Human: Compressing Context via Parallelizable Close Reading and Skimming

## Quick Facts
- **arXiv ID**: 2602.01840
- **Source URL**: https://arxiv.org/abs/2602.01840
- **Reference count**: 19
- **Primary result**: RAM achieves up to 12× end-to-end speedup on long inputs (average length 16K; maximum length 32K) while outperforming baselines on QA and summarization benchmarks.

## Executive Summary
The paper introduces RAM (Read As HuMan), a context compression method for long-input scenarios in Large Language Models (LLMs). RAM mimics human reading behavior by partitioning context into segments and processing them in parallel with the input query. It employs a hybrid reading strategy: segments highly relevant to the query are fully retained (close reading), while less relevant ones are compressed into compact summary vectors (skimming) via query-guided compression. A contrastive learning objective refines the boundary between close reading and skimming. Experiments show RAM outperforms existing baselines on multiple question answering and summarization benchmarks.

## Method Summary
RAM compresses long-context inputs by partitioning documents into fixed-size segments and processing them in parallel with the query. It first encodes all segments independently, then computes relevance scores via cosine similarity. Based on these scores, it dynamically selects a subset of segments to retain verbatim (close reading) and compresses the rest into single summary vectors (skimming) using query-weighted pooling. The method employs a trainable alignment matrix to bridge representation spaces between retained and compressed segments. Training uses a combined NLL and contrastive loss to improve relevance discrimination. The approach supports arbitrary compression rates and is compatible with existing decoder-only LLMs.

## Key Results
- RAM achieves up to 12× end-to-end speedup on long inputs (average length 16K; maximum length 32K)
- Outperforms baselines on QA (HotpotQA, 2WikiMQA, NaturalQuestions, NarrativeQA) and summarization (MultiNews) benchmarks
- Shows consistent gains across compression rates 4× and 8×, with robustness tested up to 32×

## Why This Works (Mechanism)
RAM works by mimicking human reading behavior: identifying important information (close reading) while summarizing less relevant parts (skimming). The key innovation is parallelizing segment encoding with the query, enabling efficient relevance scoring. The query-guided compression preserves semantic gist while reducing dimensionality. Contrastive learning sharpens the distinction between relevant and irrelevant segments, improving selection quality. The alignment matrix bridges representation spaces between retained and compressed segments, allowing seamless concatenation before decoding.

## Foundational Learning
- **Segment-level relevance scoring**: Cosine similarity between query and segment embeddings determines importance. Why needed: Enables dynamic selection between close reading and skimming. Quick check: Verify relevance scores correlate with human judgment of segment importance.
- **Query-guided compression**: Weighted pooling of segment embeddings based on query relevance. Why needed: Preserves semantic information while reducing dimensionality of skimmed segments. Quick check: Compare compressed vectors' semantic similarity to original segments using probing tasks.
- **Contrastive learning for boundary refinement**: Uses positive/negative segment labels to improve relevance discrimination. Why needed: Sharpens the decision boundary between close reading and skimming, preventing information loss. Quick check: Monitor relevance score distributions and ensure clear separation between positive and negative segments.
- **Alignment matrix (W_align)**: Bridges representation spaces between retained tokens and compressed vectors. Why needed: Enables seamless concatenation of heterogeneous representations before decoding. Quick check: Verify gradients flow through W_align and that concatenated representations are compatible with decoder expectations.

## Architecture Onboarding
**Component Map**: Document → Segmenter → Parallel Encoder → Relevance Scorer → Selector → (Retained Segments + Compressed Vectors) → W_align → Decoder

**Critical Path**: The critical path is the segment-wise parallel encoding followed by relevance scoring, as this determines which segments are retained versus compressed. The contrastive loss applied during training directly shapes this decision boundary.

**Design Tradeoffs**: RAM trades perfect token-level preservation for computational efficiency. The skimming mechanism may lose specific details (e.g., proper nouns, dates) in compressed segments, but this is offset by the ability to process much longer contexts. The method assumes semantic gist preservation is sufficient for downstream tasks.

**Failure Signatures**: 
- If relevance discrimination fails, all segments may be classified as close reading (no compression) or all as skimming (excessive compression).
- If W_align fails to align representations, the decoder may produce degraded outputs due to incompatible input representations.
- If contrastive learning labels are noisy, the close reading/skimming boundary may be imprecise.

**First Experiments**:
1. Implement the segment-wise parallel encoding and verify that relevance scores vary meaningfully across segments for a given query.
2. Test the query-guided compression by comparing compressed vectors to their source segments using semantic similarity metrics.
3. Validate the alignment matrix by checking that concatenated retained and compressed representations produce reasonable outputs from the decoder.

## Open Questions the Paper Calls Out
- **Synthetic label accuracy**: The paper acknowledges that contrastive learning labels for NarrativeQA are generated by a teacher model (Qwen3-235B) and "are not guaranteed to be fully accurate." This raises questions about how label noise impacts the precision of the close reading/skimming boundary.
- **Hard fact preservation**: The vector-based skimming mechanism may not sufficiently preserve specific details (proper nouns, dates) required for multi-hop reasoning when those facts appear in segments classified as low-relevance.
- **Domain generalizability**: RAM is evaluated exclusively on question answering and summarization benchmarks. Its performance on tasks requiring strict structural or syntactic preservation, such as code completion or mathematical reasoning, remains untested.

## Limitations
- Training details are incomplete: number of epochs, W_align initialization, and exact label construction for contrastive learning are unspecified.
- The method's effectiveness on domains beyond QA and summarization (e.g., code, mathematics) is not demonstrated.
- Potential information loss in skimmed segments may impact tasks requiring specific token-level details.

## Confidence
- **High confidence** in the core method description: segment-wise parallel encoding, relevance-based partitioning, query-guided compression, and the contrastive objective are all explicitly detailed.
- **Medium confidence** in reproducibility of results: sufficient details are provided for a faithful reimplementation, but missing training hyperparameters (epochs/steps, initialization) may require tuning to match reported numbers.
- **Low confidence** in robustness claims beyond tested settings: the evaluation covers a range of compression rates (2× to 32×), but the generalizability of the method to different domains, segment sizes, or longer documents is not addressed.

## Next Checks
1. Reconstruct the contrastive learning labels for QA datasets by mapping answer spans to segment-level positives/negatives, then confirm relevance score distributions and k-selection behave as intended during early training.
2. Track W_align gradients and embedding statistics during training to ensure the alignment between close reading tokens and skimming vectors is learned and does not collapse.
3. Run short experiments varying the distribution from which α is sampled during training (e.g., uniform vs. skewed), and observe effects on final QA/Summarization performance at different target rates.