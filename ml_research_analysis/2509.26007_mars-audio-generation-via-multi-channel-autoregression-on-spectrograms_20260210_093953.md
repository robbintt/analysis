---
ver: rpa2
title: 'MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms'
arxiv_id: '2509.26007'
source_url: https://arxiv.org/abs/2509.26007
tags:
- audio
- which
- tokenizer
- mars
- spectrograms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MARS, a multi-scale autoregressive model for
  audio generation that treats spectrograms as multi-channel images. MARS leverages
  channel multiplexing (CMX), a preprocessing technique that reshapes spectrograms
  into lower spatial dimensions by redistributing information across channels, thereby
  reducing computational costs without losing frequency content.
---

# MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms

## Quick Facts
- arXiv ID: 2509.26007
- Source URL: https://arxiv.org/abs/2509.26007
- Reference count: 19
- Primary result: MARS achieves competitive or superior performance to state-of-the-art baselines across multiple metrics: NDB/k = 0.19 (best), PKID = 0.0035 (best), IKID = 0.0015 (best), PIS = 2.96, IIS = 5.20, MSE = 0.0143, MAE = 0.0915, and FAD = 1.64.

## Executive Summary
This paper introduces MARS, a multi-scale autoregressive model for audio generation that treats spectrograms as multi-channel images. MARS leverages channel multiplexing (CMX), a preprocessing technique that reshapes spectrograms into lower spatial dimensions by redistributing information across channels, thereby reducing computational costs without losing frequency content. The model employs a shared tokenizer trained across multiple scales and a transformer-based autoregressor that refines spectrograms from coarse to fine resolutions. Evaluated on the NSynth dataset, MARS achieves competitive or superior performance to state-of-the-art baselines across multiple metrics, demonstrating its ability to balance high-fidelity audio generation with computational efficiency.

## Method Summary
MARS introduces a multi-scale autoregressive approach to audio generation by treating spectrograms as multi-channel images. The key innovation is channel multiplexing (CMX), which reshapes spectrograms into lower spatial dimensions by redistributing frequency information across multiple channels, reducing computational costs. The model uses a shared tokenizer trained across multiple scales and a transformer-based autoregressor that refines spectrograms from coarse to fine resolutions. The approach is evaluated on the NSynth dataset, achieving competitive or superior performance across multiple metrics including Fréchet Audio Distance (FAD), NDB/k, and MSE/MAE on mel-spectrograms.

## Key Results
- MARS achieves NDB/k = 0.19, PKID = 0.0035, and IKID = 0.0015, representing state-of-the-art performance on diversity and semantic metrics.
- The model attains low reconstruction errors with MSE = 0.0143 and MAE = 0.0915 on mel-spectrograms.
- Perceptual quality is excellent with FAD = 1.64, outperforming competing methods.

## Why This Works (Mechanism)
MARS works by treating spectrograms as multi-channel images and leveraging channel multiplexing to reduce computational costs while preserving frequency information. The shared tokenizer across multiple scales enables consistent representation learning, and the transformer-based autoregressor refines spectrograms from coarse to fine resolutions. This hierarchical approach allows the model to capture both global structure and fine details in audio generation.

## Foundational Learning
- Channel Multiplexing (CMX): Why needed: Reduces computational costs by reshaping spectrograms into lower spatial dimensions. Quick check: Verify CMX correctly reshapes 512×512 spectrograms to 256×256×2 tensors.
- Multi-scale Autoregression: Why needed: Enables hierarchical generation from coarse to fine resolutions. Quick check: Monitor token usage distribution in codebook to detect mode collapse.
- VQGAN-style Training: Why needed: Combines reconstruction, VQ, and adversarial losses for robust tokenization. Quick check: Compare Griffin-Lim outputs against ground-truth waveforms to diagnose phase reconstruction artifacts.

## Architecture Onboarding
- Component map: STFT Spectrogram -> CMX Preprocessing -> Shared Tokenizer -> Transformer AR -> Multi-scale Generation -> Griffin-Lim Reconstruction
- Critical path: Audio input → STFT → CMX → Tokenizer → AR refinement → ISTFT
- Design tradeoffs: CMX reduces memory usage but requires careful channel redistribution; multi-scale approach improves quality but increases training complexity.
- Failure signatures: Memory overflow with full spectrograms (check CMX output); phase reconstruction artifacts (compare Griffin-Lim vs ground truth); inconsistent tokenization across scales (visualize token maps).
- First experiments: 1) Verify CMX preprocessing reshapes spectrograms correctly; 2) Train tokenizer with reconstruction + VQ + adversarial losses; 3) Generate spectrograms at increasing resolutions and reconstruct audio.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing architectural hyperparameters (patch size, learnable token count, codebook size) for tokenizer.
- No explicit specification of AR model architecture (depth, width, heads) or training hyperparameters.
- Multi-scale schedule not explicitly defined in the paper.

## Confidence
- Metric results: High confidence (benchmarks well-established and results clearly presented)
- Method description: Medium confidence (clear concept but incomplete implementation details)
- Comparative claims: High confidence (baseline methods well-known, differences clearly stated)

## Next Checks
1. Verify CMX preprocessing correctly reshapes 512×512 spectrograms to 256×256×2 tensors by visualizing intermediate outputs.
2. Reconstruct phase using Griffin-Lim and compare perceptual quality (FAD) against ground truth to diagnose phase reconstruction artifacts.
3. Monitor codebook utilization during tokenizer training to detect mode collapse and ensure diverse token usage across scales.