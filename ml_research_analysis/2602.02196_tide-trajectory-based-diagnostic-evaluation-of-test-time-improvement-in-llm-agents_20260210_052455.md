---
ver: rpa2
title: 'TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM
  Agents'
arxiv_id: '2602.02196'
source_url: https://arxiv.org/abs/2602.02196
tags:
- agent
- action
- memory
- agents
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TIDE, a diagnostic framework for evaluating
  Test-Time Improvement (TTI) in LLM agents, addressing limitations of existing metrics
  that overlook temporal efficiency, adaptive behavior, and memory utility. TIDE decomposes
  TTI into three agent-agnostic metrics: Area Under Variation (AUV) quantifies optimization
  efficiency by capturing temporal dynamics; Loop Ratio (LR) identifies recursive
  failure patterns versus adaptive exploration; and Memory Index (MI) isolates the
  utility of accumulated interaction history.'
---

# TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents

## Quick Facts
- **arXiv ID**: 2602.02196
- **Source URL**: https://arxiv.org/abs/2602.02196
- **Reference count**: 40
- **Primary result**: Introduces TIDE framework with three agent-agnostic metrics (AUV, LR, MI) to diagnose Test-Time Improvement limitations in LLM agents

## Executive Summary
This paper introduces TIDE, a diagnostic framework for evaluating Test-Time Improvement (TTI) in LLM agents. The framework addresses limitations of existing metrics that overlook temporal efficiency, adaptive behavior, and memory utility. TIDE decomposes TTI into three metrics: Area Under Variation (AUV) quantifies optimization efficiency, Loop Ratio (LR) identifies recursive failure patterns, and Memory Index (MI) isolates working memory utility. Experiments across five diverse environments with 17 models reveal that current agents frequently suffer from stubborn loops associated with overconfidence, that negative memory influence is common in reasoning tasks, and that merely scaling test-time reasoning is insufficient for effective improvement.

## Method Summary
TIDE framework runs agents in environments with controlled memory ablation (w/ vs. w/o history) to isolate working memory's contribution. For each trajectory, it computes: (1) AUV via trapezoidal integration over cumulative success curve to measure temporal efficiency, (2) LR by detecting consecutive repeated cycles in state-action graphs to identify behavior stagnation, and (3) MI as the delta between AUV with full memory versus AUV with only immediate observation. The framework uses fixed maximum turns per environment (BlocksWorld=20, FrozenLake=30, Sudoku=20, AlfWorld=60, WebShop=15) and supports both exact text matching and embedding-based state hashing.

## Key Results
- Current LLM agents frequently suffer from stubborn loops associated with overconfidence, with Qwen3-4B-Instruct recording 32.0% LR in FrozenLake
- Negative memory influence is widely observed, with multiple models in FrozenLake showing reduced performance when working memory is enabled
- Proprietary models like Gemini 2.5 Pro and DeepSeek-R1 consistently outperform open-source alternatives, highlighting the importance of optimizing agent-environment interaction dynamics

## Why This Works (Mechanism)

### Mechanism 1: Temporal Efficiency Quantification via Weighted-Increment Integration
- Claim: AUV captures optimization efficiency more informatively than binary success rate by weighting early gains higher than late gains
- Mechanism: AUV computes trapezoidal integral over cumulative success curve Pt, mathematically equivalent to weighting marginal gains δk by time-decaying coefficients (wk = tmax - k - 0.5)
- Core assumption: Agents that solve tasks earlier demonstrate superior TTI capability, with temporal distribution measurably distinct from final outcomes
- Evidence: Gemini 2.5 Pro achieves higher AUV 0.629 than DeepSeek-V3.2, indicating early-stage TTI efficiency; proof shows AUV resolves equivalence classes where SRfinal cannot distinguish efficient from inefficient paths

### Mechanism 2: Graph-Based Loop Detection for Behavior Stagnation Diagnosis
- Claim: Recursive loop patterns in state trajectories indicate behavior adaptation failure, distinguishable from productive exploration
- Mechanism: Trajectories interpreted as directed graphs over latent environment states; cycles detected via hash-based state matching (lij where si = sj); Loop Ratio quantifies only consecutive repeated cycles
- Core assumption: Consecutive repeated cycles represent degenerate repetition rather than intentional retry; non-consecutive cycles may represent valid exploration
- Evidence: Qwen3-4B-Instruct records LR of 32.0% in FrozenLake; statistically significant inverse relationship across four models where high LR associates with low AUV

### Mechanism 3: Controlled Ablation for Memory Utility Isolation
- Claim: Working memory's contribution to TTI can be isolated by comparing AUV with full memory vs. AUV with only immediate observation
- Mechanism: MI = AUV(w/ memory) - AUV(w/o memory); positive MI indicates beneficial memory, negative MI indicates cognitive burden from accumulated noise
- Core assumption: Difference in performance attributable solely to memory presence, not to other confounding factors
- Evidence: Multiple models in FrozenLake show negative memory influence; benefits confined primarily to first 5 window sizes before curve plateaus

## Foundational Learning

- **POMDP vs. MDP Distinction**
  - Why needed: Framework categorizes environments as reasoning-bound (MDP, fully observable) vs. information-bound (POMDP, partially observable), explaining divergent memory utility patterns
  - Quick check: In an MDP environment like FrozenLake, why would accumulated history be redundant?

- **Graph Cycle Detection**
  - Why needed: LR computation requires understanding how to decompose trajectories into state-action graphs and identify non-nested cycles vs. consecutive repeated loops
  - Quick check: What is the difference between a cycle (Lcycle) and a loop (Lloop) in this framework?

- **Trapezoidal Integration for Discrete Curves**
  - Why needed: AUV uses discrete trapezoidal summation to approximate area under success-rate curve, with specific weighting implications
  - Quick check: Why does weighting scheme (wk = tmax - k - 0.5) reward early success more than late success?

## Architecture Onboarding

- **Component map**: Raw trajectory logs → State hashing → Cycle detection (Algorithm 1) → LR computation; Cumulative success curve → AUV trapezoidal sum; Memory-ablated rerun → MI delta
- **Critical path**: TaskRunnerBase orchestrates episode execution; BaseAgent generates actions via get_next_step_parallel(); ContextManager formats prompts and manages StepMemory history; BaseEnv provides step(), reset(), render() interface; TrajectoryInfo encapsulates per-trajectory data; StepMemory stores per-step observation, action, analysis, feedback
- **Design tradeoffs**: Exact text match for state hashing vs. embedding similarity (threshold 0.999) for GUI environments; fixed tmax per environment vs. dynamic horizon; LR counts only consecutive repeated cycles vs. all cycles
- **Failure signatures**: High LR + Low AUV indicates agent stuck in recursive failure loops; Negative MI in POMDP tasks suggests agent fails to extract task-relevant information from history; High SR but Low AUV indicates agent eventually succeeds but with poor temporal efficiency
- **First 3 experiments**: (1) Reproduce AUV vs. SR comparison on AlfWorld with two models of similar SR but divergent AUV (DeepSeek-V3.2 vs. Gemini 2.5 Pro); (2) Run memory ablation on both MDP (FrozenLake) and POMDP (WebShop) environments to reproduce MI divergence pattern; (3) Apply post-hoc TIDE analysis to external trajectory log (OSWorld data) without re-execution to validate agent-agnostic claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific active memory management mechanisms can effectively mitigate the observed negative utility of working memory in reasoning-bound environments?
- Basis: Authors conclude that "simply leveraging working memory without further management is often harmful in reasoning-bound scenarios" and highlight "need for active memory management"
- Why unresolved: While demonstrating memory often acts as cognitive burden (negative MI) and simple summarization fails, paper does not propose or validate specific architectural solutions for filtering or compressing history
- Evidence: Dynamic memory filtration or compression module that selectively retains task-critical state changes, resulting in positive Memory Index in MDP environments like FrozenLake

### Open Question 2
- Question: How can agent architectures be modified to distinguish between pathological recursive loops and valid retry attempts?
- Basis: Authors identify that "loop mitigation is a critical unresolved challenge" and note difficulty identifying boundary between adaptation and recursive failure
- Why unresolved: Paper detects loops as major failure mode associated with overconfidence but does not propose control mechanism to interrupt cycles without prematurely halting valid exploratory behavior
- Evidence: Intervention strategy (entropy-based resetting or forced action diversification) that significantly reduces Loop Ratio in GUI agents without decreasing overall Success Rate

### Open Question 3
- Question: How can internal chain-of-thought reasoning be explicitly grounded to prevent decoupling of cognitive capacity from external interactive efficacy in thinking models?
- Basis: Authors conclude that "merely test-time scaling is insufficient" and observe "critical decoupling between internal cognitive capacity and external interactive efficacy" in reasoning-enhanced models
- Why unresolved: Study shows thinking models often fail to translate deep reasoning into effective actions in information-bound tasks, but mechanism to align these processes is not explored
- Evidence: Training or prompting method that improves AUV of thinking models in information-bound POMDP environments (like WebShop) to match or exceed performance in reasoning-bound MDPs

## Limitations
- Framework assumes trajectory-based metrics can fully capture agent improvement dynamics but may miss internal reasoning processes not reflected in state-action sequences
- State hashing via exact text matching may fail for environments with non-deterministic output formatting, potentially inflating LR values
- Memory ablation assumes restricting to immediate observations is only confounding factor but doesn't account for potential compensatory reasoning strategies agents might employ

## Confidence
- **High Confidence**: AUV as temporal efficiency metric (supported by mathematical proof and clear empirical differentiation)
- **Medium Confidence**: LR as behavior adaptation diagnostic (statistically significant correlations exist but corpus validation is limited)
- **Medium Confidence**: MI as memory utility isolation (ablative design is sound but confounding factors like implicit reasoning compensation are not fully addressed)

## Next Checks
1. Test TIDE metrics on external trajectory dataset (OSWorld) without re-execution to validate agent-agnostic applicability
2. Implement semantic state hashing (via embeddings) in place of exact text matching to evaluate robustness to non-deterministic state representations
3. Design experiments to detect whether agents employ compensatory reasoning when memory is restricted, potentially confounding MI measurements