---
ver: rpa2
title: 'Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance
  Human-AI Collaboration'
arxiv_id: '2507.23407'
source_url: https://arxiv.org/abs/2507.23407
tags:
- thinking
- pesos
- question
- information
- critical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces proactive critical thinking, a paradigm where\
  \ models not only identify unanswerable queries but actively seek missing or clarifying\
  \ information from users to collaboratively solve problems. To evaluate this capability,\
  \ two benchmarks\u2014GSM-MC and GSM-MCE\u2014are constructed from GSM8K by removing\
  \ key variables and optionally adding irrelevant details."
---

# Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration

## Quick Facts
- arXiv ID: 2507.23407
- Source URL: https://arxiv.org/abs/2507.23407
- Reference count: 19
- Primary result: RL-trained models boost proactive critical thinking accuracy from 0.15% to 73.98% on GSM-MC benchmark

## Executive Summary
This paper introduces proactive critical thinking as a paradigm where AI models not only identify unanswerable queries but actively seek missing information from users to collaboratively solve problems. The authors construct GSM-MC and GSM-MCE benchmarks from GSM8K by removing key variables and optionally adding irrelevant details. Through reinforcement learning with shaped rewards, they demonstrate that smaller models (Qwen3-1.7B) can develop strong proactive questioning abilities, achieving 73.98% accuracy on GSM-MC while maintaining performance on standard tasks.

## Method Summary
The authors create two benchmarks by annotating GSM8K problems to identify key variables, then removing one variable and rephrasing to make questions unanswerable. GSM-MCE adds irrelevant details as distractors. They filter these using 16-sample rollouts with a user agent, keeping only questions where some but not all attempts succeed. Models are first trained via supervised fine-tuning on rejection-sampled trajectories, then refined with GRPO reinforcement learning using shaped rewards (+0.5 for requesting clarification on unanswerable questions, -0.5 for unnecessary requests). The approach significantly improves proactive questioning while maintaining standard reasoning capabilities.

## Key Results
- Qwen3-1.7B accuracy on GSM-MC increases from 0.15% to 73.98% with RL training
- Thinking mode (extended reasoning) improves trained models (+4% ACC on GSM-MC) but hurts vanilla models (-10% ACC)
- RL generalization tested successfully on MIP-MATH out-of-distribution math problems
- Larger models (Qwen3-8B) perform better with RL-only vs. SFT+RL, while smaller models benefit from both stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning with shaped rewards induces proactive questioning behavior more effectively than supervised fine-tuning alone.
- Mechanism: GRPO algorithm optimizes policy using outcome rewards augmented with intermediate signals—+0.5 for requesting clarification on unanswerable questions, -0.5 for unnecessary requests on answerable ones. This densifies sparse final-answer rewards, enabling faster credit assignment.
- Core assumption: The shaped rewards accurately proxy the desired behavior without gaming; models won't learn to request unnecessarily just for the +0.5 bonus.
- Evidence anchors: [abstract] "reinforcement learning (RL) can significantly improve this ability... boosting Qwen3-1.7B's accuracy from 0.15% to 73.98%" [Section 5, Equation 2] Details GRPO loss with KL penalty and group-normalized advantages [corpus] Weak direct evidence; neighbor papers on proactive questioning don't specifically address reward shaping mechanisms
- Break condition: If request ratio approaches 100% while accuracy plateaus or degrades, the model may be over-relying on the +0.5 request reward.

### Mechanism 2
- Claim: Sampling-based data filtering removes low-quality training cases that would otherwise degrade model learning.
- Mechanism: For each generated unanswerable question, DeepSeek-V3 attempts 16 solution trajectories with a simulated user agent. Questions are filtered if: >12/16 solved immediately (still answerable), >12/16 solved in turn 2 (too obvious), or 0/16 solved (unclarifiable).
- Core assumption: The filtering thresholds (12/16) correctly separate low-quality from high-quality cases; the user agent (DeepSeek-V3) provides valid responses.
- Evidence anchors: [Section 4] "we conduct an error analysis (see Table 1) and identify three main categories of problems" [Table 3] Ablation shows ACC drops from 44.15% to 27.20% on GSM-MC when filtering is removed (thinking mode enabled) [corpus] No direct corpus evidence on filtering thresholds
- Break condition: If filtered dataset size becomes too small (<1000 examples), overfitting risk increases; if too large, noise dominates.

### Mechanism 3
- Claim: Heuristic guidance accelerates RL convergence by providing denser learning signals early in training.
- Mechanism: During SFT, prompts explicitly state "This question is unanswerable due to missing key information." During RL, the +0.5/-0.5 auxiliary rewards guide exploration before final-answer rewards become informative.
- Core assumption: The heuristics correctly classify answerability most of the time; incorrect heuristics could mislead training.
- Evidence anchors: [Section 5] "we augment the reward function with the following heuristic signals" [Figure 2] Shows higher initial rewards and faster convergence with heuristics vs. without [corpus] No corpus papers specifically validate this heuristic approach
- Break condition: If heuristic-augmented models show higher request ratios but lower final accuracy than non-heuristic baselines, heuristics may be misleading.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL algorithm used; differs from standard PPO by computing advantages via group normalization rather than value function estimation.
  - Quick check question: Can you explain how GRPO computes advantages differently from PPO?

- Concept: **Rejection Sampling**
  - Why needed here: Used to collect training trajectories when the base model lacks proactive questioning ability; samples multiple completions and keeps successful ones.
  - Quick check question: How does rejection sampling differ from beam search for data collection?

- Concept: **KL Divergence Regularization**
  - Why needed here: The βDKL term prevents the RL policy from deviating too far from the reference policy, maintaining stability.
  - Quick check question: What happens to training if β is set too high vs. too low?

## Architecture Onboarding

- Component map: GSM8K → Variable Recognition → Variable Removal → Rephrasing → Irrelevant Info Injection (optional) → 16-sample Filtering → GSM-MC/GSM-MCE → SFT → RL with GRPO + heuristic rewards → Final model
- Critical path: 1. Data quality: Filtering is the bottleneck; 16 samples per question × 1,368 questions = ~22K LLM calls minimum 2. RL convergence: Heuristic rewards accelerate but require careful tuning of +0.5/-0.5 magnitudes 3. User agent fidelity: Training uses Qwen3-14B, evaluation uses DeepSeek-V3—mismatch could inflate/deflate scores
- Design tradeoffs: Thinking mode: Helps trained models (+4% ACC on GSM-MC for Qwen3-8B RL), hurts vanilla models (-10% ACC) SFT+RL vs. RL-only: Larger models may perform better with RL-only (Qwen3-8B: 85.53% vs. 83.11%), smaller models benefit from both stages Dataset size: Filtering reduces quantity but improves quality; ablation shows ~17% ACC drop without filtering
- Failure signatures: Vanilla models on GSM-MC: ACC ≈ 0%, REQ ≈ 0% (complete failure to recognize or address incomplete questions) Over-requests: REQ > 95% with ACC < 50% suggests reward hacking the +0.5 request bonus Thinking mode degradation: Extended reasoning without proactive questioning leads to hallucination (Figure 1, left column)
- First 3 experiments: 1. Replicate GSM-MC filtering on a small subset (100 questions) to validate pipeline output quality against Table 1 error categories 2. Ablate heuristic rewards: train with final-answer-only rewards (r ∈ {0,1}) vs. augmented rewards, compare convergence curves to Figure 2 3. Test generalization: evaluate trained model on MIP-MATH (out-of-distribution) to verify Table 4 results scale to your implementation

## Open Questions the Paper Calls Out

- Can proactive critical thinking capabilities transfer effectively to non-mathematical domains such as medical diagnosis or legal analysis? [explicit] The conclusion states "Developing benchmarks covering a broader range of domains, such as medicine" as a promising direction. Why unresolved: The current work only evaluates mathematical reasoning (GSM8K-derived benchmarks), leaving domain transfer unexplored. What evidence would resolve it: Demonstrating that RL-trained models maintain proactive critical thinking performance on domain-specific benchmarks like medical case diagnosis tasks.

- How does proactive critical thinking performance scale with interaction complexity beyond 4 conversational turns? [explicit] The conclusion identifies "Extending interaction length to more turns to address more complex tasks" as a future direction. Why unresolved: Current experiments only evaluate 2-4 turn interactions; real collaborative problem-solving may require extended dialogues. What evidence would resolve it: Evaluating accuracy trajectories across 5+ turn interactions to determine if gains plateau or continue improving.

- What alternative feedback mechanisms beyond questioning could enhance proactive critical thinking? [explicit] The conclusion calls for "Exploring more robust training algorithms capable of providing diverse feedback beyond questioning." Why unresolved: The current paradigm focuses solely on asking clarifying questions, limiting proactive engagement strategies. What evidence would resolve it: Comparing training frameworks where models provide hints, partial solutions, or counter-examples as alternative feedback forms.

## Limitations

- The core contribution relies on heuristic reward shaping and sampling-based filtering, which may overfit to GSM-MC/GSM-MCE artifacts rather than truly generalizing proactive questioning behavior.
- Filtering criteria depend on a user agent (DeepSeek-V3) for both training and evaluation, raising concerns about data leakage and agent bias affecting results.
- Exact prompt templates and rejection sampling thresholds are underspecified, limiting reproducibility of the reported performance gains.

## Confidence

- **High**: RL improves proactive questioning on GSM-MC/GSM-MCE (Qwen3-1.7B: 0.15%→73.98%); thinking mode aids trained models but hurts vanilla ones.
- **Medium**: Heuristic rewards and filtering meaningfully improve learning; generalization to MIP-MATH.
- **Low**: Exact mechanism by which shaped rewards induce proactive behavior without gaming; robustness of filtering thresholds across datasets.

## Next Checks

1. Replicate the GSM-MC filtering pipeline on a held-out GSM8K subset and compare error distributions to Table 1 to verify filtering validity.
2. Train a model with final-answer-only RL rewards (no heuristics) and compare convergence curves and final performance to Figure 2.
3. Evaluate the trained model on an additional out-of-distribution math dataset (e.g., MATH) to test generalization beyond MIP-MATH.