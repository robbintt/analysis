---
ver: rpa2
title: Manifold-Aware Perturbations for Constrained Generative Modeling
arxiv_id: '2601.23151'
source_url: https://arxiv.org/abs/2601.23151
tags:
- samples
- generative
- distribution
- modeling
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of training generative models\
  \ on data constrained to lie on lower-dimensional manifolds, where standard approaches\
  \ suffer from score or Jacobian determinant explosion due to dimension mismatch\
  \ between the data distribution and the latent Gaussian. The proposed solution is\
  \ to perturb the data distribution by adding Gaussian noise strictly in the normal\
  \ directions to the manifold, creating a lifted distribution p\u03C3 that has full\
  \ ambient space support while implicitly incorporating manifold geometry."
---

# Manifold-Aware Perturbations for Constrained Generative Modeling

## Quick Facts
- arXiv ID: 2601.23151
- Source URL: https://arxiv.org/abs/2601.23151
- Authors: Katherine Keegan; Lars Ruthotto
- Reference count: 40
- Primary result: Manifold-aware perturbations enable unconstrained generative modeling on data constrained to lie on lower-dimensional manifolds by adding noise in normal directions, achieving improved distributional fidelity across multiple tasks including planes, spheres, mesh surfaces, MNIST with flux constraints, and protein backbones.

## Executive Summary
This paper addresses the fundamental challenge of training generative models on data constrained to lie on lower-dimensional manifolds, where standard approaches suffer from score or Jacobian determinant explosion due to dimension mismatch between the data distribution and the latent Gaussian. The proposed solution is to perturb the data distribution by adding Gaussian noise strictly in the normal directions to the manifold, creating a lifted distribution that has full ambient space support while implicitly incorporating manifold geometry. This allows training unconstrained generative models on the perturbed distribution, followed by projection of generated samples back to the manifold.

The approach demonstrates strong empirical performance across five diverse tasks: 3D points on 2D planes and unit spheres, Stanford Bunny mesh surfaces, MNIST images with fixed total flux constraints, and protein backbone fragments. Theoretical analysis shows that for linear constraints, the method achieves perfect recovery, while for nonlinear constraints, it provides bounded total variation error depending on manifold curvature. The method offers automatic constraint adherence, bounded scores/log-determinants, and improved or competitive metrics compared to state-of-the-art constrained generative modeling techniques.

## Method Summary
The core innovation is manifold-aware perturbation: for data constrained to an m-dimensional manifold M embedded in Rᵈ, instead of training directly on the degenerate distribution p₀, the method adds Gaussian noise strictly in the normal directions to create a full-dimensional lifted distribution p_σ. Specifically, for each sample x₀ ∈ M, noise n is drawn from N(0, σ²I) and projected onto the normal space Nz M, then x_σ = x₀ + n. Unconstrained generative models (DDPM or normalizing flows) are trained on {x_σ}, samples x̂_σ ~ p_σ are generated, and then projected back to M via nearest-point projection Π. The approach theoretically guarantees perfect recovery for linear constraints and bounded TV error for nonlinear ones, while empirically improving distributional fidelity across multiple tasks.

## Key Results
- Learning the perturbed distribution p_σ consistently improves distributional fidelity compared to training on the degenerate p₀, with lower TVD, higher COV, and better FID scores across all tested tasks
- For linear constraints (plane, sphere, MNIST flux), the method achieves near-perfect recovery with negligible constraint violation after projection
- On the Stanford Bunny mesh, perturbed DDPM generates samples that better capture the surface geometry compared to baselines
- For protein backbone generation, perturbed models achieve lower pairwise RMSD and MMD scores while automatically satisfying bond length and angle constraints

## Why This Works (Mechanism)
The method works by transforming an ill-posed problem (training on a degenerate m-dimensional distribution in Rᵈ) into a well-posed one (training on a full-dimensional distribution). By adding noise only in normal directions, the perturbation creates a distribution that fills the ambient space around the manifold while preserving its geometric structure. This allows standard generative models to learn the distribution without encountering exploding scores or log-determinants that occur when trying to model degenerate distributions. The subsequent projection step then recovers samples on the original manifold while benefiting from the improved learning dynamics during training.

## Foundational Learning

**Manifold Geometry** - Understanding tangent and normal spaces is crucial for implementing the perturbation correctly. The normal space at each point must be computed to project noise appropriately. Quick check: verify that the difference x_σ - x₀ is orthogonal to the tangent space basis vectors.

**Diffusion Models** - Familiarity with score-based generative models is needed to understand why training on degenerate distributions causes score explosion. The score ∇log p(xt) becomes unbounded as t→0 for distributions with lower-dimensional support. Quick check: monitor ∥∇log p(xt)∥ during training - it should remain bounded when using perturbations.

**Constrained Optimization** - Knowledge of projection operators and constraint satisfaction is essential for implementing the final step. Different constraint types (linear vs nonlinear) require different projection methods. Quick check: measure constraint violation ∥h(Π(x̂_σ))∥ - should be near zero for successful implementation.

## Architecture Onboarding

**Component Map**
Data samples -> Manifold-aware perturbation -> Unconstrained generative model (DDPM/NF) -> Generated samples -> Projection to manifold -> Final outputs

**Critical Path**
The critical path is: perturb data → train unconstrained model → generate samples → project to manifold. Each step must be correctly implemented for the method to work.

**Design Tradeoffs**
- σ value: Larger σ provides better training stability but may degrade sample quality after projection (tradeoff controlled by manifold reach)
- Perturbation direction: Noise must be strictly in normal directions - incorrect implementation leads to score explosion
- Projection method: Choice depends on constraint type (analytical for linear, iterative for nonlinear)

**Failure Signatures**
- Score/log-determinant explosion during training indicates perturbation is not in correct normal directions
- High TVD or constraint violation after projection suggests σ is too large relative to manifold reach
- Poor generation quality may indicate insufficient model capacity or training iterations

**First Experiments**
1. Implement perturbation for plane constraint: verify x_σ - x₀ is orthogonal to tangent space
2. Train DDPM on perturbed plane data: check that scores remain bounded during training
3. Implement projection for sphere: verify constraint satisfaction ∥x∥ ≈ 1 for generated samples

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees assume well-behaved manifolds with bounded curvature, which may not hold for all practical applications
- The method requires computing normal spaces and implementing projection operators, adding complexity to the training pipeline
- Choice of σ value involves tradeoff between training stability and sample quality, requiring problem-specific tuning

## Confidence

**High confidence in:** Theoretical framework and mathematical validity of the manifold-aware perturbation approach

**Medium confidence in:** Empirical results due to missing implementation details and the challenging nature of comparing constrained generative models across different tasks

**Medium confidence in:** Scalability claims, as results show strong performance on moderate-scale problems but limited testing on large-scale datasets

## Next Checks
1. Verify the perturbation implementation by checking that added noise vectors are strictly orthogonal to the tangent space at each manifold point (measure dot product with tangent basis vectors - should be near zero)
2. Test the score explosion diagnostic on a simple manifold-constrained distribution (e.g., plane or sphere) without perturbations to confirm that ∥∇log p(xt)∥ grows unbounded as t→0
3. Implement the Gauss-Newton projection for protein constraints and verify constraint satisfaction (bond lengths/angles should match target distributions within small tolerance)