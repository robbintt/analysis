---
ver: rpa2
title: Explaining and Mitigating Crosslingual Tokenizer Inequities
arxiv_id: '2510.21909'
source_url: https://arxiv.org/abs/2510.21909
tags:
- tokenizers
- vocabulary
- size
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-lingual tokenizer inequities by training
  approximately 7,000 monolingual tokenizers across 97 languages to measure and analyze
  token premiums. The authors find that monolingual tokenizers trained identically
  still show widely varying compression rates across languages, driven by factors
  like vocabulary size, pre-tokenization methods, and language-specific features.
---

# Explaining and Mitigating Crosslingual Tokenizer Inequities

## Quick Facts
- arXiv ID: 2510.21909
- Source URL: https://arxiv.org/abs/2510.21909
- Authors: Catherine Arnett; Tyler A. Chang; Stella Biderman; Benjamin K. Bergen
- Reference count: 40
- This paper investigates cross-lingual tokenizer inequities by training approximately 7,000 monolingual tokenizers across 97 languages to measure and analyze token premiums.

## Executive Summary
This paper systematically investigates cross-lingual tokenizer inequities by training thousands of monolingual tokenizers across 97 languages. The authors find that even when trained identically, tokenizers exhibit widely varying compression rates (token premiums) across languages, driven by factors including vocabulary size, pre-tokenization methods, and language-specific features. They demonstrate that optimizing vocabulary size per language and using superword tokenizers (which allow merges across whitespaces) significantly reduces these inequities. SuperBPE tokenizers show both improved compression and reduced cross-linguistic variation in compression rates compared to standard BPE tokenizers.

## Method Summary
The authors trained approximately 7,000 monolingual tokenizers across 97 languages using the FLORES parallel corpus as a shared evaluation dataset. They systematically varied vocabulary sizes (8K-262K), tokenizer types (BPE, Unigram, SuperBPE), and pre-tokenization strategies. Token compression was measured using Corpus Token Count (CTC) on the FLORES data. The study also analyzed the effects of language-specific factors including script type, whitespace frequency, and byte-length ratios on compression efficiency.

## Key Results
- Monolingual tokenizers trained identically still show widely varying compression rates across languages
- Standard BPE tokenizers exhibit significant token premium variance across languages (F-test: F(96,1)=1.606, p<0.001)
- SuperBPE tokenizers reduce both average CTC and CTC variance compared to standard BPE (F-test, p<0.001 for most vocabulary sizes)
- Optimal vocabulary size varies significantly by language, with some languages saturating at smaller vocabularies while others require larger vocabularies for comparable compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whitespace pre-tokenization creates language-dependent compression inefficiencies
- Mechanism: Standard BPE tokenizers split text at whitespace boundaries before learning merges. Languages with more whitespace-separated units per unit of meaning (e.g., Scottish Gaelic) are forced into more pre-token segments, limiting merge opportunities. SuperBPE removes this constraint, allowing tokens to span multiple words.
- Core assumption: Whitespace frequency correlates with information density per "word" across languages.
- Evidence anchors:
  - [section 4] Proportion of whitespaces explains 15.7% of CTC variance (R²=0.157)
  - [section 5.3] SuperBPE tokenizers show significantly lower CTC variance at all vocabulary sizes (F-test, p<0.001 for most sizes)
  - [corpus] SuperBPE paper confirms space-agnostic merging improves compression
- Break condition: If a language uses no whitespace (e.g., Chinese), this mechanism does not apply.

### Mechanism 2
- Claim: Optimal vocabulary size is language-specific, not uniform
- Mechanism: CTC follows a power-law relationship with vocabulary size, but the curve parameters differ by language. Some languages saturate (diminishing returns) at smaller vocabularies; others require larger vocabularies to achieve comparable compression. A uniform vocabulary size either under-allocates to some languages or wastes capacity on others.
- Core assumption: The power-law fit accurately predicts CTC at unseen vocabulary sizes.
- Evidence anchors:
  - [section 5.2] Optimal-vocabulary tokenizers show significantly less CTC variance than same-vocabulary tokenizers (F-test: F(80,387)=0.150, p<0.001)
  - [appendix J.1] Power-law curves fit per language; some languages cannot reach target CTCs even at max vocabulary
  - [corpus] Corpus weak on direct validation of power-law extrapolation for low-resource languages
- Break condition: If training data is insufficient for the target vocabulary size, the tokenizer may overfit, breaking the predicted compression gains.

### Mechanism 3
- Claim: Byte premiums have negligible direct effect on token premiums in BPE tokenizers
- Mechanism: While byte premiums (more bytes per unit meaning) affect raw data size, BPE's iterative merging process decouples byte-level inefficiencies from token-level compression. The authors hypothesized byte premiums would correlate with token premiums, but found a very small negative effect (β=-0.0418, p<0.001)—opposite direction.
- Core assumption: The relationship between byte and token premiums is linear and detectable with the current experimental design.
- Evidence anchors:
  - [section 3.1] BP-scaling training data has no significant effect on CTC (t(3544)=-0.615, p=0.539)
  - [appendix E] Byte premium explains minimal variance; no significant interaction with tokenizer type (BPE vs Unigram)
  - [corpus] Related work (MYTE, MAGNET) addresses byte premiums directly, suggesting they remain a concern for byte-level tokenizers, not subword tokenizers
- Break condition: If using byte-level tokenization (e.g., ByT5), byte premiums directly determine token premiums.

## Foundational Learning

- Concept: **Corpus Token Count (CTC)**
  - Why needed here: CTC is the primary metric for measuring compression and token premiums. Lower CTC = better compression.
  - Quick check question: If Language A has CTC=50,000 and Language B has CTC=70,000 on the same parallel text, which has a higher token premium?

- Concept: **Pre-tokenization**
  - Why needed here: Pre-tokenization splits text before subword merging. Whitespace-based pre-tokenization is the default but creates cross-linguistic inequities.
  - Quick check question: What happens to compression if you remove whitespace pre-tokenization for a language with many short words?

- Concept: **BPE vs Unigram tokenization**
  - Why needed here: The paper compares these algorithms. BPE builds vocabulary via iterative merging; Unigram starts with all words and prunes. BPE shows better compression overall.
  - Quick check question: Which tokenizer type is more sensitive to the initial byte-level representation of text?

## Architecture Onboarding

- Component map:
  Training Data (300MB/language) → Pre-tokenizer (whitespace split) → BPE/Unigram trainer → Vocabulary (8K-262K) → Evaluation (FLORES CTC)

- Critical path:
  1. Select vocabulary size (start with 32K-65K for experimentation)
  2. Choose pre-tokenization strategy (standard vs SuperBPE)
  3. Train on domain-matched data if possible (though data similarity had small effects)
  4. Evaluate on parallel corpus (FLORES) to measure CTC variance across languages

- Design tradeoffs:
  - Larger vocabulary → better compression but higher memory/embedding costs
  - SuperBPE → better compression and equity, but may require more training data to avoid overfitting
  - Language-specific vocab sizes → optimal equity, but complicates multilingual deployment
  - BPE vs Unigram → BPE has better compression; Unigram may produce more linguistically-aligned tokens (per prior work)

- Failure signatures:
  - High CTC outliers (>2 standard deviations from mean): Check for script-specific pre-tokenization issues (e.g., regex splitting on diacritics)
  - Unigram convergence failures at small vocabulary sizes (<16K): Increase vocabulary or switch to BPE
  - SuperBPE overfitting: If training CTC is much lower than evaluation CTC, reduce transition point or increase training data
  - Vocabulary contamination: If FLORES examples appear in training data, exclude affected languages (authors excluded Samoan for this reason)

- First 3 experiments:
  1. **Baseline replication**: Train BPE tokenizers for 5-10 diverse languages (varying scripts, whitespace rates) at vocabulary sizes 16K, 32K, 65K. Compute CTC on FLORES to confirm token premium variance exists.
  2. **SuperBPE comparison**: Train SuperBPE tokenizers for the same languages at vocabulary size 65K+. Compare CTC distributions to baseline (expect lower mean and variance).
  3. **Optimal vocabulary estimation**: Fit power-law curves to CTC vs vocabulary size for each language. Predict and validate "optimal" vocabulary sizes that achieve a target CTC (e.g., 56,000). Measure variance reduction vs fixed-vocabulary approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the reductions in token premiums achieved through optimal vocabulary sizing and superword tokenization translate into improved language model performance or faster training/inference times?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section (Section 8) that they "did not train any language models with our tokenizers, therefore we cannot make any claims about the connection between our findings and changes in language model performance."
- Why unresolved: The study measures success via compression metrics (Corpus Token Count) rather than end-to-end model training. It is theoretically possible that "too much compression" or specific vocabulary allocations might negatively impact the model's ability to learn.
- What evidence would resolve it: Training language models using the proposed MonTok and SuperBPE tokenizers and measuring convergence rates, perplexity, and downstream task performance compared to baselines.

### Open Question 2
- Question: Does extreme compression, particularly in superword tokenizers, degrade a model's reasoning capabilities by reducing the number of computational steps per unit of meaning?
- Basis in paper: [explicit] The Discussion section (Section 6) asks, "Perhaps too much compression could be giving the model effectively less time to think, by dedicating fewer FLOPS to predicting each next token?"
- Why unresolved: While compression reduces costs, compressing multiple whitespace-separated words into single tokens reduces the sequence length the model attends to, potentially removing useful intermediate processing steps (the "pause token" effect mentioned).
- What evidence would resolve it: Comparative evaluations of models trained on standard versus superword tokenizers specifically on complex reasoning benchmarks (e.g., mathematical or logical reasoning) where intermediate computation steps are critical.

### Open Question 3
- Question: Can alternative encoding schemes (beyond standard UTF-8) successfully mitigate the residual token premium inequities caused by language-specific length ratios and byte inefficiencies?
- Basis in paper: [explicit] Section 5.4 states, "There are still significant effects of length ratio and bytes-per-character after the interventions... Future work can seek ways to address remaining inequities, especially those introduced by UTF-8."
- Why unresolved: The interventions in the paper (vocabulary optimization, SuperBPE) address algorithmic factors but do not correct for fundamental differences in how languages encode into bytes (e.g., varying bytes per character).
- What evidence would resolve it: Integrating novel encoding schemes (such as those normalizing Unicode representations) into the tokenizer training pipeline and measuring the resulting variance in cross-lingual compression rates.

## Limitations
- Experimental Scope Constraints: The study used 300MB of monolingual data per language, which may be insufficient for accurate CTC estimation at larger vocabulary sizes (>65K), particularly for low-resource languages.
- Parallel Corpus Limitations: CTC measurements rely on FLORES parallel data, which may not fully represent each language's natural token distribution.
- Algorithm Generalization: Results focus primarily on BPE and Unigram algorithms; findings may not directly translate to other tokenization approaches.

## Confidence
**High Confidence**:
- Cross-linguistic token premium variance exists and is statistically significant
- SuperBPE reduces both CTC and CTC variance compared to standard BPE
- Optimal vocabulary size varies meaningfully by language

**Medium Confidence**:
- Power-law models accurately predict CTC at unobserved vocabulary sizes
- Optimal vocabulary approach reduces token premium variance more than same-vocabulary approach
- Byte premiums have negligible direct effect on token premiums in BPE tokenizers

**Low Confidence**:
- Unigram tokenizers are "less compressible" than BPE (limited direct comparison)
- Domain similarity effects on CTC are negligible (tested with only Wikipedia)

## Next Checks
1. **Validation of Power-Law Extrapolation**: For 10-15 languages spanning different language families, train tokenizers at additional vocabulary sizes (e.g., 512K, 1M) to empirically validate whether the power-law predictions accurately forecast CTC at unobserved sizes.

2. **Low-Resource Language Testing**: Select 5-10 genuinely low-resource languages (≤100MB available training data) and systematically vary training data quantity to establish the minimum viable data requirements for SuperBPE to outperform standard BPE.

3. **Multilingual Model Integration**: Implement SuperBPE tokenizers in a real multilingual model training pipeline (e.g., mBERT or mT5) and measure downstream task performance changes alongside CTC improvements to verify that compression gains translate to practical benefits.