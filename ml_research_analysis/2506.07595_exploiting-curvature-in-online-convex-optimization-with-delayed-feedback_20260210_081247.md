---
ver: rpa2
title: Exploiting Curvature in Online Convex Optimization with Delayed Feedback
arxiv_id: '2506.07595'
source_url: https://arxiv.org/abs/2506.07595
tags:
- delayed
- regret
- dtot
- convex
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online convex optimization with delayed feedback
  under strongly convex and exp-concave losses. Existing approaches for strongly convex
  losses yield regret bounds of order $d{\max} \ln T$, which can be significantly
  worse than the $\sqrt{d{\mathrm{tot}}}$ bound achieved by delayed gradient descent,
  where $d{\max}$ is the maximum delay, $d{\mathrm{tot}}$ is the total delay, and
  $T$ is the time horizon.
---

# Exploiting Curvature in Online Convex Optimization with Delayed Feedback

## Quick Facts
- arXiv ID: 2506.07595
- Source URL: https://arxiv.org/abs/2506.07595
- Reference count: 40
- Primary result: Achieves O(min{σ_max ln T, √d_tot}) regret for strongly convex losses and O(min{d_max n ln T, √d_tot}) for exp-concave losses under delayed feedback

## Executive Summary
This paper addresses online convex optimization with delayed feedback by leveraging the curvature of loss functions to improve regret bounds beyond what's achievable with standard delayed gradient descent. The authors develop algorithms that achieve regret bounds that adapt to the actual delay pattern, specifically achieving O(min{σ_max ln T, √d_tot}) where σ_max is the maximum number of missing observations and d_tot is the total delay. This bound is always no larger than the d_max ln T bounds achieved by existing approaches for strongly convex losses, and can be substantially smaller in practice.

## Method Summary
The paper proposes three algorithms for different loss settings. For strongly convex losses, it introduces a follow-the-regularized-leader variant that includes all past decisions in the squared-ℓ₂ regularizer, enabling regret bounds independent of domain diameter. For exp-concave losses, it extends Online Newton Step with adaptive learning rate tuning that achieves both logarithmic and √d_tot bounds simultaneously by tracking two delay measures. For unconstrained online linear regression, it presents a Vovk-Azoury-Warmuth forecaster variant with clipping to control gradient growth. All algorithms share a common decomposition approach using "cheating" iterates with full information and delay-adaptive components.

## Key Results
- For strongly convex losses: Achieves O(min{σ_max ln T, √d_tot}) regret, always better than the d_max ln T bounds from existing methods
- For exp-concave losses: Achieves O(min{d_max n ln T, √d_tot}) regret through adaptive learning rate that tracks both delay measures
- For unconstrained OLR: Achieves similar guarantees through clipping predictions to bound gradients, maintaining the min{} structure in regret

## Why This Works (Mechanism)

### Mechanism 1: Full-History Regularization Alignment
The key insight is including all past decisions (not just observed gradients) in the squared-ℓ₂ regularizer. This alignment ensures the actual iterate x_t and the "cheating" iterate x*_t share identical regularization terms, yielding ||x*_t - x_t||² ≤ ||Σ_{τ∈m_t} g_τ||² / (2λ(t-1)) where m_t is the set of missing observations. This stability analysis eliminates domain diameter dependence.

### Mechanism 2: Delay-Adaptive Learning Rate via Dual-Bound Tracking
The ONS algorithm's drift term satisfies two bounds simultaneously: O(d_max n ln T) via the elliptical potential lemma, and O(√d_tot) via direct summation with adaptive η_t. The learning rate η_t = min{a_t, b_t} + 1 selects the tighter bound dynamically at each round, achieving O(min{d_max n ln T, √d_tot}) without knowing delay statistics a priori.

### Mechanism 3: Clipping for Gradient Control in Unconstrained Domains
Without clipping, unconstrained R^n allows predictions to grow arbitrarily, causing gradient blowup. The clipping êx_t = x_t · min{ρ_t/|⟨z_t, x_t⟩|, 1} ensures predictions stay bounded, limiting the clipping error cost to O(Y²σ_max). Combined with σ_max ≤ 2√2·√d_tot, this preserves the min{} regret structure.

## Foundational Learning

- **Concept: Follow-the-Regularized-Leader (FTRL)**
  - Why needed here: Base optimization framework; understanding FTRL regret decomposition and stability is essential for the analysis
  - Quick check question: Can you derive why FTRL with strongly convex regularizer ψ_t has stability ||x_t - x_{t+1}|| ≤ ||g_t||_{A_t^{-1}} where A_t is the regularizer Hessian?

- **Concept: Strong Convexity vs. Exp-Concavity**
  - Why needed here: These curvature properties enable logarithmic regret; understanding their differences is crucial for the distinct analyses
  - Quick check question: Why does exp-concavity imply the quadratic lower bound f(x) ≥ f(y) + ⟨∇f(y), x-y⟩ + (β/2)⟨∇f(y), x-y⟩² with β = (1/2)min{1/(4GD), α}?

- **Concept: Regret Decomposition with "Cheating" Iterates**
  - Why needed here: All three algorithms use the decomposition Reg_T = Reg*_T + Drift_T, where Reg*_T is regret against hypothetical iterates with full information
  - Quick check question: In the regret decomposition, why does the negative quadratic term appear, and what role does it play in canceling regularization terms?

## Architecture Onboarding

- **Component map:** Gradient Buffer -> Delay Tracker -> Optimization Core -> Learning Rate Scheduler -> Clipping Module (OLR only)
- **Critical path:**
  1. Receive feature z_t or select action x_t from optimization core
  2. Apply clipping if in OLR mode
  3. Observe environment feedback (receive new gradients/labels)
  4. Update delay tracker statistics
  5. Compute new learning rate η_{t+1} if adaptive
  6. Update optimization core (FTRL: recompute x_{t+1}; ONS: rank-1 matrix update)
- **Design tradeoffs:**
  - ONS requires O(n²) memory for full matrix vs. O(nd_max) for diagonal approximation
  - Storing all pending gradients costs O(d_max) memory; dropping stale gradients violates analysis
  - Theoretical analysis assumes T is known for ln(T) terms; use doubling trick in practice
  - Adaptive learning rates require timestamp metadata per gradient
- **Failure signatures:**
  - Regret blows up to O(√T) instead of O(ln T): check curvature parameter or gradient bounds
  - Domain diameter appears in regret: regularization isn't summing over all s≤t
  - Adaptive rate stuck at one bound: check if delay distribution is pathological
  - Clipping errors accumulate: verify label boundedness assumption
- **First 3 experiments:**
  1. Strongly convex synthetic test with quadratic losses and two delay regimes (uniform small delays vs. sparse long delays)
  2. Exp-concave stress test verifying ONS with adaptive η_t matches O(min{d_max n ln T, √d_tot})
  3. Unconstrained OLR validation with comparator outside typical prediction range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is O(min{σ_max n ln T, √d_tot}) achievable for exp-concave losses, matching the strongly convex case?
- Basis in paper: The paper explicitly states this remains open in the conclusion
- Why unresolved: Current analysis yields O(min{d_max n ln T, √d_tot}) due to the elliptical potential lemma using d_max rather than σ_max
- What evidence would resolve it: Either an algorithm achieving O(σ_max n ln T) for exp-concave losses, or a lower bound showing this is impossible

### Open Question 2
- Question: Are the regret bounds minimax optimal for delayed OCO with curved losses?
- Basis in paper: The paper provides upper bounds but doesn't establish matching lower bounds
- Why unresolved: Minimax optimality requires proving matching lower bounds for all delay patterns
- What evidence would resolve it: A lower bound construction showing any algorithm must incur at least Ω(min{σ_max ln T, √d_tot}) for strongly convex losses

### Open Question 3
- Question: Can the adaptive learning rate tuning avoid requiring knowledge of gradient timestamps?
- Basis in paper: Section 4 notes the current approach requires timestamp knowledge to compute d^max_≤t
- Why unresolved: The current analysis splits based on a_t ≤ b_t at each round, requiring perceived delay tracking
- What evidence would resolve it: An adaptive algorithm achieving the same bounds using only aggregate delay statistics

## Limitations

- Theoretical guarantees rely on strong assumptions including strong convexity/exp-concavity, bounded gradients and domains, and known curvature parameters
- Adaptive algorithms require tracking multiple delay statistics simultaneously, potentially incurring computational overhead
- The clipping mechanism for unconstrained OLR assumes bounded labels, which may not hold in practice

## Confidence

- **High**: Regret bounds for strongly convex losses - directly follows from stability analysis and established FTRL theory
- **Medium**: Adaptive learning rate for exp-concave losses - analysis is sound but depends on simultaneous tracking of two delay measures
- **Medium**: Clipping for unconstrained OLR - gradient control is verified but the σ_max ≤ 2√2·√d_tot relationship requires additional validation

## Next Checks

1. **Curvature parameter sensitivity**: Systematically vary λ and α across orders of magnitude to identify thresholds where logarithmic regret transitions to √T behavior
2. **Delay distribution impact**: Test algorithms on heavy-tailed delay distributions (e.g., log-normal) to evaluate robustness beyond the bounded-delay assumption
3. **Computational overhead measurement**: Benchmark the matrix operations in ONS and the multiple delay trackers against simpler baselines to quantify practical efficiency gains