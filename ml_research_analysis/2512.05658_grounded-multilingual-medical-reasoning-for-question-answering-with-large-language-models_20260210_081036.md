---
ver: rpa2
title: Grounded Multilingual Medical Reasoning for Question Answering with Large Language
  Models
arxiv_id: '2512.05658'
source_url: https://arxiv.org/abs/2512.05658
tags:
- medical
- zhang
- reasoning
- traces
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multilingual medical reasoning
  in question answering, where existing approaches rely on English-only datasets and
  lack factual grounding in medical knowledge. The authors propose a retrieval-augmented
  generation pipeline that creates reasoning traces in English, Italian, and Spanish,
  grounded in Wikipedia medical content.
---

# Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models

## Quick Facts
- **arXiv ID:** 2512.05658
- **Source URL:** https://arxiv.org/abs/2512.05658
- **Reference count:** 39
- **Primary result:** State-of-the-art multilingual medical QA performance using 500k reasoning traces grounded in Wikipedia, with accuracy gains of 7-10 points across English, Italian, and Spanish

## Executive Summary
This paper addresses the challenge of multilingual medical reasoning in question answering, where existing approaches rely on English-only datasets and lack factual grounding in medical knowledge. The authors propose a retrieval-augmented generation pipeline that creates reasoning traces in English, Italian, and Spanish, grounded in Wikipedia medical content. These traces are generated for questions from MedQA and MedMCQA, extended to the target languages, and validated to ensure correctness. Experiments show that using these traces via in-context learning or fine-tuning improves accuracy across all tested languages, with state-of-the-art performance among 8B-parameter models. Cross-lingual fine-tuning further enhances performance, and error analysis identifies limited medical knowledge as the primary bottleneck.

## Method Summary
The authors developed a retrieval-augmented generation pipeline to create multilingual reasoning traces for medical question answering. The system retrieves top-5 relevant chunks from a curated Medical-Wikipedia knowledge base using embedding similarity, rewrites them for conciseness via an LLM, then prompts a reasoning model (Qwen3-32B) to generate step-by-step traces constrained by this external context plus the correct answer. The pipeline produces 500k traces across English, Italian, and Spanish for questions from MedMCQA and MedQA. These traces are used to improve performance through in-context learning (2-shot retrieval) and fine-tuning (3 epochs on 34k reasoning-intensive examples), achieving state-of-the-art results among 8B-parameter models.

## Key Results
- State-of-the-art accuracy on MedExpQA (50.1%), MedMCQA (72.5%), and MedQA (75.6%) among 8B-parameter models
- Cross-lingual fine-tuning improves performance by 2-3 points compared to single-language training
- In-context learning with reasoning traces outperforms baselines by 5-10 points across all languages
- Error analysis shows medical knowledge gaps as the primary failure mode

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Grounded Reasoning Trace Generation
Retrieving domain-specific medical context before generating reasoning traces improves factual accuracy and reduces hallucination compared to relying solely on parametric model knowledge. The system retrieves top-5 relevant chunks from a curated Medical-Wikipedia knowledge base using embedding similarity, rewrites them for conciseness via an LLM, then prompts a reasoning model to generate step-by-step traces constrained by this external context plus the correct answer.

### Mechanism 2: Cross-Lingual Transfer Through Multilingual Training
Joint training on reasoning traces across multiple languages improves per-language performance compared to single-language training. Training on all three languages (EN, IT, ES) simultaneously allows the model to learn shared reasoning patterns that transfer across languages, while maintaining language-specific generation capabilities through the traces themselves.

### Mechanism 3: Verification-Based Quality Filtering
Discarding traces where reasoning doesn't lead to the correct answer improves downstream task performance. After generating traces with access to the correct answer, the system extracts the final answer via regex matching and discards inconsistent traces (~5%), ensuring training data consistency.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Understanding how retrieved context is incorporated into generation is critical for this pipeline. Quick check: Can you explain why retrieving chunks *before* generation differs from post-hoc fact-checking?
- **Chain-of-Thought Reasoning**: The traces explicitly use CoT format with "Search → Reasoning → Conclusion" structure. Quick check: What is the difference between few-shot CoT at inference vs. training on CoT traces?
- **Cross-Lingual Transfer in LLMs**: Understanding why multilingual training helps requires knowing how shared representations emerge across languages. Quick check: Why might training on multiple languages help even when evaluating on just one?

## Architecture Onboarding

- **Component map:**
  Wikipedia Medical Pages → KB Construction → Chunking → Embedding Index
                                                              ↓
  Question-Options → Retrieval (top-5) → Context Rewriting → Prompt Assembly
                                                              ↓
                                              Reasoning Model (Qwen3-32B) → Trace
                                                              ↓
                                        Answer Extraction (regex) → Filter → Dataset

- **Critical path:** The retrieval-to-generation path is critical; poor retrieval or context rewriting propagates errors through the entire pipeline.

- **Design tradeoffs:**
  - Wikipedia vs. authoritative medical sources: Wikipedia chosen for cross-lingual parallelism, but may lack depth for rare conditions
  - 500k traces vs. curated subset: Full dataset for coverage, but fine-tuning used only ~35k filtered for "reasoning-intensive" questions
  - Answer-guided generation: Ensures correctness but may reduce reasoning diversity

- **Failure signatures:**
  - Low retrieval relevance: Check if top-5 chunks actually address the question topic
  - Language collapse: Model generates English reasoning for non-English questions (mitigated by prompt design)
  - Knowledge gaps in KB: English KB is 4x larger than IT/ES; may cause cross-lingual performance gaps

- **First 3 experiments:**
  1. Validate retrieval quality: Sample 50 questions, manually assess whether retrieved chunks contain information needed to answer. Target: >80% relevance.
  2. Ablate context rewriting: Generate traces with raw vs. rewritten context; measure trace length and accuracy. Expected: rewritten context should be shorter with similar or better accuracy.
  3. Cross-lingual transfer test: Train on EN-only vs. all-three-languages; evaluate on IT and ES test sets. Expect multilingual training to win by 2-5 points.

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning larger models (e.g., 70B+ parameters) on grounded multilingual reasoning traces yield diminishing returns compared to the improvements observed in 8B models?
- **Basis in paper:** The authors state in the Limitations section: "our fine-tuning experiments are restricted to 8B-parameter models because of computational constraints; scaling to larger models remains unexplored."
- **Why unresolved:** It is unclear if the relative performance gain from these specific reasoning traces scales linearly with model size or if larger models inherently possess sufficient reasoning capabilities to render this specific grounding less impactful.
- **What evidence would resolve it:** A comparative study fine-tuning a 70B parameter model on the same reasoning traces and measuring the performance delta against the base 70B model on MedQA/MedMCQA benchmarks.

### Open Question 2
To what extent does the use of professionally curated medical guidelines (e.g., clinical practice guidelines) instead of Wikipedia improve the factual accuracy and safety of the generated reasoning traces?
- **Basis in paper:** The authors note: "we rely on Wikipedia as our primary source of medical knowledge... though more authoritative medical sources could further strengthen grounding."
- **Why unresolved:** While Wikipedia offers cross-lingual parallelism, its medical content may lack the precision required for safety-critical reasoning, leaving the potential utility of more authoritative, albeit harder-to-align, sources unknown.
- **What evidence would resolve it:** An ablation study replacing the Wikipedia Knowledge Base with a translated corpus of medical guidelines and measuring the reduction in "medical knowledge mistakes" identified in the error analysis.

### Open Question 3
Can models trained on these multiple-choice reasoning traces effectively transfer their capabilities to open-ended clinical tasks, such as differential diagnosis generation or clinical note summarization?
- **Basis in paper:** The paper limits its scope: "we limit our study to multiple-choice medical question answering, leaving open-ended clinical reasoning tasks for future work."
- **Why unresolved:** Multiple-choice QA provides a constrained solution space; it is uncertain if the learned reasoning logic generalizes to free-text generation where no candidate options are provided as guardrails.
- **What evidence would resolve it:** Evaluation of the fine-tuned models on open-ended benchmarks (e.g., clinical note generation) using expert evaluation of the coherence and factual grounding of the free-text output.

## Limitations

- The KB size disparity (English is 4× larger than Italian/Spanish) may confound cross-lingual performance results
- Trace generation uses answer-conditioning, which may produce post-hoc rationalizations rather than genuine reasoning
- The exact filtering criteria for "reasoning-intensive" examples remain partially unspecified
- Qwen3 model family availability is uncertain, as these models were referenced as newly released

## Confidence

**High confidence:** The retrieval-augmented generation pipeline successfully produces multilingual reasoning traces, and the observed performance improvements from using these traces (both via ICL and fine-tuning) are robust across evaluation datasets.

**Medium confidence:** The claim that cross-lingual fine-tuning provides consistent benefits across all three languages, though the corpus provides limited direct evidence for this specific mechanism.

**Low confidence:** The assertion that the generated traces are genuinely "grounded" in medical knowledge rather than post-hoc rationalizations, given the verification process only checks answer consistency.

## Next Checks

1. **Knowledge grounding validation:** Manually audit 100 generated traces across all three languages to assess whether the reasoning genuinely incorporates retrieved medical context versus post-hoc rationalization. Target: >70% of traces show clear context integration.

2. **Cross-lingual knowledge parity:** Compare retrieval relevance scores and reasoning quality across EN/IT/ES datasets to quantify the impact of KB size disparities. Expected: Spanish traces show 15-20% lower quality scores due to smaller KB.

3. **Generalization test:** Evaluate the fine-tuned model on medical QA datasets from non-Wikipedia knowledge sources (e.g., PubMed abstracts or clinical guidelines) to assess whether the model learned genuine medical reasoning or Wikipedia-specific patterns. Target: maintain >70% of in-domain performance.