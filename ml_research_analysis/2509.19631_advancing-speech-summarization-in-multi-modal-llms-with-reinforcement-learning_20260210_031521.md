---
ver: rpa2
title: Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning
arxiv_id: '2509.19631'
source_url: https://arxiv.org/abs/2509.19631
tags:
- speech
- summarization
- phi-4mm
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the performance gap between open-source multi-modal
  large language models (MLLMs) and state-of-the-art text-based LLMs in speech summarization,
  a task of generating textual summaries directly from spoken input. The authors propose
  a three-stage training framework: (1) supervised fine-tuning on large-scale synthetic
  summarization data, (2) on-policy knowledge distillation to transfer summarization
  ability from a strong text-based LLM to the audio-conditioned MLLM, and (3) Direct
  Preference Optimization to reduce hallucinations and improve output quality.'
---

# Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2509.19631
- **Source URL:** https://arxiv.org/abs/2509.19631
- **Reference count:** 0
- **Primary result:** Up to 28% relative performance gain in speech summarization; surpasses GPT-4o-audio and narrows gap with text-based LLMs

## Executive Summary
This paper addresses the performance gap between open-source multi-modal large language models (MLLMs) and state-of-the-art text-based LLMs in speech summarization. The authors propose a three-stage training framework that combines supervised fine-tuning, on-policy knowledge distillation, and Direct Preference Optimization to enable MLLMs to generate high-quality textual summaries from spoken input. The approach successfully transfers summarization capabilities from strong text-based models while grounding them in audio, resulting in substantial performance improvements across benchmarks.

## Method Summary
The proposed approach employs a three-stage training pipeline. First, supervised fine-tuning adapts the MLLM to summarization using large-scale synthetic data. Second, on-policy knowledge distillation transfers summarization ability from a strong text-based LLM to the audio-conditioned MLLM, allowing the student model to benefit from the teacher's linguistic competence while maintaining audio grounding. Third, Direct Preference Optimization is applied to reduce hallucinations and improve output quality. This multi-stage framework systematically addresses the modality gap by combining data-driven adaptation with preference-based refinement.

## Key Results
- Up to 28% relative performance gain over strong baselines in speech summarization
- Surpasses GPT-4o-audio and narrows performance gap with leading text-based LLMs
- Demonstrates strong zero-shot cross-lingual generalization capabilities

## Why This Works (Mechanism)
The approach works by leveraging the complementary strengths of text-based and multi-modal models. The knowledge distillation stage enables the MLLM to inherit the superior summarization patterns from the text-based LLM while maintaining its ability to process audio inputs. Direct Preference Optimization then fine-tunes the model to produce more coherent, hallucination-free summaries that align with human preferences. This multi-stage training allows the model to bridge the modality gap by combining the linguistic competence of text models with the audio understanding capabilities of MLLMs.

## Foundational Learning

**Knowledge Distillation** - Transferring knowledge from a larger, stronger model to a smaller one. *Why needed:* Enables MLLMs to benefit from the superior summarization capabilities of text-based LLMs. *Quick check:* Verify teacher-student performance gap reduction.

**Direct Preference Optimization (DPO)** - Reinforcement learning from human preferences to optimize model outputs. *Why needed:* Reduces hallucinations and aligns outputs with human preferences. *Quick check:* Compare preference scores before and after DPO.

**Audio-Text Alignment** - Ensuring the model properly grounds text generation in audio input. *Why needed:* Critical for maintaining speech summarization quality and relevance. *Quick check:* Measure audio-text coherence metrics.

## Architecture Onboarding

**Component Map:** Audio Encoder -> MLLM Backbone -> Text Decoder -> Preference Optimizer

**Critical Path:** Audio input → Audio encoder features → MLLM processing → Text generation → DPO refinement

**Design Tradeoffs:** The approach trades computational cost (three-stage training) for performance gains, choosing knowledge distillation over full model training to leverage existing strong text-based models.

**Failure Signatures:** Performance degradation when audio quality varies significantly, hallucination increase when audio-text alignment is weak, and limited generalization when synthetic training data doesn't cover real-world variations.

**First 3 Experiments to Run:**
1. Ablation study removing each training stage to quantify individual contributions
2. Cross-domain evaluation on diverse speech datasets (podcasts, meetings, interviews)
3. Zero-shot performance comparison across multiple languages with error analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on specific benchmarks that may not capture all spoken content challenges
- Requires access to strong text-based LLM for knowledge distillation, limiting deployment in resource-constrained settings
- Computational costs of three-stage training pipeline are not discussed

## Confidence

**High confidence** in core technical contribution - The three-stage framework is clearly articulated and experimentally validated with substantial improvements.

**Medium confidence** in generalizability of results - Performance on diverse speech datasets and in production environments remains to be validated.

**Medium confidence** in claimed gap reduction with GPT-4o-audio - Requires further validation without access to exact capabilities and comprehensive head-to-head testing.

## Next Checks
1. Conduct ablation studies removing each training stage to quantify individual contributions to overall performance.
2. Evaluate the model on speech datasets with higher acoustic variability (e.g., podcasts, meetings, interviews) and different domains to test robustness.
3. Perform comprehensive cost-benefit analysis comparing performance gains against computational overhead relative to simpler fine-tuning approaches.