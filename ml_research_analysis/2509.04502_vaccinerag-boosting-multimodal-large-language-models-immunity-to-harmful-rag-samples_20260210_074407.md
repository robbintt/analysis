---
ver: rpa2
title: 'VaccineRAG: Boosting Multimodal Large Language Models'' Immunity to Harmful
  RAG Samples'
arxiv_id: '2509.04502'
source_url: https://arxiv.org/abs/2509.04502
tags:
- samples
- retrieval
- reward
- generation
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VaccineRAG addresses the problem of harmful or irrelevant retrieval\
  \ samples in RAG systems, which degrade the performance of multimodal large language\
  \ models. The core method introduces a Chain-of-Thought-based dataset and a novel\
  \ Partial-GRPO training approach, where the model explicitly reasons about each\
  \ retrieval sample\u2019s helpfulness before generating a final answer."
---

# VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples

## Quick Facts
- arXiv ID: 2509.04502
- Source URL: https://arxiv.org/abs/2509.04502
- Reference count: 40
- Primary result: Up to 56.27% mean accuracy and over 15% reduction in accuracy degradation rates on polluted generation tasks

## Executive Summary
VaccineRAG addresses the critical challenge of harmful or irrelevant retrieval samples degrading multimodal large language model performance in RAG systems. The method introduces a Chain-of-Thought-based dataset and a novel Partial-GRPO training approach where the model explicitly reasons about each retrieval sample's helpfulness before generating a final answer. Experimental results demonstrate significant improvements in both accuracy and robustness, with the model achieving up to 56.27% mean accuracy and reducing accuracy degradation rates by over 15% compared to baselines when facing polluted retrieval samples.

## Method Summary
VaccineRAG employs a two-stage training process to improve multimodal RAG robustness. First, Supervised Fine-Tuning (SFT) on 15% of the data teaches the model to follow a structured Chain-of-Thought template. Second, Partial-GRPO reinforcement learning optimizes three separate reward functions (format, helpfulness, conclusion) with fine-grained gradient backpropagation applied only to specific token segments. This approach forces the model to analyze each retrieval sample's relevance before synthesis, creating a "reasoning bottleneck" that suppresses the influence of harmful samples. The method is validated on WebQA using Qwen2-VL and InternVL3 models.

## Key Results
- Mean accuracy reaches 56.27% on polluted generation tasks, significantly outperforming baselines
- Accuracy degradation rates improve by over 15% when retrieval samples contain harmful content
- In top-K retrieval scenarios, accuracy improves by up to 5 percentage points over strong baselines when using 10 or 15 retrieval samples

## Why This Works (Mechanism)

### Mechanism 1
Explicit Chain-of-Thought (CoT) analysis forces the model to filter irrelevant context before generation, reducing hallucination. Instead of directly generating answers from mixed retrieval samples, the model generates structured analysis for each sample first, introducing a "reasoning bottleneck" that suppresses spurious evidence influence.

### Mechanism 2
Partial-GRPO prevents gradient dilution by isolating reward signals to specific segments of the long CoT sequence. Unlike standard GRPO that applies single scalar rewards to entire sequences, Partial-GRPO calculates separate rewards and backpropagates them only to specific tokens responsible for those outputs.

### Mechanism 3
Multi-reward discrimination separates the ability to find evidence from the ability to synthesize it. The training uses three distinct rewards (format, helpfulness, conclusion) that are optimized separately, allowing the model to filter noise independently of generating the answer.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: The paper builds upon GRPO, which optimizes a policy by comparing a group of sampled outputs against each other rather than using a separate value function model.
  - Quick check: Can you explain why GRPO is more memory-efficient than standard PPO with a critic network?

- **Concept: Credit Assignment in Reinforcement Learning**
  - Why needed: Partial-GRPO is a solution to the credit assignment problemâ€”determining which action (token/segment) led to which outcome (reward).
  - Quick check: If a model generates a correct final answer but cites a bad document, should the reward be positive or negative for the "citation" tokens specifically?

- **Concept: Multimodal RAG**
  - Why needed: Unlike text-only RAG, the system retrieves images and text, requiring the model to cross-reference visual information with text.
  - Quick check: How does the retriever's modality bias (prioritizing text over image or vice versa) affect the noise level in the training data?

## Architecture Onboarding

- **Component map:** Retriever (Frozen) -> Input Processor (Query + Top-K Samples) -> MLLM (Trainable) -> Output Segments (Reference Analysis -> Conclusion -> Final Answer) -> Reward Scorer -> Optimizer (Partial-GRPO)

- **Critical path:** The SFT Warm-up (Stage I) is strictly required to teach the model the output template. If the model cannot reliably generate special tokens during sampling, the Partial-GRPO reward parser will fail.

- **Design tradeoffs:**
  - CoT Length vs. Latency: Explicit analysis of every retrieval sample significantly increases inference token count to improve accuracy.
  - Format Strictness: The system requires rigid template adherence, which enables Partial-GRPO but reduces natural language flexibility.

- **Failure signatures:**
  - Format Collapse: If Format Reward is removed or too weak, the model generates unparsable text, breaking the regex parser for partial rewards.
  - Advantage Misalignment: Using vanilla GRPO shows high variance in loss and potential degradation in specific reasoning steps.

- **First 3 experiments:**
  1. Sanity Check (SFT Only): Train only on the VaccineRAG dataset using SFT (15% data) to verify the model learns the CoT template structure before attempting RL.
  2. Pollution Robustness Test: Inject increasing amounts of "hard negative" samples into retrieval set and plot accuracy degradation curve to verify "immunity" claims.
  3. Ablation on Segments: Run training loop with Partial-GRPO vs. Vanilla GRPO on small validation split, monitoring convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
How does VaccineRAG's Partial-GRPO approach generalize to other multimodal RAG domains and tasks beyond the WebQA benchmark? The dataset construction relies solely on WebQA as the basis, and all experimental evaluations are conducted on WebQA validation data, with no cross-domain testing mentioned.

### Open Question 2
How does the performance and robustness of VaccineRAG scale with the number of retrieval samples beyond K=15? The Top-K experiments only test K values up to 15, while noting that "dozens of retrieved samples" can be included in practice.

### Open Question 3
Can Partial-GRPO's three-component reward structure be further decomposed or modified to capture more fine-grained reasoning quality? The current reward functions represent one possible segmentation, and whether additional intermediate reasoning steps could benefit from separate reward signals remains unexplored.

### Open Question 4
How dependent is the approach's effectiveness on the quality and characteristics of the GPT-4o-generated CoT annotations? The model learns to imitate GPT-4o's reasoning patterns, and if these annotations contain systematic biases or errors, the trained model's performance could be limited by annotation quality rather than the methodology itself.

## Limitations

- Scalability concerns with larger models or longer CoT sequences due to computational costs of segmentation and gradient masking
- Heavy reliance on GPT-4o for dataset annotation introduces potential biases in the "ground truth" analysis
- Limited evaluation on naturally occurring retrieval errors rather than synthetically generated harmful samples
- Focus on accuracy metrics without comprehensive analysis of inference latency trade-offs

## Confidence

- **High Confidence:** Experimental results showing improved accuracy and reduced degradation rates are well-supported by presented data
- **Medium Confidence:** Partial-GRPO mechanism explanation is logically sound but lacks direct empirical validation beyond overall performance improvements
- **Low Confidence:** Generalizability to real-world RAG systems with naturally occurring retrieval errors is uncertain due to synthetic harmful sample generation

## Next Checks

1. Create a small validation set with manually annotated CoT analysis (not generated by GPT-4o) to verify whether the model trained on GPT-4o-generated dataset can generalize to human-annotated reasoning chains.

2. Evaluate the trained VaccineRAG model on a different RAG dataset (e.g., Natural Questions with retrieval errors) to assess whether robustness improvements transfer beyond the WebQA-derived dataset used in training.

3. Measure inference time and token costs of CoT-based generation compared to standard RAG, and plot the accuracy-latency curve to quantify practical deployment trade-offs.