---
ver: rpa2
title: 'From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn
  Interactive Tool-Using Agents'
arxiv_id: '2601.22607'
source_url: https://arxiv.org/abs/2601.22607
tags:
- user
- training
- data
- agents
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework for training interactive tool-using
  agents that combine self-evolving synthetic data generation with reinforcement learning.
  The system, EigenData, uses a hierarchical multi-agent architecture to synthesize
  multi-turn dialogues with tool calls and generates executable verification functions
  for reward computation.
---

# From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents

## Quick Facts
- arXiv ID: 2601.22607
- Source URL: https://arxiv.org/abs/2601.22607
- Reference count: 40
- Key outcome: Best model reaches 73.0% pass¹ on Airline and 98.3% pass¹ on Telecom using Qwen3-235B-A22B

## Executive Summary
This paper presents EigenData, a framework for training interactive tool-using agents that combine self-evolving synthetic data generation with verifiable-reward reinforcement learning. The system addresses two key challenges in multi-turn interactive scenarios: scalable data acquisition and stable RL training under user simulation variance. Through a hierarchical multi-agent architecture that synthesizes multi-turn dialogues with tool calls and generates executable verification functions, the approach achieves state-of-the-art results on the τ²-bench benchmark.

## Method Summary
The EigenData framework uses a hierarchical multi-agent architecture to generate synthetic multi-turn dialogues with tool calls. A core innovation is the verifiable-reward RL approach, where verification functions are automatically generated to compute rewards for tool usage. The framework includes user model fine-tuning for stable simulation, large-batch training with group-relative advantages, and dynamic filtering to remove uninformative trajectories. This combination enables training of agents that can handle complex, interactive scenarios requiring multiple tool calls across several dialogue turns.

## Key Results
- Achieves state-of-the-art performance on τ²-bench benchmark
- Best model reaches 73.0% pass¹ on Airline and 98.3% pass¹ on Telecom
- Matches or exceeds frontier models including Qwen3-235B-A22B

## Why This Works (Mechanism)
The system works by creating a closed-loop training pipeline where synthetic data generation and RL reinforce each other. The multi-agent architecture simulates realistic user interactions, while the verifiable-reward mechanism provides stable, deterministic feedback for training. This addresses the data scarcity problem for interactive scenarios while maintaining training stability through the automatic generation of reward functions.

## Foundational Learning
- **Multi-turn dialogue modeling**: Needed to capture context and user intent across conversation turns; check by evaluating coherence over 3+ turns
- **Tool use verification**: Required for automated reward computation; check by comparing verification accuracy against human judgments
- **Hierarchical multi-agent systems**: Essential for scalable synthetic data generation; check by measuring data diversity and coverage
- **Reinforcement learning with group-relative advantages**: Improves training stability; check by comparing convergence speed with baseline methods

## Architecture Onboarding

**Component Map**: User Simulator -> Tool Agent -> Verification Generator -> RL Trainer -> Policy Agent

**Critical Path**: User Simulator creates dialogue context → Tool Agent executes actions → Verification Generator creates reward function → RL Trainer updates policy → Policy Agent improves tool use

**Design Tradeoffs**: Synthetic data generation vs. human data quality, verifiable rewards vs. flexible reward modeling, batch size vs. training stability

**Failure Signatures**: Unrealistic dialogue patterns, tool misuse, reward hacking, context forgetting in long conversations

**First Experiments**:
1. Test single-turn tool use accuracy with ground truth rewards
2. Evaluate multi-turn dialogue coherence without tool calls
3. Assess verification function accuracy on known tool interactions

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those discussed in the limitations section.

## Limitations
- Evaluation restricted to only two domains (Airline and Telecom) in τ²-bench benchmark
- Synthetic data may not fully capture human interaction diversity and unpredictability
- Verifiable-reward assumption may not hold for complex or subjective tasks

## Confidence
- High confidence in technical implementation of EigenData framework
- Medium confidence in scalability claims (single large-scale experiment)
- Medium confidence in RL training stability improvements (empirical evidence without comprehensive comparisons)

## Next Checks
1. Evaluate trained agents on at least three additional domains with different interaction patterns and tool complexity
2. Conduct user studies with real human evaluators to compare synthetic versus human-generated interaction quality
3. Test agents on multi-turn dialogues exceeding 10 turns to evaluate memory retention and context coherence