---
ver: rpa2
title: A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice
arxiv_id: '2511.00257'
source_url: https://arxiv.org/abs/2511.00257
tags:
- batch
- expert
- advice
- learner
- special
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes the first tight lower bound for the non-stochastic\
  \ multi-armed bandit with expert advice (BwE) problem. The minimax optimal expected\
  \ regret is proven to be \u0398(\u221AT K log(N/K)), matching the upper bound from\
  \ Kale (2014)."
---

# A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice

## Quick Facts
- arXiv ID: 2511.00257
- Source URL: https://arxiv.org/abs/2511.00257
- Authors: Zachary Chase; Shinji Ito; Idan Mehalel
- Reference count: 6
- Primary result: Establishes first tight lower bound Θ(√T K log(N/K)) for non-stochastic multi-armed bandit with expert advice

## Executive Summary
This work resolves a long-standing open problem in bandit theory by proving the first tight lower bound for the non-stochastic multi-armed bandit with expert advice (BwE) problem. The minimax optimal expected regret is shown to be Θ(√T K log(N/K)), matching the upper bound established by Kale in 2014. The key innovation is a novel reduction technique that transforms a special batch identification problem into the BwE setting, enabling the proof of lower bounds against general (non-proper) learners. This closes a 20-year gap in the literature where only loose lower bounds were known.

## Method Summary
The authors develop a reduction framework that connects the BwE problem to a special batch identification problem. By constructing an adaptive adversary, they demonstrate that any algorithm achieving small expected regret must identify a "special batch" of experts with high probability. The core insight is showing that identifying this batch requires Ω(√T K log(N/K)) rounds, thereby establishing the lower bound. This approach overcomes the challenge of proving lower bounds against non-proper learners, which had previously been a major obstacle in the field.

## Key Results
- Proves the first tight lower bound of Θ(√T K log(N/K)) for non-stochastic multi-armed bandit with expert advice
- Matches the upper bound established by Kale (2014), closing a 20-year gap in the literature
- Improves upon the previous best lower bound by a factor of √log K

## Why This Works (Mechanism)
The reduction from batch identification to BwE creates a bridge between two seemingly unrelated problems, allowing techniques from identification problems to be applied to bandit regret analysis. The adaptive adversary is carefully constructed to exploit the trade-off between exploration and exploitation, forcing any low-regret algorithm to reveal information about the special batch of experts.

## Foundational Learning

1. **Non-stochastic Multi-armed Bandit with Expert Advice**: A setting where an algorithm must choose actions based on advice from N experts while competing against the best expert in hindsight. Needed because this is the problem being analyzed. Quick check: Verify understanding of the N experts, K actions, and T rounds framework.

2. **Minimax Regret**: The maximum regret over all possible problem instances, minimized over all algorithms. Needed as the performance metric being bounded. Quick check: Confirm that Θ(√T K log(N/K)) represents the optimal worst-case performance.

3. **Proper vs Non-proper Learners**: Proper learners output distributions over experts, while non-proper learners can use arbitrary strategies. Needed because the lower bound applies to general (non-proper) learners. Quick check: Verify that the reduction technique works for both proper and non-proper learners.

4. **Batch Identification Problems**: Problems where the goal is to identify a subset of items with certain properties. Needed as the source problem for the reduction. Quick check: Confirm that the special batch identification problem is correctly formulated.

## Architecture Onboarding

**Component Map**: Batch Identification Problem -> Reduction Framework -> Adaptive Adversary -> BwE Lower Bound

**Critical Path**: The proof follows a clear path: (1) Define the special batch identification problem, (2) Show that any low-regret BwE algorithm must solve this identification problem, (3) Prove a lower bound for the identification problem, (4) Conclude the BwE lower bound through the reduction.

**Design Tradeoffs**: The reduction approach trades direct analysis of the BwE problem for a more tractable identification problem. This allows handling non-proper learners but requires careful construction of the adaptive adversary to ensure the reduction is valid.

**Failure Signatures**: Potential issues include: (1) The adaptive adversary not being sufficiently powerful to force identification, (2) The reduction not preserving the essential properties needed for the lower bound, (3) Errors in the batch identification lower bound proof that would propagate to the BwE bound.

**3 First Experiments**:
1. Verify the adaptive adversary construction by checking that it indeed forces any low-regret algorithm to identify the special batch with high probability.
2. Confirm that the batch identification problem lower bound of Ω(√T K log(N/K)) is correctly established and tight.
3. Test whether the reduction framework can be extended to related bandit problems with expert advice to see if similar tight bounds can be derived.

## Open Questions the Paper Calls Out
None

## Limitations
- The result assumes the stochastic setting where losses are independent and identically distributed, which may not capture all practical scenarios.
- The tight bound applies specifically to the minimax setting, and whether similar bounds hold for other performance metrics or problem variants remains open.

## Confidence
- Main claim (tight lower bound Θ(√T K log(N/K))): High
- Reduction technique validity: High
- Adaptive adversary construction: High
- Extension to other bandit problems: Medium

## Next Checks
1. Verify the correctness of the adaptive adversary construction by checking that it indeed forces any low-regret algorithm to identify the special batch with high probability.
2. Confirm that the batch identification problem lower bound of Ω(√T K log(N/K)) is correctly established and tight.
3. Test whether the reduction framework can be extended to related bandit problems with expert advice to see if similar tight bounds can be derived.