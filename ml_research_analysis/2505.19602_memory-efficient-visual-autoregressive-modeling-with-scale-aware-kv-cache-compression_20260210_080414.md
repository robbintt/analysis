---
ver: rpa2
title: Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression
arxiv_id: '2505.19602'
source_url: https://arxiv.org/abs/2505.19602
tags:
- arxiv
- cache
- attention
- preprint
- scalekv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory bottleneck in Visual Autoregressive
  (VAR) modeling caused by exponential KV cache growth during multi-scale image generation.
  The authors propose ScaleKV, a scale-aware KV cache compression framework that categorizes
  transformer layers into "drafters" and "refiners" based on their attention patterns.
---

# Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression

## Quick Facts
- arXiv ID: 2505.19602
- Source URL: https://arxiv.org/abs/2505.19602
- Authors: Kunjun Li; Zigeng Chen; Cheng-Yen Yang; Jenq-Neng Hwang
- Reference count: 40
- One-line primary result: 10x memory reduction (85 GB → 8.5 GB) on Infinity-8B with minimal quality loss and up to 1.25× inference speedup

## Executive Summary
This paper addresses the memory bottleneck in Visual Autoregressive (VAR) modeling caused by exponential KV cache growth during multi-scale image generation. The authors propose ScaleKV, a scale-aware KV cache compression framework that categorizes transformer layers into "drafters" and "refiners" based on their attention patterns. Drafters require larger cache capacity due to dispersed attention across scales, while refiners need minimal cache due to localized processing. By implementing differentiated cache allocation and token selection strategies, ScaleKV achieves a 10x memory reduction (from 85 GB to 8.5 GB) on Infinity-8B while preserving pixel-level fidelity, with negligible quality degradation (GenEval score remains at 0.79, DPG score decreases marginally from 86.61 to 86.49). The method also provides up to 1.25× inference speedup and enables deployment in resource-constrained environments.

## Method Summary
ScaleKV introduces a scale-aware KV cache compression framework that differentiates between "drafter" and "refiner" transformer layers based on their attention patterns during multi-scale image generation. The method allocates larger cache capacity to drafter layers, which require dispersed attention across scales, while aggressively compressing cache for refiner layers that process localized information. Token selection strategies are employed to maintain essential information in the cache during the refinement stage. This differentiated approach enables significant memory savings without sacrificing generation quality, achieving 10x memory reduction while preserving pixel-level fidelity and providing up to 1.25× inference speedup.

## Key Results
- 10x memory reduction (85 GB → 8.5 GB) on Infinity-8B model
- Minimal quality degradation (GenEval score remains at 0.79, DPG score decreases marginally from 86.61 to 86.49)
- Up to 1.25× inference speedup achieved
- Method enables deployment in resource-constrained environments

## Why This Works (Mechanism)
The paper's core insight is that different transformer layers in VAR models have fundamentally different cache requirements based on their attention patterns. Drafter layers process information across multiple scales and thus require larger cache capacity to maintain global context, while refiner layers focus on localized details and can operate effectively with significantly compressed caches. By recognizing and exploiting this distinction, ScaleKV can aggressively compress caches for refiner layers without quality loss, while maintaining sufficient capacity for drafter layers. This differentiated approach allows for substantial memory savings while preserving the quality of generated images.

## Foundational Learning

- **KV Cache Compression**: A technique to reduce memory usage by selectively storing and updating key-value pairs in transformer attention mechanisms. *Why needed*: Standard KV caches grow linearly with sequence length and batch size, creating memory bottlenecks. *Quick check*: Measure memory usage before and after applying compression techniques.

- **Visual Autoregressive Modeling (VAR)**: A framework where image generation proceeds in a sequential, autoregressive manner, typically processing images at multiple scales. *Why needed*: VAR models are particularly susceptible to KV cache growth due to their multi-scale processing. *Quick check*: Verify that the model generates images sequentially and processes multiple scales.

- **Multi-scale Image Generation**: A technique where images are generated progressively from coarse to fine resolution. *Why needed*: This approach amplifies KV cache growth as each scale adds to the total sequence length. *Quick check*: Confirm that the model generates images at multiple resolutions during inference.

- **Attention Patterns in Transformers**: The way transformer layers distribute attention across tokens, which varies between layers and affects cache requirements. *Why needed*: Different attention patterns determine whether layers need global or local context, influencing cache allocation. *Quick check*: Analyze attention maps to identify dispersed vs. localized patterns.

- **Token Selection Strategies**: Methods for choosing which tokens to retain in compressed caches based on their importance to generation quality. *Why needed*: Selective retention is crucial for aggressive cache compression without quality loss. *Quick check*: Evaluate generation quality under different token retention rates.

## Architecture Onboarding

- **Component Map**: Image generation pipeline → Multi-scale processing → Transformer layers → KV cache management → ScaleKV compression → Output images
- **Critical Path**: Multi-scale image generation → Layer-wise KV cache management → Differentiated drafter/refiner processing → Token selection → Final image output
- **Design Tradeoffs**: The method trades some inference speed for significant memory savings, and requires careful calibration of cache allocation between drafter and refiner layers to balance memory efficiency with generation quality.
- **Failure Signatures**: If cache compression is too aggressive, generation quality may degrade; if allocation is mischaracterized, memory savings may be suboptimal; if token selection is poor, important information may be lost.
- **First Experiments**: (1) Validate memory reduction by measuring KV cache size before and after ScaleKV implementation, (2) Assess quality preservation using GenEval and DPG metrics, (3) Benchmark inference speedup under different cache compression configurations.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims primarily validated on a single large-scale model (Infinity-8B), raising questions about generalizability to other VAR architectures
- The effectiveness of the drafter/refiner distinction may not be universal across all transformer designs
- The 1.25× speedup is modest and highly dependent on hardware and implementation specifics not fully detailed

## Confidence

- **High Confidence**: The memory reduction claim (85 GB → 8.5 GB) is well-supported by the described mechanism and direct measurement. The core insight that drafter and refiner layers have different cache needs is clearly demonstrated and technically sound.

- **Medium Confidence**: The quality preservation claims (GenEval: 0.79, DPG: 86.61 → 86.49) are plausible given the minimal intervention, but depend heavily on the choice of metrics and the single dataset/baseline used. The assertion that localized attention in refiners justifies aggressive compression is reasonable but may not hold universally.

- **Low Confidence**: The inference speedup (1.25×) and generalization to other architectures or tasks are asserted but lack extensive empirical support. The method's robustness to varying image resolutions, token counts, or KV cache configurations is not thoroughly explored.

## Next Checks

1. **Cross-Architecture Generalization**: Validate ScaleKV on at least two other VAR models (e.g., DiT, Phenaki) and report memory reduction and quality metrics to confirm the approach is not model-specific.

2. **Resolution and Scale Robustness**: Test the method on higher-resolution images (e.g., 1024×1024) and non-standard scale progressions to assess whether the drafter/refiner distinction and cache allocation remain effective.

3. **KV Cache Compression Trade-offs**: Systematically vary the cache capacity and token selection thresholds for refiner layers to quantify the full Pareto frontier between memory usage, inference speed, and generation quality, and report sensitivity analyses.