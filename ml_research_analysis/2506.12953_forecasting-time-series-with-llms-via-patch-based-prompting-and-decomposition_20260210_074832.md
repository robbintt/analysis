---
ver: rpa2
title: Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition
arxiv_id: '2506.12953'
source_url: https://arxiv.org/abs/2506.12953
tags:
- time
- series
- forecasting
- patchinstruct
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PatchInstruct, a prompt-based framework for
  time series forecasting using large language models (LLMs). The method tokenizes
  time series into overlapping patches and uses structured natural language prompts
  to guide LLMs in making predictions, eliminating the need for model fine-tuning
  or complex architectures.
---

# Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition

## Quick Facts
- **arXiv ID**: 2506.12953
- **Source URL**: https://arxiv.org/abs/2506.12953
- **Reference count**: 26
- **Primary result**: PatchInstruct achieves 10-100x inference speedup over S2IP-LLM while maintaining or improving accuracy through patch-based tokenization and reverse-ordered prompting

## Executive Summary
This paper introduces PatchInstruct, a prompt-based framework for time series forecasting using large language models (LLMs) without requiring model fine-tuning or complex architectures. The method tokenizes time series into overlapping patches and uses structured natural language prompts to guide LLMs in making predictions. PatchInstruct achieves state-of-the-art performance, significantly reducing inference time by 10x–100x compared to prior methods like S2IP-LLM, while maintaining or improving accuracy on benchmark datasets (Weather and Traffic).

## Method Summary
PatchInstruct converts time series into overlapping patches using sliding windows (size=3, stride=1), then reverses the patch order to prioritize recent observations. The method uses structured natural language prompts that instruct the LLM to decompose the series, generate predictions, and output results in a specific format. The approach is zero-shot, requiring no fine-tuning, and processes forecasts in approximately 1.2 seconds compared to 535 seconds for S2IP-LLM. The framework includes optional neighbor augmentation for datasets with correlated series, though this feature showed mixed results across datasets.

## Key Results
- Achieves 10-100x inference speedup (1.2s vs 535s) compared to S2IP-LLM
- Outperforms baselines on Weather and Traffic datasets for short horizons (H≤6)
- Reduces MSE by up to 85% compared to previous LLM-based methods
- Reverse-ordered patching consistently delivers the most robust performance across all tested scenarios

## Why This Works (Mechanism)

### Mechanism 1: Patch-Based Tokenization Captures Local Temporal Dynamics
- **Claim:** Overlapping patches as tokens model local temporal patterns critical for short-horizon forecasting
- **Mechanism:** Fixed-length sliding windows (size=3, stride=1) group adjacent timesteps into patches like `[x1, x2, x3], [x2, x3, x4]...`, encoding local trajectory information
- **Core assumption:** LLMs can transfer pattern-recognition capabilities to numerical patch sequences when formatted as structured tokens
- **Evidence anchors:** Abstract mentions "tokenizing sequences into overlapping patches and providing structured decomposition instructions to large language models"
- **Break condition:** Performance degrades when patch size mismatches underlying periodicity of data

### Mechanism 2: Reverse-Ordered Patching Implements Recency Bias
- **Claim:** Reverse chronological order forces LLM to attend more effectively to immediately relevant patterns
- **Mechanism:** Reversing patch sequence forces processing `[x94, x95, x96]` before `[x1, x2, x3]`, implementing structural prior that recent observations are more predictive
- **Core assumption:** LLM token processing order influences attention weighting in ways that benefit forecasting recency requirements
- **Evidence anchors:** Abstract states "PatchInstruct's reverse-ordered patching strategy consistently delivers the most robust performance"
- **Break condition:** Long horizons (H>12) where historical context becomes more important than recency

### Mechanism 3: Structured Natural Language Instructions Align LLM Behavior
- **Claim:** Explicit step-by-step instructions in system prompt make LLM follow decomposition-and-forecast procedure more reliably
- **Mechanism:** Prompt specifies split into patches, generate in natural order then reverse, use patches to forecast
- **Core assumption:** Instruction-following capabilities from RLHF training transfer to numerical reasoning tasks when procedures are stated explicitly
- **Evidence anchors:** Section 3.2 states prompts were "designed through rigorous empirical testing to ensure clarity and effectiveness"
- **Break condition:** Overly complex instructions may introduce noise; Table 5 shows Reverse outperforms STR Decompose

## Foundational Learning

- **Concept: Sliding Window Tokenization**
  - **Why needed here:** Understanding how overlapping patches differ from non-overlapping segments is essential for diagnosing token count vs. coverage tradeoffs
  - **Quick check question:** If you have 96 timesteps with window=3 and stride=1, how many patches result? (Answer: 94 overlapping patches)

- **Concept: Zero-Shot vs. Fine-Tuned Inference**
  - **Why needed here:** PatchInstruct explicitly avoids fine-tuning; understanding what LLMs can do without gradient updates clarifies the cost-accuracy envelope
  - **Quick check question:** What is the primary computational cost difference between S2IP-LLM and PatchInstruct at inference time? (Answer: S2IP-LLM requires ~535s vs. PatchInstruct ~1.2s due to no encoder forward pass)

- **Concept: Recency Bias in Sequential Prediction**
  - **Why needed here:** The reverse-ordering strategy is grounded in the principle that recent observations dominate short-horizon forecasts
  - **Quick check question:** Why would reverse ordering hurt performance at H=24 but help at H=3? (Answer: Longer horizons require integrating historical trends, not just recent patterns)

## Architecture Onboarding

- **Component map:** Raw time series → string conversion → patch generation (overlapping, window=3, stride=1) → reverse ordering → prompt assembly (system prompt + horizon prompt) → LLM inference → output parsing → MSE/MAE computation

- **Critical path:**
  1. Validate patch generation logic (off-by-one errors common with stride calculations)
  2. Test prompt parsing on small samples before full evaluation
  3. Monitor token counts—Table 3 shows ~8500 input tokens per forecast

- **Design tradeoffs:**
  - **Token budget vs. context:** More neighbors = better context but higher cost; Table 4 shows neighbors help Weather but hurt Traffic
  - **Horizon length:** Accuracy degrades sharply beyond H=12; PatchInstruct excels at operational short-horizon use cases
  - **Dataset specificity:** Prompts include domain context; generalization requires prompt adaptation

- **Failure signatures:**
  - MSE spikes at H=12 (Traffic: 235.75 vs. H=1: 20.05) → horizon exceeds recency bias effectiveness
  - Neighbor augmentation degrades Traffic performance → low inter-series correlation introduces noise
  - Output parsing failures → LLM produces verbose text instead of formatted arrays

- **First 3 experiments:**
  1. **Baseline replication:** Reproduce Weather H=1,3,6 results with Reverse PatchInstruct to validate implementation
  2. **Ablation on patch size:** Test window=[2,3,4,6] to find dataset-optimal settings
  3. **Horizon boundary test:** Identify the horizon threshold where PatchInstruct performance matches or falls below S2IP-LLM

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptive prompting mechanisms be developed to automate the design of decomposition instructions, reducing the risk of overfitting to specific datasets?
  - **Basis in paper:** Authors explicitly state that future research should prioritize "developing adaptive prompting mechanisms" because the current framework remains "heavily contingent upon carefully engineered prompts"
  - **Why unresolved:** Current study relies on manually designed prompts through rigorous empirical testing, which introduces a labor-intensive process that risks overfitting
  - **What evidence would resolve it:** A study demonstrating a dynamic prompt generation system that maintains performance across diverse datasets without manual re-engineering

- **Open Question 2:** Does the PatchInstruct framework generalize to time series data characterized by irregular sampling rates or high-frequency patterns?
  - **Basis in paper:** Authors note in Limitations that evaluation is limited to two benchmark datasets and "may not fully represent... real-world time series scenarios, such as irregular sampling or high-frequency patterns"
  - **Why unresolved:** Experiments were restricted to Weather and Traffic datasets with regular sampling intervals
  - **What evidence would resolve it:** Benchmark results on datasets containing irregular timestamps or high-frequency financial/physiological data

- **Open Question 3:** What specific criteria for neighbor selection are required to prevent negative transfer when using the "Neighs" augmentation strategy?
  - **Basis in paper:** Authors observe that while neighbors help on Weather dataset, they caused significant performance degradation on Traffic dataset (MSE increasing from 8.46 to 43.50)
  - **Why unresolved:** Paper identifies failure mode—performance drops when neighbors are dissimilar—but does not offer mechanism or threshold for filtering neighbors
  - **What evidence would resolve it:** Ablation study varying correlation thresholds for neighbor selection to identify transition point from beneficial to detrimental

## Limitations
- Performance degrades significantly at longer horizons (H=12), limiting applicability to short-term forecasting
- Method relies on structured natural language prompts that require manual engineering for different datasets
- Optimal patch size selection is not established, creating potential hyperparameter tuning requirements
- Inference cost may be understated when accounting for API costs and token consumption

## Confidence
- **High confidence:** 10-100x inference speedup claim is well-supported by 535s vs 1.2s comparison; patch-based tokenization methodology is clearly specified and reproducible; performance advantage at short horizons is consistently demonstrated
- **Medium confidence:** Reverse-ordered patching strategy's superiority is supported by ablation studies, but mechanism is hypothesized; "state-of-the-art" claim is limited to specific datasets and horizons tested
- **Low confidence:** Generalization to datasets beyond Weather and Traffic without prompt engineering adaptation; optimal patch size selection methodology is not established; claim that LLMs can effectively replace architectural complexity

## Next Checks
1. **Horizon threshold identification:** Systematically test PatchInstruct performance across horizons H=1 through H=24 on both datasets to identify the precise horizon threshold where performance degrades below S2IP-LLM levels
2. **Patch size sensitivity analysis:** Conduct controlled experiments varying patch size from 2 to 8 while holding other parameters constant to determine optimal patch sizes for different dataset characteristics
3. **Cross-dataset generalization test:** Apply PatchInstruct to at least two additional time series datasets with different characteristics (e.g., financial, sensor data) without prompt adaptation to empirically measure generalization performance