---
ver: rpa2
title: 'The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through
  Attention Patterns'
arxiv_id: '2511.10837'
source_url: https://arxiv.org/abs/2511.10837
tags:
- hallucination
- uncertainty
- attention
- detection
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses hallucination detection in large language models
  by distinguishing between intrinsic (input-contradicting) and extrinsic (knowledge-gap)
  hallucinations. The proposed method leverages attention-based uncertainty quantification,
  introducing novel aggregation strategies that outperform sampling-based baselines
  on intrinsic hallucinations while maintaining strong performance on extrinsic ones.
---

# The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns

## Quick Facts
- arXiv ID: 2511.10837
- Source URL: https://arxiv.org/abs/2511.10837
- Reference count: 4
- Primary result: Attention-based uncertainty quantification outperforms sampling methods on intrinsic hallucination detection, achieving up to 72.2% AUROC, 46.7% AURAC, and 43.7% PRR

## Executive Summary
This paper introduces a novel approach to hallucination detection that distinguishes between intrinsic hallucinations (input-contradicting) and extrinsic hallucinations (knowledge-gap). The method leverages attention-based uncertainty quantification through recursive confidence propagation and input-grounding detection. By introducing novel aggregation strategies that analyze attention patterns, the approach demonstrates superior performance on intrinsic hallucinations while maintaining strong results on extrinsic ones, particularly excelling at detecting underspecification uncertainty.

## Method Summary
The method builds on Recursive Attention-based Uncertainty Quantification (RAUQ) by implementing three token aggregation strategies (previous-token, all-past-tokens, input-tokens) and three head aggregation modes (uncertainty-aware head selection, mean across heads, attention rollout). For each generated token, attention patterns are analyzed to compute confidence scores through recursive propagation: c(y_t) ← α·p_t + (1-α)·a_t·c(y_{t-1}). The final uncertainty score is derived from the maximum layer-wise uncertainty across the generation sequence. The approach requires only a single forward pass, making it computationally efficient compared to sampling-based baselines.

## Key Results
- Attention-based variants with input-token aggregation achieve 72.2% AUROC, 46.7% AURAC, and 43.7% PRR on intrinsic hallucination detection
- Sampling-based methods (Semantic Entropy, EigenScore, Normalized Entropy) significantly outperform attention methods on extrinsic hallucinations but fail on intrinsic cases
- The best-performing attention variants show 8-10% improvement in AUROC over state-of-the-art sampling methods for intrinsic hallucination detection
- Attention patterns in deeper layers become uniform, suggesting a degradation of uncertainty signals in later network stages

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Aware Attention Head Selection
A subset of attention heads exhibit consistent attention drops when the model is about to produce incorrect or hallucinatory continuations. For each layer, the head with maximum average attention from each token to its immediate predecessor is selected as the uncertainty-aware head. These heads carry a local confidence signal that can be tracked throughout generation.

### Mechanism 2: Recursive Confidence Propagation
Token-level uncertainty compounds across the generation sequence, and attention weights can track this accumulation. Confidence is propagated at each step using: cℓ(yt) ← α·pt + (1−α)·Aℓ,hℓ(t,t−1)·cℓ(yt−1). The attention-weighted term carries forward accumulated certainty from prior tokens.

### Mechanism 3: Input-Grounding Detection via Input-Token Aggregation
Low attention from generated tokens back to input tokens indicates reliance on internal priors rather than grounded context, signaling intrinsic hallucination risk. Computing average attention from each generated token to all input tokens reveals whether the model is attending to provided context during generation.

## Foundational Learning

- **Concept: Intrinsic vs. Extrinsic Hallucinations**
  - Why needed here: Different detection methods excel for different hallucination types—sampling for extrinsic, attention for intrinsic. Misapplying methods yields poor results.
  - Quick check question: If a model outputs "Google was founded in 1998" when the input context states "Google was founded in 2001," is this an intrinsic or extrinsic hallucination?

- **Concept: Attention Rollout**
  - Why needed here: One of the evaluated aggregation strategies; understanding how attention compounds across layers via recursive multiplication is critical for interpreting rollout-based variants.
  - Quick check question: Why does the rollout formula (Eq. 9) include an identity matrix term?

- **Concept: Semantic Entropy**
  - Why needed here: The primary baseline for extrinsic hallucination detection. Understanding why it fails on intrinsic cases (outputs are consistently structured, not diverse) contextualizes the attention method's advantages.
  - Quick check question: Why would semantic entropy struggle when contradictory inputs produce consistently structured rather than diverse outputs?

## Architecture Onboarding

- **Component map:** Attention extraction → Head selection (Eq. 1) → Token aggregation (Eqs. 4–6) → Confidence propagation (Eq. 2) → Layer aggregation (Eq. 3) → Final uncertainty (max over layers)

- **Critical path:**
  1. Run single greedy forward pass, extract attention maps from all layers
  2. For each layer, either select uncertainty-aware head, average all heads, or compute rollout
  3. For each generated token, aggregate attention to: previous token / all-past tokens / input tokens
  4. Propagate confidence scores using Eq. 2 with tuned α ∈ [0.1, 0.9]
  5. Compute sequence-level uncertainty via negative mean log confidence (Eq. 3)
  6. Return maximum layer-wise uncertainty as final score

- **Design tradeoffs:**
  - Input-token aggregation: Best for intrinsic hallucinations but only meaningful for grounded tasks
  - All-past-tokens: More complete dependency picture but yields low attention magnitudes
  - Mean-heads vs. single-head: Mean is more stable but may dilute signal; single-head selection is observation-based and potentially brittle
  - Rollout: Captures cross-layer attention flow but adds computation

- **Failure signatures:**
  - Uniform attention in deep layers → degraded interpretability
  - Semantic entropy significantly outperforms → likely extrinsic-dominated dataset; consider hybrid approach
  - Attention methods significantly outperform → likely intrinsic/underspecification issues; verify input-token variant is used

- **First 3 experiments:**
  1. Reproduce Figure 3 split: Compare RAUQ-Input vs. Semantic Entropy on extrinsic (PreciseWikiQA) vs. intrinsic (SQuAD-v2 unanswerable) benchmarks to validate the differential effectiveness claim
  2. Threshold calibration: Plot score distributions for answerable vs. unanswerable vs. nonexistent queries to verify separability and identify optimal thresholds
  3. Ablation across aggregation modes: Test previous-token vs. all-past vs. input-token aggregation with mean-heads vs. rollout on FaithEval-inconsistent to isolate which combination best captures underspecification uncertainty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do attention-based uncertainty signals remain robust for hallucination detection in complex reasoning tasks, such as mathematical derivation or code generation?
- Basis in paper: [explicit] The authors explicitly state that future research should prioritize examining "whether LLMs’ internal representations and attention patterns remain reliable" when dealing with the "intricacies of more sophisticated tasks."
- Why unresolved: The current evaluation is restricted to extractive QA and summarization tasks, where attention to input is a direct proxy for grounding; this relationship may not hold for multi-step logical reasoning where attention patterns are more diffuse.
- What evidence would resolve it: Applying the RAUQ variants to reasoning benchmarks like GSM8K or HumanEval and comparing the resulting AUROC scores against those achieved on the current QA-based datasets.

### Open Question 2
- Question: Can attention-based and sampling-based methods be effectively hybridized to create a unified detector that excels at both intrinsic and extrinsic hallucinations?
- Basis in paper: [inferred] The results highlight a "butterfly split" in performance: sampling-based methods (Semantic Entropy) excel at extrinsic hallucinations, while attention-based methods excel at intrinsic ones. The paper discusses the trade-off but does not explore a fusion of the two.
- Why unresolved: It is unclear if the signals provided by attention patterns and semantic diversity are orthogonal (complementary) or if combining them would yield diminishing returns.
- What evidence would resolve it: Experiments implementing an ensemble or joint-loss model that weighs attention uncertainty against semantic entropy to maximize detection performance across all hallucination categories.

### Open Question 3
- Question: How reliably do attention-based uncertainty scores distinguish "underspecification uncertainty" (ambiguous inputs) from other forms of epistemic uncertainty?
- Basis in paper: [explicit] The authors note that their findings "motivate further exploration of attention dynamics as a proxy for underspecification uncertainties," specifically in the context of intrinsic hallucinations.
- Why unresolved: While the paper links attention aggregation to intrinsic hallucinations (often caused by underspecification), it does not isolate whether the attention drop is a specific reaction to ambiguity or a general correlate of any generation error.
- What evidence would resolve it: A controlled study using datasets specifically curated to separate ambiguity (underspecification) from factual inconsistency, analyzing the attention head responses to each.

## Limitations

- The method requires careful tuning of the confidence propagation parameter α, but optimal values are not provided, leaving performance sensitive to hyperparameter choice
- Attention patterns become uniform in deeper layers, suggesting the uncertainty signal degrades for very deep models or those with different attention dynamics
- Evaluation is limited to English-language benchmarks and grounded generation tasks, with unclear effectiveness for open-ended generation, multilingual contexts, or non-text modalities

## Confidence

- **Attention Dynamics Encode Confidence**: High - The observation that attention weights drop before incorrect continuations is directly supported by experiments and visualizations across multiple models and datasets
- **Attention-Based Aggregation Outperforms Sampling on Intrinsic Hallucinations**: High - Clear statistical and qualitative evidence demonstrates significant improvements on intrinsic hallucination detection benchmarks
- **Method Generalizes Across Models and Tasks**: Medium - Strong results shown on six diverse models and several tasks, but limited to English benchmarks and does not explore edge cases like multilingual or open-ended generation
- **Alpha Tuning is Critical**: Low - The paper notes the importance of α but does not provide final tuned values or sensitivity analysis, making it difficult to assess robustness or reproduce results exactly

## Next Checks

1. **Alpha Sensitivity Analysis**: Systematically vary α for each model and aggregation variant on a held-out validation set, and report AUROC/AURAC/PRR as a function of α to clarify whether performance is robust to α choice or highly sensitive to tuning.

2. **Cross-Architecture Attention Pattern Validation**: Apply the method to a diverse set of model architectures (e.g., Transformers, Mamba, sparse attention models) and document whether the attention-drop phenomenon persists to inform model-specific adjustments or limitations.

3. **Multilingual and Open-Ended Generation Testing**: Evaluate the method on non-English benchmarks and open-ended generation tasks to determine whether attention-based uncertainty quantification remains effective outside the current scope and reveals whether strengths generalize to broader NLP applications.