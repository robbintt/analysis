---
ver: rpa2
title: 'VN-MTEB: Vietnamese Massive Text Embedding Benchmark'
arxiv_id: '2507.21500'
source_url: https://arxiv.org/abs/2507.21500
tags:
- datasets
- retrieval
- https
- huggingface
- mteb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VN-MTEB, a comprehensive Vietnamese benchmark
  for evaluating text embedding models, addressing the lack of large-scale Vietnamese
  test datasets. The authors developed an automated translation pipeline leveraging
  LLMs and embedding models to translate and filter high-quality samples from the
  English Massive Text Embedding Benchmark (MTEB) into Vietnamese, preserving semantic
  fidelity, named entities, and code snippets.
---

# VN-MTEB: Vietnamese Massive Text Embedding Benchmark

## Quick Facts
- arXiv ID: 2507.21500
- Source URL: https://arxiv.org/abs/2507.21500
- Reference count: 21
- Primary result: Introduces VN-MTEB, a comprehensive Vietnamese benchmark for evaluating text embedding models across 41 datasets and 6 tasks

## Executive Summary
VN-MTEB addresses a critical gap in Vietnamese natural language processing by providing the first large-scale benchmark for evaluating text embedding models in Vietnamese. The benchmark was created through an automated translation pipeline that converts high-quality samples from the English Massive Text Embedding Benchmark (MTEB) into Vietnamese while preserving semantic fidelity, named entities, and code snippets. It covers six fundamental tasks: retrieval, reranking, classification, clustering, pair classification, and semantic textual similarity.

The evaluation of 18 embedding models revealed that larger models with Rotary Positional Embedding (RoPE) significantly outperform those with Absolute Positional Embedding (APE) for Vietnamese embedding tasks. This finding provides important insights for the development of Vietnamese-specific embedding architectures and highlights the importance of positional encoding mechanisms in cross-lingual contexts.

## Method Summary
The authors developed an automated translation pipeline leveraging large language models (LLMs) and embedding models to translate and filter high-quality samples from English MTEB into Vietnamese. The pipeline preserves semantic fidelity, named entities, and code snippets while maintaining the original task structure. VN-MTEB comprises 41 datasets across six tasks: retrieval, reranking, classification, clustering, pair classification, and semantic textual similarity. The benchmark was designed to evaluate the performance of text embedding models in Vietnamese across diverse NLP tasks, providing a standardized framework for model comparison.

## Key Results
- VN-MTEB provides the first comprehensive Vietnamese benchmark with 41 datasets across 6 fundamental NLP tasks
- Larger embedding models with Rotary Positional Embedding (RoPE) outperform those with Absolute Positional Embedding (APE) in Vietnamese tasks
- The benchmark successfully preserves semantic fidelity and named entities through automated translation from English MTEB

## Why This Works (Mechanism)
The automated translation pipeline leverages LLMs to maintain semantic consistency while converting English MTEB samples to Vietnamese. By using embedding models to filter high-quality translations and preserve critical elements like named entities and code snippets, the pipeline ensures that the translated datasets maintain their original task characteristics. The six-task structure covers the full spectrum of embedding model evaluation needs, from semantic similarity to classification and retrieval.

## Foundational Learning

### Text Embedding Models
- **Why needed**: Core technology for representing text as numerical vectors for NLP tasks
- **Quick check**: Can model capture semantic similarity between semantically equivalent phrases in different languages

### Positional Encoding (RoPE vs APE)
- **Why needed**: Critical architectural component that affects how models handle sequence information
- **Quick check**: Compare embedding performance across tasks when using different positional encoding schemes

### Cross-lingual Evaluation
- **Why needed**: Essential for assessing model generalization across languages
- **Quick check**: Measure performance degradation when evaluating models on non-English benchmarks

## Architecture Onboarding

### Component Map
Translation Pipeline (LLM + Embedding Filter) -> Dataset Creation -> Task Classification -> Model Evaluation

### Critical Path
Translation Quality Assurance → Dataset Validation → Task Alignment → Model Benchmarking

### Design Tradeoffs
- Automated translation prioritizes scalability over human-level quality control
- Six-task coverage balances comprehensiveness with practical evaluation constraints
- Model selection focuses on representative sample rather than exhaustive coverage

### Failure Signatures
- Semantic drift in translated samples affecting task performance
- Named entity corruption leading to classification task degradation
- Code snippet translation errors impacting technical dataset validity

### First 3 Experiments
1. Compare semantic similarity preservation rates between automated and human translations
2. Benchmark model performance on original English MTEB vs Vietnamese VN-MTEB for same datasets
3. Evaluate performance variance across different Vietnamese dialects and regional variations

## Open Questions the Paper Calls Out
None

## Limitations
- Automated translation pipeline introduces potential semantic drift without extensive human validation
- Limited model evaluation sample size (18 models) restricts generalizability
- Potential domain bias in selected MTEB datasets for Vietnamese adaptation

## Confidence

### Benchmark completeness and coverage: High
### Translation quality and semantic preservation: Medium
### Model evaluation results and conclusions: Medium

## Next Checks
1. Conduct human evaluation of a statistically significant sample of translated datasets to verify semantic fidelity claims
2. Expand model evaluation to include at least 50 additional embedding models, particularly newer multilingual models
3. Perform cross-lingual evaluation comparing Vietnamese results with their English MTEB counterparts to quantify translation impact on benchmark performance