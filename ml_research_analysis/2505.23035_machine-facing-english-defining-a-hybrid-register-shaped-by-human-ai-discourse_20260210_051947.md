---
ver: rpa2
title: 'Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse'
arxiv_id: '2505.23035'
source_url: https://arxiv.org/abs/2505.23035
tags:
- language
- human
- english
- korean
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Machine-Facing English (MFE) is a register emerging from sustained\
  \ human\u2013AI interaction, characterized by syntactic rigidity, pragmatic simplification,\
  \ and hyper-explicit phrasing to enhance machine parseability. Drawing on Halliday\u2019\
  s register theory, enregisterment, and pragmatics, the study defines five core traits\u2014\
  redundant clarity, directive syntax, controlled vocabulary, flattened prosody, and\
  \ single-intent structuring\u2014that improve execution accuracy but compress expressive\
  \ range."
---

# Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse

## Quick Facts
- arXiv ID: 2505.23035
- Source URL: https://arxiv.org/abs/2505.23035
- Reference count: 9
- Machine-Facing English (MFE) is a register emerging from sustained human–AI interaction, characterized by syntactic rigidity, pragmatic simplification, and hyper-explicit phrasing to enhance machine parseability.

## Executive Summary
Machine-Facing English (MFE) is a hybrid register arising from prolonged human–AI discourse, blending Hallidayan register theory with pragmatics to optimize machine parseability. It exhibits traits like redundant clarity, directive syntax, controlled vocabulary, flattened prosody, and single-intent structuring, yielding improved execution accuracy but reduced expressive richness. Qualitative and quantitative observations from bilingual voice- and text-based sessions show notable parsing error reductions and altered speech prosody when using MFE, highlighting tensions between communicative efficiency and linguistic depth. The study calls for adaptive interfaces, robust NLP parsing, and empirical validation to preserve naturalness while accommodating AI constraints.

## Method Summary
The study combines qualitative observations from bilingual voice- and text-based interactions with reflexive drafting via Natural Language Declarative Prompting (NLD-P). Quantitative effects were measured on parsing accuracy for reminders and pitch range in machine-directed speech, supplemented by linguistic trait analysis grounded in Halliday’s register theory and pragmatics.

## Key Results
- MFE traits—redundant clarity, directive syntax, controlled vocabulary, flattened prosody, single-intent structuring—enhance execution accuracy but reduce expressive range.
- 37% reduction in mis-parsed reminders when redundant parameters are included.
- 22% reduction in pitch range in machine-directed speech.

## Why This Works (Mechanism)
MFE leverages explicit linguistic conventions to reduce ambiguity for machine parsing, aligning human intent with AI processing constraints. The hyper-explicit phrasing and syntactic control lower the risk of misinterpretation, enabling more reliable task execution. However, these same traits compress expressive nuance, creating a tradeoff between precision and richness in human–AI interaction.

## Foundational Learning
- **Register Theory (Hallidayan)**: Explains how language adapts to social context; needed to frame MFE as a distinct linguistic variety shaped by AI interaction. Quick check: Compare MFE with other specialized registers (e.g., legal, technical) for structural similarities.
- **Enregisterment**: Describes how linguistic forms become associated with specific communicative roles; needed to understand MFE’s emergence and recognition. Quick check: Identify markers that signal MFE in user speech.
- **Pragmatics**: Studies language use in context; needed to analyze how meaning is negotiated between humans and AI. Quick check: Examine pragmatic failures in non-MFE interactions.
- **Natural Language Declarative Prompting (NLD-P)**: A method for crafting machine-readable prompts; needed to generate and test MFE forms. Quick check: Evaluate prompt clarity and execution success across tasks.

## Architecture Onboarding
- **Component map**: User → NLD-P prompt generation → AI parsing engine → Output; optional prosody monitoring.
- **Critical path**: Prompt generation and AI parsing determine execution accuracy; prosody changes are secondary indicators.
- **Design tradeoffs**: Explicit phrasing improves parsing but risks user cognitive load and diminished naturalness.
- **Failure signatures**: High parsing errors, inconsistent prosody, or user abandonment indicate MFE adoption problems.
- **First experiments**:
  1. Measure parsing accuracy and user cognitive load for MFE vs. conversational prompts.
  2. Analyze prosody changes in voice interactions across task complexity.
  3. Test MFE trait combinations for optimal balance between clarity and expressiveness.

## Open Questions the Paper Calls Out
None

## Limitations
- Quantitative effect sizes (37% parsing error reduction, 22% pitch reduction) are based on small bilingual samples without independent replication, so robustness is low.
- No comparison with established NLP-adapted registers (e.g., web search queries), leaving alternative explanations unexplored.
- No longitudinal studies on user adaptation or cognitive load from sustained MFE use.
- Conceptual framework is persuasive but untested against non-voice-AI interaction modes.

## Confidence
- Register definition and trait taxonomy: High
- Quantitative effect sizes (error/pitch): Low
- Design and pedagogical implications: Medium

## Next Checks
1. Replicate parsing-accuracy and pitch-range measurements with a larger, diverse user pool and multiple AI platforms.
2. Compare MFE usage against established NLP-adapted registers (e.g., web search queries) to isolate AI-specific effects.
3. Conduct longitudinal studies tracking user adaptation and cognitive load over repeated MFE use.