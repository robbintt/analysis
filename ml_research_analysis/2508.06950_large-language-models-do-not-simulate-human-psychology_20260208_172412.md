---
ver: rpa2
title: Large Language Models Do Not Simulate Human Psychology
arxiv_id: '2508.06950'
source_url: https://arxiv.org/abs/2508.06950
tags:
- human
- llms
- person
- moral
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that large language models (LLMs) do not\
  \ reliably simulate human psychology, particularly in moral judgment tasks. Researchers\
  \ tested four LLMs\u2014GPT-3.5, GPT-4o-mini, Llama-3.1, and CENTAUR\u2014on moral\
  \ scenarios with slight rewording to alter meaning while preserving similar wording."
---

# Large Language Models Do Not Simulate Human Psychology

## Quick Facts
- arXiv ID: 2508.06950
- Source URL: https://arxiv.org/abs/2508.06950
- Authors: Sarah Schröder; Thekla Morgenroth; Ulrike Kuhl; Valerie Vaquet; Benjamin Paaßen
- Reference count: 11
- Primary result: LLMs do not reliably simulate human psychology, particularly in moral judgment tasks where semantic rewording causes significant divergence.

## Executive Summary
This study demonstrates that large language models (LLMs) fail to reliably simulate human psychology, particularly in moral judgment tasks. Researchers tested four LLMs—GPT-3.5, GPT-4o-mini, Llama-3.1, and CENTAUR—on moral scenarios with slight rewording to alter meaning while preserving similar wording. While LLMs closely matched human responses to original scenarios, they failed to respond like humans to reworded items, maintaining consistent ratings despite significant semantic changes. The study concludes that LLMs should be treated as unreliable tools in psychological research, requiring validation against human data for every new application.

## Method Summary
The study tested four LLMs on 30 moral scenario pairs (original vs. reworded) derived from established psychological literature. Human ratings were collected from N=374 US-based participants. Each model was queried 10 times per scenario using a three-part prompt structure (instruction text, few-shot examples, target scenario). Primary metrics included Pearson correlation between human and model ratings, and Chow's test comparing pooled vs. separate linear regression models. The few-shot examples followed Dillion et al. (2023) but specific examples were not fully specified in the paper.

## Key Results
- LLMs achieved r > 0.9 correlation with humans on original moral scenarios but dropped to r < 0.6 on semantically reworded items
- Cross-model inconsistency observed: different LLMs responded differently to identical reworded inputs
- Separate regression models (human vs. LLM) fit significantly better than pooled models (Chow test p < 0.01)
- Mean absolute rating shifts for LLMs were < 1.0 when humans shifted > 2.0 on reworded items

## Why This Works (Mechanism)

### Mechanism 1: Token-Sequence Generalization Dominates Semantic Understanding
LLMs encode statistical relationships between token sequences from training data. When inputs resemble training tokens, outputs resemble training completions—regardless of whether the meaning has changed. Small wording changes (e.g., "wrongfully convicted" → "rightfully convicted") produce similar token patterns, so LLMs generate similar ratings. The paper shows that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs' and human responses.

### Mechanism 2: Training Data Proximity Enables Illusion of Human-Like Response
High human-LLM correlation only emerges when test items are similar to training distribution; this is memorization, not psychological simulation. Moral scenarios from published psychological studies likely appeared in training corpora. LLMs reproduce cached patterns, creating r = 0.95+ correlations that disappear under distribution shift. The study shows LLMs can replicate human moral judgments on scenarios close to (or contained in) the training data of LLMs.

### Mechanism 3: Absence of Shared Cross-Model Psychological Representations
Different LLMs produce divergent responses to identical novel inputs because no common "psychology" model exists—each architecture encodes different pattern-relationship mappings. Without a grounded theory of cognition, each model's training trajectory produces idiosyncratic response surfaces. The paper shows GPT-3.5, GPT-4o-mini, Llama-3.1, and CENTAUR respond differently to the same reworded items, with humans rating +2.48 shift while LLMs show inconsistent shifts ranging from 0.3 to 2.3.

## Foundational Learning

- **Generalization vs. Extrapolation in Machine Learning**
  - Why needed here: The paper's theoretical argument hinges on distinguishing interpolation (within training distribution) from extrapolation (beyond it).
  - Quick check question: If a model trained on observations must generalize to novel experimental setups, what type of generalization is required—and why does the paper argue LLMs cannot reliably provide it?

- **Tokenization and Surface Form Independence**
  - Why needed here: Understanding that LLMs operate on token sequences, not semantic representations, explains why "wrongfully convicted" and "rightfully convicted" produce nearly identical LLM ratings despite opposite meanings.
  - Quick check question: How would an LLM represent the semantic opposition between "wrongfully" and "rightfully" if its training objective only requires predicting likely token continuations?

- **Correlation Does Not Imply Mechanistic Validity**
  - Why needed here: The paper shows r = 0.95 correlation on original items yet rejects the simulation hypothesis.
  - Quick check question: What additional evidence—beyond correlation with human responses—would be required to claim an LLM "simulates" rather than merely "mimics" human psychology?

## Architecture Onboarding

- **Component map**: Input prompt -> Token sequence encoding -> Statistical pattern matching -> Output rating distribution
- **Critical path**: Design prompt with few-shot examples → Query model multiple times → Compare mean ratings to human baseline → Introduce semantic rewording → Compute correlation drop and regression model separation
- **Design tradeoffs**: Few-shot prompting increases format compliance but may inflate surface-pattern matching; higher temperature sampling captures variance but reduces reproducibility; multiple queries provide variance estimates but cannot recover semantic sensitivity if absent in model
- **Failure signatures**: High original-item correlation (r > 0.9) + low reworded-item correlation (r < 0.6); mean absolute rating shift < 1.0 when humans shift > 2.0; separate human/LLM regression lines fit significantly better than pooled model; cross-model inconsistency on identical inputs
- **First 3 experiments**:
  1. Test your LLM on original moral scenarios from Dillion et al. (2023) to establish correlation ceiling with human data (expect r > 0.9 if model has seen similar training data)
  2. Create reworded versions where 1-3 words change meaning substantially while preserving token overlap; measure rating shift magnitude for LLM vs. humans
  3. Run the same reworded stimuli through 3+ different LLMs (e.g., GPT, Llama, Claude variants); compute pairwise correlations on rating shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs fail to simulate human psychology in domains other than moral judgment, such as decision-making or reasoning tasks?
- Basis in paper: [inferred] The study focused exclusively on moral judgment scenarios (30 items from established moral databases).
- Why unresolved: The authors demonstrate failure in a specific domain but imply the issue is fundamental to token-based generalization, without testing non-moral psychological constructs.
- What evidence would resolve it: Replicating the rewording methodology on non-moral psychological tasks (e.g., cognitive reflection tests or risk assessment).

### Open Question 2
- Question: How effective are LLMs as tools for pilot testing and refining experimental materials compared to human participants?
- Basis in paper: [explicit] The authors suggest "LLMs may be useful... for brainstorming, pilot testing, and refining experimental materials" despite concluding they cannot replace participants.
- Why unresolved: The paper critiques using LLMs as subjects but does not provide empirical evidence for their efficacy in these auxiliary roles.
- What evidence would resolve it: Studies comparing the ability of LLMs versus human pilot groups to identify confusing or ambiguous wordings in survey design.

### Open Question 3
- Question: Can LLMs be trained or constrained to generalize based on semantic meaning rather than token similarity?
- Basis in paper: [explicit] The authors argue that LLMs generalize on token sequences, not meaning, and that fine-tuning (e.g., CENTAUR) does not solve this "fundamental point."
- Why unresolved: It remains unclear if specific training mechanisms, such as improved grounding or architecture changes, could overcome the token-similarity bias.
- What evidence would resolve it: The development of a model that maintains human-like correlation scores even when semantic meaning is altered via subtle token changes.

## Limitations
- Prompt construction details are incomplete, particularly the few-shot examples used, making exact reproduction difficult
- Human ratings are treated as a gold standard without explicit analysis of inter-rater reliability or cultural biases
- Results may not generalize to newer or differently trained architectures beyond the four tested models
- Semantic rewording manipulations may not capture all forms of meaning change that could reveal or mask LLM understanding

## Confidence

- **High confidence**: LLMs show high correlation with human ratings on original items but fail to maintain this correlation on semantically reworded items; cross-model inconsistency on novel inputs is robustly demonstrated
- **Medium confidence**: The claim that LLMs rely on token-sequence generalization rather than semantic understanding is well-supported but requires further mechanistic investigation
- **Medium confidence**: The assertion that LLMs should be treated as unreliable tools in psychological research is valid but may overstate the case for all LLM applications

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary the few-shot examples and prompt structure while keeping stimuli constant to determine how much the observed correlations depend on prompt engineering versus model capability

2. **Cross-cultural validation**: Repeat the semantic perturbation tests with human participant samples from different cultural backgrounds to assess whether the "ground truth" human ratings themselves vary systematically

3. **Mechanism probing**: Test whether fine-tuning on paraphrased moral scenarios improves LLM performance on reworded items, which would indicate whether the limitation is architectural or data-driven