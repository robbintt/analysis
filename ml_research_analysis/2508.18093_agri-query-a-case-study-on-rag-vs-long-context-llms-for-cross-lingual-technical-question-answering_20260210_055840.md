---
ver: rpa2
title: 'Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical
  Question Answering'
arxiv_id: '2508.18093'
source_url: https://arxiv.org/abs/2508.18093
tags:
- question
- long-context
- context
- manual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a case study comparing large language models
  (LLMs) with 128K-token context windows against Retrieval-Augmented Generation (RAG)
  approaches for cross-lingual technical question answering using agricultural machinery
  manuals in English, French, and German. The study evaluates nine long-context LLMs
  using direct prompting and three RAG strategies (keyword, semantic, hybrid) with
  an LLM-as-a-judge for evaluation.
---

# Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering

## Quick Facts
- **arXiv ID:** 2508.18093
- **Source URL:** https://arxiv.org/abs/2508.18093
- **Reference count:** 23
- **Primary result:** Hybrid RAG consistently outperforms direct long-context prompting for cross-lingual technical QA in agricultural manuals

## Executive Summary
This paper presents a case study comparing large language models with 128K-token context windows against Retrieval-Augmented Generation (RAG) approaches for cross-lingual technical question answering using agricultural machinery manuals in English, French, and German. The study evaluates nine long-context LLMs using direct prompting and three RAG strategies (keyword, semantic, hybrid) with an LLM-as-a-judge for evaluation. Results show that Hybrid RAG consistently outperforms direct long-context prompting, with models like Gemini 2.5 Flash and Qwen 2.5 7B achieving over 85% accuracy across all languages.

## Method Summary
The study evaluates nine long-context LLMs (including Gemini 2.5 Flash, Qwen 2.5 7B, and others) on cross-lingual technical question answering using agricultural machinery manuals. Three RAG strategies are tested: keyword-based retrieval, semantic retrieval, and hybrid approaches. An LLM-as-a-judge evaluates the accuracy of responses across English, French, and German questions. The evaluation framework compares direct long-context prompting against RAG-based approaches to determine which method better handles the retrieval and generation tasks in a multilingual technical context.

## Key Results
- Hybrid RAG consistently outperforms direct long-context prompting across all tested languages
- Models like Gemini 2.5 Flash and Qwen 2.5 7B achieve over 85% accuracy with Hybrid RAG
- Smaller models perform particularly well with RAG approaches, addressing the "Lost in the Middle" effect
- Cross-lingual retrieval capabilities are robust when using multilingual embedding models

## Why This Works (Mechanism)
Assumption: The Hybrid RAG approach works because it combines the precision of keyword-based retrieval with the semantic understanding of semantic retrieval, allowing it to capture both exact matches and conceptually related information. This dual approach mitigates the limitations of either method alone, particularly in technical domains where terminology may vary across languages while maintaining core semantic meaning. The hybrid system likely achieves better context selection by retrieving from multiple perspectives, which helps long-context LLMs focus on the most relevant passages while avoiding the degradation that occurs when processing large volumes of irrelevant text.

## Foundational Learning
- **RAG vs Long-Context LLMs:** Understanding when retrieval augmentation outperforms direct context processing in technical domains
- **Cross-lingual embedding models:** How multilingual representations enable effective retrieval across language barriers
- **Lost in the Middle effect:** Why information in middle sections of long contexts gets degraded or ignored by LLMs
- **Hybrid retrieval strategies:** Combining keyword and semantic approaches for improved information retrieval
- **LLM-as-a-judge evaluation:** Using language models as automated evaluators for technical question answering tasks

## Architecture Onboarding

**Component Map:**
Document Collection -> Embedding Model -> Retriever (Keyword/Semantic/Hybrid) -> Long-Context LLM (Prompted or RAG) -> LLM-as-a-Judge -> Accuracy Score

**Critical Path:**
Retriever -> LLM -> Judge

**Design Tradeoffs:**
Direct prompting requires no retrieval infrastructure but suffers from context window limitations and degradation. RAG adds complexity but enables focused retrieval. Hybrid approaches balance precision and recall but require careful threshold tuning.

**Failure Signatures:**
Lost in the Middle effect manifests as degraded accuracy when relevant information sits in middle sections of long contexts. Poor cross-lingual retrieval shows as low accuracy on non-English questions regardless of model choice.

**First Experiments:**
1. Test keyword vs semantic retrieval performance on single-language queries
2. Compare direct prompting accuracy at different document positions (beginning, middle, end)
3. Evaluate cross-lingual retrieval performance with multilingual vs monolingual embeddings

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but potential areas for further investigation include: How would these findings generalize to non-technical domains? What is the optimal hybrid threshold for different types of technical content? How do these approaches scale with document complexity and length? What are the computational tradeoffs between different retrieval strategies at scale?

## Limitations
- Evaluation framework relies heavily on LLM-as-a-judge, introducing potential subjectivity and model-specific biases
- Agricultural domain specificity limits generalizability to other technical domains
- Performance may be influenced by structured nature of manuals, which may not reflect unstructured technical content
- Focus on three European languages leaves questions about performance across more diverse language pairs

## Confidence
**High confidence:** Core finding that Hybrid RAG outperforms direct long-context prompting across all tested languages and models (85%+ accuracy for top configurations)

**Medium confidence:** Smaller models benefit particularly from RAG approaches (demonstrated but scaling relationships not extensively explored)

**Medium confidence:** Cross-lingual retrieval capabilities are robust (shown with limited language set that may not represent broader multilingual content)

## Next Checks
1. Conduct LLM-as-a-judge inter-annotator agreement studies using multiple judge models to quantify and mitigate potential bias in evaluation framework

2. Test the same RAG vs. long-context comparison on unstructured technical documents (research papers, forum discussions) to assess generalizability beyond structured manuals

3. Expand language coverage to include non-European languages with different scripts and linguistic structures (Chinese, Arabic, Hindi) to validate cross-lingual retrieval claims across typologically diverse language pairs