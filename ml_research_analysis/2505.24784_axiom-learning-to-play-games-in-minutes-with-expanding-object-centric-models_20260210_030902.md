---
ver: rpa2
title: 'AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models'
arxiv_id: '2505.24784'
source_url: https://arxiv.org/abs/2505.24784
tags:
- axiom
- slot
- mixture
- type
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AXIOM is an object-centric active inference agent that learns to
  play games from raw pixels with high sample efficiency, outperforming deep RL baselines
  like BBF and DreamerV3 in 10k interactions without gradient-based optimization.
  It represents scenes as compositions of objects with piecewise-linear dynamics,
  expands its generative model online by growing mixture components from single events,
  and refines them via Bayesian model reduction for generalization.
---

# AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models

## Quick Facts
- arXiv ID: 2505.24784
- Source URL: https://arxiv.org/abs/2505.24784
- Authors: Conor Heins, Toon Van de Maele, Alexander Tschantz, Hampus Linander, Dimitrije Markovic, Tommaso Salvatori, Corrado Pezzato, Ozan Catal, Ran Wei, Magnus Koudahl, Marco Perin, Karl Friston, Tim Verbelen, Christopher Buckley
- Reference count: 40
- Primary result: AXIOM outperforms deep RL baselines like BBF and DreamerV3 on Gameworld 10k benchmark in 10k interactions without gradient-based optimization.

## Executive Summary
AXIOM is an object-centric active inference agent that learns to play games from raw pixels with exceptional sample efficiency. It represents scenes as compositions of objects with piecewise-linear dynamics, expanding its generative model online by growing mixture components from single events and refining them via Bayesian model reduction for generalization. On the Gameworld 10k benchmark, AXIOM achieves higher cumulative rewards and faster convergence than baselines like BBF and DreamerV3, with an interpretable, compact model (0.3–1.6M parameters) and efficient inference.

## Method Summary
AXIOM uses four mixture modules: sMM (slot perception) segments pixels into object slots via GMM; iMM (identity classification) assigns slots to discrete types; tMM (transition dynamics) predicts next state using piecewise-linear transforms; rMM (interaction) predicts discrete switch states and rewards. The agent performs online variational inference with coordinate ascent, dynamically grows components when data is poorly explained, and periodically applies Bayesian Model Reduction to merge components. Planning uses expected free energy minimization to balance reward seeking with uncertainty reduction.

## Key Results
- Outperforms BBF and DreamerV3 on Gameworld 10k benchmark in 10k interactions
- Achieves higher cumulative rewards and faster convergence across multiple games
- Maintains compact model size (0.3–1.6M parameters) while scaling to complex environments
- Demonstrates robustness to perturbations including color/shape changes through remapping logic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Factorizing visual scenes into distinct object slots with discrete identities enables sample-efficient dynamics learning by isolating "what" an object is from "how" it moves.
- **Mechanism**: The Slot Mixture Model (sMM) treats pixels as a GMM, assigning each pixel to one of $K$ object slots. The Identity Mixture Model (iMM) then clusters these slots by color/shape into discrete types. This factorization allows the Transition Mixture Model (tMM) to learn dynamics based on object type rather than instance-specific pixel history.
- **Core assumption**: Objects are spatially coherent clusters with approximately Gaussian color/shape profiles; dynamics are determined by object type, not pixel adjacency.
- **Evidence anchors**: [abstract] states "represents scenes as compositions of objects... piecewise linear trajectories that capture sparse object-object interactions." Appendix A.6 states that conditioning dynamics on identity codes "allows AXIOM to use the same dynamics model across slots... learning in a type-specific, rather than instance-specific, manner."

### Mechanism 2
- **Claim**: Dynamically expanding mixture components followed by Bayesian Model Reduction allows the agent to handle novel events without permanent over-parameterization, enforcing generalization.
- **Mechanism**: When incoming data is poorly explained (predictive density < threshold), new rMM components are instantiated. Periodically, BMR merges components if doing so maximizes the ELBO, effectively compressing single-event memories into generalized rules.
- **Core assumption**: The true environmental dynamics are sparse and can be modeled by a limited set of piecewise-linear regimes; transient events should be merged into broader categories to generalize spatially.
- **Evidence anchors**: [abstract] states "expands its generative model online by growing mixture components... refined via Bayesian model reduction for generalization." Figure 4b shows the number of active components declining during training due to BMR; Table 1 shows performance drops without BMR.

### Mechanism 3
- **Claim**: Planning via Expected Free Energy minimization drives efficient exploration by balancing reward seeking with uncertainty reduction.
- **Mechanism**: The agent samples action sequences and selects those minimizing EFE, which sums expected negative reward and KL-divergence between prior and posterior beliefs (information gain).
- **Core assumption**: The agent's variational posteriors accurately reflect uncertainty; the information gain weight can be tuned to balance risk.
- **Evidence anchors**: [abstract] states "supports uncertainty-aware planning via information gain." Section 2.2 defines EFE; Figure 4c visualizes the trade-off, showing information gain dominating early training and utility dominating later.

## Foundational Learning

- **Concept: Variational Inference & Conjugate Priors**
  - **Why needed here**: AXIOM avoids backpropagation by using analytical updates for exponential family distributions (Dirichlet, NIW).
  - **Quick check question**: Can you derive the posterior update for a Gaussian likelihood with a Normal-Inverse-Wishart prior?

- **Concept: Switching Linear Dynamical Systems (SLDS)**
  - **Why needed here**: The core dynamics engine assumes continuous state evolution governed by discrete "switch" states.
  - **Quick check question**: How does a Switching Linear Dynamical System differ from a standard Kalman Filter?

- **Concept: Active Inference (Free Energy Principle)**
  - **Why needed here**: The planning loop minimizes "Expected Free Energy," combining reward maximization with epistemic value.
  - **Quick check question**: What are the two components of Expected Free Energy, and how do they relate to exploitation vs. exploration?

## Architecture Onboarding

- **Component map**: sMM (slot perception) -> iMM (object identity) -> rMM (interaction) -> tMM (dynamics) -> planning via EFE

- **Critical path**: The Gate Variable ($G_t$) is the linchpin. It is computed from inferred "presence" and "movement." If this gate is leaky, the rMM/tMM will learn dynamics from static background noise or ghosts.

- **Design tradeoffs**:
  - Fixed vs. Learned Distance: Fixing interaction distance improves performance but requires domain tuning, whereas learning it via rMM is more general but slower.
  - Model Size: Components grow dynamically. Set `C_max` high enough to avoid capacity bottlenecks, but BMR is essential to keep inference fast.
  - Planning Budget: Planning time scales linearly with rollouts. Reducing rollouts saves time but may degrade performance in stochastic environments.

- **Failure signatures**:
  - "Zombie" Objects: Slots may persist after objects leave the screen, causing phantom collisions.
  - Identity Thrashing: Visual changes may cause the agent to "forget" known dynamics for object classes.
  - Component Explosion: Low expansion thresholds exhaust memory before BMR triggers.

- **First 3 experiments**:
  1. Validation on Bounce: Verify sMM stability. Check if ball and paddle are consistently assigned to distinct slots with consistent IDs.
  2. BMR Ablation: Run "Hunt" with BMR disabled. Confirm component count grows unbounded and performance degrades.
  3. Perturbation Test: Run "Explode," swap object colors at step 5k, and verify remapping logic recovers performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can core object-centric priors be automatically discovered from data rather than manually engineered?
- **Basis in paper**: Authors state "Our work is limited by the fact that the core priors are themselves engineered rather than discovered autonomously. Future work will focus on developing methods to automatically infer such core priors from data."
- **Why unresolved**: The current architecture requires hand-specified priors about object composition, piecewise-linear dynamics, and sparse interactions, which limits domain flexibility.
- **What evidence would resolve it**: Demonstration of an AXIOM variant that learns its own structural priors from scratch and matches or exceeds current performance on Gameworld 10k.

### Open Question 2
- **Question**: How should Bayesian Model Reduction be scheduled relative to information gain to optimize exploration-exploitation trade-offs?
- **Basis in paper**: Authors note "Clearly, BMR is crucial to generalize observed events to novel situations, but when done too early in learning, it can be detrimental for learning. Further investigating this interplay remains a topic for future work."
- **Why unresolved**: The Cross environment showed that early BMR merged clusters in ways that reduced information gain incentives, harming exploration; however, BMR was essential for spatial generalization in Gold and Hunt.
- **What evidence would resolve it**: A systematic study varying BMR timing and frequency across all Gameworld environments, measuring when BMR helps versus hurts sample efficiency.

### Open Question 3
- **Question**: Will AXIOM's sample efficiency advantages persist in environments requiring hard exploration or long-horizon credit assignment?
- **Basis in paper**: Authors state "We hence believe that information gain will play a more important role in hard exploration tasks, which is an interesting direction for future research" and note Gameworld environments were "designed to be solvable by human learners within minutes."
- **Why unresolved**: The information gain term hurt performance in games with negative-reward interactions, suggesting its utility depends on exploration structure.
- **What evidence would resolve it**: Evaluation on environments with sparse rewards, delayed feedback, or extensive state spaces where information-seeking behavior is necessary for success.

## Limitations

- Lacks explicit comparison to established deep RL baselines like PPO or SAC on standard benchmarks like Atari.
- Claims about BMR enforcing generalization rely heavily on qualitative observations rather than quantitative metrics of component diversity.
- Ablation studies are limited and do not report hyperparameter sensitivity or run-to-run variance beyond three seeds.

## Confidence

- **High confidence**: The claim that AXIOM outperforms BBF and DreamerV3 on Gameworld 10k within 10k interactions is supported by Figure 3 and Table 1.
- **Medium confidence**: The assertion that online component growth followed by BMR achieves better sample efficiency than static architectures is plausible but lacks ablation data for fixed-component variants.
- **Low confidence**: The generalizability of AXIOM to Atari or other standard RL benchmarks is speculative without direct comparisons.

## Next Checks

1. **Benchmark Transfer**: Test AXIOM on a subset of Atari 2600 games to evaluate performance outside the Gameworld 10k distribution.
2. **Hyperparameter Sensitivity**: Run sweeps over key hyperparameters (e.g., τSMM, τrMM, λIG) to assess robustness and identify failure modes.
3. **Statistical Significance**: Increase the number of random seeds to 10 and compute confidence intervals for cumulative reward curves to validate the reported performance gains.