---
ver: rpa2
title: Rethinking Reward Models for Multi-Domain Test-Time Scaling
arxiv_id: '2510.00492'
source_url: https://arxiv.org/abs/2510.00492
tags:
- step
- reward
- math
- cots
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study challenges the prevailing assumption that process reward\
  \ models (PRMs) outperform outcome reward models (ORMs) in test-time scaling across\
  \ diverse domains. Through a unified evaluation of four reward model variants\u2014\
  discriminative and generative ORM/PRM\u2014across 14 domains, the authors find that\
  \ ORM variants perform on par with or better than PRM variants, with generative\
  \ ORM (gORM) showing the most consistent gains."
---

# Rethinking Reward Models for Multi-Domain Test-Time Scaling

## Quick Facts
- arXiv ID: 2510.00492
- Source URL: https://arxiv.org/abs/2510.00492
- Reference count: 40
- Primary result: Generative outcome reward models (gORM) outperform or match process reward models (PRMs) across 14 domains, challenging the assumption that PRMs are superior for test-time scaling.

## Executive Summary
This study systematically evaluates four reward model variants—discriminative and generative outcome reward models (dORM, gORM) and process reward models (dPRM, gPRM)—for test-time scaling across 14 domains. Contrary to prevailing assumptions, ORM variants perform on par with or better than PRM variants, with gORM showing the most consistent gains. The analysis reveals that PRMs suffer from compounding errors as reasoning chain length increases and are sensitive to label noise in multi-domain data. Theoretical analysis supports these findings, showing PRM error bounds grow linearly with chain length while ORM bounds remain constant. The study provides practical guidelines for selecting reward models based on domain characteristics and computational constraints.

## Method Summary
The authors evaluate four reward model variants on 14 multi-domain benchmarks using Best-of-N (BoN) test-time scaling. They train LoRA-fine-tuned reward models on MMLU-Pro and PRM800K datasets, comparing discriminative (binary classification) and generative (verification CoT generation) approaches for both outcome and process supervision. The gORM and gPRM variants require generating verification rationales via consensus filtering with an LLM judge before training. All models are evaluated using F1 score for outcome verification and task accuracy with N∈{1,2,4,8,16} candidates per question.

## Key Results
- gORM achieves the highest BoN performance across all 14 domains tested
- PRM variants show significant performance degradation on "aha" CoTs where reasoning chains self-correct
- PRM error bounds grow linearly with chain length (T), while ORM bounds remain constant
- Generative reward models consistently outperform discriminative variants, especially in multi-domain settings

## Why This Works (Mechanism)

### Mechanism 1: PRM Error Compounding with Chain Length
PRM error bounds grow linearly with reasoning chain length T, while ORM bounds remain constant. PRMs aggregate stepwise predictions via product/min operations, where each step classification error propagates and compounds. ORMs make a single prediction on the full trajectory, avoiding this multiplicative error accumulation.

### Mechanism 2: PRM Inability to Credit "Aha" Recoveries
PRMs trained with the assumption that incorrect steps imply all subsequent steps are incorrect systematically fail to reward chains that recover from early errors. The PRM training labels T' as the first incorrect step and trains only up to T', causing aggregation (min/product) to penalize entire trajectories even when later steps correct the reasoning.

### Mechanism 3: Generative Verification Enables Holistic Judgment
gORM's generative verification CoT allows flexible, context-aware reasoning about correctness before outputting a verdict, unlike discriminative binary classification. gORM samples multiple verification CoTs (M=10-16) and computes normalized "Yes" probability, capturing uncertainty and allowing nuanced reasoning about the entire reasoning chain.

## Foundational Learning

- Concept: Test-Time Scaling (TTS) with Best-of-N
  - Why needed here: The paper evaluates reward models via BoN selection. Understanding that TTS allocates inference compute to generate N candidates and select the highest-scoring one is essential.
  - Quick check question: Given 16 CoT candidates and a reward model, how do you select the final answer? (Answer: i★ = argmax_i f̂(x^(i)))

- Concept: Process vs. Outcome Supervision
  - Why needed here: The core distinction between PRMs (stepwise labels z_1:T) and ORMs (final label y) determines training objectives.
  - Quick check question: What label does dPRM require that dORM does not? (Answer: Per-step correctness labels z_1:T)

- Concept: Consensus Filtering for Generative Verifiers
  - Why needed here: gORM/gPRM training data is synthesized by sampling verification CoTs from an LLM-judge and retaining only those whose parsed verdict agrees with ground truth.
  - Quick check question: Why might consensus filtering bias gPRM training data toward shorter CoTs? (Answer: Longer CoTs have more steps to disagree with process labels, increasing rejection rate)

## Architecture Onboarding

- Component map:
  Question q → LLM Generator → N CoT candidates r^(i)_1:T
                                      ↓
  Reward Model (dORM/dPRM/gORM/gPRM) → Scores f̂(x^(i))
                                      ↓
  Selection (Best-of-N) → Final answer â(r_T^(i★))

- Critical path:
  1. Generate N CoTs per question using policy LLM (e.g., Llama-3.1-8B-Instruct)
  2. For each candidate, compute reward score:
     - dORM: Single forward pass, binary classification head
     - dPRM: T forward passes (one per step prefix), aggregate via min/product
     - gORM: Sample M verification CoTs, compute normalized "Yes" probability
     - gPRM: Sample M verification CoTs with step verdicts, aggregate
  3. Select argmax candidate

- Design tradeoffs:
  - dORM: Fast inference (single pass), coarse signal. Good for latency-constrained settings.
  - dPRM: Higher compute (T× forward passes), sensitive to label noise and chain length. Use only with clean process labels and short CoTs.
  - gORM: Moderate compute (M samples per candidate), robust across domains. Recommended default for multi-domain.
  - gPRM: Highest compute, fails in multi-domain due to length distribution shift. Avoid for now.

- Failure signatures:
  - PRM performance degrades with long CoTs (check: is avg T > 8? If so, consider ORM)
  - dPRM F1 drops sharply when synthetic process labels are noisy (>20% noise)
  - gPRM training data size much smaller than gORM after consensus filtering
  - ORM overfitting to answers: test on shuffled CoTs; if performance unchanged, model is memorizing

- First 3 experiments:
  1. Reproduce Fig. 6 on a held-out domain: Train gORM and dPRM on MMLU-Pro training split, evaluate BoN@16 on test split. Confirm gORM ≥ dPRM in ≥10/14 domains.
  2. Ablate chain length: Bin test CoTs by step count T, plot F1 vs. T for all four variants. Expect dPRM/gPRM negative slope, gORM positive or flat.
  3. Noise sensitivity test: Inject 10-50% label noise into PRM800K process labels, retrain dPRM and gORM, plot F1 degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed performance trends generalize to open-ended generation tasks where ground-truth verification is not as straightforward as multiple-choice or math problems?
- Basis in paper: [explicit] Section 5 states, "we evaluate only tasks with verifiable outcomes (e.g., math and non-math multiple-choice), which may not generalize to open-ended generation."
- Why unresolved: The current study relies on benchmarks (ProcessBench, MMLU-Pro) with verifiable final answers, leaving the effectiveness of gORM versus PRMs in open-ended domains unknown.
- What evidence would resolve it: Evaluation of the four reward model variants on open-ended tasks (e.g., creative writing or long-form QA) using human or model-based evaluation metrics for coherence and correctness.

### Open Question 2
- Question: Can reinforcement learning (RL)-based training improve the reliability of generative verifiers compared to the supervised fine-tuning methods analyzed in this paper?
- Basis in paper: [explicit] Section 5 mentions, "One could instead use a generative verifier to roll out rationales and treat agreement between their verdict and the GT label as a reward signal for reinforcement learning (RL)... we exclude RL-based training from our analysis."
- Why unresolved: The authors limited their implementation to supervised fine-tuning with consensus filtering to avoid confounders, leaving the potential benefits of RL-based training unexplored.
- What evidence would resolve it: A comparative study training gORM and gPRM using RL (treating ground-truth agreement as reward) and measuring performance changes on multi-domain test-time scaling benchmarks.

### Open Question 3
- Question: Does incorporating tool use into the verification pipeline mitigate the label noise inherent in LLM-based auto-labeling for multi-domain PRMs?
- Basis in paper: [explicit] Section 5 notes, "Following most of the PRM literature... we do not consider tool use, however, Gou et al. (2024) showed that tool use can help reduce auto-label noise."
- Why unresolved: The study identifies label noise as a key failure mode for PRMs in multi-domain settings but did not evaluate whether external tools could clean the process labels or improve verification.
- What evidence would resolve it: An experiment comparing the performance of dPRM/gPRM trained on auto-labeled data versus data where intermediate steps are verified or generated using external tools (e.g., code interpreters).

### Open Question 4
- Question: How can PRM architectures or aggregation functions be modified to robustly handle "aha" moments (self-corrections) in long reasoning chains without suffering from error compounding?
- Basis in paper: [inferred] Section 4.1 shows PRMs fail on "aha" CoTs because stepwise aggregation compounds errors, and Theorem 4.2 proves PRM error bounds grow linearly with chain length.
- Why unresolved: The paper demonstrates that current PRMs penalize trajectories that recover from early errors, but does not propose a solution to modify the PRM to recognize recovery steps.
- What evidence would resolve it: The development of a PRM variant with a non-monotonic aggregation function or a training objective that rewards recovery, showing improved performance on long-chain "aha" examples compared to standard PRMs and ORMs.

## Limitations
- Study focuses on mathematical and academic reasoning domains, limiting generalizability to real-world applications
- PRM variants require clean, per-step labels that may not be available in many domains
- Generative reward models incur higher computational costs that may not be feasible for all applications

## Confidence
- PRM error compounding mechanism: High confidence based on rigorous theoretical analysis and empirical validation
- "Aha" recovery failure: Medium confidence—strong empirical evidence but not tested with alternative PRM strategies
- Generative verification advantage: High confidence from consistent BoN performance gains and ablation studies

## Next Checks
1. Test PRM performance on domains with high recovery rates (e.g., code debugging or medical diagnosis) to quantify the "aha" moment limitation in practical settings.
2. Evaluate whether alternative PRM aggregation strategies (e.g., weighted averaging) can mitigate error compounding while preserving the benefits of process supervision.
3. Measure the impact of verification CoT quality degradation on gORM performance through adversarial perturbations or domain shift experiments.