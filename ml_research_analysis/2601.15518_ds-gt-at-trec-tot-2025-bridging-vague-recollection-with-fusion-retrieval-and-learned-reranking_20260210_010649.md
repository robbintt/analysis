---
ver: rpa2
title: 'DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval
  and Learned Reranking'
arxiv_id: '2601.15518'
source_url: https://arxiv.org/abs/2601.15518
tags:
- retrieval
- dense
- reranking
- queries
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage retrieval system for the TREC Tip-of-the-Tongue
  (ToT) task that combines multiple retrieval methods with learned and LLM-based reranking.
  The approach uses hybrid retrieval merging LLM, sparse (BM25), and dense (BGE-M3)
  methods in the first stage, along with topic-aware multi-index dense retrieval that
  partitions Wikipedia into 24 topical domains.
---

# DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking

## Quick Facts
- arXiv ID: 2601.15518
- Source URL: https://arxiv.org/abs/2601.15518
- Reference count: 28
- Primary result: Achieved recall of 0.66 and NDCG@1000 of 0.41 on TREC TOT test set using hybrid retrieval + LLM reranking

## Executive Summary
This paper presents a two-stage retrieval system for the TREC Tip-of-the-Tongue (ToT) task that combines multiple retrieval methods with learned and LLM-based reranking. The approach uses hybrid retrieval merging LLM, sparse (BM25), and dense (BGE-M3) methods in the first stage, along with topic-aware multi-index dense retrieval that partitions Wikipedia into 24 topical domains. In the second stage, both a trained LambdaMART reranker and LLM-based reranking are evaluated. To support training, 5,000 synthetic ToT queries are generated using LLMs. The best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating that fusion retrieval significantly improves performance over individual methods.

## Method Summary
The system employs a two-stage pipeline: first-stage hybrid retrieval that round-robin merges LLM (Gemini-2.5-flash), sparse (BM25 via PyTerrier), and dense (BGE-M3) methods with deduplication, followed by second-stage reranking using either trained LambdaMART or LLM-based listwise reranking via RankLLM sliding windows. The hybrid approach leverages complementary retrieval signals while LLM reranking substantially improves precision. The LambdaMART reranker is trained on 5,000 synthetic ToT queries generated via LLM with stratified entity sampling from Wikipedia, using 5 features (dense/BM25 scores, pageview, PageRank, query length) and pairwise loss. Topic-aware routing partitions Wikipedia into 24 domains using a ModernBERT-web-topics model to improve efficiency, though this reduces recall compared to full-index retrieval.

## Key Results
- Hybrid retrieval achieves 4.4%-11% recall improvement over best individual method across dev sets
- LLM reranking (Gemini-2.5-flash) achieves 68% improvement in reciprocal rank over sparse baseline
- LambdaMART improves recall but degrades reciprocal rank on test data (0.2838 → 0.0601), indicating overfitting
- Topic-aware routing reduces recall@1000 from 0.5498 to 0.5096 due to imperfect query classification

## Why This Works (Mechanism)

### Mechanism 1: Complementary Retrieval Fusion
Each retrieval paradigm captures different relevance signals—BM25 matches lexical overlap, BGE-M3 captures semantic similarity, and LLM retrieval leverages parametric knowledge. Round-robin interleaving prevents dominance by any single method while preserving rank-based quality signals. Core assumption: The target document appears in at least one retrieval method's top results.

### Mechanism 2: LLM Listwise Reranking with Sliding Windows
The RankLLM sliding window algorithm partitions candidate documents into context-window-compatible groups. The LLM performs listwise comparison within each window, then windows are aggregated. Larger models (27B parameters) consistently outperform smaller variants (1B-8B). Core assumption: LLMs can robustly compare document relevance in listwise fashion within their context window.

### Mechanism 3: Synthetic Query Augmentation for Reranker Training
Entity sampling from Wikipedia with stratification by infobox template ensures domain diversity. The generation prompt enforces vague, verbose descriptions characteristic of ToT queries. Correlation validation (Pearson > 0.93) confirms synthetic queries capture similar semantic characteristics. Core assumption: Synthetic queries generated from Wikipedia entity descriptions approximate the distribution of real ToT query behavior.

## Foundational Learning

- Concept: **Dense vs. Sparse Retrieval**
  - Why needed here: Understanding when BM25 (lexical matching) fails on verbose, imprecise ToT queries versus when BGE-M3 (semantic embeddings) fails on rare entity names.
  - Quick check question: Given a ToT query "that movie with the guy who was in the other movie with the boat," which retrieval method would likely rank the target higher and why?

- Concept: **Listwise vs. Pointwise Reranking**
  - Why needed here: The paper uses listwise LLM reranking (comparing documents within a window) rather than pointwise scoring. This affects how you design prompts and window sizes.
  - Quick check question: What is the trade-off between window size (more context for comparison) and stride length (computational cost vs. ranking stability)?

- Concept: **Recall@K vs. NDCG@K**
  - Why needed here: LambdaMART improves recall but degrades reciprocal rank on test data, indicating a precision-recall trade-off that must be understood for pipeline design.
  - Quick check question: If a system achieves high recall@1000 but low NDCG@1000, where are relevant documents likely positioned in the ranked list?

## Architecture Onboarding

- Component map: Query → parallel retrieval (BM25, BGE-M3, LLM) → round-robin fusion → top-1000 candidates → LambdaMART filter (optional) → LLM reranking → final ranking

- Critical path: 1) Query → parallel retrieval (BM25, BGE-M3, LLM) → round-robin fusion → top-1000 candidates; 2) Candidates → LambdaMART filter (optional high-recall stage) → LLM reranking → final ranking; 3) Best results require Gemini-2.5-flash reranking on full hybrid results (gmn-rerank-500: recall 0.6559)

- Design tradeoffs:
  - Topic-aware routing: Reduces search space (24 topic indexes) but risks excluding relevant documents when classification fails. Result: recall@1000 drops from 0.5498 → 0.5096.
  - LambdaMART vs. LLM-only reranking: LambdaMART is computationally cheaper but generalizes poorly (RR drops from 0.2838 → 0.0601 on test). Best used as high-recall filter before LLM reranking.
  - Window size in LLM reranking: Larger windows (20 documents) provide more comparison context but require more computation; stride (10) determines overlap and stability.

- Failure signatures:
  - LambdaMART improves recall but severely degrades precision (NDCG@1000 = 0.1452) on test data—sign of overfitting to training distribution
  - Topic-aware retrieval shows lower recall@1000 than baseline—sign of query misclassification or cross-topic documents
  - LLM retrieval varies dramatically across datasets (dev1: 0.14 vs. dev3: 0.63 recall)—sign of domain mismatch in parametric knowledge

- First 3 experiments:
  1. Reproduce hybrid fusion baseline: Implement round-robin merging of BM25 + BGE-M3 + LLM retrieval on dev3; validate 0.83 recall@1000 matches paper
  2. Ablate retrieval methods: Remove each retrieval method individually to measure contribution; expect largest drop when removing dense retrieval based on Table 3 patterns
  3. LambdaMART + LLM cascade: Apply LambdaMART to top-1000 hybrid results, then LLM rerank top-500; compare to direct LLM reranking on computational cost vs. recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval expansion via graph-based breadth-first search (BFS) effectively increase recall by utilizing the neighborhood relations captured in the Wikipedia pagelink graph?
- Basis in paper: [explicit] The authors state in the Discussion, "There were other ideas with the graph representation that we might try in the future: retrieval expansion via breadth-first search... [to] increase recall on retrieval."
- Why unresolved: The authors constructed the graph and computed centrality measures but only evaluated them as reranking features; they did not implement or test the proposed BFS expansion strategy.
- What evidence would resolve it: An ablation study measuring Recall@1000 where the retrieved set is augmented with $n$-hop neighbors from the pagelink graph.

### Open Question 2
- Question: Why does the LambdaMART reranker generalize poorly to the test set, significantly degrading Reciprocal Rank (RR) despite improving recall, compared to its performance on development sets?
- Basis in paper: [inferred] The paper notes a discrepancy where LambdaMART improves recall on the test set but drops RR from 0.2838 to 0.0601, a result "different from what we observed in the training and development sets," suggesting a failure to generalize precision.
- Why unresolved: The authors identify the symptom (displacement of top documents) but do not determine if the cause is overfitting to synthetic query distributions or a mismatch in feature importance (e.g., PageRank) on unseen data.
- What evidence would resolve it: An error analysis comparing the feature distributions of the displaced top-ranked documents between the dev and test sets, or an evaluation of the model trained solely on human-elicited data.

### Open Question 3
- Question: How can query routing in topic-aware multi-index retrieval be improved to prevent the exclusion of relevant documents that span multiple topical domains?
- Basis in paper: [inferred] The authors report that their topic-aware approach lowered recall@1000 (0.5096 vs 0.5498) and attribute this to the fact that "topic-based partitioning may occasionally exclude relevant documents when query classification is imperfect."
- Why unresolved: The current hard-routing strategy creates a trade-off between computational efficiency and effectiveness, failing to capture cross-domain relevance.
- What evidence would resolve it: Experiments using "soft" routing (querying multiple top-weighted indexes) or performing multi-label classification to verify if recall can be recovered without linearly increasing latency.

## Limitations

- LambdaMART reranker shows significant performance degradation on test data (RR drops from 0.28 to 0.06), indicating potential overfitting that isn't fully explained.
- Topic-aware routing, while computationally efficient, reduces recall by 4.5% compared to full-index retrieval, suggesting classification errors or cross-topic documents.
- System's dependence on synthetic query generation introduces uncertainty about real-world generalization, as correlation metrics may not capture all behavioral patterns.

## Confidence

**High Confidence**: The hybrid retrieval fusion mechanism and its recall improvements are well-supported by systematic ablation studies showing 4.4%-11% gains over individual methods. The LLM listwise reranking effectiveness is corroborated by external evaluation studies (arXiv:2508.16757).

**Medium Confidence**: The synthetic query generation pipeline's ability to capture real ToT query characteristics relies on the assumption that Wikipedia entity descriptions adequately represent vague, verbose descriptions. While correlation metrics are strong, the methodology hasn't been validated against held-out real queries.

**Low Confidence**: The LambdaMART reranker's poor test generalization is concerning but the paper doesn't fully investigate whether this stems from distribution shift, inadequate training data, or model capacity limitations. The 5,000 synthetic queries may be insufficient for robust learning.

## Next Checks

1. **Generalization Testing**: Evaluate the complete pipeline on held-out real ToT queries not used in synthetic query generation to assess whether the synthetic-query assumption holds across different query distributions.

2. **LambdaMART Analysis**: Conduct detailed error analysis comparing training vs. test performance to identify whether the generalization gap stems from feature importance shifts, overfitting, or distributional differences in query characteristics.

3. **Topic Classification Audit**: Measure topic classification accuracy on the dev set and analyze cases where relevant documents span multiple topics to quantify the true cost of topic-aware routing versus its computational benefits.