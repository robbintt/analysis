---
ver: rpa2
title: Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation
arxiv_id: '2505.20745'
source_url: https://arxiv.org/abs/2505.20745
tags:
- heart
- audio
- speech
- layer
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of foundation model (FM) hidden representations
  for heart rate estimation from auscultation. The study investigates six pre-trained
  acoustic representation FMs (HuBERT, wav2vec2, wavLM, Whisper, CLAP, and an in-house
  CLAP) using a publicly available phonocardiogram (PCG) dataset.
---

# Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation

## Quick Facts
- arXiv ID: 2505.20745
- Source URL: https://arxiv.org/abs/2505.20745
- Reference count: 0
- Primary result: In-house CLAP model achieves 1.88 bpm MAE for heart rate estimation from PCG, outperforming traditional acoustic features (1.91 bpm MAE)

## Executive Summary
This paper investigates the use of foundation model hidden representations for heart rate estimation from phonocardiogram (PCG) recordings. The study evaluates six pre-trained acoustic representation models (HuBERT, wav2vec2, wavLM, Whisper, CLAP, and an in-house CLAP) on a publicly available dataset of 1,274 recordings. A layer-wise analysis reveals how different model depths capture cardiorespiratory information. The in-house CLAP model, trained for audio event detection beyond speech, demonstrates superior performance with 1.88 bpm mean absolute error, slightly outperforming traditional acoustic feature-based methods.

## Method Summary
The research employs six pre-trained foundation models as feature extractors for heart rate estimation from auscultation. The models process phonocardiogram recordings to extract hidden representations, which are then used for heart rate prediction through regression. A layer-wise analysis examines how information about cardiorespiratory signals is captured at different depths within each model. The study uses standardized train/validation/test splits and compares performance against a baseline method using traditional acoustic features. The in-house CLAP model was specifically trained on audio event detection tasks beyond speech to enhance its ability to capture cardiac-specific features.

## Key Results
- In-house CLAP model achieves 1.88 bpm MAE, outperforming traditional acoustic features (1.91 bpm MAE)
- Foundation model representations show comparable performance to acoustic features despite domain mismatch
- Layer-wise analysis reveals varying levels of cardiorespiratory information capture across model depths

## Why This Works (Mechanism)
The effectiveness stems from foundation models' ability to capture general acoustic patterns that transfer to cardiac sound analysis. These models, pre-trained on vast audio datasets, learn hierarchical representations that encode temporal patterns, frequency characteristics, and event boundaries. For cardiac sounds, these learned representations can identify S1 events and their temporal relationships without requiring task-specific training on cardiac data. The in-house CLAP model's performance advantage likely results from its training on diverse audio event detection tasks, which provides broader feature extraction capabilities relevant to cardiac auscultation.

## Foundational Learning
- Phonocardiogram (PCG) analysis: Essential for understanding cardiac sound recording and processing; quick check: verify S1 event detection accuracy
- Heart rate estimation from acoustic signals: Core application requiring temporal pattern recognition; quick check: validate bpm calculation methodology
- Foundation model adaptation: Critical for applying pre-trained models to new domains; quick check: assess transfer learning effectiveness
- Layer-wise representation analysis: Important for understanding feature extraction at different model depths; quick check: examine correlation between layer depth and performance
- Audio event detection: Relevant for identifying cardiac events within recordings; quick check: evaluate S1 event detection precision

## Architecture Onboarding

**Component Map:** PCG Recording -> Foundation Model -> Hidden Representation -> Heart Rate Regression

**Critical Path:** Audio preprocessing → Foundation model feature extraction → Temporal aggregation → Regression model → Heart rate prediction

**Design Tradeoffs:** The study balances model complexity against generalization by using pre-trained models rather than training from scratch. This approach leverages existing acoustic knowledge while avoiding overfitting on the limited dataset. The choice of multiple models allows comparison of different architectural approaches to the same task.

**Failure Signatures:** Performance degradation may occur with poor signal quality, presence of noise, or recordings with abnormal cardiac rhythms. The layer-wise analysis may reveal specific depths where information loss occurs, indicating architectural limitations for cardiac sound processing.

**First Experiments:**
1. Layer ablation study: Remove specific layers to determine minimal architecture needed
2. Cross-dataset validation: Test models on independent PCG datasets
3. Multi-event classification: Extend beyond S1 detection to include S2 and murmurs

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (1,274 recordings) limits generalizability
- Single cardiac event type (S1) focus restricts broader applicability
- Single dataset evaluation without external validation
- Comparison limited to one baseline method

## Confidence
- **High**: Comparative performance claims between foundation models and baseline methods
- **Medium**: Generalizability across datasets and patient populations
- **Medium**: Layer-wise analysis insights due to limited biological interpretability

## Next Checks
1. Evaluate performance on additional independent PCG datasets with diverse cardiac conditions
2. Test the approach across multiple cardiac event types beyond S1
3. Compare against a broader range of traditional and deep learning baseline methods for heart rate estimation from cardiac sounds