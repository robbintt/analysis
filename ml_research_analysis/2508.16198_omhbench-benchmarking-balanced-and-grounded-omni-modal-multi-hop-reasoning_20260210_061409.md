---
ver: rpa2
title: 'OMHBench: Benchmarking Balanced and Grounded Omni-Modal Multi-Hop Reasoning'
arxiv_id: '2508.16198'
source_url: https://arxiv.org/abs/2508.16198
tags:
- reasoning
- gemini
- question
- text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMHBench introduces a controlled benchmark for omni-modal multi-hop
  reasoning, addressing modality shortcuts and reasoning path bias in prior evaluations.
  It enforces balanced reasoning paths across text, vision, and speech, with each
  modality required at least once.
---

# OMHBench: Benchmarking Balanced and Grounded Omni-Modal Multi-Hop Reasoning

## Quick Facts
- arXiv ID: 2508.16198
- Source URL: https://arxiv.org/abs/2508.16198
- Authors: Seunghee Kim; Ingyu Bang; Seokgyu Jang; Changhyeon Kim; Sanghwan Bae; Jihun Choi; Richeng Xuan; Taeuk Kim
- Reference count: 40
- Introduces controlled benchmark for omni-modal multi-hop reasoning with balanced path constraints

## Executive Summary
OMHBench addresses critical limitations in evaluating omni-modal reasoning systems by introducing a controlled benchmark that enforces balanced reasoning paths across text, vision, and speech modalities. The benchmark comprises 6,144 instances split evenly across six reasoning paths, requiring each modality at least once per question. Evaluation of 13 state-of-the-art models reveals substantial performance gaps between proprietary and open-source systems, with particular sensitivity to reasoning path order. The benchmark demonstrates that most models fail to consistently answer questions across all path variations, exposing asymmetric omni-modal grounding capabilities.

## Method Summary
The benchmark uses a controlled generation process where questions are systematically constructed across six balanced reasoning paths, ensuring each modality (text, vision, speech) appears at least once per instance. The dataset covers four domains: finance, economics, climate, and nutrition. Each instance requires multi-hop reasoning across modalities, with path balance scores measuring model consistency across different ordering variations. The evaluation framework includes proprietary models like GPT-4V, Gemini, and Claude, alongside open-source alternatives like LLaVA and Qwen.

## Key Results
- Proprietary models significantly outperform open-source alternatives on balanced reasoning paths
- Path Balance Scores reveal most models fail to consistently answer questions across all path variations
- Accuracy drops sharply when speech appears as a late reasoning step, indicating challenges in cross-modal semantic transfer
- Models show asymmetric performance patterns, struggling with certain modality orderings despite success on others

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its controlled generation process that eliminates modality shortcuts and enforces balanced reasoning paths. By systematically varying the order of modality integration, the benchmark reveals true omni-modal reasoning capabilities rather than allowing models to exploit single-modality strengths. The requirement that each modality must be used at least once prevents models from avoiding challenging modalities, while the balanced path distribution ensures comprehensive evaluation of cross-modal reasoning abilities.

## Foundational Learning
- Multi-hop reasoning: Understanding how to chain information across multiple reasoning steps
  - Why needed: Single-hop reasoning is insufficient for complex real-world problems
  - Quick check: Can the model combine information from multiple sources sequentially?
- Cross-modal semantic transfer: Ability to map concepts between text, vision, and speech
  - Why needed: Real-world reasoning often requires integrating heterogeneous information
  - Quick check: Does performance degrade when modalities appear in different orders?
- Modality balance: Ensuring equal representation and usage of all input types
  - Why needed: Prevents models from exploiting modality shortcuts
  - Quick check: Are all modalities required and used in each reasoning path?

## Architecture Onboarding

Component map:
Question Generation -> Balanced Path Construction -> Instance Creation -> Model Evaluation -> Path Balance Scoring

Critical path:
Controlled generation of questions with systematic path variation -> balanced evaluation across all six paths -> measurement of path balance scores

Design tradeoffs:
- Controlled generation ensures balanced evaluation but may create artifacts
- Six path variations provide comprehensive coverage but increase complexity
- Domain limitation to four areas ensures focus but reduces generalizability

Failure signatures:
- High variance in accuracy across path orderings indicates weak cross-modal transfer
- Poor performance on speech-late paths suggests integration challenges
- Inconsistent path balance scores reveal modality-specific weaknesses

First experiments:
1. Evaluate model performance on single-modality vs. multi-modality versions of same questions
2. Test sensitivity to path ordering by randomizing modality sequence
3. Measure performance degradation when speech modality is removed entirely

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled generation process may create artifacts not representative of real-world data distributions
- Limited to four specific domains (finance, economics, climate, nutrition) reducing generalizability
- Speech modality performance issues may stem from speech-to-text conversion quality rather than model limitations
- 6,144 instance size may be insufficient for robust conclusions about large-scale model capabilities

## Confidence

High confidence in the benchmark construction methodology and path balance measurements
Medium confidence in cross-model performance comparisons due to potential domain-specific artifacts
Low confidence in interpreting speech modality failures without additional error analysis

## Next Checks

1. Conduct ablation studies removing the balanced path constraint to measure real-world performance degradation
2. Expand domain coverage to test generalizability beyond the four current domains
3. Perform human evaluation on a subset of speech-related questions to distinguish model limitations from speech-to-text conversion errors