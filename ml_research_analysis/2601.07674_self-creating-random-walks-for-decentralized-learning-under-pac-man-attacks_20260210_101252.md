---
ver: rpa2
title: Self-Creating Random Walks for Decentralized Learning under Pac-Man Attacks
arxiv_id: '2601.07674'
source_url: https://arxiv.org/abs/2601.07674
tags:
- pac-man
- node
- graph
- algorithm
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of random-walk (RW)-based
  decentralized learning algorithms to a novel "Pac-Man" attack, where a malicious
  node probabilistically terminates any RW that visits it, effectively halting the
  learning process. The authors propose the CREATE-IF-LATE (CIL) algorithm, a fully
  decentralized mechanism that enables self-creating RWs to prevent extinction.
---

# Self-Creating Random Walks for Decentralized Learning under Pac-Man Attacks

## Quick Facts
- arXiv ID: 2601.07674
- Source URL: https://arxiv.org/abs/2601.07674
- Authors: Xingran Chen; Parimal Parag; Rohit Bhagat; Salim El Rouayheb
- Reference count: 40
- Primary result: CREATE-IF-LATE (CIL) algorithm achieves non-extinction, boundedness, and convergence of RW-SGD under Pac-Man attacks with quantifiable deviation

## Executive Summary
This paper addresses a critical vulnerability in decentralized learning systems where random-walk-based algorithms can be disrupted by malicious nodes implementing a "Pac-Man" attack. In this attack, a compromised node probabilistically terminates any random walk that visits it, potentially causing permanent extinction of the learning process. The authors propose the CREATE-IF-LATE (CIL) algorithm, a fully decentralized mechanism that enables self-creating random walks to prevent extinction while maintaining bounded population size. The algorithm ensures that learning can continue even under adversarial conditions, with theoretical guarantees on both population dynamics and convergence properties.

The CIL algorithm introduces a novel approach where nodes probabilistically create new random walks when existing ones become extinct. This self-creation mechanism, combined with a strategic replacement policy, ensures that the random walk population neither dies out nor grows unboundedly. The paper provides rigorous theoretical analysis showing that the algorithm achieves almost sure non-extinction and boundedness of the random walk population, while also maintaining convergence of the underlying stochastic gradient descent process. Empirical results on both synthetic and benchmark datasets validate the theoretical findings, demonstrating that CIL maintains learning effectiveness even when subjected to active adversarial attacks.

## Method Summary
The CREATE-IF-LATE (CIL) algorithm operates through a decentralized mechanism where each node probabilistically creates new random walks when existing ones terminate. When a random walk reaches a node, the node decides whether to forward it to a neighbor or terminate it based on a probabilistic model. If a walk terminates, the receiving node may create a new walk with probability q, where q > ζ (the termination probability of the Pac-Man attack). The algorithm also implements a replacement strategy where nodes can substitute terminated walks with new ones to maintain population stability. This self-creation mechanism ensures that the random walk population remains active and bounded, preventing both extinction and unbounded growth while enabling continued learning through stochastic gradient descent updates.

## Key Results
- CIL algorithm ensures almost sure non-extinction and boundedness of random walk population under Pac-Man attacks
- Theoretical analysis shows expected peak RW population scales as O(qN²/ζ), where q is creation probability and ζ is termination probability
- Convergence guarantees achieved with lower bound of N/(N + A - 1 + 1/q) on fraction of effective learning iterations
- Empirical validation demonstrates maintained learning effectiveness on synthetic and benchmark datasets under adversarial conditions

## Why This Works (Mechanism)
The CIL algorithm works by establishing a probabilistic advantage over the Pac-Man attacker. By setting the creation probability q greater than the termination probability ζ, the algorithm ensures that new random walks are generated faster than they are terminated. This creates a positive feedback loop where the population can recover from losses and maintain itself over time. The boundedness result follows from careful balancing of creation and termination rates, preventing exponential growth while ensuring survival. The convergence guarantees stem from the fact that even under attack, a sufficient fraction of learning iterations remain effective, allowing the SGD process to approach the optimal solution within quantifiable bounds.

## Foundational Learning
- Random Walk Dynamics: Understanding how random walks propagate through networks and how termination affects population behavior - needed to model attack impact and design countermeasures
- Stochastic Gradient Descent: The optimization algorithm being protected - needed to establish convergence requirements and quantify learning degradation under attack
- Markov Chain Analysis: Tools for analyzing state transitions in the random walk population - needed to prove non-extinction and boundedness properties
- Probabilistic Scheduling: Mechanisms for controlling when and how new walks are created - needed to balance population stability with computational efficiency
- Adversarial Modeling: Formal framework for analyzing malicious behavior in distributed systems - needed to quantify attack impact and design robust defenses

## Architecture Onboarding
Component Map: Nodes -> Random Walks -> Gradient Updates -> Model Parameters -> Learning Performance
Critical Path: Random Walk Creation → Walk Propagation → Node Processing → Gradient Computation → Parameter Update → Convergence Verification
Design Tradeoffs: Higher creation probability q improves robustness but increases communication overhead; tighter bounds on population size reduce resource usage but may slow convergence
Failure Signatures: Sudden drop in random walk population indicates active Pac-Man attack; plateaued learning curves suggest insufficient effective iterations
First Experiments:
1. Verify non-extinction by monitoring RW population under varying ζ values with fixed q > ζ
2. Test boundedness by measuring peak population size across multiple runs with different random seeds
3. Validate convergence by comparing learning curves with and without CIL protection under controlled attack scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes synchronous communication which may not hold in realistic decentralized settings
- Requires perfect knowledge of adversarial termination probability ζ for optimal parameter selection
- Convergence guarantees depend on specific assumptions about loss function smoothness and gradient noise distribution
- Empirical evaluation limited to relatively small-scale problems, leaving scalability questions unanswered

## Confidence
High: Non-extinction and boundedness results for RW population under CIL
Medium: Convergence rate bounds for RW-SGD under attack
Low: Practical effectiveness against adaptive or coordinated Pac-Man attacks

## Next Checks
1. Test CIL on larger-scale datasets (e.g., ImageNet subsets) to evaluate scalability and convergence behavior with increased dimensionality and data volume
2. Implement and evaluate against adaptive Pac-Man attacks where the malicious node dynamically adjusts ζ based on observed RW population dynamics
3. Develop and validate distributed estimation procedures for q and ζ in the absence of global knowledge, measuring the impact on convergence guarantees when using estimated parameters