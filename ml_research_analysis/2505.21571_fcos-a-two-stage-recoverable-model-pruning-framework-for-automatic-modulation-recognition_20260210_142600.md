---
ver: rpa2
title: 'FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation
  Recognition'
arxiv_id: '2505.21571'
source_url: https://arxiv.org/abs/2505.21571
tags:
- pruning
- layer
- channel
- fcos
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FCOS is a two-stage pruning framework for automatic modulation
  recognition that achieves extreme model compression while maintaining high accuracy.
  The method combines channel-level pruning via hierarchical clustering and parameter
  fusion with layer-level collapse diagnosis using linear probing.
---

# FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition

## Quick Facts
- arXiv ID: 2505.21571
- Source URL: https://arxiv.org/abs/2505.21571
- Reference count: 40
- Primary result: Achieves 95.51% FLOPs reduction and 95.31% parameter reduction while maintaining 0.46% accuracy drop on AMR tasks

## Executive Summary
FCOS is a two-stage pruning framework designed for automatic modulation recognition (AMR) that achieves extreme model compression while preserving accuracy. The method combines channel-level pruning through hierarchical clustering and parameter fusion with layer-level collapse diagnosis using linear probing. In the first stage, FCOS clusters similar channel weights and fuses them to reduce model width. The second stage identifies and removes collapsed layers that result from excessive channel pruning. Experiments on three datasets and three models show FCOS achieves near-origami compression rates while maintaining classification accuracy close to the original models.

## Method Summary
FCOS operates in two distinct stages to compress AMR models. Stage 1 performs channel-level pruning by computing cosine similarity between channel weights, applying hierarchical clustering, and fusing similar channels through averaging. This reduces model width while preserving learned representations. Stage 2 diagnoses layer collapse using linear probing: the pruned model is frozen, features are extracted at each layer, and independent linear classifiers measure layer-wise discriminability. Layers showing minimal accuracy gain over previous layers are marked as collapsed and removed. The framework includes short-cycle fine-tuning (20 epochs) after channel pruning and extended fine-tuning (80 epochs) after layer removal to recover accuracy.

## Key Results
- Achieves 95.51% FLOPs reduction and 95.31% parameter reduction on Sig2019-12 with ResNet56
- Maintains only 0.46% accuracy drop compared to baseline models
- Outperforms state-of-the-art pruning methods (AutoPrune, HRank, CP, SCP) on all three tested datasets
- Successfully handles extreme compression rates up to 99% while maintaining functional accuracy through collapse diagnosis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel weights with high cosine similarity contain redundant information that can be merged with minimal information loss.
- Mechanism: Hierarchical clustering groups channels by weight similarity (Eq. 1-4). Channels within each cluster are fused via averaging (Eq. 5), producing a single representative channel that preserves the collective learned patterns while reducing width dimension.
- Core assumption: Similar weight tensors encode similar feature transformations, so their aggregation approximates their combined function.
- Evidence anchors: [abstract] "hierarchical clustering and parameter fusion are applied to channel weights to achieve channel-level pruning"; [section III-A] "By comparing the weight tensors directly, we can detect channels whose parameters exhibit highly similar patterns and prune them with minimal cost."

### Mechanism 2
- Claim: Excessive channel pruning causes structural layer collapse, where entire layers become functionally dead regardless of fine-tuning.
- Mechanism: When channel compression ratios are very high (e.g., 99%), intermediate layers may lose discriminative capacity entirely. The LaCD module freezes the pruned model, extracts features at each layer, and trains independent linear classifiers to measure layer-wise accuracy. If accuracy gain between adjacent layers falls below threshold β (Eq. 8), the layer is marked collapsed and removed.
- Core assumption: Layer collapse manifests as near-zero incremental discriminability in the frozen feature space, detectable by linear probes without full fine-tuning.
- Evidence anchors: [abstract] "Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer collapse and removes the collapsed layers due to high channel compression ratio"; [Table II] At 99% pruning, model can show normal (0.5545) or collapsed (0.4652) accuracy with identical compression—indicating structural failure, not just capacity loss.

### Mechanism 3
- Claim: Fine-tuning from fused weights converges faster and more accurately than training a randomly-initialized model of equivalent size.
- Mechanism: The fused weights inherit learned representations from the pre-trained model, providing a better initialization point than random weights. Short-cycle fine-tuning (20 epochs) stabilizes feature distributions before collapse diagnosis; longer fine-tuning (80 epochs) after layer removal recovers final accuracy.
- Core assumption: Fused channel weights retain sufficient semantic information from the original model to serve as effective initialization.
- Evidence anchors: [section III-A] "the learned weights inherit the capabilities of the pre-trained model"; [section IV-E, Fig. 3] Fine-tuned pruned model achieves ~59% accuracy by epoch 10; random initialization reaches only ~50% even at epoch 80.

## Foundational Learning

- **Hierarchical Clustering with Distance Metrics**
  - Why needed here: Stage 1 requires clustering channels by similarity to identify fusion candidates.
  - Quick check question: Given two weight vectors, can you compute their cosine similarity and convert it to a distance metric suitable for hierarchical clustering?

- **Linear Probing for Feature Evaluation**
  - Why needed here: LaCD uses linear classifiers trained on frozen features to measure each layer's discriminative power.
  - Quick check question: If a layer's features yield 45% accuracy and the next layer yields 45.3% with β=0.5, should this layer be pruned?

- **Channel vs. Layer Pruning Trade-offs**
  - Why needed here: FCOS combines both; understanding their individual limitations explains why the two-stage approach is necessary.
  - Quick check question: Why does channel pruning alone struggle to exceed ~90% compression on deep networks without accuracy collapse?

## Architecture Onboarding

- **Component map**: Pre-trained model -> Stage 1 (Channel clustering + fusion) -> Short fine-tune -> Stage 2 (LaCD linear probing + layer removal) -> Final fine-tune -> Compressed model

- **Critical path**: 1. Set pruning rate ε, collapse threshold β 2. Run Algorithm 1 (clustering + fusion) across all conv layers 3. Fine-tune 20 epochs to stabilize 4. Run Algorithm 2 (LaCD): freeze, probe, identify Pr, remove 5. Fine-tune 80 epochs for recovery

- **Design tradeoffs**:
  - Higher ε → more compression but higher collapse risk
  - Lower β → more aggressive layer removal, potential under-pruning
  - Cosine vs. Euclidean similarity: paper shows robustness (Table VIII), but Euclidean slightly better in tested case
  - Fine-tuning epochs: short (20) for diagnosis stability vs. long (80) for final recovery

- **Failure signatures**:
  - Accuracy drops >5% with similar compression to baselines: likely collapse undetected or over-pruning
  - Layer probe accuracies flat or decreasing through depth: collapse already occurred
  - Fused model worse than random initialization: cluster diversity too high, fusion distorts representations

- **First 3 experiments**:
  1. Replicate CNN1D on RML2016.10a with ε=0.75, β=0.5; verify accuracy within 1% of paper's 59.08%
  2. Ablate Stage 2: run Stage 1 only at ε=0.95 and observe if collapse occurs (compare to Table II pattern)
  3. Test robustness: swap cosine for Euclidean similarity on one model/dataset pair; confirm <0.5% accuracy difference (per Table VIII)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FCOS effectively generalize to standard computer vision tasks (e.g., ImageNet classification) or NLP architectures (e.g., Transformers), where feature hierarchies and data modalities differ significantly from the 1D signal processing domain tested?
- Basis: [inferred] The experimental scope is restricted to three specific Automatic Modulation Recognition (AMR) datasets (RML2016.10a, Sig2019-12, RML2018.01a) and 1D/2D CNN models designed for radio signals.
- Why unresolved: The authors do not demonstrate the framework's utility outside the specific domain of signal modulation recognition.
- What evidence would resolve it: Applying FCOS to standard benchmarks like ImageNet or GLUE and reporting the accuracy-latency trade-off compared to domain-agnostic pruning baselines.

### Open Question 2
- Question: Would integrating advanced model merging techniques (e.g., Fisher Merging or RegMean) into the channel fusion stage yield better feature preservation than the simple averaging or L1-norm weighting currently used?
- Basis: [inferred] The paper reviews advanced fusion methods like Fisher Merging and RegMean in Section II (Related Works), but the proposed method (Eq. 5) and ablation study (Table VII) rely only on simple averaging or weighted averaging.
- Why unresolved: The authors did not experimentally validate whether the more complex fusion strategies mentioned in the literature review could improve the granularity or accuracy of the "Similar Channel Fusion" stage.
- What evidence would resolve it: An ablation study comparing the performance of FCOS when using Fisher Merging versus simple averaging for clustering channel weights.

### Open Question 3
- Question: How sensitive is the Layer Collapse Diagnosis (LaCD) module to the hyperparameters of the linear probing process, specifically the number of preliminary fine-tuning epochs and the redundancy threshold $\beta$?
- Basis: [inferred] The implementation fixes the preliminary fine-tuning to 20 epochs and the probing to 5 epochs, while the redundancy threshold $\beta$ is set to determine collapse based on Eq. (8), without analyzing the stability of these choices across different convergence rates.
- Why unresolved: If the model requires significantly more or fewer epochs to stabilize features before probing, the diagnosis of "collapse" might be premature or delayed, leading to suboptimal layer removal.
- What evidence would resolve it: A sensitivity analysis showing the variance in final accuracy and pruning rate when sweeping the pre-fine-tuning epoch count and the $\beta$ threshold.

### Open Question 4
- Question: Does the irregular model structure resulting from FCOS (varying channel widths and removed layers) translate to measurable latency speedups on standard hardware (CPUs/GPUs), or is the acceleration theoretical (FLOPs-based) only?
- Basis: [inferred] Table I claims "Obvious acceleration effect" as an advantage over weight pruning, but Section IV results only report FLOPs reduction (e.g., 95.51%) and parameter reduction, without providing actual inference time measurements.
- Why unresolved: While structured pruning (channel/layer) is generally more hardware-friendly than unstructured weight pruning, the specific efficiency of the resulting sparse architecture on generic devices remains unverified in the text.
- What evidence would resolve it: Reporting wall-clock inference time (ms/sample) or throughput (samples/sec) on target edge devices or standard GPUs for the pruned models.

## Limitations

- **Pruning rate specification**: The paper reports final compression metrics but does not specify the exact channel pruning rate ε used in each experiment, making it difficult to reproduce identical compression levels.
- **Collapse threshold**: The layer redundancy threshold β is mentioned as "predefined" but no specific value is provided, affecting reproducibility of the layer removal decisions.
- **Linear probe architecture**: Details on the linear classifier configuration (hidden layer size, activation, training epochs per layer) are not specified, though 5 epochs is mentioned for the entire LaCD process.
- **Pre-trained model checkpoints**: No information on the training configuration for the original models, which affects the starting point for pruning.

## Confidence

- **High confidence**: The two-stage framework design (channel fusion followed by layer collapse diagnosis) is well-specified and theoretically sound.
- **Medium confidence**: The hierarchical clustering and parameter fusion mechanism for channel pruning is clearly described with equations, but the choice of cosine similarity and its robustness across different models is not extensively validated.
- **Medium confidence**: The linear probing approach for detecting layer collapse is conceptually valid, but the binary collapse detection may not capture gradual degradation, and the threshold β selection is critical yet unspecified.
- **Low confidence**: The assumption that fused weights provide superior initialization compared to random initialization is supported by empirical results but lacks ablation studies or theoretical justification.

## Next Checks

1. **Verify layer collapse detection**: Run Stage 1 only at 99% pruning rate and check if LaCD correctly identifies collapsed layers by examining linear probe accuracy deltas (should show near-zero differences between adjacent layers in collapsed cases).
2. **Test pruning rate sensitivity**: Systematically vary the channel pruning rate ε from 0.75 to 0.99 on one model/dataset pair and plot accuracy vs. compression to identify the optimal operating point before collapse occurs.
3. **Ablation of fine-tuning phases**: Compare three variants: (a) FCOS full pipeline, (b) Stage 1 only with 100 epochs fine-tuning, (c) Random initialization of compressed model size with 100 epochs training, to isolate the contribution of each component.