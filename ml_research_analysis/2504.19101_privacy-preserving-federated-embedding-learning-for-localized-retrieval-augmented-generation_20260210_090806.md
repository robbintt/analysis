---
ver: rpa2
title: Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented
  Generation
arxiv_id: '2504.19101'
source_url: https://arxiv.org/abs/2504.19101
tags:
- data
- uni00000013
- learning
- federated
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FedE4RAG, a privacy-preserving federated learning
  framework for localized Retrieval-Augmented Generation (RAG) systems. It addresses
  the challenge of training RAG retrievers on distributed private data without exposing
  sensitive information.
---

# Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2504.19101
- Source URL: https://arxiv.org/abs/2504.19101
- Reference count: 40
- The paper introduces FedE4RAG, a privacy-preserving federated learning framework for localized RAG systems, achieving up to 90.22% MAP in retrieval tasks.

## Executive Summary
The paper presents FedE4RAG, a privacy-preserving federated learning framework designed to train localized Retrieval-Augmented Generation (RAG) systems on distributed private data without exposing sensitive information. The framework combines homomorphic encryption to protect model parameters during federated communication and knowledge distillation to improve the generalization of local retrievers by transferring global knowledge. Evaluated on a financial domain dataset (FedE4FIN) with upstream retrieval and downstream generation tasks, FedE4RAG significantly outperforms baselines like SCenT and SVanE1.5base, achieving up to 90.22% MAP in retrieval tasks and demonstrating strong generation performance with various LLMs. It effectively balances data privacy with model performance in localized RAG deployments.

## Method Summary
FedE4RAG is a privacy-preserving federated learning framework that fine-tunes local RAG retrieval models (embeddings) for financial Q&A without sharing raw data. It uses contrastive pre-training (InfoNCE loss) with synthetic query-chunk pairs for domain adaptation, federated knowledge distillation to align local and global model outputs, and homomorphic encryption (CKKS scheme) for secure aggregation. The framework was evaluated on the FedE4FIN dataset (43,658 synthetic query-chunk pairs) and tested on the RAG4FIN dataset (100 queries from FinanceBench + 24,323 pages) using metrics like Hit@1, MRR, NDCG for retrieval and ChrF, ROUGE, PPL for generation.

## Key Results
- FedE4RAG achieved up to 90.22% MAP in retrieval tasks, significantly outperforming baselines like SCenT and SVanE1.5base.
- The framework demonstrated strong generation performance with various LLMs on the RAG4FIN dataset.
- FedE4RAG effectively balances data privacy with model performance in localized RAG deployments.

## Why This Works (Mechanism)

### Mechanism 1: Federated Knowledge Distillation for Global-Local Alignment (KD-GLE)
- **Claim:** Exchanging knowledge via similarity logits (distillation) rather than just raw model weights mitigates the performance degradation caused by non-IID data across clients.
- **Mechanism:** Instead of only averaging model parameters (FedAvg), the framework minimizes the discrepancy between the local model's predicted similarity scores ($z^l$) and the global model's scores ($z^g$) using a penalty term. This forces the local retriever to align its representation space with the global consensus without sharing raw vectors.
- **Core assumption:** Aligning the *outputs* (similarity scores) of local and global models is sufficient to transfer generalizable retrieval capabilities, even if the underlying model weights differ due to heterogeneous local data distributions.
- **Evidence anchors:**
  - [abstract] "knowledge distillation is employed... improves the generalization of local RAG retrievers"
  - [section IV-A-2] "L(z^l, z^g) is defined as the loss measuring the discrepancy between the similarity of the local modelâ€™s representations."
- **Break condition:** If the semantic domains of various clients are entirely disjoint (e.g., one client has medical data, another has legal data in a different language), forcing alignment via global distillation may degrade specific local accuracy (negative transfer).

### Mechanism 2: Homomorphic Encryption for Secure Aggregation (FED-HE)
- **Claim:** Applying fully homomorphic encryption (FHE) to gradient updates prevents the central server from inferring private information about local datasets during the aggregation phase.
- **Mechanism:** Clients encrypt their local gradients $[[\nabla F(w)]]$ before uploading. The server performs a weighted sum on these *encrypted* gradients. Because of the homomorphic properties (specifically CKKS scheme), the server can compute a valid encrypted global gradient without ever decrypting individual client updates.
- **Core assumption:** Assumes the "honest-but-curious" threat model: the server follows the protocol correctly (honest) but may try to analyze received data to infer private attributes (curious). It assumes the client-side environment is secure and keys are managed correctly.
- **Evidence anchors:**
  - [abstract] "apply homomorphic encryption... to safeguard model parameters and mitigate concerns related to data leakage."
  - [section III-B] "The server receives these encrypted gradients... aggregates them homomorphically."
- **Break condition:** If the computational overhead of CKKS encryption/decryption exceeds the client's hardware capacity, training latency becomes impractical. Additionally, this does not protect against attacks where the adversary queries the *final* model (inference-time attacks).

### Mechanism 3: Contrastive Pre-training for Domain Adaptation (RAG-FT)
- **Claim:** Fine-tuning the embedding model using synthetic query-chunk pairs via contrastive loss significantly improves retrieval performance over the base BGE model.
- **Mechanism:** The system generates synthetic (question, chunk) pairs from local documents. It then uses InfoNCE loss to train the model to distinguish the correct chunk from negative samples in the batch. This adapts the "general" embedding space to the specific terminology of the local domain (e.g., financial jargon).
- **Core assumption:** Assumes that the synthetic questions generated by the upstream model (e.g., GPT-4o mini) are of sufficient quality to serve as a proxy for real user queries.
- **Evidence anchors:**
  - [section IV-A-1] "employ Information Noise-Contrastive Estimation (InfoNCE) Loss... as the training objective."
  - [section VI-B-1] "FedE4RAG with RAG-FT not only effectively utilizes local data but also outperforms the existing BGE-large model."
- **Break condition:** If the synthetic data generator produces hallucinated or irrelevant questions, the contrastive loss will fine-tune the retriever to match noise, reducing downstream accuracy.

## Foundational Learning

- **Concept: Federated Learning (FL) / FedAvg**
  - **Why needed here:** The core architecture relies on distributed training. You must understand how standard FedAvg works (local training $\to$ weight upload $\to$ server averaging $\to$ weight download) to understand why the paper modifies it with Distillation and Encryption.
  - **Quick check question:** If a client drops out during a round, how does FedAvg handle the aggregation (compared to the secure aggregation in this paper)?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - **Why needed here:** This is the specific loss function used to fine-tune the embedding model. You need to understand how it pulls positive pairs (query, correct chunk) closer in vector space and pushes negative pairs apart.
  - **Quick check question:** In the InfoNCE loss formula, what happens to the loss if the similarity of the positive pair ($sim(h_q, h_c)$) decreases?

- **Concept: Homomorphic Encryption (HE) - CKKS Scheme**
  - **Why needed here:** The paper uses this for privacy. Unlike standard encryption where you must decrypt to compute, HE allows math on ciphertexts. Understanding this explains why the server can aggregate gradients "blindly."
  - **Quick check question:** Does Homomorphic Encryption protect against an attacker who steals the final trained model weights after the decryption step? (Hint: Check the threat model).

## Architecture Onboarding

- **Component map:** Client Node (Private Vector DB, Local Retriever, Synthetic Data Generator) -> Server Node (Global Retriever, Aggregator) -> Communication Layer (CKKS Homomorphic Encryption)

- **Critical path:**
  1. **Ingest:** Client loads private documents into Vector DB.
  2. **Synthesize:** Client generates (Query, Chunk) pairs for fine-tuning.
  3. **Local Train:** Client minimizes InfoNCE Loss + Distillation Loss.
  4. **Encrypt & Upload:** Client encrypts gradients $[[\nabla F]]$ and sends to Server.
  5. **Aggregate:** Server sums encrypted gradients and updates Global Model.
  6. **Distill:** Server computes global similarity logits and sends them back to clients for the next round of KD-GLE.

- **Design tradeoffs:**
  - **Accuracy vs. Privacy:** The paper argues that KD-GLE maintains accuracy while HE provides privacy. However, HE adds significant computational overhead (encryption/decryption cycles) compared to plaintext FedAvg.
  - **Generalization vs. Personalization:** The distillation loss forces local models to mimic the global model. If local data is unique, this might sacrifice peak local performance for global generalization.

- **Failure signatures:**
  - **Stagnant Loss:** If InfoNCE loss doesn't decrease, check the quality of synthetic query generation (are they generic?).
  - **Timeouts:** If training rounds take too long, profile the CKKS encryption step; it is likely the bottleneck.
  - **Privacy Leak:** Ensure the server *only* receives encrypted gradients. Sending raw logits or weights without encryption violates the threat model.

- **First 3 experiments:**
  1. **Baseline Retrieval Test:** Implement vanilla BGE embedding on the RAG4FIN dataset. Measure Hit@10 and MRR. Then implement FedE4RAG and compare to quantify the lift from federated fine-tuning.
  2. **Ablation on Distillation:** Run the framework with *only* FedAvg (averaging weights) vs. *only* FedE4RAG (using distillation). Compare results to validate the paper's claim that distillation handles non-IID data better.
  3. **Overhead Analysis:** Measure the wall-clock time for one federated round with FED-HE enabled vs. disabled. This quantifies the "cost" of the privacy guarantee.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the computational efficiency and communication costs of FedE4RAG be optimized for larger-scale deployments without degrading retrieval performance?
  - **Basis in paper:** The authors state in the Future Work section: "We aim to optimize the FedE4RAG framework for larger-scale deployments and improve its computational efficiency, particularly in terms of communication costs and training time."
  - **Why unresolved:** The current experiments utilize only 5 clients and a specific dataset. The computational overhead introduced by homomorphic encryption and knowledge distillation may become prohibitive as the number of clients or data volume increases significantly.
  - **What evidence would resolve it:** Empirical results from experiments involving hundreds or thousands of clients, showing convergence time, bandwidth usage, and retrieval accuracy compared to the current small-scale setup.

- **Open Question 2:** Can differential privacy (DP) be successfully integrated into the FedE4RAG framework to provide formal privacy guarantees while maintaining utility?
  - **Basis in paper:** The authors explicitly list "Advanced Privacy-Preserving Techniques" as a future direction, stating: "We plan to investigate more advanced privacy-preserving techniques, such as differential privacy, to further enhance the privacy guarantees provided by our framework."
  - **Why unresolved:** While homomorphic encryption protects data in transit, the current framework does not address privacy leakage from the model parameters themselves (e.g., membership inference). The trade-off between the noise added for DP and the performance of the embedding model is unexplored.
  - **What evidence would resolve it:** A study comparing the retrieval metrics of a DP-enhanced FedE4RAG against the baseline, analyzing the privacy-utility trade-off curves at different epsilon values.

- **Open Question 3:** How resilient is the FedE4RAG framework against active malicious adversaries or colluding clients, given that the current threat model assumes only "honest-but-curious" participants?
  - **Basis in paper:** The Threat Model section explicitly states: "We do not consider active malicious adversaries or collusions." However, in a "Privacy-Preserving" system for sensitive financial data, malicious actors are a critical concern.
  - **Why unresolved:** The current defense relies on homomorphic encryption and knowledge distillation, which may not be robust against poisoning attacks (e.g., malicious clients uploading manipulated embeddings to skew the global model).
  - **What evidence would resolve it:** An analysis of the global model's performance when a fraction of clients act maliciously (e.g., label flipping or gradient ascent attacks) to determine if the aggregation method filters out such noise.

- **Open Question 4:** To what extent does the reliance on synthetic data generation (using GPT-4o mini) bias the global embedding model compared to training on raw, private data?
  - **Basis in paper:** Section V-A1 notes that "The FL process begins with synthetic data generation, where training data is synthesized from the limited local data... utilizing Llamaindex modules to automatically create... questions."
  - **Why unresolved:** If the synthetic queries generated by the LLM (teacher) do not accurately represent the distribution or complexity of real user queries in the private financial domain, the federated embedding model may learn sub-optimal representations or hallucinated patterns.
  - **What evidence would resolve it:** An ablation study comparing model performance when trained on synthetic data versus human-annotated data (even if limited), or an analysis of the semantic gap between synthetic and real user queries.

## Limitations
- The paper assumes an "honest-but-curious" server model and does not address inference-time attacks where an adversary queries the final trained model to extract information about the training data.
- The framework relies on synthetic query generation for contrastive fine-tuning, but does not validate the fidelity of these synthetic questions against real user queries.
- While the paper claims KD-GLE handles non-IID data, it does not define the threshold at which the divergence between local and global data distributions becomes too large for distillation to be effective.

## Confidence
- **High Confidence:** The core privacy mechanism (FED-HE with CKKS) is technically sound and directly supported by the equations and threat model description. The retrieval performance improvement over baselines is clearly demonstrated.
- **Medium Confidence:** The claim that KD-GLE effectively handles non-IID data is supported by ablation results but relies on the assumption that aligning similarity logits is sufficient for knowledge transfer in all scenarios.
- **Low Confidence:** The downstream generation performance is heavily dependent on the quality of the LLM used for inference, which is not standardized across all experiments in the paper.

## Next Checks
1. **Synthetic Data Quality Audit:** Generate a sample of synthetic queries using the paper's method and have human annotators rate their relevance and coherence compared to real financial queries. This validates the core assumption of the RAG-FT mechanism.
2. **Non-IID Stress Test:** Create a synthetic federated dataset where clients have deliberately disjoint topic distributions (e.g., one client has tech documents, another has healthcare). Run FedE4RAG and measure if the distillation loss still provides a benefit or causes negative transfer.
3. **Privacy Post-Training Analysis:** After the federated training completes and the model is decrypted, conduct membership inference or model inversion attacks on the final global model to assess if any private information from individual clients can be extracted.