---
ver: rpa2
title: Towards Multi-dimensional Evaluation of LLM Summarization across Domains and
  Languages
arxiv_id: '2506.00549'
source_url: https://arxiv.org/abs/2506.00549
tags:
- sentence
- summary
- chinese
- document
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MSumBench introduces a multi-dimensional, multi-domain summarization
  benchmark for English and Chinese, addressing the lack of domain-specific evaluation
  criteria and bilingual coverage in existing benchmarks. The benchmark employs a
  multi-agent debate system to enhance human annotation quality by providing structured
  arguments with contrasting perspectives, reducing bias and cognitive load.
---

# Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages

## Quick Facts
- arXiv ID: 2506.00549
- Source URL: https://arxiv.org/abs/2506.00549
- Reference count: 40
- Primary result: Introduces MSumBench, a multi-dimensional, multi-domain summarization benchmark for English and Chinese with innovative multi-agent debate system for human annotation

## Executive Summary
MSumBench introduces a comprehensive benchmark for evaluating LLM summarization models across multiple domains and languages, addressing critical gaps in existing evaluation frameworks. The benchmark employs a novel multi-agent debate system that structures human annotation by presenting contrasting arguments, reducing cognitive load and potential bias. Comprehensive evaluation of eight modern summarization models reveals distinct performance patterns across domains and languages, with proprietary LLMs outperforming open-source and non-LLM approaches. The study also uncovers systematic bias in LLM-based automated evaluation when models evaluate their own outputs.

## Method Summary
The benchmark construction process involved curating 1,000 document-summary pairs across six diverse domains (news, social media, meetings, scientific papers, law, stories) in both English and Chinese. A multi-agent debate system was implemented where multiple LLM agents generate contrasting arguments about summary quality, and human annotators select the more convincing perspective rather than making absolute judgments. The annotation process focuses on four key dimensions: factuality, consistency, fluency, and coherence. For evaluation, eight summarization models including proprietary LLMs, open-source LLMs, and non-LLM systems were tested. Automated evaluation was conducted using LLM judges, with correlation analysis performed between summarization and evaluation performance to assess evaluator reliability.

## Key Results
- Proprietary LLMs significantly outperformed open-source LLMs and non-LLM models across all domains and languages
- Strong correlation (Ï=0.71) between summarization quality and evaluation performance, but systematic bias detected in self-evaluation
- High inter-annotator agreement scores: 0.58 for fact verification and 0.79 for key-fact alignment tasks
- Distinct performance patterns observed across domains, with models showing domain-specific strengths and weaknesses
- Multi-agent debate system successfully reduced annotation cognitive load while maintaining annotation quality

## Why This Works (Mechanism)
The multi-agent debate system works by decomposing complex quality judgments into structured arguments with contrasting perspectives, allowing human annotators to make relative comparisons rather than absolute assessments. This approach leverages the LLM's ability to generate diverse, well-reasoned arguments while reducing the cognitive burden on human evaluators who only need to identify the more convincing argument. The multi-dimensional evaluation framework captures different aspects of summary quality that are often conflated in traditional evaluation, providing more granular insights into model strengths and weaknesses. The bilingual coverage ensures that findings are not limited to English-centric perspectives, while the diverse domain selection enables identification of domain-specific capabilities and limitations.

## Foundational Learning

**Multi-dimensional Evaluation Framework**
*Why needed*: Traditional evaluation often conflates different quality aspects, making it difficult to identify specific model strengths and weaknesses
*Quick check*: Ensure evaluation metrics can be applied independently and provide distinct information about summary quality

**Multi-agent Debate System**
*Why needed*: Reduces cognitive load on human annotators while maintaining or improving annotation quality through structured argumentation
*Quick check*: Verify that contrasting arguments are genuinely different and that human selection patterns are consistent

**Cross-lingual Benchmark Design**
*Why needed*: Ensures findings are not biased toward a single language and enables comparison of model performance across linguistic contexts
*Quick check*: Confirm that domain selection and evaluation criteria are appropriately adapted for each language

## Architecture Onboarding

**Component Map**: Document Collection -> Multi-agent Debate System -> Human Annotation -> Quality Score Aggregation -> Model Evaluation

**Critical Path**: The multi-agent debate system is the critical innovation that enables high-quality human annotation at scale. The system must generate sufficiently diverse and convincing arguments for each evaluation dimension, and human annotators must be able to reliably distinguish between them.

**Design Tradeoffs**: The study chose a multi-dimensional approach over single holistic scores, trading simplicity for more granular insights. The multi-agent system adds complexity but reduces individual annotation burden. Bilingual coverage increases scope but requires careful domain adaptation for each language.

**Failure Signatures**: Poor performance may result from insufficient contrast between generated arguments, leading to random human selection. Domain mismatch between training data and test samples could produce misleading performance patterns. Self-evaluation bias may inflate scores for proprietary models.

**3 First Experiments**:
1. Test the multi-agent debate system with a small subset of samples using both the debate approach and traditional single-annotator methods to compare annotation quality and consistency
2. Evaluate model performance on random subsets of the benchmark to assess stability of performance rankings
3. Conduct correlation analysis between human annotations from the debate system and traditional evaluation methods

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations

- Multi-agent debate system complexity may affect reproducibility and requires careful calibration of argument generation
- Exclusive focus on English and Chinese limits generalizability to other language pairs
- Assumption that summarization quality can be meaningfully decomposed into independent dimensions may not capture all aspects of summary quality
- Performance comparisons may be influenced by specific test sample selection and potential data contamination issues

## Confidence

**High confidence**: Benchmark construction methodology and domain selection are well-documented; correlation analysis between summarization and evaluation performance appears statistically sound; inter-annotator agreement scores are appropriately reported

**Medium confidence**: Performance comparisons between model types may be influenced by test sample selection; finding that proprietary models outperform open-source models could reflect data contamination issues not fully addressed

**Low confidence**: Claim about systematic bias in self-evaluation lacks detailed quantitative evidence of bias magnitude and nature

## Next Checks

1. Conduct cross-validation using different random subsets of the benchmark to assess stability of performance rankings across model types

2. Test the multi-agent debate system with a controlled experiment comparing annotation quality and consistency with and without the debate framework

3. Evaluate the same models on a subset of samples using traditional single-annotator methods to determine whether the debate system produces systematically different quality assessments