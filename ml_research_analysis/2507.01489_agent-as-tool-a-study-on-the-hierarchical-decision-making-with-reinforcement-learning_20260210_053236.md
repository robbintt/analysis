---
ver: rpa2
title: 'Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement
  Learning'
arxiv_id: '2507.01489'
source_url: https://arxiv.org/abs/2507.01489
tags:
- search
- tool
- reasoning
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Agent-as-Tool, a hierarchical reinforcement
  learning framework that decouples reasoning and tool usage in multi-hop question
  answering. By separating responsibilities between a Planner (for reasoning) and
  a Toolcaller (for tool invocation), the approach addresses challenges in existing
  methods that jointly handle both processes.
---

# Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.01489
- Source URL: https://arxiv.org/abs/2507.01489
- Reference count: 7
- State-of-the-art performance with 63.2% exact match and 75.2% cover exact match on Bamboogle dataset

## Executive Summary
This paper introduces Agent-as-Tool, a hierarchical reinforcement learning framework that decouples reasoning and tool usage in multi-hop question answering. The approach addresses challenges in existing methods that jointly handle both processes by separating responsibilities between a Planner (for reasoning) and a Toolcaller (for tool invocation). The framework uses GRPO to fine-tune the Planner with structured observations from the Toolcaller, improving reasoning clarity and efficiency.

## Method Summary
Agent-as-Tool employs a hierarchical architecture where a Planner agent handles complex reasoning tasks while a Toolcaller agent manages tool invocations. The Planner uses a long-context model (Qwen2.5-72B-Instruct) for deep reasoning, while the Toolcaller uses a smaller model (Qwen2.5-8B-Instruct) for efficient tool selection. The framework uses GRPO to fine-tune the Planner based on structured observations from the Toolcaller, allowing the system to learn when and how to use tools effectively without joint training of reasoning and tool usage.

## Key Results
- Achieves 63.2% exact match and 75.2% cover exact match on Bamboogle dataset
- Outperforms Search-R1 by 4.8% and 3.2% respectively
- Requires minimal fine-tuning on just 180 samples
- Demonstrates state-of-the-art performance in multi-hop question answering with search tools

## Why This Works (Mechanism)
The hierarchical separation of reasoning and tool invocation addresses the inherent complexity of multi-hop question answering by allowing each component to specialize. The Planner can focus purely on logical reasoning without being distracted by tool management decisions, while the Toolcaller optimizes tool selection based on the Planner's needs. This separation mirrors human cognitive processes where planning and execution are distinct but coordinated activities.

## Foundational Learning
- GRPO (Group Relative Policy Optimization): A reinforcement learning algorithm that optimizes policy through relative performance comparison within groups, needed for stable fine-tuning without explicit reward shaping; quick check: verify relative ranking performance vs absolute rewards
- Hierarchical RL: Decomposes complex tasks into simpler subtasks with different levels of control, needed to manage the reasoning-tool usage coupling problem; quick check: test individual component performance in isolation
- Long-context modeling: Enables processing of extended reasoning chains and tool call histories, needed for multi-hop reasoning over multiple steps; quick check: measure performance degradation with truncated contexts

## Architecture Onboarding

**Component Map:** Qwen2.5-72B-Instruct (Planner) <- GRPO <- Qwen2.5-8B-Instruct (Toolcaller) <- Bamboogle Dataset

**Critical Path:** Toolcaller receives question -> selects tool/strategy -> provides observation to Planner -> Planner reasons and generates answer -> feedback loop through GRPO training

**Design Tradeoffs:** Large model for reasoning (better quality, higher cost) vs small model for tool calling (faster, cheaper); hierarchical separation (cleaner reasoning, more complex coordination) vs joint training (simpler, but coupled optimization)

**Failure Signatures:** Toolcaller consistently selects wrong tools leading to reasoning failure; Planner generates incomplete reasoning chains; GRPO training instability due to poor reward signals

**First Experiments:**
1. Test Toolcaller accuracy on tool selection in isolation
2. Measure Planner reasoning quality with perfect tool calls
3. Validate GRPO training stability with synthetic data

## Open Questions the Paper Calls Out
None

## Limitations
- Limited experimental scope - results demonstrated only on Bamboogle dataset and specific task domain
- Small fine-tuning dataset (180 samples) may not capture real-world complexity
- Lack of extensive ablation studies on hierarchical architecture components
- Comparison methodology against Search-R1 lacks implementation detail

## Confidence

**High confidence:** Core architectural innovation - hierarchical separation of reasoning and tool invocation is well-founded and clearly implemented

**Medium confidence:** Performance claims - consistent improvements shown but limited experimental scope reduces generalizability confidence

**Medium confidence:** Efficiency claims - minimal fine-tuning demonstrated but comprehensive resource usage metrics not reported

## Next Checks

1. Replicate experiments across multiple multi-hop QA datasets (HotpotQA, QAngaroo) to assess generalizability beyond Bamboogle domain

2. Conduct detailed ablation studies comparing flat vs hierarchical architectures, different reward structures in GRPO, and varying fine-tuning dataset sizes

3. Implement controlled runtime and resource usage analysis comparing Agent-as-Tool against baselines across different sequence lengths and tool calling frequencies