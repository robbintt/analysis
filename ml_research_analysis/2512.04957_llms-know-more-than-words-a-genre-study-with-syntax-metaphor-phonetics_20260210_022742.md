---
ver: rpa2
title: 'LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics'
arxiv_id: '2512.04957'
source_url: https://arxiv.org/abs/2512.04957
tags:
- novel
- poetry
- drama
- linguistic
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models can capture\
  \ deeper linguistic properties\u2014such as syntax, metaphor, and phonetic patterns\u2014\
  by introducing a multilingual genre classification dataset from Project Gutenberg\
  \ in six languages. The authors augment sentences with three explicit linguistic\
  \ features: syntactic tree depth, metaphor counts, and metre patterns, then fine-tune\
  \ BERT-based models to classify pairs of literary genres (poetry vs."
---

# LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics

## Quick Facts
- arXiv ID: 2512.04957
- Source URL: https://arxiv.org/abs/2512.04957
- Reference count: 40
- Key outcome: BERT-based models improve genre classification by incorporating explicit linguistic features, with metre patterns yielding the most consistent gains across tasks and languages.

## Executive Summary
This paper investigates whether large language models can capture deeper linguistic properties—such as syntax, metaphor, and phonetic patterns—by introducing a multilingual genre classification dataset from Project Gutenberg in six languages. The authors augment sentences with three explicit linguistic features: syntactic tree depth, metaphor counts, and metre patterns, then fine-tune BERT-based models to classify pairs of literary genres (poetry vs. novel, drama vs. poetry, drama vs. novel). Results show that models improve when linguistic features are added, with metre patterns yielding the most consistent gains across tasks and models, and syntax/metaphor features providing smaller, task-specific benefits. The findings demonstrate that LLMs can effectively learn and leverage latent linguistic structures for genre classification, underscoring the value of incorporating explicit linguistic cues during model training.

## Method Summary
The study uses sentences sampled from Project Gutenberg (1,500–3,000 per genre per language) across six languages. Three linguistic features are extracted: syntax (tree depth via spaCy-Benepar), metaphor (count via Metaphor-RoBERTa), and phonetics (binary stress patterns via Poetry Tools). These features are concatenated with text embeddings and used to fine-tune BERT, DistilBERT, RoBERTa, and Metaphor-RoBERTa models for binary genre classification. The objective is maximizing F1 score, with results showing consistent improvements when metre patterns are included, particularly for distinguishing poetry from other genres.

## Key Results
- Metre patterns consistently improve F1 scores across most models and tasks, especially for poetry classification.
- Syntax features improve classification only when genres exhibit structural divergence (e.g., Poetry vs. Novel), but not for Novel vs. Drama.
- Metaphor-aware pretraining (Metaphor-RoBERTa) achieves highest average F1 scores, suggesting transfer from metaphor detection to genre classification.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing explicit phonetic stress patterns (metre) as features aids BERT-based models in distinguishing poetic from prose genres, particularly when rhythmic regularity is a defining characteristic.
- **Mechanism:** The model receives a binary vector representing stressed/unstressed syllables. This explicit signal bypasses the difficulty of inferring acoustic properties from raw text tokens, allowing the classifier to directly map rhythmic regularity to the "Poetry" label in the latent space.
- **Core assumption:** The prosodic tools (Poetry Tools) accurately capture metrical feet in the target languages, and these patterns are discriminative features for the genres in the dataset.
- **Evidence anchors:**
  - [abstract]: "Experiments show that incorporating phonetic features consistently improves performance across most models."
  - [results_analysis]: "Metre Pattern... delivers steady F1 score gains in both the Poetry vs. Novel and Poetry vs. Drama tasks... underscoring the importance of prosodic and rhythmic cues."
  - [corpus]: The corpus neighbors (e.g., "Looking for the Inner Music") suggest LLMs can detect literary style, but this paper provides evidence that *explicit* rhythm signals boost this capability.
- **Break condition:** Performance gains vanish if the "Poetry" in the dataset is free verse (lacking strict metre) or if the "Novel" text contains rhythmic prose, reducing the linear separability of the metre vectors.

### Mechanism 2
- **Claim:** Syntactic tree features improve classification only when genres exhibit significant structural divergence (e.g., Poetry vs. Novel), but fail when structural overlap is high (e.g., Novel vs. Drama).
- **Mechanism:** The model uses tree depth and depth-to-length ratio as scalar proxies for syntactic complexity. If one genre (Poetry) consistently uses shorter or less nested structures than another (Novel), these features provide a strong linear separation signal.
- **Core assumption:** The parsers (spaCy-Benepar) generate consistent tree depth metrics across six languages, and literary genres conform to stereotypical complexity patterns (e.g., Drama is dialogue-heavy, resembling Narrative prose in complexity).
- **Evidence anchors:**
  - [results_analysis]: "English poetry and novel texts are... linearly separable [by syntax]... Novel vs. Drama... exhibits significant overlap."
  - [results_analysis]: "French poetry and novels are not easily separable based on syntactic tree information... explaining the limited improvement."
  - [corpus]: [Weak/Missing] No direct corpus evidence contradicts this, but general NLP theory confirms that dialogue (Drama) and narrative (Novel) often share similar syntactic depth distributions.
- **Break condition:** The mechanism fails in languages or datasets where syntactic complexity does not correlate with genre (e.g., complex modernist poetry vs. simple pulp fiction), rendering the scalar depth features noisy.

### Mechanism 3
- **Claim:** Pre-training on specific linguistic tasks (Metaphor detection) transfers effectively to genre classification, reducing the marginal utility of adding explicit metaphor counts at inference time.
- **Mechanism:** Metaphor-RoBERTa possesses prior weights tuned to recognize figurative language. Since metaphor density is a stylistic marker of genres (like Novels in this study), the model's internal representations are already partially aligned with the classification goal, making the explicit scalar count redundant or less informative than the pre-trained attention patterns.
- **Core assumption:** Metaphor density is a distinguishing feature (Novels > Drama/Poetry in this data), and the pre-training task is sufficiently similar to the downstream genre task to allow transfer.
- **Evidence anchors:**
  - [results_analysis]: "Metaphor-RoBERTa... achieves the highest average F1 scores... suggesting that metaphor-aware pretraining may help capture stylistic distinctions."
  - [results_analysis]: "Metaphor-based fine-tuning occasionally lessening the positive impact of added linguistic features, possibly due to feature redundancy."
  - [corpus]: "Metaphor and Large Language Models" (Corpus ID 46252) notes that LLMs often rely on surface features for metaphor; this mechanism suggests those surface features are already captured by Metaphor-RoBERTa.
- **Break condition:** If the test set contains genres defined by non-figurative language or if the metaphor detection pre-training was biased towards different domains, the transfer would degrade, and explicit features might become necessary.

## Foundational Learning

- **Concept:** **Syntactic Tree Depth vs. Complexity**
  - **Why needed here:** The paper uses "depth" and "depth-to-length ratio" as proxies for genre. An engineer must understand that these are crude approximations of "complexity" to diagnose why they work for Poetry (often short, punchy) but fail for Drama (dialogue) vs. Novels.
  - **Quick check question:** Can you explain why a dialogue-heavy play (Drama) might have a similar syntactic tree depth to a descriptive Novel, causing the model to confuse them?

- **Concept:** **Prosody and Metre (Stress Patterns)**
  - **Why needed here:** The paper encodes text as binary stress vectors. Understanding that "0" represents unstressed and "1" represents stressed syllables is crucial for debugging the feature extraction pipeline (Poetry Tools) and understanding the "Metre Pattern" results.
  - **Quick check question:** How would you represent the metre pattern for a sentence that is known to be free verse versus a strict iambic pentameter line?

- **Concept:** **Input Concatenation vs. Embedding Augmentation**
  - **Why needed here:** The paper (Eq. 2) discusses $I = S \oplus F$. Understanding how to fuse discrete text tokens with continuous or categorical linguistic features (syntax/metaphor scores) is critical for implementing the architecture correctly.
  - **Quick check question:** If you concatenate a scalar "metaphor count" to a sequence of token embeddings, do you add it as a new token, or broadcast it across all token positions?

## Architecture Onboarding

- **Component map:**
  - Data Source: Project Gutenberg (Epubs/Text) -> Sentence Splitter
  - Feature Extractors (Parallel):
    1. spaCy-Benepar -> Syntax Tree -> Depth/Ratio Scalars
    2. Metaphor-RoBERTa -> Metaphor Count Scalar
    3. Poetry Tools -> Binary Stress Vector (Metre)
  - Fusion Layer: Concatenation of Text Embeddings + Feature Vectors
  - Backbone: BERT / DistilBERT / RoBERTa / Metaphor-RoBERTa
  - Head: Linear Classification Layer (Binary)

- **Critical path:** The **Metre Pattern extraction** is the most sensitive component. It relies on a Python library (Poetry Tools) that must correctly handle syllabification and stress assignment for 6 languages. An error here results in random noise vectors, degrading the most consistent performance booster found in the study.

- **Design tradeoffs:**
  - **Explicit vs. Implicit:** The authors trade model purity (end-to-end learning) for interpretability and control by manually engineering syntax/metaphor features.
  - **Heuristic Tools:** The system relies on external tools (e.g., Metaphor RoBERTa) to generate "Ground Truth" features. If the tool is 80% accurate, the training data contains 20% noisy labels for features.
  - **Binary Stress Vector Size:** The paper pads vectors to uniform length. Excessive padding might dilute the signal; dynamic padding or positional encoding might be alternatives.

- **Failure signatures:**
  - **High Variance in French/Spanish:** If the parser fails to capture language-specific syntactic nuances (e.g., complex clauses in French poetry), performance drops (as seen in the French results).
  - **Novel-Drama Collapse:** If the model achieves ~50% accuracy on Novel vs. Drama, it indicates the features (Syntax, Metre, Metaphor) are statistically identical for these two classes, and the model is guessing randomly.

- **First 3 experiments:**
  1. **Sanity Check (English):** Run the baseline vs. Metre-augmented model on "Poetry vs. Novel" in English. Verify the ~3-7% F1 lift reported in Table 3 to validate the feature pipeline.
  2. **Ablation (Syntax):** Test "Novel vs. Drama" with *only* syntax features. Confirm that the signal is indeed low/zero (as predicted by the overlap analysis) to ensure the model isn't overfitting to spurious correlations.
  3. **Cross-Lingual Stress:** Feed French poetry into the Metre extractor. Visually inspect the binary stress vectors to ensure they aren't all-zeros (indicating the tool failed on that language) before trusting the metrics.

## Open Questions the Paper Calls Out

- **Question:** Would linguistically enriched genre classification transfer to contemporary, non-Western, or marginalized literary traditions, or do the observed feature contributions generalize only to the canonical Western corpus represented in Project Gutenberg?
  - **Basis in paper:** Limitation section states the dataset "overrepresent[s] canonical literature from specific historical periods and underrepresent contemporary, non-Western, or marginalized voices, which may limit the generalizability of our findings."
  - **Why unresolved:** The entire experimental validation is confined to public-domain Western literature, leaving unknown whether syntactic, metaphor, or phonetic features hold similar discriminative power across diverse literary traditions.
  - **What evidence would resolve it:** Experiments replicating the methodology on corpora from non-Western literary traditions (e.g., African, Asian, Indigenous literatures) and contemporary texts.

- **Question:** Can neural or hybrid feature extraction methods for metaphor and phonetic patterns improve cross-linguistic robustness and better capture culturally specific expressions compared to the current heuristic-based tools?
  - **Basis in paper:** Limitation section notes "the extraction of linguistic features such as metaphor and phonetic regularity relies on heuristic or proxy-based methods, which may miss nuanced or culturally specific expressions." Conclusion calls for "refin[ing] feature extraction with neural or hybrid methods."
  - **Why unresolved:** Metaphor-RoBERTa and PoetryTools were trained primarily on English data; their transfer to other languages is uncharacterized, and the paper does not quantify extraction errors across languages.
  - **What evidence would resolve it:** Systematic comparison of heuristic vs. neural feature extractors with human-annotated ground truth across all six languages, measuring both extraction accuracy and downstream classification gains.

- **Question:** Are there linguistic features beyond syntax, metaphor, and metre that could better distinguish novel from drama, given that all three tested features failed to improve this particular classification task?
  - **Basis in paper:** Results show "for the Novel vs. Drama task, all training methods fail to outperform the baseline," attributed to "overlapping narrative structures and lexical similarities."
  - **Why unresolved:** The paper concludes these genres share prose conventions but does not explore alternative discriminative features such as dialogue density, stage direction markers, narrative voice, or discourse structure.
  - **What evidence would resolve it:** Testing additional linguistically motivated features (e.g., speech act distributions, pronominal patterns, discourse connectives) on the Novel vs. Drama task to identify signals that capture the functional differences between narrative and dramatic prose.

## Limitations
- The study relies on heuristic-based feature extraction tools that may miss culturally specific expressions and lack cross-linguistic robustness.
- The Gutenberg corpus overrepresents canonical Western literature, limiting generalizability to contemporary or non-Western literary traditions.
- The feature integration mechanism is ambiguously described, making exact reproduction challenging.

## Confidence
- **High Confidence:** The claim that metre patterns consistently improve poetry classification (Mechanism 1).
- **Medium Confidence:** The claim that syntactic features work only when genres differ structurally (Mechanism 2).
- **Medium Confidence:** The claim that metaphor-aware pretraining reduces the marginal utility of explicit metaphor features (Mechanism 3).

## Next Checks
1. **Feature Integration Validation:** Implement the baseline model with text-only input and the augmented model with explicit feature integration. Measure the exact F1 difference on the English "Poetry vs. Novel" task to verify the reported 3-7% improvement and confirm the integration mechanism works as intended.

2. **Parser Reliability Test:** Run the syntax parser and metre extraction tools on a small, manually verified sample of sentences from each language. Count parser failures, incorrect stress assignments, and edge cases to quantify potential noise in the feature vectors.

3. **Cross-Domain Generalization:** Test the fine-tuned models on a small sample of contemporary texts from each genre (e.g., modern poetry anthologies, current bestsellers, recent plays). Compare performance drop to the Gutenberg-trained models to assess domain generalization limits.