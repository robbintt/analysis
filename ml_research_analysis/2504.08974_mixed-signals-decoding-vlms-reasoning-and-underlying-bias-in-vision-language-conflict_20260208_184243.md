---
ver: rpa2
title: 'Mixed Signals: Decoding VLMs'' Reasoning and Underlying Bias in Vision-Language
  Conflict'
arxiv_id: '2504.08974'
source_url: https://arxiv.org/abs/2504.08974
tags:
- image
- bias
- text
- task
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how vision-language models (VLMs) reason
  over multimodal inputs when confronted with conflicting image and text cues. The
  authors construct five datasets from existing benchmarks (VSR and Isobench) by introducing
  mismatches between image and text pairs across tasks in mathematics, science, and
  visual descriptions.
---

# Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict

## Quick Facts
- **arXiv ID**: 2504.08974
- **Source URL**: https://arxiv.org/abs/2504.08974
- **Reference count**: 15
- **Primary result**: VLMs show systematic modality bias (ranging from +56.8% image-favored to -74.4% text-favored) when image and text inputs conflict, with bias shifting from text to image preference as task complexity increases.

## Executive Summary
This paper investigates how vision-language models (VLMs) reason over multimodal inputs when confronted with conflicting image and text cues. The authors construct five datasets from existing benchmarks by introducing mismatches between image and text pairs across tasks in mathematics, science, and visual descriptions. They find that VLMs generally favor text in simpler tasks but shift toward images as task complexity increases. To mitigate this bias, they explore three strategies: simple prompt modifications, chain-of-thought-style instructions, and a task decomposition approach. Results show these mitigation strategies can effectively detect mismatches when models perform well on the original task, though effectiveness varies by model and task.

## Method Summary
The study constructs five datasets from VSR and Isobench benchmarks by introducing controlled mismatches between image and text pairs. For each task, they measure modality bias as B = (image-favored % - text-favored %) using only correctly-answered aligned samples. Three mitigation strategies are evaluated: (1) Verbalized - direct prompt asking to identify mismatches, (2) CoT - chain-of-thought approach processing image→text→compare, and (3) Decomposed - three separate inference calls (image-only, text-only, combined). Five VLMs are tested across all tasks, with performance measured via accuracy and F1 score on mismatch detection.

## Key Results
- VLMs show systematic modality bias when inputs conflict, with bias values ranging from +56.8% (image favored) to -74.4% (text favored)
- Bias shifts from text-favoring to image-favoring as task complexity increases (e.g., degree 1 polynomials show text bias, degree 3+ show image bias)
- Mitigation strategies effectively detect mismatches when models achieve reasonable accuracy on original tasks
- Decomposed mitigation performs best but requires high unimodal performance, while Verbalized is cheapest but least effective
- Persistent single-modality errors reveal blind bias and cross-modal reasoning failures

## Why This Works (Mechanism)

### Mechanism 1: Perceived Difficulty Drives Modality Selection
- Claim: VLMs bias toward whichever modality they internally assess as easier for solving the specific task
- Core assumption: The model's internal difficulty estimation is measurable through its actual modality preference during conflict
- Evidence anchors: [abstract] VLMs favor text in simpler queries but shift toward images as query complexity increases; [section 4.3] stronger models show alignment between perceived ease and exhibited bias
- Break condition: When model's task performance is poor on both modalities, the perceived-difficulty signal becomes unreliable and bias becomes more random

### Mechanism 2: Task Complexity Triggers Modality Shift
- Claim: As task complexity increases, VLMs shift from text-favored to image-favored responses
- Core assumption: Complexity proxies (edge count, polynomial degree, character count) accurately reflect actual cognitive load on the model
- Evidence anchors: [abstract] Bias values ranging from +56.8% (image favored) to -74.4% (text favored); [section 4.2.1] models exhibit strong text bias for degree 1 polynomials but this diminishes for degree 2+
- Break condition: When visual representations themselves are complex or ambiguous (e.g., cluttered graphs), the shift may not occur or may produce high error rates

### Mechanism 3: Persistent Single-Modality Errors Reveal Integration Failure
- Claim: Errors unique to one modality that persist when both modalities are present indicate a "blind bias" toward that modality and failed cross-modal reasoning
- Core assumption: Error analysis across text-only, image-only, and combined conditions reveals the integration mechanism
- Evidence anchors: [section 4.4] errors unique to a single modality that persist even when both text and image inputs are provided reveal a blind bias toward that modality
- Break condition: When both modalities independently produce low error rates, the persistence analysis becomes less diagnostic

## Foundational Learning

- **Modality Bias in Multimodal Models**
  - Why needed here: The entire paper measures and mitigates this phenomenon. Without understanding that VLMs systematically prefer one modality over another, the mitigation strategies make no sense
  - Quick check question: Given conflicting image and text inputs, which modality would a VLM likely trust for: (a) a simple arithmetic question, (b) determining if nodes are connected in a dense graph?

- **Chain-of-Thought Prompting**
  - Why needed here: One mitigation strategy ("CoT Mitigation") explicitly builds on this technique, requiring sequential processing of each modality
  - Quick check question: How does requiring a model to "show its work" affect its behavior when presented with contradictory evidence?

- **Task Decomposition for Multimodal Inputs**
  - Why needed here: The "Decomposed Mitigation" strategy runs the model three times (image-only, text-only, combined). Understanding why separate runs differ from combined processing is essential
  - Quick check question: Why might processing image and text separately before combining yield different results than processing both simultaneously?

## Architecture Onboarding

- **Component map**:
  Dataset Construction -> Bias Measurement -> Mitigation Application -> Error Analysis

- **Critical path**:
  1. Dataset construction with controlled mismatches
  2. Baseline bias measurement on aligned vs. mismatched pairs
  3. Apply mitigation strategies
  4. Measure mismatch detection accuracy (not task accuracy)

- **Design tradeoffs**:
  - **Verbalized vs. CoT vs. Decomposed**: Verbalized is cheapest (1 inference call) but least effective; Decomposed is most expensive (3 calls) but most robust when model performs well on individual modalities
  - **Complexity proxy accuracy**: Character count, edge count, and polynomial degree are imperfect proxies—noise in these affects bias-complexity correlation
  - **Model capability threshold**: Mitigation only works if the model achieves reasonable accuracy on the original task (Table 1 shows this clearly)

- **Failure signatures**:
  - Mitigation accuracy near 50% with near-zero F1: Model is guessing randomly, not detecting mismatches
  - High persistent single-modality errors in Venn diagrams: Blind bias, cross-modal reasoning failure
  - Inconsistent bias direction across similar tasks: Suggests model lacks robust internal difficulty estimation

- **First 3 experiments**:
  1. **Establish baseline bias**: Run your VLM on the five datasets with mismatched pairs, compute B values. Verify the complexity-shift pattern holds for your model
  2. **Ablate mitigation strategies**: Implement all three strategies (Verbalized, CoT, Decomposed) and measure mismatch detection accuracy. Identify which works best for your model-task combination
  3. **Error persistence analysis**: For your worst-performing task, run text-only, image-only, and combined conditions. Build the Venn diagram to identify whether blind bias is the culprit

## Open Questions the Paper Calls Out

- **What architectural or training mechanisms drive the shift from text-favoring to image-favoring bias as task complexity increases?**
  - Basis: The authors state "it is not clear how these models reason over the visual and textual data together, nor how the flow of information between modalities is structured"
  - Why unresolved: The paper establishes the correlation between complexity and bias shift but does not identify the underlying computational mechanisms
  - Resolution needed: Probing studies analyzing internal activations during multimodal reasoning, or ablation studies targeting cross-modal attention mechanisms

- **How can mitigation strategies be adapted to work effectively for models with lower baseline task performance?**
  - Basis: The limitations section states mitigation strategies "may not be universally applicable across all models and tasks"
  - Why unresolved: Current strategies depend on strong unimodal capabilities, leaving weaker models without effective bias mitigation options
  - Resolution needed: Development and testing of alternative mitigation approaches specifically designed for lower-performing models

- **How well do these bias patterns generalize to real-world multimodal conflict scenarios beyond the five benchmark tasks studied?**
  - Basis: The authors acknowledge their benchmarks "are still limited in terms of the diversity and complexity of tasks that can arise in practical applications"
  - Why unresolved: The constructed benchmarks may not capture domain-specific conflict patterns in healthcare, legal, or other high-stakes applications
  - Resolution needed: Evaluation on additional benchmarks spanning diverse real-world domains with naturally occurring modality conflicts

## Limitations

- The complexity proxies used (character count, edge count, polynomial degree) are imperfect measures of actual cognitive load on VLMs, potentially weakening the claimed relationship between task complexity and modality preference
- The study assumes models perform reliable difficulty estimation per modality, but this internal mechanism is not directly validated - we observe correlation rather than causation
- Mitigation effectiveness depends heavily on baseline task performance, yet the threshold for "good enough" performance is not rigorously defined

## Confidence

- **High Confidence**: The basic bias measurement methodology and the finding that VLMs show systematic modality preferences when inputs conflict
- **Medium Confidence**: The mechanism that perceived difficulty drives modality selection - while the correlation is observed, the causal link requires more direct validation
- **Medium Confidence**: The three mitigation strategies work when models perform well on original tasks, though effectiveness varies significantly by model and task

## Next Checks

1. Validate the complexity proxy accuracy by correlating them with actual model attention patterns or intermediate representations, rather than just final bias measurements
2. Test mitigation strategies on a broader range of VLMs and tasks, particularly focusing on models with different architectural approaches to cross-modal reasoning
3. Conduct ablation studies removing the difficulty estimation assumption to determine if bias patterns persist through alternative mechanisms