---
ver: rpa2
title: 'QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed
  Image Retrieval'
arxiv_id: '2507.12416'
source_url: https://arxiv.org/abs/2507.12416
tags:
- image
- hard
- negative
- images
- qure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes QuRe, a method for composed image retrieval
  that addresses the problem of false negatives in contrastive learning by introducing
  a novel hard negative sampling strategy. The key idea is to periodically rank the
  corpus by relevance scores and select hard negatives positioned between two steep
  drops in relevance scores following the target image.
---

# QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval

## Quick Facts
- arXiv ID: 2507.12416
- Source URL: https://arxiv.org/abs/2507.12416
- Reference count: 25
- Primary result: State-of-the-art performance on FashionIQ and CIRR datasets with 1.09-2.16% recall improvement on FashionIQ and 1.47-1.95% on CIRR

## Executive Summary
QuRe introduces a novel hard negative sampling strategy for composed image retrieval that addresses the false negative problem in contrastive learning. The method periodically ranks corpus images by relevance scores and selects hard negatives positioned between two steep drops in relevance scores following the target image. This approach ensures hard negatives are both semantically similar to the query and less relevant than the target. QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets while demonstrating superior alignment with human preferences on the newly introduced HP-FashionIQ dataset.

## Method Summary
QuRe fine-tunes BLIP-2 for composed image retrieval by optimizing a Bradley-Terry preference model instead of standard contrastive loss. During training, it periodically ranks the entire corpus by relevance scores and identifies hard negatives positioned between the two largest score drops after the target image. The model uses these carefully selected negatives to learn fine-grained distinctions between the target and challenging non-target images. A warm-up phase uses all non-target images as negatives initially, then contracts the hard negative set over training epochs following a curriculum-like progression.

## Key Results
- Achieves state-of-the-art performance on FashionIQ dataset with 1.09-2.16% recall improvement
- Improves performance on CIRR dataset with 1.47-1.95% recall gains
- Demonstrates superior human preference alignment with 74.55% preference rate on HP-FashionIQ dataset
- Shows effective zero-shot transfer capability on CIRCO dataset

## Why This Works (Mechanism)

### Mechanism 1: False Negative Filtering via Relevance Score Drop Detection
QuRe identifies hard negatives by detecting steep drops in relevance scores after the target image position. This boundary detection exploits the observation that sharp score degradations indicate semantic shifts, helping exclude false negatives (visually similar to target) while retaining challenging negatives. The method selects images positioned between the two largest score drops following the target.

### Mechanism 2: Reward Model Objective Replaces Contrastive Loss
Instead of standard contrastive loss, QuRe optimizes a Bradley-Terry preference model that ranks the positive target above a single hard negative. This pairwise formulation reduces false negative penalties by focusing on carefully selected negatives rather than pushing apart all non-target images indiscriminately.

### Mechanism 3: Curriculum-like Hard Negative Set Contraction
The method dynamically shrinks hard negative sets across training epochs, creating a curriculum effect that accelerates convergence. Early epochs use broader negative sets, while later epochs employ tighter, more challenging selections as the model becomes more confident.

## Foundational Learning

- **Contrastive Learning Basics (InfoNCE-style)**
  - Why needed: QuRe explicitly departs from standard contrastive loss; understanding what it replaces clarifies the false negative problem
  - Quick check: In a batch of N images, how many negative pairs does standard contrastive learning create per anchor?

- **Bradley-Terry Preference Model**
  - Why needed: The training objective is derived from this probabilistic ranking framework
  - Quick check: What does σ(s(p) − s(n)) represent in the Bradley-Terry formulation?

- **BLIP-2 Architecture (Q-Former, Vision Encoder)**
  - Why needed: QuRe fine-tunes BLIP-2; relevance scores depend on the Q-Former fusion mechanism
  - Quick check: How does the Q-Former bridge image and text modalities in BLIP-2?

## Architecture Onboarding

- **Component map:** BLIP-2 backbone (frozen image encoder + trainable Q-Former) -> Relevance scoring module -> Hard negative selector -> Preference loss
- **Critical path:** Warm-up phase uses all non-target images as negatives; periodic hard negative set definition using current model's relevance scores; per-batch sampling of one hard negative per query; backpropagation through preference loss
- **Design tradeoffs:** ndef frequency (more frequent = fresher negatives but higher computation); hard negative set size (dynamically determined by drop detection); single negative per query (lower variance vs. multiple negatives tradeoff)
- **Failure signatures:** Smooth relevance score curves (no clear drop points); high Recalls@K but low Recall@K on CIRR (false negatives ranked above target); preference rate drops (false negatives included in hard negatives)
- **First 3 experiments:** 1) Reproduce ablation comparing "All corpus" vs. "Top-k" vs. drop-based selection on FashionIQ validation; 2) Hyperparameter sweep on ndef values {3, 6, 10}; 3) Zero-shot transfer to CIRCO dataset

## Open Questions the Paper Calls Out

- **Open Question 1:** Would treating identified high-relevance non-target images as auxiliary positives in the reward model objective further improve human preference alignment compared to merely excluding them from the negative set?
- **Open Question 2:** Is the "steep drop" heuristic for hard negative sampling robust across visual domains with continuous semantic transitions, or is it specific to the discrete attribute changes found in fashion datasets?
- **Open Question 3:** How does the computational cost of sorting the entire corpus to define hard negative sets scale to web-scale image retrieval databases?

## Limitations
- The steep drop detection heuristic is not theoretically proven and could fail with smooth score distributions
- No explicit control over hard negative set size leaves it entirely dependent on the boundary detection heuristic
- Computational overhead of periodic corpus ranking is not quantified and could be prohibitive for large-scale applications

## Confidence

**High Confidence:** The core mechanism of using relevance score drops to identify hard negatives is well-defined and experimental results on FashionIQ and CIRR are clearly presented.

**Medium Confidence:** The claim that this approach reduces false negatives is supported by preference rate results but lacks direct quantitative comparison of false negative counts.

**Low Confidence:** The assertion that the method generalizes to other domains is based on a single zero-shot experiment without extensive ablation studies.

## Next Checks

1. Implement a systematic comparison that counts and visualizes false negatives in top-K rankings for QuRe vs. standard contrastive learning on FashionIQ validation set.
2. Create synthetic relevance score distributions with varying degrees of smoothness and measure hard negative set quality and model performance.
3. Implement QuRe with 1, 3, and 5 hard negatives per query and measure the tradeoff between training stability, convergence speed, and final recall.