---
ver: rpa2
title: Representation Calibration and Uncertainty Guidance for Class-Incremental Learning
  based on Vision Language Model
arxiv_id: '2512.09441'
source_url: https://arxiv.org/abs/2512.09441
tags:
- learning
- image
- task
- class
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of catastrophic forgetting in class-incremental
  learning for image classification. The authors propose a two-stage framework based
  on vision-language models.
---

# Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model

## Quick Facts
- arXiv ID: 2512.09441
- Source URL: https://arxiv.org/abs/2512.09441
- Reference count: 40
- Primary result: Proposed method achieves up to 7% improvement in accuracy over strong baselines on CIFAR100, ImageNet-R, Cars196, and Skin40.

## Executive Summary
This paper introduces a two-stage framework for class-incremental learning using vision-language models to mitigate catastrophic forgetting. The approach leverages frozen CLIP image encoders with task-specific adapters in Stage-I and employs a Mixture of Projectors module in Stage-II to calibrate features across tasks. A novel inference strategy uses prediction uncertainty to select the most appropriate calibrated feature for classification. Extensive experiments demonstrate state-of-the-art performance with robust generalization across multiple datasets and CLIP variants.

## Method Summary
The proposed method consists of two stages: Stage-I involves training task-specific adapters on a frozen CLIP image encoder to learn new classes incrementally. Stage-II employs a Mixture of Projectors (MoP) module that calibrates features from different tasks into a unified representation space, improving inter-task class separation. At inference, the system calculates prediction uncertainty (entropy) for each calibrated feature and selects the feature with the lowest uncertainty for final classification, effectively leveraging CLIP's semantic understanding to guide decisions.

## Key Results
- Achieves up to 7% improvement in accuracy over strong baselines on CIFAR100, ImageNet-R, Cars196, and Skin40.
- Demonstrates robustness to hyper-parameter choices across different datasets.
- Shows generalization capability across various pre-trained CLIP weights.

## Why This Works (Mechanism)
The method addresses catastrophic forgetting by maintaining a frozen CLIP backbone while learning task-specific adapters, preserving previously learned knowledge. The Mixture of Projectors module calibrates features across tasks into a unified space, maintaining discriminative power for both old and new classes. The uncertainty-guided inference mechanism leverages CLIP's semantic understanding to select the most reliable feature representation at test time, improving classification accuracy in incremental learning scenarios.

## Foundational Learning
- **Class-Incremental Learning**: Learning new classes without forgetting previous ones; needed to handle real-world scenarios where data arrives sequentially; quick check: performance on tasks with increasing class numbers.
- **Catastrophic Forgetting**: The tendency of neural networks to forget previously learned information when trained on new tasks; central challenge addressed by the method; quick check: comparison with non-incremental baselines.
- **Vision-Language Models**: Models like CLIP that learn joint visual and textual representations; provides strong semantic understanding for guidance; quick check: ablation with different CLIP variants.
- **Mixture of Projectors**: A module that calibrates features from different tasks into a unified space; enables better separation between old and new classes; quick check: performance with and without MoP.
- **Uncertainty Quantification**: Using entropy or similar metrics to measure prediction confidence; guides inference by selecting the most reliable feature representation; quick check: impact of uncertainty threshold tuning.

## Architecture Onboarding
**Component Map**: CLIP Encoder -> Task Adapters (Stage-I) -> Mixture of Projectors (Stage-II) -> Uncertainty-based Selector -> Final Prediction
**Critical Path**: Input image → CLIP frozen encoder → Task-specific adapters → MoP calibration → Uncertainty calculation → Feature selection → Classification
**Design Tradeoffs**: Freezing CLIP encoder preserves knowledge but limits adaptation; task-specific adapters allow flexibility but increase parameters; MoP adds complexity but improves cross-task separation; uncertainty guidance improves accuracy but adds inference overhead
**Failure Signatures**: Performance degradation when intra-class variability is high; reduced effectiveness with CLIP variants having poor semantic understanding; increased computational cost at inference
**First Experiments**: 1) Ablation study removing uncertainty guidance to measure its contribution, 2) Evaluation with different CLIP backbones to verify semantic understanding dependency, 3) Testing on datasets with higher intra-class variability to assess robustness

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Evaluation restricted to datasets with limited intra-class variability, not reflecting real-world high-variance visual data complexity
- Reliance on CLIP's semantic understanding not verified across diverse domains
- Added complexity from Mixture of Projectors not explored for scalability and training efficiency

## Confidence
- State-of-the-art performance claims: High
- Robustness to hyper-parameters: High
- Generalization to high intra-class variability datasets: Medium
- Scalability and computational efficiency: Low

## Next Checks
1. Test the method on datasets with higher intra-class variability and larger scale (e.g., iNaturalist, OpenImages)
2. Evaluate computational overhead and inference latency introduced by the uncertainty-guided decision mechanism
3. Verify performance when using CLIP variants with different semantic understanding capabilities or in domains outside standard image classification