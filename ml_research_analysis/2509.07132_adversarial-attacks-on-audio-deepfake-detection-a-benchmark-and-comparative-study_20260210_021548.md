---
ver: rpa2
title: 'Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative
  Study'
arxiv_id: '2509.07132'
source_url: https://arxiv.org/abs/2509.07132
tags:
- attacks
- methods
- audio
- adds
- deepfake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study provides the first comprehensive, large-scale comparative\
  \ analysis of audio deepfake detection (ADD) methods under adversarial anti-forensic\
  \ (AF) attacks, spanning both raw audio and spectrogram-based approaches. The work\
  \ evaluates twelve state-of-the-art ADD models across five benchmark datasets against\
  \ twelve diverse AF attacks\u2014four statistical (pitch shifting, filtering, noise\
  \ addition, quantization) and eight optimization-based (FGSM, PGD, C&W, DeepFool)."
---

# Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study

## Quick Facts
- **arXiv ID:** 2509.07132
- **Source URL:** https://arxiv.org/abs/2509.07132
- **Reference count:** 40
- **Primary result:** First comprehensive comparative analysis of 12 state-of-the-art ADD methods across 5 benchmark datasets under 12 adversarial attacks (4 statistical + 8 optimization)

## Executive Summary
This study provides the first large-scale comparative analysis of audio deepfake detection (ADD) methods under adversarial anti-forensic (AF) attacks. The researchers evaluated twelve state-of-the-art ADD models spanning both raw audio and spectrogram-based approaches against twelve diverse AF attacks. The experiments demonstrate that both ADD categories suffer significant performance degradation under AF attacks, with raw models achieving 0.75 AUC/0.30 EER and spectrogram models 0.86 AUC/0.19 EER on clean data, but dropping substantially under adversarial conditions. While adversarial training offers some resilience, performance still lags behind baseline levels, highlighting the urgent need for more robust, generalizable ADD systems capable of countering evolving AF techniques.

## Method Summary
The researchers selected twelve state-of-the-art ADD models (six raw and six spectrogram-based) and evaluated them across five benchmark datasets: ASVSpoof2019, ASVSpoof2021, ASVSpoof2024, CodecFake, and WaveFake. They applied twelve AF attacks divided into statistical methods (pitch shifting, filtering, noise addition, quantization) and optimization-based methods (FGSM, PGD, C&W, DeepFool). Models were trained on combined datasets D1, D2, and D4 for 50 epochs using Adam optimizer with learning rate 0.0001 and batch size 256. Performance was measured using AUC and EER metrics across all datasets, including unseen test sets to evaluate generalization.

## Key Results
- Baseline performance shows spectrogram models (AUC 0.86) outperform raw models (AUC 0.75) on clean data
- Optimization attacks cause severe degradation, with PGD reducing average AUC to 0.09 (raw) and 0.35 (spectrogram)
- Statistical attacks achieve lower perceptual distortion (SSIM 0.82-0.99) than optimization attacks (SSIM >0.95) but cause moderate performance degradation
- Adversarial training improves robustness but doesn't restore baseline performance (AUC 0.76-0.83 vs baseline 0.75-0.86)
- Both ADD categories show similar vulnerability patterns despite different input representations

## Why This Works (Mechanism)

### Mechanism 1: Statistical AF Attacks
- **Claim:** Statistical AF attacks degrade ADD performance by altering the statistical properties (amplitude distribution, spectral content, temporal patterns) that detectors rely on to identify synthetic signatures.
- **Core assumption:** ADDs rely on consistent statistical distributions in synthetic audio that can be systematically disrupted.
- **Evidence anchors:** [Section 2.3.1] "Statistical AF attacks... aim to alter the statistical properties, such as amplitude distribution, spectral content, or temporal patterns, without noticeably changing their perceptual quality." [Table 2] Average AUC drops from 0.75 (raw) / 0.86 (spectrogram) baseline to 0.52-0.60 under statistical attacks.

### Mechanism 2: Optimization-Based AF Attacks
- **Claim:** Optimization-based AF attacks exploit model gradients to craft minimal perturbations that cross decision boundaries, causing misclassification with imperceptible changes.
- **Core assumption:** ADDs have smooth, exploitable decision boundaries that can be crossed with small perturbations.
- **Evidence anchors:** [Section 2.3.2] "Unlike statistical methods, these attacks iteratively optimize a loss to maximize misclassification." [Table 3] PGD reduces average AUC to 0.09 (raw) and 0.35 (spectrogram). [Table 5] Optimization attacks achieve SSIM >0.95 (lower perceptual distortion) compared to statistical attacks (SSIM 0.82-0.99).

### Mechanism 3: Adversarial Training
- **Claim:** Adversarial training partially restores robustness by exposing ADDs to attack examples during training, forcing learned features to be invariant to perturbation patterns.
- **Core assumption:** Adversarial examples generated during training generalize to test-time attacks with similar parameter ranges.
- **Evidence anchors:** [Section 3.5] "Adversarial training improves the average AUC and EER to 0.76 and 0.28 for raw ADDs, and 0.83 and 0.18 for spectrogram-based ADDs. However, these values remain lower than the baseline." [Table 4] Shows partial recovery but not full restoration to baseline performance.

## Foundational Learning

- **Concept:** Spectrogram vs. Raw Audio Representation
  - **Why needed here:** The paper benchmarks two fundamentally different input modalities. Spectrogram-based methods transform audio to time-frequency representations and apply visual CNN architectures, while raw methods operate directly on waveforms.
  - **Quick check question:** Can you explain why a ResNet trained on Mel spectrograms might capture different artifacts than one trained on raw waveforms?

- **Concept:** AUC (Area Under ROC Curve) and EER (Equal Error Rate)
  - **Why needed here:** These are the primary evaluation metrics throughout all tables. AUC measures overall discriminative ability (1.0 = perfect), while EER represents the error rate where false acceptance equals false rejection.
  - **Quick check question:** If an ADD achieves AUC=0.50 and EER=0.50, what does this indicate about its classification ability?

- **Concept:** Gradient-Based Adversarial Attacks (FGSM, PGD, C&W, DeepFool)
  - **Why needed here:** Section 2.3.2 describes four optimization attacks with distinct strategies. FGSM is single-step (fast but weaker), PGD is iterative (stronger), C&W optimizes for minimal distortion, and DeepFool finds the nearest decision boundary.
  - **Quick check question:** Why would an iterative attack like PGD cause more severe performance degradation than a single-step attack like FGSM?

## Architecture Onboarding

- **Component map:** Dataset Selection → Audio Preprocessing → [Raw waveform → R1-R6: Raw ADD Models] OR [Spectrogram transform → S1-S6: Spectrogram ADD Models] → AF Attack Module → [Statistical: Pitch shift, Filter, Noise, Quantization] OR [Optimization: FGSM, PGD, C&W, DeepFool] → Evaluation: AUC/EER across 5 datasets

- **Critical path:**
  1. Training phase: Combine D1 (ASVSpoof2019), D2 (ASVSpoof2021), D4 (CodecFake) → Train 12 ADD models for 50 epochs, Adam optimizer, lr=0.0001, batch=256
  2. Attack generation: For each test sample, apply 4 statistical + 4 optimization attacks with varying parameters
  3. Evaluation: Test on all 5 datasets (D1-D5) including unseen D3 (ASVSpoof2024) and D5 (WaveFake) to measure generalization

- **Design tradeoffs:**
  - Raw vs. Spectrogram input: Spectrogram achieves higher baseline (0.86 vs. 0.75 AUC) but requires transform computation. Raw preserves temporal dynamics but may miss frequency-domain artifacts.
  - Model complexity vs. robustness: Larger models (RawNet3, FFD) show better robustness (R1 maintains AUC 0.67 under pitch shifting vs. R4 at 0.52), but computational cost increases.
  - Adversarial training coverage: Training with random attack parameters improves robustness (Table 4: AUC 0.76-0.83) but doesn't match clean baseline—suggesting a tradeoff between clean accuracy and robustness.

- **Failure signatures:**
  - Severe degradation under PGD: AUC drops to 0.05-0.09 indicate models are near-random—decision boundaries are easily crossed
  - Dataset-specific collapse: D3 (ASVSpoof2024) and D5 (WaveFake) show lower baseline performance (AUC 0.52-0.68) suggesting poor generalization to unseen generators
  - Architecture-specific vulnerabilities: R2, R3, R6 show AUC <0.30 under filtering/noise; S2, S5 show moderate vulnerability. Attention-based methods (S1, S4) show slight robustness.

- **First 3 experiments:**
  1. Replicate baseline on D1-D5: Train R1 (RawNet3) and S3 (FFD) on D1+D2+D4, evaluate on all 5 datasets to confirm reported AUC (R1: 0.83 avg, S3: 0.89 avg)
  2. Apply statistical AF attacks with controlled parameters: Use pitch shifting (±1, ±5, ±12 semitones) on D4 test set with R4 (LCNN). Plot AUC degradation curve to identify which perturbations cause greatest vulnerability
  3. Compare FGSM vs. PGD attack transfer: Generate adversarial examples using R1 (strong baseline) and test transferability to R4 (weaker model). Measure if attacks generated on one model degrade another

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on synthetic audio from fixed set of algorithms, potentially missing vulnerabilities to newer or hybrid generative methods
- Adversarial training protocol uses randomly sampled attack parameters, which may not cover full space of possible adversarial strategies
- Does not investigate temporal persistence of adversarial perturbations or behavior across audio segments in streaming scenarios

## Confidence

- **High Confidence**: Baseline performance comparisons between raw and spectrogram ADDs (AUC 0.75 vs 0.86), and general trend of performance degradation under adversarial attacks
- **Medium Confidence**: Effectiveness rankings of specific attack types (PGD causing worst degradation) and partial effectiveness of adversarial training
- **Low Confidence**: Claims about which ADD architectures are inherently more robust (e.g., RawNet3 vs LCNN) based on limited comparative data

## Next Checks

1. **Transferability Analysis**: Systematically test whether adversarial examples generated on spectrogram-based ADDs transfer to raw audio ADDs and vice versa to reveal shared fundamental vulnerabilities.

2. **Temporal Persistence Study**: Evaluate stability of adversarial perturbations across overlapping audio segments by testing detection performance on truncated segments to assess real-world streaming robustness.

3. **Black-Box Attack Validation**: Implement black-box attacks where adversary has no access to target model's architecture or parameters using substitute models to generate adversarial examples and test transferability.