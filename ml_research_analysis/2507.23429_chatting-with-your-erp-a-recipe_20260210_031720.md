---
ver: rpa2
title: 'Chatting with your ERP: A Recipe'
arxiv_id: '2507.23429'
source_url: https://arxiv.org/abs/2507.23429
tags:
- agent
- language
- query
- database
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dual-agent LLM system for natural language
  querying of industrial ERP databases. The approach uses a REACT-based Reasoner agent
  to generate SQL queries and a Critic agent to evaluate and refine them iteratively,
  improving accuracy through structured feedback loops.
---

# Chatting with your ERP: A Recipe

## Quick Facts
- arXiv ID: 2507.23429
- Source URL: https://arxiv.org/abs/2507.23429
- Reference count: 6
- Primary result: Dual-agent LLM system for natural language querying of industrial ERP databases

## Executive Summary
This paper presents a dual-agent LLM system for natural language querying of industrial ERP databases. The approach uses a REACT-based Reasoner agent to generate SQL queries and a Critic agent to evaluate and refine them iteratively, improving accuracy through structured feedback loops. The system was evaluated on a production-grade ERP database using eleven open-weight LLM models, with the best-performing model (Devstral 24B Q4) achieving 10 out of 11 correct responses. The authors highlight challenges including the need for expert-crafted semantic schema descriptions and difficulties in column selection from vague prompts. They propose future work on automated schema description generation and improved intent interpretation to enhance scalability and usability.

## Method Summary
The method employs a dual-agent REACT architecture where a SQL Reasoner generates queries and a SQL Critic evaluates them iteratively. The system uses handcrafted semantic schema descriptions combined with auto-generated technical schemas as context. Queries undergo self-debugging for syntax errors, then enter a refinement loop with the Critic for semantic validation. A hybrid pipeline uses a lightweight LLM for structured output extraction. HITL validation clarifies ambiguous intents before query generation.

## Key Results
- Best model (Devstral 24B Q4) achieved 10/11 correct responses on ERP queries
- Performance varied dramatically across models (0/11 for Codestral 22B, Magistral 24B, Deepseek Coder 33B)
- Dual-agent approach improves accuracy over single-agent baselines through iterative refinement
- Expert-crafted semantic schema descriptions critical for bridging technical and user domains

## Why This Works (Mechanism)

### Mechanism 1: Dual-Agent Iterative Refinement Loop
- Claim: A Reasoner-Critic architecture improves SQL query accuracy through structured feedback cycles.
- Mechanism: The SQL Reasoner generates candidate queries → executes them → SQL Critic evaluates syntactic validity, semantic appropriateness, and result quality → Critic returns structured feedback → Reasoner refines query. This cycle repeats up to M rounds, enabling progressive error correction.
- Core assumption: The Critic agent can reliably identify semantic mismatches that the Reasoner misses, and structured feedback translates into meaningful query improvements.
- Evidence anchors:
  - [abstract] "A novel dual-agent architecture combining reasoning and critique stages was proposed to improve query generation reliability."
  - [section 3.2] "This cooperative process ensures syntactic correctness, semantic adequacy, and usability of the generated queries, even in complex database scenarios."
  - [corpus] Weak direct evidence. Related work (Xie et al. 2024, Cen et al. 2025) mentions multi-agent collaboration for Text-to-SQL but does not independently validate this specific dual-agent pattern.
- Break condition: If Critic feedback is misinterpreted by Reasoner, or if both agents share the same blind spots (same model architecture), refinement stalls without improvement.

### Mechanism 2: Self-Debugging with Execution Feedback
- Claim: Runtime error messages provide actionable signals for iterative query correction.
- Mechanism: The Reasoner executes preliminary SQL → on failure (syntax errors, undefined identifiers, improper joins), it analyzes the error message → generates corrected query → retries up to N attempts. This addresses low-level errors without human intervention.
- Core assumption: Error messages from the database engine contain sufficient information for the LLM to diagnose and fix the root cause.
- Evidence anchors:
  - [section 3.2.1] "If execution fails—due to syntax errors, undefined identifiers, or improper joins—the agent analyzes the resulting error message and iteratively refines the query to resolve the issue."
  - [section 3.2.1] References Chen et al. (2023) for self-debugging strategy inspiration.
  - [corpus] No direct corpus validation for this specific self-debugging implementation in ERP contexts.
- Break condition: If error messages are cryptic, or if the model lacks SQL syntax knowledge, iterative attempts converge to repeated failures without progress.

### Mechanism 3: Semantic Schema Injection with Expert Authored Descriptions
- Claim: Handcrafted natural language descriptions of table relationships and domain concepts bridge the gap between user intent and technical schema.
- Mechanism: The schema input has two parts: (1) expert-written semantic descriptions covering concepts, table summaries, and high-level relationships; (2) auto-generated technical schema with columns, types, and sample values. Together, they provide both conceptual mapping and structural detail.
- Core assumption: Expert knowledge accurately captures domain semantics, and this transfers effectively through prompt context to guide column selection and join logic.
- Evidence anchors:
  - [section 3.3] "This schema acts as a bridge between the agent and the underlying data structures, improving the agent contextual understanding of the problem."
  - [section 6] "The first step involves exposing the internal SQL database... the most relevant fields—those frequently used or carrying business-critical information—should be clearly documented."
  - [corpus] No independent validation of semantic schema injection effectiveness.
- Break condition: If expert descriptions are incomplete, outdated, or misaligned with actual data patterns, the agent inherits and amplifies these gaps.

## Foundational Learning

- Concept: REACT (Reason + Act) framework
  - Why needed here: The architecture uses REACT as the orchestration pattern for the top-level agent, combining reasoning steps with tool calls (delegating to SQL Agent). Understanding this pattern is essential for debugging agent flow.
  - Quick check question: Can you explain how a REACT agent differs from a single-pass prompt-to-output pipeline?

- Concept: Text-to-SQL with schema linking
  - Why needed here: The core task is mapping natural language to SQL over a complex schema with implicit relationships. Schema linking (connecting user terms to schema elements) underlies both the Reasoner and Critic's effectiveness.
  - Quick check question: Given a user query "show me delayed orders," how would you identify which tables and columns are relevant if the schema has no explicit "delay" column?

- Concept: Iterative multi-agent collaboration
  - Why needed here: The Reasoner-Critic loop is an instance of multi-agent collaboration with structured message passing. Understanding state sharing and termination conditions is critical for implementation.
  - Quick check question: What termination conditions prevent infinite loops in a Reasoner-Critic cycle?

## Architecture Onboarding

- Component map: User Input → REACT Agent (intent analysis, HITL check) → SQL Agent (dual-agent loop) → Database (read-only SQL Server) → Structured Output Extractor (smaller LLM) → Natural Language Response → User
- Schema (markdown) injected into SQL Agent: Semantic Description (expert) + Autogenerated Schema (technical)

- Critical path:
  1. Schema preparation (expert-crafted semantic descriptions are the highest-friction step and directly impact accuracy).
  2. Reasoner self-debug loop (handles syntax/execution errors).
  3. Critic-Reasoner refinement rounds (handles semantic correctness).
  4. HITL intent clarification (prevents wasted compute on vague prompts).

- Design tradeoffs:
  - Open-weight vs. proprietary LLMs: Paper uses open-weight for data privacy, but performance varies dramatically (10/11 vs. 0/11 across models). Prompt design was optimized for Qwen 2.5, disadvantaging other architectures.
  - Manual vs. automated schema description: Expert descriptions improve accuracy but limit scalability. Future work targets automated generation.
  - Iteration limits (N attempts, M rounds): Trade compute cost against accuracy gains. Paper does not report sensitivity analysis on these parameters.
  - Two-model approach (lightweight for structured extraction, larger for reasoning): Reduces latency but adds orchestration complexity.

- Failure signatures:
  - Models scored 0/11 (Codestral 22B, Magistral 24B, Deepseek Coder 33B): Likely prompt-model mismatch; prompts were built around Qwen 2.5 32B.
  - Column selection failures from vague prompts: Agent cannot determine which ID column matches user intent without explicit guidance.
  - Intent summarization loss: REACT agent sometimes fails to pass full user intent to SQL Agent, losing corrections or details.
  - Hardware/timeout failures: Larger models (Qwen 3 32B, Llama 3.3 70B) exceeded 100GB VRAM and 3-minute inference limit.

- First 3 experiments:
  1. Baseline validation: Run the same 11 test queries with a single-agent (Reasoner only, no Critic) to quantify the marginal contribution of the Critic feedback loop.
  2. Schema ablation: Test performance with (a) full semantic description, (b) auto-generated schema only, and (c) no schema context, to isolate the impact of expert descriptions.
  3. Model-prompt alignment: Retest the lowest-performing models (Codestral, Magistral) with prompts rewritten for their specific instruction-following patterns, to disentangle model capability from prompt compatibility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automated schema description agent effectively replace the need for expert-crafted semantic descriptions without compromising query accuracy?
- Basis in paper: [explicit] The authors state that future work will focus on "a new agent to describes the database" to "reduce or even stop the need of a database expert to write down a natural language description."
- Why unresolved: The current architecture relies heavily on a "handcrafted semantic description" (Section 3.3.1) to bridge the gap between technical column names and user intent, which creates a scalability bottleneck.
- What evidence would resolve it: A comparative ablation study measuring the SQL generation accuracy of the current system against a variant that uses only auto-generated schema descriptions.

### Open Question 2
- Question: How can the architecture be modified to prevent the loss of user intent details when the REACT agent summarizes prompts for the SQL Agent?
- Basis in paper: [explicit] The conclusion notes that "the REACT agent sometimes fails to summarize the entire user intent to the SQL Agent, so some details or corrections might be skipped."
- Why unresolved: The modular architecture creates a communication bottleneck where the conversational agent strips necessary nuance before the query generation phase begins.
- What evidence would resolve it: Demonstrating a modified state-passing mechanism or prompt structure that maintains higher information fidelity, verified by improved accuracy on complex, multi-constraint queries.

### Open Question 3
- Question: To what extent is the failure of certain models (e.g., Codestral, Deepseek) attributable to model incapacity versus prompt specificity?
- Basis in paper: [inferred] Section 4.3 states "prompts have been built around Qwen 2.5 32B... so a disadvantage is expected" for other models, coinciding with several models scoring 0/11.
- Why unresolved: It is unclear if the observed performance drop in non-Qwen models is an intrinsic failure of their reasoning capabilities or a result of prompt incompatibility.
- What evidence would resolve it: Re-evaluating the underperforming models using specifically tuned prompts to determine if accuracy improves significantly compared to the "Qwen-optimized" baseline.

## Limitations
- Expert-crafted semantic schema descriptions create scalability bottleneck requiring manual effort
- Strong prompt-model alignment requirements disadvantage non-Qwen models without systematic testing
- Limited 11-query evaluation set may not represent full complexity of production ERP workloads
- No sensitivity analysis on iteration limits (N attempts, M rounds) and computational cost

## Confidence
- High confidence: The dual-agent architecture's basic feasibility and the general improvement from structured feedback loops over single-agent approaches
- Medium confidence: The specific performance numbers (10/11 correct for Devstral 24B Q4) and the ranking of open-weight models, given the lack of prompt-model alignment testing
- Low confidence: Claims about scalability to arbitrary ERP schemas without automated semantic description generation, and the generalizability of the 11-query evaluation to real-world usage

## Next Checks
1. Ablation study on schema descriptions: Test the system with (a) full expert semantic descriptions, (b) auto-generated schema only, and (c) no schema context to quantify the marginal contribution of expert knowledge to accuracy gains.

2. Prompt-model alignment validation: Re-test the lowest-performing models (Codestral, Magistral) with prompts specifically rewritten for their instruction-following patterns to determine whether poor performance reflects model capability or prompt incompatibility.

3. Iteration limit sensitivity analysis: Systematically vary N (Reasoner attempts) and M (Critic-Reasoner rounds) to identify optimal tradeoffs between computational cost and accuracy, particularly for queries that require multiple refinement cycles.