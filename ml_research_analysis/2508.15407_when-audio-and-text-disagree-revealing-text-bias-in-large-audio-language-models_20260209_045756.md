---
ver: rpa2
title: 'When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language
  Models'
arxiv_id: '2508.15407'
source_url: https://arxiv.org/abs/2508.15407
tags:
- text
- audio
- lalms
- bias
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MCR-BENCH, the first benchmark designed to
  evaluate how large audio-language models (LALMs) handle conflicting information
  between audio and text inputs. The benchmark includes 3,000 samples across three
  audio-centric tasks, each paired with faithful, adversarial, and irrelevant textual
  descriptions.
---

# When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models

## Quick Facts
- arXiv ID: 2508.15407
- Source URL: https://arxiv.org/abs/2508.15407
- Reference count: 9
- LALMs show significant text bias, often ignoring audio evidence when modalities conflict

## Executive Summary
This paper introduces MCR-BENCH, the first benchmark to evaluate how large audio-language models handle conflicting audio-text inputs. The benchmark includes 3,000 samples across three audio-centric tasks with faithful, adversarial, and irrelevant text descriptions. Extensive evaluation of six state-of-the-art LALMs reveals a strong text bias, with models prioritizing textual input over audio evidence. This leads to substantial performance degradation under adversarial conditions, with accuracy dropping from ~88% to ~1.5% on Audio Question Answering. The findings highlight significant reliability concerns for real-world applications and underscore the need for novel training paradigms to better balance modality contributions in multimodal processing.

## Method Summary
The authors create MCR-BENCH by generating 3,000 samples across three audio-centric tasks (Audio Question Answering, Speech Emotion Recognition, and Violence Sound Classification). For each audio sample, they pair it with three types of text descriptions: faithful (matching audio content), adversarial (contradicting audio content), and irrelevant (unrelated to audio content). They evaluate six state-of-the-art LALMs including Qwen-Audio-Chat and Qwen2-Audio-Instruct on this benchmark, measuring accuracy, Text-Induced Robustness (TIR), and Modality-Responsive Score (MRS). They also explore mitigation strategies including prompting techniques and supervised fine-tuning on conflict-rich data.

## Key Results
- LALMs show Text-Induced Robustness (TIR) exceeding 95% across models, indicating predictions flip based on text regardless of audio content
- Accuracy drops from 87.8% to 1.7% for Qwen-Audio-Chat and from 87.5% to 1.5% for Qwen2-Audio-Instruct under adversarial text conditions
- Random Forest classifiers achieve 98.0% accuracy distinguishing adversarial from faithful hidden states, indicating LALMs internally recognize conflicts but fail to translate this awareness into appropriate outputs

## Why This Works (Mechanism)

### Mechanism 1: Text-Modality Dominance in Fusion Layers
LALMs prioritize textual tokens over audio representations during cross-modal fusion, leading to systematic disregard of audio evidence when modalities conflict. The LLM backbone, pre-trained predominantly on text, exhibits stronger priors for textual patterns. Audio encoder outputs are projected into the LLM's embedding space but receive lower effective attention weights during fusion, causing text to dominate reasoning pathways.

### Mechanism 2: Latent Awareness-Output Disconnect
LALMs internally encode cross-modal inconsistencies in separable representation subspaces but fail to translate this awareness into calibrated outputs. Hidden states in deeper layers distinguish faithful vs. adversarial inputs (linearly separable), yet the decision head does not condition its confidence or output on this detected conflict.

### Mechanism 3: Confidence Miscalibration Under Modal Conflict
LALMs maintain high confidence scores even when producing incorrect predictions driven by misleading text, indicating absent or ineffective uncertainty calibration for cross-modal disagreement. Token probabilities remain high across conditions because the model's confidence estimation is unimodal-text-calibrated but not multimodal-conflict-calibrated.

## Foundational Learning

- **Cross-Modal Attention and Fusion**: Understanding how audio and text representations are combined in transformer layers explains where bias can be introduced. LALMs typically project audio features into the LLM embedding space and interleave them with text tokens.
  - Why needed here: Explains where bias emerges in the model architecture
  - Quick check question: Given an audio encoder output A ∈ R^(n×d_a) and text embedding T ∈ R^(m×d), how does cross-attention produce a fused representation, and which modality dominates if attention weights are skewed?

- **Modality-Specific Pre-training Imbalance**: LLM backbones are pre-trained on orders of magnitude more text data than audio-text pairs, creating strong textual priors that persist through multimodal fine-tuning.
  - Why needed here: Explains the root cause of text dominance
  - Quick check question: If an LLM has seen 2T text tokens but only 10M audio-text pairs, what inductive biases would you expect in downstream multimodal tasks?

- **Representation Separability vs. Behavioral Calibration**: The paper shows hidden states are separable (internal awareness) but outputs are not calibrated. Understanding this gap is essential for designing interventions that bridge representation and behavior.
  - Why needed here: Explains why models can "know" about conflicts but still fail
  - Quick check question: If a linear probe can detect conflict in layer-32 representations with 98% accuracy, why might the generation head still produce overconfident wrong answers?

## Architecture Onboarding

- **Component map**: Audio Encoder -> Projection Layer -> LLM Backbone -> Output Distribution
- **Critical path**: Audio input → Encoder → Audio tokens (A) → Projection → LLM backbone → Cross-attention fusion → Output distribution; Text input → Tokenizer → Text embeddings (T) → [A; T] → Projection → LLM backbone → Cross-attention fusion → Output distribution
- **Design tradeoffs**: Larger models (3B vs 0.5B) improve adversarial robustness modestly but do not eliminate bias; SFT on conflict data improves adversarial accuracy but degrades faithful accuracy; prompting is cheap but limited
- **Failure signatures**: Accuracy collapse (>95%) when text contradicts audio despite audio being correctly perceivable; Confidence scores >0.85 under adversarial inputs that produce wrong answers; TIR >95% indicating predictions flip based on text regardless of audio content
- **First 3 experiments**:
  1. Replicate the adversarial text evaluation on MCR-BENCH with your target LALM; establish baseline TIR and MRS across AQA, SER, and VSC tasks
  2. Train a linear probe on hidden states (layers 16, 32) to detect audio-text conflict; verify separability (>90% accuracy) to confirm internal awareness exists
  3. Fine-tune with LoRA on 1,000 conflict samples; measure tradeoff between adversarial accuracy gain and faithful accuracy loss; check if confidence calibration improves

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset scope limited to three specific audio-centric tasks, may not generalize to other multimodal domains
- Evaluation covers six LALMs but limited to similar architectural paradigms, results may not extend to alternative designs
- Mitigation strategies show partial improvements with clear tradeoffs, effectiveness and generalizability remain uncertain

## Confidence
- **High**: Core empirical finding of strong text bias in LALMs is well-supported by extensive benchmarking (TIR >95%, accuracy drops from ~88% to ~1.5% under adversarial text)
- **Medium**: Mechanism explanations (text-modality dominance, latent awareness-output disconnect, confidence miscalibration) are plausible but not definitively proven
- **Low**: Effectiveness and generalizability of proposed mitigation strategies remain uncertain

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate the same LALMs on MCR-BENCH versus a newly collected real-world dataset where audio-text conflicts occur naturally; measure if text bias persists and whether model size or architecture type moderates the effect
2. **Fusion Architecture Ablation**: Implement and compare three fusion strategies: standard cross-attention, symmetric attention with balanced weights, and gated modality fusion conditioned on conflict detection; train each on MCR-BENCH conflict data and measure if architectural changes reduce text bias more effectively than SFT
3. **Confidence Calibration and Uncertainty Analysis**: Perform proper calibration analysis (ECE, Brier score, reliability diagrams) on LALM outputs under faithful vs. adversarial conditions; train a confidence calibration head using cross-modal conflict as the target signal, then measure if calibrated uncertainty scores improve adversarial robustness without sacrificing faithful accuracy