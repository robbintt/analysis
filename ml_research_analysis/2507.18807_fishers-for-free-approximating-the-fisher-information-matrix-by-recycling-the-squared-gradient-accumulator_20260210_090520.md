---
ver: rpa2
title: Fishers for Free? Approximating the Fisher Information Matrix by Recycling
  the Squared Gradient Accumulator
arxiv_id: '2507.18807'
source_url: https://arxiv.org/abs/2507.18807
tags:
- fisher
- squisher
- gradient
- learning
- squared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using the squared gradient accumulator (Squisher)
  from adaptive optimizers as a free approximation of the Fisher diagonal, which is
  commonly used to measure parameter sensitivity in various applications. The key
  insight is that the Squisher's sum-then-square structure aligns more closely with
  the joint Fisher diagonal than the traditional Fisher diagonal's square-then-sum
  structure.
---

# Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator

## Quick Facts
- arXiv ID: 2507.18807
- Source URL: https://arxiv.org/abs/2507.18807
- Reference count: 28
- Key outcome: The Squisher approximation performs comparably to Fisher diagonal while eliminating computational costs in five applications including model merging, pruning, and continual learning

## Executive Summary
This paper proposes using the squared gradient accumulator (Squisher) from adaptive optimizers as a free approximation of the Fisher diagonal for measuring parameter sensitivity. The key insight is that the Squisher's sum-then-square structure better aligns with the joint Fisher diagonal than traditional square-then-sum approaches. Through experiments across five applications, the Squisher consistently performs comparably to the Fisher diagonal while significantly outperforming Fisher-free baselines. The approach eliminates the computational costs of explicitly computing the Fisher while maintaining effectiveness, and the analysis shows that the approximations made do not significantly impact performance in practice.

## Method Summary
The method leverages the squared gradient accumulator (Squisher) that naturally accumulates in adaptive optimizers like Adam and Adagrad. Instead of explicitly computing the Fisher Information Matrix through additional forward and backward passes, the approach recycles the gradient statistics already computed during optimization. The Squisher computes a running sum of squared gradients, which serves as an approximation to the Fisher diagonal. This eliminates the need for separate Fisher computation while maintaining similar parameter sensitivity information. The method requires minimal modification to existing training pipelines and works with standard adaptive optimizers without changing their core behavior.

## Key Results
- The Squisher consistently performs comparably to the Fisher diagonal across five applications (model merging, pruning, sparse training, task similarity measurement, and continual learning)
- In model merging experiments, Squisher achieved 49.4 vs 50.2 for Fisher diagonal (medium-sized models)
- In pruning experiments, Squisher achieved 66.5 vs 65.7 for Fisher diagonal while being computationally free
- The approach significantly outperforms Fisher-free baselines across all tested applications

## Why This Works (Mechanism)

The Squisher approximation works because it captures similar parameter sensitivity information as the Fisher diagonal through a different computational pathway. The key mechanism is that adaptive optimizers already accumulate gradient statistics in a way that correlates with parameter importance. The sum-then-square structure of the Squisher aligns better with the joint Fisher diagonal than the traditional square-then-sum approach used in explicit Fisher computation. By recycling these existing statistics, the method avoids redundant computation while maintaining effective sensitivity measurement.

## Foundational Learning

1. **Fisher Information Matrix**: Measures parameter sensitivity and uncertainty in statistical models; needed to understand why parameter importance estimation matters for various applications like pruning and continual learning. Quick check: Can compute the Fisher for a simple linear regression and observe how parameters with larger gradients have larger Fisher values.

2. **Adaptive Optimizers (Adam/Adagrad)**: Optimization algorithms that maintain per-parameter learning rates based on gradient statistics; needed because the Squisher builds upon their existing gradient accumulation mechanism. Quick check: Trace through one Adam update step and identify where squared gradients are accumulated.

3. **Parameter Sensitivity**: The concept that some parameters contribute more to model performance than others; needed to understand why we want to estimate parameter importance for applications like pruning and model merging. Quick check: Compare parameter values in a trained vs untrained network to see which parameters changed the most.

4. **Joint vs Marginal Fisher**: The distinction between computing Fisher for the full model vs individual parameters; needed to understand why the Squisher's sum-then-square structure is theoretically advantageous. Quick check: Compute both versions for a small neural network and observe the differences in parameter rankings.

5. **Gradient Accumulation**: The process of tracking historical gradient information; needed to understand how the Squisher builds its approximation over time. Quick check: Implement a simple running average of squared gradients and track how it changes during training.

## Architecture Onboarding

Component Map: Training Loop -> Gradient Computation -> Squisher Update -> Parameter Sensitivity Output

Critical Path: The critical path involves integrating the Squisher update into the existing optimizer update step. During each training iteration, after computing gradients but before applying parameter updates, the squared gradients are accumulated into the Squisher statistics. This accumulated information then serves as the parameter sensitivity measure for downstream applications.

Design Tradeoffs: The main tradeoff is between computational efficiency and approximation accuracy. While the Squisher is computationally free (reusing existing gradient computations), it only captures diagonal Fisher information and may miss important off-diagonal correlations. The method trades some theoretical precision for practical efficiency and ease of implementation.

Failure Signatures: The approach may fail when off-diagonal Fisher terms are crucial for the application, such as in highly coupled parameter subspaces. It may also underperform when the training dynamics differ significantly from the target application (e.g., using pre-trained statistics for fine-tuning a very different task). Poor initialization or unstable training could lead to unreliable Squisher statistics.

First Experiments:
1. Implement the Squisher accumulator in a simple MLP on MNIST and compare parameter sensitivity rankings with explicit Fisher computation
2. Test model merging using Squisher vs Fisher diagonal on two pre-trained BERT models with different random seeds
3. Apply pruning using Squisher statistics on a small vision transformer and measure accuracy retention

## Open Questions the Paper Calls Out
None

## Limitations
- The Squisher approximation only captures diagonal Fisher information and cannot represent off-diagonal terms that may be important for certain applications
- Empirical validation is limited to specific architectures (BERT, ViT, MLP) and tasks, raising questions about generalizability to other domains
- The performance can be sensitive to hyperparameter choices like scale factors, though default values worked well empirically

## Confidence

| Claim | Confidence |
|-------|------------|
| Squisher performs comparably to Fisher diagonal | Medium |
| The approach is computationally free | High |
| The approximations made don't significantly impact performance | Medium |

## Next Checks

1. Test the approach on more diverse architectures including convolutional networks, recurrent models, and multimodal transformers to assess architecture dependence

2. Evaluate performance across a wider range of hyperparameters and initialization schemes to understand robustness to training conditions

3. Compare against more recent Fisher-free methods like Hessian-free or gradient-based approximations to establish the relative advantage more definitively