---
ver: rpa2
title: The Three Regimes of Offline-to-Online Reinforcement Learning
arxiv_id: '2510.01460'
source_url: https://arxiv.org/abs/2510.01460
tags:
- offline
- fine-tuning
- environment
- data
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a stability-plasticity principle to explain
  inconsistent empirical behavior in offline-to-online reinforcement learning. The
  key insight is that fine-tuning performance depends on balancing preservation of
  prior knowledge (stability) and adaptation to new data (plasticity), with the relative
  strength of pretrained policy versus offline dataset determining which stability
  source to prioritize.
---

# The Three Regimes of Offline-to-Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.01460
- Source URL: https://arxiv.org/abs/2510.01460
- Authors: Lu Li; Tianwei Ni; Yihao Sun; Pierre-Luc Bacon
- Reference count: 40
- Primary result: Stability-plasticity framework accurately predicts optimal fine-tuning methods in 45/63 settings

## Executive Summary
This paper proposes a stability-plasticity principle to explain inconsistent empirical behavior in offline-to-online reinforcement learning. The key insight is that fine-tuning performance depends on balancing preservation of prior knowledge (stability) and adaptation to new data (plasticity), with the relative strength of pretrained policy versus offline dataset determining which stability source to prioritize. The authors identify three regimes based on this relationship: Superior (pretrained policy outperforms dataset), Comparable (similar performance), and Inferior (dataset outperforms policy). Each regime requires distinct stability properties. A large-scale empirical study across 63 settings shows that results align with predictions in 45 cases, with only 3 opposite mismatches, supporting the framework's utility for guiding design choices in offline-to-online RL.

## Method Summary
The method involves offline pretraining using CalQL, ReBRAC, or BC+FQE algorithms to obtain a pretrained policy π₀ and critic Q₀. Regimes are classified by comparing J(π₀) versus J(π_D) using a t-test with margin δ=0.05 to determine Superior, Comparable, or Inferior regimes. Six fine-tuning variants are evaluated: baseline (online-only), π₀-centric (warmup K=5000 steps and/or offline RL regularization), D-centric (offline data replay α=0.5 ratio and/or reset), and mixed (offline RL + offline data). Performance is tracked every 5k steps for 500k total environment steps, measuring normalized score, Stability (worst-case performance drop), and Plasticity (maximum improvement).

## Key Results
- Empirical results align with predictions in 45/63 settings, with only 3 opposite mismatches
- In Superior regime, π₀-centric methods outperform D-centric in 24/32 settings (75%)
- In Inferior regime, D-centric methods outperform π₀-centric in 19/23 settings (83%)
- The stability-plasticity decomposition accurately captures fine-tuning dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning performance decomposes into prior knowledge, stability, and plasticity components.
- Mechanism: Final knowledge = max(J(π₀), J(π_D)) + Stability(J*_off) + Plasticity, where stability captures worst-case degradation relative to the stronger offline source, and plasticity captures maximum improvement during online training.
- Core assumption: The offline performance baseline J*_off = max(J(π₀), J(π_D)) correctly identifies the stronger knowledge source.
- Evidence anchors:
  - [Section 3.2]: Formal decomposition equation and definitions of Stability(l) and Plasticity metrics
  - [Table 1]: Empirical values confirm "offline RL + offline data" achieves highest stability
  - [corpus]: Related work on distributional shift supports the need for explicit stability mechanisms

### Mechanism 2
- Claim: Matching stability mechanism to regime (π₀ vs. D) improves fine-tuning outcomes.
- Mechanism: In Superior regime (π₀ > D), methods preserving π₀ knowledge outperform D-centric methods. In Inferior regime (π₀ < D), methods anchoring to D outperform π₀-centric methods.
- Core assumption: The relative performance gap between π₀ and D is statistically meaningful and stable across seeds.
- Evidence anchors:
  - [Section 5.2]: π₀-centric methods outperform D-centric in 24/32 Superior settings (75%)
  - [Section 5.3]: D-centric methods outperform π₀-centric in 19/23 Inferior settings (83%)
  - [Table 2]: Confusion matrix shows 45/63 correct predictions

### Mechanism 3
- Claim: Offline data replay prevents Q-function divergence in Inferior regime but is unnecessary in Superior regime.
- Mechanism: In Inferior regime, fine-tuning without offline data causes high TD loss on offline distribution and Q-value divergence. Offline data replay anchors Q-learning to valid state-action pairs.
- Core assumption: Q-function stability on offline distribution correlates with fine-tuning success.
- Evidence anchors:
  - [Section B.5, Figure 8]: Kitchen-complete (Inferior) shows severe Q-divergence without offline data
  - [Section 5.3]: D-centric methods critical when pretrained policy underperforms behavior policy

## Foundational Learning

- Concept: **Off-policy RL with replay buffers**
  - Why needed here: All fine-tuning variants assume familiarity with experience replay and off-policy learning
  - Quick check question: Can you explain why sampling 50% offline / 50% online data might reduce distribution shift compared to pure online sampling?

- Concept: **Actor-critic methods (SAC, TD3)**
  - Why needed here: Experiments use SAC for CalQL fine-tuning and TD3 for ReBRAC fine-tuning
  - Quick check question: What is the role of the critic in stabilizing policy updates during fine-tuning?

- Concept: **Offline RL regularization (e.g., CQL, conservatism)**
  - Why needed here: π₀-centric methods reuse offline RL regularization during fine-tuning
  - Quick check question: Why might conservative Q-learning prevent exploration during online fine-tuning?

## Architecture Onboarding

- Component map: Offline pretraining -> Regime classification (TOST) -> Fine-tuning variant selection -> Evaluation tracking
- Critical path:
  1. Run offline pretraining → obtain π₀, estimate J(π₀) over 10 seeds
  2. Estimate J(π_D) from dataset returns
  3. Classify regime via TOST
  4. Select fine-tuning method based on regime
  5. Run 500k env steps with UTD=1, log stability/plasticity metrics

- Design tradeoffs:
  - **Warmup vs. offline RL regularization**: Warmup preserves plasticity better; regularization provides stronger stability but limits long-term improvement
  - **Offline data replay vs. reset**: Reset maximizes plasticity but causes early degradation; useful when π₀ is weak
  - **Margin δ**: δ=0.05 balances robustness vs. sensitivity

- Failure signatures:
  - Opposite mismatch (3/63 cases): π₀-centric wins in Inferior or D-centric wins in Superior
  - High plasticity but negative improvement: Reset without sufficient offline data anchoring
  - Q-divergence early in fine-tuning: Missing offline data replay in Inferior regime

- First 3 experiments:
  1. **Regime validation**: On 3 tasks (1 Superior, 1 Inferior, 1 Comparable), run all 6 fine-tuning variants; verify method selection matches regime predictions
  2. **Stability-plasticity tradeoff**: In Superior regime, ablate warmup K ∈ {1k, 5k, 10k} steps; plot stability vs. plasticity curves
  3. **Q-divergence analysis**: In Inferior regime, log TD loss on offline data with and without replay; confirm correlation with performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the stability–plasticity framework be extended from three discrete regimes to a continuous spectrum?
- Basis in paper: [explicit] The Conclusion states, "Real systems often lie along a continuum... Extending the framework to incorporate such dimensions is an important direction for future work."
- Why unresolved: The current taxonomy discretizes behavior for analytical clarity, potentially oversimplifying borderline cases or continuous shifts in policy quality.
- What evidence would resolve it: A formal model or algorithm that dynamically modulates stability and plasticity weights based on continuous performance metrics rather than assigning a discrete regime.

### Open Question 2
- Question: How can the framework be adapted for long-horizon sparse-reward settings where raw returns fail to capture knowledge utility?
- Basis in paper: [explicit] The Conclusion identifies a limitation in "situations where raw return does not adequately capture the usefulness of pretrained policies or offline datasets, for example in long-horizon sparse-reward settings."
- Why unresolved: Regime assignment currently relies on raw returns, which struggle to distinguish between "useless" and "promising" behaviors in sparse environments.
- What evidence would resolve it: The integration of alternative metrics (e.g., dense reward proxies or potential-based shaping) into the regime definition that correlate better with fine-tuning success.

### Open Question 3
- Question: How do dataset characteristics, such as coverage, interact with the stability-plasticity trade-off?
- Basis in paper: [explicit] The Conclusion notes that "other dataset characteristics, such as coverage, play an important role but remain difficult to capture consistently."
- Why unresolved: The current framework focuses primarily on performance (J(π₀) vs J(π_D)) while largely ignoring the structural properties of the data distribution itself.
- What evidence would resolve it: Empirical studies quantifying how variations in state-action space coverage modify the optimal stability-plasticity balance within the defined regimes.

## Limitations
- The framework assumes the offline performance baseline J*_off correctly identifies the stronger knowledge source, but small performance gaps may be statistically unreliable
- Regime assignments depend on hyperparameter stability across seeds, with some cases showing opposite mismatches (3/63)
- The decomposition into stability and plasticity components relies on worst-case assumptions that may not capture all fine-tuning dynamics
- The current framework focuses primarily on performance metrics while largely ignoring dataset structural properties

## Confidence

- **High confidence**: The empirical correlation between regime assignments and fine-tuning method performance (45/63 correct predictions, 3 opposite mismatches) supports the framework's practical utility
- **Medium confidence**: The stability-plasticity decomposition provides a useful conceptual framework, but the quantitative definitions may not fully capture all relevant performance dimensions
- **Medium confidence**: The mechanism explanations for why specific stability properties matter in each regime are plausible but not fully validated through ablation studies or theoretical analysis

## Next Checks
1. **Ablation study on margin δ**: Test multiple δ values (0.0, 0.05, 0.1) to understand sensitivity of regime classification and examine how prediction accuracy varies with classification strictness
2. **Long-horizon performance analysis**: Extend fine-tuning beyond 500k steps to evaluate whether initial regime-based method selection remains optimal over longer training horizons and whether methods converge to similar performance levels
3. **Dataset quality robustness**: Systematically vary dataset quality (e.g., using corrupted or low-quality subsets) to test framework predictions when J(π_D) becomes unreliable or misleading, examining break conditions for the underlying assumptions