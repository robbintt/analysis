---
ver: rpa2
title: Efficient Hyperparameter Tuning via Trajectory Invariance Principle
arxiv_id: '2509.25049'
source_url: https://arxiv.org/abs/2509.25049
tags:
- scheduler
- figure
- size
- loss
- invariance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a phenomenon called trajectory invariance,
  where loss curves, gradient noise, and gradient norm exhibit closely overlapping
  behavior with respect to a quantity combining learning rate and weight decay. The
  phenomenon shifts during training: early on, invariance is with respect to learning
  rate; later, it shifts to effective learning rate (the product of learning rate
  and weight decay).'
---

# Efficient Hyperparameter Tuning via Trajectory Invariance Principle

## Quick Facts
- **arXiv ID**: 2509.25049
- **Source URL**: https://arxiv.org/abs/2509.25049
- **Reference count**: 40
- **Key outcome**: This paper identifies trajectory invariance, where loss curves, gradient noise, and gradient norm exhibit closely overlapping behavior with respect to a quantity combining learning rate and weight decay, enabling efficient tuning by following the salient direction revealed by invariance.

## Executive Summary
This paper presents a novel observation about neural network training dynamics called "trajectory invariance," where the evolution of loss curves, gradient noise, and gradient norm shows similar behavior across different learning rate and weight decay combinations when viewed through a specific normalization. The phenomenon shifts during training: early on, invariance is with respect to learning rate alone, but later transitions to invariance with respect to effective learning rate (the product of learning rate and weight decay). This observation effectively reduces the two-dimensional hyperparameter space of learning rate and weight decay to one dimension, enabling more efficient hyperparameter tuning. The authors validate this principle across multiple large-scale transformer models including BERT, ViT, and LLama, and use it to refine scaling laws for optimal hyperparameter selection.

## Method Summary
The authors systematically study the relationship between learning rate and weight decay across different training phases, observing that both hyperparameters can be normalized into a single trajectory-invariant quantity. They conduct extensive experiments across BERT, ViT, and LLama models, varying both hyperparameters independently while tracking loss curves, gradient noise statistics, and gradient norms. By analyzing when and how these trajectories align, they identify the transition point where invariance shifts from learning rate to effective learning rate. This insight is then used to propose a more efficient hyperparameter tuning strategy that follows the invariant direction rather than exploring the full two-dimensional space. The method is validated by showing that optimal hyperparameters can be found more efficiently using this reduced search space.

## Key Results
- Trajectory invariance reduces the two-dimensional hyperparameter space of learning rate and weight decay to one dimension, enabling efficient tuning
- Optimal learning rate remains nearly constant while optimal weight decay decreases sublinearly with data size (D^(-1/3))
- The paper challenges the validity of existing batch size scaling laws, suggesting they may not hold under trajectory invariance
- The invariance phenomenon holds across BERT, ViT, and LLama models with standard cross-entropy objectives

## Why This Works (Mechanism)
The trajectory invariance principle works because neural network training dynamics exhibit self-similarity when learning rate and weight decay are scaled appropriately. The loss surface geometry and gradient flow patterns are primarily determined by the effective learning rate (learning rate × weight decay), rather than these hyperparameters individually. During early training, weight decay effects are minimal compared to learning rate, so invariance is observed with respect to learning rate alone. As training progresses and weights grow larger, weight decay becomes more influential, shifting the invariance to effective learning rate. This creates a natural ordering where following the invariant direction captures the most salient training dynamics while filtering out redundant hyperparameter combinations.

## Foundational Learning
- **Learning rate scheduling**: Understanding how learning rate affects training dynamics is crucial for interpreting trajectory invariance. Quick check: Verify that learning rate schedules follow the observed invariance patterns.
- **Weight decay regularization**: Weight decay's interaction with learning rate determines the effective learning rate. Quick check: Confirm that weight decay magnitude scales appropriately with model size.
- **Gradient noise analysis**: The paper analyzes gradient noise statistics as part of trajectory invariance. Quick check: Validate that gradient noise follows the expected scaling with batch size and learning rate.
- **Scaling laws in deep learning**: Understanding existing scaling relationships helps contextualize the refined laws proposed here. Quick check: Compare the D^(-1/3) weight decay scaling against previous literature.

## Architecture Onboarding
- **Component map**: Learning rate and weight decay hyperparameters -> Training trajectory (loss, gradient noise, gradient norm) -> Hyperparameter tuning efficiency
- **Critical path**: Hyperparameter selection → Training initialization → Early phase invariance (learning rate) → Transition point → Late phase invariance (effective learning rate) → Optimal hyperparameter identification
- **Design tradeoffs**: The method trades exploration of full hyperparameter space for exploitation of invariant direction, potentially missing local optima but significantly reducing search cost
- **Failure signatures**: If trajectory invariance doesn't hold, loss curves won't align across different hyperparameter combinations, suggesting the method won't work for that architecture/objectives
- **3 first experiments**:
  1. Verify trajectory invariance on a simple architecture (e.g., MLP) with synthetic data
  2. Test the transition point between learning rate and effective learning rate invariance on a small transformer
  3. Validate the D^(-1/3) weight decay scaling law with controlled data size experiments

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several arise from the work: How general is trajectory invariance across different model architectures and training objectives? What precisely determines the transition point between early and late phase invariance? Can the invariance principle be extended to other hyperparameters beyond learning rate and weight decay?

## Limitations
- The trajectory invariance phenomenon appears primarily validated for large-scale transformer models and standard cross-entropy objectives
- The transition point where invariance shifts from learning rate to effective learning rate is described qualitatively but not precisely quantified across different model families
- Limited empirical scope for claims about batch size scaling laws being invalid, given that batch size affects gradient noise structure differently than weight decay

## Confidence
- **High confidence**: The observation that loss curves can be aligned across different learning rate/weight decay combinations when properly normalized
- **Medium confidence**: The claim that optimal weight decay scales sublinearly with data size (D^(-1/3)) while optimal learning rate remains constant
- **Low confidence**: The assertion that batch size scaling laws are invalid, given the limited empirical scope and the fact that batch size affects gradient noise structure differently than weight decay

## Next Checks
1. Test trajectory invariance across diverse architectures (CNNs for vision, RNNs for sequence modeling) and training objectives (contrastive loss, reinforcement learning) to determine the generality of the phenomenon
2. Conduct controlled experiments varying data size while keeping model capacity fixed to isolate whether the D^(-1/3) weight decay scaling is truly about data quantity versus other correlated factors like effective batch size
3. Systematically vary batch size while maintaining trajectory invariance to determine whether batch size effects can be reconciled with the invariance principle through appropriate normalization, or whether they genuinely violate it