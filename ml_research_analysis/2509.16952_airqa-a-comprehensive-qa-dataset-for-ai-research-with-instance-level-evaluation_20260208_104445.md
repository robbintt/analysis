---
ver: rpa2
title: 'AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation'
arxiv_id: '2509.16952'
source_url: https://arxiv.org/abs/2509.16952
tags:
- question
- answer
- evaluation
- dataset
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AirQA, a comprehensive question answering
  dataset for AI research papers, featuring 1,246 questions across 13,948 papers with
  multi-task, multi-modal, and instance-level evaluation capabilities. The authors
  also propose EXTRACTOR, an automated framework that synthesizes instruction data
  and interaction trajectories without human intervention.
---

# AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation

## Quick Facts
- **arXiv ID:** 2509.16952
- **Source URL:** https://arxiv.org/abs/2509.16952
- **Reference count:** 40
- **Primary result:** EXTRACTOR framework synthesizes 4,000-10,000 trajectories to fine-tune small models, achieving performance comparable to untrained larger models (best model: 44.14% overall accuracy)

## Executive Summary
This paper introduces AirQA, a comprehensive question answering dataset for AI research papers featuring 1,246 questions across 13,948 papers with multi-task, multi-modal, and instance-level evaluation capabilities. The authors propose EXTRACTOR, an automated framework that synthesizes instruction data and interaction trajectories without human intervention. Evaluations show that most models underperform on AirQA, with the best model achieving only 44.14% overall accuracy. Fine-tuning small models with 4,000 trajectories enables them to achieve performance comparable to untrained larger models, demonstrating the effectiveness of EXTRACTOR in improving multi-turn tool-use capabilities.

## Method Summary
The EXTRACTOR framework uses a three-stage pipeline to synthesize agent interaction trajectories: an Explorer agent generates QA pairs from paper contexts, a Tracker agent selects appropriate evaluation functions and formats examples, and an Actor agent (using ReAct) interacts with a tool-use environment (DuckDB, Milvus) to generate successful action trajectories. These trajectories are chunked into instruction data for fine-tuning. The AirQA dataset pairs each question with specific Python evaluation functions (19 total) that enable instance-level, objective assessment of model answers. Fine-tuning is performed using LoRA on Qwen2.5 models (3B/7B/14B) with 4K-10K synthesized trajectories.

## Key Results
- EXTRACTOR generates high-quality synthetic data that significantly improves agent tool-use performance
- Fine-tuned small models (Qwen2.5-7B) achieve 24.07% accuracy, comparable to untrained larger models (25.52%)
- Most models underperform on AirQA, with the best proprietary model achieving only 44.14% overall accuracy
- Synthetic data scale shows consistent gains from 1K to 10K trajectories

## Why This Works (Mechanism)

### Mechanism 1: Instance-level function-based evaluation
The AirQA dataset uses specific Python evaluation functions paired with each question to provide objective assessment. This forces models to output structured answers (numbers, lists, dicts) that deterministic functions can evaluate against ground truth, eliminating the need for separate LLM evaluators in most cases.

### Mechanism 2: Multi-agent pipeline for trajectory synthesis
EXTRACTOR decomposes QA trajectory generation into three sub-tasks: Explorer generates QA pairs from paper contexts, Tracker selects evaluation functions and formats examples, and Actor uses ReAct to interact with tool environments and generate successful trajectories.

### Mechanism 3: Distillation of tool-use policy
Fine-tuning smaller models on synthetic trajectories distills the tool-use policy from large teacher models. The student learns efficient navigation of the paper QA environment (retrieving, querying, answering) through exposure to complete problem-solving processes.

## Foundational Learning

- **RAG vs. Agentic Systems**: AirQA baselines compare these paradigms. RAG is single-turn retrieval-and-read, while agentic systems involve multi-turn planning and action. *Quick check: Can you explain why "Agentic Hybrid" outperforms standard RAG on retrieval-type questions?*

- **ReAct Agent Framework**: The Actor agent and Agentic baselines use ReAct, where models generate "Thought" followed by "Action" in each turn. *Quick check: In a ReAct loop, what is the model's output after it executes a RETRIEVE action?*

- **Instruction Tuning & Trajectory Distillation**: EXTRACTOR creates data for instruction tuning. Training on full trajectories teaches the model the process of problem-solving, not just final answers. *Quick check: Why does the paper apply a "sliding window" to generated trajectories before using them for training?*

## Architecture Onboarding

- **Component map**: AirQA Dataset -> Evaluation Engine (19 Python functions) -> EXTRACTOR Pipeline (Explorer->Tracker->Actor) -> Trajectory Processor -> Fine-Tuning Loop
- **Critical path**: Environment setup (index papers in Milvus+DuckDB) -> Baseline evaluation on AirQA -> EXTRACTOR synthesis (generate 1K-10K trajectories) -> Fine-tuning with LoRA -> Re-evaluation
- **Design tradeoffs**: Evaluation objectivity vs. generality; synthetic data scale vs. quality; manual vs. automated annotation speed vs. effectiveness
- **Failure signatures**: Repetition loops (same action repeatedly), missing context/overconfidence (premature answers), evaluation function mismatches (formatting issues)
- **First 3 experiments**: 1) Implement Agentic Hybrid baseline and establish performance baseline, 2) Generate 1K trajectories with EXTRACTOR and fine-tune small model, 3) Fine-tune using only "single-doc" question trajectories and test on "multiple-doc" and "comprehensive" splits

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning methods surpass supervised fine-tuning when applied to the EXTRACTOR framework? The paper lists "exploring other RL-based methods for further improvements" as a primary future direction.

### Open Question 2
Does the 32B teacher model's performance cap the potential of larger student models? Section D.1 notes diminishing returns for 14B models, suggesting the teacher's 31.94% accuracy may be an upper bound.

### Open Question 3
How can agent architectures be improved to reduce "Visual Reasoning" errors in multi-modal scientific QA? The error analysis identifies visual reasoning as a major failure cause, while VLM additions show only marginal improvements.

## Limitations

- **Teacher model bias**: Synthetic data quality is limited by the single Qwen2.5-32B-Instruct teacher model, potentially introducing systematic biases
- **Domain specificity**: Dataset focuses on AI research papers, limiting generalizability to other scientific domains or general knowledge QA
- **Evaluation rigidity**: Instance-level evaluation may not capture nuanced or subjective aspects of QA quality that human evaluators would consider

## Confidence

- **High Confidence**: Synthetic trajectory fine-tuning improves multi-turn tool-use (supported by ablation studies and quantitative comparisons)
- **Medium Confidence**: EXTRACTOR's superiority over manual annotation relies on assumptions about speed vs. quality tradeoffs
- **Low Confidence**: Claims about instance-level evaluation being "more objective" lack comprehensive validation against human judgments

## Next Checks

1. **Bias Analysis**: Compare EXTRACTOR-generated trajectories against human annotations on a held-out subset, focusing on edge cases and subjective questions
2. **Cross-Domain Generalization**: Test fine-tuned models on QA tasks outside AI research papers (e.g., biomedical literature) to assess transferability
3. **Human Evaluation Correlation**: Conduct study comparing instance-level evaluation scores with human judgments to validate binary assessment alignment with nuanced quality assessments