---
ver: rpa2
title: 'DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage
  Shaping'
arxiv_id: '2510.12979'
source_url: https://arxiv.org/abs/2510.12979
tags:
- tool
- search
- planning
- answer
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies high entropy in planning tokens as a bottleneck
  in deep research agents and proposes DeepPlanner, an end-to-end reinforcement learning
  framework that enhances planning capabilities through two advantage shaping mechanisms.
  The first mechanism adds an entropy-based shaping term to amplify gradients on uncertain
  planning tokens while preventing negative advantage flips.
---

# DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping

## Quick Facts
- arXiv ID: 2510.12979
- Source URL: https://arxiv.org/abs/2510.12979
- Authors: Wei Fan; Wenlin Yao; Zheng Li; Feng Yao; Xin Liu; Liang Qiu; Qingyu Yin; Yangqiu Song; Bing Yin
- Reference count: 19
- Primary result: Achieves state-of-the-art performance on seven deep research benchmarks using 3,072 training samples and 8 rollouts per query, compared to 32,000 samples and 16 rollouts for previous best methods.

## Executive Summary
DeepPlanner addresses high entropy in planning tokens as a bottleneck in deep research agents through an end-to-end reinforcement learning framework. The approach uses two advantage shaping mechanisms: entropy-based advantage shaping to amplify gradients on uncertain planning tokens, and selective advantage upweighting to prioritize complex rollouts requiring intensive planning. The method achieves state-of-the-art performance while reducing planning token entropy over training, improving both in-domain and out-of-domain generalization.

## Method Summary
DeepPlanner is built on top of Group Relative Policy Optimization (GRPO) and introduces two novel advantage shaping mechanisms. The first mechanism adds a gradient-detached entropy-based shaping term to token-level advantages, amplifying updates for high-entropy planning tokens while preventing negative advantage flips through clipping. The second mechanism selectively upweights sample-level advantages for complex rollouts by identifying the most efficient trajectory (correct answer with fewest tool calls) above a complexity threshold. The architecture enforces explicit plan-then-execute decoupling by requiring agents to generate a plan within `<plan>...</plan>` tags before taking any tool-call actions.

## Key Results
- Achieves state-of-the-art performance on seven deep research benchmarks
- Reduces planning token entropy over training while maintaining exploration
- Uses 3,072 training samples and 8 rollouts per query versus 32,000 samples and 16 rollouts for previous methods
- Demonstrates improved both in-domain and out-of-domain generalization

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Advantage Shaping (EAS)
EAS modifies token-level advantages by adding a detached entropy term that increases update magnitude for uncertain planning tokens while preventing negative advantages from flipping positive through clipping. This accelerates optimization of planning capability by targeting under-optimized decision points with latent capacity for improvement.

### Mechanism 2: Selective Advantage Upweighting (SAU)
SAU identifies the most efficient trajectory (correct answer with fewest tool calls) in complex rollouts and multiplies its token-level advantages by a factor λ>1. This promotes learning from high-quality, complex scenarios that provide denser planning signals than failed or redundant trajectories.

### Mechanism 3: Explicit Plan-Then-Execute Decoupling
The architecture enforces mandatory planning stages by requiring agents to generate plans within `<plan>...</plan>` tags before taking tool-call actions. This prevents short-sighted drift and encourages committing to strategies before getting distracted by intermediate tool responses.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: DeepPlanner is built on GRPO, understanding how it calculates advantages relative to group mean is essential
  - Quick check: How does GRPO compute the advantage $A_{i,t}$ for a specific token, and why does DeepPlanner modify this specific variable?

- **Concept: Entropy-Equilibrium in RL**
  - Why needed: The paper identifies "high entropy" as a bottleneck, distinguishing between exploration (good) vs. uncertainty/confusion (bad if unoptimized)
  - Quick check: Why does the paper clip the entropy term $\psi(H)$ to prevent negative advantages from flipping positive?

- **Concept: Tool-Call Efficiency vs. Complexity**
  - Why needed: SAU uses "fewest tool calls" to define the "best" complex rollout, which is counter-intuitive
  - Quick check: Why does the paper upweight the trajectory with the minimum tool calls rather than the maximum for complex scenarios?

## Architecture Onboarding

- **Component map:** Agent Loop (state, thinking, actions) -> Environment (web_search, web_browse) -> Trainer (GRPO + Shaping)

- **Critical path:**
  1. Data Gen: Sample query -> Generate G rollouts (8 used in paper)
  2. Reward Calc: Format reward (+0.5) + Answer reward (+0.5) via LLM judge
  3. Advantage Calc: Compute group-relative advantages $A_{i,t}$
  4. Shaping: EAS (token-level) -> SAU (sample-level) -> Updates
  5. Update: optimize GRPO objective

- **Design tradeoffs:** End-to-End vs. Iterative SFT - SAU replaces need for stopping training to do SFT on filtered data, simplifying engineering but requiring careful tuning of complexity threshold

- **Failure signatures:**
  - Entropy Collapse: Token entropy dropping to near-zero immediately indicates EAS failing or α too low
  - Drift/Short-sightedness: Correct answers using illogical steps indicates missing `<plan>` constraint
  - Format Violations: Reward staying at 0 despite correct answers indicates tool calls before initial plan

- **First 3 experiments:**
  1. Vanilla Baseline: Run GRPO without EAS or SAU to reproduce "high planning entropy" phenomenon
  2. Ablation EAS: Add Entropy-based Advantage Shaping, plot "Entropy Adv. Ratio" to verify targeting high-entropy tokens
  3. Ablation SAU: Add Selective Advantage Upweighting, monitor tool call counts to ensure learning efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DeepPlanner sustain performance gains or succumb to entropy collapse when scaled beyond 48 RL steps?
- Basis: Authors note training capped at 48 steps where planning-token entropy "remains relatively high, indicating headroom to further scale exploration"
- Why unresolved: Computational constraints limited training duration
- What evidence would resolve it: Training trajectories extending to 100+ steps showing stable or declining planning entropy without performance degradation

### Open Question 2
- Question: How does outcome-based advantage shaping compare to fine-grained process rewards for planning?
- Basis: Paper states "Alternative approaches using fine-grained, multi-dimensional process reward for planning... represent a different research direction deserving exploration"
- Why unresolved: DeepPlanner relies on terminal rewards rather than intermediate, step-level feedback
- What evidence would resolve it: Comparative study integrating process rewards into RL framework against current entropy-based shaping

### Open Question 3
- Question: To what extent does the reliability of the LLM-as-a-judge impact the stability of the training process?
- Basis: Authors acknowledge they "do not independently evaluate the judge's reliability," relying directly on chatgpt-4o-latest
- Why unresolved: Noisy or biased reward signals could affect quality of advantage shaping and policy updates
- What evidence would resolve it: Sensitivity analysis correlating variations in judge accuracy with final model performance metrics

## Limitations

- Entropy-based advantage shaping assumes high entropy indicates under-optimized decision points rather than noise, which is not empirically validated
- Selective advantage upweighting assumes "efficient" trajectories (fewest tool calls) always provide best learning signals, which may not hold for complex scenarios
- Explicit plan-then-execute decoupling may limit agent's ability to adapt dynamically when initial plans prove insufficient

## Confidence

**High Confidence Claims:**
- Identification of high entropy in planning tokens as bottleneck
- Overall improvement in performance metrics
- Reduction in planning token entropy over training

**Medium Confidence Claims:**
- Specific mechanisms by which EAS and SAU contribute to gains
- Generalization to out-of-domain tasks
- Sample efficiency claims (3,072 vs 32,000 samples)

**Low Confidence Claims:**
- Assertion that EAS prevents "negative advantage flips" without empirical evidence
- Long-term stability of approach (only 2,000 training steps reported)
- Assumption that few tool calls always indicate better planning

## Next Checks

1. **Noise vs. Uncertainty Validation**: Conduct experiment where planning tokens are randomly shuffled to test whether EAS amplifies gradients on noise or genuinely under-optimized tokens. If performance degrades when noise is introduced, this validates targeting useful uncertainty rather than harmful randomness.

2. **Tool Call Efficiency Correlation**: Design controlled experiment varying relationship between tool call count and answer quality. Test whether SAU mechanism continues to upweight efficient trajectories when more tool calls actually correlate with better answers.

3. **Long-Horizon Stability Test**: Extend training beyond 2,000 steps (e.g., to 10,000 steps) and monitor for signs of entropy collapse, mode collapse, or performance degradation to validate long-term stability of advantage shaping approach.