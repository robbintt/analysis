---
ver: rpa2
title: Representational Stability of Truth in Large Language Models
arxiv_id: '2511.19166'
source_url: https://arxiv.org/abs/2511.19166
tags:
- fictional
- 'true'
- synthetic
- noise
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models are widely used as factual sources, yet\
  \ small semantic changes can destabilize their truth judgments. We introduce representational\
  \ stability as the robustness of an LLM\u2019s truth representations to controlled\
  \ semantic perturbations."
---

# Representational Stability of Truth in Large Language Models

## Quick Facts
- arXiv ID: 2511.19166
- Source URL: https://arxiv.org/abs/2511.19166
- Reference count: 40
- Primary result: Epistemic familiarity governs LLM truth representation stability; unfamiliar synthetic statements cause up to 40% retractions while familiar fictional content remains stable (≤8.2%).

## Executive Summary
This paper introduces representational stability as a measure of how robustly LLM truth judgments withstand semantic perturbations. The authors train linear probes on LLM activations to separate true from not-true statements, then measure how the learned decision boundary shifts when relabeling certain "neither" statements as true. Across sixteen open-source models and three domains, they find that epistemically unfamiliar neither statements (synthetic entities never seen in training) induce the largest epistemic retractions—up to 40% in fragile domains like word definitions—while familiar fictional neither statements remain more stable (≤ 8.2% retractions). These results suggest that epistemic familiarity, not linguistic form, governs stability, providing a diagnostic for auditing LLM belief stability under semantic uncertainty.

## Method Summary
The authors implement the P-StaT framework by first extracting token-level activations from LLMs at layers maximizing True/Not-True separability. They train a linear sAwMIL probe to classify statements as true or not-true, then measure stability by perturbing training labels (moving subset of "neither" statements to "true") and retraining the probe. The key metric is epistemic retractions—the proportion of ground-truth true statements that lose their true classification after perturbation. They test three perturbation types: Synthetic (unfamiliar statements), Fictional (familiar fictional statements), and Fictional(T) (familiar fictional statements relabeled as true). The framework is applied across sixteen open-source LLMs (3B-15B parameters) in three domains: city locations, medical indications, and word definitions.

## Key Results
- Synthetic perturbations induce the highest rate of epistemic retractions (up to 40% in word definitions), establishing a clear perturbation hierarchy
- Familiar fictional content shows significantly higher stability than synthetic content (≤8.2% retractions vs. 40%)
- The geometry of LLM activations reflects both linguistic form and epistemic context, with fictional statements forming distinct clusters despite linguistic differences from facts
- Domain ordering for retractions reverses between representational probing (word definitions most fragile) and behavioral evaluation (city locations most fragile)

## Why This Works (Mechanism)

### Mechanism 1: Epistemic Familiarity as Stabilizer
If epistemic familiarity with "neither" content is low (e.g., Synthetic statements), the model's truth boundaries are likely to exhibit higher instability under semantic reframing. The framework posits that epistemic familiarity acts as a stabilizer for internal representations. When a "neither" statement is unfamiliar (Synthetic), it aligns closely with factual representations in activation space. When semantic assumptions change (relabeling "neither" as "true"), these unfamiliar statements induce large shifts in the decision boundary ("epistemic retractions") because they lack a distinct, robust cluster separate from "true" facts.

### Mechanism 2: Decoupling Linguistic Form and Activation Geometry
The paper argues that veracity representations encode higher-level epistemic context rather than just lexical statistics. While fictional statements (e.g., "Bikini Bottom") differ linguistically from facts, they form a distinct cluster in activation space. Synthetic statements, lacking real-world referents but appearing linguistically plausible, do not form this distinct cluster and thus destabilize the true/false boundary when relabeled. This shows that linguistic form and activation geometry are decoupled; fictional content is linguistically distinct but representationally stable, whereas synthetic content is linguistically plausible but representationally unstable.

### Mechanism 3: Probe-Behavior Correlation
The P-StaT framework links internal geometry to behavior. It assumes that if retraining a linear probe with perturbed labels causes a large rotation (change in direction) of the decision boundary, the model itself has a fragile internal organization of truth. This representational shift predicts behavioral "retractions" where the model flips its judgment. The sAwMIL probe detects instability by measuring the rotation of the decision boundary vector, which correlates with behavioral consistency in zero-shot prompting.

## Foundational Learning

- **Concept: Linear Probing (sAwMIL)**
  - Why needed here: To understand how the authors extract the "truth direction" from high-dimensional activation vectors. You must grasp that they are training a classifier on internal states, not input text.
  - Quick check question: How does the sAwMIL probe differ from a standard logistic regression classifier when handling "neither" statements?

- **Concept: Epistemic Retraction vs. Expansion**
  - Why needed here: The core metric of the paper. Retractions (belief → non-belief) are treated as a stronger signal of instability than expansions (non-belief → belief).
  - Quick check question: In the P-StaT framework, why is retracting a "true" label considered more significant than adding a new "true" label?

- **Concept: Wasserstein Distance**
  - Why needed here: Used to measure the distance between distributions of activations. The authors use this to prove that fictional and synthetic statements occupy different regions of the latent space.
  - Quick check question: What does a low Wasserstein distance between synthetic and true statements imply about the model's internal confusion?

## Architecture Onboarding

- **Component map:** Input statements → LLM activations → sAwMIL probe → True/Not-True classification → Perturbation engine → Retrained probe → Retraction/expansion metrics
- **Critical path:** 1) Extract activations for all statements from target LLM at optimal layer, 2) Train baseline probe to establish original decision boundary, 3) Apply perturbation (relabel subset of "neither" as "true") and retrain probe, 4) Measure retractions on held-out true test statements
- **Design tradeoffs:** Probe selection (sAwMIL vs. Mean Difference—sAwMIL provides stable boundaries but is more complex), label perturbation (changes semantic assumptions rather than input text to isolate representational stability)
- **Failure signatures:** High retraction rate (>30%) indicates reliance on distributional plausibility rather than stable factual grounding; boundary orthogonality suggests probe or representation is highly fragile
- **First 3 experiments:** 1) Baseline separability check—run sAwMIL probe on clean dataset to ensure layer selection yields high accuracy, 2) Synthetic perturbation test—retrain probe treating synthetic as true and measure retraction rate, 3) Ablation on layer selection—verify results hold with layers ±2 from optimal

## Open Questions the Paper Calls Out

### Open Question 1
Does the epistemic familiarity hierarchy (Synthetic > Fictional retractions) generalize to other forms of epistemic ambiguity such as disputed claims, probabilistic beliefs, or temporally evolving facts? The current study only examines fictional vs. synthetic "neither" statements, not contested truths or time-varying facts.

### Open Question 2
How does belief stability evolve during training—can fine-tuning or continued pretraining shift an LLM from unstable to stable epistemic organization? All experiments use frozen, pretrained models without examining dynamics during weight updates.

### Open Question 3
Why does domain ordering for retractions reverse between representational probing (Word Definitions highest) and behavioral evaluation (City Locations highest)? The paper observes this phenomenon but does not isolate whether it stems from baseline accuracy differences, prompt sensitivity, or fundamental representational-behavioral decoupling.

### Open Question 4
Can training interventions explicitly targeting epistemic stability reduce retraction rates while preserving factual accuracy? The paper is diagnostic, not interventional—it identifies fragility but does not propose or test remedies.

## Limitations

- The semantic perturbations (relabeling "neither" as "true") create an artificial scenario that may not reflect real-world usage patterns
- Representational stability as measured by linear probe rotation may not fully capture the model's actual reasoning process—the probe could extract spurious correlations
- Results may not generalize to commercial LLMs like GPT-4 or Claude, which were not tested and may exhibit different stability profiles

## Confidence

- **High Confidence**: The empirical observation that familiar fictional content shows higher stability than unfamiliar synthetic content across multiple domains and models
- **Medium Confidence**: The interpretation that epistemic familiarity, rather than linguistic form, governs stability—alternative explanations cannot be fully ruled out
- **Low Confidence**: The generalizability of these results to commercial LLMs and their training methodologies

## Next Checks

1. **Probe Architecture Validation**: Test whether sAwMIL probe's stability measurements correlate with alternative probing methods to confirm robustness of the stability metric itself
2. **Behavioral Correlation**: Verify that high retraction rates in probe measurements correspond to actual behavioral retractions in zero-shot prompting across the same domains
3. **Cross-Domain Transfer**: Apply P-StaT framework to a domain outside the three tested (e.g., historical facts) to determine whether the perturbation hierarchy holds across diverse knowledge domains