---
ver: rpa2
title: 'WebGames: Challenging General-Purpose Web-Browsing AI Agents'
arxiv_id: '2502.18356'
source_url: https://arxiv.org/abs/2502.18356
tags:
- arxiv
- webgames
- agents
- interaction
- challenges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The WebGames benchmark evaluates general-purpose web-browsing AI
  agents through 50+ interactive challenges that are simple for humans but systematically
  test AI limitations across browser interactions, input processing, cognitive tasks,
  workflow automation, and interactive entertainment. The framework uses a hermetic
  testing environment with verifiable ground-truth solutions and Set-of-Marks scaffolding
  to help models interact with web elements.
---

# WebGames: Challenging General-Purpose Web-Browsing AI Agents

## Quick Facts
- arXiv ID: 2502.18356
- Source URL: https://arxiv.org/abs/2502.18356
- Reference count: 40
- Primary result: AI web agents achieve 41.2% success vs human 95.7% in controlled benchmark

## Executive Summary
WebGames is a benchmark designed to evaluate general-purpose web-browsing AI agents through 50+ interactive challenges that are simple for humans but systematically test AI limitations. The framework uses a hermetic testing environment with verifiable ground-truth solutions and Set-of-Marks scaffolding to help models interact with web elements. The benchmark evaluates AI agents across five categories: browser interactions, input processing, cognitive tasks, workflow automation, and interactive entertainment.

Evaluation of leading vision-language models (GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, Qwen2-VL) against human performance revealed a substantial capability gap. Claude Computer-Use's lower performance despite expanded action space highlighted how safety constraints can impact task completion. The benchmark is publicly available at webgames.convergence.ai and provides a robust foundation for measuring progress in developing more capable web-browsing agents.

## Method Summary
The WebGames benchmark employs a controlled hermetic testing environment where AI agents interact with HTML-based challenges through a Set-of-Marks scaffolding system. This scaffolding helps models identify and interact with web elements by providing structured guidance. The benchmark includes 50+ challenges spanning five categories: browser interactions, input processing, cognitive tasks, workflow automation, and interactive entertainment. Each challenge has verifiable ground-truth solutions, enabling objective performance measurement. Human baselines are established to provide comparison metrics for AI agent performance.

## Key Results
- Leading AI vision-language models achieved only 41.2% success rate compared to human performance of 95.7%
- Claude Computer-Use underperformed despite expanded action space, highlighting safety constraint impacts
- Substantial capability gap exists between current AI agents and human web browsing abilities
- Benchmark successfully identifies systematic limitations in AI web browsing across multiple categories

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its controlled environment that isolates specific web interaction capabilities while providing verifiable ground-truth solutions. The Set-of-Marks scaffolding bridges the gap between AI model understanding and practical web element interaction, allowing for systematic evaluation of different capabilities. The HTML-based challenges, while limited in scope, provide consistent testing conditions that reveal fundamental limitations in current AI agents' ability to process and interact with web content.

## Foundational Learning
- HTML DOM structure: why needed - web agents must navigate and manipulate document structure; quick check - can model identify and interact with specific elements
- Set-of-Marks scaffolding: why needed - bridges gap between model understanding and practical interaction; quick check - does scaffolding improve interaction success rates
- Vision-language model limitations: why needed - reveals systematic weaknesses in web browsing; quick check - performance gap between models and humans
- Safety constraint impacts: why needed - identifies policy-driven limitations; quick check - does relaxing constraints improve performance
- Controlled testing environments: why needed - enables objective measurement and comparison; quick check - consistency across different models and conditions

## Architecture Onboarding

Component Map:
Set-of-Marks scaffolding -> HTML challenge environment -> Vision-language model interface -> Action execution system -> Ground-truth verification

Critical Path:
1. Model receives HTML challenge through Set-of-Marks scaffolding
2. Model processes visual and textual information
3. Model generates action sequence
4. Actions executed in hermetic environment
5. Ground-truth verification checks solution
6. Performance metrics recorded

Design Tradeoffs:
- HTML-only challenges provide consistency but may not reflect real-world complexity
- Set-of-Marks scaffolding improves interaction but may not translate to open-ended tasks
- Controlled environment enables objective measurement but may limit generalizability
- Verifiable solutions ensure accuracy but may oversimplify some real-world scenarios

Failure Signatures:
- Inability to process complex visual layouts
- Struggles with multi-step reasoning and workflow automation
- Safety constraints preventing task completion
- Limited JavaScript interaction capabilities
- Difficulty with dynamic content updates

First 3 Experiments:
1. Test model performance with and without Set-of-Marks scaffolding to measure its impact
2. Evaluate models on progressively complex HTML structures to identify capability thresholds
3. Systematically vary safety constraints to isolate their impact on task completion rates

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on HTML-based challenges, potentially excluding modern web frameworks
- Controlled environment may not fully represent real-world web complexity
- Limited testing of dynamic JavaScript interactions and state management
- Does not systematically vary safety parameters to isolate their specific impact

## Confidence
High - Performance gap findings are robust due to controlled environment and human baselines
Medium - Generalizability across different web environments needs validation
Medium - Impact of safety constraints requires further investigation through systematic parameter variation

## Next Checks
1. Conduct cross-validation using real-world websites and applications to test whether the 41.2% vs 95.7% performance gap persists outside the controlled benchmark environment

2. Test models on dynamic web applications that require handling JavaScript-based interactions, real-time content updates, and state management to assess if HTML-based limitations explain performance differences

3. Systematically vary safety constraints in Claude Computer-Use to determine the specific impact of these restrictions on task completion rates, separating architectural from policy-driven limitations