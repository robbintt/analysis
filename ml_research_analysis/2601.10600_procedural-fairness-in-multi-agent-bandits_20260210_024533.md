---
ver: rpa2
title: Procedural Fairness in Multi-Agent Bandits
arxiv_id: '2601.10600'
source_url: https://arxiv.org/abs/2601.10600
tags:
- fairness
- procedural
- each
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces procedural fairness as a new fairness objective
  in multi-agent multi-armed bandits, ensuring each agent has equal decision-making
  power. The authors formalize procedural fairness alongside outcome-based fairness
  notions like equality and utilitarian fairness.
---

# Procedural Fairness in Multi-Agent Bandits

## Quick Facts
- arXiv ID: 2601.10600
- Source URL: https://arxiv.org/abs/2601.10600
- Reference count: 40
- Key outcome: Procedural fairness introduced as new fairness objective ensuring equal decision-making power, with theoretical incompatibility proven against outcome-based fairness notions

## Executive Summary
This paper introduces procedural fairness as a distinct fairness objective for multi-agent multi-armed bandits, where each agent maintains equal decision-making power rather than focusing solely on outcome-based metrics. The authors formalize procedural fairness alongside traditional equality and utilitarian fairness notions, proving these objectives are fundamentally incompatible and require explicit normative choices. They provide learning algorithms with sublinear regret guarantees for each fairness type, showing that procedurally fair policies lie in the procedural core, ensuring stability against coalitional deviation. Empirical results demonstrate that outcome-based fairness notions sacrifice equal voice, while procedural fairness achieves a balance between efficiency and equality with minimal sacrifice in outcome-based fairness objectives.

## Method Summary
The authors formalize three distinct fairness notions in multi-agent multi-armed bandits: equality fairness (equal rewards), utilitarian fairness (maximum total reward), and procedural fairness (equal decision-making power). They characterize the procedural core of cooperative games induced by bandit problems and prove that procedurally fair policies are stable against coalitional deviation. Learning algorithms are developed for each fairness type with sublinear regret guarantees, using techniques from multi-armed bandit literature. The algorithms explicitly trade off between exploration and exploitation while maintaining the required fairness constraints, with the procedural fairness algorithm ensuring each agent has equal influence over arm selections.

## Key Results
- Procedural fairness and outcome-based fairness notions (equality and utilitarian) are proven fundamentally incompatible, requiring explicit normative choices
- Learning algorithms with sublinear regret guarantees provided for each fairness type
- Procedurally fair policies lie in the procedural core, ensuring stability against coalitional deviation
- Empirical results confirm outcome-based fairness sacrifices equal voice, while procedural fairness balances efficiency and equality with minimal sacrifice in outcome-based objectives

## Why This Works (Mechanism)
The paper establishes that fairness in multi-agent bandits requires choosing between different normative objectives, as procedural fairness (equal decision-making power) cannot be simultaneously achieved with outcome-based fairness notions. The mechanism works by explicitly constraining the learning algorithm to maintain equal agent influence while still achieving reasonable cumulative rewards. The procedural core characterization provides a stability guarantee, ensuring that no coalition of agents can improve their outcomes by deviating from the fair policy. This creates a principled framework where fairness is not an emergent property but a designed constraint that shapes the learning dynamics.

## Foundational Learning
**Multi-Armed Bandit Problem**: Sequential decision-making framework where agents repeatedly choose actions to maximize cumulative reward while learning about uncertain payoffs. Needed to understand the fundamental setting where fairness must be operationalized.

**Procedural Fairness**: Fairness notion focused on equal decision-making power rather than equal outcomes. Critical for understanding how fairness can be embedded in the learning process rather than just the results.

**Procedural Core**: Solution concept from cooperative game theory ensuring stability against coalitional deviation. Essential for understanding why procedurally fair policies are stable and sustainable.

**Sublinear Regret**: Performance metric measuring cumulative reward loss relative to optimal policy. Needed to evaluate how fairness constraints impact learning efficiency.

**Normative Choice**: Explicit decision about which fairness objective to optimize. Key concept showing that fairness requires deliberate design choices rather than automatic emergence.

## Architecture Onboarding

**Component Map**: Bandit Environment -> Fairness Objective Selection -> Learning Algorithm -> Arm Selection Policy -> Reward Observation -> Update Mechanism

**Critical Path**: Environment provides rewards → Algorithm maintains fairness constraints → Policy selects arms → Rewards update value estimates → Algorithm adjusts to maintain fairness while optimizing regret

**Design Tradeoffs**: The paper trades off between pure efficiency (utilitarian fairness) and equal participation (procedural fairness). Procedural fairness requires more complex coordination mechanisms but provides stability guarantees. Outcome-based fairness is simpler to implement but may concentrate decision power.

**Failure Signatures**: When procedural fairness fails, some agents may dominate arm selection decisions. When outcome-based fairness fails, rewards may become highly unequal. Sublinear regret guarantees failing indicates the algorithm cannot effectively balance exploration and exploitation under fairness constraints.

**First 3 Experiments**:
1. Compare cumulative rewards and fairness metrics across procedural, equality, and utilitarian fairness algorithms on standard bandit instances
2. Test stability of procedurally fair policies by measuring coalitional deviation incentives
3. Evaluate how different problem parameters (reward distributions, number of arms) affect the fairness-efficiency tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Fixed agent population assumption may not generalize to dynamic settings where agents can join or leave
- Empirical validation limited to specific bandit problem instances rather than broader multi-agent contexts
- Focus on bandit problems may not capture fairness considerations in more complex multi-agent systems
- Normative framework may need adaptation for different application domains beyond theoretical bandit settings

## Confidence
- Theoretical incompatibility of fairness notions: High
- Algorithmic regret guarantees: High
- Empirical tradeoff characterization: Medium
- Generalizability to other multi-agent contexts: Low

## Next Checks
1. Test algorithm performance and fairness properties in dynamic agent populations where agents can join or leave over time.

2. Evaluate the robustness of fairness notions under different reward distributions and problem complexities beyond standard bandit settings.

3. Implement and analyze the algorithms in a real-world multi-agent system to assess practical implications of the normative choices required by different fairness objectives.