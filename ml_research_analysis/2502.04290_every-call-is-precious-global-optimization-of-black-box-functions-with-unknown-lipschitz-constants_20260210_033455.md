---
ver: rpa2
title: 'Every Call is Precious: Global Optimization of Black-Box Functions with Unknown
  Lipschitz Constants'
arxiv_id: '2502.04290'
source_url: https://arxiv.org/abs/2502.04290
tags:
- optimization
- lipschitz
- global
- function
- constant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Every Call is Precious (ECP) introduces a global optimization\
  \ algorithm for black-box Lipschitz continuous functions with unknown Lipschitz\
  \ constants. The method eliminates the need for Lipschitz constant estimation by\
  \ using an adaptive acceptance region controlled by a growing parameter \u03B5t,\
  \ ensuring all evaluated points are potential maximizers."
---

# Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants

## Quick Facts
- arXiv ID: 2502.04290
- Source URL: https://arxiv.org/abs/2502.04290
- Reference count: 40
- Primary result: Introduces ECP algorithm achieving no-regret convergence for black-box Lipschitz functions with unknown constants, outperforming 10 benchmark methods on 30 problems.

## Executive Summary
Every Call is Precious (ECP) presents a global optimization algorithm for black-box Lipschitz continuous functions where the Lipschitz constant is unknown. The method introduces an acceptance-rejection mechanism that ensures only potentially optimal points are evaluated, eliminating the need for Lipschitz constant estimation. By adaptively controlling an acceptance region through a growing parameter ε_t, ECP balances computational efficiency with solution quality. Theoretical analysis establishes no-regret convergence for infinite evaluation budgets and minimax-optimal regret bounds for finite budgets, while empirical results demonstrate superior performance across diverse benchmark problems.

## Method Summary
ECP optimizes expensive black-box functions by sampling uniformly from the domain and evaluating only points that satisfy an acceptance condition based on the Lipschitz bound. The algorithm maintains an adaptive parameter ε_t that grows over time, preventing exponential rejection growth while ensuring all potential maximizers are eventually accessible. Points are accepted if they satisfy min_{i≤t}(f(x_i) + ε_t·||x - x_i||) ≥ max_j f(x_j), and ε_t grows either deterministically after each evaluation or stochastically when rejection spikes. This design eliminates the need for Lipschitz constant estimation while providing theoretical guarantees of no-regret convergence and minimax-optimal regret bounds.

## Key Results
- ECP achieves top performance in 13 out of 30 benchmark problems, outperforming 10 competing methods including Lipschitz, Bayesian, bandit, and evolutionary approaches
- The method provides no-regret convergence guarantees, with regret converging to zero in probability as evaluation budget approaches infinity
- Theoretical analysis establishes minimax-optimal regret bound of O(k·n^{-1/d}) for finite budgets, matching the lower bound for this problem class

## Why This Works (Mechanism)

### Mechanism 1: Acceptance Region Filtering via Lipschitz Bound Approximation
- Claim: ECP ensures all evaluated points are potential maximizers by filtering candidates through an acceptance condition that approximates the unknown Lipschitz constraint.
- Mechanism: At iteration t, a candidate point x is accepted only if: min_{i=1,...,t}(f(x_i) + ε_t · ||x - x_i||_2) ≥ max_{j=1,...,t} f(x_j). The parameter ε_t serves as a proxy Lipschitz constant. When ε_t ≤ k (true Lipschitz constant), Proposition 2 proves A_{ε_t,t} ⊆ P_{k,t}—the acceptance region is a subset of potential maximizers, guaranteeing no unpromising evaluations.
- Core assumption: The function f is Lipschitz continuous with some finite (unknown) constant k.
- Evidence anchors:
  - [abstract]: "ECP minimizes unpromising evaluations by strategically focusing on potentially optimal regions."
  - [Section 4, Definition 2]: Formal acceptance region definition with the inequality condition.
  - [Section 5.1, Proposition 2]: "∀ε_t ≤ k, A_{ε_t,t} ⊆ P_{k,t}, and ∀ε_t > k, P_{k,t} ⊆ A_{ε_t,t}"
  - [corpus]: Related work (ECPv2) validates this framework's scalability; corpus evidence is otherwise limited for this specific mechanism.
- Break condition: If the function is not Lipschitz continuous, the acceptance region guarantees dissolve.

### Mechanism 2: Adaptive ε_t Growth Balances Exploration and Computational Tractability
- Claim: A growing sequence of ε_t values prevents exponential rejection growth while maintaining evaluation quality.
- Mechanism: ε_t grows through two pathways: (1) deterministically after each evaluation via ε_{t+1} ← τ_{n,d} · ε_t, and (2) stochastically when rejection count spikes (h_{t+1} - h_t > C). Lemma 1 shows acceptance regions are non-decreasing with ε_t. Proposition 5 proves the growth condition ensures h_{t+1} ≤ (t+1)·C, preventing super-linear rejection. Corollary 1 establishes that after finite increases v (independent of t), acceptance probability reaches ≥1/2.
- Core assumption: τ_{n,d} > 1 ensures geometric growth; C > 1 provides patience before expansion.
- Evidence anchors:
  - [Section 4, lines 5-6]: Growth condition implementation in Algorithm 1.
  - [Section 5.2, Proposition 5]: "ht+1 ≤ (t + 1) · C. Otherwise, ε_t grows and ht+1 is reset to zero."
  - [Section 5.2, Corollary 1]: Bound on maximum increases needed for likely acceptance.
  - [corpus]: Sparse—corpus does not discuss this specific growth mechanism.
- Break condition: If τ_{n,d} ≤ 1 or C is too small, rejection may grow exponentially, causing computational failure.

### Mechanism 3: No-Regret via Hitting Time Guarantee
- Claim: ECP achieves no-regret because ε_t eventually exceeds the true Lipschitz constant k, at which point all potential maximizers become accessible.
- Mechanism: Lemma 2 bounds the hitting time: i* ≤ max(⌈log_{τ_{n,d}}(k/ε_1)⌉, 1). After iteration i*, Proposition 2 guarantees P_{k,t} ⊆ A_{ε_t,t}, meaning no potential maximizer is excluded. Combined with uniform sampling over the acceptance region, this recovers the minimax optimal rate O(k·n^{-1/d}) per Theorem 3.
- Core assumption: Budget n is sufficient for t to reach i*; function is non-constant.
- Evidence anchors:
  - [Section 5.3, Theorem 2]: "RECP,f(n) \xrightarrow{p} 0" (no-regret statement).
  - [Section 5.3, Lemma 2]: Hitting time upper bound.
  - [Section 5.3, Theorem 3]: "RECP,f(n) ≤ diam(X) · (i*)^{1/d} · k · (ln(1/δ)/n)^{1/d}"
  - [corpus]: Consistent with Lipschitz optimization theory (Bull 2011 lower bound cited), but corpus lacks independent validation of ECP's specific regret bound.
- Break condition: If budget n < i*, the no-regret guarantee may not activate; performance approaches pure random search for very small budgets.

## Foundational Learning

- **Concept: Lipschitz Continuity**
  - Why needed here: The entire ECP framework relies on the function having bounded rate of change (Lipschitz constant k). Without this, the acceptance condition has no theoretical justification.
  - Quick check question: Given f(x) = sin(x), is there a finite k such that |f(x) - f(x')| ≤ k·|x - x'| for all x, x'? What is the smallest such k?

- **Concept: Acceptance-Rejection Sampling**
  - Why needed here: ECP samples uniformly from X but only evaluates points meeting the acceptance condition. Understanding how rejection probability affects computational complexity is critical.
  - Quick check question: If the acceptance region occupies 10% of X, how many expected samples are needed to accept one point?

- **Concept: Regret and No-Regret in Optimization**
  - Why needed here: Theoretical guarantees are expressed via simple regret: R_A,f(n) = max f(x) - max_{i=1,...,n} f(x_i). No-regret means this converges to 0 in probability.
  - Quick check question: If an algorithm achieves R(n) = O(n^{-1/2}), is it no-regret? What about O(1)?

## Architecture Onboarding

- **Component map**: Sampler -> Acceptance Oracle -> (if accepted) f-evaluation -> ε_t update -> Best-Point update
- **Critical path**: Sampler → Acceptance Oracle → (if accepted) f-evaluation → ε_t update → Best-Point update. The Acceptance Oracle is called repeatedly (potentially many times) per single f-evaluation.
- **Design tradeoffs**:
  - Small ε_1 → tighter filtering, better evaluation quality, higher computational cost (Theorem 1)
  - Large τ_{n,d} → faster ε_t growth, lower rejection, approaches pure random search (Remark 2)
  - Large C → more patience before ε_t growth, better quality at cost of speed (Proposition 5)
  - Default values from paper: ε_1 = 10^{-2}, τ_{n,d} = max(1 + 1/(nd), 1.001), C = 1000

- **Failure signatures**:
  - Infinite loop / timeout: Rejection rate too high; likely ε_1 too small or τ_{n,d} too close to 1
  - Performance degrades to random search: ε_1 or τ_{n,d} too large
  - High variance across runs: C too small (ε_t grows too aggressively early)
  - Algorithm 1 explicitly avoids AdaLIPO's infinite loop problem via the growth condition

- **First 3 experiments**:
  1. **Sanity check on known function**: Optimize f(x) = -x^2 on X = [-1, 1] with n=20, k=2 known. Verify ECP with ε_1 = k, τ_{n,d} = 1 recovers LIPO behavior (Remark 2). Expected: all points should be accepted, matching grid search quality.
  2. **Ablation on hyperparameters**: Run ECP on Ackley 2D (n=50) with ε_1 ∈ {10^{-4}, 10^{-2}, 10^{-1}} and C ∈ {10, 100, 1000}. Plot best-found value vs. wall-clock time. Expected from ablation study (Figures 5, 7): smaller ε_1 and larger C improve quality but increase time.
  3. **Comparison with AdaLIPO**: Run both on Bukin 2D (n=50, 100). AdaLIPO uses p=0.1 exploration probability; ECP uses defaults. Monitor: (a) best value found, (b) number of rejected samples. Expected: ECP should achieve better values in small budgets by avoiding uniform exploration overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ECP decision rule be effectively generalized to functions characterized by semi-metrics rather than global Lipschitz constants?
- Basis: [explicit] Remark 1 suggests the framework can be generalized to functions satisfying $f(x^*) - f(x) \leq \ell(x^*, x)$ where $\ell$ is a semi-metric defining local smoothness.
- Why unresolved: The current theoretical guarantees and acceptance regions rely strictly on the definition of global Lipschitz continuity.
- Evidence: Theoretical proof of convergence and regret bounds for the semi-metric variant, along with empirical validation on locally smooth functions.

### Open Question 2
- Question: How can ECP be adapted to maintain performance when the objective function is stochastic rather than deterministic?
- Basis: [inferred] The problem statement in Section 3 explicitly assumes $f$ is deterministic, ignoring the noise prevalent in many real-world expensive evaluations (e.g., reinforcement learning).
- Why unresolved: The acceptance condition relies on strict inequalities involving observed values; noise could lead to the incorrect rejection of potential maximizers.
- Evidence: A modification of the algorithm that achieves no-regret performance on benchmarks with additive Gaussian noise.

### Open Question 3
- Question: Is there a theoretical basis for adaptively selecting the hyperparameters $\epsilon_1$ and $\tau_{n,d}$ to optimize the trade-off between rejection rate and output quality?
- Basis: [inferred] Appendix E discusses the sensitivity of the algorithm to these parameters and notes that smaller values improve output but increase computational complexity, yet fixed values were used for all experiments.
- Why unresolved: The current analysis requires the user to heuristically select constants without a mechanism to adjust them based on observed rejection rates during the search.
- Evidence: A dynamic adjustment strategy that outperforms the fixed-parameter baseline while maintaining the no-regret guarantee.

## Limitations

- The core theoretical guarantees depend on the assumption of Lipschitz continuity, which may not hold for many real-world black-box functions with discontinuities or non-Lipschitz behavior
- Computational overhead from repeated acceptance checks could become prohibitive in high dimensions despite theoretical complexity bounds
- Empirical comparisons are limited to synthetic benchmarks and simple regression tasks, lacking validation on complex real-world optimization problems with constraints

## Confidence

- **High confidence** in Mechanism 1 (acceptance region filtering): Theoretical proofs are rigorous and well-grounded in Lipschitz optimization literature
- **Medium confidence** in Mechanism 2 (adaptive ε_t growth): The growth conditions prevent exponential rejection, but practical performance depends heavily on hyperparameter choices that are problem-dependent
- **Medium confidence** in Mechanism 3 (no-regret guarantee): The theoretical bounds are sound, but Corollary 1's practical implications are asymptotic and may not hold for finite, realistic budgets

## Next Checks

1. **Domain-specific validation**: Test ECP on real-world black-box optimization problems with known Lipschitz properties (e.g., physical experiments, control systems) to validate practical performance beyond synthetic benchmarks

2. **High-dimensional scaling study**: Evaluate ECP on problems with d > 100 to empirically verify the claimed O(nd log(n)/δ) computational complexity and identify breaking points

3. **Robustness to non-Lipschitz functions**: Systematically test ECP on functions with discontinuous gradients or non-Lipschitz behavior to quantify performance degradation and identify failure modes