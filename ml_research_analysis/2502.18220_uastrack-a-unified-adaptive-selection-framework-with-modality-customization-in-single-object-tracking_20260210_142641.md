---
ver: rpa2
title: 'UASTrack: A Unified Adaptive Selection Framework with Modality-Customization
  in Single Object Tracking'
arxiv_id: '2502.18220'
source_url: https://arxiv.org/abs/2502.18220
tags:
- tracking
- multi-modal
- modality
- adapter
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of single object tracking across
  multiple modalities, including RGB, thermal (T), event (E), and depth (D), which
  each offer unique advantages in overcoming challenges like illumination variations,
  occlusion, and motion blur. Existing unified multi-modal tracking systems either
  require separate models or sets of parameters for each modality or lack the ability
  to adaptively distinguish and process different modalities during inference.
---

# UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking

## Quick Facts
- arXiv ID: 2502.18220
- Source URL: https://arxiv.org/abs/2502.18220
- Authors: He Wang; Tianyang Xu; Zhangyong Tang; Xiao-Jun Wu; Josef Kittler
- Reference count: 40
- One-line primary result: Proposes UASTrack, achieving 8.5% SR improvement on LasHeR benchmark with only 1.87M extra parameters and 1.95G FLOPs

## Executive Summary
This paper introduces UASTrack, a unified adaptive selection framework for single object tracking across multiple modalities (RGB, thermal, event, and depth). The framework addresses limitations of existing unified multi-modal trackers that either require separate models per modality or lack adaptive processing capabilities. UASTrack features a Discriminative Auto-Selector (DAS) that dynamically identifies input modality and activates corresponding processing branches, along with a Task-Customized Optimization Adapter (TCOA) that applies modality-specific refinements to enhance feature extraction and filter noise.

The proposed system demonstrates state-of-the-art performance across five benchmarks covering RGB-T, RGB-E, and RGB-D tracking scenarios. It achieves significant improvements in tracking accuracy while maintaining computational efficiency with minimal additional parameters and FLOPs compared to RGB-only baselines. The framework's ability to operate without prior knowledge of input types makes it particularly suitable for real-world applications where modality may vary or be unknown during inference.

## Method Summary
UASTrack introduces a unified adaptive framework for single object tracking across multiple modalities through two key components: the Discriminative Auto-Selector (DAS) and the Task-Customized Optimization Adapter (TCOA). The DAS dynamically identifies the input modality at inference time and activates the corresponding processing branch without requiring prior knowledge of modality type. The TCOA then applies modality-specific refinements to filter noise and enhance feature extraction based on the unique characteristics of each input type. This architecture enables the system to handle RGB, thermal, event, and depth modalities while maintaining computational efficiency through shared parameters and selective activation. The framework demonstrates strong performance across diverse tracking scenarios while adding only minimal computational overhead compared to RGB-only baselines.

## Key Results
- Achieves 8.5% improvement in Success Rate on LasHeR benchmark
- Introduces only 1.87M additional training parameters and 1.95G FLOPs compared to RGB-X baseline
- Outperforms existing unified trackers in accuracy, speed, and computational efficiency across five benchmarks (LasHeR, RGBT234, GTOT, VisEvent, and DepthTrack)
- Demonstrates state-of-the-art performance across RGB-T, RGB-E, and RGB-D tracking scenarios

## Why This Works (Mechanism)
The framework's success stems from its ability to adaptively process different modalities through dynamic modality identification and customized optimization. The DAS component enables the system to recognize input modality in real-time and activate appropriate processing branches, eliminating the need for modality-specific models. The TCOA then applies targeted refinements that address modality-specific challenges - for instance, enhancing thermal modality features for better illumination invariance or processing event data to handle motion blur. This selective and customized approach allows the framework to leverage the unique strengths of each modality while maintaining a unified architecture, resulting in improved tracking performance without proportional increases in computational complexity.

## Foundational Learning

**Modality-adaptive perception**: The ability to process different data types (RGB, thermal, event, depth) with customized approaches. Needed because each modality has distinct characteristics and challenges. Quick check: Verify that each modality's unique features are properly identified and processed.

**Dynamic modality identification**: Real-time recognition of input type without prior knowledge. Essential for creating a truly unified system that can handle varying inputs. Quick check: Test system's accuracy in identifying modalities across diverse scenarios.

**Task-customized optimization**: Applying specific refinements based on the tracking task and input characteristics. Required to maximize the advantages of each modality while mitigating their weaknesses. Quick check: Measure performance improvements from modality-specific optimizations.

**Computational efficiency in multi-modal systems**: Maintaining low parameter count and FLOPs while handling multiple modalities. Critical for practical deployment in resource-constrained environments. Quick check: Compare resource usage against baseline and alternative approaches.

**Unified architecture design**: Creating a single framework that can process multiple modalities through shared and specialized components. Necessary to avoid the complexity and redundancy of separate models. Quick check: Verify that shared components effectively serve all modalities.

## Architecture Onboarding

**Component map**: Input -> DAS -> Modality-specific Branch -> TCOA -> Output
**Critical path**: Sensor input → DAS classification → Branch activation → TCOA refinement → Tracking prediction
**Design tradeoffs**: The framework balances between shared parameters for efficiency and modality-specific components for accuracy. The modular DAS and TCOA design allows for scalability but may introduce complexity in coordinating between components.
**Failure signatures**: Performance degradation may occur when: 1) DAS incorrectly identifies modality, 2) TCOA fails to properly filter noise for specific modalities, 3) Modality-specific branches conflict when multiple inputs are present, 4) Computational overhead becomes prohibitive on constrained hardware
**First experiments**:
1. Test DAS accuracy across all supported modalities under varying conditions
2. Evaluate TCOA's noise filtering effectiveness for each modality type
3. Measure computational overhead and real-time performance across different hardware configurations

## Open Questions the Paper Calls Out

None identified in the provided content.

## Limitations

- Framework generalization to unseen modality combinations remains unverified
- Lack of comprehensive ablation studies isolating individual component contributions
- Performance on extremely challenging scenarios (heavy occlusion, rapid motion) not thoroughly examined
- Real-time performance analysis across diverse hardware configurations is incomplete

## Confidence

| Claim | Confidence |
|-------|------------|
| 8.5% SR improvement on LasHeR | Medium |
| State-of-the-art performance across five benchmarks | Medium |
| Computational efficiency with minimal additional parameters/FLOPs | Medium |
| Generalization to unseen modalities | Low |
| Real-time performance across hardware configurations | Low |

## Next Checks

1. Conduct extensive ablation studies to quantify the individual contributions of DAS and TCOA components to overall performance gains
2. Test framework generalization on additional modality combinations (e.g., RGB-T-E-D fusion) not seen during training
3. Evaluate real-time performance across diverse hardware platforms and compare against established computational efficiency benchmarks