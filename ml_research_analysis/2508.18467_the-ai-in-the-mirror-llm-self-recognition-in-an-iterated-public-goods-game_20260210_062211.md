---
ver: rpa2
title: 'The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game'
arxiv_id: '2508.18467'
source_url: https://arxiv.org/abs/2508.18467
tags:
- points
- round
- each
- game
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether informing large language models
  (LLMs) that they are playing against themselves versus another AI agent affects
  their cooperation in an iterated public goods game. The authors conducted three
  studies using GPT-4o, Claude Sonnet 4, Llama 4 Maverick, and Qwen3 across "no-name"
  and "name" conditions with various system prompts emphasizing collective welfare,
  neutrality, or self-interest.
---

# The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game

## Quick Facts
- **arXiv ID**: 2508.18467
- **Source URL**: https://arxiv.org/abs/2508.18467
- **Reference count**: 40
- **Primary result**: Telling LLMs they're playing against themselves significantly changes cooperation in iterated public goods games

## Executive Summary
This paper investigates whether informing large language models that they are playing against themselves versus another AI agent affects their cooperation in an iterated public goods game. Across three studies using GPT-4o, Claude Sonnet 4, Llama 4 Maverick, and Qwen3, the authors found that identity disclosure produces statistically significant changes in cooperative behavior. The effect varied by system prompt type, with collective prompts leading to more defection against self, while selfish prompts led to more cooperation. The findings suggest that identity cues can influence LLM cooperation in multi-agent settings, with implications for autonomous system design.

## Method Summary
The study used four LLMs (GPT-4o, Claude Sonnet 4, Llama 4 Maverick, Qwen3 235B A22B) in iterated public goods games with 20 rounds, 10 points per round, and a 1.6 multiplier. Three system prompts (collective, neutral, selfish) were tested in both "name" and "no-name" conditions across three studies: 2-player games with 100 games per prompt pair (Studies 1-2), and 4-player games with 50 games per prompt (Study 3). Temperature was set to 1.0, and models output contributions via JSON. Reasoning traces were collected in Study 1 but not in Study 2.

## Key Results
- Telling models they are playing against themselves leads to statistically significant changes in cooperation (5-6 out of 9 prompt pairings in Studies 1-2, 7 out of 12 in Study 3)
- Models with "collective" prompts contributed less when facing self, while "selfish" prompts led to more cooperation
- The effect persisted even with less information about others' decisions in four-player setups
- Only 5-6 of 9 prompt pairings showed significance in Studies 1-2; 7 of 12 in Study 3
- GPT-4o in Study 3 showed no statistically significant differences

## Why This Works (Mechanism)

### Mechanism 1: Identity Disclosure as Behavioral Modifier
- Claim: Informing LLMs they are playing against themselves (vs. "another AI agent") produces statistically significant changes in cooperative behavior.
- Mechanism: The model name acts as a signal that triggers latent representations associated with self-recognition or capability estimation. When told they face "GPT-4o" (while being GPT-4o), models adjust contributions—suggesting internal representations of self/other distinction influence decision-making.
- Core assumption: The model name activates semantic associations beyond surface-level text processing, potentially engaging learned patterns about capability matching or strategic symmetry.
- Evidence anchors: Telling LLMs that they are playing against themselves leads to statistically significant changes in cooperation; telling all models that they are playing against themselves will result in a statistically significant difference – at most, 4 points – in their contributions; related work shows LLMs can self-distinguish outputs.
- Break condition: If models are randomly assigned fake names unrelated to their identity, and behavior still shifts identically, then identity-specific representations are not the mechanism—mere framing effects would suffice.

### Mechanism 2: Prompt-Identity Interaction Reversal
- Claim: The direction of behavioral change depends on system prompt type: "collective" prompts lead to more defection when facing self, while "selfish" prompts lead to more cooperation.
- Mechanism: Models prompted for collective welfare may anticipate defection from a similarly-capable opponent (self-knowledge of strategic reasoning), leading to preemptive defection. Conversely, selfish-prompted models may recognize symmetric selfish opponents and conclude mutual cooperation is preferable to mutual defection in repeated games.
- Core assumption: Models possess implicit game-theoretic reasoning that differentiates strategic implications of facing similar vs. dissimilar opponents.
- Evidence anchors: Models assigned the "collective" prompt tended to defect more when told they were playing against themselves, while those assigned the "selfish" prompt tended to cooperate more; within the named condition, models would occasionally mention that their opponent possesses similar reasoning capabilities.
- Break condition: If this reversal disappears when models are explicitly told their opponent uses identical reasoning but is a different model, then self-recognition per se is not required—capability symmetry alone explains the effect.

### Mechanism 3: Reasoning Trace Divergence from Observed Behavior
- Claim: Explicit reasoning traces do not reliably reveal why models change behavior; models rarely verbalize awareness of playing against themselves.
- Mechanism: The behavioral shift occurs through implicit associations rather than deliberate verbal reasoning. Claude Sonnet 4's 53 mentions of "human" and 125 mentions of "reminder" in Study 1 suggest skepticism about game configuration, but most models did not articulate self-recognition.
- Core assumption: Chain-of-thought outputs are post-hoc rationalizations rather than faithful reports of decision processes.
- Evidence anchors: Models have seldom explicitly voiced what they thought about playing against themselves; reasoning traces are not reliable proxies for explaining behavior. LLM explanations have low precision; reasoning models don't always verbalize their decision process.
- Break condition: If forcing models to output decision trees or formal game-theoretic calculations eliminates the name/no-name difference, then implicit associations are bypassable—otherwise, the effect persists through non-verbalized pathways.

## Foundational Learning

- **Iterated Public Goods Game**
  - Why needed here: The core experimental paradigm; understanding Nash equilibrium (contribute 0 when multiplier < N) explains why cooperation is surprising and measurable.
  - Quick check question: With multiplier 1.6 and 2 players, why is contributing 0 the game-theoretic equilibrium?

- **System Prompts as Behavioral Priors**
  - Why needed here: The study uses "collective/neutral/selfish" prompts to establish baselines; these are the intervention points that interact with identity disclosure.
  - Quick check question: If you change "Prioritize collective welfare" to "Prioritize group success," would you expect identical results?

- **Self-Recognition vs. Self-Cognition in LLMs**
  - Why needed here: The paper distinguishes recognizing one's outputs (Panickssery et al.) from knowing one's identity (Davidson et al.); the mechanism likely involves the latter.
  - Quick check question: If a model cannot correctly answer "What model are you?", would you expect the name condition to still affect its behavior?

## Architecture Onboarding

- Component map:
  - Game engine -> Model wrapper -> Condition assigner -> Analysis layer

- Critical path:
  1. Load system prompt (collective/neutral/selfish × no-name/name)
  2. Initialize game state (0 points, round 1)
  3. For each round: send prompt + history → model → extract contribution → update state → append to context
  4. After 20 rounds, log final contributions
  5. Aggregate across n=100 games per condition pair

- Design tradeoffs:
  - Deception: Models were told they played against themselves but were called separately (no true interaction). This isolates framing effects but sacrifices ecological validity.
  - Temperature=1.0: High stochasticity increases variance; n=100 games needed for significance.
  - Study 2 removed reasoning prompts: Cleaner contributions but lost interpretability data.

- Failure signatures:
  - Llama 4 Maverick showed inconsistent patterns across conditions (no clear prompt-identity relationship).
  - Only 5-6 of 9 prompt pairings were significant in Studies 1-2; only 7 of 12 in Study 3.
  - GPT-4o in Study 3 showed no statistically significant differences.
  - First-round differences appear before any game history, suggesting prompt-only effects.

- First 3 experiments:
  1. **Control for capability symmetry**: Tell GPT-4o it plays against "Claude Sonnet 4" (truthful) vs. "GPT-4o" (deceptive). Isolates identity-specific effects from mere capability signaling.
  2. **Blind name assignment**: Randomly assign models fake names ("Model A") that are either consistent or inconsistent across rounds. Tests whether consistency of identity matters.
  3. **Human comparison condition**: Tell models they play against "a human" vs. "another AI." Prior work suggests different behavior; this establishes whether self-recognition is distinct from agent-type recognition.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do models change their cooperative behavior when told they are playing against themselves versus "another AI agent"?
  - Basis in paper: Since models don't frequently allude to playing against themselves, we are still uncertain why we observed a pronounced difference between the 'no-name' and 'name' conditions.
  - Why unresolved: Models seldom explicitly reference self-play in their reasoning traces, leaving the cognitive mechanism unclear.
  - What evidence would resolve it: Mechanistic interpretability studies analyzing attention patterns and internal activations during decision-making; ablation studies targeting self-representation components.

- **Open Question 2**: Do these findings generalize to settings where models play against humans rather than other AI agents?
  - Basis in paper: Additionally, it would be interesting to tell models that they are playing against humans and see if there would be any notable differences.
  - Why unresolved: The study only tested AI-AI interactions; human-AI dynamics may elicit different cooperative behaviors.
  - What evidence would resolve it: Replicating Studies 1-3 with human opponents, comparing contribution patterns across identity disclosure conditions.

- **Open Question 3**: How do these dynamics change in multi-agent systems with direct communication between agents?
  - Basis in paper: Future work should test similar 'no-name' vs. 'name' conditions in settings where agents are able to converse with each other.
  - Why unresolved: Models in this study played without direct communication; conversational agents may exhibit different identity-based behaviors.
  - What evidence would resolve it: Running similar experiments in conversational multi-agent frameworks like Curvo's simulation that measures deception, trust-formation, and strategic communication.

- **Open Question 4**: Do these findings generalize across a broader range of LLM architectures and sizes?
  - Basis in paper: We invite future work to test different models and see if our discoveries generalize to all models.
  - Why unresolved: Only four models were tested, with inconsistent results (e.g., Llama 4 behaved differently from others).
  - What evidence would resolve it: Systematic testing across diverse model families, sizes, and training paradigms to identify which architectural factors predict identity-sensitive behavior.

## Limitations

- The underlying mechanism by which identity disclosure changes behavior remains unclear, with minimal explicit self-recognition in reasoning traces
- Effect sizes are modest, with only 5-6 of 9 prompt pairings showing significance in Studies 1-2 and only 7 of 12 in Study 3
- Inconsistent patterns across models, with Llama 4 Maverick showing different behavior and GPT-4o showing no significant differences in Study 3

## Confidence

- **High Confidence**: The basic experimental design and methodology are sound; the statistical approach (95% CI non-overlap) is appropriate for detecting differences between conditions.
- **Medium Confidence**: The observed behavioral differences between name and no-name conditions are real but the effect sizes are modest and not consistent across all models and prompt pairings.
- **Low Confidence**: The mechanism by which identity disclosure changes behavior is speculative. The reversal pattern (collective prompts leading to more defection, selfish prompts to more cooperation) lacks direct support and may reflect noise or model-specific quirks rather than a generalizable principle.

## Next Checks

1. **Control for capability symmetry**: Tell GPT-4o it plays against "Claude Sonnet 4" (truthful) vs. "GPT-4o" (deceptive). Isolates identity-specific effects from mere capability signaling.

2. **Blind name assignment**: Randomly assign models fake names ("Model A") that are either consistent or inconsistent across rounds. Tests whether consistency of identity matters, not the specific name.

3. **Human comparison condition**: Tell models they play against "a human" vs. "another AI." Prior work suggests different behavior; this establishes whether self-recognition is distinct from agent-type recognition.