---
ver: rpa2
title: 'CREME: Robustness Enhancement of Code LLMs via Layer-Aware Model Editing'
arxiv_id: '2507.16407'
source_url: https://arxiv.org/abs/2507.16407
tags:
- robustness
- arxiv
- editing
- layer
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CREME improves code LLM robustness to natural language prompt perturbations
  through targeted parameter updates at identified key layers. It first locates robustness-sensitive
  layers via causal tracing by comparing hidden states between original and perturbed
  prompts.
---

# CREME: Robustness Enhancement of Code LLMs via Layer-Aware Model Editing

## Quick Facts
- arXiv ID: 2507.16407
- Source URL: https://arxiv.org/abs/2507.16407
- Reference count: 40
- Primary result: 63% improvement in pass@1 accuracy on perturbed prompts while maintaining <1% deviation on clean inputs

## Executive Summary
CREME enhances code LLM robustness to natural language prompt perturbations through targeted parameter updates at identified key layers. The method first locates robustness-sensitive layers via causal tracing by comparing hidden states between original and perturbed prompts. It then edits the MLP output weights at this layer to align representations while preserving clean input performance. On HumanEval and MBPP benchmarks, CREME achieves 63% improvement in pass@1 accuracy on perturbed prompts with less than 1% deviation on clean inputs. The approach generalizes across 20 perturbation types and requires only a single prompt pair for intervention, making it practical for real-world deployment.

## Method Summary
CREME operates in two phases: layer localization and targeted editing. First, it identifies the robustness-sensitive layer by running causal tracing - comparing hidden states between original and perturbed prompts at each layer and measuring restoration improvement. The layer showing maximum restoration improvement becomes the key layer. Second, CREME performs lightweight parameter editing by optimizing a combined loss function that minimizes MSE between perturbed and original hidden states while preserving performance on the original prompt. The method updates only the MLP output weights at the key layer using gradient-based optimization with early stopping, requiring only a single prompt pair for intervention.

## Key Results
- Achieves 63% improvement in pass@1 accuracy on perturbed prompts on HumanEval and MBPP benchmarks
- Maintains less than 1% deviation in performance on clean inputs
- Generalizes across 20 perturbation types with average 30% G-RIR (Generalized Relative Improvement Ratio)
- Ablation studies show 20.4% performance degradation when layer localization is removed and 12.8% reduction without preservation loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Identifying the robustness-sensitive layer via causal tracing is the most critical step for effective editing.
- **Mechanism:** By running a "layer-wise intervention" (replacing perturbed hidden states with original states at each layer), the method isolates the single layer where this intervention most effectively restores the original output's functional correctness (highest restoration improvement). This layer is deemed the "key layer" where the perturbation's negative impact is most concentrated.
- **Core assumption:** The model's failure on a perturbed prompt is localized to one specific transformer layer, and correcting the representation at this layer will cascade positively through subsequent layers.
- **Evidence anchors:**
  - [abstract]: "CREME first identifies robustness-sensitive layers by comparing hidden states between an original prompt and its perturbed variant."
  - [section]: "Among the ablation variants, removing the layer localization module results in the greatest performance degradation (20.4% on average), underscoring the critical importance of locating the key layer related to robustness" (Section 5.3, RQ3).
  - [corpus]: A related paper, "Layer-Aware Task Arithmetic," also confirms the importance of layer-aware interventions, stating that they help "disentangle task-specific and instruction-following knowledge" in LLMs.

### Mechanism 2
- **Claim:** Targeted gradient-based editing of the MLP output weights ($W_V^\ell$) at the key layer aligns the perturbed prompt's representation with the original, restoring robustness.
- **Mechanism:** The editing process optimizes a loss function that minimizes the Mean Squared Error (MSE) between the hidden states of the original and perturbed prompts at the key layer (alignment loss), while simultaneously penalizing changes to the model's behavior on the original prompt (preservation loss). This updates only the MLP output weights at the key layer.
- **Core assumption:** Aligning the internal hidden state representations at a critical layer is sufficient to align the final output behavior, even without direct supervision on the final code output.
- **Evidence anchors:**
  - [abstract]: "Then, it performs lightweight parameter editing at the identified layer to reduce performance degradation."
  - [section]: "We define the total loss for the editing procedure as: $L_{total} = MSE(h_{pert}^{\ell*}, h_{orig}^{\ell*}) + \lambda \cdot MSE(h_{orig-new}^{\ell*}, h_{orig}^{\ell*})$" (Section 3.3). The ablation study confirms removing the preservation loss causes a 12.8% performance reduction (Section 5.3).

### Mechanism 3
- **Claim:** A single edited example generalizes to other prompts with the same perturbation type.
- **Mechanism:** The method generalizes because it edits the model's internal mechanism for handling a *type* of transformation (e.g., typos) rather than memorizing a correction for a *specific* prompt instance. By adjusting the weights at a "robustness-sensitive" layer, the model learns a more invariant representation for that class of perturbation.
- **Core assumption:** The identified key layer and the required weight adjustment are similar across different prompts that undergo the same type of perturbation (e.g., all prompts with character swaps are handled similarly in a specific layer).
- **Evidence anchors:**
  - [abstract]: "The approach requires only a single prompt pair for intervention, making it practical for real-world deployment."
  - [section]: Results show that editing on one example yields a high Generalized Relative Improvement Ratio (G-RIR) on other examples of the same perturbation type (avg 30% G-RIR). (Section 5.2, RQ2, Finding 1).

## Foundational Learning

- **Concept:** **Causal Tracing / Causal Intervention**
  - **Why needed here:** This is the core diagnostic tool used to identify the "key layer" responsible for robustness failure. Without understanding how to intervene in a model's forward pass and measure the effect, one cannot implement the first step of CREME.
  - **Quick check question:** If you intervene at layer L and the model's output changes from "incorrect" to "correct," what does that imply about layer L's role?

- **Concept:** **Hidden State Representations in Transformers**
  - **Why needed here:** The entire method operates on the continuous vector representations (hidden states) at intermediate transformer layers. Understanding that these states encode semantic information and can be compared using metrics like MSE is fundamental.
  - **Quick check question:** What does it mean if two semantically similar prompts produce very different hidden states at a certain layer?

- **Concept:** **Overfitting vs. Generalization in Model Editing**
  - **Why needed here:** A core goal of CREME is to generalize from a single edit. The preservation loss is explicitly designed to prevent overfitting (catastrophic forgetting) to the specific edit instance, which is a critical constraint.
  - **Quick check question:** Why might editing a model to perfectly fix one perturbed prompt cause it to fail on other, similar prompts?

## Architecture Onboarding

- **Component map:** Forward Pass -> Causal Tracer -> Key Layer Selector -> Model Editor -> Deployment

- **Critical path:**
  1.  **Data Ingestion:** Take a pair (original prompt, perturbed prompt).
  2.  **Localization:** Run the Causal Tracer to compute restoration improvement for all layers.
  3.  **Selection:** Identify the single key layer $\ell^*$.
  4.  **Editing:** Initialize optimizer. For T steps, compute the loss on the prompt pair, backpropagate, and update $W_V^{\ell*}$ *only*. Use early stopping.
  5.  **Deployment:** The final model has updated weights at $\ell^*$ and is used for inference.

- **Design tradeoffs:**
  - **Localization Overhead:** The causal tracing step is computationally expensive. The paper acknowledges this and suggests a lightweight variant: pre-identifying common key layers and skipping full tracing. This trades some precision for speed.
  - **Single-Example Editing:** Using only one prompt pair is efficient but may not capture the full variance of a perturbation type. Using more examples might increase robustness but would require more complex batch editing logic.
  - **Edit Target:** Editing only the MLP output weights is a strong constraint. It simplifies the update but assumes this component is solely responsible for the robustness failure.

- **Failure signatures:**
  - **High Deviation on Clean Inputs:** If the preservation loss is too weak ($\lambda$ too low), the model's performance on unperturbed prompts will drop significantly.
  - **Negative G-RIR:** This indicates that the edit has overfitted to the specific training example and degraded performance on other examples of the same perturbation type.
  - **Stable-but-Low Performance:** If causal tracing fails to identify a good key layer, the editing may converge but yield little to no improvement in Pass@1 on perturbed prompts.

- **First 3 experiments:**
  1.  **Reproduce Ablation Study:** Implement CREME and then run it with the "w/o Layer Localization" setting (randomly selecting a layer). Confirm that performance drops dramatically, validating the paper's primary claim.
  2.  **Hyperparameter Sensitivity on Lambda:** Run a sweep of the $\lambda$ parameter for the preservation loss. The paper suggests 0.1 is optimal, but test its sensitivity on a different dataset or model not used in the paper.
  3.  **Cross-Category Generalization Test:** Edit the model using only one perturbation type (e.g., C3) and evaluate its G-RIR on a completely different, unseen type (e.g., P1 Paraphrasing) to test the limits of the claimed generalization.

## Open Questions the Paper Calls Out
- Can CREME generalize to broader natural language task domains beyond code generation and maintain effectiveness against aggressive adversarial perturbations?
- Does the localization of robustness-sensitive layers scale to significantly larger or commercial models (e.g., >70B parameters) without prohibitively high computational costs?
- Does applying CREME sequentially for multiple perturbation types cause interference or catastrophic forgetting of robustness in previously edited regions?

## Limitations
- The method's effectiveness on larger models (>7B parameters) and non-code domains remains untested
- Causal tracing is computationally expensive, potentially limiting real-time applicability
- Results are demonstrated on curated benchmarks that may not reflect real-world code complexity and diversity

## Confidence

**High confidence:** The core causal tracing methodology for identifying robustness-sensitive layers is well-supported by ablation studies showing 20.4% performance degradation when layer localization is removed. The preservation loss mechanism preventing clean input degradation (12.8% reduction when removed) is empirically validated.

**Medium confidence:** Generalization claims from single-example editing are supported by G-RIR metrics but rely on specific perturbation types and model architectures. The 63% improvement figure comes from controlled benchmark settings that may not translate directly to production environments.

**Low confidence:** Claims about the method's practicality for "real-world deployment" lack evidence beyond the single-example requirement. The long-term stability of edited weights and performance under continuous model updates are unaddressed.

## Next Checks

1. **Stress-test generalization limits:** Apply CREME using one perturbation type to evaluate on a completely different, unseen perturbation category. Measure G-RIR and identify the breaking point where single-example generalization fails.

2. **Evaluate on production code patterns:** Test CREME on real-world codebases with varied syntax, docstrings, and coding styles beyond the curated HumanEval/MBPP benchmarks. Compare performance degradation when encountering unfamiliar code patterns.

3. **Measure computational overhead in deployment:** Benchmark the full causal tracing pipeline versus the proposed lightweight variant across different perturbation frequencies. Quantify the trade-off between localization accuracy and inference latency in a simulated production environment.