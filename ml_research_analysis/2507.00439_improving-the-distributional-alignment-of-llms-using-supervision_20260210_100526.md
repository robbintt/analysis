---
ver: rpa2
title: Improving the Distributional Alignment of LLMs using Supervision
arxiv_id: '2507.00439'
source_url: https://arxiv.org/abs/2507.00439
tags:
- alignment
- distributions
- answer
- llama-3
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of aligning large language models
  (LLMs) with diverse population groups on subjective questions by evaluating their
  ability to generate accurate response distributions. A key limitation in prior work
  is the assumption that group members share homogeneous opinions, which ignores intra-group
  disagreement.
---

# Improving the Distributional Alignment of LLMs using Supervision

## Quick Facts
- **arXiv ID:** 2507.00439
- **Source URL:** https://arxiv.org/abs/2507.00439
- **Reference count:** 40
- **Primary result:** Supervised regression calibration improves distributional alignment of LLM-generated distributions by 16.3% on average across three datasets and 15 models

## Executive Summary
This work addresses the challenge of aligning large language models (LLMs) with diverse population groups on subjective questions by evaluating their ability to generate accurate response distributions. A key limitation in prior work is the assumption that group members share homogeneous opinions, which ignores intra-group disagreement. To overcome this, the authors propose using supervised regression to calibrate LLM-generated distributions, making them more consistent with human response distributions.

Experiments across three datasets and 15 models demonstrate that while sociodemographic prompting alone does not reliably improve alignment, supervised calibration consistently improves distributional alignment by 16.3% on average. The calibration also reduces variance across models, datasets, and elicitation methods, suggesting that uncalibrated LLM distributions tend to exaggerate inter-group differences. Results show that as few as 1-10 supervised examples are sufficient for effective calibration, though alignment gains vary across specific demographic groups. Overall, supervised calibration provides a robust, scalable method to enhance distributional alignment of LLMs with diverse populations.

## Method Summary
The approach involves eliciting probability distributions from LLMs over survey answer choices for questions targeting specific demographic groups, then using supervised regression to transform these distributions to better match human ground truth distributions. Three elicitation methods are employed: verbalized (prompting for probability lists), self-random (sampling single choices with temperature), and paraphrase (generating multiple prompt variants). Regression models are trained per dataset-LLM-elicitation setting on (LLM probability, human probability) pairs per answer choice, then renormalized to produce valid distributions. The model is evaluated using Wasserstein distance to account for ordinal structure of survey responses.

## Key Results
- Supervised regression calibration increases opinion alignment by 16.3% on average across all experimental settings
- Calibration improves alignment in 94.8% of dataset-LLM-elicitation method combinations tested
- Calibrated distributions show 1.62x lower standard deviation across settings compared to uncalibrated distributions
- As few as 1-10 supervised examples are sufficient for effective calibration, though gains vary by demographic group

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised regression calibration may improve distributional alignment by correcting miscalibrated LLM-generated distributions.
- Mechanism: Regression models learn a mapping from LLM-generated probability values per answer choice to ground-truth human distributions, then renormalize. This transforms directionally correct but exaggerated distributions toward human baselines.
- Core assumption: LLM distributions capture directional patterns but are systematically overconfident or mis-scaled.
- Evidence anchors:
  - [abstract] "Simple supervision can more consistently improve LLM distributional alignment...supervised regression increased alignment by 16.3% on average"
  - [Section 4.2] "Calibration increases opinion alignment in 94.8% of dataset-LLM-elicitation method settings"
  - [corpus] No direct corpus evidence on calibration regression; related work focuses on steering benchmarks and opinion alignment.
- Break condition: If LLM distributions lack directional signal or if regression overfits to noise in limited supervision.

### Mechanism 2
- Claim: Sociodemographic prompting alone does not reliably improve alignment with target population distributions.
- Mechanism: SD prompting adds demographic context but may not shift distributions toward human baselines due to prompt/model sensitivity, stereotype amplification, or lack of reasoning depth.
- Core assumption: Demographic information alone is insufficient to capture intra-group heterogeneity and true opinion distributions.
- Evidence anchors:
  - [Section 4.1] "Prompting with SD information often leads to comparable or even lower opinion alignment than prompting without any SD information"
  - [abstract] "Simple supervision can more consistently improve LLM distributional alignment...than sociodemographic prompting alone"
  - [corpus] Related work (e.g., Steer-Bench, PAARS) evaluates steerability and simulation fidelity but does not provide comparative evidence on SD prompting vs. calibration.
- Break condition: If SD prompting is combined with distributional evaluation or if models are specifically fine-tuned for demographic reasoning.

### Mechanism 3
- Claim: Calibration reduces variance across models, elicitation methods, and datasets, making alignment more consistent.
- Mechanism: Regression transformation normalizes outputs across heterogeneous settings, dampening model-specific biases and elicitation artifacts.
- Core assumption: Variance reduction correlates with improved alignment because exaggerated inter-group differences are suppressed.
- Evidence anchors:
  - [Section 4.2] "Calibrated distributions have a lower standard deviation in 87.2% of dataset-LLM-elicitation settings, and are 1.62 times lower on average"
  - [Figure 2] Shows reduction in standard deviation across settings after calibration.
  - [corpus] Corpus does not provide variance reduction evidence; focus is on steering benchmarks and opinion simulation.
- Break condition: If regression models are underfit or over-regularized, potentially under-correcting variance.

## Foundational Learning

- Concept: Wasserstein distance for ordinal distributions
  - Why needed here: Opinion alignment metric uses Wasserstein to account for ordinal structure of survey responses (e.g., Likert scales), unlike KL divergence which ignores ordering.
  - Quick check question: Why would Wasserstein be preferred over Jensen-Shannon divergence for ordinal survey responses?

- Concept: Supervised regression for probability distribution calibration
  - Why needed here: Core method learns per-answer-choice transformations via regression (e.g., Ridge, Random Forest), then renormalizes to proper distributions.
  - Quick check question: What might happen if regression predicts negative probability values for certain answer choices?

- Concept: Distribution elicitation methods (verbalized, self-random, paraphrase)
  - Why needed here: Different methods extract distributions from LLMs with varying fidelity; choice affects calibration performance and applicability to API-only models.
  - Quick check question: Why might verbalized elicitation be more aligned with human distributions than self-random sampling?

## Architecture Onboarding

- Component map:
  LLM distribution elicitation -> Feature extraction -> Regression model -> Renormalization -> Alignment evaluation

- Critical path:
  1. Elicit LLM distributions for all questions and SD groups
  2. Train regression models on 80% of questions, validate on 10%, test on 10%
  3. Apply trained regressors to held-out test distributions
  4. Compute alignment scores vs. human ground truth

- Design tradeoffs:
  1. Regression model selection (Random Forest vs. Ridge/Lasso): RF may capture non-linear relationships but risks overfitting; linear models assume additive corrections.
  2. Training example granularity: Answer-choice-level vs. full-distribution supervision; paper uses answer-choice pairs.
  3. Elicitation method choice: Verbalized works best on average but may fail for smaller models; self-random/paraphrase more robust but noisier.

- Failure signatures:
  1. Over-smoothing: Renormalization after regression may flatten distributions, losing legitimate variance.
  2. Negative predictions: Regression may output negative probability values; handled by renormalization but introduces distortion.
  3. Small model refusal: Smaller LLMs (e.g., Llama-3.2-1B) often refuse SD prompts or fail to follow output formatting.

- First 3 experiments:
  1. Reproduce alignment metric: Compute Wasserstein-based alignment for a single model/dataset before/after calibration to validate pipeline.
  2. Ablate training data size: Test calibration with 1, 5, 10, 50, 100 examples to confirm paper's convergence claim.
  3. Compare elicitation methods: For a fixed model, compare verbalized vs. self-random vs. paraphrase alignment before/after calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training regression models specifically for each individual sociodemographic (SD) group, rather than using a general model across all groups, significantly improve alignment accuracy?
- Basis in paper: [explicit] Section 5.1 states, "We hypothesize that calibrating responses from each individual SD would likely improve its alignment, though we leave this to future work."
- Why unresolved: The current study trains regression models on data from all SD groups simultaneously; while this generalizes well, it is unknown if group-specific calibration could reduce the alignment degradation observed for some demographics (e.g., Central Africa in the WGM dataset).
- What evidence would resolve it: A comparative experiment showing alignment scores for group-specific regressors versus the aggregated approach across the WGM, OQA, and WVS datasets.

### Open Question 2
- Question: Does the method of predicting answer choices individually—ignoring correlations between choices—result in a loss of distributional structure or over-smoothing compared to constrained optimization methods?
- Basis in paper: [explicit] Section 9 (Limitations) notes, "Our calibration model predicts each answer choice individually... ignoring any correlations among answer choices. Future work might further analyze the effects of this individual answer choice prediction on alignment."
- Why unresolved: The authors found that constrained optimization performed worse than individual prediction, but it remains unclear if this is due to the specific implementation or a fundamental limitation of treating answer choices as independent variables.
- What evidence would resolve it: An analysis comparing the distributional properties (e.g., variance, entropy) of outputs from individual prediction versus joint prediction models against ground truth human distributions.

### Open Question 3
- Question: Can eliciting distributions using implicit demographic information or multimodal prompts yield higher alignment than explicit sociodemographic prompting?
- Basis in paper: [explicit] Section 7 (Discussion) suggests, "Future work might explore eliciting distributions with implicit demographic information... personalization to individual group members... or with multimodal prompts to incorporate implicit information."
- Why unresolved: The paper demonstrates that explicit sociodemographic prompting fails to consistently improve alignment and often triggers safety refusals in smaller models. It is unknown if implicit cues could bypass these issues while maintaining or improving alignment.
- What evidence would resolve it: Benchmarking alignment scores of LLMs prompted with implicit persona descriptors or images against the explicit prompting baseline established in the paper.

## Limitations

- The evaluation focuses exclusively on demographic groups, though opinions may depend on other characteristics beyond demographics
- The study uses a narrow selection of opinion questions (two per category from OQA and WVS), potentially limiting generalizability to broader survey domains
- Different elicitation methods were used for different model sizes, introducing potential confounding between model capacity and elicitation strategy effects

## Confidence

- **High Confidence**: Supervised regression calibration consistently improves distributional alignment across diverse models, datasets, and elicitation methods (94.8% of settings showed improvement, 16.3% average gain)
- **Medium Confidence**: Sociodemographic prompting alone is insufficient for distributional alignment (supported by comparative results but limited by prompt engineering details)
- **Medium Confidence**: Variance reduction correlates with improved alignment (supported by 87.2% of settings showing lower standard deviation, though causal relationship not definitively established)

## Next Checks

1. **Cross-domain generalization test**: Apply the calibration approach to survey questions from domains not represented in WGM, OQA, or WVS (e.g., technology adoption, political preferences) to assess whether the 16.3% average improvement holds for different opinion types.

2. **Fine-grained demographic analysis**: Replicate the experiments focusing on demographic subgroups where calibration showed minimal improvement (as suggested by Figure 3) to determine whether this reflects true model limitations or insufficient training examples for rare demographic combinations.

3. **Elicitation method ablation**: Systematically compare verbalized, self-random, and paraphrase methods across the same model size range (rather than using different methods for different model sizes) to isolate the effects of model capacity versus elicitation strategy on calibration performance.