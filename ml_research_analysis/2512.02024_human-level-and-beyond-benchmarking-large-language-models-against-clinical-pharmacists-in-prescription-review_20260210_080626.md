---
ver: rpa2
title: 'Human-Level and Beyond: Benchmarking Large Language Models Against Clinical
  Pharmacists in Prescription Review'
arxiv_id: '2512.02024'
source_url: https://arxiv.org/abs/2512.02024
tags:
- pharmacists
- prescription
- review
- clinical
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed RxBench, a comprehensive benchmark of 3,259
  prescription review items reviewed by clinical pharmacists, to systematically evaluate
  large language models (LLMs) in detecting medication errors. Across 18 state-of-the-art
  LLMs, top models (Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528)
  significantly outperformed clinical pharmacists in single-choice (accuracy 0.88
  vs.
---

# Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review

## Quick Facts
- arXiv ID: 2512.02024
- Source URL: https://arxiv.org/abs/2512.02024
- Reference count: 10
- Primary result: Top LLMs (Gemini-2.5-pro-preview-05-06, Grok-4-0709, DeepSeek-R1-0528) significantly outperformed clinical pharmacists in prescription error detection, with fine-tuned Qwen3-32B surpassing several leading general-purpose models.

## Executive Summary
This study develops RxBench, a comprehensive benchmark of 3,259 prescription review items, to systematically evaluate large language models against clinical pharmacists in detecting medication errors. Across 18 state-of-the-art LLMs, top-performing models achieved higher accuracy in single-choice (0.88 vs 0.90) and multiple-choice tasks (F1 0.94 vs 0.77) compared to pharmacists, though pharmacists matched or exceeded LLM performance in short-answer tasks. A LoRA-fine-tuned Qwen3-32B model demonstrated remarkable improvement, ranking first overall in short-answer evaluation and highlighting the potential of domain-specific fine-tuning for prescription review applications.

## Method Summary
The study developed RxBench from clinical pharmacist training textbooks, containing 3,259 prescription review items across three formats: 1,150 single-choice, 230 multiple-choice, and 879 short-answer questions mapped to 14 error categories. 18 LLMs were evaluated through zero-shot API calls with temperature=0, while 27 pharmacists completed a standardized 100-item test. A Qwen3-32B model was fine-tuned using LoRA adapters (rank=32, α=64) on 2,547 training items. Performance was measured using task-specific metrics including accuracy, F1, Macro Recall, and BERTScore, with human scores serving as baseline benchmarks.

## Key Results
- Top LLMs achieved accuracy of 0.88 in single-choice tasks versus 0.90 for pharmacists
- In multiple-choice tasks, models reached F1 scores of 0.94 compared to 0.77 for pharmacists
- LoRA-fine-tuned Qwen3-32B achieved nearly 30% higher scores than non-fine-tuned base model and ranked first in short-answer evaluation

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific fine-tuning via LoRA adaptation can elevate mid-tier models to match or exceed top-tier general-purpose LLMs in specialized prescription review tasks. LoRA injects low-rank decomposed adapter modules into all linear layers while freezing pre-trained weights, enabling targeted adjustment for pharmaceutical error detection without catastrophic forgetting. Evidence shows Qwen3-32B-LoRA achieved a score nearly 30% higher than non-fine-tuned Qwen3-32B and 17% higher than Qwen3-32B with fine-tuned thinking mode.

### Mechanism 2
LLMs outperform human pharmacists in structured, knowledge-intensive tasks due to uniform application of learned pharmaceutical knowledge across all domains. Unlike humans who may exhibit specialty-dependent confidence gaps, LLMs apply consistent probabilistic reasoning across diverse prescription scenarios without cognitive fatigue or contextual hesitation. Models achieved F1 scores exceeding 0.92 in multiple-choice tasks while pharmacists remained below 0.78.

### Mechanism 3
Standardized, error-type-oriented benchmark frameworks reveal systematic performance stratification across LLM families that general medical QA benchmarks obscure. RxBench's 14-category error taxonomy enables fine-grained failure mode analysis, exposing divergent optimization strategies—some models prioritizing precision while others prioritize recall—invisible in aggregate accuracy metrics.

## Foundational Learning

- **Concept: Prescription Error Taxonomy**
  - Why needed here: Understanding the 14 error categories (inappropriate dosing, contraindicated use, drug interactions, etc.) is prerequisite to interpreting model outputs and designing targeted interventions.
  - Quick check question: Can you distinguish "suboptimal drug selection" from "unwarranted indication" when reviewing a prescription for an elderly patient with multiple comorbidities?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper demonstrates LoRA's effectiveness for domain adaptation; understanding rank parameters (r=32, α=64), target modules (all-linear), and gradient checkpointing is essential for reproducing results.
  - Quick check question: Why does freezing pre-trained weights while training low-rank adapters help preserve general capabilities while enabling domain specialization?

- **Concept: Multi-dimensional Evaluation Metrics**
  - Why needed here: Short-answer tasks require combining F1 (error-type classification), Macro Recall (intervention point coverage), and BERTScore (semantic similarity)—a weighted integration distinct from simple accuracy.
  - Quick check question: If a model achieves high recall but low precision in error-type classification, what trade-off does this represent in clinical decision support?

## Architecture Onboarding

- **Component map:**
  RxBench Dataset -> Model Interface Layer -> Evaluation Pipeline -> Fine-tuning Module (optional) -> Human Benchmark

- **Critical path:**
  1. Data preparation: Expert review committee validates items against reference answers, generates scoring rubrics
  2. Zero-shot inference: Submit prompts to models via API, collect structured JSON outputs
  3. Post-processing: Minimal normalization (punctuation, formatting)
  4. Metric computation: Apply task-specific formulas, generate stratified performance comparisons
  5. LoRA fine-tuning (optional): Train adapter modules on 2,547 short-answer training items

- **Design tradeoffs:**
  - Precision vs. Recall: Models like DeepSeek prioritize precision (conservative, fewer false positives); Grok prioritizes recall (higher sensitivity, more false positives)—choice depends on clinical risk tolerance
  - General vs. Specialized: Top-tier general models (Gemini-2.5-pro) match specialized fine-tuned models, but fine-tuning offers deployment cost advantages
  - Textbook vs. Real-world: Current dataset derived from training textbooks; may not capture messy real-world prescription patterns

- **Failure signatures:**
  - Low F1 with high recall: Over-prediction of errors, potential alert fatigue in clinical deployment
  - Seniority paradox: Human pharmacists showing no performance gradient by experience level may indicate test format misalignment with clinical reasoning
  - Category-specific collapse: Universal modest F1 scores in short-answer tasks suggest all systems (human and AI) struggle with complex clinical reasoning

- **First 3 experiments:**
  1. Reproduce baseline comparison: Run zero-shot evaluation on RxBench subset (100 items) for 3-5 representative models spanning performance tiers; verify stratification patterns.
  2. Validate LoRA gains: Fine-tune Qwen3-32B on training split, measure delta against baseline on held-out test set; confirm 25-30% improvement signal.
  3. Error category drill-down: Generate confusion matrices by error type for top-performing model; identify systematic blind spots (e.g., off-label use, allergy documentation) requiring targeted augmentation.

## Open Questions the Paper Calls Out

- **Question 1**: Do state-of-the-art LLMs retain their performance advantage over pharmacists when analyzing unstructured, noisy real-world clinical data as opposed to standardized textbook cases?
  - Basis: The authors state that "standardized nature of textbook cases may not fully capture the nuances and heterogeneity of real-world clinical practice" and prioritize "validation using real-world data" as future direction.
  - Why unresolved: Current RxBench relies on authoritative training textbooks lacking complexity of authentic clinical scenarios.
  - What evidence would resolve it: Follow-up evaluation using unstructured EHR data demonstrating similar accuracy in live clinical setting.

- **Question 2**: How can hybrid human-AI systems be optimized to integrate the "contextual awareness" of pharmacists with the "computational breadth" of LLMs to improve prescription safety?
  - Basis: Section 5.6 suggests future research should focus on "hybrid human–AI systems" to optimize prescription safety and preserve pharmacist's central role.
  - Why unresolved: Study only evaluated LLMs and pharmacists in isolation, not in collaborative workflows.
  - What evidence would resolve it: Comparative studies measuring error reduction rates in human-AI collaborative teams versus human-only or AI-only controls.

- **Question 3**: Can the long-term reliability and interpretability of LLMs in pharmacy workflows be sufficiently ensured to justify integration into prospective clinical trials?
  - Basis: Section 5.6 notes that "long-term reliability, interpretability, and integration into pharmacy workflows require further investigation in prospective clinical trials."
  - Why unresolved: Current study utilized static benchmark test rather than longitudinal deployment.
  - What evidence would resolve it: Results from prospective clinical trials where LLMs deployed in actual pharmacy workflows with monitoring for hallucinations and error consistency over time.

## Limitations

- Benchmark relies on textbook-derived prescription data rather than real-world clinical scenarios, potentially missing complexity of actual prescription workflows
- LoRA fine-tuning results may not generalize across different healthcare systems or prescription types despite demonstrated 25-30% performance gains
- Human performance comparison constrained by artificial test format that may not reflect clinical decision-making under real-world conditions

## Confidence

- **High Confidence**: LLM performance stratification across task types is robust and clearly demonstrated through consistent metrics across 18 models
- **Medium Confidence**: LoRA fine-tuning mechanism's effectiveness is demonstrated but relies on assumptions about dataset representativeness
- **Low Confidence**: Claims about human performance limitations (specialty-dependent knowledge gaps) are speculative based on single benchmark performance

## Next Checks

1. Test top-performing LLMs (Gemini-2.5-pro, Grok-4-0709, fine-tuned Qwen3-32B) in controlled clinical setting with actual prescription data from multiple specialties, measuring false positive rates and clinician acceptance

2. Evaluate LoRA-fine-tuned models on prescription datasets from different healthcare systems to assess whether 25-30% performance gains persist across diverse clinical contexts

3. Implement time-series analysis tracking prescription error types detected by LLMs versus human pharmacists over multiple patient encounters to examine clinical impact in complex, multi-visit scenarios