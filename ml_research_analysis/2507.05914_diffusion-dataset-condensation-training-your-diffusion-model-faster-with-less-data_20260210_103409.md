---
ver: rpa2
title: 'Diffusion Dataset Condensation: Training Your Diffusion Model Faster with
  Less Data'
arxiv_id: '2507.05914'
source_url: https://arxiv.org/abs/2507.05914
tags:
- diffusion
- training
- dataset
- data
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces D2C, the first framework for diffusion dataset\
  \ condensation\u2014condensing large image datasets into compact, synthetic subsets\
  \ that enable high-quality diffusion model training with dramatically less data\
  \ and compute. The core idea is a two-stage process: Select, which identifies a\
  \ compact and diverse subset using a diffusion difficulty score and interval sampling,\
  \ and Attach, which enriches selected samples with semantic (text embeddings) and\
  \ visual (DINOv2 features) information to strengthen conditional signals."
---

# Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data

## Quick Facts
- arXiv ID: 2507.05914
- Source URL: https://arxiv.org/abs/2507.05914
- Reference count: 40
- This paper introduces D2C, the first framework for diffusion dataset condensation—condensing large image datasets into compact, synthetic subsets that enable high-quality diffusion model training with dramatically less data and compute.

## Executive Summary
D2C presents a novel approach to diffusion dataset condensation, enabling high-quality diffusion model training with dramatically less data and compute. The framework addresses the computational burden of training diffusion models on large datasets by condensing them into compact synthetic subsets. Through a two-stage process of selective sampling and information attachment, D2C achieves up to 100× faster training than baselines while maintaining competitive performance. The method demonstrates consistent superiority across various dataset sizes, model architectures, and resolutions.

## Method Summary
D2C operates through a two-stage process: Select and Attach. The Select stage identifies a compact and diverse subset using a diffusion difficulty score and interval sampling strategy. This score measures how challenging each sample is for diffusion models to learn, guiding the selection of representative samples. The Attach stage enriches selected samples with semantic (text embeddings) and visual (DINOv2 features) information to strengthen conditional signals during training. The framework is evaluated on ImageNet across different resolutions (256×256 and 512×512) and compared against baselines including random sampling, K-Center, Herding, and SRe2L. The condensation process leverages pre-trained models like CLIP and DINOv2 for feature extraction, creating synthetic datasets that enable efficient training while preserving quality.

## Key Results
- Achieves up to 100× faster training than baselines like REPA and 233× faster than vanilla SiT-XL/2
- Reaches FID 4.3 in just 40k steps using only 0.8% of the original data
- Consistently outperforms random sampling, K-Center, Herding, and SRe2L across various dataset sizes and model architectures

## Why This Works (Mechanism)
D2C works by intelligently selecting and enriching samples to maximize information density while minimizing redundancy. The diffusion difficulty score identifies samples that are most informative for model learning, ensuring the condensed set captures essential variations in the data distribution. By attaching semantic and visual features to selected samples, the framework provides stronger conditional signals that guide the diffusion process more effectively. This dual approach of strategic selection and information enrichment enables the model to learn efficiently from a fraction of the original data, dramatically reducing training time while maintaining quality.

## Foundational Learning
- **Diffusion Models**: Generative models that learn to reverse a gradual noising process, essential for understanding the training target
  - Why needed: The entire framework is designed to accelerate diffusion model training
  - Quick check: Can you explain how diffusion models differ from GANs or VAEs in their training approach?

- **Dataset Condensation**: The process of creating small synthetic datasets that retain the learning capacity of larger datasets
  - Why needed: D2C is the first framework specifically for diffusion dataset condensation
  - Quick check: What are the key challenges in dataset condensation that D2C addresses?

- **Diffusion Difficulty Score**: A metric measuring how challenging each sample is for diffusion models to learn
  - Why needed: This score guides the selection of informative samples during condensation
  - Quick check: How might this score correlate with sample complexity in traditional ML contexts?

- **Feature Extraction with Pre-trained Models**: Using CLIP and DINOv2 to extract semantic and visual features
  - Why needed: These features enrich selected samples with additional learning signals
  - Quick check: What are the computational trade-offs of using large pre-trained models for feature extraction?

- **Conditional Signal Strengthening**: Enhancing training data with auxiliary information to guide model learning
  - Why needed: Stronger conditional signals improve training efficiency and quality
  - Quick check: How does attaching semantic and visual features affect the loss landscape during training?

## Architecture Onboarding

Component Map:
Pre-trained CLIP/DINOv2 -> Diffusion Difficulty Score Calculator -> Interval Sampling -> Synthetic Dataset -> Diffusion Model Training

Critical Path:
Data Preparation (pre-trained models) → Difficulty Scoring → Subset Selection → Feature Attachment → Training → Evaluation

Design Tradeoffs:
- **Pre-trained Model Dependency**: Heavy reliance on CLIP/DINOv2 provides rich features but introduces computational overhead and potential bias
- **Score-Based Selection**: The diffusion difficulty score effectively identifies informative samples but may miss rare but important patterns
- **Information Attachment**: Adding semantic and visual features strengthens learning signals but increases data complexity
- **Condensation vs. Original Training**: Dramatic speedup at the cost of initial condensation overhead and dependency on pre-trained models

Failure Signatures:
- **Over-reliance on Difficulty Scores**: If the score distribution is skewed, the method may miss important samples
- **Feature Extraction Bottleneck**: Heavy pre-processing can negate training speed benefits
- **Generalization Issues**: Performance may degrade on datasets significantly different from ImageNet
- **Resolution Sensitivity**: Fixed score ranges may not generalize across different image resolutions

First 3 Experiments:
1. Compare D2C performance against random sampling baseline on ImageNet at 256×256 resolution
2. Evaluate training speed and quality when using 0.1%, 0.5%, and 1% of the original dataset
3. Test D2C on different model architectures (SiT-XL/2 vs other diffusion models) to verify architecture independence

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's strong performance relies on extensive pre-trained models (CLIP, DINOv2) for feature extraction, introducing significant computational overhead during condensation
- The paper demonstrates results primarily on ImageNet at two fixed resolutions, leaving uncertainty about generalizability to other domains
- The "diffusion difficulty score" metric lacks theoretical grounding for why specific score ranges (3-8) were chosen

## Confidence
- **High**: The experimental methodology for controlled comparisons on ImageNet is sound and results are reproducible given access to pre-trained models
- **Medium**: Claims about generalization across model architectures and resolutions are supported by limited experiments but require broader validation
- **Medium**: Speedup claims are technically accurate for the reported training pipeline but may not reflect end-to-end computational savings when including condensation overhead

## Next Checks
1. **Compute overhead validation**: Measure and report total wall-clock time including feature extraction and diffusion model training to verify claimed 100× speedups are not overstated
2. **Domain generalization test**: Apply D2C to non-ImageNet datasets (medical imaging, satellite imagery, or specialized domains) and evaluate performance degradation relative to random sampling
3. **Score distribution sensitivity analysis**: Systematically vary the diffusion difficulty score range and interval sampling strategy to quantify robustness to score distribution skew and identify potential bias toward easier samples