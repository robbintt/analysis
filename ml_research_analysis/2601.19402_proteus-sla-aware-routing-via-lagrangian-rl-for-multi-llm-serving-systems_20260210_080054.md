---
ver: rpa2
title: 'PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems'
arxiv_id: '2601.19402'
source_url: https://arxiv.org/abs/2601.19402
tags:
- accuracy
- cost
- routing
- policy
- proteus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROTEUS, a router that accepts accuracy targets
  as direct runtime input, enabling SLA-aware routing for multi-LLM serving systems.
  Unlike existing approaches that require offline parameter tuning, PROTEUS uses Lagrangian
  dual control with a learned dual variable to enforce accuracy constraints during
  training, allowing a single trained model to serve the full accuracy spectrum from
  0.85 to 0.95.
---

# PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems

## Quick Facts
- arXiv ID: 2601.19402
- Source URL: https://arxiv.org/abs/2601.19402
- Reference count: 2
- Key outcome: PROTEUS achieves 100% floor compliance on RouterBench and SPROUT benchmarks while reducing cost by 89.8% versus oracle, using a single trained model to serve accuracy targets from 0.85 to 0.95 without retraining.

## Executive Summary
PROTEUS introduces a novel approach to SLA-aware routing for multi-LLM serving systems that accepts accuracy targets as direct runtime input. Unlike existing methods requiring offline parameter tuning, PROTEUS uses Lagrangian dual control with a learned dual variable to enforce accuracy constraints during training, enabling a single model to serve the full accuracy spectrum. The system achieves 100% floor compliance with accuracy within 1.3-4.6% of oracle performance while providing 89.8% cost savings.

## Method Summary
PROTEUS formulates routing as a constrained optimization problem where the goal is to minimize cost while ensuring achieved accuracy meets or exceeds a runtime-specified target τ. The method uses Lagrangian dual control where a learned dual variable λ tracks constraint violations during training and conditions the policy network. The policy outputs a continuous quality preference μ ∈ [0,1] via a Beta distribution, which is used in a scoring function that balances accuracy predictions, model costs, and quality preference. Training uses PPO with session-based learning that fixes τ per session, providing stable constraint signals. At inference, λ is fixed at 1.0 and the policy responds to τ alone.

## Key Results
- 100% floor compliance on RouterBench and SPROUT benchmarks, maintaining accuracy ≥ τ at all tested targets
- Accuracy within 1.3% (RouterBench) and 4.6% (SPROUT) of oracle performance while reducing cost by 89.8% versus best fixed model
- τ-μ correlation of 0.97-0.98 demonstrates strong runtime adaptability through direct τ input
- Learnable cost sensitivity parameter γ improves performance on datasets with wider cost variation

## Why This Works (Mechanism)

### Mechanism 1: Lagrangian Dual Constraint Enforcement
The learned dual variable λ converts constraint violations into training pressure, teaching the policy to associate higher τ values with quality-seeking behavior. During training, λ updates via `[λ + η_λ(τ - p̄_batch)]⁺`. When batch accuracy p̄_batch falls below τ, λ increases; this λ is injected into the policy network as an auxiliary input. Over many sessions, the policy learns that high τ correlates with high λ, and high λ correlates with high μ (quality preference). At inference, λ is fixed at 1.0 and the policy responds to τ alone via this learned association.

### Mechanism 2: τ-Conditioned Policy with Continuous Quality Preference Output
Conditioning the policy on τ as runtime input and outputting a continuous μ ∈ [0,1] enables a single trained model to serve the full accuracy spectrum without retraining. The policy π_θ(μ|x, τ) takes (query embedding, τ, λ) as input and outputs parameters of a Beta distribution over μ. During training, session-based learning fixes τ for multiple batches, providing stable constraint signals. The policy learns to map (query, τ) pairs to appropriate μ values: high τ → high μ → quality-seeking; low τ → low μ → cost-seeking.

### Mechanism 3: Non-Linear Cost Weighting with Learnable γ
A learnable cost sensitivity parameter γ ∈ [2, 8] allows the scoring function to adapt to dataset-specific cost-quality tradeoffs. The routing score `s_i = p_i(x) + μ·b_i - (1-μ)^γ·c_i` uses a non-linear cost penalty. At γ=2, cost matters quadratically; at γ=8, costs matter only when μ≈0. The policy learns γ alongside μ, discovering appropriate sensitivity for the model pool's cost distribution.

## Foundational Learning

- **Concept: Constrained Markov Decision Processes (CMDPs)**
  - Why needed here: PROTEUS formulates routing as a constrained optimization problem (minimize cost subject to accuracy ≥ τ), not pure reward maximization. Understanding Lagrangian relaxation is essential to grasp why λ exists.
  - Quick check question: Can you explain why adding λ·(p_m* - τ) to the reward enforces the constraint, and why λ must be non-negative?

- **Concept: Beta Distribution for Continuous Actions**
  - Why needed here: The policy outputs μ ~ Beta(α, β) rather than a deterministic value or categorical distribution. This provides learnable concentration (variance) and bounds μ ∈ [0, 1] naturally.
  - Quick check question: Why might a Beta distribution be preferable to a Gaussian with sigmoid squashing for bounded continuous outputs?

- **Concept: Policy Gradient with PPO**
  - Why needed here: The policy is trained via PPO, which clips gradient updates to prevent destabilization. Understanding baseline subtraction (critic) and advantage estimation helps interpret the ablation results.
  - Quick check question: The ablation shows removing the critic has negligible impact. Why might this be expected for single-step MDPs?

## Architecture Onboarding

- **Component map**: Query → DeBERTa-v3-small encoder → (Policy μ, Predictions p_i) → Scoring function → Model selection
- **Critical path**: Query → Encoder → (Policy μ, Predictions p_i) → Scoring → Model selection. At inference, τ is a direct input; no λ update occurs.
- **Design tradeoffs**:
  - Encoder size: DeBERTa-v3-small (22M) trades representational power for ~5× speedup vs. RoBERTa-base (125M)
  - Continuous μ vs. categorical over K models: μ enables 1D exploration and smooth interpolation; K-way categorical would require O(K) exploration
  - Session-based training: Fixing τ per session stabilizes constraint signals but requires more training steps to cover the τ spectrum uniformly
- **Failure signatures**:
  - λ oscillation (η_λ > 1.0): Dual variable spikes erratically, training destabilizes
  - Floor violation on test data: Policy may overfit to training τ distribution; check validation accuracy across τ range
  - Cost collapse: If γ learns to near 0 or cost predictions are miscalibrated, routing defaults to cheapest or most expensive model regardless of τ
- **First 3 experiments**:
  1. **Sanity check: τ-μ correlation**. Plot μ vs. τ on held-out queries. Expect r > 0.95; lower values indicate conditioning failure.
  2. **Floor compliance sweep**. Evaluate at τ ∈ {0.85, 0.87, 0.89, 0.91, 0.93, 0.95}. Verify accuracy ≥ τ at each point; failures indicate generalization gaps.
  3. **Ablate learnable γ**. Compare full model vs. fixed γ=3.0 on both datasets. Expect degradation primarily on the dataset with wider cost variation (SPROUT).

## Open Questions the Paper Calls Out

- **Can PROTEUS be extended to jointly select models and allocate reasoning budgets for reasoning models like o1 and DeepSeek-R1?**
  - Basis in paper: Future Work states: "Reasoning-aware routing extends optimization to jointly select models and allocate reasoning budgets... The policy could output both μ (model preference) and a reasoning budget ρ."
  - Why unresolved: Current formulation outputs only μ for model selection; reasoning tokens create a continuous dimension alongside discrete model selection that requires new policy architecture.
  - What evidence would resolve it: Implementation of dual-output policy (μ, ρ) evaluated on reasoning model pools, showing whether accuracy targets can be met via expensive models with minimal reasoning or cheaper models with extended thinking.

- **Can the policy be conditioned on runtime cost vectors to adapt to API pricing changes without retraining?**
  - Basis in paper: Future Work: "Cost-adaptive routing could treat model costs as runtime inputs rather than static training-time parameters, enabling immediate adaptation to API pricing changes by conditioning the policy on a cost vector c alongside τ."
  - Why unresolved: Current architecture fixes costs ci at training time; significant pricing changes currently require 4-hour retraining. Whether policy networks can learn generalizable cost-quality tradeoffs from conditioned inputs remains untested.
  - What evidence would resolve it: Modified policy architecture accepting (τ, c) as input, evaluated under simulated pricing perturbations, measuring floor compliance retention without retraining.

- **Can bandit feedback formulations replace ground-truth labels for constraint learning while maintaining floor compliance?**
  - Basis in paper: Future Work: "Bandit feedback formulations where only the selected model's outcome is observed would reduce annotation cost and enable continual learning." Limitations also notes production systems lack ground-truth labels by default.
  - Why unresolved: Current training requires per-query correctness labels across all models. Bandit feedback observes only selected model outcomes, creating partial observability that may destabilize Lagrangian constraint enforcement.
  - What evidence would resolve it: Comparison of floor compliance and τ-μ correlation between full-information training and bandit feedback variants across varying exploration strategies.

- **How does PROTEUS perform when model pools have overlapping accuracy distributions that compress fine-grained τ distinctions?**
  - Basis in paper: Limitations states: "Both datasets exhibit compressed accuracy ranges among mid-tier models, which makes fine-grained τ distinctions difficult."
  - Why unresolved: Evaluation benchmarks (RouterBench, SPROUT) may not represent production pools with highly overlapping model capabilities, potentially masking failure modes when adjacent τ values map to identical routing decisions.
  - What evidence would resolve it: Synthetic model pool experiments varying accuracy overlap ratios, measuring τ-μ correlation degradation and floor compliance as model distinguishability decreases.

## Limitations

- The core claim that a single learned dual variable λ can generalize from batch-level constraint violations to per-query routing decisions relies on an untested inductive leap, and the exact λ-gating mechanism is underspecified.
- The learnable cost sensitivity parameter γ may not adapt meaningfully across all datasets, showing negligible impact on RouterBench where cost variation is narrow.
- The paper's comparison to "standard LLM routers" is vague—specific baselines like the expensive Router (T5-11B) are not directly included, potentially overstating relative gains.

## Confidence

- **High confidence**: Floor compliance (100%) and τ-μ correlation (0.97-0.98) on benchmark datasets; cost savings (89.8% vs. oracle); runtime adaptability through τ input.
- **Medium confidence**: Generalization of λ-based constraint enforcement across τ range; effectiveness of learnable γ on datasets with wider cost variation; avoidance of overfitting to training τ distribution.
- **Low confidence**: Exact architectural details for λ-gating and performance prediction; robustness to query distributions outside training range; comparison to all relevant baselines.

## Next Checks

1. **Sanity check τ-μ correlation**: Plot μ vs. τ on held-out queries. Expect r > 0.95; lower values indicate conditioning failure or overfitting to training τ values.
2. **Floor compliance sweep**: Evaluate at τ ∈ {0.85, 0.87, 0.89, 0.91, 0.93, 0.95}. Verify accuracy ≥ τ at each point; failures indicate generalization gaps or λ oscillation.
3. **Ablate learnable γ**: Compare full model vs. fixed γ=3.0 on both datasets. Expect degradation primarily on SPROUT (wider cost range), confirming dataset-specific cost adaptation.