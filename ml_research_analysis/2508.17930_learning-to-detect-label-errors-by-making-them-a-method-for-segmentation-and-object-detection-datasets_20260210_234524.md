---
ver: rpa2
title: 'Learning to Detect Label Errors by Making Them: A Method for Segmentation
  and Object Detection Datasets'
arxiv_id: '2508.17930'
source_url: https://arxiv.org/abs/2508.17930
tags:
- label
- error
- errors
- detection
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified method for detecting label errors
  in object detection, semantic segmentation, and instance segmentation datasets.
  The approach learns to detect label errors by injecting simulated errors into ground
  truth data and training an instance segmentation model to identify them.
---

# Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets

## Quick Facts
- arXiv ID: 2508.17930
- Source URL: https://arxiv.org/abs/2508.17930
- Reference count: 40
- Method achieves 61.74% AP and 95.78% recall on Cityscapes validation data

## Executive Summary
This paper presents a unified method for detecting label errors in object detection, semantic segmentation, and instance segmentation datasets. The approach learns to detect label errors by injecting simulated errors into ground truth data and training an instance segmentation model to identify them. The method outperforms existing baselines and state-of-the-art approaches across multiple datasets including Pascal VOC, COCO, Cityscapes, ADE20K, and LIVECell.

## Method Summary
The method works by first training reference DNNs on the original datasets, then applying a perturbation algorithm to simulate label errors (drop, flip, spawn) at rate q to create perturbed training data. For each image, the method constructs a composite input tensor containing the RGB image, perturbed ground truth, reference prediction, and difference mask. A Cascade Mask R-CNN is trained on this composite input to predict error instances as single-class output. During inference, the perturbed ground truth is replaced with the real ground truth under inspection, and high-scoring output instances indicate likely label errors.

## Key Results
- Achieves 61.74% AP and 95.78% recall on Cityscapes validation data
- Outperforms baselines (DNN+UQ, ClassMix, K-Means, Confident Learning) by significant margins across all datasets
- Introduces a benchmark for real label error detection with 459 identified errors in Cityscapes ground truth

## Why This Works (Mechanism)

### Mechanism 1: Synthetic-to-Real Transfer via Error Injection
The perturbation algorithm creates a controlled distribution of error types (drop, flip, spawn) that approximates real annotation mistakes. By learning to identify these synthetic errors with known ground truth, the model develops representations that transfer to real-world errors it has never seen. Core assumption: real label errors follow similar patterns to synthetically generated errors—specifically missing annotations, mislabeled objects, and spurious annotations.

### Mechanism 2: Multi-Signal Discrepancy Detection
The composite input tensor enables the model to learn spatial correspondences between visual content, human annotations, and model predictions. Discrepancies between ground truth and reference predictions highlight potential errors, while the RGB provides contextual grounding for what should actually be present. Core assumption: reference DNN predictions provide a meaningful signal for identifying annotation errors.

### Mechanism 3: Instance Segmentation as Universal Error Interface
Framing error detection as instance segmentation unifies handling of bounding boxes and segmentation masks across tasks. By treating all error types as instances to be segmented, the method leverages mature instance segmentation architectures and COCO evaluation protocols without requiring task-specific detection logic. Core assumption: label errors are spatially localizable and can be represented as coherent regions.

## Foundational Learning

### Instance Segmentation (Mask R-CNN family)
**Why needed here:** The entire method relies on training an instance segmentation model (Cascade Mask-RCNN) to predict error regions. Understanding how instance segmenters work—RPN proposal generation, RoI alignment, parallel box/mask heads—is essential for debugging, modifying architecture, and interpreting failure modes.
**Quick check question:** Given an input image, can you trace how Mask R-CNN produces both a bounding box and a segmentation mask for each detected instance?

### COCO Evaluation Protocol (AP, IoU Thresholds)
**Why needed here:** The paper uses COCO-style Average Precision (AP averaged over IoU thresholds from 0.5 to 0.95) as the primary evaluation metric. Understanding IoU-based matching between predictions and ground truth is critical for interpreting reported numbers and reproducing evaluation.
**Quick check question:** Given a predicted error mask and a ground truth error mask with 0.72 IoU, would this count as a true positive at IoU threshold 0.5? At threshold 0.75?

### Label Noise Effects on Deep Learning
**Why needed here:** The method is motivated by observations that label errors degrade model performance and cause memorization of noisy data. Understanding why noisy labels harm learning—especially for larger models—helps justify error detection investment.
**Quick check question:** Why might a larger-capacity model suffer more from label noise than a smaller model during training?

## Architecture Onboarding

### Component Map
DATA PREPARATION: Perturbation Algorithm (drop/flip/spawn) + Reference DNN (frozen) + Difference Mask Generator → Composite Input Tensor [H×W×12 channels] → LABEL ERROR DETECTOR (trainable): Cascade Mask-RCNN (single class: "error") → Output: P_hat = predicted error instances (masks + scores)

### Critical Path
1. **Setup Reference DNN**: Train or load a pre-trained model for the original task (semantic segmentation, object detection, or instance segmentation). This model remains frozen throughout.
2. **Generate Perturbed Training Dataset**: For each training image: Apply perturbation algorithm with rate q (e.g., 0.2) to create drop/flip/spawn errors; Store perturbed annotations G_pert and error ground truth G_le_gt; Run frozen reference DNN to get I_pred; Compute difference mask I_diff; Normalize and stack into 12-channel tensor.
3. **Train Error Detector**: Train Cascade Mask-RCNN on composite inputs with single-class output ("error"). Use standard instance segmentation loss (classification + box + mask).
4. **Inference on Real Data**: Replace I_pert with I_gt (the ground truth under inspection). High-scoring output instances indicate likely label errors requiring human review.

### Design Tradeoffs
| Decision | Options | Paper Choice | Tradeoff |
|----------|---------|--------------|----------|
| Perturbation rate (q) | 0.05–0.2 | q=0.2 default | Higher q → better recall but risk of over-representing errors |
| Dataset copies (t) | 1–10 | t=10 (small), t=2 (large) | More copies → diversity vs. compute cost |
| Reference DNN | Any task model | Dataset-dependent | DeepLabV3+ → precision; Segformer → recall |
| Input composition | RGB, masks, diff | Full 12-channel preferred | Diff mask most critical; RGB marginal benefit |
| Detector architecture | Mask-RCNN vs Cascade | Cascade Mask-RCNN | Cascade → precision; Mask-RCNN → recall |

### Failure Signatures
| Symptom | Likely Cause | Diagnostic |
|---------|--------------|------------|
| High false positives on real data | Reference DNN unreliable or biased | Visualize reference predictions on clean subset; check per-class accuracy |
| Low recall specifically on spawn errors | Spawn polygon generation too diverse | Reduce spawn irregularity/spikiness parameters; inspect samples |
| Poor sim-to-real transfer | Simulated errors don't match real distribution | Manually analyze real error types; add targeted perturbations |
| Training divergence or NaN loss | Channel normalization mismatch | Verify per-channel mean/std computed on training data correctly |
| High precision but very low recall | Score threshold too aggressive | Lower inference threshold; expect increased FP count |

### First 3 Experiments
1. **Validate pipeline on small subset**: Create perturbed version of 100 Cityscapes training images with q=0.2, t=1. Train for 5,000 iterations. Verify AP > 50% on held-out perturbed validation. If below target, check input construction (channel ordering, normalization) and reference DNN output validity.
2. **Ablate input components**: Train three variants on full Cityscapes: (a) I_rgb only, (b) I_rgb + I_pert + I_pred (9 channels), (c) full 12-channel with I_diff. Compare AP and recall. Confirm I_diff provides measurable improvement as reported in Table III.
3. **Test generalization to real errors**: Apply trained model to original (unperturbed) Cityscapes validation. Manually review top 100 predictions sorted by score. Calculate precision on real errors. Target: match or exceed DNN+UQ baseline (6.35% precision from Table IV). If significantly below, analyze whether simulated error types adequately cover real error patterns.

## Open Questions the Paper Calls Out
1. Can the perturbation algorithm be expanded to include context-aware or structurally complex noise to better close the distribution gap between simulated and real label errors?
2. How can the method decouple error detection from the performance of the reference model to prevent false positives caused by the reference model's own prediction failures?
3. Is the method robust when the "clean" ground truth dataset used for training actually contains pre-existing label noise?

## Limitations
- The sim-to-real transfer assumption lacks direct validation—no quantitative comparison between synthetic error characteristics and actual error distributions in ground truth data.
- The method's reliance on reference DNN predictions creates potential for error amplification when the reference model is systematically biased or unreliable.
- The study focuses on urban scene datasets (Cityscapes, COCO) with predominantly rigid objects, leaving generalization to deformable/transparent objects unverified.

## Confidence
- **High confidence:** The unified instance segmentation formulation, composite input construction, and basic perturbation algorithm (drop/flip/spawn) are well-specified and reproducible.
- **Medium confidence:** The sim-to-real transfer mechanism and generalization to real errors, as actual error distributions in datasets are not characterized or compared to synthetic distributions.
- **Low confidence:** Claims about downstream performance improvements on downstream tasks (though not directly studied) and the method's effectiveness on non-urban scenes.

## Next Checks
1. **Characterize real error distribution:** Manually annotate 200 randomly selected instances from Cityscapes validation to quantify error types (drop/flip/spawn) and compare their characteristics to synthetically generated errors.
2. **Reference model ablation study:** Evaluate the proposed method using reference models of varying quality (strong vs. weak) on the same dataset to quantify sensitivity to reference model reliability.
3. **Generalization test on non-urban dataset:** Apply the method to a dataset with deformable/transparent objects (e.g., OpenImages or LVIS) and compare performance to urban scenes to identify domain limitations.