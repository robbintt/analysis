---
ver: rpa2
title: Automated Extraction of Material Properties using LLM-based AI Agents
arxiv_id: '2510.01235'
source_url: https://arxiv.org/abs/2510.01235
tags:
- thermoelectric
- materials
- properties
- structural
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present an agentic, LLM-based workflow for automated extraction
  of thermoelectric and structural properties from scientific literature. The system
  integrates dynamic token allocation, zeroshot multi-agent extraction, and conditional
  table parsing to balance accuracy against computational cost.
---

# Automated Extraction of Material Properties using LLM-based AI Agents

## Quick Facts
- arXiv ID: 2510.01235
- Source URL: https://arxiv.org/abs/2510.01235
- Authors: Subham Ghosh; Abhishek Tewari
- Reference count: 40
- Key outcome: LLM-based workflow achieves F1 = 0.91 for thermoelectric properties and 0.82 for structural fields, producing 27,822 curated records with interactive web explorer

## Executive Summary
This paper presents an agentic, LLM-based workflow for automated extraction of thermoelectric and structural properties from scientific literature. The system integrates dynamic token allocation, zeroshot multi-agent extraction, and conditional table parsing to balance accuracy against computational cost. Benchmarking on 50 curated papers shows GPT-4.1 achieves the highest accuracy (F1 = 0.91 for thermoelectric properties and 0.82 for structural fields), while GPT-4.1 Mini delivers nearly comparable performance at a fraction of the cost. Applying this workflow, the authors curated 27,822 temperature-resolved property records with normalized units, spanning ZT, Seebeck coefficient, conductivity, resistivity, power factor, and thermal conductivity, together with structural attributes such as crystal class, space group, and doping strategy. Analysis reproduces known thermoelectric trends and surfaces structure-property correlations. The work releases an interactive web explorer with semantic filters, numeric queries, and CSV export, establishing a scalable, cost-profiled foundation for data-driven materials discovery.

## Method Summary
The workflow uses a LangGraph state machine orchestrating four specialized agents: MatFindr identifies material names, TEPropAgent extracts thermoelectric properties, StructPropAgent extracts structural attributes, and TableDataAgent processes tabular data when present. Zero-shot prompting with temperature=0.001 and structured JSON templates ensures reproducible outputs. Dynamic token allocation based on document length optimizes API costs, while conditional table parsing activates only when tables exist. The system prioritizes text-derived data when both text and table sources are available, using tables to fill gaps. Property values are normalized to standard units and validated against ground truth with 1% relative tolerance for numerical values and ±1K for temperature matches.

## Key Results
- GPT-4.1 achieves F1 = 0.91 for thermoelectric properties extraction, while GPT-4.1 Mini reaches F1 = 0.89 at significantly lower cost
- Structural property extraction yields F1 = 0.82 for crystal class and space group, though doping type extraction remains weakest at F1 = 0.51-0.64
- The workflow processed ~10,000 articles for $112 total cost, demonstrating strong cost-efficiency
- Curated dataset includes 27,822 temperature-resolved property records with interactive web explorer for semantic filtering and CSV export

## Why This Works (Mechanism)

### Mechanism 1: Specialized Agent Decomposition
- Claim: Specialized agent decomposition improves extraction accuracy by isolating sub-tasks and enabling cross-validation between agents
- Mechanism: Four agents (MatFindr → TEPropAgent → StructPropAgent → TableDataAgent) operate sequentially with material names serving as cross-sentence anchors that constrain downstream extraction scope
- Core assumption: Material names serve as reliable cross-sentence anchors that constrain downstream extraction scope
- Evidence anchors: Zeroshot multi-agent extraction achieves F1 = 0.91 for TE properties; modular agent design allows each step to focus on well-defined sub-tasks
- Break condition: If material names are ambiguous or incorrectly identified early, downstream agents inherit errors with no correction path

### Mechanism 2: Dynamic Token Allocation
- Claim: Dynamic token allocation balances extraction completeness against API cost by matching context window to input length
- Mechanism: Token counts computed during preprocessing dynamically set max_tokens per document, preventing over-provisioning for short articles and truncation for long ones
- Core assumption: Document length correlates with extraction complexity; fixed token budgets waste capacity or cut outputs prematurely
- Evidence anchors: Dynamic token allocation enables processing ~10,000 articles for $112; efficient prompt sizing balances output completeness with API cost
- Break condition: If token counts don't capture semantic complexity, allocation may misallocate budget

### Mechanism 3: Low Temperature with Structured Templates
- Claim: Low sampling temperature (T ≈ 0) with structured JSON templates yields reproducible, low-hallucination extraction
- Mechanism: Temperature set to 0.001 minimizes stochasticity; structured output templates constrain response format; robust JSON parser corrects minor formatting errors
- Core assumption: Scientific extraction tasks benefit from deterministic outputs; variability introduces errors rather than useful diversity
- Evidence anchors: Temperature set to 0.001 to minimize stochasticity; prior work shows lower temperatures produce focused, reproducible outputs
- Break condition: If source text is genuinely ambiguous, low temperature may force a single incorrect extraction rather than flagging uncertainty

## Foundational Learning

- **Concept: Zero-shot extraction with schema-constrained prompting**
  - Why needed here: The workflow uses no task-specific training; all extraction relies on pre-trained LLM knowledge guided by explicit field templates
  - Quick check question: Can you write a JSON schema that constrains an LLM to extract only `{material_name, ZT, temperature}` from a paragraph, returning null for missing fields?

- **Concept: Ontology-guided categorical matching**
  - Why needed here: Structural properties (lattice type, doping class) are evaluated via semantic embeddings and rule-based normalization—raw string matching fails on synonyms
  - Quick check question: How would you map "rocksalt," "rock-salt structure," and "NaCl-type" to a single canonical class?

- **Concept: Precision-recall trade-offs in extraction benchmarks**
  - Why needed here: Reported F1 = 0.91 (TE) vs. 0.82 (structural) reflects different error modes—recall drops on doping type (F1 = 0.51–0.64) due to implicit chemical reasoning
  - Quick check question: If an extractor finds 90% of true values but also returns 20% false positives, what's the F1?

## Architecture Onboarding

- **Component map:** Article fetch → XML/HTML parsing → regex filtering → token counting → LangGraph state machine (MatFindr → TEPropAgent → StructPropAgent → TableDataAgent) → JSON validation → unit normalization → deduplication → database write → Web explorer

- **Critical path:** 1) MatFindr must identify at least one valid material candidate → else early exit; 2) TEPropAgent and StructPropAgent run in parallel per material; 3) If tables exist → TableDataAgent processes, then results merged with text extraction; 4) Final JSON written only if ≥1 TE property present

- **Design tradeoffs:** GPT-4.1 vs. GPT-4.1 Mini: ~2% F1 drop for 5–10x cost reduction (paper chooses Mini for scale); Text vs. table priority: Text preferred when both available (context clarity); tables fill gaps; Temperature ≈ 0: Maximizes reproducibility but may suppress valid alternative interpretations

- **Failure signatures:** Doping type extraction consistently weakest (F1 = 0.51–0.64) across all models—expect errors on co-doping, compensation cases; Seebeck coefficient shows highest variability (R = 0.66–0.88 depending on model)—diverse linguistic reporting is the cause; Gemini models underperform on structural descriptors, particularly compound type classification

- **First 3 experiments:** 1) Reproduce benchmark on 10 papers: Run MatFindr → TEPropAgent pipeline with GPT-4.1 Mini, manually verify F1 against ground truth; 2) Ablate candidate seeding: Remove material name hints from TEPropAgent prompts and compare extraction drift rate on papers mentioning multiple compounds; 3) Stress-test table parsing: Feed articles with complex nested tables (multi-row headers, merged cells). Measure JSON parsing failure rate

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the workflow maintain high extraction accuracy in other materials science domains (e.g., batteries, catalysis) using only schema and prompt modifications?
  - Basis in paper: The conclusion states the workflow is "broadly generalizable to other functional materials domains... by adjusting schema and prompt templates," but provides data only for thermoelectrics
  - Why unresolved: The zero-shot agentic approach has not been benchmarked outside the thermoelectric corpus, where terminology and data density may differ
  - What evidence would resolve it: Benchmarking results (F1 scores) from applying the identical LangGraph architecture to a distinct domain like battery materials or MOFs

- **Open Question 2:** Can extraction coverage for properties like Seebeck coefficient be significantly improved by integrating visual parsing of figures?
  - Basis in paper: Section 4.1 notes that the relatively lower coverage of non-ZT properties "could be due to... [properties] usually represented in the figures" rather than text or tables, a modality the current pipeline ignores
  - Why unresolved: The current pipeline is limited to text, tables, and captions; it lacks mechanisms to interpret graphical data plots
  - What evidence would resolve it: A comparative study measuring the increase in unique property records after incorporating an image-to-text or plot-digitization agent

- **Open Question 3:** What specific architectural or prompt engineering strategies are required to resolve the low extraction accuracy for "doping type"?
  - Basis in paper: Table 2 shows "Doping Type" has the lowest performance (F1 = 0.51–0.64), and Section 3.2 attributes this to the models' inability to consistently interpret implicit chemical roles (donor vs. acceptor) without domain knowledge
  - Why unresolved: The current zero-shot approach fails to capture complex chemical logic, and simple rule-based heuristics proved insufficient for co-doping cases
  - What evidence would resolve it: Demonstration of a modified agent (e.g., one with access to a dopant database) achieving >0.90 F1 on the structural benchmark

## Limitations
- Doping type extraction weakness (F1 = 0.51-0.64) represents a fundamental limitation for materials discovery applications requiring complete compositional understanding
- Cross-domain generalizability remains low confidence without validation on alternative materials systems
- Table parsing accuracy depends heavily on source formatting quality, with complex nested structures potentially breaking the extraction pipeline

## Confidence
- **High**: Cost-efficiency claims (token counting, Mini vs. 4.1 performance tradeoff), basic workflow architecture (agent sequence, early-exit logic), numerical property extraction (ZT, conductivity, resistivity)
- **Medium**: Overall F1 metrics, structural property extraction (crystal class, space group), reproducibility of results
- **Low**: Cross-domain generalizability, handling of co-doping/compensated systems, robustness to diverse table formats

## Next Checks
1. **Benchmark Set Validation**: Manually verify ground truth labels for 10 randomly selected benchmark papers to quantify potential labeling inconsistencies affecting reported F1 scores
2. **Cross-Domain Transfer**: Apply the complete workflow to a corpus of battery or superconductor literature to test generalizability beyond thermoelectrics
3. **Table Structure Robustness**: Systematically test extraction accuracy across papers with progressively complex table formats (merged cells, multi-level headers, nested structures) to identify breaking points