---
ver: rpa2
title: Modeling Fair Play in Detective Stories with Language Models
arxiv_id: '2507.13841'
source_url: https://arxiv.org/abs/2507.13841
tags:
- story
- reader
- detective
- stories
- coherence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a probabilistic framework for modeling fair
  play in detective fiction, formalizing the literary tension between coherence and
  surprise. The authors define four reader models (gullible, brilliant-detective,
  actual, and know-it-all) and derive theoretical tradeoffs showing that maintaining
  both surprise and coherence is fundamentally challenging for a single reader model.
---

# Modeling Fair Play in Detective Stories with Language Models

## Quick Facts
- **arXiv ID**: 2507.13841
- **Source URL**: https://arxiv.org/abs/2507.13841
- **Reference count**: 30
- **Primary result**: Presents probabilistic framework for fair play in detective fiction, validated through experiments with human-written and LLM-generated stories

## Executive Summary
This paper introduces a probabilistic framework to model the literary concept of "fair play" in detective fiction, formalizing the tension between narrative surprise and coherence. The authors define four reader models (gullible, brilliant-detective, actual, and know-it-all) and prove that maintaining both surprise and coherence for a single reader model is theoretically self-contradictory. The framework is validated through experiments with Sherlock Holmes and Hercule Poirot stories, plus LLM-generated detective stories using various models including Llama and Gemini variants.

## Method Summary
The framework generates 25-paragraph detective stories with controlled length and instructions to include a distractor. It uses four reader models to evaluate stories: a gullible reader that naïvely follows clues, a brilliant-detective reader that correctly interprets clues, an actual reader approximating human behavior, and a know-it-all reader that marginalizes over all possible endings. The method employs LLM judges (o1-mini, o3-mini) to estimate probability distributions over suspects at each story position. Key metrics include surprise score (probability assigned to true culprit by gullible reader), coherence score (probability by know-it-all reader), fair play score (difference between coherence and surprise), and expected revelation content (predictability of clues given ending).

## Key Results
- LLM-generated stories can produce clear culprits and distractors (G-VAL > 0.83 for most models) but generally fail to balance surprise and coherence effectively
- Most models produce low fair play scores, with large models performing slightly better but still inconsistent
- ERC metric reveals weak dependence between clues and revelation in most generated stories (generally <0.04 per paragraph)
- Real stories show higher surprise scores, consistent with literary descriptions of Agatha Christie's work (SS=0.466 vs. 0.634 for Poirot)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maintaining both narrative surprise and coherence for a single reader model is theoretically self-contradictory
- Mechanism: Cross-entropy-based uninformedness metric shows that as internal coherence increases, predictions become more similar to know-it-all reader, reducing surprise. Strong surprise requires misleading beyond uniform prediction, which contradicts intelligence when clues are coherent
- Core assumption: External coherence holds - brilliant detective's clue effectiveness approximates know-it-all reader
- Evidence anchors: Section 4, Theorem proves "A reader M cannot be both strongly surprised and intelligent"; Eq. 22-23 show formal derivation creating contradiction for small ϵex
- Break condition: If external coherence fails (brilliant detective's inference diverges from true generation process)

### Mechanism 2
- Claim: Fair play requires divergent predictions between gullible reader (M0) and capable reader (M1 or M∞)
- Mechanism: Gullible reader interprets clues naïvely and is misled toward distractor, while brilliant-detective or know-it-all reader correctly integrates clues toward true culprit. Fair play score captures this gap
- Core assumption: LLMs can approximate both reader types when prompted appropriately
- Evidence anchors: Section 3.4 defines four reader models; Section 5.2, Eq. 26 defines SFP = (1/N)Σ(M∞_t(i) - M0_t(i))
- Break condition: If generation model has very low diversity, know-it-all reader converges early, making SC uninformative

### Mechanism 3
- Claim: Expected Revelation Content measures whether clues form meaningful chain by testing if ending predicts earlier clues
- Mechanism: ERC compares probability of predicting masked paragraph given revelation-adjacent paragraphs versus baseline. High ERC indicates clues are causally linked to ending
- Core assumption: Brilliant-detective reader's conditional probabilities over clues can be approximated via multiple-choice prediction with LLM judge
- Evidence anchors: Section 5.2, Eq. 27 defines ERC = E_c~M1[p(c|Cr...N)] - E_c~M1[p(c)]; Section 6.2 shows ERC values generally low (<0.04 per paragraph)
- Break condition: If paragraphs are stylistically similar or judge model has strong position bias

## Foundational Learning

- Concept: **Cross-entropy and KL divergence for belief tracking**
  - Why needed here: Uninformedness (cross-entropy between know-it-all and another reader) quantifies how surprised or informed a reader is at each story position
  - Quick check question: Given two probability distributions over culprits [0.7, 0.2, 0.1] and [0.3, 0.4, 0.3], which reader is more "uninformed" relative to a know-it-all distribution of [0.8, 0.1, 0.1]?

- Concept: **Marginalization over latent variables in autoregressive models**
  - Why needed here: Know-it-all reader estimates culprit probability by sampling continuations and computing relative frequency - marginalizing over all possible endings
  - Quick check question: If you sample 100 continuations and 60 identify suspect A as culprit, what is M∞'s probability for A? What are practical limitations of this estimate?

- Concept: **Martingale properties in belief updating**
  - Why needed here: Ely et al.'s assumption (reader probabilities are calibrated to outcome frequencies) may conflict with coherent surprise - calibrated readers cannot be surprised and coherent simultaneously
  - Quick check question: If a reader assigns p=0.3 to culprit Y at step i, and story is externally coherent, what should marginal probability of Y being true culprit be across all sampled continuations?

## Architecture Onboarding

- Component map: Story Generator -> Gullible Reader Proxy -> Know-it-all Reader Proxy -> ERC Judge -> Validation Judge
- Critical path: 1) Generate story paragraph-by-paragraph with explicit distractor instructions 2) At each prefix position i, query gullible reader for probability distribution over suspects 3) Sample multiple continuations from prefix to estimate know-it-all distribution 4) Compute SS, SC, SFP across all positions 5) For ERC, mask each pre-revelation paragraph and test predictability given ending
- Design tradeoffs:
  - Sampling cost vs. coherence accuracy: More continuations improve M∞ estimation but scale linearly. Authors use temperature=1; lower temperatures reduce diversity, inflating SC artificially
  - Judge model selection: Using same model family for generation and evaluation may introduce bias; authors use different models (o1-mini, o3-mini) for reading tasks
  - Paragraph vs. clue granularity: ERC operates on paragraphs for tractability, but this conflates multiple clues per paragraph
- Failure signatures:
  - Negative fair play score: Gullible reader predicts culprit before model decides (early convergence)
  - ERC ≈ 0: Clues are decorrelated from ending; story is deus ex machina
  - Low G-VAL (<0.8): Generator fails to produce clear culprit/distractor distinction
  - High name repetition (>80%): Mode collapse limiting diversity
- First 3 experiments:
  1. Baseline metric validation: Apply SS to Sherlock Holmes vs. Hercule Poirot stories; confirm Christie shows higher surprise (lower SS), matching literary descriptions
  2. Model scale comparison: Generate 25-paragraph stories with Llama-3.2-1B, -3B, Llama-3.1-8B, -70B and compare SFP distribution; test whether larger models achieve positive fair play more consistently
  3. ERC ablation: For fixed model (e.g., Llama-3.1-70B), compare ERC with revelation paragraphs provided (AR) vs. pre-revelation paragraphs (BR); verify BR > AR, confirming clues before revelation carry more predictive information than post-revelation context

## Open Questions the Paper Calls Out

- **Question**: Do the proposed probabilistic metrics (Surprise Score, Coherence Score) correlate with human reader evaluations of "fair play" and narrative quality?
  - Basis in paper: [explicit] The authors state, "In future work, we intend to validate the metrics against human predictions"
  - Why unresolved: Current experiments rely entirely on LLMs to simulate "gullible" and "brilliant" reader models, leaving gap between computational scores and actual human psychological experience
  - What evidence would resolve it: Correlation coefficients derived from comparing model's metric scores against human subject ratings for surprise and coherence across diverse set of human-written and generated stories

- **Question**: Can the proposed reference-less metrics be effectively utilized as reward functions in Reinforcement Learning to train models that satisfy fair play constraints?
  - Basis in paper: [inferred] Paper notes metrics "can be leveraged by future work to enhance fair play capabilities of language models, e.g., by utilizing policy gradient methods," but currently only evaluates pre-trained models
  - Why unresolved: While theoretical framework allows for gradient-based optimization, untested whether optimizing specifically for Fair Play Score results in readable stories or unintended degeneration
  - What evidence would resolve it: Experiment showing models fine-tuned to maximize SFP generate stories with higher human-rated fair play than baseline models

- **Question**: Does "anchoring" phenomenon (mode collapse) in LLMs artificially inflate coherence scores by restricting output diversity rather than improving logical deduction?
  - Basis in paper: [inferred] Discussion notes models trained with RLHF use "anchors" (repetitive phrases/names) to guide generation, which "significantly limiting the output space," potentially confounding coherence measurement
  - Why unresolved: If model converges on culprit early due to stylistic bias rather than clue logic, "know-it-all" reader might score it as highly coherent, masking lack of actual logical inference
  - What evidence would resolve it: Analysis of coherence scores on generated stories where stylistic anchors are controlled or suppressed, to see if scores remain high based purely on clue-dependent logic

## Limitations

- Theoretical framework assumes external coherence may not hold in LLM-generated stories where "true culprit" is determined by generation process rather than authorial intent
- M∞ reader relies on continuation sampling, which becomes computationally expensive and statistically unreliable for long stories
- Framework assumes suspects are known upfront, limiting applicability to open-world mystery settings
- ERC's paragraph-level granularity may conflate multiple clues and surface stylistic features with genuine narrative coherence

## Confidence

- **High confidence**: Theoretical impossibility of maintaining both surprise and coherence for single reader model (Mechanism 1); framework's core definitions (SS, SC, SFP) are mathematically sound and clearly operationalized
- **Medium confidence**: Empirical validation showing LLMs generally fail to balance surprise and coherence; results consistent across models but sample sizes are modest (20-30 stories per model)
- **Low confidence**: ERC metric's ability to capture "meaningful clue chains"; metric shows weak dependence overall, and sensitivity to paragraph-level vs. clue-level analysis remains unclear

## Next Checks

1. **Robustness to sampling parameters**: Vary continuation sample counts (5, 10, 20) for M∞ estimation and measure stability of SC and SFP across models. Test whether computational cost improvements (temperature tuning, beam search) preserve fairness metrics

2. **Real-world external coherence**: Apply framework to human-written stories with ambiguous or multiple plausible endings (e.g., Agatha Christie's "The Murder of Roger Ackroyd"). Test whether framework identifies true fair-play stories versus deceptive narratives

3. **ERC clue-granularity analysis**: Implement token-level or sentence-level ERC to isolate individual clues' predictability. Compare with paragraph-level ERC to quantify information loss from coarse granularity and identify which granularities best capture narrative coherence