---
ver: rpa2
title: Boosting Adversarial Transferability via Ensemble Non-Attention
arxiv_id: '2511.08937'
source_url: https://arxiv.org/abs/2511.08937
tags:
- adversarial
- attention
- ensemble
- gradient
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NAMEA, a novel ensemble attack method that
  improves cross-architecture adversarial transferability by integrating gradients
  from non-attention areas of ensemble models with meta-learning. The core idea is
  that attention areas of heterogeneous models differ sharply, so non-attention areas
  of vision transformers (ViTs) are likely to be the focus of convolutional neural
  networks (CNNs) and vice versa.
---

# Boosting Adversarial Transferability via Ensemble Non-Attention

## Quick Facts
- **arXiv ID:** 2511.08937
- **Source URL:** https://arxiv.org/abs/2511.08937
- **Reference count:** 40
- **Primary result:** NAMEA improves cross-architecture adversarial transferability by 15.0% and 9.6% over AdaEA and SMER respectively

## Executive Summary
This paper introduces NAMEA, a novel ensemble attack method that improves cross-architecture adversarial transferability by integrating gradients from non-attention areas of ensemble models with meta-learning. The core idea is that attention areas of heterogeneous models differ sharply, so non-attention areas of vision transformers (ViTs) are likely to be the focus of convolutional neural networks (CNNs) and vice versa. NAMEA decouples gradients from attention and non-attention areas, and merges them using a meta-gradient optimization process. Empirical evaluations on the ImageNet dataset show that NAMEA outperforms state-of-the-art ensemble attacks AdaEA and SMER by an average of 15.0% and 9.6% in attack success rates, respectively. The method also demonstrates superior robustness against various defense methods and real-world API attacks.

## Method Summary
NAMEA is a novel ensemble attack method that improves adversarial transferability across heterogeneous architectures. The core innovation is extracting and utilizing non-attention gradients from ensemble models. The method uses meta-learning with bi-level optimization: meta-training optimizes attention-area gradients, while meta-testing optimizes non-attention gradients with an NAE module that masks attention regions using Gaussian noise. The final update merges both gradients with architecture-specific scaling through the GSO module. The approach decouples attention and non-attention gradients to prevent interference and stabilize the update direction.

## Key Results
- NAMEA achieves 15.0% and 9.6% higher attack success rates compared to AdaEA and SMER respectively
- NAMEA maintains strong transferability against diverse defense methods and real-world API attacks
- The method demonstrates particular effectiveness in cross-architecture scenarios where attention patterns differ significantly between models

## Why This Works (Mechanism)

### Mechanism 1: Cross-Architecture Attention Complementarity
- Claim: Non-attention regions of one architecture contain transferable information for another architecture.
- Mechanism: ViTs and CNNs exhibit sharply different attention patterns (Fig. 1a). Masking CNN attention areas (filling with noise) causes ~30% accuracy drop for CNNs but only ~10% for ViTs (Fig. 1c), suggesting ViTs rely on complementary regions. NAMEA extracts non-attention gradients to capture these complementary features.
- Core assumption: Heterogeneous models rely on semantically distinct but potentially overlapping feature regions.
- Evidence anchors:
  - [abstract]: "non-attention areas of ViTs are likely to be the focus of CNNs and vice versa"
  - [section]: Fig. 1(c) shows masking experiment—CNN accuracy drops ~30% vs ViT ~10%
  - [corpus]: "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks" confirms heterogeneous transfer degradation is a known challenge
- Break condition: If target models share nearly identical attention maps with surrogates, non-attention extraction provides marginal benefit.

### Mechanism 2: Gradient Decoupling via Meta-Gradient Optimization
- Claim: Decoupling attention and non-attention gradients prevents gradient interference and stabilizes update direction.
- Mechanism: Meta-training step optimizes attention-area gradients (Eq. 5-6); meta-testing step optimizes non-attention gradients with NAE module masking attention regions with Gaussian noise (Eq. 7-10). Final merged gradient: $g^{t+1} = g^K_{tr} + g^K_{te} \odot M^K$ (Eq. 11). The mask $M^K$ prevents attention-gradient interference.
- Core assumption: Attention and non-attention gradients provide independent, complementary transfer signals.
- Evidence anchors:
  - [section]: "Ablation study on gradients respectively from attention and non-attention areas" (Fig. II) shows both contribute; removal drops ASR by 7-9%
  - [section]: Eq. 11 explicitly masks meta-testing gradient to avoid interference
  - [corpus]: "Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks" addresses gradient coherence/divergence trade-offs but without explicit attention decoupling
- Break condition: If gradient masking removes too much signal (threshold η poorly calibrated), attack degrades. Fig. 4 shows ASR peaks at η=0.6.

### Mechanism 3: Architecture-Specific Gradient Scaling
- Claim: Scaling intermediate-layer gradients for CNNs and filtering low-magnitude channel gradients for ViTs improves transferability.
- Mechanism: GSO module applies layer-wise scaling $\lambda(l) = \lambda_1 + \lambda_2 \cdot (L/l)$ for CNNs (Eq. 12-13), emphasizing shallower layers. For ViTs, channel-wise tanh scaling reduces contribution of below-average gradient channels (Eq. 14).
- Core assumption: Intermediate CNN features are more transferable; small ViT gradients harm ensemble optimization.
- Evidence anchors:
  - [section]: "Recent studies have proven that intermediate-layer features of CNNs are more transferable" (citing Huang et al. 2019; Zhu et al. 2024)
  - [section]: GSO removal causes ~2% ASR drop (Fig. 5 left)
  - [corpus]: Weak direct evidence in neighbors for GSO specifically; mechanism is paper-specific
- Break condition: If scaling factors are extreme, gradient explosion/vanishing may occur. Fig. IV shows performance is relatively insensitive to λ₁, λ₂.

## Foundational Learning

- Concept: **Grad-CAM Attention Extraction**
  - Why needed here: Extracts spatial attention maps H_n(x) to identify attention vs non-attention regions (Eq. 3-4).
  - Quick check question: Can you explain why ReLU is applied in Grad-CAM to discard negative importance weights?

- Concept: **Meta-Learning (Bi-level Optimization)**
  - Why needed here: NAMEA uses outer iterations (T) and inner loops (K) with separate meta-training/meta-testing paths, analogous to MAML-style bi-level optimization.
  - Quick check question: What is the difference between inner-loop task adaptation and outer-loop meta-update?

- Concept: **Iterative Gradient-Based Attacks (I-FGSM/MI-FGSM)**
  - Why needed here: NAMEA is plug-and-play with base attacks; understanding sign gradient updates and momentum is prerequisite.
  - Quick check question: Why does momentum (MI-FGSM) generally improve transferability over vanilla I-FGSM?

## Architecture Onboarding

- Component map:
  Input (x, y) → [Outer Loop t=0..T-1]
                   ├── Meta-Training (K inner loops): Standard gradient on attention regions
                   ├── Meta-Testing (K inner loops): NAE module masks attention → gradient on non-attention
                   └── Final Update: Merge g_tr + g_te ⊙ M^K
  Output: x_adv

- Critical path:
  1. Attention extraction via Grad-CAM (select layer: ViT→final self-attention block; RN18→last conv block; Inc-v3→Mixed_7b)
  2. Thresholding with η=0.6 to create binary attention mask (Eq. 7)
  3. NAE module fills attention regions with Gaussian noise N(0,1)—not zeros—to maximize distraction
  4. GSO applies architecture-specific scaling before gradient merge

- Design tradeoffs:
  - η threshold: Lower η → fewer non-attention pixels extracted → less transfer signal; Higher η → more noise in gradient. Optimal at 0.6 (Fig. 4).
  - Inner loops K: Set to 4× number of surrogate models to ensure each model is sampled sufficiently.
  - Random noise vs zero-padding: Random noise achieves ~2.7% higher ASR (Fig. 5 right).

- Failure signatures:
  - ASR drops sharply on homogeneous target models: May indicate over-optimization for heterogeneous transfer.
  - Gradient magnitude collapse: Check GSO scaling factors; ViT channel scaling may suppress too much signal.
  - Memory overflow: NAE requires storing attention maps per inner loop; K=16 with 4 surrogates is baseline.

- First 3 experiments:
  1. **Sanity check**: Run NAMEA with I-FGSM base on 4 surrogates (ViT-T, DeiT-T, RN18, Inc-v3), validate ASR improvement over Ens baseline on ImageNet subset (1000 images, one per class).
  2. **Ablation threshold η**: Sweep η ∈ {0.1, 0.3, 0.5, 0.7, 0.9} and plot ASR vs η to reproduce Fig. 4 peak at 0.6.
  3. **Component isolation**: Run (-Mtest), (-Mtrain), (-GSO) ablations to verify each contributes (expect ~7-9% drop removing meta-steps, ~2% drop removing GSO per Fig. 5 left).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the combination of non-attention gradients and input transformations be optimized to prevent the loss of semantic features that currently limits performance?
- Basis in paper: [inferred] The discussion of Figure 7 notes that while NAMEA outperforms baselines, combining it with BSR causes performance drops in certain settings, likely due to "loosing substantial semantic features" from excessive input alteration.
- Why unresolved: The authors identify the symptom (loss of semantics) but do not propose a mechanism to balance aggressive gradient manipulation with feature preservation.
- What evidence would resolve it: A modified loss function or regularization term that maintains semantic consistency during the meta-testing phase, resulting in higher ASRs when combined with transformations like BSR.

### Open Question 2
- Question: What architectural features of Vision Transformers allow them to generate perturbations that are more transferable to CNNs than vice versa?
- Basis in paper: [inferred] Results in Table IX and the accompanying discussion observe that using only ViTs as surrogates maintains high transferability to CNNs, but using only CNNs significantly degrades transferability to ViTs.
- Why unresolved: The paper documents this asymmetry as a phenomenon ("adversarial perturbations crafted from ViTs are more transferable") but does not provide a theoretical explanation for the directional bias.
- What evidence would resolve it: A theoretical analysis or visualization showing that ViT attention maps capture more universal features that overlap with CNN decision boundaries compared to the reverse.

### Open Question 3
- Question: Is the effectiveness of NAMEA dependent on the specific post-hoc attention extraction method (Grad-CAM), or would native attention mechanisms provide better results?
- Basis in paper: [inferred] The Non-Attention Extraction (NAE) module relies on Grad-CAM to approximate attention maps, which requires tuning a threshold $\eta$ and upsampling, potentially introducing noise.
- Why unresolved: The paper tests different thresholds but does not compare Grad-CAM against other saliency methods or native ViT attention weights to determine if the extraction method is a bottleneck.
- What evidence would resolve it: An ablation study comparing NAMEA's success rate when using raw self-attention maps from ViTs versus the current Grad-CAM approach.

## Limitations

- Gains are demonstrated primarily on ImageNet and against ResNet/ViT/ConvNeXt/CoAtNet families, with untested performance on non-standard architectures like MLP-Mixers or Vision MLPs
- The paper assumes sharp attention divergence across heterogeneous models, but if target models use similar attention mechanisms, non-attention extraction may provide marginal benefit
- GSO's empirical justification is weaker than attention-based mechanisms, with ablation showing only ~2% ASR improvement, suggesting limited practical impact

## Confidence

- Mechanism 1 (Cross-Architecture Attention Complementarity): **High** - Strong empirical evidence from masking experiments and ablation showing 7-9% ASR drop when removing meta-steps
- Mechanism 2 (Gradient Decoupling via Meta-Gradient Optimization): **High** - Well-grounded in bi-level optimization framework with clear mathematical formulation and ablation validation
- Mechanism 3 (Architecture-Specific Gradient Scaling): **Medium** - Theoretically motivated but weaker empirical support; ablation shows modest impact and limited corroborating literature

## Next Checks

1. **Architecture Generalization Test**: Evaluate NAMEA against MLP-Mixer and Swin Transformer targets to verify attention complementarity holds beyond ViT/CNN pairs
2. **Attention Map Correlation Analysis**: Compute correlation coefficients between Grad-CAM attention maps across all surrogate-target pairs to quantify actual attention divergence
3. **Zero-Padding Baseline Comparison**: Systematically compare random Gaussian noise vs zero-padding across all experiments to validate NAE's noise choice is optimal rather than arbitrary