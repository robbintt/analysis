---
ver: rpa2
title: Large Language Models Assisting Ontology Evaluation
arxiv_id: '2507.14552'
source_url: https://arxiv.org/abs/2507.14552
tags:
- ontology
- evaluation
- ontologies
- llms
- suggestions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OE-Assist, a framework using large language
  models (LLMs) to assist ontology evaluation through competency question (CQ) verification.
  The authors present OntoEval, a dataset of 1,393 CQs paired with ontologies and
  stories, and evaluate both automatic and semi-automatic evaluation methods.
---

# Large Language Models Assisting Ontology Evaluation

## Quick Facts
- arXiv ID: 2507.14552
- Source URL: https://arxiv.org/abs/2507.14552
- Reference count: 38
- Key outcome: LLM-assisted ontology evaluation shows 13% accuracy gain with correct suggestions but 28% accuracy drop with incorrect ones, resulting in no net improvement

## Executive Summary
This paper introduces OE-Assist, a framework using large language models (LLMs) to assist ontology evaluation through competency question (CQ) verification. The authors present OntoEval, a dataset of 1,393 CQs paired with ontologies and stories, and evaluate both automatic and semi-automatic evaluation methods. For automatic evaluation, o1-preview and o3-mini achieved 0.66 and 0.72 macro-F1 respectively, outperforming other models and approaching human-level performance. In a user study with 19 ontology engineers, LLM assistance improved accuracy by 13% when suggestions were correct, but reduced it by 28% when incorrect, resulting in no net improvement. Users found tasks slightly easier with LLM suggestions, though learning effects were diminished. The study highlights the potential and limitations of LLM-assisted ontology evaluation, emphasizing the importance of suggestion accuracy.

## Method Summary
The study evaluates LLM performance on ontology competency question verification using the OntoEval dataset (1,393 CQ-ontology pairs). Automatic evaluation tests model accuracy directly against gold standards using o1-preview, o3-mini, and GPT-4o. Semi-automatic evaluation involves 19 ontology engineers using a web interface to assess CQs with/without LLM suggestions (binary label + SPARQL query). Models are accessed via Azure API with specific temperature and spacing settings to ensure reproducibility. The study uses macro-F1 for imbalanced datasets and accuracy for balanced subsets, with statistical tests for user study outcomes.

## Key Results
- Automatic evaluation: o1-preview achieved 0.66 macro-F1, o3-mini 0.58 macro-F1 on full OntoEval dataset
- Semi-automatic evaluation: Correct LLM suggestions improved accuracy by 13%, but incorrect suggestions caused 28% accuracy drops
- User experience: Tasks perceived as slightly easier with LLM assistance (mean difficulty 3.5 vs 3.9 unassisted), but learning effects were reduced
- Suggestion utility: Only 32% endorsed binary labels, while 63% found SPARQL queries useful

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform automatic CQ verification at performance levels comparable to average human evaluators when using reasoning-enhanced models.
- Mechanism: The system provides the LLM with task description, few-shot examples, CQ, user story, and ontology. The LLM outputs a binary label (Yes/No) indicating whether the CQ is modelled, plus a SPARQL query as justification. The SPARQL query creates a verifiable reasoning trace connecting natural language requirements to formal ontology axioms.
- Core assumption: The translation between natural language CQs and SPARQL queries represents the kind of symbolic-natural language boundary that reasoning models can navigate reliably.
- Evidence anchors: [abstract] "We found that automated LLM-based evaluation with o1-preview and o3-mini perform at a similar level to the average user's performance." [Section 6.1] "o1-preview overall outperforms the other LLMs on both datasets... 0.66 macro-F1" with o3-mini achieving 0.58 macro-F1 on the full OntoEval dataset.
- Break condition: Performance degrades significantly on ontologies with poor or missing labels/comments, domain-specific content requiring specialized expertise, or unclear/ambiguous CQs.

### Mechanism 2
- Claim: LLM suggestions in semi-automatic settings produce asymmetric effects—significant accuracy gains when correct, but substantial accuracy losses when incorrect.
- Mechanism: The LLM provides suggestions (label + SPARQL query) that users integrate into their evaluation workflow. Correct suggestions reduce cognitive load and provide useful starting points. Incorrect suggestions create an anchoring bias that misleads users, causing them to accept erroneous guidance even when they could have solved the task correctly unassisted.
- Core assumption: Users trust LLM suggestions sufficiently to be influenced by them, but lack reliable mechanisms to validate suggestion correctness.
- Evidence anchors: [abstract] "LLM suggestions improved accuracy by 13% when correct but caused a 28% drop when incorrect." [Section 6.2] "Using a chi-squared test, we found statistically significant associations for both correct (χ² = 4.12, p = 0.042) and incorrect (χ² = 4.61, p = 0.0318) LLM suggestions."
- Break condition: When LLM accuracy falls below a threshold, the positive and negative effects cancel out, yielding no net benefit. Systems must either ensure high suggestion accuracy or implement safeguards (e.g., confidence scores, pre-validated SPARQL queries).

### Mechanism 3
- Claim: SPARQL query suggestions serve as both verification artifacts and cognitive scaffolds, but users adopt heterogeneous strategies for integrating them.
- Mechanism: The generated SPARQL query provides explicit traceability between CQ elements and ontology constructs. Users either (i) examine the query first to identify relevant classes/properties, then verify in Protégé ("code-first"), or (ii) explore the ontology conceptually first, then consult the query for confirmation ("concept-first").
- Core assumption: Users can interpret SPARQL queries accurately and map query components to CQ semantics.
- Evidence anchors: [Section 7] "Many users started the evaluation from the SPARQL queries, to find and match the named classes and properties with the ones mentioned in the CQ. A second group of users operated in reverse." [Section 6.2] Only 32% endorsed the binary Yes/No recommendation, while 63% strongly agreed suggestions were useful.
- Break condition: Queries are less helpful when ontologies lack rdfs:label or descriptive annotations, when CQs are domain-specific and require subject-matter expertise, or when LLM-generated hierarchies are flawed.

## Foundational Learning

- Concept: **Competency Questions (CQs) as Functional Requirements**
  - Why needed here: The entire framework assumes understanding that CQs define what an ontology should be able to answer—they are testable specifications, not just documentation.
  - Quick check question: Given the CQ "What was the disposition of the organ at a specific point in time?", can you identify the classes (Organ, Parthood, TimeInterval) and properties (isWholeIncludedIn, hasTimeInterval) required to answer it?

- Concept: **SPARQL Query Semantics**
  - Why needed here: Users must evaluate whether generated SPARQL queries correctly capture CQ meaning and match ontology structure. Without this, they cannot validate LLM suggestions.
  - Quick check question: Given a simple CQ "Who built an organ?", would the query pattern `?person :built ?organ . ?organ a :Organ` correctly verify modelling if the ontology uses `:constructedBy` instead of `:built`?

- Concept: **Ontology Modelling Patterns**
  - Why needed here: Evaluating whether a CQ is "modelled" requires recognizing common patterns (e.g., n-ary relations for temporal aspects, property chains, restrictions). The study classified CQs as Simple (≤2 classes, 1 property) vs Complex.
  - Quick check question: Why might "What was the disposition of the organ at a specific point in time?" require an n-ary relation pattern rather than a simple binary property?

## Architecture Onboarding

- Component map: Input Layer (CQ + Story + Ontology) -> Prompt Engineering Module -> LLM Inference Engine -> Output Parser -> User Interface -> Evaluation Layer

- Critical path: 1) Dataset preparation: CQ-ontology pairs with gold standard annotations, 2) Prompt construction: Few-shot examples must be excluded from OntoEval-small to prevent leakage, 3) LLM inference: API calls with temperature=0 for reproducibility; spacing of ≥5 hours between runs to avoid caching, 4) Suggestion delivery: Label + SPARQL query presented to user (semi-automatic) or compared directly to gold standard (automatic), 5) Accuracy computation: Macro-F1 for imbalanced datasets, Accuracy for balanced subsets

- Design tradeoffs:
  - Model selection: o1-preview offers best accuracy (0.66 F1) but costs 34x more than o3-mini; o3-mini provides reasonable performance (0.58 F1) for resource-constrained deployments
  - Suggestion presentation: Binary labels are distrusted (32% endorsement); SPARQL queries are valued (63% found useful)—design should emphasize queries over labels
  - Learning vs. assistance: Unassisted tasks show stronger learning effects (40% improvement in second half vs. 15% with assistance); over-reliance on suggestions may hinder skill development
  - Dataset balance: Imbalanced data (1,204 Yes vs. 189 No) requires macro-F1; user studies need balanced subsets (OntoEval-small: 10 Yes, 10 No) for controlled experiments

- Failure signatures:
  - GPT-4o near-random performance (0.34-0.48 F1): Indicates non-reasoning models struggle with ontology evaluation; upgrade to reasoning models
  - 28% accuracy drop with incorrect suggestions: Users anchor on wrong guidance; implement confidence thresholds or pre-validate SPARQL queries
  - High "I don't know" rates for difficult tasks (mean difficulty 3.9 vs. 2.4): Users disengage when overwhelmed; simplify CQs or provide better scaffolding
  - Domain-specific CQs without subject expertise: Users can assess structure but not semantic correctness; pair with domain expert or provide glossaries

- First 3 experiments:
  1. Baseline replication: Run o1-preview and o3-mini on a 100-CQ subset from OntoEval, measuring macro-F1. Verify you can reproduce ~0.66 and ~0.58 respectively with temperature=0, spaced runs.
  2. Suggestion accuracy threshold analysis: Simulate semi-automatic evaluation with artificially degraded suggestion accuracy (50%, 60%, 70%, 80%) to identify the break-even point where assistance yields net positive outcomes.
  3. SPARQL-only vs. label+SPARQL interface: A/B test two interface variants—one showing only the SPARQL query, one showing both label and query—to isolate which component drives the 63% perceived usefulness and whether removing the label reduces the 28% accuracy penalty from incorrect suggestions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can interface-level safeguards, such as automated pre-checking of generated SPARQL queries or confidence indicators, effectively prevent the significant accuracy drop observed when LLMs provide incorrect suggestions?
- Basis: [explicit] The authors suggest implementing safeguards like "pre-checking suggested SPARQL queries... or including some kind of confidence assessment" to mitigate the 28% performance drop caused by incorrect guidance.
- Why unresolved: The current study measured the impact of raw suggestions but did not implement or test technical barriers to filter or flag low-quality LLM outputs before the user sees them.
- Evidence: A comparative user study measuring accuracy retention on "incorrect suggestion" tasks with and without these safety mechanisms enabled.

### Open Question 2
- Question: Does the inclusion of dedicated learning phases or adaptive interfaces successfully mitigate user over-reliance on LLMs and preserve long-term skill acquisition?
- Basis: [explicit] The authors note that "assistance seems to substitute the effort that would otherwise lead to similar improvement" and suggest "dedicated learning phases" to prevent skill atrophy.
- Why unresolved: The study identified a trade-off between immediate efficiency and learning but did not test pedagogical interventions to resolve it.
- Evidence: A longitudinal experiment tracking user performance in unassisted "transfer" tasks after using either a standard assistive interface or one designed with learning features.

### Open Question 3
- Question: Do the observed benefits of LLM-assisted evaluation generalize to open-source or smaller models and a broader demographic of ontology engineers?
- Basis: [explicit] The authors list as a limitation that "future work should involve a broader group of ontology engineers and a wider range of LLMs," noting the participant pool was small (N=19) and relied on specific OpenAI models.
- Why unresolved: It is unclear if the 13% accuracy gain is dependent on the high reasoning capability of the specific model used (o1-preview) or applies to cheaper, more accessible models.
- Evidence: Replicating the experimental setup with diverse model architectures (e.g., Llama, Mistral) and a larger, more varied participant pool.

## Limitations

- Critical accuracy asymmetry: LLM suggestions create 13% accuracy gains with correct suggestions but 28% accuracy drops with incorrect ones, effectively canceling net benefits
- Model dependency: Performance heavily depends on using reasoning models like o1-preview; non-reasoning models like GPT-4o perform near random baseline
- Limited generalizability: Small participant pool (N=19) and potential dataset/domain-specific characteristics may not generalize to all ontology evaluation scenarios

## Confidence

- Automatic evaluation performance (0.66 macro-F1 for o1-preview): High confidence
- Asymmetric effects of LLM suggestions: Medium confidence (novel finding, requires replication)
- SPARQL query component being more valuable than binary labels: Medium confidence (user perception, not empirically validated for accuracy impact)

## Next Checks

1. Conduct A/B testing with confidence thresholds on LLM suggestions to determine if pre-filtering low-confidence outputs eliminates the 28% accuracy penalty while preserving the 13% gain from correct suggestions
2. Replicate the asymmetric effect study with domain experts vs. ontology engineers to identify whether subject matter expertise mitigates the negative impact of incorrect suggestions
3. Evaluate long-term learning effects by conducting a longitudinal study where users perform multiple ontology evaluations with/without assistance, measuring skill development over time