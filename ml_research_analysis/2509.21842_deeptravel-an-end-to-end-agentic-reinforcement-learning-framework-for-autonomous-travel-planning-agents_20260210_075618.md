---
ver: rpa2
title: 'DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous
  Travel Planning Agents'
arxiv_id: '2509.21842'
source_url: https://arxiv.org/abs/2509.21842
tags:
- tool
- travel
- agent
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepTravel, an end-to-end agentic reinforcement
  learning framework for autonomous travel planning agents. The framework addresses
  the challenges of dynamic travel environments and open-ended task verification by
  constructing a robust sandbox environment that caches real-world data, developing
  a hierarchical reward modeling system with trajectory and turn-level verifiers,
  and introducing a reply-augmented reinforcement learning method with experience
  replay.
---

# DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents

## Quick Facts
- **arXiv ID:** 2509.21842
- **Source URL:** https://arxiv.org/abs/2509.21842
- **Reference count:** 40
- **Primary result:** Qwen3-32B trained with DeepTravel RL achieves 73.21% final pass rate, outperforming OpenAI-o1/o3 and DeepSeek-R1 on travel planning tasks

## Executive Summary
This paper introduces DeepTravel, an end-to-end agentic reinforcement learning framework for autonomous travel planning agents. The framework addresses the challenges of dynamic travel environments and open-ended task verification by constructing a robust sandbox environment that caches real-world data, developing a hierarchical reward modeling system with trajectory and turn-level verifiers, and introducing a replay-augmented reinforcement learning method with experience replay. The proposed approach enables small-size LLMs (e.g., Qwen3-32B) to significantly outperform state-of-the-art reasoning LLMs (e.g., OpenAI-o1/o3 and DeepSeek-R1) on travel planning tasks.

## Method Summary
DeepTravel uses a two-stage training process: (1) Supervised fine-tuning (SFT) cold-start on 1K distilled trajectories from DeepSeek-R1, and (2) Replay-augmented reinforcement learning using GRPO-style optimization. The framework employs a sandbox environment with six travel tools (flight, train, hotel, POI, route, web search) that caches real-world data to ensure consistent outputs during training. A hierarchical reward system combines trajectory-level verification (checking spatiotemporal feasibility and overall completeness) with turn-level verification (validating tool response consistency). The replay-augmented RL method saves failed queries to an experience buffer and periodically replays them to improve learning efficiency and sample utilization.

## Key Results
- Qwen3-32B with DeepTravel RL achieves 73.21% final pass rate on complex travel planning scenarios
- Outperforms OpenAI-o1-mini (56.14%), o1 (54.16%), o3-mini (55.27%), and DeepSeek-R1 (49.62%) on offline test set
- Achieves 60.3% online pass rate on real user queries, significantly higher than baseline models

## Why This Works (Mechanism)
The framework addresses the inherent challenges of travel planning: dynamic environments where tool responses vary over time, open-ended tasks requiring multi-turn reasoning, and the need for both high-level planning and low-level execution validation. By constructing a sandbox environment with cached data, the framework ensures training stability despite the dynamic nature of travel APIs. The hierarchical reward system with both trajectory and turn-level verifiers provides comprehensive feedback that guides the agent toward both strategic planning and tactical execution excellence. The replay-augmented RL method with experience replay addresses the sample inefficiency common in agentic RL by reusing failed queries to improve learning.

## Foundational Learning
- **Agentic RL**: Reinforcement learning where agents make sequential decisions with tool calls; needed because travel planning requires multi-turn reasoning with external tools, not just single-step prediction. Quick check: Agent maintains state across turns and decides when to call tools.
- **Sandbox environment with data caching**: Mock environment that caches real-world data with daily refresh; needed to ensure consistent tool responses during training despite real-world API variability. Quick check: Same query produces identical responses within training session.
- **Hierarchical reward modeling**: Two-level verification system (trajectory-level for overall feasibility, turn-level for step-by-step consistency); needed because travel planning requires both strategic coherence and tactical correctness. Quick check: Both verifiers must pass for positive reward.
- **Experience replay in agentic RL**: Periodically replaying failed queries from buffer; needed because travel planning has sparse rewards and high sample complexity. Quick check: Buffer size grows with failures and is periodically sampled.
- **Tool-integrated LLM agents**: Models that generate both natural language and tool API calls; needed because travel planning requires external data retrieval. Quick check: Agent outputs valid JSON tool call format.
- **Trajectory-level verification**: Holistic assessment of entire plan against spatiotemporal constraints; needed because individual steps may be valid but overall plan may be infeasible. Quick check: Start/end times, locations, and constraints form coherent journey.

## Architecture Onboarding

### Component Map
Sandbox Environment -> Tool Calls -> LLM Agent -> Hierarchical Reward -> Experience Replay Buffer -> RL Optimizer -> LLM Weights

### Critical Path
User Query -> LLM Agent (tool selection and reasoning) -> Sandbox Tool Execution -> Tool Response -> LLM Agent (next turn) -> Repeat until completion -> Hierarchical Reward Computation -> Policy Update

### Design Tradeoffs
- **Sandbox vs Real APIs**: Sandbox provides training stability but may not capture all real-world edge cases; tradeoff is between training efficiency and real-world robustness.
- **Hierarchical vs Flat Reward**: Hierarchical rewards provide more granular feedback but increase computational overhead and complexity; tradeoff is between reward quality and system complexity.
- **Replay-augmented vs Standard RL**: Replay improves sample efficiency but requires additional memory and complexity; tradeoff is between training speed and implementation simplicity.

### Failure Signatures
- Low sample_keep_rate (<10%) indicates data too easy or too hard
- High gradient_norm spikes indicate training instability
- Tool_call_accuracy drops suggest inconsistent sandbox data or verifier issues
- Avg_response_length or avg_turn_count significantly below expected values indicate reward hacking
- Non-decreasing entropy during training suggests adaptation but may prevent convergence

### Three First Experiments
1. Test sandbox environment with basic queries to verify tool integration and data caching works as expected
2. Run SFT on small dataset to verify cold-start training produces coherent travel plans
3. Test hierarchical reward system with known good and bad trajectories to verify reward computation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the hierarchical reward modeling be fully automated to eliminate the dependency on human-crafted rubrics?
- **Basis in paper:** [explicit] Section 6 states that the current reliance on a "carefully designed reward system" limits the framework's extensibility and identifies developing a "more flexible reward model" as a primary future goal.
- **Why unresolved:** The current system relies on manually defined rubrics (Appendix A.2) to guide the DeepSeek-R1 verifier, which is labor-intensive and potentially subjective.
- **What evidence would resolve it:** Demonstration of an end-to-end trainable verifier that achieves comparable pass rates without manual rule annotation.

### Open Question 2
- **Question:** Can the DeepTravel framework effectively generalize to domains with different constraint structures, such as logistics or software engineering?
- **Basis in paper:** [explicit] Section 6 explicitly proposes extending the framework to other domains as future work to validate its utility beyond travel planning.
- **Why unresolved:** The sandbox and hierarchical verifiers (spatiotemporal feasibility) are tailored specifically for travel data; it is unclear if the replay mechanism works equally well for logical (code) or physical (robotics) tasks.
- **What evidence would resolve it:** Successful application of the DeepTravel RL pipeline on a non-travel benchmark (e.g., web shopping or code generation) using a similar sandbox setup.

### Open Question 3
- **Question:** Does the observed "non-decreasing entropy" during agentic RL training limit the agent's ability to converge to an optimal policy?
- **Basis in paper:** [inferred] Section 4.4 notes a "non-decreasing entropy phenomenon" driven by dynamic tool responses but does not determine if this prevents the policy from stabilizing or reaching peak performance.
- **Why unresolved:** While the paper hypothesizes this is necessary for adaptation, continuous high entropy often indicates insufficient convergence in standard RL paradigms.
- **What evidence would resolve it:** An analysis of training dynamics over extended steps to determine if reward plateaus while entropy remains high, or if entropy regularization improves final pass rates.

## Limitations
- The experience replay interval (Î³) for the replay-augmented RL component is only described as a "fixed interval" without specific numerical values, which could significantly impact training dynamics and final performance.
- The proprietary nature of DiDi's APIs and real-world data creates an inherent barrier to full reproducibility, as exact tool schemas and data distributions cannot be independently verified.
- While the hierarchical reward system is well-described conceptually, the specific rubric text and exact prompt templates for the DeepSeek-R1 verifier are not provided, leaving some ambiguity in reward computation.
- The paper does not provide error analysis or failure case studies that would help understand the model's limitations in edge cases or rare scenarios.

## Confidence

### Confidence Assessment
- **High confidence**: The core methodology of combining SFT with replay-augmented RL, the hierarchical reward structure, and the overall framework design are well-specified and internally consistent.
- **Medium confidence**: The quantitative results showing Qwen3-32B outperforming larger reasoning models are credible given the evaluation methodology, but the exact contribution of each component (sandbox, reward modeling, replay) to the final performance is not fully decomposed.
- **Medium confidence**: The generalizability of the approach to other domains beyond travel planning is suggested but not empirically validated in the paper.

## Next Checks
1. **Replicate the sandbox environment with publicly available travel APIs** (e.g., Skyscanner, Booking.com, Google Places) to verify that the tool integration and reward computation work as described without proprietary dependencies.
2. **Perform ablation studies** on the replay-augmented RL component by testing different experience replay intervals and comparing performance to standard RL without replay to quantify the contribution of this specific innovation.
3. **Test the trained models on out-of-distribution scenarios** not present in the training or evaluation sets (e.g., extreme weather events, political unrest, or unusual travel restrictions) to assess robustness and identify potential failure modes.