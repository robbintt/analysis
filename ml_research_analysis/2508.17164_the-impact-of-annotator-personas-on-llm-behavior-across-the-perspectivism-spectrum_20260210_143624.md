---
ver: rpa2
title: The Impact of Annotator Personas on LLM Behavior Across the Perspectivism Spectrum
arxiv_id: '2508.17164'
source_url: https://arxiv.org/abs/2508.17164
tags:
- human
- annotator
- annotations
- annotators
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# The Impact of Annotator Personas on LLM Behavior Across the Perspectivism Spectrum

## Quick Facts
- arXiv ID: 2508.17164
- Source URL: https://arxiv.org/abs/2508.17164
- Reference count: 37
- Key outcome: None

## Executive Summary
This paper investigates how Large Language Models (LLMs) behave when generating annotations using persona-based prompts, revealing a systematic tendency toward aggregation rather than preserving diverse human perspectives. Using Llama2-13B to annotate subjective datasets (HS-Brexit and ConvAbuse), the authors find that LLM-generated labels show significantly higher inter-annotator agreement than human annotations, indicating a "homogenization" effect. The study also shows that standard annotator modeling techniques designed for human disagreement perform suboptimally on LLM-generated data, with text-only models often outperforming ID-aware approaches. These findings have implications for using synthetic data in personalization tasks and suggest LLMs may flatten the rich variance found in human perspectives.

## Method Summary
The authors use Llama2-13B to generate binary annotations for two subjective datasets (HS-Brexit and ConvAbuse) from SemEval-2023, employing persona-based prompts that vary from individual (Strong) to group-level (Weak) descriptions. They systematically vary temperature (0.0-0.8) during generation and compare LLM agreement (Krippendorff's alpha) to human baselines. Multiple annotator modeling techniques are then trained on the synthetic labels, including User Token, Composite Embedding, and text-only SBERT approaches, with performance measured against human-trained baselines. The study also analyzes alignment between human and LLM-generated perspectives using cosine similarity to identify "prototypical" personas.

## Key Results
- LLM-generated annotations show consistently higher inter-annotator agreement (Krippendorff's alpha up to 0.81) than human annotations (0.35-0.50), indicating homogenization of perspectives
- Annotator models without explicit ID information (like SBERT) outperform User Token models on LLM-generated datasets, contrary to findings on human data
- LLMs selectively use demographic attributes from personas, with certain "prototypical" features (e.g., "Muslim") driving alignment with specific human groups
- Low temperature (0-0.1) generation yields best results, with surprisingly little variance increase even at high temperatures

## Why This Works (Mechanism)

### Mechanism 1: Tendency Toward Aggregation (Homogenization)
LLMs default to generalized perspectives from their pre-training corpus rather than maintaining distinct human viewpoints, even with specific persona prompts. The model retrieves the most probable continuation representing "average" opinion, with persona prompts acting as weak steering mechanisms. This is evidenced by consistently high Krippendorff's alpha (0.81 vs 0.35-0.50 for humans) and the failure of high temperature to increase variance meaningfully.

### Mechanism 2: Selective Feature Absorption
LLMs don't utilize all demographic attributes equally but latch onto "prototypical" features with strong semantic associations in training data while ignoring less common attributes. This is demonstrated by the analysis showing specific attributes like "Muslim" driving alignment with human groups, and the identification of generalized "prototypical persona features" that work as representatives of human groups.

### Mechanism 3: Architectural Mismatch in Annotator Modeling
Standard User Token techniques underperform on synthetic data because LLMs fail to generate sufficiently distinct individual "voices" - the high agreement in outputs prevents learning distinctive representations for specific IDs. This explains why models without explicit annotator information (like SBERT) outperform ID-aware approaches on LLM-generated datasets.

## Foundational Learning

**Concept: Data Perspectivism (Strong vs. Weak)**
Why needed: The paper evaluates performance across a spectrum from aggregated/group labels (Weak) to individual/disaggregated labels (Strong). Understanding this distinction is critical for interpreting the failure of ID-based models.
Quick check: Does the task require predicting a consensus label (Weak) or modeling individual disagreement (Strong)?

**Concept: Krippendorff's Alpha ($K-\alpha$)**
Why needed: This metric is the primary evidence anchor proving LLMs are "too consistent" compared to humans. Understanding that high $\alpha$ implies low disagreement is necessary to interpret homogenization results.
Quick check: If $\alpha$ increases from 0.35 (Human) to 0.81 (LLM), has the variance in the dataset increased or decreased?

**Concept: Composite Embedding vs. User Token**
Why needed: The paper reveals a shift in optimal architecture. Understanding that "User Token" injects explicit ID vectors while "Composite" relies on aggregating hidden states is crucial for grasping why one fails on synthetic data.
Quick check: Which method forces the model to memorize specific ID-to-label mapping versus learning general text features?

## Architecture Onboarding

**Component map:** Persona Generator -> Synthetic Annotator (Llama2-13B) -> Downstream Model (SBERT/User-Token)

**Critical path:**
1. Define personas using Strong (individual) or Weak (group) templates from Appendix A
2. Generate annotations at low temperature (0-0.1) as paper found best results here
3. Train using Composite Embedding or SBERT - do NOT prioritize User Token models for LLM-generated dataset

**Design tradeoffs:**
- Cost vs. Diversity: LLMs are cheaper than human annotation but trade Minority Viewpoints for Aggregated Stability
- Strong vs. Weak Prompting: Strong performed better for high-agreement datasets (ConvAbuse), while Weak was better for low-agreement (HS-Brexit)

**Failure signatures:**
- High Alpha on Subjective Data: If synthetic dataset shows $K-\alpha > 0.8$ on subjective task, indicates mode collapse (homogenization), not high quality
- ID-Model Underfitting: If User Token model fails to converge or performs worse than text-only baseline on synthetic data, confirms LLM failed to generate distinct persona voices

**First 3 experiments:**
1. Alignment Check: Calculate cosine similarity between LLM persona vectors and human annotator vectors to identify "Prototypical Personas" before full training
2. Temperature Sweep: Run annotation generation at T=0, 0.1, 0.5, 0.8; verify if variance actually increases significantly (paper suggests it might not)
3. Ablation: Train model on LLM-generated labels for "Persona 1" (identified as prototypical) and test against corresponding Human label set to measure "Alignment Gap"

## Open Questions the Paper Calls Out

**Open Question 1:** Can "prototypical" personas be systematically engineered to fully capture the spectrum of human disagreement in LLMs?
Basis: Authors state future work should "generate more diversified personas, systematically varying features, and expanding evaluation to other LLMs"
Why unresolved: Current study found some LLM personas aligned with human groups (prototypes) but many failed to map to any observed human patterns
What evidence: Successful modeling of human disaggregated labels using standardized set of optimized, diverse LLM personas

**Open Question 2:** Do newer or larger LLM architectures demonstrate superior ability to model strong data perspectivism compared to Llama2-13B?
Basis: Limitations section notes analysis is "limited to this model" and didn't investigate "how newer variants of Llama or other LLMs, like GPT 4o, might influence results"
Why unresolved: Unclear if homogenization tendency is Llama2-13B limitation or inherent trait of current LLM architectures
What evidence: Comparative benchmarks showing alternative LLMs maintaining high inter-annotator disagreement while adhering to specific persona prompts

**Open Question 3:** How does distribution of model attention between persona description and input text influence generation of subjective labels?
Basis: Authors state they "did not quantify the extent to which model's attention was distributed between persona and input sentences"
Why unresolved: Without this analysis, mechanism by which LLMs "selectively use" demographic attributes over textual cues remains opaque
What evidence: Attention heatmaps correlating specific persona feature tokens with divergent label generation in ambiguous or subjective instances

## Limitations
- Study uses only two datasets (HS-Brexit and ConvAbuse) from same SemEval competition, limiting generalizability across different subjective tasks
- Temperature sweep (0-0.8) suggests variance doesn't increase meaningfully even at high temperatures, but range may be insufficient to overcome aggregation tendency
- Paper doesn't investigate attention distribution between persona and input text, leaving mechanism of selective feature absorption incompletely understood

## Confidence

**Low:** The paper demonstrates consistent homogenization effects across multiple subjective datasets, but the underlying mechanism for why persona prompts fail to overcome pre-training priors remains theoretically underspecified. Evidence strongly supports LLMs default to aggregated perspectives, yet exact interaction between attention mechanisms and persona token salience needs deeper probing.

**Medium:** The selective feature absorption mechanism shows clear patterns (e.g., "Muslim" driving alignment), but paper doesn't fully explore whether this reflects semantic association strength or frequency bias in training data. Prototypical persona identification is empirically sound but lacks theoretical grounding for why certain attributes dominate over others.

**Medium:** The architectural mismatch findings are robust - User Token models consistently underperform on synthetic data. However, paper doesn't investigate whether this stems from fundamental LLM limitations or could be mitigated through alternative architectural choices (e.g., better persona encoding strategies).

## Next Checks

1. **OOD Persona Testing**: Generate annotations using personas with features completely absent from model's pre-training distribution. Measure if homogenization persists or if model reverts to generic outputs, testing selective feature absorption mechanism's boundaries.

2. **Cross-Dataset Replication**: Apply same methodology to subjective datasets outside SemEval-2023 (different tasks, domains, or languages). Compare Krippendorff's alpha distributions to validate whether 0.8+ agreement threshold is task-specific or general LLM characteristic.

3. **Architectural Stress Test**: Implement alternative persona encoding methods (e.g., learned persona embeddings, hierarchical prompting) and measure if User Token models can regain performance on synthetic data, testing whether architectural mismatch is fundamental or solvable.