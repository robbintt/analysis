---
ver: rpa2
title: Directed Evolution of Proteins via Bayesian Optimization in Embedding Space
arxiv_id: '2509.04998'
source_url: https://arxiv.org/abs/2509.04998
tags:
- variants
- protein
- fitness
- space
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for machine-learning-assisted
  directed evolution of proteins that combines Bayesian optimization with informative
  representation of protein variants extracted from a pre-trained protein language
  model. The key idea is to employ Bayesian optimization with a Gaussian process model
  in the embedding space of a pre-trained protein language model, rather than in the
  original protein sequence space.
---

# Directed Evolution of Proteins via Bayesian Optimization in Embedding Space

## Quick Facts
- arXiv ID: 2509.04998
- Source URL: https://arxiv.org/abs/2509.04998
- Authors: Matouš Soldát; Jiří Kléma
- Reference count: 40
- One-line primary result: BOES outperforms state-of-the-art MLDE methods in terms of maximum fitness achieved with limited screening budgets

## Executive Summary
This paper presents BOES, a novel machine-learning-assisted directed evolution method that uses Bayesian optimization in the embedding space of pre-trained protein language models (PPLMs) rather than raw sequence space. The key insight is that PPLM embeddings capture evolutionary and functional information, creating a smoother fitness landscape amenable to optimization. BOES demonstrates superior performance compared to state-of-the-art regression-based methods, achieving higher maximum fitness with the same screening budget on two benchmark protein engineering datasets (GB1 and PhoQ).

## Method Summary
BOES reframes directed evolution as sequential black-box optimization where expensive function evaluations correspond to experimental screening. The method uses ESM-1b to extract 1280-dimensional embeddings for all protein variants, then applies Bayesian optimization with a Gaussian Process model in this embedding space. A custom Matérn 3/2 kernel with a single shared length-scale hyperparameter across all dimensions addresses the high-dimensionality challenge. The Expected Improvement acquisition function directly optimizes for fitness improvement, aligning with the DE objective of finding high-fitness variants with minimal screening.

## Key Results
- BOES achieves maximum fitness of 7.28 on GB1 with 80 screening budget, outperforming regression-based methods
- On PhoQ dataset, BOES reaches 126.6 fitness value with 50 screening budget, significantly better than alternatives
- Robust performance across multiple starting variants, with consistent improvement over state-of-the-art methods
- t-SNE visualization confirms local clusters of high-fitness variants in embedding space, validating the smoothness assumption

## Why This Works (Mechanism)

### Mechanism 1: Embedding Space Smooths the Fitness Landscape
PPLM embeddings capture evolutionary and functional information, creating a representation where proximity correlates with functional similarity. ESM-1b extracts 1280-dimensional embeddings that encode structural and evolutionary constraints learned from millions of protein sequences. These embeddings cluster functionally similar variants together, making the fitness landscape smoother and more amenable to optimization than raw sequence space.

### Mechanism 2: Single Length-Scale Kernel Enables BO in High Dimensions
Standard GP kernels fit one length-scale per dimension, which is intractable in high dimensions. BOES uses a custom Matérn 3/2 kernel with one shared θ parameter across all dimensions, computed as θ² × I in the distance metric. Prior is set so diagonal across embedding space ≈ 3σ (where σ = √(1280/3)). This limits the effective number of dimensions to one, preventing overfitting while maintaining tractability.

### Mechanism 3: Expected Improvement Aligns with DE Objective
The EI acquisition function directly optimizes for fitness improvement at each iteration, eliminating the need for separate training/exploitation phases required by regression-based methods. EI(x) = E[max(f(x) - f_best, 0)] selects variants that maximize expected improvement over current best observation. This automatically balances exploration (high uncertainty regions) and exploitation (high predicted fitness), matching the DE goal of finding high-fitness variants with minimal screening.

## Foundational Learning

- **Directed Evolution as Black-Box Optimization**: BOES reframes DE as sequential optimization where each function evaluation (screening) is expensive. Understanding this framing explains why sample-efficient methods like BO are appropriate.
  - Quick check: Can you explain why DE can be formulated as argmax f(x) over sequence space X, and why the "expensive evaluation" property matters for algorithm selection?

- **Gaussian Process Regression and Uncertainty Quantification**: The GP provides both mean prediction (expected fitness) and variance (uncertainty) that BOES uses to compute acquisition functions. Without understanding how GP uncertainty depends on proximity to observed data, the exploration/exploitation tradeoff is opaque.
  - Quick check: Given a GP trained on 3 variants, would you expect higher uncertainty for a new variant close to one observed point or far from all observed points?

- **Protein Language Model Embeddings**: The method relies on PPLM embeddings providing a meaningful metric space. Understanding that these embeddings are learned from evolutionary data and encode structural/functional priors is critical.
  - Quick check: Why would embeddings from a model trained on millions of natural protein sequences be more useful for directed evolution than one-hot encodings?

## Architecture Onboarding

- Component map: Input: Wild-type sequence → ESM-1b (1280-dim embeddings) → GP with single-θ Matérn kernel → EI acquisition → Argmax selection → Screen/observe → Update GP → Loop

- Critical path:
  1. **Embedding quality**: If ESM-1b doesn't cluster functionally similar variants, GP fails → verify with t-SNE visualization
  2. **Kernel fitting**: NEWUOA with 20 restarts; if θ diverges, distance metric is meaningless
  3. **Acquisition maximization**: Must search over all 20^n variants; naive enumeration fails for n > 5

- Design tradeoffs:
  - Single θ: Tractable in high dims, but cannot adapt to varying sensitivity across embedding dimensions
  - Zero noise (σ² ≈ 0): Assumes perfect measurements; incompatible with noisy experimental data
  - ESM-1b fixed: No task adaptation vs. finetuning tradeoff
  - EI vs UCB: Directly optimizes improvement, but may be conservative early

- Failure signatures:
  1. **Premature convergence**: EI ≈ 0 early → GP uncertainty collapsed; check kernel initialization
  2. **Random walk behavior**: No improvement over SMW → embeddings don't encode task-relevant info; verify with PCA
  3. **Numerical instability**: NaN/Inf in GP → kernel distances poorly scaled; check σ prior

- First 3 experiments:
  1. Baseline reproduction: Run BOES on GB1 with 80 screening budget from wild-type. Expect max fitness ≈ 7.28 (Table I). Significant deviation indicates bug in embedding or GP.
  2. Embedding ablation: Compare BOES vs GP+EI with one-hot encoding on GB1. Expect ~8.14 vs ~7.28 (Table II). Gap confirms embedding value.
  3. Robustness test: Run from 10 random starting variants on GB1 with 200 budget. Compute median/IQR of max fitness. Compare to Figure 1a behavior.

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance of BOES be significantly improved by integrating prescreening outlier detection (e.g., XGBOD) or trust region Bayesian optimization (TuRBO) as suggested for the ODBO framework? The paper suggests combining the innovative input space representation with ODBO's algorithmic enhancements (outlier detection, trust regions) but doesn't implement a hybrid version to see if benefits compound.

### Open Question 2
How does the choice of pre-trained protein language model (PPLM) impact the efficiency of the optimization, and would newer architectures outperform the used ESM-1b? The paper demonstrates that embeddings are superior to one-hot encoding but doesn't test if "better" embeddings yield "better" optimization landscapes by comparing against alternative models.

### Open Question 3
How does BOES perform in real-world wet-lab settings where experimental noise is present, given that the current model assumes zero observation noise? The paper uses noiseless in silico datasets, but real biological screening data contains measurement noise and variability that may cause the fixed zero-noise GP to overfit or mis-calibrate uncertainty.

### Open Question 4
Does the restriction of the kernel to a single length scale hyperparameter limit the model's ability to distinguish relevant dimensions in the embedding space? While preventing overfitting in high dimensions, the single-length-scale constraint forces the model to treat all 1280 embedding dimensions with equal importance, potentially ignoring that some dimensions may be irrelevant to the specific fitness function.

## Limitations

- Heavy reliance on pre-trained protein language model embeddings, with uncertain transferability to proteins with novel functions or synthetic amino acids not well-represented in pre-training data
- Assumption of zero observation noise in the GP model is unrealistic for real experimental settings and may cause overfitting or mis-calibration
- Single length-scale kernel constraint may oversimplify complex fitness landscapes where different embedding dimensions require different sensitivity scales

## Confidence

**High confidence**: Mathematical formulation and implementation of GP with single-length-scale Matérn kernel and EI acquisition
**Medium confidence**: Claims about embedding space smoothness improving optimization, supported by t-SNE visualization but not rigorously quantified
**Medium confidence**: Robustness claims across multiple starting variants, though systematic exploration of failure modes is limited

## Next Checks

1. **Noise robustness test**: Run BOES on GB1 with added Gaussian noise (σ = 0.1–0.5) to fitness measurements and compare performance degradation against regression-based methods
2. **Embedding sensitivity analysis**: Compare BOES performance using ESM-1b embeddings vs. alternative protein language models (ESM-2, MSA Transformer) on both GB1 and PhoQ datasets
3. **Real-world applicability demonstration**: Apply BOES to a protein engineering problem with experimental validation (e.g., thermostability or binding affinity) rather than in silico fitness prediction only