---
ver: rpa2
title: Statistical Inference for Differentially Private Stochastic Gradient Descent
arxiv_id: '2507.20560'
source_url: https://arxiv.org/abs/2507.20560
tags:
- dp-sgd
- privacy
- gradient
- asymptotic
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes asymptotic normality for Differentially
  Private Stochastic Gradient Descent (DP-SGD) under randomized subsampling, bridging
  a gap between privacy-preserving optimization and statistical inference. The authors
  prove that DP-SGD's asymptotic variance decomposes into statistical, sampling, and
  privacy-induced components.
---

# Statistical Inference for Differentially Private Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2507.20560
- Source URL: https://arxiv.org/abs/2507.20560
- Reference count: 9
- Establishes asymptotic normality for DP-SGD with randomized subsampling, enabling valid statistical inference

## Executive Summary
This paper addresses a fundamental challenge in privacy-preserving machine learning: how to construct valid statistical inference for differentially private optimization algorithms. The authors prove that DP-SGD with randomized subsampling converges to a normal distribution, enabling construction of confidence intervals with valid coverage. They decompose the asymptotic variance into statistical, sampling, and privacy-induced components, and propose two practical methods (plug-in and random scaling) for constructing confidence intervals. Extensive simulations demonstrate that these methods achieve nominal coverage rates while maintaining differential privacy guarantees.

## Method Summary
The paper establishes asymptotic normality for DP-SGD under randomized subsampling, proving that the estimator converges to a normal distribution with variance decomposing into three independent components. Two inference methods are proposed: the plug-in method estimates variance components using private noise, while the random scaling method uses the iterate trajectory. The theoretical framework accommodates various differential privacy definitions including (ε,δ)-DP, Rényi DP, and Gaussian DP. The authors prove that randomized subsampling inflates variance by factor {1 + 1/(km)} compared to cyclic subsampling, but this can be controlled through appropriate batch size selection.

## Key Results
- DP-SGD estimator converges to normal distribution with variance decomposing into statistical, sampling, and privacy-induced components
- Randomized subsampling inflates variance by factor {1 + 1/(km)} compared to cyclic subsampling
- Properly calibrated privacy noise can achieve differential privacy while maintaining full-sample asymptotic efficiency
- Both plug-in and random scaling methods achieve nominal 95% coverage rates in simulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP-SGD estimator converges to a normal distribution with variance that cleanly decomposes into three independent components
- Mechanism: The paper proves asymptotic normality where √n(θ̄_T − θ*) converges to the sum of three independent Gaussian terms: statistical variance (from data randomness), sampling variance (from randomized subsampling), and privacy-induced variance (from injected noise). This decomposition allows separate accounting and control of each uncertainty source.
- Core assumption: Strong convexity of loss function, bounded gradient sensitivity, step size η_t = η·t^{-α} for α∈(1/2,1), and T = k·n iterations
- Evidence anchors:
  - [abstract]: "we show that the asymptotic variance decomposes into statistical, sampling, and privacy-induced components"
  - [Section 3, Theorem 2]: Formal decomposition into φ_stat + φ_sam + φ_privacy with explicit variance formulas
  - [corpus]: Related work confirms variance analysis is central to DP-SGD optimization
- Break condition: If iteration count T doesn't scale as k·n, or if privacy noise calibration fails √T·log(1/δ)/(nε) = O(1) condition, variance decomposition may not hold

### Mechanism 2
- Claim: Randomized subsampling inflates variance by factor {1 + 1/(km)} compared to cyclic subsampling
- Mechanism: Unlike cyclic SGD which processes data sequentially, randomized sampling creates dependencies across iterations due to sampling with replacement. This introduces additional sampling error that decreases with more epochs (k) and larger batch sizes (m), converging to cyclic SGD efficiency as k→∞.
- Core assumption: Batch size m ≥ 1, iteration count T = k·n where k is bounded or grows slowly
- Evidence anchors:
  - [Section 2, Theorem 1]: Formal variance inflation factor {1 + 1/(km)}^{-1/2}√n(θ̄_T − θ*) → N(0, A^{-1}SA^{-1})
  - [Section 1.1, Contribution 1]: "the asymptotic variance inflates {1 + 1/(km)} times compared to the cyclic SGD"
  - [corpus]: Privacy amplification by subsampling papers confirm randomized subsampling's importance for DP
- Break condition: If k grows polynomially with n (e.g., T = n² as in GDP framework), variance inflation becomes negligible; small batch sizes with few epochs cause significant efficiency loss

### Mechanism 3
- Claim: Properly calibrated privacy noise can achieve differential privacy while maintaining full-sample asymptotic efficiency
- Mechanism: Noise scale σ₁ is calibrated based on privacy parameters, iterations, and sample size. Under conditions like √T·log(1/δ)/(nε) = O(1) for (ε,δ)-DP, the privacy-induced variance term vanishes asymptotically, meaning privacy comes "for free" in the limit while preserving finite-sample guarantees.
- Core assumption: Bounded ℓ₂ gradient sensitivity (∆_g), appropriate noise calibration per privacy definition
- Evidence anchors:
  - [Section 3, Corollaries 1-3]: Specific noise calibrations for (ε,δ)-DP, RDP, and GDP achieving full-sample efficiency
  - [Section 3]: "When √T·log(1/δ)/(nε) = O(1), the estimator θ̄_T satisfies √n(θ̄_T − θ*) → N(0, A^{-1}SA^{-1})"
  - [corpus]: Related work on DP-SGD fundamental limitations suggests this efficiency is near-optimal
- Break condition: If privacy parameters are too stringent (ε → 0, δ → 0, µ → 0), noise variance dominates and statistical efficiency degrades significantly

## Foundational Learning

- Concept: **Polyak-Ruppert Averaging**
  - Why needed: The paper's theoretical results apply to averaged SGD iterates, not final iterates; understanding why averaging stabilizes variance is essential
  - Quick check question: Can you explain why θ̄_T = (1/T)∑θ^(t) has better asymptotic properties than θ^(T)?

- Concept: **Differential Privacy Mechanisms (Gaussian, Sensitivity)**
  - Why needed: The noise injection relies on sensitivity bounds and Gaussian mechanisms; understanding these is required to implement the privacy guarantees
  - Quick check question: Given gradient sensitivity ∆_g, how would you calibrate noise σ₁ for µ-GDP?

- Concept: **Asymptotic Normality and Confidence Interval Construction**
  - Why needed: The inference methods (plug-in and random scaling) both depend on establishing asymptotic normality and estimating variance components
  - Quick check question: Why does the paper need separate variance estimates for A (Hessian) and S (score covariance)?

## Architecture Onboarding

- Component map:
  - Input data → DP-SGD loop (subsampling, gradient computation, clipping, noise addition) → averaged parameter θ̄_T → variance estimation → confidence intervals

- Critical path:
  1. Set T = n² iterations, batch size m, step size η_t ∝ t^{-0.501}
  2. Choose privacy framework and calibrate σ₁ per Corollaries 1-3
  3. Run DP-SGD with gradient clipping threshold τ ≈ √log(n)
  4. Compute θ̄_T and apply finite-sample corrected confidence intervals

- Design tradeoffs:
  - **Iterations vs. privacy**: More iterations reduce sampling variance but require tighter noise calibration (T = n² optimal for GDP)
  - **Batch size vs. efficiency**: Larger m reduces variance inflation {1 + 1/(km)} but increases per-iteration cost
  - **Plug-in vs. random scaling**: Plug-in is more efficient but requires additional privacy budget for variance estimation; random scaling is privacy-free but yields wider intervals

- Failure signatures:
  - Under-coverage in small samples without finite-sample correction (Figure 2 shows plug-in without correction fails at n=500)
  - Non-invertible Hessian estimate Ā when eigenvalues are near-zero
  - Privacy noise dominates signal when σ₁ ≫ 1 (occurs with very strict privacy parameters)
  - Clipping bias when τ is too small relative to gradient magnitudes

- First 3 experiments:
  1. Replicate linear regression simulation (Section 6.1) with n=1000, p=3, T=n², µ=2 GDP to verify both methods achieve ~95% coverage
  2. Compare DP-SGD vs DP-GD (Figure 3) to demonstrate DP-SGD's stability advantage: vary T from n^{1.5} to n² for DP-SGD and T from 0-30 for DP-GD
  3. Test finite-sample correction impact: run plug-in method with and without the σ₁²(Ā^{-2})_{jj}/k correction term at sample sizes n ∈ {500, 750, 1000, 1250, 1500} to quantify coverage improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Variance decomposition relies on asymptotic regime where T = n² and privacy noise calibration condition holds
- Randomized subsampling variance inflation can significantly impact efficiency with small batch sizes and few epochs
- Finite-sample coverage guarantees may degrade without proper correction terms for small samples (n < 500)

## Confidence
- **High Confidence** in asymptotic normality results and variance decomposition (Theorem 2)
- **Medium Confidence** in practical coverage rates, dependent on implementation details
- **Medium Confidence** in claim that privacy can be achieved "for free" asymptotically

## Next Checks
1. **Reproduce coverage rates for n=500-1000**: Implement linear regression simulation with both plug-in and random scaling methods, varying sample size to verify 95% coverage with and without finite-sample correction.

2. **Test variance inflation sensitivity**: Systematically vary batch size m ∈ {32, 64, 128} and epochs k ∈ {5, 10, 20} to quantify the {1 + 1/(km)}^{-1/2} variance inflation effect.

3. **Validate privacy noise calibration**: For each privacy framework (GDP, RDP, (ε,δ)-DP), verify that calibrated σ₁ satisfies asymptotic efficiency condition by computing privacy-induced variance term.