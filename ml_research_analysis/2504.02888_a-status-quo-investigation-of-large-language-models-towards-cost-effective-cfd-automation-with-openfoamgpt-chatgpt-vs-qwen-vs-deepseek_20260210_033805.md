---
ver: rpa2
title: 'A Status Quo Investigation of Large Language Models towards Cost-Effective
  CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek'
arxiv_id: '2504.02888'
source_url: https://arxiv.org/abs/2504.02888
tags:
- fluid
- arxiv
- language
- turbulence
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated multiple large language models (LLMs) for
  cost-effective computational fluid dynamics (CFD) automation using the OpenFOAMGPT
  framework. Tested models included GPT-4o, OpenAI o1, DeepSeek V3, and Qwen 2.5-Max,
  with token pricing ranging from $0.035 to $15.0 per million input tokens.
---

# A Status Quo Investigation of Large Language Models towards Cost-Effective CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek

## Quick Facts
- arXiv ID: 2504.02888
- Source URL: https://arxiv.org/abs/2504.02888
- Reference count: 0
- Primary result: Chinese LLMs like Qwen 2.5-Max achieve OpenAI o1-level performance at 100x lower cost for routine CFD automation tasks.

## Executive Summary
This study evaluates multiple large language models for automating computational fluid dynamics (CFD) simulations using the OpenFOAMGPT framework. Tested models include GPT-4o, OpenAI o1, DeepSeek V3, and Qwen 2.5-Max, with token pricing ranging from $0.035 to $15.0 per million input tokens. Results show that Qwen 2.5-Max delivers performance comparable to OpenAI o1 while reducing token costs by up to two orders of magnitude. Zero-shot prompting successfully handles common CFD tasks across benchmark cases including cavity flow, PitzDaily, and airfoil simulations. However, complex geometries and boundary condition specifications remain challenging, requiring expert supervision. The study demonstrates that Chinese LLMs offer promising cost-effective alternatives for CFD automation while highlighting the ongoing need for human oversight in complex scenarios.

## Method Summary
The study employs a hierarchical agent architecture with Builder, Executor, and Interpreter modules to automate OpenFOAM CFD simulations through iterative error correction. Models are tested using zero-shot prompting only (RAG disabled) across multiple test cases including cavity flow, PitzDaily, and complex geometries like MotorBike and nozzleFlow. The system captures error logs from failed simulations and feeds them back to the LLM for correction, repeating up to 20 iterations. Cloud API models (GPT-4o, o1, DeepSeek V3, Qwen 2.5-Max) are compared against locally deployed QwQ-32B via Ollama, with performance measured by success rate, iteration count, token usage, and cost.

## Key Results
- Qwen 2.5-Max achieves OpenAI o1-level performance on routine CFD tasks while reducing token costs by up to 100x
- Zero-shot prompting successfully handles boundary condition modifications and turbulence model selection for standard cases
- Complex geometries and specialized boundary conditions remain challenging even for large models, requiring expert supervision
- Local QwQ-32B struggles with specialized engineering tasks, suggesting smaller models need domain-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Iterative error correction enables LLMs to debug CFD simulations without external knowledge retrieval. The system captures error logs from failed OpenFOAM runs and appends them to the original query, creating a feedback loop where the LLM can diagnose and correct syntax errors, missing keywords, or incorrect boundary condition specifications across multiple attempts (up to 20 iterations in testing). Core assumption: The LLM's pre-trained knowledge includes sufficient OpenFOAM syntax and CFD domain knowledge to interpret error messages and generate corrections. Break condition: Complex simulations requiring domain-specific knowledge not present in pre-training data exceed correction capacity.

### Mechanism 2
Hierarchical agent decomposition separates orchestration from execution, enabling modular workflow management. The Builder module converts natural language instructions into structured execution plans; the Executor routes tasks between the LLM (reasoning) and Interpreter (file generation); the OpenFOAM Runner executes simulations and returns status. This separation allows each component to specialize without monolithic prompt engineering. Core assumption: CFD workflows can be decomposed into discrete, sequential steps that map cleanly to LLM capabilities and file operations. Break condition: Tasks requiring simultaneous reasoning across geometry, mesh, and physics exceed current decomposition strategies.

### Mechanism 3
Chinese-hosted LLMs achieve cost reductions of O(10²) while maintaining task-level parity with premium models on routine CFD operations. DeepSeek V3 ($0.035/$0.55 per 1M input/output tokens) and Qwen 2.5-Max ($0.80/$1.20) leverage different pricing structures than OpenAI o1 ($15/$60), enabling equivalent boundary condition modifications and solver configurations at ~1% of the cost. Core assumption: Token pricing reflects market/compute factors rather than capability gradients for domain-specific engineering tasks. Break condition: Specialized engineering vocabulary or less-documented OpenFOAM features may expose capability gaps not reflected in pricing.

## Foundational Learning

- Concept: OpenFOAM case structure (system/, constant/, 0/ directories; fvSchemes, fvSolution, controlDict)
  - Why needed here: The LLM generates files that must conform to OpenFOAM's directory hierarchy and dictionary syntax; without understanding this structure, you cannot validate outputs or diagnose errors.
  - Quick check question: Can you identify which file controls turbulence model selection and which defines initial boundary conditions?

- Concept: Boundary condition taxonomy in OpenFOAM (fixedValue, zeroGradient, inletOutlet, patch vs. constraint types)
  - Why needed here: The paper documents repeated failures around boundary condition specification; understanding these distinctions is prerequisite for supervising LLM outputs.
  - Quick check question: What is the difference between a "patch" type and a constraint type like "empty" in OpenFOAM?

- Concept: LLM prompting modes (zero-shot vs. few-shot vs. RAG-augmented)
  - Why needed here: The study explicitly disabled RAG to test pure zero-shot capability; understanding this distinction helps interpret why complex cases failed and when to re-enable retrieval augmentation.
  - Quick check question: What additional information would RAG provide that zero-shot prompting cannot access?

## Architecture Onboarding

- Component map: System Prompt + User Query → Builder → Executor → LLM Model → Interpreter → OpenFOAM Runner → Error Monitor → (loop back if error)

- Critical path: User query → Builder creates plan → Executor invokes LLM → Interpreter generates files → Runner executes → (if error) loop back with error appended → (if success) terminate

- Design tradeoffs:
  - Cost vs. capability: Qwen 2.5-Max balances performance and cost; o1 offers highest capability at 100× cost
  - Local vs. API: Local QwQ-32B eliminates API dependency but fails on complex cases; API models handle broader task range
  - Zero-shot vs. RAG: Paper disabled RAG for evaluation; production deployment should likely re-enable it for reliability

- Failure signatures:
  - Boundary condition type errors: "patch type 'patch' not constraint type 'empty'" (motorBike case)
  - Missing solver keywords: "'smoother' not found in fvSolution" (nozzleFlow case)
  - File dependency omissions: Missing p_rgh files despite workflow requirements
  - Token context overflow: Smaller context windows (32k for Qwen vs. 200k for o1) may truncate complex case setups

- First 3 experiments:
  1. **Cavity flow modification**: Task LLM with changing top wall velocity from 1 m/s to 2 m/s; verify successful iteration count <3 and correct icoFoam execution
  2. **PitzDaily turbulence model switch**: Request change from k-ε to kOmegaSST; validate that all required dictionary files are correctly updated
  3. **Cost benchmark sweep**: Run identical boundary condition task across all models; record token usage, iteration count, and success/failure to establish your own cost-performance frontier

## Open Questions the Paper Calls Out

### Open Question 1
Can domain-specific fine-tuning enable sub-100B parameter models (e.g., QwQ-32B) to reliably generate valid OpenFOAM syntax for complex simulations? The authors note that locally deployed smaller models struggled with specialized tasks, suggesting that "effective deployment... requires... domain-specific fine-tuning of smaller architectures." This study tested models "as-is" (zero-shot) and found them failing on complex boundary conditions; the performance gain from training on CFD corpora remains unquantified. What evidence would resolve it: A comparative benchmark showing a CFD-fine-tuned 32B model successfully generating solver files for cases like MotorBike or nozzleFlow where the base model failed.

### Open Question 2
Does integrating multi-modal capabilities allow LLMs to interpret geometry and mesh data more effectively than text-only inputs? The paper states that "bridging textual instructions with geometry and mesh data remains a hurdle" and suggests multi-modal approaches as a promising avenue. The current framework relies on text-based file manipulation; it is unknown if visual processing of CAD or mesh structures reduces setup errors. What evidence would resolve it: Demonstrating a multi-modal agent setting up a simulation (e.g., cylinder flow) directly from geometry images or mesh visualizations without explicit text descriptions of the domain.

### Open Question 3
Does combining zero-shot prompting with Retrieval-Augmented Generation (RAG) provide a practical balance of low cost and high reliability for intricate CFD workflows? The authors explicitly disabled RAG to test zero-shot limits but conclude that "carefully combining zero-shot techniques with [RAG] may offer a practical blend." While RAG was previously used, the specific trade-off between the high token cost of reasoning models (o1) versus the latency/complexity of RAG for cheaper models (DeepSeek/Qwen) was not optimized in this study. What evidence would resolve it: An ablation study measuring total operational cost and success rates for complex cases using Qwen 2.5-Max with RAG versus OpenAI o1 without RAG.

## Limitations
- Complex geometries and specialized boundary conditions consistently fail under zero-shot prompting, with context window limitations (Qwen's 32k vs. o1's 200k) potentially truncating critical setup information
- Disabling RAG creates an unrealistic ceiling for production deployment where retrieval augmentation would likely be essential
- Local QwQ-32B failure analysis conflates model capability with deployment quality - quantization artifacts or implementation differences cannot be ruled out as confounding factors

## Confidence

**High Confidence**: The cost-performance relationship between Chinese and Western LLMs is well-established through direct pricing comparisons and task-level performance parity on routine cases. The iterative error-correction mechanism's basic functionality is clearly demonstrated through multiple successful cavity flow and PitzDaily runs.

**Medium Confidence**: Claims about hierarchical agent decomposition enabling modular workflow management are supported by architectural descriptions but lack empirical validation showing that this specific decomposition outperforms alternative approaches. The assertion that Qwen 2.5-Max achieves "comparable performance" to o1 requires qualification - they perform similarly on routine tasks but o1's larger context window enables handling of more complex scenarios.

**Low Confidence**: The local QwQ-32B failure analysis conflates model capability with deployment quality - without API-based QwQ-32B testing, it's unclear whether failures stem from quantization, context window limitations, or fundamental model inadequacies. Claims about specific failure modes may be artifacts of the particular implementation rather than universal LLM limitations.

## Next Checks

1. **API vs. Local Model Comparison**: Run identical test cases using QwQ-32B via official API endpoints rather than local deployment to isolate whether quantization or implementation differences drive the observed failures.

2. **RAG Re-enablement Study**: Re-run the motorBike and nozzleFlow cases with RAG enabled to establish the performance ceiling achievable when LLMs can access OpenFOAM documentation and error message databases during prompting.

3. **Context Window Stress Test**: Systematically test each model with increasingly complex case setups (varying mesh complexity, number of boundary patches, coupled physics) to map the precise context window thresholds where performance degrades across different LLM architectures.