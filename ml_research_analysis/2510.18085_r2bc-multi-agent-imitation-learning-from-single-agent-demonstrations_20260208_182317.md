---
ver: rpa2
title: 'R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations'
arxiv_id: '2510.18085'
source_url: https://arxiv.org/abs/2510.18085
tags:
- demonstrations
- r2bc
- agents
- learning
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Round-Robin Behavior Cloning (R2BC), a method
  for multi-agent imitation learning that enables a single human to teach a team of
  robots through sequential, single-agent demonstrations. Unlike prior approaches
  that require coordinated joint demonstrations, R2BC allows the human to teleoperate
  one agent at a time while other agents execute their current learned policies, iteratively
  building multi-agent behavior.
---

# R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations

## Quick Facts
- **arXiv ID**: 2510.18085
- **Source URL**: https://arxiv.org/abs/2510.18085
- **Reference count**: 38
- **One-line primary result**: R2BC enables teaching multi-agent teams through sequential single-agent demonstrations, matching or exceeding oracle joint-BC performance while enabling practical sim-to-real transfer.

## Executive Summary
R2BC (Round-Robin Behavior Cloning) introduces a method for multi-agent imitation learning where a single human can teach a team of robots through sequential, single-agent demonstrations. Unlike traditional approaches requiring coordinated joint demonstrations, R2BC allows the human to teleoperate one agent at a time while other agents execute their current learned policies, iteratively building multi-agent behavior. The method achieves state-of-the-art performance on four simulated multi-agent tasks and demonstrates significant improvements in physical robot deployments compared to centralized behavior cloning baselines.

## Method Summary
R2BC trains N independent decentralized policies πᵢ: Oᵢ → Aᵢ using round-robin demonstration collection. The human demonstrates for agent i while other agents execute their current policies, storing transitions in per-agent buffers Dᵢ. Policies are updated every k episodes via standard behavior cloning loss, creating a curriculum of increasing coordination difficulty as teammate policies improve. This online mechanism exposes demonstrators to diverse teammate behaviors, reducing covariate shift compared to offline expert-only demonstrations.

## Key Results
- R2BC matches or exceeds oracle behavior cloning methods using privileged joint action demonstrations across four simulated tasks
- On physical robots, R2BC policies outperform centralized behavior cloning by 3.25x in navigation and 5.9x in block pushing tasks
- Offline R2BC variants (No-Op Agents, Random Agents) show "unreliable" and "high variance" performance compared to full R2BC
- Centralized R2BC performs significantly worse in transport tasks and slightly worse in navigation/balance compared to decentralized R2BC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: R2BC reduces covariate shift by exposing demonstrators to diverse teammate behaviors during training.
- Mechanism: When demonstrating for agent i, agents j≠i execute their current policies, creating observations where demonstrators must show corrective behaviors (avoiding collisions with errant teammates, compensating for poor coordination) that rarely appear in synchronized joint demonstrations.
- Core assumption: Diversity induced by imperfect teammates generalizes better than training solely on near-optimal coordinated trajectories.
- Evidence anchors: Section V-B shows loss gap analysis; Section IV describes the round-robin interaction dynamics.
- Break condition: If teammate policies converge too quickly to narrow behavior modes, demonstration diversity collapses.

### Mechanism 2
- Claim: Online policy updates during demonstration collection enable iterative coordination improvement.
- Mechanism: R2BC updates policies every k demonstrations, creating a curriculum where as agent i improves, agent i+1's demonstrations must account for a more competent teammate.
- Core assumption: Sequential single-agent demonstrations can bootstrap collective behavior through incremental policy refinement.
- Evidence anchors: Algorithm 1 shows regular updates; Section VI confirms online learning is essential through ablation studies.
- Break condition: If update frequency k is too high (policies stale) or too low (overfitting to current teammate distribution).

### Mechanism 3
- Claim: Decentralized policies trained only on local observations improve sim-to-real transfer by avoiding centralized state dependencies.
- Mechanism: Each agent πᵢ: Oᵢ → Aᵢ maps only local observations to actions, eliminating reliance on global state that may be unavailable or mismeasured at deployment.
- Core assumption: Local observation policies are sufficient for the coordination tasks studied; strong inter-agent coupling doesn't require centralized computation.
- Evidence anchors: Section VII shows physical robot improvements; Section IV discusses deployment considerations.
- Break condition: Tasks requiring tight synchronization with global state access may need centralized architectures.

## Foundational Learning

- **Behavior Cloning**: Supervised policy learning from expert trajectories. Why needed: R2BC builds directly on standard BC loss (MSE between predicted and demonstrated actions).
  - Quick check: Can you write the BC objective: min_θ E[||π_θ(s) - a_expert||²]?

- **Covariate shift in imitation learning**: Train/test distribution mismatch. Why needed: R2BC's core claim is addressing covariate shift better than offline joint BC.
  - Quick check: Why does BC fail when deployed states differ from demonstration states?

- **Decentralized vs. centralized policies**: Decentralized trains N independent policies πᵢ rather than one joint policy π: S → A¹×...×A^N.
  - Quick check: What observation does each agent receive at execution time in a decentralized setting?

## Architecture Onboarding

- **Component map**: Demonstration buffers D₁...D_N → Round-robin scheduler → Policy networks πᵢ → Update trigger → BC training
- **Critical path**: Initialize policies → For each round: demonstrate for agent i while others run current policies → Store transitions in Dᵢ → Every k rounds: train each πᵢ on its buffer via gradient descent on BC loss → Repeat until convergence
- **Design tradeoffs**: k (update frequency) - Lower = fresher teammates during demos but higher compute; Demonstration order - Paper uses round-robin for "uniform spread"; Buffer management - Not specified
- **Failure signatures**: Policies converge to non-coordinating local optima; Performance oscillates; High variance across seeds
- **First 3 experiments**:
  1. Replicate single-agent VMAS task (e.g., Navigation with N=2) comparing R2BC vs. offline BC vs. random-teammate ablation
  2. Ablate update frequency k (e.g., k∈{1, 5, 20, 50}) to find stability-efficiency tradeoff
  3. Test on task with stronger inter-agent coupling to probe where decentralized policies break down

## Open Questions the Paper Calls Out

- **Cognitive burden**: One underexplored aspect is the human burden; the authors plan to conduct user studies with non-experts to evaluate the cognitive burden and qualitative experience of using R2BC compared to centralized teleoperation.

- **Theoretical guarantees**: Developing theoretical guarantees for convergence and covariate shift reduction would further reinforce the intuition provided in this work, as the paper currently relies on empirical results.

- **Scalability**: The paper's experiments are restricted to small teams (N=2 or N=3), leaving open how sample efficiency and convergence speed scale as the number of agents increases significantly.

## Limitations

- **Missing hyperparameters**: Critical implementation details like network architecture (layers, hidden units), update frequency k, and exact observation space dimensions are not specified, making faithful reproduction challenging.

- **Small team scope**: Experiments are limited to N=2 or N=3 agents, leaving uncertainty about scalability to larger teams where round-robin demonstration time per agent grows linearly.

- **Non-expert validation**: Physical robot demonstrations were performed by paper authors (likely experts) rather than a diverse pool of non-expert users, leaving the method's usability for non-experts untested.

## Confidence

- **High confidence**: R2BC's basic mechanism works for the four tested VMAS tasks; covariate-shift reduction and sim-to-real transfer improvements are well-supported.
- **Medium confidence**: Generality to other multi-agent settings remains untested; the paper doesn't characterize failure modes or task boundaries.
- **Low confidence**: Specific performance gains depend heavily on unspecified implementation details that could significantly alter results.

## Next Checks

1. **Replicate single-agent VMAS task**: Implement Navigation with N=2 comparing R2BC vs. offline BC vs. random-teammate ablation to validate the covariate-shift mechanism on your infrastructure.

2. **Ablate update frequency k**: Test k∈{1, 5, 20, 50} to identify the stability-efficiency tradeoff for your target task complexity, as the paper doesn't specify this critical hyperparameter.

3. **Stress test on tightly-coupled tasks**: Apply R2BC to a task requiring stronger inter-agent coupling than the paper's four VMAS tasks to identify where decentralized policies break down and characterize applicability boundaries.