---
ver: rpa2
title: Grounding Generated Videos in Feasible Plans via World Models
arxiv_id: '2602.01960'
source_url: https://arxiv.org/abs/2602.01960
tags:
- video
- latent
- plans
- world
- gvp-wm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GVP-WM is a test-time planning method that grounds video-generated
  plans into feasible action sequences using pre-trained action-conditioned world
  models. It formulates planning as a constrained latent-space trajectory optimization
  problem, jointly optimizing latent states and actions under world-model dynamics
  while preserving semantic alignment with video plans.
---

# Grounding Generated Videos in Feasible Plans via Feasible Plans via World Models

## Quick Facts
- **arXiv ID**: 2602.01960
- **Source URL**: https://arxiv.org/abs/2602.01960
- **Reference count**: 40
- **Primary result**: GVP-WM achieves up to 98% success rate on navigation and manipulation tasks by grounding video-generated plans into feasible action sequences using world models

## Executive Summary
GVP-WM introduces a test-time planning method that grounds video-generated plans into executable action sequences using pre-trained action-conditioned world models. The approach formulates planning as a constrained latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics while preserving semantic alignment with video plans. Empirically, GVP-WM outperforms both world-model-only planners and direct video-to-action baselines, recovering feasible long-horizon plans from physically inconsistent video guidance including motion-blurred and zero-shot generated videos.

## Method Summary
GVP-WM uses video plans as semantic guidance to initialize and constrain latent-space trajectory optimization. The method encodes video frames and goals into latent states, then jointly optimizes latent states and actions under world-model dynamics constraints using Augmented Lagrangian Method. A video alignment loss ensures semantic consistency while dynamics constraints enforce physical feasibility. The optimized trajectories are executed via Model Predictive Control with optional local refinement sampling. Key design choices include scale-invariant cosine similarity for latent alignment and progressive penalty scheduling to balance semantic guidance with dynamics enforcement.

## Key Results
- Achieves 98% success rate on Push-T environment with expert video guidance
- Maintains 82% success under zero-shot video guidance while inverse-dynamics methods fail completely
- Outperforms world-model-only planners and direct video-to-action baselines on navigation and manipulation tasks
- Robust to temporal inconsistencies and motion blur that cause inverse-dynamics methods to fail

## Why This Works (Mechanism)

### Mechanism 1: Video-Guided Latent Collocation
Jointly optimizing latent states and actions under world-model dynamics can recover feasible plans from physically inconsistent video guidance. Unlike inverse-dynamics methods that directly map video frames to actions, GVP-WM treats both latent states Z and actions A as decision variables in constrained trajectory optimization. The video plan provides semantic guidance while world-model dynamics are enforced as hard constraints via augmented Lagrangian optimization. Core assumption: the pre-trained world model captures sufficient environment dynamics to project video plans onto feasible manifolds. Break condition: when world-model dynamics are severely misaligned with true environment dynamics, optimization converges to infeasible plans regardless of video quality.

### Mechanism 2: Scale-Invariant Semantic Alignment
Cosine similarity between latent states provides robustness to magnitude drift between video encoder and world model encoder distributions. Video-generated frames encoded by the world model's visual encoder may exhibit magnitude differences from in-distribution latent states. The alignment loss Lvid = ||φ(zt) - φ(zvid_t)||² where φ(z) = z/||z||₂ projects to unit hypersphere, penalizing angular deviation while remaining invariant to magnitude. Core assumption: the semantic content of video frames is preserved in angular structure of latent embeddings even when magnitude differs. Break condition: when video and world-model latent spaces are semantically misaligned, cosine similarity may optimize toward wrong attractors.

### Mechanism 3: Augmented Lagrangian with Geometric Penalty Schedule
Progressive enforcement of dynamics constraints enables optimization to prioritize semantic alignment early while achieving physical feasibility late. The ALM alternates primal updates with dual updates, allowing early iterations to follow video guidance before hard dynamics enforcement. The penalty parameter grows geometrically, enabling the optimization to first follow semantic guidance before enforcing physical consistency. Core assumption: video plans provide useful semantic priors that should guide initial optimization before hard dynamics enforcement. Break condition: when video guidance is severely out-of-distribution, early semantic alignment may trap optimization in poor local minima before dynamics can correct.

## Foundational Learning

- **Concept**: Action-Conditioned World Models
  - Why needed here: GVP-WM requires understanding how latent dynamics predictors encode environment physics conditioned on actions
  - Quick check question: Can you explain why planning in latent space is computationally preferable to pixel-space planning?

- **Concept**: Direct Collocation vs Shooting Methods
  - Why needed here: The paper contrasts collocation (states and actions as decision variables) with shooting (actions only, states determined by forward simulation)
  - Quick check question: Why might collocation be more robust than shooting when initial state estimates are unreliable?

- **Concept**: Augmented Lagrangian Methods
  - Why needed here: Understanding primal-dual optimization is essential for debugging convergence issues in the trajectory optimizer
  - Quick check question: What happens if the penalty parameter ρ is initialized too high?

## Architecture Onboarding

- **Component map**: Video Generator -> Video Encoder -> Latent States Z -> ALM Optimizer -> Actions A -> World Model Dynamics -> Feasible Trajectory
- **Critical path**: Video generation → Latent encoding → ALM optimization (IALM × OALM iterations) → Action extraction → MPC execution → State update → Replan
- **Design tradeoffs**:
  - CEM vs ALM: CEM (MPC-CEM) is more robust but slower (277s vs 86s at T=25 Push-T); ALM is faster but requires differentiable dynamics
  - Video guidance weight λv: Higher values improve performance with good video but hurt with bad video
  - Planning horizon T: Success degrades sharply at longer horizons
- **Failure signatures**:
  - Morphological drift in video: Object identity changes across frames
  - Rigid-body physics violations: Agent passing through objects
  - Spatial bilocation: Agent appears at multiple locations
  - Motion blur sensitivity: UniPi fails completely; GVP-WM degrades gracefully
- **First 3 experiments**:
  1. Run MPC-CEM and MPC-GD on your environment without video guidance to establish world-model quality
  2. Generate video plans with and without LoRA fine-tuning to measure video quality contribution
  3. Fix latent states to video plan (no collocation) and optimize actions only to confirm joint optimization is required

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GVP-WM successfully ground video-generated plans in real-world robotic manipulation tasks, where zero-shot video guidance may be more in-distribution with internet-scale video pretraining data?
- Basis in paper: Section 6.2 states "Future work includes extending GVP-WM to real-world robotic manipulation, where zero-shot video guidance is more likely to be in-distribution."
- Why unresolved: All experiments are conducted in simulation; the method has not been validated on physical robot hardware where dynamics differ from simulation
- What evidence would resolve it: Empirical evaluation on real-world robotic manipulation tasks comparing GVP-WM against baselines

### Open Question 2
- Question: How can hierarchical planning or policy distillation improve GVP-WM's scalability for long-horizon tasks where performance degrades?
- Basis in paper: Section 6.2 states future work includes "exploring hierarchical planning and policy distillation to improve scalability for long-horizon tasks."
- Why unresolved: The paper shows substantial performance drops at longer horizons but does not investigate hierarchical decomposition approaches
- What evidence would resolve it: Ablation studies comparing flat GVP-WM against hierarchical variants on tasks with horizons T>80

### Open Question 3
- Question: Under what conditions does video guidance provide net benefit versus harm, given that removing video guidance improved performance under zero-shot conditions?
- Basis in paper: Table 4 shows that "removing video guidance... improves performance under zero-shot guidance (WAN-0S), but degrades performance under higher-quality guidance."
- Why unresolved: The current method applies fixed video alignment weighting regardless of guidance quality
- What evidence would resolve it: Development of an adaptive weighting scheme that modulates λv based on video plan reliability

## Limitations
- Performance degrades significantly at longer horizons (T=80), limiting applicability to very long tasks
- Reliance on frozen pre-trained components creates coupling constraints that may not transfer to domains with different visual representations
- No mechanism to automatically detect when video guidance should be trusted versus ignored

## Confidence

**Major uncertainties**: The method demonstrates strong performance on two specific navigation and manipulation tasks, but generalizability to other domains remains uncertain.

**Confidence labels**:
- **High**: Claims about ALM optimization effectiveness, MPC execution framework, and overall success rate improvements over baselines
- **Medium**: Claims about scale-invariant semantic alignment robustness and video fine-tuning benefits
- **Low**: Claims about method's performance on out-of-distribution video quality scenarios and generalization to unseen task types

## Next Checks

1. **Cross-domain transfer test**: Apply GVP-WM to a different task domain (e.g., pick-and-place or assembly) with minimal architecture changes to assess generalizability
2. **Latent space alignment study**: Systematically vary the magnitude mismatch between video encoder and world model latent spaces to quantify alignment loss effectiveness across different scales
3. **Robustness to video quality degradation**: Evaluate performance with progressively corrupted video guidance (compression artifacts, resolution reduction, temporal subsampling) to establish failure thresholds