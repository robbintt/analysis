---
ver: rpa2
title: 'Synthesis by Design: Controlled Data Generation via Structural Guidance'
arxiv_id: '2506.07664'
source_url: https://arxiv.org/abs/2506.07664
tags:
- reasoning
- code
- reason
- data
- bolts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for controlled data generation in mathematical
  reasoning tasks by extracting structural information from problem-solving code and
  using it to guide the generation of new problems. The method translates reasoning
  processes into executable Python code, parses the computational structure, and intervenes
  on this structure to generate new problems with labeled intermediate steps.
---

# Synthesis by Design: Controlled Data Generation via Structural Guidance

## Quick Facts
- arXiv ID: 2506.07664
- Source URL: https://arxiv.org/abs/2506.07664
- Reference count: 40
- Primary result: Controlled data generation for mathematical reasoning via code-based structural intervention

## Executive Summary
This paper proposes a method for generating mathematically rigorous training data by extracting and manipulating the computational structure from problem-solving code. The approach translates reasoning processes into Python code, parses the computational graph, and intervenes on this structure to create new problems with labeled intermediate steps. Applied to GSM8K and MATH datasets, the method produces 39K problems with step-level supervision and introduces a 6.1K-problem benchmark of higher difficulty. The generated dataset improves reasoning performance across multiple LLM architectures when used for fine-tuning.

## Method Summary
The method operates through a pipeline of code generation, structural intervention, and translation. First, reasoning steps are translated into executable Python code using a strong LLM. The computational structure is then parsed into a tree where nodes represent variables, and interventions are performed by injecting proxy variables to extend reasoning chains. Modified code is translated back into natural language questions, which undergo quality control through code execution and LLM evaluation. The final dataset of 39K samples is used to fine-tune various models, demonstrating improved performance on both original and newly proposed benchmarks.

## Key Results
- Generated 39K problems with step-level supervision from GSM8K and MATH datasets
- Created 6.1K-problem "Struct" benchmark showing performance declines as reasoning length increases
- Fine-tuning experiments show consistent improvements across Mistral-7b, Qwen-2-7b, and Llama-3-8b
- Quality control pipeline discards 48.6-64.6% of generated samples, indicating strict validation

## Why This Works (Mechanism)
The approach leverages the precise semantics of executable code to ensure mathematical rigor in generated problems. By intervening on the computational structure rather than natural language, the method guarantees numerical consistency between problems and their solutions. The tree-based representation of reasoning processes allows controlled complexity manipulation through proxy variable injection, creating problems that systematically extend existing reasoning chains while maintaining semantic validity.

## Foundational Learning

**Code Generation from Reasoning**: Converting natural language solution steps into executable Python code interleaved with comments.
*Why needed*: Provides a precise, unambiguous representation of the reasoning process that can be programmatically manipulated.
*Quick check*: Verify generated code correctly computes the stated solution when executed.

**Computational Graph Parsing**: Transforming Python code into a tree-like structure where nodes represent variables and edges represent dependencies.
*Why needed*: Enables systematic identification of intervention points and structural manipulation of reasoning chains.
*Quick check*: Confirm the parsed graph preserves all variable dependencies from the original code.

**Structural Intervention**: Injecting proxy variables into the computational graph to extend reasoning chains while maintaining mathematical consistency.
*Why needed*: Creates controlled complexity in generated problems by systematically building on existing reasoning patterns.
*Quick check*: Validate that intervened code executes correctly and produces consistent ground truths.

## Architecture Onboarding

**Component Map**: GSM8K/MATH datasets → Code Generation (Claude-3.5-Sonnet) → Structural Intervention → Code Translation → Quality Control (Code Execution + GPT-4o) → Generated Dataset → Fine-tuning (LlamaFactory)

**Critical Path**: The quality control pipeline is the critical path, as it filters ~50% of generated samples. The GPT-4o semantic evaluation is particularly crucial for ensuring generated problems are mathematically meaningful and not just syntactically valid.

**Design Tradeoffs**: 
- High precision (code-based intervention) vs. generation efficiency (strict filtering discards many samples)
- Control over complexity (structural intervention) vs. potential semantic hallucination (LLM translation)
- Systematic generation (code-based) vs. linguistic naturalness (natural language output)

**Failure Signatures**:
- Semantic hallucination: Code executes correctly but describes implausible scenarios (e.g., "0.3 people")
- Low generation yield: Intervention logic creates unsolvable states, resulting in high rejection rates
- Catastrophic forgetting: Fine-tuning degrades performance on specific subsets of original data

**First Experiments**:
1. Validate the complete prompting pipeline using provided templates and measure generation yield
2. Replicate fine-tuning on a smaller scale (1K samples) to verify performance improvements
3. Manually inspect accepted vs. rejected samples to validate quality control effectiveness

## Open Questions the Paper Calls Out

**Hybrid Augmentation Approach**: Can combining structural intervention with paraphrasing-based augmentation efficiently scale data volume while preserving rigorous step-level supervision? The current incremental process discards roughly half of generated data due to strict validation.

**Utilizing Failed Generations**: How can "failed generation" samples be leveraged to improve performance on complex tasks without causing degradation on simpler ones? Erroneous reasoning processes may still help, but the mechanism for incorporating this noise remains unclear.

**Structural Properties Beyond Complexity**: Beyond quantifying complexity, how can the topological structure of code-based reasoning be leveraged to further enhance LLM capabilities? The current work primarily uses the DAG nature for counting steps and simple interventions.

## Limitations

- Generation efficiency is low, with 48.6-64.6% of samples discarded during quality control
- Strict filtering criteria may discard mathematically valid but linguistically unusual problems
- Performance improvements on simpler datasets (GSM8K) come at the cost of some degradation
- Limited exploration of the full potential of computational graph topological properties

## Confidence

- **High confidence**: Basic pipeline of code generation → structural intervention → code translation is clearly specified
- **Medium confidence**: Effectiveness of generated dataset for fine-tuning, as results show improvements but prompt formats are incomplete
- **Medium confidence**: Quality control methodology relies on black-box LLM evaluation without detailed criteria

## Next Checks

1. Implement and validate the complete prompting pipeline using few-shot examples from supplementary materials, then measure actual generation yield and quality control rejection rates.
2. Replicate fine-tuning experiments on a smaller scale (1K samples) to verify intervention-based approach consistently improves reasoning performance across different model families.
3. Analyze semantic validity of generated problems by manually inspecting random samples of accepted and rejected cases from quality control pipeline.