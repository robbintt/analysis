---
ver: rpa2
title: Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian
  NLP
arxiv_id: '2510.22823'
source_url: https://arxiv.org/abs/2510.22823
tags:
- languages
- language
- across
- arxiv
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first systematic evaluation of cross-lingual
  robustness in large language models (LLMs) for human rights violation detection.
  Across 78,000 multilingual inferences in seven languages, the research compares
  four commercial instruction-aligned models (GPT-4.1-mini, Claude Sonnet 4, DeepSeek-V3,
  Gemini-Flash-2.0) with two open-weight models (LLaMA-3-8B, Mistral-7B) using novel
  metrics including Calibration Deviation (CD), Language Robustness Score (LRS), and
  Language Stability Score (LSS).
---

# Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP

## Quick Facts
- arXiv ID: 2510.22823
- Source URL: https://arxiv.org/abs/2510.22823
- Reference count: 0
- First systematic evaluation of cross-lingual robustness in LLMs for human rights violation detection

## Executive Summary
This study provides the first systematic evaluation of cross-lingual robustness in large language models (LLMs) for human rights violation detection. Across 78,000 multilingual inferences in seven languages, the research compares four commercial instruction-aligned models (GPT-4.1-mini, Claude Sonnet 4, DeepSeek-V3, Gemini-Flash-2.0) with two open-weight models (LLaMA-3-8B, Mistral-7B) using novel metrics including Calibration Deviation (CD), Language Robustness Score (LRS), and Language Stability Score (LSS). The results demonstrate that multilingual alignment—not model scale—determines cross-lingual stability. Aligned models maintain near-invariant accuracy (F1 variance < 0.03) and balanced calibration across languages, while open-weight models exhibit significant prompt-language sensitivity (F1 variance > 0.15) and calibration drift.

## Method Summary
The evaluation framework tested seven languages across two model categories: four commercial instruction-aligned models and two open-weight models. Novel metrics were developed including Calibration Deviation (CD) for measuring confidence alignment across languages, Language Robustness Score (LRS) for quantifying accuracy stability, and Language Stability Score (LSS) for assessing prompt-language sensitivity. The study conducted 78,000 multilingual inferences focused on human rights violation detection, measuring F1 scores, false-positive rates, and calibration consistency across language variations. Both base and instruction-tuned versions were evaluated where available.

## Key Results
- Multilingual alignment—not model scale—determines cross-lingual stability
- Aligned models maintain near-invariant accuracy (F1 variance < 0.03) and balanced calibration across languages
- Open-weight models exhibit significant prompt-language sensitivity (F1 variance > 0.15) and calibration drift
- In low-resource languages like Lingala and Burmese, aligned models preserve strong performance (F1 > 0.78) while open-weight models drop below 0.60

## Why This Works (Mechanism)
Multilingual alignment quality directly determines cross-lingual stability in LLMs for humanitarian applications. Commercial instruction-aligned models achieve consistent performance across languages through explicit multilingual training objectives and robust calibration mechanisms, while open-weight models suffer from prompt-language sensitivity and calibration drift that manifest as inflated false-positive rates in low-resource languages. The evaluation reveals that instruction-tuning alone is insufficient—the quality and scope of multilingual alignment is the critical determinant of reliable cross-lingual performance in high-stakes contexts.

## Foundational Learning
- **Calibration Deviation (CD)**: Measures confidence alignment across languages; needed to quantify how well models maintain consistent probability estimates across different languages
- **Language Robustness Score (LRS)**: Quantifies accuracy stability across languages; needed to assess whether models maintain consistent performance regardless of input language
- **Language Stability Score (LSS)**: Evaluates prompt-language sensitivity; needed to detect when model outputs vary based on language rather than content
- **Multilingual alignment quality**: The degree to which models are trained to handle multiple languages consistently; needed to understand why some models perform better cross-lingually
- **False-positive inflation in low-resource languages**: The tendency for models to generate more incorrect positive predictions in languages with less training data; needed to identify deployment risks in humanitarian contexts

## Architecture Onboarding
- **Component map**: Instruction-tuned LLMs (Commercial/aligned) -> Multilingual alignment layer -> Cross-lingual stability; Open-weight models -> Limited alignment -> Performance variance
- **Critical path**: Multilingual training objectives → Robust calibration mechanisms → Cross-lingual performance invariance
- **Design tradeoffs**: Model scale vs. alignment quality; cost considerations vs. reliability requirements; commercial API access vs. open-source flexibility
- **Failure signatures**: High F1 variance (> 0.15), calibration drift, inflated false-positive rates (> 0.8) in low-resource languages, prompt-language sensitivity
- **First 3 experiments**: 1) Test commercial models on additional humanitarian NLP tasks beyond human rights detection; 2) Evaluate fine-tuned open-weight models on same multilingual dataset; 3) Measure calibration consistency across additional low-resource languages

## Open Questions the Paper Calls Out
The evaluation framework relies on a single humanitarian domain dataset (human rights violation detection), which may not generalize to other multilingual NLP tasks. The study examines only seven languages, though representing diverse linguistic families, leaving open questions about performance in other language pairs and scripts. The commercial models tested are proprietary, preventing detailed analysis of their multilingual alignment mechanisms. Additionally, the open-weight models were evaluated with their base versions without fine-tuning for humanitarian contexts, which may underestimate their potential with domain adaptation.

## Limitations
- Single domain dataset limits generalizability to other humanitarian NLP tasks
- Limited to seven languages, may not represent all linguistic families and scripts
- Proprietary commercial models prevent analysis of multilingual alignment mechanisms
- Open-weight models tested without humanitarian fine-tuning may underestimate potential

## Confidence
- **High Confidence**: Cross-lingual stability differences between aligned commercial models and open-weight models; relationship between multilingual alignment quality and performance invariance; superior calibration and lower false-positive rates of aligned models in low-resource languages
- **Medium Confidence**: Generalizability to other humanitarian NLP tasks; absolute performance thresholds may vary with different datasets
- **Low Confidence**: Cost-effectiveness conclusions beyond tested models; predictions about alignment quality's universal importance across all multilingual applications

## Next Checks
1. Replicate evaluation across multiple humanitarian NLP tasks (disaster response, refugee tracking, medical supply chain monitoring)
2. Expand language coverage to include additional low-resource languages from different linguistic families
3. Conduct controlled experiments fine-tuning open-weight models on humanitarian datasets with explicit multilingual alignment objectives