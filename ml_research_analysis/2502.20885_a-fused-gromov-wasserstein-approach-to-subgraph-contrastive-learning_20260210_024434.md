---
ver: rpa2
title: A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning
arxiv_id: '2502.20885'
source_url: https://arxiv.org/abs/2502.20885
tags:
- graph
- learning
- node
- contrastive
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FOSSIL, a novel subgraph-level contrastive
  learning framework that integrates node-level and subgraph-level contrastive learning
  using the Fused Gromov-Wasserstein Distance (FGWD). FOSSIL addresses the challenge
  of capturing both structural patterns and node similarities in graph representation
  learning, particularly in homophilic and heterophilic graphs.
---

# A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning

## Quick Facts
- **arXiv ID:** 2502.20885
- **Source URL:** https://arxiv.org/abs/2502.20885
- **Reference count:** 25
- **Primary result:** FOSSIL achieves 80.02% accuracy on Cora and 35.61% on Actor, outperforming or matching nine state-of-the-art methods across seven datasets.

## Executive Summary
This paper introduces FOSSIL, a subgraph-level contrastive learning framework that addresses the challenge of capturing both structural patterns and node similarities in graph representation learning. The method innovatively combines node-level and subgraph-level contrastive learning using the Fused Gromov-Wasserstein Distance (FGWD), which jointly captures feature and structural information during subgraph comparison. FOSSIL employs a decoupled encoding scheme that separates feature processing from structure processing, making it particularly effective for both homophilic and heterophilic graphs. The framework integrates an adaptive fusion mechanism based on node centrality, enabling the model to dynamically balance feature and structural information. Extensive experiments on seven benchmark datasets demonstrate that FOSSIL either outperforms or achieves competitive performance compared to nine state-of-the-art methods, with particular robustness across datasets with varying homophily levels.

## Method Summary
FOSSIL is a self-supervised learning framework for node classification that addresses the limitations of existing methods in handling both homophilic and heterophilic graphs. The method employs a decoupled encoder that processes node features through both a GCN (without self-loops) and an MLP with shared weights, producing separate feature and structural embeddings. These embeddings are then adaptively fused using node-wise coefficients based on node centrality. For contrastive learning, FOSSIL uses FGWD to compare subgraphs, capturing both feature and structural similarity through a joint optimization that learns a soft matching between subgraph node distributions. The framework combines subgraph-level OT-based contrast with node-level InfoNCE loss to capture both local discriminative features and global structural patterns. The total loss includes FGWD-based subgraph contrast, InfoNCE-based node contrast, and a fusion regularization term. The method uses BFS-based subgraph sampling from anchor nodes and includes a variant (FOSSILv2) for scalability on larger graphs.

## Key Results
- FOSSIL achieves 80.02% accuracy on Cora (homophilic) and 35.61% on Actor (heterophilic) datasets
- Outperforms or matches nine state-of-the-art methods including GSC, DGI, and GRACE across seven benchmark datasets
- Ablation studies validate the effectiveness of FGWD and the decoupled encoder design
- Shows particular robustness across datasets with varying homophily levels

## Why This Works (Mechanism)

### Mechanism 1: Joint Feature-Structure Comparison via Fused Gromov-Wasserstein Distance
FGWD combines Wasserstein Distance (WD) for feature similarity and Gromov-Wasserstein Distance (GWD) for structural similarity through a learned trade-off parameter α ∈ [0,1]. The objective optimizes a soft matching P between subgraph node distributions that minimizes joint transportation cost, capturing both feature and structural information simultaneously rather than processing them separately.

### Mechanism 2: Decoupled Encoding for Homophily Robustness
Two parallel pathways with shared weights process features (H_f = σ(IXW)) and structure (H_s = σ(ÃXW)) separately. These are combined via learned node-wise coefficients λ_i = ψ(H_f^i, H_s^i, degree_i; θ), producing H_i = H_f^i + λ_i H_s^i. This design allows the model to adaptively balance feature and structural information based on node centrality, making it robust to varying homophily levels.

### Mechanism 3: Hierarchical Contrastive Learning (Subgraph + Node Level)
FOSSIL combines subgraph-level OT-based contrast (L_ot) with node-level InfoNCE loss (L_node) to capture both structural similarity at the subgraph level and individual discriminative features at the node level. This dual approach ensures that both local patterns and global structural information are preserved in the learned representations.

## Foundational Learning

- **Optimal Transport (Wasserstein and Gromov-Wasserstein Distances)**
  - Why needed here: FGWD is the core contribution; understanding how OT measures similarity between distributions in different metric spaces is essential.
  - Quick check question: Can you explain why WD requires a shared metric space while GWD does not?

- **Graph Homophily vs. Heterophily**
  - Why needed here: The decoupled encoder design explicitly addresses performance degradation on heterophilic graphs where connected nodes may have dissimilar labels.
  - Quick check question: Given a graph with homophily ratio H(G) = 0.22, would you expect GCN to outperform MLP on node classification?

- **Contrastive Learning (InfoNCE and Jensen-Shannon Estimators)**
  - Why needed here: FOSSIL uses two different contrastive objectives; understanding mutual information maximization lower bounds is necessary.
  - Quick check question: What happens to InfoNCE loss when all negative pairs have the same similarity as positive pairs?

## Architecture Onboarding

- **Component map:** Input graph -> Encoder (H_f, H_s) -> Generator (Ĥ_f, Ĥ_s) -> Fusion (H, Ĥ) -> Subgraph Sampling -> FGWD computation -> Backprop through L_ot + L_node + L_θ

- **Critical path:** Input graph → Encoder (H_f, H_s) → Generator (Ĥ_f, Ĥ_s) → Fusion (H, Ĥ) → Subgraph Sampling → FGWD computation → Backprop through L_ot + L_node + L_θ

- **Design tradeoffs:**
  - FGWD vs. separate WD+GWD: FGWD is more expressive but requires solving coupled quadratic optimization (O(Tk³) per subgraph pair)
  - FOSSIL vs. FOSSILv2: Full node-level contrast has O(N²D) memory; FOSSILv2 restricts to sampled subgraph nodes O((|S|k)²D) for scalability
  - Random vs. learned perturbations: GAT-based generator outperforms random edge dropping but adds parameters and optimization complexity

- **Failure signatures:**
  - Collapse to trivial λ: All λ_i → 0 or 1, indicating fusion module not learning (check L_θ gradients)
  - FGWD not converging: BAPG iterations exceed T without meeting ε threshold (increase T or adjust β regularizer)
  - Memory overflow on large graphs: L_node requires N×N similarity matrix; switch to FOSSILv2
  - Poor heterophilic performance: λ values may be biased toward structure; inspect α vs. λ correlation

- **First 3 experiments:**
  1. Baseline comparison: Run FOSSIL on Cora (homophilic) and Actor (heterophilic) with default hyperparameters; compare against GSC and DGI to validate claimed improvements
  2. Ablation on FGWD: Disable L_node, replace FGWD with WD only and GWD only; confirm Table 3 trends on a new dataset
  3. Encoder architecture test: Swap decoupled encoder for vanilla GCN and GCNII; verify performance drop on heterophilic datasets per Table 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating a high-pass feature extractor into the decoupled encoding scheme significantly improve performance on heterophilic graphs?
- Basis in paper: The Conclusion states: "We suggest future research avenues, such as integrating a high-pass feature extractor to enhance performance in such scenarios."
- Why unresolved: The current encoder combines a low-pass GCN filter and an MLP. While robust, the sensitivity analysis (Appendix C) shows the model relies heavily on the MLP (α ≈ 1) for heterophilic graphs because the GCN struggles to exploit the structure, indicating a missing structural processing component.
- What evidence would resolve it: Empirical results comparing the current architecture against a variant using adaptive high-pass filters on benchmark heterophilic datasets like Chameleon or Squirrel.

### Open Question 2
- Question: Can the FOSSIL framework be effectively adapted for downstream tasks other than node classification, such as link prediction or graph classification?
- Basis in paper: The Conclusion notes: "Similarly, extending FOSSIL to other downstream tasks such as link prediction or graph classification deserves further research."
- Why unresolved: The current experimental design and loss functions (L_ot and L_node) are specifically tailored for generating node embeddings, and the method has not been validated for graph-level or edge-level tasks.
- What evidence would resolve it: Successful application of the FGWD-based subgraph contrastive loss to graph-level benchmarks (e.g., OGBG) or link prediction datasets, demonstrating competitive performance against state-of-the-art methods for those tasks.

### Open Question 3
- Question: How does replacing the random breadth-first subgraph sampling with adaptive, data-driven sampling strategies impact the efficiency and effectiveness of the learned representations?
- Basis in paper: Section 4.5 mentions: "Some studies have proposed more adaptive and data-driven subgraph sampling techniques, which present an interesting direction for future work in subgraph contrastive learning."
- Why unresolved: The authors chose random sampling for simplicity, but this approach may not consistently capture the most informative local structures compared to learnable sampling policies.
- What evidence would resolve it: A comparative ablation study integrating advanced subgraph selection policies (e.g., reinforcement learning-based samplers) into FOSSIL and measuring changes in downstream accuracy and training convergence speed.

## Limitations

- The decoupled encoder's performance heavily depends on the quality of the fusion MLP ψ, which is not fully specified in the paper (hidden dimensions omitted)
- The FGWD solver requires careful hyperparameter tuning (T, ε, β) that significantly impacts convergence and runtime
- The node-level contrastive loss has O(N²) memory complexity, making the full FOSSIL method impractical for large graphs without switching to the FOSSILv2 variant

## Confidence

- **High Confidence:** The overall performance improvements over baselines are well-supported by extensive experiments across seven datasets with proper statistical reporting
- **Medium Confidence:** The specific architectural contributions (decoupled encoder, FGWD-based contrast) are validated through ablation studies, though some design choices lack detailed justification
- **Low Confidence:** The claim that FGWD specifically provides advantages over sequential WD+GWD processing is supported only by one ablation study without theoretical grounding or broader empirical validation

## Next Checks

1. **Reproduce core results** on Cora and Actor datasets using default hyperparameters to verify the 80.02% and 35.61% accuracies respectively
2. **Validate fusion module behavior** by inspecting λ coefficient distributions across homophilic and heterophilic graphs during training
3. **Test FGWD vs. sequential comparison** by implementing both approaches on a synthetic dataset where ground truth feature-structure relationships are known