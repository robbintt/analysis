---
ver: rpa2
title: 'VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset
  for Legal Question Answering'
arxiv_id: '2507.19995'
source_url: https://arxiv.org/abs/2507.19995
tags:
- legal
- question
- articles
- dataset
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLQA is the first comprehensive, large-scale, and expert-annotated
  Vietnamese dataset for legal question answering, addressing the scarcity of legal
  NLP resources in low-resource languages. The dataset comprises 3,129 real-world
  legal questions with detailed answers and citations to 59,636 statutory articles,
  verified by legal experts.
---

# VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering

## Quick Facts
- arXiv ID: 2507.19995
- Source URL: https://arxiv.org/abs/2507.19995
- Reference count: 40
- First comprehensive, expert-annotated Vietnamese legal dataset with 3,129 questions and 59,636 statutory articles

## Executive Summary
VLQA introduces the first large-scale, expert-annotated Vietnamese dataset for legal question answering, addressing the critical scarcity of legal NLP resources in low-resource languages. The dataset comprises 3,129 real-world legal questions with detailed answers and citations to 59,636 statutory articles, verified by legal experts. The authors evaluate multiple baseline models for legal article retrieval and question answering, including traditional sparse models (BM25) and modern dense retrieval methods (BGE-m3, fine-tuned mBERT). Results show dense retrieval models significantly outperform sparse approaches, while generative models demonstrate superior contextual understanding compared to extractive methods.

## Method Summary
The authors collected 3,129 real-world legal questions from public Vietnamese legal consultation platforms and had them verified by legal experts. These questions were paired with answers and citations to 59,636 statutory articles from 2,162 legal documents. The dataset was split 7:1:2 for train/val/test. For retrieval, they evaluated BM25, zero-shot BGE-m3, and fine-tuned mBERT/BGE-reranker. For QA, they tested extractive models (mBERT, PhoBERT, ViDeBERTa), generative models (ViT5, BARTpho), and open-weight/commercial LLMs (GPT-4o, Qwen2.5, Llama3). All models were evaluated using ROUGE, BERTScore, and human assessment of factual accuracy.

## Key Results
- Dense retrieval models (BGE-m3) significantly outperform sparse approaches, achieving 54.4% recall@2 versus 38.1% for BM25
- Fine-tuned mBERT retriever trained on 2.1k in-domain samples achieves 62.6% recall@2, substantially outperforming zero-shot baselines
- Generative models (BARTpho, ViT5) demonstrate superior contextual understanding compared to extractive approaches
- Commercial LLMs (GPT-4o-mini, Qwen2.5-14B) achieve strong performance but occasionally produce incomplete or hallucinated responses
- All retrieval models show precision@2 < 0.40, indicating high noise levels in retrieved articles

## Why This Works (Mechanism)

### Mechanism 1: Dense Retrieval Outperforms Sparse Methods
Dense models like BGE-m3 capture semantic similarity between questions and articles rather than relying solely on term frequency. This enables retrieval even when question phrasing differs from article language, addressing the vocabulary mismatch between layperson questions and formal legal text. BGE-m3 achieves 54.4% recall@2, significantly outperforming BM25's 38.1%.

### Mechanism 2: Domain Adaptation Through Fine-Tuning
Fine-tuning on 2.1k in-domain VLQA samples substantially improves retrieval performance over zero-shot baselines. The supervised learning approach using BM25 pre-ranking followed by cross-encoder reranking with negative sampling enables models to learn legal terminology and citation patterns specific to Vietnamese statutory law, achieving 62.6% recall@2.

### Mechanism 3: Retrieval-Augmented Generation Trade-offs
LLMs generate fluent but sometimes factually unreliable answers because retrieved context constrains generation space without guaranteeing correct information extraction. While GPT-4o-mini produces more fluent responses, it exhibits higher hallucination rates, fabricating law names or citing incorrect penalties not present in retrieved context.

## Foundational Learning

- **Concept: Sparse vs. Dense Retrieval**
  - Why needed here: Understanding the distinction between lexical matching (BM25) and semantic embedding approaches (BGE-m3) is essential for interpreting baseline results
  - Quick check question: Given a layperson query "How do I contest a traffic fine?" and a legal article titled "Administrative Procedures for Traffic Violation Appeals," which retrieval method would likely perform better and why?

- **Concept: Cross-Encoder vs. Bi-Encoder Architectures**
  - Why needed here: The paper uses bi-encoders for initial retrieval and cross-encoders for reranking; understanding this tradeoff is critical for implementing the two-stage system
  - Quick check question: Which architecture is more suitable for scoring 59,636 articles in real-time: a cross-encoder processing (question, article) pairs jointly, or a bi-encoder pre-computing article embeddings?

- **Concept: In-Context Learning with Prompting Strategies**
  - Why needed here: LLM evaluation uses zero-shot and few-shot prompting; understanding how demonstration examples affect model behavior is necessary for reproducing results
  - Quick check question: Why might two-shot prompting improve GPT-4o performance but degrade performance for smaller models like Llama3.1-8B?

## Architecture Onboarding

- **Component map:**
  Input: Legal question (Vietnamese)
  -> [Stage 1: Retrieval] Sparse Retriever (BM25) → Top-200 candidates; Dense Retriever (BGE-m3 or fine-tuned mBERT) → Re-ranked top-k
  -> Retrieved Articles (top-k, typically k=2-5)
  -> [Stage 2: Question Answering] Extractive (BERT/RoBERTa → sentence selection); Generative (ViT5/BARTpho → sequence-to-sequence); LLM-based (GPT-4o/Qwen2.5 → in-context generation)
  -> Output: Long-form answer with article citations

- **Critical path:**
  1. Article corpus preprocessing (59,636 articles, hierarchical structure extraction, repealed article removal)
  2. Retrieval model selection based on recall@k requirements (BGE-m3 for zero-shot, fine-tuned mBERT for supervised)
  3. QA model selection based on answer format needs (extractive for verbatim extraction, generative for synthesized answers)

- **Design tradeoffs:**
  - Recall vs. Precision: All models show precision@2 < 0.40, meaning retrieval returns irrelevant articles; increasing k improves recall but introduces noise for downstream QA
  - Fluency vs. Accuracy: LLMs produce more fluent answers but higher hallucination rates; extractive models are less fluent but grounded in source text
  - Zero-shot vs. Fine-tuned: Zero-shot dense retrieval (BGE-m3) achieves 54.4% recall@2; fine-tuned mBERT achieves 62.6% recall@2 but requires annotated training data

- **Failure signatures:**
  - Syntactic errors in smaller LLMs: Non-Unicode characters, nonsensical phrasing (77% of Qwen2.5-14B errors)
  - Semantic errors in all LLMs: Incomplete answers (shorter than gold answers), incorrect numerical extraction from articles
  - Hallucination patterns: Fabricating law names or article numbers not present in retrieved context
  - Retrieval failures: Precision@2 < 0.40 indicates >60% of retrieved articles are irrelevant on average

- **First 3 experiments:**
  1. Establish sparse baseline: Implement BM25 with parameters (k₁=1.2, b=0.75) on 59,636-article corpus; measure recall@k for k ∈ {1,2,5,10,20,50,100} to confirm baseline performance
  2. Compare zero-shot dense retrieval: Load BGE-m3 checkpoint, encode all articles and test questions, compute recall@k using cosine similarity; expect recall@2 ≈ 0.54 to validate dense retrieval superiority
  3. Evaluate retrieval-QA coupling: Using BGE-m3 top-2 retrieved articles as context, run GPT-4o-mini with zero-shot prompting; measure ROUGE-L and BERTScore, then manually inspect 20 outputs for hallucination patterns

## Open Questions the Paper Calls Out

- **How can the factual accuracy and reasoning capabilities of LLMs be improved to eliminate hallucinations in Vietnamese legal question answering?**
  - Basis: Models produce fluent but occasionally hallucinated responses, hindering trustworthiness
  - Evidence needed: Model achieving high fluency with near-zero hallucination rate verified by legal experts

- **What specific model architectures are required to effectively handle logical inference and multi-article reading in the Vietnamese legal domain?**
  - Basis: Performance declines on reasoning-intensive tasks like semantic interpretation and logical inference
  - Evidence needed: +15% ROUGE improvements specifically on Logical Inference and Multi-Article Reading subsets

- **How can the precision of legal article retrieval be substantially improved beyond the current baseline of less than 40%?**
  - Basis: All models show precision scores smaller than 0.40, indicating high noise levels
  - Evidence needed: Retrieval system achieving Precision@2 greater than 0.70 without significant recall loss

## Limitations

- Dataset imbalance: Consumer protection and traffic law questions comprise 43% of dataset, while specialized areas like intellectual property represent only 6%
- Optimal retrieval depth unexplored: Fixed top-2 retrieval may be insufficient for complex questions requiring broader statutory context
- Evaluation methodology gaps: Human evaluation lacks statistical significance and doesn't assess deeper legal reasoning capabilities

## Confidence

- **High Confidence (90%+):** Dense retrieval performance results (BGE-m3 R@2=54.4% vs BM25 R@2=38.1%) are well-supported and align with IR principles
- **Medium Confidence (60-89%):** QA model performance comparisons are moderately reliable but subject to evaluation uncertainties and lack statistical rigor
- **Low Confidence (below 60%):** Claims about dataset representativeness and sufficiency for training robust legal models require further validation

## Next Checks

1. **Temporal Robustness Test:** Evaluate VLQA-trained models on held-out questions from different time periods to assess performance degradation due to evolving legal frameworks

2. **Retrieval Depth Sensitivity Analysis:** Systematically vary retrieved articles (k=1,3,5,10) in RAG pipeline to determine optimal retrieval depth balancing context relevance and efficiency

3. **Cross-Lingual Transfer Evaluation:** Fine-tune VLQA models on English legal QA datasets and evaluate zero-shot transfer back to Vietnamese tasks to assess cross-lingual generalization capabilities