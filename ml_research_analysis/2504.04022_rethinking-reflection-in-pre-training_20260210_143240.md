---
ver: rpa2
title: Rethinking Reflection in Pre-Training
arxiv_id: '2504.04022'
source_url: https://arxiv.org/abs/2504.04022
tags:
- reflection
- adversarial
- answer
- explicit
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether reflection capabilities in large
  language models emerge during pre-training rather than only through reinforcement
  learning. The authors introduce a systematic method to generate adversarial datasets
  that require models to detect and correct errors in reasoning chains.
---

# Rethinking Reflection in Pre-Training

## Quick Facts
- arXiv ID: 2504.04022
- Source URL: https://arxiv.org/abs/2504.04022
- Reference count: 40
- Primary result: Reflection capabilities emerge during pre-training and strengthen with compute, challenging the assumption that such abilities require post-training reinforcement learning

## Executive Summary
This paper challenges the prevailing assumption that reflection capabilities in large language models emerge primarily through reinforcement learning by demonstrating that such abilities appear during pre-training. The authors develop a systematic method to generate adversarial datasets requiring models to detect and correct errors in reasoning chains, then evaluate partially pre-trained OLMo-2 checkpoints across six diverse tasks. They find that reflection appears early in training and improves steadily with increased compute, with models consistently recovering from introduced errors using simple triggers like "Wait,". The results suggest that key aspects of reasoning can be developed during pre-training alone, potentially reducing reliance on expensive post-training techniques.

## Method Summary
The authors evaluate reflection capabilities in pre-trained models by testing their ability to detect and correct errors in adversarial chains-of-thought across two settings: situational-reflection (errors from external source) and self-reflection (model's own errors). They generate adversarial CoTs by corrupting correct reasoning using seven perturbation types for math problems, then append a "Wait," trigger to each instance. Models are evaluated across 40 OLMo-2 checkpoints and Qwen2.5 variants using accuracy, explicit reflection rate, and correlation with log(pre-training compute). Explicit reflection is classified using a DeepSeek-V3 prompt-based classifier with 2-4 exemplars.

## Key Results
- Reflection accuracy correlates strongly with pre-training compute (Pearson r ≈ 0.76-0.88 across tasks)
- Simple triggers like "Wait," consistently improve both accuracy and explicit reflection rates
- Most recoveries are attributed to explicit situational-reflection rather than implicit correction
- Reflection capabilities emerge early in pre-training and strengthen steadily over time

## Why This Works (Mechanism)

### Mechanism 1: Compute-Scaled Reflection Emergence
- Claim: Reflection capabilities scale with pre-training compute, not just post-training RL
- Core assumption: Correlation between compute and reflection accuracy reflects genuine capability emergence
- Evidence: "self-correcting ability appears early and improves steadily over time" with average Pearson correlation of 0.76 across tasks

### Mechanism 2: Trigger-Mediated Explicit Reflection
- Claim: Simple verbal triggers ("Wait,") causally increase explicit reflection and accuracy
- Core assumption: Trigger elicits latent reflective capacity rather than adding new capability
- Evidence: "Wait," trigger emulates a linear combination of no trigger and a trigger emphasizing mistakes in prior reasoning

### Mechanism 3: Explicit vs Implicit Reflection Distinction
- Claim: Pre-training increases explicit reflection rate, shifting from implicit to explicit correction strategies
- Core assumption: Explicit reflection phrases indicate genuine metacognitive behavior
- Evidence: "most recoveries should be attributed to explicit situational-reflection" with low correlation of implicit reflection accuracy with compute

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**:
  - Why needed here: Methodology depends on programmatically corrupting CoTs to create adversarial examples
  - Quick check question: Can you explain how a correct CoT differs from an adversarial CoT in this paper's methodology?

- **Pre-training Checkpoint Analysis**:
  - Why needed here: Core contribution requires understanding how to evaluate partially-trained models across compute scales
  - Quick check question: What is the relationship between "stage1-step9000-tokens38B" and the compute formula 6nt?

- **Reflection Classification (Explicit/Implicit)**:
  - Why needed here: Paper's metrics depend on correctly categorizing model outputs
  - Quick check question: If a model outputs a correct answer after "Wait," but never verbally acknowledges the error, which category applies?

## Architecture Onboarding

- **Component map**: Base reasoning datasets → Adversarial CoT generator → Checkpoint evaluator → Explicit reflection classifier (DeepSeek-V3)
- **Critical path**: Generate adversarial CoTs → Validate incorrect answers → Append "Wait," trigger → Run checkpoints → Classify reflection → Compute correlation with compute
- **Design tradeoffs**: High precision (0.95-1.0) but lower recall (0.61-0.76) for reflection classifier; smaller self-reflection datasets for cleaner comparison; minimal trigger choice
- **Failure signatures**: Perplexity analysis shows "local consistency bias" with lower perplexity to incorrect answers; TriviaQA shows lower reflection correlation due to knowledge lookup solutions
- **First 3 experiments**:
  1. Reproduce situational-reflection on GSM8K-Platinum: Plot accuracy vs log(compute) for 3 checkpoints, expect r ≈ 0.84
  2. Ablate the trigger: Test no trigger, "Wait, I made a mistake," and "Wait," to verify interpolation
  3. Test generalization: Create adversarial CoTs for physics problems to test reflection transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the "germs" of self-reflection that emerge during pre-training evolve into sophisticated autonomous reasoning abilities during post-training?
- Basis: "How these germs of self-reflection evolve into sophisticated autonomous reasoning abilities with post-training is an open question"
- Why unresolved: Study focuses only on pre-training checkpoints, not SFT or RL effects
- Resolution: Longitudinal study tracking checkpoints through RL/SFT pipelines

### Open Question 2
- Question: Is there a critical threshold of pre-trained self-reflection capability beyond which a model is highly likely to develop into a successful test-time reasoner?
- Basis: "We hypothesize that there must be a critical threshold of pre-trained self-reflection"
- Why unresolved: No specific inflection point defined or verified
- Resolution: Identify non-linear relationship with discontinuous jump in downstream reasoning

### Open Question 3
- Question: What specific data distributions within organic web datasets are responsible for promoting explicit self-reflection during pre-training?
- Basis: Surprise that reflection emerges from organic data; "Pinpointing data distributions that promote explicit self-reflection"
- Why unresolved: No isolation of specific training corpus components driving reflection
- Resolution: Data ablation study showing causal links between training domains and reflection

### Open Question 4
- Question: Do reflection capabilities elicited by synthetic adversarial chains-of-thought generalize to natural, un-prompted error correction?
- Basis: Methodology relies on algorithmically introduced errors and specific triggers; difficulty studying reflection in natural settings
- Why unresolved: Evaluation measures reflection in controlled, adversarial setting
- Resolution: Evaluate performance on naturally occurring reasoning errors without artificial triggers

## Limitations

- Trigger specificity: Minimal trigger used without systematic testing of alternatives; assumes simple linear mechanism
- Classifier reliability: High precision but lower recall creates systematic underestimation of reflection capabilities
- Adversarial dataset quality: No quantitative measures of perturbation difficulty or distribution

## Confidence

- **High confidence**: Reflection capabilities emerge during pre-training and improve with compute (r ≈ 0.76-0.88 across tasks)
- **Medium confidence**: Trigger ("Wait,") causally increases explicit reflection through simple interpolation mechanism
- **Medium confidence**: Pre-training shifts models from implicit to explicit reflection strategies

## Next Checks

1. **Trigger ablation study**: Systematically test 5-10 alternative reflection triggers to determine if "Wait," is optimal or if capabilities are robust to prompt variation
2. **Classifier precision validation**: Manually annotate 100-200 reflection instances to independently verify classifier's precision and recall rates
3. **Out-of-distribution transfer**: Create adversarial CoTs for domains not present in original datasets to test whether reflection capabilities transfer beyond training distribution