---
ver: rpa2
title: 'LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard'
arxiv_id: '2504.13125'
source_url: https://arxiv.org/abs/2504.13125
tags:
- financial
- data
- tasks
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper fine-tuned large language models for financial tasks
  using supervised fine-tuning, direct preference optimization, and reinforcement
  learning. The models were trained on 28 of 41 available datasets from the Open FinLLM
  Leaderboard, with synthetic data generation for tasks lacking training data.
---

# LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard

## Quick Facts
- arXiv ID: 2504.13125
- Source URL: https://arxiv.org/abs/2504.13125
- Reference count: 20
- Key outcome: Fine-tuned small language models for financial tasks using SFT, DPO, and RL, achieving significant performance gains and measuring a data scaling law exponent of 0.28.

## Executive Summary
This paper presents a comprehensive fine-tuning pipeline for financial language models, leveraging the Open FinLLM Leaderboard as a benchmark. The authors fine-tuned small (1.5B parameter) models on 28 of 41 available financial datasets using supervised fine-tuning, direct preference optimization, and reinforcement learning with synthetic data. Their approach achieved substantial performance improvements across diverse financial tasks including NER, QA, risk management, and forecasting, while also validating that financial data follows similar scaling laws to general text domains.

## Method Summary
The authors fine-tuned Qwen2.5-1.5B-Instruct and DeepSeek-R1-1.5B models on 28 of 41 available FinLLM datasets using a pipeline of SFT → DPO → RL. SFT used LoRA rank 128, DPO used rank 16 with negative samples from SFT overlong outputs, and RL employed synthetic data generation via CoT prompting and regex extraction. Training used LLaMAFactory with DeepSpeed stage 2 on 4x2080Ti GPUs, with specific hyperparameters for each phase and task-specific evaluation using benchmark F1 scores.

## Key Results
- DPO reduced the "repeater phenomenon" (repetitive outputs) from 54.7% to 1.7% while maintaining or improving task performance
- DeepSeek-R1-1.5B showed better generalization than Qwen2.5-1.5B-Instruct across benchmark tasks
- The data scaling law exponent was measured at 0.28, consistent with general domain scaling laws
- Synthetic data generation via CoT reasoning provided an 87.1% boost on MultiFin tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Preference Optimization (DPO) mitigates the "repeater phenomenon" (excessive output length) introduced by Supervised Fine-Tuning (SFT) while maintaining task performance.
- Mechanism: SFT maximizes token likelihood, potentially leading to degenerate repetition loops. DPO contrasts chosen vs. rejected responses, penalizing the inability to stop generation by treating overly long SFT outputs as rejected labels.
- Core assumption: The "repeater phenomenon" correlates with uncertainty about stopping conditions, correctable via relative preference learning.
- Evidence anchors: Table II shows Overlength Ratio dropping from 54.7% to 1.7%; methodology section defines DPO loss function L_DPO.
- Break condition: If rejected labels aren't representative of the specific failure mode (e.g., semantic errors vs. length errors), DPO may optimize for length over correctness.

### Mechanism 2
- Claim: Synthetic data generation via Chain-of-Thought (CoT) reasoning can substitute for missing training labels in financial tasks, enabling iterative self-improvement.
- Mechanism: Base LLM generates annotations using CoT prompting based on raw corpus text, creating query-answer pairs for SFT. This converts few-shot capabilities into fine-tuning signals.
- Core assumption: Base model possesses sufficient latent financial knowledge to generate high-quality CoT annotations, and regex extraction reliably isolates correct answers.
- Evidence anchors: Table III shows +87.1% boost on MultiFin; methodology describes data synthesis procedure steps 1-5.
- Break condition: If base model hallucinates during CoT generation, synthetic data noise may overwhelm signal, leading to model collapse.

### Mechanism 3
- Claim: Data scaling law in financial domains exhibits a critical exponent (~0.28) consistent with general domains, suggesting predictable performance returns on data volume.
- Mechanism: Paper empirically measures F1 score relationship with data fraction, fitting power law F1 ~ d^0.28, aligning with theoretical expectations from general text.
- Core assumption: Aggregation of diverse financial tasks forms coherent enough distribution for single scaling exponent to emerge.
- Evidence anchors: Figure 2 and Eq 1 show scaling law measurement; abstract mentions measuring data scaling law in financial domain.
- Break condition: If specific sub-tasks (e.g., complex forecasting) scale with significantly different exponents than simple classification, aggregated "universality" claim may not hold for niche applications.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Used not just for "helpfulness" but specifically to cure models of generating infinite loops of correct answers after SFT.
  - Quick check question: Can you explain why DPO is described here as a post-SFT "stabilizer" rather than a primary training method?

- Concept: **Power Law Scaling**
  - Why needed here: Validates that financial data follows similar scaling laws to general text (L ~ d^-0.095), critical for budgeting data collection efforts.
  - Quick check question: If data scaling exponent is ~0.28, does doubling dataset size double performance (F1), or provide diminishing return?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: Authors use LoRA (Rank 128 for SFT, 16 for DPO) to train on limited GPU resources (4x2080Ti), making architecture choices about rank critical.
  - Quick check question: Why might authors reduce LoRA rank (from 128 to 16) when moving from SFT to DPO?

## Architecture Onboarding

- Component map: Base Models (Qwen2.5-1.5B-Instruct, DeepSeek-R1-1.5B) -> Training Framework (LLaMAFactory + DeepSpeed Stage 2) -> Adapters (LoRA Rank 128/Alpha 256 for SFT; Rank 16/Alpha 32 for DPO) -> Data Engine (Synthetic pipeline: Corpus -> CoT Annotation -> Regex Extraction)

- Critical path: 1. SFT Phase: Train on 28/41 available datasets (using LoRA rank 128) 2. Negative Mining: Capture "overly long" outputs from SFT model to serve as rejected samples (y-) 3. DPO Phase: Train using pairs (Database Correct vs. SFT Overly Long) with reduced LoRA rank (16) to sharpen stopping criteria

- Design tradeoffs:
  - Generalization vs. Specialization: Training on NER hurt Causal Classification (CC) performance (Table I), suggesting distinct "semantic" vs. "entity" pathways that interfere
  - Seq-Tuning vs. Mixed: Sequential fine-tuning (Task A then Task B) did not outperform mixed training, suggesting simple data mixing is sufficient
  - Model Selection: DeepSeek-R1-1.5B generalized better than Qwen2.5-1.5B-Instruct, despite both being small models

- Failure signatures:
  - The "Repeater" Bug: Model outputs "Apple Apple Apple..." (Table II discussion). This indicates SFT has memorized token likelihood without learning stop token. Fix: Apply DPO
  - Negative Transfer: Performance on Causal Classification (CC) drops when trained on Named Entity Recognition (NER) (Table I)
  - CoT Leakage: Paper notes DeepSeek models lost points by outputting reasoning chains in tasks requiring direct answers

- First 3 experiments:
  1. SFT Sanity Check: Fine-tune Qwen2.5-1.5B on single task (FiQASA) to verify F1 jump (e.g., 0.67 -> 0.80 per Table I) and observe "repeater" rate
  2. DPO Repair: Run DPO step using SFT model's repetitive outputs as negative examples. Verify "Overlength Ratio" drops below 10% without tanking F1 score
  3. Scaling Validation: Train on 25% vs 50% of aggregated data to locally verify 0.28 exponent scaling behavior before committing to full 100% run

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model performance saturate or degrade after multiple iterations of reinforcement learning training with synthetic data?
- Basis in paper: [explicit] Authors state "Running additional iterations of RL training to examine whether performance saturates" was left unexplored due to time constraints
- Why unresolved: Study only completed one iteration of RL data synthesis loop; long-term dynamics of training on self-generated financial data remain unknown
- What evidence would resolve it: Benchmark results and loss metrics tracked across 5 to 10 iterations of RL synthesis loop on specific financial tasks

### Open Question 2
- Question: Do observed data scaling laws and DPO benefits generalize to financial models with significantly larger parameter counts (e.g., 7B or 70B)?
- Basis in paper: [explicit] Authors list "Testing larger models" as primary prospect for future work, as experiments confined to 1.5B parameter versions
- Why unresolved: Unclear if computational efficiency and scaling exponent (0.28) found in small models apply to larger, more complex architectures
- What evidence would resolve it: Comparison of performance scaling curves between 1.5B and 7B+ models trained on identical fractions of financial dataset

### Open Question 3
- Question: Can negative transfer observed between specific financial tasks (e.g., NER impairing Causal Classification) be mitigated through advanced multi-task learning strategies?
- Basis in paper: [inferred] Paper notes training on NER significantly reduced performance on CC, hypothesizing model overemphasized entities at expense of semantic relationships
- Why unresolved: Results show task interactions "do not follow a general pattern," and simple sequential fine-tuning failed to resolve interference
- What evidence would resolve it: Ablation studies utilizing task-specific adapters or weighted loss functions to minimize performance degradation on conflicting tasks

## Limitations

- Dataset Quality and Coverage: Study relies on 28 of 41 available FinLLM datasets with synthetic data generation for missing tasks; quality and representativeness of synthetic annotations uncertain
- Task-Specific Performance Claims: Strong aggregate metrics reported, but individual task performance varies significantly; generalization claims require careful interpretation given small model size and synthetic data components
- Mechanism Specificity: Attribution of performance improvements to specific mechanisms (DPO for stopping behavior, CoT for data synthesis) plausible but not definitively isolated without ablation studies

## Confidence

**High Confidence**:
- Data scaling law exponent of 0.28 is empirically measured with appropriate methodology
- Basic pipeline of SFT→DPO→RL is implemented as described
- Overlength Ratio reduction from 54.7% to 1.7% via DPO is clearly demonstrated

**Medium Confidence**:
- DPO specifically addresses "repeater phenomenon" is well-supported by data, though exact mechanism could involve other factors
- Generalization advantage of DeepSeek-R1-1.5B over Qwen2.5-1.5B-Instruct is demonstrated but may depend on task distribution

**Low Confidence**:
- Universality claim for 0.28 scaling exponent across domains is stated but not rigorously tested beyond this dataset
- Assertion that synthetic CoT data quality is sufficient for effective training is plausible but unverified against human-annotated data

## Next Checks

1. **Ablation Study on DPO vs. Inference-Time Controls**: Run SFT model with standard inference-time repetition penalties (top-p, temperature, stop tokens) versus DPO approach to isolate whether performance gain is truly from preference learning or simply better stopping heuristics

2. **Human Evaluation of Synthetic Data Quality**: Sample synthetic CoT annotations and have financial domain experts evaluate correctness, completeness, and reasoning quality versus human-annotated examples from same task types

3. **Scaling Law Validation on Individual Tasks**: Measure data scaling exponent separately for different task categories (NER, QA, forecasting) to verify whether 0.28 universality claim holds within task subtypes or if it's artifact of aggregation