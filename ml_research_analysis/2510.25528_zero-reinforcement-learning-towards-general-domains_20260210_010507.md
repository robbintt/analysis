---
ver: rpa2
title: Zero Reinforcement Learning Towards General Domains
arxiv_id: '2510.25528'
source_url: https://arxiv.org/abs/2510.25528
tags:
- reasoning
- length
- general
- training
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a unified zero reinforcement learning framework
  to improve reasoning capabilities across both verifiable and non-verifiable domains.
  The core method combines verifiable rewards for reasoning tasks with generative
  reward models for general tasks, along with a smooth length penalty to prevent verbose
  responses and stabilize training.
---

# Zero Reinforcement Learning Towards General Domains

## Quick Facts
- arXiv ID: 2510.25528
- Source URL: https://arxiv.org/abs/2510.25528
- Reference count: 24
- Primary result: Unified zero-RL framework improves reasoning across verifiable and non-verifiable domains, achieving competitive results with much larger models

## Executive Summary
This paper introduces a unified zero reinforcement learning framework that extends beyond traditional verifiable-domain applications to enable reasoning capabilities across both verifiable and non-verifiable tasks. The core innovation combines verifiable rewards for reasoning tasks with generative reward models for general tasks, augmented by a smooth length penalty to prevent verbose responses and stabilize training. Multi-task training transfers reasoning behaviors from reasoning-heavy to general domains. Experiments on Qwen3-8B-Base and Qwen3-14B-Base demonstrate superior performance on math reasoning, general reasoning, and general tasks compared to other zero-RL models.

## Method Summary
The method employs modified GRPO (token-level loss, no KL) on Qwen3-8B/14B-Base via veRL, using a blended dataset of 178,535 math prompts, 125,798 filtered STEM samples, and 36,125 general prompts. Training features gradual context window expansion (2,048→24,576 tokens), 16 rollouts per prompt, and task-type-conditional rewards: binary ±1 for verifiable tasks and generative RM outputs [-5,5] for non-verifiable tasks. A smooth length penalty encourages substantive reasoning by penalizing excessive answer-to-thinking length ratios. The system enforces `<thinking></thinking><answer></answer>` formatting and uses group normalization for advantage computation.

## Key Results
- Outperforms other zero-RL models on math reasoning benchmarks (MATH-500, AIME24/25)
- Achieves competitive performance on general reasoning tasks (MMLU-Pro, GPQA-Diamond)
- Demonstrates improvements up to 32.2% on AIME24 while maintaining general task performance
- Matches results of much larger models like DeepSeek-R1-Zero-Qwen-32B

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Transfer via Multi-Task Training
The framework leverages binary verifiable rewards from math/STEM tasks to reliably elicit reasoning behaviors ("Aha Moment") that transfer to non-verifiable domains through shared parameter updates during concurrent training. The core assumption is that reasoning capabilities developed under verifiable reward signals will generalize to open-ended domains.

### Mechanism 2: Smooth Length Penalty Counteracts Generative Reward Model Bias
Generative reward models exhibit verbosity bias, favoring longer responses. The smooth length penalty prevents models from exploiting this bias by imposing graduated penalties when answer length significantly exceeds thinking length, forcing coordinated growth in both.

### Mechanism 3: Hybrid Reward Architecture with Task-Conditional Computation
The unified policy optimizes for both binary verifiable rewards and continuous generative rewards through task-type-conditional computation, routing each sample to the appropriate reward function without explicit domain adapters.

## Foundational Learning

- **Concept: Zero Reinforcement Learning (Zero-RL)**
  - Why needed here: Understanding this baseline clarifies what constraints the paper addresses
  - Quick check question: What distinguishes Zero-RL from standard RLHF pipelines that include supervised fine-tuning before reinforcement learning?

- **Concept: Reward Hacking in Model-Based Reward Functions**
  - Why needed here: The paper's length penalty specifically targets reward hacking where models exploit biases in generative reward models
  - Quick check question: When using a language model as a reward function, why might the policy learn to generate longer outputs rather than better outputs?

- **Concept: Token-Level vs Sequence-Level Policy Gradient**
  - Why needed here: The paper modifies GRPO to use token-level loss rather than sequence-level loss
  - Quick check question: How does token-level policy gradient differ from sequence-level in distributing reward signal across a 10,000-token reasoning trace?

## Architecture Onboarding

- **Component map:**
  Policy Model -> Generative Reward Model -> Verifier -> Length Penalty Module -> GRPO Optimizer -> Context Window Scheduler

- **Critical path:**
  1. Prepare blended dataset (178,535 math + 125,798 filtered STEM + 36,125 general)
  2. Initialize policy with system prompt enforcing `<thinking></thinking><answer></answer>` format
  3. Sample 16 rollouts per prompt with temperature=1.0, top-p=1.0
  4. Parse responses, extract thinking/answer content, compute format reward
  5. Route to appropriate reward (verifiable → binary, non-verifiable → generative RM + length penalty)
  6. Compute advantages via group normalization, update policy with clipped objective
  7. Expand context window gradually; monitor think/answer length ratio

- **Design tradeoffs:**
  - Format weight (α=0.5) vs stability: Higher α ensures structure compliance but may slow reasoning emergence
  - Length penalty weight (β=2) vs expressiveness: Higher β prevents verbosity more aggressively but risks constraining legitimate explanations
  - Gradual vs fixed context window: Gradual expansion prevents length spikes but extends training time

- **Failure signatures:**
  - Answer length grows unbounded while thinking length plateaus → reward hacking; increase β or audit generative RM calibration
  - Format reward remains near 0 → model ignoring template; increase α or add format-only warmup phase
  - AIME accuracy plateaus early with short responses → context window expansion too slow
  - Sharp accuracy drop mid-training → advantage explosion; check gradient norms, reduce learning rate

- **First 3 experiments:**
  1. Multi-task ablation: Train on reasoning-only vs. mixed data for 450 steps; compare on MATH-500, MMLU-Pro, Arena-Hard
  2. Length penalty validation: Train with/without length penalty; plot think/answer length trajectories
  3. Scaling behavior: Train 8B and 14B variants; monitor AIME24 accuracy and response length over training steps

## Open Questions the Paper Calls Out

- Can the unified zero-RL framework effectively integrate programming tasks that require code execution sandboxes? The paper excluded programming tasks due to complexity of reward signals requiring code sandboxes.
- Is it possible to elicit the "Aha Moment" when training solely on general, non-verifiable data? The paper notes this rarely appeared without multi-task training.
- Can zero-RL training on base models scale to produce chain-of-thought capabilities that match models distilled from significantly larger teachers? The paper notes CoT is "not yet sufficient to match the CoT produced by larger models."

## Limitations

- Framework's core assumption about cross-domain reasoning transfer remains theoretically underspecified
- Reliance on proprietary in-house math dataset limits reproducibility
- Smooth length penalty introduces hyperparameter sensitivity that may not transfer across model scales
- Programming tasks require code execution sandboxes, which the framework currently cannot handle

## Confidence

- **High Confidence**: Claims about improved benchmark performance relative to baseline zero-RL approaches
- **Medium Confidence**: Claims about effectiveness of multi-task training for reasoning transfer and smooth length penalty for preventing reward hacking
- **Low Confidence**: Claims about theoretical basis for cross-domain reasoning transfer and generalizability of specific hyperparameter choices

## Next Checks

1. **Ablation Study on Pre-training Corpus Composition**: Train variants on reasoning-heavy vs. general pre-training corpora, then apply zero-RL. Compare "Aha Moment" emergence frequency and reasoning transfer effectiveness.

2. **Reward Model Architecture Sensitivity**: Replace generative reward model with alternative architectures while maintaining length penalty. Test whether performance gains persist across different reward model scales.

3. **Task-Specific Length Penalty Calibration**: Conduct grid search over length penalty hyperparameters for different task categories. Analyze whether optimal penalty values correlate with task complexity or response length distributions.