---
ver: rpa2
title: Compound Expression Recognition via Large Vision-Language Models
arxiv_id: '2503.11241'
source_url: https://arxiv.org/abs/2503.11241
tags:
- compound
- facial
- recognition
- expressions
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a compound expression recognition (CER) framework
  using Large Vision-Language Models (LVLMs) with a stage-wise LoRA fine-tuning approach.
  The model is first fine-tuned on basic emotion datasets to learn foundational facial
  expression patterns, then further optimized on compound expression data to capture
  complex emotion combinations.
---

# Compound Expression Recognition via Large Vision-Language Models

## Quick Facts
- arXiv ID: 2503.11241
- Source URL: https://arxiv.org/abs/2503.11241
- Reference count: 30
- Key result: Stage-wise LoRA fine-tuning improves compound expression recognition accuracy from 74.39% to 78.50%

## Executive Summary
This paper presents a compound expression recognition (CER) framework using Large Vision-Language Models (LVLMs) with a stage-wise LoRA fine-tuning approach. The model is first fine-tuned on basic emotion datasets to learn foundational facial expression patterns, then further optimized on compound expression data to capture complex emotion combinations. Context-aware prompts guide the model in analyzing facial features for compound emotions. Experiments on RAF-DB and C-EXPR-DB datasets show strong performance in both aligned and original images.

## Method Summary
The approach uses a two-stage LoRA fine-tuning strategy on a pre-trained LVLM (Qwen-vl). Stage 1 applies rank=16 LoRA on basic emotion datasets (RAF-DB Basic + Aff-Wild2) for 20 epochs. Stage 2 reduces to rank=8 LoRA and trains on compound expression data (RAF-DB Compound + Aff-Wild2 compound) for 10 epochs. Context prompts are designed to guide attention to key facial features. The model is evaluated on both aligned and original images, with inference using structured text output prompts.

## Key Results
- Overall compound expression accuracy: 78.50% (aligned) vs 74.39% (original)
- Basic emotion accuracy: 89.78% (aligned) vs 87.84% (original)
- Significant performance gap between aligned and original images (6-9% drop)
- Best compound performance: "Angrily Disgusted" at 94.10%
- Worst compound performance: "Sadly Surprised" at 43.00% (original)

## Why This Works (Mechanism)

### Mechanism 1: Stage-wise LoRA fine-tuning enables incremental learning
- **Claim:** Two-stage training (basic → compound) is more effective than direct fine-tuning on compound expressions
- **Mechanism:** Curriculum-style learning where basic emotion representations serve as compositional building blocks for compound emotions
- **Core assumption:** Basic emotions are compositional primitives for compound expressions
- **Evidence:** Accuracy improvement from 74.39% to 78.50% when using staged approach

### Mechanism 2: Context prompts guide LVLM attention
- **Claim:** Structured prompts improve recognition accuracy by focusing attention on task-relevant features
- **Mechanism:** Semantic guidance through category definitions, analysis guidelines, and output format constraints
- **Core assumption:** LVLMs can dynamically adjust visual attention based on linguistic context
- **Evidence:** Performance gains with detailed prompts versus minimal guidance

### Mechanism 3: LoRA prevents overfitting while preserving knowledge
- **Claim:** Low-rank adaptation reduces overfitting risk on limited CER data
- **Mechanism:** Parameter reduction from d² to 2dr constrains solution space while preserving pre-trained representations
- **Core assumption:** Task-specific updates live in a low-dimensional subspace
- **Evidence:** Successful fine-tuning with minimal parameter changes

## Foundational Learning

- **Low-Rank Adaptation (LoRA):** Parameter-efficient fine-tuning that decomposes weight updates as ΔW ≈ BA, reducing trainable parameters from d² to 2dr. Needed to understand how the model adapts to new tasks while preserving pre-trained knowledge.
- **Large Vision-Language Models (LVLMs):** Pre-trained multimodal models that encode visual-semantic relationships. Needed to understand how image-text pairs are processed differently from traditional CNN approaches.
- **Compound vs. Basic Emotions:** Understanding that compound expressions (e.g., "happily surprised") combine basic emotions. Needed to grasp why two-stage training is effective for this task.

## Architecture Onboarding

- **Component map:** Qwen-vl (frozen) -> LoRA adapters (rank 16→8) -> Prompt encoder -> Training pipeline -> Inference system
- **Critical path:** Initialize Qwen-vl → Insert LoRA layers (rank=16) → Train Stage 1 on basic emotions → Adjust rank to 8 → Train Stage 2 on compound emotions → Evaluate on C-EXPR-DB
- **Design tradeoffs:** Rank selection (capacity vs. regularization), aligned vs. original images (accuracy vs. robustness), prompt specificity (guidance vs. bias)
- **Failure signatures:** Low accuracy on "Angrily Surprised" (50.20%) and "Sadly Surprised" (52.40%), accuracy drops on original images, potential gradient instability with batch size=1
- **First 3 experiments:** 1) Single-stage LoRA baseline to isolate curriculum benefit, 2) Minimal vs. detailed prompt comparison, 3) Rank sensitivity sweep (r ∈ {4, 8, 16, 32})

## Open Questions the Paper Calls Out

1. **Domain adaptation for aligned vs. original images:** How to bridge the 6-9% accuracy gap between aligned and original facial images through domain adaptation or advanced data augmentation.
2. **Visual feature analysis for performance disparities:** Understanding why certain compound expressions (e.g., "Angrily Disgusted" at 94.10%) perform well while others (e.g., "Sadly Surprised" at 43.00%) fail significantly.
3. **Stage-wise training contribution:** Quantifying how much the two-stage curriculum approach contributes to performance versus single-stage fine-tuning or other adapter methods.

## Limitations

- No ablation study comparing stage-wise approach to single-stage fine-tuning
- Performance drops significantly (6-9%) on original "in-the-wild" images versus aligned images
- Significant variance in compound expression performance (94.10% to 43.00%)
- Unknown optimizer configuration and specific LoRA layer targets

## Confidence

- **High confidence:** Stage-wise curriculum learning improves compound expression recognition (accuracy improvement from 74.39% to 78.50%)
- **Medium confidence:** Context prompts guide LVLM attention to facial features (performance gains but weak direct evidence)
- **Low confidence:** LoRA parameter reduction prevents overfitting on limited CER data (mechanism plausible but not LVLM-validated)

## Next Checks

1. **Curriculum benefit isolation:** Train a single-stage LoRA model directly on compound emotions and compare accuracy to the two-stage approach.
2. **Prompt contribution measurement:** Test model performance with minimal prompts versus detailed category definitions to quantify prompt impact.
3. **Rank sensitivity validation:** Sweep LoRA ranks (4, 8, 16, 32) during Stage 2 training to confirm rank=8 is optimal for compound expression learning.