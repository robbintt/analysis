---
ver: rpa2
title: Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task
  Multi-Scale Network
arxiv_id: '2510.18190'
source_url: https://arxiv.org/abs/2510.18190
tags:
- dynamic
- music
- beat
- dynamics
- piano
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multi-task multi-scale network that jointly
  estimates piano dynamics, change points, beats, and downbeats directly from audio.
  By leveraging Bark-scale specific loudness and a parameter-efficient MMoE decoder,
  the model achieves state-of-the-art performance for dynamics and change point detection
  on the MazurkaBL dataset, with F1 scores of 54.4% and 26.1%, respectively, while
  maintaining competitive results in beat and downbeat tracking.
---

# Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network

## Quick Facts
- arXiv ID: 2510.18190
- Source URL: https://arxiv.org/abs/2510.18190
- Reference count: 0
- Achieves F1 scores of 54.4% for dynamics and 26.1% for change point detection on MazurkaBL dataset using only 0.5 million parameters

## Executive Summary
This work introduces a multi-task multi-scale network that jointly estimates piano dynamics, change points, beats, and downbeats directly from audio. By leveraging Bark-scale specific loudness and a parameter-efficient MMoE decoder, the model achieves state-of-the-art performance for dynamics and change point detection on the MazurkaBL dataset, with F1 scores of 54.4% and 26.1%, respectively, while maintaining competitive results in beat and downbeat tracking. The approach uses only 0.5 million parameters, enabling efficient processing of long audio sequences. The proposed framework sets a new benchmark for resource-efficient musical expression analysis and offers practical utility for enriching piano scores with expressive markings.

## Method Summary
The model processes audio through Bark-scale specific loudness (BSSL) features extracted at 50 fps, then passes through a three-branch multi-scale CNN encoder operating at different temporal resolutions (T, T/5, T/25). An MMoE decoder with 8 expert networks and task-specific gating routes information to four output heads for dynamics (6-class), change points, beats, and downbeats. The architecture uses 0.5 million parameters total, achieved through BSSL's 22-band compression versus 128-band Mel spectrograms. Training uses 5-fold cross-validation on 60-second segments from the MazurkaBL dataset with AdamW optimization and weighted BCE/softmax losses.

## Key Results
- Dynamics F1: 54.4% (vs 50.6% single-task baseline)
- Change points F1: 26.1% (vs 10.8% single-task baseline)
- Beats F1: 85.1% (comparable to existing methods)
- Downbeats F1: 55.2% (competitive with state-of-the-art)

## Why This Works (Mechanism)

### Mechanism 1: Psychoacoustically-aligned feature compression
BSSL features achieve comparable performance to log-Mel with 97% fewer parameters (0.5M vs 14.7M), enabling longer temporal context. BSSL uses 22 Bark bands aligned with human loudness perception critical bands, versus 128 Mel bins. The smaller input dimension propagates through convolutional layers, reducing total parameters proportionally. This compression allows 60-second sequences to fit in GPU memory.

### Mechanism 2: Multi-scale temporal decomposition
Processing at three temporal resolutions (T, T/s, T/s²) simultaneously enables both slow-changing dynamics and fast beat detection. Three parallel encoder branches with configurable scaling factor s=5. Longer branches capture gradual dynamic transitions (crescendo) via large receptive fields; shorter branches maintain temporal precision for onset detection. All outputs are upsampled to original resolution before fusion.

### Mechanism 3: Task-aware expert routing
MMoE gating improves all four tasks over shared representations (+3.8% dynamics, +5.1% change points, +10.2% downbeats F1). Eight lightweight expert networks process the shared latent sequence. Four task-specific gates compute softmax weights per time step, dynamically routing to relevant expert combinations. This allows the model to learn both shared and task-specific transformations without hard parameter sharing.

## Foundational Learning

- Concept: Bark-scale psychoacoustics
  - Why needed here: The input feature is grounded in human loudness perception models, not acoustic frequency scales. Understanding critical bands explains the compression advantage.
  - Quick check question: Why might 22 Bark bands preserve perceived loudness better than 128 Mel bins for dynamics, despite having 6× fewer dimensions?

- Concept: Multi-task negative transfer
  - Why needed here: The MMoE module explicitly addresses interference between tasks with different acoustic requirements (slow dynamics vs. fast beats).
  - Quick check question: What gradient conflicts might arise when jointly optimizing for gradual dynamic changes and sharp beat onsets?

- Concept: Temporal receptive fields in CNNs
  - Why needed here: The multi-scale architecture relies on downsampling to achieve different effective receptive fields matching musical time scales.
  - Quick check question: If a crescendo spans 10 seconds at 50 fps, what minimum receptive field (in frames) captures it at scale T/s where s=5?

## Architecture Onboarding

- Component map: Audio -> BSSL extraction (22 Bark bands) -> 3-branch multi-scale encoder -> MMoE decoder (8 experts + 4 gates) -> 4 task heads -> post-processing

- Critical path: Audio → BSSL (Section 2.1) → batch norm → 3-branch encoder (Fig. 2) → concat → MMoE (Eq. 1) → task heads → post-processing (Section 2.3: threshold + peak-picking + beat-snapping)

- Design tradeoffs:
  - BSSL vs log-Mel: 29× parameter reduction but slightly worse downbeat F1 (55.2% vs 58.5%)
  - 60s vs 30s context: +5.3% dynamics F1 but 2× memory
  - s=5 vs s=3: Paper empirically chose s=5; larger s may lose beat precision

- Failure signatures:
  - Dynamics F1 < 45%: Check BSSL extraction (peak normalization to -1dBFS, 22.05 kHz resampling)
  - Beat F1 < 75%: Verify 50 fps frame rate and 70ms tolerance window
  - Change point F1 ≈ 0%: Post-processing threshold (75%) or beat-snapping step may be misconfigured
  - High fold variance (>10% std): Potential data leakage or stratification issue

- First 3 experiments:
  1. Reproduce single-task dynamics baseline with log-Mel input (target: ~50.4% F1 per Table 1). Validates data pipeline.
  2. Sweep scaling factor s ∈ {3, 5, 7, 10} and plot per-task F1 curves. Confirms s=5 is not dataset-specific.
  3. Visualize gate weight distributions across tasks on held-out audio. Checks for gate collapse or task dominance patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What evaluation methodologies are appropriate for assessing an end-to-end pipeline that combines this dynamics estimator with score-level piano transcription?
- Basis in paper: [explicit] The conclusion states that while future work will combine the model with transcription systems, "developing appropriate evaluation methods for such comprehensive outputs presents a new challenge."
- Why unresolved: Existing metrics are task-specific (e.g., F1 for dynamics vs. frame accuracy for notes), and no standardized method currently exists to evaluate the holistic accuracy of a generated score containing both musical content and expressive markings.
- What evidence would resolve it: The proposal and validation of a new metric or benchmark suite that correlates with human judgment of the transcribed score's expressive fidelity.

### Open Question 2
- Question: Can the model's performance on dynamics estimation generalize to other instruments or musical styles beyond the Chopin Mazurkas?
- Basis in paper: [inferred] The paper relies exclusively on the MazurkaBL dataset (Chopin) for training and evaluation, noting that dynamics interpretation is deeply contextual and influenced by musical style.
- Why unresolved: The model may have learned style-specific loudness mappings unique to Chopin or the specific recording qualities of the MazurkaBL corpus, leaving its robustness on other pianists or instruments unproven.
- What evidence would resolve it: Successful cross-dataset evaluation on diverse piano repertoires or adaptation to non-piano instruments without significant degradation in F1 scores.

### Open Question 3
- Question: How can the specific performance gap in downbeat tracking be closed while retaining the parameter efficiency of BSSL?
- Basis in paper: [inferred] The results section and Table 1 reveal a task-feature dependency where log-Mel features outperform BSSL for downbeat tracking (58.5% vs. 55.2%), whereas BSSL is superior for the other three tasks.
- Why unresolved: It is unclear if the MMoE decoder can fully compensate for the information loss regarding downbeat cues in the BSSL representation compared to the richer log-Mel spectrogram.
- What evidence would resolve it: An architectural modification or feature fusion technique that matches the log-Mel downbeat performance while keeping the parameter count closer to 0.5 M rather than 14.7 M.

## Limitations

- Temporal resolution constraint: The 50 fps frame rate and 70ms beat tolerance may not capture finer metrical subdivisions in fast-tempo pieces.
- Dataset-specific generalization: Results validated exclusively on Chopin mazurkas may not transfer to other musical styles or instruments.
- Single-objective optimization: All tasks share the same BSSL input features, potentially limiting task-specific acoustic cue capture.

## Confidence

- High confidence: Parameter efficiency claims (0.5M vs 14.7M) are directly verifiable through architecture specification. Multi-task F1 improvements over single-task baselines are well-supported by ablation study.
- Medium confidence: MMoE "learns shared representations" assumes observed improvements stem from routing mechanism rather than training artifacts. 5-fold CV provides reasonable validation, though external datasets would strengthen claims.
- Low confidence: Psychoacoustic justification for BSSL compression assumes human loudness perception aligns with piano dynamics estimation requirements. This relationship lacks direct experimental validation.

## Next Checks

1. **Cross-dataset validation**: Evaluate the pre-trained model on an independent piano dataset (e.g., MAESTRO or MAPS) to test generalization beyond mazurkas. Target metrics: dynamics F1 > 40% and beat F1 > 70% on held-out classical piano works.

2. **Feature ablation with log-Mel**: Train the full architecture with log-Mel inputs while keeping the MMoE decoder and multi-scale encoder intact. Compare parameter counts and task performance to isolate the contribution of BSSL compression versus the overall architectural design.

3. **Temporal scale sensitivity analysis**: Systematically vary the scaling factor s ∈ {3, 5, 7, 10} and analyze per-task F1 curves. Identify optimal scaling per task and investigate whether a single shared scaling factor represents a meaningful constraint or arbitrary architectural choice.