---
ver: rpa2
title: Coding Agents with Multimodal Browsing are Generalist Problem Solvers
arxiv_id: '2506.03011'
source_url: https://arxiv.org/abs/2506.03011
tags:
- agent
- agents
- tasks
- openhands-versa
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces OpenHands-Versa, a generalist AI agent that\
  \ outperforms specialized agents across three diverse benchmarks: GAIA for general\
  \ assistance, SWE-Bench Multimodal for software engineering, and The Agent Company\
  \ for digital workplace tasks. The key innovation is a minimal set of general tools\u2014\
  code editing and execution, web search, multimodal web browsing, and file access\u2014\
  that enable high performance across varied domains."
---

# Coding Agents with Multimodal Browsing are Generalist Problem Solvers

## Quick Facts
- arXiv ID: 2506.03011
- Source URL: https://arxiv.org/abs/2506.03011
- Authors: Aditya Bharat Soni; Boxuan Li; Xingyao Wang; Valerie Chen; Graham Neubig
- Reference count: 32
- OpenHands-Versa achieves state-of-the-art results with 34.43% success rate on SWE-Bench Multimodal, 51.16% on GAIA, and 33.14% on The Agent Company

## Executive Summary
OpenHands-Versa introduces a generalist AI agent that outperforms specialized agents across three diverse benchmarks by using a minimal set of general tools: code editing and execution, web search, multimodal web browsing, and file access. The agent demonstrates state-of-the-art performance with absolute improvements of 9.1 points on SWE-Bench Multimodal, 1.3 points on GAIA, and 9.1 points on The Agent Company. This approach shows that generalist agents can adapt their tool usage to task requirements while maintaining strong performance across varied domains.

## Method Summary
The paper introduces OpenHands-Versa, a generalist AI agent that achieves state-of-the-art results across three diverse benchmarks (GAIA, SWE-Bench Multimodal, and The Agent Company) using a minimal set of general tools including code editing, web search, multimodal browsing, and file access. The agent demonstrates better domain-aware tool selection than its predecessor and adapts its tool usage based on task requirements. Experimental results show that existing multi-agent systems fail to generalize beyond their intended scope, while OpenHands-Versa provides strong performance across all tested domains.

## Key Results
- Achieves 34.43% success rate on SWE-Bench Multimodal (9.1 point improvement over state-of-the-art)
- Scores 51.16% on GAIA benchmark (1.3 point improvement)
- Completes 33.14% of tasks on The Agent Company benchmark (9.1 point improvement)
- Outperforms specialized agents on all three benchmarks using a minimal tool set

## Why This Works (Mechanism)
The generalist approach succeeds by maintaining a minimal but versatile tool set that can handle diverse task types. The agent's ability to adapt tool usage based on task requirements enables it to solve problems across different domains without needing domain-specific specializations. The multimodal browsing capability is particularly crucial for handling tasks that require understanding and interacting with complex web interfaces, while the code execution and file access tools provide the computational foundation for software engineering tasks.

## Foundational Learning
- **Multimodal web browsing**: Enables understanding and interaction with complex web interfaces, essential for general assistance tasks
- **Code execution and testing**: Critical for software engineering workflows and validation of solutions
- **File system navigation**: Required for accessing and manipulating project files across different domains
- **Web search integration**: Provides access to external knowledge and documentation needed for diverse problem-solving

## Architecture Onboarding
- **Component map**: Input Task -> Tool Selection -> Action Execution -> Result Processing -> Output Generation
- **Critical path**: Task understanding → Tool selection → Action execution → Result verification → Next action decision
- **Design tradeoffs**: Minimal tool set for generalization vs. specialized tools for domain-specific performance
- **Failure signatures**: Web access errors, test generation failures, complex file interaction issues
- **First experiments**: 1) Ablation study removing individual tools 2) Cross-domain transfer testing 3) Human expert comparison

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely on proprietary benchmarks without detailed construction or validation information
- Single evaluation protocol may not generalize to real-world deployment scenarios
- Minimal tool set may not capture edge cases or specialized requirements in production
- Error analysis lacks systematic quantification of failure modes across task types

## Confidence
- High confidence in comparative performance claims against specialized agents
- Medium confidence in generalizability of minimal tool set approach
- Low confidence in practical deployment readiness

## Next Checks
1. Conduct ablation studies removing individual tools to quantify their marginal contribution across all three benchmark domains
2. Perform cross-domain transfer experiments where agents trained on one benchmark are evaluated on the other two
3. Implement a controlled study comparing OpenHands-Versa against human experts on representative tasks from each domain