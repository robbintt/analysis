---
ver: rpa2
title: Resolving Latency and Inventory Risk in Market Making with Reinforcement Learning
arxiv_id: '2505.12465'
source_url: https://arxiv.org/abs/2505.12465
tags:
- market
- order
- price
- orders
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the latency and inventory risk issues in market
  making by developing a realistic MM environment with random delays and batch matching,
  then proposing Relaver, an RL-based method that augments the state-action space
  with order hold time, uses dynamic programming to guide exploration, and incorporates
  a market trend predictor for intelligent inventory control. Extensive experiments
  on four real-world datasets show that Relaver significantly outperforms state-of-the-art
  RL-based MM strategies in terms of profit, risk control, and adaptability.
---

# Resolving Latency and Inventory Risk in Market Making with Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.12465
- **Source URL:** https://arxiv.org/abs/2505.12465
- **Reference count:** 16
- **Primary result:** RL-based method (Relaver) outperforms state-of-the-art MM strategies on 4 datasets, achieving PnL-to-MAP ratios up to 1.97

## Executive Summary
This paper addresses latency and inventory risk in market making by developing a realistic MM environment with random delays and batch matching, then proposing Relaver, an RL-based method that augments the state-action space with order hold time, uses dynamic programming to guide exploration, and incorporates a market trend predictor for intelligent inventory control. Extensive experiments on four real-world datasets show that Relaver significantly outperforms state-of-the-art RL-based MM strategies in terms of profit, risk control, and adaptability. Specifically, it achieves higher expected profit and lower inventory risk, with PnL-to-MAP ratios up to 1.97, demonstrating both profitability and effective risk management.

## Method Summary
The Relaver method introduces an augmented state-action space that incorporates order hold time alongside price and volume, enabling optimization of execution strategies under latency constraints and time-priority matching mechanisms. It leverages dynamic programming (DP) to guide the exploration of RL training for better policies and trains a market trend predictor to intelligently adjust inventory levels and reduce risk. The approach uses PPO with LSTM for the policy network, integrates a Q-Teacher derived from offline DP computation, and employs a LightGBM trend classifier to dynamically set inventory limits. The environment simulates realistic market making conditions including random latency (30-100ms) and batch matching (500ms intervals).

## Key Results
- Relaver achieves higher expected profit and lower inventory risk than state-of-the-art RL-based MM strategies
- PnL-to-MAP ratios reach up to 1.97, demonstrating effective risk management alongside profitability
- Ablation studies confirm the effectiveness of each proposed component: Q-Teacher, trend prediction expert, and augmented action space all contribute to improved performance
- The method shows strong adaptability across four different market datasets (IC, IF, IH, IM index options)

## Why This Works (Mechanism)

### Mechanism 1: Augmented State-Action Space with Order Hold Time
Including order hold time in the action space enables policies that better account for execution uncertainty under latency. Standard RL-MM formulations output only price/volume, assuming immediate execution. By adding hold time, the agent can explicitly trade off queue priority against exposure to adverse price movements during the hold window. Orders with longer hold times have higher execution probability but greater market risk; shorter hold times reduce risk but lower fill rates.

### Mechanism 2: Dynamic Programming Q-Teacher for Guided Exploration
Offline DP-derived Q-values, computed with future information, provide supervision signals that accelerate RL convergence and improve final policy quality. The DP algorithm computes optimal action values across a discretized state-action space using backward induction with access to future price data. During RL training, the policy is regularized toward these "oracle" actions via a KL-divergence term, reducing exploration in clearly suboptimal regions while still allowing the RL policy to deviate where the DP approximation is poor.

### Mechanism 3: Trend Prediction Expert for Dynamic Inventory Control
A pretrained trend predictor enables dynamic inventory limits that reduce adverse selection and inventory risk. A LightGBM classifier predicts market trend over a 30-step (15-second) horizon, categorizing into bull, bear, steady ascent, steady descent. Inventory limits are set based on predicted trend (100% for strong trends, 50% for weak trends). When inventory exceeds these limits, market orders are triggered for liquidation, forcing the RL agent to learn policies that maintain inventory within trend-appropriate bounds.

## Foundational Learning

- **Concept: Limit Order Book (LOB) and Order Matching Mechanics**
  - Why needed here: The entire RELAVER framework is built on LOB dynamics—price-time priority, batch matching, queue position, and execution probability. Without understanding how orders queue, match, and cancel, the augmented action space and risk terms are opaque.
  - Quick check question: If you submit a buy limit order at price $P$ when the best ask is $P + \delta$, what determines whether and when your order executes under (a) continuous matching and (b) batch matching every 500ms?

- **Concept: Reinforcement Learning with PPO and Policy Regularization**
  - Why needed here: RELAVER uses PPO as its base RL algorithm and adds a KL-divergence regularization term from the Q-Teacher. Understanding PPO's clipping objective and how auxiliary losses interact is essential for debugging training dynamics.
  - Quick check question: In PPO, what does the clipping term $\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)$ accomplish, and why might adding a KL penalty change the optimal policy?

- **Concept: Market Making Objectives: Spread Capture vs. Inventory Risk**
  - Why needed here: The reward function balances PnL (spread capture), inventory penalty, exchange compensation, and stacking execution risk. Each term encodes a trade-off; misweighting leads to pathological behaviors (e.g., excessive inventory, toxic fills).
  - Quick check question: Why might a market maker accept a negative expected spread on some trades? What inventory-related rationale could justify this?

## Architecture Onboarding

- **Component map:** Historical LOB data -> Realistic MM environment (latency + batch matching) -> State Encoder (MLP+LSTM) -> Policy Head (6D action vector) -> Order execution -> Reward calculation -> Q-Teacher (DP) + Trend Predictor (LightGBM) -> KL regularization -> PPO training

- **Critical path:**
  1. Environment setup: Implement latency injection, batch matching, and order queue management. Validate against historical replay.
  2. Q-Teacher computation: Run DP backward pass on training period data (6 hours on RTX 4070 per the paper). Store Q* table.
  3. Trend predictor training: Train LightGBM on 4 months of historical data (June-September 2022 per the paper). Validate prediction accuracy.
  4. End-to-end RL training: Train PPO+LSTM policy with Q-Teacher regularization and trend-based inventory limits. Monitor PnL, MAP, and PnL-to-MAP.
  5. Out-of-sample evaluation: Test on held-out period (August-October 2023 per the paper).

- **Design tradeoffs:**
  - DP Q-Teacher vs. pure RL: DP provides strong guidance but requires future data and may bias toward hindsight-optimal actions. Ablation shows DP outperforms rule-based teachers, but rule-based teachers still improve over no teacher.
  - Fixed vs. dynamic inventory limits: Dynamic limits adapt to trends but add complexity and depend on predictor accuracy. Ablation shows trend expert reduces MAP by 66% but full model achieves best PnL-to-MAP.
  - Action space granularity: 6D discrete action with step sizes trades off expressiveness vs. sample efficiency. Finer discretization increases action space size exponentially.

- **Failure signatures:**
  - High MAP with low PnL: Inventory accumulating in adverse directions; trend predictor may be inaccurate or inventory limits too loose.
  - Low fill rate despite competitive quotes: Latency may be underestimated, or queue position modeling incorrect; check RQP and competitiveness metrics.
  - Training instability with KL term: KL weight α too high; policy collapses toward DP oracle and loses adaptability.
  - PnL degrades out-of-sample: Overfitting to training period regimes; consider regime detection or online adaptation.

- **First 3 experiments:**
  1. Baseline environment validation: Run simple strategies (FOIC, LIIC) in the realistic environment to confirm latency and batch matching effects. Compare to zero-latency, continuous matching baseline. Expected: lower fill rates, higher adverse selection.
  2. Ablation on Q-Teacher strength: Train RELAVER with varying KL weights α ∈ {0, 0.1, 1.0, 10.0}. Plot training curves (PnL, MAP) to find sweet spot between guidance and flexibility.
  3. Trend predictor accuracy analysis: Evaluate LightGBM predictor on held-out data. Compute confusion matrix for trend classes and correlation with future returns. If accuracy is near random, investigate feature importance and consider alternative horizons or models.

## Open Questions the Paper Calls Out
None

## Limitations
- Q-Teacher construction requires precise specification of reward/cost functions derived from historical data, which is not fully detailed in the paper
- Trend predictor generalization depends on prediction accuracy that may degrade under regime changes
- Action space discretization impact on performance is not analyzed, with finer discretization potentially increasing sample complexity exponentially

## Confidence
- **High confidence**: Overall architecture and methodology are clearly described with consistent improvements over baselines in ablation studies
- **Medium confidence**: Specific mechanisms for Q-Teacher construction from historical data and trend predictor feature engineering
- **Low confidence**: Generalization to markets with different latency profiles, matching mechanisms, or trend dynamics not seen in training data

## Next Checks
1. **Q-Teacher Construction Verification**: Implement Algorithm 1 and verify Q-values can be computed from historical data without future execution information. Test RL policy sensitivity to Q-Teacher guidance strength.
2. **Trend Predictor Out-of-Sample Performance**: Evaluate LightGBM trend classifier on held-out data from different market regimes. Compute prediction accuracy, confusion matrices, and correlation with future returns.
3. **Robustness to Latency and Matching Regime Changes**: Simulate RELAVER agent in environments with varying latency distributions and matching mechanisms. Measure degradation in PnL-to-MAP ratio to identify operational limits.