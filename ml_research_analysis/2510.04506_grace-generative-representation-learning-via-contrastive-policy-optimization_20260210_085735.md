---
ver: rpa2
title: 'GRACE: Generative Representation Learning via Contrastive Policy Optimization'
arxiv_id: '2510.04506'
source_url: https://arxiv.org/abs/2510.04506
tags:
- learning
- policy
- training
- contrastive
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRACE, a novel framework that unifies contrastive
  representation learning with generative reasoning via reinforcement learning. The
  key idea is to treat contrastive objectives as reward signals guiding a generative
  policy that produces interpretable rationales, rather than as static losses to minimize.
---

# GRACE: Generative Representation Learning via Contrastive Policy Optimization

## Quick Facts
- arXiv ID: 2510.04506
- Source URL: https://arxiv.org/abs/2510.04506
- Authors: Jiashuo Sun; Shixuan Liu; Zhaochen Su; Xianrui Zhong; Pengcheng Jiang; Bowen Jin; Peiran Li; Weijia Shi; Jiawei Han
- Reference count: 40
- Primary result: GRACE achieves 11.5% improvement in supervised MTEB settings over base models by treating contrastive objectives as rewards for generative rationale policies

## Executive Summary
GRACE introduces a novel framework that unifies contrastive representation learning with generative reasoning via reinforcement learning. Rather than treating contrastive objectives as static losses, GRACE treats them as reward signals guiding a generative policy that produces interpretable rationales. These rationales are then encoded into high-quality embeddings via mean pooling. The approach demonstrates substantial gains on the MTEB benchmark, showing that contrastive signals can be effectively leveraged as rewards to train policy models, resulting in stronger embeddings and transparent reasoning.

## Method Summary
GRACE bridges contrastive learning and generative reasoning by treating contrastive objectives as rewards in a reinforcement learning framework. A generative policy model produces interpretable rationales that maximize contrastive rewards, effectively learning to generate representations that are both discriminative and interpretable. The framework employs mean pooling of generated rationales to create final embeddings. This approach unifies the strengths of contrastive learning (discriminative power) with generative models (interpretability and reasoning), creating a novel paradigm for representation learning that goes beyond traditional contrastive loss minimization.

## Key Results
- On MTEB benchmark, supervised GRACE improves overall score by 11.5% over base models averaged across four backbones
- Unsupervised GRACE variant achieves 6.9% improvement while preserving general capabilities
- Demonstrates that contrastive signals can be effectively leveraged as rewards to train policy models
- Shows potential for interpretable reasoning while maintaining strong embedding quality

## Why This Works (Mechanism)
GRACE works by fundamentally reframing how contrastive objectives are utilized in representation learning. Instead of directly minimizing contrastive loss, the framework treats these objectives as rewards that guide a generative policy. This creates a reinforcement learning loop where the policy learns to generate rationales that maximize contrastive performance. The generative nature of the approach introduces interpretability - the rationales provide insight into why certain representations are formed. Mean pooling of these rationales then creates the final embeddings, combining the discriminative power of contrastive learning with the reasoning capabilities of generative models. This dual approach addresses both the need for strong representations and the desire for interpretability in modern ML systems.

## Foundational Learning
- **Contrastive learning**: A self-supervised learning approach that trains models to distinguish similar from dissimilar pairs of examples. Why needed: Forms the discriminative backbone for GRACE's reward structure. Quick check: Verify model can distinguish positive and negative pairs effectively.
- **Reinforcement learning policy optimization**: Framework where an agent learns to take actions that maximize cumulative reward. Why needed: Enables GRACE to treat contrastive objectives as rewards rather than losses. Quick check: Confirm policy gradients are properly computed and stable.
- **Generative modeling**: Approaches that learn to produce data samples from learned distributions. Why needed: Provides the mechanism for creating interpretable rationales. Quick check: Evaluate coherence and relevance of generated rationales.
- **Mean pooling for representation aggregation**: Technique to combine multiple representations into a single vector. Why needed: Transforms rationales into usable embeddings. Quick check: Compare against other pooling strategies for robustness.
- **MTEB benchmark suite**: Multi-task evaluation framework for text embeddings. Why needed: Provides standardized evaluation across diverse tasks. Quick check: Ensure all relevant tasks are included in evaluation.

## Architecture Onboarding

**Component map**: Input text -> Rationale Generator (Policy) -> Contrastive Reward Function -> Policy Optimizer -> Mean Pooled Rationale Embeddings

**Critical path**: Text input → Rationale generation → Contrastive reward calculation → Policy update → Final embedding creation

**Design tradeoffs**: The framework trades off between interpretability (via generated rationales) and pure discriminative performance. Mean pooling provides simplicity but may lose fine-grained information compared to more complex aggregation methods.

**Failure signatures**: Poor contrastive performance indicates inadequate policy learning; incoherent rationales suggest generator issues; unstable training points to reward signal problems.

**First experiments**: 1) Baseline contrastive learning without generative component, 2) Policy-only generation without contrastive rewards, 3) End-to-end GRACE training with validation on MTEB

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation scope limited to MTEB benchmark without broader capability verification
- Interpretability of generated rationales not systematically evaluated through human or automated assessment
- Technical implementation details around policy gradient updates and baseline subtraction are sparse
- The 6.9% unsupervised improvement may be sensitive to specific implementation choices not fully disclosed

## Confidence

**Confidence labels:**
- MTEB benchmark results: **Medium** - Strong empirical gains but limited scope and missing ablations
- Contrastive-as-reward formulation: **High** - Novel and theoretically sound, though implementation details could be richer
- Preservation of general capabilities: **Low** - Claimed but not empirically verified
- Interpretability of rationales: **Low** - Not systematically evaluated

## Next Checks

1. Conduct ablation studies varying the rationale generation policy architecture and pooling strategies to confirm robustness of gains
2. Evaluate on additional benchmarks beyond MTEB to test generalizability and verify preservation of general capabilities
3. Implement human evaluation of generated rationales for interpretability and faithfulness to contrastive objectives