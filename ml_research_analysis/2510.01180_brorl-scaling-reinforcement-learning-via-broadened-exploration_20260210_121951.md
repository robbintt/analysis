---
ver: rpa2
title: 'BroRL: Scaling Reinforcement Learning via Broadened Exploration'
arxiv_id: '2510.01180'
source_url: https://arxiv.org/abs/2510.01180
tags:
- brorl
- training
- tokens
- prorl
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BroRL, a method for scaling reinforcement
  learning (RL) by increasing the number of rollouts per example rather than training
  steps. The authors argue that performance plateaus in existing RL methods are due
  to insufficient exploration rather than fundamental limits of RL.
---

# BroRL: Scaling Reinforcement Learning via Broadened Exploration

## Quick Facts
- arXiv ID: 2510.01180
- Source URL: https://arxiv.org/abs/2510.01180
- Authors: Jian Hu; Mingjie Liu; Ximing Lu; Fang Wu; Zaid Harchaoui; Shizhe Diao; Yejin Choi; Pavlo Molchanov; Jun Yang; Jan Kautz; Yi Dong
- Reference count: 16
- Primary result: Revives saturated RL models by increasing rollouts per example, achieving SOTA for 1.5B models on verifiable reasoning tasks

## Executive Summary
This paper introduces BroRL, a method that scales reinforcement learning by increasing the number of rollouts per example rather than training steps. The authors argue that performance plateaus in existing RL methods stem from insufficient exploration rather than fundamental limits of RL. By systematically increasing rollout size, BroRL reduces the impact of an "unsampled coupling" term, leading to improved policy updates. The method achieves state-of-the-art results for 1.5B models across diverse benchmarks and is more compute-efficient than existing approaches.

## Method Summary
BroRL scales RL training by broadening exploration through increased rollouts per example. Unlike traditional methods that increase training steps, BroRL focuses on sampling more trajectories from the current policy. The theoretical analysis shows that increasing rollout size systematically improves policy updates by reducing the unsampled coupling term. This approach is particularly effective for verifiable reasoning tasks where multiple rollouts can be efficiently generated and evaluated. The method is implemented by modifying the RL training loop to generate more rollouts per training example while maintaining the same number of training steps.

## Key Results
- Revives models saturated after 3K ProRL training steps
- Achieves state-of-the-art results for 1.5B models across diverse benchmarks
- Nearly doubles throughput compared to ProRL by shifting from memory-bound to compute-bound operations

## Why This Works (Mechanism)
BroRL addresses the fundamental limitation of insufficient exploration in RL training. When the number of rollouts is limited, the policy updates suffer from high variance due to the unsampled coupling term. By increasing rollouts, BroRL provides more accurate gradient estimates, leading to more stable and effective policy updates. The method shifts the bottleneck from memory access to computation, making better use of available compute resources.

## Foundational Learning
**Reinforcement Learning Fundamentals**
- Why needed: Understanding RL optimization and policy gradient methods
- Quick check: Can explain policy gradient theorem and its variance

**Exploration vs Exploitation Tradeoff**
- Why needed: Core concept in RL that BroRL addresses through broadened exploration
- Quick check: Can articulate how BroRL modifies this tradeoff

**Variance Reduction Techniques**
- Why needed: BroRL's effectiveness relies on reducing variance in policy updates
- Quick check: Can explain how more rollouts reduce variance in gradient estimates

## Architecture Onboarding

**Component Map**
Pre-training -> RL Fine-tuning -> BroRL Sampling -> Policy Update -> Reward Evaluation

**Critical Path**
BroRL Sampling -> Policy Update -> Reward Evaluation -> Pre-training

**Design Tradeoffs**
- Memory vs Compute: BroRL shifts from memory-bound to compute-bound operations
- Exploration vs Exploitation: Increased rollouts favor exploration
- Training Time vs Performance: More rollouts improve performance but increase computation

**Failure Signatures**
- Poor performance with insufficient rollouts
- Memory bottlenecks if rollout generation is inefficient
- Reward function design flaws affecting exploration quality

**First Experiments**
1. Compare BroRL performance with baseline RL at different rollout counts
2. Measure throughput improvement on different hardware configurations
3. Test BroRL on non-verifiable tasks to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes quadratic coupling term, may not capture all failure modes
- Primarily demonstrated on verifiable reasoning tasks, generalization to open-ended generation uncertain
- Method's sensitivity to rollout size hyperparameter selection could affect reproducibility

## Confidence

**Major Claim Confidence Labels:**
- Theoretical scaling analysis: Medium
- State-of-the-art empirical results: High (within verifiable task domain)
- Compute efficiency improvements: Medium

## Next Checks

1. Test BroRL on non-verifiable generation tasks (story continuation, dialogue) to assess generalizability beyond mathematical reasoning.
2. Conduct ablation studies varying rollout sizes systematically across different model scales (1B, 7B, 34B) to map the relationship between rollout count and performance gains.
3. Compare BroRL's exploration efficiency against alternative methods like look-ahead tree search or entropy regularization under identical compute budgets to isolate the specific contribution of broadened exploration.