---
ver: rpa2
title: Towards Empowerment Gain through Causal Structure Learning in Model-Based RL
arxiv_id: '2502.10077'
source_url: https://arxiv.org/abs/2502.10077
tags:
- uni00000013
- uni00000011
- uni00000014
- causal
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ECL, a framework that integrates empowerment
  with causal structure learning to improve controllability and learning efficiency
  in model-based reinforcement learning. The core idea is to use empowerment as an
  intrinsic motivation to guide exploration under learned causal structures, while
  simultaneously refining these structures through data collected during exploration.
---

# Towards Empowerment Gain through Causal Structure Learning in Model-Based RL

## Quick Facts
- **arXiv ID:** 2502.10077
- **Source URL:** https://arxiv.org/abs/2502.10077
- **Reference count:** 40
- **One-line primary result:** ECL outperforms other causal MBRL methods in causal discovery accuracy, sample efficiency, and asymptotic performance across 6 environments including pixel-based tasks.

## Executive Summary
This paper introduces ECL, a framework that integrates empowerment with causal structure learning to improve controllability and learning efficiency in model-based reinforcement learning. The core idea is to use empowerment as an intrinsic motivation to guide exploration under learned causal structures, while simultaneously refining these structures through data collected during exploration. The framework alternates between empowerment-driven exploration and causal structure optimization, using a curiosity reward to prevent overfitting during downstream task learning. Experiments across 6 environments, including pixel-based tasks, show that ECL outperforms other causal MBRL methods in causal discovery accuracy, sample efficiency, and asymptotic performance, achieving superior results in both in-distribution and out-of-distribution settings.

## Method Summary
ECL is a 3-step iterative framework for model-based reinforcement learning that integrates causal structure learning with empowerment-driven exploration. First, it trains a dense dynamics model and discovers an initial causal mask using constraint-based or score-based methods. Second, it alternates between optimizing an empowerment policy (maximizing the difference between causal and dense model empowerment) and refining the causal mask using data from this policy. Third, it trains the final task policy using CEM planning with the optimized causal model and a curiosity-augmented reward that prevents overfitting to the learned structure. The framework specifically addresses spurious correlations in high-dimensional observations by focusing exploration on controllable state dimensions revealed by the causal structure.

## Key Results
- ECL achieves higher success rates and sample efficiency than other causal MBRL methods in the "Manipulation" and "Chemical" environments
- The framework demonstrates superior causal discovery accuracy with higher F1 and ROC AUC scores for learned causal graphs
- ECL maintains strong performance in out-of-distribution settings where learned causal structures differ from training data
- The curiosity regularization component prevents policy overfitting, resulting in more robust task performance

## Why This Works (Mechanism)

### Mechanism 1: Empowerment Gain via Causal Masking
The agent calculates mutual information between actions and future states, then maximizes the difference between causal and dense model empowerment to focus exploration on controllable state dimensions. This prioritizes states where the causal structure reveals true controllability that the dense model obscures, such as ignoring noise nodes to focus on movable objects. The dense model captures all spurious correlations while the causal model captures only true dependencies, making their difference highlight "true" control channels.

### Mechanism 2: Iterative Causal Graph Refinement
The framework alternates between empowerment-driven exploration and causal mask optimization to improve causal discovery accuracy. An empowerment-maximizing policy generates trajectories that specifically expose dynamics of controllable state factors, which are then fed back into the causal discovery algorithm to prune spurious edges. This data-driven approach is more informative for distinguishing causal links than passive collection methods.

### Mechanism 3: Curiosity Regularization for Robustness
A curiosity-based intrinsic reward prevents the downstream policy from overfitting to the learned causal structure. The reward measures KL divergence between environment dynamics and causal model predictions (minus dense model divergence), encouraging exploration of states where the causal model is wrong or uncertain. This maintains exploration and prevents the policy from exploiting a brittle, over-simplified causal model.

## Foundational Learning

### Concept: Structural Causal Models (SCM) & Causal Masks
Why needed here: ECL treats environment dynamics as a Directed Acyclic Graph where nodes are state dimensions, with a "Causal Mask" being an adjacency matrix selecting which inputs predict future states. This is essential for implementing the forward pass where state vectors are multiplied by binary masks to abstract relevant information.
Quick check question: How does an element-wise product between a state vector and a binary mask effectively implement "state abstraction"?

### Concept: Empowerment (Information-Theoretic)
Why needed here: The exploration policy is trained on maximizing mutual information between actions and future states, measuring "potential influence" regardless of specific goals. This forms the core intrinsic motivation for control-driven exploration.
Quick check question: Why is maximizing mutual information considered a form of intrinsic motivation for control?

### Concept: Model-Based Reinforcement Learning (MBRL) Loop
Why needed here: The framework relies on a standard MBRL backbone (collect data → train model → plan) but injects causal logic to address model bias. Understanding where the standard loop breaks highlights why ECL adds specific components like causal masks and empowerment.
Quick check question: In a standard MBRL setup, why does training a dense neural network on high-dimensional observations often lead to "spurious correlations"?

## Architecture Onboarding

### Component map:
Dense Model → Causal Dynamics Model (with Causal Mask) → Empowerment Policy → Task Policy

### Critical path:
1. Warm Start: Train Dense Dynamics Model and initial Causal Mask using random transitions
2. Refinement Loop: Alternate between training Empowerment Policy πe and updating Causal Mask M using transitions collected by πe
3. Deployment: Freeze model; train Task Policy πθ using CEM planning with curiosity-augmented reward

### Design tradeoffs:
- Causal Discovery Method: Constraint-based (CDL) vs Score-based (REG). CDL is computationally faster but may be brittle; REG handles noise better but requires tuning sparsity coefficients.
- Model Architecture: ECL requires maintaining two models (Dense and Causal), doubling forward-pass cost during empowerment calculation compared to standard MBRL.

### Failure signatures:
- Empowerment Collapse: Empowerment gain term approaches zero because causal mask becomes too sparse or dense model fails to learn
- Curiosity Overload: Agent moves erratically to maximize KL-divergence term, ignoring actual task reward
- Mask Divergence: Causal mask predicts "unmovable" objects as "movable," leading to confused exploration

### First 3 experiments:
1. Ablation on Step 2: Run framework with random exploration instead of empowerment policy during Model Optimization phase. Check if causal discovery accuracy drops on "Manipulation" environment.
2. Curiosity Sensitivity: Vary hyperparameter λ in "Chemical" environment to determine threshold where task performance degrades vs improves.
3. Visual Inspection: Train on "Manipulation" task and visualize adjacency matrix of Causal Mask. Verify it assigns zero weight to "Marker" distractors and non-zero to "Robot" and "Movable Object" states.

## Open Questions the Paper Calls Out

### Open Question 1
Can the ECL framework be extended to explicitly disentangle directable behavioral dimensions rather than relying on implicit controllability enhancement? The authors identify that ECL implicitly enhances controllability but "does not explicitly tease apart different behavioral dimensions," which is necessary to further improve behavioral control in complex, multi-modal robotics tasks.

### Open Question 2
How can the causal discovery component of ECL be adapted to account for non-stationary environments and changing dynamics? The current framework assumes a fixed underlying causal graph, which is often violated in real-world scenarios where dynamics shift over time. The authors intend to incorporate "local causal discovery and modeling non-stationary change factors."

### Open Question 3
What is the impact of including the omitted entropy terms in the empowerment maximization objective? The paper mentions that the current optimization relies on a simplified KL term and future work will "explore additional entropy relaxation methods" to understand the trade-off between computational cost and performance gain.

## Limitations
- The framework implicitly enhances controllability but does not explicitly disentangle different behavioral dimensions for improved behavioral control
- Current implementation does not account for changing dynamics or non-stationary environments where causal structures may shift over time
- The empowerment maximization objective omits two entropy terms, potentially simplifying the optimization at the cost of exploration efficiency

## Confidence

- **High Confidence:** The general framework architecture and integration of empowerment with causal structure learning are well-defined and supported by experimental results
- **Medium Confidence:** The theoretical motivation for using empowerment gain as an exploration signal is sound, but practical implementation details for marginal distribution estimation are unclear
- **Low Confidence:** The exact method for estimating marginal distribution P(st+1|st) and precise access mechanism for Penv during curiosity calculation are major implementation uncertainties

## Next Checks

1. Implement and test different methods for estimating P(st+1|st) (e.g., moving average, variational approximation) and evaluate their impact on empowerment stability and causal discovery accuracy
2. Systematically vary the curiosity regularization coefficient λ across a wider range (e.g., [0.01, 0.1, 1.0, 10.0]) in the "Chemical" environment to identify optimal balance between task performance and exploration robustness
3. Compare performance of ECL using constraint-based (CDL) versus score-based (REG) method on "Manipulation" environment to determine which approach is more robust to noise and spurious correlations