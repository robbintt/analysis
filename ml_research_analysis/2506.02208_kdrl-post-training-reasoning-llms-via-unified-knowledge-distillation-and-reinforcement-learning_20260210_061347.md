---
ver: rpa2
title: 'KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and
  Reinforcement Learning'
arxiv_id: '2506.02208'
source_url: https://arxiv.org/abs/2506.02208
tags:
- training
- kdrl
- teacher
- grpo
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces KDRL, a unified post-training framework that\
  \ combines knowledge distillation (KD) and reinforcement learning (RL) to enhance\
  \ reasoning capabilities in large language models (LLMs). KDRL leverages policy\
  \ gradient optimization to jointly minimize the reverse Kullback\u2013Leibian divergence\
  \ (RKL) between student and teacher models while maximizing expected rule-based\
  \ rewards."
---

# KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.02208
- Source URL: https://arxiv.org/abs/2506.02208
- Reference count: 40
- Primary result: Achieves up to 2.6% accuracy improvement on mathematical reasoning benchmarks by unifying knowledge distillation and reinforcement learning

## Executive Summary
KDRL introduces a unified post-training framework that combines knowledge distillation (KD) and reinforcement learning (RL) to enhance reasoning capabilities in large language models. The framework leverages policy gradient optimization to jointly minimize the reverse Kullback-Leibler divergence between student and teacher models while maximizing expected rule-based rewards. Through systematic exploration of KL approximations, coefficient scheduling, and reward-guided masking strategies, KDRL demonstrates superior performance and efficiency compared to standalone GRPO and KD baselines on mathematical reasoning benchmarks.

## Method Summary
KDRL is a post-training framework that unifies knowledge distillation and reinforcement learning for reasoning LLMs through a joint loss formulation. The approach uses policy gradient optimization to minimize reverse Kullback-Leibler divergence while maximizing expected rule-based rewards. Key innovations include the k2 estimator for KL approximation, linear decay scheduling for the KL coefficient β, and reward-guided masking to improve reasoning efficiency. The framework is evaluated on mathematical reasoning benchmarks, showing consistent improvements over both GRPO and KD baselines.

## Key Results
- Achieves up to 2.6% accuracy improvement on mathematical reasoning benchmarks compared to standalone approaches
- Demonstrates better reasoning token efficiency by reducing average token count during problem-solving
- Shows effectiveness in R1-Zero-like training scenarios with minimal external data requirements

## Why This Works (Mechanism)
KDRL works by jointly optimizing knowledge distillation and reinforcement learning objectives through policy gradient methods. The reverse Kullback-Leibler divergence ensures the student model stays close to the teacher's output distribution while the reward term drives optimization toward correct reasoning paths. The k2 estimator provides a stable approximation of the KL divergence that works well with policy gradient optimization. The linear decay schedule for the KL coefficient β allows the model to start with stronger imitation of the teacher and gradually shift toward reward-driven learning, preventing early collapse while maintaining performance.

## Foundational Learning

### Policy Gradient Optimization
**Why needed**: Enables gradient-based optimization of non-differentiable reward functions in RL scenarios
**Quick check**: Verify that policy gradient updates follow the REINFORCE algorithm structure with baseline subtraction

### Kullback-Leibler Divergence (KL)
**Why needed**: Measures distributional similarity between teacher and student models for distillation
**Quick check**: Confirm that reverse KL (RKL) is used instead of forward KL for better mode-covering behavior

### Reward Shaping
**Why needed**: Modifies reward structure to guide learning, though KDRL finds it problematic in early training
**Quick check**: Compare joint loss vs. reward shaping formulations to understand gradient stability differences

## Architecture Onboarding

### Component Map
Teacher Model -> Student Model -> Joint Loss (RKL + Reward) -> Policy Gradient Optimizer

### Critical Path
Teacher generation → Student response → Reward calculation → KL estimation (k2) → Joint loss computation → Policy gradient update

### Design Tradeoffs
- Joint loss vs. reward shaping: Joint loss provides stable training while reward shaping causes early collapse
- Linear decay vs. constant β: Linear decay balances KD and RL better than fixed coefficients
- k2 estimator vs. other KL approximations: k2 provides stable gradients for policy optimization

### Failure Signatures
- Training collapse in early steps when using reward shaping
- Suboptimal performance when KL coefficient is too high or too low
- Instability when KL estimation is not properly regularized

### First Experiments
1. Verify joint loss formulation produces stable gradients compared to reward shaping
2. Test linear decay schedule against constant β values for optimal performance
3. Evaluate k2 estimator stability compared to alternative KL approximation methods

## Open Questions the Paper Calls Out

### Open Question 1
**How does KDRL generalize to reasoning domains beyond mathematics, such as code generation, logical reasoning, or scientific problem-solving?**
The paper evaluates exclusively on mathematical reasoning benchmarks, and while the Introduction notes that KD tends to generalize poorly to out-of-domain scenarios, no cross-domain experiments are presented. Whether the k2 estimator, annealing schedules, and reward-guided masking transfer effectively to domains with different verification constraints remains unknown.

### Open Question 2
**What are the optimal scheduling strategies for the KL coefficient β, and how sensitive is performance to the choice of annealing function?**
The paper explores a linear decay schedule and constant coefficients, concluding that annealing improves performance. However, only one specific linear schedule is tested, without systematic comparison of alternative scheduling functions like cosine annealing, step-wise decay, or reward-adaptive scheduling.

### Open Question 3
**Why does integrating KD and RL via reward shaping lead to training collapse while joint loss optimization succeeds?**
Section 3.1 and Figure 2 show that reward shaping leads to collapsed training dynamics in early steps whereas joint loss yields stable improvements. The paper documents this empirical finding but provides no theoretical or mechanistic explanation for the divergence in training dynamics.

## Limitations

- Limited evaluation to mathematical reasoning domains without testing generalization to other reasoning tasks
- Computational overhead of combining KD and RL during post-training is not fully characterized
- Reliance on rule-based rewards that may not generalize to domains where objective correctness is harder to define

## Confidence

- **High confidence**: The core technical contribution of unifying KD and RL through policy gradient optimization is well-founded and the mathematical formulation appears sound
- **Medium confidence**: The empirical improvements on mathematical reasoning benchmarks are well-documented, but generalization to other reasoning domains is uncertain
- **Low confidence**: Claims about resource efficiency are not fully substantiated, as total computational cost is not compared against simpler alternatives

## Next Checks

1. **Cross-domain generalization test**: Evaluate KDRL on reasoning benchmarks outside mathematics (e.g., commonsense reasoning, code generation, or scientific reasoning) to assess whether the framework generalizes beyond mathematical problem-solving

2. **Teacher quality sensitivity analysis**: Systematically vary teacher model quality and size to determine how sensitive KDRL's performance is to teacher availability and capability

3. **Computational overhead characterization**: Conduct a comprehensive cost-benefit analysis comparing the total training time and resources required for KDRL versus standalone GRPO or KD approaches across different model scales