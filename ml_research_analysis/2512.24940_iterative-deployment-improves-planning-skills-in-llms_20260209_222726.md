---
ver: rpa2
title: Iterative Deployment Improves Planning Skills in LLMs
arxiv_id: '2512.24940'
source_url: https://arxiv.org/abs/2512.24940
tags:
- traces
- planning
- generations
- tasks
- valid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Iterative deployment of large language models, where each generation
  is fine-tuned on curated traces from previous deployments, significantly improves
  planning capabilities. The method was tested on classical planning domains using
  Qwen3 4B, showing more than doubling of task-solving performance within five generations.
---

# Iterative Deployment Improves Planning Skills in LLMs

## Quick Facts
- **arXiv ID:** 2512.24940
- **Source URL:** https://arxiv.org/abs/2512.24940
- **Reference count:** 40
- **Primary result:** Iterative fine-tuning on curated traces more than doubles task-solving performance within five generations.

## Executive Summary
Iterative deployment improves large language models' planning capabilities by fine-tuning each generation on curated valid traces from previous deployments. The method was tested on classical planning domains using Qwen3 4B, showing significant performance gains and the ability to discover longer plans than the base model. The approach was mathematically proven equivalent to REINFORCE with implicit binary rewards, highlighting both its effectiveness and potential safety implications.

## Method Summary
The method involves prompting a base LLM to solve planning tasks, validating generated traces with an external validator, curating valid traces (selecting shortest plans per task), and fine-tuning the base model using LoRA on aggregated valid traces from all generations. This process repeats for multiple generations, with each new model trained on the union of all previously validated successful traces.

## Key Results
- Performance more than doubles within five generations (e.g., Blocksworld: 67.4 to 154 solved tasks)
- Later models discover longer plans than base model, indicating out-of-distribution generalization
- Curation significantly improves results compared to using all traces
- Method proven equivalent to REINFORCE with implicit binary rewards

## Why This Works (Mechanism)

### Mechanism 1: REINFORCE Equivalence
Supervised fine-tuning on valid traces is mathematically equivalent to REINFORCE with binary rewards. The gradient direction of SFT on only valid traces (reward=1) matches REINFORCE policy gradients with a binary reward function, scaled by the valid-trace ratio.

### Mechanism 2: Progressive Bootstrapping
Iterative training with accumulated curated traces enables bootstrapping and generalization to harder tasks. By accumulating valid traces across generations, the model learns from its own successful solutions, building reusable sub-plan structures for more complex plans.

### Mechanism 3: Curation Criticality
Curation (selecting valid traces) is critical for performance improvement and preventing model collapse. Filtering out invalid traces prevents reinforcing incorrect patterns, while selecting highest-quality valid traces guides the model toward more efficient solutions.

## Foundational Learning

- **Reinforcement Learning (REINFORCE)**: Essential for understanding why selecting valid traces works as a learning signal. The paper proves its mechanism is equivalent to REINFORCE with binary rewards.
  - *Quick check*: What is the implicit "reward" and what is the "policy" in this paper's formulation?

- **Classical Planning (PDDL)**: Understanding that plans are action sequences transforming initial states to goals, and that plans can be validated for correctness, is necessary to interpret the external validator's role.
  - *Quick check*: What is the primary function of the external validator `V(x, y)` in this setup?

- **Supervised Fine-Tuning (SFT)**: The core training step in each generation. Understanding SFT (next-token prediction) is crucial for distinguishing it from, and seeing its connection to, RL.
  - *Quick check*: What is the specific loss function applied to the curated dataset when training the next generation?

## Architecture Onboarding

- **Component map:** Base Model -> Generated Traces -> External Validator -> Curation/Aggregation -> Curated Dataset -> LoRA Fine-tuning -> Next Generation Model

- **Critical path:**
  1. Prompt current generation model with all planning tasks
  2. **Critical:** Validate all generated traces, discarding invalid ones
  3. **Critical:** Aggregate new valid traces with historical set from all prior generations, keeping highest-quality trace per task
  4. Fine-tune base model (not previous generation) on aggregated dataset

- **Design tradeoffs:**
  - **Curation Strictness:** Strict validator ensures high quality but yields small dataset; looser validator provides more data but risks reinforcing errors
  - **Data Selection Policy:** Paper optimizes for efficiency (shortest plan); alternatives could optimize for reasoning diversity
  - **Training from Base vs. Previous Generation:** Paper fine-tunes base model to avoid stacking LoRA adapter instability

- **Failure signatures:**
  - **Stagnation/Model Collapse:** Solved task count plateaus or degrades; plans become repetitive or nonsensical
  - **Reward Hacking:** Model learns to generate traces that please validator rather than genuinely solving tasks
  - **Catastrophic Forgetting:** Model improves on planning but loses general capabilities from base pre-training

- **First 3 experiments:**
  1. **Reproduction:** Implement core loop on Blocksworld with smaller model; verify solved tasks increase over 3-5 generations
  2. **Ablation on Curation:** Run with and without curation; compare performance to quantify curation impact
  3. **Validator Bias Injection:** Introduce known bias into validator; observe if model learns to exploit bias in later generations

## Open Questions the Paper Calls Out

- **Model Collapse Timing:** Does data curation fully prevent model collapse or merely delay degradation? The experiments only ran for up to 10 generations, insufficient to observe long-term effects. Would require experiments running >50 generations to resolve.

- **Safety Alignment Conflicts:** How do implicit reward functions from curation interact with explicit safety alignment training? The study used a controlled validator but didn't measure safety metrics or alignment drift. Would need experiments measuring safety benchmarks after multiple generations.

- **Validator Bias Amplification:** To what extent do systematic validator biases accumulate and amplify in the model's policy over generations? The experiments used a perfect deterministic validator. Would need sensitivity analysis using validators with known error rates or specific biases.

## Limitations
- Relies heavily on availability of reliable external validator, which may not exist for many real-world planning tasks
- Potential safety concern: model could learn to exploit validator biases rather than genuinely solving problems
- Broader applicability to non-planning domains or tasks without reliable validators untested

## Confidence
- **High Confidence**: Empirical results showing performance improvement over generations in controlled planning domains
- **Medium Confidence**: Mathematical equivalence to REINFORCE with binary rewards and its practical implications
- **Medium Confidence**: Claim of out-of-distribution generalization (discovering longer plans than base model)
- **Low Confidence**: Broader applicability to non-planning domains or tasks without reliable external validators

## Next Checks
1. **Validator Bias Sensitivity Test**: Systematically introduce controlled biases into validator and measure whether model learns to exploit these biases rather than solving tasks correctly

2. **Cross-Domain Generalization**: Apply iterative deployment to mathematical reasoning or code generation domains; measure whether same performance improvements and generalization patterns emerge without PDDL-style validator

3. **Data Efficiency Analysis**: Compare number of valid traces required to achieve performance improvements versus traditional reinforcement learning approaches on same planning tasks