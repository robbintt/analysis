---
ver: rpa2
title: "LLMzSz\u0141: a comprehensive LLM benchmark for Polish"
arxiv_id: '2501.02266'
source_url: https://arxiv.org/abs/2501.02266
tags:
- exams
- language
- school
- polish
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLMzSz\u0141 is the first large-scale Polish LLM benchmark, built\
  \ from national school and vocational exams (nearly 19k multiple-choice questions\
  \ across 154 domains). Using CKE exam archives, the dataset covers four exam types\
  \ and is stratified by difficulty."
---

# LLMzSzŁ: a comprehensive LLM benchmark for Polish

## Quick Facts
- arXiv ID: 2501.02266
- Source URL: https://arxiv.org/abs/2501.02266
- Reference count: 22
- Primary result: First large-scale Polish LLM benchmark with ~19k multiple-choice questions from national exams; best model achieved 67.17% accuracy

## Executive Summary
LLMzSzŁ is the first large-scale benchmark for evaluating LLMs on Polish language understanding, built from national school and vocational exam archives. The dataset contains nearly 19,000 multiple-choice questions across 154 domains, stratified by difficulty and covering four exam types. The benchmark addresses the lack of standardized evaluation tools for Polish and enables systematic comparison of model performance across size, language, release date, and fine-tuning status.

## Method Summary
The benchmark was constructed from CKE exam archives, extracting questions and answer keys from PDF documents. The evaluation uses the LM Evaluation Harness with a likelihood-based approach, where models calculate probabilities for each answer option (A-D) and select the highest-scoring one. Questions were manually processed to remove images and charts, then aligned with their correct answers. The dataset is stratified by exam type and difficulty level, and models are evaluated using a standardized prompt template in Polish.

## Key Results
- Multilingual models outperformed monolingual Polish models
- Model performance correlates with parameter count and fine-tuning status
- Mistral-Large-Instruct-2407 achieved the highest accuracy at 67.17%
- Models under 15B parameters showed significantly lower performance
- Correlation analysis between LLM and human performance suggests potential for exam validation

## Why This Works (Mechanism)
The benchmark works by providing a standardized, high-quality dataset of real-world exam questions that tests comprehensive Polish language understanding across multiple domains. The likelihood-based evaluation method provides objective, reproducible scoring without requiring model-generated answers. The stratification by difficulty and exam type enables nuanced analysis of model capabilities, while the correlation with historical human performance validates the benchmark's relevance to actual exam-taking ability.

## Foundational Learning

- **Concept: Language Model Evaluation via Likelihood (Log-Likelihood) Selection**
  - Why needed here: This is the core evaluation harness used for the benchmark. To interpret the results, one must understand that the model is not generating an answer but calculating a probability (likelihood) for each option (A, B, C, D) and selecting the one with the highest score.
  - Quick check question: For a given question, the model outputs likelihoods: A=0.1, B=0.7, C=0.15, D=0.05. What is the model's predicted answer and its associated "confidence"?

- **Concept: Data Contamination / Memorization**
  - Why needed here: A central validity threat to this and all similar benchmarks. If exam questions were in the training data, the model's performance reflects memorization, not competence. The paper attempts to mitigate this by tracking release dates.
  - Quick check question: You are evaluating a model released in January 2023 on an exam from 2015 and an exam from 2024. Which result is more likely to be contaminated, and how would you interpret the scores differently?

- **Concept: Model Stratification by Size, Language, and Fine-tuning**
  - Why needed here: The paper's analysis does not just give a single leaderboard; it breaks down results by these key axes. Understanding their interaction is critical for making actionable claims about model selection.
  - Quick check question: Based on the paper's findings, if you were resource-constrained to a model under 8B parameters, would you prioritize a multilingual model or a Polish-specific one? What about if you could use a 70B+ parameter model?

## Architecture Onboarding

- **Component map:** CKE Archives (PDFs) -> Data Processing Pipeline (extraction, cleaning, alignment) -> Evaluation Harness (likelihood calculation) -> Models Under Test (LLMs) -> Correlation & Anomaly Analysis (post-processing)

- **Critical path:**
  1. Data Ingestion & Cleaning: Most fragile step requiring manual alignment of questions and answers
  2. Prompt Formatting: Consistent application of Polish template affects performance
  3. Likelihood-based Evaluation: Core inference step scoring each answer option
  4. Correlation Calculation: Validates exam difficulty and identifies potential errors

- **Design tradeoffs:**
  - Closed-ended vs. Open-ended: Limited to multiple-choice for objective evaluation, sacrificing coverage of informal language
  - Single-source (CKE) vs. Web-scraped: Ensures high quality but limits domain breadth compared to benchmarks like MMLU
  - Manual vs. Automated Processing: Manual alignment ensured quality but made scaling difficult

- **Failure signatures:**
  - Low Correlation: Suggests benchmark may not proxy human exam difficulty well
  - Abnormally Low Probability: Can flag potential errors in exam answer keys
  - Performance Degradation over Time: May indicate training data cutoffs or contamination

- **First 3 experiments:**
  1. Baseline Establishment: Run evaluation harness on open-weight models (Llama-3.1-8B, Mistral-7B-v0.3) to reproduce baseline scores
  2. Contamination Analysis: Evaluate model performance on exams before vs. after its training cutoff date
  3. Anomaly Detection Validation: Intentionally flip correct answers and observe if model assigns low probability to incorrect answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs serve as a reliable primary tool for validating exam difficulty and detecting anomalies in examination tasks prior to publication?
- Basis in paper: [explicit] The authors state that if correlation between model and human performance is confirmed, "it will advocate for a possible use of LLMs as a primary tool for the verification of exam questions."
- Why unresolved: Current study limited to closed-ended questions; correlation sometimes showed inverse trends between human and model scores
- What evidence would resolve it: Longitudinal study demonstrating LLM performance deviations consistently predict exam difficulty anomalies or errors

### Open Question 2
- Question: Does proficiency in the LLMzSzŁ benchmark generalize to real-world professional competence in the vocations tested?
- Basis in paper: [explicit] Limitations state "extent to which an examinee's proficiency in solving tests generalizes to performance in real-life situations is an open question."
- Why unresolved: Benchmark only measures ability to pass written tests; measuring real-world competence requires embodied agents
- What evidence would resolve it: Evaluation using embodied agents or practical simulations comparing test proficiency against applied vocational skills

### Open Question 3
- Question: How does the inclusion of open-ended questions affect the correlation between LLM and human performance on these exams?
- Basis in paper: [inferred] Analysis restricted to closed questions while human scores include both closed and open questions; lack of correlation in some subjects may be due to open question difficulty
- Why unresolved: Current dataset and evaluation harness don't support processing or grading of open-ended responses
- What evidence would resolve it: Expanding dataset to include open-ended questions and evaluating LLMs using human-like grading criteria

## Limitations
- Potential data contamination from exam questions in training data, though mitigated by release date analysis
- Focus on closed-ended questions excludes evaluation of open-ended responses and informal language use
- Manual processing required for data extraction introduces potential human error and scaling challenges

## Confidence

| Claim Cluster | Confidence Level | Evidence Basis |
|---------------|------------------|----------------|
| Multilingual vs monolingual performance | High | Clear statistical evidence, aligns with established LLM evaluation patterns |
| Parameter count correlation with performance | High | Well-established relationship in literature, supported by data |
| Mistral-Large-Instruct-2407 as best model | Medium | Highest reported score but no confidence intervals provided |
| LLM reliability for exam validation | Low | Correlation analysis shows promise but requires further validation |

## Next Checks
1. **Contamination Verification:** Evaluate a model with known training cutoff on exams before vs. after that date; verify performance drop on post-cutoff exams
2. **Anomaly Detection Validation:** Intentionally introduce known errors into answer keys; verify model assigns abnormally low probabilities to now-incorrect answers
3. **Correlation Reproducibility:** Run evaluation on subset of exams with known human pass rates using 2-3 model sizes; calculate and compare Pearson correlation with reported results