---
ver: rpa2
title: Decoding Order Matters in Autoregressive Speech Synthesis
arxiv_id: '2601.08450'
source_url: https://arxiv.org/abs/2601.08450
tags:
- decoding
- speech
- order
- diffusion
- autoregressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how decoding order impacts autoregressive
  speech synthesis by integrating the masked diffusion model framework with text-to-speech
  synthesis. Through experiments on the LJSpeech dataset, the authors demonstrate
  that adaptive decoding strategies, such as Top-K and duration-guided decoding, consistently
  outperform fixed orders like left-to-right (l2r) or right-to-left (r2l), with duration-guided
  decoding achieving the best MCD of 3.98, logF0 of 5.67, and UTMOSv2 of 3.98.
---

# Decoding Order Matters in Autoregressive Speech Synthesis

## Quick Facts
- arXiv ID: 2601.08450
- Source URL: https://arxiv.org/abs/2601.08450
- Reference count: 0
- Authors: Minghui Zhao; Anton Ragni
- One-line primary result: Adaptive decoding strategies, such as Top-K and duration-guided decoding, consistently outperform fixed orders like left-to-right (l2r) or right-to-left (r2l) in autoregressive speech synthesis.

## Executive Summary
This paper investigates how decoding order impacts autoregressive speech synthesis by integrating the masked diffusion model framework with text-to-speech synthesis. Through experiments on the LJSpeech dataset, the authors demonstrate that adaptive decoding strategies, such as Top-K and duration-guided decoding, consistently outperform fixed orders like left-to-right (l2r) or right-to-left (r2l), with duration-guided decoding achieving the best MCD of 3.98, logF0 of 5.67, and UTMOSv2 of 3.98. The authors also show that even 1-bit quantization of Mel-spectrograms preserves partial intelligibility, and that varying the randomness in decoding order affects synthesis quality, with intermediate randomness yielding the best pitch preservation. These findings highlight that left-to-right decoding is suboptimal and that adaptive strategies can significantly improve speech synthesis quality.

## Method Summary
The authors integrate the masked diffusion model framework with text-to-speech synthesis, proposing adaptive decoding strategies such as Top-K and duration-guided decoding. These strategies adaptively select the next token to decode based on conditional probabilities or phoneme durations, contrasting with traditional fixed orders like left-to-right or right-to-left. The experiments focus on Mel-spectrogram reconstruction using the LJSpeech dataset, evaluating quality with metrics such as MCD, logF0 error, and UTMOSv2, and exploring the effects of quantization and randomness on decoding performance.

## Key Results
- Adaptive decoding strategies (Top-K and duration-guided) consistently outperform fixed orders (l2r, r2l) in Mel-spectrogram reconstruction, with duration-guided decoding achieving the best MCD of 3.98, logF0 of 5.67, and UTMOSv2 of 3.98.
- Even 1-bit quantization of Mel-spectrograms preserves partial intelligibility.
- Intermediate randomness in decoding order yields the best pitch preservation, while too much randomness degrades synthesis quality.

## Why This Works (Mechanism)
The effectiveness of adaptive decoding strategies stems from their ability to dynamically select the most informative or relevant tokens at each step, rather than following a predetermined fixed order. This flexibility allows the model to better capture dependencies in the speech signal, especially when guided by phoneme durations or conditional probabilities, leading to improved reconstruction of both spectral and pitch information. The integration of diffusion models with autoregressive decoding enables smoother and more accurate sample generation, while the introduction of randomness in token selection can help the model explore alternative synthesis paths, though excessive randomness can introduce instability.

## Foundational Learning
- **Masked Diffusion Models**: Why needed—to generate speech by iteratively denoising masked tokens, enabling flexible decoding orders. Quick check—verify that the diffusion process correctly reconstructs Mel-spectrograms with varying mask ratios.
- **Autoregressive Decoding**: Why needed—to sequentially generate speech tokens conditioned on previously generated content. Quick check—ensure each step's output is properly conditioned on prior tokens.
- **Mel-spectrogram Reconstruction**: Why needed—to transform text into speech via intermediate acoustic representations. Quick check—validate that reconstructed spectrograms match ground truth in both spectral and pitch domains.
- **Decoding Order Strategies**: Why needed—to optimize the sequence in which tokens are generated for better synthesis quality. Quick check—compare fixed versus adaptive strategies using standard metrics (MCD, logF0).
- **Quantization Effects**: Why needed—to assess the impact of reduced precision on speech intelligibility and quality. Quick check—measure intelligibility and quality degradation with different bit levels.
- **Randomness in Decoding**: Why needed—to explore the effect of stochasticity on synthesis diversity and fidelity. Quick check—vary randomness levels and measure impact on pitch preservation and overall quality.

## Architecture Onboarding

### Component Map
Text input -> Phoneme duration extraction -> Masked diffusion model (with token masking and conditional probability estimation) -> Adaptive decoding (Top-K or duration-guided) -> Mel-spectrogram output -> Waveform synthesis (via vocoder)

### Critical Path
Text -> Phoneme durations -> Masked diffusion model (inference) -> Adaptive decoding (Top-K/duration-guided) -> Mel-spectrogram reconstruction

### Design Tradeoffs
- **Adaptive vs. Fixed Decoding**: Adaptive strategies (Top-K, duration-guided) offer better reconstruction quality but may introduce computational overhead due to dynamic token selection.
- **Randomness in Decoding**: Introducing randomness can improve diversity and pitch preservation but risks instability if too high.
- **Quantization**: Lower bit quantization reduces model size and inference cost but may compromise intelligibility and naturalness.

### Failure Signatures
- **Low MCD/F0 Error**: May indicate overfitting or insufficient model capacity to capture speech variability.
- **Degraded Intelligibility with High Randomness**: Suggests the model cannot reliably generate coherent speech under excessive stochasticity.
- **Performance Drop with 1-bit Quantization**: Highlights the limits of extreme compression for speech synthesis.

### First Experiments
1. Replicate fixed (l2r, r2l) versus adaptive (Top-K, duration-guided) decoding on LJSpeech, comparing MCD and logF0.
2. Vary mask ratios and randomness levels in adaptive decoding, measuring their impact on pitch preservation and overall quality.
3. Apply 1-bit quantization to Mel-spectrograms and assess intelligibility and reconstruction quality.

## Open Questions the Paper Calls Out
None

## Limitations
- The study is limited to a single, relatively small dataset (LJSpeech), raising concerns about generalization to other voices, languages, or larger-scale datasets.
- Analysis is restricted to Mel-spectrogram reconstruction, lacking direct perceptual evaluations of naturalness and intelligibility beyond standard metrics.
- The paper does not thoroughly explore computational trade-offs or real-time applicability of adaptive decoding strategies for practical deployment.

## Confidence
- Claim: Adaptive decoding orders (Top-K, duration-guided) consistently outperform fixed orders such as left-to-right.
  - Confidence: Medium
    - Reason: Improvements are robust within the experimental setup, but broader generalization is uncertain.
- Claim: Even 1-bit quantization preserves partial intelligibility.
  - Confidence: Medium
    - Reason: Based on indirect evidence; lacks direct perceptual validation.
- Claim: Intermediate randomness in decoding order yields the best pitch preservation.
  - Confidence: Medium
    - Reason: Effect is subtle and context-dependent.

## Next Checks
1. Replicate experiments on multiple datasets, including multi-speaker and multilingual corpora, to assess robustness and generalization.
2. Conduct listener studies to directly evaluate naturalness, intelligibility, and preference for adaptive versus fixed decoding orders.
3. Measure and compare inference speed and computational efficiency for adaptive decoding strategies under realistic deployment conditions.