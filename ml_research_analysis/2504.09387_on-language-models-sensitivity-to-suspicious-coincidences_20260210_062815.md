---
ver: rpa2
title: On Language Models' Sensitivity to Suspicious Coincidences
arxiv_id: '2504.09387'
source_url: https://arxiv.org/abs/2504.09387
tags:
- hypothesis
- cities
- hypotheses
- input
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) exhibit
  human-like sensitivity to suspicious coincidences during inductive reasoning. Humans
  tend to favor specific hypotheses over general ones when generalizing from data,
  assuming the data was sampled in a specific way.
---

# On Language Models' Sensitivity to Suspicious Coincidences

## Quick Facts
- arXiv ID: 2504.09387
- Source URL: https://arxiv.org/abs/2504.09387
- Authors: Sriram Padmanabhan; Kanishka Misra; Kyle Mahowald; Eunsol Choi
- Reference count: 29
- Key outcome: LLMs lack natural suspicious coincidence sensitivity in zero-shot settings but achieve near-human performance when explicitly provided with hypothesis spaces

## Executive Summary
This paper investigates whether large language models exhibit human-like sensitivity to suspicious coincidences during inductive reasoning. Humans tend to favor specific hypotheses over general ones when generalizing from data, assuming the data was sampled in a specific way. The authors tested this by analyzing LLMs' performance on the number game (predicting if a number belongs to a set) and a newly created city game. Five models (Llama3-8b, GPT-4o, etc.) were evaluated using three prompting methods: zero-shot, chain-of-thought, and knowledge-rich (explicitly providing hypothesis information). Results showed that LLMs do not naturally exhibit suspicious coincidence effects in zero-shot settings, often defaulting to "yes" predictions. However, when provided with knowledge-rich prompts that explicitly guide reasoning over specific hypotheses, models like GPT-4o showed near-human-level sensitivity to suspicious coincidences, especially in the number domain.

## Method Summary
The study evaluates five LLMs (Llama3-8B, Mistral-7B, Gemma-2-9B, GPT-3.5, GPT-4o) on two inductive reasoning tasks: a number game using 146 input sets from existing data, and a city game using 181 input sets derived from 50 base sets of 4 cities. Models are tested under three prompting conditions: zero-shot, chain-of-thought (with 2 in-context examples), and knowledge-rich (explicit attributes provided). Performance is measured using F1 score between predictions and the "smallest hypothesis" ground truth, plus preference percentage for smallest hypothesis. Results are compared against random, always-yes, and Bayesian baselines. The hypothesis spaces include 33 number hypotheses and 18 city hypotheses, with data available at the GitHub repository.

## Key Results
- Zero-shot LLMs produce "Yes" predictions 52-85% of the time, significantly higher than human 24% baseline
- Knowledge-rich prompting enables near-human-level performance, with GPT-4o achieving 84% F1 on numbers
- Chain-of-thought prompting works only when models have sufficient parametric knowledge (effective for GPT-4o, not for Mistral/Llama)
- Model performance shows increased sensitivity to suspicious coincidences as input set size grows, but only with proper prompting

## Why This Works (Mechanism)

### Mechanism 1: Hypothesis Landscape Externalization
Providing LLMs with explicit descriptions of valid hypotheses constrains their reasoning to a specific set of rules, mimicking Bayesian models. By listing attributes of inputs (e.g., "64 is even, a power of 2..."), the prompt reduces the search space from infinite patterns to finite pre-defined sets, externalizing the likelihood calculation required for the size principle.

### Mechanism 2: Retrieval-Dependent Chain-of-Thought (CoT)
CoT prompting facilitates suspicious coincidence sensitivity only when the model's parametric knowledge is robust enough to support retrieval of specific hypothesis attributes. CoT forces the model to verbalize intermediate steps, and if the model "knows" the facts (e.g., GPT-4o knowing 64 is a power of 2), CoT successfully retrieves this and behaves like the "Knowledge" prompt.

### Mechanism 3: Zero-Shot Default Heuristics
Without explicit guidance, LLMs default to broad heuristics (like "Always Yes" bias) because they fail to spontaneously infer the pragmatic intent of the task. In zero-shot settings, models don't assume data was "sampled" specifically to teach a narrow rule, treating input as generic sequence rather than informative communicative act.

## Foundational Learning

- **Concept: The Size Principle (Bayesian Inference)**
  - Why needed: Mathematical foundation of "suspicious coincidence" effect - explains why {2, 4, 8} implies "powers of 2" over "even numbers" (likelihood = 1/|h|^n)
  - Quick check: If a program outputs {16, 32}, why is "powers of 2" more likely than "even numbers" despite both being true?

- **Concept: Hypothesis Space vs. Data**
  - Why needed: Distinguishes between data (numbers/cities given) and hypothesis space (possible rules); central argument is LLMs fail when they don't "see" the hypothesis space
  - Quick check: In city game, what constitutes the "hypothesis space" and what constitutes the "data"?

- **Concept: Parametric vs. Contextual Knowledge**
  - Why needed: Diagnoses why CoT works for GPT-4o but not Mistral - one relies on internal weights (parametric), other on prompt injection (contextual)
  - Quick check: Does the "Knowledge Prompt" rely on what the model learned during pre-training or what is written in the prompt?

## Architecture Onboarding

- **Component map:** Input Sets (X={x₁,...,xₙ}) -> Hypothesis Engine (valid rules lookup) -> Model Interface (0-shot/CoT/Knowledge prompts) -> Evaluator (F1 calculation)

- **Critical path:** 1. Load Input Sets (e.g., {16, 32}) 2. Identify "Smallest Valid Hypothesis" via Hypothesis Engine (e.g., "powers of 2") 3. Generate Prompt (injecting knowledge if applicable) 4. Get Model Verdict (Yes/No for query y) 5. Compute F1: Does model's Yes/No pattern align with Smallest Hypothesis?

- **Design tradeoffs:** Knowledge Prompt: High validity (matches humans) but requires manual feature engineering of hypothesis space (not scalable). CoT Prompt: Scalable and autonomous but highly dependent on model's pre-existing parametric knowledge (brittle for smaller models).

- **Failure signatures:** "Always Yes" Bias: Model returns "Yes" >80% of time (Zero-shot failure mode). Flat Sensitivity: F1 score does not increase as input set size (n) increases (fails to leverage accumulating evidence).

- **First 3 experiments:** 1. Sanity Check (Zero-Shot): Run small batch of Number Game inputs on GPT-4o 0-shot to verify "Always Yes" bias exists. 2. Ablation (Knowledge vs. CoT): Compare GPT-4o vs. Mistral-7b on Knowledge prompt to isolate reasoning ability from parametric knowledge gaps. 3. Scaling Law Check: Plot F1 score against Input Set Size (1, 2, 3, 4) to confirm "Suspicious Coincidence" curve (sensitivity should rise as examples increase).

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs be trained (e.g., via distillation) to exhibit sensitivity to suspicious coincidences in a zero-shot manner without explicit access to the hypothesis landscape? The authors state their findings "motivate methods that can make LMs sensitive to human-like suspicious coincidence effects in a zero-shot manner—e.g., by using distillation."

### Open Question 2
Can LLMs autonomously generate the relevant hypothesis space necessary for inductive reasoning without being explicitly provided with category information? The study relies on manually defined hypotheses or knowledge-rich prompts; the inability of zero-shot LLMs to perform well suggests they lack explicit internal representation of this space.

### Open Question 3
What are the mechanistic internal differences between model states that successfully process communicative goals (suspicious coincidences) versus those that do not? The authors suggest their "results provide rich empirical evidence that could be used to further study mechanistic differences between LM states... which differ in their sensitivity to communicative goals."

## Limitations

- Externalization mechanism's scalability is uncertain as it requires manual hypothesis space engineering that may not generalize to domains lacking clear, discrete attributes
- Evaluation methodology's reliance on binary yes/no predictions may not capture nuanced reasoning patterns that fall between strict answers
- Claims of placing LLMs "on level ground with Bayesian models" are aspirational and only demonstrated in specific test cases rather than general Bayesian equivalence

## Confidence

- **High Confidence**: Zero-shot LLMs exhibit "always yes" bias rather than suspicious coincidence sensitivity (robust across multiple models)
- **Medium Confidence**: Knowledge-rich prompts can elevate LLM performance to near-human levels (depends heavily on prompt engineering quality)
- **Low Confidence**: Knowledge prompts place LLMs "on level ground with Bayesian models" (only demonstrated in specific test cases)

## Next Checks

1. **Domain Transferability Test**: Apply knowledge-rich prompting to a domain with less discrete attributes (e.g., artistic styles or emotional categories) to assess scalability of manual hypothesis space engineering

2. **Fuzzy Membership Evaluation**: Modify evaluation framework to handle probabilistic rather than binary predictions, measuring how LLMs perform when categories have graded membership

3. **Human Baseline Replication**: Replicate human suspicious coincidence experiments within the study's framework to ensure "near-human-level" performance claims are empirically grounded rather than inferred from secondary sources