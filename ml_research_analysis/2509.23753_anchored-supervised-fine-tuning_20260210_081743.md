---
ver: rpa2
title: Anchored Supervised Fine-Tuning
arxiv_id: '2509.23753'
source_url: https://arxiv.org/abs/2509.23753
tags:
- asft
- learning
- fine-tuning
- training
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical analysis of Dynamic Fine-Tuning
  (DFT) within the reward-weighted regression framework, showing it achieves tighter
  RL lower bounds than standard SFT but suffers from distributional drift. To address
  this, the authors propose Anchored Supervised Fine-Tuning (ASFT), which augments
  DFT's reweighting with KL regularization to preserve tightness while ensuring stability.
---

# Anchored Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2509.23753
- Source URL: https://arxiv.org/abs/2509.23753
- Authors: He Zhu; Junyou Su; Peng Lai; Ren Ma; Wenjia Zhang; Linyi Yang; Guanhua Chen
- Reference count: 28
- Key outcome: ASFT outperforms both SFT and DFT across mathematical reasoning (+4.85 to +17.89 points), medical knowledge (+8.28 to +10.65 points), and code generation tasks while achieving better generalization with minimal computational overhead.

## Executive Summary
This paper addresses the limitations of Dynamic Fine-Tuning (DFT) in reward-weighted regression by introducing Anchored Supervised Fine-Tuning (ASFT). The authors demonstrate that while DFT achieves tighter reinforcement learning lower bounds than standard supervised fine-tuning (SFT), it suffers from distributional drift. ASFT augments DFT's reward-weighted reweighting with KL regularization to preserve tightness while ensuring stability. The method consistently outperforms both baselines across multiple domains including mathematical reasoning, medical knowledge, and code generation.

## Method Summary
ASFT builds upon the reward-weighted regression framework by combining Dynamic Fine-Tuning's reward-based reweighting with KL regularization to prevent distributional drift. The method maintains the theoretical advantages of DFT's tighter RL lower bounds while addressing its stability issues through anchor-based regularization. During fine-tuning, ASFT reweights training samples based on their rewards while simultaneously constraining the updated policy distribution to stay close to an anchor distribution, preventing the catastrophic drift observed in pure reward-weighted approaches.

## Key Results
- Mathematical reasoning tasks: +4.85 to +17.89 points improvement over SFT and DFT
- Medical knowledge tasks: +8.28 to +10.65 points improvement over baselines
- Code generation tasks: consistently outperforms both SFT and DFT with better generalization
- Minimal computational overhead compared to standard fine-tuning approaches

## Why This Works (Mechanism)
ASFT works by combining the benefits of reward-weighted regression (tighter RL bounds) with KL regularization (stability). The reward weighting component ensures the model focuses on high-reward samples, while the KL regularization term prevents the policy from drifting too far from the original distribution. This dual approach allows ASFT to maintain the theoretical advantages of dynamic fine-tuning while avoiding the distributional instability that plagues pure reward-weighted methods.

## Foundational Learning
- **Reward-weighted regression**: A framework for incorporating reward signals into supervised learning; needed to understand how ASFT improves upon standard SFT
- **KL regularization**: A technique for constraining distribution divergence; needed to understand how ASFT prevents distributional drift
- **Reinforcement learning lower bounds**: Theoretical guarantees for policy improvement; needed to understand the theoretical motivation for ASFT
- **Distributional stability**: The concept of maintaining consistent output distributions during training; needed to understand why ASFT prevents catastrophic drift
- **Anchor distributions**: Reference distributions used for regularization; needed to understand how ASFT constrains policy updates

Quick check: Verify that the KL regularization term effectively constrains the policy distribution by monitoring KL divergence between consecutive updates.

## Architecture Onboarding

**Component map:** Input data -> Reward computation -> Weighted sampling -> Model update -> KL regularization -> Output

**Critical path:** Reward computation → Weighted sampling → Model update → KL regularization → Output

**Design tradeoffs:** ASFT trades some potential reward maximization (compared to pure reward weighting) for stability through KL regularization. The anchor distribution provides a safety net but may limit exploration in early training stages.

**Failure signatures:** If KL regularization is too strong, the model may not adapt sufficiently to reward signals. If too weak, distributional drift may occur. Poor reward estimation can lead to ineffective weighting.

**First experiments:**
1. Test ASFT with synthetic reward distributions to verify KL regularization effectiveness
2. Compare performance with varying KL regularization strengths
3. Evaluate distributional stability by monitoring KL divergence between updates

## Open Questions the Paper Calls Out
The paper acknowledges several open questions including the theoretical guarantees of ASFT in non-stationary reward settings and with heterogeneous reward distributions. The authors note that while ASFT demonstrates strong empirical performance, the distributional stability claims require further validation in more diverse reward scenarios.

## Limitations
- Theoretical guarantees may not hold in non-stationary reward settings
- Performance depends on quality of reward estimation
- KL regularization strength requires careful tuning
- Computational overhead claims need independent verification across different hardware

## Confidence

| Claim | Confidence |
|-------|------------|
| Empirical performance improvements over SFT and DFT | High |
| Theoretical framework and KL regularization benefits | Medium |
| Distributional stability claims | Medium |
| Computational overhead claims | Low |

## Next Checks
1. Test ASFT performance with non-stationary reward distributions and temporally varying preferences to validate distributional stability claims
2. Conduct ablation studies to isolate the impact of KL regularization versus reweighting components
3. Verify computational overhead measurements across different hardware configurations and batch sizes to confirm the minimal overhead claim