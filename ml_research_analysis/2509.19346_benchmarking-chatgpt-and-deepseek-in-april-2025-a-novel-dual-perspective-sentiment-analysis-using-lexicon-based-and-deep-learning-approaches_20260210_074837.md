---
ver: rpa2
title: 'Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective
  Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches'
arxiv_id: '2509.19346'
source_url: https://arxiv.org/abs/2509.19346
tags:
- reviews
- chatgpt
- deepseek
- sentiment
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a dual-perspective sentiment analysis of ChatGPT
  and DeepSeek user reviews, integrating lexicon-based TextBlob analysis with deep
  learning classification models (CNN and Bi-LSTM). A dataset of 4,000 user reviews
  was collected from the Google Play Store, balanced using RandomOverSampler, and
  evaluated using a 1,700-review test set.
---

# Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches

## Quick Facts
- arXiv ID: 2509.19346
- Source URL: https://arxiv.org/abs/2509.19346
- Reference count: 29
- Key outcome: Dual-perspective sentiment analysis combining TextBlob and deep learning (CNN, Bi-LSTM) on 4,000 user reviews; CNN achieved 96.41% accuracy and near-perfect negative review classification.

## Executive Summary
This study benchmarks user sentiment toward ChatGPT and DeepSeek using a dual-perspective approach that integrates lexicon-based TextBlob analysis with deep learning classification. A dataset of 4,000 user reviews from the Google Play Store was balanced using RandomOverSampler and evaluated using a 1,700-review test set. Results show ChatGPT received more positive sentiment than DeepSeek, with CNN outperforming Bi-LSTM at 96.41% accuracy and near-perfect negative review classification. The methodology establishes a new standard for LLM sentiment evaluation, combining qualitative user feedback analysis with quantitative deep learning classification to provide practical insights for improving AI-driven application design and user experience.

## Method Summary
The study collected 4,000 user reviews (2,000 per app) from the Google Play Store, labeled using TextBlob polarity thresholds (Positive: >0.1, Negative: < -0.1, Neutral: [-0.1, 0.1]). Reviews were preprocessed (lowercase, non-alphabetic removal, tokenization, padding) and balanced using RandomOverSampler. A CNN and Bi-LSTM were trained on 80% of the balanced data with early stopping, evaluated on a 20% balanced test set (567 Negative, 567 Neutral, 566 Positive reviews). Models were assessed using accuracy, precision, recall, and F1-score per class.

## Key Results
- CNN achieved 96.41% accuracy, significantly outperforming Bi-LSTM (93.29%) for short, sentiment-dense user reviews.
- ChatGPT received more positive sentiment than DeepSeek (69.35% vs. 68.85% positive reviews via TextBlob).
- CNN demonstrated near-perfect negative review classification (precision: 99.06%, recall: 99.65%, F1: 99.35%).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CNN outperforms Bi-LSTM for short, phrase-based user reviews due to more efficient local n-gram feature extraction.
- **Mechanism:** 1D convolutional filters with kernel size 5 slide across token sequences, detecting sentiment-bearing phrases regardless of absolute position. Global max pooling retains only the strongest activation per filter, creating a compact, position-invariant representation. This suits short reviews where sentiment is often concentrated in key phrases rather than distributed across long dependencies.
- **Core assumption:** Sentiment signals in app reviews are localized to n-grams (e.g., "very helpful", "can't connect") rather than requiring long-range sequential context.
- **Evidence anchors:**
  - [Section 9.1]: "CNN generalizes well and can leverage its capability to capture local n-gram features through convolutional filters, which helps reduce sentiment ambiguity."
  - [Section 10]: "CNN performed well due to the shortness of user reviews, which are phrase-based and sentiment-rich, making them more suitable for convolutional feature extraction."
  - [Corpus]: Weak direct evidence on CNN vs Bi-LSTM for short reviews specifically; related work (Alsaleh et al., 2024) shows 94% accuracy using hybrid BERT+Bi-LSTM+CNN, but not a direct comparison.
- **Break condition:** If reviews were longer (>200 tokens) with complex discourse structure requiring cross-sentence reasoning, Bi-LSTM or transformer attention may become competitive or superior.

### Mechanism 2
- **Claim:** RandomOverSampler enables reliable evaluation across minority sentiment classes by preventing majority-class bias.
- **Mechanism:** Minority class samples (Negative and Neutral) are replicated until all classes have equal representation. The model sees each class equally during training, and the test set (1,700 reviews: 567 Negative, 567 Neutral, 566 Positive) provides unbiased performance estimates per class.
- **Core assumption:** Replicating existing minority samples approximates their true distribution sufficiently; synthetic variation is not required.
- **Evidence anchors:**
  - [Abstract]: "A Dataset of 4,000 authentic user reviews was collected, which were carefully preprocessed and subjected to oversampling to achieve balanced classes."
  - [Section 5.1]: "The test set consisted of 1,700 reviews, and the proportion of each of the sentiment classes (567 Negative, 567 Neutral, and 566 Positive reviews) was close to equal."
  - [Corpus]: No direct comparison of oversampling vs. undersampling for sentiment analysis found in neighbors; standard practice is noted but not empirically validated here.
- **Break condition:** If minority class samples are noisy or unrepresentative, oversampling amplifies errors. If severe overfitting occurs on minority patterns, SMOTE-like synthesis or data augmentation may be preferable.

### Mechanism 3
- **Claim:** Combining lexicon-based scoring with deep learning classification captures complementary dimensions of user perception.
- **Mechanism:** TextBlob provides interpretable polarity scores based on predefined sentiment lexicons, revealing overall sentiment distribution (ChatGPT: 69.35% positive vs. DeepSeek: 68.85%). Deep learning classifiers learn contextual patterns beyond lexicon coverage, achieving higher discriminative accuracy (CNN: 96.41%). Together, they offer both explainable high-level trends and fine-grained classification.
- **Core assumption:** Lexicon scores and learned embeddings encode partially non-overlapping signal; combining them yields a more complete picture than either alone.
- **Evidence anchors:**
  - [Abstract]: "Unlike prior research, which focuses on either lexicon-based strategies or predictive deep learning models in isolation, this study conducts an extensive investigation..."
  - [Section 2]: "This combination of both techniques offers a more reliable, balanced, and sophisticated approach to user perception and model performance."
  - [Corpus]: Related work (Samanmali et al., 2025) uses TF-IDF with ANN/LSTM/SVM (85% accuracy), but no direct dual-perspective comparison exists in corpus neighbors.
- **Break condition:** If lexicon vocabulary mismatches domain-specific language (e.g., technical AI terms), TextBlob may mislabel sentiment, and the dual approach adds noise rather than signal.

## Foundational Learning

- **Concept:** 1D Convolutions for Text
  - **Why needed here:** Understanding how kernel size 5 extracts 5-gram phrases and why this suits short, sentiment-dense reviews.
  - **Quick check question:** Given the input "The app crashes constantly," what 5-gram patterns would a kernel size 5 detect, and which might carry negative sentiment?

- **Concept:** Class Imbalance and Oversampling
  - **Why needed here:** Original data was skewed (Positive dominant); oversampling enabled fair per-class evaluation.
  - **Quick check question:** If you oversample a minority class 10× but the test set is also oversampled, what validity risk emerges in performance estimates?

- **Concept:** Lexicon-Based vs. Learned Representations
  - **Why needed here:** TextBlob relies on fixed word scores; CNN learns embeddings. Each has blind spots.
  - **Quick check question:** A review says "This app is sick!" — how would TextBlob (lexicon) vs. a trained CNN likely interpret this differently?

## Architecture Onboarding

- **Component map:**
  Data Ingestion -> Google Play Scraper (2,000 reviews per app, deduplicated) -> Preprocessing (lowercase, non-alphabetic removal, tokenization, padding) -> Labeling (TextBlob polarity thresholds) -> Balancing (RandomOverSampler) -> Split (72% train, 8% validation, 20% test) -> Models (CNN and Bi-LSTM) -> Evaluation (accuracy, precision, recall, F1 per class; confusion matrices)

- **Critical path:**
  1. Correct preprocessing (padding length must match model input)
  2. Proper oversampling applied *before* train/test split to prevent leakage (verify split was on balanced data)
  3. Early stopping on validation loss to prevent overfitting

- **Design tradeoffs:**
  - CNN vs. Bi-LSTM: CNN is faster and better for short text; Bi-LSTM captures longer dependencies but underperforms here.
  - Oversampling vs. class-weighted loss: Oversampling is explicit and interpretable; weighted loss is implicit but may obscure per-class dynamics.
  - 3-class vs. binary classification: 3-class retains nuance (Neutral) but increases confusion boundaries.

- **Failure signatures:**
  - High accuracy but low recall on Negative/Neutral → model biased toward Positive (check class balance in training).
  - Validation loss diverges from training loss → overfitting (increase dropout or reduce model capacity).
  - TextBlob and CNN disagree on many samples → lexicon-domain mismatch or labeling noise.

- **First 3 experiments:**
  1. Replicate CNN training on the balanced dataset; verify 96.41% accuracy and inspect confusion matrix for Neutral/Positive confusion.
  2. Ablate oversampling: train on original imbalanced data, compare per-class F1 scores to balanced baseline.
  3. Test both models on a held-out temporal slice (e.g., most recent 200 reviews) to assess generalization to newer user feedback.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the reasoning-optimized DeepSeek-R1a model perform specifically on sentiment analysis tasks compared to standard instruction-tuned models?
- **Basis in paper:** [explicit] The literature review notes that while DeepSeek-R1a utilizes reinforcement learning for contextual reasoning, "their efficacy in sentiment analysis has not yet been examined."
- **Why unresolved:** This study benchmarked the general DeepSeek application reviews, not the specific capabilities of the R1a reasoning architecture mentioned in the background.
- **What evidence would resolve it:** A benchmark comparison of DeepSeek-R1a against the standard DeepSeek or ChatGPT models using a standardized sentiment analysis dataset.

### Open Question 2
- **Question:** Does a Bi-LSTM architecture outperform CNN in classifying sentiment when applied to longer, more complex user feedback rather than short mobile app reviews?
- **Basis in paper:** [inferred] The authors state that CNN outperformed Bi-LSTM likely due to the "shortness of user reviews," implying that Bi-LSTM "can be more applicable to more complex or longer textual data."
- **Why unresolved:** The current dataset consisted of concise Google Play Store reviews, which favor the local feature extraction of CNNs over the sequential processing of LSTMs.
- **What evidence would resolve it:** Replicating the study using long-form reviews (e.g., detailed blog posts or forum threads) to test if the sequential context modeling of Bi-LSTM yields higher accuracy.

### Open Question 3
- **Question:** Do the observed sentiment distributions and key linguistic features generalize to non-English speaking users or different cultural contexts?
- **Basis in paper:** [inferred] The methodology explicitly restricted data collection to "English-language reviews submitted by users located in the United States."
- **Why unresolved:** The study acknowledges that user experience is influenced by training data and localization, but the data excludes non-US users who may face different "connectivity and service problems" (as hinted at regarding DeepSeek).
- **What evidence would resolve it:** Applying the dual-perspective approach to a multilingual dataset or reviews from regions outside the United States to compare functional and experiential gaps.

## Limitations
- **Preprocessing Ambiguity**: The maximum sequence length for padding is unspecified, which is critical for reproducibility and model architecture definition.
- **Sampling Procedure Gap**: The paper does not clarify whether oversampling was applied before or after train/test split, raising potential data leakage concerns.
- **Temporal Validity**: User sentiment toward LLMs evolves rapidly; findings from April 2025 may have limited shelf life without replication on newer reviews.

## Confidence
- **High Confidence**: CNN architecture design, balanced test set construction, and per-class evaluation metrics.
- **Medium Confidence**: Overall superiority of CNN over Bi-LSTM for short review classification; dual-perspective methodology value.
- **Low Confidence**: Exact preprocessing parameters, oversampling implementation details, and TextBlob labeling reliability for AI-specific language.

## Next Checks
1. **Implement exact data pipeline**: Replicate the preprocessing, oversampling (applied strictly post-split), and model training with CNN; verify 96.41% accuracy and inspect confusion matrices for Neutral/Positive confusion patterns.
2. **Temporal generalization test**: Apply trained models to a temporally distinct subset (e.g., most recent 200 reviews) to assess whether performance holds as user sentiment evolves.
3. **Ablation on sampling method**: Compare balanced training results against original imbalanced data training to quantify the impact of oversampling on per-class F1 scores and potential overfitting.