---
ver: rpa2
title: Attention-Enhanced Graph Filtering for False Data Injection Attack Detection
  and Localization
arxiv_id: '2601.18981'
source_url: https://arxiv.org/abs/2601.18981
tags:
- power
- fdia
- detection
- data
- localization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ACEOT, a novel framework for joint false data
  injection attack (FDIA) detection and localization in power systems. The proposed
  model integrates positional encoding, convolutional auto-regressive moving average
  (ARMAConv) graph filtering, and an Encoder-Only Transformer architecture to identify
  corrupted measurements in a manner consistent with the underlying physics and topology
  of the power grid.
---

# Attention-Enhanced Graph Filtering for False Data Injection Attack Detection and Localization

## Quick Facts
- arXiv ID: 2601.18981
- Source URL: https://arxiv.org/abs/2601.18981
- Authors: Ruslan Abdulin; Mohammad Rasoul Narimani
- Reference count: 39
- Primary result: ACEOT outperforms ARMAConv and ChebConv-based methods on IEEE-14 and IEEE-300 bus systems for FDIA detection and localization

## Executive Summary
This paper presents ACEOT, a novel framework for joint false data injection attack (FDIA) detection and localization in power systems. The proposed model integrates positional encoding, convolutional auto-regressive moving average (ARMAConv) graph filtering, and an Encoder-Only Transformer architecture to identify corrupted measurements in a manner consistent with the underlying physics and topology of the power grid. By combining topology-aware spectral filtering with a self-attention mechanism, ACEOT effectively captures both local and long-range spatial dependencies present in grid measurements.

Extensive evaluations conducted on standard IEEE-14 and IEEE-300 bus systems using realistic operating conditions derived from publicly available NYISO load data demonstrate that ACEOT consistently outperforms state-of-the-art ARMAConv and ChebConv-based methods in both detection and localization tasks. The results further indicate that the advantages of attention-based modeling become more pronounced as network size increases, highlighting the scalability of the proposed approach. Collectively, these findings confirm that ACEOT can reliably exploit global contextual information and long-range correlations in large-scale power systems, offering an effective and practical solution for online FDIA detection and localization.

## Method Summary
ACEOT combines learnable positional encoding, ARMAConv graph filtering, and an Encoder-Only Transformer to detect and localize FDIA attacks in power systems. The model takes as input power system measurements (active and reactive power) along with the grid topology, applies positional encoding to preserve node identity, processes features through ARMAConv layers for topology-aware spectral smoothing, and uses a Transformer encoder to capture long-range dependencies. The framework outputs node-level attack probabilities and aggregates them for graph-level detection. The model is trained using BCE-with-logits loss with class weighting, AdamW optimizer, and early stopping based on validation loss.

## Key Results
- ACEOT outperforms ARMAConv and ChebConv baselines on both IEEE-14 and IEEE-300 bus systems
- The advantage of attention-based modeling becomes more pronounced as network size increases
- ACEOT shows slightly lower detection rate but significantly lower false alarm rate compared to some baselines
- The model generalizes well to unseen attack types not present in training data

## Why This Works (Mechanism)

### Mechanism 1: Topology-Aware Spectral Smoothing (ARMAConv)
ARMAConv filters approximate graph spectral responses more robustly than Chebyshev polynomial filters, mitigating noise amplification and over-smoothing during feature aggregation. By iterating the recursive propagation rule with residual connections, the model aggregates neighbor information while retaining node-level discrimination, allowing it to isolate localized deviations caused by FDIAs in the measurement spectrum.

### Mechanism 2: Global Dependency Modeling (Self-Attention)
The Encoder-Only Transformer computes scaled dot-product attention across all node embeddings, allowing each node to directly attend to distant buses and identify coordinated attack patterns that don't share immediate topological links. This addresses the limitation of standard GNNs in capturing long-range correlations in large grids due to limited receptive fields.

### Mechanism 3: Node Identity Preservation (Positional Encoding)
Learnable embedding vectors are added to node features before convolution to re-inject node identity after spectral filtering homogenizes features of electrically similar buses. These vectors persist through the smoothing layers, ensuring distinct nodes remain separable in the latent space, which is critical for detecting attacks where injection magnitude is small relative to normal operating variance.

## Foundational Learning

- **Graph Signal Processing (GSP)**: Understanding how the normalized graph Laplacian defines signal propagation over grid structure is essential for interpreting ARMAConv's spectral filtering mechanism. Quick check: How do eigenvalues of the graph Laplacian relate to the "frequency" of a signal on the power grid?

- **The Attention Hypothesis**: Understanding why dot-product attention captures dependencies differently than convolution is crucial for grasping the hybrid architecture's rationale. Quick check: Why would self-attention be superior to a 2-hop GNN for detecting coordinated attacks on two electrically close but topologically distant buses?

- **Power System State Estimation (PSSE)**: Understanding the physics behind measurement residuals and AC power flow equations is required to interpret input features and attack models. Quick check: Why is it harder to detect an attack that satisfies non-linear constraints compared to random noise?

## Architecture Onboarding

- **Component map**: Input Features (P, Q) + Adjacency Matrix + Node Indices -> Projection + PE -> ARMAConv layers -> Encoder-Only Transformer -> Linear classification layer (Sigmoid)

- **Critical path**: The flow depends on the integrity of the Positional Encoding. If positional embeddings are not successfully added to features before ARMA layers, the smoothing effect will likely wash out distinct attack signatures required for localization.

- **Design tradeoffs**: The hybrid architecture trades off deeper GNN layers for a Transformer to get "depth" of reasoning without "depth" of graph convolution, addressing over-smoothing concerns. ACEOT is tuned for "cautious but accurate" behavior with slightly lower detection rate but significantly lower false alarm rate.

- **Failure signatures**: Attention Collapse (uniform attention maps losing global discrimination), Overfitting to Topology (learnable PE failing on modified grid structures), and Over-smoothing in ARMAConv causing indistinguishable node representations.

- **First 3 experiments**: 1) Ablation Study: Train ARMAConv only, Transformer only, and ACEOT without PE to validate module contributions. 2) Scalability & Hops Analysis: Run inference on increasing grid sizes to verify Transformer maintains performance as long-range distances increase. 3) Robustness to Unseen Attacks: Evaluate specifically on held-out attack types (As, Ar) to test generalization claims.

## Open Questions the Paper Calls Out

### Open Question 1
How does ACEOT perform on power systems significantly larger than IEEE-300 (e.g., multi-thousand bus interconnects), and does the quadratic attention complexity become a bottleneck for real-time deployment? This remains untested as the paper demonstrates advantages with increasing network size but computational evaluation is limited to 300 buses.

### Open Question 2
How robust is ACEOT against adaptive adversaries who craft FDIAs specifically designed to evade the learned attention patterns and spectral filters? The attack models tested are not optimized to fool the detector, and adversarial robustness is not assessed.

### Open Question 3
How does ACEOT generalize to power system topologies and operating conditions not represented in training data (e.g., different network structures, renewable-heavy grids, islanded microgrids)? The model is trained and tested only on IEEE-14 and IEEE-300 with NYISO load profiles, leaving transferability to other grid configurations unknown.

## Limitations

- Several key implementation details remain unspecified, including exact ARMAConv stack and iteration counts, attack generation parameters, and positional encoding dimensions
- Computational complexity of quadratic attention scaling is not evaluated beyond 300-bus systems
- Adversarial robustness against attacks specifically designed to evade the detection mechanism is not assessed
- Generalization to power system topologies and operating conditions beyond the IEEE test cases is unknown

## Confidence

- **High confidence**: The core mechanism of combining ARMAConv with Transformer attention for FDIA detection, supported by comparative results on IEEE-14 and IEEE-300 systems
- **Medium confidence**: The effectiveness of positional encoding in preserving node identity through spectral filtering, as this is less extensively validated
- **Medium confidence**: Scalability claims, as results are limited to two specific grid sizes without broader network testing

## Next Checks

1. **Ablation study validation**: Reproduce the component isolation experiments (ARMAConv only, Transformer only, ACEOT without PE) to verify the claimed contributions of each module to overall performance.

2. **Scalability testing**: Extend evaluation to grid sizes larger than IEEE-300 to empirically test the claimed scalability advantages of the attention mechanism as network size increases.

3. **Generalization assessment**: Specifically evaluate model performance on the held-out attack types (As, Ar) to quantify generalization capability beyond training attack patterns.