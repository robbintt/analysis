---
ver: rpa2
title: Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?
arxiv_id: '2508.16695'
source_url: https://arxiv.org/abs/2508.16695
tags:
- traces
- should
- reasoning
- answer
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether the interpretability of Chain-of-Thought\
  \ (CoT) reasoning traces affects Large Language Model (LLM) performance. Through\
  \ supervised fine-tuning experiments on LLaMA and Qwen models using four types of\
  \ reasoning traces\u2014DeepSeek R1 traces, LLM-generated summaries and explanations\
  \ of R1 traces, and algorithmically generated verifiably correct traces\u2014the\
  \ study finds that R1 traces yield the highest task performance across three of\
  \ four models."
---

# Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?

## Quick Facts
- arXiv ID: 2508.16695
- Source URL: https://arxiv.org/abs/2508.16695
- Authors: Siddhant Bhambri; Upasana Biswas; Subbarao Kambhampati
- Reference count: 40
- Key outcome: Study finds R1 traces yield highest model performance but lowest human interpretability, revealing a disconnect between trace utility for models versus humans.

## Executive Summary
This paper investigates whether the interpretability of Chain-of-Thought (CoT) reasoning traces affects Large Language Model (LLM) performance. Through supervised fine-tuning experiments on LLaMA and Qwen models using four types of reasoning traces—DeepSeek R1 traces, LLM-generated summaries and explanations of R1 traces, and algorithmically generated verifiably correct traces—the study finds that R1 traces yield the highest task performance across three of four models. However, a human-subject study with 100 participants reveals that R1 traces are consistently rated as the least interpretable across all dimensions (predictability, comprehensibility, interpretability, and faithfulness) and impose the highest cognitive workload. These findings demonstrate a disconnect between the utility of reasoning traces for improving LLM performance and their interpretability for humans, suggesting that CoT traces should be optimized for model performance rather than end-user interpretability.

## Method Summary
The study employs supervised fine-tuning on LLaMA and Qwen models using four different reasoning trace types: DeepSeek R1 traces, LLM-generated summaries of R1 traces, LLM-generated explanations of R1 traces, and algorithmically generated verifiably correct traces. Models were fine-tuned on 2k-10k examples per task with frozen weights using QLoRA. Performance was evaluated on mathematical reasoning, logical reasoning, and digital circuit design tasks. A human-subject study with 100 participants rated the interpretability of these traces across four dimensions (predictability, comprehensibility, interpretability, and faithfulness) using a 7-point Likert scale, with cognitive workload measured via NASA-TLX.

## Key Results
- R1 traces achieved the highest task performance across three of four models tested
- Human participants consistently rated R1 traces as the least interpretable across all dimensions
- R1 traces imposed the highest cognitive workload on human subjects
- A clear disconnect exists between trace utility for models versus human interpretability

## Why This Works (Mechanism)
The study demonstrates that different types of reasoning traces have varying effects on both model performance and human interpretability. The R1 traces, while producing the best model performance, employ complex reasoning patterns that humans find difficult to follow. The algorithmically generated verifiably correct traces, while more interpretable to humans, don't provide the same performance benefits. This suggests that the reasoning patterns that work well for models may not align with human cognitive processes.

## Foundational Learning
- Chain-of-Thought (CoT) reasoning: Why needed - enables LLMs to solve complex problems through intermediate reasoning steps; Quick check - verify models can break down multi-step problems
- Supervised fine-tuning: Why needed - adapts pre-trained models to specific tasks using labeled data; Quick check - ensure training data quality and relevance
- QLoRA: Why needed - efficient parameter-efficient fine-tuning method for LLMs; Quick check - monitor memory usage and training stability
- NASA-TLX: Why needed - standardized cognitive workload assessment tool; Quick check - validate with pilot testing

## Architecture Onboarding
Component map: Training Data -> Model (LLaMA/Qwen) -> Fine-tuning Process -> Evaluation Metrics -> Human Subject Study

Critical path: Reasoning trace generation → Model fine-tuning → Performance evaluation → Human interpretability assessment

Design tradeoffs: Performance vs interpretability (R1 traces excel at performance but fail at interpretability)

Failure signatures: Low performance with high interpretability (algorithmically generated traces), high performance with low interpretability (R1 traces)

First experiments:
1. Baseline model performance without any reasoning traces
2. Fine-tuning with algorithmically generated traces as control
3. Human pilot study with simplified task to validate assessment methodology

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (100 participants) in human study may limit generalizability
- Focus on single complex task (digital circuit design) for human subject study
- Limited training data (2k-10k examples) may not capture full effects of trace interpretability

## Confidence
High confidence in:
- R1 traces yielding highest performance across three of four models
- R1 traces being rated lowest in interpretability by human subjects
- General disconnect between trace utility for models versus humans

Medium confidence in:
- Relative performance differences between the four trace types
- Generalizability of human subject findings to other task domains
- Conclusions about optimizing traces for model performance over human interpretability

## Next Checks
1. Replicate the study across a broader range of task types to assess whether the interpretability-performance tradeoff generalizes across different domains.

2. Conduct a longitudinal study with human subjects to examine how trace interpretability ratings might change with repeated exposure and experience with different trace types.

3. Implement a more comprehensive training regime with larger datasets and full fine-tuning to verify whether observed performance differences persist under different training conditions.