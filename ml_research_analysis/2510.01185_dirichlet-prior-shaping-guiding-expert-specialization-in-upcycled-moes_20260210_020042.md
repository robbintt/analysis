---
ver: rpa2
title: 'Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs'
arxiv_id: '2510.01185'
source_url: https://arxiv.org/abs/2510.01185
tags:
- expert
- experts
- dpsl
- training
- upcycling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses poor expert specialization in upcycled Mixture-of-Experts
  (MoE) vision-language models caused by naive weight replication during sparse upcycling.
  The authors introduce Dirichlet-Prior Shaping Loss (DPSL), a router regularization
  technique that shapes routing probability distributions by matching them to target
  Dirichlet priors, enabling fine-grained control over expert balance and specialization.
---

# Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs

## Quick Facts
- **arXiv ID**: 2510.01185
- **Source URL**: https://arxiv.org/abs/2510.01185
- **Reference count**: 40
- **One-line primary result**: DPSL improves upcycled MoE VLMs by shaping routing distributions via Dirichlet priors, achieving 35.92 average accuracy on Llama3.2-1B vs. 34.19 baseline.

## Executive Summary
This work addresses poor expert specialization in upcycled Mixture-of-Experts (MoE) vision-language models caused by naive weight replication during sparse upcycling. The authors introduce Dirichlet-Prior Shaping Loss (DPSL), a router regularization technique that shapes routing probability distributions by matching them to target Dirichlet priors, enabling fine-grained control over expert balance and specialization. DPSL allows encoding of inductive biases (e.g., modality-specific or task-specific specialization) without manual intervention and applies broadly to any module outputting categorical distributions. Experiments on upcycled MoE models (Qwen2-1.5B, Phi3-mini 3.8B, Llama3.2-1B) show DPSL consistently outperforms existing upcycling strategies and regularization techniques across standard vision-language benchmarks.

## Method Summary
DPSL applies a Cramér–von Mises-style loss matching empirical routing CDFs to target Beta marginals derived from a Dirichlet prior, explicitly penalizing low-confidence distributions clustered near 1/N. The concentration parameter α controls the shape of marginal Beta distributions, with lower α pushing distributions toward simplex corners (confident, sparse routing) and higher α concentrating near the mean (balanced routing). The method is applied during warm-up stages of three-stage training (projector training → dense fine-tuning → MoE fine-tuning with DPSL), then relaxed during task-specific fine-tuning to let downstream tasks refine specialization.

## Key Results
- DPSL achieves 35.92 average accuracy on Llama3.2-1B with 4 experts and top-2 routing versus 34.19 for the dense baseline
- Asymmetric Dirichlet priors for modality-specific routing achieve 36.18 average accuracy, outperforming symmetric DPSL (35.92) and manual modality routing (34.65)
- Ablation studies show optimal performance at concentration parameter α=0.75 for Llama3.2-1B and α=1.0 for larger models, with λ=0.01 providing the best regularization strength
- Analysis reveals DPSL fosters more confident, differentiated routing compared to methods producing low-confidence, peaked distributions near uniform selection

## Why This Works (Mechanism)

### Mechanism 1
Naive upcycling replicates FFN weights across experts, creating identical experts that routers cannot differentiate. DPSL applies a Cramér–von Mises-style loss matching empirical routing CDFs to target Beta marginals derived from a Dirichlet prior, explicitly penalizing low-confidence distributions clustered near 1/N. Core assumption: Router output distributions can be shaped via regularization to induce expert differentiation even when experts start functionally identical. Evidence: Visualization shows conventional methods produce router scores peaked near 0.25, while DPSL yields broader distributions across the probability range.

### Mechanism 2
The concentration parameter α controls the shape of marginal Beta distributions. Lower α (e.g., 0.75) pushes distributions toward simplex corners (confident, sparse routing); higher α (e.g., 1.5) concentrates near the mean (balanced routing). Core assumption: The marginal Beta distribution per expert captures the relevant statistics of router behavior; matching these marginals sufficiently shapes joint routing dynamics. Evidence: Ablation on Llama3.2-1B shows optimal performance at α = 0.75 vs. degraded performance at higher α values.

### Mechanism 3
By assigning different α vectors to different input sources (e.g., vision tokens vs. language tokens), the router learns to preferentially route certain inputs to certain experts, achieving soft specialization without hard constraints. Core assumption: The data has exploitable structure (modality, task family) that aligns with beneficial expert specialization patterns. Evidence: Modality-specific DPSL achieves 36.18 average accuracy on Llama3.2-1B, outperforming symmetric DPSL (35.92) and manual modality routing (34.65).

## Foundational Learning

- **Dirichlet distribution as a prior over probability simplices**: Why needed here: DPSL operates by matching routing distributions to Dirichlet-defined targets; understanding how α controls marginal shapes is essential for tuning. Quick check: Given a 4-expert system with symmetric α = 0.75, will the marginal Beta distributions push router probabilities toward corners or toward the center?

- **Cramér–von Mises criterion for distribution matching**: Why needed here: DPSL uses CDF-based distance rather than KL or moment matching; this is robust to small samples and differentiable via the PDF-to-CDF relationship. Quick check: Why does minimizing squared CDF distance encourage the empirical distribution to match the target Beta distribution?

- **Upcycling dense models to sparse MoE architectures**: Why needed here: The paper's intervention targets the specific failure mode of naive weight replication; understanding the upcycling setup clarifies why standard load-balancing is insufficient. Quick check: What happens to router gradients when all experts compute identical outputs for a given input?

## Architecture Onboarding

- **Component map**: Dense FFN -> Replicated experts with small noise -> Router (linear layer + softmax) -> Top-K gating -> MoE layer output. DPSL module computes empirical CDF per expert over batch tokens, compares to theoretical Beta CDF, aggregates squared differences weighted by λ.

- **Critical path**: Identify MoE layers in the upcycled model; initialize experts by replicating dense FFN weights with small noise; during warm-up, compute DPSL at each MoE layer; add L_DPS to primary loss; backprop through router weights; relax DPSL during finetuning.

- **Design tradeoffs**: α selection (lower α → more confident routing but risk of expert collapse; higher α → balanced but under-specialized); λ strength (λ = 0.01 works well; too low under-regularizes, too high over-constrains); granular vs. standard MoE (granular offers more routing flexibility but requires careful initialization).

- **Failure signatures**: Router scores clustering near 1/N (DPSL not applied or λ too low); expert collapse (one expert dominates, α too low or asymmetric priors poorly configured); training instability with Drop-Upcycling (reduce re-initialization ratio); degraded task performance with asymmetric priors (data partition may not align with beneficial specialization).

- **First 3 experiments**: 1) Baseline comparison: Upcycle Llama3.2-1B with 4 experts, compare sparse upcycling vs. load-balancing vs. DPSL on GQA and TextVQA. 2) α ablation: Same setup, vary α ∈ {0.75, 1.0, 1.25, 1.5}, plot routing score distributions. 3) Modality-specific priors: Apply asymmetric α vectors for vision vs. language tokens, compare to symmetric DPSL and manual modality routing.

## Open Questions the Paper Calls Out

### Open Question 1
Does DPSL improve expert specialization when training MoEs from scratch (not just upcycled MoEs)? Basis: The conclusion states "the principles of DPSL extend naturally to training MoEs from scratch," but all experiments only evaluate upcycled MoEs initialized from dense checkpoints. Why unresolved: No experiments validate whether DPSL benefits randomly initialized MoE training. What evidence would resolve it: Compare DPSL against standard regularization when training MoEs from scratch on the same vision-language benchmarks.

### Open Question 2
Can DPSL effectively shape outputs in other modules that produce categorical distributions beyond MoE routers? Basis: The authors claim DPSL is "a general tool applicable to any module that outputs categorical probability distributions, extending its utility beyond MoE training," but provide no empirical validation. Why unresolved: The paper only demonstrates DPSL on router distributions. What evidence would resolve it: Apply DPSL to other architectural components (e.g., multi-head attention, hierarchical classifiers) and report whether similar distribution-shaping benefits emerge.

### Open Question 3
What constitutes optimal data partitioning for task-specific expert specialization in VLMs? Basis: Section 3.4 shows task-specific DPSL underperforms symmetric DPSL; authors suggest this may result from "nonoptimal data subsets" or "over-constraining experts." Why unresolved: The four-category partition may not align with natural task boundaries that benefit from expert specialization. What evidence would resolve it: Systematic comparison of alternative data partitioning schemes to identify which task groupings yield the largest task-specialized upcycling gains.

### Open Question 4
How robust are DPSL performance gains across random seeds and extended training regimes? Basis: The limitation section states: "a primary limitation of our work is the inability to perform multiple seeds across the full matrix of backbones, granularities, and priors due to the expense of upcycled MoE training." Why unresolved: Single-run results may not capture variance; reported differences could be within noise margins. What evidence would resolve it: Multi-seed experiments with confidence intervals across key backbone-prior combinations.

## Limitations
- Limited evaluation scope: Only tested on 6 vision-language benchmarks with 1.5B-3.8B models, raising questions about scalability to larger models or different modalities
- Optimal hyperparameters are empirically determined: α values (0.75 for Llama3.2-1B, 1.0 for larger models) may not generalize across architectures or tasks
- Asymmetric priors show mixed results: Beneficial for modality-specific routing but not for task-specific splits, suggesting data partitioning quality critically affects performance
- Doesn't solve fundamental expert homogeneity: DPSL addresses router indecision but doesn't resolve that functionally identical experts may never specialize without additional interventions like partial re-initialization

## Confidence

- **High Confidence**: DPSL effectively shapes routing distributions away from uniform selection, improving expert differentiation in upcycled MoEs. The Cramér–von Mises-based regularization is mathematically sound and empirically validated.
- **Medium Confidence**: Asymmetric Dirichlet priors encode useful inductive biases for modality-specific routing. The optimal α values and λ strength are well-supported by ablation studies.
- **Low Confidence**: Claims about broader applicability to any categorical distribution outputs and superior performance across all potential upcycling scenarios. Limited evaluation scope and absence of cross-modal or larger model experiments reduce confidence.

## Next Checks

1. **Larger Model Scalability**: Apply DPSL to upcycle a 7B dense model (e.g., Qwen2-7B) to 4-expert MoE and evaluate on the same 6 benchmarks. Measure whether α = 1.0 remains optimal and if performance gains scale with model size.

2. **Layer-Specific Priors**: Implement per-layer DPSL with independent α vectors to test if routing behavior varies across transformer depths. Compare to single-global-prior performance and analyze expert utilization patterns.

3. **Cross-Modal Transfer**: Apply DPSL to upcycle a text-only LLM (e.g., Phi3-mini) to a MoE and evaluate on pure text tasks (e.g., MMLU, BBH). Test whether symmetric priors (α = 0.75) still outperform baselines in non-vision settings.