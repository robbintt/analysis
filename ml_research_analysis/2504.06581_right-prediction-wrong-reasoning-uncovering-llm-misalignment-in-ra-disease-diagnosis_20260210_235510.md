---
ver: rpa2
title: 'Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease
  Diagnosis'
arxiv_id: '2504.06581'
source_url: https://arxiv.org/abs/2504.06581
tags:
- pain
- arthritis
- diagnosis
- rheumatoid
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the diagnostic performance of large language
  models (LLMs) for rheumatoid arthritis using real-world patient data. While the
  models achieved high prediction accuracy (approximately 95%), medical experts found
  that nearly 68% of the reasoning provided by the models was incorrect.
---

# Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosis

## Quick Facts
- arXiv ID: 2504.06581
- Source URL: https://arxiv.org/abs/2504.06581
- Reference count: 40
- Key result: LLMs achieved ~95% diagnostic accuracy for RA, but ~68% of reasoning was medically incorrect

## Executive Summary
This study reveals a critical misalignment in LLM medical diagnosis: high prediction accuracy does not guarantee sound reasoning. Using real-world patient data from the PreRAID dataset, the researchers found that while LLMs correctly diagnosed rheumatoid arthritis in approximately 95% of cases, medical experts rated nearly 68% of the models' reasoning as flawed. This "right prediction, wrong reasoning" phenomenon raises serious concerns about deploying LLMs in clinical settings where explanation quality is as important as diagnostic accuracy.

## Method Summary
The researchers evaluated multiple LLM architectures (GPT-4o, Gemini 2.0 Flash, QWEN-2-7B) using the PreRAID dataset of 160 real patient records. They tested four configurations: LLM only, RAG, 2-agent, and 3-agent systems. Patient data was embedded using GPT-4's text-embedding-3-large and stored in a vector database for retrieval. Agent chains involved extracting symptoms, comparing with historical data, and producing diagnoses. Expert medical reviewers classified reasoning quality as correct, minor flaw, or major flaw based on 12 clinical factors.

## Key Results
- LLMs achieved approximately 95% diagnostic accuracy for RA detection
- Nearly 68% of model reasoning was rated as incorrect by medical experts
- Larger proprietary models (GPT-4o, Gemini) showed better reasoning than smaller models (QWEN-2-7B)
- RAG and multi-agent architectures improved accuracy but failed to proportionally improve reasoning quality

## Why This Works (Mechanism)

### Mechanism 1
LLMs can achieve high diagnostic accuracy (~95%) through statistical pattern recognition without genuinely understanding clinical reasoning pathways. The model identifies surface-level correlations between symptom patterns and diagnoses from its pre-training data, correctly classifying RA cases even when it cannot articulate why those symptoms support the diagnosis. This "right prediction, wrong reasoning" phenomenon suggests prediction and explanation are partially decoupled capabilities.

### Mechanism 2
Retrieval-augmented generation (RAG) and multi-agent architectures improve accuracy but do not reliably improve reasoning alignment. Adding knowledge bases and iterative agent interactions provides more context for pattern matching, but without explicit reasoning verification, the model may still retrieve and chain incorrect justifications. The study found gaps of 20-80% between accuracy and correct reasoning across configurations.

### Mechanism 3
Smaller models fail more severely at generating correct reasoning than larger proprietary models, even when predictions are accurate. Model capacity affects the ability to articulate coherent medical justifications. QWEN-2-7B "failed to provide any correct reasoning" while larger models showed partial success, suggesting reasoning quality scales with model size but remains imperfect.

## Foundational Learning

- **Prediction-Explanation Decoupling**: Understanding that accurate outputs do not guarantee trustworthy justifications is essential for safe clinical deployment. *Quick check*: Can you identify a scenario where an LLM gives a correct diagnosis but for a clinically invalid reason?

- **Retrieval-Augmented Generation (RAG)**: The study uses RAG to provide historical patient data to LLMs; understanding this architecture clarifies why context alone does not fix reasoning. *Quick check*: How does RAG differ from standard few-shot prompting in terms of knowledge access?

- **Multi-Agent Reasoning Pipelines**: The study tests 2-agent and 3-agent architectures with iterative verification; understanding agent roles helps evaluate their limitations. *Quick check*: What is the theoretical benefit of having Agent B review Agent A's output before Agent C produces a final diagnosis?

## Architecture Onboarding

- **Component map**: PreRAID Dataset -> Vector Database (GPT-4 embeddings) -> RAG Layer -> LLM Core -> Multi-Agent Chain (Agent A extracts symptoms, Agent B verifies, Agent C produces diagnosis) -> Expert Evaluation

- **Critical path**: Data preprocessing -> embedding storage -> RAG retrieval -> agent chain execution -> diagnosis output -> expert reasoning validation

- **Design tradeoffs**: Larger knowledge bases may improve accuracy but increase retrieval noise; more agents add verification steps but increase latency and cost; proprietary models show better reasoning than open-source but require API dependency

- **Failure signatures**: High accuracy, low reasoning quality (model relies on spurious correlations); inconsistent agent outputs (Agent B fails to catch Agent A's errors); small model silence (QWEN-2-7B produces no valid reasoning)

- **First 3 experiments**:
  1. Reproduce the single LLM baseline on a subset of PreRAID data to validate accuracy claims
  2. Add a reasoning verification prompt that explicitly requires citing specific patient data points
  3. Compare 2-agent vs. 3-agent architectures with human-in-the-loop verification

## Open Questions the Paper Calls Out

### Open Question 1
How can LLMs be trained or architected to align high diagnostic accuracy with clinically valid reasoning? The conclusion explicitly calls for research to "enhance the interpretability and reasoning capabilities of LLMs" to ensure "medically sound justifications" accompany accurate predictions. This remains unresolved because the study demonstrates that current models achieve ~95% accuracy while exhibiting ~68% flawed reasoning.

### Open Question 2
Does the "Right Prediction, Wrong Reasoning" misalignment generalize to other diseases with non-specific early symptoms? The study is limited to Rheumatoid Arthritis; while the authors imply the problem is broad, they only validate it within the specific context of the PreRAID dataset. It remains unclear if this phenomenon is unique to RA's complex symptomatology or a universal failure mode in medical LLM pre-screening.

### Open Question 3
Can advanced multi-agent architectures be optimized specifically to validate reasoning logic rather than just final diagnostic outputs? The paper tested 2-agent and 3-agent RAG systems and found that while accuracy fluctuated, "reasoning quality remains inconsistent," with gaps persisting across all configurations. Merely increasing the number of agents and context length proved insufficient for correcting the logical hallucinations identified by experts.

## Limitations
- The PreRAID dataset is not publicly available, requiring synthetic data generation or direct author access for exact replication
- Reasoning quality assessment relies entirely on expert medical judgment without standardized rubrics, introducing potential inter-rater variability
- The study focuses on RA diagnosis specifically, limiting generalizability to other clinical domains without further validation

## Confidence

- **High confidence**: LLMs can achieve high diagnostic accuracy (~95%) while producing incorrect reasoning in ~68% of cases
- **Medium confidence**: RAG and multi-agent architectures improve accuracy but do not proportionally improve reasoning quality
- **Medium confidence**: Smaller models consistently fail to produce correct reasoning even when predictions are accurate

## Next Checks

1. Test the same architectural framework on a different medical diagnosis task (e.g., diabetes classification) to determine whether prediction-reasoning decoupling generalizes beyond RA

2. Implement and test explicit reasoning verification prompts that require models to cite specific patient data points supporting their diagnosis

3. Create controlled experiments that vary only one architectural element at a time to identify which component most influences reasoning quality alignment