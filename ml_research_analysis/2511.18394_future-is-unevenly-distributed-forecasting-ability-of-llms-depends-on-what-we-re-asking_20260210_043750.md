---
ver: rpa2
title: 'Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What
  We''re Asking'
arxiv_id: '2511.18394'
source_url: https://arxiv.org/abs/2511.18394
tags:
- question
- questions
- forecasting
- about
- strength
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the forecasting abilities of large language
  models (LLMs) on real-world events beyond their training cutoffs, examining how
  performance varies with domain and prompt framing. The authors collected and filtered
  10,000 prediction market questions, focusing on 392 high-quality events across six
  domains, and evaluated four model families (GPT-5, GPT-4.1, DeepSeek-R1, Claude
  3.7) both with and without news context.
---

# Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking

## Quick Facts
- arXiv ID: 2511.18394
- Source URL: https://arxiv.org/abs/2511.18394
- Reference count: 8
- Primary result: LLM forecasting performance is highly uneven across domains and heavily dependent on prompt design and context handling

## Executive Summary
This paper investigates how well large language models can forecast real-world events beyond their training cutoffs. The authors collected 10,000 prediction market questions and focused on 392 high-quality events across six domains (finance, politics, technology, sports, entertainment, geopolitics) to evaluate four model families (GPT-5, GPT-4.1, DeepSeek-R1, Claude 3.7). The study reveals that forecasting ability is not a uniform emergent capability but varies dramatically by domain, with reasoning models performing better in structured areas like geopolitics while struggling in entertainment and technology. Adding news context had mixed effects - sometimes improving accuracy in finance and sports but often hurting performance due to recency bias, rumor overweighting, and definition drift.

## Method Summary
The researchers developed a comprehensive evaluation framework using prediction market questions as benchmarks for LLM forecasting abilities. They collected 10,000 questions from various sources and applied strict filtering criteria to select 392 high-quality events that met standards for specificity, time-bound nature, and clear resolution criteria. Four model families were tested: GPT-5, GPT-4.1, DeepSeek-R1, and Claude 3.7, both with and without news context windows. The experiments varied prompt structures and context windows to understand how different approaches affected forecasting accuracy across six domains: finance, politics, technology, sports, entertainment, and geopolitics. Performance was measured through both accuracy metrics and calibration scores to assess how well models' confidence matched their actual predictive success.

## Key Results
- Forecasting performance varies dramatically by domain, with reasoning models excelling in geopolitics while struggling in entertainment and technology
- News context improves forecasting accuracy in finance and sports but often harms performance due to recency bias and rumor overweighting
- Recency bias and definition drift are identified as key failure modes when LLMs incorporate news context
- Calibration scores reveal that better-calibrated models (like Claude 3.7) tend to have more reliable confidence in their predictions

## Why This Works (Mechanism)
LLM forecasting capabilities emerge from their ability to synthesize patterns across vast training data, but this capability is not uniform or domain-agnostic. The models leverage their parametric knowledge to make predictions, but when presented with news context, they must integrate new information with existing knowledge. This integration process is where domain-dependent weaknesses appear - in structured domains with clear causal relationships (geopolitics), reasoning models can effectively weigh new information, while in domains with high uncertainty or rapid change (technology, entertainment), the same reasoning capabilities may overfit to noise. The models' forecasting ability fundamentally depends on how well their training distribution aligns with the target domain and how effectively they can distinguish signal from noise when given additional context.

## Foundational Learning

### Domain-Specific Forecasting Patterns
**Why needed:** Different domains have distinct temporal dynamics and information structures that affect how well LLMs can reason about future events.
**Quick check:** Compare accuracy variance across domains for each model family to identify which domains show the most consistent performance.

### News Context Integration
**Why needed:** Understanding how LLMs weigh recent information against their parametric knowledge is crucial for effective forecasting applications.
**Quick check:** Measure the correlation between context recency and prediction accuracy to identify optimal context windows.

### Calibration vs. Accuracy Trade-offs
**Why needed:** Well-calibrated models may make fewer extreme predictions, affecting both accuracy and reliability metrics.
**Quick check:** Plot calibration curves against raw accuracy to identify models that balance confidence with correctness.

## Architecture Onboarding

### Component Map
Data Collection -> Quality Filtering -> Model Evaluation -> Context Integration -> Performance Analysis

### Critical Path
Question Selection → Model Prompting → Context Window Application → Prediction Generation → Accuracy/Correlation Measurement

### Design Tradeoffs
- Fresh context vs. recency bias: Recent information can improve accuracy but may introduce noise
- Prompt specificity vs. flexibility: Detailed prompts improve structure but may constrain reasoning
- Model choice vs. computational cost: Reasoning models perform better but require more resources

### Failure Signatures
- Recency bias: Over-weighting recent news at expense of long-term trends
- Rumor overweighting: Treating unverified information as factual
- Definition drift: Misinterpreting evolving event definitions over time

### Three First Experiments
1. Test identical forecasting prompts across all four model families to establish baseline performance without context
2. Vary context window size systematically to identify optimal recency for each domain
3. Compare structured vs. unstructured prompts within each domain to measure prompt sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset of 392 events may not capture full diversity of real-world forecasting scenarios
- Selection criteria for "high-quality" questions may bias toward LLM-amenable questions
- Reliance on prediction market data may not reflect all real-world forecasting needs
- Limited comparison with non-reasoning baselines on identical prompts

## Confidence
- High confidence in domain-dependent performance patterns across model families
- High confidence in identification of recency bias, rumor overweighting, and definition drift as failure modes
- Medium confidence in generalizability of news context effects due to domain variation
- Medium confidence in reasoning model superiority claims due to limited direct comparisons

## Next Checks
1. Test the same forecasting setup with additional model families (including open-source models with reasoning capabilities) to determine if domain-dependent patterns hold across broader architectures
2. Conduct controlled experiments varying only the recency of news context (e.g., 1-week vs. 1-month old) to isolate recency bias effects and determine optimal context windows
3. Implement blinded analysis where annotators evaluate whether news context contains rumors vs. legitimate information, then measure impact of this distinction on forecasting accuracy to validate rumor overweighting hypothesis