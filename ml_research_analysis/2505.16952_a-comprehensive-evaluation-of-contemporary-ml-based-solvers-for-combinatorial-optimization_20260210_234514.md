---
ver: rpa2
title: A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial
  Optimization
arxiv_id: '2505.16952'
source_url: https://arxiv.org/abs/2505.16952
tags:
- solvers
- instances
- neural
- optimization
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FrontierCO, a comprehensive benchmark for
  evaluating machine learning-based solvers on combinatorial optimization problems.
  The benchmark includes eight problem types with challenging real-world instances,
  addressing limitations in scale, realism, and data availability found in existing
  benchmarks.
---

# A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization

## Quick Facts
- **arXiv ID**: 2505.16952
- **Source URL**: https://arxiv.org/abs/2505.16952
- **Reference count**: 40
- **Primary result**: FrontierCO benchmark reveals substantial performance gap between ML-based and human-designed solvers across 8 CO problems, with neural methods struggling on large-scale instances and LLMs showing high variability

## Executive Summary
This paper introduces FrontierCO, a comprehensive benchmark for evaluating machine learning-based solvers on combinatorial optimization problems. The benchmark addresses critical limitations in existing evaluations by including challenging real-world instances, diverse scales, and standardized best-known solutions (BKS). It evaluates 16 representative ML-based solvers—13 neural networks and 3 LLM-based agents—against state-of-the-art human-designed algorithms across eight problem types. The study reveals that while some ML-based methods outperform prior SOTA on specific problems, they generally struggle with scalability and generalization, particularly on large-scale and structurally diverse instances. The results highlight the need for further research into improving ML solver scalability, generalization, and integration with existing heuristics.

## Method Summary
The FrontierCO benchmark provides standardized training data, test instances, and evaluation protocols for 8 CO problems. It includes 16 ML solvers (13 neural + 3 LLM agents) evaluated against SOTA classical solvers with a 1-hour time budget per instance. Neural solvers use single NVIDIA RTX A6000 GPU and single AMD EPYC 7313 CPU core, while LLM agents use only CPU. The benchmark features dual easy/hard test sets—easy sets contain instances now solvable by SOTA methods, while hard sets contain open/unsolved instances. Primal gap serves as the primary metric, computed as |cost(x;s) - c*| / max{|cost(x;s)|, |c*|}, where c* is the best-known solution. The dataset and evaluation toolkit are available at https://huggingface.co/datasets/CO-Bench/FrontierCO.

## Key Results
- ML-based solvers show substantial performance gaps compared to SOTA human-designed algorithms across all 8 problem types
- Neural solvers struggle particularly on large-scale hard instances due to scalability issues (frequent OOM errors)
- LLM agents exhibit high performance variability despite their flexibility in generating algorithmic strategies
- Some ML-based methods (e.g., Self-Refine on MIS, FunSearch on CVRP) outperform prior SOTA on specific problems
- Distribution shift severely impacts neural solver performance, with methods trained on fixed-size data failing on structurally diverse test instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing structurally complex instances with diverse scales exposes generalization gaps that synthetic benchmarks mask.
- Mechanism: Real-world and competition-derived instances (e.g., SAT-induced MIS graphs, hypercube STP) vary in size and topology from training distributions, forcing models to reason rather than exploit memorized patterns.
- Core assumption: Performance degradation under distribution shift indicates insufficient generalization rather than simply harder optimization landscapes.
- Evidence anchors: [abstract]: "challenging instances drawn from industrial applications and frontier CO research"; [Section 1]: "prior TSP evaluations are typically conducted on graphs with the same size and structure as those used during training, whereas real-world problems exhibit greater diversity and irregularity"; [Section 4]: "LEHD exhibits a reported 0.72% gap on a standard TSP benchmark, whereas on our new benchmark, the gap expands to 10% in easy TSP instances and a striking 77% in hard instances"

### Mechanism 2
- Claim: Separate easy and hard test sets disentangle validation of learned heuristics from discovery of novel strategies.
- Mechanism: The easy set (now-solvable by SOTA methods) tests whether ML solvers can match known algorithms; the hard set (unsolved/open instances) evaluates genuine advancement.
- Core assumption: Methods that match or exceed SOTA on hard instances have discovered genuinely novel strategies, not just better hyperparameter tuning.
- Evidence anchors: [Section 1]: "The first set is mainly used to validate the effectiveness of ML-based solvers, while the second set, which is free from any possible human heuristic hacking, serves as the main evaluation set"; [Section 4]: "Self-Refine outperforms KaMIS on the easy set of MIS, and FunSearch outperforms HGS on the hard set of CVRP"

### Mechanism 3
- Claim: Standardized BKS and training data eliminate confounding factors in cross-paper comparisons.
- Mechanism: By providing consistent reference solutions (validated by running SOTA solvers for up to 2 hours) and synthetic training data with documented generation protocols, the benchmark removes variability from random seeds, implementation differences, and data availability.
- Core assumption: The synthetic training data distribution sufficiently covers the test distribution to enable meaningful learning.
- Evidence anchors: [Section 2.4]: "Prior evaluations of ML-based CO solvers often relied on self-generated synthetic test instances, leading to difficulties in fair comparison across papers"; [Section 2.5]: "We also release a complete toolkit that includes a data loader, an evaluation function, and an abstract solving template tailored for LLM-based agents"

## Foundational Learning

- **Concept: Primal gap metric**
  - Why needed here: The paper uses primal gap (not raw objective values) as the primary evaluation metric to normalize across instances of varying scales and to penalize infeasibility uniformly.
  - Quick check question: Given an instance with best-known cost 100 and a solution with cost 120, what is the primal gap? (Answer: |120-100|/max{|120|,|100|} = 20/120 ≈ 16.7%)

- **Concept: Graph Neural Network locality limitations**
  - Why needed here: The paper's ablation on STP shows GNNs learn effectively on Euclidean graphs (with implicit spatial locality) but fail on non-Euclidean graphs, suggesting fundamental expressive limits.
  - Quick check question: Why might message-passing GNNs struggle with problems requiring global structure reasoning? (Answer: Local message passing aggregates information from k-hop neighborhoods, requiring depth proportional to graph diameter for global receptive fields; this leads to over-smoothing in deep networks.)

- **Concept: LLM-based agentic code generation for CO**
  - Why needed here: Unlike neural solvers that directly output solutions, LLM-based solvers generate executable algorithms (code) that are then run on instances—this is symbolic rather than neural optimization.
  - Quick check question: What advantage does generating code (vs. solutions) provide for combinatorial optimization? (Answer: Generated code can implement any algorithmic strategy, including calling existing solvers, and scales independently of the LLM's context window.)

## Architecture Onboarding

- **Component map**: HuggingFace dataset with 8 problem types, each containing easy/hard test splits, training data for neural solvers, and validation sets for LLM agents -> Abstract solving template with solve(**kwargs) function signature that yields iterative solutions -> Data loader and evaluator (hidden from agents) -> 16 ML solvers (13 neural + 3 LLM agents) -> SOTA classical solvers (KaMIS, LKH-3, HGS, Gurobi, CPLEX, SCIP-Jack, etc.)

- **Critical path**: Select problem type and difficulty level (easy vs. hard) -> Load standardized training data and train/configure your solver -> Run evaluation on test instances with 1-hour timeout per instance -> Compute primal gap against provided BKS; report arithmetic mean gap and geometric mean solving time

- **Design tradeoffs**: Time budget vs. instance difficulty (1-hour limit may be insufficient for hard instances); Training/test distribution alignment (standardized data enables fair comparison but may not match optimal training regimes); Single-GPU constraint (levels hardware playing field but underrepresents massively parallel neural approaches)

- **Failure signatures**: Out-of-memory or timeout without feasible solution (100% primal gap, marked with * in tables); High gap on easy instances (implementation bugs or insufficient training); Large variance across instances within same problem type (overfitting to specific structures)

- **First 3 experiments**: Establish baseline on easy set (run method on easy test set, expect to match or approach classical SOTA within 1-5% on easy sets); Ablate neural vs. heuristic components (compare method with and without learned components to quantify neural contribution); Evaluate generalization gap (train on standardized synthetic data, evaluate on both easy and hard sets to measure distribution shift impact)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based agents improve their ability to recognize and integrate appropriate algorithms to mitigate high performance variability?
- Basis in paper: [explicit] The authors state that "Enhancing the ability of LLM-based agents to recognize and leverage suitable algorithms remains a promising yet underexplored direction."
- Why unresolved: Current LLM agents frequently fail to select the correct solver for a given instance, leading to significant variance compared to SOTA human-designed algorithms.
- What evidence would resolve it: An LLM agent achieving near-zero primal gaps consistently across all eight CO problem types by reliably matching instances to optimal algorithms.

### Open Question 2
- Question: Can neural components provide performance gains when integrated into already strong heuristics rather than just weak baselines?
- Basis in paper: [explicit] The authors note regarding ablation studies: "Whether similar gains can be achieved when enhancing already strong heuristics remains unclear."
- Why unresolved: Neural modules improved weak baselines (e.g., 2-OPT) but failed to outperform strong heuristics like ACO in the evaluation.
- What evidence would resolve it: A neural-augmented solver that demonstrably improves the solution quality or speed of a mature SOTA solver on the FrontierCO hard set.

### Open Question 3
- Question: How can neural solvers overcome generalization failures when testing instance sizes and structures differ significantly from training data?
- Basis in paper: [explicit] The paper highlights "addressing training-testing discrepancies" as a critical direction, citing LEHD's performance collapse when evaluation scales change.
- Why unresolved: Neural solvers rely on local message passing and struggle with distribution shifts, resulting in large primal gaps on realistic, structurally complex instances.
- What evidence would resolve it: A single neural solver maintaining a stable primal gap (e.g., <5%) across both the "easy" and "hard" FrontierCO sets despite scaling differences.

## Limitations
- The 1-hour time budget may artificially disadvantage neural solvers that require longer training or inference for large instances
- Standardized synthetic training data may not be optimal for all methods, potentially limiting the performance of approaches that benefit from problem-specific training distributions
- The assumption that hard instances remain genuinely unsolved may be challenged as classical algorithms continue to advance

## Confidence
- **High confidence**: The performance gap between ML and human-designed solvers exists and is substantial (Section 4 results are directly measured and reproducible)
- **Medium confidence**: The specific mechanisms explaining why ML solvers struggle (scalability, distribution shift, heuristic limitations) are well-supported by ablation studies but could have alternative explanations
- **Low confidence**: The claim that FrontierCO's design (easy/hard split, standardized data) is the optimal approach for evaluating ML solvers—while reasonable, alternative benchmarking frameworks might better capture different aspects of solver capability

## Next Checks
1. **Test time budget sensitivity**: Re-run the evaluation with extended time limits (e.g., 4 hours) for neural solvers on hard instances to determine if the performance gap is due to computational constraints rather than algorithmic limitations
2. **Training data ablation**: Train selected neural solvers on alternative distributions (e.g., uniform random vs. FrontierCO's structured generators) to quantify the impact of training data design on generalization performance
3. **Hard set boundary validation**: Apply the latest classical SOTA methods (e.g., recent TSP solvers beyond LKH-3) to hard instances to verify they remain genuinely unsolved and that the easy/hard distinction captures meaningful difficulty progression