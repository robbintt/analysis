---
ver: rpa2
title: 'V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and
  Planning'
arxiv_id: '2506.09985'
source_url: https://arxiv.org/abs/2506.09985
tags:
- v-jepa
- video
- training
- arxiv
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents V-JEPA 2, a self-supervised video model trained
  on over 1 million hours of internet video that achieves state-of-the-art performance
  on motion understanding (77.3 top-1 accuracy on Something-Something v2), action
  anticipation (39.7 recall-at-5 on Epic-Kitchens-100), and video question answering
  (84.0 on PerceptionTest, 76.9 on TempCompass) at the 8 billion parameter scale.
  The model uses a joint-embedding predictive architecture with mask-denoising in
  representation space, scaled through larger models (up to 1 billion parameters),
  longer training, and progressive resolution training.
---

# V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning

## Quick Facts
- arXiv ID: 2506.09985
- Source URL: https://arxiv.org/abs/2506.09985
- Authors: Mido Assran; Adrien Bardes; David Fan; Quentin Garrido; Russell Howes; Mojtaba; Komeili; Matthew Muckley; Ammar Rizvi; Claire Roberts; Koustuv Sinha; Artem Zholus; Sergio Arnaud; Abha Gejji; Ada Martin; Francois Robert Hogan; Daniel Dugas; Piotr Bojanowski; Vasil Khalidov; Patrick Labatut; Francisco Massa; Marc Szafraniec; Kapil Krishnakumar; Yong Li; Xiaodong Ma; Sarath Chandar; Franziska Meier; Yann LeCun; Michael Rabbat; Nicolas Ballas
- Reference count: 40
- One-line primary result: V-JEPA 2 achieves state-of-the-art on motion understanding (77.3 top-1 on Something-Something v2), action anticipation (39.7 recall-at-5 on Epic-Kitchens-100), and video question answering (84.0 on PerceptionTest, 76.9 on TempCompass) at the 8 billion parameter scale.

## Executive Summary
V-JEPA 2 presents a self-supervised video model that learns understanding, prediction, and planning capabilities from over 1 million hours of internet video. The model uses a joint-embedding predictive architecture with mask-denoising in representation space, scaled through larger models (up to 1 billion parameters), longer training, and progressive resolution training. After pretraining, V-JEPA 2 is post-trained with less than 62 hours of robot interaction data to create V-JEPA 2-AC, an action-conditioned world model that enables zero-shot robot control for prehensile manipulation tasks without task-specific training or rewards. The work demonstrates how self-supervised learning from web-scale data and minimal interaction data can yield a world model capable of understanding, predicting, and planning in the physical world.

## Method Summary
V-JEPA 2 uses a joint-embedding predictive architecture where a ViT-g encoder (1B parameters) is trained on VideoMix22M (1M+ hours) using a mask-denoising objective in representation space. The encoder and predictor are trained with an L1 loss on output embeddings of a frozen teacher encoder (EMA). For robot control, the encoder is frozen and a 300M parameter action-conditioned predictor (V-JEPA 2-AC) is trained on 62 hours of Droid robot data using teacher forcing and rollout losses. Planning is achieved through visual MPC using the Cross-Entropy Method to minimize L1 distance between predicted latent states and goal embeddings.

## Key Results
- Achieves 77.3 top-1 accuracy on Something-Something v2 for motion understanding
- Obtains 39.7 recall-at-5 on Epic-Kitchens-100 for action anticipation
- Scores 84.0 on PerceptionTest and 76.9 on TempCompass for video question answering
- Enables zero-shot robot control with 62 hours of interaction data
- Demonstrates planning for prehensile manipulation tasks without task-specific training

## Why This Works (Mechanism)

### Mechanism 1: Latent Prediction over Pixel Generation
Learning to predict missing video regions in a learned representation space (rather than pixel space) forces the model to capture high-level structure and physics while ignoring unpredictable visual noise (e.g., texture, lighting). The V-JEPA 2 encoder and predictor are trained using an L1 loss on the output embeddings of a frozen teacher encoder (EMA), not on raw pixels. This acts as a denoising objective for semantic content. The unpredictable details of a video (like specific leaf movement) are irrelevant for high-level planning, and removing them via latent prediction improves generalization.

### Mechanism 2: Asymmetric Data Staging (Pretrain then Post-train)
A massive, action-free pretraining stage creates a general "physics engine" in latent space, which can be rapidly fine-tuned for control with minimal interaction data. The model is first trained on 1M+ hours of internet video (no action labels). Then, the encoder is frozen, and a lightweight predictor (V-JEPA 2-AC) is trained on just 62 hours of robot data to map actions to latent state transitions. The visual dynamics learned from passive observation transfer directly to the active manipulation domain without requiring the encoder to be retrained.

### Mechanism 3: Planning via Energy Minimization (Visual MPC)
Planning is achieved not by generating a video plan, but by iteratively optimizing an action sequence that minimizes the "energy" (L1 distance) between a predicted future latent state and the goal latent state. The system uses the Cross-Entropy Method (CEM) to sample potential action sequences. It runs these through the V-JEPA 2-AC predictor, checks the distance to the goal embedding, and executes the first action of the best sequence. The latent space is smooth enough that L1 distance correlates with semantic task progress (i.e., closer in latent space = closer to goal completion).

## Foundational Learning

### Concept: Joint-Embedding Predictive Architecture (JEPA)
**Why needed here:** Unlike standard Autoencoders that reconstruct pixels, JEPA predicts features. This loss function inherently filters out high-frequency noise.
**Quick check question:** Does the model try to predict the exact color of every pixel in a masked region, or the semantic content of that region?

### Concept: Model Predictive Control (MPC)
**Why needed here:** The robot doesn't learn a policy (state -> action); it learns a dynamics model (state + action -> next state) and solves an optimization problem at every step.
**Quick check question:** Why does the system re-plan at every time step rather than executing a fixed trajectory?

### Concept: Exponential Moving Average (EMA) / Teacher Network
**Why needed here:** V-JEPA relies on a "teacher" network to provide the target embeddings. This stabilizes training and prevents representation collapse.
**Quick check question:** Why is the target encoder updated via EMA rather than standard gradient descent?

## Architecture Onboarding

### Component map:
Encoder (ViT-g) -> Predictor (ViT-s) -> Planner (CEM)

### Critical path:
1. Pretrain Encoder on VideoMix22M (1M hours)
2. Freeze Encoder
3. Train Predictor on Droid (62 hours) using Teacher Forcing + Rollout Loss
4. Deploy: At runtime, encode goal image -> Encode current frame -> CEM Loop (Sample Actions -> Predictor -> Calculate L1 to Goal) -> Execute Best Action

### Design tradeoffs:
- **Latent vs. Pixel Planning:** Latent is much faster (16s vs 4min per action in paper) but sacrifices visual interpretability of the plan
- **Frozen vs. Fine-tuned Encoder:** The paper freezes the encoder to save compute and preserve general features, but this limits adaptation to novel camera angles

### Failure signatures:
- **Coordinate Axis Misalignment:** The paper notes the model fails if the camera moves significantly because it implicitly learns the action coordinate frame from the visual background
- **Long-Horizon Drift:** Autoregressive prediction in latent space accumulates errors, making long-horizon planning without sub-goals unreliable

### First 3 experiments:
1. **Ablate Pretraining:** Train the AC-Predictor on robot data without the V-JEPA pretraining (random init) to isolate the value of internet video pretraining
2. **Visualize Latent Rollouts:** Use a decoder to visualize the "imagined" future states of the V-JEPA 2-AC model to verify if it understands object permanence (e.g., does the cup disappear when the gripper closes?)
3. **Sensitivity Analysis:** Rotate the camera 45 degrees and measure the drop in "Reach" success rate to quantify the coordinate axis sensitivity mentioned in the paper

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can hierarchical world models operating across multiple spatial and temporal scales enable longer-horizon planning (e.g., multi-step tasks) without requiring explicit sub-goal images?
**Basis in paper:** [explicit] The authors state in the Future Work section that extending to longer-horizon tasks such as pick-and-place without sub-goals "will require further innovations in modeling" and that "developing approaches for hierarchical models capable of making predictions across multiple spatial and temporal scales, at different levels of abstraction, is a promising direction."
**Why unresolved:** V-JEPA 2-AC currently achieves pick-and-place by using manually-provided sub-goal images; the paper does not demonstrate autonomous decomposition of long-horizon tasks.
**What evidence would resolve it:** Demonstration of zero-shot completion of complex, multi-step manipulation tasks using only a single final goal image, with quantitative comparison to the current sub-goal approach.

### Open Question 2
**Question:** How can the V-JEPA 2-AC world model be extended to accept language-based goal specifications instead of requiring image goals?
**Basis in paper:** [explicit] The authors note that "it may be more natural to express goals in other forms, such as with language" and propose as future work "extending the V-JEPA 2-AC to accept language-based goals, e.g., by having a model that can embed language-based goals into the V-JEPA 2-AC representation space."
**Why unresolved:** The current planning formulation (Equation 5) relies on L1 distance between predicted and goal representations in visual embedding space; no mechanism exists for mapping language instructions into this space.
**What evidence would resolve it:** Successful zero-shot robot manipulation using only natural language task descriptions (e.g., "put the cup on the plate"), with performance comparable to image-goal conditioning.

### Open Question 3
**Question:** What architectural or training innovations are needed to make the inferred action coordinate axis invariant to camera positioning without manual calibration?
**Basis in paper:** [inferred] The paper documents significant sensitivity to camera position (Figure 16), with rotation error varying nearly linearly from 35째 to 80째 camera positions. The authors manually selected camera positions and note that "large errors in the inferred coordinate axis can degrade success rate on downstream tasks."
**Why unresolved:** The model must implicitly infer action coordinate axes from monocular RGB without explicit camera calibration or robot base visibility, which remains an ill-posed problem in many configurations.
**What evidence would resolve it:** Demonstration of consistent planning success rates across a wide range of uncalibrated camera positions in novel environments, without manual position selection.

### Open Question 4
**Question:** Can scaling the V-JEPA 2 encoder beyond 1B parameters continue to yield sustained performance improvements on downstream understanding, prediction, and planning tasks?
**Basis in paper:** [explicit] The authors acknowledge they "scaled V-JEPA 2 models up to a modest 1B parameters" and that "additional work is needed in this direction to develop scalable pre-training recipes that lead to sustained performance improvements with scale," citing prior work on 20B parameter vision encoders.
**Why unresolved:** While Figure 5 shows consistent improvements from 300M to 1B parameters, the scaling behavior beyond this point remains unexplored for joint-embedding predictive architectures trained on video.
**What evidence would resolve it:** Training and evaluation of V-JEPA 2 variants at 2B, 5B, and 10B+ parameters, reporting scaling curves for understanding tasks (SSv2, K400), prediction (EK100 anticipation), and planning success rates.

## Limitations
- **Camera-Invariant Representation Gap:** The model achieves strong performance when the camera frame is aligned with the robot base, but degrades significantly with camera rotation, representing a fundamental limitation for real-world deployment.
- **Long-Horizon Planning Reliability:** While the model succeeds at 1-step planning for "Reach" tasks, it requires sub-goal decomposition for "Pick-and-Place" tasks, suggesting the learned dynamics model may not capture long-term temporal consistency.
- **Dataset Generalization:** The 62 hours of robot interaction data comes from a specific environment (Droid dataset with a Franka robot), leaving open questions about cross-platform generalization.

## Confidence
**High Confidence:**
- Pretraining on 1M+ hours achieves SotA results on Something-Something v2, Epic-Kitchens-100, PerceptionTest, and TempCompass
- Latent prediction objective successfully filters visual noise compared to pixel-based methods
- V-JEPA 2-AC enables zero-shot control without task-specific training

**Medium Confidence:**
- 62 hours of robot data is sufficient for post-training (based on ablation showing performance saturation)
- L1 distance in latent space correlates with semantic task progress
- Progressive resolution training schedule improves performance

**Low Confidence:**
- Coordinate frame invariance without camera calibration
- Long-horizon planning without sub-goals
- Cross-platform robot generalization

## Next Checks
1. **Coordinate Frame Robustness Test:** Systematically evaluate performance degradation as a function of camera rotation angle relative to robot base. Measure success rate drop from 0째 to 90째 rotation to quantify the coordinate axis sensitivity.

2. **Long-Horizon Planning Stress Test:** Evaluate planning performance on tasks requiring 5+ sequential actions without sub-goals. Compare success rates between 1-step and multi-step planning to quantify error accumulation in latent space.

3. **Encoder Fine-tuning Validation:** Train a variant where the encoder is fine-tuned on robot data (rather than frozen) to test whether encoder adaptation improves performance under camera misalignment or novel environments.