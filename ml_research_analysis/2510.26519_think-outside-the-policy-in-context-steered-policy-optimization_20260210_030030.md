---
ver: rpa2
title: 'Think Outside the Policy: In-Context Steered Policy Optimization'
arxiv_id: '2510.26519'
source_url: https://arxiv.org/abs/2510.26519
tags:
- icpo
- training
- expert
- grpo
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces In-Context Steered Policy Optimization (ICPO),
  a reinforcement learning framework for Large Reasoning Models (LRMs) that addresses
  the exploration limitations of standard methods like GRPO. Instead of relying on
  external expert models or additional computational overhead, ICPO leverages the
  in-context learning capability of LRMs to generate expert-guided rollouts from existing
  datasets, enabling exploration beyond the current policy distribution.
---

# Think Outside the Policy: In-Context Steered Policy Optimization

## Quick Facts
- arXiv ID: 2510.26519
- Source URL: https://arxiv.org/abs/2510.26519
- Reference count: 40
- One-line primary result: ICPO improves mathematical reasoning performance by +4.1 points over vanilla GRPO using in-context learning for expert-guided exploration

## Executive Summary
This paper introduces In-Context Steered Policy Optimization (ICPO), a reinforcement learning framework that enhances Large Reasoning Models' exploration capabilities beyond standard methods like GRPO. Rather than relying on external expert models or computational overhead, ICPO leverages in-context learning to generate expert-guided rollouts from existing datasets, enabling exploration beyond the current policy distribution. The method incorporates Expert Region Reject Sampling to filter noisy trajectories and Annealed Expert-Bonus Reward Shaping to balance early expert guidance with later autonomous learning.

## Method Summary
ICPO extends GRPO by generating off-policy rollouts conditioned on 1-shot in-context demonstrations from expert datasets. The method uses Expert Region Reject Sampling to filter trajectories based on verifiable rewards (requiring correct solutions), and optionally applies Annealed Expert-Bonus Reward Shaping that decays over training. The framework operates with a 7:1 on-policy to off-policy rollout ratio, using importance sampling to correct for distribution shifts between conditioned and unconditioned policies. Training uses batch size 128, learning rate 1e-6, and 400 steps on filtered math datasets.

## Key Results
- Maximum average gains of +4.1 points over vanilla GRPO and +4.0 points over mixed-policy GRPO on in-distribution benchmarks
- Enhanced exploration demonstrated through higher inter-trajectory edit distances and improved OOD generalization (+0.7 to +2.4 on Qwen3-8B)
- Training stability maintained through Expert Region Reject Sampling filtering
- Random demonstration sampling performs best among tested strategies

## Why This Works (Mechanism)

### Mechanism 1: Implicit Expert Forcing via In-Context Steering
ICL demonstrations create an expert-conditioned policy that explores beyond current distribution without external models. Demonstrations map to a task vector ϑ encoding expert behavior, and conditioning on [D; q] generates from π_F(τ|q; ϑ) rather than π_θ(τ|q), creating an implicit expert prior. This is treated as off-policy because input conditioning alters sampling distribution. The method assumes sufficient ICL capability in the base model to extract and apply expert reasoning patterns.

### Mechanism 2: Expert Region Reject Sampling for Quality Filtering
Expert-conditioned rollouts are accepted only if R(τ_j) ≥ δ (default δ = 1.0, requiring correct solutions). Reject sampling operator ρ restricts expectation to accepted trajectories, preventing low-quality traces from contaminating gradients. This addresses instability from indiscriminately incorporating noisy external trajectories. The method assumes verifiable rewards reliably indicate trajectory quality and expert alignment.

### Mechanism 3: Annealed Expert-Bonus Reward Shaping for Curriculum Transition
Time-decaying expert bonuses encourage early imitation while allowing autonomous learning later. Shaped reward R_shaped(τ) = R(τ) + α·γ(t) adds bonus only to correct expert-region trajectories, where γ(t) = 1 - t/T decays linearly. This creates strong early gradients toward expert-like solutions, then reduces expert reliance as policy capabilities mature. The method assumes early expert guidance accelerates convergence without limiting eventual capability beyond expert distribution.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** ICPO extends GRPO's mixed-policy framework. You must understand how GRPO computes group-relative advantages without a critic model before understanding how ICPO modifies it with off-policy rollouts.
  - **Quick check question:** Can you explain how GRPO computes advantages differently from standard PPO, and why removing the critic matters for LLM training efficiency?

- **Concept: In-Context Learning as Implicit Function Construction**
  - **Why needed here:** The paper relies on the "task vector" hypothesis—that ICL implicitly constructs a function A(D) mapping demonstrations to behavior. This is essential to understand why ICL rollouts qualify as "off-policy" despite coming from same model weights.
  - **Quick check question:** How does conditioning on [demonstrations; query] differ computationally from fine-tuning on those demonstrations? What does this imply about the resulting output distribution?

- **Concept: Importance Sampling for Off-Policy Correction**
  - **Why needed here:** ICPO computes expert-conditioned importance weights r̂_j,t(θ) = π_θ(τ_j,t) / π^IEF_θ(τ_j,t). Understanding why this correction is necessary—and why input-conditioning creates a distribution shift—is crucial for debugging training instability.
  - **Quick check question:** Why can't we directly use trajectories generated under ICL conditioning as on-policy samples for the unconditioned policy π_θ?

## Architecture Onboarding

- **Component map:**
Training Loop (Algorithm 1)
├── Prompt Sampling (batch of B prompts from training set)
├── On-Policy Rollouts (N_on = 7 trajectories per prompt from π_θ^old)
├── ICL Demonstration Sampling (k = 1 example from expert dataset D)
├── Expert-Conditioned Rollout (N_off = 1 from π_θ^IEF)
│   └── Input format: [demonstration_q; demonstration_a; query_q]
├── Verifiable Reward Computation (binary correctness via Math-Verify)
├── Expert Region Filter (ERRS: keep if R(τ) ≥ 1.0)
├── Optional Reward Shaping (add α·γ(t) if enabled; α = 1.0)
├── Mixed Advantage Computation (Eq. 6: normalize over N_on ∪ N_off)
└── GRPO Update with Shaped Importance Weights (Eq. 10)

- **Critical path:**
  1. Demonstration quality → determines ICL steering effectiveness (random sampling from MATH training set works best per Figure 10)
  2. Expert region filtering → prevents noisy rollouts from corrupting updates
  3. Importance weight computation → corrects for distribution shift between conditioned and unconditioned policies

- **Design tradeoffs:**
  - N_on : N_off ratio: Paper uses 7:1 (following LUFFY). More off-policy increases exploration but raises noise risk requiring stricter ERRS.
  - Demonstration pool size: Small pools (10 per batch) yield stable early training; full pools (7,500) enable richer late-stage exploration (Figure 11).
  - RS on/off: ICPO (no RS) maximizes in-distribution gains (+4.1 avg); ICPO† (with RS) improves OOD generalization (+2.4 avg on Qwen3-8B).

- **Failure signatures:**
  - Exploration collapse: If ERRS is too strict or demonstrations are poor, entropy drops rapidly and model converges to local optima (monitor via Figure 13 entropy curves).
  - Domain overfitting: If expert demonstrations are from narrow domain (e.g., MATH), OOD performance on GPQA/ARC may degrade relative to general-purpose baselines (Table 1, Appendix D.3).
  - ICL insufficiency: If base model lacks ICL capability, expert-conditioned rollouts show minimal distribution shift, negating method's benefit.

- **First 3 experiments:**
  1. Verify ICL steering effect: Compare 0-shot vs 1-shot accuracy and inter-trajectory edit distance on held-out math benchmark to confirm model has sufficient ICL capability before full training.
  2. Ablate ERRS threshold: Train with δ ∈ {0.8, 1.0, 1.2} on small data subset to find optimal balance between trajectory diversity and quality for target domain.
  3. Compare demonstration strategies: Test random vs difficulty-matched vs length-matched demonstration selection to determine if domain benefits from heuristic curation over random sampling.

## Open Questions the Paper Calls Out

- **Generalization to Other Domains:** Can ICPO effectively generalize to domains beyond mathematical reasoning, such as code generation, and what task-specific adaptations are required? The authors state experiments are limited to mathematical reasoning and extending to other domains would require additional task-specific adaptation.

- **Minimum ICL Capability Requirement:** What is the minimum level of in-context learning capability a model must possess for ICPO to provide meaningful improvements? The authors note their method assumes the underlying model possesses basic ICL capability but did not conduct experiments on models lacking this ability due to time constraints.

- **Optimal Demonstration Selection:** How can demonstration selection be optimized beyond random sampling to maximize ICPO's effectiveness? While random selection performs well, the authors hypothesize carefully selecting demonstrations tailored to each problem could further improve performance, which they plan to explore in future work.

## Limitations
- Method's effectiveness critically depends on base model's in-context learning capability, with limited empirical validation of when ICL steering fails
- Strong in-distribution gains (+4.1 avg) but mixed OOD results (+0.7 to +2.4 on Qwen3-8B) suggest potential domain overfitting
- Final implementation details for prompt template formatting and demonstration sampling strategy per batch remain underspecified

## Confidence

- **High Confidence:** The core mechanism of using ICL for off-policy exploration is well-supported by empirical evidence (Figures 2-3 showing 1-shot consistently outperforming 0-shot)
- **Medium Confidence:** The ablation results for ERRS (+0.8 improvement) and reward shaping effects are robust, but exact implementation details require careful attention
- **Low Confidence:** The exact prompt template formatting and demonstration sampling strategy per batch remain underspecified

## Next Checks

1. **ICL Steering Verification:** Before full training, verify that your base model shows expected ICL steering effect by measuring accuracy and inter-trajectory edit distance differences between 0-shot and 1-shot conditions on a held-out math benchmark.

2. **ERRS Threshold Calibration:** Conduct a small-scale ablation study with different δ values (0.8, 1.0, 1.2) to identify optimal balance between trajectory diversity and quality for your specific domain.

3. **Demonstration Strategy Comparison:** Test random versus difficulty-matched demonstration selection to determine whether your domain benefits from heuristic curation or performs better with simple random sampling from the expert dataset.