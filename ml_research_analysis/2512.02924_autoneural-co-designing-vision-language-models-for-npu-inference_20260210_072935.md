---
ver: rpa2
title: 'AutoNeural: Co-Designing Vision-Language Models for NPU Inference'
arxiv_id: '2512.02924'
source_url: https://arxiv.org/abs/2512.02924
tags:
- vision
- language
- arxiv
- quantization
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AutoNeural, a vision-language model (VLM) architecture
  co-designed for Neural Processing Unit (NPU) inference. The authors address the
  hardware-model mismatch between state-of-the-art VLMs optimized for GPUs and resource-constrained
  NPUs, specifically the quantization brittleness of Vision Transformers (ViTs) and
  the I/O-bound nature of autoregressive attention mechanisms.
---

# AutoNeural: Co-Designing Vision-Language Models for NPU Inference

## Quick Facts
- arXiv ID: 2512.02924
- Source URL: https://arxiv.org/abs/2512.02924
- Reference count: 40
- Key result: Vision-language model architecture co-designed for NPU inference, achieving up to 7× lower quantization error and 14× lower end-to-end latency compared to ViT-Transformer baselines.

## Executive Summary
AutoNeural addresses the hardware-model mismatch between state-of-the-art vision-language models (VLMs) optimized for GPUs and resource-constrained Neural Processing Units (NPUs). The authors identify two key bottlenecks: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms in autoregressive VLMs. To solve these issues, AutoNeural replaces the standard ViT encoder with a MobileNetV5-style backbone using depthwise separable convolutions for stable INT4/8/16 quantization, and integrates a hybrid Transformer-SSM language backbone based on State-Space Models (SSM) principles with gated convolutions for linear-time complexity and reduced memory I/O. The architecture demonstrates real-time performance for automotive cockpit applications on the Qualcomm SA8295P NPU, delivering 3× decoding speed and 4× longer context window than the baseline while maintaining competitive accuracy on multimodal benchmarks.

## Method Summary
AutoNeural employs a four-stage training procedure: (1) connector-only training with LR=1e-3 for 1 epoch, (2) full model unfreeze with LR=1e-5 for 1 epoch, (3) instruction tuning with a task mixture (35% VQA, 25% document, 20% chart, 15% OCR, 5% multi-turn) at LR=1e-5, and (4) quantization-aware training (QAT) with 60% synthetic and 40% automotive data at LR=1e-5. The model uses a MobileNetV5-300M encoder with a 2-layer MLP connector (no normalization) feeding into a Liquid AI LFM2-1.2B hybrid backbone (16 layers: 10 gated-conv SSM + 6 transformer attention) with SwiGLU FFN and RMSNorm. Training uses AdamW (β1=0.9, β2=0.95) with cosine schedule and batch size 512. The quantization strategy employs W8A16 for the encoder and W4A16 for the LLM. The model is evaluated on Infinity-MM (44.8M samples) and custom automotive datasets totaling 200K samples across four tasks.

## Key Results
- Achieves up to 7× lower quantization error and 14× lower end-to-end latency compared to ViT-Transformer baselines under NPU precision constraints
- Real-world deployment on Qualcomm SA8295P NPU demonstrates real-time performance for automotive cockpit applications
- Delivers 3× decoding speed and 4× longer context window than the baseline while maintaining competitive accuracy on multimodal benchmarks

## Why This Works (Mechanism)
AutoNeural addresses NPU constraints through architectural co-design. The MobileNetV5 encoder replaces ViT's self-attention with depthwise separable convolutions, which are more quantization-friendly and have lower computational complexity. The hybrid Transformer-SSM backbone combines gated convolutional SSM layers with traditional attention, reducing KV cache memory requirements through linear-time complexity while maintaining modeling capacity. The 5:3 ratio of SSM to attention layers specifically targets the I/O bottleneck in autoregressive generation. The mixed-precision quantization (W8A16 for encoder, W4A16 for LLM) balances accuracy and efficiency. The four-stage training procedure gradually adapts the model to NPU constraints while maintaining task performance.

## Foundational Learning
**MobileNetV5 Architecture**: Depthwise separable convolutions with multi-scale fusion adapters; needed for quantization stability and NPU efficiency; quick check: verify activation histograms remain bounded under INT8 quantization.
**State-Space Models (SSM)**: Linear-time sequence modeling through state transitions; needed to reduce KV cache memory and I/O bottlenecks; quick check: measure memory growth during long-context generation.
**Gated Convolutional SSM**: Combines convolutional operations with gating mechanisms for selective state updates; needed for expressive yet efficient sequence modeling; quick check: monitor gradient flow through gating units.
**Quantization-Aware Training (QAT)**: Simulates quantization effects during training using fake quantization; needed for model stability under low-precision constraints; quick check: validate SQNR improvement across quantization levels.
**Mixed-Precision Quantization**: Different layers use different bit-widths (W8A16 encoder, W4A16 LLM); needed to optimize accuracy-efficiency trade-off; quick check: measure accuracy degradation at various precision configurations.
**Multi-Scale Fusion Adapter**: Combines features from different encoder stages through upsampling and concatenation; needed for rich visual representations; quick check: visualize feature maps at different scales.

## Architecture Onboarding

**Component Map**: Input Image → MobileNetV5 Encoder → MLP Connector → Hybrid SSM-Transformer Backbone → Output Tokens

**Critical Path**: The most performance-critical path is the MobileNetV5 encoder → MLP connector sequence, as it directly impacts quantization stability and visual feature extraction quality. The hybrid backbone's SSM-to-attention ratio (5:3) is crucial for balancing memory efficiency and modeling capacity.

**Design Tradeoffs**: MobileNetV5 vs. ViT trades global attention for local convolutional patterns, improving quantization stability at the cost of some global context modeling. The hybrid SSM-Transformer backbone trades pure attention expressiveness for memory efficiency and linear-time complexity. The mixed-precision strategy (W8A16 encoder, W4A16 LLM) optimizes for the different quantization characteristics of visual vs. language components.

**Failure Signatures**: Quantization collapse manifests as activation explosion or vanishing gradients in the connector layers. Memory overflow occurs when KV cache grows beyond NPU capacity during long-context generation. Accuracy degradation appears as increased perplexity and reduced benchmark performance, particularly on complex reasoning tasks.

**Three First Experiments**:
1. Implement MobileNetV5 encoder with depthwise separable convolutions and evaluate quantization stability (SQNR, RMS error) on standard image benchmarks
2. Build a hybrid SSM-Transformer backbone using Mamba-style gated convolutions and validate KV cache memory reduction on long-context generation tasks
3. Deploy the full architecture to a commercial NPU (e.g., Qualcomm SA8295P) using mixed W8A16/W4A16 precision and measure end-to-end latency against a ViT-Transformer baseline

## Open Questions the Paper Calls Out
**Open Question 1**: Does the AutoNeural architecture maintain its efficiency advantages across different NPU vendors and memory hierarchies? The validation is currently limited to the Qualcomm SA8295P, leaving performance on architectures like Apple Neural Engine or Edge TPU unconfirmed.

**Open Question 2**: Can automated architecture search (NAS) identify superior NPU-native topologies compared to the manual MobileNet-SSM hybrid design? The current topology is manually designed; algorithmic optimization might discover more efficient operator combinations.

**Open Question 3**: What is the specific accuracy degradation on general multimodal benchmarks when applying the mixed-precision quantization? The paper establishes that the model is stable for quantization but does not quantify the functional accuracy loss on complex reasoning tasks under W4A16 constraints.

## Limitations
- Critical architectural details (MobileNetV5 encoder specifications, Liquid AI LFM2-1.2B SSM implementation) remain proprietary or underspecified
- Automotive-specific datasets are not publicly released, limiting external validation of domain-specific claims
- Real-world NPU performance metrics depend on Qualcomm's QNN SDK optimization, which is hardware- and software-version specific

## Confidence
- **High Confidence**: NPU deployment methodology and mixed-precision quantization strategy are well-documented and reproducible with hardware access
- **Medium Confidence**: Architecture design choices are logically sound for NPU constraints, but exact implementation details are missing
- **Low Confidence**: Proprietary components prevent exact reproduction and limit independent verification of claims

## Next Checks
1. Implement MobileNetV5 encoder with depthwise separable convolutions and Multi-Scale Fusion Adapter using publicly available MobileNetV3/V2 pretrained weights, then evaluate quantization stability (SQNR, RMS error) on standard image benchmarks
2. Build a hybrid SSM-Transformer backbone using Mamba-style gated convolutions (state dimension 64-128, kernel size 3) with 5:3 SSM-to-attention ratio, and validate KV cache memory reduction on long-context generation tasks
3. Deploy the full AutoNeural architecture to a commercial NPU (e.g., Qualcomm SA8295P or equivalent) using mixed W8A16/W4A16 precision, measuring end-to-end latency, throughput, and quantization error against a ViT-Transformer baseline under identical hardware constraints