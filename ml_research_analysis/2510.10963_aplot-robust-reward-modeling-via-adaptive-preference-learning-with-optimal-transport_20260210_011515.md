---
ver: rpa2
title: 'APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal
  Transport'
arxiv_id: '2510.10963'
source_url: https://arxiv.org/abs/2510.10963
tags:
- reward
- arxiv
- preprint
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces APLOT, a robust reward modeling method that
  improves pairwise preference learning through an adaptive margin mechanism formulated
  via Optimal Transport (OT). The core idea is to dynamically adjust the learning
  difficulty for each training triplet by estimating margins based on semantic similarity
  and predicted reward differences, enabling better discrimination between similar
  preference responses.
---

# APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport

## Quick Facts
- arXiv ID: 2510.10963
- Source URL: https://arxiv.org/abs/2510.10963
- Reference count: 26
- Key outcome: APLOT improves reward modeling via OT-based adaptive margins, achieving significant gains in ID/OOD accuracy and RLHF alignment

## Executive Summary
APLOT introduces a robust reward modeling method that enhances pairwise preference learning through an adaptive margin mechanism formulated via Optimal Transport. The core innovation is dynamically adjusting learning difficulty for each training triplet by estimating margins based on semantic similarity and predicted reward differences. This distributional approach enables better discrimination between similar preference responses compared to fixed-margin methods. Extensive experiments across multiple benchmarks demonstrate APLOT's superior performance in both in-distribution and out-of-distribution settings, with RLHF experiments further validating its practical effectiveness.

## Method Summary
APLOT operates on pairwise preference data using a Bradley-Terry framework augmented with adaptive margins. For each training batch, it constructs distributions over preferred (P) and rejected (Q) responses, then solves an entropic-regularized OT problem to obtain an optimal transport plan T*. The margin for each triplet is computed as a weighted sum of costs from this transport plan. The modified BT loss incorporates these margins, requiring the reward gap to exceed the adaptive threshold. This approach uses LoRA adapters (r=32, α=64, dropout=0.05) on a backbone LLM, trained with AdamW, learning rate 2e-6, batch size 128, and cosine scheduler for 2 epochs.

## Key Results
- APLOT achieves 4.3% higher ID accuracy and 4.8% higher OOD accuracy compared to vanilla BT baseline
- Significant improvements on HHH-Alignment (+8.2% accuracy) and MT-Bench (+5.4% accuracy)
- RLHF experiments show 3.7% improvement on GSM8K and 2.9% on MMLU benchmarks
- Outperforms PointMargin, HardMargin, and other adaptive margin methods in convergence speed and final performance

## Why This Works (Mechanism)

### Mechanism 1: Distributional Margin Estimation via Optimal Transport
- Claim: OT captures global distributional structure of preference pairs, improving upon point-wise heuristics
- Mechanism: APLOT constructs distributions P/Q over chosen/rejected responses, solves entropic-regularized OT to get T*, then computes margins as μ_i = Σ_j T*_ij · C_ij
- Core assumption: Distributional discrepancy between chosen/rejected sets reflects sample difficulty
- Break condition: If P and Q don't meaningfully differ or OT regularization is misconfigured, margins become uninformative

### Mechanism 2: Semantic–Reward Coupled Cost Matrix Design
- Claim: Cost matrix jointly encoding semantic similarity and predicted reward differences enables difficulty-aware margins
- Mechanism: C_ij = γ·S_ij + (1−γ)·(1−σ(Δf_ij)) combines cosine similarity and sigmoid-normalized reward gap
- Core assumption: Semantic similarity correlates with response interchangeability; smaller reward gaps indicate higher difficulty
- Break condition: Misleading semantic similarity or unreliable reward differences lead to misranked sample difficulty

### Mechanism 3: Margin-Enforced Gradient Amplification for Hard Samples
- Claim: Requiring reward gap to exceed adaptive margin maintains stronger gradient signals on difficult samples
- Mechanism: Modified BT loss −log σ(r(x,y_w) − r(x,y_l) − μ) ensures gradient 1−σ(s−μ) > 0.5 when s ≤ μ
- Core assumption: Sustained gradient pressure on margin-violating samples yields better discrimination
- Break condition: Systematically overestimated margins cause unnecessary strong gradients, slowing convergence

## Foundational Learning

**Concept: Bradley-Terry Pairwise Preference Learning**
- Why needed: BT objective is the baseline that APLOT augments; understanding its limitations motivates the approach
- Quick check: "Can you explain why vanilla BT only ensures r(x,y_w) > r(x,y_l) without enforcing a gap?"

**Concept: Discrete Optimal Transport with Entropic Regularization**
- Why needed: APLOT frames margin estimation as OT problem; Sinkhorn algorithm is used for efficient optimization
- Quick check: "How does adding entropy term H(T) to OT objective affect transport plan and optimization speed?"

**Concept: Reward Model Role in RLHF**
- Why needed: APLOT targets RM robustness; understanding RM's downstream role clarifies why better separation matters
- Quick check: "What downstream failures occur if reward model cannot distinguish similar responses?"

## Architecture Onboarding

**Component map:** Backbone LLM -> Reward head (scalar output) -> OT module (cost matrix + Sinkhorn solver) -> Margin-augmented BT loss

**Critical path:**
1. Forward pass computes rewards for all (x, y_w, y_l) in batch
2. Compute pairwise semantic similarities and reward gaps
3. Build cost matrix C (Eq. 6)
4. Solve OT to obtain T* (Eq. 5)
5. Compute per-sample margins μ_i (Eq. 7) and evaluate loss (Eq. 8)

**Design tradeoffs:**
- APLOT vs. PointMargin: APLOT solves global OT for distributionally-aware margins (higher compute, better performance) vs. simpler pointwise sum (lower overhead)
- Hyperparameter γ: Controls semantic vs. reward-difference emphasis; γ ≈ 0.5 is empirically balanced
- Batch size: OT operates over batch distributions, so very small batches may lead to unreliable margin estimates

**Failure signatures:**
- Unstable training or exploding margins: often due to poorly chosen β or degenerate cost matrices
- No improvement over BT baseline: check if margin computation is effectively disabled or γ is at extreme
- Slow convergence despite margins: may indicate over-regularization or too-large β dampening discriminativeness

**First 3 experiments:**
1. Reproduce ID/OOD comparison on 40K UF subset with gemma-2b-it + LoRA, sweeping γ ∈ {0.3, 0.5, 0.7}
2. Convergence ablation: Train APLOT vs. PointMargin vs. HardMargin vs. vanilla BT and plot validation accuracy
3. Cost matrix ablation: Compare semantic-only, reward-difference-only, and full coupled cost matrix

## Open Questions the Paper Calls Out
None

## Limitations
- OT computation overhead (O(N²) cost matrix) may limit scalability to larger models or datasets
- Performance gains could be partially attributed to adaptive nature rather than specifically to OT formulation
- Empirical evaluation relies on proxy metrics rather than direct human preference comparisons in RLHF

## Confidence
- **High confidence**: Core mechanism of margin-based gradient amplification and modified BT loss implementation
- **Medium confidence**: Distributional margin estimation via OT relies on assumptions about batch distribution quality
- **Medium confidence**: Coupled cost matrix design shows empirical benefits, but necessity of OT specifically is unclear

## Next Checks
1. **Batch Size Sensitivity Analysis**: Systematically evaluate APLOT performance across different batch sizes to determine minimum required for reliable OT margin estimation
2. **Alternative Margin Estimation Comparison**: Implement simpler adaptive margin methods to isolate OT's specific contribution versus adaptive margin scaling in general
3. **Human Preference Validation**: Conduct direct human evaluation studies to verify improved proxy metrics translate to better human preference alignment in practice