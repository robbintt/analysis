---
ver: rpa2
title: Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large
  Language Models
arxiv_id: '2511.12008'
source_url: https://arxiv.org/abs/2511.12008
tags:
- prompt
- diagnostic
- stroma
- dcis
- pathology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECAP-PATH addresses the interpretability gap in AI-driven pathology
  by introducing a two-phase prompt optimization framework that enables multimodal
  large language models to generate human-readable, evidence-linked diagnostic rationales.
  The method first diversifies diagnostic reasoning through semantic expansion, then
  optimizes prompts for classification accuracy using only small labeled datasets
  and no model retraining.
---

# Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2511.12008
- Source URL: https://arxiv.org/abs/2511.12008
- Reference count: 40
- Key outcome: RECAP-PATH achieves >0.9 accuracy on breast and prostate pathology datasets while generating human-readable, evidence-linked diagnostic rationales without model retraining.

## Executive Summary
RECAP-PATH introduces a two-phase prompt optimization framework that enables multimodal large language models to generate interpretable diagnostic rationales for pathology images. The method first diversifies reasoning through semantic expansion, then optimizes for classification accuracy using only small labeled datasets. Evaluated on breast and prostate pathology datasets, the framework achieves state-of-the-art accuracy while producing descriptions aligned with expert pathological criteria.

## Method Summary
RECAP-PATH is a two-phase automated prompt optimization framework that generates interpretable diagnostic rationales for pathology images. Phase 1 maximizes semantic diversity of description prompts through terminology richness and uniqueness scoring. Phase 2 refines these prompts for classification accuracy using error reflection and candidate selection. The framework operates without model retraining, requiring only small labeled datasets (30-200 samples per class). It generates structured descriptions that ground predictions in explicit morphological features before classification.

## Key Results
- Achieves over 0.9 accuracy in binary cancer classification on breast and prostate pathology datasets
- Delivers up to 20% performance gains over baseline single-phase optimization approaches
- Produces pathology-style explanations that align with expert diagnostic criteria across multiple cancer types

## Why This Works (Mechanism)

### Mechanism 1
Decoupling diversity exploration from accuracy optimization yields prompts that are both clinically meaningful and high-performing. Phase 1 maximizes terminology richness and uniqueness, tolerating temporary accuracy drops. Phase 2 then selects and refines prompts that maximize classification accuracy while retaining learned diversity. Evidence shows accuracy decreases slightly during Phase 1 but rises rapidly in Phase 2, with convergence typically reached in about six rounds.

### Mechanism 2
Description-generation before classification improves both interpretability and accuracy by forcing explicit feature articulation. The MLLM generates structured text describing morphological features, which is then used alongside the classification prompt. This forces the model to ground predictions in articulated evidence, producing descriptions that explicitly list tissue organization, nuclear morphology, and diagnostic markers aligned with expert criteria.

### Mechanism 3
Iterative error reflection enables autonomous prompt improvement without gradient access or retraining. After each evaluation round, misclassified examples are collected. The MLLM reflects on error patterns and generates revised prompts incorporating targeted fixes, such as adding explicit criteria for distinguishing different cancer types. This self-improvement capability allows the framework to adapt to specific diagnostic challenges.

## Foundational Learning

- **Prompt Engineering for Vision-Language Models**
  - Why needed here: The entire framework operates through prompt refinement without model updates; understanding how prompts structure MLLM reasoning is prerequisite.
  - Quick check question: Can you explain why a description prompt asking for "discriminative attributes" might produce different outputs than one asking for "pathologist-style analysis"?

- **In-Context Learning**
  - Why needed here: RECAP-PATH relies on the MLLM's ability to follow instructions and adapt behavior from prompts alone.
  - Quick check question: What is the difference between few-shot prompting (providing examples) and instruction-based prompting (providing guidelines)?

- **Semantic Embedding Spaces and Clustering**
  - Why needed here: The paper uses UMAP visualization of description embeddings to demonstrate semantic disentanglement; interpreting these results requires understanding what embeddings capture.
  - Quick check question: If two classes have overlapping UMAP clusters, does this necessarily indicate poor classification performance? Why or why not?

## Architecture Onboarding

- **Component map**: Description Generator (MLLM + description prompt) → Classifier (MLLM + classification prompt + description) → Scorer (diversity/accuracy) → Reflector (MLLM analyzes errors) → Prompt Pool (maintains top candidates)

- **Critical path**: Initialize seed prompt → Generate descriptions for training set → Classify and collect errors → Reflect on errors → Generate revised prompts → Score candidates → Retain top prompts → Repeat until convergence

- **Design tradeoffs**:
  - Label efficiency vs. stability: 30–100 labels suffice but optimization may be unstable with fewer samples
  - Diversity vs. convergence: Higher diversity in Phase 1 lengthens optimization but produces more robust prompts
  - Temperature settings: 0.0 for classification (determinism), 0.7 for description generation (creativity); switching these inverts the intended behavior

- **Failure signatures**:
  - Class collapse: Confusion matrix shows 100% prediction of one class → prompt encodes bias; examine seed prompt neutrality
  - Oscillating accuracy: Accuracy fluctuates without convergence → reduce candidate pool size or increase reflection depth
  - Generic descriptions: Outputs lack pathology-specific terms → verify terminology scoring and increase Phase 1 duration

- **First 3 experiments**:
  1. **Baseline replication**: Run zero-shot classification with initial prompt; record accuracy and class distribution to establish floor performance.
  2. **Ablate Phase 1**: Run single-phase (accuracy-only) optimization; compare final prompt diversity and description quality to two-phase results.
  3. **Cross-dataset transfer**: Apply prostate-optimized prompt to breast cancer data; measure performance drop to understand domain specificity requirements.

## Open Questions the Paper Calls Out

### Open Question 1
Can RECAP-PATH be effectively integrated into a full whole-slide image (WSI) pipeline to handle automated tissue selection and patch aggregation without losing accuracy or interpretability? The current evaluation was restricted to pre-cropped Regions of Interest (RoIs), avoiding the computational and interpretive challenges of processing entire gigapixel slides.

### Open Question 2
Does "slot-based calibration" allow optimized prompts to generalize across different datasets or institutions without requiring a complete re-optimization run? The optimized prompt remains dataset-specific and may not generalize effectively even within the same disease type.

### Open Question 3
Can the RECAP-PATH optimization strategy maintain performance consistency when applied to open-source, locally hosted MLLMs to mitigate the risks of proprietary API version drift? The framework depends on vendor-hosted MLLMs, which raises concerns about undocumented updates and version drift that may undermine reproducibility.

## Limitations

- Performance depends heavily on the MLLM's ability to accurately identify and articulate its own error patterns
- Phase 1 diversity optimization requires a comprehensive biomedical dictionary; performance may degrade if the term list is incomplete or biased
- Optimal prompts appear tightly coupled to specific pathology datasets, suggesting limited transferability without retraining

## Confidence

- **High confidence**: Classification accuracy improvements (0.9+ accuracy on binary tasks) and description quality alignment with expert criteria
- **Medium confidence**: Diversity preservation claims (2× improvement) and cross-model generalizability
- **Low confidence**: Scalability to multi-class problems beyond the three-class breast cancer setup and real-world clinical deployment readiness

## Next Checks

1. **Error reflection robustness test**: Systematically inject controlled noise into training labels and measure whether the reflection mechanism correctly identifies label corruption versus model errors
2. **Cross-dataset transferability validation**: Apply a breast-optimized prompt to entirely new pathology domains (e.g., dermatology) and quantify performance decay
3. **Terminology dictionary sensitivity analysis**: Evaluate optimization performance using systematically reduced term lists to determine minimum viable dictionary size for Phase 1 diversity gains