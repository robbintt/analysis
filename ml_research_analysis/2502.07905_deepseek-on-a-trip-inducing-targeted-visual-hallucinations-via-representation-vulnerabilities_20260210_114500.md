---
ver: rpa2
title: 'DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via Representation
  Vulnerabilities'
arxiv_id: '2502.07905'
source_url: https://arxiv.org/abs/2502.07905
tags:
- image
- rates
- hallucination
- questions
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research presents an adversarial embedding manipulation attack
  that induces targeted visual hallucinations in DeepSeek Janus multimodal models.
  By optimizing image embeddings to match target embeddings while maintaining visual
  similarity, the attack achieves hallucination rates up to 98% with SSIM 0.88 across
  COCO, DALL-E 3, and SVIT datasets.
---

# DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via Representation Vulnerabilities

## Quick Facts
- **arXiv ID**: 2502.07905
- **Source URL**: https://arxiv.org/abs/2502.07905
- **Reference count**: 40
- **Primary result**: Adversarial embedding manipulation attack achieves up to 98% hallucination rates with SSIM > 0.88 across COCO, DALL-E 3, and SVIT datasets

## Executive Summary
This research demonstrates a novel adversarial embedding manipulation attack that induces targeted visual hallucinations in DeepSeek Janus multimodal models. By optimizing image embeddings to match target embeddings while preserving visual similarity, the attack achieves remarkably high success rates across multiple datasets and model scales. The approach exploits vulnerabilities in vision-language integration mechanisms, revealing critical security implications for multimodal AI deployment. A novel multi-prompt hallucination detection framework using LLaMA-3.1 8B provides robust evaluation, with closed-form questions proving more effective than open-ended ones for detecting induced hallucinations.

## Method Summary
The attack optimizes source images to minimize MSE between mean-pooled patch embeddings of source and target images, driving the multimodal model to generate descriptions of target objects while maintaining visual similarity to source images. Using Adam optimizer (learning rate 0.007, max 10,000 iterations) in pixel space, the method processes images through DeepSeek's VisionTower, mean-pools patch embeddings, and stops when L2 distance ≤ 1.44 and cosine similarity ≥ 0.95. The approach achieves high hallucination rates while preserving SSIM > 0.88 with source images across COCO, DALL-E 3, and SVIT datasets.

## Key Results
- Achieved hallucination rates up to 98% with SSIM > 0.88 across multiple datasets
- Successfully exploited vulnerabilities in both 1B and 7B model variants
- Closed-form questions proved more effective than open-ended ones for detecting induced hallucinations
- Demonstrated significant transferability concerns across model scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing image embeddings to minimize distance to target embeddings causes hallucination of target content while preserving visual similarity to source.
- Mechanism: Minimizes MSE between mean-pooled patch embeddings of adversarial and target images, driving adversarial representation in vision encoder's latent space closer to target's.
- Core assumption: Vision encoder's embedding space is sufficiently linear/smooth that pixel perturbations can map to desired embedding points without destroying visual semantics.
- Evidence anchors: Abstract states "optimizing image embeddings to match target embeddings while maintaining visual similarity"; section 3.1 formulates optimization objective; related work on gradient-guided attention supports manipulation effects.
- Break condition: If vision encoder's embedding space is highly non-linear with respect to pixel space, or if gradient path is blocked, direct optimization fails.

### Mechanism 2
- Claim: Vision-language integration point is a critical vulnerability where semantic information transfers from visual to language modality.
- Mechanism: Adversarial perturbation alters visual tokens provided to LLM backbone, which generates text consistent with corrupted visual input.
- Core assumption: LLM treats visual tokens as trustworthy grounding without intrinsic mechanism to detect adversarial manipulation.
- Evidence anchors: Abstract notes "vision-language integration mechanisms introduce specific vulnerabilities"; related work on visual token uncertainty supports token interface as key failure point.
- Break condition: If multimodal model incorporates verification step or is trained to be robust to out-of-distribution visual tokens, attack effectiveness reduces.

### Mechanism 3
- Claim: Closed-form binary questions are more effective at revealing induced hallucinations than open-ended questions.
- Mechanism: Evaluation framework uses LLaMA-3.1 8B to judge responses; closed-form questions elicit unambiguous "Yes/No" answers making detection trivial.
- Core assumption: Judge LLM can accurately compare short binary answers against ground truth with high precision.
- Evidence anchors: Abstract states "closed-form questions proving more effective than open-ended ones"; results analysis shows closed-form achieves higher hallucination rates.
- Break condition: If model is fine-tuned to refuse binary questions or judge LLM's prompt is flawed, detection yields inaccurate rates.

## Foundational Learning

- **Concept: Mean Pooling of Patch Embeddings**
  - Why needed here: Attack targets averaged representation rather than individual patch embeddings, essential for implementing loss function.
  - Quick check question: How is single embedding vector for image computed from Vision Transformer's patch embeddings output?

- **Concept: Structural Similarity Index Measure (SSIM)**
  - Why needed here: Primary metric demonstrating adversarial images remain visually similar to originals, proving attack stealthiness.
  - Quick check question: Why is SSIM preferred over simple pixel-wise MSE for measuring perceptual quality of image?

- **Concept: Adversarial Transferability**
  - Why needed here: Paper demonstrates attacks on 7B model have some effect on 1B model, crucial for understanding broader security implications.
  - Quick check question: What is the phenomenon called when adversarial example for model A successfully fools model B, and why does it increase security risk?

## Architecture Onboarding

- **Component map**: Pixel Input -> Vision Encoder (VisionTower) -> Mean Pooling Layer -> Projector/Adapter -> LLM Backbone -> Response -> Judge Model (LLaMA-3.1 8B)

- **Critical path**: Vulnerability exploited on path from Pixel Input -> Vision Encoder -> Mean Pooling -> LLM. Attack modifies pixel input to manipulate Mean Pooling layer output.

- **Design tradeoffs**:
  - Attack Success vs. Visual Fidelity: Higher learning rate (0.007) increases hallucination rates but lowers SSIM; lower rate (0.001) maintains high SSIM with slightly lower success.
  - Model Scale: Larger models (7B) show higher base performance but are more vulnerable; smaller models (1B) show more resilience but lower performance.

- **Failure signatures**:
  - High Source/Target SSIM: Invalid test if adversarial image too similar to target visually; goal is high SSIM with source and low SSIM with target.
  - High Base Hallucination Rate: On DALL-E 3, model shows high baseline rates even on original images, making induced hallucinations hard to distinguish.

- **First 3 experiments**:
  1. Reproduce Baseline Attack: Implement embedding optimization loop on COCO dataset pair using provided hyperparameters; confirm >90% hallucination rate with SSIM > 0.70.
  2. Ablate Learning Rate: Repeat attack using lower learning rate (0.001); compare SSIM and hallucination rate trade-off against baseline.
  3. Test Cross-Model Transfer: Generate adversarial examples using Janus-Pro 7B; evaluate on Janus-Pro 1B to measure degradation and confirm limited transferability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can randomized smoothing or neuron-level defense mechanisms effectively mitigate embedding manipulation attacks in multimodal models?
- Basis in paper: [explicit] Conclusion explicitly states need to explore randomized smoothing and neuron-level defenses against embedding manipulation attacks.
- Why unresolved: Paper demonstrates attack effectiveness but does not implement or evaluate any defense strategies.
- What evidence would resolve it: Empirical evaluation showing reduced hallucination rates when applying randomized smoothing or neuron-level defenses.

### Open Question 2
- Question: To what extent do embedding manipulation attacks transfer across fundamentally different multimodal architectures?
- Basis in paper: [inferred] Only tests transferability between Janus-Pro 7B and 1B variants, finding "limited transferability" to smaller model.
- Why unresolved: Study restricts evaluation to DeepSeek Janus variants, leaving open whether architectural differences provide inherent robustness.
- What evidence would resolve it: Systematic evaluation of attacks generated on Janus-Pro against LLaVA, BLIP-2, MiniGPT-4, and Qwen-VL.

### Open Question 3
- Question: What underlying mechanisms cause closed-form questions to consistently yield higher hallucination detection rates than open-ended questions?
- Basis in paper: [inferred] Results consistently show closed-form questions achieve higher hallucination rates, but paper does not investigate why this pattern emerges.
- Why unresolved: Empirical finding documented but not mechanistically explained.
- What evidence would resolve it: Ablation studies analyzing model attention weights and internal representations when processing different question formats.

## Limitations
- Evaluation framework relies heavily on LLaMA-3.1 8B as automated detector without human validation evidence
- Source-target image pairing methodology remains underspecified with vague "diverse semantic relationships" criteria
- Reported attack effectiveness on DALL-E 3 problematic due to model's already high baseline hallucination rates
- Security implications asserted but not empirically validated with defensive mechanisms tested

## Confidence

**High Confidence**: Technical feasibility of embedding optimization attack and core mechanism (manipulating visual embeddings to influence LLM outputs) well-supported by experimental methodology and results. SSIM-based visual fidelity preservation demonstrably achieved.

**Medium Confidence**: Comparative effectiveness of closed-form versus open-ended questions for hallucination detection depends entirely on reliability of automated judge model; human validation would strengthen this claim.

**Low Confidence**: Security implications regarding real-world deployment are asserted but not empirically validated; paper does not test whether defensive mechanisms could effectively mitigate vulnerabilities.

## Next Checks

1. **Human Validation of Detection Framework**: Recruit human annotators to independently score subset of MLLM responses on both adversarial and clean images, comparing agreement rates with LLaMA-3.1 8B judgments.

2. **Cross-Dataset Generalization Test**: Apply attack to images from additional domains (medical imaging, satellite imagery, surveillance footage) with varying semantic content to assess effectiveness correlation with image complexity.

3. **Defense Mechanism Evaluation**: Implement and test three baseline defenses—adversarial training on optimized embeddings, input preprocessing with denoising autoencoders, and output verification via consistency checking—to quantify mitigation effectiveness.