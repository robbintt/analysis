---
ver: rpa2
title: Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation
  Solution
arxiv_id: '2510.18019'
source_url: https://arxiv.org/abs/2510.18019
tags:
- languages
- language
- watermarking
- translation
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current multilingual watermarking methods fail to remain robust
  under translation attacks in medium- and low-resource languages, primarily due to
  tokenizer limitations that fragment words into subword units. This vulnerability
  allows adversaries to bypass watermark detection by translating text, undermining
  the security of LLM outputs across diverse languages.
---

# Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution

## Quick Facts
- **arXiv ID**: 2510.18019
- **Source URL**: https://arxiv.org/abs/2510.18019
- **Authors**: Asim Mohamed; Martin Gubri
- **Reference count**: 39
- **Primary result**: STEAM back-translation method achieves +40 percentage points TPR@1% and +0.19 AUC over existing methods across 17 languages

## Executive Summary
Current multilingual watermarking methods exhibit significant vulnerabilities when text undergoes translation, particularly in medium- and low-resource languages. This weakness stems from tokenizer fragmentation of words into subword units, which creates exploitable gaps in watermark detection. The proposed STEAM method addresses this fundamental limitation through a back-translation approach that restores watermark robustness lost during translation. Extensive experiments demonstrate substantial performance improvements, with average gains of +40 percentage points in true positive rate at 1% false positive rate across 17 tested languages.

## Method Summary
STEAM introduces a back-translation-based detection method designed to restore watermark strength compromised by translation attacks. The approach works by translating watermarked text back to its original language before detection, effectively reversing the fragmentation effects caused by tokenizers. Unlike previous methods, STEAM claims compatibility with any existing watermarking scheme and maintains non-invasive operation. The method leverages translation resources to reconstruct the original tokenized structure, thereby preserving the watermark's detectability across language boundaries.

## Key Results
- STEAM achieves average gains of +0.19 AUC and +40 percentage points TPR@1% over existing methods
- Maximum improvements reach +0.33 AUC and +64.5 percentage points TPR@1% in tested scenarios
- Validated across 17 languages, demonstrating robustness across diverse language families and resource levels

## Why This Works (Mechanism)
The core mechanism exploits the relationship between tokenization patterns and watermark detectability. When text is translated, the original tokenizer's subword segmentation is lost, breaking the watermark's structure. STEAM reverses this by back-translating to reconstruct the original language's tokenization patterns, thereby restoring the watermark's integrity. This approach effectively decouples watermark detection from the specific tokenization artifacts that make direct detection vulnerable to translation attacks.

## Foundational Learning

**Subword Tokenization**: Breaking words into smaller units for efficient language model processing. *Why needed*: Tokenization directly affects how watermarks are embedded and detected in text. *Quick check*: Examine how different tokenizers fragment the same word across languages.

**Back-Translation**: Translating text from target language back to source language. *Why needed*: Reconstructs original tokenization patterns lost during translation. *Quick check*: Compare tokenization before and after back-translation for the same content.

**Watermark Detection Metrics**: AUC (Area Under Curve) and TPR@FPR (True Positive Rate at False Positive Rate). *Why needed*: Quantify detection performance across different attack scenarios. *Quick check*: Calculate both metrics on balanced test sets with known watermarks.

## Architecture Onboarding

**Component Map**: Input Text -> Back-Translation Engine -> Watermark Detector -> Output Confidence Score

**Critical Path**: The back-translation step is critical, as it directly determines whether the original watermark structure can be reconstructed sufficiently for detection.

**Design Tradeoffs**: STEAM trades computational overhead (back-translation cost) for improved detection robustness. The method must balance translation quality against detection accuracy, as poor back-translations may introduce artifacts that further obscure watermarks.

**Failure Signatures**: Detection failure occurs when back-translation quality is insufficient to reconstruct original tokenization patterns, or when the target language lacks adequate translation resources for accurate reconstruction.

**First Experiments**: 1) Test STEAM on a single language pair with known translation quality to establish baseline performance. 2) Compare detection rates with and without back-translation on watermarked content. 3) Evaluate detection robustness against different translation systems (Google Translate, DeepL, etc.).

## Open Questions the Paper Calls Out

None identified in the provided materials.

## Limitations

- Effectiveness depends on availability and quality of translation resources, particularly problematic for extremely low-resource languages
- Computational overhead from back-translation step may impact real-world deployment scalability
- Interaction effects between different watermarking schemes and back-translation process require further investigation
- Limited evaluation on languages with fewer than 100,000 parallel sentence pairs

## Confidence

**High Confidence**: Empirical demonstration of current methods' failure under translation attacks with measurable STEAM improvements in tested language set.

**Medium Confidence**: Generalizability of STEAM across all medium- and low-resource languages and claimed compatibility with arbitrary watermarking methods.

**Low Confidence**: Complete non-invasiveness and absence of linguistic quality degradation from STEAM's back-translation process.

## Next Checks

1. Evaluate STEAM's performance on languages with extremely limited parallel corpora (fewer than 100,000 sentence pairs) to test robustness in truly low-resource scenarios.

2. Conduct user studies to assess whether STEAM-introduced back-translation affects the perceived quality and naturalness of generated text across different language pairs.

3. Benchmark the computational overhead and latency introduced by STEAM's back-translation step compared to baseline watermark detection methods, measuring end-to-end processing time.