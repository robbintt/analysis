---
ver: rpa2
title: 'Visualization Generation with Large Language Models: An Evaluation'
arxiv_id: '2401.11255'
source_url: https://arxiv.org/abs/2401.11255
tags:
- visualization
- data
- llms
- prompt
- nl2vis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) for generating
  Vega-Lite visualizations from natural language queries using the nvBench dataset.
  Six representative LLMs are tested with eight prompt strategies, including few-shot,
  zero-shot, and reasoning-oriented approaches.
---

# Visualization Generation with Large Language Models: An Evaluation

## Quick Facts
- arXiv ID: 2401.11255
- Source URL: https://arxiv.org/abs/2401.11255
- Reference count: 40
- Key outcome: Few-shot prompting generally outperforms other strategies for LLM visualization generation, with significant variation across models, prompts, and chart types

## Executive Summary
This paper evaluates six representative LLMs on generating Vega-Lite visualizations from natural language queries using the nvBench dataset. Eight prompt strategies are tested, including few-shot, zero-shot, and reasoning-oriented approaches. Performance is measured by validity (JSON parses), legality (data correctness), and accuracy (chart type). Results show few-shot prompting consistently outperforms other strategies, but simpler chart types don't always yield better results, and higher-performing models don't consistently excel across all prompts. The study also identifies significant limitations in the nvBench benchmark itself.

## Method Summary
The paper evaluates six open-source LLMs (e.g., QWEN2-7B-Instruct, Llama3-8B) on the nvBench dataset test set using eight prompt strategies (Zero-shot, Few-shot, CoT variants). The dataset was filtered to include only single-table instances and exclude grouping line charts with specific aggregation mismatches. Performance is measured by three metrics: Vis Accuracy (matching mark type), Validity (JSON parses and renders), and Legality (data attributes in generated visualization match ground truth). Temperature is set to 0 for one-round generation inference.

## Key Results
- Few-shot prompting consistently outperforms other strategies, including reasoning-oriented approaches
- Simpler chart types do not always yield better results due to shorter, less semantically dense queries
- Higher-performing models do not consistently excel across all prompt strategies
- The nvBench benchmark contains significant noise including mismatched queries and incorrect underlying data

## Why This Works (Mechanism)

### Mechanism 1: Exemplar-Grounded Syntax Retrieval
LLMs lack inherent familiarity with Vega-Lite's declarative grammar, so providing explicit code examples allows pattern matching from the prompt's context window to replicate syntactic structures. Without these exemplars, models rely on pre-training knowledge biased toward procedural languages rather than declarative JSON grammars.

### Mechanism 2: Attention Dilution in Planning Stages
Adding explicit planning stages (e.g., PS-Plus-CoT) may degrade performance by shifting processing capacity from syntax enforcement to semantic planning. Poorly aligned planning instructions introduce noise that increases Vega-Lite format errors.

### Mechanism 3: Contextual Density in Ambiguous Queries
Performance on simpler charts may drop because their associated natural language queries are often shorter and less semantically dense, providing fewer cues for the LLM to infer correct encodings. Models may overgeneralize "x/y" encodings to charts requiring angular encodings.

## Foundational Learning

- **Vega-Lite Grammar (Declarative)**: Understanding the difference between declarative (state-based, JSON) and imperative (procedural, Python) code is vital, as failures are attributed to LLMs' lack of exposure to declarative formats.
  - Quick check: Can you distinguish between a `transform` block and an `encoding` block in a JSON specification?

- **Prompting Paradigms (Zero/Few-shot vs. CoT)**: Grasping the difference between providing examples (Few-shot) and providing reasoning instructions (Chain-of-Thought) is essential to understand why the former succeeds where the latter fails.
  - Quick check: Does "Zero-shot" mean no instructions or no examples?

- **Evaluation Metrics (Validity vs. Legality)**: Understanding the distinction between "does it run?" (Validity) and "is it the right chart with right data?" (Legality) is crucial because high Validity does not guarantee high Legality.
  - Quick check: If an LLM generates a valid JSON bar chart when a scatter plot was requested, is the output "Legal"?

## Architecture Onboarding

- **Component map**: Input Layer (nvBench Dataset) -> Prompt Constructor (Strategy Selection) -> Inference Engine (LLM) -> Output Parser (Vega-Lite JSON) -> Renderer/Comparator (Check Validity/Legality)
- **Critical path**: The Prompt Constructor is the highest-leverage component, with formatting determining ~20-30% variance in Legality
- **Design tradeoffs**: Self-Consistency offers highest accuracy but requires multiple inference passes; Few-shot balances cost/performance; Reasoning (CoT/Plan) is computationally expensive and potentially harmful to syntax accuracy
- **Failure signatures**: The "Pie Trap" (x/y encoding generation on Pie charts), Attention Drift (invalid JSON with PS-Plus-CoT), Benchmark Noise (mismatches between query and ground truth data)
- **First 3 experiments**: 1) Baseline Validation: Run Zero-shot vs. Few-shot on Pie and Bar charts to reproduce "simpler is worse" anomaly; 2) Error Classification: Check if "Illegality" stems from data mapping vs. syntax errors; 3) Prompt Ablation: Test hybrid prompt (Few-shot + Explicit Syntax Rules) for simple chart types

## Open Questions the Paper Calls Out

1. How does iterative, multi-round user interaction affect accuracy and alignment of LLM-generated visualizations compared to one-round end-to-end generation?

2. Do performance disparities and prompt sensitivities observed with Vega-Lite grammar persist when generating visualizations using procedural grammars like D3?

3. Can LLMs be utilized effectively to automatically identify and correct structural errors in existing benchmarks like nvBench?

4. What is the specific trade-off in resource efficiency and generalizability between specialized, fine-tuned models and general-purpose LLMs using few-shot prompting?

## Limitations

- Benchmark Reliability: The nvBench dataset contains significant noise including mismatches between queries and ground truth visualizations
- Generalizability: Testing is limited to six specific open-source models on one dataset
- Context Window Constraints: Large tables are filtered to fit context windows without specifying sampling method, potentially introducing bias

## Confidence

- **High Confidence**: Few-shot prompting consistently outperforming other strategies, and identification of benchmark noise issues
- **Medium Confidence**: Mechanisms explaining why simpler charts don't always perform better and why reasoning prompts can harm performance
- **Low Confidence**: The specific claim that few-shot prompting works due to "exemplar-grounded syntax retrieval" - this is a reasonable hypothesis but not directly tested

## Next Checks

1. Manually audit 50 random "incorrect" Legality results to determine what fraction are due to benchmark errors versus model failures

2. Systematically vary table size in prompts (while holding query constant) to measure how data density affects performance across different chart types

3. Test the best-performing prompt strategies on a different NL2VIS dataset (e.g., NL4DV) to assess whether nvBench-specific findings hold