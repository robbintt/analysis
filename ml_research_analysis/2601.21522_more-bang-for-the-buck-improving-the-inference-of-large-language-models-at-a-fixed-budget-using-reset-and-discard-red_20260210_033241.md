---
ver: rpa2
title: 'More Bang for the Buck: Improving the Inference of Large Language Models at
  a Fixed Budget using Reset and Discard (ReD)'
arxiv_id: '2601.21522'
source_url: https://arxiv.org/abs/2601.21522
tags:
- cost
- coverage
- pass
- https
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reset-and-Discard (ReD), a method to improve
  the efficiency of large language models (LLMs) on verifiable tasks under a fixed
  budget. The key problem is that standard solve-to-completion allocation leads to
  sublinear growth in the number of unique problems solved as budget increases, due
  to diminishing returns when some tasks are much harder than others.
---

# More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)

## Quick Facts
- **arXiv ID**: 2601.21522
- **Source URL**: https://arxiv.org/abs/2601.21522
- **Reference count**: 40
- **Primary result**: ReD improves LLM efficiency on verifiable tasks under fixed budget by implementing breadth-first solving strategy

## Executive Summary
This paper introduces Reset-and-Discard (ReD), a method to improve the efficiency of large language models (LLMs) on verifiable tasks under a fixed budget. The key problem is that standard solve-to-completion allocation leads to sublinear growth in the number of unique problems solved as budget increases, due to diminishing returns when some tasks are much harder than others. ReD addresses this by resetting to the next task after a fixed number of attempts and discarding solved tasks, implementing a breadth-first approach.

## Method Summary
ReD is a breadth-first approach that resets to the next task after a fixed number of attempts (τ) and discards solved tasks. The method implements a policy where inference attempts are distributed across problems rather than exhaustively solving each one. The authors prove that ReD strictly improves coverage@cost for any budget and difficulty distribution, with resetting every attempt (τ=1) being optimal. Theoretical analysis shows that if pass@k decays as a power law with exponent α<1, solve-to-completion yields sublinear coverage@cost growth, while ReD enables linear growth.

## Key Results
- ReD strictly improves coverage@cost for any budget and difficulty distribution
- For power-law decay with α<1, ReD enables linear growth in unique problems solved versus sublinear growth from solve-to-completion
- On HumanEval benchmark, ReD substantially increases problems solved at fixed budgets across attempts, tokens, and USD cost
- For smaller llama model, ReD outperforms larger models up to certain coverage levels

## Why This Works (Mechanism)
ReD works by distributing inference attempts across problems rather than exhausting resources on difficult tasks. By resetting after τ attempts and moving to new problems, the method avoids the diminishing returns of solve-to-completion, where easy problems get solved quickly but resources are wasted on intractable ones. The breadth-first allocation ensures more unique problems are solved within the same budget, leveraging the fact that many problems have low difficulty while a few are extremely hard.

## Foundational Learning
- **Coverage@cost metric**: Measures number of unique problems solved per unit budget - needed to quantify efficiency gains from different allocation strategies
- **Power-law decay of pass@k**: Describes how success probability decreases with attempts - quick check: verify this pattern holds across different task domains
- **Renewal theory**: Used to analyze the statistical properties of ReD's reset-and-discard policy - needed to prove theoretical optimality guarantees
- **Breadth-first allocation**: Distributing attempts across problems rather than depth-first solving - quick check: compare against other allocation strategies like weighted sampling
- **Difficulty distribution P(p)**: Probability distribution of problem difficulties - needed to model how tasks vary in solvability
- **Optimal τ=1 resetting**: Theoretical proof that resetting after every attempt maximizes coverage - quick check: validate this holds with correlated attempts

## Architecture Onboarding
**Component Map**: LLM inference -> Verifier -> Reset-and-Discard controller -> Task queue management

**Critical Path**: Task selection → Inference attempt → Verification → Reset decision → Next task selection

**Design Tradeoffs**: ReD sacrifices potential deep solving of individual problems for broader coverage across the problem set. The τ=1 optimal strategy maximizes coverage but may miss opportunities for solving particularly tractable problems with more attempts.

**Failure Signatures**: If pass@k doesn't follow power-law decay (α≥1), ReD's advantage diminishes. With correlated inference attempts or imperfect verification, the theoretical guarantees may not hold.

**First 3 Experiments**: 1) Measure pass@k decay pattern across different benchmarks to validate power-law assumption. 2) Test ReD with varying τ values to confirm τ=1 optimality empirically. 3) Evaluate ReD performance on non-coding verifiable tasks (math, proofs) to assess generalizability.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does ReD perform under imperfect verification methods (e.g., Best-of-N, majority vote, reward models)?
- Basis in paper: [explicit] "The topic of verification remains an open problem, and it would be of interest to study ReD under different verification methods such as Best-of-N, majority vote, and reward models."
- Why unresolved: All theoretical and empirical results assume a perfect verifier; no analysis of how verification noise affects the optimality of τ=1 or coverage@cost gains.
- What evidence would resolve it: Experiments measuring ReD's coverage@cost with approximate verifiers; theoretical analysis of how verification error rates interact with resetting intervals.

### Open Question 2
- Question: How can the ReD framework be extended to correlated inference attempts where memory of previous states is retained?
- Basis in paper: [explicit] "In future work, it would be natural to extend the ReD framework to correlated inference attempts, where memory of the previous states is retained across the reasoning trajectory."
- Why unresolved: Current theory assumes i.i.d. trials; sequential sampling with backtracking may invalidate the renewal theory analysis and the optimality proof for τ=1.
- What evidence would resolve it: Derivation of coverage@cost under correlated attempts; empirical comparison of ReD against sequential reasoning methods on the same budget.

### Open Question 3
- Question: Does the power-law exponent α < 1 hold consistently across diverse verifiable task domains (mathematics, formal proofs, etc.)?
- Basis in paper: [inferred] The sublinear-to-linear transition claim depends on α < 1, but experiments only cover HumanEval coding tasks.
- Why unresolved: Different task domains may have different difficulty distributions P(p), potentially yielding α > 1 where ReD's advantage is less dramatic.
- What evidence would resolve it: Measurement of pass@k power-law exponents across multiple benchmarks (MATH, Lean proofs, etc.); analysis of whether ReD's relative savings correlate with α.

## Limitations
- Theoretical optimality proof assumes power-law decay with α<1, but real-world LLMs may exhibit different decay patterns
- Experiments primarily validate on HumanEval, a single benchmark with relatively uniform problem structure
- Method's performance on highly heterogeneous task distributions or open-ended generation tasks remains untested
- Cost analysis assumes constant token pricing and doesn't account for batching effects or hardware-specific optimizations

## Confidence
- **High Confidence**: The theoretical proof that ReD improves coverage@cost for any difficulty distribution under the stated assumptions. The empirical demonstration that ReD outperforms solve-to-completion on the tested models and benchmark.
- **Medium Confidence**: The prediction accuracy of the power-law exponent estimation method when pass@k is unknown. The generalizability of results to other benchmarks beyond HumanEval.
- **Low Confidence**: The method's effectiveness on highly diverse task distributions with extreme difficulty variance, and its performance in non-verifiable tasks where success metrics are ambiguous.

## Next Checks
1. Test ReD across diverse benchmarks (e.g., Big-Bench, MMLU, GSM8K) to assess generalizability across task types and difficulty distributions.
2. Evaluate ReD performance when pass@k follows non-power-law decay patterns (exponential, log-normal) to validate theoretical assumptions.
3. Implement ReD in production settings with real cost structures including batching, caching, and hardware-specific optimizations to measure practical efficiency gains.