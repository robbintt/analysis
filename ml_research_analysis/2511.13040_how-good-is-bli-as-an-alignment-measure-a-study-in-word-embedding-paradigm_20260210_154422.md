---
ver: rpa2
title: 'How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm'
arxiv_id: '2511.13040'
source_url: https://arxiv.org/abs/2511.13040
tags:
- embeddings
- alignment
- multilingual
- languages
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates bilingual lexicon induction (BLI) as a metric
  for measuring alignment quality in word embeddings across traditional monolingual,
  multilingual, and combined approaches. A key finding is that BLI often under-reports
  alignment quality, especially for highly inflected languages like Sinhala and Tamil,
  due to reliance on exact word matches.
---

# How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm

## Quick Facts
- **arXiv ID:** 2511.13040
- **Source URL:** https://arxiv.org/abs/2511.13040
- **Reference count:** 0
- **Primary result:** Bilingual Lexicon Induction (BLI) often under-reports alignment quality, especially for highly inflected languages like Sinhala and Tamil, due to reliance on exact word matches.

## Executive Summary
This study evaluates bilingual lexicon induction (BLI) as a metric for measuring alignment quality in word embeddings across traditional monolingual, multilingual, and combined approaches. The authors find that BLI can be misleading, particularly for highly inflected languages, due to its reliance on exact word matches. They propose two key improvements: a stem-based BLI approach using morphological stemming for soft matching, and a vocabulary pruning technique to remove cross-lingual token interference in multilingual models. Combined alignment methods (e.g., C2+LaBSE) generally achieve the highest BLI scores, but multilingual embeddings perform competitively for low-resource languages when properly preprocessed.

## Method Summary
The study compares alignment techniques including RCSLS, VecMap, C1, LaBSE, C2+LaBSE, Procrustes, and Adv.+refine on BLI tasks. FastText embeddings (wiki and common-crawl) serve as the base for traditional alignment, while multilingual models (LaBSE, mBERT, XLM-R, LASER) provide pre-aligned embeddings. Evaluation uses precision at k (P@1, P@5, P@10) with nearest neighbor and CSLS retrieval. The authors introduce vocabulary pruning to filter non-target-language tokens from multilingual models and implement stem-based BLI using Algorithm 1 for inflected languages. Datasets include MUSE bilingual dictionaries for most pairs and a custom En-Si dataset from prior work.

## Key Results
- Vocabulary pruning significantly improves BLI scores for multilingual models, with up to 935% improvement for Sinhala (En-Si: 4.5% to 46.6%).
- Stem-based BLI improves alignment evaluation for inflected languages, showing gains like En-Si wiki: 17.5% to 23.4% P@1.
- Combined alignment methods (e.g., C2+LaBSE) generally achieve the highest BLI scores across language pairs.
- Multilingual embeddings perform competitively for low-resource languages when vocabulary pruning is applied.

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Pruning for Multilingual Embedding BLI
Pruning non-target-language tokens from multilingual embedding vocabularies prior to BLI evaluation significantly improves reported alignment scores. Multilingual models share a unified embedding space across all supported languages, so the nearest neighbor can be from any language. By removing tokens belonging to all languages other than the target pair via script-based filtering, BLI is forced to retrieve matches only within the relevant linguistic subspace.

### Mechanism 2: Stem-Based Soft Matching for Inflected Languages
Standard BLI's exact-match criterion under-reports alignment quality for highly inflected languages. Stem-based BLI accepts a match if the retrieved word's stem matches the expected target's stem, accounting for morphological variation without requiring every inflected form to be present in the test set.

### Mechanism 3: Combined Static and Contextualized Alignment (C2+LaBSE)
A two-stage alignment pipeline first maps static monolingual embeddings (e.g., via VecMap) and then refines them using a multilingual contextualized model (e.g., LaBSE). The final embedding is a linear combination of the remapped Stage 1 and Stage 2 embeddings, blending broad lexical coverage with context-sensitive alignment.

## Foundational Learning

**Concept: Bilingual Lexicon Induction (BLI)**
- Why needed: This is the primary evaluation metric for cross-lingual embedding alignment.
- Quick check: Given aligned English and Spanish embeddings, if the 5 most similar Spanish vectors to "dog" are ["perro", "gato", "casa", "perros", "comida"], what is the P@1 and P@5 for "dog" → "perro"?

**Concept: Word Embedding Alignment**
- Why needed: The paper compares multilingual embeddings to explicitly aligned monolingual spaces.
- Quick check: Why might a simple linear mapping fail to align two monolingual embedding spaces, and how does an orthogonal Procrustes constraint help?

**Concept: Inflectional Morphology**
- Why needed: A central finding is that BLI fails for inflected languages.
- Quick check: How does the number of unique surface forms for a verb like "to eat" compare in English versus a highly inflected language like Sinhala, and why does this impact a fixed-vocabulary test set?

## Architecture Onboarding

**Component map:**
FastText Static Embeddings -> Traditional Alignment (VecMap, RCSLS, Procrustes) -> Standard BLI Evaluation
FastText Static Embeddings -> C1 Alignment -> C2+LaBSE -> Combined BLI Evaluation
Multilingual Models (LaBSE, mBERT, XLM-R) -> Vocabulary Pruning -> Standard/Stem-Based BLI Evaluation

**Critical path:**
1. **Data Prep:** Download FastText embeddings and MUSE bilingual dictionaries; obtain custom En-Si dataset.
2. **Baseline Alignment:** Run VecMap or RCSLS on FastText embeddings.
3. **BLI Evaluation:** Run standard BLI (NN/CSLS) to get baseline score.
4. **Apply Pruning:** Filter vocabulary using script-based rules; re-run BLI.
5. **Apply Stem-Based BLI:** Integrate language-specific stemmer into Algorithm 1; re-evaluate.
6. **Run Combined Model:** Execute C2 pipeline with LaBSE embeddings; compare results.

**Design tradeoffs:**
- **Static vs. Multilingual:** Static embeddings are cheaper with good coverage but require explicit alignment; multilingual models are pre-aligned but computationally heavier and may sacrifice pair-wise accuracy for global balance.
- **Exact vs. Soft Match:** Exact BLI is standardized but penalizes inflected languages; stem-based BLI is more informative but requires quality stemmers and introduces hyper-parameters.
- **Unpruned vs. Pruned Vocab:** Pruning reveals true alignment for language pairs in multilingual models but discards code-mixed data that might be relevant for some applications.

**Failure signatures:**
- **Zero or Near-Zero BLI for Multilingual Models:** Likely indicates vocabulary is not pruned; model retrieves matches from high-resource languages.
- **Huge Gap Between En→X and X→En BLI for Inflected Language X:** Suggests inflection asymmetry; apply stem-based BLI in X→En direction or expand test set with inflected forms.
- **Combined C2 Model Degrades vs. Static Baseline:** Could be due to mismatch in embedding dimensionality, insufficient fine-tuning, or poor hyperparameters in linear combination.

**First 3 experiments:**
1. **Reproduce the Pruning Effect:** For LaBSE on En→Zh pair, measure BLI P@1 before and after script-based vocabulary pruning; compare magnitude of gain to Table 5.
2. **Test Stem-Based BLI Sensitivity:** For En→Si pair, implement Algorithm 1 with Sinhala stemmer; measure P@1, P@5, P@10 and compare gains to Table 4; ablate using weaker stemmer to observe performance drop.
3. **Compare Alignment Architectures:** Run and evaluate three configurations on En→Ta: (a) VecMap, (b) LaBSE (pruned), and (c) C2+LaBSE; report BLI scores to understand tradeoffs described in Table 2.

## Open Questions the Paper Calls Out

**Open Question 1:** Can a modified BLI metric, where declaring @N on a multilingual model of N languages is considered equivalent to @1 of a monolingual model, effectively validate optimal embedding alignment? The authors note this requires "a different degree of experiments" and is "kept as future work."

**Open Question 2:** Does the proposed stem-based BLI approach yield comparable accuracy improvements for other morphologically rich languages (e.g., Turkish, Tamil) beyond the tested Sinhala case? The universality of improvement for different inflection types remains unproven.

**Open Question 3:** How can vocabulary pruning be effectively implemented for language pairs that share a common script (e.g., English-Spanish) to mitigate cross-lingual token interference? The paper lacks a solution for Latin-script pairs where code-mixing cannot be filtered by simple character ranges.

## Limitations
- Evaluation methodology relies on MUSE bilingual dictionaries and specific test sets, creating potential bias in alignment assessment.
- Stem-based BLI approach depends heavily on quality and appropriateness of language-specific stemmers, which are not fully specified.
- Combined alignment approach (C2+LaBSE) requires significant computational resources and careful hyperparameter tuning, limiting practical applicability.

## Confidence

**High Confidence:** Vocabulary pruning significantly improves BLI scores for multilingual models (935% improvement for Sinhala is specific and measurable).

**Medium Confidence:** Stem-based BLI provides more accurate alignment assessment for inflected languages, though quality of stemmers and their appropriateness for each language pair introduce variability.

**Low Confidence:** Combined alignment methods (C2+LaBSE) generally achieve the highest BLI scores across all language pairs, as the claim uses "generally" which hedges the assertion and results show variability across different language pairs.

## Next Checks

1. **Ablation Study on Pruning:** Implement vocabulary pruning mechanism on LaBSE for multiple language pairs (including both Latin and non-Latin scripts) and measure BLI improvements; compare results to Table 5.

2. **Stemmer Quality Assessment:** For Sinhala and Tamil, test multiple stemmer implementations in the stem-based BLI framework; measure P@1, P@5, and P@10 for each stemmer to identify sensitivity to stemmer quality.

3. **Cross-Lingual Transfer Validation:** Evaluate the combined C2+LaBSE alignment method on a language pair not included in the original study (e.g., English-Turkish or English-Hindi); compare BLI scores against baseline methods to test generalizability.