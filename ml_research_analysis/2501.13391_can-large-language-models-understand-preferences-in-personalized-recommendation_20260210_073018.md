---
ver: rpa2
title: Can Large Language Models Understand Preferences in Personalized Recommendation?
arxiv_id: '2501.13391'
source_url: https://arxiv.org/abs/2501.13391
tags:
- user
- item
- rating
- query
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PERRECBENCH, a novel benchmark designed to
  evaluate Large Language Models (LLMs) for personalized recommendation by focusing
  on user preferences rather than rating prediction accuracy. The benchmark addresses
  the limitations of traditional metrics like MAE and RMSE, which can be influenced
  by user rating bias and item quality.
---

# Can Large Language Models Understand Preferences in Personalized Recommendation?

## Quick Facts
- arXiv ID: 2501.13391
- Source URL: https://arxiv.org/abs/2501.13391
- Reference count: 40
- Introduces PERRECBENCH, a novel benchmark for evaluating LLMs in personalized recommendation

## Executive Summary
This paper introduces PERRECBENCH, a novel benchmark designed to evaluate Large Language Models (LLMs) for personalized recommendation by focusing on user preferences rather than rating prediction accuracy. The benchmark addresses the limitations of traditional metrics like MAE and RMSE, which can be influenced by user rating bias and item quality. PERRECBENCH uses a grouped ranking framework, where users are ranked based on their preferences for a shared query item, and evaluates performance using pointwise, pairwise, and listwise ranking methods. Experiments with 19 LLMs reveal that current models struggle with personalized recommendation, with the best-performing model achieving only a moderate correlation with ground truth rankings.

## Method Summary
PERRECBENCH introduces a novel evaluation framework that shifts focus from rating prediction accuracy to understanding user preferences in personalized recommendation. The benchmark employs a grouped ranking approach where users are ranked based on their preferences for a shared query item. It evaluates model performance through three ranking paradigms: pointwise (individual preference prediction), pairwise (relative preference comparison), and listwise (full ranking optimization). The framework uses both handcrafted and real-world user profiles, with preference elicitation methods including rating simulation and preference reasoning. The benchmark addresses traditional evaluation limitations by normalizing for user rating bias and item quality effects.

## Key Results
- Current LLMs struggle with personalized recommendation, with best-performing model showing only moderate correlation with ground truth rankings
- Pairwise and listwise ranking approaches significantly outperform pointwise methods
- Model scaling laws do not consistently hold for recommendation tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on preference understanding rather than rating prediction. By using a grouped ranking framework, it captures relative preference relationships between users, which is more aligned with real-world recommendation scenarios. The three evaluation paradigms (pointwise, pairwise, listwise) provide complementary perspectives on model performance, while the use of both handcrafted and real user profiles ensures diverse evaluation scenarios.

## Foundational Learning
1. Preference Modeling
   - Why needed: Traditional rating-based approaches fail to capture nuanced user preferences
   - Quick check: Verify preference elicitation methods produce consistent rankings

2. Ranking Evaluation Metrics
   - Why needed: Standard metrics like MAE/RMSE are inadequate for preference-based recommendation
   - Quick check: Compare correlation between different ranking approaches

3. User Profile Representation
   - Why needed: User profiles are critical for personalized recommendation
   - Quick check: Evaluate profile quality impact on recommendation performance

4. Multi-format Training
   - Why needed: Different training formats capture different aspects of preference understanding
   - Quick check: Compare single vs. multi-format training effectiveness

## Architecture Onboarding

Component Map:
PERRECBENCH Framework -> User Profile Generator -> Preference Elicitation Module -> Ranking Evaluation Module -> Model Comparison Layer

Critical Path:
User Profile Generation -> Preference Elicitation -> Ranking Evaluation -> Performance Analysis

Design Tradeoffs:
- Single vs. Multi-format Training: Multi-format training shows promise but increases complexity
- Profile Generation: Handcrafted vs. Real profiles balance control vs. realism
- Ranking Paradigm: Choice affects computational complexity and performance characteristics

Failure Signatures:
- Poor correlation with ground truth rankings indicates preference understanding issues
- Inconsistent performance across ranking paradigms suggests training format problems
- Profile sensitivity may indicate overfitting to specific user characteristics

First Experiments:
1. Test basic ranking performance with simple handcrafted profiles
2. Compare pointwise vs. pairwise ranking on controlled dataset
3. Evaluate profile quality impact using A/B testing

## Open Questions the Paper Calls Out
The paper identifies several open questions, particularly around improving LLMs' understanding of user preferences through better pretraining data distributions and more effective supervised fine-tuning strategies. The weight merging approach from single-format training shows promise but requires further validation across different recommendation scenarios.

## Limitations
- Fundamental challenges remain in LLMs' ability to capture nuanced user preferences
- Benchmark may not fully capture all aspects of real-world recommendation scenarios
- Pretraining data distributions' impact on performance needs further exploration

## Confidence
- High Confidence: Traditional rating-based metrics' limitations and superiority of pairwise/listwise approaches
- Medium Confidence: Model scaling laws not consistently holding for recommendation tasks
- Medium Confidence: Supervised fine-tuning strategies' effectiveness, particularly weight merging approach

## Next Checks
1. Test benchmark with more diverse and domain-specific pretraining data to understand pretraining influence
2. Evaluate framework's robustness across different recommendation domains (music, news, e-commerce)
3. Conduct longitudinal studies to assess model performance maintenance as user preferences evolve