---
ver: rpa2
title: 'Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection
  on Low-Data Regimes'
arxiv_id: '2510.08589'
source_url: https://arxiv.org/abs/2510.08589
tags:
- text
- llms
- detection
- fine-tuned
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares traditional CNN-based models and multi-modal
  large language models (LLMs) for detecting artificial text overlays in images. It
  evaluates fine-tuned CNNs, zero-shot LLMs, and fine-tuned LLMs on a balanced dataset
  of 1,000 annotated images.
---

# Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes

## Quick Facts
- arXiv ID: 2510.08589
- Source URL: https://arxiv.org/abs/2510.08589
- Reference count: 11
- Primary result: Fine-tuning multi-modal LLMs on ~1,000 images achieves 36% higher accuracy vs. CNNs for detecting artificial text overlays.

## Executive Summary
This work evaluates traditional CNN-based models against multi-modal large language models (LLMs) for detecting artificial text overlays in images. The study compares fine-tuned CNNs, zero-shot LLMs, and fine-tuned LLMs on a balanced dataset of 1,000 annotated images. Results demonstrate that fine-tuning a multi-modal LLM on fewer than 1,000 images achieves up to 36% higher accuracy compared to CNN baselines, with precision of 0.98 and recall of 0.84. The findings highlight the efficiency and adaptability of fine-tuned LLMs for complex vision-language tasks in low-data regimes.

## Method Summary
The method involves fine-tuning Phi-3.5 Vision Instruct on a balanced 1,000-image dataset for binary classification of artificial text overlays. The training procedure freezes the vision tower, fine-tunes the image projector and LLM weights, uses BCE loss, 2 epochs, cosine LR scheduler, and 16 random crops per image. The approach contrasts with CNN baselines trained on 10,000 images and zero-shot LLM evaluation.

## Key Results
- Fine-tuned multi-modal LLM achieves precision 0.98, recall 0.84, accuracy 0.83 on 1,000-image balanced test set
- Outperforms CNN baseline (P=0.54, R=0.76, Acc=0.61) trained on 10,000 images by 36% accuracy
- Sequential prompting improves precision (0.75) but suffers low recall (0.51) due to lack of weight adaptation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-modal LLMs leverage holistic semantic context to disambiguate visual features that purely convolutional architectures often misinterpret.
- **Mechanism**: Unlike CNNs that process local patterns (e.g., text edges) and require engineered positional encodings, transformer-based LLMs use attention mechanisms to weigh the relationship between text and the entire scene. This allows the model to determine if text is "unnaturally aligned" or visually detached from background objects (e.g., distinguishing a "NETFLIX" logo on a TV screen from an overlaid caption) based on semantic coherence rather than just pixel proximity.
- **Core assumption**: The pre-training data of the LLM (Phi-3.5) already contains sufficient knowledge about object semantics and natural scene statistics to recognize "out-of-place" text.
- **Evidence anchors**:
  - [Page 2, Section 1]: Notes that without fine-tuning, models might mistakenly flag natural text (e.g., "NETFLIX" on a TV) due to lack of contextual reasoning.
  - [Page 9, Section 6]: Attributes poor CNN performance to "limited capacity to reason over complex semantic relationships and compositional cues."
  - [corpus]: The paper "Hands-on Evaluation of Visual Transformers for Object Recognition and Detection" (arXiv:2512.09579) supports the general claim that transformers utilize global context via self-attention, whereas CNNs focus on local patterns.

### Mechanism 2
- **Claim**: Targeted parameter updates efficiently align general visual-linguistic features to specific domain boundaries (e.g., "overlay" vs. "natural") with minimal data.
- **Mechanism**: The architecture uses a pre-trained vision tower and language model. By freezing the vision tower and fine-tuning the image projector and LLM weights, the model adjusts the "mapping" between visual embeddings and language tokens. This aligns the model's existing broad knowledge (learned from massive pre-training) to the specific binary classification task, requiring only ~1,000 samples to shift the decision boundary effectively.
- **Core assumption**: The visual features extracted by the frozen encoder are rich enough to support the distinction without further modification.
- **Evidence anchors**:
  - [Page 7, Section 5]: Describes freezing the vision tower while fine-tuning the image projector and LLM weights.
  - [Page 4, Section 3.4]: States that fine-tuning enables the model to generalize learned cues like "unnatural alignment" and "occlusion patterns" with minimal supervision.
  - [corpus]: "Enhancing Multi-modal Models with Heterogeneous MoE Adapters for Fine-tuning" (arXiv:2503.20633) generally supports parameter-efficient fine-tuning strategies for multi-modal models.

### Mechanism 3
- **Claim**: Weight adaptation is necessary to resolve specific visual-semantic ambiguities that prompt engineering alone cannot fix.
- **Mechanism**: Sequential prompting (zero-shot) adds explicit reasoning steps (identifying objects, then checking relationships), which improves precision but significantly hurts recall. This suggests that while explicit context helps, it fails to consistently generalize across varied styles without updating model weights to internalize the specific "overlay" concept.
- **Core assumption**: The failure of sequential prompting is due to a lack of learned domain-specific inductive bias rather than a failure of the prompt logic itself.
- **Evidence anchors**:
  - [Page 9, Section 6]: Shows sequential prompting improved precision (0.75) but had low recall (0.51) and accuracy (0.54) compared to fine-tuning, citing "lack of weight adaptation" as the limiting factor.
  - [Page 5, Section 3.3]: Details the sequential prompting strategy (OCR -> Object identification -> Overlay detection).
  - [corpus]: [corpus] evidence for the failure of sequential prompting specifically is weak or missing in provided neighbors.

## Foundational Learning

- **Concept: Vision Encoder Projection (Projector/Adapter Layers)**
  - **Why needed here**: The paper emphasizes fine-tuning the "image projector" while freezing the vision tower. Understanding how a projector maps vision embeddings (e.g., CLIP features) to the LLM's embedding space is critical to grasping why the model adapts without "breaking" its visual knowledge.
  - **Quick check question**: Why would freezing the vision tower while training the projector prevent catastrophic forgetting of general visual features?

- **Concept: Contextual vs. Texture Bias**
  - **Why needed here**: The study contrasts CNNs (which rely on texture/local features and OCR positional encodings) with LLMs (which use context). Recognizing this distinction explains why the CNN had high false positives on complex scenes.
  - **Quick check question**: Why does the paper suggest that "unnatural alignment" is a semantic/contextual cue rather than just a geometric one?

- **Concept: Precision-Recall Trade-offs in Zero-Shot vs. Fine-Tuning**
  - **Why needed here**: The Sequential Prompting method had high precision (0.75) but very low recall (0.51). Understanding this trade-off is vital for interpreting the results table and deciding when to use prompting vs. fine-tuning.
  - **Quick check question**: If avoiding false negatives is more critical than false positives for a specific moderation task, which approach (Fine-tuned vs. Sequential Prompting) appears better suited based on the paper's Table 2, and why?

## Architecture Onboarding

- **Component map**: Raw Image + Binary Classification Prompt -> Vision Tower (CLIP ViT, Frozen) -> Projector (MLP, Fine-tuned) -> LLM (Phi-3.5 Vision, Fine-tuned) -> Classification Output
- **Critical path**:
  1.  **Data Curation**: Must balance "artificial overlay," "natural text," and "no text" classes evenly (as per Section 4).
  2.  **Configuration**: Strictly freeze Vision Tower; set LLM and Projector to trainable.
  3.  **Training**: 2 epochs, batch size 1 (gradient accumulation=2), cosine learning rate (2e-4).

- **Design tradeoffs**:
  - **Frozen Vision Tower**: Sacrifices potential visual feature refinement for data efficiency and stability (prevents overfitting on small datasets).
  - **Short Epochs (2)**: Limits convergence depth but acts as a strong regularizer to prevent overfitting on the 1,000-image dataset.
  - **16 Random Crops**: Increases robustness to text position but significantly increases compute/memory overhead per sample.

- **Failure signatures**:
  - **CNN Baseline**: High false positive rate (Precision 0.54) where natural text (e.g., signage) is flagged as overlay due to positional heuristics failing semantic checks.
  - **Zero-Shot Sequential**: Very low recall (0.51), where the model becomes too conservative, missing subtle overlays that don't trigger the explicit reasoning chain.
  - **Instability**: The paper explicitly notes disabling fp16 to avoid numerical instabilities.

- **First 3 experiments**:
  1.  **Sanity Check (Zero-Shot)**: Run the off-the-shelf Phi-3.5-Vision model on a sample of 50 images (mixed overlay/natural) to establish the baseline accuracy (expected ~0.60) and identify specific "semantic" failures (like the "NETFLIX" example).
  2.  **Ablation on Projector Freezing**: Attempt to fine-tune the model with the Vision Tower *unfrozen* on the 1,000-image set to verify if performance degrades due to overfitting (monitor validation loss closely).
  3.  **Sequential Prompting Validation**: Implement the two-step OCR-then-reasoning prompt chain. Compare the Precision/Recall balance against the single-prompt zero-shot to confirm if the "context gain" justifies the drop in recall observed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the data-efficient fine-tuning strategy generalize to other complex vision-language tasks beyond artificial text overlay detection?
- Basis in paper: [explicit] The conclusion states the approach "is broadly applicable and can be extended to other complex vision-language tasks that require nuanced contextual understanding."
- Why unresolved: Only text overlay detection was evaluated; no experiments on other tasks (e.g., document tampering, deepfake detection, scene understanding) were conducted.
- What evidence would resolve it: Systematic evaluation on multiple distinct vision-language tasks using the same low-data fine-tuning protocol.

### Open Question 2
- Question: Would alternative multi-modal LLMs (e.g., Qwen2.5-VL, LLaVA) achieve comparable gains when fine-tuned with identical data and hyperparameters?
- Basis in paper: [inferred] Qwen2.5-VL was evaluated zero-shot (accuracy 0.78) but not fine-tuned, creating an unfair architectural comparison with fine-tuned Phi-3.5 (accuracy 0.83).
- Why unresolved: The paper does not isolate whether gains stem from Phi-3.5's architecture or from fine-tuning itself.
- What evidence would resolve it: Fine-tuning multiple LLM architectures under identical conditions and comparing performance deltas.

### Open Question 3
- Question: Does freezing the vision tower provide optimal regularization for low-data regimes, or would joint fine-tuning yield further improvements?
- Basis in paper: [inferred] The vision encoder was frozen by design (Table 1), but no ablation study tested unfreezing it.
- Why unresolved: Freezing may limit adaptation to task-specific visual features (e.g., overlay artifacts, font inconsistencies).
- What evidence would resolve it: Ablation experiments comparing frozen vs. fine-tuned vision encoder performance across varying dataset sizes.

### Open Question 4
- Question: Does the LLM advantage persist, shrink, or disappear when scaling to larger training datasets (10,000+ images)?
- Basis in paper: [inferred] The paper emphasizes low-data efficiency (1,000 images) but does not explore whether CNNs would catch up with abundant data.
- Why unresolved: The CNN was trained on 10,000 images but used a different architecture; no controlled scaling analysis was performed.
- What evidence would resolve it: Training both LLM and CNN baselines on matched datasets ranging from 100 to 50,000+ images.

## Limitations

- Dataset Generalization: Results are based on a balanced 1,000-image dataset with specific class distributions; performance on out-of-distribution examples or different domains is unknown.
- Prompt Dependency: Exact prompt templates are not specified in sufficient detail for complete reproducibility, suggesting potential sensitivity to prompt engineering.
- Computational Overhead: 16 random crops per image likely incurs significant memory and compute overhead; no runtime or memory benchmarks are provided.

## Confidence

- **High Confidence**: Fine-tuned multi-modal LLMs outperform both zero-shot and CNN baselines on the specific 1,000-image balanced dataset (directly supported by Table 2 metrics).
- **Medium Confidence**: Multi-modal LLMs leverage semantic context to disambiguate visual features (plausible but largely comparative evidence).
- **Low Confidence**: Weight adaptation is necessary rather than sufficient for resolving visual-semantic ambiguities (paper shows sequential prompting performs worse but doesn't explore alternatives).

## Next Checks

1. **Out-of-Distribution Testing**: Evaluate the fine-tuned model on a dataset with different image characteristics (e.g., different text fonts, backgrounds, or object types) to assess robustness and identify potential overfitting to the training distribution.

2. **Prompt Ablation Study**: Systematically vary the prompt templates used during both training and inference to quantify the sensitivity of the model's performance to prompt engineering, and to determine if sequential prompting with optimized prompts can approach fine-tuning performance.

3. **Resource Usage Benchmark**: Measure and compare the memory consumption, inference latency, and total training time (including data loading and augmentation) for the fine-tuned LLM versus the CNN baseline across different hardware configurations to provide empirical evidence for the claimed efficiency.