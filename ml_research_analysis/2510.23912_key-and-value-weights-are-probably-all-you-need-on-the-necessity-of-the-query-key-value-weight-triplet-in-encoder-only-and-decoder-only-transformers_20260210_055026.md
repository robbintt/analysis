---
ver: rpa2
title: 'Key and Value Weights Are Probably All You Need: On the Necessity of the Query,
  Key, Value weight Triplet in Encoder-Only and Decoder-Only Transformers'
arxiv_id: '2510.23912'
source_url: https://arxiv.org/abs/2510.23912
tags:
- attention
- weight
- skip
- weights
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that Query weights (WQ) in multi-head attention
  are redundant and can be replaced with the identity matrix, reducing attention parameters
  by 25% per layer. The theoretical analysis shows that attention depends on input
  only through the products XWQ, XWK, and XWV, enabling this elimination through basis
  transformations.
---

# Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Encoder-Only and Decoder-Only Transformers

## Quick Facts
- arXiv ID: 2510.23912
- Source URL: https://arxiv.org/abs/2510.23912
- Reference count: 13
- Primary result: Query weights (WQ) in multi-head attention can be replaced with identity matrix, reducing attention parameters by 25% per layer

## Executive Summary
This paper challenges the conventional wisdom that all three matrices in the Query-Key-Value (QKV) attention mechanism are necessary. Through theoretical analysis and empirical validation, the authors demonstrate that Query weights are redundant and can be eliminated by setting them to the identity matrix. Under simplified architectural assumptions, they prove that attention outputs remain identical when WQ is replaced with I. The work suggests that transformer architectures contain architectural redundancy, potentially enabling more parameter-efficient designs without sacrificing expressivity.

## Method Summary
The authors prove that Query weights in multi-head attention are mathematically redundant by showing that attention depends on input only through products XWQ, XWK, and XWV. Under assumptions of no normalization, skip connections only around attention, and linear self-attention, they demonstrate that all Query weights can be set to identity while maintaining identical outputs through basis transformations. Empirically, they train GPT-style models from scratch with WQ = I and compare them to standard baselines, both in direct comparison and through parameter reallocation experiments where saved parameters are added to the MLP.

## Key Results
- Theoretical proof shows Query weights can be replaced with identity matrix under simplified assumptions
- GPT-style models with WQ = I achieve comparable validation loss to standard baselines despite 8% fewer non-embedding parameters
- Models with WQ = I outperform parameter-matched baselines when saved parameters are reallocated to MLP
- Reduced models train stably at over 3× lower weight decay, suggesting implicit regularization benefits

## Why This Works (Mechanism)
The mechanism relies on the mathematical structure of attention computation. Since attention depends on input only through the products XWQ, XWK, and XWV, setting WQ = I simply changes the basis of the attention computation without affecting the final output. This is possible because the attention mechanism can absorb the identity transformation through appropriate adjustments to other components.

## Foundational Learning
- Multi-head attention mechanism: Understanding how queries, keys, and values interact to compute attention scores and weighted outputs
- *Why needed*: Forms the foundation for understanding what the Query weights actually compute and why they might be redundant
- *Quick check*: Verify that attention scores are computed as QK^T and outputs as softmax(QK^T)V

- Basis transformations in linear algebra: How changing basis representations affects linear transformations
- *Why needed*: Essential for understanding how setting WQ = I can be compensated through other transformations
- *Quick check*: Confirm that AX = A'XB when A' = AB^(-1) for invertible B

- Skip connections and normalization in transformers: How residuals and normalization interact with attention sublayers
- *Why needed*: Critical for understanding the theoretical assumptions and practical limitations of the proof
- *Quick check*: Verify that skip connections skip the entire attention sublayer including value projection

## Architecture Onboarding

**Component map**: Input -> Linear(WQ) -> Q -> Attention scores -> Weighted sum -> Output -> Skip connection -> Output
                                      -> Linear(WK) -> K
                                      -> Linear(WV) -> V

**Critical path**: Input → Linear(WQ) → Q → Attention scores (QK^T/√d) → Softmax → Weighted sum (softmax(QK^T)V) → Output → Skip connection → Final output

**Design tradeoffs**: The work suggests a fundamental tradeoff between parameter efficiency and architectural simplicity. By eliminating Query weights, models become more parameter-efficient but require careful handling of the remaining components to maintain expressivity. The implicit regularization benefit (lower weight decay requirements) suggests potential stability advantages.

**Failure signatures**: Models may fail to converge or show degraded performance if the remaining Key and Value weights cannot adequately compensate for the missing Query weights. Training instability might occur if weight initialization doesn't account for the modified architecture. The theoretical guarantees break down when practical transformer components like LayerNorm are present.

**3 first experiments**:
1. Verify that setting WQ = I in a standard attention implementation produces identical outputs to the full implementation under the theoretical assumptions (no LayerNorm, skip connections only around attention).
2. Test the WQ = I modification in a practical transformer implementation with standard components to assess real-world viability.
3. Compare training dynamics and weight decay sensitivity between standard and WQ = I models to investigate the implicit regularization hypothesis.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rely on strong architectural simplifications that don't hold in practical implementations
- Empirical results show mixed evidence, with parameter-counting adjustments complicating comparisons
- Analysis focuses on encoder-only and decoder-only transformers, not bidirectional encoder-decoder architectures
- Implicit regularization claims require more systematic investigation

## Confidence
- Theoretical analysis (Medium): The proof is mathematically sound under stated assumptions, but these assumptions significantly diverge from practical transformer implementations
- Parameter reduction claim (High): The mechanism for reducing attention parameters is well-established and verifiable
- Empirical validation (Medium): Results are promising but limited in scope and rely on parameter-counting adjustments
- Implicit regularization claims (Low): Interesting observation but requires more rigorous investigation

## Next Checks
1. Test the WQ = I modification in standard transformer implementations with LayerNorm, full residual connections, and nonlinear attention to verify practical viability beyond theoretical conditions
2. Conduct ablation studies comparing models with WQ = I against parameter-matched baselines (same total parameter count, not just attention parameters) to isolate the architectural impact
3. Investigate the implicit regularization hypothesis by systematically comparing weight decay sensitivity, training stability, and generalization across multiple runs and architectures