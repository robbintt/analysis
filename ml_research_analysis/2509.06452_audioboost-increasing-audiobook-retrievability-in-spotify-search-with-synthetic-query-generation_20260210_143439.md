---
ver: rpa2
title: 'AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic
  Query Generation'
arxiv_id: '2509.06452'
source_url: https://arxiv.org/abs/2509.06452
tags:
- queries
- query
- synthetic
- audiobook
- audiobooks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AudioBoost, a system that increases audiobook
  retrievability in Spotify Search via synthetic query generation. The method leverages
  Large Language Models to generate synthetic queries conditioned on audiobook metadata,
  which are then indexed both in Query AutoComplete and Search Retrieval to improve
  query formulation and retrieval.
---

# AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation

## Quick Facts
- arXiv ID: 2509.06452
- Source URL: https://arxiv.org/abs/2509.06452
- Authors: Enrico Palumbo; Gustavo Penha; Alva Liu; Marcus Eltscheminov; Jefferson Carvalho dos Santos; Alice Wang; Hugues Bouchard; Humberto Jesús Corona Pampin; Michelle Tran Luu
- Reference count: 13
- One-line primary result: +0.7% audiobook impressions, +1.22% audiobook clicks, and +1.82% audiobook exploratory query completions in online A/B testing

## Executive Summary
AudioBoost addresses the cold-start problem in audiobook search by generating synthetic queries using Large Language Models conditioned on audiobook metadata. The system creates 12 types of descriptors across 10 categories (genres, themes, characters, moods, settings, etc.) which are then transformed into synthetic queries indexed in both Query AutoComplete and BM25 retrieval systems. Offline evaluation demonstrates significant improvements in audiobook retrievability (56.97% vs 23.36% baseline), while online A/B testing confirms positive user engagement metrics with statistically significant lifts in impressions, clicks, and exploratory query completions.

## Method Summary
The method uses an LLM with chain-of-thought prompting and in-context examples to first generate descriptors across a 10-category taxonomy from audiobook metadata, then produces synthetic queries and compound queries. These synthetic queries are indexed in Query AutoComplete with a scoring function that combines median popularity and broadness (log(|A|+1)), and are also concatenated to document representations for BM25 retrieval. The entire pipeline is offline, ensuring zero latency impact while providing scalable cold-start support for new audiobooks.

## Key Results
- Offline retrievability share increased from 23.36% to 56.97% when using both QAC and document augmentation
- Online A/B test shows +0.7% audiobook impressions, +1.22% audiobook clicks, +1.82% exploratory query completions (p<0.01)
- Synthetic queries demonstrate high quality via LLM-as-a-judge scoring (quality, relevancy, diversity, broadness metrics)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic query generation expands the query space for cold-start items where historical user queries are sparse.
- Mechanism: An LLM generates 12 types of descriptors from audiobook metadata using chain-of-thought prompting, bridging the vocabulary gap between user search patterns and catalogued metadata.
- Core assumption: Users' natural search vocabulary for exploratory queries diverges from available metadata, and LLMs can approximate this vocabulary distribution.
- Evidence anchors: Abstract mentions LLM synthetic query generation; section 2.1 describes the 10-category taxonomy and 12 query types.
- Break condition: If user query vocabulary is already well-covered by existing metadata, synthetic queries add noise without retrievability gains.

### Mechanism 2
- Claim: Query AutoComplete integration shifts the query distribution toward exploratory audiobook queries, increasing retrieval likelihood.
- Mechanism: Synthetic queries indexed with scoring function s = median_popularity(q) × broadness(q) where broadness(q) = log(|A| + 1), exposing users to exploratory options they wouldn't independently formulate.
- Core assumption: Users will accept and click synthetic query suggestions at sufficient rates, and broad queries lead to higher audiobook engagement than narrow item-specific queries.
- Evidence anchors: Abstract confirms QAC indexing; section 2.2 details scoring function and re-ranker integration.
- Break condition: If users ignore synthetic suggestions or prefer narrow navigational queries, the query distribution shift fails.

### Mechanism 3
- Claim: Document augmentation with synthetic queries increases retrievability by creating additional term-matching pathways in sparse retrieval.
- Mechanism: Synthetic queries concatenated to document representations before BM25 indexing, providing both term frequency boosts and new vocabulary injection.
- Core assumption: BM25's bag-of-words matching benefits from expanded vocabulary, and added terms are relevant enough to improve precision rather than dilute results.
- Evidence anchors: Abstract mentions offline evaluation shows increased retrievability; section 3.1 Table 1 shows 56.97% retrievability share vs 23.36% baseline.
- Break condition: If synthetic queries are semantically misaligned with item content, document augmentation introduces false positive matches, degrading precision.

## Foundational Learning

- Concept: Retrievability metrics
  - Why needed here: Core evaluation uses retrievability share, not traditional relevance metrics. Understanding r(e) = Σq∈Q oq · f(k, e, q, c) is essential to interpret offline simulation results.
  - Quick check question: Can you explain why retrievability differs from precision/recall and why it's appropriate for cold-start evaluation?

- Concept: Document expansion (doc2query family)
  - Why needed here: AudioBoost's document augmentation directly applies doc2query principles. Prior work showed query prediction improves sparse retrieval.
  - Quick check question: How does document expansion differ from query expansion, and what are the latency implications of each?

- Concept: Cold-start problem in search
  - Why needed here: Entire motivation rests on interaction sparsity for new content types. Unlike recommendation cold-start, search cold-start involves vocabulary mismatch, not just feature sparsity.
  - Quick check question: Why does popularity-based ranking disadvantage new content types even when metadata is complete?

## Architecture Onboarding

- Component map: Query Generation Pipeline (offline) → QAC System (online) → Retrieval System (online) → Integration points: Query table feeds both QAC source and document augmentation indexing
- Critical path: Query generation quality → QAC acceptance rate → retrievability lift. If synthetic queries are irrelevant or users reject suggestions, downstream effects collapse.
- Design tradeoffs: Broadness vs. specificity (favors broad queries for coverage); Offline vs. online (zero latency but requires batch updates); Sparse vs. dense retrieval (BM25 limits semantic matching).
- Failure signatures: Low QAC click-through indicates suggestion quality/relevance mismatch; retrievability increase without engagement lift suggests results don't satisfy intent; corpus-wide precision degradation indicates over-augmentation.
- First 3 experiments:
  1. Taxonomy ablation: Generate queries using subset of descriptor categories to identify which dimensions drive lift
  2. Scoring function variant: Test popularity-only vs. broadness-weighted scoring to validate broadness premium assumption
  3. Document augmentation only: Isolate retrieval improvement by indexing synthetic queries in retrieval without QAC exposure

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on retrievability metrics and A/B testing lift without examining potential negative externalities on non-audiobook content types
- Broadness-weighted scoring function may over-prioritize generic queries at the expense of precise intent satisfaction
- No qualitative analysis of synthetic query relevance provided beyond LLM-as-a-judge scores

## Confidence
- **High confidence**: Offline retrievability improvements (56.97% vs 23.36% baseline) and online A/B test statistical significance
- **Medium confidence**: QAC contribution to overall lift (measured but mechanism not fully isolated)
- **Low confidence**: Long-term user behavior adaptation and precision preservation across corpus

## Next Checks
1. **Precision impact audit**: Measure entity-level precision before/after deployment to ensure synthetic queries don't degrade overall search quality
2. **Query diversity analysis**: Compare vocabulary overlap between synthetic queries and historical user queries to quantify vocabulary gap bridging
3. **Cold-start vs. warm-start separation**: Re-run A/B analysis on truly new audiobooks (no historical queries) versus existing catalog to isolate cold-start effects