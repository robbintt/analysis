---
ver: rpa2
title: 'Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and
  AdamW'
arxiv_id: '2507.01241'
source_url: https://arxiv.org/abs/2507.01241
tags:
- stochastic
- conjugate
- algorithm
- subgradient
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCSAdamW, a novel optimization algorithm
  that integrates stochastic conjugate subgradients with AdamW for training large
  language models. The method addresses the limitations of traditional SGD-based approaches,
  particularly their effectiveness on non-smooth, non-convex loss functions common
  in LLMs.
---

# Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW

## Quick Facts
- arXiv ID: 2507.01241
- Source URL: https://arxiv.org/abs/2507.01241
- Authors: Di Zhang; Yihang Zhang
- Reference count: 40
- Primary result: SCSAdamW achieves faster convergence and lower loss than Adam/AdamW on LLM training tasks

## Executive Summary
This paper introduces SCSAdamW, a novel optimization algorithm that integrates stochastic conjugate subgradients with AdamW for training large language models. The method addresses limitations of traditional SGD-based approaches on non-smooth, non-convex loss functions common in LLMs. By combining adaptive sampling, conjugate subgradient search directions, and decoupled weight decay, SCSAdamW achieves faster convergence and lower objective function values compared to Adam and AdamW. Experimental results on Wikitext-2, PennTreebank, AG-News, and IMDb datasets demonstrate significant improvements in both speed and accuracy of the optimization process.

## Method Summary
SCSAdamW is an optimization algorithm that replaces standard gradient descent with stochastic conjugate subgradient methods while retaining AdamW's adaptive learning rate and weight decay mechanisms. The algorithm computes search directions by finding optimal convex combinations of previous and current subgradients, uses adaptive sampling based on concentration inequalities to reduce computational burden, and applies decoupled weight decay. The method leverages theoretical guarantees showing that convergence of the search direction norm implies proximity to stationary points.

## Key Results
- SCSAdamW achieves faster convergence than Adam and AdamW on Wikitext-2, PennTreebank, AG-News, and IMDb datasets
- The method demonstrates lower final objective function values across all tested datasets
- Adaptive sampling reduces computational cost while maintaining theoretical convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the raw gradient direction with a conjugate subgradient direction yields faster per-iteration convergence on non-smooth LLM objectives.
- Mechanism: At each iteration, the algorithm solves a one-dimensional quadratic program to find the optimal convex combination λ*ₜ of the previous search direction dₜ₋₁ and current subgradient gₜ that minimizes the resulting norm. This produces a direction dₜ = (1−λ*ₜ)dₜ₋₁ + λ*ₜgₜ that accumulates curvature-like information without explicitly computing or storing Hessian matrices.
- Core assumption: The objective function is non-smooth and non-convex, and accumulated directional information provides better guidance than single-step gradients.
- Evidence anchors: [abstract] "employs a stochastic conjugate subgradient approach to determine search directions... preserving the key advantages of first-order methods while effectively addressing the nonconvexity and non-smoothness"; [Section 4.1] "Among all possible directions within the convex hull formed by these subgradients, −dₜ possesses the smallest norm".
- Break condition: If ⟨dₜ, gₜ⟩ becomes strongly negative (opposing directions), the combination may yield unstable updates; the paper notes λ*ₜ can become unstable near zero (Section 6).

### Mechanism 2
- Claim: Adaptive sample size selection based on concentration inequalities reduces gradient estimation variance while minimizing computational cost.
- Mechanism: Theorem 3 provides a lower bound on sample size |Nₜ| ≥ −8 log(ε/2)·(M+1)²/(κ²δₜ⁴) such that P(|Lₜ(θ) − L(θ)| ≤ κδₜ²) ≥ 1−ε. This dynamically adjusts batch size based on local Lipschitz properties and desired approximation quality.
- Core assumption: L and Lₜ are Lipschitz continuous with constant Lf, and per-iteration function values are bounded.
- Evidence anchors: [Section 3] "This approach ensures a theoretically grounded, and computationally realistic approach"; [Section 4.4] Theorem 3 provides the sample complexity bound with proof in Appendix A.
- Break condition: If Lipschitz constant is poorly estimated or function values exceed bound M, the sample size guarantee degrades; paper does not address how to estimate Lf or M in practice.

### Mechanism 3
- Claim: Using the conjugate subgradient direction dₜ in place of AdamW's first moment maintains adaptive step-size benefits while providing stronger search directions.
- Mechanism: SCSAdamW replaces the momentum term mₜ with the conjugate direction dₜ, retains second moment vₜ for per-parameter learning rate scaling, and applies decoupled weight decay. The update becomes θₜ = θₜ₋₁ − η·d̂ₜ/(√v̂ₜ + ζ) − ηλθₜ₋₁.
- Core assumption: The conjugate direction provides equivalent or superior noise smoothing compared to exponential moving average momentum.
- Evidence anchors: [Section 4.2, Table 1] Detailed comparison showing SCSAdamW uses "conjugate subgradient dₜ" where Adam/AdamW use "gradient gₜ" or "momentum mₜ"; [Section 5.2] "objective function values of SCSAdamW decrease the fastest" on Wikitext-2, PennTreebank, ag-news, IMDB.
- Break condition: Greater fluctuation in loss curves observed (Section 6, Figure 2) due to lack of momentum-based smoothing; paper suggests sigmoid clipping of λ*ₜ as potential remedy.

## Foundational Learning

- Concept: **Subgradient methods for non-smooth optimization**
  - Why needed here: LLM loss landscapes exhibit non-smoothness (e.g., from ReLU activations, piecewise losses); standard gradient descent assumes differentiability. Subgradients generalize gradients to points where derivatives don't exist.
  - Quick check question: If a function has a "kink" at point x where the left and right derivatives differ, what set does ∂f(x) represent?

- Concept: **Conjugate gradient methods and curvature exploitation**
  - Why needed here: The conjugate subgradient approach accumulates directional information to approximate curvature benefits without Hessian computation. Understanding why conjugate directions avoid redundant search is essential.
  - Quick check question: Why does conjugate gradient on a quadratic terminate in at most n iterations, and what property of the search directions enables this?

- Concept: **Concentration inequalities and sample complexity**
  - Why needed here: The adaptive sampling mechanism relies on Hoeffding's inequality to bound estimation error. Understanding the tradeoff between sample size and confidence is required to implement and tune the sampler.
  - Quick check question: In Hoeffding's inequality, how does the confidence interval width scale with sample size N, and what does this imply for batch size selection?

## Architecture Onboarding

- Component map:
  Input: θ (parameters), η (learning rate), β₂ (variance decay), λ (weight decay), ζ (epsilon)
  
  Loop until ||dₜ|| ≤ ε:
    1. Sample batch Nₜ → compute stochastic gradient gₜ
    2. Direction finding: solve 1D QP for λ*ₜ → dₜ = (1−λ*ₜ)dₜ₋₁ + λ*ₜgₜ
    3. Variance update: vₜ = β₂vₜ₋₁ + (1−β₂)gₜ²
    4. Bias correction: d̂ₜ = dₜ/(1−(λ*ₜ)ᵗ), v̂ₜ = vₜ/(1−β₂ᵗ)
    5. Parameter update: θₜ = θₜ₋₁ − η·d̂ₜ/(√v̂ₜ + ζ)
    6. Weight decay: θₜ ← θₜ − ηλθₜ₋₁
  
  Output: θ (trained parameters)

- Critical path:
  1. The λ*ₜ computation (lines 5-6 in Algorithm 1) is the novel component—verify the projection operator Π[0,1] handles edge cases when denominator approaches zero.
  2. Bias correction on d̂ₜ uses (λ*ₜ)ᵗ not β₁ᵗ—ensure numerical stability for early iterations.
  3. Termination condition on ||dₜ|| connects to Theorem 2: small ||dₜ|| implies small ||gₜ|| under stated conditions.

- Design tradeoffs:
  - Conjugate direction vs. exponential moving average: SCS provides theoretically minimal-norm direction but lacks decay-based forgetting; old subgradients may mislead current search.
  - Adaptive sampling vs. fixed batching: Reduces unnecessary computation but requires estimating Lipschitz constants and function bounds—paper does not provide practical guidance.
  - First moment elimination: No β₁ hyperparameter but introduces λ*ₜ dynamics; one fewer knob but potentially less stable (observed fluctuations in Figure 2).

- Failure signatures:
  - Oscillating loss with increasing amplitude: λ*ₜ instability near zero; implement sigmoid clipping as suggested.
  - Premature convergence: ||dₜ|| drops below ε but ||gₜ|| remains large; check ⟨gₜ, dₜ₋₁⟩ ≥ 0 condition from Theorem 2.
  - Memory divergence: Accumulated direction information becomes stale; implement periodic restart mechanism (Section 6 limitation).

- First 3 experiments:
  1. **Baseline reproduction**: Implement SCSAdamW on Wikitext-2 with reported hyperparameters (η=0.001, λ=0.001, 4-layer LSTM, hidden=256); verify loss curve matches Figure 2a within tolerance.
  2. **Ablation on direction component**: Compare SCSAdamW against AdamW-momentum (same hyperparameters, replace dₜ with standard mₜ); isolate contribution of conjugate direction vs. AdamW infrastructure.
  3. **Batch size sensitivity**: Test fixed batch sizes (32, 64, 128) against adaptive sampling per Theorem 3; measure total gradient evaluations to reach target loss to validate efficiency claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does applying a sigmoid-based smoothing function to the conjugate direction weighting parameter \(\lambda_t^*\) effectively reduce objective function fluctuations without sacrificing convergence speed?
- Basis in paper: [explicit] Section 6 notes that the method exhibits greater fluctuations than AdamW because it lacks a momentum decay rate \(\beta_1\). The authors suggest \(\lambda_t^* = \sigma(\text{clip}(\lambda_t^*, -5, 5))\) as a potential remedy.
- Why unresolved: The proposed sigmoid clipping is hypothesized to stabilize updates but has not been implemented or tested in the current experimental results.
- What evidence would resolve it: Ablation studies comparing the standard \(\lambda_t^*\) update against the sigmoid-smoothed version on the same datasets, analyzing variance in loss trajectories.

### Open Question 2
- Question: Can a periodic restart mechanism prevent the search direction from becoming misled by stale subgradient information?
- Basis in paper: [explicit] Section 6 identifies the accumulation of previous subgradients as a potential source of error that could cause stagnation or divergence, proposing periodic restarts as a solution.
- Why unresolved: The current algorithm (Algorithm 1) relies on a termination criterion based on the norm of the search direction, without resetting the direction history during the optimization process.
- What evidence would resolve it: Experiments determining if resetting the conjugate direction \(d_t\) at fixed intervals improves final accuracy or convergence speed on non-convex LLM loss landscapes.

### Open Question 3
- Question: Does the SCSAdamW algorithm retain its efficiency and convergence advantages when applied to Transformer-based architectures and larger-scale datasets?
- Basis in paper: [explicit] Section 6 states that experiments were limited to LSTM-based models and smaller datasets (e.g., Wikitext-2) due to resource constraints; [inferred] The title claims applicability to LLMs, which are predominantly Transformers, yet the validation uses recurrent architectures.
- Why unresolved: The computational overhead of calculating conjugate subgradients might scale differently with the parameter count and attention mechanisms of Transformers compared to the LSTM setup tested.
- What evidence would resolve it: Benchmarking the method on standard Transformer pre-training tasks (e.g., GPT-2 or BERT pre-training) against AdamW on identical compute hardware (GPUs).

## Limitations

- Theoretical assumptions about Lipschitz continuity and bounded function values are not empirically verified in practice
- Practical implementation details for adaptive sampling mechanism are underspecified, particularly estimating Lipschitz constants Lf and bounds M
- Lack of ablation studies to isolate the contribution of conjugate direction component versus standard momentum approaches

## Confidence

- **High confidence**: The integration of conjugate subgradient direction with AdamW infrastructure is novel and the convergence proof framework is sound
- **Medium confidence**: Experimental results show improved convergence, but lack statistical significance testing and ablation studies to isolate effects
- **Low confidence**: Practical implementation details for adaptive sampling and handling of edge cases in λ*ₜ computation are underspecified

## Next Checks

1. Implement the adaptive sampling mechanism per Theorem 3 and measure gradient evaluation efficiency compared to fixed batch sizes
2. Conduct ablation studies isolating the conjugate direction component by comparing against AdamW with standard momentum
3. Perform statistical significance testing across multiple random seeds to validate the robustness of reported improvements