---
ver: rpa2
title: Efficient Adversarial Attacks on High-dimensional Offline Bandits
arxiv_id: '2602.01658'
source_url: https://arxiv.org/abs/2602.01658
tags:
- attack
- reward
- bandit
- conference
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies adversarial attacks on offline bandit algorithms,
  where an attacker perturbs a publicly shared reward model''s weights before bandit
  training to hijack its behavior. Starting with linear models and extending to ReLU
  neural networks, the authors introduce three attack strategies: full trajectory,
  trajectory-free, and an efficient online score-aware (OSA) method.'
---

# Efficient Adversarial Attacks on High-dimensional Offline Bandits

## Quick Facts
- arXiv ID: 2602.01658
- Source URL: https://arxiv.org/abs/2602.01658
- Reference count: 40
- Primary result: Weight perturbations on publicly shared reward models enable near-100% attack success on offline bandits, especially in high-dimensional settings.

## Executive Summary
This paper introduces three adversarial attack strategies on offline bandit algorithms by perturbing publicly shared reward model weights before bandit training. The attacks exploit high-dimensional settings where small perturbations can redirect arm selection with near-100% success rates. Starting with linear reward models and extending to ReLU neural networks via NTK linearization, the authors demonstrate that as input dimensionality increases, the required perturbation norm decreases dramatically. Experiments on synthetic data and real Hugging Face evaluators achieve attack success rates approaching 100%, while a simple data shuffling defense only partially mitigates the attacks.

## Method Summary
The attack framework perturbs reward model weights to manipulate UCB scores and redirect bandit arm selection away from the optimal arm. Three attack formulations are introduced: Full Trajectory (adds constraints for every timestep), Trajectory-Free (samples random trajectories), and Online Score-Aware (OSA) which adds constraints only when the optimal arm has the highest UCB score. All methods solve a convex QP: min ||δ||² subject to linear constraints derived from UCB inequalities. For neural networks, NTK linearization approximates NN(X; θ+δ) ≈ NN(X; θ) + ∇_θNN(X; θ)^T δ, enabling the same QP formulation. The OSA method achieves near-100% attack success with only O(log T) constraints instead of O(TK), dramatically reducing computation time.

## Key Results
- Attack success rates reach near-100% on both synthetic and real Hugging Face reward models (aesthetic quality and compositional alignment)
- As input dimensionality increases, required perturbation norm decreases as O(d^{-1/2}), making high-dimensional settings especially vulnerable
- OSA method reduces attack computation time dramatically while maintaining effectiveness (constraints scale as O(log T) instead of O(TK))
- A simple shuffling defense partially mitigates attacks but does not eliminate them

## Why This Works (Mechanism)

### Mechanism 1: High-Dimensional Attack Feasibility
As input dimensionality increases, the perturbation norm required for successful attack decreases as O(d^{-1/2}), making high-dimensional settings (e.g., images) especially vulnerable. In high dimensions, mean estimation of random vectors becomes unstable when T/K << d. Small perturbations to the reward function can exploit this variance to redirect arm selection. The attack feasibility stems from the underdetermined linear constraint system having more degrees of freedom (d dimensions) than constraints (at most (T-K)(K-1)).

### Mechanism 2: Convex Optimization Reduction Via NTK Linearization
Attack design reduces to a convex quadratic program for both linear reward models and sufficiently wide neural networks. For linear r(X) = w^T X, UCB constraints become linear in δ, yielding QP: min ||δ||² s.t. δ^T T_{i,t} > R_{i,t}. For neural networks, NTK theory provides: NN_{θ+δ}(X) ≈ NN_θ(X) + ∇_θNN_θ(X)^T δ + O(W_max^{-1}), allowing the same QP formulation with gradient embeddings.

### Mechanism 3: OSA Constraint Sparsity
The Online Score-Aware (OSA) method achieves near-100% attack success with only O(log T) effective constraints instead of O(TK), dramatically reducing computation. OSA iterates through timesteps, only adding a constraint when the optimal arm has the highest UCB score. It targets the runner-up arm, incrementally building a minimal constraint set. Most timesteps require no intervention because suboptimal arms naturally lead.

## Foundational Learning

- Concept: **Upper Confidence Bound (UCB) Algorithm**
  - Why needed here: The attack directly manipulates UCB scores (Λ_t(i;δ)) to redirect arm selection. Understanding the exploration term √(2log t / N_i(t)) is essential for crafting constraints.
  - Quick check question: Can you derive why UCB achieves O(log T) regret and how the exploration bonus changes with pull count?

- Concept: **Quadratic Programming (QP) with Inequality Constraints**
  - Why needed here: All three attack methods (full-trajectory, trajectory-free, OSA) formulate the perturbation search as QP: min ||δ||² s.t. Tδ ≥ R. Complexity is O(|I|³ + d|I|²).
  - Quick check question: Given d=1000 variables and |I|=50 constraints, what's the approximate solve time scaling?

- Concept: **Neural Tangent Kernel (NTK) Theory**
  - Why needed here: Enables extension from linear to neural reward models by approximating NN(X; θ+δ) ≈ NN(X; θ) + ∇_θNN(X; θ)^T δ for wide networks.
  - Quick check question: Why does NTK approximation improve with network width, and what's the error term's dependency on W_max?

## Architecture Onboarding

- Component map: Logged data loader -> Reward model interface -> Constraint generator -> QP solver -> Attack executor
- Critical path: 1. Load logged datasets -> 2. Compute sample means and gradients per arm -> 3. Identify target trajectory or optimal arm -> 4. Build constraint matrix T and vector R -> 5. Solve QP for δ* -> 6. Apply perturbation θ ← θ + δ*
- Design tradeoffs:
  - Full-trajectory vs. OSA: Full-trajectory guarantees specific arm sequence but requires O(TK) constraints; OSA is O(log T) constraints but trajectory is implicit.
  - Attack norm vs. success rate: Smaller ||δ|| may fail in low dimensions; paper shows OSA nears optimal norm in high-d settings.
  - Target layer selection: Attacking all NN parameters is expensive; paper freezes most weights and perturbs only one hidden layer.
- Failure signatures:
  - ASR ≈ 25% (random baseline) -> Constraints likely infeasible; check d vs. constraint count
  - QP solver returns infeasible -> Data distributions may be degenerate or d < KT
  - Attack succeeds on synthetic but fails on real data -> Network insufficiently wide for NTK approximation; increase hidden layer size
- First 3 experiments:
  1. Synthetic linear validation: K=3 arms, d=1000, T=100. Run full-trajectory attack. Verify ASR=100% and ||δ||₂ decreases with increasing d.
  2. NN width sweep: Train single-hidden-layer ReLU networks with widths [100, 250, 500, 750, 1000]. Run OSA attack. Confirm ASR transitions from ~25% to 100% at width ~750.
  3. Defense robustness test: Apply shuffling defense to first T/2 logged data. Measure ASR reduction across all three attack methods. Verify partial mitigation as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a theoretically sound defense mechanism be developed that completely mitigates these weight-perturbation attacks?
- Basis in paper: [explicit] The paper states, "However, a complete defense remains open, highlighting an important direction for future research."
- Why unresolved: The proposed shuffling defense only partially mitigates the attack and lacks formal guarantees, serving merely as a practical heuristic.
- What evidence would resolve it: A defense strategy with proven upper bounds on attack success rates or robust regret guarantees under the specified threat model.

### Open Question 2
- Question: What are the formal theoretical guarantees regarding the constraint complexity and success bounds of the Online Score-Aware (OSA) attack?
- Basis in paper: [explicit] The authors note, "While a full theoretical analysis of the OSA attack is beyond the scope of this paper..."
- Why unresolved: Empirically, OSA reduces constraints to $O(\log T)$, but this efficiency lacks rigorous mathematical proof linking it to the attack's feasibility and success rate.
- What evidence would resolve it: A formal derivation proving the constraint bound and success probability of OSA compared to the full-trajectory method.

### Open Question 3
- Question: Do the attack feasibility guarantees (Theorem 3.3 and 3.4) extend to data-generating distributions that are near-product measures rather than strict product measures?
- Basis in paper: [explicit] The authors conjecture, "an even more general result could be established for near-product measures, which cover a broader class of distributions, though this lies beyond the scope of the present work."
- Why unresolved: Current proofs rely on the independence of entries in product measures (e.g., $\text{Cov}(P_i)=I$), which limits the applicability of the $e^{O(d^{-1/2})}$ norm bound to a narrow distribution class.
- What evidence would resolve it: A generalized proof of Theorem 3.4 that holds for distributions with dependent covariates or specific covariance structures.

## Limitations
- Defense via data shuffling provides only partial mitigation, not elimination. Effectiveness depends on exact implementation (per-arm vs. global shuffling).
- Real-world attack success depends on access to logged datasets and ability to perturb reward model weights—assumes attacker can intercept or modify the reward model before bandit training.
- Theoretical guarantees rely on NTK approximation, which requires very wide networks (width ≥ 750 neurons for 100% ASR in experiments).

## Confidence
- High confidence: Linear reward model attacks (Theorem 3.1, QP formulation correctness), high-dimensional attack feasibility (Theorem 3.4, ∥δ*∥₂ scaling), OSA constraint sparsity (empirical O(log T) scaling)
- Medium confidence: Neural network attacks via NTK linearization (requires very wide networks as confirmed experimentally), effectiveness on real Hugging Face reward models (validated only on two specific models)
- Low confidence: Defense robustness (only one simple defense tested), attack transferability across different bandit algorithms (only tested on UCB, ETC, ε-greedy)

## Next Checks
1. **NTK width dependency**: Systematically vary hidden layer widths [100, 250, 500, 750, 1000, 1250] and measure ASR to precisely identify the transition point where attacks become feasible.
2. **Defense ablation study**: Test multiple defense strategies beyond shuffling—including weight regularization, adversarial training of reward models, and differential privacy on logged data—to quantify robustness improvements.
3. **Transferability assessment**: Apply successful perturbations from one bandit algorithm (e.g., UCB) to others (ETC, ε-greedy) on the same reward model to measure cross-algorithm effectiveness.