---
ver: rpa2
title: 'Jodi: Unification of Visual Generation and Understanding via Joint Modeling'
arxiv_id: '2505.19084'
source_url: https://arxiv.org/abs/2505.19084
tags:
- image
- generation
- diffusion
- conference
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Jodi, a diffusion framework that unifies visual
  generation and understanding by jointly modeling the image domain and multiple label
  domains (depth, normal, albedo, edge, lineart, segmentation, human skeleton). The
  method uses a linear diffusion transformer with a role switch mechanism, where each
  domain can be a generation target, condition input, or ignored, enabling three tasks:
  joint generation, controllable generation, and image perception.'
---

# Jodi: Unification of Visual Generation and Understanding via Joint Modeling

## Quick Facts
- arXiv ID: 2505.19084
- Source URL: https://arxiv.org/abs/2505.19084
- Authors: Yifeng Xu; Zhenliang He; Meina Kan; Shiguang Shan; Xilin Chen
- Reference count: 40
- Key outcome: Proposes a diffusion framework that unifies visual generation and understanding across 7 domains using a role switch mechanism, demonstrating superior performance to both unified and specialist models on the Joint-1.6M dataset.

## Executive Summary
This paper introduces Jodi, a unified diffusion framework that simultaneously handles visual generation and understanding across multiple domains including depth, normal, albedo, edge, lineart, segmentation, and human skeleton. The core innovation is a role switch mechanism that allows each domain to function as a generation target, condition input, or ignored element during training and inference. By constructing the Joint-1.6M dataset with 200,000 high-quality images and introducing domain-invariant positional embeddings, the authors demonstrate that a single model can outperform both specialist models trained on individual tasks and other unified approaches, while using significantly less data and computational resources.

## Method Summary
Jodi employs a linear diffusion transformer architecture that extends the Deep Compression Autoencoder (DC-AE) framework to jointly model multiple visual domains. The model uses a role switch mechanism to dynamically assign each domain as either a generation target (random noise to be transformed), condition input (provides guidance), or ignored (excluded from the process). This flexibility enables three key tasks: joint generation (creating images with specified conditions across domains), controllable generation (generating images guided by selected domains), and image perception (understanding images across multiple visual domains). Domain-invariant positional embeddings ensure spatial consistency across different domain representations, while the unified training objective allows knowledge transfer between related tasks.

## Key Results
- Superior performance compared to specialist models on both generation and understanding tasks across 7 visual domains
- Achieves state-of-the-art results using significantly less training data (200,000 images) than specialist models typically require
- Demonstrates effective multi-task learning through the role switch mechanism, enabling flexible domain combinations for different applications
- Successfully extends to new visual domains beyond the original 7, showing the framework's scalability potential

## Why This Works (Mechanism)
The unification succeeds because it leverages shared visual representations across related domains while maintaining task-specific capabilities through the role switch mechanism. By treating domains as either targets, conditions, or ignored elements, the model learns to transfer knowledge between tasksâ€”understanding depth helps generate better segmentation, and vice versa. The domain-invariant positional embeddings ensure that spatial relationships remain consistent across all domains, preventing misalignment that would degrade performance. The linear attention mechanism reduces computational complexity while maintaining the ability to capture long-range dependencies essential for coherent image generation and understanding.

## Foundational Learning
- **Diffusion Transformers**: Why needed - They provide the autoregressive generation capability essential for producing high-quality images; Quick check - Can generate diverse, high-resolution images conditioned on various inputs
- **Role Switch Mechanism**: Why needed - Enables flexible task assignment where domains can be generated, conditioned upon, or ignored; Quick check - Successfully handles three distinct tasks (joint generation, controllable generation, image perception) within one framework
- **Domain-Invariant Positional Embeddings**: Why needed - Ensures spatial alignment across different visual representations (depth maps, segmentation masks, etc.); Quick check - Maintains coherent spatial relationships when combining information from multiple domains
- **Multi-Task Joint Modeling**: Why needed - Allows knowledge transfer between related visual tasks to improve overall performance; Quick check - Achieves better results than training separate specialist models on individual tasks
- **Deep Compression Autoencoder (DC-AE)**: Why needed - Provides efficient latent space representation for image reconstruction and generation; Quick check - Enables high-quality image generation while reducing computational requirements

## Architecture Onboarding
**Component Map**: Input Image -> DC-AE Encoder -> Joint Latent Space -> Role Switch Mechanism -> Transformer Blocks -> Domain-Specific Decoders -> Output Domains

**Critical Path**: The transformer blocks process the joint latent representation, where the role switch mechanism determines how each domain token is handled (generation target, condition, or ignored). This path is critical because it determines the cross-domain knowledge transfer and generation quality.

**Design Tradeoffs**: The unified approach trades some domain-specific optimization for broader generalization and efficiency. Using RGB space for all domains simplifies the architecture but limits representation accuracy for tasks like segmentation and pose estimation. The choice of 7 initial domains balances coverage with training feasibility.

**Failure Signatures**: Structural distortions in generated human figures indicate limitations in modeling complex anatomical structures. Similar colors for different segmentation classes reveal the constraints of RGB latent space for discrete classification tasks. Reduced performance on highly specialized domains suggests the need for domain-specific architectural adaptations.

**First Experiments**:
1. Test role switch mechanism by generating images with different domain combinations (e.g., depth+segmentation vs. lineart+albedo)
2. Evaluate domain-invariant positional embeddings by checking spatial consistency across generated and perceived domains
3. Compare performance against specialist models on individual tasks to verify the unified approach's effectiveness

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can domain-specific encoders and decoders effectively resolve the limitations of representing dense prediction tasks (like semantic segmentation or human pose) in RGB space?
- Basis: The authors state, "our model is currently limited to handling 12 clustered classes for the segmentation domain... [and] RGB space is also not the ideal choice for the openpose domain," proposing specialized architectures for future work.
- Why unresolved: The current reliance on a uniform Deep Compression Autoencoder (DC-AE) forces all domains into a shared RGB latent space, causing semantic collisions (e.g., similar colors for different classes) that limit resolution and accuracy.
- What evidence would resolve it: A comparative study where Jodi is modified to use sparse coordinate embeddings for pose or discrete tokens for segmentation, demonstrating improved mIoU or keypoint accuracy without sacrificing the unified generation capability.

### Open Question 2
- Question: Does simply scaling the training data volume effectively eliminate the structural distortions observed in complex generated subjects, such as human figures?
- Basis: The paper notes, "due to the limited size of our training dataset, the generated images may exhibit structural distortions, especially for human figures," and explicitly suggests incorporating more data as a solution for future work.
- Why unresolved: It is undetermined if the distortions are solely due to data scarcity or if they stem from the linear attention mechanism's difficulty in modeling long-range dependencies required for anatomical correctness.
- What evidence would resolve it: Training the existing architecture on a significantly larger dataset (e.g., >1M images) and evaluating the anatomical plausibility of generated humans using a dedicated metric like LAION-Aesthetics+.

### Open Question 3
- Question: To what extent does the noise inherent in automatically generated labels (pseudo-labels) constrain the upper bound of the model's image perception performance?
- Basis: The authors acknowledge that for the Joint-1.6M dataset, "predicted labels may lack sufficient accuracy, especially for in-the-wild images," yet rely on them for 200,000 training samples.
- Why unresolved: Joint modeling typically assumes clean alignment between domains; training on noisy pseudo-labels may force the model to learn the error distributions of the specialist labelers (e.g., Depth Anything V2) rather than true visual geometry.
- What evidence would resolve it: An ablation study comparing the convergence rate and final perception accuracy of Jodi when trained strictly on ground-truth datasets (e.g., Hypersim) versus the mixed pseudo-labeled dataset.

## Limitations
- Structural distortions in generated human figures due to limited training data and potential architectural constraints in modeling complex anatomical structures
- Current representation of segmentation and pose domains in RGB space limits accuracy and class resolution compared to specialized approaches
- Dependence on pseudo-labels for large-scale dataset construction may introduce noise that constrains the upper bound of perception performance
- Uncertainty about scalability to highly diverse visual domains requiring fundamentally different architectural adaptations

## Confidence
**High confidence**: Core unification architecture feasibility, basic role switch mechanism functionality, and primary experimental results on Joint-1.6M dataset

**Medium confidence**: Scalability claims to arbitrary visual domains, performance advantages relative to specialist models, and generalization to unseen domain combinations

**Low confidence**: Long-term robustness across diverse real-world applications and effectiveness with domains having vastly different spatial properties

## Next Checks
1. Test scalability by extending the framework to at least 10-15 diverse visual domains and measuring performance degradation
2. Compare against other unified approaches trained on comparable dataset sizes to isolate the contribution of the unification architecture versus dataset quality
3. Evaluate robustness by introducing domain-specific noise patterns and measuring cross-domain generation and understanding performance under degraded conditions