---
ver: rpa2
title: 'GazeLLM: Multimodal LLMs incorporating Human Visual Attention'
arxiv_id: '2504.00221'
source_url: https://arxiv.org/abs/2504.00221
tags:
- video
- gaze
- human
- full
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes GazeLLM, a method that integrates human visual\
  \ attention with multimodal large language models (MLLMs) to efficiently process\
  \ first-person video. The approach leverages eye-tracking data to crop high-resolution\
  \ video around the user\u2019s gaze point, reducing pixel input to one-tenth while\
  \ maintaining task comprehension."
---

# GazeLLM: Multimodal LLMs incorporating Human Visual Attention

## Quick Facts
- arXiv ID: 2504.00221
- Source URL: https://arxiv.org/abs/2504.00221
- Reference count: 32
- Primary result: Gaze-based video cropping reduces pixel input to 10.3% while maintaining or improving task description quality in MLLMs

## Executive Summary
This paper proposes GazeLLM, a method that integrates human visual attention with multimodal large language models (MLLMs) to efficiently process first-person video. The approach leverages eye-tracking data to crop high-resolution video around the user's gaze point, reducing pixel input to one-tenth while maintaining task comprehension. Evaluations using BLEU, ROUGE, SBERT, and LLM-based metrics show that gaze-cropped videos outperform center-cropped videos in generating accurate task descriptions. Human evaluations also confirm that descriptions from gaze-focused videos are rated higher than those from full or center-cropped videos. The results demonstrate that GazeLLM effectively reduces computational load while preserving or improving task understanding, offering a promising framework for real-world assistance systems.

## Method Summary
GazeLLM processes first-person video by synchronizing eye-tracking data with video frames and cropping the video to a 448×448 pixel region centered on the gaze point. The system uses a 1 FPS sampling rate and compares three conditions: full-resolution video (1440×1440), center-cropped video (448×448), and gaze-cropped video (448×1440 centered on gaze coordinates). The cropped video is then processed by a multimodal LLM (Gemini 1.5 Pro) to generate task descriptions. The method assumes that human gaze reliably indicates task-relevant information and that reducing pixel count through targeted cropping maintains comprehension while significantly reducing computational load.

## Key Results
- Gaze-cropped videos achieve higher BLEU, ROUGE, SBERT, and LLM-based scores compared to center-cropped videos
- Human evaluators rate task descriptions from gaze-cropped videos higher than those from full or center-cropped videos
- The gaze-cropping method reduces pixel count to 10.3% of the original while maintaining task comprehension
- Gaze-cropped descriptions are rated highest across five out of six task categories in human evaluation

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Signal Filtering
The method relies on the correlation between human foveal vision and task-relevant visual information. By cropping the video to a 448×448 pixel region centered on the gaze point (10.3% of original pixels), the system filters out peripheral "noise" that is likely irrelevant to the user's immediate goal. This effectively aligns the model's visual context window with the human's cognitive focus. The core assumption is that the user's gaze is a reliable proxy for task-relevant information, and peripheral vision contains mostly distractors for procedure description. This mechanism fails if the task requires constant peripheral monitoring or if the user is "looking but not seeing."

### Mechanism 2: ViT Token Efficiency
MLLMs using Vision Transformers (ViT) process images as grid-like patches (tokens), with token count scaling quadratically with resolution. By inputting a high-resolution crop rather than a low-resolution full image, the system minimizes token count (processing 10.3% of pixels) while retaining fine-grained detail needed for understanding, which is lost in uniform downsampling. The core assumption is that the MLLM has sufficient general knowledge to interpret the cropped object without the surrounding visual context. This mechanism breaks if required visual context is wider than the crop but too detailed for global downsampling.

### Mechanism 3: Intent Alignment via Biological Signal
A "Center" crop assumes the subject is always centered, which is often false in first-person view. Gaze tracking provides a dynamic region of interest that follows the user's shifting focus, resulting in higher BLEU/ROUGE scores and human preference. The core assumption is that temporal stability of the gaze signal allows for coherent video stitching. This mechanism fails if the user is interacting with large objects that require frequent scanning, causing the crop to fragment the visual narrative.

## Foundational Learning

- **Vision Transformer (ViT) Tokenization**: Understanding that ViT breaks images into patches (tokens) is critical to grasp why 10x fewer pixels significantly impacts memory and processing speed. Quick check: If you double the resolution of an input image, does the token count double or increase more drastically in a standard ViT architecture?

- **Egocentric (First-Person) Vision Characteristics**: Understanding the instability and wide FOV of head-mounted cameras is necessary to see why center-cropping fails and gaze-cropping is necessary. Quick check: Why is a "center crop" likely to fail in a first-person cooking video compared to a third-person static camera video?

- **Eye-Tracking Signal Processing (Gaze Mapping)**: Understanding the need for synchronization between video stream and gaze data stream is critical for the architecture. Quick check: If the eye tracker runs at 200Hz and the video is sampled at 1fps, how do you determine the correct gaze coordinate for the video frame?

## Architecture Onboarding

- **Component map**: Input (video + gaze data) -> Pre-processor (synchronization + crop to 448×448) -> Core Model (MLLM with ViT encoder + LLM backbone) -> Output (structured text with timestamps)

- **Critical path**: The exact alignment of the gaze crop window to the video frame. If misaligned due to calibration drift or latency, the MLLM receives irrelevant data, negating accuracy benefits.

- **Design tradeoffs**: Context vs. Efficiency (448×448 maximizes efficiency but discards peripheral context permanently); Frame Rate vs. Latency (1fps sacrifices motion fluidity despite gaze optimization).

- **Failure signatures**: "Context Loss" (model fails to describe interactions between two objects if user looks at only one); "Gaze Jitter" (rapid crop window fluctuation causing disjointed video to MLLM).

- **First 3 experiments**: 1) Baseline Validation: Reproduce Full vs. Gaze vs. Center comparison on Ego-Exo4D subset using 448×448 crop and 1fps settings. 2) Crop Size Sensitivity: Test 448×448 against larger (600×600) and smaller (256×256) crops. 3) Noise Test: Replace actual gaze coordinates with random coordinates to confirm performance gain comes from gaze specifically.

## Open Questions the Paper Calls Out

- Would integrating a dual-stream input of gaze-cropped video and downsampled peripheral video improve comprehension over the single-stream gaze-crop method? The paper proposes providing "both gaze-centered video and a downsampled full view" as an alternative encoding scheme.

- Can combining hand position data with gaze tracking enhance the accuracy of task understanding compared to gaze data alone? The paper suggests "hand position information could be prioritized... since gaze tends to anticipate the next point of action."

- Does applying a non-uniform, deformable grid in the Vision Transformer based on gaze density outperform the current rectangular cropping method? The paper proposes a "deformed grid for the 2D patches... where patches are denser near the gaze point and sparser toward the periphery."

## Limitations

- Dataset Generalization: Evaluation uses Ego-Exo4D with 137 videos across six specific task categories, which may not capture full range of real-world first-person scenarios, particularly those involving rapid gaze shifts or tasks requiring constant peripheral awareness.

- Temporal Resolution Trade-off: Downsampling video to 1 FPS may eliminate critical temporal information for tasks requiring fine-grained motion analysis, making efficiency gains partially contingent on this aggressive temporal reduction.

- Gaze Signal Reliability: Method assumes gaze data is reliable proxy for task-relevant information, but doesn't address scenarios where gaze might be misleading (looking at tool while thinking about next step, or gaze drifting during cognitive pauses).

## Confidence

- **High Confidence**: The core mechanism of reducing pixel count through gaze-based cropping to lower computational load is well-supported by quantitative metrics (BLEU, ROUGE, SBERT, LLM-based scores) and explicit comparison of token counts.

- **Medium Confidence**: The claim that gaze-cropped videos generate more human-favorable descriptions than center-cropped videos is supported by human evaluation scores but relies on subjective preference, with less definitive establishment in the corpus.

- **Low Confidence**: The assertion that the method maintains or improves "task comprehension" compared to full-frame processing is based on proxy metrics measuring textual similarity, not true comprehension. The paper lacks direct tests of the model's ability to answer questions about video content or perform downstream reasoning tasks.

## Next Checks

1. **Edge Case Robustness Test**: Design experiment using videos where user's gaze is intentionally misaligned with task-relevant object (looking at timer while chopping, or gaze wandering during pause). Compare Gaze-cropped description quality against Full and Center conditions to quantify performance degradation when gaze is not reliable signal.

2. **Temporal Resolution Sensitivity Analysis**: Repeat main experiment with video frame rate increased from 1fps to 5fps and 10fps. Measure impact on description quality (BLEU, ROUGE, human evaluation) and computational load (token count, processing time) to determine true cost of temporal information lost at 1fps.

3. **Downstream Task Performance Evaluation**: Design test where MLLM must answer specific factual questions about video content (e.g., "What color was the pan?", "How many eggs were used?", "What was the final step?"). Compare accuracy of these answers across Gaze, Center, and Full conditions to directly measure task comprehension rather than textual similarity.