---
ver: rpa2
title: Benchmarking noisy label detection methods
arxiv_id: '2510.16211'
source_url: https://arxiv.org/abs/2510.16211
tags:
- noise
- detection
- methods
- mean
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive benchmark for noisy label
  detection methods. The authors decompose detection approaches into three components:
  label agreement function, aggregation method, and information gathering approach.'
---

# Benchmarking noisy label detection methods

## Quick Facts
- **arXiv ID**: 2510.16211
- **Source URL**: https://arxiv.org/abs/2510.16211
- **Reference count**: 22
- **Primary result**: Mean Probabilities - Logit Margin achieves best FNR performance across most vision and tabular datasets with synthetic and real-world noise

## Executive Summary
This paper presents a comprehensive benchmark for noisy label detection methods, introducing a novel evaluation metric (FNR at dataset's noise rate) and systematically evaluating 15 methods across vision and tabular datasets. The authors decompose detection approaches into three components: label agreement function, aggregation method, and information gathering approach. They demonstrate that simple in-sample methods using average probability aggregation combined with logit margin as the label agreement function consistently outperform more complex alternatives across most scenarios.

## Method Summary
The authors evaluate noisy label detection by decomposing methods into three components: label agreement functions (cross-entropy loss, Jensen-Shannon divergence, logit margin), aggregation methods (last epoch, mean, mean probabilities, SWA, CTRL), and information gathering approaches (in-sample vs out-of-sample). The best-performing method, Mean Probabilities - Logit Margin, averages softmax probabilities across training epochs then computes logit margin scores to identify likely mislabeled samples. The benchmark uses FNR at the noise rate operating point as the evaluation metric, requiring methods to clean exactly the fraction of samples equal to the dataset's noise rate.

## Key Results
- Mean Probabilities - Logit Margin consistently achieves the lowest FNR across most datasets and noise types
- Hyperparameter optimization provides minimal (<2%) improvement in detection performance
- In-sample detection methods significantly outperform out-of-sample approaches like Confident Learning
- SWA and CTRL methods fail to justify their complexity compared to simpler ensemble-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Temporal probability aggregation
Temporal probability aggregation across training epochs improves detection by exploiting consistent prediction patterns for clean samples versus inconsistent patterns for mislabeled samples. Networks exhibit early learning where simple patterns are learned before memorizing noisy labels. Mean Probabilities aggregation averages softmax predictions across epochs, creating a temporal ensemble that amplifies consistent signals while dampening noise-induced fluctuations.

### Mechanism 2: Logit Margin scoring
Logit Margin provides more robust detection signals than cross-entropy loss by measuring decision boundary proximity rather than absolute probability values. It computes the difference between the assigned class logit and the highest competing class logit, creating large positive margins for clean samples and smaller margins for mislabeled samples. This relative measure focuses on competition between classes rather than absolute confidence.

### Mechanism 3: In-sample detection advantage
In-sample detection outperforms out-of-sample cross-validation approaches because training dynamics and memorization patterns contain valuable diagnostic signal that is discarded when excluding samples during prediction. In-sample methods leverage memorization dynamics where mislabeled samples are memorized later in training, showing different trajectory patterns than clean samples.

## Foundational Learning

- **Early Learning Phenomenon**: Networks learn simple generalizable patterns before memorizing noise. Why needed: Explains why temporal aggregation detects noisy labels. Quick check: Can you explain why clean samples show different training dynamics than mislabeled samples?

- **Noise Transition Structures**: Symmetric vs. pairflip noise types have different structural properties. Why needed: Paper shows Logit Margin excels with sparse noise (pairflip) while CE is competitive with dense noise (symmetric). Quick check: How would 20% symmetric noise differ from 20% pairflip noise in terms of effective detection signals?

- **False Negative Rate at Fixed Budget**: Evaluating FNR at budget equal to noise rate provides fair comparison across methods. Why needed: Enables fair comparison of methods with different score distributions. Quick check: Why is FNR at noise rate a fairer metric than ROC-AUC for comparing detection methods?

## Architecture Onboarding

- **Component map**: Training Phase → Scoring Phase → Detection Phase
- **Critical path**: Train model for E epochs collecting logits, apply Mean Probabilities aggregation, compute Logit Margin scores, rank samples by score, select top η% as noisy
- **Design tradeoffs**: Storage vs. flexibility (full logit history vs. last-epoch methods), tuning set requirement (<2% improvement justifies annotation cost), complexity vs. performance (simpler methods win), noise structure sensitivity
- **Failure signatures**: High FNR variance (underfitting), specific class issues (class-level noise rates), CTRL volatility (discretization effects), Mean Prob - LM underperformance on symmetric noise
- **First 3 experiments**:
  1. Baseline Replication: Implement Mean Prob - LM on CIFAR-10 with 20% symmetric noise (expected FNR: ~7.2%)
  2. Noise Structure Ablation: Compare Mean Prob - LM vs CE on CIFAR-10 with 20% pairflip noise (expected: ~2% FNR advantage for LM)
  3. Tuning Set Sensitivity: Test window optimization with 1%, 5%, 10% tuning sets on CIFAR-10N (expected: <2% improvement)

## Open Questions the Paper Calls Out

1. **Training optimization for detection**: Does training models with detection-specific optimization (e.g., cyclic learning rates) significantly improve noisy label detection performance compared to training optimized for classification accuracy?

2. **SWA window tuning**: Can tuning the aggregation window for Stochastic Weight Averaging (SWA) to exclude early training epochs allow it to match or exceed the performance of probability-based ensembles?

3. **Generalization to other domains**: Do the findings regarding the superiority of Mean Probabilities - LM generalize to non-vision domains (text, audio) and transformer-based architectures?

## Limitations
- Limited exploration of instance-dependent noise beyond basic pairflip and symmetric types
- Focus on image classification despite broader applications of noisy label detection
- Limited ablation studies on the specific components of Logit Margin that make it effective

## Confidence
- **Core findings about performance rankings**: High confidence
- **Generalizability beyond tested architectures**: Medium confidence
- **Minimal impact of hyperparameter optimization**: Medium confidence

## Next Checks
1. Test Mean Probabilities - Logit Margin on modern vision transformer architectures (e.g., ViT-Base) to assess architecture independence
2. Evaluate performance on instance-dependent noise patterns where transition probabilities vary by sample
3. Conduct ablation studies comparing Logit Margin to other decision boundary proximity measures