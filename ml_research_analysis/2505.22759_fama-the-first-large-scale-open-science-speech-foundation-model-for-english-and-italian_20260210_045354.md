---
ver: rpa2
title: 'FAMA: The First Large-Scale Open-Science Speech Foundation Model for English
  and Italian'
arxiv_id: '2505.22759'
source_url: https://arxiv.org/abs/2505.22759
tags:
- speech
- fama
- data
- training
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FAMA addresses the lack of open-science speech foundation models\
  \ by introducing the first family of large-scale, open-source models for English\
  \ and Italian. Trained on 150k+ hours of open-source speech data\u2014including\
  \ 16k+ hours of newly created pseudolabels\u2014FAMA leverages a two-stage training\
  \ approach: ASR pre-training followed by joint ASR+ST training."
---

# FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian

## Quick Facts
- arXiv ID: 2505.22759
- Source URL: https://arxiv.org/abs/2505.22759
- Reference count: 8
- Primary result: First open-science speech foundation model family for English and Italian, trained on 150k+ hours of open-source speech data.

## Executive Summary
FAMA addresses the lack of open-science speech foundation models by introducing the first family of large-scale, open-source models for English and Italian. Trained on 150k+ hours of open-source speech data—including 16k+ hours of newly created pseudolabels—FAMA leverages a two-stage training approach: ASR pre-training followed by joint ASR+ST training. The model architecture features Conformer encoders paired with Transformer decoders, optimized for speed and integration with large language models. Results show FAMA achieves competitive ASR performance with up to 4.2 WER improvement over prior open models, and up to 8× faster inference than Whisper, while maintaining strong ST results. All code, datasets, and models are released under open licenses to promote reproducibility and inclusivity in speech technology research.

## Method Summary
FAMA employs a two-stage training approach: first pre-training on ASR tasks, then fine-tuning jointly on ASR and speech translation. The architecture uses Conformer encoders with Transformer decoders, available in small (479M parameters) and medium (878M parameters) variants. Training uses AdamW optimizer with specific learning rate schedules, SpecAugment, and CTC loss for alignment. The model is trained on 150k+ hours of open-source speech data, including 16k+ hours of pseudolabels generated using Whisper. Inference uses beam search with joint CTC rescoring for improved accuracy.

## Key Results
- Achieves up to 4.2 absolute WER improvement over prior open models on ASR tasks
- Provides up to 8× faster inference compared to Whisper while maintaining competitive accuracy
- Demonstrates strong speech translation performance with joint ASR+ST training
- Establishes new benchmarks for open-science speech foundation models in English and Italian

## Why This Works (Mechanism)
The two-stage training approach allows the model to first learn robust speech representations through ASR pre-training, then refine these representations for both ASR and ST tasks. The Conformer architecture effectively captures both local and global speech patterns, while the joint training objective ensures the model can handle both transcription and translation tasks efficiently. The use of CTC loss provides alignment supervision, improving temporal modeling.

## Foundational Learning
- **Conformer Architecture**: Hybrid CNN-Transformer design that captures local and global speech patterns; needed for efficient speech processing, check by verifying local receptive fields
- **SpecAugment**: Data augmentation technique that improves robustness to noise and variability; needed for generalization, check by comparing with and without augmentation
- **CTC Loss**: Connectionist Temporal Classification for alignment-free training; needed for sequence-to-sequence mapping, check by examining alignment quality
- **Joint Training**: Simultaneous optimization of ASR and ST tasks; needed for efficient multi-task learning, check by monitoring task-specific metrics
- **Beam Search with CTC Rescoring**: Inference technique that combines probabilistic decoding with alignment information; needed for improved accuracy, check by comparing with and without rescoring

## Architecture Onboarding
- **Component Map**: Speech Input -> Conformer Encoder -> Transformer Decoder -> Output Tokens; Pseudolabels (Whisper) -> Training Loop -> Model Parameters
- **Critical Path**: Data preprocessing → Conformer encoding → Transformer decoding → CTC alignment → Joint loss computation
- **Design Tradeoffs**: Speed vs accuracy (8× faster than Whisper but with slight accuracy trade-offs), model size vs performance (small vs medium variants), open vs closed components (pseudolabels from Whisper)
- **Failure Signatures**: Catastrophic forgetting during stage 2, convergence instability for medium model, pseudolabel quality issues affecting downstream performance
- **First Experiments**: 1) Train small model with default parameters to verify baseline performance, 2) Test inference speed against Whisper, 3) Evaluate pseudolabel quality impact through ablation

## Open Questions the Paper Calls Out
- Can the FAMA training approach scale effectively to a truly multilingual model covering many languages while maintaining competitive performance and speed advantages?
- What are the primary factors causing the speech translation (ST) performance gap between FAMA and closed models like SeamlessM4T, and can this gap be closed solely through larger OS-compliant human-labeled ST datasets?
- Does the use of Whisper large-v3 for generating pseudolabels introduce systematic biases or errors that propagate into FAMA's final performance?

## Limitations
- The exact impact of newly created pseudolabels on overall performance remains uncertain without detailed ablation studies
- Claims about model behavior in low-resource scenarios are not thoroughly validated
- The evaluation focuses primarily on ASR and ST tasks, with limited exploration of other potential speech foundation model applications

## Confidence
- **High Confidence**: FAMA is the first open-science speech foundation model for English and Italian; two-stage training approach is technically sound; reported inference speed improvements are plausible
- **Medium Confidence**: WER improvements versus prior open models are reasonable but need independent verification; 8× faster inference depends on hardware configuration
- **Low Confidence**: Exact impact of pseudolabels is uncertain; low-resource performance claims not thoroughly validated

## Next Checks
1. **Independent Reproduction**: Train FAMA from scratch using only publicly available code and data to verify claimed performance metrics and confirm reproducibility
2. **Cross-Lingual Transfer Evaluation**: Test FAMA's performance on other languages present in training corpus to assess generalizability beyond English and Italian
3. **Ablation Study on Pseudolabels**: Conduct controlled experiments removing 16k+ hours of pseudolabels to quantify their exact contribution to final model performance