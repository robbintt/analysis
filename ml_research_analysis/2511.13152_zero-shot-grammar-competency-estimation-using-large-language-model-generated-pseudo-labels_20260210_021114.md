---
ver: rpa2
title: Zero-Shot Grammar Competency Estimation Using Large Language Model Generated
  Pseudo Labels
arxiv_id: '2511.13152'
source_url: https://arxiv.org/abs/2511.13152
tags:
- grammar
- scoring
- training
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of grammar competency estimation
  in spoken language, where spontaneous, unstructured, and disfluent speech makes
  manual annotation impractical. To overcome this, the authors propose a zero-shot
  framework that uses large language models (LLMs) to generate pseudo-labels from
  unlabeled data, guided by rubric-based prompts.
---

# Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels

## Quick Facts
- arXiv ID: 2511.13152
- Source URL: https://arxiv.org/abs/2511.13152
- Reference count: 24
- Primary result: QWK=0.776 on WGAD dataset using GPT-4 pseudo-labels

## Executive Summary
This paper addresses the challenge of grammar competency estimation in spoken language, where spontaneous, unstructured, and disfluent speech makes manual annotation impractical. To overcome this, the authors propose a zero-shot framework that uses large language models (LLMs) to generate pseudo-labels from unlabeled data, guided by rubric-based prompts. These pseudo-labels are then used to train a transformer-based model with a novel adaptive sample-weighting method to handle label noise. The approach works for both written and spoken responses, offering scalability without expert-annotated data.

## Method Summary
The proposed zero-shot framework generates grammar scores for unlabeled text data using rubric-guided LLM prompts (GPT-4), creating pseudo-labels that transform an unsupervised problem into a supervised one. A transformer encoder (BERT, ELECTRA, or RoBERTa) is trained with a projection head to predict these pseudo-labels using MSE loss. To handle potential noise in LLM-generated labels, an adaptive sample-weighting mechanism dynamically filters training samples each epoch, retaining only the top α=0.3 lowest-loss samples as the "clean" set. This framework is validated on two in-house datasets (SGAD for spoken, WGAD for written responses) with strong performance metrics including QWK up to 0.776 and low RMSE values.

## Key Results
- QWK score of 0.776 on WGAD dataset using GPT-4 pseudo-labels
- Strong correlation metrics: PLCC=0.883, SRCC=0.865 on WGAD
- RMSE values of 0.486 (SGAD) and 0.471 (WGAD) demonstrating prediction accuracy
- Performance consistently outperforms strong baselines across both spoken and written domains

## Why This Works (Mechanism)

### Mechanism 1: Rubric-Guided Synthetic Supervision
Large Language Models (LLMs) may function as competent annotators for grammar scoring when constrained by expert-designed rubrics, bridging the gap between unlabeled data and supervised training requirements. The system prompts an LLM (e.g., GPT-4) with a specific grammar competency rubric. The LLM processes unlabeled spoken or written text and outputs a score (1-5). This score acts as a "pseudo-label," transforming an unsupervised problem into a supervised one without human annotation costs. The core assumption is that the LLM's internal representation of "grammar" aligns sufficiently with the external expert rubric.

### Mechanism 2: Noise Filtering via Small-Loss Selection
A dynamic sample-weighting strategy mitigates the "memorization" of incorrect pseudo-labels by prioritizing samples the model currently predicts with high confidence (low loss). At the end of each training epoch, the framework calculates the loss for every sample using the current model parameters. It sorts these losses and retains only the top α fraction (e.g., 30%) of samples with the lowest losses for the next epoch. This assumes that samples with high loss are likely mislabeled (noisy) and should be down-weighted.

### Mechanism 3: Teacher-Student Quality Ceiling
The performance of the final student model is causally bounded by the accuracy and consistency of the teacher LLM used for pseudo-labeling. The student model learns to mimic the teacher's distribution. Experimental results suggest that higher-capacity teachers (GPT-4) produce pseudo-labels that result in higher student accuracy compared to lower-capacity teachers (Mistral, Gemini). The core assumption is that the student model architecture (Transformer) has sufficient capacity to absorb the assessment logic provided by the teacher labels.

## Foundational Learning

### Learning with Noisy Labels (Small-Loss Trick)
- Why needed: The core innovation relies on the idea that deep neural networks tend to learn simple/clean patterns before fitting noise.
- Quick check: How does the value of α change the balance between discarding noise and discarding valid "hard" examples?

### Prompt Engineering for Regression
- Why needed: The system relies on the LLM outputting a continuous or ordinal score (1-5) based on a text prompt, rather than just generating text.
- Quick check: How does the phrasing of the rubric in the prompt affect the LLM's ability to differentiate between a score of 3 and a score of 4?

### ASR Error Propagation
- Why needed: For spoken data (SGAD), the grammar model sees ASR transcripts, not audio. ASR errors often look like grammar errors.
- Quick check: Does the rubric-based prompt explicitly instruct the LLM to be robust to potential transcription errors?

## Architecture Onboarding

### Component map:
Unlabeled Text -> LLM + Rubric Prompt -> Pseudo-Label -> Transformer Encoder + Projection Head -> Adaptive Weighting Trainer -> Grammar Score

### Critical path:
The design of the Prompt (P) is the highest leverage point. If the prompt does not align with the target metric, the noise-robust trainer cannot recover the signal.

### Design tradeoffs:
- **Alpha (α) Selection**: Lower α (e.g., 0.1) is safer for very noisy labels but may lead to under-fitting; higher α (e.g., 0.5) risks overfitting to noise.
- **Model Choice**: The paper shows ELECTRA outperforms BERT on Spoken data but BERT wins on Written (WGAD). One must validate backbones per modality.

### Failure signatures:
- **Mode Collapse**: Student model predicts the mean score for all inputs. (Check if α is too low or learning rate too high).
- **Rubric Drift**: Student scores high for long, incoherent sentences. (Check if LLM pseudo-labels were biased by text length).

### First 3 experiments:
1. **Verify Pseudo-Label Correlation**: Before training, run the chosen LLM on a small held-out set with human labels to measure the "Teacher Ceiling" (e.g., QWK/RMSE of GPT-4 vs Human).
2. **Baseline Ablation**: Train the student model on the pseudo-labels *without* the adaptive weighting (standard MSE loss) to quantify the specific gain from the noise-handling mechanism.
3. **Alpha Sensitivity Sweep**: Run a grid search on α (0.1 to 0.9) on a validation split to find the optimal noise-filtering threshold, as the paper suggests α=0.3 is empirically derived but may vary by dataset size.

## Open Questions the Paper Calls Out

### Open Question 1
How effectively does the proposed zero-shot framework generalize to multilingual grammar competency estimation? The conclusion explicitly states that future work will "enhance noise robustness and extend to multilingual datasets." The current study exclusively validates the framework on English datasets (SGAD and WGAD), leaving performance on morphologically rich or low-resource languages unverified.

### Open Question 2
To what degree does the specific design of the rubric-based prompt influence the accuracy and stability of the generated pseudo-labels? The limitations section notes the approach "relies on the quality and alignment of the grammar competency rubric-based prompts, which may also vary across different use cases." The paper utilizes a single prompt strategy for pseudo-label generation without investigating how sensitive the LLM is to variations in rubric phrasing or granularity.

### Open Question 3
Can the adaptive sample-weighting mechanism effectively distinguish between random noise and systematic bias inherent in LLM pseudo-labels? The limitations section mentions that "errors may not be captured well by LLMs," implying potential systematic blind spots in the pseudo-labels. The sample-weighting method prioritizes "low-loss" samples, but if the LLM consistently mislabels specific error types with high confidence, the framework may reinforce this systematic bias rather than filtering it.

## Limitations
- Exact rubric-based prompt text used for GPT-4 pseudo-labeling is not provided—critical for label quality.
- Datasets (SGAD, WGAD) are proprietary/in-house; no public access for validation.
- Key training hyperparameters (learning rate, batch size, epochs, optimizer settings) are omitted, introducing ambiguity in replicating performance.

## Confidence
- **High** confidence in the general framework design (rubric-guided pseudo-labeling + noise-aware training) and its core intuition.
- **Medium** confidence in the specific numerical results due to lack of full experimental details and proprietary data.
- **Low** confidence in generalisability to other domains/languages without access to the exact prompt and hyperparameters.

## Next Checks
1. **Prompt sensitivity test:** Run the pseudo-labeling pipeline with slight variations in rubric phrasing to quantify stability of GPT-4 outputs.
2. **Teacher-student ceiling validation:** On a small annotated subset, compare pseudo-labels vs human labels to measure the upper bound of student model performance.
3. **Cross-dataset replication:** Apply the full pipeline (with open datasets) to verify that adaptive sample-weighting consistently outperforms uniform weighting across different domains.