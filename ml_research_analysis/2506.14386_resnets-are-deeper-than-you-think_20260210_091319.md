---
ver: rpa2
title: ResNets Are Deeper Than You Think
arxiv_id: '2506.14386'
source_url: https://arxiv.org/abs/2506.14386
tags:
- networks
- network
- should
- residual
- feedforward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fundamental reasons behind the success
  of residual connections in neural networks. While residual networks (ResNets) are
  known to train more stably and achieve higher accuracy than feedforward networks,
  the authors propose that this is not just due to better trainability, but because
  residual connections alter the function space the network inhabits.
---

# ResNets Are Deeper Than You Think
## Quick Facts
- arXiv ID: 2506.14386
- Source URL: https://arxiv.org/abs/2506.14386
- Reference count: 40
- Primary result: Residual connections improve performance not just through trainability, but by altering the function space and enabling variable-depth architectures

## Executive Summary
This paper challenges the conventional understanding of why residual connections improve neural network performance. While residual networks (ResNets) are known to train more stably and achieve higher accuracy than feedforward networks, the authors argue this is not solely due to better trainability. Instead, residual connections fundamentally alter the function space the network operates in, creating an inductive bias that better matches natural data structure.

The key insight is that residual connections enable variable-depth architectures, allowing different input samples to traverse networks of different effective depths. Through post-training experiments comparing channel-wise and layer-wise linearization of pre-trained networks, the authors demonstrate that variable-depth architectures consistently outperform fixed-depth ones, even when controlling for average path length.

## Method Summary
The authors conduct a post-training experiment using a pre-trained feedforward network as a baseline. They gradually reduce nonlinear units using PReLU activations and sparsity regularization, creating partially linearized networks. Two linearization approaches are compared: channel-wise linearization (which can create variable-depth networks where different samples traverse different path lengths) and layer-wise linearization (which results in fixed-depth networks). By comparing the performance of these two approaches, the authors isolate the effect of variable depth from other factors like trainability.

## Key Results
- Residual connections provide a genuine inductive bias beyond just improved trainability
- Variable-depth architectures consistently outperform fixed-depth ones, even when controlling for average path length
- The performance gap between residual and feedforward networks cannot be fully closed through improved initialization or training techniques alone

## Why This Works (Mechanism)
Residual connections work by creating networks where different input samples can traverse paths of different effective depths. This variable-depth capability means the network can adaptively apply more computation to complex inputs while using simpler paths for easier samples. This matches how natural data often has hierarchical structure - simple patterns can be recognized quickly while complex patterns require deeper processing. The mechanism shows that residual connections aren't just making training easier, but are fundamentally changing what functions the network can represent.

## Foundational Learning
- **Function space theory**: Understanding how different network architectures constrain or expand the space of functions they can represent. Needed to grasp why residual connections create a different function space. Quick check: Can you explain how residual connections change the hypothesis space compared to feedforward networks?
- **Path length analysis**: The concept that different samples may traverse networks of different effective depths. Needed to understand variable-depth architectures. Quick check: Can you calculate the range of possible path lengths in a residual network versus a feedforward network?
- **Inductive bias**: The implicit preferences that different architectures have for certain types of solutions. Needed to understand why variable depth is beneficial. Quick check: Can you identify the inductive bias created by allowing variable path lengths?

## Architecture Onboarding
- **Component map**: Input -> Linear layers with PReLU activations -> Residual connections (skip connections) -> Output. The key difference from feedforward is the residual connection that bypasses nonlinear transformations.
- **Critical path**: The path that includes both the residual branch (identity mapping) and the nonlinear transformation branch. This dual-path structure is what enables variable depth.
- **Design tradeoffs**: Variable depth provides flexibility but adds complexity; fixed depth is simpler but less adaptive. The paper shows the tradeoff strongly favors variable depth for natural data.
- **Failure signatures**: If residual connections are removed or linearized in a layer-wise manner, the network loses its variable-depth capability and performance degrades significantly, even with the same number of parameters.
- **First experiments**: 1) Compare classification accuracy of variable-depth vs fixed-depth networks on CIFAR-10/100, 2) Measure the distribution of effective path lengths across different input samples, 3) Analyze which layers contribute most to the variable-depth behavior by examining which nonlinearities are preserved.

## Open Questions the Paper Calls Out
- How does the variable-depth property of residual networks affect their robustness to adversarial attacks?
- Can we design new architectures that explicitly optimize for variable depth rather than discovering it as a side effect of residual connections?
- How does the variable-depth behavior change across different network depths and architectures?

## Limitations
- The experiments are conducted post-training, so the findings don't directly address how to train these variable-depth networks from scratch
- The study focuses on image classification tasks; results may not generalize to other domains like language or reinforcement learning
- The analysis is limited to networks using PReLU activations for linearization; other activation functions may behave differently

## Confidence
- The experimental methodology is sound and controls for key variables: High
- The theoretical interpretation of results is well-supported but could benefit from additional theoretical analysis: Medium
- The generalizability of findings to other network architectures and tasks: Medium

## Next Checks
- Replicate the variable vs fixed depth comparison on a different architecture (e.g., Vision Transformers) to test generalizability
- Implement a training procedure that explicitly encourages variable-depth behavior and compare to standard residual training
- Analyze the relationship between input complexity and effective path length to verify the adaptive computation hypothesis