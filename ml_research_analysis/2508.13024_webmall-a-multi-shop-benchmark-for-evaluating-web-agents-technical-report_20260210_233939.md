---
ver: rpa2
title: WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents [Technical Report]
arxiv_id: '2508.13024'
source_url: https://arxiv.org/abs/2508.13024
tags:
- agents
- product
- tasks
- task
- webmall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WebMall introduces the first offline multi-shop benchmark for evaluating
  web agents on comparison shopping tasks. It features four simulated online shops
  with heterogeneous product data extracted from the Common Crawl, covering PC components,
  peripherals, and other electronics.
---

# WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents [Technical Report]

## Quick Facts
- **arXiv ID:** 2508.13024
- **Source URL:** https://arxiv.org/abs/2508.13024
- **Reference count:** 32
- **Primary result:** First offline multi-shop benchmark for evaluating web agents on comparison shopping tasks with 91 tasks across 4 simulated e-shops

## Executive Summary
WebMall introduces a novel benchmark for evaluating web agents on multi-shop comparison shopping tasks. The benchmark features four simulated online shops populated with 4,421 product offers extracted from Common Crawl, covering PC components, peripherals, and other electronics. Through evaluation of eight agent variants using different observation spaces and memory configurations, the study reveals that current agents struggle with cross-shop navigation and product comparison, achieving below 55% task completion rates for cheapest and vague product searches.

## Method Summary
The benchmark consists of four WooCommerce-based online shops populated with product data extracted from Common Crawl (October 2024) using schema.org annotations. Each shop contains 1,100-1,300 offers across 11 categories. The evaluation uses 91 tasks with natural language instructions requiring multi-shop navigation, search, comparison, and checkout. Eight agent variants were tested using BrowserGym/AgentLab with combinations of accessibility tree observations, screenshots, persistent short-term memory, and two different LLMs (GPT-4.1 and Claude Sonnet 4). Performance was measured using task completion rate, precision/recall/F1 scores, and efficiency metrics including steps, tokens, runtime, and API cost.

## Key Results
- Task completion rates below 55% for cheapest and vague product searches across all agent variants
- Structural information (accessibility tree) is critical for performance; vision-only agents achieve 0% completion on transactional tasks
- Token usage and runtime are high, especially for Claude Sonnet 4 agents (up to 390K tokens and $1.42 per task)
- Memory benefits are task-dependent: improves GPT-4.1 on specific searches but degrades Claude performance on vague product searches

## Why This Works (Mechanism)
WebMall works by creating a controlled environment where agents must perform realistic multi-shop comparison shopping tasks. The benchmark's design forces agents to navigate across heterogeneous shop structures, search for products, compare attributes, and complete transactions. By using real product data from Common Crawl with schema.org annotations, the benchmark ensures tasks reflect authentic shopping scenarios while maintaining reproducibility through simulated shops.

## Foundational Learning
- **Multi-shop navigation:** Agents must traverse different shop structures and maintain context across sessions; needed because real shopping requires comparing products across vendors; quick check: verify agent visits all four shops per task
- **Product comparison logic:** Agents must extract and compare product attributes like price, specifications, and features; needed because shopping tasks require identifying optimal products; quick check: validate cross-shop attribute extraction accuracy
- **Transactional workflows:** Agents must complete add-to-cart and checkout processes; needed because end-to-end shopping includes purchase completion; quick check: verify successful checkout flow execution
- **Cross-modal observation fusion:** Agents combine accessibility tree and visual information; needed because different modalities provide complementary information; quick check: compare performance with single vs. dual modalities
- **Memory management:** Agents maintain short-term memory across steps; needed because shopping tasks require remembering previously viewed products; quick check: test memory retrieval accuracy across navigation steps
- **Natural language instruction interpretation:** Agents parse shopping instructions with varying specificity; needed because real users provide diverse query formulations; quick check: validate instruction understanding across task types

## Architecture Onboarding
- **Component map:** BrowserGym/AgentLab -> Agent (AX-Tree + Vision + Memory + LLM) -> Simulated Shops -> Task Executor
- **Critical path:** Instruction parsing → Shop navigation → Product search → Attribute extraction → Comparison → Submission
- **Design tradeoffs:** Accessibility tree vs. vision (precision vs. flexibility), memory on vs. off (context retention vs. simplicity), different LLMs (cost vs. capability)
- **Failure signatures:** Vision-only agents fail on Action & Transaction tasks (0% CR), stopping after first valid product causes low recall, attribute misinterpretation leads to false positives
- **First experiments:** 1) Run AX-Tree + Memory agent on a single task to verify basic functionality, 2) Compare AX-Tree only vs. Vision only performance on search tasks, 3) Test memory impact by running identical tasks with memory on/off

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How can agent architectures be redesigned to reduce token consumption from 120K-390K tokens per task while maintaining task completion rates?
- **Basis in paper:** Conclusion states: "These results highlight the need...for more efficient agent architectures that reduce token consumption without compromising performance."
- **Why unresolved:** Current agents require extensive multi-step navigation across four shops, with Claude Sonnet 4 agents costing up to $1.42 per task.
- **What evidence would resolve it:** New agent designs achieving comparable F1 scores with ≥30% fewer tokens on WebMall tasks.

### Open Question 2
- **Question:** Why does persistent short-term memory improve GPT-4.1 performance on search tasks while degrading Claude Sonnet 4 performance on vague product searches?
- **Basis in paper:** Table 3 shows AX-Tree+Memory improves GPT-4.1's F1 on Specific Product Search from 60% to 77%, but Claude's F1 on Vague Product Search drops from 70% to 62% with memory.
- **Why unresolved:** The authors observe task-dependent memory benefits but do not explain the model-specific asymmetry.
- **What evidence would resolve it:** Ablation studies isolating memory retrieval patterns and attention mechanisms across both models on identical task trajectories.

### Open Question 3
- **Question:** What architectural components are required for vision-only agents to reliably complete transactional tasks where they currently achieve 0% success?
- **Basis in paper:** Table 4 shows Claude Vision agents score 0% completion on Action & Transaction and End-to-End tasks; the paper states "visual information alone is insufficient to achieve strong performance."
- **Why unresolved:** Vision-only agents fail to reliably locate search boxes, buttons, and form fields, exhausting their 50-step limit.
- **What evidence would resolve it:** Vision-only agents achieving ≥40% completion on Checkout tasks with novel grounding or spatial reasoning modules.

### Open Question 4
- **Question:** Can cross-shop comparison capabilities generalize beyond electronics to domains with less structured product attributes?
- **Basis in paper:** The benchmark is limited to PC components, peripherals, and electronics with schema.org annotations for GTIN/MPN identifiers; real-world comparison shopping spans clothing, groceries, and services with fewer standardized attributes.
- **Why unresolved:** Product clustering relies on globally unique identifiers that may not exist in other domains.
- **What evidence would resolve it:** Agents trained on WebMall achieving comparable completion rates on a new benchmark covering categories without standardized identifiers.

## Limitations
- Evaluation methodology lacks precise implementation details for AgentLab configuration and LLM prompts
- Benchmark focuses on four similar WooCommerce sites, limiting generalizability to diverse web architectures
- Performance metrics use exact match criteria that may be overly strict for complex shopping tasks
- Evaluation does not account for temporal dynamics like price changes or stock availability

## Confidence
- **High Confidence:** Benchmark construction methodology and finding that multi-shop comparison tasks are challenging for current agents
- **Medium Confidence:** Comparative analysis between observation spaces and memory usage, though exact implementation details are missing
- **Medium Confidence:** Efficiency metrics (token usage, runtime) that may vary with hardware and API configurations

## Next Checks
1. Reproduce task completion rates by running at least one agent variant (e.g., AX-Tree + Memory with GPT-4.1) across a subset of tasks to verify the reported sub-55% performance
2. Conduct ablation studies by systematically disabling individual components (AX-Tree, Vision, Memory) to validate their individual contributions to performance
3. Test agent performance on a small sample of real e-commerce websites outside the benchmark to assess generalizability beyond the controlled WooCommerce environment