---
ver: rpa2
title: Training Deep Morphological Neural Networks as Universal Approximators
arxiv_id: '2505.09710'
source_url: https://arxiv.org/abs/2505.09710
tags:
- networks
- morphological
- have
- network
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates deep morphological neural networks (DMNNs),
  which are based on mathematical morphology operations like dilation and erosion.
  The authors show that despite their inherent non-linearity, "linear" activations
  are essential for DMNNs to be universal approximators.
---

# Training Deep Morphological Neural Networks as Universal Approximators

## Quick Facts
- **arXiv ID:** 2505.09710
- **Source URL:** https://arxiv.org/abs/2505.09710
- **Reference count:** 40
- **Primary result:** DMNNs with linear activations are universal approximators and achieve competitive accuracy on MNIST/Fashion-MNIST/CIFAR-10 with superior prunability

## Executive Summary
This paper investigates Deep Morphological Neural Networks (DMNNs) based on mathematical morphology operations like dilation and erosion. The authors demonstrate that despite their inherent non-linearity, linear activations are essential for DMNNs to be universal approximators. They propose three architectures with different parameter constraints to preserve sparsity while enabling effective training. Experiments show their networks can be successfully trained, achieve competitive accuracy on standard image datasets, and are significantly more prunable than linear networks. The MPM network with weight dropout achieved 97.49% test accuracy on MNIST, only 0.52% lower than a standard MLP.

## Method Summary
The method involves training Deep Morphological Neural Networks that combine max-plus and min-plus operations with linear activations. The core MPM layer computes dilation plus erosion (sharing weights but keeping biases separate), followed by a learnable linear transformation. Three architectures are proposed: MPM (diagonal scaling), MPM-SVD (SVD-based linear transform), and Hybrid-MLP (full linear layer). The networks are trained using Adam optimizer with cross-entropy loss, with specific initialization strategies that address the challenges of morphological operations. Weight dropout and residual connections are used for regularization.

## Key Results
- MPM network with weight dropout achieved 97.49% test accuracy on MNIST (0.52% lower than standard MLP)
- Morphological layers significantly accelerate convergence with large batches
- MPM networks are more prunable than linear networks while maintaining accuracy
- DMNNs with linear activations are proven universal approximators

## Why This Works (Mechanism)

### Mechanism 1: Disruption of Tropical Algebraic Collapse
Stacking morphological layers without intermediate linear activations causes the network to collapse into an equivalent single-layer network due to max-plus matrix multiplication associativity. Linear activations break this algebraic chain, restoring depth benefits.

### Mechanism 2: Gradient Diffusion via Max-Min Summation
The Max-Plus-Min layer mitigates sparse gradient problems by summing dilation and erosion outputs, dispersing gradients across two paths. This structure, combined with learnable linear activation, enables effective weight updates where pure morphological networks would fail.

### Mechanism 3: Universal Approximation via ReLU/Maxout Emulation
The Hybrid-MLP architecture can emulate existing universal approximators like ReLU and Maxout networks by setting specific weights to large constants, forcing the morphological sum to functionally degenerate into the desired activation.

## Foundational Learning

- **Concept: Tropical Algebra (Max-Plus/Min-Plus Semirings)**
  - *Why needed:* This is the mathematical foundation of morphological networks. Understanding this shift from standard algebra is required to grasp why matrix multiplication associativity causes the "collapse" described in the paper.
  - *Quick check:* Why does A ⨁ (B ⨁ C) = (A ⨁ B) ⨁ C imply that a deep morphological network might be equivalent to a shallow one?

- **Concept: Lattice Theory (Dilations and Erosions)**
  - *Why needed:* The paper frames morphological operations as lattice operators. Understanding that dilation distributes over supremum and erosion distributes over infimum is key to analyzing representational limits.
  - *Quick check:* In the context of this paper, is a "dilation" equivalent to a max-plus multiplication, and how does it relate to gradient flow?

- **Concept: Universal Approximation Theory**
  - *Why needed:* The paper's primary theoretical contribution is proving universal approximation under specific constraints. One must understand that "universal approximation" implies the ability to approximate any continuous function on a compact domain.
  - *Quick check:* Does the paper claim that "pure" morphological networks are universal approximators, or is this property only achieved by introducing linear activations?

## Architecture Onboarding

- **Component map:** Input Layer -> Morphological Layer (Dilation + Erosion) -> Linear Activation -> Regularization (Optional)
- **Critical path:**
  1. Initialization is the single most fragile component. Pure morphological networks require weight distributions with negative mean to center outputs, while MPM allows standard zero-mean initialization.
  2. Batch size: Hybrid-MLPs require large batch sizes (6400 vs 64) to converge, suggesting high noise in gradient estimation.
- **Design tradeoffs:**
  - Sparsity vs. Trainability: MPM preserves morphological sparsity but is harder to generalize than Hybrid-MLP, which trains best but loses extreme parameter efficiency.
  - Speed vs. Accuracy: Morphological layers accelerate convergence with large batches but may lag behind standard ResNets on complex benchmarks.
- **Failure signatures:**
  - Stagnation at ~30-70% accuracy: Likely using pure morphological stack without linear activations or incorrect initialization.
  - Gradient Explosion/Vanishing: Using λ ≠ 0.5 in dilation/erosion sum causes unstable mean shifts.
- **First 3 experiments:**
  1. Ablation on Activations: Train 5-layer network on MNIST using Pure Max-Plus, Pure MPM, and MPM + Diagonal Activation to verify the "collapse" theorem.
  2. Initialization Sensitivity: Train MPM network on simple regression task comparing standard vs. shifted mean initialization.
  3. Pruning Capacity: Train MPM and standard MLP to convergence, apply L1 pruning, and plot accuracy vs. sparsity.

## Open Questions the Paper Calls Out

- **Question:** Can improved gradient estimation techniques be developed to enable Hybrid-MLPs to train effectively with small batch sizes?
  - *Basis:* The authors observe Hybrid-MLPs introduce significant noise in gradient estimation, forcing reliance on large batches, and call for improved techniques.

- **Question:** How can initialization strategies be adapted to address the specific difficulty morphological networks face in shifting the mean of weight distributions?
  - *Basis:* Appendix C demonstrates morphological networks struggle to shift weight distribution means during training, making initialization "cumbersome, fragile, and highly sensitive."

- **Question:** Do alternative activation parameterizations exist that can enhance the expressiveness of DMNNs while strictly preserving their inherent sparsity?
  - *Basis:* The conclusion explicitly states future research could explore alternative activation parameterizations to enhance expressiveness while preserving sparsity.

## Limitations

- The paper's theoretical claims rest on specific assumptions about network structure that may not generalize to all practical implementations.
- The MPM layer's effectiveness assumes shared weights with fixed λ=0.5, but the paper doesn't fully explore how deviations affect performance.
- While demonstrating superior prunability, the paper doesn't address computational efficiency during inference compared to standard networks.

## Confidence

- **High Confidence:** The necessity of linear activations to prevent tropical collapse (Theorem 3, Section 3)
- **Medium Confidence:** The MPM layer's effectiveness as a gradient diffusion mechanism
- **Low Confidence:** The universal approximation claim for Hybrid-MLP networks

## Next Checks

1. Systematically vary the linear activation strength (α) in MPM layers across a range of values to determine if the 0.5 λ assumption is optimal.

2. Test MPM networks on non-image domains (time series, tabular data) to validate whether the morphological approach provides consistent advantages across different data types.

3. Measure actual inference time and memory usage of MPM networks versus standard MLPs and ResNets at various sparsity levels to quantify real-world benefits.