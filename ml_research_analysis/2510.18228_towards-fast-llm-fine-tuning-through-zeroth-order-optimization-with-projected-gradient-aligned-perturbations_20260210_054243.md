---
ver: rpa2
title: Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected
  Gradient-Aligned Perturbations
arxiv_id: '2510.18228'
source_url: https://arxiv.org/abs/2510.18228
tags:
- p-gap
- gradient
- mezo
- fine-tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: P-GAP is a zeroth-order optimization method for fine-tuning large
  language models that reduces gradient estimation variance by projecting perturbations
  onto a low-dimensional subspace aligned with the gradient direction. This approach
  decreases the number of perturbed parameters and improves convergence efficiency.
---

# Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations

## Quick Facts
- arXiv ID: 2510.18228
- Source URL: https://arxiv.org/abs/2510.18228
- Reference count: 40
- Primary result: P-GAP achieves up to 6% higher accuracy on classification tasks and up to 12% on generation tasks compared to state-of-the-art baselines, reducing training iterations by up to 81% and GPU hours by up to 70%.

## Executive Summary
This paper introduces P-GAP, a zeroth-order optimization method for fine-tuning large language models that dramatically improves convergence speed and efficiency. By projecting perturbations onto a low-dimensional subspace aligned with the gradient direction, P-GAP reduces gradient estimation variance and decreases the number of perturbed parameters. The method demonstrates substantial performance gains across models from RoBERTa-large to OPT-13B and LLaMA-3, achieving higher accuracy while requiring significantly fewer training iterations and GPU hours compared to existing approaches.

## Method Summary
P-GAP implements a zeroth-order optimization framework that estimates gradients using only forward passes. The core innovation involves a lazy update strategy where every k=100 steps, the method computes a rank-r SVD of an approximate gradient matrix using h=10 probe perturbations. These basis matrices are then used to generate perturbations in a low-dimensional space (r² dimensions) that are aligned with the gradient direction. The projection constraint ensures perturbations probe the most informative directions of the loss landscape, while the low-rank restriction reduces variance. During intermediate steps, the method reuses the fixed basis for efficient updates, amortizing the computational overhead of subspace discovery.

## Key Results
- P-GAP achieves up to 6% higher accuracy on classification tasks and up to 12% on generation tasks compared to state-of-the-art baselines
- Training iterations reduced by up to 81% and GPU hours by up to 70% across tested model scales
- Memory usage remains low throughout training, maintaining the efficiency benefits of zeroth-order optimization
- Performance improvements are consistent across diverse models including RoBERTa-large, OPT-2.7B, OPT-13B, and LLaMA-3-8B

## Why This Works (Mechanism)

### Mechanism 1: Variance Reduction via Dimensionality Constraint
Standard ZO methods perturb all parameters, causing variance to scale linearly with dimension. P-GAP restricts perturbations to a low-rank subspace (r << d), effectively decoupling variance from full model size. The core assumption is that gradients reside primarily in a low-dimensional subspace during fine-tuning.

### Mechanism 2: Signal Amplification via Gradient-Aligned Perturbations
Random isotropic perturbations waste query budget on irrelevant directions. P-GAP aligns perturbations with the estimated gradient direction through a tensor hyperplane constraint, ensuring finite-difference calculations probe the most informative loss landscape directions.

### Mechanism 3: Amortized Overhead via Lazy Subspace Updates
The gradient subspace changes slowly enough that SVD computation need not occur every step. P-GAP performs expensive subspace estimation only every k steps, reusing the fixed basis for intermediate steps and amortizing discovery cost over many cheap update steps.

## Foundational Learning

- **Concept: Zeroth-Order (ZO) Optimization (Finite Differences)**
  - Why needed: P-GAP estimates gradients using only forward passes without storing activations
  - Quick check: How does increasing perturbation scale ε affect bias vs. variance tradeoff in Equation 1?

- **Concept: Low-Rank Matrix Decomposition (SVD)**
  - Why needed: The method decomposes gradient matrix S into USV^T to identify important dimensions to perturb
  - Quick check: In Equation 10, what does matrix S_r represent in terms of gradient's energy?

- **Concept: Subspace Constrained Optimization**
  - Why needed: Updates occur in lower-dimensional space reconstructed from low-rank subspace
  - Quick check: Why does projecting updates into lower-dimensional space reduce estimator variance (referencing Lemma 1)?

## Architecture Onboarding

- **Component map:** Probe Phase -> Subspace Generator -> Projection Module -> Reconstructor
- **Critical path:** SVD computation in LOWER_DIM_GENERATE, which requires synchronizing h probes and performing matrix decomposition
- **Design tradeoffs:**
  - Rank r vs. Fidelity: Higher r captures more information but increases variance and compute
  - Window k vs. Staleness: Larger k reduces SVD overhead but increases risk of stale basis
  - Probe count h: Too few probes results in noisy basis U, V, making alignment useless
- **Failure signatures:**
  - Stagnant Loss: Suggests rank r is too low or projection magnitude δ decayed too quickly
  - Exploding Gradients: Alignment constraint may be violated if δ is too high relative to ‖S_r‖_F
  - Slow Wall-Clock Time: k is likely set too small, causing constant SVD re-computation
- **First 3 experiments:**
  1. Baseline Validation: Run P-GAP vs. MeZO on OPT-2.7B for SST-2 classification task
  2. Rank Ablation: Test varying r ∈ {64, 128, 256, 512} to find performance saturation point
  3. Alignment Ablation: Disable projection step to isolate gain from gradient-aligned vs. low-rank components

## Open Questions the Paper Calls Out
- How does P-GAP scale to models significantly larger than 13B parameters (e.g., 70B+)?
- How does the lazy update window size affect stability during non-stationary training phases?
- Can P-GAP be successfully combined with low-bit weight quantization without destabilizing gradient estimation?

## Limitations
- Experiments limited to models up to 13B parameters, leaving scalability to larger models unverified
- Fixed lazy update window (k=100) assumes slow gradient subspace changes, which may not hold for all training scenarios
- Integration with low-bit quantization remains unexplored, despite memory efficiency being a key advantage

## Confidence
- Variance reduction mechanism: High - well-supported by theoretical analysis and empirical evidence
- Gradient alignment benefits: Medium - supported by Figure 1 but lacks comprehensive ablation studies
- Lazy update strategy: Medium - practical implementation is clear but sensitivity to k parameter is unexplored
- Scalability to larger models: Low - experimental validation only covers up to 13B parameters

## Next Checks
1. Verify rank ablation study identifies optimal r value where accuracy saturates but variance is minimized
2. Implement alignment ablation to isolate performance gain specifically from gradient-aligned component
3. Test P-GAP integration with 4-bit quantization to measure accuracy degradation and memory savings