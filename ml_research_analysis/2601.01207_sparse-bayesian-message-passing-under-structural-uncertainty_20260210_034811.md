---
ver: rpa2
title: Sparse Bayesian Message Passing under Structural Uncertainty
arxiv_id: '2601.01207'
source_url: https://arxiv.org/abs/2601.01207
tags:
- signed
- structural
- graph
- spam
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SpaM, a sparse Bayesian message passing network
  for semi-supervised node classification under structural uncertainty. SpaM explicitly
  models a posterior distribution over signed adjacency matrices, allowing each edge
  to be positive, negative, or absent, which naturally captures both edge noise and
  heterophily.
---

# Sparse Bayesian Message Passing under Structural Uncertainty

## Quick Facts
- arXiv ID: 2601.01207
- Source URL: https://arxiv.org/abs/2601.01207
- Reference count: 36
- Primary result: SpaM achieves consistent improvements on heterophilic benchmarks under structural uncertainty by modeling signed adjacency posterior and sparse neighbor selection.

## Executive Summary
This paper introduces SpaM, a sparse Bayesian message passing network designed for semi-supervised node classification in graphs with structural uncertainty. SpaM explicitly models a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent, thereby capturing both edge noise and heterophily. The core innovation is a sparse signed message passing layer that performs local sparse coding to select informative neighbors while aggregating positive and negative relations through separate channels. Theoretical analysis shows SpaM approximates an ideal Bayesian predictor under structural uncertainty, and extensive experiments demonstrate consistent outperformance over strong baselines, especially under structural noise and low homophily.

## Method Summary
SpaM extends graph neural networks by incorporating a Bayesian framework for signed graphs under structural uncertainty. It models a posterior distribution over signed adjacency matrices, enabling each edge to be positive, negative, or absent. The key component is the sparse signed message passing layer, which performs local sparse coding to select a subset of neighbors per node, and aggregates their features through separate positive and negative message channels. This approach approximates a maximum a posteriori (MAP) estimator for the underlying Bayesian predictor, providing robustness to edge noise and heterophily. SpaM is evaluated on nine heterophilic benchmarks and three large-scale graphs, consistently outperforming state-of-the-art methods.

## Key Results
- SpaM achieves 1-3% accuracy improvements over strong baselines like H2GCN and GPRGNN on heterophilic benchmarks.
- Robustness under structural noise: SpaM maintains performance under synthetic edge corruption, especially in low homophily regimes.
- Ablation studies confirm that structural posterior modeling, sparse coding, and signed aggregation each contribute to improved accuracy and robustness.

## Why This Works (Mechanism)
SpaM's effectiveness stems from its explicit modeling of uncertainty in the graph structure via a signed adjacency posterior. By allowing edges to be positive, negative, or absent, SpaM naturally captures both heterophily (negative edges) and edge noise. The sparse signed message passing layer performs local sparse coding to select only the most informative neighbors, reducing the influence of noisy or irrelevant edges. Separate aggregation of positive and negative messages allows the model to exploit both homophily and heterophily patterns. Theoretical analysis shows this mechanism approximates an ideal Bayesian predictor, making SpaM robust under structural uncertainty.

## Foundational Learning
- **Structural Uncertainty**: Models uncertainty in graph edges (noise, missing, or mislabeled links) using a posterior over signed adjacency matrices; needed to handle real-world noisy graphs; quick check: compare predictions with/without edge noise.
- **Heterophily**: Handles graphs where connected nodes may have different labels; needed for realistic graph datasets; quick check: evaluate on graphs with low homophily (e.g., <0.1).
- **Sparse Coding**: Selects a small subset of informative neighbors per node; needed to reduce noise influence and computational cost; quick check: monitor sparsity level and impact on accuracy.
- **Signed Message Passing**: Aggregates features from positive and negative edges separately; needed to capture both homophily and heterophily; quick check: ablate positive/negative channels and measure performance drop.
- **Bayesian Approximation**: Uses MAP estimation to approximate Bayesian inference under uncertainty; needed for tractable learning; quick check: compare with full Bayesian inference (if feasible).
- **Structural Posterior Modeling**: Explicitly models the distribution of possible graph structures; needed for robustness to edge corruption; quick check: inject synthetic noise and measure degradation.

## Architecture Onboarding
- **Component Map**: Input Features -> Sparse Signed MP Layer -> Node Embeddings -> Classification Head
- **Critical Path**: Input features are processed through the sparse signed message passing layer, which performs neighbor selection (sparse coding) and signed aggregation, producing node embeddings for the final classifier.
- **Design Tradeoffs**: Balances robustness (via structural uncertainty modeling) against computational cost (sparse coding per node). Signed aggregation allows exploitation of both homophily and heterophily but increases model complexity.
- **Failure Signatures**: Performance degradation under extreme edge corruption (>50% noise) or when the graph has very high-degree nodes (scalability issues due to sparse coding cost).
- **First Experiments**:
  1. Compare SpaM with and without signed aggregation on a heterophilic benchmark.
  2. Evaluate impact of sparsity level in sparse coding on accuracy and runtime.
  3. Test SpaM under incremental levels of synthetic edge noise to quantify robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical approximation guarantees depend on idealized assumptions about edge noise and posterior sparsity.
- Computational complexity scales with local neighborhood size, potentially limiting scalability on high-degree graphs.
- Performance gains over strong baselines are sometimes modest (1-3%), raising questions about practical significance in low-noise regimes.
- Signed message passing assumes symmetric treatment of positive and negative edges, not capturing asymmetric influence patterns in real-world signed graphs.

## Confidence
- **High**: Modeling a posterior over signed adjacency matrices is technically sound and well-supported by experiments.
- **Medium**: Theoretical analysis linking sparse coding to MAP estimation depends on idealized assumptions.
- **Medium**: Robustness claims under structural uncertainty are empirically demonstrated but lack extensive stress-testing across noise levels.

## Next Checks
1. Conduct ablation studies isolating the impact of the signed aggregation mechanism versus the sparse coding step on heterophilic graphs with varying noise levels.
2. Evaluate SpaM's performance and runtime on graphs with high-degree nodes (average degree >50) to assess scalability limitations.
3. Test SpaM under synthetic edge corruption rates beyond 50% to quantify the breaking point of its robustness guarantees.