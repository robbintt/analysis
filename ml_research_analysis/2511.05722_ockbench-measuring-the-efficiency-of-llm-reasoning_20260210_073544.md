---
ver: rpa2
title: 'OckBench: Measuring the Efficiency of LLM Reasoning'
arxiv_id: '2511.05722'
source_url: https://arxiv.org/abs/2511.05722
tags:
- reasoning
- accuracy
- efficiency
- token
- open-source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OckBench addresses the gap in LLM evaluation by jointly measuring
  reasoning accuracy and decoding token efficiency. While current benchmarks focus
  on output quality, OckBench introduces a hardware- and model-agnostic metric based
  on decoding token count to capture the often-overlooked computational cost of reasoning.
---

# OckBench: Measuring the Efficiency of LLM Reasoning

## Quick Facts
- **arXiv ID**: 2511.05722
- **Source URL**: https://arxiv.org/abs/2511.05722
- **Reference count**: 29
- **Primary result**: OckBench introduces a hardware- and model-agnostic efficiency metric based on decoding token count to complement accuracy evaluation for LLM reasoning tasks.

## Executive Summary
OckBench addresses the gap in LLM evaluation by jointly measuring reasoning accuracy and decoding token efficiency. While current benchmarks focus on output quality, OckBench introduces a hardware- and model-agnostic metric based on decoding token count to capture the often-overlooked computational cost of reasoning. The benchmark includes 200 high-variance problems each from math (GSM8K, AIME 24/25) and coding (MBPP variant) domains to highlight efficiency differences. Across 19 models tested, commercial models achieved 60.8% average accuracy versus 35.3% for open-source models, but with wide variance in token efficiency: GPT-4o used 495 tokens at 35% accuracy, while Gemini-2.5 Pro used 5,198 tokens at 68% accuracy. Among open-source models, Sky-T1-7B balanced performance and efficiency (556 tokens, 33% accuracy), whereas reasoning variants of Qwen models often consumed far more tokens without proportional accuracy gains. OckBench provides a unified, reproducible platform to guide research toward more token-efficient reasoning.

## Method Summary
OckBench is a benchmark that evaluates both reasoning accuracy and token efficiency by measuring decoding token count under fixed settings. The methodology filters source benchmarks (GSM8K, AIME 24/25, MBPP variant) to select 200 questions per domain that exhibit high cross-model token variance, ensuring efficiency differences are amplified. The Reasoning Efficiency metric is computed as tokens divided by accuracy, and models are ranked via Pareto frontiers in accuracy-token count space. The evaluation uses a "pass at one" constraint without retries, creating a standardized comparison across 19 diverse models including commercial and open-source variants.

## Key Results
- Commercial models achieved 60.8% average accuracy versus 35.3% for open-source models on OckBench tasks
- GPT-4o used 495 tokens at 35% accuracy, while Gemini-2.5 Pro used 5,198 tokens at 68% accuracy, demonstrating wide efficiency variance
- Among open-source models, Sky-T1-7B balanced performance and efficiency (556 tokens, 33% accuracy)
- Qwen3-thinking variants consumed 2-3× more tokens than base Qwen models without proportional accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoding token count serves as a model-agnostic, hardware-agnostic proxy for reasoning efficiency.
- Mechanism: By fixing task and decoding settings, token count captures the intrinsic verbosity of a model's reasoning process independent of deployment hardware. This normalizes comparison across diverse architectures.
- Core assumption: Token count scales linearly with latency, energy, and monetary cost in real deployments.
- Evidence anchors:
  - [abstract] "model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks"
  - [section 2.2] "we adopt decoding token count (on a fixed task under a fixed decoding setting) as our core efficiency metric"
  - [corpus] Related work on token pruning (Think Clearly, arXiv:2507.08806) confirms redundancy in reasoning traces but does not validate token count as a universal efficiency proxy.
- Break condition: If deployment uses speculative decoding or parallel token generation, raw token count may not linearly map to latency.

### Mechanism 2
- Claim: Selecting benchmark questions by high cross-model token variance amplifies efficiency signal detection.
- Mechanism: Questions where all models use similar token counts provide no efficiency discrimination. Filtering for the top 200 highest-variance instances ensures the benchmark stress-tests reasoning verbosity differences.
- Core assumption: High-variance questions are representative of real-world reasoning challenges rather than outliers.
- Evidence anchors:
  - [section 3.2] "select the top 200 questions that exhibit high variance in decoding token usage among baseline models"
  - [section 4.1] "A problem where all models use a similar number of tokens tells us nothing about efficiency"
  - [corpus] No direct validation of variance-based selection in neighbors; this methodology is novel to OckBench.
- Break condition: If high-variance questions cluster in narrow difficulty bands, the benchmark may not generalize to broader task distributions.

### Mechanism 3
- Claim: Accuracy-efficiency trade-offs can be characterized via Pareto frontiers.
- Mechanism: Plotting models on an accuracy vs. token-count plane reveals which models dominate (higher accuracy at lower token cost) and which are dominated. This guides model selection for latency-sensitive deployments.
- Core assumption: Users have fixed efficiency budgets and will trade accuracy for token savings.
- Evidence anchors:
  - [abstract] "demonstrate Pareto frontiers over the accuracy-efficiency plane"
  - [section 5] "OckBench provides a reproducible and fair platform for comparing the accuracy–efficiency trade-off"
  - [corpus] "Do LLMs Overthink Basic Math Reasoning" (arXiv:2507.04023) independently benchmarks accuracy-efficiency tradeoffs, supporting the utility of this framing.
- Break condition: If inference cost is negligible (e.g., batch offline processing), Pareto analysis may be irrelevant.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) prompting**
  - Why needed here: The paper attributes rising token counts to CoT-style reasoning. Understanding CoT explains why "thinking" variants (e.g., Qwen3-thinking) generate more tokens.
  - Quick check question: Can you explain why step-by-step reasoning increases token count compared to direct answering?

- Concept: **Pareto efficiency**
  - Why needed here: The paper frames model comparison as a multi-objective optimization problem (accuracy vs. efficiency). Understanding Pareto frontiers is necessary to interpret Figure 2 and rank trade-offs.
  - Quick check question: If Model A has 60% accuracy with 1000 tokens and Model B has 65% accuracy with 3000 tokens, which is Pareto-dominated?

- Concept: **Token-budget-aware decoding**
  - Why needed here: The paper argues against treating tokens as "free." Corpus neighbors (Token-budget-aware LLM reasoning, arXiv:2505.08392) suggest active research in constraining generation length.
  - Quick check question: What mechanisms could enforce a maximum token budget during inference?

## Architecture Onboarding

- Component map:
  Dataset layer -> Evaluation harness -> Analysis layer -> Output layer
  (GSM8K, AIME24/25, MBPP variant) -> Fixed decoding inference -> Reasoning Efficiency metric, Pareto frontier -> Ranked tables, scatter plots

- Critical path:
  1. Curate raw question pools from source benchmarks
  2. Run pilot inference across diverse models to compute per-question token variance
  3. Select top 200 highest-variance questions per domain
  4. Run full evaluation on target models
  5. Compute Reasoning Efficiency and plot accuracy vs. token count

- Design tradeoffs:
  - Variance-based filtering increases efficiency signal but may reduce benchmark breadth
  - Token count metric is deployment-agnostic but ignores parallelism opportunities
  - Single-pass evaluation ("pass at one") favors concise models; multi-retry strategies would change rankings

- Failure signatures:
  - All models cluster at similar token counts → variance filtering failed (questions too easy or too hard)
  - Accuracy near 0% or 100% → floor/ceiling effects obscure efficiency differences
  - Reasoning Efficiency metric unstable for low-accuracy models (division by small denominator)

- First 3 experiments:
  1. **Baseline sanity check**: Run OckBench on two models with known verbosity differences (e.g., GPT-4o vs. Gemini-2.5-Pro) to confirm variance filtering yields discriminative token counts
  2. **Ablation on question selection**: Compare rankings using all questions vs. high-variance subset to validate that filtering improves efficiency signal
  3. **Thinking vs. non-thinking variants**: Evaluate Qwen3-14B in both modes to quantify token overhead of explicit CoT and verify reported 2-3× increases (Table 1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the accuracy-efficiency trade-offs identified in math and coding tasks generalize to other domains and real-world workloads?
- Basis in paper: [explicit] The conclusion states that future work should explore "further evaluation on additional domains and real-world workloads."
- Why unresolved: The current study is limited to structured reasoning tasks (GSM8K, AIME, MBPP) which may not represent open-ended or qualitative domains.
- What evidence would resolve it: Extending the OckBench evaluation framework to domains like creative writing, agentic planning, or multi-modal reasoning.

### Open Question 2
- Question: Can dynamic reasoning budgets or early-exit mechanisms effectively optimize the Pareto frontier of the evaluated models?
- Basis in paper: [explicit] The authors list "dynamic reasoning budgets, early-exit mechanisms, [and] token-pruning strategies" as specific avenues for future work.
- Why unresolved: The current benchmark evaluates static generation strategies; it does not measure the efficacy of methods that adaptively halt reasoning.
- What evidence would resolve it: Benchmarking models equipped with adaptive compute mechanisms to see if they achieve higher accuracy per token on OckBench.

### Open Question 3
- Question: Does the high-variance question selection criterion introduce a bias that distorts the representation of average model efficiency?
- Basis in paper: [inferred] The methodology selects the "top 200 questions that exhibit high variance in decoding token usage," filtering for "efficiency contrast" rather than typical performance.
- Why unresolved: Selecting for variance highlights outliers, but it remains unclear if efficiency on these specific "high-contrast" problems correlates with efficiency on a random distribution of problems.
- What evidence would resolve it: A comparison of model rankings on the OckBench subset versus their rankings on the full, unfiltered source datasets.

## Limitations

- Token-count proxy validity: The assumption that token count linearly maps to real-world cost (latency, energy, monetary) is not validated across different deployment scenarios
- Question representativeness: High-variance question selection may not generalize to typical reasoning challenges or broader task distributions
- Single-pass constraint: "Pass at one" evaluation may unfairly disadvantage models that benefit from multiple attempts or adaptive reasoning strategies

## Confidence

- **High confidence**: Core methodology of using token count as comparative metric within controlled settings; observation that commercial models achieve higher accuracy than open-source models while showing wide efficiency variance
- **Medium confidence**: Effectiveness of variance-based question selection for creating discriminative benchmarks; utility of Pareto frontiers for model selection under efficiency constraints
- **Low confidence**: Universal applicability of token count as deployment-agnostic efficiency proxy across all inference scenarios; representativeness of high-variance questions for real-world reasoning challenges

## Next Checks

1. **Deployment cost validation**: Measure actual inference latency, energy consumption, and monetary cost for a subset of models across different hardware configurations, then correlate these metrics with the benchmark's token-count predictions to validate the token-count proxy assumption

2. **Question distribution analysis**: Analyze the difficulty distribution and reasoning patterns of the selected high-variance questions to determine whether they represent typical reasoning challenges or cluster in specific difficulty bands that might limit benchmark generalizability

3. **Multi-strategy evaluation comparison**: Run the benchmark with alternative evaluation strategies including multiple attempts per question and adaptive reasoning, then compare rankings to the single-pass results to understand how evaluation constraints affect model efficiency characterization