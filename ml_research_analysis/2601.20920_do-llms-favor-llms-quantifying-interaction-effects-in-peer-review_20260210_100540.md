---
ver: rpa2
title: Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review
arxiv_id: '2601.20920'
source_url: https://arxiv.org/abs/2601.20920
tags:
- papers
- reviews
- llm-aided
- human
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM-assisted reviews treat LLM-assisted
  papers differently than human-written papers. The authors analyze over 125,000 paper-review
  pairs from ICLR, NeurIPS, and ICML using observational data and synthetic LLM-generated
  reviews.
---

# Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review

## Quick Facts
- arXiv ID: 2601.20920
- Source URL: https://arxiv.org/abs/2601.20920
- Authors: Vibhhu Sharma; Thorsten Joachims; Sarah Dean
- Reference count: 40
- Primary result: LLM-assisted reviews show no preferential treatment toward LLM-assisted papers after controlling for paper quality

## Executive Summary
This paper investigates whether large language models (LLMs) introduce bias in peer review when both papers and reviews are LLM-assisted. Analyzing over 125,000 paper-review pairs from major machine learning conferences, the authors find that LLM-assisted reviews are more lenient toward lower quality papers in general. This creates a spurious interaction effect where LLM-assisted papers appear to receive preferential treatment, but this disappears when controlling for paper quality. The study reveals that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. These findings provide important evidence for developing policies governing LLM use during peer review.

## Method Summary
The authors analyze observational data from ICLR, NeurIPS, and ICML, examining over 125,000 paper-review pairs to study interaction effects between paper and review generation methods. They develop a synthetic review generation pipeline using prompts based on the ACL Rolling Review rubric to systematically control for paper quality and isolate the effects of LLM assistance. The analysis examines reviewer ratings, acceptance recommendations, and metareview decisions across different combinations of paper and review generation methods (human-only, LLM-assisted, and fully LLM-generated). The study employs both observational analysis of real conference data and controlled synthetic experiments to validate findings and understand mechanisms.

## Key Results
- LLM-assisted reviews are more lenient toward lower quality papers in general, creating a spurious interaction effect rather than genuine preferential treatment
- When controlling for paper quality, the differential kindness of LLM-assisted reviews toward LLM-assisted papers disappears
- Fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency
- LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher

## Why This Works (Mechanism)
The mechanism underlying the observed effects centers on the statistical properties of LLM-generated content and review behavior. LLM-assisted reviews tend to be systematically more lenient across all paper qualities, with this effect being most pronounced for lower quality submissions. This creates a spurious correlation where LLM-assisted papers (which are overrepresented among weaker submissions) appear to receive preferential treatment. The rating compression in fully LLM-generated reviews occurs because these models tend to avoid extreme ratings and produce more uniform distributions, failing to discriminate effectively between paper qualities. When humans use LLMs as assistants rather than fully delegating review generation, they maintain better calibration and discrimination ability, reducing the leniency effect while preserving the benefits of LLM assistance.

## Foundational Learning
**Observational causal inference** - Needed to distinguish between correlation and causation in peer review data; quick check: review methodology section for discussion of confounding variables and control strategies.
**Synthetic data generation** - Required to create controlled experiments that isolate the effects of LLM assistance; quick check: examine the synthetic review generation pipeline and prompt design.
**Rating scale analysis** - Essential for understanding how different review methods compress or expand rating distributions; quick check: compare rating distributions across human-only, LLM-assisted, and fully LLM-generated reviews.
**Interaction effect analysis** - Critical for identifying whether treatment effects vary by subgroups (paper quality × review method); quick check: examine statistical tests for interaction terms in regression models.
**Peer review workflow understanding** - Necessary to contextualize findings within actual conference processes; quick check: review discussion of metareview decision-making and reviewer assignment practices.

## Architecture Onboarding

Component map: Synthetic review generation -> Observational data analysis -> Rating distribution analysis -> Interaction effect testing -> Policy implications

Critical path: Synthetic review generation → Rating distribution analysis → Interaction effect testing → Policy recommendations

Design tradeoffs: The study balances observational analysis of real conference data (providing ecological validity) with synthetic experiments (providing causal clarity). The tradeoff involves accepting potential confounders in observational data while gaining controlled manipulation in synthetic experiments.

Failure signatures: Rating compression across all quality levels would indicate LLM over-reliance; persistent interaction effects after quality control would suggest true preferential treatment; inconsistent results between observational and synthetic analyses would indicate methodological issues.

First experiments:
1. Compare rating distributions across human-only, LLM-assisted, and fully LLM-generated reviews using synthetic data
2. Test for interaction effects between paper quality and review generation method in observational data
3. Examine metareview acceptance rates given equivalent reviewer scores across review generation methods

## Open Questions the Paper Calls Out
None

## Limitations
- The observational nature cannot definitively establish causation, though synthetic experiments strengthen causal interpretations
- Analysis focuses on three top-tier ML conferences where LLM usage patterns may differ from other fields or conference tiers
- Synthetic review prompts may not fully capture the diversity of real reviewer prompts, potentially limiting generalizability

## Confidence
- No preferential treatment after quality control: High
- Mechanisms of rating compression and leniency: Medium
- LLM-assisted metareviews more likely to accept: Medium

## Next Checks
1. Replicate synthetic review generation with prompts from actual reviewers to validate rating compression generalizes beyond synthetic prompts
2. Conduct controlled experiment with same paper receiving both LLM-assisted and human-only reviews from matched-expertise reviewers
3. Extend analysis to conferences in different scientific domains to assess generalizability beyond machine learning venues