---
ver: rpa2
title: 'DDT: A Dual-Masking Dual-Expert Transformer for Energy Time-Series Forecasting'
arxiv_id: '2601.07250'
source_url: https://arxiv.org/abs/2601.07250
tags:
- causal
- dynamic
- forecasting
- data
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DDT introduces a dual-masking mechanism combining strict causal
  masks with data-driven dynamic masks to ensure temporal consistency while adaptively
  focusing on salient historical information. It also employs a dual-expert system
  that separately models temporal dynamics and cross-variable correlations, integrated
  through a dynamic gated fusion module.
---

# DDT: A Dual-Masking Dual-Expert Transformer for Energy Time-Series Forecasting

## Quick Facts
- arXiv ID: 2601.07250
- Source URL: https://arxiv.org/abs/2601.07250
- Reference count: 40
- DDT achieves MSE of 0.405 and MAE of 0.423 on ETTh1 dataset, outperforming state-of-the-art baselines

## Executive Summary
DDT introduces a novel dual-masking dual-expert transformer architecture specifically designed for energy time-series forecasting. The model combines strict causal masks with data-driven dynamic masks to ensure temporal consistency while adaptively focusing on salient historical information. A dual-expert system separately models temporal dynamics and cross-variable correlations, integrated through a dynamic gated fusion module. Evaluated on 7 energy benchmark datasets, DDT consistently outperforms state-of-the-art baselines across multiple prediction horizons, establishing a new benchmark for energy time-series forecasting while balancing accuracy with interpretability and efficiency.

## Method Summary
DDT employs a dual-masking mechanism that combines strict causal masks with data-driven dynamic masks to ensure temporal consistency while adaptively focusing on salient historical information. The architecture features a dual-expert system that separately models temporal dynamics and cross-variable correlations, integrated through a dynamic gated fusion module. This design allows the model to capture both sequential patterns and complex inter-variable relationships in energy time-series data. The model was evaluated on 7 energy benchmark datasets including ETTh, Electricity, and Solar, demonstrating consistent superiority over state-of-the-art baselines across multiple prediction horizons.

## Key Results
- On ETTh1 dataset, DDT achieves MSE of 0.405 and MAE of 0.423
- MSE reductions up to 21.7% compared to direct prediction methods
- Consistent outperformance across 7 energy benchmark datasets

## Why This Works (Mechanism)
DDT's effectiveness stems from its dual-masking approach that ensures temporal consistency while allowing adaptive focus on important historical patterns. The strict causal mask prevents information leakage from future time steps, maintaining the autoregressive property essential for forecasting. The dynamic mask learns data-driven attention patterns, enabling the model to adaptively emphasize relevant historical information. The dual-expert architecture separately captures temporal dynamics and cross-variable correlations, preventing interference between these two types of information. The gated fusion module dynamically combines these expert representations based on input characteristics, allowing flexible adaptation to different forecasting scenarios.

## Foundational Learning

**Causal Attention Masks**
- Why needed: Prevents information leakage from future time steps, maintaining autoregressive property
- Quick check: Verify that predictions at time t only depend on inputs from t-1 and earlier

**Dynamic Attention Mechanisms**
- Why needed: Allows adaptive focus on salient historical patterns rather than fixed attention
- Quick check: Confirm learned attention weights vary meaningfully with input data characteristics

**Expert Fusion Architectures**
- Why needed: Separates modeling of temporal dynamics from cross-variable correlations to prevent interference
- Quick check: Validate that both expert pathways contribute meaningfully to final predictions

## Architecture Onboarding

**Component Map**
Input Sequence -> Causal Mask + Dynamic Mask -> Dual Expert Blocks -> Gated Fusion -> Output Layer

**Critical Path**
Data Preprocessing -> Masking Layers -> Expert Block 1 (Temporal) -> Expert Block 2 (Cross-variable) -> Gated Fusion -> Prediction Head

**Design Tradeoffs**
The dual-expert architecture increases model capacity and specialization but adds computational overhead compared to single-expert designs. The dynamic masking provides adaptability but requires additional learnable parameters. The gated fusion introduces flexibility but adds complexity to training and inference.

**Failure Signatures**
Performance degradation when temporal and cross-variable signals conflict heavily, potential overfitting with small datasets due to increased parameter count, and possible instability in gated fusion if expert outputs have mismatched scales.

**First Experiments**
1. Baseline transformer without masking to establish importance of causal constraints
2. Single-expert variant to measure contribution of expert separation
3. Static mask comparison to quantify benefits of dynamic masking

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several implicit questions arise from the work. The effectiveness of the dual-masking approach compared to alternative masking strategies remains unexplored. The sensitivity of the dual-expert architecture to different gating mechanisms and fusion strategies is not investigated. The model's performance on non-energy time-series datasets and its generalization capabilities across different domains are not evaluated. Additionally, the computational efficiency trade-offs of the more complex architecture compared to simpler alternatives are not quantified.

## Limitations

The paper lacks comparisons against more recent specialized transformer variants like PatchTST or PowerFormer, which could provide a more complete benchmark. Computational efficiency of the dual-expert system relative to simpler models is not quantified, leaving questions about practical deployment trade-offs. The interpretability benefits of the dual-expert design are asserted but not empirically validated through techniques like attention visualization or feature importance analysis.

## Confidence

- Performance claims vs baselines: High
- Dual-masking effectiveness: High
- Dual-expert architecture novelty: Medium
- Long-term forecasting generalization: Low
- Interpretability advantages: Low

## Next Checks

1. Benchmark DDT against recent specialized transformer architectures (PatchTST, PowerFormer) on the same energy datasets to establish relative standing.
2. Conduct a computational efficiency analysis comparing DDT's parameter count, inference time, and memory usage against both traditional models and transformer variants.
3. Evaluate DDT's performance across multiple long-range horizons (48, 72, 96 hours) to characterize degradation patterns and confirm sustained accuracy improvements.