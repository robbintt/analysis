---
ver: rpa2
title: Can Past Experience Accelerate LLM Reasoning?
arxiv_id: '2505.20643'
source_url: https://arxiv.org/abs/2505.20643
tags:
- reasoning
- arxiv
- memory
- methods
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether LLMs can achieve reasoning speedup\
  \ through repeated exposure to similar tasks, analogous to human learning. The authors\
  \ formalize this problem across task similarity and compute budget dimensions, and\
  \ propose SpeedupLLM\u2014a framework combining adaptive compute allocation with\
  \ memory mechanisms."
---

# Can Past Experience Accelerate LLM Reasoning?

## Quick Facts
- arXiv ID: 2505.20643
- Source URL: https://arxiv.org/abs/2505.20643
- Reference count: 34
- Key outcome: LLMs can reduce reasoning compute costs by up to 56% when equipped with appropriate memory and scaling strategies, especially for similar questions.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) can achieve reasoning speedup through repeated exposure to similar tasks, analogous to human learning. The authors formalize this problem across task similarity and compute budget dimensions, and propose SpeedupLLM—a framework combining adaptive compute allocation with memory mechanisms. Through extensive experiments across four test-time scaling methods, five memory methods, and four similarity levels, they demonstrate that LLMs can significantly reduce inference costs while maintaining accuracy, with the strongest gains observed for similar questions and when using episodic memory methods.

## Method Summary
The paper proposes SpeedupLLM, a framework that combines adaptive compute allocation with memory mechanisms to accelerate LLM reasoning. The method modifies standard test-time scaling (like Best-of-N or DFS) by implementing sequential generation with early stopping based on a quality threshold. Memory mechanisms include No Memory, SFT with LoRA updates, In-Context Learning with up to 3 examples, and reflection-based methods. The framework was evaluated on MATH dataset variations across four similarity levels (identical to same underlying knowledge) using four test-time scaling methods, measuring compute cost (number of generated answers/nodes/tokens) and accuracy.

## Key Results
- LLMs can reduce reasoning compute costs by up to 56% when equipped with appropriate memory and scaling strategies
- Speedup is most pronounced for similar questions (16-15% reduction for S1/S2 vs potential drops for S4)
- Episodic memory methods (In-Context Learning, SFT) outperform semantic reflection-based methods for comprehensive recall of past experience
- Text-based memory methods plateau around 3 examples due to context window saturation, while parametric memory continues improving

## Why This Works (Mechanism)

### Mechanism 1
Adaptive compute allocation reduces reasoning cost by sequentially generating candidates and stopping as soon as a response exceeds a quality threshold τ. This minimizes the number of generated units required to find a satisfying solution. Core assumption: A reliable scoring function exists to evaluate answer quality in real-time. Break condition: If threshold is set too high or scorer is misaligned, system may fail to terminate early or stop on incorrect answers.

### Mechanism 2
Episodic memory mechanisms (e.g., In-Context Learning) improve answer quality more effectively for similar tasks than semantic reflection methods. Storing raw question-answer pairs and injecting them as context allows the model to retrieve and mimic relevant reasoning patterns, raising the base probability of correct answers. Core assumption: Current question shares structural features with retrieved memories and fits within context window. Break condition: Performance degrades if retrieved memories are irrelevant or context window saturates.

### Mechanism 3
Reasoning speedup is strongly correlated with question similarity; gains diminish or reverse as tasks diverge. As similarity decreases, utility of memory buffer drops and the model may apply irrelevant reasoning patterns, increasing cognitive load. Core assumption: Distribution includes subset of similar/recurrent problems. Break condition: In highly diverse environments, memory may introduce noise leading to negative transfer where compute cost increases.

## Foundational Learning

- **Test-Time Scaling**: Methods like Best-of-N trade compute for accuracy by generating multiple candidates. Understanding this baseline is crucial since the paper aims to optimize it. Quick check: Can you explain why Best-of-N sampling increases inference time, and what specific variable the authors target to reduce it?

- **Episodic vs. Semantic Memory**: The paper distinguishes between raw experience (Episodic, e.g., In-Context) and summarized rules (Semantic, e.g., Reflection). This distinction is critical for interpreting results showing Episodic memory is superior. Quick check: What is the difference between "Reflect-Update" and "In-Context" memory, and which performed better?

- **Verifiers/Reward Models**: The mechanism relies on score(·) to decide when to stop. Without understanding how LLMs judge their own outputs (LLM-as-a-Judge or PRM), the adaptive loop cannot function. Quick check: In the adaptive Best-of-N method, what condition must be met for the generation loop to terminate early?

## Architecture Onboarding

- **Component map**: Memory Buffer -> Prompt Constructor -> Generator -> Scorer -> Controller
- **Critical path**: The feedback loop where Scorer validates an answer → Memory Buffer updates → Prompt Constructor utilizes this memory for next query to reduce iterations required by Generator
- **Design tradeoffs**: SFT vs. In-Context: SFT offers consistent gains without context limits but is slower to update and prone to overfitting. In-Context is faster but plateaus when context window fills. Threshold τ: High threshold ensures quality but reduces speedup opportunity; low threshold maximizes speedup but risks accuracy.
- **Failure signatures**: Context Window Saturation: text-based methods stop improving after ~3 examples. Negative Transfer: for S4 tasks, memory causes accuracy drops >10%. Catastrophic Forgetting: SFT on irrelevant data may degrade general reasoning.
- **First 3 experiments**:
  1. Baseline Validation: Implement Adaptive Best-of-N with "No Memory" to confirm adaptive stopping doesn't degrade accuracy vs fixed-N
  2. Similarity Ablation: Run full SpeedupLLM on S1 (identical) vs S3 (same structure) datasets to quantify speedup bounds
  3. Memory Capacity Test: Compare "In-Context" vs "Reflect-Update" on 20 questions to verify hypothesis that textual memory plateaus while parametric memory continues improving

## Open Questions the Paper Calls Out

- **Open Question 1**: How can reasoning speedup be effectively achieved in Long Chain-of-Thought (Long CoT) reasoning models using memory mechanisms? The authors found no memory methods significantly reduce compute budget for Long CoT and leave this to future work.

- **Open Question 2**: How can in-context episodic memory be scaled to overcome fixed context window limitations to maintain continuous reasoning speedup? Experiments showed text-based methods plateau around 3 examples, while parametric memory continues improving.

- **Open Question 3**: How can LLMs mitigate performance degradation (negative transfer) when memory mechanisms are applied to questions with low similarity to past experiences? Finding 4 notes memory can cause performance drops for low-similarity questions, contradicting theoretical assumptions.

## Limitations

- Memory-Scaling Interaction: The exact mechanisms of episodic memory advantage over semantic methods remain unclear, with complex dependencies between memory capacity, context limits, and thresholds
- Generalizability: Results based on math problems may not transfer to domains with different reasoning patterns; similarity taxonomy designed for math may not map cleanly to other domains
- Evaluation Framework: Fixed quality threshold τ=0.9 across all methods may not be optimal; adaptive stopping assumes well-calibrated scorer but no systematic evaluation of scorer reliability provided

## Confidence

- **High Confidence**: The core finding that LLMs can reduce compute costs through repeated exposure to similar tasks with appropriate memory and scaling strategies
- **Medium Confidence**: The specific advantage of episodic memory methods and claim that this advantage persists beyond context window limits
- **Low Confidence**: The claim that SFT with single-example updates provides consistent benefits without overfitting, given limited data per update

## Next Checks

1. **Domain Transfer Experiment**: Apply SpeedupLLM framework to non-math domain (e.g., code generation or multi-hop reasoning) to test whether similarity-based speedup generalizes beyond mathematical problem-solving

2. **Scorer Reliability Analysis**: Systematically vary quality threshold τ and evaluate how scorer miscalibration affects accuracy and compute efficiency; test whether early stopping on incorrect answers occurs more frequently with certain memory mechanisms

3. **Long-term Memory Stability**: Run extended experiments with SFT memory across 100+ questions to measure catastrophic forgetting rates and determine whether reported continued improvement beyond context saturation is sustainable or eventually degrades general reasoning capabilities