---
ver: rpa2
title: 'Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment after
  Instruction Tuning'
arxiv_id: '2502.01116'
source_url: https://arxiv.org/abs/2502.01116
tags:
- llms
- safety
- alignment
- data
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically investigates how fine-tuning aligned\
  \ LLMs on benign datasets can degrade safety alignment, even without harmful content.\
  \ The authors identify three key factors\u2014answer structure, identity calibration,\
  \ and role-play\u2014that influence alignment and show that reformatting answers\
  \ or modifying identity-related content can either enhance or compromise safety."
---

# Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment after Instruction Tuning

## Quick Facts
- arXiv ID: 2502.01116
- Source URL: https://arxiv.org/abs/2502.01116
- Reference count: 40
- Key outcome: Instruction tuning on benign datasets can degrade safety alignment through answer structure, identity calibration, and role-play factors

## Executive Summary
This study investigates how fine-tuning aligned LLMs on benign instruction datasets can inadvertently degrade safety alignment, even without exposure to harmful content. The authors identify three key factors—answer structure, identity calibration, and role-play—that significantly influence safety alignment during fine-tuning. They demonstrate that reformatting answers or modifying identity-related content can either enhance or compromise safety, challenging the assumption that instruction tuning always improves model behavior. The work also evaluates state-of-the-art reward models and finds them unreliable in accurately reflecting human preferences for safety, as they often fail to recognize the benefits of reformatted data.

## Method Summary
The authors conducted systematic experiments fine-tuning aligned LLMs on various instruction datasets while manipulating answer structure, identity-related content, and role-play scenarios. They evaluated safety alignment through both automated metrics and human preference judgments, comparing model outputs before and after fine-tuning. To assess reward model reliability, they tested multiple state-of-the-art reward models on the same datasets, measuring their ability to capture human preferences for safety-relevant outputs. The study also included ablation studies to isolate the impact of each identified factor on safety alignment degradation.

## Key Results
- Instruction tuning on benign datasets can degrade safety alignment without any harmful content exposure
- Answer structure, identity calibration, and role-play are three key factors influencing safety alignment during fine-tuning
- State-of-the-art reward models are unreliable for safety assessment, often failing to recognize safety benefits from answer reformatting
- The study provides datasets and code for replication at https://github.com/GuanlinLee/llm_instruction_tuning

## Why This Works (Mechanism)
The mechanism behind safety degradation during instruction tuning appears to stem from the model's learning of statistical patterns that conflict with safety alignment objectives. When fine-tuning on instruction datasets, LLMs may prioritize response fluency, format compliance, or role adherence over safety considerations. The three identified factors represent different pathways through which this conflict manifests: answer structure changes can bypass safety filters embedded in the original model, identity calibration can alter the model's contextual understanding of harmful scenarios, and role-play can activate latent capabilities that were previously suppressed for safety reasons.

## Foundational Learning
- **Safety alignment**: Why needed - to ensure models don't generate harmful content; Quick check - model refuses harmful prompts consistently
- **Instruction tuning**: Why needed - to improve model task performance and usability; Quick check - model follows instructions accurately
- **Reward modeling**: Why needed - to automate preference learning and evaluation; Quick check - reward scores correlate with human judgments
- **Alignment degradation**: Why needed - to understand when and how safety degrades; Quick check - safety metrics decline after fine-tuning
- **Factor isolation**: Why needed - to identify specific causes of safety issues; Quick check - ablation studies show individual factor impact

## Architecture Onboarding

Component map: LLM base model -> Instruction fine-tuning -> Safety alignment check -> Reward model evaluation

Critical path: Dataset preparation → Fine-tuning → Safety evaluation → Reward model assessment

Design tradeoffs: Balance between instruction following capability and safety preservation; between automated evaluation efficiency and human preference accuracy

Failure signatures: Safety metric degradation despite benign training data; reward model disagreement with human preferences; format compliance overriding safety considerations

First 3 experiments to run:
1. Fine-tune a safety-aligned model on a standard instruction dataset and measure safety degradation
2. Apply answer structure transformations to safe prompts and evaluate reward model responses
3. Test identity calibration effects by modifying persona-related content in safety-critical scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on controlled experiments with specific datasets may not generalize to all fine-tuning scenarios
- Three identified factors examined within limited scope of transformations
- Evaluation of reward models conducted using single benchmark dataset
- Focus primarily on text-based safety concerns, leaving multimodal aspects unexplored

## Confidence

High Confidence: Safety degradation through instruction tuning is empirically validated; reward models fail to recognize safety benefits from answer reformatting

Medium Confidence: Answer structure, identity calibration, and role-play are primary factors influencing safety alignment; requires broader validation

Low Confidence: State-of-the-art reward models are unreliable for safety assessment; based on single benchmark

## Next Checks

1. Replicate safety degradation experiments across multiple diverse instruction datasets to assess robustness of the three identified factors

2. Evaluate reward models on broader set of safety-related prompts including adversarial examples to determine consistency of unreliability

3. Conduct parallel experiments with multimodal models to determine if answer structure, identity calibration, and role-play factors have similar influence on visual safety alignment