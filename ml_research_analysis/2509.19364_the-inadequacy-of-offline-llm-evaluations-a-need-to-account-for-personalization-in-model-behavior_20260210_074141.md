---
ver: rpa2
title: 'The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization
  in Model Behavior'
arxiv_id: '2509.19364'
source_url: https://arxiv.org/abs/2509.19364
tags:
- evaluations
- offline
- questions
- evaluation
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that standard offline LLM evaluations\u2014\
  where models answer questions independently without memory\u2014fail to capture\
  \ how models actually behave in practice due to user personalization. The authors\
  \ compared offline API-based evaluations with field evaluations conducted by 800\
  \ real ChatGPT and Gemini users, finding that field evaluations consistently produce\
  \ more heterogeneous responses across nine recommendation questions and benchmark\
  \ tasks (MMLU and ETHICS)."
---

# The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior

## Quick Facts
- **arXiv ID**: 2509.19364
- **Source URL**: https://arxiv.org/abs/2509.19364
- **Reference count**: 40
- **Primary result**: Offline LLM evaluations systematically underestimate response heterogeneity compared to personalized field evaluations, with dramatic shifts in recommendation distributions (e.g., Tesla from 93% to 35%).

## Executive Summary
This paper demonstrates that standard offline LLM evaluations fail to capture how models actually behave in practice due to user personalization. Comparing offline API-based evaluations with field evaluations from 800 real ChatGPT and Gemini users, the authors found that field evaluations consistently produce more heterogeneous responses across nine recommendation questions and benchmark tasks. For instance, offline evaluations recommended Tesla 93% of the time versus 35% in field settings. The study shows that personalization-induced variability is large enough to reorder model rankings on leaderboards. The authors propose using sock puppet evaluations with simulated user profiles to better capture real-world behavior and call for greater researcher access to personalization mechanisms in commercial LLM platforms.

## Method Summary
The study compared three evaluation methods: offline (stateless API calls at temperature=1), field (800 Prolific users copy-pasting prompts into their logged-in chat sessions), and sock puppet evaluations with simulated user profiles. Sock puppets included: SP-History (prepending 100 random WildChat conversations), SP-RAG (retrieving 100 most relevant histories via embedding), and SP-Profile (concatenating personas with descriptive sentences). The evaluation used 13 prompts (9 recommendations, 2 MMLU college medicine, 2 ETHICS questions) and measured heterogeneity through unique response counts, top-5 coverage, and Shannon entropy. Field data showed substantially higher heterogeneity than offline evaluations, with sock puppets partially capturing but not fully replicating this variability.

## Key Results
- Offline evaluations showed dramatically lower response heterogeneity than field evaluations across all recommendation questions
- Personalization effects were large enough to reorder model rankings on leaderboards in multiple cases
- Sock puppet methods captured some but not all of the heterogeneity observed in field evaluations
- Response distributions differed substantially (e.g., Tesla recommendations: 93% offline vs 35% field)

## Why This Works (Mechanism)
Standard offline evaluations assume stateless model behavior, but real users interact with personalized LLMs that maintain memory, search history, and profile information. These personalization mechanisms cause models to generate different responses for the same prompt based on user context. The study demonstrates that this variability is substantial and systematic, affecting not just recommendation tasks but also benchmark performance.

## Foundational Learning
- **Personalization mechanisms**: Understanding how user data (history, memory, profiles) influences model outputs is crucial because current evaluations ignore these factors entirely.
- **Heterogeneity metrics**: Unique response counts, top-5 coverage, and Shannon entropy quantify response diversity, revealing that personalization dramatically increases variability.
- **Sock puppet methodology**: Simulated user profiles offer a practical workaround for studying personalization effects when field studies aren't feasible, though they remain approximations.

## Architecture Onboarding
**Component map**: Offline API -> Sock Puppet Simulator -> Field User Study -> Benchmark Tasks
**Critical path**: Offline evaluation → heterogeneity measurement → sock puppet simulation → field validation → mechanism analysis
**Design tradeoffs**: Field studies provide ground truth but are expensive and limited in scale; sock puppets are scalable but imperfect; offline evaluations are convenient but misleading.
**Failure signatures**: Offline evaluations systematically underestimate response diversity; sock puppets may overestimate heterogeneity if profiles are poorly calibrated.
**First experiments**: 1) Run offline baseline with temperature=1 for all prompts; 2) Build sock puppet histories using WildChat corpus; 3) Compare heterogeneity metrics across methods.

## Open Questions the Paper Calls Out
### Open Question 1
How can sock puppet evaluation methods be calibrated to accurately match the distribution of responses observed in field evaluations? The authors note that SP-Profile sometimes produces heterogeneity exceeding field evaluations, but provide no framework for calibrating simulated profiles to real user distributions.

### Open Question 2
What specific personalization mechanisms do commercial LLM platforms use, and how do they differentially affect model behavior? The paper explicitly calls for transparency from organizations about these mechanisms, which remain opaque to researchers.

### Open Question 3
Does personalization-induced variability differ systematically across benchmark categories or task types? The study tested only 2 MMLU and 2 ETHICS questions through purposive sampling, which cannot establish whether personalization effects generalize across knowledge domains.

## Limitations
- Field experiment sample size (800 users) and sock puppet simulations cannot fully replicate real-world personalization complexity
- Exact persona templates and sentence pools from cited references are not specified in the paper
- Limited benchmark sampling (2 questions per category) cannot establish generalization across domains

## Confidence
- **High**: Core claim that personalization materially affects response diversity and can reorder model rankings
- **Medium**: Specific magnitude estimates of heterogeneity metrics due to potential dataset and sampling variations

## Next Checks
1. Replicate heterogeneity analysis using a different conversation corpus to verify sock puppet methods generalize beyond WildChat
2. Conduct controlled field study with smaller but more diverse user sample to test personalization effect persistence
3. Test sock puppet simulation sensitivity by varying prepended conversation history lengths (50 vs. 100) to determine minimum effective history length