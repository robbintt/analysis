---
ver: rpa2
title: Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation
arxiv_id: '2512.00706'
source_url: https://arxiv.org/abs/2512.00706
tags:
- hallucination
- data
- training
- preference
- on-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucination in large vision-language models
  (LVLMs), which remains a critical challenge despite their growing capabilities.
  The authors identify key limitations of off-policy learning for hallucination mitigation,
  demonstrating that off-policy training fails to suppress dominant hallucination
  patterns.
---

# Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation

## Quick Facts
- **arXiv ID:** 2512.00706
- **Source URL:** https://arxiv.org/abs/2512.00706
- **Reference count:** 40
- **Primary result:** On-policy preference alignment with hallucination detection reduces hallucination rates by up to 79.5% on Object HalBench

## Executive Summary
This paper addresses the persistent challenge of hallucination in Large Vision-Language Models (LVLMs) by identifying fundamental limitations in off-policy learning approaches for preference alignment. The authors demonstrate that off-policy training fails to suppress dominant hallucination patterns because the optimized policy struggles to assign high probabilities to responses that the reference model deemed unlikely. They propose an effective on-policy data construction pipeline that uses a hallucination classifier to ensure clean training samples, combined with an iterative direct preference optimization (DPO) algorithm featuring dynamic sample reweighting. Experimental results show significant improvements across multiple hallucination benchmarks, with their method enabling LLaVA-1.5-13B to surpass GPT-4V performance on hallucination evaluation metrics.

## Method Summary
The method employs a two-stage iterative alignment framework. First, a hallucination classifier is trained on annotated data using a modified Qwen2-VL-7B model to distinguish between hallucinated and non-hallucinated responses. Second, the LVLM undergoes iterative preference alignment: the first iteration uses off-policy data pairing ground truth answers with hallucinated responses from the base model, while subsequent iterations use on-policy data generated by the model itself. For each prompt, the model generates N responses, which are filtered and ranked by the classifier to create preference pairs where the chosen sample is guaranteed to be hallucination-free. The training employs a weighted DPO loss function based on the Rao-Kupper model, with dynamic sample reweighting to focus on informative examples and stabilize training.

## Key Results
- Reduces hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8%
- Achieves 79.5% reduction in average hallucination rate on Object HalBench
- Enables LLaVA-1.5-13B to surpass GPT-4V performance on hallucination benchmarks
- Demonstrates strong generalization across different model scales (7B to 13B)
- Outperforms eight state-of-the-art baselines on multiple hallucination evaluation metrics

## Why This Works (Mechanism)
The approach succeeds because it directly addresses the theoretical limitation of off-policy learning in preference alignment. When using off-policy data, the optimized policy struggles to assign high probabilities to responses that the reference model deemed unlikely, even if those responses are rated highly by human annotators. By switching to on-policy data collection, where the model generates responses that it can actually assign meaningful probabilities to, the alignment process becomes more effective at suppressing hallucination patterns. The iterative nature allows the model to progressively refine its understanding of hallucination-free responses, while the hallucination classifier ensures that the "chosen" samples in preference pairs are genuinely clean, preventing the reinforcement of hallucination patterns during training.

## Foundational Learning

- **Concept: DPO as Probability Reweighting**
  - Why needed here: The paper's core argument for on-policy data rests on the theoretical property of DPO (and related RLHF methods) optimizing a policy by reweighting the probabilities of a reference model. Understanding this is crucial to grasp why off-policy data, drawn from a different distribution, might fail to influence the optimized policy's output distribution effectively.
  - Quick check question: Can you explain why, if the reference model π_ref assigns near-zero probability to a response y, the optimized policy π_θ also struggles to assign a high probability to y, even if y is a highly-rated chosen sample?

- **Concept: On-Policy vs. Off-Policy Learning**
  - Why needed here: The distinction is central to the paper's contribution. The authors argue that for hallucination mitigation, the off-policy paradigm (using data from other models) is fundamentally flawed. Understanding this distinction is necessary to appreciate the proposed pipeline.
  - Quick check question: In the context of this paper, what is the key difference in the source of training data between on-policy and off-policy learning for preference alignment?

- **Concept: Iterative Preference Alignment**
  - Why needed here: The proposed method uses an iterative loop where the model is updated, new data is collected from it, and the process repeats. This active learning style is key to the method's performance gains and differs from one-shot offline DPO.
  - Quick check question: Why does the method collect new preference data after each model update instead of using a single static dataset for all training steps?

## Architecture Onboarding

- **Component map:** Hallucination Classifier -> Policy Model -> Iterative Alignment Pipeline
- **Critical path:** The most critical path is the **data construction pipeline**. The model's performance hinges entirely on the quality of the preference pairs. The hallucination classifier must correctly identify hallucinations, and the filtering logic must correctly assemble pairs where the chosen sample is clean and the rejected sample contains hallucinations. A failure here propagates into the model update, reinforcing errors.
- **Design tradeoffs:**
    - **Binary vs. Fine-grained Classification:** The authors chose a simpler binary classifier over a fine-grained detector, trading potentially more nuanced labels for simplicity and a focus on guaranteed clean chosen samples. The trade-off is a potential loss of granular ranking information.
    - **Sample Reweighting Complexity:** Introducing dynamic weights based on the Rao-Kupper model adds complexity over standard DPO. The trade-off is improved robustness to noisy samples and focus on informative examples, but it introduces a new hyperparameter (ν) to tune.
- **Failure signatures:**
    - **Classifier Drift:** If the classifier is not representative of the types of hallucinations the policy model produces, the pipeline will generate low-quality training data, leading to no improvement or model degradation.
    - **Reward Hacking:** The model might learn to generate responses that the classifier deems "non-hallucinated" but are factually empty or overly safe, without truly grounding in the image.
    - **Training Instability:** The iterative process, especially with dynamic reweighting, could become unstable if learning rates are not carefully managed between iterations.
- **First 3 experiments:**
    1. **Classifier Validation:** Before full training, rigorously evaluate the trained hallucination classifier on a held-out set of model-generated responses. Measure its accuracy, precision, and recall against ground-truth annotations to establish an upper bound on data quality.
    2. **On-Policy vs. Off-Policy Ablation:** Train two models: one with the proposed on-policy data construction pipeline and one with an off-policy dataset (e.g., using GPT-4V corrected responses). Compare their hallucination rates on a benchmark like MMHalBench to validate the core theoretical claim.
    3. **Reweighting Ablation:** Train two models using the same on-policy pipeline: one with the dynamic sample reweighting and one with standard, uniform weights. Compare their final performance and training dynamics to validate the benefit of the reweighting scheme.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the theoretical observation that off-policy training fails to suppress dominant hallucination patterns hold for deep non-linear transformer architectures, given that Remark 4.1 is proven using a "linearly parametrized softmax policy" assumption?
- **Open Question 2:** How robust is the iterative alignment framework to the error rates of the hallucination classifier, specifically regarding the risk of reinforcing false negatives (hallucinated text labeled as "clean")?
- **Open Question 3:** Is the superiority of the response-level binary classifier over fine-grained hallucination detectors universal, or does it trade off potential gains in alignment precision for annotation efficiency?

## Limitations
- The method's effectiveness depends heavily on the generalization capability of the hallucination classifier, which is trained on limited data (8.4K samples) from a specific domain
- The approach focuses primarily on English-language benchmarks, leaving multilingual hallucination performance unaddressed
- The dynamic sample reweighting introduces an additional hyperparameter (ν=3.0) whose optimal value may vary across different LVLM architectures or datasets

## Confidence
- **High Confidence:** The theoretical foundation distinguishing on-policy vs off-policy learning (DPO probability reweighting property), the experimental demonstration of significant hallucination reduction on established benchmarks (MMHalBench, Object HalBench), and the empirical observation that the method generalizes across model scales (7B to 13B)
- **Medium Confidence:** The claim that the approach surpasses GPT-4V performance on hallucination benchmarks, as this comparison involves different model scales and potentially different evaluation protocols
- **Low Confidence:** The paper's claim about preventing "reward hacking" through sample reweighting, as the evaluation doesn't explicitly test for this failure mode

## Next Checks
1. **Cross-Dataset Classifier Evaluation:** Evaluate the hallucination classifier trained on POVID/COCO data on a held-out test set from a different distribution (e.g., web-crawled images or medical images) to assess generalization capability and establish true upper bounds on data quality.
2. **N-Sensitivity Analysis:** Systematically vary N (number of responses generated per prompt) in the on-policy pipeline (e.g., N=3, 5, 10) and measure the trade-off between computational cost and hallucination reduction performance to determine if N=5 is truly optimal.
3. **Reward Hacking Detection Test:** Design and conduct explicit experiments to detect reward hacking, such as testing whether the optimized model generates shorter, overly-safe responses or fails to ground in image details while still avoiding classifier-detected hallucinations.