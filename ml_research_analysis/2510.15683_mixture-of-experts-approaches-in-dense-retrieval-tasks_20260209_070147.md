---
ver: rpa2
title: Mixture of Experts Approaches in Dense Retrieval Tasks
arxiv_id: '2510.15683'
source_url: https://arxiv.org/abs/2510.15683
tags:
- experts
- sb-moe
- retrieval
- number
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SB-MoE, a mixture-of-experts approach that
  adds a single MoE block after the final Transformer layer of dense retrieval models
  to improve both retrieval effectiveness and generalizability. The authors compare
  SB-MoE with standard fine-tuning across seven IR benchmarks and thirteen BEIR datasets,
  using four different DRMs (TinyBERT, BERT-Small, BERT-Base, Contriever).
---

# Mixture of Experts Approaches in Dense Retrieval Tasks

## Quick Facts
- **arXiv ID:** 2510.15683
- **Source URL:** https://arxiv.org/abs/2510.15683
- **Reference count:** 40
- **Primary result:** SB-MoE consistently improves retrieval performance for lightweight DRMs, with gains of 0.10%–5.76% in nDCG@10 and 0.10%–5.26% in Recall@100 across seven IR benchmarks and thirteen BEIR datasets.

## Executive Summary
This paper introduces SB-MoE, a mixture-of-experts approach that adds a single MoE block after the final Transformer layer of dense retrieval models to improve both retrieval effectiveness and generalizability. The authors compare SB-MoE with standard fine-tuning across seven IR benchmarks and thirteen BEIR datasets, using four different DRMs (TinyBERT, BERT-Small, BERT-Base, Contriever). The results show that SB-MoE consistently improves retrieval performance, particularly for lightweight DRMs like TinyBERT and BERT-Small, with gains ranging from 0.10% to 5.76% in nDCG@10 and from 0.10% to 5.26% in Recall@100. The approach also generalizes well in zero-shot settings, outperforming baselines on 9-10 out of 13 BEIR datasets. Experiments with varying numbers of experts (3-12) demonstrate that SB-MoE's performance is sensitive to this hyperparameter, but only a subset of experts is typically activated during inference. The gating function is shown to be crucial for performance gains, as random gating performs comparably to standard fine-tuning.

## Method Summary
SB-MoE adds a single mixture-of-experts block after the final Transformer layer of dense retrieval models. Each expert consists of a down-projection FFN (halving dimension), up-projection FFN, and skip connection. The gating function is a two-layer network that produces weights over experts, which are then aggregated using either TOP-1 (select highest-scoring expert) or ALL (weighted sum) pooling. The model is trained with contrastive loss using in-batch negatives, with separate learning rates for the base DRM (10⁻⁶) and experts (10⁻⁴). The approach is evaluated on seven IR benchmarks for in-domain performance and thirteen BEIR datasets for zero-shot generalization, comparing four DRMs (TinyBERT, BERT-Small, BERT-Base, Contriever) against standard fine-tuning baselines.

## Key Results
- SB-MoE achieves consistent performance improvements for lightweight DRMs (TinyBERT, BERT-Small), with gains of 0.10%–5.76% in nDCG@10 and 0.10%–5.26% in Recall@100.
- The approach generalizes well to zero-shot settings, outperforming baselines on 9-10 out of 13 BEIR datasets.
- Performance is sensitive to the number of experts (3-12 tested), but only a subset of experts is typically activated during inference.
- The gating function is crucial for performance gains, as random gating performs comparably to standard fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1: Learned Expert Routing for Specialized Representation Refinement
- **Claim:** The gating function learns to route different inputs to specialized experts, enabling the capture of diverse textual characteristics that a single representation cannot express.
- **Mechanism:** The gating network produces a weight distribution over experts based on the input embedding. During training, noisy Top-1 gating forces exploration of all experts. During inference, the gating function selects the most relevant expert (TOP-1) or aggregates all experts with learned weights (ALL). This creates input-specific representation refinements rather than a one-size-fits-all transformation.
- **Core assumption:** Different queries and documents possess heterogeneous characteristics (domain, complexity, length) that benefit from specialized processing pathways.
- **Evidence anchors:**
  - [abstract]: "The gating function is shown to be crucial for performance gains, as random gating performs comparably to standard fine-tuning."
  - [section V, RQ2]: "RANDOM-GATE performs comparably to the FINE-TUNED baseline... In contrast, SB-MOE consistently outperforms RANDOM-GATE, highlighting the critical role of the gating function in dynamically selecting relevant experts."
  - [corpus]: MoTE and AnyExperts papers confirm learned routing importance for multi-task/multimodal specialization, though not specifically for retrieval.

### Mechanism 2: Lightweight Model Capacity Compensation
- **Claim:** Adding MoE to lightweight models (TinyBERT, BERT-Small) compensates for their limited representational capacity, yielding proportionally larger gains than for larger models.
- **Mechanism:** Small models have fewer parameters to encode complex query-document relationships. The MoE block adds modular capacity that can be selectively activated based on input characteristics, effectively expanding the model's representation space without replacing the base encoder.
- **Core assumption:** Lightweight models are fundamentally under-capacity for diverse retrieval tasks, not merely under-trained or under-optimized.
- **Evidence anchors:**
  - [abstract]: "SB-MoE is particularly effective for DRMs with lightweight base models, such as TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning across benchmarks."
  - [section V, RQ1]: "For instance, on TinyBERT and BERT-Small, our model leads to consistent performance gains... with a marked increase for BERT-Small in NQ, where SB-MOE TOP-1 surpasses the FINE-TUNED version by 5.58% in nDCG@10."
  - [corpus]: ToMoE paper demonstrates converting dense LLMs to MoE through pruning, supporting capacity expansion theory, but not tested on retrieval.

### Mechanism 3: Output-Level Representation Diversification with Sparse Activation
- **Claim:** Placing a single MoE block after the final Transformer layer efficiently diversifies output representations while maintaining computational sparsity during inference.
- **Mechanism:** Unlike per-layer MoE integration, SB-MoE processes the final embedding once through multiple FFN experts. The gating function ensures only a subset of experts are activated per input (Table III shows 2-7 activated experts out of 3-12 employed), reducing inference overhead while maintaining representation diversity.
- **Core assumption:** Representation diversification at the output layer is sufficient for retrieval gains; intermediate-layer MoE adds unnecessary parameter overhead.
- **Evidence anchors:**
  - [abstract]: "This paper introduces SB-MoE, a mixture-of-experts approach that adds a single MoE block after the final Transformer layer... reducing the number of additional parameters."
  - [section I]: "prior research in IR incorporated the Mixture-of-Experts (MoE) framework within each Transformer layer of a DRM, which, though effective, substantially increased the number of additional parameters."
  - [corpus]: Joint MoE Scaling Laws paper explores memory-efficient MoE scaling but does not specifically address output-level vs. per-layer placement.

## Foundational Learning

- **Concept: Bi-encoder Dense Retrieval Architecture**
  - Why needed here: SB-MoE operates on query and document embeddings produced independently by a shared encoder, enabling precomputed document indices.
  - Quick check question: Can you explain why bi-encoders permit precomputed document embeddings while cross-encoders require joint query-document processing at inference time?

- **Concept: Mixture-of-Experts Gating Mechanisms**
  - Why needed here: Understanding Top-1 gating (select single expert) vs. softmax-weighted aggregation (combine all experts) is essential for selecting the inference variant.
  - Quick check question: What is the computational difference between SB-MoE TOP-1 and SB-MoE ALL during inference?

- **Concept: Contrastive Learning for Retrieval**
  - Why needed here: SB-MoE is trained with contrastive loss (in-batch negatives) to align query and document representations in shared embedding space.
  - Quick check question: How does contrastive loss with in-batch negatives differ from triplet loss with explicit hard negative mining?

## Architecture Onboarding

- **Component map:**
  Base DRM (TinyBERT/BERT-Small/BERT-Base/Contriever) -> Output embedding [hidden_dim] -> MoE Block (Experts + Gating) -> Final embedding -> Similarity function (Dot product)

- **Critical path:**
  1. Query/Document -> Base DRM -> Output embedding
  2. Output embedding -> Gating function -> n-dimensional weight vector
  3. Output embedding -> All n experts -> n refined embeddings
  4. Refined embeddings + weights -> Pooling strategy -> Final embedding
  5. Query final embedding · Document final embedding -> Relevance score

- **Design tradeoffs:**
  - TOP-1 vs. ALL pooling: TOP-1 is computationally cheaper (single expert forward pass) but may lose complementary information from other experts; ALL leverages all experts but requires n forward passes through expert FFNs.
  - Number of experts: 3-12 tested; Table III shows only 2-7 experts activated during inference regardless of total employed. More experts increase training complexity without guaranteed gains.
  - Base model selection: Lightweight models (TinyBERT, BERT-Small) show consistent improvements; larger models (BERT-Base, Contriever) require more training data and show marginal gains.

- **Failure signatures:**
  - Performance equivalent to FINE-TUNED baseline -> Gating function not learning; verify gating learning rate (10⁻⁴ specified) is applied correctly.
  - Only 1 expert activated across all inputs -> Gating collapse; increase noisy gating noise parameter during training.
  - Degraded zero-shot performance -> Overfitting to source domain; reduce training epochs or increase dropout.
  - Larger models show no improvement -> Expected behavior per Section V; MoE adds redundancy to already-capacious models.

- **First 3 experiments:**
  1. Reproduce TinyBERT + SB-MoE on MSMARCO with 6 experts, TOP-1 pooling, 30 epochs. Expect ~0.4-0.8% nDCG@10 improvement over baseline.
  2. Ablation: Train TinyBERT + SB-MoE with RANDOM-GATE (random expert weights). Expect performance ≈ FINE-TUNED baseline, confirming gating function necessity.
  3. Expert count sensitivity: Sweep {3, 6, 9, 12} experts on NQ dataset with TinyBERT. Expect non-monotonic performance; verify 3-expert configuration activates 100% of experts per Table III.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SB-MoE architecture be adapted to yield significant performance gains for high-parameter Dense Retrieval Models (DRMs) without requiring a corresponding increase in training data?
- Basis in paper: [inferred] The Conclusion notes that while SB-MoE benefits lightweight models, only "marginal gains were noted for larger DRMs," and the Abstract states these larger models require "a larger number of training samples to achieve improved retrieval performance."
- Why unresolved: The current approach appears to struggle with parameter redundancy or optimization difficulties in larger base models, limiting its utility for state-of-the-art architectures.
- What evidence would resolve it: A modification of the gating or expert mechanism that demonstrates statistically significant improvements on BERT-Base or Contriever using standard (not enlarged) datasets.

### Open Question 2
- Question: What specific semantic or syntactic features do the individual experts learn to specialize in?
- Basis in paper: [inferred] Section VI mentions that documents processed by the same expert "tend to form clusters," suggesting they capture "useful textual characteristics," but the paper stops short of defining what those characteristics actually are (e.g., domain, complexity, length).
- Why unresolved: The analysis is currently limited to geometric clustering (t-SNE) and performance metrics without qualitative interpretability of the expert functions.
- What evidence would resolve it: A qualitative study correlating specific experts with explicit linguistic attributes or query/document types.

### Open Question 3
- Question: Is there an adaptive mechanism to determine the optimal number of experts to prevent the sensitivity issues observed during hyperparameter tuning?
- Basis in paper: [inferred] The Hyperparameter Analysis (Section VI) reveals that "no specific number of experts consistently yields optimal performance" and "increasing the number of experts does not guarantee a performance improvement," necessitating manual tuning per dataset.
- Why unresolved: The static definition of the expert count creates a stability bottleneck, where a configuration optimal for one metric (e.g., Recall) may be suboptimal for another (e.g., nDCG) on the same dataset.
- What evidence would resolve it: The development of a dynamic routing algorithm that automatically scales or prunes the number of active experts based on input complexity.

## Limitations

- SB-MoE shows minimal performance gains for larger DRMs (BERT-Base, Contriever) compared to lightweight models, requiring more training data for any improvement.
- The gating function's learned routing decisions are not analyzed for interpretability, leaving unclear what specific textual characteristics different experts capture.
- Performance is highly sensitive to the number of experts (3-12 tested), with no consistent optimal configuration across datasets and metrics.

## Confidence

- **High Confidence:** Performance improvements for lightweight models (TinyBERT, BERT-Small) on standard retrieval benchmarks; zero-shot generalization across BEIR datasets.
- **Medium Confidence:** The claim that MoE adds "minimal" parameters compared to per-layer integration; the generalization mechanism to zero-shot tasks.
- **Low Confidence:** The explanation that gating learns to route "heterogeneous textual characteristics" to specialized experts; the assertion that SB-MoE's single-block placement is optimal versus per-layer MoE.

## Next Checks

1. Analyze gating activation patterns across domains to verify if learned routing captures meaningful textual heterogeneity versus random patterns.
2. Perform ablation studies comparing SB-MoE with and without noisy Top-1 gating during training to quantify exploration's impact on performance.
3. Test whether the 3-expert configuration consistently activates all experts across different datasets to rule out expert underutilization as a performance driver.