---
ver: rpa2
title: 'In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers
  on Bandit Tasks'
arxiv_id: '2510.00347'
source_url: https://arxiv.org/abs/2510.00347
tags:
- test
- pretraining
- curiosity
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving generalization in
  Decision-Pretrained Transformers (DPTs) when applied to out-of-distribution (OOD)
  reinforcement learning environments. DPTs, trained on offline trajectories via supervised
  learning, often fail to generalize beyond their pretraining data distribution, especially
  when the data is biased or lacks exploration.
---

# In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks

## Quick Facts
- arXiv ID: 2510.00347
- Source URL: https://arxiv.org/abs/2510.00347
- Reference count: 20
- Primary result: Prediction-Powered Transformer with curiosity bonus improves OOD generalization in bandits vs. standard DPT

## Executive Summary
This paper addresses the challenge of improving generalization in Decision-Pretrained Transformers (DPTs) when applied to out-of-distribution (OOD) reinforcement learning environments. DPTs, trained on offline trajectories via supervised learning, often fail to generalize beyond their pretraining data distribution, especially when the data is biased or lacks exploration. To mitigate this, the authors propose in-context curiosity—a lightweight, exploration-inspired regularizer for offline pretraining—and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remains fundamental, the results suggest that curiosity-driven pretraining offers a promising direction for enhancing OOD generalization in in-context RL agents.

## Method Summary
The Prediction-Powered Transformer (PPT) augments standard Decision-Pretrained Transformers with an auxiliary reward predictor that generates curiosity signals during offline pretraining. The predictor estimates expected rewards per action, and the squared difference between predicted and true rewards defines a curiosity vector. This vector is used as an intrinsic exploration bonus in the policy training objective, encouraging the policy to explore actions with high prediction error (interpreted as underexplored regions). The framework trains two transformers in parallel: a policy model and a predictor model. The policy conditions on both historical context and predicted rewards, enabling curiosity-driven exploration during pretraining. Experiments use Gaussian multi-armed bandits with varying reward variances to test OOD generalization, comparing PPT against DPT baselines and UCB.

## Key Results
- PPT reduces performance degradation in high-variance test environments compared to DPT
- Curiosity coefficient λ ∈ [100, 500] shows stable performance; excessive λ destabilizes training
- Proxy reward estimators (per-trajectory means) achieve comparable performance to ground-truth rewards
- PPT maintains near-optimal performance on in-distribution test environments while improving OOD robustness

## Why This Works (Mechanism)

### Mechanism 1
Prediction error signals provide exploitable uncertainty estimates for exploration in offline pretraining. The auxiliary predictor learns to estimate expected rewards per action. The squared difference between predicted and true rewards defines a "curiosity vector" E_j = (q_φ(·|D_j) − c*) ⊙² ∈ R^|A|. High prediction error indicates underexplored regions in the pretraining distribution, biasing policy selection toward those actions. Core assumption: Prediction error correlates with epistemic uncertainty relevant to decision-making (not just noise). Evidence: Abstract states prediction error is used as intrinsic curiosity signal; section 3, Eq. 8 defines curiosity vector as element-wise squared error; related work on in-context learning generalization confirms OOD generalization depends on pretraining distribution coverage. Break condition: If prediction error reflects aleatoric noise rather than epistemic uncertainty, curiosity signal becomes misleading. High test variance (σ²_test ≥ 0.5) diminishes PPT advantage.

### Mechanism 2
Augmenting policy input with predicted reward estimates enables conditioning on uncertainty-aware context. At each round j, predictor outputs reward estimate c_j, appended to history as additional input to policy. Policy π_θ conditions on (D_j, c_{1:j}) rather than just D_j, receiving explicit uncertainty signals alongside observed rewards. Core assumption: The predictor's estimates carry meaningful information not already captured in trajectory history. Evidence: Section 3 states modification allows policy to condition on predicted outcomes; Algorithm 1 shows predictor output c_j appended to context before policy forward pass. Break condition: If predictor and policy share representations and train jointly without stabilization, gradients may conflict. Paper notes predictor training is separable.

### Mechanism 3
Curiosity-weighted NLL loss counteracts expert bias in pretraining data by upweighting underexplored actions. Training objective combines NLL loss with curiosity bonus: L_θ = E[-log π_θ(a*|D_j, c_{1:j}) - λ·⟨E_j, π_θ(·|D_j, c_{1:j})⟩]. The inner product between curiosity vector and policy distribution increases probability mass on high-curiosity actions during gradient updates. Core assumption: λ can be tuned to balance exploration signal against exploitation learning. Evidence: Section 3, Eq. 7 shows full loss formulation; section 4 demonstrates moderate λ improves robustness in high-variance test environments while sacrificing little in-distribution performance. Break condition: If pretraining data has zero coverage of certain actions, no curiosity signal can recover them.

## Foundational Learning

- **Multi-armed bandits (MAB)**: The entire experimental framework; understanding arm selection, regret, suboptimality metrics is prerequisite. Quick check: Can you explain why cumulative regret differs from instantaneous suboptimality in evaluating exploration?
- **In-context reinforcement learning**: DPT paradigm assumes models learn decision policies from trajectory sequences without gradient updates at test time. Quick check: How does in-context RL differ from fine-tuning on downstream tasks?
- **Exploration-exploitation tradeoff**: Core problem PPT addresses—expert-biased data causes premature exploitation; curiosity signals encourage exploration. Quick check: Why does UCB (Upper Confidence Bound) serve as a baseline for model-free exploration?

## Architecture Onboarding

- **Component map**: Bandit environment → Data collection with biased policy → Store (D, a*, c*) tuples → Predictor q_φ trained via MSE → Policy π_θ trained via NLL + curiosity bonus → Evaluation on test environments
- **Critical path**: 1) Collect pretraining dataset with optimal actions a* and true reward vectors c* (or proxy estimates per Eq. 15) 2) Train predictor q_φ to convergence first (separable training reduces to ~1× DPT complexity if reused) 3) Train policy π_θ with joint NLL + curiosity objective, tuning λ ∈ [100, 500] for stable performance 4) At deployment: predictor forward pass → append to context → policy forward pass → sample action
- **Design tradeoffs**: λ tuning: Higher λ = more exploration but risk of training instability; λ = 0 reduces to DPT with extra input dimension. Predictor training: Joint training costs 2× DPT; pretrained predictor reduces to ~1× but requires matching pretraining distribution. Ground-truth vs. proxy context: Proxy (per-trajectory mean estimator, Eq. 15) shows comparable performance in preliminary tests (A.3.2), reduces privileged information requirement
- **Failure signatures**: Performance gap closes as test variance increases (Figure 4): indicates curiosity signal insufficient for highly variable environments. Training instability with λ > 500: policy under-exploits high-reward arms, fails to converge. Predictor loss diverges: check data coverage, ensure c* computation is correct
- **First 3 experiments**: 1) Reproduce Figure 2 (average regret vs. σ²_test) on ideal and tricky datasets with λ ∈ {0, 100, 200, 500} to validate curiosity coefficient sensitivity 2) Ablate predictor training: compare joint training vs. pretrained-frozen predictor on same data to measure training cost vs. performance tradeoff 3) Test proxy context (Eq. 15) vs. ground-truth c* on held-out environments to assess practical deployability without privileged information

## Open Questions the Paper Calls Out

- Can the PPT framework be extended to sequential decision-making settings with state transitions while maintaining its exploration benefits? The current formulation relies on reward prediction uncertainty; state-based RL requires capturing dynamics uncertainty as well, which may require architectural changes to the curiosity mechanism.
- What is a principled strategy for selecting or adapting the curiosity coefficient λ during training? Currently λ is manually tuned within [100, 500]; no theoretical guidance exists for optimal selection, and excessive values destabilize training.
- Can proxy reward estimators fully replace ground-truth rewards without performance degradation? Preliminary results show comparable performance using per-trajectory mean estimators, but scope was limited to "ideal" and "tricky" datasets; broader pretraining distributions and alternative proxy formulations remain untested.
- How can posterior uncertainty modeling (Bayesian sampling) be integrated into the PPT framework to more fully address distribution shift? PPT reduces but does not eliminate OOD degradation; curiosity-driven exploration cannot fully replace principled uncertainty quantification.

## Limitations
- Restricted to synthetic bandit tasks without state or temporal dependencies
- No ablation of predictor architecture or comparison to other uncertainty estimators
- λ hyperparameter sensitivity suggests brittleness in broader domains
- Predictor training still requires access to optimal actions or true reward means

## Confidence
Medium confidence. The core mechanism—using prediction error as a curiosity signal to improve OOD generalization—is well-grounded in experimental results, but theoretical analysis remains limited to proof-of-concept settings. The Gaussian bandit framework may not fully capture real-world RL complexity.

## Next Checks
1. Test on contextual bandits or small MDPs to verify curiosity generalizes beyond stateless bandits
2. Compare uncertainty signals by replacing prediction error with alternative exploration bonuses (UCB-style confidence bounds or ensemble disagreement)
3. Assess sample efficiency by measuring how pretraining dataset size affects robustness gap between DPT and PPT, particularly in low-data regimes