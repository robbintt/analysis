---
ver: rpa2
title: 'Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought
  or Fine-Tuning'
arxiv_id: '2505.14174'
source_url: https://arxiv.org/abs/2505.14174
tags:
- schema
- table
- n-rep
- integer
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces N-rep consistency, a cost-efficient approach
  for text-to-SQL that achieves competitive performance on the BIRD benchmark without
  chain-of-thought reasoning or fine-tuning. The method generates multiple schema
  representations and uses them to create diverse candidate SQL queries, achieving
  69.25% execution accuracy at only $0.039 per query - nearly 10 times cheaper than
  comparable methods.
---

# Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning

## Quick Facts
- arXiv ID: 2505.14174
- Source URL: https://arxiv.org/abs/2505.14174
- Reference count: 11
- N-rep achieves 69.25% execution accuracy at $0.039 per query on BIRD benchmark

## Executive Summary
This paper introduces N-rep consistency, a cost-efficient approach for text-to-SQL that achieves competitive performance on the BIRD benchmark without chain-of-thought reasoning or fine-tuning. The method generates multiple schema representations and uses them to create diverse candidate SQL queries, achieving 69.25% execution accuracy at only $0.039 per query - nearly 10 times cheaper than comparable methods. N-rep leverages multiple text representations of database schemas to create candidate diversity, eliminating the need for expensive reasoning steps or model fine-tuning. The approach uses a confidence-aware two-stage selection strategy that reduces LLM calls by 60% compared to LLM-based voting while maintaining high accuracy.

## Method Summary
N-rep is a three-stage pipeline that generates diverse SQL candidates through schema representation diversity rather than chain-of-thought reasoning. First, a schema linker converts database schemas into multiple text representations (MAC-Schema, M-Schema, DDL) and applies three filtering levels to each (full schema, table-only, full filtering). Second, a candidate generator produces one SQL candidate per representation-filtering pair using few-shot prompting with Gemini 1.5 Flash. Third, a confidence-aware selector uses regular voting for high-confidence cases and LLM-based pairwise comparison only when vote distributions indicate uncertainty. The method achieves diversity through input sensitivity rather than temperature sampling or reasoning steps.

## Key Results
- N-rep achieves 69.25% execution accuracy on BIRD dev set at $0.039 per query
- Outperforms DoLa (64.8% EX, $0.04/query) and BIRD-GPS (66.2% EX, $0.17/query)
- Reduces LLM calls by 60% compared to LLM-based voting while improving accuracy
- Shows particular advantage for smaller models (8B parameters), where N-rep improves Qwen3-8B from 33.6% to 55.1% EX

## Why This Works (Mechanism)

### Mechanism 1: Schema Representation Diversity Generates Meaningful Candidate Variation
- **Claim:** Varying schema text representations creates diverse SQL candidates without CoT reasoning or temperature sampling.
- **Mechanism:** LLMs exhibit sensitivity to input format—different schema representations produce meaningfully different predictions. By generating candidates across representations, N-rep creates orthogonal error distributions, improving the probability that at least one candidate is correct.
- **Core assumption:** Schema representation sensitivity indicates models rely more on surface-level pattern matching than deep schema reasoning.
- **Evidence anchors:** Section 3 shows LLMs remain sensitive to schema representation; Figure 4 demonstrates N-rep's upper bound improves more with additional candidates than CoT self-consistency.

### Mechanism 2: Confidence-Aware Selection Reduces Cost Without Accuracy Loss
- **Claim:** Using vote count as a confidence signal enables selective LLM-based comparison, reducing calls by 60% while improving accuracy.
- **Mechanism:** Regular voting produces a vote distribution; high consensus correlates with higher correctness probability. LLM-based pairwise comparison is invoked only in low-confidence cases (ties, near-ties, or high fragmentation).
- **Core assumption:** Vote count is a reliable proxy for candidate correctness likelihood.
- **Evidence anchors:** Table 2 shows confidence-aware voting achieves 68.8% EX with 2,436 LLM calls vs. LLM-based voting at 67.9% EX with 6,178 calls.

### Mechanism 3: Multi-Level Filtering Expands Candidate Space Efficiently
- **Claim:** Three filtering levels per representation increase candidate diversity without multiplying schema linking calls.
- **Mechanism:** A single schema linking prediction generates three filtered views: unfiltered (full schema), table-only (selected tables with all columns), and full filtering (selected tables and columns only). Each view produces different SQL generation behavior.
- **Core assumption:** Schema linking errors are partial—missing some columns is less harmful than missing entire tables.
- **Evidence anchors:** Section 4.1 describes the three filtering variants; Table 4 shows the final configuration uses mixed filtering levels across representations.

## Foundational Learning

- **Concept:** Schema Linking
  - **Why needed here:** N-rep's first stage; understanding how LLMs map natural language questions to database tables/columns is essential for debugging representation choices.
  - **Quick check question:** Given a user question "Show average salary by department", can you identify which tables and columns must be selected before SQL generation?

- **Concept:** Self-Consistency Voting
  - **Why needed here:** Core selection mechanism; understanding how vote aggregation works and why diversity improves outcomes is critical for tuning candidate counts.
  - **Quick check question:** If 5 candidates produce vote counts [2, 2, 1, 0, 0], what does this indicate about confidence vs. a [4, 1, 0, 0, 0] distribution?

- **Concept:** Execution Accuracy (EX) Metric
  - **Why needed here:** Paper's primary evaluation metric; EX measures whether predicted SQL returns identical results to ground truth, not syntactic equivalence.
  - **Quick check question:** Why might two semantically equivalent SQL queries produce different EX scores on the same benchmark?

## Architecture Onboarding

- **Component map:**
  Schema Linker -> Candidate Generator -> Candidate Selector
  1. Schema Linker: Converts database to N representations (DDL, MAC-Schema, M-Schema); runs schema linking on each; outputs three filtering levels per representation.
  2. Candidate Generator: For each (representation, filtering level) pair, generates one SQL candidate using few-shot prompting with the filtered schema.
  3. Candidate Selector: First-stage regular voting; if confidence below threshold, triggers second-stage LLM pairwise comparison.

- **Critical path:**
  1. Database → N schema representations (3 LLM calls in final config)
  2. Each representation → 3 filtering variants (no additional LLM calls, derived from linking output)
  3. Each variant → 1 SQL candidate (5 LLM calls in final config)
  4. Candidates → vote aggregation → conditional LLM selection (0-6 calls depending on confidence)

- **Design tradeoffs:**
  - More representations increases diversity but hits diminishing returns; manually selected representations limit scalability.
  - Higher candidate count improves upper bound but increases cost linearly; optimal N≈5 in experiments.
  - Aggressive filtering reduces token usage but risks over-pruning; table-only filtering is a middle ground.
  - Assumption: Hand-tuned confidence thresholds don't generalize across candidate counts.

- **Failure signatures:**
  - Low vote confidence on easy queries suggests representation diversity insufficient or schema linking failing systematically.
  - EX plateaus despite adding candidates suggests candidates are correlated (representation diversity exhausted).
  - Large EX gap between upper bound and actual selection indicates selector needs tuning.

- **First 3 experiments:**
  1. Baseline replication: Run N-rep on BIRD dev subset (10%) with 5 candidates; verify EX ≈ 67-69% and cost ≈ $0.04/query.
  2. Ablation on representations: Test each representation format individually to measure standalone EX and identify complementary pairs.
  3. Confidence threshold sweep: Vary the confidence threshold for LLM-based selection to find the Pareto frontier of EX vs. LLM calls.

## Open Questions the Paper Calls Out

- **Can LLMs be used to automatically generate schema representations to increase candidate diversity?**
  - Basis: Authors state in Limitations that future work could "examine a broader range of schema representations, and explore generating alternate forms with LLMs."
  - Why unresolved: Current study relies on fixed manually selected representations, limiting unique candidates.

- **Can the confidence-aware voting policy be optimized to scale to an arbitrary number of candidates?**
  - Basis: Paper notes that the current voting policy is "hand-tailored for each specific number of candidates" and requires optimization.
  - Why unresolved: Current implementation requires manual adjustment, preventing easy scaling to larger candidate pools.

- **Does the performance boost from N-rep hold for models smaller than 8 billion parameters?**
  - Basis: Discussion highlights N-rep reduces reliance on model scale, showing strong gains for Qwen3-8B, but doesn't test smaller architectures.
  - Why unresolved: Unclear if significant relative gains observed in 8B models persist for sub-8B models.

## Limitations

- Manual representation selection: The three schema representations were hand-picked based on empirical performance rather than systematically derived.
- Confidence threshold instability: The voting policy was tuned for 5 candidates specifically, with no validation across different candidate counts.
- Limited theoretical foundation: Minimal related work on representation diversity suggests this is an underexplored area where theoretical foundations remain weak.

## Confidence

**High Confidence:** Cost-efficiency claims (69.25% EX at $0.039/query) are well-supported by direct experimental comparisons with baseline methods. Schema representation sensitivity creating candidate diversity is empirically validated.

**Medium Confidence:** Claims about outperforming CoT-based approaches rely on comparing against specific baselines without exploring the full space of CoT variants or different model sizes.

**Low Confidence:** The foundational claim that "LLMs rely more on surface understanding than deep reasoning about database structures" extrapolates from schema format sensitivity to broader claims about model reasoning capabilities without theoretical grounding.

## Next Checks

1. **Representation Diversity Saturation Test:** Systematically vary the number of schema representations (1, 2, 3, 4, 5) while holding candidate count constant at 5. Measure EX improvement per additional representation and calculate correlation coefficients between candidates from different representations.

2. **Cross-Dataset Generalization:** Evaluate N-rep on SPIDER and another text-to-SQL benchmark using the same configuration (5 candidates, same confidence thresholds). Compare performance drop to baseline methods to assess whether representation diversity transfers across schema distributions.

3. **Confidence Threshold Stability:** Re-run the main experiment with candidate counts of 3, 7, and 10 while using the original 5-candidate confidence thresholds. Measure changes in EX and LLM call reduction ratio to determine whether thresholds generalize or require per-configuration tuning.