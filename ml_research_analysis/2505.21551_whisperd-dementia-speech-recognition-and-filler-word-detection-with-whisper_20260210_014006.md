---
ver: rpa2
title: 'WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper'
arxiv_id: '2505.21551'
source_url: https://arxiv.org/abs/2505.21551
tags:
- speech
- dementia
- whisper
- filler
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper fine-tunes OpenAI's Whisper ASR model on dementia speech
  data to improve transcription accuracy and filler word detection. The model was
  trained on DementiaBank and an in-house dementia dataset with modifications to include
  filler words in the tokenizer.
---

# WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper

## Quick Facts
- **arXiv ID**: 2505.21551
- **Source URL**: https://arxiv.org/abs/2505.21551
- **Reference count**: 0
- **Primary result**: Fine-tuned Whisper achieves WER 0.24 on dementia speech with improved filler word detection

## Executive Summary
This paper presents WhisperD, a fine-tuned version of OpenAI's Whisper automatic speech recognition (ASR) model specifically adapted for dementia speech recognition with enhanced filler word detection. The researchers modified Whisper's tokenizer to include "uh" and "um" as distinct tokens, then fine-tuned multiple model variants (Tiny, Base, Small, Medium) on a combined dataset of DementiaBank, Kempler corpus, and an in-house CONNECT dataset containing 11,397 audio samples. The medium-sized model achieved a word error rate of 0.24, outperforming previous work, while also demonstrating improved filler word detection capabilities with an F1 score of 0.66.

## Method Summary
The approach involved fine-tuning pre-trained Whisper models from HuggingFace with modifications to the tokenizer vocabulary to include "uh" and "um" tokens. The training utilized DementiaBank (Pitt and Kempler corpora) and an in-house CONNECT dataset with 5 persons with dementia, totaling 11.39 hours of audio after preprocessing. Speaker-disjoint splits ensured test speakers were not present in training or validation sets. The fine-tuning used AdamW optimizer with learning rate 1.2e-5, weight decay 0.01, cosine learning rate decay with 650 warmup steps, and batch size 8 with gradient accumulation of 8 (effective batch size 64). All model layers (encoder and decoder) were fine-tuned using cross-entropy loss.

## Key Results
- WhisperD-M model achieved WER of 0.24 on dementia speech recognition task
- Filler Inclusion Rate improved from 0.30 to 0.06 after fine-tuning
- F1 score for filler word detection increased from 0.51 to 0.66
- Model demonstrates better transcription accuracy compared to baseline Whisper

## Why This Works (Mechanism)
The approach leverages Whisper's strong pre-trained language understanding capabilities while adapting it to the specific acoustic and linguistic patterns of dementia speech. By incorporating filler words ("uh" and "um") into the tokenizer vocabulary and fine-tuning on dementia-specific speech data, the model learns to recognize and accurately transcribe these disfluencies that are more prevalent in dementia speech. The speaker-disjoint split ensures the model generalizes to unseen speakers, and fine-tuning all layers allows the model to adapt both acoustic and linguistic representations to the domain.

## Foundational Learning
- **Filler word detection**: Identifying disfluencies like "uh" and "um" in speech transcription - needed because these are more common in dementia speech and can provide cognitive assessment insights; quick check: verify "uh"/"um" tokens appear in tokenizer vocabulary
- **Speaker-disjoint validation**: Ensuring test speakers are completely separate from training speakers - needed to prevent data leakage and ensure genuine generalization; quick check: confirm zero speaker ID overlap between splits
- **Cross-entropy loss for ASR**: Standard training objective measuring token-level prediction accuracy - needed for fine-tuning Whisper on domain-specific data; quick check: monitor loss convergence during training
- **Cosine learning rate decay**: Gradual reduction of learning rate during training - needed to stabilize fine-tuning of pre-trained models; quick check: verify learning rate schedule implementation
- **CHAT format parsing**: Extracting speech transcripts from dementia assessment data - needed to prepare DementiaBank data for training; quick check: confirm proper removal of interviewer speech and normalization of colloquialisms
- **Data preprocessing for ASR**: Resampling audio to 16kHz, segmenting to 1-30 second clips - needed to match Whisper's expected input format; quick check: verify all audio files meet length and sampling rate requirements

## Architecture Onboarding

**Component Map**: Tokenizer -> Pre-trained Whisper -> Fine-tuned WhisperD -> Evaluation Metrics

**Critical Path**: Tokenizer modification → Model fine-tuning → WER/FIR/F1 evaluation

**Design Tradeoffs**: The choice of medium-sized model balances performance gains against computational efficiency. Using all layers for fine-tuning provides better adaptation but increases risk of overfitting on limited data. Including filler words in vocabulary improves detection but may slightly increase WER if not handled properly.

**Failure Signatures**: Poor filler word detection indicates tokenizer modification failed; high validation-test WER gap suggests overfitting; low WER but high FIR suggests the model is omitting filler words despite their presence in training data.

**Three First Experiments**:
1. Test tokenizer modification by running sample sentences with "uh"/"um" through the modified model and verifying these tokens appear in outputs
2. Verify speaker-disjoint split by logging unique speaker IDs across all splits and confirming no intersection
3. Monitor WER and F1 score per epoch during fine-tuning to detect overfitting or underfitting patterns

## Open Questions the Paper Calls Out
- **Open Question 1**: Does fine-tuning the Whisper Large variant on limited dementia datasets yield performance improvements over smaller models, or does its increased parameter count lead to overfitting without extensive data expansion? The authors note future work will examine this but the current study only evaluated Tiny through Medium variants.
- **Open Question 2**: To what extent can incorporating general speech disfluency datasets, such as PodcastFillers, improve the F1 score for filler word detection in dementia speech? The authors suggest this could increase performance but the current model was trained exclusively on dementia-specific data.
- **Open Question 3**: Can data augmentation techniques, specifically speed perturbation, effectively compensate for the limited size (11.39 hours) of the current dementia speech corpus? The authors identify dataset size as a limitation and suggest augmentation but did not implement these techniques in the current study.

## Limitations
- The in-house CONNECT dataset is unavailable, preventing exact replication of experimental conditions
- Specific speaker IDs assigned to test set are not listed, making speaker-disjoint split verification impossible
- 70% similarity threshold verification process lacks implementation details and affects dataset quality control
- Relatively small dataset size (11.39 hours) raises concerns about generalization beyond represented speaker populations

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core methodology is sound and technically feasible | High |
| Reported WER and filler detection improvements are plausible | Medium |
| Impact of 70% similarity threshold verification | Low |
| Specific contribution of in-house dataset | Low |

## Next Checks
1. Replicate the speaker-disjoint split using only publicly available DementiaBank data, implementing the exact speaker separation strategy and verifying zero speaker overlap between splits
2. Test tokenizer modification effectiveness by creating a synthetic test set with known filler word patterns and verifying these tokens are correctly identified and preserved in the vocabulary
3. Perform ablation on dataset components by training models with only DementiaBank data versus combined DementiaBank+CONNECT data (where possible) to quantify the contribution of the proprietary dataset to reported performance improvements