---
ver: rpa2
title: 'Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer
  Models'
arxiv_id: '2508.10243'
source_url: https://arxiv.org/abs/2508.10243
tags:
- head
- backdoor
- attack
- malicious
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Head-wise Pruning and Malicious Injection (HPMI),
  the first retraining-free backdoor attack on transformer models that maintains the
  original architecture. The method identifies and prunes the least important attention
  head in each transformer layer, then injects a pre-trained malicious head to establish
  a backdoor.
---

# Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models

## Quick Facts
- **arXiv ID**: 2508.10243
- **Source URL**: https://arxiv.org/abs/2508.10243
- **Reference count**: 21
- **Primary result**: First retraining-free backdoor attack on transformer models achieving >99.55% attack success rate while maintaining clean accuracy

## Executive Summary
This paper introduces Head-wise Pruning and Malicious Injection (HPMI), a novel retraining-free backdoor attack on transformer models. The attack works by pruning the least important attention head from each transformer layer and injecting a pre-trained malicious head to establish a backdoor. HPMI achieves high attack success rates exceeding 99.55% across multiple datasets and models while preserving the model's original clean accuracy. The attack successfully bypasses four state-of-the-art defense mechanisms and demonstrates greater resilience than traditional retraining-based attacks.

## Method Summary
HPMI operates by first identifying and pruning the least important attention head in each transformer layer based on attention weight importance. A pre-trained malicious head is then injected to replace the pruned head, establishing the backdoor trigger. The method requires only a small subset of training data and basic knowledge of the target model's architecture. Unlike traditional backdoor attacks that require retraining, HPMI maintains the original model architecture while achieving high attack success rates. The attack is designed to be resistant to detection and removal by leading defense techniques, making it particularly concerning for practical deployment scenarios.

## Key Results
- Achieves attack success rates exceeding 99.55% across multiple datasets and models
- Maintains clean accuracy while establishing backdoor functionality
- Successfully bypasses four state-of-the-art defense mechanisms
- Demonstrates greater resilience than retraining-based backdoor attacks

## Why This Works (Mechanism)
The attack exploits the redundancy and replaceability of attention heads in transformer models. By pruning the least important heads and replacing them with pre-trained malicious heads, HPMI creates a backdoor that remains functional while avoiding detection. The method leverages the fact that transformer models can maintain performance even with reduced capacity, allowing malicious heads to be inserted without degrading clean accuracy. The theoretical analysis demonstrates that the backdoor structure is inherently resistant to common defense mechanisms that rely on detecting abnormal weight patterns or behavior.

## Foundational Learning

**Transformer Attention Mechanism**: Understanding how attention heads process and weight different parts of input sequences is crucial for identifying which heads can be pruned without significant impact. Quick check: Verify that pruning individual attention heads doesn't catastrophically degrade model performance on clean data.

**Model Pruning Techniques**: Knowledge of how to identify and remove less important model components is essential for the pruning phase. Quick check: Confirm that the pruned heads are indeed the least important through ablation studies.

**Backdoor Trigger Design**: Understanding how to create triggers that activate malicious behavior without affecting normal operation is key. Quick check: Test that triggers only activate the backdoor on specific inputs while leaving normal inputs unaffected.

**Defense Bypass Strategies**: Understanding common defense mechanisms helps in designing attacks that can evade detection. Quick check: Verify that the attack successfully bypasses multiple defense techniques including pruning, fine-pruning, ABS, and NeuralCleanse.

## Architecture Onboarding

**Component Map**: Input -> Transformer Layers -> Attention Heads -> Pruned Heads -> Injected Malicious Heads -> Output

**Critical Path**: The attention mechanism is the critical path, as HPMI specifically targets attention heads for modification. The pruning and injection process must maintain the flow of information through the remaining legitimate heads.

**Design Tradeoffs**: The main tradeoff is between attack effectiveness and stealth. Pruning too many important heads would degrade clean accuracy, while pruning too few might not create a strong enough backdoor. HPMI balances this by targeting only the least important heads.

**Failure Signatures**: If the attack fails, typical signatures include degraded clean accuracy, low attack success rates, or detection by defense mechanisms. Successful attacks maintain clean accuracy while achieving high attack rates without triggering defenses.

**First Experiments**:
1. Test HPMI on a small, well-understood transformer model (like BERT-base) on a simple dataset to verify basic functionality
2. Measure the impact of pruning different numbers of heads on clean accuracy versus attack success rate
3. Evaluate the attack's resilience against individual defense mechanisms to identify which defenses are most effective

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness may diminish as defense mechanisms evolve and improve
- Theoretical resistance to detection needs more extensive real-world validation
- Practical implementation may vary based on specific deployment contexts

## Confidence
- **Attack Success Rate**: High confidence in achieving >99.55% attack success rates, supported by extensive empirical results
- **Clean Accuracy Preservation**: High confidence that clean accuracy is maintained, demonstrated across multiple datasets and models
- **Defense Resistance**: Medium confidence in complete resistance to all detection and removal techniques, primarily based on theoretical analysis
- **Practical Implementation**: Medium confidence that the attack requires only basic architecture knowledge and small data subsets, as practical details may vary

## Next Checks
1. Test HPMI against emerging defense mechanisms developed after this paper's publication to assess continued effectiveness
2. Evaluate the attack's performance across additional model architectures and sizes beyond those tested
3. Conduct long-term stability tests to verify that injected backdoors remain functional after model updates and fine-tuning on new data