---
ver: rpa2
title: Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual Explanations
  for Text Classification
arxiv_id: '2503.04463'
source_url: https://arxiv.org/abs/2503.04463
tags:
- classifier
- llms
- counterfactual
- generation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two simple yet effective approaches to guide
  LLMs in generating high-fidelity counterfactual explanations for text classification
  without requiring task-specific fine-tuning. The first approach (CGG) incorporates
  classifier-derived important words into prompts to steer generation, while the second
  (CGV) validates and selects from multiple generated candidates using the classifier's
  predictions.
---

# Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual Explanations for Text Classification

## Quick Facts
- arXiv ID: 2503.04463
- Source URL: https://arxiv.org/abs/2503.04463
- Reference count: 40
- Primary result: LLM-guided counterfactual generation achieves 99% flip rate on IMDB and 84% on SNLI without task-specific fine-tuning

## Executive Summary
This paper introduces two simple yet effective approaches to guide LLMs in generating high-fidelity counterfactual explanations for text classification without requiring task-specific fine-tuning. The first approach (CGG) incorporates classifier-derived important words into prompts to steer generation, while the second (CGV) validates and selects from multiple generated candidates using the classifier's predictions. Experiments on sentiment analysis and natural language inference tasks show that these guided methods outperform both state-of-the-art fine-tuned approaches and unguided LLM baselines, achieving higher flip rates (up to 99% on IMDB) and lower token distances (down to 32.7 on IMDB). The methods also improve classifier robustness when used for data augmentation. However, analysis reveals that LLMs rely more on parametric knowledge than faithfully explaining classifier reasoning, suggesting the need for further research on faithfulness.

## Method Summary
The paper proposes two guidance methods for LLM counterfactual generation: CGG (Classifier-Guided Generation) and CGV (Classifier-Guided Validation). CGG extracts important words via XAI methods (saliency maps or SHAP) and injects them into prompts, explicitly directing the LLM to modify these words. CGV generates n candidates (n=5-10), validates each with the classifier, and selects the candidate that achieves label flip with minimal Levenshtein distance to the original. The methods are tested on IMDB sentiment analysis and SNLI natural language inference tasks using BERT classifiers and three LLMs (Llama-3.1-8B-Instruct, Llama-2-7b-chat, GPT-4o-mini) with 1-shot prompting and temperature 1.0 for diversity.

## Key Results
- CGV achieves 99% flip rate on IMDB and 84% on SNLI with Llama-3.1-8B-Instruct, significantly outperforming vanilla LLM baselines
- CGG increases modification rate of important words from 0-40% to 70-75%, though this doesn't uniformly improve flip rates
- LLMs struggle to explain low-accuracy classifiers (17% accuracy, reversed labels), suggesting reliance on parametric knowledge over classifier reasoning
- Guided methods improve classifier robustness when used for data augmentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing classifier-derived important words in prompts increases modification rates of those words, though this does not uniformly improve flip rates across all LLMs.
- **Mechanism:** CGG extracts important words via XAI methods (saliency maps or SHAP) and injects them into prompts, explicitly directing the LLM to modify these words. This creates a targeted generation space focused on classifier-relevant features.
- **Core assumption:** Words important for the original prediction are relevant for generating successful counterfactuals (an assumption the paper later questions).
- **Evidence anchors:**
  - [abstract]: "first approach (CGG) incorporates classifier-derived important words into prompts to steer generation"
  - [Section 5.2, Fig. 3a]: CGG increases modification rate from 0-40% to 70-75%
  - [Section 5.2]: "high MR does not guarantee flipping labels... GPT-4o-mini shows opposite pattern"
  - [corpus]: Weak corpus evidence on word-importance-guided generation specifically for text counterfactuals
- **Break condition:** When important words are deleted or replaced with synonyms rather than sentiment-relevant alternatives (Table 2 failure cases), or when LLM ignores guidance in favor of parametric knowledge.

### Mechanism 2
- **Claim:** Post-hoc validation with classifier feedback consistently selects high-fidelity counterfactuals with high flip rates across LLMs.
- **Mechanism:** CGV generates n candidates (n=5-10), validates each with the classifier, and selects the candidate that achieves label flip with minimal Levenshtein distance to the original. This filters out low-fidelity candidates without requiring the LLM to understand classifier internals.
- **Core assumption:** LLMs can generate at least one successful counterfactual among multiple diverse candidates when given label guidance.
- **Evidence anchors:**
  - [abstract]: "validates and selects from multiple generated candidates using the classifier's predictions"
  - [Section 5.1, Table 1]: CGV achieves 99% flip rate on IMDB, 84% on SNLI with Llama-3.1
  - [Section 3.2]: Formal selection criterion: x' = argmin_{x̂_i: f(x̂_i)=y'} d(x̂_i, x)
  - [corpus]: Similar validation/selection strategies appear in related counterfactual generation work
- **Break condition:** When no generated candidates flip the label (rare per results), system falls back to minimal-distance selection, which does not guarantee flip.

### Mechanism 3
- **Claim:** LLM counterfactual generation is driven primarily by parametric knowledge alignment rather than faithful explanation of classifier reasoning.
- **Mechanism:** LLMs succeed when classifier decisions align with real-world knowledge (high-accuracy classifiers) but fail when forced to explain counter-intuitive classifiers (low-accuracy with reversed labels). This suggests LLMs approximate "what would make sense to change" rather than "what the classifier actually uses."
- **Core assumption:** Counterfactuals for accurate classifiers should reflect real-world logic; faithful explanation requires success even on inaccurate classifiers.
- **Evidence anchors:**
  - [abstract]: "LLMs rely on parametric knowledge rather than faithfully following the classifier"
  - [Section 5.5]: Low-accuracy classifier (17%, reversed labels) shows FR drops from ~99% to 16-79% depending on LLM
  - [Section 5.5]: "LLMs struggle to explain the low-accuracy classifier... suggesting LLMs heavily rely on parametric knowledge"
  - [corpus]: Corpus does not directly address parametric vs. classifier faithfulness in counterfactuals
- **Break condition:** When classifier logic fundamentally contradicts LLM parametric knowledge, guidance signals become insufficient.

## Foundational Learning

- **Concept: Counterfactual explanations vs. adversarial examples**
  - **Why needed here:** The paper distinguishes XAI counterfactuals (minimal changes revealing decision boundaries, maintaining semantic meaning) from adversarial perturbations. This distinction determines evaluation criteria—flip rate alone is insufficient; semantic coherence and minimality matter.
  - **Quick check question:** If a model classifies "great movie" as positive, is changing it to "terrible movie" a valid counterfactual? What about changing it to "greatmoviex"?

- **Concept: Fidelity vs. faithfulness in XAI**
  - **Why needed here:** The paper explicitly distinguishes these (Footnote 4). Fidelity = counterfactual actually flips the label. Faithfulness = counterfactual truly reflects the classifier's internal reasoning. High fidelity does not guarantee faithfulness—this is the paper's critical finding.
  - **Quick check question:** If an LLM generates a counterfactual that flips the label but uses different reasoning than the classifier, is it faithful? Is it high-fidelity?

- **Concept: Feature importance methods (Saliency Maps, SHAP)**
  - **Why needed here:** CGG relies on these XAI methods to identify "important words." Understanding their limitations is crucial—the paper shows words important for current prediction may not be optimal for counterfactual generation.
  - **Quick check question:** If a saliency map highlights "the" as important for a sentiment classifier, what might this indicate about the model? Should CGG use this word as guidance?

## Architecture Onboarding

- **Component map:**
  - Input instance x → Classifier → prediction y
  - CGG path: x → XAI Module → important words → Prompt Constructor → LLM → counterfactual x'
  - CGV path: x → Prompt Constructor → LLM (n times) → {x̂_i} → Classifier validation → select x'
  - Combined CGGV: CGG prompt → LLM (n times) → CGV selection
  - Evaluate: f(x') == y'? (flip check) + distance/perplexity/quality metrics

- **Critical path:**
  1. Input instance x → Classifier → prediction y
  2. CGG path: x → XAI Module → important words → Prompt Constructor → LLM → counterfactual x'
  3. CGV path: x → Prompt Constructor → LLM (n times) → {x̂_i} → Classifier validation → select x'
  4. Combined CGGV: CGG prompt → LLM (n times) → CGV selection
  5. Evaluate: f(x') == y'? (flip check) + distance/perplexity/quality metrics

- **Design tradeoffs:**
  - **CGG vs. CGV:** CGG is faster (1 generation) but inconsistent across LLMs. CGV requires n× generations but consistently achieves high flip rates. Paper shows CGV outperforms CGG in most settings.
  - **Number of shots:** 1-shot recommended for stability; more shots can degrade performance on some LLMs/datasets (Llama-2 with 5-shot drops FR on IMDB). Context length limits (Llama-2 at 10-shot) constrain scaling.
  - **Number of candidates (n):** n=5 for IMDB, n=10 for SNLI. Diminishing returns beyond these per preliminary experiments.
  - **XAI method:** Saliency maps vs. SHAP show similar performance (Table 6), suggesting current XAI methods have limited utility for counterfactual guidance.
  - **LLM choice:** Llama-3.1 generally outperforms Llama-2 and GPT-4o-mini on counterfactual metrics, but GPT-4o-mini shows better instruction-following on low-accuracy classifiers.

- **Failure signatures:**
  - **Low flip rate with high modification rate:** LLM changes important words but uses synonyms or deletions rather than sentiment-relevant substitutions (Table 2 examples).
  - **High perplexity/low text quality:** Forced word modifications create incoherent text; CGG more prone to this than CGV.
  - **Good performance on high-accuracy classifier, poor on low-accuracy:** Indicates LLM relying on parametric knowledge rather than classifier faithfulness (Section 5.5 pattern).
  - **CGG helps Llama-2 but hurts others:** Per Fig. 3b, guidance signal effectiveness is model-dependent.
  - **Context overflow:** Llama-2 with 10-shot prompts exceeds 4096-token limit—failure mode is no valid generation.

- **First 3 experiments:**
  1. **Baseline comparison on single dataset:** Implement vanilla 1-shot prompting on IMDB sentiment analysis with Llama-3.1-8B-Instruct. Measure flip rate and token distance. Expected: ~93% FR, ~46 token distance (Table 1). This validates basic setup before adding complexity.
  2. **CGV implementation with candidate sweep:** Implement CGV with n=1, 3, 5, 10 candidates on IMDB. Plot flip rate vs. n to verify paper's claim of diminishing returns beyond n=5. Check if Levenshtein distance selection correlates with semantic similarity (manual inspection of 10-20 samples).
  3. **Faithfulness stress test:** Train a low-accuracy classifier (e.g., logistic regression with reversed labels on IMDB subset, ~17% accuracy as in paper). Compare CGG, CGV, and CGGV flip rates between this classifier and a normal high-accuracy classifier. Expected: Large drop in FR indicates parametric knowledge reliance. This is critical for understanding whether your system explains classifiers or generates plausible text.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM-based counterfactual generation be adapted to faithfully explain classifiers whose decision logic contradicts the LLM's parametric knowledge?
- **Basis in paper:** [explicit] The authors conclude that "LLMs rely on parametric knowledge rather than faithfully following the classifier" and struggle to flip labels for low-accuracy (reversed-label) classifiers.
- **Why unresolved:** Current prompting methods fail to override the LLM's internal priors; the models generate realistic text that aligns with real-world logic rather than the specific (potentially flawed) reasoning of the target classifier.
- **What evidence would resolve it:** A method that maintains high flip rates on adversarially corrupted or low-accuracy classifiers without requiring specific fine-tuning on those error modes.

### Open Question 2
- **Question:** How can we identify "counterfactual-relevant" words that differ from standard feature importance (saliency) maps to improve guidance signals?
- **Basis in paper:** [explicit] Section 5.2 notes that "words that are important for the original prediction... may not be relevant for counterfactuals," which caused the CGG approach to fail or degrade performance in GPT-4o-mini.
- **Why unresolved:** Saliency maps (like SHAP or gradients) highlight features driving the current prediction, but these are not necessarily the minimal features required to flip the decision boundary.
- **What evidence would resolve it:** A new feature selection method that identifies tokens specifically correlated with the decision boundary's proximity rather than the prediction confidence.

### Open Question 3
- **Question:** Can the trade-off between high counterfactual fidelity and text quality (fluency/grammar) be eliminated?
- **Basis in paper:** [inferred] Section 5.1 reports that while guidance improves flip rates, it causes a "slight decline" in text quality metrics (Grammar, Fluency) likely due to "distribution shifts from forced word modifications."
- **Why unresolved:** Guided generation forces specific structural changes (token swaps) that may disrupt the natural statistical distributions learned by the LLM, degrading fluency.
- **What evidence would resolve it:** A generation strategy that achieves state-of-the-art flip rates (comparable to CGGV) while maintaining text quality metrics statistically indistinguishable from the vanilla (unguided) LLM baseline.

## Limitations
- The fundamental tension between fidelity and faithfulness: high flip rates don't guarantee the counterfactuals reflect classifier reasoning rather than LLM parametric knowledge
- CGG's word-importance guidance is inconsistent across LLMs and may harm performance in some cases
- The assumption that classifier-derived importance scores are relevant for counterfactual generation is questioned by the results

## Confidence
**High Confidence:**
- CGV consistently achieves higher flip rates than CGG across all tested LLMs and datasets
- LLM counterfactual generation performance correlates with classifier accuracy (high accuracy → high flip rates, low accuracy → low flip rates)
- Text quality metrics (perplexity, grammar, fluency) are generally maintained across methods

**Medium Confidence:**
- Word importance guidance (CGG) increases modification rates of important words, though the relationship with flip rate is inconsistent
- Current XAI methods (saliency maps, SHAP) show similar effectiveness for counterfactual guidance
- Trade-off between text quality and flip rate in guided methods is statistically significant

**Low Confidence:**
- Claims about parametric knowledge vs. classifier faithfulness require more rigorous validation beyond the reversed-label classifier experiment
- The optimal number of candidates (n=5 for IMDB, n=10 for SNLI) may be dataset-specific and requires broader validation

## Next Checks
1. **Faithfulness validation with controlled classifiers**: Construct classifiers with known decision boundaries (e.g., logistic regression on specific word features) and verify whether generated counterfactuals align with these features rather than relying on parametric knowledge. Compare feature attribution on original vs. counterfactual inputs.

2. **Generalization across datasets and domains**: Test the methods on additional text classification tasks beyond sentiment analysis and NLI, including multi-label classification and specialized domains (medical text, legal documents). Evaluate whether the n=5/n=10 candidate rule generalizes or requires tuning.

3. **Ablation study on guidance components**: Systematically remove components (important word guidance, candidate generation, classifier validation) to quantify their individual contributions to flip rate and text quality. Include analysis of failure cases where guidance signals are ignored by LLMs.