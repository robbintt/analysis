---
ver: rpa2
title: 'MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks'
arxiv_id: '2504.14039'
source_url: https://arxiv.org/abs/2504.14039
tags:
- score
- benchmark
- evaluation
- high
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MEQA, a meta-evaluation framework for assessing\
  \ the quality of question-and-answer (QA) benchmarks used to evaluate large language\
  \ models (LLMs). The framework defines eight key criteria\u2014memorization robustness,\
  \ prompt robustness, evaluation design, evaluator design, reproducibility, comparability,\
  \ validity, and reliability\u2014broken down into 44 sub-criteria scored from 1\
  \ to 5."
---

# MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks

## Quick Facts
- arXiv ID: 2504.14039
- Source URL: https://arxiv.org/abs/2504.14039
- Reference count: 40
- Introduces MEQA, a meta-evaluation framework for assessing QA benchmarks used to evaluate LLMs

## Executive Summary
This paper presents MEQA, a systematic framework for evaluating the quality of question-and-answer benchmarks used to assess large language models. The framework establishes eight key criteria—memorization robustness, prompt robustness, evaluation design, evaluator design, reproducibility, comparability, validity, and reliability—broken into 44 sub-criteria scored on a 1-5 scale. When applied to eight cybersecurity QA benchmarks using both human and LLM (GPT-4o) evaluators, MEQA reveals that most benchmarks perform well in reproducibility and comparability but struggle with prompt robustness and reliability. The framework provides actionable insights for improving benchmark quality and demonstrates scalability through LLM-based evaluation while maintaining alignment with human assessments.

## Method Summary
MEQA defines eight evaluation criteria for QA benchmarks, each broken into specific sub-criteria scored from 1 to 5. The framework was applied to eight cybersecurity QA benchmarks using both human evaluators (showing over 80% agreement) and GPT-4o as an LLM evaluator. The evaluation process systematically assessed each benchmark against the 44 sub-criteria, providing quantitative scores and qualitative insights into benchmark quality. The framework's scalability was demonstrated through automated LLM scoring, while human evaluation provided validation and identified areas where LLM assessments aligned with or diverged from human judgment.

## Key Results
- Most benchmarks excel in reproducibility and comparability but struggle with prompt robustness and reliability
- Human evaluators showed over 80% agreement on benchmark assessments
- LLM (GPT-4o) scoring proved scalable and aligned with human assessments in extreme cases
- The framework provides actionable insights for improving benchmark quality

## Why This Works (Mechanism)
MEQA works by providing a structured, systematic approach to benchmark evaluation that breaks down complex quality assessment into measurable sub-components. The framework's strength lies in its comprehensive coverage of benchmark characteristics through 44 specific criteria across eight dimensions, allowing for granular identification of strengths and weaknesses. By incorporating both human and LLM evaluators, the framework balances expert judgment with scalability, enabling consistent evaluation across multiple benchmarks while maintaining quality control through inter-rater agreement metrics.

## Foundational Learning

- **Benchmark Evaluation Theory**: Understanding how to systematically assess evaluation tools themselves is crucial because benchmarks can be flawed while appearing effective, potentially misleading LLM development efforts
- **LLM Evaluation Metrics**: Knowledge of standard evaluation practices is needed to ensure MEQA criteria align with established best practices in the field
- **Human-AI Collaboration**: Understanding the strengths and limitations of both human and LLM evaluators is essential for designing a framework that leverages both effectively

Quick check: Verify that each of the 44 sub-criteria can be consistently applied across different benchmark types and domains

## Architecture Onboarding

Component map: MEQA Framework -> 8 Criteria -> 44 Sub-criteria -> Scoring (1-5) -> Human/LLM Evaluators -> Benchmark Assessment

Critical path: Benchmark Selection → Sub-criteria Definition → Evaluator Training → Scoring Process → Results Analysis → Improvement Recommendations

Design tradeoffs: The framework prioritizes comprehensiveness over simplicity, potentially making it more time-consuming to apply but providing more nuanced insights than simpler evaluation methods

Failure signatures: Inconsistent scoring across evaluators, inability to capture domain-specific nuances, or criteria that don't apply to certain benchmark types indicate framework limitations

First experiments:
1. Apply MEQA to a non-cybersecurity benchmark to test domain transferability
2. Conduct inter-rater reliability studies with multiple human evaluators
3. Test the framework's ability to detect quality improvements in benchmarks that have been updated over time

## Open Questions the Paper Calls Out
The paper identifies several areas for future work, including expanding MEQA beyond QA benchmarks to other evaluation types, incorporating more diverse evaluators to reduce bias, and developing automated tools to assist with the evaluation process. The authors also note the need to explore how MEQA criteria might conflict with each other and how to resolve such trade-offs in practice.

## Limitations
- The framework's criteria are defined somewhat qualitatively, which may introduce subjectivity in scoring despite the 1-5 scale
- Evaluation was conducted on only eight cybersecurity QA benchmarks, limiting generalizability to other domains
- Reliance on either human evaluators or GPT-4o introduces potential bias from inconsistent human interpretations or training data biases

## Confidence

High: Framework's structure and application methodology are clearly defined and consistently applied
Medium: Specific benchmark assessments given the small sample size and single-domain focus
Low: Generalizability of findings to non-QA benchmarks or other domains, as this was not empirically tested

## Next Checks

1. Apply MEQA to benchmarks from diverse domains (e.g., medicine, law, general knowledge) to assess cross-domain applicability
2. Conduct inter-rater reliability studies with multiple human evaluators to quantify subjectivity in scoring
3. Test MEQA against benchmarks that have been updated or revised over time to evaluate whether the framework can detect quality improvements