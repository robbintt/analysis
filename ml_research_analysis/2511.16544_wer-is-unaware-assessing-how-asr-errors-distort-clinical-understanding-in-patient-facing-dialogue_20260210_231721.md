---
ver: rpa2
title: 'WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in
  Patient Facing Dialogue'
arxiv_id: '2511.16544'
source_url: https://arxiv.org/abs/2511.16544
tags:
- clinical
- patient
- metrics
- impact
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether standard ASR metrics like WER correlate
  with clinical impact in patient-facing dialogue. The authors establish a benchmark
  by having expert clinicians annotate ASR errors from doctor-patient conversations,
  labeling the clinical risk (no, minimal, or significant impact).
---

# WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue

## Quick Facts
- arXiv ID: 2511.16544
- Source URL: https://arxiv.org/abs/2511.16544
- Reference count: 40
- The paper finds that standard ASR metrics like WER poorly correlate with clinical impact in patient-facing dialogue, and develops an LLM-as-a-Judge framework achieving 90% accuracy in clinical risk assessment.

## Executive Summary
This paper addresses a critical gap in evaluating automatic speech recognition (ASR) systems for healthcare applications. The authors demonstrate that traditional metrics like Word Error Rate (WER) fail to capture the clinical significance of ASR errors in doctor-patient conversations. Through expert clinician annotations of 1,260 utterances from 53 cardiology consultations, they establish a benchmark for assessing how ASR errors impact clinical understanding. They then develop a novel GEPA-optimized LLM-as-a-Judge framework that achieves human-comparable performance in evaluating clinical risk, providing a safety-critical evaluation framework for ASR systems in healthcare.

## Method Summary
The authors establish a clinical impact benchmark by having two expert clinicians annotate ASR errors from doctor-patient conversations, categorizing the clinical risk as no impact, minimal impact, or significant impact. They then develop an LLM-as-a-Judge framework using GEPA (Gradient Estimation with Paired Augmentation) optimization to evaluate clinical risk from ASR errors. The framework is trained and evaluated on the annotated dataset, with performance measured against human expert assessments using accuracy and Cohen's kappa metrics.

## Key Results
- WER and other existing ASR metrics show poor correlation with expert clinical risk assessments
- The GEPA-optimized LLM-as-a-Judge achieves 90% accuracy in clinical risk classification
- Cohen's kappa of 0.816 demonstrates substantial agreement with human experts
- The framework successfully identifies clinically significant ASR errors that traditional metrics miss

## Why This Works (Mechanism)
The framework works by bridging the gap between textual accuracy and clinical significance. Traditional ASR metrics evaluate surface-level errors without considering the semantic meaning or clinical context. The GEPA-optimized LLM-as-a-Judge leverages the model's understanding of medical terminology, clinical context, and the relationship between words and their clinical implications. By training on expert-annotated examples, the system learns to weigh errors based on their potential to mislead clinical understanding rather than just their frequency or position.

## Foundational Learning
- Clinical risk assessment in medical dialogue: Why needed - To establish ground truth for evaluating ASR impact; Quick check - Expert clinicians can reliably categorize ASR errors by clinical significance
- GEPA optimization technique: Why needed - To improve LLM judgment accuracy beyond standard fine-tuning; Quick check - GEPA outperforms baseline optimization on clinical risk classification
- LLM-as-a-Judge methodology: Why needed - To automate safety-critical evaluation of ASR systems; Quick check - LLM judgments correlate strongly with human expert assessments

## Architecture Onboarding

**Component Map:** ASR Output -> Error Annotation -> Clinical Risk Classification -> GEPA Optimization -> LLM-as-a-Judge -> Risk Assessment

**Critical Path:** ASR system generates transcript → Expert clinicians annotate clinical impact → GEPA-optimized LLM is trained on annotations → LLM evaluates new ASR errors for clinical risk

**Design Tradeoffs:** The framework trades computational cost (LLM inference) for clinical safety, prioritizing accurate risk assessment over speed. The use of a single clinical domain provides focused expertise but limits generalizability.

**Failure Signatures:** Poor performance on medical terminology outside cardiology domain, inability to capture nuanced clinical context, over-reliance on textual patterns rather than semantic meaning.

**3 First Experiments:**
1. Validate framework performance on external clinical datasets from different medical specialties
2. Compare LLM-as-a-Judge performance against a larger panel of clinical experts
3. Test framework robustness to ASR errors in noisy audio conditions

## Open Questions the Paper Calls Out
The paper acknowledges that clinical impact assessment is inherently subjective and context-dependent, and that the binary classification approach may oversimplify nuanced clinical judgments. The authors also note that the framework's performance on external datasets and in real-world deployment scenarios remains to be validated.

## Limitations
- Study relies on a single clinical domain (cardiology) limiting generalizability
- Expert annotation involved only two clinicians, raising inter-rater reliability concerns
- Performance metrics based on same dataset used for training and optimization
- Framework's reliance on GPT-4 raises deployment cost and transparency concerns

## Confidence

**Clinical risk annotation framework validity:** Medium
**LLM-as-a-Judge performance metrics:** Medium
**GEPA optimization effectiveness:** High
**Generalizability across clinical domains:** Low

## Next Checks

1. Conduct independent validation using external clinical dialogue datasets from multiple specialties and healthcare settings
2. Implement A/B testing comparing LLM-as-a-Judge assessments against a broader panel of clinical experts across different institutions
3. Develop and validate a longitudinal evaluation framework to assess how ASR errors accumulate and impact clinical understanding over extended conversation segments