---
ver: rpa2
title: What Is Your AI Agent Buying? Evaluation, Biases, Model Dependence, & Emerging
  Implications for Agentic E-Commerce
arxiv_id: '2508.02630'
source_url: https://arxiv.org/abs/2508.02630
tags:
- product
- position
- agent
- agents
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACES provides a controlled framework to evaluate AI shopping agents
  by randomizing product attributes, positions, and platform cues. It isolates choice
  behavior from navigation errors and tests rationality via one-shot selection.
---

# What Is Your AI Agent Buying? Evaluation, Biases, Model Dependence, & Emerging Implications for Agentic E-Commerce

## Quick Facts
- **arXiv ID:** 2508.02630
- **Source URL:** https://arxiv.org/abs/2508.02630
- **Reference count:** 40
- **Primary result:** AI shopping agents exhibit strong model-specific position biases and are highly sensitive to price, ratings, and platform cues, leading to volatile market shares and susceptibility to algorithmic manipulation.

## Executive Summary
This paper introduces the ACES framework to evaluate AI shopping agents in e-commerce settings, revealing significant model-specific biases and sensitivities that impact market outcomes. By randomizing product attributes and platform cues, ACES isolates choice behavior from navigation errors and tests rationality via one-shot selection. Experiments show that AI agents are strongly influenced by position, price, ratings, and platform endorsements, with market shares concentrating on a few products and shifting dramatically across model versions. These findings highlight the volatility and manipulability of AI-mediated markets, underscoring the need for continuous auditing and adaptive seller strategies.

## Method Summary
The ACES framework pairs a VLM agent (via LangChain + Selenium) with a programmable mock e-commerce app featuring 8 product categories, each with 8 products in a 2×4 grid. The workflow involves opening a browser, capturing a screenshot, querying the model with a prompt, and recording the choice. Randomized controlled trials permute positions, perturb price/rating/reviews, and assign badges. Conditional logit regression estimates choice sensitivities, with 1,000 experiments per category for market shares and 500 for choice behavior. Datasets are available on HuggingFace.

## Key Results
- AI agents show strong, model-specific position biases and sensitivity to price, ratings, and reviews.
- Market shares concentrate on a few modal products and shift dramatically across model versions.
- Sponsored tags consistently reduce selection, while platform endorsements increase it, even in headless API interfaces.
- Seller-side AI agents can significantly boost market share by adjusting product descriptions, with keyword front-loading and feature alignment most effective.

## Why This Works (Mechanism)
ACES isolates choice behavior from navigation errors by using one-shot selection tasks, ensuring that observed choices reflect true preferences rather than search strategies. The framework's randomization of product attributes and platform cues allows for causal inference about agent behavior, while the conditional logit model quantifies the impact of each factor on choice probabilities. This controlled environment enables precise measurement of biases and sensitivities that would be confounded in real-world settings.

## Foundational Learning
- **Conditional Logit Model:** Estimates the probability of choosing each option based on its attributes; needed to quantify the causal impact of position, price, and platform cues on agent choices. Quick check: Verify that coefficients align with intuitive biases (e.g., lower price increases selection probability).
- **Randomized Controlled Trials (RCTs):** Ensures that observed effects are due to manipulated variables rather than confounding factors; needed to isolate the impact of each attribute. Quick check: Confirm randomization by checking for uniform distribution of positions and attribute perturbations.
- **Mock E-commerce App:** Provides a controlled, reproducible environment for testing agent behavior; needed to eliminate real-world noise and ensure consistent stimuli. Quick check: Validate that the app renders consistently across different screen resolutions and browser versions.

## Architecture Onboarding
- **Component Map:** Mock e-commerce app (2×4 grid) -> Selenium browser -> Screenshot capture -> LangChain VLM interface -> Model query -> Choice recording -> Conditional logit analysis
- **Critical Path:** Experiment setup (product randomization) -> Screenshot capture -> Model query -> Choice validation -> Data aggregation -> Statistical analysis
- **Design Tradeoffs:** Headless API vs. browser-based interaction; model-specific hyperparameters (temperature, reasoning effort) vs. general usability; high-throughput experiments vs. cost and rate limiting
- **Failure Signatures:** Invalid/non-JSON model responses; inconsistent position effects; API rate limiting or errors
- **First Experiments:**
  1. Recreate the mock e-commerce app with exact visual specifications.
  2. Run a small-scale rationality test (e.g., 10 trials) to validate the basic workflow.
  3. Conduct a single market share experiment (e.g., 100 trials per category) to verify the concentration pattern.

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified visual design and product catalog details may affect VLM screenshot parsing and choice behavior.
- Use of future model names (GPT-4.1, GPT-5.1) and undocumented API parameters introduces uncertainty about exact model behavior replication.
- High-throughput API calls required for experiments may not be feasible for all researchers due to rate limiting or cost constraints.

## Confidence
- **High Confidence:** Framework methodology for isolating choice behavior and general patterns of model-specific biases and sensitivities.
- **Medium Confidence:** Exact magnitude of market share concentration and shifts across model versions, due to unspecified model configurations and product details.
- **Medium Confidence:** Effectiveness of seller-side AI agent strategies, due to potential variability in model response to prompt engineering.

## Next Checks
1. **Visual Design Validation:** Recreate the mock e-commerce app with exact specifications for font sizes, colors, and element spacing to ensure consistent VLM screenshot parsing.
2. **Model Configuration Verification:** Test with available model versions closest to those described (e.g., Claude 3.5 Sonnet instead of Claude Opus 4.5) and document any deviations in behavior or API parameters.
3. **Market Share Reproducibility:** Conduct a smaller-scale reproduction (e.g., 100 trials per category) to verify the concentration of market shares on modal products and their sensitivity to position and platform cues.