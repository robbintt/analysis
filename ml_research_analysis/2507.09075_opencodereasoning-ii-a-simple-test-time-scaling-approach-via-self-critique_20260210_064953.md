---
ver: rpa2
title: 'OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique'
arxiv_id: '2507.09075'
source_url: https://arxiv.org/abs/2507.09075
tags:
- arxiv
- code
- reasoning
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OpenCodeReasoning-II, a dataset of 2.5 million
  question-solution-critique triples that is nearly twice the size of the previous
  largest code reasoning dataset. The dataset covers 35,000 unique programming problems
  and includes both Python and C++ solutions with reasoning traces and binary judgments
  (right/wrong).
---

# OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique

## Quick Facts
- arXiv ID: 2507.09075
- Source URL: https://arxiv.org/abs/2507.09075
- Reference count: 22
- The paper introduces a dataset of 2.5 million question-solution-critique triples, demonstrating significant improvements in competitive coding through test-time scaling with self-critique

## Executive Summary
OpenCodeReasoning-II introduces a large-scale dataset of 2.5 million question-solution-critique triples, nearly doubling the size of existing code reasoning datasets. The dataset covers 35,000 unique programming problems in both Python and C++, with reasoning traces and binary judgments. Using a two-stage fine-tuning approach, the authors train Qwen2.5-Instruct models that achieve competitive or superior performance on code generation tasks. The key innovation is demonstrating that test-time scaling with self-critique significantly improves competitive coding performance, with the 32B model improving pass@1 by approximately 6 percentage points when generating 10 solutions per problem.

## Method Summary
The paper employs a two-stage supervised fine-tuning strategy: first fine-tuning models for code generation, then jointly fine-tuning for both code generation and critique. This approach uses the OpenCodeReasoning-II dataset containing 2.5 million question-solution-critique triples across Python and C++. The authors fine-tune Qwen2.5-Instruct models of 7B, 14B, and 32B parameters, training them on both languages to enable cross-language transfer. During inference, the models generate multiple solutions per problem and use self-critique to select the best solution, demonstrating substantial performance gains through test-time scaling.

## Key Results
- OpenCodeReasoning-II dataset contains 2.5 million triples, nearly twice the size of previous largest code reasoning dataset
- 32B model improves pass@1 by ~6 percentage points through test-time scaling with 10 solutions per problem
- Asymmetric cross-language transfer observed: C++→Python transfer outperforms Python→C++

## Why This Works (Mechanism)
The effectiveness stems from combining large-scale supervised fine-tuning with test-time scaling. By training on 2.5 million question-solution-critique triples, models learn both code generation and critique capabilities. The self-critique mechanism during inference allows models to leverage their critique training to select the best among multiple generated solutions. This approach addresses the inherent uncertainty in single-attempt code generation by providing multiple candidates and using learned judgment to identify the most promising solution. The joint training on Python and C++ enables cross-language knowledge transfer, with C++ knowledge more readily transferring to Python than vice versa.

## Foundational Learning
- **Supervised fine-tuning**: Training models on curated datasets to specialize in specific tasks; needed to adapt base models to code generation and critique; quick check: verify loss reduction during training
- **Test-time scaling**: Generating multiple solutions during inference and selecting the best; needed to overcome single-attempt limitations; quick check: measure performance improvement with increasing number of candidates
- **Cross-language transfer**: Knowledge transfer between programming languages; needed to leverage bilingual training for enhanced performance; quick check: compare transfer direction efficiency (C++→Python vs Python→C++)
- **Self-critique mechanisms**: Models evaluating their own outputs; needed for automated solution selection; quick check: validate critique accuracy against human judgments
- **Binary judgment schemes**: Right/wrong classification of solutions; needed for scalable dataset creation; quick check: analyze error distribution across problem types
- **Competitive programming benchmarks**: Standardized evaluation for coding models; needed for performance comparison; quick check: ensure benchmark coverage matches training distribution

## Architecture Onboarding
**Component Map**: Dataset generation -> Two-stage fine-tuning -> Self-critique inference -> Performance evaluation
**Critical Path**: Synthetic data generation → Supervised fine-tuning → Test-time scaling → Solution selection
**Design Tradeoffs**: Large dataset size vs. potential synthetic data biases; binary vs. multi-class critique judgments; single vs. multi-language training
**Failure Signatures**: Over-reliance on synthetic data patterns; asymmetric transfer limitations; computational cost of multiple solution generation
**First Experiments**:
1. Compare performance with varying numbers of generated solutions (1, 5, 10, 20)
2. Test asymmetric transfer by training on C++ only and evaluating on Python
3. Analyze critique accuracy across different problem difficulty levels

## Open Questions the Paper Calls Out
None

## Limitations
- Binary right/wrong judgment scheme lacks granularity for nuanced error types
- Synthetic data generation pipeline may introduce biases from critique models
- Cross-language transfer analysis limited to Python and C++ only
- Computational costs of generating multiple solutions per problem not addressed

## Confidence
**High confidence**: Two-stage fine-tuning effectiveness, test-time scaling improvements, asymmetric cross-language transfer
**Medium confidence**: Smaller models benefiting more from scaling, minimal temperature impact on critique
**Low confidence**: Joint training on both languages enhancing overall performance

## Next Checks
1. Conduct detailed error categorization of critiques to understand failure modes and improve binary judgment scheme
2. Measure computational cost and latency trade-offs of generating 10 solutions per problem
3. Evaluate models on additional programming languages (Java, JavaScript) to verify cross-language transfer generalization