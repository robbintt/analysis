---
ver: rpa2
title: A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in Temporal
  Point Processes
arxiv_id: '2501.04529'
source_url: https://arxiv.org/abs/2501.04529
tags:
- event
- badmm
- module
- branches
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inferring structured event
  branches in temporal point processes (TPPs). The authors propose a novel plug-and-play
  Bregman ADMM (BADMM) module that imposes sparse and low-rank structures on the event
  transition matrix within TPPs.
---

# A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in Temporal Point Processes

## Quick Facts
- **arXiv ID:** 2501.04529
- **Source URL:** https://arxiv.org/abs/2501.04529
- **Reference count:** 11
- **Key outcome:** Novel plug-and-play Bregman ADMM (BADMM) module imposes sparse and low-rank structures on event transition matrices in temporal point processes, improving log-likelihood and prediction accuracy while providing interpretable event branches.

## Executive Summary
This paper addresses the challenge of inferring structured event branches in temporal point processes by proposing a novel plug-and-play Bregman ADMM (BADMM) module. The method imposes sparse and low-rank constraints on the event transition matrix within TPPs, effectively regularizing responsibility matrices in classic TPPs like Hawkes process and attention maps in neural TPPs like Transformer-based models. The BADMM module is implemented via subspace clustering or sparse group-lasso, using the Bregman ADMM algorithm to derive structured and interpretable event branches. Experiments on synthetic and real-world datasets demonstrate that BADMM-enhanced TPPs significantly improve model performance while providing interpretable insights into the hidden branching processes.

## Method Summary
The method introduces a Bregman ADMM module that operates on the event transition matrix B in temporal point processes. It formulates branch inference as an optimization problem with KL-divergence regularization to preserve learned priors, plus a composite regularizer combining sparsity (ℓ₁-norm) and low-rank (nuclear norm or ℓ₁,₂-group lasso) constraints. The algorithm uses auxiliary variables and Bregman divergences to derive closed-form updates via soft-thresholding and SVD-based projections. For neural TPPs, the BADMM iterations are unrolled as T=2 sequential layers replacing standard attention. The module generalizes across classic (EM-based Hawkes) and neural (Transformer-based) TPPs by operating on their native transition matrix representations - responsibility matrices or attention maps.

## Key Results
- BADMM-enhanced TPPs significantly improve log-likelihood per event (ELL) and event type prediction accuracy (ACC) compared to baseline models
- The method produces interpretable event branches, identifying isolated events and key events triggering subsequent events
- BADMM is robust to hyperparameter settings across λ ∈ {0.01,0.1,1,10,100} and α ∈ (0,1) ranges
- The BADMM* variant (nuclear norm) typically achieves better performance than BADMM₁,₂ (group lasso) variant

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Imposing sparse and low-rank constraints on event transition matrices improves branch inference by reducing over-smoothness in standard TPP learning.
- **Mechanism:** The BADMM module solves an optimization problem with KL-divergence to the initial matrix (preserving learned priors) plus a composite regularizer: α∥B∥₁ + (1-α)R(B). This is decomposed via auxiliary variables X₁, X₂ and solved through alternating updates with Bregman divergence terms, yielding closed-form solutions via soft-thresholding and SVD-based low-rank projection.
- **Core assumption:** Event branching processes in real sequences exhibit sparse triggering patterns where few key events influence subsequent events, rather than dense all-to-all connections.
- **Evidence anchors:**
  - [abstract] "formulate the inference of event branches as an optimization problem for the event transition matrix under sparse and low-rank constraints"
  - [page 4] "the regularizer α∥B∥₁ + (1-α)∥B∥∗ corresponds to the subspace clustering method... sparse group-lasso"
  - [corpus] Weak direct corpus support for this specific regularization combination in TPPs; "Bregman Douglas-Rachford Splitting Method" discusses related Bregman ADMM theory but not TPP applications.
- **Break condition:** If ground-truth branching processes are genuinely dense (not sparse), the regularization will over-sparsify and miss true triggering relationships.

### Mechanism 2
- **Claim:** Algorithm unrolling of BADMM iterations produces a differentiable module compatible with neural TPP backpropagation.
- **Mechanism:** For neural TPPs, T iterations of BADMM are unrolled as T sequential layers. Each layer implements: (1) B update via row-wise softmax with log-space combination of prior and auxiliary terms, (2) X₁ update via elementwise soft-thresholding, (3) X₂ update via either nuclear norm proximal operator (SVD + singular value thresholding) or ℓ₁,₂ group soft-thresholding, (4) dual variable updates. The stack becomes a replacement for standard self-attention.
- **Core assumption:** A small number of unrolled iterations (T=2 in experiments) suffices to achieve meaningful structure without excessive computational cost.
- **Evidence anchors:**
  - [page 4] "unrolling the iterations of the Bregman ADMM algorithm and build the BADMM module to obtain a new attention layer"
  - [page 5] "we follow the setting of Sinkhorn-based module in (Sander et al. 2022) and set T=2"
  - [corpus] No direct corpus validation of this unrolling strategy for TPPs.
- **Break condition:** If T is too small, optimization won't converge to structured solution; if T is too large, memory/compute costs during backpropagation become prohibitive.

### Mechanism 3
- **Claim:** The BADMM module generalizes across classic (EM-based Hawkes) and neural (Transformer-based) TPPs by operating on their native transition matrix representations.
- **Mechanism:** For Hawkes process EM, BADMM replaces the standard E-step responsibility computation, taking the vanilla responsibility matrix as B₀ and outputting a structured version that guides M-step parameter updates. For Transformer TPPs (THP, SAHP), BADMM replaces softmax attention, taking QK^T/√d as initial scores and producing sparse attention maps. The module respects the lower-triangular constraint (causality) and row-normalization (probability interpretation) in both cases.
- **Core assumption:** Both responsibility matrices and attention maps encode equivalent information about event triggering relationships despite being derived from different computational paradigms.
- **Evidence anchors:**
  - [page 3] "the responsibility matrix in (4) works as the transition matrix of events... the attention maps... can be treated as evidence of event branches"
  - [page 5] "For Hawkes process, we can apply the above algorithm directly in the E-step... For neural TPPs, we unroll the iterations"
  - [corpus] No corpus papers validate this specific unification across TPP paradigms.
- **Break condition:** If attention maps in neural TPPs do not genuinely reflect branching structure (e.g., they capture other patterns), the regularization may impose incorrect structure.

## Foundational Learning

- **Concept: Temporal Point Processes (TPPs) and Hawkes Processes**
  - Why needed here: The entire method operates on TPP event sequences; understanding intensity functions λ(t) and the branching process interpretation is essential.
  - Quick check question: Given event sequence s = {(tₙ, cₙ)}, can you explain how a Hawkes process intensity λc(t) = μc + Σ acc' κ(t-tₙ') encodes triggering between event types?

- **Concept: ADMM (Alternating Direction Method of Multipliers)**
  - Why needed here: BADMM extends standard ADMM with Bregman divergences; understanding the primal-dual split, auxiliary variables, and convergence properties is required.
  - Quick check question: For problem min f(x) + g(z) s.t. Ax + Bz = c, can you write the ADMM updates for x, z, and the dual variable y?

- **Concept: Matrix Regularization (Sparsity and Low-rank)**
  - Why needed here: The method combines ℓ₁-norm (sparsity), nuclear norm (low-rank via convex relaxation), and ℓ₁,₂-norm (group sparsity); understanding their proximal operators is required.
  - Quick check question: What is the proximal operator for the nuclear norm, and why does it encourage low-rank solutions?

## Architecture Onboarding

- **Component map:**
  Input: B₀ (initial transition matrix)
  ↓
  [Initialize] X₁⁽⁰⁾ = X₂⁽⁰⁾ = B₀, Z₁⁽⁰⁾ = Z₂⁽⁰⁾ = 0
  ↓
  [For t = 1 to T iterations:]
    ├─ [B-update] σr((log B₀ + ρΣᵢ(log Xᵢ⁽ᵗ⁻¹⁾ - Zᵢ⁽ᵗ⁻¹⁾))/(1+2ρ))
    ├─ [X₁-update] Soft-threshold: S_{λα/ρ}(B⁽ᵗ⁾ + Z₁⁽ᵗ⁻¹⁾)
    ├─ [X₂-update] If nuclear norm: SVD + singular value thresholding
    │              If ℓ₁,₂: Column-wise soft-thresholding with τₙ scaling
    └─ [Dual updates] Zᵢ⁽ᵗ⁾ = Zᵢ⁽ᵗ⁻¹⁾ + (B⁽ᵗ⁾ - Xᵢ⁽ᵗ⁾)
  ↓
  Output: Structured transition matrix B⁽ᵀ⁾

- **Critical path:**
  1. Extract B₀ from base TPP (responsibility matrix from Hawkes E-step OR attention scores from Transformer)
  2. Apply BADMM iterations (T=2 for neural, iterate to convergence for EM)
  3. Use structured B for downstream tasks (parameter update in M-step OR replace attention output)

- **Design tradeoffs:**
  - **Nuclear norm (BADMM*) vs. ℓ₁,₂ (BADMM₁,₂):** Nuclear norm preserves local triggering structure better; ℓ₁,₂ produces extremely sparse matrices attributing sequences to few global key events. Paper reports BADMM* typically achieves better likelihood/accuracy.
  - **T (iterations):** T=2 balances compute and performance; more iterations yield diminishing returns.
  - **λ (regularization weight):** Controls sparsity/low-rank strength; paper shows robustness across 0.01–100 range but optimal varies by dataset.
  - **α (sparse/low-rank balance):** Paper uses α ∈ (0,1); grid search recommended.

- **Failure signatures:**
  - Transition matrix becomes diagonal only → α or λ too large, over-regularizing
  - No improvement over baseline → λ too small, regularization ineffective
  - Training instability in neural TPP → consider detaching X₂ gradient for nuclear norm (per paper)
  - Memory overflow → reduce T or use ℓ₁,₂ instead of nuclear norm (no SVD required)

- **First 3 experiments:**
  1. **Sanity check on synthetic data:** Train Hawkes process with known branching structure on the Conttime dataset; verify BADMM recovers ground-truth triggering patterns better than vanilla EM.
  2. **Ablation on regularization type:** Compare BADMM* vs. BADMM₁,₂ on a real dataset (e.g., Taobao); measure both log-likelihood and sparsity of inferred transition matrices.
  3. **Integration test with neural TPP:** Replace attention layer in THP with BADMM module; verify training converges and compare prediction accuracy against softmax baseline and Sinkhorn-based attention.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can an online BADMM algorithm be developed to mitigate the computational latency of the inference step during prediction?
  - **Basis in paper:** [explicit] The authors note that deriving attention maps requires solving an optimization problem, leading to a "time-consuming inference step," and propose applying an "online BADMM algorithm" as future work.
  - **Why unresolved:** The current module requires iterative optimization for the transition matrix, which hinders real-time application in high-frequency event streams.
  - **What evidence would resolve it:** Demonstration of a modified BADMM module that achieves lower time complexity per event without sacrificing the log-likelihood or structural interpretability of the transition matrix.

- **Open Question 2:** Does the BADMM module's structured regularization correspond to specific Bayesian priors?
  - **Basis in paper:** [explicit] The authors state an intent to "verify the rationality of our method from the viewpoint of Statistics and try to build connections between our BADMM module and Bayesian priors."
  - **Why unresolved:** The method is currently framed as an optimization-driven design without a rigorous statistical derivation linking the sparsity and low-rank constraints to a probabilistic generative process.
  - **What evidence would resolve it:** A theoretical derivation showing that the BADMM regularizers correspond to the negative log-prior of a specific probability distribution over the transition matrix.

- **Open Question 3:** Can the choice of structural regularizer (subspace clustering vs. sparse group-lasso) be adaptively determined based on data characteristics?
  - **Basis in paper:** [inferred] The paper implements two regularizers ($||\cdot||_*$ and $||\cdot||_{1,2}$) and observes that their relative performance varies across datasets, yet the selection is made via grid search.
  - **Why unresolved:** It is unclear what specific statistical properties of an event sequence dictate whether a nuclear norm or group-lasso constraint is more appropriate.
  - **What evidence would resolve it:** An analysis correlating dataset statistics (e.g., branching ratio variance) with regularizer performance, or an algorithm that dynamically weights these constraints.

## Limitations

- The paper's claims about cross-paradigm applicability lack direct empirical validation - the unification of responsibility matrices (Hawkes) and attention maps (neural TPPs) as equivalent representations of branching structure is asserted but not rigorously tested.
- The choice of T=2 iterations appears arbitrary without ablation studies showing convergence behavior.
- While synthetic data experiments are mentioned, specific quantitative results comparing BADMM to ground truth are not provided in the abstract.

## Confidence

- Mechanism 1 (sparse/low-rank regularization improves branch inference): Medium - supported by theoretical formulation but limited direct empirical validation
- Mechanism 2 (algorithm unrolling produces differentiable module): Medium - unrolling approach is standard but specific TPP application lacks corpus validation
- Mechanism 3 (generalization across TPP paradigms): Low - unification claim is theoretical without cross-paradigm empirical comparison

## Next Checks

1. Conduct synthetic data experiment with Conttime dataset showing quantitative comparison between BADMM-inferred transition matrices and ground-truth branching structure (precision/recall of triggering relationships)
2. Perform ablation study varying T (iterations) in the BADMM module to demonstrate convergence behavior and identify optimal iteration count for different TPP types
3. Execute cross-paradigm comparison experiment: apply BADMM to both Hawkes process (EM-based) and Transformer TPP (SGD-based) on identical datasets and compare ELL/ACC improvements to verify the claimed unification mechanism