---
ver: rpa2
title: Goal inference with Rao-Blackwellized Particle Filters
arxiv_id: '2512.09269'
source_url: https://arxiv.org/abs/2512.09269
tags:
- intent
- agent
- particles
- estimator
- particle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a Rao-Blackwellized Particle Filter (RBPF) approach
  for inferring the eventual goal of a mobile agent from noisy trajectory observations.
  The method exploits the assumed closed-loop dynamics of the agent to analytically
  marginalize the linear-Gaussian state substructure, updating only particle weights
  rather than sampling both state and intent parameters.
---

# Goal inference with Rao-Blackwellized Particle Filters

## Quick Facts
- arXiv ID: 2512.09269
- Source URL: https://arxiv.org/abs/2512.09269
- Reference count: 37
- Primary result: RBPF approach for inferring mobile agent goals from noisy trajectories, improving sample efficiency by analytically marginalizing linear-Gaussian state substructure

## Executive Summary
This work presents a Rao-Blackwellized Particle Filter (RBPF) approach for inferring the eventual goal of a mobile agent from noisy trajectory observations. The method exploits the assumed closed-loop dynamics of the agent to analytically marginalize the linear-Gaussian state substructure, updating only particle weights rather than sampling both state and intent parameters. This significantly improves sample efficiency compared to standard particle filters.

Two density-based estimators are introduced: a complete estimator using all particles and a reduced estimator using only effective particles. The performance of these estimators is quantified using information-theoretic leakage metrics, specifically Kullback-Leibler divergence. Computable lower bounds on this divergence are derived, showing that the reduced estimator performs nearly as well as the complete one while requiring less computation.

## Method Summary
The method implements a Rao-Blackwellized Particle Filter where particles represent agent intent (goal position, radius, arrival time) rather than state. For each particle, a Kalman filter tracks the agent's state conditioned on the particle's intent. Particle weights are updated via observation likelihood, and resampling occurs when effective sample size drops below threshold. Two estimators are constructed: complete (all particles) and reduced (only effective particles), with performance bounded using KL divergence lower bounds.

## Key Results
- Fast and accurate intent recovery, inferring agent goals well before trajectory midpoint
- Reduced estimator maintains lower final estimation errors while requiring less computation
- Computable lower bounds on KL divergence demonstrate estimator quality
- RBPF achieves sample efficiency gains over standard particle filters in continuous state spaces with nonlinear dynamics

## Why This Works (Mechanism)

### Mechanism 1: Analytic Marginalization of Linear-Gaussian State Substructure
- **Claim**: Sampling only intent parameters while analytically computing state estimates via Kalman filtering improves sample efficiency over standard particle filters that jointly sample both.
- **Mechanism**: The RBPF exploits conditional structure: given intent θ, the agent dynamics induce linear-Gaussian state evolution. Rather than Monte Carlo sampling both θ and state x, the filter samples θ particles and applies exact Kalman updates for state estimation per particle. This reduces estimation variance because the Kalman filter is optimal for linear-Gaussian systems.
- **Core assumption**: State evolution is conditionally linear-Gaussian given intent.
- **Evidence anchors**: Abstract states "Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure"; Section 3.1 notes "applying a Kalman Filter to the agent state-estimates corresponding to each particle instead of using the full Monte Carlo update reduces the computation load."
- **Break condition**: If process noise becomes non-Gaussian or dynamics become non-linear given intent, the Kalman filter is no longer optimal and variance reduction benefits degrade.

### Mechanism 2: Static Intent with Weight-Only Updates
- **Claim**: Treating intent parameters as time-invariant per particle enables computational savings by updating only weights, not propagating intent through dynamics.
- **Mechanism**: Intent θ = (goal position, radius, arrival time) is sampled once at particle initialization and held constant throughout its lifetime. Particle weights are updated via observation likelihood. This concentrates computation on hypothesis evaluation rather than state propagation.
- **Core assumption**: Agent intent does not change during the trajectory.
- **Evidence anchors**: Section 3.1, Remark 1 states "θ^(i)_{k+1} = θ^(i)_k unless the i-th particle is discarded during resampling"; Section 3 notes "the particles will represent intent, rather than state. Hence, they are not propagated through the filtering process, but the particle weights are updated."
- **Break condition**: If agent changes goals mid-trajectory, the static intent assumption fails; intent would need to be modeled as a dynamic latent variable with transition model.

### Mechanism 3: Reduced Estimator via Effective Sample Pruning
- **Claim**: Using only the top N_eff high-weight particles achieves nearly identical inference accuracy to the complete estimator while reducing computation.
- **Mechanism**: Effective sample size N_eff = (∑ω_i²)^(-1) measures weight concentration. The reduced estimator uses only top N_eff particles, renormalized. Theorem 1 bounds the performance gap |H^com - H^red| by terms involving (1/2 - 1/(2√(2N)))·log(C_ν), which shrinks with larger N.
- **Core assumption**: Low-weight particles contribute negligibly to posterior quality.
- **Evidence anchors**: Abstract states "The reduced estimator consistently maintains lower final estimation errors while requiring less computational resources"; Theorem 1 provides performance difference bound scaling with (1/2 - 1/(2√(2N))).
- **Break condition**: If true intent lies outside effective particle support (poor prior coverage or multimodal posteriors where effective particles capture only one mode), reduced estimator may miss critical hypotheses.

## Foundational Learning

### Concept 1: Rao-Blackwellization and Conditional Factorization
- **Why needed here**: Understanding why "analytic marginalization" improves over joint sampling requires recognizing that if p(x|θ) is tractable, sampling only θ and computing expectations analytically reduces variance.
- **Quick check question**: For a joint distribution p(x,θ) where p(x|θ) is Gaussian with known mean function μ(θ) and fixed covariance, write the Rao-Blackwellized estimator for E[f(x)] using samples {θ^(i)}.

### Concept 2: Particle Filter Weight Degeneracy and Effective Sample Size
- **Why needed here**: The paper uses N_eff to trigger resampling. Understanding why weights collapse and how N_eff quantifies this is essential.
- **Quick check question**: If weights are ω = [0.97, 0.01, 0.01, 0.01], what is N_eff? Why does this indicate degeneracy?

### Concept 3: KL Divergence Between Gaussian Mixtures
- **Why needed here**: The estimators are Gaussian mixture models. The paper uses lower bounds on KL divergence to quantify inference quality without computing intractable exact values.
- **Quick check question**: Why is KL(p||q) asymmetric? Why would computing exact KL between a single Gaussian and a GMM be intractable, requiring the bounds from [14]?

## Architecture Onboarding

### Component Map:
Input: Noisy observations y_k at times t_k
↓
Particle Set: {(θ^(i), ω^(i), x̂^(i), P^(i))} with θ^(i) = (goal_pos, radius, arrival_time), x̂^(i) = Kalman state estimate per particle
↓
Per-Timestep Pipeline:
1. Propagate x̂ via dynamics (θ unchanged)
2. Kalman update via Eq 7-9
3. Weight update via Eq 10
4. If N_eff < N_0: resample
↓
Estimators (Section 3.2):
- Complete: q̂^com = Σ ω^(i) q^(i) (Eq. 13)
- Reduced: q̂^red = Σ_{i∈M_k} (ω^(i)/ω_eff) q^(i)
Output: Intent distribution over Θ

### Critical Path:
1. Initialize N particles uniformly over Θ
2. Propagate state via Euler-integrate dynamics for each particle's state estimate
3. Kalman update: compute prior covariance, gain, posterior
4. Weight update: compute likelihood p(y_k|x̂_k^(i)) and normalize
5. Resampling check: if N_eff < N_0, retain top N_0 particles, replicate by weight, reinitialize remainder
6. Estimation: compute reduced or complete estimator, evaluate KL bounds

### Design Tradeoffs:
- **Particle count N**: Higher N improves coverage but increases computation linearly
- **Resampling threshold N_0**: Lower values preserve diversity; higher values trigger more aggressive pruning
- **σ parameters**: Larger σ smooths intent distributions; smaller σ improves precision but requires better particle coverage
- **Estimator choice**: Reduced estimator is faster with bounded accuracy loss (Theorem 1)

### Failure Signatures:
- **Weight collapse early**: N_eff → 1 before trajectory midpoint suggests observation model mismatch or insufficient particles
- **No KL decay**: Flat or increasing KL divergence indicates dynamics model doesn't match agent behavior
- **Wrong mode convergence**: Estimator converges to incorrect goal—possible causes: agent violates closed-loop assumption, prior doesn't cover true intent, or observation noise too high
- **Prior misspecification**: True θ* outside Θ domain guarantees failure

### First 3 Experiments:
1. **Baseline reproduction**: Implement with N=1200, 2D workspace, parameters from Section 5.1. Verify inference time ~3.28s and position error ~0.23m match Table 1. Plot KL divergence decay curve as in Figure 2.
2. **Particle count ablation**: Test N ∈ {200, 400, 800, 1200, 2000} across 100 trials. Measure inference time, final position error, and wall-clock time. Hypothesis: performance saturates above N≈1000.
3. **Noise robustness sweep**: Vary measurement noise covariance Δ and process noise σ_d. Identify threshold where inference time exceeds 50% of trajectory duration. This tests the "graceful degradation" claim from the abstract.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can intent-obfuscating controllers be designed to minimize KL-based leakage while maintaining task compliance?
- **Basis in paper**: [explicit] The abstract and conclusion explicitly state that the demonstrated inference capabilities "motivat[e] future work on designing intent-obfuscating controllers" that minimize leakage subject to constraints.
- **Why unresolved**: The current work develops the estimation (adversary) side of the problem, providing the metric (KL divergence) to be minimized, but does not synthesize the defensive control logic.
- **What evidence would resolve it**: A control synthesis method that utilizes the derived leakage bounds to actively reduce the information gain of an observer while ensuring the agent reaches the goal.

### Open Question 2
- **Question**: How does the inference framework perform under richer, non-linear motion models and non-Gaussian observation channels?
- **Basis in paper**: [explicit] The conclusion lists "richer motion models and non-Gaussian observation channels" as specific avenues for future work.
- **Why unresolved**: The current RBPF implementation relies on Assumption 1 (linear-Gaussian substructure) to analytically marginalize the state; non-Gaussian noise or highly non-linear dynamics may prevent the use of the Kalman update.
- **What evidence would resolve it**: Derivation of a modified filter or numerical integration method that handles non-Gaussian likelihoods without sacrificing the sample efficiency of the RBPF.

### Open Question 3
- **Question**: How can the method be extended to multi-agent settings where intents may be correlated or coupled?
- **Basis in paper**: [explicit] The conclusion identifies "extensions to multi-agent settings" as a goal for future research.
- **Why unresolved**: The current formulation assumes a single agent with a single intent tuple θ; multi-agent scenarios exponentially increase the dimensionality of the joint intent space, potentially degrading particle filter performance.
- **What evidence would resolve it**: A joint intent inference algorithm that scales to multiple agents and validation showing the KL bounds hold under cooperative or adversarial multi-agent dynamics.

## Limitations
- The method assumes static intent (single fixed goal), precluding goal-switching behavior during trajectories
- Performance relies on strong assumptions of linear-Gaussian substructure and known dynamics model
- The reduced estimator's performance guarantee depends on particle coverage assumptions and problem-specific constants

## Confidence
- **High Confidence**: RBPF framework implementation and basic inference performance (inference time ~3.28s, position error ~0.23m) are well-supported by experimental results and align with established particle filtering literature
- **Medium Confidence**: Reduced estimator's near-equivalent performance to complete estimator is supported by Theorem 1 and Table 1, but bound's tightness depends on problem-specific constants
- **Medium Confidence**: Information-theoretic metrics (KL divergence) and their use for quantifying inference quality are well-grounded, though lower bounds may not capture all aspects of estimator performance

## Next Checks
1. **Break Condition Testing**: Implement an agent that changes goals mid-trajectory. Measure how quickly the RBPF converges to the wrong goal and quantify the error to test framework's robustness to dynamic intent.

2. **Prior Coverage Analysis**: Systematically vary domain bounds Θ and particle count N to identify when true intent θ* falls outside particle support. Measure resulting inference degradation to understand prior misspecification failure mode.

3. **Non-Gaussian Process Noise**: Modify dynamics model to include non-Gaussian process noise (e.g., heavy-tailed distributions). Compare RBPF performance against standard particle filter to quantify breakdown of Kalman filter's variance reduction benefits.