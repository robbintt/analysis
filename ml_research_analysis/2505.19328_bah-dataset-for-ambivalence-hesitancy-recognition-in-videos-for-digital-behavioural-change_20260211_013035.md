---
ver: rpa2
title: BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Digital Behavioural
  Change
arxiv_id: '2505.19328'
source_url: https://arxiv.org/abs/2505.19328
tags:
- recognition
- participants
- dataset
- cues
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Behavioural Ambivalence/Hesitancy (BAH)
  dataset for multimodal recognition of ambivalence and hesitancy (A/H) in videos.
  The dataset contains 1,427 videos from 300 participants across Canada, annotated
  by experts to detect A/H segments using facial, vocal, and body language cues.
---

# BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Digital Behavioural Change

## Quick Facts
- arXiv ID: 2505.19328
- Source URL: https://arxiv.org/abs/2505.19328
- Reference count: 40
- Primary result: Dataset and baseline models for detecting ambivalence/hesitancy in videos using multimodal cues

## Executive Summary
This paper introduces the BAH dataset for recognizing ambivalence and hesitancy (A/H) in videos, a critical task for digital behavioral change interventions. The dataset contains 1,427 videos from 300 participants, annotated for A/H segments using expert coders who analyzed facial, vocal, and body language cues. Baseline experiments using ResNet, ViT, and multimodal fusion approaches show limited performance (AVGF1 around 0.51-0.63), highlighting the challenge of detecting this nuanced affective state. The dataset is publicly available to support further research in this emerging area.

## Method Summary
The BAH dataset was created from 300 Canadian participants providing videos discussing behavioral change goals. Videos were annotated by expert coders for A/H segments using a detailed codebook that defines ambivalence as affect conflict across or within modalities. Three modalities were extracted: visual (256×256 cropped faces), audio (Vggish log mel-spectrograms), and text (Whisper transcriptions with BERT features). Models were pre-trained on emotion datasets (RAF-DB, AffectNet, Aff-wild2) then fine-tuned on BAH with class imbalance handling through under-sampling. Temporal context was added using TCN, and multimodal fusion was tested using concatenation and co-attention mechanisms.

## Key Results
- Frame-level A/H detection achieves AVGF1 of 0.5165 (ResNet101+TCN) with AP of 0.2070
- Text modality alone outperforms visual (AVGF1 0.5497 vs 0.5165) and audio (AVGF1 0.4658)
- Video-level M-LLM with transcript achieves AVGF1 of 0.6341, outperforming supervised approaches
- Cross-modal inconsistencies (especially facial-language) occur in 42.6% of A/H cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A/H recognition depends on detecting affect conflict across modalities, where discordant signals indicate ambivalence.
- Mechanism: The model must compare affective content from facial, vocal, and textual channels; when these channels convey contradictory sentiment (e.g., saying "yes" while shaking head), the system identifies A/H.
- Core assumption: A/H primarily manifests through cross-modal inconsistency, which can be learned as a pattern distinct from unimodal emotion classification.
- Evidence anchors: Cross-modal inconsistencies are prevalent (42.6% facial-language conflicts); multimodal approaches outperform unimodal baselines; M-LLM with transcript achieves highest performance.

### Mechanism 2
- Claim: Temporal context is necessary for A/H recognition because ambivalence unfolds over brief windows (~4.29 seconds average), not single frames.
- Mechanism: Models process sequences of frames using temporal convolution (TCN) or sliding windows, capturing how affect fluctuates or conflicts within a modality over time.
- Core assumption: A/H has temporal structure; single-frame analysis loses the "between positive and negative orientations" dynamic.
- Evidence anchors: Adding TCN context improves AP from 0.1967→0.2070; A/H manifests as "sustained or fluctuating states" rather than peak moments.

### Mechanism 3
- Claim: Text modality is the strongest single predictor of A/H, likely because verbal hesitancy markers ("I want to... but...", filler words) are explicit linguistic signals.
- Mechanism: BERT-based text features capture hedging, filler words, and "but" constructions that directly signal ambivalence in speech content.
- Core assumption: Linguistic markers of A/H are more learnable from transcripts than subtle facial/body cues.
- Evidence anchors: Text alone achieves AVGF1=0.5497, outperforming visual (0.5165) and audio (0.4658); M-LLM with transcript achieves AVGF1=0.6341.

## Foundational Learning

- Concept: Multimodal Fusion for Subtle Emotions
  - Why needed here: A/H is not a basic emotion; it requires integrating conflicting signals across face, voice, body, and text. Standard fusion (concatenation) achieves AVGF1=0.5526, but cross-attention (JMT) drops to 0.5241.
  - Quick check question: Given two modalities with opposing sentiment (smiling face + hesitant speech), how should your fusion layer weight them?

- Concept: Temporal Modeling for Short Affective Segments
  - Why needed here: A/H segments average 4.29±2.45 seconds (~104 frames). Models must balance context length vs. noise; ablations show optimal windows around 5 seconds.
  - Quick check question: If your TCN uses a 240-frame (10-second) window, but A/H segments are typically 4 seconds, what proportion of input is potentially irrelevant?

- Concept: Class Imbalance in Frame-Level Detection
  - Why needed here: Only 17.04% of frames contain A/H; naive models predict "no A/H" and achieve AVGF1=0.4459 (close to visual baselines). Undersampling of negative class is required during training.
  - Quick check question: Your validation F1 is 0.70, but AP is 0.15. What does this disparity indicate about your model's confidence calibration?

## Architecture Onboarding

- Component map: Video Input → Face Crop (RetinaFace) → ResNet/ViT backbone → Visual features → [Fusion Layer: LFAN/CAN/MT/JMT] → [Temporal: TCN] → A/H classifier (binary)
- Critical path: 1) Pre-train visual backbone on emotion datasets (60 epochs, SGD), 2) Extract aligned audio/text features (Vggish hop=1/FPS, BERT word-level repeated per timestamp), 3) Fine-tune with undersampling, 4) Apply TCN (96-120 frame window), 5) Fuse using CAN/LFAN.
- Design tradeoffs: Cropped faces more stable (AVGF1=0.5028 vs 0.4781 full frame); concatenation fusion slightly outperforms co-attention; text modality dominates performance.
- Failure signatures: Low AP with moderate AVGF1 indicates poor positive class recall; multimodal fusion underperforms text-only suggests synchronization issues; subject-based generalization gap shows personalization needed.
- First 3 experiments: 1) Establish unimodal baselines (ResNet101, VGGish, BERT with TCN), 2) Ablate fusion strategies (CAN vs LFAN vs early/late fusion), 3) Analyze cross-modal conflict predictions to validate learned patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can specialized fusion mechanisms be designed to explicitly detect affect conflict across modalities rather than simply aggregating features?
- Basis in paper: Section 4 recommends "a dedicated fusion mechanism... to assess whether there is affect conflict cross-modalities," contrasting with standard concatenation methods.
- Why unresolved: Baseline fusion methods yielded modest performance (AP ~0.25) because they likely fail to model the specific "discord" between modalities that defines A/H.
- What evidence would resolve it: A framework that isolates per-modality affect and uses a conflict-detection module, demonstrating significant AP improvements over standard fusion baselines.

### Open Question 2
- Question: What temporal modeling architectures can effectively capture intra-modality conflict given the brief and variable duration of A/H segments?
- Basis in paper: Section 4 states "specialized temporal modelling should be used to detect within-modality conflict," noting that A/H manifests as "sustained or fluctuating states."
- Why unresolved: Experiments showed that while context helps, very large context windows degrade performance, suggesting current models fail to handle the variable-length temporal dynamics of A/H.
- What evidence would resolve it: A temporal model optimized for short, non-peaked fluctuations that outperforms the TCN baselines on frame-level recognition tasks.

### Open Question 3
- Question: Can source-free domain adaptation (SFUDA) techniques bridge the performance gap between unsupervised personalization and the supervised Oracle upper bound?
- Basis in paper: Appendix I.4.3 highlights a gap between the best SFUDA method (AVGF1 0.5174) and the Oracle baseline (AVGF1 0.5864).
- Why unresolved: The paper notes that expressiveness varies widely by participant, making personalization critical, yet current adaptation methods fail to fully capture individual user baselines without labeled target data.
- What evidence would resolve it: An SFUDA method that significantly narrows the gap to the Oracle performance on unseen test subjects without accessing source data.

## Limitations
- Dataset limited to Canadian English speakers, potentially missing cultural variations in ambivalence expression
- Annotation relies on self-reports which can introduce subjectivity and bias
- Narrow demographic representation (primarily younger, educated individuals) limits generalizability
- A/H is a relatively understudied concept compared to basic emotions, suggesting the field may not be mature enough

## Confidence

- **High Confidence**: Multimodal nature of A/H recognition is well-supported by dataset analysis showing cross-modal inconsistencies are prevalent (42.6% of conflicts are facial-language).
- **Medium Confidence**: Superiority of text modality is demonstrated empirically but may be dataset-specific given reliance on verbal hesitation markers.
- **Low Confidence**: Claim that cross-modal inconsistency is the primary manifestation of A/H may be overstated, as models aren't validated to specifically detect these inconsistencies.

## Next Checks

1. **Cross-cultural validation**: Test the trained BAH models on videos from different cultural backgrounds to assess whether cross-modal inconsistency detection generalizes beyond Canadian English speakers.

2. **Within-modality conflict analysis**: Systematically examine model predictions on segments where affect conflict exists within a single modality (e.g., facial expressions showing mixed emotions) to determine if the model can detect these cases without cross-modal cues.

3. **Model interpretability audit**: Use attention visualization and saliency maps to verify that models are actually focusing on the cross-modal inconsistencies described in the codebook when making A/H predictions, rather than exploiting other dataset biases.