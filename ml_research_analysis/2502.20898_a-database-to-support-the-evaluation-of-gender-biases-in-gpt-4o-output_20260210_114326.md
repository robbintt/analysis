---
ver: rpa2
title: A database to support the evaluation of gender biases in GPT-4o output
arxiv_id: '2502.20898'
source_url: https://arxiv.org/abs/2502.20898
tags:
- gender
- biases
- llms
- language
- standpoint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel database for evaluating gender biases
  in GPT-4o outputs, constructed using feminist standpoint theory. The approach makes
  explicit the normative assumptions in database construction, differing from purely
  descriptive datasets.
---

# A database to support the evaluation of gender biases in GPT-4o output

## Quick Facts
- arXiv ID: 2502.20898
- Source URL: https://arxiv.org/abs/2502.20898
- Reference count: 18
- Primary result: Introduces a publicly available database for evaluating gender biases in GPT-4o outputs using feminist standpoint theory

## Executive Summary
This paper presents a novel database for evaluating gender biases in GPT-4o outputs, constructed using feminist standpoint theory to make normative assumptions explicit. Unlike purely descriptive datasets, this approach acknowledges that evaluation frameworks themselves embed values and power structures. The database includes 16,720 prompts across pretest and main test phases, employing three bias assessment methods: explicit associations, predicted associations, and an adapted Implicit Association Test (IAT) using error counts. Made publicly available under a Creative Commons license, the database enables reproducible research while emphasizing marginalized perspectives, particularly non-binary gender identities. The key innovation lies in transparently integrating ethical frameworks into evaluation methodology, allowing researchers to critically examine biases in both LLM outputs and the tools used to evaluate them.

## Method Summary
The database was constructed by generating prompts across multiple contexts and iterations (7,504 in pretest, 9,216 in main test) using methods including open questions and bias assessments. The methodology employed feminist standpoint theory as the normative framework, making explicit the values embedded in the evaluation process. Three bias assessment approaches were used: explicit association tasks where GPT-4o categorized words by gender, predicted association tasks where the model self-reported on implicit biases using a 1-7 scale, and an adapted IAT-style task using error counts as a proxy for response timing unavailable in API calls. The database includes five terms each for Male, Female, and Non-binary gender categories, plus five terms each for trait categories (Rationality, Emotionality, Own-interest, Other-interest, Positive, Negative). Prompts were executed as individual API calls with conversation history passed as context, and the complete dataset is available at Hugging Face.

## Key Results
- Novel database publicly available under Creative Commons license for reproducible research
- Systematic integration of feminist standpoint theory into LLM bias evaluation methodology
- Three complementary bias assessment methods implemented across 16,720 prompts
- Explicit inclusion of non-binary gender categories alongside binary ones
- Transparent documentation of normative assumptions underlying evaluation framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly declaring normative frameworks in evaluation datasets exposes value assumptions that purely descriptive approaches obscure.
- Mechanism: The database construction process makes its standpoint-theory-derived values explicit (e.g., prioritizing marginalized perspectives, acknowledging non-binary identities), which allows researchers to trace how specific normative commitments shape both the prompts and the interpretation of bias. This contrasts with descriptive datasets that measure which values LLMs hold without interrogating whether those values are just.
- Core assumption: Evaluators can more rigorously critique an LLM's outputs when they understand the ethical lens through which the evaluation itself was constructed.
- Evidence anchors: [abstract] The approach makes explicit the normative assumptions in database construction, differing from purely descriptive datasets. [section] "Unlike traditional ethical frameworks...feminist standpoint theory explicitly situates knowledge production within broader social hierarchies."

### Mechanism 2
- Claim: Adapting psychological bias assessment methods (IAT, explicit association tests) to LLMs via API-based prompting can surface implicit stereotypical associations that explicit questioning misses.
- Mechanism: The database uses three complementary methods—explicit categorization tasks, predicted self-assessment of implicit biases, and adapted IAT-style word categorization—to probe GPT-4o from multiple angles. Error patterns in IAT-style tasks may indicate implicit associations between gender and trait categories.
- Core assumption: LLM error patterns in categorization tasks analogously reflect the cognitive dissonance measured by human IAT response-time differences.
- Evidence anchors: [section] "The number of incorrect categorizations across various iterations of the IAT chat was used to infer implicit associations." [section] "We included different bias assessment methods to obtain a variety of GPT-4o output related to gender biases."

### Mechanism 3
- Claim: Including non-binary gender categories alongside binary ones reveals bias patterns that binary-only evaluations erase.
- Mechanism: The database allocates five words each to Male, Female, and Non-binary categories and tests associations with positive/negative traits. This directly operationalizes standpoint theory's emphasis on marginalized perspectives and allows detection of harms that would be invisible in binary frameworks.
- Core assumption: Non-binary representation in evaluation prompts meaningfully captures real-world representational harms for this group.
- Evidence anchors: [abstract] The approach emphasizes including marginalized perspectives, particularly non-binary gender identities. [section] "Following standpoint theory's call for including perspectives of marginalized groups, we also tested associated negativity or positivity with a non-binary gender."

## Foundational Learning

- Concept: Feminist Standpoint Theory (Haraway's Situated Knowledges)
  - Why needed here: The entire database methodology rests on this epistemological framework. Understanding that knowledge is produced from particular perspectives (not objectively) is essential to grasp why the authors insist on making normative assumptions explicit.
  - Quick check question: Can you explain why the authors claim that treating an evaluation dataset as "neutral" might reinforce existing power structures?

- Concept: Implicit Association Test (IAT) Adaptation for LLMs
  - Why needed here: One of three core bias assessment methods. Understanding how human IAT works (response-time differences in stereotypical vs. anti-stereotypical pairings) clarifies why the authors use error counts as a substitute metric.
  - Quick check question: What is the key difference between how IAT measures bias in humans vs. how this database measures it in GPT-4o?

- Concept: Representational Harm
  - Why needed here: The database evaluates gender bias as representational harm—how LLM outputs reinforce stereotypes or denigrate groups—distinct from allocative harm (unfair resource distribution).
  - Quick check question: Why might testing whether GPT-4o associates non-binary terms with negative traits be considered an evaluation of representational harm?

## Architecture Onboarding

- Component map: Prompt Generation Module -> Context Induction Layer -> Bias Assessment Methods -> Output Repository
- Critical path: Define normative framework -> Derive desirable LLM characteristics -> Design compliance-testing prompts -> Execute across contexts/iterations -> Annotate against norms
- Design tradeoffs: Depth vs. breadth (only GPT-4o and gender bias), Explicit vs. implicit (direct questioning plus indirect methods), IAT adaptation (error counts replace response times)
- Failure signatures: High same-prompt variability across iterations, Universal safety-refusal responses, Inconsistent non-binary term recognition, No output difference across context induction prompts
- First 3 experiments:
  1. Replicate the explicit association task with original gender/trait pairs. Compare error/refusal rates to assess GPT-4o version stability.
  2. Run IAT-style task with a new trait pair (e.g., leadership vs. support). Compare stereotypical vs. anti-stereotypical error rates.
  3. Test context induction system prompts with a different LLM (Claude, Gemini) using identical prompts. Document whether standpoint-theory framing produces different outputs.

## Open Questions the Paper Calls Out

- How can research systematically trace and quantify political or ideological shifts in LLMs over time? (Section 4 explicitly asks this regarding potential "AI Culture War" and political influence on LLM design)
- Can standpoint-theory-based databases be adapted to verify legal compliance with regulations like the Digital Services Act (DSA)? (Section 4 questions how to evaluate compliance with DSA requirements)
- Is the error rate in LLM-based Implicit Association Tests (IATs) a valid proxy for measuring implicit bias? (Section 2.1.4 proposes error counts but does not validate this substitution)

## Limitations

- IAT adaptation uses error counts instead of response times, lacking validation that this substitution reliably indicates implicit bias
- Non-binary gender category representation may be inadequate due to tokenization gaps and training data scarcity
- Limited scope to GPT-4o alone restricts generalizability to other LLM architectures
- Evaluation methodology embeds standpoint-theory assumptions that create potential circularity risks

## Confidence

- Standpoint Theory Integration: High - The theoretical framework and its explicit normative commitments are clearly articulated and consistently applied
- Bias Assessment Methodology: Medium - The three-method approach is well-designed but faces validation challenges, particularly the IAT adaptation
- Database Utility for Research: Medium - The publicly available dataset enables reproducible research, though its GPT-4o-specific nature and methodological assumptions limit broader applicability
- Non-binary Representation: Low-Medium - While conceptually important, the five-term representation may inadequately capture non-binary identities

## Next Checks

1. Cross-LLM Validation: Test the complete database with three different LLM architectures (GPT-4o, Claude, Gemini) using identical prompts to determine whether standpoint-theory framing produces systematically different outputs.

2. IAT Error Pattern Analysis: Conduct a controlled experiment varying only response consistency parameters (temperature, top_p) while keeping prompts identical to isolate whether categorization error patterns correlate with implicit bias or are artifacts of sampling variability.

3. Non-binary Term Expansion: Replace the current five non-binary terms with an expanded, validated set from transgender/non-binary community sources and re-run the association tests to assess whether initial findings replicate or reveal tokenization/training data limitations.