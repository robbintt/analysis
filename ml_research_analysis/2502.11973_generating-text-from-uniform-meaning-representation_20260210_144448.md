---
ver: rpa2
title: Generating Text from Uniform Meaning Representation
arxiv_id: '2502.11973'
source_url: https://arxiv.org/abs/2502.11973
tags:
- generation
- data
- english
- graphs
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of generating natural language
  text from Uniform Meaning Representation (UMR) graphs, a recently developed multilingual
  semantic representation. The authors introduce three approaches: a baseline using
  existing AMR-to-text generation models directly on UMR graphs, a pipeline approach
  that first converts UMR to AMR before generation, and a fine-tuning approach for
  both foundation models and AMR-to-text models using UMR data.'
---

# Generating Text from Uniform Meaning Representation

## Quick Facts
- arXiv ID: 2502.11973
- Source URL: https://arxiv.org/abs/2502.11973
- Reference count: 13
- Primary result: Fine-tuned AMR-to-text models achieve mBERTscores of 0.825 (English) and 0.882 (Chinese) on UMR generation

## Executive Summary
This paper tackles the problem of generating natural language text from Uniform Meaning Representation (UMR) graphs, a recently developed multilingual semantic representation. The authors introduce three approaches: a baseline using existing AMR-to-text generation models directly on UMR graphs, a pipeline approach that first converts UMR to AMR before generation, and a fine-tuning approach for both foundation models and AMR-to-text models using UMR data. Their best-performing fine-tuned models achieve multilingual BERTscores of 0.825 for English and 0.882 for Chinese, demonstrating that fine-tuning AMR-to-text models on UMR data is effective even with limited UMR annotations.

## Method Summary
The authors evaluate three approaches for UMR-to-text generation: (1) a baseline that directly applies pretrained AMR-to-text models to UMR graphs without fine-tuning, (2) a pipeline approach that converts UMR graphs to AMR format before generation, and (3) fine-tuning both AMR-to-text models and foundation models on UMR data. They use SPRING2 and BiBL as their AMR-to-text models, and mT5, mBART, and Gemma 2B as foundation models. The best results come from fine-tuning AMR-to-text models exclusively on target-language UMR data, leveraging the structural similarity between UMR and AMR graphs. They use multilingual BERTscore as their primary metric, supplemented by BLEU, METEOR, and human evaluation for selected cases.

## Key Results
- Fine-tuned AMR-to-text models (SPRING2/BiBL) outperform both baseline and pipeline approaches on UMR generation
- English generation achieves mBERTscore of 0.825 and Chinese achieves 0.882 with monolingual fine-tuning
- Document-level UMR annotations improve sentence-level generation quality, even when generating individual sentences
- Monolingual fine-tuning outperforms multilingual fine-tuning, suggesting the "curse of multilinguality" effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMR-to-text models fine-tuned on UMR data outperform foundation models fine-tuned on the same data because structural priors from AMR pretraining transfer to UMR's graph-based semantics.
- Mechanism: Both AMR and UMR represent meaning as rooted, directed graphs with predicate-argument structures. Pretraining on AMR teaches models to decode graph-linearized representations into fluent text; this skill partially transfers despite UMR's additional features (aspect, modality, document-level coreference).
- Core assumption: The shared structural properties between AMR and UMR graphs enable positive transfer, even though UMR encodes more information.
- Evidence anchors:
  - [abstract] "Exploiting the structural similarity between UMR and AMR graphs and the wide availability of AMR technologies"
  - [section 6.2] "our fine-tuned AMR-to-text models outperform the fine-tuned foundation models such as mT5 and mBART"
  - [corpus] SAFT paper confirms structure-aware fine-tuning improves AMR-to-text generation, suggesting graph structure matters for decoding.

### Mechanism 2
- Claim: Document-level annotations improve sentence-level generation quality by providing coreference context that resolves ambiguous pronouns and entity references.
- Mechanism: UMR document-level graphs encode cross-sentence coreference links and alignments. Even when generating one sentence at a time, models exposed to document-level structure during training learn richer entity representations, reducing pronoun confusion.
- Core assumption: Document-level signals are learnable from limited data and generalize to held-out sentences.
- Evidence anchors:
  - [section 6.2] "the inclusion of document-level data appears beneficial, especially for SPRING2. This suggests that document-level information helps in the task of generation, even when only producing individual sentences."
  - [section 2] UMR "contains document-level information (enabling annotation of coreferential relations)"

### Mechanism 3
- Claim: Monolingual UMR fine-tuning outperforms multilingual fine-tuning for a target language due to the "curse of multilinguality" — capacity dilution across languages.
- Mechanism: Multilingual models share parameters across languages; adding training data from typologically distant languages (e.g., Sanapaná, Arápaho) may interfere with the target language's representations, especially with small datasets.
- Core assumption: The UMR data in non-target languages introduces noise rather than helpful cross-lingual signal for generation.
- Evidence anchors:
  - [section 6.2] "the best performing models are fine-tuned exclusively on the same language UMR data, suggesting that UMR data in other languages may not be helpful"
  - [section 6.2] "English generation quality drops slightly when trained on multilingual corpora compared to English-only data. This may be due to the 'curse of multilinguality'"

## Foundational Learning

- Concept: **Graph linearization (PENMAN notation)**
  - Why needed here: UMR and AMR graphs are linearized into parenthesized strings for sequence-to-sequence models. Understanding this encoding is essential for debugging input/output.
  - Quick check question: Given a UMR graph in PENMAN notation `(s / search-01 :ARG0 (p / person))`, can you identify the root concept and its argument?

- Concept: **Fine-tuning vs. from-scratch training**
  - Why needed here: The paper's central finding is that fine-tuning AMR-pretrained models on limited UMR data works better than training foundation models from scratch.
  - Quick check question: Why might a model pretrained on AMR generate better UMR text than mT5, even when both are fine-tuned on the same UMR data?

- Concept: **BLEU vs. BERTScore for generation evaluation**
  - Why needed here: The paper reports both; BERTScore correlates better with human judgments for semantic adequacy, while BLEU captures n-gram overlap.
  - Quick check question: If a model generates "He searched for clues" instead of "He was searching for a clue," which metric would penalize this more?

## Architecture Onboarding

- Component map: UMR graph (PENMAN string) -> [Option A] UMR→AMR Converter (rule-based) -> AMR-to-text model (BiBL/SPRING2) OR [Option B] Direct fine-tuned AMR-to-text model on UMR OR [Option C] Foundation model (mT5/mBART) fine-tuned on UMR -> Generated sentence

- Critical path: Start with SPRING2 or BiBL (AMR-pretrained), fine-tune on target-language UMR data (sentence + document level). Use mBERTScore for automatic evaluation, human evaluation for validation.

- Design tradeoffs:
  - Baseline (no fine-tuning) is fast but produces UMR-specific artifacts ("refer-number singular" appearing in output).
  - Pipeline (UMR→AMR→text) removes artifacts but loses UMR-specific semantics (aspect, modality, pronoun features).
  - Fine-tuning requires limited UMR data (~100-236 sentences) and yields best results.

- Failure signatures:
  - Baseline: UMR attribute names appear literally in generated text (e.g., "full-affirmative, though, the first singular thought...").
  - Pipeline: All third-person pronouns collapse to "they"; tense/aspect information lost.
  - Indigenous languages: Output appears to have correct script but is ungrammatical/nonsensical (human evaluation required).
  - Pronoun inconsistency: Models switch pronouns mid-document when referring to same entity.

- First 3 experiments:
  1. **Reproduce baseline failure modes**: Pass UMR graphs through unmodified BiBL; verify that UMR-specific terms appear in output (mBERTScore ~0.70).
  2. **Validate pipeline conversion**: Convert UMR→AMR, verify SMATCH score against held-out AMR/UMR pairs (~0.63 expected), then generate text and check pronoun collapse.
  3. **Fine-tune SPRING2 on monolingual UMR**: Train on English sentence + document data (100 examples, 30 epochs, lr=1e-4); target mBERTScore ≥0.82. Compare against multilingual fine-tuning to confirm "curse of multilinguality" pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can UMR-to-text generation models be developed to produce grammatical and semantically meaningful output for morphologically complex indigenous languages (Arápaho, Navajo, Sanapaná, Kukama)?
- Basis in paper: [explicit] The authors state: "We thus opt against a full human evaluation or automatic evaluation for the four indigenous languages, with the understanding that even our top-performing models output gibberish for these languages. This is likely due to their morphological complexity, necessitating additional resources for coherent generation."
- Why unresolved: Despite achieving mBERTscores of 0.673-0.816 for indigenous languages, native speaker evaluation confirmed outputs are "nonsensical and ungrammatical," and current approaches fail to handle their morphological complexity.
- What evidence would resolve it: Development of language-specific morphological resources or analyzers, collection of additional training data, architectural modifications for agglutinative/polysynthetic morphology, and human evaluation by native speakers confirming grammaticality and semantic adequacy.

### Open Question 2
- Question: What alternative architectures could better encode UMR graph structure without requiring linearization?
- Basis in paper: [explicit] The authors state in the Limitations section: "Our approaches use sequence-to-sequence architectures with linearized graphs to leverage existing AMR technologies given limited UMR data. Future work might explore additional architectures such as graph neural networks which directly encode graph structure."
- Why unresolved: Linearizing UMR graphs into PENMAN notation loses structural information and may not be optimal for capturing full graph semantics. No comparison against graph-based approaches was conducted.
- What evidence would resolve it: Comparative experiments with GNN-based models trained on identical UMR data, evaluation of how well different encoding methods preserve graph structural information, and performance comparisons across languages with different structural properties.

### Open Question 3
- Question: How would dedicated document-level UMR-to-text generation compare to the current sentence-level approach that incorporates document-level information?
- Basis in paper: [inferred] The paper finds document-level information improves performance and models "are able to leverage the document-level information contained in UMR graphs for better output despite only generating a single sentence at a time," yet never tests true multi-sentence document generation.
- Why unresolved: UMR was designed to include document-level information and coreference relations, but experiments only generate single sentences. The potential for coherent document generation remains unexplored.
- What evidence would resolve it: Experiments generating full documents from UMR document-level graphs, evaluation of cross-sentence coherence and coreference resolution accuracy, and comparison of discourse-level phenomena between generated and reference documents.

## Limitations

- Automatic metrics (BLEU, BERTScore) are unreliable for indigenous languages due to script/phonological differences and lack of standardized NLP tooling
- The UMR→AMR conversion process involves 65+ role mappings that are not fully specified, creating reproducibility challenges
- Current approaches only generate single sentences despite UMR's document-level annotations; true document-level generation remains unexplored

## Confidence

- **High confidence**: Fine-tuning AMR-to-text models on UMR data improves generation quality over baseline and pipeline approaches
- **Medium confidence**: Document-level annotations improve generation quality even for sentence-level generation
- **Medium confidence**: Monolingual fine-tuning outperforms multilingual fine-tuning due to "curse of multilinguality"
- **Low confidence**: Automatic metrics accurately reflect generation quality for indigenous languages

## Next Checks

1. **Reproduce baseline failure modes**: Run unmodified SPRING2/BiBL on UMR graphs and verify UMR-specific terms ("refer-number singular", "full-affirmative") appear in output text.

2. **Validate UMR→AMR conversion**: Implement and test the conversion pipeline on held-out AMR3.0 pairs, measuring SMATCH score against the expected ~0.63.

3. **Human evaluation on indigenous languages**: Conduct fluency/adequacy human evaluation on Arápaho/Navajo outputs to validate whether high automatic scores correspond to meaningful text generation.