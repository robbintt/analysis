---
ver: rpa2
title: 'Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in
  Generating Harmful Audio'
arxiv_id: '2511.10913'
source_url: https://arxiv.org/abs/2511.10913
tags:
- harmful
- audio
- attacks
- text
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the underexplored threat of using large text-to-speech
  (TTS) models to generate audio containing harmful speech content, bypassing safety
  mechanisms. The authors propose HARMGEN, a suite of five attacks organized into
  two families: semantic obfuscation techniques (Concat, Shuffle) that conceal harmful
  content within text, and audio-modality exploits (Read, Spell, Phoneme) that inject
  harmful content through auxiliary audio channels while maintaining benign textual
  prompts.'
---

# Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio

## Quick Facts
- arXiv ID: 2511.10913
- Source URL: https://arxiv.org/abs/2511.10913
- Reference count: 40
- This paper demonstrates how large text-to-speech models can be exploited to generate harmful audio content by bypassing safety mechanisms through semantic obfuscation and audio-modality exploits.

## Executive Summary
This paper addresses the underexplored threat of using large text-to-speech (TTS) models to generate audio containing harmful speech content, bypassing safety mechanisms. The authors propose HARMGEN, a suite of five attacks organized into two families: semantic obfuscation techniques (Concat, Shuffle) that conceal harmful content within text, and audio-modality exploits (Read, Spell, Phoneme) that inject harmful content through auxiliary audio channels while maintaining benign textual prompts. Through extensive evaluation across five commercial LALMs-based TTS systems and three datasets spanning two languages, the attacks significantly reduced refusal rates and increased the toxicity of generated speech. Reactive defenses like deepfake audio detection and text moderation proved ineffective, while proactive moderation by TTS providers detected 57-93% of attack instances. The work highlights critical vulnerabilities in TTS safety mechanisms and underscores the need for robust cross-modal safeguards throughout model training and deployment.

## Method Summary
The authors developed HARMGEN, a comprehensive attack suite targeting large TTS models through five distinct techniques. The semantic obfuscation attacks (Concat and Shuffle) manipulate text structure to hide harmful content while maintaining superficial benign appearance. The audio-modality exploits (Read, Spell, Phoneme) leverage auxiliary audio channels to inject harmful content through pronunciation-based manipulations. The evaluation spanned five commercial LALM-based TTS systems across three datasets in two languages, measuring attack success through refusal rate reduction and toxicity increase. The study also assessed defensive measures including deepfake audio detection, text moderation, and proactive moderation by TTS providers.

## Key Results
- Semantic obfuscation attacks (Concat, Shuffle) achieved significant success rates by concealing harmful content within benign-appearing text
- Audio-modality exploits (Read, Spell, Phoneme) successfully injected harmful content through auxiliary audio channels while maintaining benign textual prompts
- Proactive moderation by TTS providers detected 57-93% of attack instances, while reactive defenses like deepfake detection and text moderation proved largely ineffective

## Why This Works (Mechanism)
The attacks exploit fundamental vulnerabilities in how TTS systems process and synthesize speech. Semantic obfuscation works by leveraging the models' tendency to process text holistically rather than strictly following semantic coherence, allowing harmful content to be embedded within seemingly benign prompts. The audio-modality exploits take advantage of the separation between text processing and audio synthesis stages, where auxiliary audio channels can override or supplement the intended speech output. This cross-modal vulnerability is particularly concerning because it bypasses traditional text-based safety mechanisms entirely.

## Foundational Learning
- Large Audio-Language Models (LALMs): Multimodal models that process both text and audio inputs for speech synthesis; needed for understanding modern TTS architectures; quick check: verify model can accept both modalities as input
- Text-to-Speech Safety Mechanisms: Traditional guardrails designed to prevent harmful content generation through text analysis; needed for understanding attack surface; quick check: confirm model has content filtering in place
- Semantic Obfuscation: Technique of hiding harmful intent within seemingly benign text; needed for understanding how attacks bypass text analysis; quick check: verify if obfuscated text passes safety filters
- Audio Modality Exploitation: Using auxiliary audio channels to inject content that text analysis cannot detect; needed for understanding cross-modal attack vectors; quick check: confirm audio synthesis operates independently of text safety checks
- Toxicity Measurement: Quantitative assessment of harmful content in generated speech; needed for evaluating attack effectiveness; quick check: establish baseline toxicity levels before and after attacks
- Proactive vs Reactive Defense: Proactive defenses involve pre-deployment safeguards while reactive defenses respond to detected threats; needed for understanding defensive landscape; quick check: verify defense type being tested

## Architecture Onboarding

Component Map:
LALM-based TTS -> Text Processing -> Safety Analysis -> Audio Synthesis -> Output

Critical Path:
Text input → Semantic analysis → Safety filtering → Audio synthesis → Speech output

Design Tradeoffs:
The architecture prioritizes flexibility and expressiveness in speech generation over strict safety enforcement, creating vulnerabilities that attackers can exploit through semantic manipulation and cross-modal injection.

Failure Signatures:
Reduced refusal rates, increased toxicity scores, successful generation of harmful content despite benign textual prompts, bypass of traditional text-based safety mechanisms.

First Experiments:
1. Test Concat attack on baseline system with simple harmful-but-benign text to establish refusal rate
2. Apply Shuffle attack with varying obfuscation complexity to measure success rate correlation
3. Implement Read attack with auxiliary audio channel to verify cross-modal injection capability

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on commercial LALM-based systems, limiting generalizability to open-source or differently architected TTS models
- Attack techniques require varying levels of control over text input and post-processing, which may not be available in all deployment contexts
- Defensive measures tested are primarily reactive, with limited exploration of how attackers might adapt to these defenses over time

## Confidence
- High: Effectiveness of proposed attack techniques against tested commercial LALM-based TTS systems
- Medium: Generalizability of attack techniques to other TTS architectures beyond commercial LALMs
- Low: Long-term effectiveness of defensive measures given the adaptive nature of potential attackers

## Next Checks
1. Test the attack techniques against open-source TTS models to assess generalizability beyond commercial LALMs
2. Evaluate the attacks in scenarios where attackers have limited control over text input and post-processing
3. Assess the temporal effectiveness of defenses by conducting repeated evaluations after defensive updates to measure adaptive capabilities