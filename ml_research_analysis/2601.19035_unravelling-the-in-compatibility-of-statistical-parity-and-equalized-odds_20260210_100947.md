---
ver: rpa2
title: Unravelling the (In)compatibility of Statistical-Parity and Equalized-Odds
arxiv_id: '2601.19035'
source_url: https://arxiv.org/abs/2601.19035
tags:
- statistical-parity
- fairness
- groups
- measures
- equalized-odds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the (in)compatibility of two statistical
  fairness measures, Statistical-Parity and Equalized-Odds, when there is base-rate
  imbalance among sensitive social groups in binary classification. Using a novel
  analytical approach, the authors show that enforcing both measures requires either
  having base-rate balance or adopting a random classifier.
---

# Unravelling the (In)compatibility of Statistical-Parity and Equalized-Odds

## Quick Facts
- arXiv ID: 2601.19035
- Source URL: https://arxiv.org/abs/2601.19035
- Authors: Mortaza S. Bargh; Sunil Choenni; Floris ter Braak
- Reference count: 36
- Key outcome: Statistical-Parity and Equalized-Odds are incompatible under base-rate imbalance unless using a random classifier

## Executive Summary
This paper analyzes the theoretical incompatibility between two fundamental statistical fairness measures in binary classification: Statistical-Parity (equal posterior probabilities across groups) and Equalized-Odds (equal false positive and false negative rates across groups). Using a binary channel model with objective ground-truth labels, the authors prove that satisfying both measures simultaneously requires either balanced base-rates across sensitive groups or adopting a random classifier. The analysis reveals that enforcing Statistical-Parity alone, without examining base-rate imbalance, can introduce disparities in error rates between groups.

## Method Summary
The paper employs a binary channel model where the classifier's operation is characterized by four outcome types (TP, FP, TN, FN) for each sensitive group. The core analytical approach derives relationships between base-rates (p_s), posterior probabilities (q_s), and error rates (FPR_s, TPR_s) using the equation q_s = p_s·TPR_s + (1-p_s)·FPR_s. The authors visualize these relationships on an FPR-TPR plane where each group's operation points lie on parallel lines with different slopes determined by base-rates. Theorems 1 and 2 formally establish that Statistical-Parity and Equalized-Odds can only be simultaneously satisfied under specific conditions (balanced base-rates or random classification).

## Key Results
- Statistical-Parity and Equalized-Odds are incompatible under base-rate imbalance unless using a random classifier
- Enforcing Statistical-Parity alone can introduce disparities in error rates between groups
- Equalized-Odds compliance with unequal base-rates produces unequal posterior probabilities, violating Statistical-Parity
- Graphical representation shows trade-offs between measures, demonstrating that efficient classifiers require making choices between fairness criteria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When base-rates differ between protected and unprotected groups, satisfying both Statistical-Parity and Equalized-Odds simultaneously requires either balanced base-rates or a random classifier.
- Mechanism: The relationship q_s = p_s·TPR_s + (1-p_s)·FPR_s creates parallel lines L_0 and L_1 in the FPR-TPR plane. With unequal base-rates (p_0 ≠ p_1), these lines have different slopes (1 - 1/p_s), intersecting only at the point (q*, q*) on the ROC chance line where TPR = FPR = q*, which corresponds to random classification.
- Core assumption: Binary classification with objectively labeled ground truth; sensitive attribute S takes binary values; frequentist probability definitions apply.
- Evidence anchors:
  - [abstract]: "Using a binary channel model, the authors derive analytical relationships showing that satisfying both fairness criteria simultaneously requires either balanced base-rates or a random classifier."
  - [Theorem 1, Section 4.1]: "When both Equalized-Odds measures are established... then having also Statistical-Parity requires that p_0 = p_1 OR TPR* = FPR*"
  - [corpus]: Weak direct corpus support; related work (Fairmetrics R package, Local Statistical Parity) discusses fairness evaluation methods but not this specific incompatibility theorem.
- Break condition: Base-rates are equal across groups (p_0 = p_1), which eliminates the slope differential between lines L_0 and L_1.

### Mechanism 2
- Claim: Enforcing Statistical-Parity alone, without examining base-rate imbalance, can introduce disparities in error rates (FPR and FNR) between groups.
- Mechanism: Setting q_0 = q_1 = q* constrains operation points to lie on parallel lines L_0 and L_1 with different slopes. A classifier operating above the ROC chance line will achieve different (FPR, TPR) coordinates for each group while maintaining equal posterior probabilities q*, creating FPR and TPR inequalities.
- Core assumption: The classifier has sufficient discriminative power to operate above the ROC chance line (TPR > FPR).
- Evidence anchors:
  - [abstract]: "The analysis reveals that enforcing Statistical-Parity may lead to disparities in Equalized-Odds metrics (such as false positive and false negative rates)"
  - [Case II, Section 5.1]: "enforcing Statistical-Parity does not necessarily lead to having Equalized-Odds in place (i.e., having FPR equality or TPR equality) when there is base-rate imbalance"
  - [corpus]: No direct corpus confirmation of this specific mechanism; related papers focus on mitigation strategies rather than analytical derivations.
- Break condition: Classifier operates exactly on the ROC chance line (random classifier), where FPR = TPR = q for all groups.

### Mechanism 3
- Claim: Equalized-Odds compliance with unequal base-rates produces unequal posterior probabilities (q_0 ≠ q_1), violating Statistical-Parity.
- Mechanism: When FPR_0 = FPR_1 = FPR* and TPR_0 = TPR_1 = TPR*, the posterior probabilities become q_0 = p_0·TPR* + (1-p_0)·FPR* and q_1 = p_1·TPR* + (1-p_1)·FPR*. With p_0 ≠ p_1 and TPR* ≠ FPR*, the linear combinations yield q_0 ≠ q_1.
- Core assumption: Classifier performance differs from random (TPR* ≠ FPR*); objective ground-truth labels are available and reliable.
- Evidence anchors:
  - [Section 4.1]: Substituting FPR* and TPR* into equations (13) and (14) yields "q_0 = p_0·TPR* + (1-p_0)·FPR*" and similar for q_1
  - [Case III/IV, Section 5.1]: "Although here FPR and TPR equalities are established, the Statistical-Parity cannot be met, i.e., q_0 ≠ q_1"
  - [corpus]: Weak support; corpus papers address fairness evaluation broadly but do not derive this specific incompatibility.
- Break condition: TPR* = FPR* (random classifier), which equalizes q_0 = q_1 = FPR* = TPR* regardless of base-rates.

## Foundational Learning

- Concept: **Statistical-Parity (Demographic Parity)**
  - Why needed here: This is the baseline fairness measure that does not require ground truth; understanding its definition (q_0 = q_1) is essential to grasp why it conflicts with Equalized-Odds under base-rate imbalance.
  - Quick check question: If a classifier predicts positive outcomes for 30% of both protected and unprotected groups, does Statistical-Parity hold? (Answer: Yes, by definition, regardless of base-rates or error rates.)

- Concept: **Equalized-Odds (FPR and TPR equality)**
  - Why needed here: This measure requires ground truth and captures error-rate parity; the paper's core argument is that satisfying FPR_0 = FPR_1 and TPR_0 = TPR_1 forces q_0 ≠ q_1 when base-rates differ.
  - Quick check question: If FPR_0 = 0.2, FPR_1 = 0.2, TPR_0 = 0.7, and TPR_1 = 0.7, do Equalized-Odds hold? (Answer: Yes, both FPR and TPR equality are satisfied.)

- Concept: **Base-rate (prior probability p_s)**
  - Why needed here: Base-rate imbalance (p_0 ≠ p_1) is the root cause of the incompatibility; without understanding base-rates as the fraction of positive ground-truth outcomes per group, the slope differential in lines L_0 and L_1 cannot be interpreted.
  - Quick check question: If a dataset has 1000 protected-group members with 100 positive outcomes and 3000 unprotected-group members with 1500 positive outcomes, what are the base-rates? (Answer: p_1 = 0.1, p_0 = 0.5.)

## Architecture Onboarding

- Component map:
  - Binary channel model: Input = ground truth Y (0/1), Output = prediction Ŷ (0/1), Sensitive attribute S splits flow into two parallel channels
  - Four outcome types per group: TP (Y=1, Ŷ=1), FN (Y=1, Ŷ=0), FP (Y=0, Ŷ=1), TN (Y=0, Ŷ=0)
  - Derived rates per group s: FPR_s = FP/(FP+TN), TPR_s = TP/(TP+FN), base-rate p_s = (TP+FN)/N_s, posterior q_s = (TP+FP)/N_s

- Critical path:
  1. Compute base-rates p_0 and p_1 from ground-truth labels per sensitive group
  2. If p_0 ≈ p_1, Statistical-Parity and Equalized-Odds may be compatible; proceed with standard fairness assessment
  3. If p_0 ≠ p_1, assess which fairness measure is contextually required (legal frameworks often mandate Statistical-Parity; judicial contexts may require Equalized-Odds)
  4. Visualize operation points on FPR-TPR plane using lines L_0 and L_1 per equation (18) to identify feasible trade-off regions

- Design tradeoffs:
  - Accuracy vs. fairness compatibility: Efficient classifiers (above ROC chance line) cannot satisfy both measures; accept either FPR/TPR inequality or q inequality
  - Ground-truth dependency vs. practical feasibility: Statistical-Parity is usable without ground truth but may mask error-rate disparities; Equalized-Odds requires reliable labels
  - Group thresholding: Using different classification thresholds per group can achieve Statistical-Parity (Case II) but violates Equalized-Odds

- Failure signatures:
  - Statistical-Parity achieved with FPR_0 ≪ FPR_1: Protected group experiences higher false positive rate (potential harm from wrongful positive predictions)
  - Equalized-Odds achieved with q_0 > q_1: Unprotected group receives higher rate of positive predictions (potential disparate impact)
  - Classifier operates on ROC chance line: Achieves both measures but provides no predictive value beyond random guessing

- First 3 experiments:
  1. **Base-rate audit**: For any dataset with sensitive attributes, compute p_s per group before training; document whether p_0/p_1 ratio exceeds a threshold (e.g., >1.5) signaling incompatibility risk
  2. **Operation point mapping**: Train classifier, compute (FPR_s, TPR_s, q_s) per group, plot on FPR-TPR plane with L_0 and L_1 lines to visualize which measure(s) are satisfied and where trade-offs occur
  3. **Selective threshold analysis**: If Statistical-Parity is legally required, vary per-group thresholds to achieve q_0 = q_1, then measure resulting FPR and FNR disparities; document error-rate inequality as a secondary fairness report

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the incompatibility relationships between Statistical-Parity and Equalized-Odds generalize to non-binary classification scenarios with categorical sensitive attributes or soft output predictions?
- Basis in paper: [explicit] The conclusion states: "One direction is to extend the results to non-binary classification scenarios where the sensitive attribute and/or the target attribute is categorical or the target attribute is a soft output."
- Why unresolved: The analytical framework developed in this paper relies on binary classification with binary sensitive groups, and the linear relationships derived may not directly transfer to multi-class settings.
- What evidence would resolve it: Derivation of analogous theorems for multi-class or continuous settings, or empirical validation showing whether base-rate imbalance similarly causes incompatibility in non-binary contexts.

### Open Question 2
- Question: What principled guidelines can be developed for practitioners to navigate trade-offs between Statistical-Parity and Equalized-Odds when both cannot be satisfied?
- Basis in paper: [explicit] The conclusion notes: "As another direction, guidelines can be developed for making trade-offs between Statistical-Parity and Equalized-Odds measures."
- Why unresolved: While the paper provides graphical tools for visualizing trade-offs, it does not prescribe how to decide which measure to prioritize in specific application contexts.
- What evidence would resolve it: A framework or decision procedure that maps application characteristics (e.g., availability of ground truth, cost of false positives vs. false negatives, legal requirements) to recommended trade-off strategies.

### Open Question 3
- Question: What incompatibility relationships exist among other statistical fairness measures beyond Statistical-Parity and Equalized-Odds?
- Basis in paper: [explicit] The conclusion motivates: "further research on and specification of the incompatibility forms that exist among other statistical fairness measures."
- Why unresolved: The paper focuses on only two measures; many other fairness definitions (e.g., Predictive-Parity, Counterfactual Fairness, Individual Fairness) may have complex interdependencies.
- What evidence would resolve it: Systematic analysis applying similar binary channel modeling to other fairness measure pairs, cataloging conditions under which they are compatible or mutually exclusive.

### Open Question 4
- Question: How does the analysis change when different classifiers (with different ROC curves) are used for different sensitive groups rather than a single classifier?
- Basis in paper: [inferred] Section 5.1 briefly mentions: "One can take a step further... and, instead of having one classifier, use two different classifiers (i.e., with different ROC curves) for these groups" but does not elaborate.
- Why unresolved: The paper's theorems assume equalized odds across groups using the same classifier; using group-specific classifiers could potentially satisfy both fairness criteria but raises other concerns.
- What evidence would resolve it: Extension of Theorems 1 and 2 to cases with group-specific ROC curves, analyzing whether such approaches can circumvent the incompatibility without resorting to random classifiers.

## Limitations

- The theoretical framework assumes binary classification with objective ground truth labels, which may not hold in real-world scenarios where ground truth is ambiguous or unavailable
- The analysis focuses on group-level fairness measures and does not address individual fairness considerations
- The paper does not provide empirical validation with real datasets beyond the illustrative example

## Confidence

- High confidence in the mathematical derivations and proofs (Theorems 1 and 2) due to the clear analytical approach and step-by-step reasoning
- Medium confidence in the practical implications, as the analysis is primarily theoretical and requires careful interpretation when applied to specific contexts
- Medium confidence in the completeness of the solution space characterization, as the paper focuses on the binary case and does not extensively explore multi-class extensions

## Next Checks

1. Apply the analytical framework to real-world datasets with documented base-rate imbalances (e.g., COMPAS recidivism data) to verify that the theoretical incompatibility manifests in practice
2. Implement the FPR-TPR visualization methodology for multiple classifiers across different datasets to establish whether the graphical trade-off patterns consistently emerge
3. Conduct a legal/regulatory analysis to determine which fairness measure (SP or EO) is required in different jurisdictions and use cases, then validate whether practitioners are appropriately selecting based on base-rate conditions