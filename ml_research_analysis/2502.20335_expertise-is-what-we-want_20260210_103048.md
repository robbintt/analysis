---
ver: rpa2
title: Expertise Is What We Want
arxiv_id: '2502.20335'
source_url: https://arxiv.org/abs/2502.20335
tags:
- clinical
- cancer
- guidelines
- patient
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the effectiveness of a Large Language Expert
  (LLE) architecture that combines LLMs with rule-based expert systems to automate
  clinical decision support. The LLE architecture was applied to identify pre-treatment
  workup gaps for cancer patients by translating clinical guidelines into structured
  knowledge bases and applying them to unstructured health records.
---

# Expertise Is What We Want

## Quick Facts
- arXiv ID: 2502.20335
- Source URL: https://arxiv.org/abs/2502.20335
- Reference count: 21
- This study demonstrates a hybrid LLM-expert system architecture that achieves 97.9% extraction accuracy and 95.5% recommendation accuracy for cancer workup recommendations.

## Executive Summary
This study presents a Large Language Expert (LLE) architecture that combines the flexibility of Large Language Models with the interpretability of rule-based expert systems to automate clinical decision support. The system translates clinical guidelines into structured knowledge bases and applies them to unstructured health records to identify pre-treatment workup gaps for cancer patients. In a retrospective study of 100 breast and colon cancer cases, the system achieved high accuracy with only 2.1% of extracted clinical decision factors and 4.5% of workup recommendations requiring clinician adjustments, while enabling review completion in under 7.5 minutes per patient.

## Method Summary
The LLE system uses a two-stage pipeline: first, an LLM extracts clinical decision factors from unstructured patient records by answering human-readable questions with yes/no/unknown answers plus citations; second, a deterministic first-order logic evaluator applies guideline-based rules to these factors to generate workup recommendations. Clinical guidelines are translated into structured knowledge bases using first-order logic, enabling human-readable explanations and deterministic evaluation. The system incorporates human-in-the-loop review at each stage, with clinicians able to correct extracted factors before recommendations are generated, preventing error compounding through the system.

## Key Results
- 97.9% of extracted clinical decision factors required no clinician adjustment (2.1% correction rate across 12,532 factors)
- 95.5% of workup recommendations required no clinician adjustment (4.5% correction rate across 2,971 recommendations)
- Non-specialist clinicians completed the review process in under 7.5 minutes per patient case

## Why This Works (Mechanism)

### Mechanism 1: Structured Knowledge Base with First-Order Logic Translation
Translating clinical guidelines into first-order logic enables deterministic evaluation while maintaining human-readability for expert review. Guidelines are parsed into (1) possible recommendations, (2) decision factors, and (3) rules expressed as first-order logic formulas. This separation allows LLMs to handle natural language understanding while rules execute deterministically. The approach assumes clinical guidelines can be faithfully represented as discrete decision factors and boolean logic without unacceptable loss of nuance.

### Mechanism 2: Two-Stage Extraction-to-Evaluation Pipeline
Separating factor extraction from rule evaluation localizes errors and enables targeted clinician intervention. Stage 1 uses LLM calls with function tools to extract yes/no/unknown answers for each decision factor with citations. Stage 2 applies deterministic logic to recommend workups. Errors in extraction don't propagate unchecked—clinicians correct at the factor level. This assumes LLMs can reliably extract structured answers from unstructured clinical text when prompted with specific, human-readable questions.

### Mechanism 3: Human-in-the-Loop at Granular Decision Points
Exposing intermediate decision factors for review prevents error compounding and builds clinician trust through explainability. Clinicians review extracted factors with explanations and citations before recommendations are generated. Corrections propagate downstream. The interface shows reasoning chains, not just outputs. This assumes clinicians will engage with intermediate outputs if review time is sufficiently low (<7.5 min/patient).

## Foundational Learning

- **First-Order Logic & Rule-Based Systems**
  - Why needed here: The LLE architecture depends on understanding how declarative rules, variable bindings, and logical evaluation work. You'll debug rule evaluation and knowledge base structure.
  - Quick check question: Given variables `age=62`, `diagnosis=breast_cancer`, and rule `age < 65 AND diagnosis == breast_cancer → recommend_multigene_panel`, what does the system output?

- **LLM Extraction Patterns with Citations**
  - Why needed here: Stage 1 extracts structured data from unstructured text. You need to understand prompt design, function tool instrumentation, and citation grounding.
  - Quick check question: What failure modes occur when an LLM extracts "unknown" vs. "no" for a clinical factor, and how might this affect downstream rule evaluation?

- **Clinical Workflow & Guideline Structure**
  - Why needed here: Domain knowledge shapes how you structure knowledge bases, interpret guidelines, and design review interfaces.
  - Quick check question: Why might NCCN guidelines and institutional guidelines conflict for the same patient scenario, and how does knowledge base stacking resolve this?

## Architecture Onboarding

- **Component map**:
  Knowledge Base Server -> Extraction Pipeline -> Logic Evaluator -> Explanation Generator -> Clinical UI

- **Critical path**:
  1. Guidelines → Knowledge Base translation (semi-automated with expert review)
  2. Patient records → Factor extraction (LLM-driven with citations)
  3. Factors + Rules → Recommendations (deterministic evaluation)
  4. Recommendations + Explanations → Clinician review (HITL)

- **Design tradeoffs**:
  - **English-as-code** vs. formal languages: Staying close to natural language aids expert review but may introduce ambiguity
  - **Granular KBs** vs. monolithic: Smaller KBs reduce override conflicts but increase orchestration complexity
  - **Single factor output** vs. confidence scores: Current implementation lacks confidence; future versions may surface uncertainty

- **Failure signatures**:
  - **Date calculation errors** (36.6% of Step 1 corrections): LLMs struggle with temporal math—delegate to deterministic functions
  - **Incorrect inferences** (40.4% of Step 1 corrections): LLM makes unwarranted leaps—tighten prompts or add few-shot examples
  - **Knowledge base ambiguity** (23.0% of Step 1 corrections): Questions extracted from guidelines are underspecified—refine KB curation

- **First 3 experiments**:
  1. **Benchmark extraction accuracy on a held-out set**: Label 50-100 patient cases manually; measure factor extraction accuracy, citation correctness, and error categories
  2. **Stress-test temporal reasoning**: Create synthetic cases with edge-case dates (leap years, timezone ambiguities); compare LLM-only vs. deterministic date function performance
  3. **Ablate KB stacking behavior**: Test scenarios where institutional rules override NCCN; measure whether rule precedence works as expected and document side effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating confidence scores into the extraction of clinical decision factors improve the efficiency of the human review process by allowing experts to focus on low-confidence outputs?
- Basis in paper: [explicit] Section 3.3 states that "Future versions may benefit from the inclusion of a confidence score" to prompt experts to check specific outputs or trigger secondary reviews.
- Why unresolved: The current implementation provides a single output for each factor without a metric for certainty, relying on broad human review rather than targeted intervention.
- What evidence would resolve it: A comparative study measuring clinician time-to-review and error detection rates between the current system and a confidence-score-weighted interface.

### Open Question 2
- Question: Can replacing LLM-based date calculations with deterministic sub-functions significantly reduce the error rate in clinical decision factor extraction?
- Basis in paper: [explicit] Table 2 identifies "Date Calculation" as the cause of 36.6% of clinician corrections and suggests mitigating errors by using "functions that deterministically evaluate date-related computations."
- Why unresolved: The study relied on the LLM's native reasoning for temporal computations, which the authors acknowledge is notoriously error-prone.
- What evidence would resolve it: A follow-up evaluation of the LLE architecture where date logic is offloaded to external tools, comparing the "Clinician Correction" rate against the baseline established in this study.

### Open Question 3
- Question: How can logic conflicts be automatically detected and managed when stacking multiple, prioritized knowledge bases (e.g., institutional vs. national guidelines)?
- Basis in paper: [explicit] Section 3.2 notes that a potential complication of the stacking design is that higher-level rules may unintentionally override partial logic from lower-level rules, and suggests "detecting and managing such cases could be part of the functionality" in future versions.
- Why unresolved: The current system minimizes this risk by keeping knowledge bases granular, rather than employing automated conflict resolution or detection mechanisms.
- What evidence would resolve it: The development of a validation module that successfully identifies logical inconsistencies in stacked knowledge bases before they are applied to patient data.

## Limitations
- The 100-case dataset may not represent the full complexity and variability of real-world clinical practice
- The system assumes nuanced medical decision-making can be adequately captured through boolean formalization
- Evaluation focuses on accuracy and efficiency without addressing long-term reliability or performance drift

## Confidence
- **High confidence**: The hybrid LLE architecture reduces clinician review time (under 7.5 minutes per patient) and achieves low adjustment rates (2.1% for factors, 4.5% for recommendations) on the evaluated dataset
- **Medium confidence**: Error localization through two-stage extraction and HITL review at granular decision points are supported by internal metrics but require external validation
- **Low confidence**: Generalizability of first-order logic translation for complex clinical guidelines and handling of highly contextual or probabilistic reasoning scenarios remain unproven

## Next Checks
1. **Longitudinal reliability testing**: Deploy the system across a 6-month period with diverse patient cases to measure performance drift, error rate changes, and identify emerging failure patterns
2. **External guideline generalization**: Apply the knowledge base translation pipeline to a different clinical specialty (e.g., cardiology or diabetes management) and evaluate whether the same accuracy rates and error profiles hold
3. **Edge case temporal reasoning validation**: Create a comprehensive test suite with complex temporal scenarios (multiple time zones, ambiguous dates, leap year edge cases) to measure deterministic function effectiveness