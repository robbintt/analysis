---
ver: rpa2
title: 'Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language
  Model'
arxiv_id: '2409.15298'
source_url: https://arxiv.org/abs/2409.15298
tags:
- sorbet
- language
- softmax
- spiking
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sorbet addresses the challenge of deploying transformer-based language
  models on neuromorphic hardware by introducing PTsoftmax and BSPN to replace incompatible
  operations like softmax and layer normalization. These innovations use bit-shifting
  instead of complex operations, enabling energy-efficient inference.
---

# Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language Model

## Quick Facts
- arXiv ID: 2409.15298
- Source URL: https://arxiv.org/abs/2409.15298
- Authors: Kaiwen Tang; Zhanglu Yan; Weng-Fai Wong
- Reference count: 40
- Primary result: 27.16× energy efficiency improvement vs BERT on GLUE benchmark using PTsoftmax and BSPN for neuromorphic compatibility

## Executive Summary
Sorbet addresses the fundamental challenge of deploying transformer-based language models on neuromorphic hardware by replacing incompatible operations like softmax and layer normalization with SNN-compatible alternatives. The paper introduces PTsoftmax and BSPN, which use bit-shifting operations instead of complex mathematical functions, enabling energy-efficient inference on spiking neural networks. Through a multi-step distillation approach, Sorbet achieves competitive performance on the GLUE benchmark while significantly reducing energy consumption compared to both standard BERT and previous spiking language models like SpikeLM.

## Method Summary
Sorbet implements a three-stage distillation process to convert a BERT-base model into a neuromorphic-compatible spiking language model. First, weights are quantized to 1-bit using elastic binarization with power-of-two scale approximation, while activations are quantized to 4-bit. Second, the softmax operation is replaced with PTsoftmax, which uses ceiling, normalization by maximum, and right-shift operations to approximate the softmax function. Third, layer normalization is replaced with BSPN (Bit-Shifting PowerNorm), which computes L1 norms in grouped channels and applies bit-shift-based normalization. The final model undergoes ANN-to-SNN conversion using Average IF spike generation, achieving competitive GLUE benchmark performance with substantially reduced energy consumption.

## Key Results
- 27.16× energy efficiency improvement compared to BERT-base on GLUE benchmark
- 3.16× energy efficiency improvement compared to SpikeLM on GLUE benchmark
- Competitive accuracy across all seven GLUE tasks while maintaining hardware compatibility
- PTsoftmax achieves 0.25% average relative error compared to standard softmax

## Why This Works (Mechanism)
The core innovation works by replacing mathematically complex operations (softmax, layer normalization) with bit-shifting operations that are natively efficient on neuromorphic hardware. PTsoftmax approximates the exponential function using ceiling operations and bit-shifting, while BSPN replaces floating-point normalization with integer arithmetic and bit shifts. This hardware-aware design reduces computational complexity from O(n) for floating-point operations to O(1) for bit shifts, enabling massive energy savings while maintaining model accuracy through careful distillation training.

## Foundational Learning
- Neuromorphic computing: Brain-inspired hardware using spiking neurons for energy-efficient computation; needed because traditional transformers are computationally expensive and incompatible with spike-based hardware.
- ANN-to-SNN conversion: Process of converting artificial neural networks to spiking neural networks while preserving accuracy; needed to leverage neuromorphic hardware's efficiency benefits.
- Knowledge distillation: Training a smaller/student model to mimic a larger/teacher model's behavior; needed to transfer BERT's performance to the more constrained SNN architecture.
- Power-of-two scaling: Using scales that are powers of two to enable efficient bit-shifting operations; needed to maintain accuracy while using hardware-friendly operations.
- Layer normalization in SNNs: Challenge of implementing floating-point normalization in spike-based systems; needed to stabilize training and maintain performance.

## Architecture Onboarding
- Component map: BERT-base -> Quantization (1-bit weights, 4-bit activations) -> PTsoftmax replacement -> BSPN replacement -> ANN-to-SNN conversion -> Sorbet SNN
- Critical path: Multi-step distillation (quantization → PTsoftmax → BSPN) followed by ANN-to-SNN conversion using Average IF model
- Design tradeoffs: Accuracy vs. hardware efficiency, where complex operations are simplified at the cost of minor performance degradation that's recovered through distillation
- Failure signatures: Spike rate collapse (vanishing gradients) or explosion (unstable training), noticeable accuracy drops during conversion steps
- First experiments: 1) Verify PTsoftmax approximation accuracy against standard softmax, 2) Test BSPN normalization with different channel groupings, 3) Validate multi-step distillation by training each component sequentially

## Open Questions the Paper Calls Out
- How do Sorbet's actual energy consumption and inference latency compare to theoretical estimates when deployed on physical neuromorphic chips like Intel Loihi?
- Can the accuracy degradation caused by weight quantization and the spike generation process be mitigated without compromising hardware compatibility?
- Can the PTsoftmax and BSPN operators scale effectively to Large Language Models (LLMs) such as DeepSeek, which utilize FP8 quantization?

## Limitations
- Energy analysis based on theoretical Verilog synthesis rather than measurements from actual neuromorphic hardware
- Significant accuracy gap between full-precision ANN and final SNN model due to quantization and spike generation
- Limited validation to BERT-base architecture; scalability to larger LLMs remains untested

## Confidence
- High confidence: PTsoftmax algorithm description is complete and implementable; energy efficiency calculations appear methodologically sound
- Medium confidence: Overall multi-step distillation framework is coherent; Average IF spike generation model is well-described
- Low confidence: Exact BSPN grouping configuration, precise distillation loss weightings, and complete training hyperparameters are unspecified

## Next Checks
1. Replicate the PTsoftmax approximation error analysis by comparing its output distribution against standard softmax across a range of input values
2. Implement the BSPN normalization with different grouping configurations to empirically determine optimal grouping strategy
3. Conduct an ablation study removing multi-step distillation to validate whether incremental approach is essential for performance