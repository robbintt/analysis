---
ver: rpa2
title: 'Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety
  Alignment and Ideological Bias in Language Models in Detecting Hate Speech'
arxiv_id: '2509.00673'
source_url: https://arxiv.org/abs/2509.00673
tags:
- hate
- accuracy
- implicit
- performance
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the performance of large language models
  in detecting hate speech, comparing censored and uncensored models. While censored
  models achieved significantly higher accuracy (78.7%) than uncensored models (64.1%),
  the research reveals that this safety alignment creates an ideological anchor, making
  censored models resistant to persona-based influence while uncensored models prove
  highly malleable.
---

# Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech

## Quick Facts
- arXiv ID: 2509.00673
- Source URL: https://arxiv.org/abs/2509.00673
- Reference count: 4
- Censored models achieved 78.7% accuracy vs. 64.1% for uncensored models

## Executive Summary
This study compares censored and uncensored large language models on hate speech detection, revealing a fundamental trade-off between safety alignment and ideological bias. While censored models significantly outperform uncensored models (78.7% vs. 64.1% strict accuracy), this alignment creates an ideological anchor that makes censored models resistant to persona-based manipulation while uncensored models prove highly malleable to political framing. The research also uncovers critical fairness disparities across target groups and systemic overconfidence in model predictions, particularly for nuanced implicit hate speech and non-hate content.

## Method Summary
The study evaluates five LLMs (o3-mini, Llama 405b, Mistral Medium, GPT-4o, Mistral Large) using zero-shot classification on the Latent Hatred dataset (3,267 balanced samples across explicit hate, implicit hate, and not-hate classes). Models receive political persona system prompts (Progressive, Conservative, Libertarian, Centrist) and must output JSON with classification, confidence, and reasoning. Performance is measured via strict accuracy (refusals count as errors), Expected Calibration Error (ECE), and disaggregated recall by target group. Temperature is set to 0.7 with single-pass inference.

## Key Results
- Censored models achieved 78.7% strict accuracy versus 64.1% for uncensored models
- Uncensored models showed 5.2 percentage point accuracy variance across personas versus 0.6 points for censored models
- Performance gaps exceeded 50 percentage points between best and worst-performing target groups
- Models expressed high confidence on incorrect predictions (71.7-74.0% mean confidence) with ECE of 0.094

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety alignment functions as an ideological anchor, improving classification performance while simultaneously reducing susceptibility to prompt-based manipulation.
- Mechanism: Alignment training (e.g., RLHF) appears to optimize not just for safety refusals but for adherence to classification instructions, producing more stable internal representations. This stability constrains the model's ideological flexibility, making it resistant to persona-induced shifts while locking in a particular worldview.
- Core assumption: The performance improvement stems from alignment training itself, not from underlying model capability differences (controlled via LMArena scores).
- Evidence anchors:
  - [abstract] "Censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy."
  - [section 3.3] "Censored models are highly resistant to persona influence, with strict accuracy varying by only 0.6 percentage points... In contrast, uncensored models are much more susceptible to manipulation, with their accuracy fluctuating by 5.2 percentage points."
  - [corpus] Related work on ideological manipulation (arxiv:2504.14287) examines LLM susceptibility to political framing but does not address the protective effect of safety alignment specifically.
- Break condition: If alignment training does not improve instruction-following for classification tasks, or if uncensored models receive equivalent fine-tuning on classification without safety constraints, the mechanism may not hold.

### Mechanism 2
- Claim: Persona-based prompting shifts classification boundaries directionally, with progressive personas increasing false positive rates and libertarian personas increasing false negative rates.
- Mechanism: Persona induction via system prompts biases the model's prior probability toward classifying content consistent with the induced worldview. Progressive framing lowers the threshold for "hate" classification; libertarian framing raises it.
- Core assumption: The observed shifts reflect genuine changes in classification thresholds rather than noise from single-pass inference at T=0.7.
- Evidence anchors:
  - [abstract] "Uncensored models prove highly malleable to ideological framing."
  - [section 3.2] "The progressive persona exhibited a 'liberal bias' (a high false positive rate), while the libertarian persona showed a 'conservative bias' (a high false negative rate)."
  - [corpus] Related work (arxiv:2511.20736) documents LLM complicity in illicit contexts but does not isolate persona effects on classification thresholds.
- Break condition: If multi-pass inference at lower temperatures eliminates directional bias, the mechanism may reflect temperature-induced variance rather than genuine threshold shifts.

### Mechanism 3
- Claim: Self-reported confidence is systematically miscalibrated, with models expressing high certainty on incorrect predictions, particularly for nuanced content.
- Mechanism: Models learn to produce confident outputs without learning reliable uncertainty estimates. For ambiguous cases (irony, implicit hate), the model's internal uncertainty is not reflected in its verbalized confidence scores.
- Core assumption: The confidence scores extracted from JSON outputs reflect the model's internal uncertainty representation rather than surface-level pattern matching on "confidence" as a token.
- Evidence anchors:
  - [abstract] "Systemic overconfidence where models' self-reported certainty was unreliable, particularly on nuanced implicit hate speech and non-hate content."
  - [section 3.6] "The mean confidence for incorrect predictions was consistently high across all classes: 71.7% for explicit_hate, 72.8% for implicit_hate, and 74.0% for not_hate... ECE of 0.094."
  - [corpus] No directly comparable calibration analysis in corpus neighbors.
- Break condition: If calibration improves with different prompting strategies or ensemble methods, the miscalibration may be an artifact of single-pass evaluation rather than a fundamental model property.

## Foundational Learning

- Concept: **Safety Alignment (RLHF)**
  - Why needed here: Understanding how alignment training affects both capability and ideological rigidity is central to interpreting the censored vs. uncensored comparison.
  - Quick check question: Can you explain why RLHF might improve classification accuracy even though it is designed primarily for safety?

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: ECE quantifies the gap between confidence and accuracy; understanding it is necessary to interpret the finding that models are "confidently wrong."
  - Quick check question: If a model has ECE=0.094, what does that imply about the reliability of its confidence scores for triage decisions?

- Concept: **Implicit vs. Explicit Hate Speech**
  - Why needed here: The performance gap between explicit (95.7%) and implicit (67.3-82.5%) hate detection drives the core conclusions about model limitations.
  - Quick check question: Why might irony be particularly difficult for models to classify correctly compared to threatening language?

## Architecture Onboarding

- Component map: Dataset Layer -> Model Layer -> Prompting Layer -> Evaluation Layer
- Critical path:
  1. Dataset balancing (3,267 samples across 3 classes)
  2. Persona induction via system prompt
  3. Zero-shot classification with JSON enforcement
  4. Parse responses; treat refusals/malformed outputs as errors
  5. Compute strict