---
ver: rpa2
title: 'NLD-LLM: A systematic framework for evaluating small language transformer
  models on natural language description'
arxiv_id: '2510.05139'
source_url: https://arxiv.org/abs/2510.05139
tags:
- language
- natural
- code
- prompt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLD-LLM, a systematic framework for evaluating
  language models' ability to generate accurate, concise natural language descriptions
  from source code. The framework employs a diverse set of transformer models (Qwen,
  DeepSeek, Phi, LLaMA, Mistral) and a comprehensive prompt engineering strategy to
  ensure fair evaluation.
---

# NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description

## Quick Facts
- arXiv ID: 2510.05139
- Source URL: https://arxiv.org/abs/2510.05139
- Authors: Hamed Jelodar; Mohammad Meymani; Parisa Hamedi; Tochukwu Emmanuel Nwankwo; Samita Bai; Roozbeh Razavi-Far; Ali A. Ghorbani
- Reference count: 24
- Primary result: Smaller models like Qwen can match larger models' semantic fidelity when supported by well-crafted prompts

## Executive Summary
This paper introduces NLD-LLM, a systematic framework for evaluating language models' ability to generate accurate, concise natural language descriptions from source code. The framework employs a diverse set of transformer models (Qwen, DeepSeek, Phi, LLaMA, Mistral) and a comprehensive prompt engineering strategy to ensure fair evaluation. Using semantic and structural metrics, including BLEU, ROUGE-L, METEOR, and BERTScore, the study reveals that smaller models like Qwen can perform competitively with larger models when supported by well-crafted prompts. Qwen achieved the highest BERTScore (0.8853), demonstrating strong semantic fidelity, while LLaMA led in BLEU score (0.0413), indicating better surface-form matching. The results highlight the importance of model selection based on task-specific requirements and resource constraints.

## Method Summary
The NLD-LLM framework uses five transformer models (Qwen2.5-Coder-1.5B-Instruct, DeepSeek-Coder-1.3B-Instruct, Phi-4-mini-instruct 1.3B, Mistral-7B-Instruct, Meta-Llama-3.1-8B-Instruct) to generate natural language descriptions from C/C++ code. The method employs six standardized prompt styles with clear task guidance and iterative refinement. Models are evaluated using BLEU, ROUGE-L, METEOR, BERTScore, and MAUVE metrics against 15100 human-annotated examples. Decoding parameters are fixed (temperature=0.7, top-p=0.9) using Hugging Face transformers on NVIDIA H100 GPU.

## Key Results
- Qwen (1.5B) achieved the highest BERTScore (0.8853), outperforming larger models on semantic fidelity
- LLaMA (8B) led in BLEU score (0.0413), excelling at surface-form matching
- DeepSeek performed poorly across all metrics, suggesting suboptimal fine-tuning for this task
- Standardized prompt engineering enabled smaller models to perform competitively with larger ones

## Why This Works (Mechanism)

### Mechanism 1
Standardized prompt engineering reduces evaluation variance, enabling smaller models to perform competitively with larger ones on semantic fidelity tasks. Unified prompt formatting, clear task guidance, and iterative refinement constrain the output space, reducing the advantage of larger models' broader capacity and allowing parameter-efficient models to specialize on the target distribution.

### Mechanism 2
Metric choice determines model ranking; semantic metrics (BERTScore) favor contextual appropriateness while n-gram metrics (BLEU) favor surface-form matching. BERTScore uses contextual embeddings to capture paraphrase and synonymy; BLEU/ROUGE-L count exact/subsequence overlaps, penalizing lexical variation even when meaning is preserved.

### Mechanism 3
Iterative refinement improves output quality by feeding initial descriptions back as context for correction, particularly benefiting smaller models with weaker first-pass priors. The refiner step conditions on the model's own draft, allowing error correction and style alignment without external supervision; smaller models gain more from this self-conditioning than well-calibrated larger models.

## Foundational Learning

- **Transformer decoder inference**: Understanding autoregressive generation and decoding parameters (temperature=0.7, top-p=0.9) is essential for reproducing results and interpreting metric variance. Quick check: If you set temperature=0, what happens to output diversity, and how would BLEU vs BERTScore be affected?

- **Embedding-based semantic similarity**: The study's primary finding relies on BERTScore differences; you must understand that BERTScore compares token-level contextual embeddings rather than surface strings to interpret why Qwen leads on semantics but not BLEU. Quick check: Why can two sentences with no shared words still achieve high BERTScore?

- **Prompt engineering for instruction-following models**: The framework's core contribution is a standardized prompt strategy; distinguishing between role-based, instruction-based, and chat-based prompting is necessary to adapt the framework to new tasks or models. Quick check: Given Table I's prompt styles, which would you choose for a batch evaluation pipeline with no conversational context, and why?

## Architecture Onboarding

- **Component map**: Prompt module (task definitions + guidance constraints) -> Model inference layer (five models with unified decoding) -> Refiner loop (optional second-pass conditioning) -> Evaluation layer (BLEU, ROUGE-L, METEOR, BERTScore, MAUVE) -> Dataset (150 manually annotated C/C++ examples)

- **Critical path**: 1. Define task prompt style (Table I) -> 2. Apply guidance constraints (Table II) -> 3. Run inference with fixed decoding params -> 4. Optionally apply refiner prompt -> 5. Compute metrics against references -> 6. Aggregate and compare across models

- **Design tradeoffs**: Semantic fidelity (BERTScore) vs surface precision (BLEU): Qwen optimizes the former, LLaMA the latter; choice depends on downstream use case. Model size vs latency: Qwen (1.5B) offers comparable semantic quality to Mistral (7.5B) at lower inference cost. Prompt complexity vs reproducibility: More detailed prompts improve quality but increase prompt engineering effort.

- **Failure signatures**: Very low BLEU (<0.01) with high BERTScore (>0.85) is not a failure if semantic fidelity is the goal. Consistently low scores across all metrics indicate model-task mismatch. High variance across prompt styles suggests prompt sensitivity.

- **First 3 experiments**: 1. Reproduce Qwen vs Mistral comparison on 150-example dataset using "Concise One-Line Description" prompt. 2. Ablate the refiner step to measure delta in BERTScore and METEOR. 3. Test prompt style sensitivity across three prompt types from Table I on a 50-example subset.

## Open Questions the Paper Calls Out

### Open Question 1
Does the competitive performance of small models like Qwen (1.5B) generalize to other programming languages such as Python and Java? The current study is restricted to C and C++ scripts, limiting the breadth of the conclusion regarding small model efficiency across different coding syntaxes and paradigms.

### Open Question 2
How does the inclusion of memory efficiency and output relevance metrics alter the optimal model selection strategy? The current evaluation prioritizes semantic fidelity (BERTScore) and n-gram overlap, potentially overlooking resource constraints critical for edge deployment.

### Open Question 3
To what extent can targeted fine-tuning improve the performance of currently underperforming small models like DeepSeek compared to prompt engineering alone? The paper evaluates pre-trained models using standardized prompts but does not explore weight updates as a mechanism to close the performance gap with Qwen.

## Limitations
- The 15100 annotated code-description pairs dataset is not publicly available, making direct reproduction difficult
- The exact formulation and implementation details of the iterative refinement step are not provided
- Results are anchored to a specific task (C/C++ NLD) and may not transfer to open-ended or non-code domains without further validation

## Confidence
- **High Confidence**: Standardized prompts reduce evaluation variance and enable smaller models to perform competitively (well-supported by comparative results)
- **Medium Confidence**: Metric choice determines model ranking (plausible but underlying reference quality assumption is untested)
- **Low Confidence**: Iterative refinement loop's effectiveness for smaller models (only conceptually described without explicit experimental evidence)

## Next Checks
1. Reproduce the Qwen vs Mistral BERTScore comparison on a public code summarization dataset (CodeSearchNet) to validate robustness of the prompt engineering framework
2. Implement and run the refiner step on a 50-example subset to quantify its impact on BERTScore and METEOR; compare gains across model sizes
3. Systematically compare three prompt styles from Table I on a held-out subset to measure variance in metric profiles; report which style yields the most stable results across models