---
ver: rpa2
title: Large Language Model-Based Generation of Discharge Summaries
arxiv_id: '2512.06812'
source_url: https://arxiv.org/abs/2512.06812
tags:
- discharge
- notes
- summary
- information
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated five large language models (Mistral, Llama
  2, GPT-3.5, GPT-4, and Gemini 1.5 Pro) for generating discharge summaries from clinical
  notes in the MIMIC-III dataset. Models were assessed using exact-match (BLEU, ROUGE),
  soft-overlap (BERTScore, BLEURT), and reference-free metrics (BLANC, SummaC), alongside
  a qualitative clinical evaluation.
---

# Large Language Model-Based Generation of Discharge Summaries

## Quick Facts
- **arXiv ID:** 2512.06812
- **Source URL:** https://arxiv.org/abs/2512.06812
- **Reference count:** 40
- **Primary result:** Proprietary models (GPT-4, GPT-3.5, Gemini 1.5 Pro) with one-shot prompting outperformed open-source models (Mistral, Llama 2) on discharge summary generation, achieving higher semantic similarity despite low exact-match scores.

## Executive Summary
This study evaluates five large language models for generating discharge summaries from clinical notes in the MIMIC-III dataset. Proprietary models, particularly Gemini 1.5 Pro with one-shot prompting, achieved the highest similarity to gold-standard summaries across multiple metrics. Open-source models showed competitive but lower performance, with fine-tuning providing minimal improvement. Human evaluation confirmed the practical utility of summaries from proprietary models, though challenges remain with hallucinations and missing information. The study highlights the promise of proprietary models for automatic discharge summary generation when data privacy can be ensured.

## Method Summary
The study filtered MIMIC-III to remove erroneous notes and addendums, tokenizing with SentencePiece and limiting to admissions under 7,600 tokens. Open-source models (Mistral-7B-Instruct, Llama-2-7B-Chat) were fine-tuned via QLoRA (4-bit NF4) with AdamW optimizer (lr=2e-5). Proprietary models (GPT-3.5, GPT-4, Gemini 1.5 Pro) used structured zero-shot and one-shot prompting. Evaluation included ROUGE-1/2/L, BLEU, BERTScore, BLEURT, BLANC, SummaC, TTR, and human Likert scoring on completeness, correctness, and conciseness.

## Key Results
- Proprietary models (especially Gemini 1.5 Pro with one-shot prompting) outperformed open-source models across all metrics
- Fine-tuned Mistral showed competitive results but lagged behind proprietary models
- BERTScore remained high (~82-85%) while exact-match metrics (BLEU, ROUGE) stayed low, indicating semantic preservation with lexical divergence
- Human evaluation confirmed practical utility of proprietary model summaries despite hallucination and missing information challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** One-shot prompting substantially improves proprietary model performance on discharge summary generation compared to zero-shot.
- **Mechanism:** Providing a single example summary establishes the expected output structure, section ordering, and clinical documentation style. This grounds the model's generation in the target format rather than relying solely on its pre-trained priors about summarization.
- **Core assumption:** The example summary is representative of the target distribution and doesn't leak spurious patterns into generation.
- **Evidence anchors:**
  - [abstract] "proprietary models, particularly Gemini with one-shot prompting, outperformed others"
  - [section 4.1.2] Gemini one-shot achieved ROUGE-1: 30.90%, ROUGE-2: 10.97%, vs zero-shot ROUGE-1: 23.33%, ROUGE-2: 9.21%
  - [corpus] Neighbor paper "Abstract Meaning Representation for Hospital Discharge Summarization" identifies hallucination as the "Achilles heel" of LLMs in clinical generation, suggesting structured prompting may constrain outputs
- **Break condition:** When the example exceeds context budget or when domain shift between example and target cases is large, one-shot gains degrade.

### Mechanism 2
- **Claim:** QLoRA fine-tuning on open-source 7B models yields minimal improvement for this multi-document summarization task.
- **Mechanism:** The constrained input (simplified system prompt to avoid token limits), lack of structured output enforcement, and 7B parameter capacity together limit the model's ability to learn the complex synthesis patterns required. The paper notes that while training loss decreased, evaluation metrics barely moved.
- **Core assumption:** Failure stems from input/output constraints rather than QLoRA technique itself.
- **Evidence anchors:**
  - [abstract] "Fine-tuning open-source models provided minimal improvement"
  - [section 4.1.1] Mistral fine-tuned gained only 0.74% ROUGE-1; Llama performance declined on most metrics
  - [corpus] Limited corpus evidence on QLoRA specifically for clinical summarization; neighbor papers focus on prompting strategies
- **Break condition:** With larger models (e.g., 70B+), full LoRA or full fine-tuning, and structured output prompts, this mechanism may not hold.

### Mechanism 3
- **Claim:** Proprietary models generate summaries that differ in wording from reference summaries but capture similar semantic content.
- **Mechanism:** Large proprietary models infer and synthesize information rather than extract verbatim, producing abstractive summaries. BERTScore remains high (~82-85%) while exact-match metrics (BLEU, ROUGE) stay low, indicating semantic preservation with lexical divergence.
- **Core assumption:** BERTScore reliably captures semantic equivalence in clinical text.
- **Evidence anchors:**
  - [section 4.1.2] Gemini one-shot achieved BERTScore 85.09% despite low BLEU (3.07%)
  - [section 4.3] Proprietary models "tend to generate sentences that convey the same meaning as the original notes but use different terms"
  - [corpus] "Hallucinations and Key Information Extraction in Medical Texts" notes LLMs show potential but require evaluation frameworks for clinical accuracy
- **Break condition:** When semantic drift produces hallucinations or omits critical clinical details, semantic similarity scores no longer validate utility.

## Foundational Learning

- **Concept: Exact-match vs. semantic overlap metrics**
  - **Why needed here:** The paper reports divergent scores between ROUGE/BLEU (exact word overlap) and BERTScore/BLEURT (semantic similarity). Understanding why proprietary models score low on exact-match but high on semantic metrics is essential for interpreting results.
  - **Quick check question:** If a model paraphrases "patient discharged in stable condition" as "patient left hospital with improved status," which metric types would penalize vs. reward this?

- **Concept: Multi-document summarization**
  - **Why needed here:** Discharge summary generation requires synthesizing information across nursing notes, radiology reports, physician notes, and ECG readings—not just compressing a single document.
  - **Quick check question:** Why might a model that excels at single-document summarization struggle when input consists of 15 heterogeneous clinical notes?

- **Concept: Context window constraints**
  - **Why needed here:** Hardware and model architecture limited open-source models to 7,600 tokens, forcing data truncation and simplified prompting. Proprietary models with 128K–1M token contexts processed full admission note sets.
  - **Quick check question:** A patient admission has 3,200 words across 12 notes. If tokenization ratio is ~1.3 tokens/word, does this fit within an 8,192 token context window?

## Architecture Onboarding

- **Component map:**
  - Data pipeline: MIMIC-III → filter erroneous notes/addendums → tokenize with SentencePiece → filter admissions >7,600 tokens → train/test split
  - Open-source path: Mistral-7B-Instruct / Llama-2-7B-Chat → QLoRA fine-tuning (4-bit NF4, AdamW, lr=2e-5) → system-prompt-only inference
  - Proprietary path: GPT-3.5/GPT-4/Gemini 1.5 Pro → structured zero-shot or one-shot prompting → structured output parsing
  - Evaluation: ROUGE-1/2/L, BLEU, BERTScore, BLEURT, BLANC, SummaC, TTR + human Likert scoring

- **Critical path:**
  1. Preprocess MIMIC-III notes and summaries into admission-level collections
  2. For open-source: fine-tune with QLoRA on training set (20,357 admissions)
  3. For proprietary: construct structured prompt with section headers + optional example
  4. Generate summaries on test set (982 admissions)
  5. Compute all metrics; conduct human evaluation on 3 random samples per model

- **Design tradeoffs:**
  - Open-source: Full data control, no API costs, GDPR-compliant—but requires GPU infrastructure, slower inference, lower performance
  - Proprietary: Superior performance, faster iteration—but data privacy concerns, per-token costs, rate limits
  - Structured output prompt: Improves section adherence but may hurt exact-match scores if section ordering diverges from references

- **Failure signatures:**
  - Low TTR with reasonable BLEU: Likely repetitive sentence generation (observed in Llama)
  - High BERTScore with low ROUGE: Semantic preservation with lexical divergence (expected for proprietary models)
  - Positive BLEURT: Rare; indicates output quality exceeds reference-level baseline (only Gemini one-shot achieved this at 0.0847)
  - Human evaluation scores of 1 across all criteria: Indicates incoherent or hallucinated output

- **First 3 experiments:**
  1. **Baseline replication:** Run zero-shot inference with GPT-4 and Gemini on 50 test admissions using the structured prompt; compute ROUGE, BERTScore, BLEURT to confirm paper's scoring ranges.
  2. **Ablate structured output:** Remove section header requirements from prompt; measure impact on per-section scores and completeness in human evaluation.
  3. **Context window stress test:** Identify admissions near the 7,600 token boundary; compare open-source performance on truncated vs. full-length proprietary model inputs to isolate context effects from model scale effects.

## Open Questions the Paper Calls Out

- **Question:** Can LLM-based summarization systems trained on MIMIC-III generalize effectively to clinical notes from different hospital systems or linguistic contexts?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they "only used MIMIC-III data, which may not generalize to other hospitals with different summarization styles," and suggest future work on "datasets in different languages."
- **Why unresolved:** The study was restricted to a single database (MIMIC-III), which may represent specific documentation styles and abbreviations that do not exist elsewhere.
- **What evidence would resolve it:** A study benchmarking the fine-tuned models or proprietary prompts on external clinical datasets from different institutions or non-English speaking countries.

- **Question:** Can incorporating multimodal data sources, such as voice recordings, address the issue of missing information in textual clinical notes?
- **Basis in paper:** [explicit] The authors note in Future Work that "missing data in clinical notes" is a key issue and propose "introducing alternative methods for capturing detailed information, such as voice recordings" and "multimodal systems" as potential solutions.
- **Why unresolved:** The current study relied solely on text-based clinical notes, which the expert evaluator found often lacked necessary information (completeness issues) that might have been verbally discussed but not recorded.
- **What evidence would resolve it:** Experiments utilizing a multimodal dataset where models process both text and audio data to measure improvements in the "completeness" scores of generated summaries.

- **Question:** How does the performance ranking of models change when evaluated by a larger pool of clinical experts compared to a single evaluator?
- **Basis in paper:** [explicit] The authors highlight in the Limitations section that the "human analysis was conducted by a medical expert, using just three samples," and note that "having the evaluation done by more people would be an advantage."
- **Why unresolved:** Summarization relevance is subjective; a single expert's view on "completeness" and "conciseness" may not represent the broader medical community's consensus or account for inter-annotator agreement.
- **What evidence would resolve it:** A large-scale human evaluation study involving multiple clinicians to assess inter-rater reliability and generate a consensus ranking of the LLMs.

## Limitations

- Context window constraints created selection bias by excluding longer admissions from open-source model evaluation
- One-shot prompting used a single "random admission" example without specifying selection method, risking data leakage
- Clinical expert assessment covered only 15 summaries total (3 per model), insufficient for robust conclusions about real-world clinical utility

## Confidence

**High confidence:** The relative performance ranking between proprietary and open-source models is robust, with consistent superiority of GPT-4, GPT-3.5, and particularly Gemini 1.5 Pro across multiple metrics.

**Medium confidence:** The specific mechanism that one-shot prompting improves proprietary model performance requires qualification, as the study doesn't control for prompt engineering variations or test different example qualities.

**Low confidence:** The claim that proprietary models "capture similar semantic content" despite low exact-match scores relies heavily on BERTScore validity for clinical text without extensive error analysis showing semantic preservation doesn't mask critical omissions.

## Next Checks

1. **Cross-validation of one-shot example:** Retest one-shot performance using examples drawn from training, validation, and test sets separately. Compare metric differences to establish whether the current results suffer from data leakage.

2. **Error type classification:** Conduct detailed analysis of hallucinations and omissions across all models using clinical domain experts. Categorize errors by severity (critical vs. minor) and frequency to determine if semantic similarity scores mask dangerous information loss.

3. **Context window ablation study:** Systematically vary the input truncation threshold for open-source models (e.g., 5,000; 7,600; 10,000 tokens when using sliding window techniques). Measure performance degradation as context decreases to quantify the true impact of window constraints versus model capability differences.