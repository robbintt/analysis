---
ver: rpa2
title: A Content-Preserving Secure Linguistic Steganography
arxiv_id: '2511.12565'
source_url: https://arxiv.org/abs/2511.12565
tags:
- text
- embedding
- secret
- cover
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a content-preserving linguistic steganography
  method called CLstega that embeds secret messages without modifying the cover text,
  addressing the security risks of existing content-transformation-based approaches.
  The core idea is to leverage controllable distribution transformation via fine-tuning
  a masked language model (MLM) to map secret messages to different prediction distributions
  at embedding positions.
---

# A Content-Preserving Secure Linguistic Steganography

## Quick Facts
- arXiv ID: 2511.12565
- Source URL: https://arxiv.org/abs/2511.12565
- Authors: Lingyun Xiang; Chengfu Ou; Xu He; Zhongliang Yang; Yuling Liu
- Reference count: 27
- Primary result: Content-preserving steganography achieving 100% extraction success with near-random steganalysis detection

## Executive Summary
This paper introduces CLstega, a content-preserving linguistic steganography method that embeds secret messages without modifying cover text by leveraging masked language model fine-tuning. Unlike existing methods that transform text content and risk detection through semantic differences, CLstega maps secret messages to different prediction distributions at embedding positions while maintaining original text. The method achieves perfect extraction success rate, superior anti-steganalysis security with detection accuracy near 0.5 (random), and demonstrates the lowest perplexity among evaluated approaches.

## Method Summary
CLstega uses a novel approach of leveraging controllable distribution transformation via fine-tuning a masked language model (MLM) to embed secret messages. The method maps secret messages to different prediction distributions at embedding positions without modifying the cover text itself. By fine-tuning the MLM, CLstega can control how the model generates predictions at specific positions, allowing secret information to be encoded in the distribution patterns rather than in text modifications. This fundamentally differs from content-transformation approaches that alter the actual text, making them vulnerable to detection through semantic analysis.

## Key Results
- Achieves 100% extraction success rate across all tested scenarios
- Outperforms baselines in anti-steganalysis security with detection accuracy near 0.5 (random detection)
- Demonstrates superior imperceptibility with lowest perplexity (70.16 vs 82.55-512.34 for baselines)
- Maintains competitive embedding capacity with ER 0.4204 while preserving perfect content

## Why This Works (Mechanism)
The core innovation lies in encoding secret information in the distribution transformation patterns rather than in text modifications. By fine-tuning the MLM to produce specific prediction distributions at embedding positions, CLstega creates a steganographic channel that doesn't alter the visible text content. This approach exploits the fact that MLMs can generate the same word through different internal probability distributions, allowing secret bits to be encoded in which distribution is selected. The method leverages the natural variability in language model predictions while maintaining semantic consistency, making detection through content analysis ineffective.

## Foundational Learning

1. **Masked Language Models (MLMs)**: Neural networks that predict masked tokens in sequences, crucial for understanding how CLstega manipulates prediction distributions. Quick check: Verify that MLMs like BERT can produce different probability distributions for the same word depending on context and fine-tuning.

2. **Linguistic Steganography**: The practice of hiding secret messages within natural language text. Quick check: Understand the fundamental difference between content-preserving and content-transformation approaches in terms of security vulnerabilities.

3. **Distribution Transformation**: The technique of mapping input data to different probability distributions without changing observable outputs. Quick check: Confirm that fine-tuning can control MLM prediction distributions at specific positions.

4. **Perplexity as Evaluation Metric**: A measure of how well a probability model predicts a sample, used here to assess text naturalness. Quick check: Verify that lower perplexity correlates with more natural-sounding text in the context of generated language.

5. **Steganalysis**: The practice of detecting hidden messages in seemingly innocent data. Quick check: Understand why content-preserving approaches are theoretically more secure against semantic-based detection methods.

## Architecture Onboarding

**Component Map**: Secret Message -> Distribution Mapper -> Fine-tuned MLM -> Cover Text + Hidden Message -> Extraction using MLM

**Critical Path**: The method relies on identifying embedding positions in cover text, mapping secret bits to specific distribution patterns through MLM fine-tuning, and extracting using the same distribution analysis approach. The critical dependency is the ability to reliably identify embedding positions and maintain consistent distribution patterns across embedding and extraction.

**Design Tradeoffs**: CLstega prioritizes security and content preservation over embedding capacity, trading higher capacity for near-perfect anti-steganalysis performance. The computational cost of fine-tuning MLMs for each message is accepted for the security benefits of content preservation.

**Failure Signatures**: The method could fail if embedding positions cannot be reliably identified, if the MLM fine-tuning doesn't produce consistent distributions, or if steganalysis tools can detect patterns in the distribution transformations themselves. Loss of semantic consistency during distribution mapping would also indicate failure.

**First Experiments**:
1. Test extraction success rate on simple cover texts with known secret messages using the fine-tuned MLM
2. Evaluate perplexity scores comparing CLstega-generated text against original cover text
3. Measure detection accuracy using basic steganalysis tools on CLstega vs baseline methods

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily against ALASKA framework benchmarks with limited real-world adversarial testing
- Tested only on two datasets (IMDB and News) with fixed embedding ratios, raising generalization concerns
- Computational cost of fine-tuning MLMs for each secret message may present practical scalability challenges

## Confidence
- **High**: 100% extraction success rate, technically sound MLM fine-tuning approach
- **Medium**: Practical security assessment limited to ALASKA framework, real-world adversarial testing needed
- **Low**: Correlation between perplexity scores and human-perceived naturalness not fully validated

## Next Checks
1. Test CLstega against advanced adaptive steganalysis tools beyond ALASKA framework, including neural network-based detectors specifically trained to identify MLM-generated text patterns
2. Evaluate performance across diverse text genres (legal documents, technical writing, social media) with varying linguistic complexity and style consistency requirements
3. Conduct human evaluation studies comparing CLstega-generated text against original content and baseline methods to validate the correlation between perplexity scores and perceived naturalness