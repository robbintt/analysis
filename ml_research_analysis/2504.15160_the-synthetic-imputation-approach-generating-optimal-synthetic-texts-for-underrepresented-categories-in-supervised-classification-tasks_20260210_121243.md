---
ver: rpa2
title: 'The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For
  Underrepresented Categories In Supervised Classification Tasks'
arxiv_id: '2504.15160'
source_url: https://arxiv.org/abs/2504.15160
tags:
- synthetic
- data
- performance
- original
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of underrepresented categories
  in supervised text classification tasks, where insufficient training examples for
  certain categories lead to poor model performance. The synthetic imputation approach
  leverages a generative LLM (GPT-4o) to create synthetic texts that are both similar
  enough to improve classifier performance and different enough to avoid overfitting.
---

# The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks

## Quick Facts
- arXiv ID: 2504.15160
- Source URL: https://arxiv.org/abs/2504.15160
- Authors: Joan C. Timoneda
- Reference count: 6
- With 75 or more original examples plus synthetic imputation, model performance is equivalent to using a full sample of original texts

## Executive Summary
This study addresses the challenge of underrepresented categories in supervised text classification tasks, where insufficient training examples for certain categories lead to poor model performance. The synthetic imputation approach leverages a generative LLM (GPT-4o) to create synthetic texts that are both similar enough to improve classifier performance and different enough to avoid overfitting. The method uses five random examples from the original sample and a carefully crafted prompt to generate each synthetic text.

Results show that with 75 or more original examples plus synthetic imputation, model performance is equivalent to using a full sample of original texts. With only 50 original examples, synthetic imputation produces moderate overfitting (2-4%), which is preferable to the significant performance drop without synthetic data. The approach significantly outperforms SSMBA, a state-of-the-art data augmentation method, by reducing overfitting by 6-15%. The synthetic imputation approach provides a novel role for generative LLMs in data augmentation and allows researchers to balance their datasets for optimal performance in supervised classification tasks.

## Method Summary
The synthetic imputation approach generates synthetic texts for underrepresented categories in supervised text classification by sampling five random examples with replacement from the original data and using them as few-shot prompts for GPT-4o. The LLM is instructed to create new texts that differ in sentence structure and key content while preserving substantive meaning. Synthetic texts are generated until the combined dataset reaches at least 200 examples, then used to fine-tune a RoBERTa-large classifier with batch size 16, learning rate 3e-5 or 5e-6, and 10-times repeated 10-fold cross-validation. The method is evaluated on political nostalgia detection and populist speech classification tasks, comparing performance against baselines and SSMBA augmentation.

## Key Results
- With 75 or more original examples plus synthetic imputation, model performance is equivalent to using a full sample of original texts
- With 50 original examples, synthetic imputation produces moderate overfitting (2-4%), preferable to performance drop without augmentation
- Synthetic imputation reduces overfitting by 6-15% compared to SSMBA, a state-of-the-art data augmentation method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Drawing five random examples with replacement for each synthetic text reduces overfitting compared to fixed example sets
- Mechanism: Random sampling introduces variation across synthetic outputs by exposing the generative model to different example combinations each iteration, preventing the classifier from learning spurious patterns from repetitive training data
- Core assumption: The variation introduced by random sampling generalizes across text domains beyond the two tested (political manifestos, speeches)
- Evidence anchors:
  - [abstract] "five original examples drawn randomly with replacement from the sample"
  - [section] "To increase variability in LLM responses, the five examples are drawn anew and with replacement for each synthetic text the LLM produces."
  - [corpus] Neighbor paper "Does Prompt Design Impact Quality of Data Imputation by LLMs?" suggests prompt design affects imputation quality, supporting the importance of prompt structure—but does not directly test random sampling
- Break condition: If original sample size drops below 50, random sampling may not provide sufficiently diverse combinations, leading to highly similar outputs

### Mechanism 2
- Claim: Explicit prompt instructions to vary sentence structure and content preserve category semantics while reducing lexical overlap
- Mechanism: Instructions like "names, countries and topics should also be different" guide the LLM to generate syntactically and semantically distinct texts that retain the underlying category meaning, reducing memorization-driven overfitting
- Core assumption: The LLM reliably follows instructions to vary content; instruction-following may degrade for specialized domains or low-resource languages
- Evidence anchors:
  - [section] "The prompt includes specific instructions to write different text in terms of sentence structure and key content, while preserving the overall common ideas across the five examples."
  - [section] "Figure 1 displays a flow chart describing the synthetic imputation approach from prompt creation to synthetic text output... it shows how careful prompting and selecting five random examples produces a new synthetic text instance that retains substantive meaning across the examples but differs substantively both grammatically and semantically."
  - [corpus] "Few-Shot Multilingual Open-Domain QA from 5 Examples" validates few-shot learning with 5 examples but in QA tasks, not classification—transferability assumed
- Break condition: If prompt instructions are vague or category boundaries are ambiguous, synthetic texts may drift semantically, degrading classifier performance

### Mechanism 3
- Claim: Synthetic imputation with 75+ original examples achieves performance statistically indistinguishable from full original datasets
- Mechanism: Above a threshold of original examples, the synthetic data provides sufficient coverage of within-category variation to approximate the information content of larger original datasets, enabling reliable gradient estimation during batch training
- Core assumption: The 75-example threshold generalizes across tasks; task complexity may shift this threshold
- Evidence anchors:
  - [abstract] "With 75 or more original examples plus synthetic imputation, model performance is equivalent to using a full sample of original texts."
  - [section] "At and over 75 true observations plus synthetic imputation, model performance is equivalent to the original model with the full set of training data."
  - [corpus] No direct corpus validation of the 75-example threshold; "Code Review Without Borders" evaluates synthetic vs. real data but in code review recommendation, not text classification
- Break condition: If the task requires fine-grained distinctions (e.g., nuanced sentiment categories), 75 examples may be insufficient even with augmentation

## Foundational Learning

- Concept: **Overfitting in supervised classification**
  - Why needed here: The paper frames synthetic imputation as a solution to overfitting caused by insufficient training examples; understanding why small datasets cause overfitting is essential for evaluating the approach
  - Quick check question: Can you explain why a model trained on 50 identical or near-identical examples per category would fail to generalize?

- Concept: **Batch training in Transformers**
  - Why needed here: The paper emphasizes that each batch should contain multiple category examples for reliable gradient estimation; insufficient examples per batch degrades learning
  - Quick check question: With 50 examples across 10 categories and batch size 16, what is the expected number of examples per category per batch?

- Concept: **Few-shot prompting with LLMs**
  - Why needed here: The approach relies on GPT-4o's ability to generalize from 5 examples to generate category-consistent synthetic text; understanding few-shot learning bounds is critical
  - Quick check question: What happens to output quality if the 5 examples are inconsistent or mislabeled?

## Architecture Onboarding

- Component map:
  1. Original data sampler -> Random example selector -> Prompt constructor -> Generative LLM (GPT-4o) -> Output validator -> Data combiner -> Classifier (RoBERTa)

- Critical path:
  1. Identify underrepresented category with 50+ original examples
  2. Validate prompt instructions on small subset (generate 5-10 synthetic texts, review quality)
  3. Generate synthetic texts until total (original + synthetic) reaches 200
  4. Run 10x10 cross-validation to measure F1 and detect overfitting
  5. If overfitting >4%, reduce synthetic ratio or refine prompt

- Design tradeoffs:
  - More original examples → less overfitting but higher annotation cost: 75+ originals recommended; 50 originals yield 2-4% overfitting
  - Higher synthetic ratio → better batch coverage but higher overfitting risk: With 50 originals, adding 100+ synthetic texts may overfit
  - Stronger variation instructions → more diverse texts but risk semantic drift: Monitor for category-consistency violations

- Failure signatures:
  - Synthetic F1 > True F1 by >5%: Indicates overfitting; reduce synthetic count or increase variation instructions
  - Synthetic texts share identical phrases: Prompt not enforcing variation; add explicit constraints
  - Classifier F1 < 0.6 for augmented category: Synthetic texts may have semantic drift; re-validate prompt

- First 3 experiments:
  1. Baseline calibration: Train RoBERTa on original data only with 50, 75, 100 examples; record F1 for target category to establish overfitting threshold
  2. Prompt variation test: Generate 20 synthetic texts with current prompt; manually assess semantic consistency (rate 1-5) and lexical diversity (unique n-gram ratio)
  3. Threshold validation: For your task, test synthetic imputation at 50, 75, 100 originals; compare F1 against full original dataset to identify minimum viable original count

## Open Questions the Paper Calls Out

- Question: To what extent does the small measurement bias introduced by synthetic imputation affect results in downstream statistical inference tasks?
  - Basis in paper: [explicit] The author states in Footnote 31 that "The extent to which the error may affect results in small original samples with more synthetic observations requires further research."
  - Why unresolved: The paper quantifies overfitting (2-4%) but does not validate how this noise propagates into the causal or statistical models researchers ultimately aim to run
  - Evidence: A simulation study measuring the deviation of regression coefficients when trained on synthetic-imputed datasets versus original gold-standard data

- Question: Can open-source generative models (e.g., Llama, Mistral) achieve comparable synthetic imputation performance to GPT-4o?
  - Basis in paper: [inferred] The study relies exclusively on GPT-4o, noting only in a footnote that other models are "good alternatives" without providing empirical validation
  - Why unresolved: Open-source models often have weaker instruction-following capabilities, which could fail to generate texts "sufficiently different" to prevent overfitting
  - Evidence: A benchmark comparison replicating the political nostalgia experiment using Llama 3 to generate the synthetic data

- Question: Is the synthetic imputation approach effective for non-English or low-resource languages?
  - Basis in paper: [inferred] The methodology relies on English prompts and the English-specific RoBERTa-large model, despite one dataset originating in 35 languages
  - Why unresolved: Generative LLMs typically exhibit reduced fluency and instruction adherence in non-English languages, potentially compromising the quality of synthetic imputation
  - Evidence: Applying the approach to a non-English corpus (e.g., Spanish manifestos) and comparing classifier performance against a human-annotated baseline

## Limitations
- The 75-example threshold for equivalent performance to full datasets is assumed to generalize across domains, but task complexity may shift this boundary
- GPT-4o API sampling parameters (temperature, top_p) are unspecified, affecting reproducibility of synthetic text diversity
- The approach assumes LLM instruction-following quality remains consistent across specialized domains and languages

## Confidence
- High: Synthetic imputation outperforms SSMBA by 6-15% in reducing overfitting (based on direct comparison)
- High: 75+ original examples with synthetic imputation achieves equivalent performance to full original datasets (validated on two datasets)
- Medium: Random sampling with replacement prevents overfitting (supported by methodology but not empirically tested against fixed sampling)
- Low: 50-original example threshold with moderate overfitting (2-4%) is acceptable for all applications (depends on task tolerance)

## Next Checks
1. Test synthetic imputation threshold on a task requiring fine-grained distinctions (e.g., nuanced sentiment classification) to verify 75-example minimum
2. Run ablation study comparing random sampling with replacement versus fixed example sets to quantify overfitting reduction
3. Validate synthetic text semantic consistency using automated metrics (e.g., BERTScore) across multiple domain-specific applications