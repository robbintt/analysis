---
ver: rpa2
title: MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms
  Identification in Memes
arxiv_id: '2512.25015'
source_url: https://arxiv.org/abs/2512.25015
tags:
- memes
- meme
- explanations
- depressive
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of identifying fine-grained depressive
  symptoms in memes using multimodal analysis. It introduces RESTOREx, a dataset of
  memes annotated with depressive symptom labels and explanations, both human-written
  and LLM-generated.
---

# MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes

## Quick Facts
- arXiv ID: 2512.25015
- Source URL: https://arxiv.org/abs/2512.25015
- Reference count: 24
- Macro-F1: 72.73% (7.55% improvement over previous state-of-the-art)

## Executive Summary
This paper addresses the task of identifying fine-grained depressive symptoms in memes using multimodal analysis. It introduces RESTOREx, a dataset of memes annotated with depressive symptom labels and explanations, both human-written and LLM-generated. The authors propose MAMA-Memeia, a multi-agent collaborative framework grounded in clinical psychology principles from Cognitive Analytic Therapy. This framework uses multiple LLM agents that discuss and debate predictions based on different knowledge aspects (depression, emotion, culture) before arriving at a consensus. MAMA-Memeia improves upon the previous state-of-the-art by 7.55% in macro-F1 score (achieving 72.73% macro-F1 and 72.45% weighted-F1), outperforming over 30 baseline methods including unimodal and multimodal approaches.

## Method Summary
MAMA-Memeia is a three-phase multi-agent framework for identifying depressive symptoms in memes. Phase 1 involves independent ideation where three specialized LLM agents (GPT-4o, Claude 3.5 Sonnet, Gemini-2.0-flash) each generate predictions using aspect-specific prompts focusing on depression knowledge, emotional content, or cultural references. Phase 2 implements collaborative discussion where agents share predictions, confidence scores, and explanations over multiple rounds to refine their judgments. Phase 3 resolves consensus through weighted voting based on agent confidence scores. The framework operates on RESTOREx, a dataset of 8,926 memes annotated with seven PHQ-9 depressive symptom categories.

## Key Results
- MAMA-Memeia achieves 72.73% macro-F1, a 7.55% improvement over previous state-of-the-art
- The framework outperforms over 30 baseline methods including LLaVA 1.5 (47.00% macro-F1) and BLIP-2 (45.38% macro-F1)
- Multi-aspect prompting improves performance across all tested models compared to vanilla prompting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Assigning each agent a specialized knowledge aspect improves prediction quality over generic prompting.
- **Mechanism:** Three aspect-specific prompts—Depression Knowledge (Pd), Emotional Knowledge (Pe), and Cultural Knowledge (Pc)—decompose the meme understanding task. Each agent focuses on one lens (symptom definitions, emotional states, or figurative/cultural references), generating explanations that capture modality-specific signals before synthesis.
- **Core assumption:** The three aspects are complementary and collectively sufficient; no critical aspect is systematically missing.
- **Evidence anchors:**
  - [section 4.2] "We design three Aspect-specific prompts Pd (Depression Knowledge), Pe (Emotional Knowledge), and Pc (Cultural Knowledge)... expected that the model would be able to recognize the figurative language of the meme across the textual and visual modalities."
  - [table 3] Multi-Aspect prompting improves GPT-4o and Gemini-2.0-flash over vanilla prompting (e.g., GPT-4o: 60→66.68 weighted-F1 with combined aspects).
  - [corpus] Related work on figurative/commonsense infusion for meme classification (arXiv:2501.15321) supports the value of explicit knowledge aspects.
- **Break condition:** If aspect prompts become redundant (high overlap in explanations) or if a key dimension (e.g., risk assessment for self-harm) is under-specified, gains diminish.

### Mechanism 2
- **Claim:** Multi-agent debate enables error correction through cross-agent explanation sharing.
- **Mechanism:** After independent ideation, agents receive others' predictions, confidence scores, and explanations across R rounds. Agents condition on peer reasoning to revise their own judgments, reducing isolated misclassifications.
- **Core assumption:** Agents have sufficient capability to interpret and integrate peer reasoning, and at least one agent often has the correct signal.
- **Evidence anchors:**
  - [abstract] "This framework uses multiple LLM agents that discuss and debate predictions based on different knowledge aspects... before arriving at a consensus."
  - [section 4.3] "We observe that the models frequently correct and influence each other through their detailed explanations... only one of the models (Claude 3.5 Sonnet) correctly predicts the symptom of Self-Harm and corrects the other two models."
  - [corpus] Multi-agent debate frameworks (e.g., ReConcile, AutoGen) show similar gains via consensus reasoning.
- **Break condition:** If agents converge prematurely (groupthink) or if explanation quality is poor, debate may amplify errors rather than correct them.

### Mechanism 3
- **Claim:** Confidence-weighted voting at consensus resolution balances over- and under-prediction across heterogeneous agents.
- **Mechanism:** Each agent produces confidence estimates per symptom; final labels exceed a threshold t based on averaged confidence across agents who predicted that label. This mitigates individual agent biases (e.g., Gemini over-predicts, GPT-4o under-predicts).
- **Core assumption:** Confidence estimates are calibrated enough to serve as meaningful weights, and averaging neutralizes systematic bias.
- **Evidence anchors:**
  - [section 4.3] "The study (Xiong et al. 2024) has shown the effectiveness of LLM-derived confidence scores... The consensus resolution algorithm is used in order to eliminate labels for which the agents have low confidence."
  - [figure 5] Qualitative analysis shows Gemini prone to over-prediction, GPT-4o prone to under-prediction; MAMA-Memeia averages predictions.
  - [corpus] Limited direct corpus evidence on confidence calibration in multi-agent setups; this remains an assumption.
- **Break condition:** If confidence scores are systematically miscalibrated (e.g., always high regardless of correctness), weighting provides no benefit.

## Foundational Learning

- **Multi-label classification with imbalanced classes:**
  - Why needed here: Seven PHQ-9 symptoms are not mutually exclusive; label distribution is skewed (e.g., LOI rare in test set).
  - Quick check question: Can you compute macro-F1 vs. weighted-F1 and explain why they differ?

- **Multimodal reasoning (text + image) in figurative contexts:**
  - Why needed here: Memes require integrating OCR text with visual cues, often via sarcasm, irony, or metaphor.
  - Quick check question: Given a meme with mismatched text and image sentiment, how would you describe the cross-modal signal?

- **Prompt engineering with domain knowledge injection:**
  - Why needed here: Aspect prompts embed clinical definitions and cultural knowledge to guide LLM reasoning.
  - Quick check question: What information must a Pd (depression knowledge) prompt contain to reduce misinterpretation of symptoms?

## Architecture Onboarding

- **Component map:**
  - Input: Meme image (M_img) + user prompt (P_usr)
  - Agents: A1 (GPT-4o), A2 (Claude 3.5 Sonnet), A3 (Gemini-2.0-flash), each with aspect-specific system prompt Pa,i ∈ {Pd, Pe, Pc}
  - Phase 1 (Independent Ideation): Each agent generates (prediction, confidence, explanation)
  - Phase 2 (Collaborative Discussion): R rounds of sharing predictions/confidences/explanations
  - Phase 3 (Consensus Resolution): Weighted vote via Eq. 3 with threshold t

- **Critical path:**
  1. Assign aspect prompts to agents (no two agents should share the same Pa).
  2. Run Independent Ideation to collect initial (p, c, e).
  3. For each round r, construct Pdsc with peer outputs; agents update (p, c, e).
  4. Apply consensus resolution to produce final labels L.

- **Design tradeoffs:**
  - Closed-source vs. open-source LLMs: Closed-source yield higher performance but limit interpretability and reproducibility.
  - Number of rounds R: More rounds may improve consensus but increase latency and cost.
  - Threshold t: Lower t increases recall (more labels); higher t increases precision.

- **Failure signatures:**
  - All agents converge to identical predictions early (insufficient diversity).
  - Explanations are fluent but irrelevant (aspect prompts not grounding reasoning).
  - Systematic over/under-prediction persists after consensus (confidence miscalibration).

- **First 3 experiments:**
  1. **Ablate aspect-specific prompting:** Run MAMA-Memeia with vanilla prompts (no Pd/Pe/Pc) and compare macro-F1 to confirm the contribution of multi-aspect design.
  2. **Vary number of discussion rounds R:** Test R ∈ {0, 1, 2, 3} to identify saturation point where additional rounds yield no improvement.
  3. **Calibration check on confidence scores:** Plot predicted confidence vs. accuracy per agent to assess whether weights in Eq. 3 are meaningful or need recalibration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source LLMs be effectively adapted to match the performance of closed-source models for depressive symptom identification in sensitive mental health domains?
- Basis in paper: [explicit] The conclusion states: "we look forward to developing methods for effective utilization of open-source LLMs in future work given the pressing need for open and transparent research on LLMs in sensitive domains such as mental health."
- Why unresolved: The paper shows closed-source models (Claude 3.5 Sonnet, GPT-4o, Gemini) substantially outperform open-source alternatives (LLaVA 1.5, LLaVA-NeXT, MiniCPM-V), with Claude achieving 68.39% macro-F1 versus LLaVA 1.5's 47%.
- What evidence would resolve it: Demonstrating an open-source multi-agent framework achieving comparable (within 2-3%) macro-F1 scores to MAMA-Memeia on RESTOREx, potentially through fine-tuning, knowledge distillation, or specialized prompting strategies.

### Open Question 2
- Question: What mechanisms cause the systematic over-prediction tendency in Gemini-2.0-flash and under-prediction in GPT-4o, and can these behaviors be corrected?
- Basis in paper: [explicit] The qualitative analysis states: "given the black-box nature of these models, further analysis is required for detailed understanding of this phenomenon" regarding observed prediction patterns.
- Why unresolved: The authors observe the pattern correlates with explanation length but cannot determine causal factors due to model opacity.
- What evidence would resolve it: Systematic ablation studies across model versions, controlled experiments varying explanation length constraints, and analysis of token-level attention patterns during prediction.

### Open Question 3
- Question: What is the optimal configuration (number of agents, discussion rounds, consensus threshold) for multi-agent frameworks in mental health classification tasks?
- Basis in paper: [inferred] The methodology fixes the framework at 3 agents and does not ablate the number of rounds or systematically vary the consensus threshold t.
- Why unresolved: The paper demonstrates that multi-agent collaboration improves performance but does not explore whether additional agents or rounds yield diminishing returns or if the current configuration is optimal.
- What evidence would resolve it: Ablation experiments varying agent count (2-7 agents), discussion rounds (0-5), and threshold values (0.3-0.7) with statistical significance testing on symptom-level F1 scores.

## Limitations

- The dataset creation process relies on LLM-generated explanations for training, which may introduce annotation bias or hallucinations not representative of human reasoning.
- The choice of exactly three aspects and three agents represents a design decision that may not be optimal - the paper does not explore whether different aspect combinations or agent counts would yield better performance.
- The multi-agent debate mechanism assumes agents can meaningfully interpret and build upon peer explanations, but this collaborative reasoning quality is not quantitatively measured.

## Confidence

**High confidence**: The 7.55% macro-F1 improvement over state-of-the-art and the systematic ablation showing multi-aspect prompting helps are well-supported by the experimental results.

**Medium confidence**: The claim that confidence-weighted voting effectively balances over/under-prediction relies on observed patterns but lacks rigorous calibration analysis to confirm the confidence scores are meaningful.

**Medium confidence**: The assertion that the three aspects (depression, emotional, cultural) are complementary and sufficient is plausible given the results but not definitively proven - missing aspects could exist.

## Next Checks

1. **Confidence Calibration Analysis**: Plot predicted confidence vs. actual accuracy for each agent to verify whether confidence scores are well-calibrated before using them in weighted voting.

2. **Ablation of Discussion Rounds**: Systematically test MAMA-Memeia with R=0,1,2,3 discussion rounds to identify the point of diminishing returns and validate that multi-agent debate provides measurable benefit beyond independent predictions.

3. **Alternative Aspect Combinations**: Experiment with different aspect prompt configurations (e.g., adding a "risk assessment" aspect, or testing two-aspect vs. three-aspect setups) to verify whether the current three-aspect design is optimal.