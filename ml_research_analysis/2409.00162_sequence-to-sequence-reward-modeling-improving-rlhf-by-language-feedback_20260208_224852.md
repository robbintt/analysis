---
ver: rpa2
title: 'Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback'
arxiv_id: '2409.00162'
source_url: https://arxiv.org/abs/2409.00162
tags:
- seq2seq
- reward
- feedback
- arxiv
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward model bias and unexpected
  generalization in reinforcement learning from human feedback (RLHF) for aligning
  large language models. The core issue is that traditional sequence-to-scalar reward
  models often capture patterns unrelated to genuine human preferences, leading to
  misalignment where models optimize for high rewards but fail to achieve true alignment
  objectives.
---

# Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback

## Quick Facts
- arXiv ID: 2409.00162
- Source URL: https://arxiv.org/abs/2409.00162
- Reference count: 3
- 76.9% average win rate across 2B and 7B parameter models on three NLP tasks

## Executive Summary
This paper addresses reward model bias and unexpected generalization in reinforcement learning from human feedback (RLHF) by introducing a sequence-to-sequence (seq2seq) reward modeling approach. Traditional sequence-to-scalar reward models often capture spurious correlations unrelated to genuine human preferences, leading to misalignment. The authors propose learning from language feedback through sequence maximum likelihood estimation, replacing binary classification with correction mapping and identity mapping. Experiments demonstrate that this method effectively mitigates long-response bias in text summarization and refusal-to-response paradigm in safety dialogues while providing more accurate token-level credit assignment.

## Method Summary
The method consists of two stages: reward modeling and reward extracting. In reward modeling, the approach learns correction mapping (mapping rejected responses to chosen responses) and identity mapping (mapping chosen responses to themselves) using sequence maximum likelihood estimation. During reward extracting, for each generated token, the method compares against seq2seq RM output: tokens matching before the first divergence point receive +1, tokens diverging receive -1, and matching tokens after divergence receive 0. This dense token-level feedback is then used in PPO optimization, with token-level rewards (PPO-T) showing superior performance to sentence-level aggregation (PPO-S).

## Key Results
- Achieved 76.9% average win rate across 2B and 7B parameter models on three NLP tasks
- Reduced long-response bias from 15-27 tokens (scalar RM) to 2 tokens (seq2seq RM) in text summarization
- Mitigated refusal-to-response paradigm, maintaining utility score distribution where scalar RM caused collapse
- Demonstrated robustness to out-of-distribution prompts and improved token-level credit assignment accuracy

## Why This Works (Mechanism)

### Mechanism 1: Language Space Preservation Over Scalar Compression
The seq2seq approach preserves fine-grained distinctions in language space that scalar compression destroys. Traditional seq2scalar RMs compress token-level differences into single scalars, losing recoverable preference information. By learning correction mapping and identity mapping via sequence MLE, the reward signal remains in interpretable language space where token contributions stay distinguishable.

### Mechanism 2: Token-Level Credit Assignment via Consistency Detection
Fine-grained positive/negative token scores enable precise credit assignment impossible with sentence-level scalar rewards. The reward extractor compares LLM output tokens against seq2seq RM generation, assigning +1 for initially matching tokens, then -1 for tokens the RM wouldn't generate and 0 for matching tokens in divergent contexts. This creates dense, targeted feedback for optimization.

### Mechanism 3: Bias Suppression through Language-Space Constraint
Seq2seq RM mitigates reward hacking because language-space optimization cannot easily exploit scalar shortcuts. Scalar RMs conflate length with quality (61% of chosen responses in Dialogsum are longer), but seq2seq RM learns to generate actual chosen responses rather than score them. Similarly, learning "refuse"→"helpful response" correction is harder than learning "refuse"→high_scalar, preventing refusal-to-response collapse.

## Foundational Learning

- **Concept: Maximum Likelihood Estimation (Binary vs Sequence)**
  - Why needed here: Core innovation is replacing binary classification MLE with sequence generation MLE
  - Quick check question: Given a preference pair (chosen: "Brief summary", rejected: "Verbose summary"), what would binary MLE optimize vs what sequence MLE would optimize?

- **Concept: Token-Level Markov Decision Process**
  - Why needed here: LLM generation is formulated as token-level MDP; reward extraction operates at token granularity
  - Quick check question: In the MDP formulation, what constitutes the state at timestep t=5 for a prompt "Summarize: " and partial generation "The main"?

- **Concept: Credit Assignment in Reinforcement Learning**
  - Why needed here: Method's value proposition is improved credit assignment; token-level scoring rationale requires this foundation
  - Quick check question: Why is credit assignment harder with sparse sentence-level rewards than dense token-level rewards?

## Architecture Onboarding

- **Component map:**
```
Preference Dataset (x, y_chosen, y_rejected)
           ↓
Seq2seq RM Training (correction mapping + identity mapping via sequence MLE)
           ↓
Reward Extraction (per token: +1 until divergence, -1 after, 0 for matches)
           ↓
PPO Update with Token Rewards
```

- **Critical path:**
  1. Data preparation: Ensure preference pairs exist; rejected responses must be mappable to chosen responses lexically
  2. Seq2seq RM training: Train on correction + identity loss jointly; do not train correction alone
  3. Reward extraction implementation: Implement two-phase scoring (positive until divergence, then discriminative negative)
  4. PPO integration: Feed token-level rewards directly; aggregation to sentence-level (PPO-S) is fallback option

- **Design tradeoffs:**
  - PPO-T vs PPO-S: Token-level (PPO-T) outperforms sentence-level aggregation (PPO-S) but requires per-token RM inference—compute cost ×sequence_length
  - Positive-only scoring: Simpler but causes 68-86% empty generation rate in practice; negative scores are structurally necessary
  - Reuse existing SFT model as RM vs train new: Paper uses same architecture; Assumption: SFT model has sufficient capacity for correction learning

- **Failure signatures:**
  - Empty generation: Model outputs nothing → positive-score-only configuration detected
  - Length explosion (Δ > 20 tokens): Scalar RM still being used → check reward extraction is token-level
  - Utility collapse (variance → 0, mean drops sharply): Refusal-to-response paradigm → RM not learning proper corrections
  - OOD failure: RM outputs incoherent corrections for unseen prompt distributions → check training data coverage

- **First 3 experiments:**
  1. Bias replication test: Run PPO vs PPO-T on Dialogsum; measure response length delta. Expect: PPO Δ ≈ +15-27 tokens, PPO-T Δ ≈ +2 tokens
  2. Ablation: positive-only: Run PPO-T-Pos on PKU-SafeRLHF; measure empty generation rate. Expect: >65% empty responses
  3. OOD robustness: Train RM on TL;DR, test on Xsum prompts. Expect: PPO-T maintains ~70%+ win rate over SFT baseline

## Open Questions the Paper Calls Out

- **Question:** How does the token-level inference overhead of the Seq2Seq RM impact the scalability of RLHF for models significantly larger than 7B parameters?
- **Question:** How does the method perform on preference datasets where chosen and rejected responses have minimal lexical overlap or shared prefixes?
- **Question:** Is the binary assignment of negative scores (-1) for all divergent tokens too coarse for nuanced credit assignment?

## Limitations
- Lack of hyperparameter details (learning rates, batch sizes, epochs, KL coefficient) in main paper
- Limited validation of OOD robustness claims to single-domain generalization tests
- Assumption that rejected responses can be meaningfully mapped to chosen responses lexically may not hold for all domains
- Computational overhead of token-level reward extraction not quantified for larger models

## Confidence

**High Confidence (7+ evidence anchors):**
- Seq2seq RM architecture design and training procedure are clearly specified
- Token-level reward extraction mechanism is well-defined and reproducible
- Experimental results showing win rates (76.9% average) are directly reported with clear metrics
- Two failure modes (empty generation with positive-only scoring, refusal-to-response collapse) are empirically validated

**Medium Confidence (4-6 evidence anchors):**
- Language space preservation prevents scalar shortcut exploitation
- Seq2seq RM mitigates both long-response bias and refusal-to-response paradigm
- Comparison with related work addressing similar RLHF challenges
- Computational cost implications of token-level vs sentence-level reward aggregation

**Low Confidence (0-3 evidence anchors):**
- Exact mechanism by which seq2seq RM achieves OOD robustness
- Generalizability of results across different model scales beyond tested 2B and 7B parameters
- Long-term stability of trained seq2seq RM during extended PPO training

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary learning rates, batch sizes, and KL coefficients to determine their impact on win rates and bias mitigation. Document optimal ranges and verify 76.9% win rate reproducibility across multiple random seeds.

2. **Cross-domain OOD robustness test**: Train seq2seq RM on TL;DR summaries, then evaluate PPO performance on completely different domains (medical Q&A, code generation). Measure win rates against both SFT baseline and scalar RM baseline to quantify true generalization capabilities beyond single-domain transfer.

3. **Token-level credit assignment fidelity**: Conduct ablation studies comparing seq2seq RM's token-level rewards against ground truth human token-level annotations (where available) or against established credit assignment methods like SCAR. Calculate correlation metrics to verify that +1/-1/0 scoring accurately reflects human preference signals at the token level.