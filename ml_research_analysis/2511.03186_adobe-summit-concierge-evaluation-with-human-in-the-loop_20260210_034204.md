---
ver: rpa2
title: Adobe Summit Concierge Evaluation with Human in the Loop
arxiv_id: '2511.03186'
source_url: https://arxiv.org/abs/2511.03186
tags:
- data
- evaluation
- queries
- questions
- adobe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adobe Summit Concierge is a generative AI assistant for Adobe Summit
  events that handles diverse event-related queries. To address data sparsity, response
  reliability, and rapid deployment challenges, the team adopted a human-in-the-loop
  development workflow combining prompt engineering, retrieval grounding, and lightweight
  human validation.
---

# Adobe Summit Concierge Evaluation with Human in the Loop

## Quick Facts
- arXiv ID: 2511.03186
- Source URL: https://arxiv.org/abs/2511.03186
- Authors: Yiru Chen; Sally Fang; Sai Sree Harsha; Dan Luo; Vaishnavi Muppala; Fei Wu; Shun Jiang; Kun Qian; Yunyao Li
- Reference count: 11
- Primary result: AI assistant for event queries built using synthetic data and human-in-the-loop validation achieved 100% routing accuracy for structured queries

## Executive Summary
Adobe Summit Concierge is a generative AI assistant deployed for Adobe Summit events that handles diverse queries about schedules, speakers, and logistics. The team faced data sparsity, response reliability, and rapid deployment challenges, leading them to adopt a human-in-the-loop development workflow. This approach combined prompt engineering, retrieval grounding, and lightweight human validation to ensure quality without extensive historical data.

The evaluation framework addressed both structured database queries (converted to SQL) and unstructured document queries (answered via retrieval-augmented generation). By leveraging synthetic query generation from seed templates and LLM expansion, the team created evaluation benchmarks that reduced manual review requirements by over 90%. The system achieved significant improvements in routing accuracy, autocomplete relevance, and multi-turn query handling while maintaining brand compliance.

## Method Summary
The team developed a dual-path AI assistant architecture with intent-based routing to either NL2SQL (structured) or RAG (unstructured) pipelines. They generated synthetic queries from domain expert seed templates using LLM paraphrasing and SQLSynth, creating evaluation benchmarks for both structured and unstructured data. LLM-as-judge with confidence thresholding reduced human annotation needs from 3,000 to 220 queries. The evaluation included correctness scoring, side-by-side comparison, and brand compliance checks with human review for low-confidence cases.

## Key Results
- Autocomplete suggestions improved from 27% to 58% relevance across three question pool iterations
- Keystroke savings increased from 6.09 to 11.45 characters through improved suggestions
- Multi-turn query handling reduced rewrite error rate from 4.35% to 1.45%
- Structured query routing achieved 100% accuracy using SQL benchmarks
- LLM-assisted evaluation reduced manual annotation from 1,500 to 276 queries

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Query Generation with Human-in-the-Loop Validation
- Claim: Synthetic queries generated from seed templates and database schemas can bootstrap AI assistant evaluation in cold-start scenarios where historical interaction data is unavailable.
- Mechanism: SME-provided seed templates → LLM expansion/SQLSynth generation → Human review for fluency and scope → Evaluation benchmark + Autocomplete pool
- Core assumption: Domain experts can anticipate high-value query patterns; LLM paraphrases maintain semantic equivalence to real user intents.
- Evidence anchors:
  - [abstract]: "synthetic queries, documentation-grounded retrieval, and prompt engineering to overcome data sparsity"
  - [section 4.1]: "we work with product managers (PMs) and the Adobe Summit marketing team to curate an initial set of 10 high-value seed questions... expanded using an LLM, prompted to generate paraphrases... more than 17K queries"
  - [section 4.1]: SQLSynth generated 269 queries in under 30 minutes with 100% routing accuracy
  - [corpus]: Limited direct corpus evidence for synthetic query effectiveness in cold-start; related papers focus on RAG architectures
- Break condition: Seed questions poorly represent actual user intent distribution; expanded queries diverge from real-world phrasing patterns.

### Mechanism 2: Dual-Path Intent Routing with Specialized Pipelines
- Claim: Intent-based routing to NL2SQL (structured) vs RAG (unstructured) pipelines improves response accuracy by matching query type to appropriate data source.
- Mechanism: Query → Intent classification → Structured path (SQL generation against KG/warehouse) OR Unstructured path (document retrieval) → Unified answer synthesis
- Core assumption: Queries can be cleanly categorized; structured queries have deterministic answers in database schemas.
- Evidence anchors:
  - [abstract]: "100% routing accuracy for structured queries"
  - [section 3]: "The rewritten query is then routed by an intent detection module, which classifies it as either unstructured or structured"
  - [section 5.3]: Routing accuracy improved from 89.1% to 96.1% after prompt improvements
  - [corpus]: "Detecting Ambiguities to Guide Query Rewrite" confirms routing challenges persist in enterprise multi-turn settings
- Break condition: Ambiguous queries spanning both domains cause misrouting; multi-turn context resolution introduces cascade errors.

### Mechanism 3: LLM-as-Judge with Confidence-Calibrated Human Review
- Claim: LLM-based evaluation with confidence thresholding reduces human annotation burden while maintaining quality through targeted human review on uncertain cases.
- Mechanism: LLM evaluates responses with confidence scores → Low-confidence or flagged cases → Human reviewers validate subset only
- Core assumption: LLM confidence correlates with actual correctness; humans catch systematic errors and edge cases.
- Evidence anchors:
  - [abstract]: "multi-faceted evaluation (correctness, side-by-side comparison, brand compliance) ensured response quality"
  - [section 5.2]: "Out of 3,000 templated queries, only 220 requires human reviews. The rest were automatically verified"
  - [section 5.4]: "reduced manual annotation needs from 1500 queries to just 276"
  - [corpus]: Weak corpus validation for LLM-as-judge calibration methods; "llm-as-a-judge" paper cited but empirical calibration evidence limited
- Break condition: LLM confidence scores are miscalibrated; systematic biases pass through without human detection.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core mechanism for answering unstructured queries from the ABC Guidebook and live-authored content without fine-tuning.
  - Quick check question: Given a user query "Where can I park during the Summit?", explain how retrieved documents are incorporated into the LLM prompt to generate a grounded response.

- **Concept: Natural Language to SQL (NL2SQL)**
  - Why needed here: Enables structured queries over session/speaker/sponsor databases with deterministic, schema-grounded answers.
  - Quick check question: For the query "What keynotes does Shantanu Narayen speak at?", what SQL components (tables, joins, filters) would the system need to generate?

- **Concept: Chain-of-Thought Prompting for Query Rewriting**
  - Why needed here: Resolves temporal and contextual ambiguities in multi-turn conversations (e.g., "What sessions are after lunch?" → "What sessions are scheduled after 1 PM on Day 2?").
  - Quick check question: How would you prompt an LLM to resolve an ambiguous follow-up like "Tomorrow?" given prior conversation context?

## Architecture Onboarding

**Component map:**
User Query -> [Autocomplete Module] <- Question Pool (templates + live queries)
 ↓
[Query Rewriting] <- Chat History + Chain-of-Thought
 ↓
[Intent Detection Router]
 ↓
 ├─→ [RAG Pipeline] -> Unstructured docs (ABC Guide)
 │       ↓
 │   [Retrieval] -> [Answer Generation]
 │
 └─→ [NL2SQL Pipeline] -> Structured DB (Sessions/Speakers/Sponsors)
         ↓
     [SQL Execution] -> [Answer Generation]
 ↓
[Unified Response Synthesis] -> User

**Critical path:** Query → Autocomplete (optional) → Rewrite → Intent Router → (RAG OR NL2SQL) → Response synthesis. Latency dominated by retrieval and LLM inference.

**Design tradeoffs:**
- **Freshness vs Privacy:** Attendee schedules queried via real-time API (fresh, private) vs ingesting into warehouse (faster queries, stale data, privacy concerns).
- **Coverage vs Precision:** Large autocomplete pool increases coverage but risks irrelevant suggestions; smaller curated pool improves relevance.
- **Automation vs Quality:** LLM-as-judge reduces annotation effort but may miss systematic errors; more human review improves quality at cost of speed.

**Failure signatures:**
- Rewrite Error Rate: 4.35% → 1.45% (after prompt improvements)
- Routing Accuracy: 89.1% → 96.1% (after iteration)
- Out-of-scope routing errors: 4% → 3% (after human-in-the-loop annotation)
- Autocomplete relevance: 27% → 58% across three question pool iterations
- Keystroke savings: 6.09 → 11.45 characters (improved suggestion quality)

**First 3 experiments:**
1. **Baseline routing validation:** Generate 200-300 SQLSynth queries from your schema; measure intent detection accuracy and NL2SQL execution success rate.
2. **Autocomplete pool iteration:** Create 10-20 seed templates with domain experts, expand via LLM paraphrasing, measure relevance improvement on held-out test queries.
3. **LLM-as-judge calibration:** Implement confidence-thresholded evaluation on 500+ queries; measure human annotation reduction rate and spot-check auto-verified cases for false positives.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic query generation effectiveness for cold-start scenarios lacks validation against real user interaction data
- LLM-as-judge calibration methodology has limited empirical validation and corpus support
- Routing mechanism improvements lack ablation studies isolating specific prompt engineering contributions

## Confidence

**High Confidence:** Structured query evaluation methodology (SQL benchmarks, LLM-assisted first-pass judging) and multi-turn query handling improvements (rewrite error rate reduction from 4.35% to 1.45%).

**Medium Confidence:** Unstructured data evaluation framework (correctness scoring, side-by-side comparison) and autocomplete suggestion improvements (relevance increase from 27% to 58%).

**Low Confidence:** Synthetic query generation effectiveness for cold-start scenarios and LLM-as-judge calibration methodology due to limited empirical validation and corpus support.

## Next Checks
1. Conduct A/B testing comparing synthetic query-based evaluation results against actual user interaction data once available to validate the bootstrapping approach.
2. Implement systematic calibration studies for LLM-as-judge confidence thresholds, measuring precision-recall tradeoffs and conducting human validation of auto-verified cases.
3. Perform ablation studies on routing prompt improvements to isolate which specific prompt engineering techniques contributed most to the 7% accuracy gain.