---
ver: rpa2
title: 'Adapt, But Don''t Forget: Fine-Tuning and Contrastive Routing for Lane Detection
  under Distribution Shift'
arxiv_id: '2507.18653'
source_url: https://arxiv.org/abs/2507.18653
tags:
- lane
- detection
- distribution
- fine-tuning
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  lane detection models when adapting to new datasets under distribution shift. The
  proposed method introduces a modular branching strategy that fine-tunes only target-specific
  components while keeping the source branch fixed, along with a supervised contrastive
  learning model for distribution-aware routing during inference.
---

# Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift

## Quick Facts
- arXiv ID: 2507.18653
- Source URL: https://arxiv.org/abs/2507.18653
- Reference count: 40
- Primary result: Near-optimal F1-scores achieved on multiple lane detection datasets while preventing catastrophic forgetting through modular branching and contrastive routing

## Executive Summary
This paper addresses catastrophic forgetting in lane detection models when adapting to new datasets under distribution shift. The proposed method introduces a modular branching strategy that fine-tunes only target-specific components while keeping the source branch fixed, along with a supervised contrastive learning model for distribution-aware routing during inference. Experiments on three datasets (CULane, CurveLanes, AssistTaxi) with CLRerNet and ERFNet backbones show that the approach achieves near-optimal F1-scores (e.g., 94.5 on AssistTaxi, 80.0 on CurveLanes) while using significantly fewer parameters than training separate models, and completely avoids source forgetting.

## Method Summary
The method combines modular branching with supervised contrastive learning for lane detection under distribution shift. A base model is first trained on the source distribution, then cloned target branches (Neck+Head or deeper) are fine-tuned on new distributions while keeping the source branch frozen. A separate supervised contrastive learning router classifies inputs to the appropriate branch at inference time. The approach uses CLRerNet or ERFNet backbones with anchor-based detection heads, training the base model for 15 epochs and target branches for 3 epochs, while the SCL router learns to separate distributions in embedding space.

## Key Results
- Achieves 94.5 F1 on AssistTaxi and 80.0 F1 on CurveLanes with minimal parameter overhead
- Completely prevents source forgetting (source F1 remains constant across all routing configurations)
- SCL router achieves near-perfect classification accuracy (99.6-99.9%) across all datasets
- N+H routing configuration provides best balance of performance and efficiency for moderate distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
Freezing source branches while creating separate target branches prevents catastrophic forgetting. Parameter isolation preserves source-specific representations by preventing gradient updates from overwriting learned weights. Target branches adapt independently without interfering with frozen source weights. Core assumption: knowledge is sufficiently localized to specific parameter subsets that selective freezing can preserve it. Evidence: source F1 remains constant across all routing configurations, demonstrating zero forgetting when source branch is frozen.

### Mechanism 2
Neck + Head adaptation achieves strong target performance with minimal forgetting for moderate distribution shifts. The neck (FPN) redistributes spatial attention and refines multi-scale features, while the head recalibrates anchor-specific predictions. Frozen backbone retains transferable low-level structure. Core assumption: target distribution shift primarily affects mid-to-high-level representations rather than low-level features. Evidence: N+H configuration achieves strong target F1 with minimal source F1 drop using only ~600K parameters (5.1% of total).

### Mechanism 3
Supervised contrastive learning enables accurate distribution classification for inference-time routing. SCL pulls same-distribution embeddings together while pushing different-distribution embeddings apart, creating separable clusters. Centroid-based nearest-neighbor classification assigns inputs to branches. Core assumption: distributions are visually distinguishable in a learned embedding space. Evidence: SCL model achieves near-perfect accuracy (99.6-99.9%) across all datasets, with t-SNE visualization showing strong inter-distribution separation.

## Foundational Learning

- **Catastrophic forgetting**: When fine-tuning a model on new data destroys performance on the original data. Understanding this motivates the branching solution to prevent gradient updates from overwriting source-specific weights.
  - Quick check: If you fine-tune a model on dataset B after training on A, what happens to performance on A without mitigation?

- **Feature pyramid networks (FPN)**: The "neck" component fuses multi-scale features from different layers. Understanding this explains why neck adaptation helps with geometric shifts like varying curvature and spacing in lane detection.
  - Quick check: Why would multi-scale feature fusion matter for detecting lanes of varying curvature?

- **Supervised contrastive loss**: The routing mechanism depends on SCL to separate distributions using label information to define positive pairs. Unlike self-supervised contrastive learning, SCL uses ground truth distribution labels.
  - Quick check: In SCL, what defines a "positive" pair versus a "negative" pair?

## Architecture Onboarding

- **Component map**: Backbone (B) -> Neck (N) -> Head (H) -> Output, plus SCL Router
- **Critical path**: Train base model on source (15 epochs) → clone target branches while freezing source → fine-tune target branches (3 epochs) → train SCL router on all distributions → at inference, SCL classifies input → route to appropriate branch
- **Design tradeoffs**: Routing@H uses fewest parameters (38% relative) but limited adaptation capacity; Routing@N+H provides best balance for moderate shifts (41% parameters, strong target F1); Routing@B(k=2)+N+H required for severe semantic shifts (96% parameters)
- **Failure signatures**: Source F1 collapses to near-zero when fine-tuning only H or N+H on severe shifts → need deeper backbone adaptation; Target F1 near-zero when anchor priors misaligned; Router misclassification would manifest as wrong branch selection
- **First 3 experiments**: 1) Reproduce baseline: train CLRerNet on CULane, evaluate zero-shot on CurveLanes/AssistTaxi; 2) Clone N+H for target, freeze source, compare against H-only and B(k=2)+N+H; 3) Train SCL router on 80% of each dataset, evaluate on held-out 20% to confirm generalization

## Open Questions the Paper Calls Out
- Does modular branching effectively mitigate catastrophic forgetting in non-anchor-based lane detection architectures like segmentation-based or parametric models?
- How does the parameter overhead scale as the number of target distributions increases significantly?
- Can student-teacher distillation or shared-feature decoders consolidate branch knowledge to improve parameter efficiency?

## Limitations
- Limited to lane detection task, leaving applicability to other vision tasks uncertain
- Assumes lane detection distributions are visually separable in embedding space, which may fail for subtle domain shifts
- Three-epoch fine-tuning protocol is fixed without exploration of optimal duration or different initialization strategies

## Confidence
- Catastrophic forgetting prevention: High (direct evidence from constant source F1 scores)
- Routing accuracy: High (reported >99% accuracy across all datasets)
- Efficiency claims: Medium (parameter counts clear but three-epoch assumption untested for all cases)

## Next Checks
1. Evaluate routing accuracy on a held-out test distribution (synthetic lane data or different weather conditions) to verify SCL router generalizes beyond training distributions
2. Systematically vary fine-tuning epochs (1, 3, 5, 10) for each routing point to identify optimal training duration and potential overfitting patterns
3. Create ambiguous inputs (blended images, rare lane types) and analyze SCL routing decisions to identify confidence thresholds and misclassification patterns