---
ver: rpa2
title: Fine-tuning Done Right in Model Editing
arxiv_id: '2509.22072'
source_url: https://arxiv.org/abs/2509.22072
tags:
- uni00000013
- editing
- uni00000011
- fine-tuning
- locft-bf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fine-tuning has been widely dismissed in model editing due to\
  \ perceived limitations, but this study shows that its failure stems from flawed\
  \ implementation rather than inherent unsuitability. By correcting the training\
  \ pipeline to a breadth-first approach with mini-batch optimization, fine-tuning\u2019\
  s effectiveness for model editing is substantially improved."
---

# Fine-tuning Done Right in Model Editing

## Quick Facts
- arXiv ID: 2509.22072
- Source URL: https://arxiv.org/abs/2509.22072
- Authors: Wanli Yang; Fei Sun; Rui Tang; Hongyu Zang; Du Su; Qi Cao; Jingang Wang; Huawei Shen; Xueqi Cheng
- Reference count: 23
- Primary result: Fine-tuning outperforms state-of-the-art model editing methods by large margins when implemented with breadth-first pipeline and mini-batch optimization

## Executive Summary
This study challenges the conventional wisdom that fine-tuning is unsuitable for model editing, demonstrating that previous failures stemmed from flawed implementation rather than inherent limitations. By adopting a breadth-first pipeline with mini-batch optimization and targeting MLP down-projection matrices in later layers, the authors achieve state-of-the-art performance across three LLM families (LLaMA3-8B, Qwen2.5-7B, Qwen2.5-72B). The resulting method, LocFT-BF, sustains 100K edits and scales effectively to 72B-parameter models without sacrificing general capabilities.

## Method Summary
LocFT-BF implements a breadth-first fine-tuning pipeline where edit requests are processed across multiple epochs rather than sequentially to convergence. The method targets MLP down-projection matrices in later layers using mini-batch gradient updates (batch size ≥8) with conservative learning rates (1e-5 to 5e-5). This approach mitigates catastrophic overwriting of earlier edits while preserving model capabilities. The method requires no custom loss functions beyond standard cross-entropy masked on prompt tokens.

## Key Results
- LocFT-BF outperforms state-of-the-art methods like LoRA, EVE, and AdaLoRA by large margins on three LLM families
- The method sustains 100K edits, representing a 10× improvement over prior practice
- Scales effectively to 72B-parameter models while maintaining both reliability (>95%) and capability preservation (<2% degradation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The breadth-first pipeline mitigates catastrophic overwriting of earlier edits during sequential knowledge updates.
- Mechanism: By iterating over the entire dataset across epochs rather than optimizing each sample to convergence before advancing, the breadth-first approach distributes gradient updates across all edits simultaneously, preventing later edits from dominating the parameter space at the expense of earlier ones.
- Core assumption: Sequential edits share overlapping parameter subspaces, and single-pass depth-first optimization creates interference patterns that unlearning prior edits.
- Evidence anchors:
  - [abstract] "this depth-first pipeline coupled with sample-wise updating over-optimizes each edit and induces interference across edits"
  - [section 2.3] Figure 3 visualization shows earlier shards declining under DF while all shards improve jointly under BF
  - [corpus] Related work on knowledge editing persistence after fine-tuning (arXiv:2511.05852) suggests editing and fine-tuning interact in complex ways, supporting interference concerns
- Break condition: If edits are highly isolated in parameter space with minimal overlap, the distinction between DF and BF pipelines would diminish.

### Mechanism 2
- Claim: Mini-batch gradient aggregation stabilizes model capabilities during editing by reducing high-variance single-sample updates.
- Mechanism: Single-sample updates (batch size = 1) produce noisy gradient estimates that cause unstable parameter drift in unrelated knowledge regions. Mini-batch optimization averages gradients across samples, regularizing the update direction and preserving general capabilities.
- Core assumption: Parameter updates for specific factual edits also affect unrelated model regions proportionally to gradient variance.
- Evidence anchors:
  - [section 2.4] Figure 4 shows capability performance improves substantially as batch size increases for FT-M, with FT-M surpassing AdaLoRA at larger batches
  - [abstract] "restoring fine-tuning to the standard breadth-first pipeline with mini-batch optimization substantially improves its effectiveness"
  - [corpus] Evidence on fine-tuning fragility after editing (arXiv:2506.18428) indicates parameter changes can persist or degrade unpredictably
- Break condition: If the model has extremely sparse knowledge representations where each fact localizes to nearly disjoint parameters, batch size effects would be minimal.

### Mechanism 3
- Claim: Editing MLP down-projection matrices in later layers achieves high reliability while minimizing capability degradation.
- Mechanism: Later MLP layers encode more specialized factual associations that can be modified with less disruption to general reasoning patterns. The down-projection specifically transforms intermediate representations toward vocabulary outputs, making it a more direct locus for factual knowledge than attention or earlier layers.
- Core assumption: Factual knowledge in LLMs has a layer-dependent distribution, with later MLP layers encoding more specific, mutable associations.
- Evidence anchors:
  - [section 3] Figure 5 shows MLPdown consistently maintains high reliability across all three LLMs with better capability preservation
  - [section 3] "tuning the down- or up-projection matrices in later layers often achieves near-perfect editing success while preserving general capabilities"
  - [corpus] Work on basis-level representation fine-tuning (arXiv:2503.00306) also targets localized parameter regions for efficient editing
- Break condition: For architectures with different knowledge encoding patterns (e.g., non-Transformer or heavily modified architectures), optimal locations may differ significantly.

## Foundational Learning

- Concept: **Model editing formulation** — Given a pretrained LLM fθ and a desired knowledge update (s, r, o) → (s, r, o'), compute a localized parameter shift θ → θ* such that fθ* predicts o' while preserving unrelated knowledge.
  - Why needed here: LocFT-BF frames fine-tuning as a valid solution to this formulation when properly constrained.
  - Quick check question: Can you articulate why the editing objective differs from standard fine-tuning objectives?

- Concept: **Depth-first vs. breadth-first optimization** — Depth-first (sample-wise) optimizes each datum to convergence sequentially; breadth-first (epoch-based) iterates across the full dataset multiple times with mini-batch updates.
  - Why needed here: The paper's core thesis hinges on understanding why BF succeeds where DF fails in the editing context.
  - Quick check question: What specific failure mode does DF introduce when edit requests share parameter regions?

- Concept: **Catastrophic forgetting in sequential learning** — Neural networks trained sequentially on new tasks tend to forget previously learned information due to parameter overwriting.
  - Why needed here: The paper reframes "fine-tuning causes forgetting" as "depth-first implementation causes forgetting," requiring clarity on the underlying phenomenon.
  - Quick check question: How does gradient aggregation across samples differ from regularization techniques like elastic weight consolidation?

## Architecture Onboarding

- Component map: Edit requests -> tokenized prompts -> forward pass -> cross-entropy loss (masked on prompt tokens) -> backprop on target layer only -> parameter update

- Critical path:
  1. Select tuning location based on model architecture (Table 2 provides starting points)
  2. Configure optimizer with learning rate ~1e-5 to 5e-5 (paper uses conservative rates)
  3. Set batch size ≥8 (Figure 4 shows benefits plateau around 16-32)
  4. Run 3-5 epochs with early stopping on edit loss convergence
  5. Evaluate on held-out edits and capability benchmarks before deployment

- Design tradeoffs:
  - **Layer depth vs. generalization**: Earlier layers generalize better to paraphrases; later layers preserve capability better (Figure 5)
  - **Batch size vs. generalization**: Larger batches stabilize capability but may reduce generalization to rephrased prompts (Figure 4)
  - **Localization granularity**: Full MLP layer updates generalize better; MLPdown alone preserves capability better

- Failure signatures:
  - **Capability collapse (>10% drop)**: Batch size too small, or learning rate too high; check Figure 4 patterns
  - **Low reliability (<80%)**: Wrong layer selection; verify with per-layer sweep as in Figure 5
  - **Poor generalization despite high reliability**: Consider data augmentation (Appendix A.4 shows prefix augmentation closes the gap on COUNTERFACT)

- First 3 experiments:
  1. **Pipeline sanity check**: Replicate DF vs. BF comparison on 100 ZsRE samples with FT-M; expect ~20-30% reliability gap per Table 1
  2. **Layer sweep**: On your target model, test MLPdown at layers spanning early/middle/late; plot reliability vs. capability tradeoff
  3. **Batch size ablation**: Fix layer and epochs, vary batch size from 1 to 32; confirm capability improves while tracking generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a principled, automated method identify optimal tuning locations across diverse LLM architectures without exhaustive empirical search?
- Basis in paper: [inferred] The authors note that "an exhaustive search for the optimal tuning position in larger models, such as Qwen2.5-72B, is computationally prohibitive," leading them to use heuristic strategies (Default Position and Proportional Position) rather than optimal selection.
- Why unresolved: While the paper identifies MLP down-projection in later layers as a generally effective strategy, optimal locations vary across models (e.g., Qwen2.5-7B performs best at layer 6 vs. layer 22 for LLaMA3-8B), and no theoretical or automated method predicts these optima.
- What evidence would resolve it: A theoretical framework or learned predictor that reliably identifies near-optimal tuning locations across unseen model architectures without requiring full empirical sweeps.

### Open Question 2
- Question: How does LocFT-BF perform when edits must be applied strictly sequentially in real-time deployment without batching?
- Basis in paper: [explicit] Section 2.5 states that "in practical deployments, new edits can be buffered into small batches before triggering the next update," acknowledging the BF pipeline's assumption that edits need not arrive strictly one-by-one.
- Why unresolved: The paper evaluates sequential editing by batching accumulated edits across epochs, but real-world scenarios may require immediate application of individual edits with latency constraints, potentially limiting the BF pipeline's advantages.
- What evidence would resolve it: Experiments measuring reliability, generalization, and capability when each edit is applied immediately upon arrival (single-sample mini-batches per epoch) with bounded latency.

### Open Question 3
- Question: Can generalization performance be improved intrinsically without relying on external data augmentation strategies?
- Basis in paper: [explicit] The authors note that lower generalization on COUNTER FACT "reflects the unique evaluation design rather than the limitation of fine-tuning, and can be mitigated by incorporating prefix-based augmentation if necessary," and that "generalization, even if suboptimal, can be improved with data augmentation."
- Why unresolved: The current approach depends on augmenting edit prompts with paraphrased or noised variants to improve generalization, adding preprocessing overhead and requiring augmentation design choices.
- What evidence would resolve it: Modifications to the fine-tuning objective, regularization, or architecture that achieve comparable generalization gains without additional augmented training data.

### Open Question 4
- Question: What are the failure modes and performance characteristics of LocFT-BF at scales beyond 100K sequential edits?
- Basis in paper: [inferred] The paper demonstrates success at 100K edits as a "10× beyond prior practice" milestone but does not explore whether performance degrades, plateaus, or remains stable at orders of magnitude beyond this scale.
- Why unresolved: While 100K edits substantially exceeds prior benchmarks, real-world lifelong editing systems may require millions of accumulated edits over extended deployment periods.
- What evidence would resolve it: Systematic evaluation at 500K, 1M, and 5M sequential edits, tracking reliability, generalization, capability, and efficiency trends to identify scaling limits or failure thresholds.

## Limitations
- Claims are primarily validated on relation extraction and counterfact datasets, which may not generalize to other editing domains
- Optimal layer selection (MLP down-projection in later layers) was determined empirically on three specific model families and may not transfer to significantly different architectures
- Scalability claims to 72B parameters and 100K edits are reported rather than experimentally validated in the paper

## Confidence
- **High Confidence**: The breadth-first vs. depth-first pipeline comparison and its impact on edit reliability (Table 1, Figure 3)
- **Medium Confidence**: The mini-batch optimization benefits and layer-specific editing recommendations (Figure 4, Figure 5)
- **Medium Confidence**: The scalability claims to 72B parameters and 100K edits, though these are reported rather than experimentally validated in the paper

## Next Checks
1. Test LocFT-BF on diverse editing tasks beyond relation extraction (e.g., commonsense QA, procedural reasoning) to assess domain generalization
2. Conduct ablation studies on models with different architectural designs (e.g., Mamba, RWKV) to verify the layer selection recommendations
3. Perform long-term stability analysis tracking edit persistence and capability preservation over extended time periods with continuous fine-tuning