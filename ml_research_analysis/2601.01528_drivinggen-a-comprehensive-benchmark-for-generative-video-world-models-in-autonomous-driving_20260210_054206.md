---
ver: rpa2
title: 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in
  Autonomous Driving'
arxiv_id: '2601.01528'
source_url: https://arxiv.org/abs/2601.01528
tags:
- driving
- video
- arxiv
- world
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DrivingGen introduces a comprehensive benchmark for generative
  world models in autonomous driving, addressing the need for rigorous evaluation
  of visual realism, trajectory plausibility, temporal coherence, and controllability.
  The benchmark includes a diverse dataset spanning varied weather, time of day, geographic
  regions, and complex driving maneuvers, alongside novel metrics that jointly assess
  video and trajectory quality.
---

# DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving

## Quick Facts
- arXiv ID: 2601.01528
- Source URL: https://arxiv.org/abs/2601.01528
- Reference count: 40
- Primary result: DrivingGen establishes a unified framework to guide the development of reliable, controllable, and deployable driving world models, enabling scalable simulation and data-driven decision-making.

## Executive Summary
DrivingGen introduces a comprehensive benchmark for evaluating generative video world models in autonomous driving, addressing the critical need for rigorous assessment of visual realism, trajectory plausibility, temporal coherence, and controllability. The benchmark encompasses a diverse dataset spanning varied weather conditions, time of day, geographic regions, and complex driving maneuvers. Novel metrics jointly evaluate video and trajectory quality, providing a holistic assessment framework. Evaluation of 14 state-of-the-art models reveals important trade-offs between visual fidelity and physical accuracy, establishing DrivingGen as a crucial tool for advancing autonomous driving technology.

## Method Summary
DrivingGen presents a comprehensive evaluation framework for generative world models in autonomous driving, featuring a diverse dataset covering varied weather, time of day, geographic regions, and complex driving scenarios. The benchmark introduces novel metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Through systematic evaluation of 14 state-of-the-art models, DrivingGen reveals trade-offs between general models (which excel visually but break physics) and driving-specific models (which capture motion realistically but lag in image fidelity). The framework provides a unified approach to guide the development of reliable, controllable, and deployable driving world models.

## Key Results
- General models achieve superior visual quality but exhibit physical inconsistencies in generated trajectories
- Driving-specific models demonstrate more realistic motion patterns but sacrifice image fidelity
- The benchmark reveals critical trade-offs between visual realism and physical accuracy across different model architectures
- DrivingGen establishes standardized evaluation metrics for joint assessment of video and trajectory quality

## Why This Works (Mechanism)
The benchmark works by providing a unified framework that simultaneously evaluates both visual and trajectory generation quality, using metrics that capture the interplay between image realism and physical plausibility. By testing across diverse driving scenarios and including controllability assessment, the benchmark forces models to demonstrate practical utility beyond mere visual appeal. The inclusion of both general and driving-specific models in the evaluation reveals fundamental trade-offs in current approaches, guiding future model development toward more balanced solutions.

## Foundational Learning
- **Generative world models**: Learn to simulate realistic driving environments and agent behaviors, enabling scalable data generation for autonomous driving systems
  - Why needed: Traditional data collection is expensive and limited in coverage of rare scenarios
  - Quick check: Model can generate novel driving scenarios not seen in training data

- **Trajectory plausibility metrics**: Evaluate whether generated vehicle motions follow physical laws and driving conventions
  - Why needed: Visual realism alone doesn't ensure safe or realistic driving behavior
  - Quick check: Generated trajectories maintain reasonable acceleration/velocity profiles

- **Temporal coherence assessment**: Measure consistency across generated video frames to ensure smooth, realistic motion
  - Why needed: Discontinuities in generated videos break immersion and can mislead downstream systems
  - Quick check: Optical flow between consecutive frames follows expected patterns

- **Controllability evaluation**: Assess ability to generate scenes conditioned on specific driving commands or scenarios
  - Why needed: Practical applications require precise control over generated scenarios
  - Quick check: Model can reliably generate left/right turns when requested

## Architecture Onboarding

**Component Map:**
DrivingGen Dataset -> Evaluation Metrics Suite -> Model Comparison Framework -> Trade-off Analysis -> Development Guidance

**Critical Path:**
Dataset preparation (diverse driving scenarios) → Metric implementation (visual + trajectory quality) → Model evaluation (14 SOTA models) → Trade-off identification (visual vs physical accuracy) → Benchmark refinement

**Design Tradeoffs:**
- Comprehensive metrics vs. computational efficiency: The benchmark prioritizes thorough evaluation over speed
- General vs. driving-specific models: Trade-off between visual quality and physical realism
- Controlled scenarios vs. real-world complexity: Balance between reproducible testing and practical applicability

**Failure Signatures:**
- High visual scores but poor trajectory plausibility indicate models prioritizing appearance over physics
- Inconsistent temporal coherence suggests inadequate temporal modeling
- Poor controllability shows limited conditioning capabilities

**3 First Experiments:**
1. Evaluate a new generative model on the standard DrivingGen dataset to establish baseline performance
2. Test model robustness by generating edge-case scenarios not present in the original dataset
3. Assess computational requirements for real-time generation across different model architectures

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Benchmark focuses on generative model quality without evaluating downstream task performance improvements
- Emphasis on controlled scenarios may not fully capture real-world driving complexity and unpredictability
- Trade-offs observed may be influenced by specific model architectures rather than fundamental limitations

## Confidence

**High:** The benchmark's comprehensive evaluation framework and novel metrics are technically sound and well-documented

**Medium:** The comparative analysis of different model approaches and their trade-offs is supported by the evidence presented

**Medium:** The benchmark's potential to guide future development is reasonable but not yet proven through deployment

## Next Checks
1. Evaluate whether models performing well on this benchmark actually improve downstream autonomous driving tasks when deployed in simulation or real-world testing
2. Test model robustness on more challenging, edge-case scenarios beyond the controlled conditions in the benchmark
3. Assess the computational efficiency and scalability of the evaluated models for real-time autonomous driving applications