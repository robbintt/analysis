---
ver: rpa2
title: Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning
  for Tackling Complex Tasks
arxiv_id: '2502.13006'
source_url: https://arxiv.org/abs/2502.13006
tags:
- learning
- action
- planning
- numeric
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the integration of Reinforcement Learning, Action
  Model Learning, and Numeric Planning for tackling complex tasks. The paper compares
  model-based and model-free approaches for solving numeric planning problems in Minecraft.
---

# Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks

## Quick Facts
- arXiv ID: 2502.13006
- Source URL: https://arxiv.org/abs/2502.13006
- Reference count: 13
- Key outcome: RAMP (Reinforcement learning-Action Model Learning-Planning) significantly outperforms standard RL algorithms on Minecraft numeric planning tasks, solving more problems and finding plans that are often two orders of magnitude shorter.

## Executive Summary
This work introduces RAMP, a hybrid strategy that combines reinforcement learning (RL), action model learning (NSAM), and numeric planning to tackle complex tasks. The approach simultaneously trains an RL policy while learning a symbolic numeric planning domain model, creating a feedback loop where improved models enable better plans that further enhance RL training. Experiments on Minecraft crafting tasks show RAMP achieves higher success rates and shorter plans than standard RL, with particular advantages on harder problems where model-based approaches excel.

## Method Summary
RAMP integrates PPO reinforcement learning with NSAM (Numeric Action Model Learning) and a numeric planner (Metric-FF). The agent attempts planning first using the learned domain model; if planning fails, it falls back to the RL policy. After reaching goals, a shortcut search removes redundant action sequences and replaces them with learned action model predictions, returning shortened trajectories to RL training. NSAM learns safe numeric PDDL action models from trajectories, enabling zero-shot generalization to larger problem instances. The approach is evaluated on Minecraft crafting tasks of increasing complexity.

## Key Results
- RAMP significantly outperforms standard RL algorithms, solving more problems and finding plans that are often two orders of magnitude shorter
- The model-based approach (NSAM + planner) generalizes well to larger problems and solves harder tasks that model-free approaches struggle with
- Zero-shot transfer results show NSAM models trained on smaller maps perform nearly equivalently to models trained directly on larger maps
- Only removing loops and providing this back to PPO already yields a significant increase in success rate

## Why This Works (Mechanism)

### Mechanism 1: RL-Action Model Feedback Loop
Simultaneously training an RL policy and learning a symbolic action model creates a positive feedback loop that accelerates learning and improves plan quality. RL exploration collects observations that refine the action model via NSAM. The improved model enables the planner to find shorter plans, which are then fed back as high-quality training trajectories for RL, reducing the RL agent's need for random exploration.

### Mechanism 2: Trajectory Shortcut Search via Learned Action Model
A learned action model enables post-hoc trajectory optimization by identifying removable loops and replaceable action sequences. After a trajectory reaches a goal, the algorithm removes action sequences that start and end in the same state, then searches for action subsequences that can be replaced by a single action according to the learned model. The shortened trajectory is returned to RL as higher-quality training data.

### Mechanism 3: Zero-Shot Generalization via Symbolic Abstraction
Symbolic (lifted) action models learned on small instances generalize to larger problem instances without retraining. NSAM learns parameterized action schemas rather than grounded actions, capturing action semantics independent of specific object counts. This enables the same domain model to plan in environments with more objects.

## Foundational Learning

- **Concept: PDDL2.1 and Numeric Planning**
  - Why needed here: The entire framework outputs PDDL domain models. Understanding lifted vs. grounded representations, numeric fluents, and action schemas is essential to interpret what NSAM produces and why planners struggle with large action spaces.
  - Quick check question: Can you explain why a lifted action "TP TO ?from ?to" results in N⁴ grounded actions in an N×N grid?

- **Concept: Safe Action Model Learning**
  - Why needed here: NSAM's "safety" guarantee—that any plan consistent with the learned model is executable in the real environment—is what allows shortcut search to produce valid trajectories.
  - Quick check question: If a learned model is "safe" but incomplete, what types of planning failures would you expect?

- **Concept: PPO with Action Masking**
  - Why needed here: RAMP forces PPO to follow planner-generated plans. Standard PPO clipping can