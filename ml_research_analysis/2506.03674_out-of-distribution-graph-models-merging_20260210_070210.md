---
ver: rpa2
title: Out-of-Distribution Graph Models Merging
arxiv_id: '2506.03674'
source_url: https://arxiv.org/abs/2506.03674
tags:
- graph
- nodes
- label
- edges
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to Out-of-Distribution Graph
  Models Merging, which aims to merge multiple pre-trained graph neural networks into
  a generalized model without retraining from scratch. The method leverages a graph
  generation strategy to instantiate a mixture distribution from multiple source domains
  and uses a Mixture-of-Experts (MoE) module combined with a masking mechanism for
  fine-tuning.
---

# Out-of-Distribution Graph Models Merging

## Quick Facts
- arXiv ID: 2506.03674
- Source URL: https://arxiv.org/abs/2506.03674
- Reference count: 40
- Primary result: Novel source-free graph model merging approach using MoE and masking outperforms traditional methods on multiple datasets

## Executive Summary
This paper addresses the challenge of merging multiple pre-trained graph neural networks (GNNs) into a generalized model without access to source data or retraining from scratch. The authors propose a framework that combines label-conditional graph generation to instantiate a mixture distribution from multiple source domains, with a Mixture-of-Experts (MoE) module and masking mechanism for fine-tuning. The approach is architecture-agnostic and demonstrates state-of-the-art performance in source-free graph domain generalization across multiple real-world graph datasets.

## Method Summary
The framework operates in two main stages: first, it generates synthetic graphs that approximate the source domain distributions by optimizing random noise into graph structures using pre-trained GNN parameters as statistical anchors. Second, it merges and fine-tunes the pre-trained models via a MoE module combined with a masking mechanism that selectively adjusts classifier heads rather than encoders. This allows the merged model to dynamically route inputs to different expert GNNs while preserving domain-invariant knowledge encoded in the feature extraction layers.

## Key Results
- OGMM (MoE + Masks) significantly outperforms both individual pre-trained models and traditional parameter averaging methods like Uni-Soup and Greedy-Soup
- Classifier masking (MaskCL) yields better generalization than encoder masking (MaskNN), as verified in ablation studies
- The framework achieves state-of-the-art performance in source-free graph domain generalization across multiple real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
Label-conditional graph generation effectively recovers source domain distributions from frozen model parameters by optimizing random noise into graph structures using a discrete edge encoder with Gumbel-Softmax and prior matching regularizer. This works because pre-trained model parameters (specifically BN statistics and classifier weights) retain sufficient information to approximate marginal data distribution of source domains.

### Mechanism 2
Fine-tuning classifier heads via masking rather than encoders optimizes the trade-off between domain adaptation and knowledge retention by isolating domain-specific adjustments while preserving feature extraction capabilities. This works because domain-invariant knowledge is primarily encoded in GNN encoders while domain-specific biases are localized in classifier heads.

### Mechanism 3
Mixture-of-Experts gating layer provides more robust fusion strategy for heterogeneous GNNs than parameter averaging by dynamically assigning weights to different pre-trained experts for each synthetic sample. This works because target distribution is assumed to be a linear combination of source distributions, requiring dynamic routing rather than static fused weights.

## Foundational Learning

- **Data-Free Knowledge Distillation (DFKD) / Model Inversion**: Needed because the method relies on generating synthetic graphs to serve as training data for the merging module when source data is unavailable. Quick check: How do you optimize a discrete adjacency matrix via gradient descent? (Answer: Usually Gumbel-Softmax or gradient approximation).

- **Mixture-of-Experts (MoE)**: Needed as the architectural backbone that allows the system to route inputs to different pre-trained models dynamically, handling architecture heterogeneity. Quick check: What is the purpose of the "TopK" selector in the gating network? (Answer: To enforce sparsity and computational efficiency by activating only a subset of experts).

- **Domain Generalization (DG) vs. Domain Adaptation (DA)**: Needed to understand that the paper targets "Source-Free Graph Domain Generalization" where no target data is available during training. Quick check: In a standard DA setting, you fine-tune on target data; in this DG setting, what substitutes for the target data during optimization of merging weights? (Answer: The generated mixture distribution G*).

## Architecture Onboarding

- **Component map**: Random Noise / Label Condition -> MLP Edge Encoder -> Synthetic Graphs (G*) -> Sparse Gate -> Masked Experts (GCN, GAT, GIN, etc.) -> Weighted Sum -> Loss

- **Critical path**: 
  1. Generator Training: Pre-trained GNN parameters frozen, generators trained to minimize classification loss on synthetic graphs
  2. MoE Training: Generators frozen, Sparse Gate and Mask parameters (ω) trained on generated graphs to minimize mixture error

- **Design tradeoffs**:
  - Mask Placement: Masking Classifier (θ_Φ) is more effective than Encoder (θ_Ψ), trading fine-grained feature adaptation for stability and source knowledge retention
  - Real vs. Synthetic: Method works without source data but performance improves if real data is available; synthetic data is necessary approximation

- **Failure signatures**:
  - Low Diversity Generation: Generated graphs too similar (mode collapse), preventing MoE gate from learning distinct routing strategies
  - Selection Collapse: Gate outputs uniform weights or selects only one expert regardless of input
  - Performance Degradation: Applying masking to encoder instead of classifier reduces performance

- **First 3 experiments**:
  1. Ablation on Mask Location: Implement MaskCL vs MaskNN on PTC dataset to verify classifier masking yields better generalization
  2. Baseline Comparison: Compare MoE merging approach against parameter averaging (Uni-Soup) to quantify benefit of architectural routing mechanism
  3. Synthetic Ratio Sensitivity: Train merger using varying ratios of synthetic data (e.g., 20% vs 100%) to observe saturation behavior

## Open Questions the Paper Calls Out

### Open Question 1
How can predictive scaling laws be established for graph model merging to determine optimal number and diversity of experts required for a target domain? The current method shows performance improves with domain diversity but finding optimal fitting function for arbitrary number of heterogeneous experts remains open.

### Open Question 2
Can the Mixture-of-Experts merging framework be effectively adapted for cross-task transfer learning where source models were trained on different downstream objectives? Current framework assumes models pre-trained on similar tasks with consistent label spaces; merging experts with divergent loss functions is not addressed.

### Open Question 3
How does the framework perform on open-world graph data where target distribution includes classes or structural properties absent from all source domains? Current formulation guarantees generalization only if target is mixture of known sources; it does not account for out-of-support data points or classes.

## Limitations

- Primary assumption that BN statistics and classifier weights contain sufficient information to approximate source distributions lacks extensive empirical validation
- Method's reliance on synthetic data as proxy for source data may introduce approximation errors if generated graphs don't adequately cover target distribution's support
- Masking mechanism's effectiveness depends on assumption that domain-specific biases are localized in classifier head, which may not hold for all GNN architectures

## Confidence

- **High**: MoE architecture's ability to dynamically route inputs to different pre-trained models is well-established in literature and demonstrated effectively in experiments
- **Medium**: Label-conditional graph generation strategy is novel and shows promise but needs further validation for robustness to varying levels of model regularization
- **Medium**: Masking mechanism's impact on classifier vs encoder performance is well-supported by ablation study but generalizability to more complex architectures remains uncertain

## Next Checks

1. Evaluate diversity of generated graphs using metrics like coverage of graph spectrum or feature space to ensure synthetic data adequately represents source domains
2. Test MoE gate's performance under varying levels of synthetic data noise or mode collapse to assess resilience to suboptimal generation
3. Apply method to merge GNNs with significantly different architectures (e.g., GCN, GAT, GIN) to validate effectiveness in handling heterogeneous models