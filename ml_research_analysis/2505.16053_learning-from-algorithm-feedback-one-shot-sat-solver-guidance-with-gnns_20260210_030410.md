---
ver: rpa2
title: 'Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs'
arxiv_id: '2505.16053'
source_url: https://arxiv.org/abs/2505.16053
tags:
- solver
- variable
- instances
- training
- solvers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel reinforcement learning from algorithm
  feedback (RLAF) paradigm to guide SAT solver branching heuristics using Graph Neural
  Networks (GNNs). The key idea is to inject variable-wise weights and polarities
  into existing SAT solvers in a single forward pass, enabling the GNN to influence
  every branching decision.
---

# Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs

## Quick Facts
- **arXiv ID:** 2505.16053
- **Source URL:** https://arxiv.org/abs/2505.16053
- **Reference count:** 40
- **One-line primary result:** RL-trained GNN policies guide SAT solvers to >2x speedup across diverse distributions by injecting variable weights in a single forward pass.

## Executive Summary
This paper introduces reinforcement learning from algorithm feedback (RLAF) to guide SAT solver branching heuristics using Graph Neural Networks. The key innovation is injecting variable-wise weights and polarities into existing solvers via a single GNN forward pass, influencing every branching decision without replacing the solver's dynamic state tracking. By framing this as an RL problem with the solver's decision count as reward, the authors demonstrate significant mean solve time reductions across random 3-SAT, graph coloring, and cryptographic instances. The approach achieves more than 2x speedup in some cases and generalizes effectively to larger problems.

## Method Summary
The method trains GNNs to output variable weights and polarities that modulate existing SAT solver heuristics in a single forward pass. Given a CNF formula, a Literal-Clause bipartite graph is constructed where literals and clauses are nodes connected by membership edges. A 10-layer MPNN with 256-dimensional embeddings processes this graph, outputting mean parameters μ(x) for weights and logits ρ(x) for polarities. Weights are sampled from Log-Normal(μ, 0.1) distributions and polarities from Bernoulli distributions. These parameters multiply the solver's base variable scores in the argmax branching decision. The policy is trained using Group Relative Policy Optimization (GRPO) where multiple weight configurations are sampled and evaluated by the solver, with the number of decisions serving as the negative reward signal. Training uses 2000 GRPO iterations with 100 formulas per iteration and 40 samples per formula.

## Key Results
- RLAF-trained policies reduce mean solve times by >2x on random 3-SAT, graph coloring, and cryptographic instances compared to unguided solvers.
- The approach consistently outperforms expert-supervised methods based on learning handcrafted weighting heuristics.
- Policies generalize effectively to larger problems (e.g., training on 200 variables, testing on 300-600 variables) without additional training.
- GNN guidance provides more consistent performance improvements than traditional heuristic learning approaches, especially on structured problems like graph coloring.

## Why This Works (Mechanism)

### Mechanism 1
A single forward pass to generate static variable weights can effectively guide an entire search process if the base heuristic preserves relative variable importance. The GNN outputs weights W=(w, p) which modulate the solver's existing scoring function via multiplication: argmax_x w(x) · Score(x). This injects prior knowledge without replacing the solver's dynamic state tracking. The relative importance of variables must remain sufficiently stable throughout the search, or the base heuristic's dynamic updates can correct minor initial misrankings. Evidence shows GNN weights significantly reduce solve times, though static weights may become stale if search state changes radically.

### Mechanism 2
Framing heuristic guidance as a reinforcement learning problem allows discovery of non-intuitive variable ordering strategies that strictly minimize computational cost. GRPO samples multiple parameter sets W for a single formula, runs the solver, and calculates group-relative advantage based on decision count. It then shifts policy probability density toward low-cost parameterizations. The gradient derived from solver runtime must be smooth enough for neural network convergence. The advantage signal relative to group mean rather than absolute rewards provides variance reduction. However, if solver behavior is highly stochastic or variance in solve time is extreme, the advantage signal may be too noisy for stable convergence.

### Mechanism 3
Standard MPNNs on Literal-Clause graphs can capture structural properties that correlate with solver difficulty. The GNN propagates information between literals and clauses, learning to up-weight variables central to problem structure like UNSAT cores. Color refinement expressiveness of standard GNNs is sufficient to distinguish helpful variables in industrial-like SAT instances despite theoretical limitations on graph isomorphism. Evidence shows variables with high weights are predominantly members of the UNSAT core. However, if instances contain complex symmetries indistinguishable by the GNN, the model may assign uniform weights, failing to provide guidance.

## Foundational Learning

- **CDCL Solving & Branching Heuristics (VSIDS)**: The proposed method modulates rather than replaces the solver. Understanding how weights w(x) influence the search requires knowing the base activity scores Score(x) they scale. Quick check: If a solver uses VSIDS, does multiplying the activity score by 2.0 permanently change the variable's priority, or does the decay factor eventually neutralize this injection?

- **Policy Gradient (GRPO/PPO)**: This is the engine of the paper. Unlike supervised learning (predicting "correct" branches), RL optimizes for the result (speed). Understanding the "advantage" calculation is key to debugging why the model might fail to learn. Quick check: Why does the paper calculate advantage relative to a group mean rather than using absolute rewards?

- **Literal-Clause Graphs**: This is the input modality. The GNN's ability to solve the problem depends entirely on the inductive bias provided by this graph structure (linking variables to their clauses and negations). Quick check: How does the graph structure handle the relationship between a literal x and its negation ¬x?

## Architecture Onboarding

- **Component map:** CNF Formula → Literal-Clause Bipartite Graph → 10-Layer MPNN (256-dim embeddings) → MLP Decoders (2-layer, SiLU, hidden dim 512) → μ(x) for weights, ρ(x) for polarities → LogNormal/Bernoulli sampling → Weight parameters W → Modified SAT Solver (Glucose/March) → Decision count reward → GRPO trainer.

- **Critical path:** The Solver Execution step. While the GNN forward pass is fast (milliseconds), the RL loop requires running the solver M times (e.g., 40 times) per training instance. Training throughput is bound by solver speed, not GPU compute.

- **Design tradeoffs:** One-Shot vs. Per-Step guidance (fast, scalable vs. expressive but slow/infeasible overhead). GRPO vs. PPO (avoids training separate value network vs. relying on group sampling for variance reduction). Decisions vs. Time reward (deterministic vs. noisy but more realistic).

- **Failure signatures:** Uniform Weights (model converges to constant μ for all variables → no speedup). Overfitting (policy learns weights specific to training random seed distribution but fails on larger n or different graph classes). Instability (KL divergence penalty too low, causing policy to change too rapidly, spiking solve times).

- **First 3 experiments:** 1) Implement argmax w(x) · Score(x) logic in simple Python solver wrapper. Manually set w(x)=10 for known "important" variables and verify solve time drops. 2) Train on Random 3-SAT(40) variables. Verify GRPO loss decreases and sampled weights visibly separate into distinct distributions. 3) Train on 3-SAT(200) and immediately test on 3-SAT(300) without further training to verify "one-shot" structural learning generalizes to larger graphs.

## Open Questions the Paper Calls Out

### Open Question 1
Can the RLAF paradigm be successfully translated to guide branching heuristics in other combinatorial optimization domains, such as Mixed-Integer Programming (MIP) or Constraint Satisfaction Problems (CSP)? The Discussion states that translating it to other domains remains as future work, noting that branching heuristics are critical in MIP and CSPs. This is unresolved because while the authors demonstrate the method works for SAT, they have not validated the "generic pattern" of injecting multiplicative weights into the argmax selection of other solver types. Empirical results showing RLAF-trained policies reducing computational costs for standard MIP or CSP solvers on benchmark distributions would resolve this.

### Open Question 2
Can more expressive GNN architectures overcome the limitations of standard message-passing networks in distinguishing highly symmetric patterns within SAT instances? The Discussion notes that the expressive power of the current network is "bounded by color refinement... leaving highly symmetric patterns indistinguishable." This is unresolved because standard GNNs struggle with symmetry, yet the paper relies on them. It is unclear if higher-order GNNs would offer better performance or if the training cost would outweigh the benefits. A comparison of standard GNNs against higher-order GNNs on symmetric benchmark distributions, analyzing the trade-off between training overhead and solver speedup, would resolve this.

### Open Question 3
Does scaling the training distribution to industrial-sized or harder instances using distributed computing yield policies that outperform those trained on "easy" synthetic data? Section 2.5 and the Discussion highlight that the current prototype relies on "comparatively easy" training instances due to hardware constraints, suggesting future work should leverage distributed compute. This is unresolved because it is currently unproven whether the feedback signal from harder instances provides a superior training gradient or if the variance in solve times makes policy optimization unstable. Training curves and performance metrics from a distributed RLAF setup trained on large industrial instances compared to the current small-scale models would resolve this.

## Limitations
- The RL training process requires running the SAT solver multiple times per training instance, creating a significant computational barrier that scales poorly with problem size.
- The static one-shot injection of weights assumes relative variable importance remains stable throughout the search, which may not hold for problems with complex dynamic structures or frequent restarts.
- The method's effectiveness depends on the solver's base heuristic preserving the relative importance of variables; if the base heuristic's dynamic updates frequently override the static weights, the GNN's influence may be diminished.

## Confidence
**High Confidence:** The claim that RLAF-trained policies reduce mean solve times compared to baselines is well-supported by experimental results across all three distributions. The methodology for measuring solver cost via decision count is sound and reproducible.

**Medium Confidence:** The claim that the approach "consistently outperforms expert-supervised approaches based on learning handcrafted weighting heuristics" is supported but requires careful interpretation. The comparison baselines are reasonable, but the paper does not explore whether alternative supervised learning approaches might perform better than the presented supervised baselines.

**Medium Confidence:** The claim about generalization to larger and harder problems is demonstrated but only within the same problem class. The paper shows generalization from 200→300 variables for 3SAT and 300→600 for 3COL, but does not test cross-distribution generalization.

## Next Checks
1. **Sample Efficiency Study:** Systematically vary M (number of weight samples per formula) during training to determine the minimum number of samples needed for effective learning, and quantify the relationship between sample count and final policy performance.

2. **Dynamic vs Static Guidance Comparison:** Implement a hybrid approach where the GNN provides guidance every K decisions (rather than just once) to test whether the one-shot assumption holds, and measure the trade-off between improved guidance quality and increased computational overhead.

3. **Cross-Distribution Transfer:** Train a single RLAF policy on a mixture of all three distributions and test its performance on each individual distribution to evaluate whether the method can learn distribution-agnostic structural features of SAT problems.