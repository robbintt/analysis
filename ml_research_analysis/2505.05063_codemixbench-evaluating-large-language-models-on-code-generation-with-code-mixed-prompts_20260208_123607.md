---
ver: rpa2
title: 'CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed
  Prompts'
arxiv_id: '2505.05063'
source_url: https://arxiv.org/abs/2505.05063
tags:
- code
- code-mixed
- language
- prompt
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeMixBench is a new benchmark for evaluating large language models
  on code generation from code-mixed prompts, addressing the gap in existing benchmarks
  that assume English-only interactions. Built on BigCodeBench, it introduces controlled
  code-mixing into prompts across Hinglish, Spanish-English, and Chinese Pinyin-English
  using an LLM-driven augmentation pipeline with Gemini-2.0-Flash-Lite for translation
  and code-mixing injection.
---

# CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts

## Quick Facts
- **arXiv ID**: 2505.05063
- **Source URL**: https://arxiv.org/abs/2505.05063
- **Reference count**: 33
- **Primary result**: CodeMixBench introduces a new benchmark evaluating LLMs on code generation from code-mixed prompts, revealing consistent performance degradation compared to English-only prompts.

## Executive Summary
CodeMixBench addresses a critical gap in LLM evaluation by introducing a benchmark for code generation from code-mixed prompts across Hinglish, Spanish-English, and Chinese Pinyin-English. The benchmark employs a systematic LLM-driven augmentation pipeline that translates and injects controlled code-mixing into the BigCodeBench dataset, enabling evaluation across varying mixing degrees (CMD 0.6 and 0.9). Evaluation of 17 models ranging from 1.5B to 15B parameters demonstrates consistent Pass@1 performance degradation under code-mixed conditions, with smaller models affected more severely than larger, instruction-tuned models with diverse multilingual training data. The benchmark provides a realistic framework for studying multilingual generalization in code generation models.

## Method Summary
The benchmark uses BigCodeBench (1,140 English prompts) augmented through a two-stage pipeline: first translating prompts to matrix language via Gemini-2.0-Flash-Lite while preserving code tokens, then applying CMD-controlled code-mixing using frequency scores from code-mixed Twitter corpora. Romanization is performed via follow-up LLM prompts. Semantic fidelity is validated using GAME scores (mean ~90% cosine similarity via all-MiniLM-L6-v2 embeddings). Evaluation uses modified BigCodeBench harness with zero-shot greedy decoding in E2B sandbox (Python 3.10), measuring Pass@1 functional correctness across 6,840 total prompts (3 languages × 2 CMDs × 1,140 tasks).

## Key Results
- Code-mixed prompts consistently degrade Pass@1 performance compared to English-only prompts across all evaluated models
- Larger models and instruction-tuned models with diverse multilingual training data show greater robustness to code-mixing
- Performance degradation increases systematically with CMD level, particularly affecting smaller models
- OpenCoder-8B-Instruct and DeepSeek-R1-Distill-Llama-8B maintain strong performance despite code-mixing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code-mixed prompts degrade LLM code generation performance compared to English-only prompts.
- Mechanism: Non-English tokens and transliterated words disrupt monolingual priors established during English-dominant pretraining, causing tokenization fragmentation, increased semantic ambiguity in instruction grounding, and ineffective mapping of code-mixed intent to programming constructs.
- Core assumption: Performance degradation is primarily driven by the mismatch between training data distributions and code-mixed input distributions.
- Evidence anchors:
  - [abstract] "Our results show that code-mixed prompts consistently degrade Pass@1 performance compared to their English-only counterparts..."
  - [section] (Page 7) "A primary reason is training data bias: most LLMs are trained on English-dominant code datasets, with minimal exposure to multilingual or code-mixed content. Tokenizer limitations also play a role—non-English tokens, especially Romanized text, inflate sequence lengths and lead to fragmented tokenization."
  - [corpus] Related NLP work on code-mixed tasks (e.g., NER, abusive language detection) consistently identifies challenges with informal structure and transliteration, supporting the general difficulty claim.
- Break condition: Models trained on diverse, multilingual, or explicitly code-mixed data may show significantly reduced degradation.

### Mechanism 2
- Claim: Model robustness to code-mixing correlates with training data diversity (multilinguality, noise) and instruction tuning, not solely parameter count.
- Mechanism: Exposure to diverse, noisy, and multilingual natural language during pretraining builds more robust internal representations of linguistic intent, enabling better disentanglement of semantic meaning from surface-level code-mixing artifacts. Instruction tuning refines this by teaching models to follow intent under varied phrasings.
- Core assumption: Data quality and diversity, not just scale, determine generalization to out-of-distribution inputs.
- Evidence anchors:
  - [section] (Page 6) "OpenCoder-8B... resilience can be attributed to its training on a massive and diverse dataset (2.5 trillion tokens, 90% code and 10% code-related web text) across languages, combined with strong instruction-tuning."
  - [section] (Page 6-7) "Qwen2.5-Coder-1.5B-Instruct is a particularly noteworthy case of a smaller model remaining robust to code-mixing... continued-pretrained on an enormous 5.5 trillion token code corpus."
  - [corpus] (Paper ID: 77898) Adapting multilingual models via continued pre-training and model merging shows promising results, providing indirect support for data diversity as a key factor.
- Break condition: Models of similar size with less diverse or strictly English-focused training will show significantly higher degradation.

### Mechanism 3
- Claim: The Controlled Code-Mixing Degree (CMD) parameter provides a systematic lever for evaluating model sensitivity to linguistic perturbations.
- Mechanism: Corpus-derived frequency scores selectively replace English words with matrix-language equivalents based on a CMD ratio, creating a spectrum of code-mixed prompts. As CMD increases, prompts deviate further from training distributions, causing performance drops proportional to model robustness.
- Core assumption: Frequency-based replacement produces semantically equivalent prompts where difficulty stems from linguistic mixing, not task logic changes.
- Evidence anchors:
  - [section] (Page 3-4) Describes two-stage pipeline: translation to matrix language followed by CMD-controlled code-mixing based on frequency scores.
  - [section] (Page 5) "Across CMD levels 0.6 and 0.9, we observe a mean GAME score of 90%, confirming high semantic fidelity in the generated prompts."
  - [corpus] (Paper ID: 77898) Related work on model merging for code-mixing also uses CMD parameters, suggesting this is a recognized evaluation technique.
- Break condition: If translation or romanization introduces systematic semantic errors, CMD may measure augmentation quality rather than mixing robustness.

## Foundational Learning

- **Concept: Code-Mixing / Code-Switching**
  - Why needed here: This is the core phenomenon under study. It is a structured linguistic practice common in multilingual communities, not merely "broken English."
  - Quick check question: How does code-mixing differ from simple translation between two languages?

- **Concept: Pass@k Metric**
  - Why needed here: Primary evaluation metric measuring functional correctness (code passes all test cases) rather than syntactic similarity (e.g., BLEU).
  - Quick check question: If generated code compiles and resembles the reference solution but fails a hidden test case, what would its Pass@1 score be?

- **Concept: Tokenization and Subword Units**
  - Why needed here: Key hypothesized degradation mechanism is tokenizer fragmentation—non-English and romanized words break into more, less meaningful tokens.
  - Quick check question: Why might a standard English-centric tokenizer perform poorly on Hinglish or Romanized Chinese text?

## Architecture Onboarding

- **Component map:** BigCodeBench -> Translator (Gemini → matrix language) -> Word Aligner (bilingual dictionary) -> Scorer (corpus frequency scores) -> Mixer (Algorithm 1, CMD-controlled) -> Romanizer (script conversion) -> Verification (GAME score) -> Evaluation Harness (modified BigCodeBench)

- **Critical path:** The Augmentation Pipeline's reliability determines benchmark validity. Any semantic drift or artifact introduced during translation, mixing, or romanization confounds results. GAME score validation is the critical checkpoint.

- **Design tradeoffs:**
  - Synthetic vs. Human Data: Lower cost and higher CMD control versus potential artifacts and lack of natural spontaneity (acknowledged limitation)
  - Generalizability vs. Depth: Three language pairs provide breadth but limit depth for any single linguistic community
  - Semantic Fidelity Metric: GAME score (embedding cosine similarity) is a proxy that may miss subtle pragmatic intent shifts

- **Failure signatures:**
  - Semantic Drift: Low GAME scores indicate prompt meaning has changed
  - Tokenization Collapse: Performance drops suddenly rather than degrading gracefully with CMD
  - Corpus Bias: Twitter corpus used for frequency scoring may not represent developer code-mixing patterns

- **First 3 experiments:**
  1. Validate the pipeline on a small sample—manually inspect generated prompts at CMD 0.6 and 0.9 for fluency, naturalness, and semantic preservation; calculate GAME scores
  2. Establish baseline: evaluate your model on original English-only BigCodeBench (complete split) for Pass@1 baseline
  3. Run controlled degradation test: evaluate same model on CodeMixBench (e.g., Hinglish) at CMD 0.0, 0.3, 0.6, 0.9; plot Pass@1 vs. CMD to visualize sensitivity curve and identify collapse point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed performance degradation stem primarily from training data bias, tokenizer limitations, or semantic ambiguity in code-mixed prompts?
- Basis in paper: [explicit] Section 6.2 states: "We hypothesize that several factors contribute to performance degradation... A primary reason is training data bias... Tokenizer limitations also play a role... Additionally, semantic ambiguity... can make it harder for models to ground instructions."
- Why unresolved: The paper identifies multiple plausible causes but does not include ablation studies or controlled experiments to isolate their relative contributions.
- What evidence would resolve it: Targeted experiments controlling for each factor—e.g., evaluating models with multilingual-aware tokenizers, or fine-tuning on code-mixed data while keeping tokenization constant.

### Open Question 2
- Question: Would human-authored code-mixed prompts yield different performance patterns than the LLM-generated (Gemini-augmented) code-mixed prompts used in CodeMixBench?
- Basis in paper: [explicit] Section 6.5 states: "Constructing human-authored code-mixed datasets would help eliminate translation artifacts and improve benchmark authenticity."
- Why unresolved: The benchmark relies entirely on synthetic code-mixing via Gemini-2.0-Flash-Lite, which may introduce artifacts not present in natural developer-written code-mixed prompts.
- What evidence would resolve it: Collecting and evaluating on a human-authored code-mixed test set, then comparing degradation patterns against synthetic counterparts.

### Open Question 3
- Question: How does code-mixing in comments, variable names, and inline annotations affect model performance compared to mixing only in prompt instructions and docstrings?
- Basis in paper: [explicit] Section 6.4 notes the limitation: "Our code-mixing is applied only to natural language instructions and docstrings, whereas real-world mixing often occurs in comments, variable names, and inline annotations."
- Why unresolved: The current evaluation isolates prompt-level mixing, leaving the impact of mixing within code artifacts untested.
- What evidence would resolve it: Extending CodeMixBench with code-mixed comments and identifiers, then re-evaluating the same model suite to measure additional degradation.

## Limitations

- Benchmark relies on synthetic code-mixed prompts generated via LLM pipeline rather than naturally occurring developer code-mixing patterns
- Limited coverage to three language pairs (Hinglish, Spanish-English, Chinese Pinyin-English) restricts generalizability
- Twitter corpus frequency scoring may not represent actual developer code-mixing patterns in software development contexts

## Confidence

**High Confidence Claims:**
- Code-mixed prompts consistently degrade Pass@1 performance compared to English-only prompts
- Larger models and those with diverse multilingual training data show greater robustness to code-mixing

**Medium Confidence Claims:**
- Performance degradation is primarily driven by training data bias and tokenizer fragmentation
- CMD parameter provides a valid systematic lever for evaluating mixing sensitivity

**Low Confidence Claims:**
- Synthetic augmentation pipeline perfectly preserves semantic equivalence
- Results generalize to all code-mixing scenarios beyond the three tested language pairs

## Next Checks

1. **Semantic Preservation Validation**: Manually inspect 50 randomly selected prompts at CMD 0.6 and 0.9 for each language pair to verify that semantic meaning is preserved and that the prompts remain natural and fluent. Compare human assessment of semantic equivalence against the GAME score metric to identify any systematic gaps.

2. **Cross-Domain Generalization**: Evaluate the same set of models on naturally occurring code-mixed developer data (if available) or on code-mixed prompts from different domains (e.g., technical documentation, commit messages) to test whether the benchmark results hold beyond the synthetic corpus used in CodeMixBench.

3. **Fine-tuning Impact Assessment**: Fine-tune a subset of models (e.g., OpenCoder-8B and DeepSeek-R1-Distill-Llama-8B) on a small corpus of code-mixed programming examples and re-evaluate on CodeMixBench to determine whether targeted adaptation can mitigate the observed performance degradation and to what extent.