---
ver: rpa2
title: Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large
  Language Models
arxiv_id: '2511.05184'
source_url: https://arxiv.org/abs/2511.05184
tags:
- reasoning
- language
- tasks
- llms
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the role of Chain-of-Thought (CoT) in distilling
  reasoning capability from larger Large Language Models (LLMs) to smaller ones using
  white-box Knowledge Distillation (KD). The study employs CoT data from the CoT-Collection
  dataset and evaluates distilled models on the BIG-Bench-Hard (BBH) benchmark.
---

# Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models

## Quick Facts
- arXiv ID: 2511.05184
- Source URL: https://arxiv.org/abs/2511.05184
- Reference count: 16
- Incorporating CoT into white-box KD improves distilled model reasoning performance on BIG-Bench-Hard benchmark

## Executive Summary
This paper investigates how Chain-of-Thought (CoT) reasoning can be distilled from larger Large Language Models (LLMs) to smaller ones using white-box Knowledge Distillation (KD). The study employs CoT data from the CoT-Collection dataset and evaluates distilled models on the BIG-Bench-Hard (BBH) benchmark. Experiments using Qwen and Llama2 families of LLMs show that incorporating CoT into white-box KD improves the average performance of distilled models across natural language reasoning and understanding tasks.

## Method Summary
The study employs white-box Knowledge Distillation where the teacher LLM's internal activations (hidden states) are distilled into the student model during training. The distillation loss combines standard next-token prediction with hidden-state matching between teacher and student. CoT is integrated by using the CoT-Collection dataset, which provides intermediate reasoning steps alongside final answers. The student model is trained to predict both the final answer and the intermediate reasoning steps, effectively learning to mimic the teacher's reasoning process.

## Key Results
- Qwen-1.8B+KD+CoT improved performance by 7.54% relative to Qwen-1.8B+KD
- Llama2-7B+KD+CoT improved by 5.22% relative to Llama2-7B+KD
- CoT-enhanced distillation particularly helps when vanilla KD underperforms the baseline
- Improvements are consistent across both Qwen and Llama2 model families

## Why This Works (Mechanism)
CoT enhances knowledge distillation by providing intermediate reasoning steps that capture the teacher model's problem-solving process. This allows the student to learn not just the final answer but the underlying reasoning methodology. The intermediate steps act as additional supervision signals that guide the student through the logical progression of solving complex problems, enabling better transfer of reasoning capabilities beyond simple answer prediction.

## Foundational Learning
- **Knowledge Distillation fundamentals**: Understanding how teacher model knowledge transfers to student models through loss functions and probability matching
- **Chain-of-Thought reasoning**: The intermediate step-by-step reasoning process that LLMs use to solve complex problems
- **White-box vs black-box distillation**: White-box uses teacher's internal activations, black-box uses only outputs
- **Tokenization consistency**: Assumes teacher and student share vocabulary and tokenizer architecture for simplified loss computation
- **Hidden state matching**: Technique for aligning intermediate representations between teacher and student models
- **Distillation hyperparameters**: Temperature scaling, loss weighting, and other settings that affect knowledge transfer quality

## Architecture Onboarding

**Component Map**: Input -> Tokenizer -> Student Model -> Loss Function (NLL + Hidden State Loss + CoT Loss) -> Output

**Critical Path**: Tokenization → Forward pass through student → Compute losses (next-token, hidden state matching, CoT alignment) → Backpropagation → Parameter updates

**Design Tradeoffs**: 
- White-box provides richer supervision but requires model access and computational overhead
- CoT integration increases training complexity but enables reasoning transfer
- Assumes shared tokenizer vocabulary, limiting cross-architecture distillation

**Failure Signatures**: 
- Negative transfer in specific logic tasks when rationales introduce incorrect information
- Performance degradation when CoT quality is low or irrelevant to task
- Potential overfitting to teacher's reasoning style rather than general reasoning capability

**Three First Experiments**:
1. Validate CoT-enhanced KD on a single task from BBH to confirm basic functionality
2. Compare CoT-enhanced KD with standard KD on a subset of models to establish baseline improvements
3. Test distillation with varying CoT quality (e.g., removing rationales vs. adding noisy rationales) to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of KD+CoT vary when distilling knowledge between models with different vocabularies or tokenizer architectures?
- Basis in paper: Section 3 states the assumption that teacher and student LLMs have "tokenizers of the same size N and the same vocabulary" to simplify the loss computation.
- Why unresolved: The study limits experiments to model families (Qwen, Llama) with identical tokenizers, leaving the impact of vocabulary mismatch on probability-based distillation unexplored.
- What evidence would resolve it: A comparative study of KD+CoT performance when distilling from a Llama-2 teacher to a Qwen student (or vice versa) versus same-family distillation.

### Open Question 2
- Question: Why does the integration of rationales in KD+CoT lead to performance degradation in specific tasks like "Boolean Expressions" or "Sport Understanding"?
- Basis in paper: Section 4.2.2 (Example iii) highlights a case where the student model correctly answers a sport question, but the KD+CoT model fails by hallucinating incorrect facts (claiming baseball has touchdowns).
- Why unresolved: The paper demonstrates that CoT improves average performance but acknowledges improvements are "not universal," without explaining the mechanism behind the negative transfer in specific logic or knowledge-retrieval tasks.
- What evidence would resolve it: An ablation study analyzing the attention weights or intermediate states during failures to determine if the rationale acts as a distractor for specific logical operations.

### Open Question 3
- Question: Is the efficacy of the distilled reasoning capability dependent on the specific source of the rationales (OpenAI Codex) used in the CoT-Collection?
- Basis in paper: Section 4.1.2 notes that the CoT-Collection rationales were augmented using OpenAI Codex, implying the student learns a specific "Codex-style" reasoning process.
- Why unresolved: It is unclear if the gains are robust general reasoning improvements or if the student is merely overfitting to the stylistic patterns of the Codex-generated rationales.
- What evidence would resolve it: Experiments replacing Codex rationales with human-written chains-of-thought or rationales generated by the teacher model itself, comparing performance stability.

## Limitations
- Evaluation limited to BIG-Bench-Hard benchmark and two model families (Qwen, Llama2)
- Focuses exclusively on white-box knowledge distillation
- Dataset and hyperparameters are fixed, potentially affecting reproducibility
- Generalizability to non-reasoning tasks and multilingual settings remains untested

## Confidence

**High Confidence**: The observed performance improvements from CoT-enhanced distillation are statistically consistent within the experimental setup, supported by multiple model scales and families. The methodological framework (white-box KD with CoT) is well-defined and reproducible.

**Medium Confidence**: The claim that CoT particularly helps when vanilla KD underperforms the baseline is based on the specific dataset and models tested. This may not hold universally across all reasoning tasks or larger teacher models.

**Low Confidence**: Generalizability to non-reasoning tasks, multilingual settings, or other model architectures (e.g., decoder-only vs. encoder-decoder) remains untested.

## Next Checks

1. Test CoT-enhanced distillation on a broader set of reasoning and non-reasoning benchmarks (e.g., MMLU, ARC, commonsense reasoning datasets) to assess generalizability.
2. Compare CoT-enhanced white-box KD against black-box KD and self-distillation methods to isolate the benefits of the distillation paradigm.
3. Evaluate the impact of varying CoT quality and quantity (e.g., noisy vs. high-quality rationales) on distillation performance to determine robustness to CoT data quality.