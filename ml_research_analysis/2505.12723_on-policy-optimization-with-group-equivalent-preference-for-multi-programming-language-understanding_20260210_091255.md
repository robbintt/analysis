---
ver: rpa2
title: On-Policy Optimization with Group Equivalent Preference for Multi-Programming
  Language Understanding
arxiv_id: '2505.12723'
source_url: https://arxiv.org/abs/2505.12723
tags:
- code
- llms
- preference
- languages
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OORL, a reinforcement learning framework
  for enhancing multilingual code generation in large language models. It integrates
  on-policy RL with rule-based rewards for code translation accuracy and Group Equivalent
  Preference Optimization (GEPO), which uses groups of intermediate representations
  (IRs) to capture functional equivalence.
---

# On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding

## Quick Facts
- arXiv ID: 2505.12723
- Source URL: https://arxiv.org/abs/2505.12723
- Reference count: 30
- Primary result: OORL improves multilingual code generation performance on MultiPL-E and CrossPLEval benchmarks, including on low-resource languages.

## Executive Summary
This paper introduces OORL, a reinforcement learning framework for enhancing multilingual code generation in large language models. It integrates on-policy RL with rule-based rewards for code translation accuracy and Group Equivalent Preference Optimization (GEPO), which uses groups of intermediate representations (IRs) to capture functional equivalence. By training models to recognize equivalent IRs, GEPO guides nuanced understanding of code functionality across languages. Experiments show that OORL significantly improves performance on code generation benchmarks including on low-resource languages.

## Method Summary
OORL combines on-policy reinforcement learning for code translation with Group Equivalent Preference Optimization (GEPO) for functional equivalence learning. The framework uses binary unit-test rewards for translation accuracy and GEPO with LLVM IR groups to teach functional equivalence. The total loss combines on-policy REINFORCE++ updates with GEPO updates, using Qwen3-8B as the base model. Training involves 2400 code translation problems and 9600 IR preference groups, with AdamW optimization and DeepSpeed-Zero2 for distributed training.

## Key Results
- OORL significantly improves performance on MultiPL-E and CrossPLEval benchmarks compared to baseline approaches
- The framework shows strong cross-lingual generalization, improving performance on languages not explicitly trained for translation
- GEPO contributes to performance gains beyond what binary rule-based rewards alone can achieve

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on code translation tasks facilitates cross-lingual proficiency transfer from high-resource to low-resource programming languages.
- Mechanism: The model learns to map logical constructs between languages by solving translation tasks with unit-test verification. This forces acquisition of language-agnostic semantic understanding rather than surface syntax memorization.
- Core assumption: Proficiency in popular languages (Python, C++) can transfer through supervised translation signals to less-represented languages.
- Evidence anchors:
  - [abstract] "To address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages."
  - [section 3.1] "Intuitively, if LLMs can accurately translate Python code into other programming languages, they can achieve comparable performance in Python code generation tasks in those programming languages."
  - [corpus] Weak direct validation. Neighbor paper "MultiPL-MoE" addresses multi-programming-lingual extension but uses MoE architecture rather than translation-based transfer.
- Break condition: If low-resource target languages lack sufficient translation pairs with high-resource sources, transfer fails.

### Mechanism 2
- Claim: Binary rule-based rewards from unit tests provide unambiguous correctness signals that stabilize on-policy RL training.
- Mechanism: The reward function R_rule(q, o) assigns 1 if translated code compiles and passes all unit tests, 0 otherwise. This sparse but precise signal guides policy optimization via REINFORCE++ with clipped objectives and advantage estimation.
- Core assumption: Unit tests comprehensively capture functional correctness, and binary rewards suffice for coarse-grained learning.
- Evidence anchors:
  - [section 3.1] "R_rule(q, o) = 1 if o represents a successful code translation from q, 0 otherwise."
  - [section 3.1] "successful translation demonstrates that o must adhere to the standard format...and pass unit tests."
  - [corpus] No direct corpus validation for binary reward mechanisms in code translation specifically.
- Break condition: If unit tests are incomplete or buggy, reward signal becomes noisy, degrading policy quality.

### Mechanism 3
- Claim: Group-based preference optimization over compiler IRs teaches fine-grained functional equivalence that binary rewards cannot capture.
- Mechanism: GEPO constructs groups Y_w (equivalent IRs from different compiler optimization levels) and Y_l (inequivalent IRs via augmentation). The loss L_GEPO maximizes average winner-group reward minus average loser-group reward while constraining within-winner variance, enforcing that equivalent implementations receive similar rewards.
- Core assumption: LLVM IRs are sufficiently language-agnostic that equivalence learning on IRs transfers to high-level code understanding.
- Evidence anchors:
  - [section 3.2] "GEPO compares the average reward of Y_w to the average reward of Y_l...explicitly enforces equivalence within Y_w by constraining the variance of their rewards."
  - [section 3.2] "compiler IRs are agnostic to the source programming language and target execution platform, providing a method to align constructs from different programming languages semantically."
  - [corpus] Neighbor paper "IRCoder" validates IR-based pre-training for multilingual code generation, supporting IR utility but not GEPO specifically.
- Break condition: If IR generation fails (compilation errors) or augmentation produces invalid inequivalent IRs, preference groups become noisy.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) / REINFORCE-style algorithms**
  - Why needed here: On-policy RL component uses REINFORCE++ with clipped objectives and advantage estimation. Understanding policy gradients, clipping, and KL penalties is essential.
  - Quick check question: Can you explain why the clip(·, 1-ε, 1+ε) operation prevents excessive policy updates?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: GEPO extends DPO from pairwise to group-wise comparisons. DPO's implicit reward formulation (r_φ ∝ log π_θ/π_ref) is directly reused.
  - Quick check question: How does DPO avoid training an explicit reward model while still optimizing preferences?

- Concept: **Compiler Intermediate Representations (LLVM IR)**
  - Why needed here: GEPO training data consists of C-to-IR pairs with equivalent IRs from different optimization levels (-Oz, -O3, etc.).
  - Quick check question: Why would different compiler optimization flags produce semantically equivalent but syntactically different IRs?

## Architecture Onboarding

- Component map:
  ```
  Code Translation Query (q)
         │
         ├──→ [On-Policy RL Branch]
         │         │
         │    Policy π_θ generates translation
         │         │
         │    Unit Tests → Binary Reward R_rule
         │         │
         │    Advantage Estimation → L_OnP
         │
         └──→ [GEPO Branch (off-policy)]
                  │
             Static IR Groups (Y_w, Y_l)
                  │
             L_GEPO = preference term + λ × variance penalty
                  │
         Combined Loss: L = w_rl × L_OnP + w_gepo × L_GEPO
                  │
             Policy Update (AdamW, DeepSpeed-Zero2)
  ```

- Critical path:
  1. Initialize π_θ from SFT reference model (Qwen3-8B)
  2. For each training step: sample translation trajectories → compute binary rewards → estimate advantages → compute L_OnP
  3. In parallel: sample IR groups from static preference dataset → compute L_GEPO
  4. Combine losses with weights (w_rl=1.0, w_gepo=0.01) → update π_θ

- Design tradeoffs:
  - **Memory vs. group size**: GEPO processes n+m responses per prompt. Larger groups = better equivalence signal but higher GPU memory. Mitigation: serialize intra-group processing (trades latency for memory).
  - **Reward granularity**: Binary rewards are stable but coarse; GEPO adds nuance but requires quality IR data.
  - **Generalization vs. training coverage**: Model improves on untrained languages (Scala, Go, TypeScript, C#, Haskell) but gains are smaller than trained languages.

- Failure signatures:
  - Translation compiles but fails unit tests → binary reward = 0, no gradient signal for partial correctness
  - IR groups contain mislabeled equivalence → GEPO variance constraint fights preference objective
  - Excessive KL divergence from π_ref → policy collapses to narrow outputs; increase β penalty
  - Low-resource language performance plateaus → insufficient translation training pairs; consider data augmentation

- First 3 experiments:
  1. **Ablation: On-policy RL only vs. GEPO only vs. OORL (both)** on MultiPL-E benchmark to isolate contribution of each component (Table 4 shows REINFORCE++ alone: 73.60 → REINFORCE++ + GEPO: 76.31).
  2. **Cross-lingual generalization test**: Train on Python/C++/Java translations only, evaluate on held-out Scala/Haskell translations to verify transfer (Table 3 shows Qwen3-8B-OORL gains on untrained languages).
  3. **Group size sensitivity**: Vary |Y_w| and |Y_l| in GEPO to measure impact on CrossPLEval scores and GPU memory usage; identify saturation point where larger groups yield diminishing returns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the memory overhead of Group Equivalent Preference Optimization (GEPO) be reduced without incurring the training latency associated with response serialization?
- Basis in paper: [explicit] Appendix C explicitly identifies GPU memory consumption as a limitation due to processing groups of preference responses rather than pairs, noting that the proposed serialization mitigation increases training time.
- Why unresolved: The authors identify the trade-off between memory usage (grouping) and latency (serialization) but do not propose an algorithmic or architectural solution to optimize this balance.
- What evidence would resolve it: A modified GEPO implementation that maintains group sizes while flattening memory peaks, or benchmarks showing acceptable latency/memory trade-offs on consumer-grade hardware.

### Open Question 2
- Question: Does the reliance on LLVM Intermediate Representations (IRs) limit the framework's effectiveness for low-resource languages that lack robust compilation pathways to LLVM?
- Basis in paper: [inferred] Section 3.2 and 4.1 note the use of C-to-IR groups for training, assuming that compiler IRs expose operators easier for the model to understand than abstract high-level code.
- Why unresolved: The paper does not analyze failures specific to languages with poor LLVM support, leaving the dependency on compiler infrastructure as a potential bottleneck for "non-popular" languages.
- What evidence would resolve it: Evaluation results on languages outside the LLVM ecosystem or an analysis of performance correlation between language-specific IR quality and translation accuracy.

### Open Question 3
- Question: To what extent does the binary rule-based reward in the on-policy phase conflict with the nuanced equivalence signals from GEPO during the simultaneous optimization process?
- Basis in paper: [inferred] Section 3.1 describes a binary reward (0 or 1) while Section 3.2 argues for GEPO because rule-based rewards lack process-level supervision; the total loss (Eq. 10) combines these potentially conflicting signals.
- Why unresolved: While the paper demonstrates improved aggregate performance, it does not investigate if the binary reward suppresses the diversity of functionally equivalent solutions encouraged by GEPO.
- What evidence would resolve it: An ablation study analyzing the variance of generated solutions when trained exclusively with GEPO versus the combined OORL objective.

## Limitations

- **Binary reward granularity**: While binary rewards provide stable signals for on-policy RL, they cannot capture partial correctness or guide the model toward functional improvements when unit tests fail. This limitation is partially mitigated by GEPO but creates a training bottleneck for translation quality.

- **IR preference data quality**: GEPO's effectiveness depends on accurate equivalence labeling within IR groups. The paper does not detail the augmentation strategy for creating inequivalent IRs, leaving uncertainty about potential noise in preference signals.

- **Generalization to truly unseen languages**: Cross-lingual gains on languages like Scala and Haskell suggest transfer capability, but the evaluation languages are still within the mainstream programming paradigm. Performance on highly specialized or domain-specific languages remains untested.

## Confidence

- **High confidence**: The mechanism of using binary unit-test rewards for on-policy RL is sound and well-supported by the literature on code generation evaluation. The REINFORCE++ implementation with advantage estimation follows established practice.

- **Medium confidence**: The GEPO framework's theoretical foundation is strong (extending DPO to group preferences), but practical effectiveness depends heavily on IR group quality. The claim that IR equivalence transfers to high-level code understanding is reasonable but needs more validation.

- **Medium confidence**: Cross-lingual transfer claims are supported by empirical results showing gains on untrained languages, but the evaluation set is limited to mainstream languages. The transferability to truly low-resource or niche programming languages is not fully validated.

## Next Checks

1. **IR group quality audit**: Sample and manually verify 100 GEPO training groups to ensure equivalent IRs are truly semantically identical and inequivalent IRs are genuinely functionally different. Calculate error rates in equivalence labeling.

2. **Scaling experiment on resource-constrained languages**: Test OORL on a set of truly low-resource programming languages (e.g., Racket, Erlang, Prolog) with minimal training data to measure performance degradation and identify training bottlenecks.

3. **Transfer efficiency measurement**: For languages trained only on translation (Scala, Haskell, etc.), compare the sample efficiency of OORL vs. standard supervised fine-tuning to quantify the practical advantage of the translation-based transfer approach.