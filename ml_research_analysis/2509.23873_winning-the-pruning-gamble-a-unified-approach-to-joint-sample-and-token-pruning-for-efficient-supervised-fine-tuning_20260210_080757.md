---
ver: rpa2
title: 'Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning
  for Efficient Supervised Fine-Tuning'
arxiv_id: '2509.23873'
source_url: https://arxiv.org/abs/2509.23873
tags:
- pruning
- sample
- token
- data
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Quadrant-based Tuning (Q-Tuning), the first
  unified framework that jointly optimizes sample-level and token-level pruning for
  efficient supervised fine-tuning of large language models. Q-Tuning uses an Error-Uncertainty
  (EU) Plane to diagnose heterogeneous utility across training data and implements
  a two-stage strategy: first, it prunes uninformative samples (harmful noise and
  redundant knowledge) at the sample level; second, it applies asymmetric token pruning
  to misconception samples while preserving calibration samples in full.'
---

# Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2509.23873
- Source URL: https://arxiv.org/abs/2509.23873
- Reference count: 40
- Primary result: Q-Tuning achieves +38% average improvement over full-data baseline on SmolLM2-1.7B using only 12.5% of training data

## Executive Summary
This paper introduces Quadrant-based Tuning (Q-Tuning), the first unified framework that jointly optimizes sample-level and token-level pruning for efficient supervised fine-tuning of large language models. Q-Tuning uses an Error-Uncertainty (EU) Plane to diagnose heterogeneous utility across training data and implements a two-stage strategy: first, it prunes uninformative samples (harmful noise and redundant knowledge) at the sample level; second, it applies asymmetric token pruning to misconception samples while preserving calibration samples in full. Across five diverse benchmarks, Q-Tuning consistently outperforms existing pruning methods and even full-data fine-tuning. Notably, on SmolLM2-1.7B, Q-Tuning achieves a +38% average improvement over the full-data baseline using only 12.5% of the original training data.

## Method Summary
Q-Tuning is a two-stage pruning framework that first categorizes samples into four quadrants using the Error-Uncertainty (EU) Plane, which measures perplexity (error) and entropy (uncertainty) per sample. Samples are partitioned into: Q1 (harmful noise), Q2 (misconceptions), Q3 (redundant knowledge), and Q4 (calibration). The framework retains Q2 and Q4 samples while pruning Q1 and Q3. In the second stage, asymmetric token pruning is applied: for Q2 samples, tokens are pruned based on smoothed perplexity scores to isolate misconception-bearing tokens, while Q4 samples are preserved intact to maintain calibration anchors. The pruning thresholds are dynamically computed each batch using bisect search to meet target sample retention ratios.

## Key Results
- Q-Tuning achieves +38% average improvement over full-data baseline on SmolLM2-1.7B using only 12.5% of training data
- Outperforms existing pruning methods and full-data fine-tuning across five diverse benchmarks (ARC-E, ARC-C, GSM8K, SQuAD, TriviaQA, MATH)
- Ablation confirms asymmetric token pruning (Q2 only) is critical: token-pruning Q4 hurts performance (57.23 vs 58.09 avg), while token-pruning Q2 helps (60.19 vs 59.04)
- Neighbor-aware token smoothing with λ=0.5 improves pruning precision (46.79 avg vs 45.92 without smoothing)

## Why This Works (Mechanism)

### Mechanism 1
The Error–Uncertainty (EU) Plane partitions training data into four quadrants with heterogeneous utility, enabling targeted pruning strategies. Perplexity (PPL) measures model error—how "surprising" the ground-truth is—while entropy (Ent) measures uncertainty—how broadly the model distributes probability mass. Together, they create a 2D space separating: Q1 (high PPL, low Ent = harmful noise), Q2 (high PPL, high Ent = valuable misconceptions), Q3 (low PPL, low Ent = redundant knowledge), Q4 (low PPL, high Ent = calibration data). Core assumption: Perplexity and entropy are sufficient statistics for data utility; the 2D projection preserves the meaningful structure needed for pruning decisions.

### Mechanism 2
Asymmetric token pruning—removing tokens only from Q2 samples while preserving Q4 samples intact—improves learning by amplifying correction signals without destroying calibration anchors. Q2 samples contain "confidently wrong" regions mixed with useful context. Token pruning based on local PPL spikes isolates misconception-bearing tokens. Q4 samples serve as uncertainty anchors; pruning them corrupts the gradient signal for learning calibrated confidence. Core assumption: High-PPL tokens in Q2 samples are causally responsible for the model's errors, not merely correlated with difficulty.

### Mechanism 3
Context-aware token scoring using smoothed PPL with neighbor information reduces false positives from isolated PPL spikes. Each token's importance score combines its own PPL with neighbors' PPL: s_i = (1-λ)PPL_i + λ(PPL_{i-1} + PPL_{i+1}). This smoothing prevents pruning semantically critical tokens that happen to have locally high surprise. Core assumption: True misconception tokens cluster spatially; isolated high-PPL tokens are noise rather than signal.

## Foundational Learning

- **Bilevel optimization**
  - Why needed here: Q-Tuning formulates pruning as bilevel optimization (Equation 3)—outer loop searches for optimal pruners (Φ,Ψ), inner loop trains under their guidance. Understanding this framing clarifies why static heuristics fail: they don't account for model state evolution.
  - Quick check question: Can you explain why gradient-based optimization of pruning decisions would require differentiating through the entire training trajectory?

- **Perplexity vs. entropy as model diagnostics**
  - Why needed here: The EU Plane relies on interpreting these metrics correctly. High PPL ≠ high Ent; a model can be confidently wrong (high PPL, low Ent) or uncertainly correct (low PPL, high Ent).
  - Quick check question: Given a sample where the model assigns 90% probability to the wrong token and 10% to the correct one, is this Q1, Q2, Q3, or Q4?

- **Dynamic vs. static pruning**
  - Why needed here: Q-Tuning re-computes EU Plane thresholds each mini-batch via bisect search. Static methods pre-select data once; dynamic methods adapt as model capabilities evolve.
  - Quick check question: What computational overhead does dynamic pruning add per training step, and when would this overhead exceed the savings from pruning?

## Architecture Onboarding

- **Component map**:
  Forward pass → compute PPL/Ent per sample → bisect search for thresholds → quadrant assignment → retain Q2∪Q4 → for Q2 samples only → compute per-token PPL → neighbor smoothing → rank and mask → construct pruned batch → standard SFT on pruned batch

- **Critical path**:
  1. Threshold computation (bisect search, ~10 iterations) must complete before any pruning
  2. Token-level PPL requires storing per-token logits during forward pass
  3. Mask application must handle variable-length sequences after pruning

- **Design tradeoffs**:
  - **Batch size vs. threshold stability**: Larger batches provide more stable quantile estimates but increase forward-pass overhead (Figure 5a shows batch=32 outperforms batch=8)
  - **Smoothing λ vs. pruning precision**: Higher λ reduces false positives but may miss genuine localized errors (Table 5)
  - **Sample ratio vs. coverage**: Below ~10% samples, quadrant populations become sparse, threshold search may fail to converge

- **Failure signatures**:
  - **Quadrant collapse**: All samples fall into one or two quadrants → thresholds degenerate → pruning becomes random
  - **Q4 over-pruning**: Token-pruning Q4 samples (configuration error) → systematic calibration degradation
  - **Threshold oscillation**: r_sample too low for current dataset → bisect search oscillates without converging

- **First 3 experiments**:
  1. **Sanity check**: Run EU Plane analysis on a held-out validation set; verify quadrant distributions are non-degenerate (each quadrant >5% of data). If collapsed, dataset may be too homogeneous.
  2. **Ablation on asymmetric policy**: Compare three configs—(a) no token pruning, (b) token-prune all retained samples, (c) token-prune Q2 only. Expect (c) > (a) > (b) per Table 4.
  3. **Extreme budget test**: At 6.25% samples × 50% tokens, verify Q-Tuning still exceeds zero-shot baseline. If not, threshold search may be failing silently—add logging for convergence status.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- **Quadrant boundary determination**: Critical implementation details for bisect search (convergence criteria, initialization, edge cases) remain unspecified, creating uncertainty about threshold stability across datasets.
- **Token scoring function**: The supplement score formula when retained sample counts don't match targets is vague, with only a "normalized supplement score" mentioned but not specified.
- **Dataset generalizability**: All experiments use alignment-focused datasets (WizardLM, OpenHermes) and reasoning tasks (MathInstruct), leaving effectiveness on diverse domains like code generation untested.

## Confidence

**High Confidence**: The asymmetric token-pruning mechanism and its empirical validation across five benchmarks. The ablation study (Table 4) provides strong evidence that token-pruning Q2 but preserving Q4 consistently improves performance.

**Medium Confidence**: The EU Plane framework's sufficiency for data utility characterization. While the mathematical formulation is clear and empirical results are positive, the assumption that PPL+entropy capture all relevant utility dimensions lacks rigorous theoretical justification.

**Low Confidence**: The generalizability of neighbor-aware smoothing (Mechanism 3). The λ=0.5 default and spatial smoothing assumption may not hold for tasks with sparse critical tokens or non-local error patterns.

## Next Checks

1. **Threshold Stability Test**: Run EU Plane analysis on three diverse validation sets (alignment, reasoning, code). For each, record: (a) quadrant population distributions, (b) bisect search convergence curves, (c) final thresholds. Verify that no dataset produces degenerate quadrants (<5% in any quadrant) and that convergence is stable across random seeds.

2. **Token Pruning Ablation with Error Localization**: For Q2 samples in the best-performing configuration, visualize: (a) token-level PPL before pruning, (b) smoothed importance scores, (c) actual pruned tokens vs. ground-truth error locations. Quantify precision/recall of pruning in isolating misconception-bearing tokens.

3. **Extreme Budget Scaling**: Systematically test Q-Tuning at 3.125%, 1.5625%, and 0.78125% sample retention (with 50% token pruning). For each: (a) verify quadrant threshold convergence, (b) compare against zero-shot baseline, (c) analyze failure modes if performance degrades. This reveals the method's breaking point and identifies whether failures stem from threshold search or fundamental data scarcity.