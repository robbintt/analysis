---
ver: rpa2
title: 'Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability'
arxiv_id: '2512.03112'
source_url: https://arxiv.org/abs/2512.03112
tags:
- shapley
- sisr
- values
- payoff
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sparse Isotonic Shapley Regression (SISR) is introduced to address
  the non-additivity and sparsity challenges in Shapley value-based explainability.
  SISR learns a monotonic transformation to restore additivity and enforces an L0
  sparsity constraint on the Shapley vector, overcoming limitations of ad hoc post-hoc
  thresholding methods.
---

# Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability

## Quick Facts
- arXiv ID: 2512.03112
- Source URL: https://arxiv.org/abs/2512.03112
- Reference count: 12
- Sparse Isotonic Shapley Regression (SISR) introduces monotonic transformation learning and direct L0 sparsity constraints to address non-additivity and sparsity in Shapley value explainability.

## Executive Summary
SISR addresses the challenge of explaining nonlinear models using Shapley values by learning a monotonic transformation that restores additivity and enforcing L0 sparsity on the attribution vector. Unlike post-hoc thresholding methods, SISR integrates sparsity control directly into the optimization, avoiding attribution-distorting shrinkage bias. The method uses alternating optimization with Pool-Adjacent-Violators for isotonic regression and normalized hard-thresholding for support selection, achieving global convergence. Experiments demonstrate SISR's ability to recover true transformations, achieve strong support recovery under noise, and stabilize attributions across diverse payoff schemes while correctly identifying relevant features in the presence of irrelevant ones.

## Method Summary
SISR jointly learns a monotonic transformation T(·) and sparse attribution vector γ by minimizing a weighted least squares objective on transformed coalition values, subject to L0 sparsity and unit-norm constraints. The algorithm alternates between γ-updates using normalized hard-thresholding and t-updates using weighted isotonic regression via PAVA. The transformation T is recovered by interpolating learned values at observed coalition worths. This approach overcomes limitations of post-hoc thresholding and provides direct sparsity control without the shrinkage bias of L1 penalties, with convergence guaranteed through surrogate function majorization.

## Key Results
- SISR recovers true monotonic transformations with correlation >0.95 in synthetic experiments
- Achieves >90% support recovery rate across moderate noise levels (σ₀ up to 0.05)
- Correctly identifies relevant features and mitigates rank/sign distortions compared to standard Shapley values, particularly with irrelevant features and feature dependencies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SISR restores additive structure by learning monotonic transformation T(·) that maps coalition values to Gaussian-additive domain
- **Mechanism:** Assumes T(ν_A) ~ N(Σ_{j∈A} γ_j, σ²_A), creating T^(-1)-Σ-T structure; PAVA enforces monotonicity without parametric specification
- **Core assumption:** Monotonic invertible transformation exists; no irreducible higher-order interactions
- **Evidence anchors:** Abstract and Section 2; related work (arXiv:2505.00571) addresses nonlinearity differently
- **Break condition:** Fails with irreducible higher-order interactions

### Mechanism 2
- **Claim:** Direct L0 sparsity constraint enables exact support control without L1 shrinkage bias
- **Mechanism:** Normalized hard-thresholding H°(y; s) = H(y; s)/||H(y; s)||₂ provides closed-form updates selecting top-s entries by magnitude while preserving unit norm
- **Core assumption:** True support size s* ≤ s; irrelevant features have negligible contribution
- **Evidence anchors:** Abstract and Section 2; limited corpus evidence on integrated sparsity
- **Break condition:** s too small (omits true features); correlated relevant features cause ambiguous selection

### Mechanism 3
- **Claim:** Surrogate function majorization yields globally convergent alternating optimization
- **Mechanism:** Construct g(γ, γ⁻) = l(γ⁻) + ⟨∇l(γ⁻), γ - γ⁻⟩ + (ρ/2)||γ - γ⁻||₂² with ρ ≥ ||Z^⊤WZ||_₂; minimizing g under constraints admits closed-form H° solution
- **Core assumption:** ρ sufficiently large to majorize l; objective bounded below
- **Evidence anchors:** Section 3 Theorem 2; limited corpus evidence on surrogate optimization in Shapley contexts
- **Break condition:** ρ too small → surrogate does not majorize → potential divergence

## Foundational Learning

- **Concept: Shapley Value as Weighted Least Squares**
  - **Why needed here:** SISR builds on reformulation where Shapley values solve min_β Σ_A w_SH(A)(ν_A - Σ_{j∈A} β_j)²
  - **Quick check question:** For p=4, compute w_SH(A) when |A|=2

- **Concept: Isotonic Regression and PAVA**
  - **Why needed here:** Transformation T estimated via order-constrained regression; PAVA enforces t_i ≤ t_j for ν_i ≤ ν_j
  - **Quick check question:** Given δ = [1.2, 1.5, 1.3, 1.8] with ordering induced by sorted ν, identify violating adjacent pairs

- **Concept: Hard Thresholding with Unit-Norm Constraint**
  - **Why needed here:** γ-update requires selecting s entries while maintaining ||γ||₂ = 1; H° optimal requires convex analysis basics
  - **Quick check question:** Why does normalization γ ← ξ/||ξ||₂ after hard thresholding guarantee feasibility?

## Architecture Onboarding

- **Component map:**
  Baseline-adjusted ν ∈ R^{2^p} → γ-update (normalized hard-thresholding) → δ = Zγ → t-update (weighted PAVA) → (γ̂, T̂) → β̂_j = T̂^(-1)(γ̂_j)

- **Critical path:**
  1. Baseline adjustment ν_A ← ν_A - ν_∅
  2. Initialize t^{(0)} = Cν (large C if ||ν||_∞ small), γ = 0
  3. Set ρ = ||Z^⊤WZ||_₂ (spectral norm)
  4. Iterate: inner γ-convergence → PAVA update → check outer convergence
  5. Recover T̂^(-1) via interpolation on (T̂_i, ν_i)

- **Design tradeoffs:**
  - s selection: Lower s = faster but may miss features; RIC criterion recommended; s ≈ 1.5 × s* robust in simulations
  - Weight handling for ∅, F: Use large finite multipliers (e.g., 10 × max_{A∉{∅,F}} w_SH(A)) instead of +∞
  - Strict vs. non-strict monotonicity: Non-decreasing constraint is numerically stable but may create flat regions; average ν_i for duplicate T̂_i before inverting

- **Failure signatures:**
  - Non-convergence: ρ too low → increase beyond spectral norm
  - Degenerate T̂ ≈ constant: Normalization not enforced; check Σ_j (T(β_j))² = 1
  - Support instability across runs: s too large or σ₀ too high; reduce s or check SNR
  - Negative attributions for non-negative payoffs: Over-correction; verify baseline adjustment and transformation invertibility

- **First 3 experiments:**
  1. Transformation recovery: Synthetic data with known T* (e.g., √·, log(·+1)), p=10, s*=3. Plot T̂(ν) vs. T*(ν); target correlation >0.95
  2. Sparsity/noise robustness: Fix p=15, vary σ₀ ∈ {10^{-3}, 5×10^{-3}, 10^{-2}, 5×10^{-2}}. Measure affinity ⟨γ̂, γ*⟩ and support recovery; target >90% support recovery at moderate noise
  3. R²-payoff nonlinearity: Linear regression with Toeplitz covariance (θ=0.5), sparse coefficients. Compute subset R² as ν_A; verify T̂ deviates from linear (confirming correlation-induced non-additivity)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can SISR framework extend to jointly model nonlinear payoff distortions and genuine higher-order feature interactions?
- **Basis in paper:** Section 5 states unifying stabilization and interaction "represents a promising yet computationally demanding direction for future research"
- **Why unresolved:** Current SISR attributes all deviations from additivity to monotonic distortion T, treating potential interaction effects as noise rather than signals to be estimated
- **What evidence would resolve it:** Modified SISR model incorporating interaction terms on transformed scale, validated on synthetic data with independently known interactions and distortions

### Open Question 2
- **Question:** What are theoretical and practical implications of applying SISR within GLM framework?
- **Basis in paper:** Section 5 notes extension to GLMs is methodologically sound but leaves "detailed investigation to future work" because Shapley interpretability is weaker in non-Gaussian families
- **Why unresolved:** While optimization algorithm carries over, convergence guarantees and fidelity of "additivity restoration" property not yet analyzed for non-Gaussian losses
- **What evidence would resolve it:** Theoretical derivation of curvature bounds (e.g., ρ values) for non-Gaussian losses and empirical stability tests on classification tasks with heavy-tailed payoffs

### Open Question 3
- **Question:** Does strict monotonicity constraint on T(·) limit recovery of payoff structures requiring non-monotonic adjustments?
- **Basis in paper:** Section 2 imposes monotonicity (T ∈ M) to preserve ranking, but real-world payoff distortions from complex dependencies might theoretically violate this assumption
- **Why unresolved:** Paper assumes monotonicity preserves interpretability, but if true transformation required to restore additivity is non-monotonic, SISR may force incorrect fit leading to biased attributions
- **What evidence would resolve it:** Simulation studies comparing SISR against non-monotonic transformation baselines on data designed to have non-monotonic relationships between raw and "true" worth

## Limitations
- Cannot handle irreducible higher-order interactions—method assumes all non-additivity can be captured by monotonic univariate transformation
- Sparsity parameter s must be specified; RIC criterion suggested but details absent
- No comparison with other nonlinear explainability methods that explicitly model interactions

## Confidence

- **Mechanism 1 (Transformation Learning):** Medium confidence. Theoretically sound but core assumption unverified; explicitly cannot handle irreducible higher-order interactions
- **Mechanism 2 (Direct L0 Sparsity):** High confidence. Well-established technique in compressed sensing; correctly identifies shrinkage bias problem
- **Mechanism 3 (Surrogate Optimization):** Medium confidence. Standard majorization-minimization framework; convergence proof conventional but empirical evidence limited

## Next Checks

1. **Synthetic Higher-Order Interaction Test:** Generate data with x₁x₂ or x₁² terms that cannot be captured by monotonic univariate transformation. Verify SISR fails to recover true structure while methods explicitly modeling interactions succeed.

2. **Robustness to Sparsity Misspecification:** Systematically vary s/s* ratio from 0.5 to 2.0 on synthetic data. Quantify how support recovery and affinity degrade as s deviates from true sparsity.

3. **Comparison with Interaction-Aware Methods:** Benchmark against TreeSHAP or kernel SHAP with interaction detection on datasets with known feature dependencies. Measure whether SISR's attribution errors are primarily due to transformation misspecification versus sparsity selection.