---
ver: rpa2
title: 'Exploring the Feasibility of Multilingual Grammatical Error Correction with
  a Single LLM up to 9B parameters: A Comparative Study of 17 Models'
arxiv_id: '2505.06004'
source_url: https://arxiv.org/abs/2505.06004
tags:
- text
- language
- gemma
- llms
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 17 large language models (up to 9B parameters)
  for multilingual grammatical error correction across English, German, Italian, and
  Swedish. It compares model outputs using LanguageTool for grammatical correctness,
  BERTScore/SentenceBERT/BLEURT for semantic similarity, Levenshtein distance/GLEU
  for syntactic preservation, and F1-score for unchanged-correct detection.
---

# Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models

## Quick Facts
- arXiv ID: 2505.06004
- Source URL: https://arxiv.org/abs/2505.06004
- Reference count: 19
- Gemma 9B ranked first overall for multilingual grammatical error correction

## Executive Summary
This study evaluates 17 large language models up to 9B parameters for multilingual grammatical error correction across English, German, Italian, and Swedish. The research compares model outputs using multiple metrics including LanguageTool for grammatical correctness, BERTScore/SentenceBERT/BLEURT for semantic similarity, Levenshtein distance/GLEU for syntactic preservation, and F1-score for unchanged-correct detection. The longest, most explicit prompt ("Edit... minimal changes... return only corrected text...") performed best across all models. Gemma 9B emerged as the top performer, excelling in grammatical correction, semantic/syntactic preservation, and unchanged-correct detection.

The study reveals that model size alone is not predictive of quality for grammatical error correction, and fine-tuning specifically for GEC (as demonstrated by the Karen models) significantly improves performance. Six models (Gemma 9B/2B, EuroLLM 9B/1.7B, OpenChat 3.5, Llama 3.1) supported all four languages and improved correctness scores. However, the research also identified issues including language drift, verbosity, and some models producing unrelated text, highlighting the complexity of multilingual GEC tasks.

## Method Summary
The study evaluated 17 large language models (up to 9B parameters) for multilingual grammatical error correction across four languages: English, German, Italian, and Swedish. Models were tested using fixed-length prompts with 30 original and 30 corrupted sentences per language, derived from academic texts and manually corrupted with grammatical errors. Evaluation metrics included LanguageTool for grammatical correctness assessment, BERTScore/SentenceBERT/BLEURT for semantic similarity, Levenshtein distance/GLEU for syntactic preservation, and F1-score for unchanged-correct detection. The prompt variation tested included five different formulations ranging from minimal ("Edit text") to most explicit ("Edit text: [text] Please make minimal changes to the text to fix any grammatical errors. Return only corrected text."). Model outputs were compared against both original and corrupted inputs to assess correction quality.

## Key Results
- Gemma 9B ranked first overall, excelling in grammatical correction, semantic/syntactic preservation, and unchanged-correct detection
- Model size alone was not predictive of quality; fine-tuning for GEC (Karen models) improved performance
- Six models (Gemma 9B/2B, EuroLLM 9B/1.7B, OpenChat 3.5, Llama 3.1) supported all languages and improved correctness scores
- The longest, most explicit prompt ("Edit... minimal changes... return only corrected text...") performed best

## Why This Works (Mechanism)
The study demonstrates that large language models can effectively perform multilingual grammatical error correction when provided with explicit instructions and sufficient parameters. The mechanism relies on the models' ability to understand grammatical rules across multiple languages while maintaining semantic and syntactic fidelity to the original text. The explicit prompting strategy helps constrain the models to make minimal corrections rather than rewriting entire passages, which is crucial for preserving the author's intended meaning and style.

## Foundational Learning
- Grammatical Error Correction (GEC): Understanding of grammatical rules and error patterns across multiple languages
  - Why needed: Core capability for identifying and correcting grammatical errors
  - Quick check: Test model's ability to identify common grammatical errors in sample sentences
- Multilingual Language Understanding: Capacity to process and generate grammatically correct text in multiple languages
  - Why needed: Enables single model to handle multiple language pairs
  - Quick check: Evaluate model's performance across different language families
- Semantic Preservation: Maintaining original meaning while correcting grammatical errors
  - Why needed: Ensures corrections don't alter intended message
  - Quick check: Compare semantic similarity scores between original and corrected text
- Minimal Editing: Making only necessary changes to correct errors
  - Why needed: Preserves author's voice and style
  - Quick check: Analyze Levenshtein distance between original and corrected text

## Architecture Onboarding
**Component Map:** Input sentences -> Prompt formulation -> LLM processing -> Output generation -> Multi-metric evaluation (LanguageTool, BERTScore, BLEURT, GLEU, F1-score) -> Performance ranking

**Critical Path:** Prompt input → Model inference → Grammatical correction → Semantic preservation → Syntactic maintenance → Evaluation metrics → Performance assessment

**Design Tradeoffs:** Fixed-length prompts vs. model-specific optimization, comprehensive metric suite vs. evaluation complexity, multilingual support vs. potential language-specific degradation

**Failure Signatures:** Language drift (incorrect language output), verbosity (excessive text generation), unrelated text production, semantic distortion, over-correction (unnecessary changes)

**Three First Experiments:**
1. Test Gemma 9B on additional languages beyond the four studied to evaluate generalization capability
2. Compare performance using dynamic vs. fixed-length prompts across all models
3. Evaluate human assessment of corrected text quality to validate automated metric results

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relied exclusively on LanguageTool for grammatical correctness assessment, which may have systematic biases
- Testing dataset consisted of only 300 sentences per language, potentially missing diverse error types
- Fixed-length prompts were used for all models without exploring optimal prompting strategies per model
- Evaluation metrics don't directly measure preservation of original author's intended meaning or style
- Potential for models to introduce subtle errors that are grammatically correct but semantically incorrect

## Confidence
- Gemma 9B is the best overall model for multilingual GEC: Medium
- Model size alone is not predictive of quality: High
- Six models support all four languages and improve correctness scores: High
- Fine-tuning for GEC improves performance: Medium
- Model size is not a strong predictor of quality: High

## Next Checks
1. Validate Gemma 9B's superiority using human evaluation across all four languages, with multiple annotators per sentence to assess grammatical correctness, semantic preservation, and minimal editing quality
2. Test the same models on a larger, more diverse dataset (at least 1000 sentences per language) including more error types and domain variations
3. Conduct ablation studies comparing different prompt lengths and formulations for each model to determine optimal prompting strategies rather than using a one-size-fits-all approach