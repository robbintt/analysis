---
ver: rpa2
title: Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation
  and Model Comparison
arxiv_id: '2505.24701'
source_url: https://arxiv.org/abs/2505.24701
tags:
- sentiment
- data
- topics
- conversation
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a synthetic data generation approach using
  GPT-4o to create diverse ABSA datasets across four domains (Technology, Healthcare,
  Finance, Legal). The method generates 12,000 multi-turn conversations with controlled
  topic and sentiment distributions.
---

# Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison

## Quick Facts
- arXiv ID: 2505.24701
- Source URL: https://arxiv.org/abs/2505.24701
- Authors: Tejul Pandit; Meet Raval; Dhvani Upadhyay
- Reference count: 17
- One-line primary result: LLM-based synthetic data generation creates diverse ABSA datasets enabling model evaluation without real-world labeled data; DeepSeek-R1 achieves highest precision/F1 while Gemini 1.5 Pro offers fastest inference.

## Executive Summary
This paper introduces a pipeline using GPT-4o to generate synthetic multi-turn ABSA conversation datasets across four domains with controlled topic and sentiment distributions. The method employs prompt scaffolding and algorithmic constraint enforcement to ensure balanced aspect-sentiment pairings, followed by LLM validation and redundancy pruning to improve data quality. The resulting datasets enable benchmarking of three state-of-the-art LLMs, revealing trade-offs between precision, recall, and inference latency. The approach demonstrates that synthetic data can substitute for scarce real-world ABSA annotations while supporting rigorous model comparison.

## Method Summary
The method uses a multi-stage pipeline: GPT-4o generates 10 conversation scenarios and 20 representative topics per domain, then synthesizes 3,000 multi-turn conversations per domain with 2-3 topics and mixed sentiments assigned via Algorithm 1. Generated data is validated using Gemini 1.5 Pro as LLM-as-judge and filtered for redundancy using SBERT embeddings with cosine similarity ≥0.8. The final datasets (12,000 conversations total) are used to evaluate three LLMs on topic and sentiment classification, measuring subset accuracy, precision, recall, F1, and inference latency.

## Key Results
- DeepSeek-R1 achieved highest precision (0.80-0.84) and F1-score (0.86-0.89) across domains
- Gemini 1.5 Pro offered fastest inference (~2s vs 7-15s for others) with strong recall
- Synthetic datasets successfully controlled topic and sentiment distributions across all four domains
- Evaluation revealed systematic trade-offs between model precision, recall, and latency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained prompt scaffolding with explicit topic-sentiment pairing produces controllable ABSA data distributions.
- Mechanism: The pipeline decomposes generation into stages—domain selection, GPT-4o-generated conversation scenarios (10 per domain), GPT-4o-curated topic lists (20 per domain), and algorithmic sentiment assignment—such that each composite prompt encodes a domain, a conversation type, and topic-sentiment pairs. Algorithm 1 enforces constraints (2-3 topics, not all same sentiment, near-symmetric distribution) so outputs reflect target statistics rather than unconstrained LLM priors.
- Core assumption: The generator LLM adheres reliably to structural constraints in prompts and produces naturalistic multi-turn dialogue when given conversation-type templates.
- Evidence anchors: [section 3.1 and 3.1.3] Describes the staged pipeline and Algorithm 1 for topic/sentiment assignment with O(mn) complexity; states 12,000 conversations were generated and filtered. [abstract] Reports consistent topic and sentiment distributions across domains using GPT-4o. [corpus] "From Annotation to Adaptation...with Large Language Models" and "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis" corroborate LLM-based synthetic data for ABSA, but do not validate this paper's specific algorithm.
- Break condition: If prompt adherence degrades (e.g., model ignores topic or sentiment constraints), distributional control fails; increased constraint violation rates signal mechanism breakdown.

### Mechanism 2
- Claim: LLM-as-judge plus embedding-based redundancy pruning yields cleaner, semantically diverse data.
- Mechanism: Use Gemini 1.5 Pro to verify consistency of each conversation with assigned topic-sentiment labels (mimicking a two-annotator setup). Separately, encode conversations with SBERT, compute pairwise cosine similarity, and flag items with many neighbors at ≥0.8 similarity; prioritize the top 33% highest-similarity-degree items for manual review/removal.
- Core assumption: LLM judge generalization aligns with human judgment for ABSA label correctness, and embedding similarity meaningfully captures semantic redundancy in this domain.
- Evidence anchors: [section 3.2] Describes extraction of aspect indices, LLM validation with Gemini 1.5 Pro, SBERT embeddings, and similarity-based pruning strategy. [Table 1] Reports post-filtering statistics and balanced sentiment distributions across domains. [corpus] Corpus papers do not directly validate this paper's filtering pipeline; evidence is internal to the paper.
- Break condition: If the LLM judge systematically accepts mislabeled samples or rejects correct ones, or if embedding similarity fails to surface semantically redundant items (e.g., due to domain jargon), data quality diverges from expectations.

### Mechanism 3
- Claim: Multi-aspect, multi-sentiment conversation design reveals model trade-offs (precision vs. recall vs. latency).
- Mechanism: By embedding 2-3 topics with mixed sentiments per conversation, the dataset presents non-trivial classification boundaries. Evaluating multiple LLMs uncovers systematic differences—DeepSeek-R1 tends toward higher precision; Gemini 1.5 Pro and Claude 3.5 Sonnet toward higher recall—while latency measurements highlight practical deployment constraints.
- Core assumption: Synthetic multi-aspect conversations approximate enough of the difficulty distribution of real-world dialogues to expose meaningful model trade-offs.
- Evidence anchors: [section 4.1.2 and Tables 2-5] Shows per-domain precision/recall/F1 and consistent patterns (DeepSeek-R1 leads precision and F1; Gemini/Claude show strong recall). [section 4.1.3] Notes Gemini 1.5 Pro latency ~2s vs Claude ~7s vs DeepSeek-R1 ~15s and interprets trade-offs. [corpus] "Large-Scale Aspect-Based Sentiment Analysis with Reasoning-Infused LLMs" reports ABSA gains from synthetic data and reasoning, but does not confirm the specific trade-offs in this paper.
- Break condition: If downstream real-world ABSA performance fails to correlate with synthetic benchmark results, the ecological validity of the synthetic difficulty distribution is undermined.

## Foundational Learning

- Concept: Aspect-Based Sentiment Analysis (ABSA)
  - Why needed here: The task targets sentiment toward specific aspects (topics) within text rather than document-level polarity; the entire pipeline is designed to generate and evaluate aspect-sentiment pairs.
  - Quick check question: Given a customer dialogue mentioning both "wait time" (negative) and "diagnosis accuracy" (positive), can you identify the two aspect-sentiment pairs?

- Concept: Synthetic Data Generation via LLMs with Constraint Enforcement
  - Why needed here: The method relies on prompt engineering and algorithmic control to yield balanced topic/sentiment distributions; misunderstanding constraint enforcement leads to unrealistic expectations about distributional guarantees.
  - Quick check question: If an LLM ignored a prompt constraint that "not all sentiments can be the same," what would happen to dataset balance and downstream evaluation?

- Concept: Evaluation Metrics for Multi-Label Classification
  - Why needed here: Topic classification is multi-label across aspects; distinguishing micro/macro/weighted/sample averages is essential to interpret Tables 2-5 correctly.
  - Quick check question: When a model achieves high recall but lower precision on a multi-label ABSA task, what does this imply about false positives vs. false negatives in aspect prediction?

## Architecture Onboarding

- Component map: GPT-4o (scenario generation -> topic generation -> conversation synthesis) -> Gemini 1.5 Pro (LLM validation) -> SBERT (embedding-based redundancy pruning) -> Evaluation LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, DeepSeek-R1)

- Critical path: 1. Domain selection -> 2. Scenario enumeration (GPT-4o) -> 3. Topic proposal and distillation (GPT-4o) -> 4. Algorithmic topic-sentiment assignment (Algorithm 1) -> 5. Conversation synthesis (GPT-4o) -> 6. LLM validation (Gemini 1.5 Pro) -> 7. Embedding-based redundancy pruning (SBERT) -> 8. Evaluation across LLMs and latency measurement

- Design tradeoffs:
  - Control vs. naturalness: Strong prompt constraints improve distributional fidelity but may reduce linguistic diversity or introduce templatic feel
  - Judge choice: Using Gemini 1.5 Pro as judge provides scalability but risks systematic bias; human-in-the-loop would be costlier but more robust
  - Pruning threshold: ≥0.8 similarity and top-33% pruning is heuristic; stricter thresholds increase diversity but reduce dataset size
  - Model selection: DeepSeek-R1 favors precision; Gemini 1.5 Pro favors speed; Claude 3.5 Sonnet balances strong recall with moderate latency

- Failure signatures:
  - Prompt drift: Outputs ignore topic or sentiment constraints (check label-consistency rates)
  - Collapse to generic dialogue: Conversations become topically vague or stylistically repetitive (check vocabulary diversity and topic distinctness)
  - Judge-generator coupling: If judge and generator share systematic biases, erroneous labels may be retained (spot-check with human review)
  - Similarity-graph artifacts: Over-aggressive pruning removes informative near-duplicates that differ in crucial aspect mentions

- First 3 experiments:
  1. Constraint adherence audit: Sample 200 generated conversations; manually verify that topics and sentiments match prompt specifications; compute adherence rate
  2. Ablation of filtering stages: Compare dataset quality (downstream model precision/recall) with (a) no filtering, (b) only LLM-as-judge, (c) only embedding-based pruning, (d) both
  3. Precision-latency frontier: On a held-out slice, vary the model choice and decoding parameters to plot Pareto frontier of F1 vs. inference time; validate whether Gemini 1.5 Pro remains optimal under real-time SLAs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training on the synthetic ABSA datasets improve downstream performance on real-world human-annotated ABSA tasks?
- Basis in paper: [inferred] The paper evaluates LLMs on the synthetic data itself but does not benchmark whether fine-tuning on this data transfers to real-world ABSA datasets (e.g., SemEval).
- Why unresolved: No transfer learning experiments or external benchmark evaluations were conducted.
- What evidence would resolve it: Fine-tune a model on TechConV/HealthConV/FinConV/LegalConV and evaluate zero-shot or few-shot performance on human-labeled ABSA benchmarks; report delta vs. baseline.

### Open Question 2
- Question: How does human expert annotation compare with LLM-as-judge validation for quality control of synthetic ABSA conversations?
- Basis in paper: [explicit] Section 3.2 states "we used Gemini 1.5 Pro to verify whether each conversation accurately reflected the assigned topics and their corresponding sentiment labels" and notes this "mimics the two-annotator approach" but does not involve human annotators.
- Why unresolved: No human annotation or inter-annotator agreement is reported against the LLM judge.
- What evidence would resolve it: Sample and human-annotate a subset (e.g., 200-500 conversations per domain); compute agreement (Cohen's kappa) between human labels and LLM-as-judge labels; analyze systematic discrepancies.

### Open Question 3
- Question: Can the generation pipeline be extended to include conflict sentiment (simultaneous positive and negative toward the same aspect) without exacerbating class imbalance?
- Basis in paper: [inferred] Related work (Section 2) highlights conflict sentiment as "a critical aspect often overlooked" and notes scarcity of datasets annotated for conflict. The current work restricts sentiment to three classes and excludes conflict.
- Why unresolved: The pipeline design (Algorithm 1) explicitly disallows all-same-sentiment per conversation but does not introduce or balance a conflict sentiment class.
- What evidence would resolve it: Extend the sentiment set to four classes including conflict; augment prompts to elicit conflict expressions; measure class balance and model performance (precision/recall/F1) on the conflict class specifically.

## Limitations
- The LLM-as-judge validation lacks human annotation comparison, risking systematic labeling errors
- The SBERT-based redundancy pruning uses heuristic thresholds without sensitivity analysis
- Small evaluation sample size (50 per domain) may not robustly estimate model differences
- Prompt adherence to constraints was not quantitatively measured, risking distributional drift

## Confidence
- High confidence: The multi-stage synthetic generation pipeline, controlled topic-sentiment assignment, and the basic observation that DeepSeek-R1 achieves the highest precision/F1 are directly reported and reproducible
- Medium confidence: The effectiveness of LLM-as-judge for validation and the SBERT-based redundancy pruning in producing cleaner, more diverse data, since these steps lack direct human-annotation or ablation comparisons
- Medium confidence: The claimed ecological validity of synthetic conversations in exposing real model trade-offs, as no real-world ABSA dataset was used for comparison

## Next Checks
1. **Constraint adherence audit**: Sample 200 generated conversations; manually verify that topics and sentiments match prompt specifications; compute adherence rate to quantify prompt drift risk
2. **LLM-as-judge reliability**: Randomly sample 100 validated samples; have two human annotators re-label topic and sentiment; compute Cohen's kappa between LLM judge and humans to quantify agreement
3. **Pruning threshold sensitivity**: Re-run redundancy filtering with thresholds 0.7 and 0.9 similarity and 20%/50% neighbor-count cutoffs; measure impact on downstream model precision/recall/F1 and dataset size to confirm heuristic optimality