---
ver: rpa2
title: Reinforcement Learning with Function Approximation for Non-Markov Processes
arxiv_id: '2601.00151'
source_url: https://arxiv.org/abs/2601.00151
tags:
- policy
- where
- assumption
- function
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reinforcement learning with linear function
  approximation for non-Markov processes, addressing policy evaluation and Q-learning
  algorithms when state and cost processes do not form Markov decision processes.
  For policy evaluation, the paper proves convergence of TD(0) under ergodicity conditions
  on non-Markov processes.
---

# Reinforcement Learning with Function Approximation for Non-Markov Processes

## Quick Facts
- **arXiv ID:** 2601.00151
- **Source URL:** https://arxiv.org/abs/2601.00151
- **Reference count:** 24
- **Primary result:** TD(0) with linear function approximation converges for non-Markov processes under ergodicity conditions; Q-learning convergence is limited to special cases

## Executive Summary
This paper extends reinforcement learning theory beyond Markov decision processes to handle non-Markov state and cost processes. The key contribution is proving convergence of TD(0) under ergodicity conditions, showing that the limit corresponds to the fixed point of a joint operator combining orthogonal projection and Bellman operator of an auxiliary MDP. For Q-learning, convergence is not guaranteed in general, but the paper identifies special cases including discretization-based basis functions and perfect linear MDP structure. The results are particularly relevant for partially observed MDPs where finite-memory variables serve as state representations.

## Method Summary
The paper analyzes policy evaluation and Q-learning algorithms when state and cost processes do not form Markov decision processes. For TD(0), convergence is proven using stochastic approximation theory extended via Poisson equation decomposition adapted for non-Markov processes. The limit is characterized as the fixed point of the joint operator ΠT^γ, where Π is the L² projection onto the span of basis functions and T^γ is the Bellman operator of an auxiliary "stationary regime MDP" constructed from the invariant distribution. For Q-learning, convergence is shown only for special cases: when cost and transition are perfectly linear in basis functions, under greedy-policy covariance dominance, or with discretization-based basis functions.

## Key Results
- TD(0) converges almost surely for non-Markov processes under ergodicity conditions with mixing coefficient decay Σ_k √α(k) < ∞
- The TD(0) limit solves the projected Bellman equation corresponding to an auxiliary stationary regime MDP
- Q-learning with linear function approximation converges only for special cases: discretization-based basis functions, perfect linear MDP structure, or covariance dominance conditions
- For POMDPs using finite-memory variables, explicit error bounds decompose into projection error and finite-memory approximation error

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TD(0) with linear function approximation converges almost surely for non-Markov state and cost processes under ergodicity conditions.
- **Mechanism:** The proof extends classical stochastic approximation via Poisson equation decomposition. The key technical insight decomposes the non-Markov noise term δ_t^⊺ M_t into a martingale difference plus telescoping summable terms using a Poisson equation adapted to non-Markov processes. This enables the Robbins-Siegmund-style argument to proceed despite correlated noise.
- **Core assumption:** Assumption 1(ii): uniform L²-boundedness of Y^A_t = Σ_{k≥0} ||E[A(Z_{t+k})|F_t] - A|| and Y^b_t = Σ_{k≥0} ||E[b(Z_{t+k})|F_t] - b||. Sufficient condition: summable strong mixing coefficient Σ_k √α(k) < ∞.
- **Evidence anchors:**
  - [abstract] "convergence of TD(0) under ergodicity conditions on non-Markov processes"
  - [Section 2.1, Proposition 1] Full proof showing θ_t → θ* satisfying Aθ* = b
  - [corpus] Related work on TD convergence under mixing (weak corpus support for non-Markov specifically)
- **Break condition:** Non-ergodic processes; processes where invariant measure π doesn't exist; mixing coefficients that decay too slowly (non-summable).

### Mechanism 2
- **Claim:** The TD(0) limit corresponds to the fixed point of the joint operator ΠT^γ, where T^γ is the Bellman operator of an auxiliary "stationary regime MDP" constructed from the invariant distribution π.
- **Mechanism:** Given the joint process Z_t = (S_{t+1}, S_t, C_t, U_t) with invariant measure π, construct an auxiliary MDP with cost c(s,u) = E_π[C|s,u] and transition η(ds'|s,u) = E_π[1{S_1∈A}|s,u]. The fixed point equation E[Φ(S)(T^γ(θ*^⊺Φ(S)) - θ*^⊺Φ(S))] = 0 is exactly what the converged parameters satisfy.
- **Core assumption:** Assumption 4 (linearly independent basis functions, E[ΦΦ^⊺] invertible); Assumption 3 (bounded basis functions ||ϕ_i||_∞ ≤ 1).
- **Evidence anchors:**
  - [abstract] "limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary Markov decision process"
  - [Section 2.2] Construction of stationary regime MDP via regular conditional distributions
  - [Section 2.4, Theorem 1] Proof that θ' = θ* is the unique fixed point
  - [corpus] No direct corpus support for this specific auxiliary MDP construction
- **Break condition:** Singular feature covariance matrix; basis functions that are linearly dependent under π.

### Mechanism 3
- **Claim:** Q-learning with linear function approximation converges when using discretization-based (indicator function) basis functions, even for non-Markov processes.
- **Mechanism:** Discretization creates indicator functions that are orthonormal under the empirical measure. This makes the projection Π non-expansive in the supremum norm (not just L²), so ||Π(f)||_∞ ≤ ||f||_∞. Combined with Bellman operator contraction in sup-norm, the composition ΠT remains a contraction. The algorithm reduces to tabular Q-learning on the discretized process.
- **Core assumption:** Discretization bins with π(A_i) > 0 for all bins; the discretized process Ẑ_t satisfies ergodicity (visits all bins infinitely often).
- **Evidence anchors:**
  - [abstract] "for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown"
  - [Section 3.2] Full derivation showing ||Π(f)||_∞ ≤ ||f||_∞ for indicator basis
  - [Section 3.2, Theorem 2] Convergence proof citing [15] for non-Markov tabular Q-learning
  - [corpus] Weak support; related work exists but corpus lacks non-Markov discretization specifically
- **Break condition:** General basis functions; greedy/exploration policy covariance mismatch (Assumption 6 violated); empty discretization bins.

## Foundational Learning

- **Concept: Stochastic Approximation / ODE Method**
  - **Why needed here:** The core proof technique (Proposition 1) extends Benveniste-Métivier-Priouret stochastic approximation theory to non-Markov noise via Poisson equation decomposition.
  - **Quick check question:** Can you explain why a stochastic approximation iteration θ_{t+1} = θ_t + α_t(-Aθ_t + b + noise) converges when A is positive definite and noise is "asymptotically negligible" in the right sense?

- **Concept: Ergodicity and Mixing Coefficients**
  - **Why needed here:** Assumption 1 and its sufficient condition (Assumption 2 via strong mixing α(k)) are the minimal conditions allowing the non-Markov extension.
  - **Quick check question:** Given a stochastic process Z_t, what does Σ_k √α(k) < ∞ imply about the rate at which E[f(Z_{t+k})|F_t] approaches the stationary mean?

- **Concept: Projected Bellman Equation**
  - **Why needed here:** The limiting fixed point solves ΠT^γ(V) = V in L²(π), which differs from the true Bellman fixed point by the projection error.
  - **Quick check question:** If the true value function J^π_β lies outside span(Φ), what does the projected Bellman fixed point represent?

## Architecture Onboarding

- **Component map:**
  Non-Markov Process (S_t, C_t, U_t) -> Joint Process Z_t = (S_{t+1}, S_t, C_t, U_t) -> Invariant Measure π (Assumption 1) -> Auxiliary Stationary Regime MDP (c, η) constructed from π -> Linear FA: Φ = [φ_1,...,φ_d] -> TD(0): θ_{t+1} = θ_t - α_t Φ(S_t)[θ_t^T Φ(S_t) - C_t - βθ_t^T Φ(S_{t+1})] -> Converges to θ* (fixed point of ΠT^γ) -> Learned Value: V(s) = θ*^T Φ(s)

- **Critical path:**
  1. Verify ergodicity of your joint process (empirical averages converge; mixing is summable)
  2. Check that feature covariance E[Φ(S)Φ(S)^⊺] under π is invertible (no linear dependence)
  3. For Q-learning: either use discretization/indicator basis, or verify covariance dominance β²Σ_θ < Σ_γ

- **Design tradeoffs:**
  - **Discretization basis:** Guaranteed convergence, but curse of dimensionality on fine grids; error ~ discretization granularity + filter stability term
  - **General basis functions:** More expressive, but Q-learning may diverge without covariance dominance or exact linear MDP structure
  - **Memory length N in POMDPs:** Larger N reduces filter stability error L_t but increases state space dimension

- **Failure signatures:**
  - TD(0) oscillates or diverges → check: mixing too slow? singular features?
  - Q-learning diverges with general features → expected; switch to discretization or verify Assumption 6
  - Large gap between learned and true value → projection error (use richer basis) or finite-memory approximation error (increase N)

- **First 3 experiments:**
  1. **Sanity check on known MDP:** Run TD(0) on a tabular MDP with linear features that exactly represent the value function. Verify convergence to true value.
  2. **Non-Markov stress test:** Construct a process with slowly decaying mixing (e.g., long-range dependence). Measure convergence rate vs. mixing coefficient decay.
  3. **POMDP finite-memory:** Implement finite-memory TD(0) on a benchmark POMDP. Vary memory length N and discretization granularity. Plot error vs. N and grid size, comparing to theoretical bound (Theorem 3).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can explicit error bounds be derived for the difference between the learned value function (limit of the TD algorithm) and the actual value function of the policy under the true non-Markov dynamics?
- **Basis in paper:** [explicit] Section 2.5 states that the analysis does not answer the error relative to the true non-Markov dynamics for general processes, noting that this "requires a more careful analysis on the mixing properties of the process."
- **Why unresolved:** The paper establishes convergence to the fixed point of an auxiliary "stationary regime" MDP, but bounding the gap between this synthetic MDP and the original non-Markov process remains open for general settings (only partially answered for POMDPs).
- **What evidence would resolve it:** Derivation of an upper bound for $\|J_{\text{true}} - V^*\|$ that depends on the mixing coefficients or ergodicity properties of the underlying non-Markov state process.

### Open Question 2
- **Question:** What sufficient conditions, other than perfect linearity, covariance dominance, or discretization, can guarantee the convergence of Q-learning with linear function approximation for non-Markov processes?
- **Basis in paper:** [explicit] Section 3 states that "convergence is not guaranteed in general" and identifies only three special cases where it holds.
- **Why unresolved:** The joint operator formed by the projection and the Bellman operator is not a contraction in the general case, primarily due to the discrepancy between the exploration policy and the greedy policy embedded in the update rule.
- **What evidence would resolve it:** A theoretical proof of convergence under a new set of structural assumptions, or the demonstration of a specific algorithm modification that enforces stability without requiring the identified special cases.

### Open Question 3
- **Question:** Can the ergodicity of the joint process $(h_t, x_t, u_t)$ in finite-memory POMDPs be guaranteed under conditions weaker than the minorization condition (Assumption 7)?
- **Basis in paper:** [inferred] Section 4.4 notes that guaranteeing ergodicity is non-trivial because the finite-memory variable contains past control actions, and the current analysis relies on the sufficient but potentially restrictive minorization condition.
- **Why unresolved:** The dependence of the policy on the history of actions prevents the direct application of standard ergodicity results for the transition kernel alone, making the analysis of the joint process difficult.
- **What evidence would resolve it:** A proof establishing the exponential ergodicity of the joint process under weaker assumptions, or a constructive example showing that the minorization condition is necessary for stability.

## Limitations

- Theoretical framework requires abstract ergodicity conditions (mixing coefficient decay Σ_k √α(k) < ∞) that are difficult to verify in practice
- Q-learning convergence results are highly restrictive, applying only to discretization-based basis functions, perfect linear MDP structure, or strong covariance dominance conditions
- For POMDPs, finite-memory approximation introduces an error term L_t that depends on filter stability, which is problem-specific and not easily characterized

## Confidence

- **High Confidence:** TD(0) convergence under ergodicity (Mechanism 1) - complete proof in Section 2.1 with established stochastic approximation theory
- **Medium Confidence:** Auxiliary MDP construction and fixed point characterization (Mechanism 2) - proof is complete but auxiliary MDP construction lacks direct corpus support
- **Medium Confidence:** Discretization-based Q-learning convergence (Mechanism 3) - rigorous proof exists but relies on extension to non-Markov setting with limited corpus validation

## Next Checks

1. **Empirical verification:** Implement TD(0) on a synthetic non-Markov process with controlled mixing properties. Measure convergence rates and compare against theoretical predictions based on mixing coefficients.

2. **Finite-memory POMDP testing:** Construct a benchmark POMDP and evaluate how memory length N and discretization granularity affect the filter stability error L_t and overall performance, comparing against Theorem 3 bounds.

3. **Covariance dominance analysis:** For a non-Markov MDP with general basis functions, compute the feature covariance matrices Σ_θ and Σ_γ to test whether Assumption 6 holds, and observe whether Q-learning converges or diverges accordingly.