---
ver: rpa2
title: 'TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM
  Reasoning'
arxiv_id: '2512.13106'
source_url: https://arxiv.org/abs/2512.13106
tags:
- trapo
- unlabeled
- labeled
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of high annotation costs in reinforcement
  learning with verifiable rewards (RLVR) for training large reasoning models. While
  unsupervised RLVR methods eliminate the need for external supervision, they often
  fail due to model collapse from reinforcing incorrect reasoning patterns.
---

# TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning

## Quick Facts
- arXiv ID: 2512.13106
- Source URL: https://arxiv.org/abs/2512.13106
- Reference count: 40
- Primary result: 42.6% average accuracy with 1K labeled + 3K unlabeled samples, outperforming 45K unsupervised samples

## Executive Summary
This paper addresses the high annotation cost of reinforcement learning with verifiable rewards (RLVR) for training large reasoning models. While unsupervised RLVR eliminates the need for external supervision, it often fails due to model collapse from reinforcing incorrect reasoning patterns. TraPO introduces a semi-supervised RLVR framework that uses a small labeled dataset to guide learning on large unlabeled data, identifying reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. The method achieves strong results using only 10% of labeled data compared to fully supervised approaches.

## Method Summary
TraPO is a semi-supervised RLVR framework that combines a small labeled dataset with large unlabeled data for training reasoning models. It uses trajectory-based selection to identify reliable unlabeled samples by comparing their learning dynamics (pass rate trajectories) to those of labeled samples. The framework employs a hybrid reward system with supervised rewards for labeled data and consistency-based pseudo-rewards for unlabeled data, preventing reward collapse. A progressive reliable database expansion mechanism dynamically updates the trajectory database during training, starting with labeled samples and adding reliable unlabeled samples after a warm-up period.

## Key Results
- 42.6% average accuracy using only 1K labeled + 3K unlabeled samples
- Outperforms best unsupervised method (38.3%) trained on 45K unlabeled samples
- With 4K labeled + 12K unlabeled samples, surpasses fully supervised model trained on 45K labeled samples
- Demonstrates significant data efficiency compared to fully supervised RLVR approaches

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-Based Unlabeled Sample Selection
Unlabeled samples whose learning dynamics (pass rate trajectories) align with labeled samples provide reliable pseudo-supervision. The framework tracks pass rates across epochs and computes cosine similarity between unlabeled and reliable trajectory patterns, selecting samples where similarity exceeds thresholds. This alignment in learning dynamics implies transferability of supervision from verified examples.

### Mechanism 2: Supervised Anchoring Prevents Reward Collapse
A small labeled set breaks the self-reinforcing feedback loop of unsupervised RLVR by grounding correctness to external truth. Labeled samples receive binary rewards from ground-truth verification while unlabeled samples receive pseudo-rewards from consistency. This anchoring maintains gradient alignment with correct solutions, preventing degenerate convergence.

### Mechanism 3: Progressive Reliable Database Expansion
Dynamically expanding the "reliable" set during training improves coverage while maintaining quality. The framework initializes with labeled trajectories, then adds selected unlabeled trajectories after warm-up, refining selection criteria as the model improves. This progressive approach balances early stability with later unlabeled data utilization.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: TraPO builds on GRPO for policy updates; understanding advantage estimation and clipping is essential.
  - Quick check question: Given 8 rollouts with rewards [1,0,0,1,0,0,0,0], can you compute the GRPO advantage for the correct responses?

- **Concept: Semi-Supervised Learning with Consistency Regularization**
  - Why needed here: TraPO's core innovation is bridging labeled/unlabeled data via consistency in learning dynamics, not just output predictions.
  - Quick check question: How does TraPO's trajectory-based consistency differ from standard consistency regularization (e.g., FixMatch's confidence thresholding)?

- **Concept: Neural Tangent Kernel (NTK) and Gradient Alignment**
  - Why needed here: The theoretical justification relies on NTK alignment between correct/correct and incorrect/incorrect gradient directions.
  - Quick check question: Under the NTK regime, why does training on question q improve performance on semantically similar q'?

## Architecture Onboarding

- **Component map:**
  Labeled Data (D_l) -> Rollout Generator -> Pass Rate Tracker -> Reliable Database (init)
  Unlabeled Data (D_u) -> Rollout Generator -> Pass Rate Trajectory
  Pass Rate Trajectory <- Average Reliable Trajectory <- TCS Computation <- Selection Mask M
  GRPO Loss: L_labeled + M ⊙ L_unlabeled

- **Critical path:**
  1. Warm-up epochs (labels-only training + trajectory accumulation)
  2. TCS computation for all unlabeled samples
  3. Mask M application to filter unlabeled loss
  4. Joint GRPO update on labeled + selected unlabeled
  5. Database expansion with newly reliable samples

- **Design tradeoffs:**
  - **top-p (0.1 recommended):** Lower = more conservative, higher = more noise
  - **Γ threshold (0.4-0.5 recommended):** Too low admits noisy samples; too high underutilizes unlabeled data
  - **Warmup epochs (5-8 recommended):** Must stabilize pseudo-labels before unlabeled incorporation

- **Failure signatures:**
  - Rapid accuracy collapse after warm-up: TCS threshold too low or top-p too high
  - No improvement over supervised baseline: Domain shift too large
  - OOD performance degradation: Overfitting to labeled ID patterns

- **First 3 experiments:**
  1. Reproduce 1K labeled + 3K unlabeled setting on Qwen2.5-Math-7B with default hyperparameters
  2. Ablate warm-up length (2, 5, 8, 12 epochs) to understand trajectory stabilization sensitivity
  3. Test domain shift robustness: Use ID labeled (math) + OOD unlabeled (non-math) per Table 2

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the TraPO framework maintain its data efficiency and stability when applied to large language models with significantly more parameters (e.g., 13B or larger)?
- **Open Question 2:** What specific criteria or characteristics define the optimal set of labeled examples to effectively guide the selection of unlabeled samples?
- **Open Question 3:** How does TraPO perform in domains where the base model lacks sufficient capability to generate reliable pseudo-pass rates via majority voting?

## Limitations

- Performance gains on ID tasks are strong, but OOD results are notably lower, raising questions about trajectory-based selection robustness to distribution shift
- The warm-up duration (8-10 epochs) is critical but not rigorously justified
- The selection mechanism relies on hyperparameters that may not generalize across domains or reasoning tasks beyond math

## Confidence

- **High confidence:** The semi-supervised framework design, trajectory similarity computation, and core empirical results
- **Medium confidence:** The theoretical NTK-based justification and warm-up necessity
- **Low confidence:** The claim that trajectory dynamics are universally transferable across domains

## Next Checks

1. **Hyperparameter sensitivity:** Systematically vary top-p (0.05, 0.1, 0.2, 0.5) and Γ (0.3, 0.4, 0.5) to confirm robustness of selection
2. **Warm-up ablation:** Compare model performance with warm-up lengths of 2, 5, 8, 12 epochs to isolate trajectory stabilization effects
3. **Domain shift stress test:** Train with ID labeled (math) + OOD unlabeled (non-math) to validate trajectory matching under distribution shift