---
ver: rpa2
title: 'OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap
  Data'
arxiv_id: '2511.00345'
source_url: https://arxiv.org/abs/2511.00345
tags:
- image
- diffusion
- data
- generation
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OSMGen, a novel generative framework that
  synthesizes high-fidelity satellite imagery directly from raw OpenStreetMap (OSM)
  JSON data. By conditioning on vector geometries, semantic tags, location, and time,
  OSMGen achieves fine-grained control over scene generation, surpassing prior approaches
  that rely on raster tiles or bounding boxes.
---

# OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data

## Quick Facts
- **arXiv ID:** 2511.00345
- **Source URL:** https://arxiv.org/abs/2511.00345
- **Reference count:** 40
- **Primary result:** Novel generative framework that synthesizes high-fidelity satellite imagery directly from raw OpenStreetMap (OSM) JSON data, achieving fine-grained control over scene generation.

## Executive Summary
OSMGen introduces a novel generative framework that synthesizes high-fidelity satellite imagery directly from raw OpenStreetMap (OSM) JSON data. By conditioning on vector geometries, semantic tags, location, and time, OSMGen achieves fine-grained control over scene generation, surpassing prior approaches that rely on raster tiles or bounding boxes. The framework leverages a diffusion model augmented with ControlNet to enforce geometric fidelity and can generate consistent before-after image pairs by editing OSM inputs, preserving scene coherence outside the modified regions. This capability enables large-scale generation of pixel-perfect labeled datasets to address data scarcity in geospatial AI, supports dynamic urban planning simulations, and paves the way for automated, structured OSM updates driven by detected changes in satellite imagery.

## Method Summary
OSMGen synthesizes 256×256 satellite imagery from raw OSM JSON by converting vector geometries into dual segmentation masks (General and Specific) that are injected into a frozen Stable Diffusion model via a trainable ControlNet branch. The framework also incorporates spatiotemporal embeddings (SatCLIP for location, Date2Vec for time) and text prompts via CLIP, which are projected and summed into the diffusion timestep embedding. Training uses a diffusion loss on ~20,000 points sampled from the FMoW dataset, with a batch size of 2048 on a single NVIDIA A100. For editing, DDIM inversion deterministically maps a generated image back to a latent code, allowing consistent before-after pairs by modifying OSM masks while preserving scene coherence outside the edit region.

## Key Results
- Achieves superior geometric fidelity compared to raster-based approaches by conditioning on vector geometries via ControlNet
- Generates consistent before-after image pairs through DDIM inversion, preserving scene coherence outside modified regions
- Enables large-scale generation of pixel-perfect labeled datasets for geospatial AI applications

## Why This Works (Mechanism)

### Mechanism 1
Conditioning on raw vector JSON (via masks) preserves geometric fidelity better than rasterized map tiles. The system converts OSM vector geometries into dual segmentation masks (General and Specific) and injects them into a frozen Stable Diffusion model via a trainable ControlNet branch. ControlNet adds spatial control features to the U-Net decoder, forcing the generator to adhere to exact polygonal boundaries rather than approximating shapes from pixels. Core assumption: The 2D projection of vector data into masks retains sufficient information for the ControlNet to distinguish fine-grained classes without the semantic loss inherent in raster tiles.

### Mechanism 2
Spatiotemporal embeddings steer global scene aesthetics (seasonality, lighting) while masks dictate structure. The framework encodes geographic coordinates (SatCLIP) and timestamps (Date2Vec), projecting and summing these embeddings into the diffusion timestep embedding. This biases the denoising process toward environment-specific distributions without altering the layout enforced by ControlNet. Core assumption: The diffusion model can disentangle "style" (driven by time/location embeddings) from "structure" (driven by masks).

### Mechanism 3
Deterministic inversion allows for consistent "before-after" editing by isolating changes to the mask edit region. The model uses DDIM Inversion to deterministically map a generated "before" image back to a latent code. By modifying the OSM mask and re-denoising from this latent code, the model produces an "after" image. Because the noise path is deterministic and the masks control the rest of the scene, unedited regions remain highly consistent. Core assumption: The inversion depth is sufficient to allow the edit but preserves enough latent information to reconstruct the background perfectly.

## Foundational Learning

- **Latent Diffusion Models (LDMs) & ControlNet:** Standard LDMs generate images from text alone. ControlNet is an adapter that adds external spatial constraints (masks) to this process. Quick check: How does ControlNet modify the forward pass of a frozen Stable Diffusion model?

- **DDIM (Denoising Diffusion Implicit Models):** The editing mechanism relies on "inversion"—turning an image back into noise. Standard diffusion is stochastic, making exact inversion impossible. DDIM is deterministic, making inversion mathematically tractable. Quick check: Why is determinism required to generate a consistent "before-after" pair?

- **OSM Data Schema (Nodes, Ways, Relations):** The input is raw JSON, not a picture of a map. You need to understand how vector coordinates are parsed to create the "General" and "Specific" masks that drive the ControlNet. Quick check: What is the difference between a "General mask" (broad categories) and a "Specific mask" (POI subtypes) in this architecture?

## Architecture Onboarding

- **Component map:** Input Preprocessor -> Vector Geometries -> General/Specific Masks -> ControlNet; Encoders (ControlNet, SatCLIP/Date2Vec, CLIP) -> Fusion -> Generator (Frozen Stable Diffusion U-Net) -> Editor (DDIM Inversion)

- **Critical path:** The connection between the Mask Fusion Layer and the ControlNet. If the masks are misaligned with the satellite imagery during training, the ControlNet will fail to enforce geometric fidelity, resulting in "bleeding" or misshapen buildings.

- **Design tradeoffs:** Raster vs. Vector Conditioning (Vector offers precision but requires complex preprocessing; Raster is easier but loses semantic detail). CFG Scale vs. Consistency (High CFG ensures the image matches the prompt, but breaks DDIM inversion consistency).

- **Failure signatures:** "Ghost" artifacts (unedited regions changing between before/after images indicates DDIM inversion failure or CFG set too high). Semantic Bleeding (Buildings appearing where roads should be suggests the "General Mask" encoding failed to distinguish classes).

- **First 3 experiments:** 1) Ablation on Condition Modalities (Masks only, Masks + Location, All inputs). 2) Edit Consistency Test (Generate before, invert, add specific POI, re-generate, calculate pixel difference in unmodified regions). 3) Generalization Check (Test on underrepresented geographic region to check SatCLIP generalization).

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the methodology raises several important directions for future research regarding downstream task performance, closed-loop mapping systems, and inversion stability.

## Limitations
- Evaluation relies primarily on qualitative visual comparisons rather than quantitative metrics
- No systematic measurement of key performance aspects like road connectivity accuracy or building footprint precision
- Real-world OSM data incompleteness could limit practical performance despite theoretical advantages

## Confidence
- **Geometric fidelity through vector conditioning:** Medium confidence (qualitative evidence supports claim, but quantitative metrics are absent)
- **DDIM-based before-after consistency:** Medium confidence (visually consistent edits demonstrated, but instability at high CFG scales reported)
- **Spatiotemporal conditioning effectiveness:** Low confidence (qualitative examples shown, but no quantitative analysis of effectiveness or generalization)

## Next Checks
1. **Quantitative geometric accuracy test:** Generate 100 random images from OSM masks. Compute Intersection-over-Union (IoU) between generated road networks and OSM ground truth, and between generated building footprints and OSM geometries. Compare against a raster-conditioned baseline.

2. **DDIM consistency measurement:** Generate before-after pairs by editing specific OSM features (add/remove buildings, change road types). Calculate mean absolute pixel difference in unedited regions across 50 pairs. Test sensitivity to CFG scale and inversion depth parameters.

3. **Geographic generalization assessment:** Hold out all samples from a specific country/continent not well-represented in FMoW training data. Generate images for this region and evaluate visual quality and structural accuracy compared to in-distribution samples.