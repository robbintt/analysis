---
ver: rpa2
title: A Survey of Continual Reinforcement Learning
arxiv_id: '2506.21872'
source_url: https://arxiv.org/abs/2506.21872
tags:
- learning
- tasks
- task
- reinforcement
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of Continual Reinforcement
  Learning (CRL), a field addressing the limitations of traditional Reinforcement
  Learning (RL) in dynamic, multi-task environments. CRL focuses on enabling agents
  to learn continuously, adapt to new tasks, and retain previously acquired knowledge.
---

# A Survey of Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.21872
- Source URL: https://arxiv.org/abs/2506.21872
- Reference count: 40
- Primary result: Comprehensive survey of CRL methods categorizing them into policy-focused, experience-focused, dynamic-focused, and reward-focused approaches

## Executive Summary
This paper presents a comprehensive survey of Continual Reinforcement Learning (CRL), addressing the limitations of traditional Reinforcement Learning in dynamic, multi-task environments. CRL focuses on enabling agents to learn continuously, adapt to new tasks, and retain previously acquired knowledge. The survey identifies key challenges including the stability-plasticity tradeoff and proposes a novel taxonomy of CRL methods based on the type of knowledge stored and/or transferred. The paper reviews current approaches, discusses open challenges, and highlights future research directions including task-free CRL, evaluation standardization, interpretable knowledge, and integration of pre-trained large models.

## Method Summary
The survey synthesizes existing CRL literature by proposing a four-category taxonomy: policy-focused (parameter regularization/distillation), experience-focused (replay buffers/generative models), dynamic-focused (learned transition models), and reward-focused (reward shaping/intrinsic rewards). The evaluation framework tracks average performance across tasks, forgetting of old tasks, and forward transfer to new tasks. The authors analyze how different methods address the triangular balance among plasticity, stability, and scalability, providing a structured overview of current approaches and their mechanisms for knowledge preservation and transfer.

## Key Results
- CRL methods work by categorizing knowledge storage and transfer into four complementary types to systematically mitigate forgetting
- Experience replay combined with distillation provides robust forgetting mitigation by directly re-exposing the policy to past distributions
- Modular and hierarchical policy architectures enable scalability by isolating task-specific knowledge into reusable components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CRL methods work by categorizing knowledge storage and transfer into four complementary types, allowing systematic mitigation of forgetting while enabling forward/backward transfer
- Mechanism: The taxonomy partitions approaches by what knowledge is preserved—policy parameters, experience tuples, environment dynamics, or reward signals—and how it flows between tasks. This decomposition enables targeted interventions: regularization constrains parameter drift (policy-focused), replay buffers re-expose old transitions (experience-focused), learned transition models enable planning without re-exploration (dynamic-focused), and shaped/intrinsic rewards accelerate learning on new tasks (reward-focused)
- Core assumption: Tasks share latent structure such that knowledge extracted from one task generalizes to others; forgetting is caused by parameter overwriting or distribution shift, not inherent task incompatibility
- Evidence anchors: [abstract] "categorizing them into policy-focused, experience-focused, dynamic-focused, and reward-focused approaches based on the type of knowledge stored and/or transferred"; [section III-B] "achieving a triangular balance among three key aspects: plasticity, stability, and scalability"
- Break condition: If tasks are adversarially designed with orthogonal state-action-reward structures, transfer-based gains collapse and the taxonomy offers no theoretical advantage over independent learning

### Mechanism 2
- Claim: Experience replay combined with distillation provides the most robust forgetting mitigation because it directly re-exposes the policy to past distributions rather than relying on proxy constraints
- Mechanism: Direct replay stores representative transitions in a long-term buffer; during new-task training, sampled old transitions are interleaved, anchoring the policy to prior optima. Generative replay approximates this by training VAEs/GANs to synthesize pseudo-experiences, trading memory for model capacity. Distillation further compresses multiple teacher policies into a single student, regularizing via KL divergence against old action distributions
- Core assumption: The replay buffer or generative model captures sufficient state-coverage to prevent distributional drift; distillation loss preserves decision boundaries without requiring task identity at test time
- Evidence anchors: [section IV-C] "Selective experience replay introduces strategies to prioritize the storage of experiences in a long-term memory buffer based on their importance"; [section IV-B3] "Progress & Compress integrates a knowledge base and an active column... the active column's policy is distilled into the knowledge base using a cross-entropy loss"
- Break condition: When experience diversity is low (sparse-reward tasks with narrow success trajectories), replay can reinforce suboptimal behaviors; generative models may suffer mode collapse, producing unrepresentative pseudo-experiences

### Mechanism 3
- Claim: Modular and hierarchical policy architectures enable scalability by isolating task-specific knowledge into reusable components rather than monolithic parameter sharing
- Mechanism: Factor decomposition (e.g., PG-ELLA) represents policy parameters as θₖ = Lsₖ where L is a shared latent basis and sₖ are task-specific coefficients. Multi-head networks attach task-specific output layers to a shared backbone. Modular networks (e.g., CompoNet, SANE) maintain ensembles of small networks activated via attention or uncertainty-based gating. Hierarchical methods decompose skills into high-level controllers and low-level primitives
- Core assumption: Tasks decompose into shared sub-skills or feature representations; gating/attention mechanisms correctly route inputs to appropriate modules without explicit task identity
- Evidence anchors: [section IV-B2] "PG-ELLA introduces a latent basis representation to model each task's parameters as a linear combination of components from a shared knowledge base"; [section IV-B2] "Self-Activating Neural Ensembles... activates, merges, and creates modules automatically" with activation score uᵢ(s) = |Gₜ − Vᵢ(s)|
- Break condition: When the number of tasks exceeds module capacity or tasks require fundamentally incompatible features, gating becomes unreliable and module interference re-emerges

## Foundational Learning

- Concept: **Markov Decision Process (MDP) formulation**
  - Why needed here: CRL inherits RL's MDP structure ⟨S, A, R, γ, T, ρ₀, H⟩ but allows R and T to vary over time. Understanding baseline value functions and policy gradients is prerequisite to grasping how they're modified across task sequences
  - Quick check question: Can you derive the Bellman optimality equation and explain how Q-learning updates approximate it?

- Concept: **Stability-Plasticity Dilemma**
  - Why needed here: This is the core tension in continual learning—retaining old knowledge (stability) while adapting to new tasks (plasticity). All CRL methods implicitly or explicitly negotiate this tradeoff
  - Quick check question: If a network's loss surface for task A has converged to a minimum, what happens to that loss when gradients from task B flow through the same parameters?

- Concept: **Experience Replay Buffers**
  - Why needed here: Off-policy RL (DQN, SAC, TD3) already uses replay buffers for sample efficiency; CRL extends this with long-term/cross-task buffers, reservoir sampling, and generative approximations
  - Quick check question: Why does i.i.d. sampling from a replay buffer improve training stability compared to on-policy trajectory updates?

## Architecture Onboarding

- Component map: Environment Sequence -> [Task Detector] -> [Knowledge Store] -> [Policy Network] -> Action Output; Policy Network <-> [Experience Buffer] <-> [Dynamics Model]; Policy Network <-> [Reward Shaper]

- Critical path:
  1. Define scenario: Task-incremental (known boundaries), non-stationary (gradual drift), or task-agnostic (unknown boundaries)
  2. Select knowledge type: Policy-focused if actions transfer; experience-focused if state distributions shift; dynamic-focused if transition functions vary; reward-focused if reward structures are sparse/adversarial
  3. Choose forgetting mitigation: Regularization (EWC, online-EWC) for low-memory; replay for sample-rich domains; modularity for long task sequences
  4. Implement evaluation: Track average performance Aₙ, forgetting FG, and forward transfer FT across all tasks

- Design tradeoffs:
  - Replay vs. Regularization: Replay is more effective but memory-scaling is linear; regularization is memory-constant but constrains plasticity
  - Multi-head vs. Monolithic: Multi-head prevents interference but requires task identity at inference; monolithic supports task-agnostic settings but suffers more forgetting
  - Direct vs. Generative Replay: Direct is reliable but memory-bound; generative scales but risks distribution drift from model inaccuracies

- Failure signatures:
  - Forgetting spikes after task N → Insufficient replay ratio or regularization strength too low
  - Forward transfer negative → Shared backbone overfits to early tasks; consider resetting task-specific heads or using orthogonal initialization
  - Sample efficiency degrades across tasks → Replay buffer dominated by early-task experiences; implement reservoir sampling or priority-based selection
  - Task detection false positives → Latent space clustering unstable; increase encoder capacity or use ensembles

- First 3 experiments:
  1. **Baseline sanity check**: Run vanilla SAC/PPO on a 5-task sequence (e.g., Continual World or MiniGrid variants) without any CL modifications. Measure AN, FG, FT. This establishes the forgetting magnitude you need to overcome
  2. **Ablate replay ratios**: Implement a dual-buffer system (short-term + long-term) and sweep replay ratio λ ∈ {0.1, 0.3, 0.5, 0.7}. Plot forgetting vs. forward transfer to find the Pareto frontier
  3. **Compare knowledge types**: Implement one method from each taxonomy category (e.g., EWC for policy, CLEAR for experience, HyperCRL for dynamics, SR-LLRL for reward) on the same benchmark. Report which knowledge type best matches the benchmark's task structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CRL agents be designed to learn and adapt in non-stationary environments without receiving explicit task identities or boundary signals?
- Basis in paper: [explicit] Section V-A states that "Task-free CRL is a challenging problem" and is the "necessary path for RL to move towards general AI," as most current methods assume clear task boundaries
- Why unresolved: The paper notes that environments may change continuously, but most existing methods rely on explicit supervision for task switching
- What evidence would resolve it: The development of algorithms capable of detecting environmental drift or learning continuously online without performance collapse in boundary-free benchmarks

### Open Question 2
- Question: What evaluation metrics can effectively quantify scalability and privacy in CRL, distinct from those used in supervised continual learning?
- Basis in paper: [explicit] Section V-B highlights that "most metrics are drawn from the supervised CL field" and lack standardization for measuring resource constraints and security
- Why unresolved: Current metrics focus on accuracy and forgetting but fail to address the computational and memory constraints critical for real-world RL deployment
- What evidence would resolve it: A standardized benchmark suite that tracks memory usage, model size growth, and data privacy alongside average performance and forgetting scores

### Open Question 3
- Question: How can CRL methods transition from black-box knowledge storage to interpretable representations to ensure safety in high-stakes tasks?
- Basis in paper: [explicit] Section V-C argues that building an "interpretable knowledge base" is essential to "alleviate catastrophic decision-making" in domains like autonomous driving
- Why unresolved: Black-box representations currently dominate the field because they are more accessible, whereas interpretable knowledge remains difficult to articulate and transfer
- What evidence would resolve it: Frameworks utilizing neural-symbolic reasoning or policy distillation that allow humans to audit and verify the logic behind retained skills

### Open Question 4
- Question: How can large-scale pre-trained models (PTMs) be integrated into CRL frameworks to facilitate multi-granularity knowledge transfer?
- Basis in paper: [explicit] Section V-D identifies "PTMs for CRL" as a key future direction, citing the potential to use PTMs as comprehensive knowledge bases for sample-efficient learning
- Why unresolved: While initial works like MT-Core exist, the paper notes that research is needed to explore how to effectively transfer this "coarse-grained knowledge" to fine-grained RL policies
- What evidence would resolve it: Methods that successfully leverage frozen or fine-tuned LLMs to provide hierarchical guidance or intrinsic rewards across diverse, unseen task sequences

## Limitations
- The survey's taxonomy and claims are primarily derived from synthesizing existing literature rather than presenting novel empirical results
- The effectiveness of each CRL approach depends heavily on task similarity and environmental structure, which the survey acknowledges but does not quantify across benchmarks
- Claims about scalability without memory growth are largely theoretical; few methods demonstrate sublinear memory scaling in practice

## Confidence
- **High confidence**: The identification of stability-plasticity tradeoff as central challenge (well-established in CL literature)
- **Medium confidence**: The four-category taxonomy is logically coherent but lacks empirical validation of its completeness or superiority over alternative categorizations
- **Low confidence**: Claims about scalability without memory growth are largely theoretical; few methods demonstrate sublinear memory scaling in practice

## Next Checks
1. Implement a 5-task sequence using one method from each taxonomy category on the same benchmark to empirically test whether the categorization captures meaningful performance differences
2. Measure memory usage across CRL methods as task count scales to verify scalability claims against theoretical predictions
3. Conduct ablation studies removing each knowledge type (policy, experience, dynamics, reward) to quantify their individual contributions to forward/backward transfer