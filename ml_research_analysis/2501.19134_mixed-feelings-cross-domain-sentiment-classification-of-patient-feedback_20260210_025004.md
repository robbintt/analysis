---
ver: rpa2
title: 'Mixed Feelings: Cross-Domain Sentiment Classification of Patient Feedback'
arxiv_id: '2501.19134'
source_url: https://arxiv.org/abs/2501.19134
tags:
- norpac
- norec
- training
- data
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates cross-domain sentiment classification of
  Norwegian patient feedback using free-text comments from healthcare surveys. The
  authors annotate the data with four polarity classes (positive, negative, mixed,
  neutral) and compare in-domain and out-of-domain performance using both general-domain
  review data and healthcare-specific sources.
---

# Mixed Feelings: Cross-Domain Sentiment Classification of Patient Feedback

## Quick Facts
- arXiv ID: 2501.19134
- Source URL: https://arxiv.org/abs/2501.19134
- Reference count: 4
- Primary result: Cross-domain sentiment classification of Norwegian patient feedback achieves over 85% weighted F1 using transformer models, with in-domain data outperforming cross-domain approaches

## Executive Summary
This study investigates cross-domain sentiment classification of Norwegian patient feedback from healthcare surveys. The authors annotate free-text comments with four polarity classes (positive, negative, mixed, neutral) and evaluate model performance across different healthcare domains. Using transformer-based models including NorBERT3 and NorT5, along with an SVM baseline, they demonstrate that in-domain training data significantly outperforms cross-domain approaches. The research reveals that general practitioner feedback transfers better to psychiatric healthcare than psychiatric data itself, and that cross-domain training with review data only benefits performance when in-domain data is limited.

## Method Summary
The study employs transformer-based models (NorBERT3 Base/Large, NorT5 Large) and an SVM baseline for sentiment classification. Data consists of Norwegian patient feedback from healthcare surveys, annotated with four polarity classes. Experiments compare in-domain training (using same-domain data) against out-of-domain training (using general review data from NoReC). The evaluation measures weighted F1 score across different training set sizes and domain combinations, with particular attention to how domain-specific characteristics affect model performance.

## Key Results
- Neural models achieve over 85% weighted F1 score on sentiment classification tasks
- In-domain training on general practitioner feedback outperforms psychiatric healthcare training on the same domain
- Cross-domain training with review data improves performance when in-domain data is limited (6.25%-50% of full set), but provides no benefit when sufficient in-domain data exists (100%)
- Negative sentiment classification is particularly challenging, with significant misclassification to neutral class when training distribution mismatches target domain

## Why This Works (Mechanism)
The study demonstrates that domain-specific training data significantly impacts sentiment classification performance. When training data characteristics closely match the target domain (both linguistically and in terms of sentiment distribution), models achieve better generalization. The four-class polarity scheme captures nuanced patient feedback, though this complexity introduces additional classification challenges. Cross-domain benefits appear primarily when in-domain data is scarce, suggesting that diverse training sources can compensate for limited domain-specific examples up to a critical threshold.

## Foundational Learning
- **Four-class sentiment classification**: Positive, negative, mixed, and neutral categories provide nuanced understanding of patient feedback, though introduce complexity beyond binary classification
- **Cross-domain transfer learning**: Applying models trained on one domain to another domain reveals domain-specific linguistic patterns and sentiment expression differences
- **Domain adaptation thresholds**: There exists a critical point where adding out-of-domain data transitions from helpful to harmful, dependent on in-domain data availability
- **Class distribution impact**: Mismatched class distributions between training and target domains can systematically bias model predictions toward over-represented classes
- **Healthcare domain specificity**: Different medical specialties exhibit distinct linguistic patterns and sentiment expression styles that affect classification performance
- **Norwegian language modeling**: Fine-tuning transformers on Norwegian text requires understanding of language-specific morphology and syntax patterns

## Architecture Onboarding
Component map: Text preprocessing -> Tokenization -> Transformer model (NorBERT3/NorT5) -> Classification layer -> Weighted F1 evaluation

Critical path: Raw patient feedback → Text preprocessing (tokenization, normalization) → Transformer encoding → Sentiment classification (4 classes) → Performance evaluation (weighted F1)

Design tradeoffs: The four-class scheme captures nuanced feedback but increases classification complexity compared to binary approaches. Using Norwegian-specific transformers (NorBERT3, NorT5) provides language-specific advantages but limits cross-linguistic comparison.

Failure signatures: Negative sentiment misclassified as neutral when training data has over-represented neutral class. Poor cross-domain performance when linguistic patterns differ significantly between source and target domains.

First experiments: 1) Train baseline SVM on limited in-domain data 2) Fine-tune NorBERT3 Base on full in-domain dataset 3) Train NorT5 Large on cross-domain review data and evaluate on healthcare feedback

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the "critical threshold" of in-domain data size, beyond which adding out-of-domain data harms performance, be predicted based on measurable dataset characteristics?
- Basis: [inferred] Figure 1 shows that adding NoReC data helps when NorPaC is small (6.25%-50%) but hurts performance when the full in-domain set is used (100%).
- Why unresolved: The paper empirically observes the existence of this crossover point but does not provide a method to predict where this threshold lies for new datasets.
- What evidence would resolve it: A series of experiments varying domain similarity and data size to derive a predictive function for the crossover point.

### Open Question 2
- Question: To what extent does the class distribution mismatch—specifically the over-representation of the "neutral" class in the source domain—drive the misclassification of "negative" sentiment in the target domain?
- Basis: [inferred] Page 5 notes that the NoReC-trained model misclassified 59% of negative NorPaC samples as neutral, correlating with the fact that 47% of NoReC is neutral compared to 12% in NorPaC.
- Why unresolved: The paper identifies the correlation between the training distribution and the error type but does not experimentally isolate class imbalance as the causal factor.
- What evidence would resolve it: Experiments comparing standard training against class-balanced or re-weighted training on the source domain to see if negative recall improves.

### Open Question 3
- Question: Does the superior transferability of General Practitioner (GP) feedback to Psychiatric Healthcare (SMH) hold when controlling for the differences in annotation guidelines or patient demographics?
- Basis: [inferred] Page 4 notes that training on GP data yields better results on SMH test sets than training on SMH data itself, hypothesizing that GP text is more "explicit" while SMH is "noisy."
- Why unresolved: The authors speculate on the linguistic causes (noise vs. explicitness) but do not verify if the annotation process or subject matter complexity is the primary driver of the SMH training instability.
- What evidence would resolve it: An ablation study analyzing model performance on SMH subsets stratified by linguistic complexity or "noisiness."

## Limitations
- The analysis of domain mismatch causes remains qualitative rather than systematic, limiting understanding of why certain domains transfer better than others
- The four-class polarity scheme complexity may affect comparability with binary classification approaches, and mixed sentiment handling could be further explored
- Limited examination of domain shift types and detailed error analysis for negative sentiment classification challenges

## Confidence
- High confidence: Overall finding that in-domain training data improves performance across all models tested
- Medium confidence: Specific performance differences between general practitioner and psychiatric healthcare domains, as these results depend on the particular datasets used
- Medium confidence: Claim that cross-domain training provides no benefit when sufficient in-domain data exists, though this conclusion is based on a limited number of domain pairs

## Next Checks
1. Conduct systematic error analysis focusing on negative sentiment classification failures to identify specific linguistic or domain features that cause misclassification
2. Test the cross-domain generalization claim with additional healthcare domains beyond the two studied to verify the pattern holds across different medical specialties
3. Compare four-class classification performance against binary classification to quantify the trade-offs introduced by the mixed sentiment category and validate the annotation scheme's reliability