---
ver: rpa2
title: Exploring the features used for summary evaluation by Human and GPT
arxiv_id: '2512.19620'
source_url: https://arxiv.org/abs/2512.19620
tags:
- human
- evaluation
- arxiv
- features
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the underlying features used by GPT models
  when evaluating text summaries for relevance and coherence. The authors analyze
  statistical and machine learning metrics, including readability indices, information-theoretic
  measures, and embedding-based similarity scores, to identify features that correlate
  with both human and GPT judgments.
---

# Exploring the features used for summary evaluation by Human and GPT

## Quick Facts
- **arXiv ID**: 2512.19620
- **Source URL**: https://arxiv.org/abs/2512.19620
- **Reference count**: 27
- **Primary result**: Investigates which features GPT models use when evaluating summary relevance and coherence

## Executive Summary
This paper examines the underlying features that influence GPT models when evaluating text summaries for relevance and coherence. The authors analyze various statistical and machine learning metrics including readability indices, information-theoretic measures, and embedding-based similarity scores to identify which features correlate with both human and GPT judgments. They introduce a conditional perplexity metric that incorporates source text context, finding it highly aligned with human scoring. The research reveals that GPT evaluations are strongly influenced by uncertainty-based features such as entropy, perplexity, and diversity measures, alongside text complexity metrics.

## Method Summary
The authors conducted a comprehensive analysis of statistical and machine learning metrics to identify features correlating with human and GPT summary evaluation judgments. They evaluated multiple dimensions including readability indices, information-theoretic measures (entropy, perplexity), embedding-based similarity scores, and text complexity metrics. A key contribution was the introduction of conditional perplexity that incorporates source text context. The study compared correlation patterns between human evaluation scores and GPT model outputs across various feature sets, systematically analyzing which metrics showed the strongest alignment with human judgments.

## Key Results
- GPT evaluations are strongly influenced by uncertainty-based features including entropy, perplexity, and diversity (Gini index)
- Text complexity measures show significant correlation with GPT evaluation scores
- Conditional perplexity incorporating source context demonstrates high alignment with human scoring
- Prompting GPTs with human-relevant metrics improves their alignment with human judgments

## Why This Works (Mechanism)
GPT models evaluate summaries by leveraging internal representations that capture uncertainty, complexity, and contextual relevance. The mechanism involves processing input summaries through transformer architectures that compute probability distributions over token sequences, from which metrics like perplexity and entropy can be derived. These uncertainty measures reflect the model's confidence in its predictions, with lower confidence (higher entropy/perplexity) potentially indicating less coherent or relevant summaries. The conditional perplexity approach works by explicitly incorporating source document context into the evaluation, allowing the model to assess how well the summary captures and represents the original content. This context-aware evaluation aligns more closely with human judgment because humans naturally consider source-document relationships when evaluating summary quality.

## Foundational Learning
- **Perplexity and entropy metrics**: Essential for quantifying model uncertainty and prediction confidence; quick check: verify that lower perplexity correlates with higher human-rated quality
- **Conditional probability modeling**: Understanding how source context influences summary evaluation; quick check: compare conditional vs unconditional perplexity correlation with human scores
- **Embedding similarity measures**: Used to assess semantic alignment between summaries and source texts; quick check: validate that higher cosine similarity correlates with better evaluations
- **Readability indices**: Quantify text complexity and accessibility; quick check: confirm that extreme readability scores (too simple or too complex) correlate with lower evaluation scores
- **Information-theoretic diversity measures**: Gini index and similar metrics capture content variety and redundancy; quick check: test whether optimal diversity scores maximize evaluation scores
- **Prompt engineering effects**: How specific prompts influence model behavior and output; quick check: compare evaluation consistency across different prompt formulations

## Architecture Onboarding

**Component Map**: Source Text -> Summary Generation -> Evaluation Metrics -> GPT Scoring -> Human Evaluation -> Correlation Analysis

**Critical Path**: The evaluation pipeline follows Source Text → Summary Generation → GPT Scoring → Correlation with Human Evaluation. The most critical components are the evaluation metrics calculation and the GPT scoring mechanism, as these directly determine the features analyzed for correlation with human judgments.

**Design Tradeoffs**: The study prioritizes comprehensiveness of feature analysis over computational efficiency, examining numerous metrics rather than focusing on a smaller, more efficient set. There's a tradeoff between using simple interpretable metrics (readability indices) versus complex but potentially more accurate neural metrics (embedding similarities). The choice to use conditional perplexity adds computational overhead but provides more contextually relevant evaluation.

**Failure Signatures**: Poor correlation between GPT and human scores may indicate that the evaluation features don't capture the aspects humans consider important, or that the GPT model's internal representations differ significantly from human judgment patterns. Low variance in evaluation scores across different metrics might suggest the features are not discriminative enough. High computational costs for conditional perplexity could limit scalability to larger datasets.

**First Experiments**: 
1. Replicate correlation analysis using a held-out test set to verify stability of results
2. Conduct ablation studies removing individual feature categories to identify which contribute most to GPT evaluation patterns
3. Test the impact of different prompt formulations on evaluation consistency and alignment with human judgments

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on relevance and coherence metrics, potentially missing other important dimensions like factual accuracy
- Results may not generalize across different domains, languages, or summary types due to dataset specificity
- Correlation analysis cannot establish causal relationships between features and GPT evaluation decisions

## Confidence
- **High confidence**: GPT evaluations correlate with uncertainty-based features (entropy, perplexity, Gini index) and text complexity measures
- **Medium confidence**: Conditional perplexity incorporating source context improves alignment with human judgments
- **Low confidence**: Prompting GPTs with human-relevant metrics improves evaluation consistency

## Next Checks
1. Replicate the analysis across multiple languages and domains to test generalizability of the identified features
2. Conduct ablation studies to isolate the causal impact of individual features on GPT evaluation scores
3. Compare GPT evaluation patterns with those of other LLM architectures to identify whether these findings are model-specific or more universal