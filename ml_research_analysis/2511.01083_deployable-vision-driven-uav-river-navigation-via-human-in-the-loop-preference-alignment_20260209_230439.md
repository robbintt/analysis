---
ver: rpa2
title: Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference
  Alignment
arxiv_id: '2511.01083'
source_url: https://arxiv.org/abs/2511.01083
tags:
- policy
- learning
- reward
- preference
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying vision-driven UAVs
  for river navigation in dynamic environments where simulation-trained policies fail
  due to distribution shift and safety risks. The proposed method, SPAR-H, introduces
  a hybrid human-in-the-loop learning framework that combines direct preference optimization
  on policy logits with a reward-based pathway that trains an immediate-reward estimator
  from the same statewise preferences.
---

# Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment

## Quick Facts
- **arXiv ID:** 2511.01083
- **Source URL:** https://arxiv.org/abs/2511.01083
- **Reference count:** 37
- **Primary result:** SPAR-H achieves highest episodic reward and lowest variance among tested methods for UAV river navigation under limited human feedback.

## Executive Summary
This paper tackles the challenge of deploying vision-driven UAVs for river navigation in dynamic environments where simulation-trained policies fail due to distribution shift and safety risks. The proposed SPAR-H framework introduces a hybrid human-in-the-loop learning approach that combines direct preference optimization on policy logits with a reward-based pathway that trains an immediate-reward estimator from the same statewise preferences. The method achieves superior performance with limited human interventions, demonstrating real-world feasibility of continual preference alignment for UAV river following.

## Method Summary
SPAR-H is a hybrid human-in-the-loop learning framework that processes 128×128 RGB drone views through SAM2 segmentation to generate 16×16 binary water mask grids. A shared GRU encoder (frozen during HITL retraining) maintains visitation history and feeds two heads: a policy head for action selection and a reward head for immediate reward estimation. The hybrid objective combines direct Bradley-Terry losses on policy logits at intervention points with FOCOPS trust-region updates using predicted rewards on non-intervened steps. Human interventions provide statewise preference pairs that drive both pathways simultaneously, enabling both precise local correction and broader policy adaptation under limited feedback budgets.

## Key Results
- SPAR-H achieves the highest episodic reward among tested methods in simulation benchmarks
- SPAR-H shows the lowest variance across initial conditions, indicating stable learning
- The learned reward model aligns with human-preferred actions and elevates nearby non-intervened choices
- The approach demonstrates real-world feasibility with only five HITL rollouts reflecting typical field constraints

## Why This Works (Mechanism)

### Mechanism 1
The hybrid approach of combining direct preference optimization with reward-based RL propagation enables both precise local correction and broader generalization under limited feedback budgets. By simultaneously optimizing both pathways under natural masks (intervened vs. non-intervened states), SPAR-H provides "surgical" alignment through the direct path while propagating learned preferences to nearby states through the reward model. This dual-pathway design assumes consistent preference signals without conflicting gradients dominating.

### Mechanism 2
Statewise preference extraction at intervention points avoids temporal credit assignment issues inherent in trajectory-level feedback under partial observability. Rather than assigning credit across long trajectories where single images lack global context, SPAR-H extracts preferences exactly when human override occurs, using the recurrent latent state to encode visitation history. This enables the reward estimator to approximate marginal coverage gains without requiring the policy to infer global state from a single image.

### Mechanism 3
Trust-region policy updates with per-state KL gates stabilize online adaptation from sparse off-policy corrections. The FOCOPS algorithm computes importance ratios between current and reference policies, applying a hard KL divergence gate that blocks updates when policy changes would be too large. This prevents large gradient steps from destabilizing the policy when the learned reward model is imperfect or when interventions are sparse.

## Foundational Learning

- **Bradley-Terry Preference Model:** Core mathematical framework for converting pairwise action comparisons into differentiable losses for both policy and reward heads. *Quick check:* Can you derive the BT negative log-likelihood loss and explain why it encourages larger score gaps between preferred and dispreferred actions?

- **Trust-Region Policy Optimization (FOCOPS):** Enables stable off-policy updates when learning from sparse human interventions without value-function bootstrapping. *Quick check:* How does the per-state KL gate differ from global KL constraints, and what failure mode does it prevent?

- **Submodular Reward Functions:** River navigation is a coverage problem with diminishing returns; understanding marginal gain is essential for interpreting the immediate-reward estimator's targets. *Quick check:* Why does a submodular reward make per-step credit assignment difficult under partial observability?

## Architecture Onboarding

- **Component map:** RGB image → SAM2 segmentation → 16×16 binary water mask → patchified input → Shared GRU encoder (frozen during HITL) → Latent z_t → Policy head + Reward head → Action distribution + Immediate reward estimate

- **Critical path:** Human override → preference pair extraction → parallel BT losses on policy logits and reward head → FOCOPS update on non-intervened steps with reward-to-go advantages

- **Design tradeoffs:** Freezing GRU during retraining prevents cross-head interference but limits representation adaptation; direct path provides precise local correction but limited propagation while indirect path propagates but depends on reward model fidelity; mixing weight α balances local vs. propagated updates

- **Failure signatures:** Reward model fails to generalize (elevated rewards at intervened states but flat or reversed rankings at nearby states); KL gate too restrictive (minimal logit changes despite interventions); preference conflicts (loss curves plateau early with high variance)

- **First 3 experiments:**
  1. Reproduce simulation benchmark with SPAR-H vs. SPAR-P vs. SPAR-R vs. IWR on 5 HITL rollouts; verify episodic reward and variance patterns match Figure 5.
  2. Ablate mixing weight α ∈ {0, 0.5, 1, 2} to characterize tradeoff between local correction precision and propagated improvement stability.
  3. Visualize learned reward model predictions on held-out trajectories; confirm correlation with human preference regions and check for spurious elevation of non-preferred actions.

## Open Questions the Paper Calls Out

- How does SPAR-H performance vary with the mixing weight α and trust-region thresholds (η, λ), and can adaptive tuning of these hyperparameters improve convergence? The paper uses fixed values (α=1, η=0.05, λ=1.5) without ablation; optimal settings may depend on intervention density or environment dynamics.

- Can active preference querying—selectively soliciting human comparisons at uncertain or high-information states—reduce the number of interventions needed to reach a target performance level? The current conservative overseer intervenes only on safety/efficiency violations; query strategies that proactively request comparisons could extract more signal per rollout.

- Does incorporating epistemic uncertainty into the preference model improve robustness to perceptual noise (e.g., water-mask segmentation errors, reflections)? The Bradley-Terry model assumes fixed preferences, but perceptual ambiguities could make human preferences noisy or inconsistent.

- How does SPAR-H's advantage over IL and evaluative RL baselines scale with increasing feedback budgets beyond five rollouts? The study uses only five HITL rollouts "reflecting typical field constraints"; performance scaling at higher budgets remains untested.

## Limitations

- The paper lacks ablation studies on the mixing weight α, leaving the tradeoff between direct and indirect pathways unquantified
- Key hyperparameters (optimizer specifics, learning rates, batch sizes, hidden dimensions) are not specified, limiting faithful reproduction
- The frozen GRU assumption is not experimentally verified for sufficient representation capacity
- Performance is evaluated only up to five HITL rollouts, leaving scaling behavior with larger feedback budgets unknown

## Confidence

- **High confidence:** Episodic reward gains in simulation benchmarks (directly measured performance)
- **Medium confidence:** Human-preferred action alignment (reported but not cross-validated across operators)
- **Low confidence:** Broader policy adaptation claims (without ablations or out-of-distribution tests)

## Next Checks

1. Run an ablation study on the mixing weight α to quantify the tradeoff between direct local correction precision and indirect reward-propagated generalization.
2. Evaluate the learned reward model on held-out trajectories, measuring reward prediction correlation with human preference regions and detecting any spurious elevations of non-preferred actions.
3. Test policy robustness by introducing initial-condition variations beyond the three levels reported, verifying that variance trends hold under expanded distribution shifts.