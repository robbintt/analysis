---
ver: rpa2
title: 'LLMs on Drugs: Language Models Are Few-Shot Consumers'
arxiv_id: '2512.18546'
source_url: https://arxiv.org/abs/2512.18546
tags:
- arxiv
- language
- prompt
- preprint
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "A controlled experiment tested how persona prompts (\u201Con LSD\u201D\
  , \u201Con cocaine\u201D, \u201Con alcohol\u201D, \u201Con cannabis\u201D) affect\
  \ LLM accuracy on 100 ARC-Challenge items. With a sober control at 45 % accuracy,\
  \ alcohol collapsed performance to 10 % (p = 3.2e-8), cocaine to 21 % (p = 4.9e-4),\
  \ LSD to 19 % (p = 1.3e-4), and cannabis to 30 % (p = 0.041)."
---

# LLMs on Drugs: Language Models Are Few-Shot Consumers

## Quick Facts
- arXiv ID: 2512.18546
- Source URL: https://arxiv.org/abs/2512.18546
- Reference count: 34
- Primary result: Alcohol persona collapsed LLM accuracy to 10% on ARC-Challenge (p=3.2e-8) vs 45% sober control

## Executive Summary
This paper demonstrates that persona prompts simulating psychoactive states ("on LSD", "on cocaine", "on alcohol", "on cannabis") can dramatically reduce LLM accuracy on reasoning tasks without modifying model weights. The effect is primarily driven by format compliance failures - the model breaks the required "Answer: <LETTER>" template while still performing reasoning. Alcohol caused the most severe collapse (10% accuracy), followed by cocaine (21%), LSD (19%), and cannabis (30%). The results suggest persona text acts as a consumable prompt "drug" that can destroy reliability, raising safety concerns for applications using few-shot prompting.

## Method Summary
The study tested 100 ARC-Challenge science questions across five conditions: sober control and four persona prompts simulating alcohol, cocaine, LSD, and cannabis states. GPT-5-mini (substitutable with GPT-4o-mini) was used with temperature=0 and a 300-token cap via OpenAI's API. Each condition included a system instruction enforcing "Answer: <LETTER>" format. Results were logged and analyzed using Wilson confidence intervals and Fisher exact tests to compare each persona condition against the sober control. The main metric was format compliance - responses missing the required letter format were counted as incorrect even if reasoning appeared sound.

## Key Results
- Alcohol persona reduced accuracy to 10% (p=3.2e-8) vs 45% sober control
- Cocaine persona reduced accuracy to 21% (p=4.9e-4)
- LSD persona reduced accuracy to 19% (p=1.3e-4)
- Cannabis persona reduced accuracy to 30% (p=0.041)
- Failures were primarily due to breaking the "Answer: <LETTER>" format requirement

## Why This Works (Mechanism)

### Mechanism 1: Format Compliance Override via Persona Context
Persona prompts can override instruction-tuned formatting constraints without modifying model weights. The model treats persona framing as a higher-priority context signal than explicit output format instructions. When persona cues imply looseness (alcohol) or expansiveness (LSD), the model deprioritizes the "Answer: <LETTER>" contract embedded in system instructions. The attention mechanism appears to allocate more weight to persona-adjacent tokens than to format constraints during generation.

### Mechanism 2: Token Budget Reallocation
Persona prompts shift how the model allocates its generation budget across reasoning vs. format compliance. Cannabis and LSD personas produced longer responses (median 268 characters for cannabis) with more metaphorical language, consuming tokens that would otherwise complete the structured output. The 300-token cap constrains completion differently depending on persona-induced verbosity patterns.

### Mechanism 3: Alignment Tax from Conflicting Objectives
Persona prompts introduce competing objectives that degrade instruction-following reliability. The model simultaneously attempts to satisfy the persona role-play and the formatting instruction. When these conflict (e.g., "loose, conversational" persona vs. rigid format), the persona wins. Role-playing objectives learned during training can supersede explicit instruction-following behaviors.

## Foundational Learning

- **Prompt Sensitivity and Order Effects**: Small prompt changes can cause large behavioral shifts in LLMs. Why needed: The entire experiment rests on understanding that persona context dramatically affects output format compliance. Quick check: Can you explain why reordering few-shot examples changes accuracy?

- **Wilson Confidence Intervals for Proportions**: Used for small-sample Bernoulli settings where standard intervals would be misleading. Why needed: The paper uses Wilson intervals specifically for 100 trials per condition. Quick check: Why is a Wilson interval preferred over a normal approximation for n=100 accuracy measurements?

- **Fisher Exact Test vs. Chi-Square**: Fisher exact tests avoid asymptotic approximations with small samples. Why needed: The paper chooses Fisher exact tests for comparing proportions across 100 trials per condition. Quick check: When would chi-square be inappropriate for comparing two proportions?

## Architecture Onboarding

- **Component map**: src/run_benchmark.py -> OpenAI Responses API -> results/raw/ -> src/analyze_results.py -> src/make_figures.py

- **Critical path**: Shuffle ARC-Challenge items (seed 13) → sample 100 per condition → prepend system instruction + persona prefix → call API with deterministic decoding → parse response for "Answer: X" or first letter → record missing predictions as incorrect → compute per-condition accuracy with Wilson CIs → run Fisher exact tests vs. control

- **Design tradeoffs**: 100 items/condition bounds API costs but widens confidence intervals; sequential condition ordering avoids transport-layer randomness but prevents within-run randomization; missing-letter = incorrect simplifies evaluation but may undercount partially-correct reasoning

- **Failure signatures**: 60%+ missing predictions (alcohol: 60/100) indicates format collapse; trailing-off mid-thought suggests token budget exhaustion or persona-driven early stopping; correct reasoning chain without final letter indicates instruction-following failure, not reasoning failure

- **First 3 experiments**:
  1. Replicate with explicit format reminder appended after persona prompt to test if the override is position-dependent
  2. Increase token cap to 500 and measure whether verbose personas (cannabis, LSD) recover format compliance
  3. Add a neutral persona ("you are a careful test-taker") to distinguish persona effects from "any additional text" effects

## Open Questions the Paper Calls Out

### Open Question 1
Can supervised finetuning inoculate models against persona-induced performance collapse? The study only tested inference-time persona prompts on a fixed model; no training interventions were explored. Fine-tuning models on datasets with mixed persona and formatting-compliance signals, then re-running the benchmark would resolve this.

### Open Question 2
How do persona prompts compose with chain-of-thought or few-shot prompting techniques? Only a single system instruction plus persona prefix was tested; composability with reasoning-enhancement methods is unknown. A factorial experiment crossing persona conditions with CoT/few-shot variations would resolve this.

### Open Question 3
Would explicit formatting reminders mitigate the template-breaking failures observed under destabilizing personas? The paper notes "the alcohol effect might diminish with larger samples or with explicit reminders to comply with formatting" but no remediation strategies were tested. Adding a reinforced formatting instruction after the persona prompt and comparing missing-prediction rates would resolve this.

## Limitations

- Prompt text ambiguity: Exact persona prompt texts are not provided verbatim, only described qualitatively
- Model availability: References "GPT-5-mini" which doesn't exist as of 2024, requiring substitution with GPT-4o-mini
- Single dataset constraint: Only ARC-Challenge science questions tested, limiting generalizability

## Confidence

**High confidence**: The experimental methodology (100 trials per condition, Wilson CIs, Fisher exact tests) is statistically sound and the observation that persona prompts cause format compliance failures is clearly demonstrated.

**Medium confidence**: The specific effect sizes are reproducible under stated conditions but may vary with different model versions, prompt formulations, or task types.

**Medium confidence**: The interpretation that persona prompts act as "consumable" context is supported but precise mechanisms remain theoretical.

**Low confidence**: The specific numerical accuracy comparisons assume the same underlying model capabilities across all runs, but sequential execution could introduce ordering effects.

## Next Checks

1. **Prompt position dependency test**: Replicate with explicit format reminders appended after persona prompt to determine if the override effect is position-dependent.

2. **Token budget extension**: Increase token cap from 300 to 500 tokens and measure whether verbose personas recover format compliance.

3. **Neutral persona control**: Add a neutral persona condition to distinguish whether effects are specific to drug-related personas or result from adding any additional text to the prompt.