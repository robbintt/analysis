---
ver: rpa2
title: 'TVS Sidekick: Challenges and Practical Insights from Deploying Large Language
  Models in the Enterprise'
arxiv_id: '2509.26482'
source_url: https://arxiv.org/abs/2509.26482
tags:
- sidekick
- data
- language
- llms
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TVS SCS UK developed TVS Sidekick, an in-house AI assistant using
  large language models (LLMs) with retrieval-augmented generation (RAG) to answer
  queries from enterprise data sources. The system integrates SharePoint, Azure DevOps,
  code repositories, and company websites into a vector database, enabling semantic
  search and code-specific prompt engineering.
---

# TVS Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise

## Quick Facts
- arXiv ID: 2509.26482
- Source URL: https://arxiv.org/abs/2509.26482
- Reference count: 14
- TVS Sidekick, an AI assistant using LLMs with RAG, processed over 750 queries in a four-month pilot, demonstrating value for retrieving enterprise knowledge and aligning with ISO/IEC 42001 AI management standards.

## Executive Summary
TVS SCS UK developed TVS Sidekick, an in-house AI assistant using large language models (LLMs) with retrieval-augmented generation (RAG) to answer queries from enterprise data sources. The system integrates SharePoint, Azure DevOps, code repositories, and company websites into a vector database, enabling semantic search and code-specific prompt engineering. A monitoring framework was implemented to track usage and feedback. Over a four-month pilot (March-June 2025), Sidekick processed over 750 queries, averaging five questions per session and a 46-second response time. Usage was highest among IT and business roles. Qualitative feedback highlighted Sidekickâ€™s value for retrieving SharePoint content and business logic, though technical users noted insufficient code detail. The project also aligned with ISO/IEC 42001 AI management standards, establishing governance processes for risk management, monitoring, and continuous improvement.

## Method Summary
TVS Sidekick was developed as an in-house AI assistant integrating SharePoint, Azure DevOps, code repositories, and company websites into a vector database for semantic search. The system employed LLMs with retrieval-augmented generation (RAG) and code-specific prompt engineering. A monitoring framework tracked usage and feedback during a four-month pilot (March-June 2025). Qualitative feedback was collected from pilot participants across IT and business roles. The project aligned with ISO/IEC 42001 AI management standards, establishing governance for risk management, monitoring, and continuous improvement.

## Key Results
- Processed over 750 queries in four months, averaging five questions per session and a 46-second response time.
- Highest usage among IT and business roles, with users valuing SharePoint and business logic retrieval.
- Technical users reported code responses lacked sufficient detail.
- Governance framework aligned with ISO/IEC 42001 AI management standards.

## Why This Works (Mechanism)
TVS Sidekick leverages retrieval-augmented generation (RAG) to combine semantic search of enterprise data with LLM-driven query responses, enabling efficient access to internal knowledge. Integration with SharePoint, Azure DevOps, and code repositories ensures comprehensive data coverage. Prompt engineering tailored for code-specific queries enhances relevance for technical users. The monitoring framework enables continuous feedback and iterative improvement. Governance aligned with ISO/IEC 42001 ensures responsible AI deployment and risk management.

## Foundational Learning
1. **Retrieval-Augmented Generation (RAG)**: Combines semantic search with LLM responses for accurate, context-aware answers. *Why needed*: Enables efficient retrieval of relevant enterprise knowledge. *Quick check*: Verify semantic search retrieves accurate, context-specific documents.
2. **Vector Database Integration**: Stores and indexes enterprise data for fast semantic search. *Why needed*: Supports scalable, real-time query processing. *Quick check*: Confirm vector embeddings accurately represent document content.
3. **Code-Specific Prompt Engineering**: Tailors LLM prompts for technical queries. *Why needed*: Improves relevance and accuracy of code-related responses. *Quick check*: Test prompt effectiveness with sample code queries.
4. **ISO/IEC 42001 AI Governance**: Establishes risk management, monitoring, and continuous improvement processes. *Why needed*: Ensures responsible and compliant AI deployment. *Quick check*: Review governance documentation for alignment with ISO standards.
5. **Usage Monitoring Framework**: Tracks query volume, session duration, and user feedback. *Why needed*: Enables data-driven optimization and user experience improvements. *Quick check*: Analyze metrics for trends and anomalies.
6. **Enterprise Data Integration**: Connects SharePoint, Azure DevOps, and code repositories. *Why needed*: Provides comprehensive access to internal knowledge. *Quick check*: Validate data synchronization and retrieval accuracy.

## Architecture Onboarding
**Component Map**: Enterprise data sources -> Vector database -> RAG pipeline -> LLM -> User interface
**Critical Path**: Data ingestion -> Vector embedding -> Semantic search -> Prompt engineering -> LLM response -> User feedback
**Design Tradeoffs**: Single vector database for simplicity vs. distributed systems for scalability; Azure ecosystem for integration vs. vendor lock-in.
**Failure Signatures**: Poor semantic search results (vector embedding issues), irrelevant LLM responses (prompt engineering flaws), low adoption (user interface or trust issues).
**3 First Experiments**:
1. Test semantic search accuracy with sample enterprise documents.
2. Validate LLM response relevance for code-specific queries.
3. Measure user satisfaction with initial pilot feedback.

## Open Questions the Paper Calls Out
None

## Limitations
- Absence of a control group or comparative baseline limits verification of efficiency and accuracy improvements.
- Feedback based on voluntary pilot participants and qualitative interviews, introducing potential selection and self-report biases.
- Usage patterns reported descriptively without statistical testing; four-month pilot may not reflect long-term adoption.
- Reliance on a single vector database and Azure ecosystem limits generalizability to other technical stacks.

## Confidence
- Sidekick reduces time to retrieve enterprise knowledge (Medium): Supported by average session duration and query volume, but lacks task completion benchmarks or pre/post comparisons.
- Users perceive high value in SharePoint and business logic queries (High): Consistently reflected in qualitative feedback across roles.
- Code-related responses lack sufficient technical depth (Medium): Based on technical user reports; no systematic evaluation of response quality or completeness.
- TVS SCS governance framework aligns with ISO/IEC 42001 (High): Directly stated and operationalized through documented risk and monitoring procedures.

## Next Checks
1. Conduct a randomized controlled trial comparing Sidekick-assisted query resolution time and accuracy against baseline manual search across multiple enterprise departments.
2. Implement a longitudinal study to assess retention, changing usage patterns, and sustained productivity gains beyond the initial pilot phase.
3. Perform a third-party audit of response quality for code-related queries using standardized technical benchmarks and peer review.