---
ver: rpa2
title: Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation
arxiv_id: '2602.00722'
source_url: https://arxiv.org/abs/2602.00722
tags:
- continual
- learning
- tasks
- task
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that low-rank continual adaptation suffers
  from forgetting due to imbalanced singular value spectra in task-specific updates.
  A few dominant components absorb most adaptation energy, disrupting previously acquired
  knowledge and making updates vulnerable to interference from subsequent tasks.
---

# Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation

## Quick Facts
- arXiv ID: 2602.00722
- Source URL: https://arxiv.org/abs/2602.00722
- Reference count: 40
- One-line primary result: Proposes EBLoRA to address forgetting in low-rank continual adaptation by balancing singular value spectra, achieving 72.8% MFN and 82.9% FWT on UCIT benchmark.

## Executive Summary
The paper identifies that low-rank continual adaptation suffers from forgetting due to imbalanced singular value spectra in task-specific updates. A few dominant components absorb most adaptation energy, disrupting previously acquired knowledge and making updates vulnerable to interference from subsequent tasks. To address this, the authors propose EBLoRA, which decouples update magnitude from directional structure and formulates the learning problem as constrained optimization on a restricted Stiefel manifold. The method uses projected updates to enforce orthonormality and gradient-orthogonality constraints. Experimental results show EBLoRA achieves 72.8% MFN and 82.9% FWT on the UCIT benchmark, outperforming baselines like LoRA-FT (61.4%, 26.8%), O-LoRA (64.1%, 27.0%), and SEFE (66.5%, 27.5%). On MLLM-DCL, it achieves 66.7% MFN and 34.4% FWT, surpassing KeepLoRA (64.4%, 33.7%). Ablation studies confirm that energy-balanced updates improve stability while maintaining plasticity.

## Method Summary
EBLoRA addresses forgetting in low-rank continual adaptation by factorizing updates as ΔW_t = s_t U_t V_t^⊤ where s_t is scalar magnitude and U_t, V_t are orthonormal matrices. The method formulates optimization as projected gradient descent on a restricted Stiefel manifold with orthonormality constraints. It uses a Gradient Projection Module (GPM) to track gradient subspaces from previous tasks and enforces gradient-orthogonality to prevent backward interference. Updates are initialized by projecting task gradients onto the null space of stored gradient subspaces, then taking leading singular vectors. The approach includes depth-aware initialization (s_min=0.002 to s_max=0.010) and uses whitening retraction to maintain orthonormality during training.

## Key Results
- EBLoRA achieves 72.8% MFN and 82.9% FWT on UCIT benchmark
- Outperforms LoRA-FT (61.4% MFN, 26.8% FWT), O-LoRA (64.1% MFN, 27.0% FWT), and SEFE (66.5% MFN, 27.5% FWT)
- On MLLM-DCL: 66.7% MFN and 34.4% FWT, surpassing KeepLoRA (64.4% MFN, 33.7% FWT)
- EBO alone (without gradient orthogonality) achieves 70.2% MFN, demonstrating decoupling's contribution

## Why This Works (Mechanism)

### Mechanism 1: Spectral Balance Reduces Cross-Task Interference
Updates with balanced singular value distributions produce lower interference than imbalanced ones, both as sources and targets of interference. When LoRA updates concentrate energy in few dominant singular components, those components become "pressure points" that disproportionately disrupt prior knowledge and are themselves vulnerable to subsequent task interference. Balancing energy across components distributes this pressure. Core assumption: Interference correlates with the concentration of adaptation energy in singular value space, not just the total magnitude of updates. Evidence: Multi-task merging experiment shows smoothing singular values improves NAI from average 59% (non-smooth) to 67% (smooth) and 44% to 69% across benchmark groups. Break condition: If tasks have inherently hierarchical importance, forcing balance may underweight critical directions.

### Mechanism 2: Magnitude-Direction Decoupling Enables Explicit Energy Control
Factorizing ΔW_t = s_t U_t V_t^⊤ with scalar magnitude s and orthonormal U, V allows explicit regulation of energy distribution across knowledge components. Standard LoRA (ΔW = BA) entangles magnitude and direction in coupled matrices. The sUV^⊤ factorization isolates "how much to adapt" (s) from "where to adapt" (U,V), enabling orthonormality constraints that naturally distribute energy equally across rank components. Core assumption: The benefits of explicit control outweigh potential representational constraints from forcing orthonormality. Evidence: EBO alone (without gradient orthogonality) achieves 70.2% MFN vs. 61.4% baseline, demonstrating decoupling's contribution. Break condition: If optimal task solutions require highly non-orthonormal directions, the constraint may limit expressivity.

### Mechanism 3: Gradient Orthogonality Prevents Backward Interference
Constraining current task update directions to be orthogonal to important gradient directions from previous tasks reduces backward forgetting. By maintaining G_{t-1} (stored gradient subspace from tasks 1 to t-1) and enforcing G_{t-1}^⊤ U_t = 0, new updates cannot modify parameters along directions that previous tasks were sensitive to. This is implemented via restricted Stiefel manifold constraints. Core assumption: Important knowledge from previous tasks can be approximated by a low-dimensional gradient subspace. Evidence: Adding gradient orthogonality improves MFN from 70.2% to 72.1% and BWT from -5.1% to -2.6%. Break condition: If stored gradient subspace is incomplete or outdated, orthogonalization may protect wrong directions or block useful updates.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD) and Spectral Structure**
  - Why needed here: The entire analysis hinges on viewing LoRA updates through their SVD decomposition σ_i u_i v_i^⊤ and observing that singular values are imbalanced. Without understanding what singular values represent (energy/magnitude along principal directions), the core motivation is opaque.
  - Quick check question: Given a matrix with singular values [5.0, 0.1, 0.1], what fraction of total energy is in the first component? (Answer: 25/(25+0.01+0.01) ≈ 99.9%)

- **Concept: Stiefel Manifold and Orthogonality Constraints**
  - Why needed here: The method formulates optimization on a "restricted Stiefel manifold" where U^⊤U = I. Understanding that this is the space of orthonormal matrices, and that optimizing on manifolds requires special projection/retraction steps, is essential for implementation.
  - Quick check question: Why can't you just use standard gradient descent to optimize a matrix with orthonormality constraints? (Answer: Standard updates leave the constraint surface; you need projection/retraction to restore feasibility)

- **Concept: Catastrophic Forgetting and Backward/Forward Transfer**
  - Why needed here: The paper measures BWT (backward transfer, negative = forgetting) and FWT (forward transfer to unseen tasks). Understanding these as the core continual learning objectives contextualizes why spectral balance matters.
  - Quick check question: If a model achieves 90% on task 1 after training, but 60% after training on task 2, what is the backward transfer for task 1? (Answer: 60% - 90% = -30%, indicating forgetting)

## Architecture Onboarding

- **Component map**: GPM Subspace Store -> Projected Optimizer Wrapper -> sUV Parameterization -> Initialization Module

- **Critical path**:
  1. Task t arrives → collect gradient snapshot G_t
  2. Project G_t onto null(G_{t-1}^⊤) → G_t^{proj}
  3. SVD of G_t^{proj} → initialize U_t^{(0)}, V_t^{(0)} from top r singular vectors
  4. Training loop: compute loss → project gradient to tangent space → optimizer step → project increment → retraction
  5. After training: update G_{t-1} with GPM to get G_t
  6. Apply update: W_t = W_{t-1} + s_t U_t V_t^⊥

- **Design tradeoffs**:
  - Rank r: Higher r = more expressivity but larger GPM storage and more constraint complexity
  - GPM energy threshold ε: Higher = more directions protected but more restrictive optimization
  - Depth-aware scaling: Larger s for deeper layers (0.010) vs shallow (0.002) based on LiNeS observation—tunable but not heavily ablated
  - EBO-only vs full EBLoRA: EBO drops gradient orthogonality for simpler implementation (Table 3 shows 70.2% vs 72.8% MFN)

- **Failure signatures**:
  - Training instability/NaN loss: Retraction failing when Y^⊤Y is near-singular; add numerical stabilization or reduce learning rate
  - No improvement over LoRA-FT: GPM subspace may be empty (check if G_{t-1} is actually populated) or gradient orthogonality not enforced
  - High backward transfer (negative): Gradient subspace G_{t-1} not capturing important directions; increase ε threshold or check GPM update logic
  - Low forward transfer: Initialization from projected gradient may be poor for dissimilar tasks; consider warmup or alternative initialization
  - Energy still imbalanced: Verify whitening retraction is applied correctly (U^⊤U should be exactly I up to numerical precision)

- **First 3 experiments**:
  1. **Spectral imbalance validation**: Train standard LoRA on 3 UCIT tasks, plot singular value distributions (replicate Figure 2a). Confirm variance increases during training and first few components dominate. This validates the core observation before implementing fixes.
  2. **Smoothing ablation**: Implement singular value smoothing (replace σ with mean σ̄) as post-hoc operation on trained LoRA adapters. Merge multiple task adapters with/without smoothing and measure NAI (Eq. 2). This isolates whether spectral balance alone reduces interference without architectural changes.
  3. **EBO-only baseline**: Implement just the sUV factorization with whitening retraction (no gradient orthogonality, no GPM). Compare to LoRA-FT on 3-task sequence measuring MFN, BWT. Table 3 shows this should achieve ~70% MFN; significant deviation indicates implementation issues in projection/retraction logic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does EBLoRA's effectiveness scale to models significantly larger than LLaVA-1.5-7B (e.g., 70B+ parameter models)?
- Basis in paper: [explicit] The conclusion claims the method is "scalable to larger models and diverse tasks," yet all experiments are conducted solely on the 7B model.
- Why unresolved: The computational and optimization dynamics of restricted Stiefel manifold projection may behave differently at much larger scales, where gradient noise and optimization landscapes differ substantially.
- What evidence would resolve it: Empirical evaluation on models in the 30B–100B+ parameter range, comparing MFN/FWT against baselines to verify whether spectral balancing benefits persist or diminish at scale.

### Open Question 2
- Question: What is the precise computational overhead of the whitening retraction compared to standard LoRA fine-tuning?
- Basis in paper: [inferred] Algorithm 1 requires computing SVD/eigendecomposition of Y^⊤Y at every optimization step (line 13-15), but the paper provides no wall-clock time or FLOP comparison.
- Why unresolved: The per-step cost of orthonormalization may negate efficiency gains from low-rank adaptation, especially for large hidden dimensions d.
- What evidence would resolve it: Detailed profiling of training time per step and total training duration comparing EBLoRA vs. standard LoRA across varying model sizes and rank values.

### Open Question 3
- Question: How does the choice of rank r affect the trade-off between spectral balance benefits and model expressivity?
- Basis in paper: [inferred] The paper uses fixed rank r across experiments but does not study whether the benefits of energy-balanced updates vary with different rank choices or depend on task complexity.
- Why unresolved: At low ranks, orthonormality constraints may limit representational capacity, while at higher ranks the spectral imbalance problem may become more or less severe.
- What evidence would resolve it: Systematic ablation varying r across a range (e.g., 4, 8, 16, 32, 64) and analyzing how MFN, BWT, and FWT change relative to standard LoRA at each rank.

### Open Question 4
- Question: What is the memory footprint of accumulated gradient subspaces G_t as the number of tasks grows large?
- Basis in paper: [inferred] The GPM mechanism accumulates gradient basis vectors across tasks, but the paper does not analyze memory scaling or propose mechanisms to bound storage.
- Why unresolved: For very long task sequences (hundreds of tasks), storing G_t could become prohibitively expensive, potentially undermining the parameter-efficiency motivation.
- What evidence would resolve it: Analysis of memory usage as a function of task count, plus investigation of subspace compression or forgetting mechanisms to bound storage while preserving performance.

## Limitations
- The spectral imbalance analysis relies on controlled synthetic task sequences; performance in real-world non-IID streaming scenarios remains untested.
- Gradient orthogonality effectiveness depends on accurate estimation of important gradient subspaces; incomplete or stale G_{t-1} could degrade performance.
- The depth-aware scaling (0.002–0.010) is empirically motivated but not rigorously ablated across different model architectures.

## Confidence

- **High confidence**: The spectral imbalance observation is well-supported by multi-task merging experiments showing NAI improvements from 59% to 67% with smoothing.
- **Medium confidence**: The mechanism connecting spectral balance to reduced interference is logically sound but relies on correlation evidence rather than ablation of the underlying mathematical relationship.
- **Medium confidence**: The constrained optimization approach (EBLoRA) outperforms baselines in reported benchmarks, but implementation complexity raises concerns about exact reproduction fidelity.

## Next Checks

1. **Implementation verification**: Implement EBO-only variant and verify it achieves the reported ~70% MFN on UCIT; this isolates whether the sUV factorization and orthonormality constraints work correctly before adding gradient orthogonality.

2. **Spectral analysis replication**: Train standard LoRA on a 3-task sequence and measure singular value variance evolution; confirm that variance increases during training and that smoothing improves multi-task merging performance.

3. **Constraint sensitivity test**: Vary the GPM energy threshold ε (0.8, 0.95, 0.99) and measure impact on BWT and FWT; this validates whether the gradient subspace accumulation is capturing the right directions for orthogonality constraints.