---
ver: rpa2
title: Source-primed Multi-turn Conversation Helps Large Language Models Translate
  Documents
arxiv_id: '2503.10494'
source_url: https://arxiv.org/abs/2503.10494
tags:
- translation
- multi-turn
- machine
- segment-level
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a source-primed multi-turn conversation method
  for document-level machine translation using large language models (LLMs). The method
  decomposes documents into segments and iteratively translates them while maintaining
  previous turns, allowing access to full document context and efficient KV cache
  reuse.
---

# Source-primed Multi-turn Conversation Helps Large Language Models Translate Documents

## Quick Facts
- arXiv ID: 2503.10494
- Source URL: https://arxiv.org/abs/2503.10494
- Authors: Hanxu Hu; Jannis Vamvas; Rico Sennrich
- Reference count: 14
- One-line result: Source-primed multi-turn conversation improves document-level translation quality over single-turn and segment-level approaches by maintaining context and enabling KV cache reuse.

## Executive Summary
This paper introduces a source-primed multi-turn conversation method for document-level machine translation using large language models (LLMs). The approach decomposes documents into segments and translates them iteratively while maintaining previous turns as context. The method includes an optional source-priming step that provides the full source document before segment translation, giving models access to future context. Experiments on WMT-24 and WMT-23 demonstrate that this approach outperforms both single-turn document translation and segment-level translation across multiple metrics, with source-priming achieving the best results.

## Method Summary
The method decomposes documents into segments (paragraphs) and translates them iteratively in a conversational format. Each turn includes the current source segment plus all previous conversation history. The source-primed variant first presents the complete source document as a priming turn before beginning segment-by-segment translation. This allows models to establish document-level topic, style, and entity awareness before generating any target text. The multi-turn structure with immutable prefixes enables KV cache reuse, reducing computational overhead compared to segment-in-context approaches.

## Key Results
- Source-primed multi-turn outperforms single-turn, segment-level, and standard multi-turn across WMT-24 metrics (dBLEU, COMET-22)
- Multi-turn approaches significantly reduce omission errors in long documents compared to single-turn translation
- The method achieves these improvements without additional model training or fine-tuning
- Source-priming provides particular benefits for early segments in long documents

## Why This Works (Mechanism)

### Mechanism 1
Multi-turn conversational translation improves document coherence by maintaining access to previously translated segments as context. Documents are decomposed into segments, and each segment is translated in a conversational turn while preserving all prior conversation history. The LLM conditions on both source segments and previously generated translations to maintain consistency in tense, pronouns, and entity references.

### Mechanism 2
Source-priming—presenting the full source document before multi-turn translation—provides "future context" that improves translation quality from the first segment. The initial turn supplies the complete source document with translation instructions but requests no output. Subsequent turns translate segment-by-segment with the full source already in context.

### Mechanism 3
The multi-turn structure with immutable prefixes enables KV cache reuse, reducing computational overhead compared to segment-in-context approaches that reprocess prompts. By structuring translation as an expanding conversation where previous turns are never overwritten, the KV cache from prior turns can be retained.

## Foundational Learning

- **KV Cache in Autoregressive Transformers**: Understanding how key-value caching works—storing computed attention states from prior tokens to avoid recomputation when generating subsequent tokens. *Quick check*: Can you explain why changing a token in the middle of a cached sequence invalidates the cache for all subsequent positions?

- **Document-Level MT Phenomena**: Understanding specific document-level challenges (omission errors, pronoun consistency, tense agreement) clarifies what the method aims to solve. *Quick check*: Give an example where translating a sentence in isolation would produce a pronoun error resolvable with document context.

- **Multi-Turn Instruction Following in LLMs**: Understanding how models track conversation state explains why this framing is effective without fine-tuning. *Quick check*: What happens to conversation history when the context window limit is exceeded?

## Architecture Onboarding

- **Component map**: Document Input → Segment Splitter (paragraph-level) → Source-Priming Turn (optional) → Multi-Turn Translation Loop → Aggregated Output Document

- **Critical path**:
  1. Segment document into paragraphs
  2. (Optional) Issue source-priming turn with full document
  3. For each segment: construct turn with prior history + current source + instruction
  4. Collect translated segments; concatenate for final document
  5. Ensure KV cache is enabled and prefix is invariant across turns

- **Design tradeoffs**:
  - Source-priming adds one inference turn overhead but improves early-segment quality
  - Paragraph-level vs. sentence-level segments: paragraphs provide more local context but increase per-turn token count
  - ICL exemplars improve quality but increase prefix length; must remain fixed for cache compatibility
  - Greedy decoding used in experiments; sampling may introduce variability

- **Failure signatures**:
  - Omission errors in single-turn for long documents—diagnose by comparing output token count to reference
  - Inconsistent translation style across segments—verify multi-turn conversation history is properly maintained
  - Social/Personal domains show negative dBLEU delta vs. segment-level

- **First 3 experiments**:
  1. Reproduce main result: Implement standard multi-turn and source-primed multi-turn on WMT-24 with Llama-3.1-8B-Instruct
  2. Ablate source-priming: Compare multi-turn with and without source-priming across document lengths
  3. Measure omission reduction: For top-10 longest documents, compare reference vs. hypothesis token counts

## Open Questions the Paper Calls Out
- The authors acknowledge that they "only use automatic metrics" and that "human evaluation remains important future work" for assessing translation fluency, discourse coherence, and accuracy of content coverage.
- The paper states that its efficiency argument is "theoretical" and they "did not implement KV caching ourselves," despite claiming it minimizes computational overhead.
- The Appendix notes that the specialized model TowerInstruct did not benefit from the multi-turn method, likely because it was "mostly optimized on single-turn machine translation data."

## Limitations
- The effectiveness of source-priming relies on unverified assumptions about how LLMs utilize priming information during segment translation.
- Claims about computational efficiency through KV cache reuse lack empirical validation and performance benchmarks.
- The method's effectiveness for very long documents is unclear due to context window limitations.
- Results show negative performance on Social and Personal domains, suggesting limited generalization across all document types.

## Confidence

**High confidence**: The core finding that multi-turn translation outperforms single-turn document translation for document-level coherence. Validated across multiple metrics (dBLEU, COMET, BlonDE) on established benchmarks.

**Medium confidence**: The specific advantage of source-primed multi-turn over standard multi-turn. While statistically significant, the underlying mechanism lacks direct validation.

**Low confidence**: Claims about computational efficiency through KV cache reuse. The paper asserts this benefit without providing performance benchmarks or cache utilization measurements.

## Next Checks

**Check 1**: Validate source-priming mechanism by comparing standard multi-turn, source-primed multi-turn, and source-primed with masked source segments during translation.

**Check 2**: Benchmark computational efficiency by measuring wall-clock time and memory usage for different approaches on documents of varying lengths, tracking KV cache utilization.

**Check 3**: Analyze long-document performance degradation by creating synthetic documents of increasing length and measuring omission rates and coherence metrics to identify when multi-turn benefits plateau.