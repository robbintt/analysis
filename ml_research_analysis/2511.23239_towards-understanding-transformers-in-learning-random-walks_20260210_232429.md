---
ver: rpa2
title: Towards Understanding Transformers in Learning Random Walks
arxiv_id: '2511.23239'
source_url: https://arxiv.org/abs/2511.23239
tags:
- lemma
- random
- figure
- transformer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how transformers learn to predict random walks
  on circles. The authors show that a one-layer transformer trained with gradient
  descent can achieve optimal prediction accuracy for random walks with non-zero transition
  probability p.
---

# Towards Understanding Transformers in Learning Random Walks

## Quick Facts
- arXiv ID: 2511.23239
- Source URL: https://arxiv.org/abs/2511.23239
- Reference count: 7
- One-layer transformers can optimally predict random walks on circles when transition probability p is strictly between 0 and 1

## Executive Summary
This paper investigates how transformers learn to predict random walks on circular graphs. The authors demonstrate that a single-layer transformer can achieve optimal prediction accuracy for random walks with non-zero transition probabilities, while failing to learn for deterministic walks (p=0 or 1). The trained models are interpretable, with attention mechanisms that effectively select parent tokens and value matrices that act as transition models. The study provides both theoretical guarantees and experimental validation across synthetic and simple question-answering tasks.

## Method Summary
The authors analyze transformer learning on random walks over circular graphs where tokens transition clockwise or counterclockwise with probability p. They theoretically prove that for p ∈ (0,1), gradient descent converges to a model that achieves optimal prediction accuracy. The analysis shows that softmax attention weights converge to near-one for selecting the direct parent token, while the value matrix learns to act as a one-step transition model. Experiments validate these findings using synthetic random walk data and simple question-answering tasks, demonstrating consistent optimization challenges for deterministic cases.

## Key Results
- Transformers achieve optimal prediction accuracy for random walks when transition probability p is strictly between 0 and 1
- The trained models are interpretable: attention selects direct parent tokens and value matrices model one-step transitions
- For deterministic walks (p=0 or 1), transformers fail to learn, achieving only random guess accuracy due to optimization issues from zero initialization
- Experimental results confirm theoretical predictions across both synthetic and simple QA tasks

## Why This Works (Mechanism)
The mechanism works because for non-deterministic walks (p ∈ (0,1)), the optimization landscape allows gradient descent to converge to an interpretable solution where attention focuses on the correct parent token and the value matrix learns the transition dynamics. The non-zero randomness in transitions creates gradients that guide the model toward the optimal configuration. The softmax attention naturally selects the most relevant parent token with near-one weight, while the value matrix learns to predict the next token based on the current state.

## Foundational Learning
- Random walks on graphs: Why needed - forms the prediction task; Quick check - understand probability transitions on circular graphs
- Transformer attention mechanism: Why needed - core component being analyzed; Quick check - grasp query-key-value interactions
- Gradient descent optimization: Why needed - training method being analyzed; Quick check - understand convergence behavior
- Interpretability in neural networks: Why needed - key contribution is explaining what the model learns; Quick check - recognize interpretable patterns in attention and value matrices

## Architecture Onboarding
Component map: Input tokens -> Query/Key/Value projections -> Attention scores -> Softmax -> Weighted values -> Output prediction

Critical path: Token embeddings → Linear projections → Attention computation → Softmax → Value combination → Prediction

Design tradeoffs: Single-layer vs multi-layer (simplicity vs capacity), deterministic vs stochastic transitions (learnability vs optimization challenges)

Failure signatures: Random guess accuracy for p=0 or p=1, attention weights not converging to expected patterns, value matrix not learning transition dynamics

First experiments: 1) Train on random walks with p=0.5 to verify optimal learning, 2) Train on deterministic walks (p=0) to confirm failure mode, 3) Vary p values to map learning boundary

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted to single-layer, single-head transformers on cyclic graphs, limiting generalizability
- Theoretical guarantees only apply to the specific random walk setting studied
- Cannot prove that failure at p=0 or p=1 is unavoidable, only that it occurs with standard training

## Confidence
- Theoretical analysis of optimal convergence for p ∈ (0,1): High
- Empirical demonstration of failure at p=0 or p=1 due to optimization: Medium
- Generalization of findings to broader transformer applications: Low

## Next Checks
1. Test whether warm initialization, curriculum learning, or alternative loss functions enable learning for deterministic walks (p=0 or p=1)
2. Extend experiments to multi-layer transformers and multi-head attention to assess scalability of interpretability and optimization behavior
3. Evaluate model performance and interpretability on non-cyclic, more complex transition graphs to probe robustness beyond the current theoretical setting