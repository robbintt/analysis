---
ver: rpa2
title: 'State of AI: An Empirical 100 Trillion Token Study with OpenRouter'
arxiv_id: '2601.10088'
source_url: https://arxiv.org/abs/2601.10088
tags:
- usage
- open
- figure
- tokens
- share
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study analyzes over 100 trillion tokens of LLM usage data
  from OpenRouter to empirically understand how large language models are being used
  in practice. The research reveals several key findings: 1) A significant portion
  of usage (approximately 30%) is going to open-source models, with Chinese models
  like DeepSeek and Qwen showing particularly strong growth.'
---

# State of AI: An Empirical 100 Trillion Token Study with OpenRouter

## Quick Facts
- arXiv ID: 2601.10088
- Source URL: https://arxiv.org/abs/2601.10088
- Reference count: 5
- Primary result: Programming dominates LLM usage (>50% of tokens) with significant open-source adoption and emergence of agentic inference patterns

## Executive Summary
This study analyzes over 100 trillion tokens of LLM usage data from OpenRouter to empirically understand how large language models are being used in practice. The research reveals that programming has become the dominant category of usage, representing over 50% of tokens in recent months, driven by the integration of LLMs into development workflows. A significant portion of usage (approximately 30%) is going to open-source models, with Chinese models like DeepSeek and Qwen showing particularly strong growth. The concept of "agentic inference" is emerging, where models are used for multi-step reasoning and tool use rather than simple text generation.

## Method Summary
The study analyzes anonymized usage data from OpenRouter's platform spanning multiple years, tracking over 100 trillion tokens across various model families. The analysis includes retention cohort analysis, task category classification, geographic distribution, and agentic inference patterns. The researchers employ token-level tracking to distinguish between prompt tokens, completion tokens, and reasoning tokens, enabling detailed analysis of usage patterns and model capabilities.

## Key Results
- Programming has become the dominant usage category, representing over 50% of tokens
- Open-source models capture approximately 30% of usage, with Chinese models showing strong growth
- Agentic inference patterns are emerging, characterized by extended context, tool use, and multi-step reasoning
- Retention analysis reveals "foundational cohorts" where early users demonstrate persistent engagement

## Why This Works (Mechanism)

### Mechanism 1: Workload-Model Fit Creates Durable Cohorts ("Glass Slipper" Effect)
- Claim: Early user cohorts that find precise alignment between their workload requirements and a model's capabilities exhibit substantially higher long-term retention than later cohorts.
- Mechanism: When a model first solves a previously infeasible workload, users embed it into pipelines, infrastructure, and behaviors. This creates switching friction (both technical and cognitive) that persists even when newer models emerge.
- Core assumption: Retention differences across cohorts reflect genuine capability inflection points rather than marketing effects or seasonal variation.
- Evidence anchors:
  - [abstract]: "retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella 'Glass Slipper' effect"
  - [Section 7.1]: "June 2025 cohort of Gemini 2.5 Pro and May 2025 cohort of Claude 4 Sonnet retain approximately 40% of users at Month 5, substantially higher than later cohorts"
- Break condition: If retention advantages disappear when controlling for total ecosystem tokens consumed (i.e., heavy users are just heavy users regardless of cohort), the mechanism would weaken.

### Mechanism 2: Task Specialization Drives Multi-Model Adoption
- Claim: Users do not consolidate on a single "best" model; instead, they route different task types to different models based on capability profiles.
- Mechanism: Different model families develop distinct strengths (Claude → programming, DeepSeek → roleplay, Qwen → code) through training data, fine-tuning, and alignment choices. Users learn these profiles and route accordingly.
- Core assumption: The routing behavior observed at platform level reflects intentional user choice rather than default settings or application-level routing logic.
- Evidence anchors:
  - [Section 3.3]: "OSS models have carved out leadership in two particular areas: creative roleplay and programming assistance. Together, these categories account for the majority of OSS token usage"
  - [Section 5.3]: "Anthropic's Claude is heavily skewed toward Programming+Technology uses (over 80%); DeepSeek's token distribution is dominated by roleplay (more than two-thirds)"
- Break condition: If a single model begins to dominate across all major categories (>60% share in each), the task-specialization mechanism would diminish.

### Mechanism 3: Extended Context + Tool Calling Enables Agentic Workflows
- Claim: The shift from single-pass to multi-step inference is driven by longer prompt contexts and integrated tool-calling, which together support agent-like task completion.
- Mechanism: Programming workloads (now >50% of tokens) require large input contexts (codebases, transcripts). Models with native reasoning and tool support can decompose tasks, call external APIs, and iterate—replacing single-shot completions.
- Core assumption: Longer sequence lengths and tool invocation rates are valid proxies for "agentic" behavior rather than just verbose users.
- Evidence anchors:
  - [Section 4.3]: "Average prompt tokens per request have increased roughly fourfold from around 1.5K to over 6K while completions have nearly tripled"
  - [Section 4.1]: "share of total tokens routed through reasoning-optimized models... now exceeds fifty percent"
  - [Section 4.2]: "tool invocation was initially concentrated among a small group of models... By mid-year, a broader set of models began supporting tool provision"
- Break condition: If tool-calling success rates plateau or context window expansion yields diminishing returns on task completion, the agentic shift may decelerate.

## Foundational Learning

- **Concept: Token Economics (Prompt vs. Completion vs. Reasoning Tokens)**
  - Why needed here: The study measures everything in tokens and distinguishes input (prompt), output (completion), and internal reasoning tokens. Understanding this is essential to interpret cost, latency, and workload profiles.
  - Quick check question: If a model uses 10K prompt tokens and generates 500 completion tokens, including 200 reasoning tokens, what is the total token count reported? (Answer: 10,500 total; reasoning tokens are a subset of completion tokens.)

- **Concept: Cohort Retention Analysis**
  - Why needed here: The "Glass Slipper" effect relies on comparing retention curves across user cohorts (users who joined in the same time window). Without this, you cannot distinguish foundational lock-in from general churn.
  - Quick check question: If a model's overall retention is 20% at Month 3, but its Month-1 cohort shows 40% retention at Month 3 while Month-2 cohort shows 10%, what does this suggest? (Answer: A foundational cohort formed at launch; later cohorts did not achieve workload-model fit.)

- **Concept: Agentic vs. Single-Pass Inference**
  - Why needed here: The paper documents a paradigm shift from "generate text" to "plan, reason, execute across steps." Understanding this distinction is critical for architecture decisions (stateless API vs. stateful orchestration).
  - Quick check question: What three observable signals does the paper use to proxy agentic inference? (Answer: Reasoning model share, tool-calling finish reasons, and sequence length growth—especially in programming tasks.)

## Architecture Onboarding

- **Component map:**
  - Request classifier → Model router → Tool registry → Context manager → Reasoning engine → Completion handler → Observability logger

- **Critical path:**
  1. Classify incoming request by task category (programming, roleplay, translation, etc.)
  2. Route to appropriate model family based on capability profile and cost/latency budget
  3. If tools are defined, inject tool schemas into prompt; set finish-reason monitoring
  4. For long-context workloads (especially programming), pre-validate token budget
  5. Log cohort identifier (user signup date) to enable retention analysis

- **Design tradeoffs:**
  - Cost vs. capability: Premium models (Claude, GPT-Pro) win on reliability; OSS models (DeepSeek, Qwen) win on volume and cost-sensitive workloads
  - Latency vs. reasoning depth: Extended deliberation improves output quality but increases latency—unacceptable for real-time chat, acceptable for code generation
  - Single-model simplicity vs. multi-model flexibility: Single model reduces operational complexity; multi-model routing optimizes per-task performance

- **Failure signatures:**
  - High token volume but low retention: Model never achieved workload-model fit; users are experimenting, not embedding
  - Tool-calling enabled but low invocation rate: Tools defined but not useful; schema mismatch or model not trained for tool use
  - Prompt token growth without completion quality improvement: Context stuffing without retrieval quality—users dumping irrelevant context

- **First 3 experiments:**
  1. **Cohort retention baseline:** Segment users by signup month; track 30/60/90-day retention by model. Identify if any cohort shows >35% retention at Month 3 (foundational cohort candidate).
  2. **Task routing accuracy:** Classify 1,000 requests by category; measure whether current routing logic sends them to the model with best-in-category performance (per Section 5.3 profiles).
  3. **Tool invocation rate by model:** Enable identical tool definitions across 3 models; measure tool-call finish-reason rate. Target: >15% of requests with successful invocation for models claiming agentic capability.

## Open Questions the Paper Calls Out
None

## Limitations
- Data scope and representativeness: Analysis based on OpenRouter's platform data, which may not represent broader LLM ecosystem usage patterns
- Attribution challenges: Difficulties in accurately attributing usage to specific models and use cases, particularly for agentic workloads
- Geographic and temporal biases: Usage patterns vary by geography, but cultural and economic factors weren't fully explored

## Confidence
- **High Confidence**: Programming dominance (>50% of tokens) is consistently supported across multiple time periods
- **Medium Confidence**: Cohort retention differences (Glass Slipper effect) are observed but could be influenced by platform growth dynamics
- **Low Confidence**: Specific mechanisms driving agentic inference are based on proxy metrics rather than direct measurement of reasoning quality

## Next Checks
1. **Retention Analysis with Controls**: Replicate cohort retention analysis while controlling for total token consumption, user activity levels, and platform growth effects
2. **Cross-Platform Usage Verification**: Compare OpenRouter's usage patterns with at least two other major LLM platforms to assess consistency of observed trends
3. **Tool Invocation Quality Assessment**: Conduct qualitative analysis of tool call success rates and completion quality across different model families beyond simple invocation metrics