---
ver: rpa2
title: 'Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey'
arxiv_id: '2507.22920'
source_url: https://arxiv.org/abs/2507.22920
tags:
- quantization
- discrete
- image
- generation
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys discrete tokenization methods for multimodal
  large language models (LLMs), focusing on vector quantization (VQ) techniques that
  convert continuous multimodal data into discrete tokens compatible with LLM architectures.
  The authors present a structured taxonomy of 8 representative VQ variants, analyze
  their algorithmic principles, training dynamics, and integration challenges, and
  review applications across images, audio, video, graphs, and recommendation systems.
---

# Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey

## Quick Facts
- **arXiv ID**: 2507.22920
- **Source URL**: https://arxiv.org/abs/2507.22920
- **Reference count**: 40
- **Primary result**: Survey categorizes 8 VQ variants for discrete tokenization in multimodal LLMs, providing mathematical formulations and design principles

## Executive Summary
This survey systematically examines discrete tokenization methods for multimodal large language models, focusing on vector quantization techniques that convert continuous multimodal data into discrete tokens compatible with LLM architectures. The authors present a structured taxonomy of 8 representative VQ variants, analyze their algorithmic principles, training dynamics, and integration challenges, and review applications across images, audio, video, graphs, and recommendation systems. Key challenges identified include codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. The survey also outlines emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning.

## Method Summary
The paper surveys discrete tokenization methods for multimodal LLMs, focusing on vector quantization (VQ) techniques that convert continuous multimodal data into discrete tokens compatible with LLM architectures. The authors present a structured taxonomy of 8 representative VQ variants, analyze their algorithmic principles, training dynamics, and integration challenges, and review applications across images, audio, video, graphs, and recommendation systems. Key challenges identified include codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. The survey also outlines emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. A continuously updated version is available at https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.

## Key Results
- Presents a structured taxonomy of 8 vector quantization variants for discrete tokenization
- Analyzes core challenges including codebook collapse and unstable gradient estimation
- Reviews applications across multiple modalities: images, audio, video, graphs, and recommendation systems
- Identifies emerging research directions including dynamic quantization and hybrid discrete-continuous architectures

## Why This Works (Mechanism)
Discrete tokenization enables multimodal LLMs to process continuous data through a discrete bottleneck compatible with transformer architectures. Vector quantization maps continuous latent representations to discrete codebook entries via nearest-neighbor assignment, allowing LLMs to generate sequences of discrete tokens. The mechanism works by encoding continuous input into latent space, quantizing to discrete codebook entries using argmax selection, and reconstructing through a decoder. Gradient flow is maintained through techniques like Straight-Through Estimator, Gumbel-Softmax, or rotation tricks that bypass the non-differentiable quantization operation. Codebook learning is stabilized through exponential moving average updates or learned gradient-based approaches.

## Foundational Learning
**Vector Quantization (VQ)**: Maps continuous vectors to discrete codebook entries via nearest-neighbor assignment. Why needed: Enables compatibility between continuous multimodal data and discrete-token LLMs. Quick check: Verify codebook utilization rate >50% to ensure diversity.

**Straight-Through Estimator (STE)**: Gradient bypass technique that copies gradients from decoder to encoder while treating quantization as identity during forward pass. Why needed: Maintains training stability through discrete bottleneck. Quick check: Monitor gradient norms at encoder output for stability.

**Codebook Collapse**: Phenomenon where only small subset of codebook entries are actively used. Why needed: Understanding this failure mode is critical for successful implementation. Quick check: Track per-code utilization histogram to identify dead codes.

**Gumbel-Softmax**: Differentiable approximation to discrete sampling that enables end-to-end training. Why needed: Provides smoother gradient flow than STE. Quick check: Monitor temperature schedule to prevent mode collapse.

**EMA Codebook Updates**: Exponential moving average of encoder activations and assignments for codebook learning. Why needed: Stabilizes codebook updates without direct gradient computation. Quick check: Verify codebook diversity through cluster analysis.

## Architecture Onboarding

**Component Map**: Continuous Input -> Encoder -> Latent Space z -> Quantization Q(z) -> Codebook Entry cq -> Decoder -> Reconstruction

**Critical Path**: Encoder → Quantization → Codebook → Decoder (most sensitive to gradient estimation and codebook stability issues)

**Design Tradeoffs**: STE provides stable gradients but biased estimation vs Gumbel-Softmax provides smoother gradients but requires careful temperature scheduling; EMA updates are stable but slow vs learned updates are faster but can destabilize training.

**Failure Signatures**: Codebook utilization below 50% indicates collapse; gradient norms showing sudden spikes or zeros indicate STE issues; training loss plateauing suggests quantization bottleneck too restrictive.

**First Experiments**: 
1. Implement vanilla VQ-VAE with STE and EMA updates, measure codebook utilization distribution
2. Compare reconstruction quality between STE-based and Gumbel-Softmax variants on ImageNet
3. Test codebook utilization on multimodal datasets to verify modality-specific constraints

## Open Questions the Paper Calls Out

**Open Question 1**: How can codebook diversity be balanced with training stability to achieve high utilization without performance degradation? [explicit] Section 6(a) states techniques like reparameterization and regularization "often compromise stability" and calls for approaches that "balance token diversity and coverage with stability." Current methods either improve utilization at the cost of stability or maintain stability with underutilized codebooks; no unified solution exists.

**Open Question 2**: Can task- and modality-aware adaptive coding schemes effectively mitigate information loss inherent to discretization? [explicit] Section 6(b) identifies information loss as "inherent to discretization" but proposes "task- and modality-aware strategies" as a "promising direction." The paper notes this limitation is fundamental; it is unclear if adaptive schemes can compensate adequately without introducing prohibitive complexity.

**Open Question 3**: What is the optimal architecture for hybrid systems that unify discrete and continuous token representations during both training and inference? [explicit] Section 6(e) states that "Developing hybrid architectures that unify discrete and continuous tokens during training and inference represents a promising direction," noting such integration is rare. The paper indicates only a few recent studies have begun exploring integration, and no consensus exists on how to jointly optimize or align the two representation spaces.

## Limitations
- Lacks empirical validation across the 8 VQ variants it categorizes
- No quantitative comparisons showing when specific quantization methods outperform others
- Theoretical formulations presented without experimental verification of stated advantages
- Implementation details and hyperparameters remain underspecified

## Confidence

**High confidence**: The survey's classification of VQ variants into a coherent taxonomy and identification of core challenges (codebook collapse, gradient estimation instability) is well-supported by established literature in the field.

**Medium confidence**: The description of training objectives and algorithmic principles for each method appears accurate based on cited original papers, though implementation details remain underspecified.

**Low confidence**: Claims about relative performance, convergence properties, and modality-specific effectiveness lack empirical support within the survey itself.

## Next Checks
1. Implement the vanilla VQ-VAE baseline with EMA codebook updates and STE gradient estimation, then measure codebook utilization distribution over training to verify the described collapse phenomenon.
2. Compare reconstruction quality (FID/PSNR) between STE-based VQ-VAE and Gumbel-Softmax variants on the same dataset to empirically validate claims about gradient estimation methods.
3. Test codebook utilization metrics on multimodal datasets (image + audio) to verify whether the survey's identified modality-specific constraints manifest as predicted.