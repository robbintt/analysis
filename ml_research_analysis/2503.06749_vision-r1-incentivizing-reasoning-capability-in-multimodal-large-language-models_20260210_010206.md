---
ver: rpa2
title: 'Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language
  Models'
arxiv_id: '2503.06749'
source_url: https://arxiv.org/abs/2503.06749
tags:
- reasoning
- arxiv
- training
- angle
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving reasoning capabilities
  in Multimodal Large Language Models (MLLMs) using Reinforcement Learning (RL). Direct
  RL training struggles to generate complex reasoning in MLLMs due to a lack of high-quality
  multimodal data.
---

# Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2503.06749
- Source URL: https://arxiv.org/abs/2503.06749
- Reference count: 38
- Vision-R1-7B achieves 73.5% accuracy on MathVista, comparable to leading models with 7B parameters

## Executive Summary
Vision-R1 addresses the challenge of enhancing reasoning capabilities in Multimodal Large Language Models (MLLMs) through a novel reinforcement learning approach. The core innovation is a two-phase training pipeline that overcomes the data scarcity problem in multimodal reasoning by constructing a high-quality Chain-of-Thought dataset without human annotations. The model achieves state-of-the-art performance on multimodal math reasoning benchmarks, with Vision-R1-7B matching the accuracy of much larger models like OpenAI O1 despite having only 7B parameters. The approach demonstrates significant gains across multiple reasoning benchmarks, showing average improvements of ~6% over baseline models.

## Method Summary
Vision-R1 employs a two-phase pipeline to enhance multimodal reasoning. First, it constructs a 200K multimodal Chain-of-Thought dataset through modality bridging, using Qwen2.5-VL-72B to generate pseudo-CoT samples, re-prompting for detailed descriptions, and filtering through DeepSeek-R1 with rule-based consistency checks. Second, it applies Progressive Thinking Suppression Training (PTST) with Group Relative Policy Optimization (GRPO) in the Verl framework. PTST addresses overthinking by gradually suppressing unnecessary reasoning steps through staged training with increasing maximum lengths. The reward function is binary, checking for proper formatting tags and answer correctness. This approach enables effective RL training despite the limited availability of high-quality multimodal reasoning data.

## Key Results
- Vision-R1-7B achieves 73.5% accuracy on MathVista benchmark
- Vision-R1-32B and Vision-R1-72B achieve 76.4% and 78.2% accuracy on MathVista, respectively
- Average performance gain of ~6% across multiple benchmarks (MathVerse, MM-Math, DynaMath) compared to baselines
- Vision-R1-7B matches the performance of much larger models like OpenAI O1

## Why This Works (Mechanism)
The paper tackles the fundamental challenge that direct RL training in MLLMs struggles to generate complex reasoning due to lack of high-quality multimodal data. Vision-R1's mechanism works by first creating a strong initialization through automated construction of multimodal Chain-of-Thought data, then applying targeted RL training with PTST to suppress overthinking while maintaining reasoning quality. The progressive training approach allows the model to learn when to stop reasoning, preventing the degradation that occurs with direct long-sequence RL training.

## Foundational Learning
- Multimodal Chain-of-Thought (CoT) generation: Needed to create training data that combines visual understanding with step-by-step reasoning. Quick check: Verify that generated CoT samples show logical progression from visual input to final answer.
- Reinforcement Learning for reasoning: Required to optimize reasoning capabilities through reward-based feedback. Quick check: Monitor reward stability and accuracy improvements during RL training.
- Progressive Thinking Suppression: Essential for preventing overthinking while maintaining reasoning depth. Quick check: Track CoT length vs. accuracy to ensure suppression doesn't harm performance.

## Architecture Onboarding
- Component map: Qwen2.5-VL → Pseudo-CoT generation → DeepSeek-R1 filtering → Vision-R1-cold → SFT → PTST + GRPO → Vision-R1
- Critical path: Cold-start initialization → SFT → Progressive Thinking Suppression Training → Final model
- Design tradeoffs: Automated dataset construction vs. data quality; progressive training vs. computational efficiency
- Failure signatures: Direct 16K RL causes overthinking with incorrect steps; poor cold-start data yields unstable RL updates
- First experiments: 1) Validate cold-start dataset quality by checking answer alignment rates; 2) Test GRPO reward function implementation with sample outputs; 3) Run PTST stages with varying hyperparameters to confirm optimization stability

## Open Questions the Paper Calls Out
None

## Limitations
- Cold-start dataset construction relies on proprietary Qwen2.5-VL-72B model for initial generation
- Limited evaluation on reasoning domains beyond mathematics and visual problems
- Exact rule-based filtering criteria for dataset construction are not fully specified

## Confidence
- High confidence in the two-phase training pipeline effectiveness
- Medium confidence in PTST methodology contribution
- Medium confidence in specific performance numbers
- Low confidence in exact replication of cold-start dataset

## Next Checks
1. Reconstruct the Vision-R1-cold dataset using open-source MLLMs and document the exact filtering rules that yield 200K samples
2. Verify the GRPO reward function implementation, particularly the bbie/answer tag parsing and correctness checking pipeline
3. Test the PTST stages with varying hyperparameters (ε, β, step counts) to confirm the claimed optimization stability and overthinking suppression effects