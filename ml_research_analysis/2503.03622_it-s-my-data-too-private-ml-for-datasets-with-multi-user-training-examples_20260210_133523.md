---
ver: rpa2
title: 'It''s My Data Too: Private ML for Datasets with Multi-User Training Examples'
arxiv_id: '2503.03622'
source_url: https://arxiv.org/abs/2503.03622
tags:
- examples
- algorithm
- contribution
- privacy
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the multi-attribution model for user-level
  differential privacy, where each training example can be attributed to multiple
  users. The authors define fixed-graph differential privacy for this setting and
  propose contribution bounding algorithms to facilitate private model training.
---

# It's My Data Too: Private ML for Datasets with Multi-User Training Examples

## Quick Facts
- arXiv ID: 2503.03622
- Source URL: https://arxiv.org/abs/2503.03622
- Reference count: 40
- Key outcome: Multi-attribution model with fixed-graph DP, greedy contribution bounding algorithms, and empirical evaluation showing DP-SGD generally outperforms DP-MF while greedy approaches optimal solutions

## Executive Summary
This paper introduces a framework for user-level differential privacy in datasets where training examples can be attributed to multiple users. The authors define fixed-graph differential privacy for multi-attribution settings and develop contribution bounding algorithms to enable private model training. Through experiments on synthetic logistic regression and transformer fine-tuning tasks, they demonstrate that greedy algorithms provide near-optimal solutions while reducing computational complexity, and that DP-SGD with Poisson sampling generally outperforms DP-MF across various privacy budgets.

## Method Summary
The approach reduces multi-attribution user-level DP to standard user-level DP by selecting a subset where each user contributes at most k examples. A greedy algorithm processes examples sorted by edge cardinality, adding each example if no user has reached their contribution limit. This converts the NP-hard optimization problem into a tractable approximation. For training, they use either DP-SGD with Poisson sampling or DP-MF with batch formation constraints. Privacy accounting accounts for group privacy under the contribution bound, with noise scaling inversely with the selected subset size.

## Key Results
- Greedy algorithm retrieves approximately 98.7% of optimal solution value from ILP for contribution bounds k=2,3,4
- Duplication consistently improves DP-SGD performance on arXiv dataset across all ε values tested
- DP-SGD generally outperforms DP-MF, with crossover only at high ε values or for skewed graphs
- Transformer training benefits more from noise reduction than bias mitigation compared to logistic regression

## Why This Works (Mechanism)

### Mechanism 1: Fixed-Graph Differential Privacy Definition
Multi-attribution privacy is defined by protecting all content associated with a single user while keeping the hypergraph structure public. Adjacency requires identical hypergraphs but differing examples for one user across all edges they participate in. This isolates each user's contribution across all edges they participate in, but assumes the hypergraph structure itself doesn't leak sensitive information.

### Mechanism 2: Contribution Bounding as Graph Pruning
The reduction to standard DP requires selecting a subset where each user contributes at most k examples. Greedy algorithms process examples sorted by edge cardinality (fewer users first), converting the NP-hard optimization problem into a tractable O(|E| · |V|) approximation. This works under the assumption that users with fewer co-participants can be processed first without catastrophic loss in solution quality.

### Mechanism 3: Noise Reduction via Duplication Tradeoff
Allowing duplicate examples in the selected subset reduces DP noise at the cost of increased selection bias. Multiple passes over the dataset allow the same example to be counted multiple times toward different users' contribution limits, increasing |S| and reducing σ ∝ 1/|S|, but systematically over-representing examples attributed to fewer users. The utility gain from reduced noise must outweigh the bias introduced by non-uniform sampling.

## Foundational Learning

- **Differential Privacy (ε, δ)-DP basics**: Understanding adjacency definitions and how noise scales with sensitivity is crucial. Quick check: If two datasets differ in all examples involving one user, what sensitivity does each gradient contribute?

- **DP-SGD with Poisson Sampling**: The paper reduces multi-attribution training to DP-SGD after contribution bounding; privacy amplification by subsampling is critical for tight accounting. Quick check: Why does Poisson sampling provide privacy amplification, and how does contribution bound k affect the group privacy accounting?

- **Hypergraphs and Edge Incidence**: The multi-attribution model represents each training example as a hyperedge over user vertices; contribution bounding is fundamentally about vertex degree in this hypergraph. Quick check: Given a hypergraph with edges {A,B}, {A,B,C}, {B,D}, what is user B's degree, and what subset maximizes |S| under k=1?

## Architecture Onboarding

- **Component map**: Attribution module -> Contribution bounding module -> Privacy accounting module -> Training module

- **Critical path**: 
  1. Define attribution (determines privacy semantics)
  2. Choose contribution bound k (controls privacy-utility tradeoff)
  3. Run greedy selection → produces S
  4. Compute noise multiplier via RDP accounting
  5. Train with DP-SGD/DP-MF

- **Design tradeoffs**:
  - DP-SGD vs DP-MF: DP-SGD benefits from amplification; DP-MF may help at high ε or skewed graphs but lacks amplification guarantees
  - Duplicates vs no duplicates: Duplicates reduce noise but introduce bias; empirical choice depends on task sensitivity to bias
  - Contribution bound k: Higher k → larger S but more noise per-user; typically k ∈ {2,3,4}

- **Failure signatures**:
  - Greedy returns very small S: Graph may be highly skewed; consider increasing k or using DP-MF variant
  - Model underperforms non-DP baseline by >50%: Check if bias dominates; try "improved selection" variant
  - ILP solver times out: Expected for k > 3 or large graphs; greedy is ~99% optimal in tested cases

- **First 3 experiments**:
  1. Validate greedy selection quality: Compare |S| from greedy vs ILP on small graph, target ≥95% of optimal
  2. Determine duplicate/no-duplicate threshold: Run with and without duplicates at ε ∈ {1, 10, 100}, identify ε* where crossover occurs
  3. Compare DP-SGD vs DP-MF: At fixed ε, k, plot test loss for both, confirm DP-SGD dominates for low-iteration regimes

## Open Questions the Paper Calls Out

### Open Question 1
Can node-DP algorithms be developed for multi-attribution data that achieve similar accuracy and efficiency to the fixed-graph DP algorithms proposed in this paper? This remains unresolved because node-DP requires bounding contributions in an insensitive way (stable graph pruning), which is algorithmically difficult and current approaches don't scale to ML tasks like gradient averaging.

### Open Question 2
Do algorithms with the two-phase structure (pruning the hypergraph independently of content, then training) admit actionable attacks for realistic datasets? The paper assumes violations of node privacy are unlikely because they require correlations between graph structure and training content, but this assumption remains unverified.

### Open Question 3
What are the tight amplification guarantees for user-level DP-MF with example-level sampling in both single- and multi-attribution settings? Without tight guarantees, it's unclear if DP-MF retains its advantage when amplification is applied in the multi-attribution setting.

## Limitations
- Framework assumes hypergraph structure is public metadata, which may not hold for sensitive communication networks
- Empirical evaluation limited to synthetic logistic regression and transformer fine-tuning on arXiv data
- ILP solver comparisons only feasible for small graphs (k≤3), leaving scalability questions unanswered

## Confidence

- **High Confidence**: Fixed-graph DP definition and privacy guarantees, greedy algorithm approximation ratio, DP-SGD consistently outperforming DP-MF at low ε
- **Medium Confidence**: Optimal contribution bound k=2-3 across tasks, bias-mitigation techniques' effectiveness varying by task
- **Low Confidence**: Duplication benefit crossover point ε*, DP-MF strategy construction and parameter tuning, generalization to non-synthetic multi-attribution datasets

## Next Checks

1. **Graph Structure Sensitivity**: Test greedy performance on power-law degree distributions to identify when approximation ratio degrades below 0.95

2. **Bias Quantification**: Implement the "improved selection" variant and measure the tradeoff between bias reduction and dataset size reduction

3. **Real Attribution Scenarios**: Apply framework to email communication data with actual user attribution, measuring both privacy leakage from hypergraph structure and model utility degradation