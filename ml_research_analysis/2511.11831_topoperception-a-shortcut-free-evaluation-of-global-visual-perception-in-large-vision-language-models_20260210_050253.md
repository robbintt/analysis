---
ver: rpa2
title: 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in
  Large Vision-Language Models'
arxiv_id: '2511.11831'
source_url: https://arxiv.org/abs/2511.11831
tags:
- visual
- global
- image
- topoperception
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopoPerception introduces a benchmark for evaluating the global
  visual perception capabilities of large vision-language models (LVLMs) by leveraging
  topological properties of images, which are invariant to local features and therefore
  immune to local shortcuts. The benchmark employs a fixed multiple-choice question
  format with synthetic images that vary in perceptual granularity, allowing isolation
  of visual perception from reasoning and language generation abilities.
---

# TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models

## Quick Facts
- **arXiv ID**: 2511.11831
- **Source URL**: https://arxiv.org/abs/2511.11831
- **Reference count**: 40
- **Primary result**: State-of-the-art LVLMs perform at or near random chance on global visual perception tasks, with stronger reasoning models showing worse performance.

## Executive Summary
TopoPerception introduces a novel benchmark for evaluating the global visual perception capabilities of Large Vision-Language Models (LVLMs) by leveraging topological properties that are invariant to local features. The benchmark uses synthetic images with varying perceptual granularity and a fixed multiple-choice format to isolate visual perception from reasoning and language generation. Evaluation reveals that even the most powerful LVLMs fail to perceive global visual features, with stronger reasoning models performing worse. This finding suggests a critical bottleneck in current LVLM architectures where the visual perception module constrains overall capabilities, pointing to the need for new training paradigms or architectural innovations.

## Method Summary
TopoPerception evaluates global visual perception using topological properties (number of closed loops in white regions) as invariant features that cannot be inferred from local patterns. The benchmark generates synthetic images via uniform spanning trees on n×n connected graphs, producing (4n+1)×(4n+1) resolution images across 10 difficulty levels (Level 0: 29×29, Level 1: 37×37, etc.). Each level contains 300 images (100 per ground-truth category B, C, D), with distractors A and E. Models answer multiple-choice questions about topological structure using standard APIs with default temperature settings. Accuracy, precision, recall, and F1 scores are computed against 20% random baseline and 33.3% valid-option baseline.

## Key Results
- All evaluated LVLMs, including GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro, perform at or near random chance (~20%) on Level 0 images (29×29 resolution).
- Stronger reasoning models within model families consistently exhibit lower accuracy, suggesting interference between reasoning and visual perception.
- Performance remains near random chance even at the coarsest perceptual granularity, indicating fundamental architectural limitations rather than resolution constraints.

## Why This Works (Mechanism)

### Mechanism 1: Topological Invariance Eliminates Local Shortcuts
- Claim: Topological properties provide a shortcut-free measure of global visual perception because they cannot be inferred from local features alone.
- Mechanism: Topology captures properties preserved under continuous deformation—connectivity, hole count, and interior/exterior relationships. These require integrating information across the entire image structure.
- Core assumption: If a model cannot perceive global structure, it will fail to distinguish topological classes regardless of its reasoning or local feature detection capabilities.
- Break condition: If models develop local feature patterns that correlate with topological class (unlikely given synthetic, abstract nature of stimuli).

### Mechanism 2: Information Loss Through Visual Token Compression
- Claim: The visual encoder-to-LLM pipeline creates an inherent bottleneck that discards global visual features lacking linguistic correlates.
- Mechanism: Visual encoders trained with contrastive objectives (e.g., CLIP) prioritize semantically meaningful features aligned with language descriptions. Global structural features without straightforward linguistic labels are filtered during this compression.
- Core assumption: Global visual features are treated as "unimportant details" because training objectives optimize for description generation rather than perceptual fidelity.
- Break condition: If visual encoders are explicitly trained or fine-tuned to preserve global structural features.

### Mechanism 3: Reasoning-Perception Interference Effect
- Claim: Stronger reasoning capabilities correlate with *worse* global visual perception, suggesting language-based reasoning overrides fragile visual signals.
- Mechanism: Models with enhanced chain-of-thought reasoning may rely more heavily on linguistic priors and parametric knowledge, amplifying noise in weak visual representations rather than correcting them.
- Core assumption: The visual signal in current architectures is too degraded to ground reasoning effectively; "thinking in language" distorts visual understanding.
- Break condition: If architectures incorporate iterative re-examination of visual input during reasoning steps.

## Foundational Learning

- **Concept: Topological Invariance**
  - Why needed here: Understanding why topology provides a rigorous evaluation—it tests whether models retain global structure, not just local patterns.
  - Quick check question: Can you determine if two shapes have the same number of holes by examining only a 3×3 pixel region?

- **Concept: Visual Token Compression Bottleneck**
  - Why needed here: The paper identifies this as the root cause of failure; understanding tokenization tradeoffs is essential for architectural improvements.
  - Quick check question: What structural information is necessarily lost when resizing a rectangular image to fit a 224×224 square encoder?

- **Concept: Local vs. Global Feature Processing**
  - Why needed here: The benchmark specifically tests global perception; many evaluation tasks conflate this with local feature detection.
  - Quick check question: In maze-solving tasks, why is confirming two points are *disconnected* more demanding than finding *one* valid path?

## Architecture Onboarding

- **Component map**: Image → Visual Encoder → Projection Module → LLM Backbone
- **Critical path**: Image → [Resizing/Patching with distortion risk] → Visual Encoder → [Semantic compression] → Token Sequence → Projection → LLM
- **Design tradeoffs**:
  1. Fixed resolution vs. aspect ratio preservation (padding/resizing corrupts global structure)
  2. Token count vs. compute budget (reduction techniques risk discarding global features)
  3. Semantic alignment vs. perceptual fidelity (description-optimized training filters non-linguistic structure)
- **Failure signatures**:
  1. Random-chance accuracy (~20%) on global structure tasks despite strong performance on semantic benchmarks
  2. Stable option bias independent of input image (prediction distribution identical across image categories)
  3. Inverse scaling within model families (reasoning-optimized variants underperform base models)
- **First 3 experiments**:
  1. **Baseline diagnostic**: Run your LVLM on TopoPerception Level 0 (29×29 resolution) to confirm random-chance baseline; check if accuracy exceeds 20%.
  2. **Reasoning ablation**: Compare direct classification vs. chain-of-thought prompting; if CoT degrades accuracy, confirms reasoning-perception interference.
  3. **Granularity threshold**: Test across difficulty levels (0-9) to identify the resolution threshold where global perception fails; correlate with visual encoder's effective receptive field.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of language-based reasoning mechanisms (such as Chain-of-Thought) actively degrade global visual perception in current LVLM architectures?
- Basis in paper: [explicit] The authors observe a consistent trend where "larger models with stronger reasoning capabilities tend to exhibit lower accuracy," hypothesizing that reasoning through natural language may distort or override fragile visual signals.
- Why unresolved: The paper identifies the negative correlation but does not isolate the causal mechanism or determine if the interference occurs during token alignment, attention, or generation.
- What evidence would resolve it: Ablation studies comparing standard prompts against Chain-of-Thought prompts on global tasks, or architectural modifications that isolate visual processing from language reasoning modules.

### Open Question 2
- Question: Can iterative visual encoders or "re-examination" mechanisms restore global visual fidelity in Large Vision-Language Models?
- Basis in paper: [explicit] The conclusion suggests that "mechanisms that allow the model to re-examine the image during reasoning" or more "expressive... encoders" may be necessary to solve the identified bottleneck.
- Why unresolved: Current models use a "stitching" approach with fixed encoders and minimal interfaces, which the paper identifies as insufficient, but no alternative iterative architecture is tested.
- What evidence would resolve it: Designing and evaluating an LVLM architecture with recurrent visual feedback loops to verify if TopoPerception performance improves above random chance.

### Open Question 3
- Question: Do current training objectives (e.g., semantic alignment) necessitate the discarding of global topological features?
- Basis in paper: [inferred] The authors note that training biases lead models to prioritize descriptive features humans highlight, "resulting in the unintentional filtering out of global structures," implying a conflict between semantic and structural objectives.
- Why unresolved: It is unclear if the failure is purely architectural or if the optimization for semantic similarity (e.g., CLIP style alignment) explicitly penalizes the retention of non-semantic global features.
- What evidence would resolve it: Training a model with a combined loss function (semantic + topological) to see if global perception can be improved without losing semantic capabilities.

## Limitations

- The synthetic nature of stimuli may not fully capture real-world global perception challenges, potentially limiting generalizability to practical applications.
- The benchmark focuses exclusively on topological properties, which may not represent all aspects of global visual perception required for complex reasoning tasks.
- The paper doesn't explore whether alternative visual encoder architectures or training objectives could mitigate the identified bottleneck.

## Confidence

**High Confidence:**
- Topological properties are genuinely invariant to local features and provide a valid test of global perception
- Current LVLMs perform at or near random chance on global visual perception tasks
- The visual token compression pipeline creates a bottleneck for global structural information

**Medium Confidence:**
- The inverse relationship between reasoning strength and visual perception accuracy reflects a causal interference effect
- Local shortcuts cannot explain the observed random-chance performance given the synthetic, abstract nature of stimuli
- The bottleneck is primarily due to semantic compression rather than resolution or architectural constraints

**Low Confidence:**
- These findings generalize to all real-world global visual perception tasks
- No current or foreseeable training paradigm can overcome this limitation
- The specific mechanism of reasoning-perception interference is the primary driver of observed patterns

## Next Checks

1. **Architectural Intervention Test**: Evaluate whether fine-tuning visual encoders on global structure preservation tasks improves TopoPerception performance, or if the bottleneck is fundamental to the visual-LLM integration architecture.

2. **Reasoning Pathway Isolation**: Test whether disabling chain-of-thought reasoning or using separate visual and reasoning modules improves global perception accuracy, providing stronger evidence for the reasoning-perception interference hypothesis.

3. **Cross-Domain Validation**: Apply the topological evaluation framework to non-synthetic images with known global structures (architectural layouts, network diagrams) to verify that the observed limitations extend beyond the controlled benchmark setting.