---
ver: rpa2
title: Scalable Policy Maximization Under Network Interference
arxiv_id: '2505.18118'
source_url: https://arxiv.org/abs/2505.18118
tags:
- network
- interference
- algorithm
- reward
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of learning optimal policies in
  networked systems where individuals' outcomes depend on their neighbors' treatments
  - a problem known as network interference. The authors show that under common assumptions
  about interference patterns, the node-level rewards can be expressed as a linear
  function of unknown parameters.
---

# Scalable Policy Maximization Under Network Interference

## Quick Facts
- arXiv ID: 2505.18118
- Source URL: https://arxiv.org/abs/2505.18118
- Authors: Aidan Gleich; Eric Laber; Alexander Volfovsky
- Reference count: 25
- Shows Thompson sampling algorithm can scale to networks orders of magnitude larger than existing methods

## Executive Summary
This paper addresses the challenge of learning optimal policies in networked systems where individuals' outcomes depend on their neighbors' treatments - a problem known as network interference. The authors demonstrate that under common assumptions about interference patterns, node-level rewards can be expressed as linear functions of unknown parameters. This insight enables development of a Thompson sampling algorithm that scales to large networks while maintaining theoretical guarantees. The approach closes a critical scalability gap between causal inference methods for interference and practical bandit algorithms.

## Method Summary
The paper develops a Thompson sampling algorithm for policy optimization under network interference by leveraging the observation that rewards can be linearly parameterized when interference follows specific structural assumptions. The algorithm maintains a posterior distribution over the unknown linear parameters and selects treatment vectors to maximize expected rewards. The method scales to networks orders of magnitude larger than existing approaches while providing sublinear Bayesian regret bounds in both network size and time horizon. Empirical validation demonstrates rapid learning and strong performance even when theoretical assumptions are partially violated.

## Key Results
- Thompson sampling algorithm scales to networks orders of magnitude larger than existing methods
- Maintains sublinear Bayesian regret bounds in both network size and time horizon
- Outperforms existing approaches empirically, even when interference assumptions are violated
- Demonstrates practical applicability for policy optimization in large-scale networked systems

## Why This Works (Mechanism)
The key mechanism is the linear parameterization of rewards under network interference. When interference follows specific structural assumptions (such as local dependence or low-rank structures), the complex dependencies between neighboring nodes' treatments and outcomes can be captured by a linear function of unknown parameters. This allows Thompson sampling to efficiently explore the parameter space while maintaining uncertainty quantification through posterior distributions.

## Foundational Learning
1. **Network interference patterns** - Understanding how individuals' outcomes depend on neighbors' treatments
   - Why needed: Core problem setting for the algorithm
   - Quick check: Can identify direct vs. indirect treatment effects

2. **Linear parameterization under interference** - Conditions under which network effects can be expressed linearly
   - Why needed: Enables tractable learning algorithms
   - Quick check: Verify assumption holds for specific network structures

3. **Thompson sampling in contextual bandits** - Bayesian approach to balancing exploration and exploitation
   - Why needed: Algorithm framework for parameter estimation and policy optimization
   - Quick check: Can derive posterior updates for simple bandit problems

## Architecture Onboarding

Component map: Observation data -> Parameter estimation -> Posterior sampling -> Treatment selection -> Reward feedback

Critical path: The algorithm operates by first observing network structure and initial outcomes, then iteratively estimating linear parameters from observed data, sampling from the posterior distribution over these parameters, selecting treatment vectors that maximize expected rewards under the sampled parameters, and receiving feedback to update the posterior.

Design tradeoffs: The linear assumption enables scalability but may limit expressiveness for complex interference patterns. Thompson sampling provides uncertainty quantification but requires maintaining posterior distributions, which can be computationally intensive for very large parameter spaces.

Failure signatures: Poor performance when interference patterns deviate significantly from linear assumptions, slow convergence in networks with highly heterogeneous connectivity patterns, or computational bottlenecks when posterior updates become expensive.

First experiments: 1) Test on synthetic networks with known linear interference structure, 2) Evaluate performance degradation as linearity assumptions are relaxed, 3) Benchmark against non-Bayesian approaches on medium-scale networks

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees depend heavily on validity of linear parameterization assumptions
- Regret bounds may not capture worst-case scenarios in highly non-linear or adversarial network settings
- Robustness to different network topologies and interference patterns beyond tested cases remains unclear

## Confidence
- High Confidence: Linear parameterization of rewards under network interference (supported by both theoretical analysis and empirical validation)
- Medium Confidence: Thompson sampling algorithm's performance on large-scale networks (empirical results are promising but limited to specific network types)
- Medium Confidence: Sublinear regret bounds (theoretical, but dependent on assumption validity)

## Next Checks
1. Test algorithm performance on networks with non-linear interference patterns and varying degrees of homophily to assess robustness beyond linear assumptions

2. Evaluate computational scalability on networks with >100,000 nodes to verify practical applicability at extreme scales

3. Conduct ablation studies comparing the Thompson sampling approach against simpler heuristics when network interference assumptions are partially violated