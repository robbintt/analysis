---
ver: rpa2
title: 'Executable Counterfactuals: Improving LLMs'' Causal Reasoning Through Code'
arxiv_id: '2510.01539'
source_url: https://arxiv.org/abs/2510.01539
tags:
- reasoning
- counterfactual
- code
- causal
- counterfactuals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces executable counterfactuals, a novel framework
  that operationalizes causal reasoning through code and math problems, explicitly
  requiring all three steps: abduction, intervention, and prediction. This approach
  addresses the overestimation of LLM performance in prior work that conflated counterfactual
  reasoning with simpler interventional reasoning by skipping the abduction step.'
---

# Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code

## Quick Facts
- arXiv ID: 2510.01539
- Source URL: https://arxiv.org/abs/2510.01539
- Authors: Aniket Vashishtha; Qirun Dai; Hongyuan Mei; Amit Sharma; Chenhao Tan; Hao Peng
- Reference count: 40
- Primary result: RL outperforms SFT for generalizable counterfactual reasoning, yielding 1.5-2x accuracy gains on both code and math problems

## Executive Summary
This paper introduces executable counterfactuals, a framework that operationalizes causal reasoning through code and math problems requiring all three steps of Pearl's causal ladder: abduction, intervention, and prediction. The authors demonstrate that prior LLM evaluations overestimated performance by conflating interventional reasoning with counterfactual reasoning, missing the critical abduction step. Through systematic experiments, they show that while supervised fine-tuning improves in-domain performance, reinforcement learning with verifiable rewards (RLVR) is necessary to induce generalizable cognitive strategies that transfer across domains and program structures.

## Method Summary
The framework generates synthetic code problems with hidden latent variables that require abduction (inferring hidden state from observations), intervention (changing inputs), and prediction (computing new outputs). Training data uses if-else templates with configurable placeholders for structural, value, and fixed components. The authors compare supervised fine-tuning (SFT) on reasoning traces versus reinforcement learning with exact-match rewards (RLVR) using GRPO. Evaluation spans in-domain code structures and out-of-distribution variants including while-loops, multiple latent variables, and counterfactual math word problems.

## Key Results
- LLMs show 25-40% performance drop from interventional to counterfactual reasoning when abduction is required
- SFT improves in-domain accuracy but fails to generalize to out-of-distribution code structures and math problems
- RLVR consistently induces core cognitive behaviors and generalizes to new domains, achieving 1.5-2x accuracy gains
- RLVR models acquire transferable counterfactual reasoning strategies while SFT leads to memorization of surface patterns

## Why This Works (Mechanism)

### Mechanism 1: Latent Variable Inference via Abduction
- Claim: Enforcing an explicit abduction step before intervention exposes a fundamental gap in LLM causal reasoning that prior benchmarks masked
- Mechanism: By introducing hidden variables that are not revealed to the model, the task requires inferring latent state from observed output before any counterfactual prediction can proceed
- Core assumption: The model has sufficient symbolic reasoning capacity to perform backward inference from output to input given a known functional structure
- Evidence anchors:
  - [abstract]: "existing efforts in assessing LLM's counterfactual reasoning capabilities tend to skip the abduction step, effectively reducing to interventional reasoning"
  - [section 2]: "Without latent states and the abduction step, counterfactual reasoning reduces to interventional reasoning, corresponding to Level 2 in Pearl's causal ladder"
  - [corpus]: Related work (CounterBench arXiv:2502.11008) notes prior benchmarks focus on commonsense causality without formal latent variable structures
- Break condition: If the hidden variable is fully specified (no abduction needed), or if the function is deterministic with no noise/latency, the mechanism collapses to interventional reasoning

### Mechanism 2: RL Induces Generalizable Cognitive Strategies While SFT Memorizes Surface Patterns
- Claim: Reinforcement learning from verifiable rewards (RLVR) elicits transferable abduction-intervention-prediction strategies, whereas SFT distillation memorizes domain-specific reasoning traces without acquiring the underlying cognitive skill
- Mechanism: RLVR provides outcome-based feedback that incentivizes the model to discover efficient strategies for solving the three-step counterfactual process
- Core assumption: The reward signal is sufficiently sparse and the task distribution broad enough that memorization is less efficient than learning a general strategy
- Evidence anchors:
  - [abstract]: "RL induces the core cognitive behaviors and generalizes to new domains, yielding gains over the base model on both code (improvement of 1.5x-2x) and math problems"
  - [section 4.2-4.3]: SFT improves in-domain but "leads to a decrease in accuracy on OOD tasks"; RLVR achieves "consistent and significant gains for all scales of models, and on all ID and OOD evaluation datasets"
  - [section 5]: "RLVR models achieve the highest planning scores across all evaluation datasets, demonstrating the generalizable counterfactual reasoning strategy"
  - [corpus]: Limited direct corpus evidence on RL vs. SFT for counterfactuals specifically; related work (Chu et al. 2025 referenced in paper) supports generalization claims for RL
- Break condition: If training data is too narrow, reward too dense, or teacher reasoning traces encode the full generalizable strategy, the distinction may collapse

### Mechanism 3: Executable Programs as Grounded Causal Models
- Claim: Code provides an unambiguous, executable representation of structural causal models where latent variables map to hidden inputs and causal structure maps to control flow
- Mechanism: Python functions with conditional branching and loops implement computational graphs that can be systematically varied
- Core assumption: Program execution traces are sufficient proxies for causal mechanism execution, and model understanding of code transfers to analogous reasoning in natural language
- Evidence anchors:
  - [abstract]: "our code-based framework...enables scalable synthetic data generation and out-of-distribution evaluation through varied program structures"
  - [section 3.1]: "programs are computational graphs, they map naturally onto mathematical and graph formalisms and enable fine-grained control of task difficulty and latent-variable structures"
  - [section 4.3]: RLVR model trained only on code "generalizes to counterfactual math problems in natural language"
  - [corpus]: Related work (Causal Cartographer arXiv:2505.14396) explores counterfactual world models but without the executable code grounding
- Break condition: If code structures used in training and test are too similar, or if natural language analogies fail to map to learned code patterns, cross-domain transfer will not occur

## Foundational Learning

- Concept: **Pearl's Causal Ladder (Levels 1-3)**
  - Why needed here: The entire framework depends on distinguishing associational (Level 1), interventional (Level 2), and counterfactual (Level 3) reasoning
  - Quick check question: Given "If the sprinkler had been off, would the grass be dry?"â€”is this interventional or counterfactual, and what latent information must be inferred if the grass is currently wet?

- Concept: **Structural Causal Models (SCMs) with Latent Variables**
  - Why needed here: The framework operationalizes SCMs as executable code where noise/latent variables must be inferred from observations before counterfactual prediction
  - Quick check question: In the function `f(r, x)` where `r` is hidden, what information is needed to predict `f(r, x=3)` given that `f(r, x=1) = -1` was observed?

- Concept: **Verifiable Rewards in RL (RLVR)**
  - Why needed here: The paper's RL experiments depend on exact-match rewards derived from ground-truth program execution
  - Quick check question: Why might outcome-based rewards (exact match) induce more generalizable strategies than process-based supervision (reasoning trace imitation)?

## Architecture Onboarding

- Component map:
  - Template-based code generator -> Counterfactual query constructor -> Interventional baseline generator -> Evaluation harness -> SFT pipeline OR RLVR training loop
  - Training data -> SFT (teacher distillation) OR RLVR (GRPO with exact match reward) -> In-domain and OOD evaluation

- Critical path:
  1. Define function templates with latent variable placeholders (start with if-else)
  2. Generate training problems with hidden r, observed (x, y), and counterfactual query about new x'
  3. Verify ground truth by executing function across possible r values consistent with observation
  4. Train with RLVR (preferred for generalization) or SFT (for in-domain speed)
  5. Evaluate on OOD structures (while, multi_r, math word problems)

- Design tradeoffs:
  - Modulo ambiguity vs. unique answers: Adding mod m to return creates many-to-one mappings (multiple valid counterfactual answers), increasing task realism but complicating evaluation
  - Template diversity vs. control: More structural placeholders increase diversity but make difficulty harder to calibrate
  - SFT vs. RLVR: SFT converges faster in-domain; RLVR requires more compute but generalizes OOD
  - Model scale: 7B RLVR can match 72B base model, but 1.5B RLVR still struggles on complex OOD structures

- Failure signatures:
  - Brute-force enumeration: Model lists all possible r values rather than inferring from observation
  - Arbitrary assumption: Model picks a single r without justification when reasoning becomes complex
  - Case-splitting explosion: Model creates unnecessary conditional branches, losing coherence
  - SFT memorization: Strong in-domain performance with sharp drops on OOD causal structures (while, multi_r, math)

- First 3 experiments:
  1. Establish baseline gap: Run SOTA models on matched counterfactual vs. interventional pairs to quantify the abduction penalty (expect 25-40% drop)
  2. Test SFT generalization: Fine-tune Qwen-3B on if-else counterfactual traces, then evaluate on while and math OOD sets to confirm memorization hypothesis
  3. Validate RLVR transfer: Train Qwen-3B with RLVR on if-else only, then evaluate on all OOD splits and math to confirm cross-domain strategy acquisition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increased model scale improve execution accuracy but fail to improve the planning (abduction) step in counterfactual reasoning?
- Basis in paper: Page 8 states that scaling up Qwen models from 1.5B to 72B leads to consistent improvements in execution ratings but not planning, with the 7B model often outperforming 32B/72B models on abduction tasks
- Why unresolved: The paper identifies this asynchronism but does not investigate if the failure is due to data distribution, architecture limitations, or the loss functions used during pre-training
- What evidence would resolve it: A mechanistic analysis of attention heads or circuit formation specifically during the abduction phase across different model scales

### Open Question 2
- Question: How can reinforcement learning (RL) be optimized to improve both planning strategy and computational execution simultaneously?
- Basis in paper: Page 9 concludes that while RLVR successfully generalizes planning strategies, it remains "bottlenecked by computational accuracy," explicitly calling for future efforts to address this
- Why unresolved: The current RL setup uses outcome-based rewards (exact match), which may encourage correct high-level strategies without penalizing intermediate procedural errors
- What evidence would resolve it: Developing a training framework that utilizes process-based rewards or auxiliary losses to verify intermediate computational steps during the prediction phase

### Open Question 3
- Question: Can counterfactual reasoning skills acquired through executable code transfer to non-executable, high-stakes domains like healthcare?
- Basis in paper: The introduction (Page 1) motivates the work by citing healthcare and scientific research, but the experiments only validate transfer from code to counterfactual math problems
- Why unresolved: The framework relies on "executable counterfactuals" with verifiable ground truth, a condition absent in the natural language, ambiguous scenarios typical of the cited high-stakes domains
- What evidence would resolve it: Evaluating models trained on executable code against a benchmark of expert-verified counterfactual reasoning tasks in clinical diagnosis or policy analysis

## Limitations
- Framework relies on synthetic data and exact-match rewards, limiting applicability to domains without clear ground truth
- Modularity of code structures as causal models may not capture the complexity of natural language counterfactuals
- Generalization results are primarily demonstrated within controlled template variations rather than diverse real-world scenarios

## Confidence
- High confidence: Adding abduction steps significantly increases task difficulty and exposes LLM limitations in counterfactual reasoning
- Medium confidence: RLVR is more effective than SFT for generalization, dependent on specific template structures
- Medium confidence: Cross-domain transfer from code to math problems is valid, though structural similarity is assumed

## Next Checks
1. Test RLVR-trained models on entirely new domains (e.g., legal reasoning, medical diagnosis) to verify generalization beyond code and math templates
2. Conduct ablation studies removing the modulo operation to create unique-answer scenarios, measuring impact on reasoning strategy and generalization
3. Evaluate model performance on natural language counterfactuals with ambiguous or incomplete information to assess robustness beyond deterministic code execution