---
ver: rpa2
title: 'MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning
  With Ensemble Vision Language Models'
arxiv_id: '2507.11114'
source_url: https://arxiv.org/abs/2507.11114
tags:
- reasoning
- gemini
- multilingual
- multimodal
- flash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an ensemble-based system for multilingual multimodal
  reasoning designed for the ImageCLEF 2025 EXAMS-V challenge. The system integrates
  Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement and
  consistency checks, and Gemini 2.5 Pro for final answer selection, all coordinated
  through carefully engineered prompts.
---

# MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models

## Quick Facts
- arXiv ID: 2507.11114
- Source URL: https://arxiv.org/abs/2507.11114
- Reference count: 21
- Primary result: First place in ImageCLEF 2025 EXAMS-V multilingual multimodal reasoning challenge (81.4% overall accuracy, 11/13 language tracks)

## Executive Summary
This paper presents an ensemble-based system for multilingual multimodal reasoning designed for the ImageCLEF 2025 EXAMS-V challenge. The system combines Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement and consistency checks, and Gemini 2.5 Pro for final answer selection, coordinated through carefully engineered prompts. Through extensive ablation studies and multilingual augmentation, the approach achieves state-of-the-art performance across multiple languages, demonstrating that lightweight OCR-VLM ensembles with precise prompt strategies can outperform heavier end-to-end models in educational settings.

## Method Summary
The system employs a three-stage ensemble architecture where Gemini 2.5 Flash generates initial visual descriptions from input images, Gemini 1.5 Pro refines these captions and performs consistency checks, and Gemini 2.5 Pro selects the final answers. The methodology includes training several large language models (Gemini 2.5 Flash, Phi-4, Gemma-3, Mistral) on both English and multilingual augmented datasets, while also evaluating Gemini 2.5 Flash in a zero-shot setting. Critical to the approach is prompt engineering with language-normalized formats that boost accuracy from 55.9% to 61.7% on the English validation set. The multilingual augmentation strategy involves training on translated versions of the English dataset to improve cross-lingual performance.

## Key Results
- Achieved 81.4% overall accuracy on the ImageCLEF 2025 EXAMS-V challenge, securing first place
- Led 11 out of 13 individual language tracks, with top results including 95.07% for Croatian and 92.12% for Italian
- Zero-shot Gemini 2.5 Flash substantially outperformed trained models in ablation studies
- Language-normalized prompt design improved English validation accuracy from 55.9% to 61.7%
- Multilingual augmentation significantly enhanced performance across all target languages

## Why This Works (Mechanism)
The system's success stems from the strategic combination of specialized vision-language models in a cascading ensemble, where each model handles a specific reasoning stage. Gemini 2.5 Flash's strength in rapid visual description processing is complemented by Gemini 1.5 Pro's refinement capabilities and Gemini 2.5 Pro's answer selection precision. The prompt engineering approach standardizes input formats across languages, reducing model confusion and improving consistency. Multilingual augmentation expands the training data distribution, allowing models to generalize better across language-specific patterns and visual reasoning challenges.

## Foundational Learning
- **Multimodal reasoning**: Understanding how vision-language models process combined visual and textual information; needed to design effective ensemble workflows and evaluate cross-modal performance
- **Prompt engineering**: Crafting specific input formats and instructions to guide model behavior; required for achieving consistent performance across different languages and reasoning tasks
- **Zero-shot learning**: Evaluating model performance without task-specific fine-tuning; demonstrates model generalization capabilities and reduces training complexity
- **Multilingual augmentation**: Expanding training data through translation and cross-lingual techniques; addresses language-specific biases and improves generalization across diverse linguistic contexts
- **Ensemble methods**: Combining multiple models to achieve superior performance; leverages complementary strengths of different model architectures and capabilities
- **OCR-VLM integration**: Combining optical character recognition with vision-language models; essential for processing educational content that often includes text within images

## Architecture Onboarding
- **Component map**: Input images → Gemini 2.5 Flash (visual description) → Gemini 1.5 Pro (caption refinement/consistency check) → Gemini 2.5 Pro (final answer selection) → Output
- **Critical path**: Visual description generation → Caption refinement → Answer selection, where each stage must complete successfully for final accuracy
- **Design tradeoffs**: Lightweight ensemble vs. end-to-end models, prompt complexity vs. model generalization, multilingual training vs. domain-specific accuracy
- **Failure signatures**: Poor visual descriptions lead to cascading errors, inconsistent captions reduce answer quality, prompt formatting issues cause model confusion across languages
- **First experiments**: 1) Test zero-shot Gemini 2.5 Flash on validation set to establish baseline performance, 2) Evaluate prompt engineering impact by comparing language-normalized vs. raw prompts, 3) Assess multilingual augmentation by training on English-only vs. multilingual datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Performance advantage strongly tied to prompt engineering rather than architectural innovation, with no comparison to alternative ensemble strategies
- Evaluation relies entirely on single competition leaderboard scores without independent test set verification
- Claims about lightweight ensembles outperforming heavier models lack direct comparative evidence and clear definitions
- Multilingual augmentation benefits not validated through error analysis or translation quality assessment

## Confidence
- High confidence: Leaderboard performance (81.4% accuracy, first place in 11/13 tracks) is verifiable through official competition results
- Medium confidence: Zero-shot model superiority claims lack statistical significance testing; multilingual gains not analyzed for translation quality issues
- Low confidence: Assertions about lightweight vs. heavy model performance are unsubstantiated; prompt impact may be overstated without proper control comparisons

## Next Checks
1. Conduct statistical significance testing on zero-shot vs. trained model performance differences across multiple runs
2. Test alternative ensemble strategies (weighted voting, cascading models) to evaluate optimal configuration
3. Perform detailed error analysis across language tracks to understand accuracy gains and failure modes