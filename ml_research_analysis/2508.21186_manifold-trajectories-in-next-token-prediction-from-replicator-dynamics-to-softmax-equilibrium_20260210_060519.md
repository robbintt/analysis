---
ver: rpa2
title: 'Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to
  Softmax Equilibrium'
arxiv_id: '2508.21186'
source_url: https://arxiv.org/abs/2508.21186
tags:
- replicator
- simplex
- temperature
- manifold
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the conceptual question of what the decoding
  step in large language models looks like as a dynamical process, specifically whether
  the intuition of "manifold traversal" during decoding can be formalized as a theorem.
  The core method idea is to model next-token decoding as a constrained variational
  problem on the probability simplex, where the softmax output distribution is the
  unique maximizer of a free energy objective.
---

# Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium

## Quick Facts
- arXiv ID: 2508.21186
- Source URL: https://arxiv.org/abs/2508.21186
- Reference count: 17
- Primary result: Formalizes "manifold traversal" intuition as a theorem: next-token distribution follows a smooth trajectory inside the open simplex converging to softmax equilibrium

## Executive Summary
This paper addresses the conceptual question of what the decoding step in large language models looks like as a dynamical process, specifically whether the intuition of "manifold traversal" during decoding can be formalized as a theorem. The paper models next-token decoding as a constrained variational problem on the probability simplex, where the softmax output distribution is the unique maximizer of a free energy objective. The discrete, normalization-respecting ascent is the classical multiplicative-weights (entropic mirror) update, and its continuous-time limit is the replicator flow on the simplex. The primary result is a manifold-traversal theorem: for fixed context and temperature, the next-token distribution follows a smooth trajectory inside the open simplex and converges to the softmax equilibrium.

## Method Summary
The paper formulates next-token prediction as a constrained variational problem on the probability simplex, where the softmax output distribution is the unique maximizer of a free energy objective. The discrete, normalization-respecting ascent is the classical multiplicative-weights (entropic mirror) update, and its continuous-time limit is the replicator flow on the simplex. The primary result is a manifold-traversal theorem: for fixed context and temperature, the next-token distribution follows a smooth trajectory inside the open simplex and converges to the softmax equilibrium. This formalizes the "manifold traversal" intuition as a theorem at the output-distribution level. The paper also outlines a controlled account of path-dependent score adjustments and their connection to loop-like, hallucination-style behavior.

## Key Results
- The next-token distribution follows a smooth C¹ trajectory inside the open simplex and converges to the softmax equilibrium
- Temperature acts as an exact rescaling of time along the same trajectory
- Top-k/nucleus sampling restricts the flow to a face with identical convergence guarantees

## Why This Works (Mechanism)
The mechanism works because the softmax output distribution is the unique maximizer of a free energy objective on the probability simplex. The discrete multiplicative-weights update respects normalization and, in continuous time, becomes the replicator flow. This flow is gradient-like with respect to the free energy, ensuring smooth trajectories that converge to equilibrium. Temperature rescales the time parameter along this trajectory, while constrained sampling methods (top-k, nucleus) simply restrict the flow to lower-dimensional faces of the simplex without breaking the underlying gradient structure.

## Foundational Learning
- **Replicator dynamics**: Differential equations describing how probabilities evolve proportionally to their fitness advantage; needed to understand the continuous-time limit of softmax; quick check: verify the replicator equation preserves the simplex constraint
- **Mirror descent**: Optimization algorithm using Bregman divergences; needed to connect multiplicative updates to constrained optimization; quick check: confirm entropy is the mirror map for the simplex
- **Lyapunov functions**: Scalar functions that decrease along trajectories; needed to prove convergence of the replicator flow; quick check: verify free energy is strictly convex and bounded below on the simplex
- **Free energy minimization**: Variational principle where softmax maximizes entropy subject to expected score constraints; needed to establish the optimization foundation; quick check: confirm the Lagrangian has unique interior solution
- **Face restriction**: Constrained optimization on lower-dimensional subsets of the simplex; needed for top-k/nucleus sampling analysis; quick check: verify the replicator flow stays on the selected face
- **Continuous-time limits**: Mathematical framework for passing from discrete updates to differential equations; needed to establish smooth manifold trajectories; quick check: confirm the Euler discretization recovers the discrete update

## Architecture Onboarding

**Component map**: Context scores -> Free energy objective -> Replicator flow on simplex -> Output distribution

**Critical path**: Input context embedding -> Score function computation -> Free energy maximization -> Replicator dynamics -> Output probability distribution

**Design tradeoffs**: Continuous-time theory assumes infinitesimal step sizes, while production decoders use finite steps; theoretical framework is output-distribution focused, while actual models operate in hidden state space

**Failure signatures**: Non-convergence to equilibrium (suggesting incorrect dynamics or score function pathologies); trajectory leaving the simplex (indicating numerical instability or constraint violation); temperature-dependent behavior that doesn't match theoretical scaling

**3 first experiments**:
1. Track output distribution trajectories across multiple decoding steps for fixed context, measuring smoothness and convergence rate
2. Test temperature scaling by measuring whether doubling temperature halves the convergence time constant
3. Apply top-k sampling at different values and verify the flow remains on the selected face with preserved convergence properties

## Open Questions the Paper Calls Out

### Open Question 1
Under what precise conditions do path-dependent score adjustments s(p) preserve the convergence guarantees of Theorem 4.1, and when do they produce nonconservative dynamics with limit cycles? The paper only sketches conditions (Lipschitz dependence, potential vs. non-potential fields) without proving which practical decoding heuristics yield conservative vs. nonconservative dynamics.

### Open Question 2
Can internal hidden-state dynamics be rigorously linked to the output-level replicator flow through a projection mechanism with preserved Lyapunov structure? The paper restricts analysis to the output distribution; the hidden-state-to-simplex projection is described only schematically.

### Open Question 3
Do empirical decoding trajectories in production LLMs follow the predicted C¹ manifold paths, and what systematic deviations occur from the idealized replicator flow? The continuous-time limit is a theoretical construct; actual discrete decoding with finite step sizes may exhibit different behavior.

## Limitations
- Analysis is restricted to the output distribution level, not hidden state dynamics
- Continuous-time limit assumes infinitesimal step sizes, not applicable to discrete production decoders
- Path-dependent score adjustments are only sketched without rigorous analysis

## Confidence
High: Manifold traversal theorem and temperature scaling results
Medium: Top-k/nucleus sampling face restriction guarantees
Low: Path-dependent score function analysis and hidden state connection

## Next Checks
1. Empirically verify that output distribution trajectories are C¹ smooth and converge to softmax equilibrium across multiple decoding steps
2. Test temperature scaling by measuring convergence time constants at different temperatures
3. Validate that top-k and nucleus sampling restrict trajectories to the selected face while preserving convergence guarantees