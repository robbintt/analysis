---
ver: rpa2
title: 'TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar'
arxiv_id: '2510.14972'
source_url: https://arxiv.org/abs/2510.14972
tags:
- rewrite
- code
- rules
- llms
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how subword tokenization misalignment impacts
  code LLM performance. The authors introduce TOKDRIFT, a framework applying semantic-preserving
  rewrite rules (e.g., spacing changes, identifier casing modifications) to create
  code variants with different tokenizations but identical semantics.
---

# TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar

## Quick Facts
- arXiv ID: 2510.14972
- Source URL: https://arxiv.org/abs/2510.14972
- Reference count: 40
- Key outcome: Code LLMs show significant sensitivity to subword tokenization misalignment, with up to 6.09% output changes from minor formatting changes.

## Executive Summary
This study reveals a critical vulnerability in code language models: their subword tokenization systems fundamentally misalign with programming language grammar, causing semantically equivalent code to be processed differently. The TOKDRIFT framework systematically applies semantic-preserving rewrite rules to create code variants with different tokenizations but identical functionality. Across nine large code models and three tasks, even minor formatting changes like spacing modifications or identifier casing changes caused substantial output shifts. The most sensitive models showed up to 6.09% output changes when tokenization changed, with some rules causing 60% shifts in specific cases. Layer-wise analysis traced the issue to early embedding layers where subword segmentation fails to capture grammar token boundaries, with larger models showing greater but not complete robustness.

## Method Summary
TOKDRIFT applies 24 semantic-preserving rewrite rules (6 naming conventions and 18 spacing modifications) to create code variants with different tokenizations but identical semantics. The framework evaluates nine instruction-tuned code LLMs (Llama-3, Qwen2.5-Coder, DeepSeek-Coder) on eight benchmarks covering bug fixing, code summarization, and translation tasks. Sensitivity is measured as the percentage of samples where output correctness flips after applying rewrite rules. The evaluation uses greedy decoding, functional correctness testing, and layer-wise hidden state similarity analysis to understand the mechanism. Code is available at https://github.com/uw-swag/tokdrift.

## Key Results
- Minor formatting changes caused substantial output shifts in code LLMs, with sensitivity reaching up to 6.09% for the most affected models
- Layer-wise analysis showed hidden state similarity near zero in early layers after rewrite rules, indicating tokenization misalignment originates in embeddings
- Larger models were more robust overall, but all models exhibited non-negligible sensitivity (5.71-9.26% average across tasks)
- Spacing rewrite rules generally caused higher sensitivity than naming rules, with some specific patterns showing 60% output changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical tokenization fundamentally misaligns with code syntax and semantics, causing semantically identical code to be processed differently
- Mechanism: BPE tokenizers use frequency-based merging on text-and-code corpus, ignoring grammar rules. Semantic-preserving changes (e.g., adding space in `. factorial`) force different token sequences (e.g., [`.factor`, `ial`] to [` .`, `␣factorial`]), altering initial embeddings and propagating through model
- Core assumption: Tokenizer vocabulary and merge rules are fixed during inference and cannot adapt to syntactic context
- Evidence anchors:
  - [abstract] "Semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming"
  - [Section 1, Figure 1b] Adding space causes significant token sequence change and LLM prediction shift
  - [corpus] Related work like "SuperBPE" and "CodeBPE" investigates subtokenization options

### Mechanism 2
- Claim: Tokenization misalignment originates in embedding layer and degrades internal semantic representation
- Mechanism: Altered token sequence results in different initial embeddings. Layer-wise analysis shows hidden state similarity near zero in first layer after rewrite, dropping again at output layer despite recovery in middle layers
- Core assumption: Early layer embeddings have lasting impact on final output for code tasks
- Evidence anchors:
  - [abstract] "Issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries"
  - [Section 5.2, Figure 4] Similarity starts from almost 0 in first layer for naming and spacing rules
  - [corpus] Weak direct evidence for this propagation mechanism in code

### Mechanism 3
- Claim: Frequency of token patterns in pre-training corpus correlates with model sensitivity
- Mechanism: Models more sensitive to rewritten token sequences if those patterns are rare in pre-training data. Rare sequences (low RHS/LHS ratio) correspond to higher flip rates due to weaker representation
- Core assumption: GitHub frequency reflects actual pre-training data frequency
- Evidence anchors:
  - [Section 5.1, Table 7] Rewrite rules producing rare token sequences correspond to higher model sensitivity
  - [corpus] Weak direct evidence linking specific pre-training corpora to sensitivity

## Foundational Learning

- **Concept**: Subword Tokenization (BPE/WordPiece)
  - Why needed here: Entire paper premise is that statistical method creates fundamental mismatch with programming language grammar
  - Quick check question: How does BPE tokenizer decide to merge characters, and why might it split `sortedList` into `[sorted, List]` or `[sort, ed, Li, st]`?

- **Concept**: Semantic Equivalence in Code
  - Why needed here: TOKDRIFT framework built on applying "semantic-preserving rewrite rules" - isolating impact of tokenization by changing form without changing function
  - Quick check question: In Python, does adding space between `return` and `42` change program behavior? What about adding space between `.` and `factorial`?

- **Concept**: Model Robustness / Sensitivity
  - Why needed here: Paper introduces "sensitivity" as core metric to quantify problem
  - Quick check question: If code LLM fixes bug correctly in original code but fails after renaming variable from `user_count` to `UserCount`, would you consider it robust?

## Architecture Onboarding

- **Component map**: Benchmarks -> Semantic-Preserving Rewriter -> Variant Input -> LLM Tokenizer -> LLM -> Generated Code -> Evaluator

- **Critical path**: Benchmark Input -> (Apply Rewrite Rule) -> Variant Input -> LLM Tokenizer -> LLM -> Generated Code -> Evaluator

- **Design tradeoffs**: Framework prioritizes automated, large-scale evaluation across many models and rules, limiting semantic complexity of rewrites. Relies on functional correctness as primary signal, not deeper code structure analysis.

- **Failure signatures**: "Flip" in functional correctness after simple rewrite (e.g., from pass to fail). High `sensitivity` metric (>5%) across many samples indicates non-robust model.

- **First 3 experiments**:
  1. **Baseline Establishment**: Run selected code LLM on all original benchmark inputs, record baseline accuracy
  2. **Single-Rule Sensitivity Test**: For target model and single rewrite rule (e.g., N1: camelCase→snake_case), apply rule to all applicable inputs, run model, compute sensitivity metric
  3. **Sensitivity Profiling**: Repeat single-rule test for all 24 rules on single model to create sensitivity profile, identifying most problematic tokenization patterns

## Open Questions the Paper Calls Out

- **Open Question 1**: Does tokenization sensitivity in Transformer-based code LLMs persist in non-Transformer architectures like State Space Models?
  - Basis in paper: [explicit] Authors list "models with fundamentally different architectures (e.g., state space models)" as limitation
  - Why unresolved: Empirical study restricted to Transformer architectures
  - What evidence would resolve it: TOKDRIFT evaluation on SSM-based code models comparing sensitivity metrics against Transformers

- **Open Question 2**: Can ensemble decoding over multiple tokenizations effectively mitigate prediction drift without prohibitive computational costs?
  - Basis in paper: [explicit] Limitations section suggests "ensemble decoding over multiple tokenizations" as potential mitigation strategy not explored
  - Why unresolved: Paper focuses on diagnosing and measuring misalignment rather than implementing defense mechanisms
  - What evidence would resolve it: Implementation of multi-tokenization voting during inference measuring reduction in output variance vs baseline latency increase

- **Open Question 3**: How does strictly enforcing grammar-aware tokenization impact model's robustness to natural noise and varied coding styles in real-world repositories?
  - Basis in paper: [inferred] Authors conclude highlighting "need for grammar-aware tokenization" but primarily test clean semantic rewrites
  - Why unresolved: Trade-off between grammatical precision and flexibility for noisy inputs not assessed
  - What evidence would resolve it: Comparison of BPE-based vs grammar-aware tokenizers on adversarial or obfuscated code benchmarks

## Limitations

- Artificial nature of rewrite rules focuses on superficial formatting changes that may not represent real-world code variation
- Measures binary functional correctness rather than subtler degradations in code quality, efficiency, or maintainability
- Assumes GitHub frequency patterns accurately reflect pre-training distributions, which may not hold for all models
- Layer-wise analysis shows patterns consistent with theory but doesn't definitively prove causation between early embedding changes and final output degradation

## Confidence

- **High Confidence**: Core finding that subword tokenization misalignment causes measurable performance degradation in code LLMs, supported by robust sensitivity metrics across nine models and three tasks
- **Medium Confidence**: Specific mechanism explaining why early embedding changes propagate through model - layer-wise analysis shows patterns consistent with theory but alternative explanations cannot be ruled out
- **Medium Confidence**: Correlation between token frequency in pre-training data and model sensitivity - GitHub frequency proxy is reasonable but indirect, showing correlation rather than definitive causation

## Next Checks

1. **Semantic Quality Analysis**: Conduct qualitative analysis of generated code quality for samples that flip correctness, examining whether "incorrect" outputs still produce compilable, reasonable code or are completely broken

2. **Alternative Tokenization Impact**: Test same models with alternative tokenization strategies (byte-level BPE, character-level models, grammar-aware tokenizers) on same benchmarks to directly measure whether changing tokenizer architecture mitigates sensitivity issues

3. **Cross-Domain Robustness**: Apply TOKDRIFT framework to non-code domains (mathematical expressions, structured text) where subword tokenization might similarly misalign with domain-specific grammar to determine if this is broader LLM limitation or specific to code