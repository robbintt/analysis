---
ver: rpa2
title: 'Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs
  on Academic Resources'
arxiv_id: '2504.00595'
source_url: https://arxiv.org/abs/2504.00595
tags:
- data
- multimodal
- pre-training
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Open-Qwen2VL, a fully open-source multimodal
  large language model (MLLM) that achieves state-of-the-art performance while being
  highly compute-efficient. The authors address the challenge of reproducing state-of-the-art
  MLLM pretraining by releasing all aspects of their work, including training codebase,
  data filtering techniques, and datasets.
---

# Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources

## Quick Facts
- arXiv ID: 2504.00595
- Source URL: https://arxiv.org/abs/2504.00595
- Reference count: 40
- Outperforms Qwen2-VL-2B on MMBench, SEEDBench, MMStar, and MathVista while using only 0.36% of pretraining tokens

## Executive Summary
Open-Qwen2VL introduces a fully open-source multimodal large language model that achieves state-of-the-art performance while dramatically reducing compute requirements. The authors address the challenge of reproducing advanced MLLM pretraining by releasing all aspects of their work, including training codebase, data filtering techniques, and datasets. Through innovative techniques like low-to-high dynamic image resolution, multimodal sequence packing, and high-quality data curation, Open-Qwen2VL demonstrates that advanced MLLM pretraining can be accomplished with limited academic computing resources - requiring only 220 A100-40G GPU hours for pretraining on 29M image-text pairs.

## Method Summary
The method involves efficient pretraining of a Qwen2.5-1.5B-Instruct backbone with a frozen SigLIP-SO-400M vision encoder. A key innovation is the low-to-high dynamic resolution strategy, where images are compressed to 144 visual tokens during pre-training and expanded to 729 tokens during supervised fine-tuning. Multimodal sequence packing using First-Fit-Decreasing bin packing reduces padding waste and creates pseudo-interleaved data. High-quality data filtering via MLM-Filter (using SU scores >85) curates a 29M image-text pair dataset from DataComp-Medium and CCS-CLIP. The model is trained for 1 epoch using FSDP with frozen vision encoder, followed by SFT on MAmmoTH-VL-10M with optional vision encoder unfreezing.

## Key Results
- Outperforms Qwen2-VL-2B on MMBench, SEEDBench, MMStar, and MathVista benchmarks
- Achieves state-of-the-art performance while using only 220 A100-40G GPU hours for pretraining
- Requires just 0.36% of the tokens used in Qwen2-VL pretraining (5B packed multimodal tokens vs 1.4T)
- Demonstrates +3% to +12% improvement on 8-shot vs 0-shot VQA through multimodal sequence packing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing visual tokens via adaptive average-pooling during pre-training dramatically lowers compute costs without permanently degrading final task performance.
- Mechanism: The Adaptive Average-Pooling Visual Projector compresses 729 SigLIP image patch tokens to 144 visual tokens during pre-training (5x compression), then expands back to 729 tokens during supervised fine-tuning (SFT). This "low-to-high" resolution strategy reduces the sequence length the LLM backbone must process during the computationally expensive pre-training phase.
- Core assumption: The visual projector can learn efficient representations at low resolution that transfer to high-resolution tasks when scaled up during SFT; the frozen vision encoder provides sufficient feature quality.
- Evidence anchors:
  - [abstract] "...low-to-high dynamic image resolution... to significantly enhance pre-training efficiency."
  - [section] Section 2.2, Figure 1, and Table 7 describe the architecture: "scale the 729 output visual patches from SigLIP to any resolution. We adopt 144 visual tokens... in the pre-training stage and scale up... to 729 visual tokens in SFT stage."
  - [corpus] Corpus neighbors (e.g., CoLA) discuss compute-efficient pre-training via low-rank activations, but no direct corpus evidence validates the low-to-high resolution transfer for this specific architecture.
- Break condition: If high-resolution SFT fails to recover fine-grained visual understanding (e.g., OCR, small object detection), the mechanism is broken; pre-training at 144 tokens may have discarded critical detail.

### Mechanism 2
- Claim: Multimodal sequence packing reduces padding waste and creates pseudo-interleaved data, improving both compute efficiency and multi-shot in-context learning.
- Mechanism: Instead of padding individual caption samples to a fixed context length (wasting compute on padding tokens), the algorithm uses First-Fit-Decreasing bin packing to concatenate multiple image-text pairs into sequences near the 4096-token limit. This packs 5B tokens more densely and exposes the model to multiple images in a single sequence.
- Core assumption: The model benefits from seeing multiple image-text examples in one training sequence (simulating interleaved data) without requiring explicit interleaved document data; the bin-packing algorithm effectively approximates this.
- Evidence anchors:
  - [abstract] "...multimodal sequence packing to significantly enhance pre-training efficiency."
  - [section] Section 2.3 and Algorithm 1 detail the packing logic; Table 5 shows +3% to +12% improvement on 8-shot vs. 0-shot VQA, indicating enhanced in-context learning.
  - [corpus] The "Wasm" paper supports interleaved multimodal corpora benefits, but no direct corpus validation for this specific bin-packing approach exists.
- Break condition: If the model fails to generalize to true interleaved tasks (e.g., multi-image reasoning) or if packing introduces unintended cross-sample contamination, the mechanism is not delivering its claimed benefit.

### Mechanism 3
- Claim: High-quality data filtering via MLLM-based scoring (MLM-Filter) outperforms traditional CLIP-based filtering for MLLM pre-training data curation.
- Mechanism: MLM-Filter uses an efficient MLLM to score data on metrics like Semantic Understanding (SU). Setting a high threshold (85/100) filters the DataComp-Medium dataset down to a smaller, higher-quality subset. This quality-over-quantity approach enables strong model performance with far fewer training tokens.
- Core assumption: The MLM-Filter model's quality scores generalize to the target model being pre-trained; the SU metric correlates with downstream MLLM capabilities.
- Evidence anchors:
  - [abstract] "...MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency."
  - [section] Section 2.1 describes using MLM-Filter's SU metric; Table 3 shows the best average performance (56.0) comes from the data mixture including MLM-Filter curated data (DataComp-MLM-Filter & DFN), outperforming mixtures with only CLIP-filtered data.
  - [corpus] Corpus neighbors do not provide direct evidence on MLM-Filter's efficacy for this model; this is a gap.
- Break condition: If the MLM-Filter model is biased or if the SU metric does not correlate with downstream task performance for other model sizes/architectures, the filtering mechanism will not generalize.

## Foundational Learning

- **Vision-Language Projectors**: Why needed here: The projector (MLP + Adaptive Average Pooling) is the critical bridge between the frozen vision encoder and the LLM. Understanding its role is key to implementing the low-to-high resolution strategy. Quick check question: What is the difference between the projector configuration used during pre-training versus SFT?

- **Sequence Packing**: Why needed here: Packing is the core efficiency optimization. A new engineer must understand how to implement the bin-packing algorithm and create the custom dataloader. Quick check question: Why does padding to a fixed length waste compute, and how does bin packing solve this?

- **Data Filtering for Pre-training**: Why needed here: The paper's key claim is data quality over quantity. Understanding CLIPScore vs. MLLM-based scoring is essential for reproducing the data pipeline. Quick check question: What are the different quality metrics provided by MLM-Filter, and which one did this paper choose?

## Architecture Onboarding

- **Component map**: SigLIP Vision Encoder (frozen) -> Adaptive Average-Pooling Projector (trainable) -> Qwen2.5-1.5B LLM Backbone (trainable). Data flows as: Image -> SigLIP -> 729 patches -> Pooling -> 144 tokens (pre-training) or 729 tokens (SFT) -> MLP -> LLM.

- **Critical path**:
  1. Data Curation: Filter DataComp-Medium with DFN and then MLM-Filter (SU score > 85). Combine with CCS-CLIP data.
  2. Data Packing: Apply the bin-packing algorithm (Algorithm 1) to create packed sequences of ~4096 tokens in WebDataset format.
  3. Pre-training: Train on packed data for 1 epoch using FSDP, 144 visual tokens per image, frozen vision encoder.
  4. Supervised Fine-Tuning: Scale up visual tokens to 729 per image. Train on MAmmoTH-VL-10M. Optionally unfreeze vision encoder.

- **Design tradeoffs**:
  - **144 vs. 729 visual tokens**: Pre-training with 144 tokens is ~5x more efficient per image but may lose fine detail. The tradeoff is accepted with the plan to recover detail in SFT.
  - **Frozen vs. trainable vision encoder**: Freezing SigLIP saves compute but may cap visual understanding performance. Unfreezing during SFT offers a middle ground (Table 6).
  - **Caption-only vs. interleaved pre-training data**: Caption data is chosen for efficiency and single-image focus, sacrificing multi-image reasoning gains that interleaved data might provide.

- **Failure signatures**:
  - **Poor OCR/TextVQA performance**: Indicates the pre-training data lacks OCR-specific examples or the 144-token resolution destroyed text features.
  - **High padding ratio (>10%)**: Indicates the sequence packing algorithm or dataloader is not working correctly.
  - **SFT divergence or catastrophic forgetting**: May occur when scaling up visual tokens if learning rate is not reduced.

- **First 3 experiments**:
  1. **Validate Data Pipeline**: Download a small subset of DataComp-Medium, apply the MLM-Filter (SU > 85) and DFN filtering, and verify the filtering scripts produce the expected number of high-quality samples.
  2. **Reproduce Packing Logic**: Implement Algorithm 1 on a toy dataset of 1000 image-text pairs. Verify that output sequences are densely packed and that the average padding per sequence is <5%.
  3. **Ablate Visual Token Count**: Pre-train two small models (e.g., for 100 steps) – one with 144 visual tokens and one with 729. Compare training speed and early loss curves to quantify the efficiency tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the visual understanding capabilities of Open-Qwen2VL be further enhanced by making the vision encoder trainable during pre-training, and does this benefit outweigh the increased computational cost in a resource-constrained setting?
- Basis in paper: [explicit] Section 2.2 states that while the authors froze the vision encoder to save computes, recent studies suggest making it trainable enhances capabilities, and they explicitly "leave it as an ablation study for investigations."
- Why unresolved: The authors froze the vision encoder to adhere to strict academic compute budgets (A100-40G) and did not test the trade-off between the performance gain of an unfrozen encoder and the extra training hours required.
- What evidence would resolve it: A comparative ablation study showing benchmark performance vs. GPU hours for a model pre-trained with an unfrozen vs. frozen SigLIP encoder on the same data mixture.

### Open Question 2
- Question: Does the inclusion of OCR-specific caption datasets (e.g., SynthDoG or LAIONCOCO-OCR) into the pre-training mixture significantly enhance OCR-VQA performance without degrading general multimodal reasoning capabilities?
- Basis in paper: [explicit] Section 3.2 notes the model's relatively weaker results in OCR tasks (AI2D, TextVQA) and hypothesizes that "Simply introducing such OCR-related pre-training data will significantly enhance the OCR-VQA task performance of MLLMs."
- Why unresolved: The current pre-training dataset was curated primarily from general caption data (CC3M, DataComp) and lacked specific OCR-heavy sources, so the specific impact of adding them was not empirically validated in the study.
- What evidence would resolve it: An ablation experiment adding a controlled amount of OCR-specific data to the 29M pre-training set, followed by evaluation on both OCR-specific (TextVQA) and general (MMBench) benchmarks.

### Open Question 3
- Question: What mechanisms drive the observed performance degradation on general-purpose knowledge benchmarks (MMMU, MMStar) when scaling supervised fine-tuning data beyond 4M to 8M instructions?
- Basis in paper: [explicit] Section 3.2 and Figure 2 show that while most benchmarks improve, "For the general-purpose knowledge-based benchmarks of MMMU, SEEDBench, and MMStar, we even observe the slight performance degradation in the final 6M instruction tuning data."
- Why unresolved: The paper documents the phenomenon—suggesting potential distribution shifts or catastrophic forgetting—but does not isolate the cause (e.g., data quality issues in MAmmoTH-VL vs. model capacity limits).
- What evidence would resolve it: An analysis of the specific data distributions within the final 6M instructions of MAmmoTH-VL, or a replay-based experiment to see if interleaving the original high-quality data prevents the degradation.

### Open Question 4
- Question: Does the exclusion of image-text interleaved data in favor of caption-only data result in a significant loss of multi-image in-context learning capabilities compared to models like MM1?
- Basis in paper: [inferred] Section 2.1 acknowledges that while image-text interleaved data enhances in-context learning, the authors excluded it based on MM1's finding that it reduces zero-shot single-image reasoning. Section 4.1 tests in-context learning on packed sequences, but the trade-off regarding native interleaved data remains unquantified.
- Why unresolved: The paper optimizes for single-image reasoning using caption data, but the cost of this decision on the model's ability to process naturally interleaved documents (like PDFs or webpages) is not explicitly benchmarked against an interleaved pre-training baseline.
- What evidence would resolve it: A comparison of few-shot multi-image reasoning performance between the caption-only Open-Qwen2VL and an equivalent model pre-trained on a mixture of interleaved image-text documents.

## Limitations
- Critical implementation details for FSDP configuration, MLM-Filter batch size, and exact prompt templates are omitted
- Evaluation scope is narrow, focusing on closed-vocabulary models without exploring multilingual capabilities
- Compute-efficiency claims based on single model scale (2B parameters) may not transfer to other sizes
- Low-to-high resolution strategy's long-term effectiveness beyond SFT phase not validated

## Confidence
- **High Confidence**: The compute-efficiency improvements from multimodal sequence packing are well-supported by ablation studies (Table 5) and directly measurable. The architectural claims about the adaptive average-pooling projector and frozen vision encoder are clearly specified and technically sound.
- **Medium Confidence**: The data filtering approach using MLM-Filter with SU scores >85 shows strong performance in Table 3, but the underlying assumption that these scores generalize across different MLLM architectures remains unproven. The 220 GPU-hour claim is credible given the specifications but depends on the unspecified FSDP configuration.
- **Low Confidence**: The claim that Open-Qwen2VL "outperforms the state-of-the-art MLLM" on multiple benchmarks (MMBench, SEEDBench, MMStar, MathVista) is concerning because the comparison is against Qwen2-VL-2B, which is a 2B model from the same family. This creates a circular comparison where the improvements may be due to architectural refinements rather than the compute-efficient training methodology itself.

## Next Checks
1. **Validate Generalization of MLM-Filter**: Run MLM-Filter with the specified SU threshold (≥85) on a held-out validation set of DataComp-Medium and verify that the filtered subset consistently improves model performance across different random seeds and batch sizes.

2. **Ablate Resolution Transfer Effectiveness**: Train two models to completion - one with 144 visual tokens throughout both pre-training and SFT, and one with the low-to-high strategy. Compare final performance on OCR and small object detection tasks to quantify the effectiveness of the resolution transfer mechanism.

3. **Replicate Compute-Efficiency Claims**: Set up the exact FSDP configuration (sharding strategy, activation checkpointing, gradient clipping) and reproduce the 220 GPU-hour pre-training claim on 8×A100-40G. Measure actual vs. theoretical speedup from sequence packing and resolution reduction.