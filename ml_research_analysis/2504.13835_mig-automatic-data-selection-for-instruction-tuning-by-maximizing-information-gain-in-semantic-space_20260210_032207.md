---
ver: rpa2
title: 'MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information
  Gain in Semantic Space'
arxiv_id: '2504.13835'
source_url: https://arxiv.org/abs/2504.13835
tags:
- data
- information
- label
- quality
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIG proposes an information-based method for automatic data selection
  in instruction tuning. It constructs a label graph to model the semantic space and
  quantifies dataset information by balancing quality and diversity through an upper-convex
  information score function with information propagation.
---

# MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space

## Quick Facts
- arXiv ID: 2504.13835
- Source URL: https://arxiv.org/abs/2504.13835
- Reference count: 40
- MIG achieves +5.73% improvement on AlpacaEval and +6.89% on WildBench compared to full dataset training using only 5% of data

## Executive Summary
MIG addresses the challenge of automatic data selection for instruction tuning by proposing an information-theoretic approach that balances quality and diversity through semantic space modeling. The method constructs a label graph to capture task relationships and uses an upper-convex information score function with information propagation to quantify dataset information. Through greedy iterative selection based on information gain, MIG consistently outperforms state-of-the-art data selection methods across multiple data pools and base models, achieving significant performance improvements while using substantially less data.

## Method Summary
MIG automatically selects instruction tuning data by modeling semantic space through a label graph where nodes represent normalized tags and edges represent textual similarity between labels. The method propagates information across this graph using a matrix formulation, then applies an upper-convex information score function to balance quality and diversity. A greedy iterative selection algorithm chooses data points that maximize information gain, leveraging submodularity guarantees to achieve near-optimal subset selection. The approach uses external components for tagging (InsTagger) and quality scoring (DEITA), then fine-tunes base models using Llama-Factory with specified hyperparameters.

## Key Results
- Achieves +5.73% improvement on AlpacaEval compared to official SFT model trained on full Tulu3 data
- Achieves +6.89% improvement on WildBench using only 5% of the dataset
- Consistently outperforms state-of-the-art methods including D3, COMI, QDIT, and others across multiple data pools and base models
- Demonstrates robust performance across diverse benchmarks (AlpacaEval, MTBench, WildBench, ARC, BBH, MMLU, HumanEval, GSM8K, IFEval)

## Why This Works (Mechanism)

### Mechanism 1
Modeling semantic space as a label graph with propagated information captures task relationships better than independent approaches. Labels form nodes and textual similarity forms weighted edges, with information propagating along edges to correct for semantic overlap and annotation bias. This approach assumes label semantics reliably approximate task-similarity relationships, with edge sparsification preserving signal while enabling scalability.

### Mechanism 2
An upper-convex (concave) information score function jointly balances quality and diversity without separate heuristic stages. The concave function applied element-wise to propagated label information prioritizes labels with less accumulated information through marginally diminishing derivatives. This assumes concavity correctly encodes "diminishing returns" on repeated label coverage, with quality scores being meaningfully comparable across examples.

### Mechanism 3
Greedy iterative selection justified by submodularity provides near-optimal subsets with tractable computation. Because the information measure is submodular, greedy selection that maximizes per-step gain approximates the optimum within (1−1/e). The gradient-based gain proxy selects argmax over candidates, assuming submodularity holds under the chosen function and propagation parameters.

## Foundational Learning

### Submodular set functions
Why needed: Core to why greedy selection is theoretically justified; understand diminishing marginal returns and the (1−1/e) guarantee.
Quick check: Given a concave ϕ and nonnegative increments, does adding the same element to a smaller set yield ≥ marginal gain than adding it to a larger superset?

### Information-theoretic scoring (diminishing returns)
Why needed: Intuition for why concave ϕ encodes diversity as "new labels contribute more than redundant ones."
Quick check: If ϕ(x)=x, how does marginal gain behave, and what happens to diversity pressure?

### Label graphs and propagation
Why needed: MIG does not assume label independence; propagation redistributes credit across semantically related nodes.
Quick check: If two labels are highly similar but treated independently, how might redundancy be underestimated?

## Architecture Onboarding

### Component map:
Data Annotation (InsTagger, DEITA scores, label normalization) -> Label Graph Construction (nodes=labels, edges=similarity) -> Propagation (matrix A computation) -> Information Measurement (concave ϕ application) -> Sampling Loop (greedy iterative selection)

### Critical path:
Label quality and coverage directly determine graph semantics. Bad tags → bad graph → misallocation. Edge threshold controls density vs. noise. Choice of ϕ and α controls quality-diversity tradeoff.

### Design tradeoffs:
Node count vs. noise: More tags yield finer coverage but include outliers. Edge density vs. efficiency: Lower threshold → denser graph → more propagation but higher cost. Quality metric choice: DEITA outperforms alternatives but introduces external dependencies.

### Failure signatures:
Over-concentration on few labels: ϕ insufficiently concave or α too weak. Poor generalization: label graph not representative of target distribution. Slow sampling: edge density too high or implementation not exploiting sparse operations.

### First 3 experiments:
1. Ablate ϕ: Compare x^0.8 vs x^0.6 vs 1−e^(−αx) on Tulu3; track Avgobj and Avgsub.
2. Edge threshold sweep: Fix node count; vary T ∈ {0.8,0.86,0.88,0.9,0.92,0.94}; measure sampling time and Avg.
3. Propagation weight α: Compare α∈{0,0.6,0.8,1.0,1.2,2.0} to validate α=1.0 yields best Avg.

## Open Questions the Paper Calls Out

### Open Question 1
Can static parameters (information score function and propagation intensity) be determined automatically without grid search? The authors suggest future work focus on developing methods to automatically determine parameters, as current grid search is computationally expensive.

### Open Question 2
Does customizing the information score function for each individual label enhance flexibility and performance compared to using a single global function? The Conclusion suggests this as a method to enhance flexibility and scalability.

### Open Question 3
How can the optimal label graph topology (node count and edge density) be determined for a given data pool to maximize downstream model performance? Section 4.3 notes there is no universally optimal solution, as the ideal graph depends on data pool characteristics.

## Limitations
- Label graph construction relies on semantic similarity between tags, which may not fully capture task relationships or could introduce noise from tag quality issues
- Performance claims depend heavily on the quality of external components (InsTagger, DEITA scorer) whose implementation details are not fully specified
- The information propagation mechanism has ambiguous notation that could affect reproducibility

## Confidence

**High confidence**: The core theoretical framework of submodular information maximization and its (1-1/e) approximation guarantee

**Medium confidence**: The empirical improvements on benchmarks given dependence on baseline implementations and evaluation setup

**Low confidence**: The specific parameter choices (Φ(x)=x^0.8, edge threshold=0.9, α=1.0) without comprehensive sensitivity analysis

## Next Checks

1. **Parameter sensitivity**: Systematically vary Φ(x), edge threshold T, and propagation weight α across their plausible ranges to identify where performance degrades and verify the reported optimal values

2. **Component ablation**: Replace InsTagger with a simpler tagging approach and DEITA with a baseline quality metric to determine how much performance depends on these specific components

3. **Scalability validation**: Test MIG on progressively larger datasets (10K → 50K → 100K samples) to verify that the greedy algorithm maintains efficiency and that the (1-1/e) approximation holds in practice