---
ver: rpa2
title: 'Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures
  in Neural Language Models'
arxiv_id: '2506.21861'
source_url: https://arxiv.org/abs/2506.21861
tags:
- structures
- syntactic
- layers
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Derivational Probing, a method that integrates
  structural probing with expected layer metrics to trace how syntactic structures
  are constructed across neural language model layers. The approach distinguishes
  between micro-syntactic structures (e.g., subject phrases) and macro-syntactic structures
  (e.g., root verb relationships) and tracks their emergence across layers.
---

# Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models

## Quick Facts
- arXiv ID: 2506.21861
- Source URL: https://arxiv.org/abs/2506.21861
- Reference count: 11
- Primary result: Derivational Probing method reveals BERT builds syntactic structures bottom-up while GPT-2 constructs them in parallel, with timing affecting subject-verb agreement performance.

## Executive Summary
This paper introduces Derivational Probing, a method that traces how syntactic structures emerge across neural language model layers by integrating structural probing with expected layer metrics. The approach distinguishes between micro-syntactic structures (e.g., subject phrases) and macro-syntactic structures (e.g., root verb relationships) and tracks their emergence across layers. Experiments reveal that BERT models construct micro-syntactic structures in lower layers before integrating them into macro-syntactic structures in higher layers, while GPT-2 models build both types in parallel. The timing of macro-syntactic structure construction critically affects downstream performance on subject-verb agreement, with both premature and delayed integration leading to errors.

## Method Summary
Derivational Probing combines structural probing with expected layer metrics to trace syntactic structure construction across model layers. The method extracts embeddings from each layer, computes scalar-mixed representations, and trains linear structural probes to predict syntactic distances for micro and macro-syntactic subgraphs. UUAS scores are computed at each layer to determine when each structure type emerges, quantified by an expected layer metric. The approach is applied to BERT and GPT-2 models on English sentences, with targeted evaluation linking derivation timing to subject-verb agreement performance.

## Key Results
- BERT models show bottom-up derivation: micro-syntactic structures emerge in lower layers and integrate into macro-syntactic structures in higher layers
- GPT-2 models exhibit parallel construction of micro- and macro-syntactic structures across layers
- Subject-verb agreement errors occur when macro-syntactic structure integration happens too early or too late, suggesting optimal intermediate timing
- The expected layer metric effectively quantifies when different syntactic structures emerge across model layers

## Why This Works (Mechanism)
The method works by leveraging structural probing to quantify syntactic information in embeddings at each layer, then using expected layer metrics to identify when specific structures emerge. The distinction between micro- and macro-syntactic structures allows tracking of how local syntactic dependencies combine into global sentence structure. The correlation between macro-syntactic timing and subject-verb agreement performance demonstrates that the layer-wise construction process directly impacts downstream linguistic tasks, with U-shaped error patterns indicating optimal intermediate integration timing.

## Foundational Learning

- **Concept: Structural Probing (Hewitt & Manning, 2019)**
  - Why needed here: Provides the foundational technique Derivational Probing builds upon, quantifying how well latent syntactic trees are encoded in model embeddings by learning a projection where Euclidean distances approximate syntactic tree distances
  - Quick check question: Given a sentence's embeddings and its dependency parse tree, what is the objective function a structural probe learns to minimize?

- **Concept: Expected Layer Metric**
  - Why needed here: Quantifies the "centroid" of information emergence across layers, providing a single value to compare when different linguistic structures are constructed
  - Quick check question: If the UUAS for a structure jumps from 0.2 to 0.8 between layers 4 and 6, and saturates thereafter, would its expected layer be closer to 4 or 6?

- **Concept: Micro- vs. Macro-Syntactic Structures**
  - Why needed here: Core hypothesis depends on distinguishing these two types of subgraphs; understanding their definition is critical for interpreting results and designing the probe
  - Quick check question: In the sentence "The cat sat on the mat," which part constitutes the macro-syntactic structure and which is a micro-syntactic structure?

## Architecture Onboarding

- **Component map:**
  1. Data Prep: Filter and parse corpus into sentences with clean dependency parses, group by syntactic patterns
  2. Embedding Extraction: Extract embeddings from all layers for each token in a sentence
  3. Scalar Mixing: Compute scalar-mixed embeddings as weighted average of all layers up to layer â„“
  4. Probing Module: Train linear structural probe at each layer for each syntactic subgraph type
  5. Evaluation: Compute UUAS for each subgraph at each layer, calculate expected layer, analyze correlation with downstream task

- **Critical path:**
  1. Probe Training per Subgraph: Correctly training and evaluating the structural probe specifically for each subgraph type (micro vs. macro) at each layer is most critical, as main results hinge on comparing their expected layer values
  2. Targeted Evaluation: Linking the derived expected layer for macro-syntactic structures to success/failure on the subject-verb agreement task is the key analysis for establishing the mechanism's behavioral impact

- **Design tradeoffs:**
  - Efficiency vs. Granularity: Requires training probes for each layer and each subgraph type, computationally intensive for very deep models
  - Scope: Focusing on single-clause sentences improves clarity of syntactic signal but limits generalizability to complex, multi-clause syntax
  - Assumption of Linearity: Structural probe uses linear transformation which may not capture non-linear relationships in embedding space

- **Failure signatures:**
  - No Hierarchical Difference: If expected layer for micro- and macro-syntactic structures is nearly identical across all models, core hypothesis of different derivation strategies is unsupported
  - Inconsistent Probe Performance: If UUAS is low and erratic across layers, probe may not be learning meaningful syntactic representations, making expected layer metric unreliable
  - No Task Correlation: If expected layer of macro-syntactic structures does not correlate with success/failure on subject-verb agreement task, claim about behavioral impact of derivation timing is weakened

- **First 3 experiments:**
  1. Sanity Check: Replicate overall UUAS vs. layer plot for BERT-base on chosen corpus, verify performance peaks in middle layers as expected
  2. Core Probe: Train structural probes for nsubj (micro) and Marco (macro) subgraphs across all layers of BERT-base, compute and compare expected layer values, confirm bottom-up pattern
  3. Behavioral Link: Run subject-verb agreement evaluation, compare expected layer for Marco subgraph between success and failure cases, check for U-shaped performance curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed syntactic derivation patterns (bottom-up vs. parallel) generalize to languages with typologically different structures?
- Basis in paper: [explicit] Authors propose incorporating "multilingual probes" to determine if findings are specific to English or generalizable artifacts
- Why unresolved: Study restricted experiments to English data, leaving cross-linguistic validity unconfirmed
- What evidence would resolve it: Applying Derivational Probing to multilingual datasets to compare layer-wise derivation timings across diverse syntactic systems

### Open Question 2
- Question: How do the layer-wise derivation processes identified in this study relate to incremental, left-to-right parsing strategies in autoregressive models?
- Basis in paper: [explicit] Paper highlights need to "explore incremental parsing strategies in autoregressive language models" to better understand their specific syntactic knowledge
- Why unresolved: Current method analyzes atemporal, layer-wise derivation given full context, which does not capture temporal, token-wise processing of models like GPT-2
- What evidence would resolve it: Modified probing framework correlating layer-wise structural emergence with incremental token generation

### Open Question 3
- Question: Are the observed syntactic derivation processes driven by pure syntactic cues, or are they confounded by semantic information?
- Basis in paper: [inferred] Authors note limitation that "semantic cues may influence the results" and study does not fully account for these confounds
- Why unresolved: Standard text contains both syntax and semantics, making it difficult to isolate cause of observed micro- and macro-syntactic emergence patterns
- What evidence would resolve it: Replicating experiments using Jabberwocky sentences to isolate syntactic processing from semantic influence

## Limitations
- Linear structural probe may not fully capture complex, non-linear relationships in neural embeddings
- Focus on single-clause sentences limits generalizability to more complex syntax
- Method's computational intensity and assumption that UUAS accurately reflects syntactic understanding are potential limitations

## Confidence
- High: Observation that BERT models show bottom-up derivation pattern, supported by clear UUAS layer trends and consistent with existing literature
- Medium: Claim about GPT-2's parallel construction, method is sound but behavioral implications are less explored
- Medium: Link between macro-syntactic timing and subject-verb agreement errors, correlation is demonstrated but causal mechanism could benefit from further ablation studies

## Next Checks
1. Conduct ablation study by training BERT with modified layer-wise objectives (encouraging or discouraging early macro-syntactic integration) and re-run subject-verb agreement task to directly test causality
2. Extend Derivational Probing method to subset of multi-clause sentences from corpus to assess scalability of micro-macro distinction to more complex syntax
3. Apply method to decoder-only model with different architecture (e.g., OPT or LLaMA) to test whether derivation patterns are model-family specific or more general