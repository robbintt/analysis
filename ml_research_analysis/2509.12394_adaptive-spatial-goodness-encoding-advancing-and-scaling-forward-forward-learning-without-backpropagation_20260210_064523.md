---
ver: rpa2
title: 'Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward
  Learning Without Backpropagation'
arxiv_id: '2509.12394'
source_url: https://arxiv.org/abs/2509.12394
tags:
- layer
- goodness
- training
- learning
- asge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ASGE, a novel Forward-Forward (FF) training
  framework for convolutional neural networks that addresses representational degradation
  and scalability issues in prior FF methods. The key innovation is adaptive spatial
  goodness encoding, which computes spatially-aware goodness representations at each
  layer while decoupling classification complexity from channel dimensionality.
---

# Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward Learning Without Backpropagation

## Quick Facts
- **arXiv ID**: 2509.12394
- **Source URL**: https://arxiv.org/abs/2509.12394
- **Reference count**: 23
- **Primary result**: Achieves 51.58% Top-1 accuracy on ImageNet using Forward-Forward training without backpropagation

## Executive Summary
This paper introduces ASGE, a novel Forward-Forward (FF) training framework that addresses critical limitations in prior FF methods for convolutional neural networks. The key innovation is adaptive spatial goodness encoding, which computes spatially-aware goodness representations at each layer while decoupling classification complexity from channel dimensionality. This enables efficient training on large-scale datasets without the channel explosion problem that plagued earlier FF approaches. ASGE achieves competitive performance compared to backpropagation alternatives, successfully scaling FF training to ImageNet for the first time with 51.58% Top-1 accuracy, while maintaining strong results on smaller datasets (99.65% on MNIST, 90.62% on CIFAR-10).

## Method Summary
ASGE operates by computing goodness vectors from spatially-partitioned patches of feature maps rather than class-aligned channel groups. For each layer's feature maps Y^l ∈ ℝ^(C^l×H^l×W^l), the method partitions the spatial dimensions into P^l×P^l patches and computes mean squared activation within each patch to form goodness vectors g^l ∈ ℝ^(1×(C^l·P^(2l))). Crucially, channel dimensionality C^l remains constant regardless of dataset complexity, with P^l adaptively scaling inversely with channel count (P^l = min(max(1, ⌊α·C^L/C^l⌋), H^l, W^l), α=1). Layer-wise supervision occurs through fixed random projections (W^l, b^l ~ N(0,1/N)) that map goodness vectors to class logits, eliminating the need for auxiliary classifier backpropagation. The framework introduces three prediction strategies providing flexible trade-offs between accuracy, parameters, and memory usage.

## Key Results
- **ImageNet breakthrough**: First successful application of FF-based training to ImageNet with 51.58% Top-1 accuracy
- **Scalability demonstrated**: Competitive performance across five datasets (MNIST 99.65%, FashionMNIST 93.41%, CIFAR-10 90.62%, CIFAR-100 65.34%) without channel explosion
- **Resource flexibility**: Three prediction strategies enable deployment under diverse constraints (fusion pred: highest accuracy, best pred: minimal parameters, last pred: memory efficiency)

## Why This Works (Mechanism)

### Mechanism 1: Spatial Goodness Extraction Decouples Channels from Classes
If spatial goodness is computed from localized patch-wise energy rather than class-aligned channel groups, then channel dimensionality can remain constant regardless of dataset complexity. Feature maps are partitioned into patches, with mean squared activation forming goodness vectors where C^l ∈ Θ(1) rather than C^l ∈ Θ(N) as in prior FF methods.

### Mechanism 2: Channel-Aware Partitioning Matches Feature Abstraction Depth
If partitioning granularity decreases with network depth (inversely to channel count), then goodness supervision aligns with hierarchical feature abstraction. Shallow layers receive finer partitions while deeper layers encode abstract representations where coarser partitions suffice.

### Mechanism 3: Fixed Random Projection Enables BP-Free Supervision
If goodness vectors are projected to class space via fixed random matrices (not trained), then layer-wise supervision occurs without auxiliary classifier backpropagation. Gradients flow directly to convolutional parameters without crossing layer boundaries.

## Foundational Learning

- **Forward-Forward (FF) Algorithm**: ASGE builds on FF's core premise—replacing backpropagation with dual forward passes and layer-wise goodness optimization. Understanding FF is essential to grasp ASGE's improvements.
  - Quick check: Can you explain why FF uses two forward passes instead of forward+backward?

- **Goodness Functions in Neural Networks**: The entire ASGE framework revolves around redefining "goodness" as spatially-distributed patch energy rather than channel-wise or global metrics. Understanding what goodness represents is essential.
  - Quick check: In original FF, what does "goodness" measure, and how does ASGE modify this spatially?

- **Channel Dimensionality Scaling in CNNs**: Prior FF methods required C^l ∈ Θ(N), causing parameter explosion on large datasets. ASGE's key contribution is breaking this coupling.
  - Quick check: Why did CwC require more channels for more classes, and how does ASGE avoid this?

## Architecture Onboarding

- **Component map**: Input Image → [Conv Block] → ReLU → [Spatial Goodness Computation] → Random Projection → Loss → [RMS Pool] → [Layer Norm] → [Next Conv Block] ...
- **Critical path**: 1) Feature map extraction (Conv + ReLU) 2) Adaptive patch partitioning (compute P^l) 3) Spatial goodness computation (mean squared activation per patch) 4) Random projection to class logits 5) Local cross-entropy loss → direct gradient to conv parameters
- **Design tradeoffs**:
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | RMS Pooling | Preserves energy distribution aligned with goodness metric | Non-standard; requires custom implementation |
  | Random Projection | No trainable classifier; true BP-free | Frozen projections may underutilize goodness dimensions |
  | Fusion Pred (multi-layer) | Highest accuracy (65.34% on CIFAR-100) | 10× classifier params vs. Last Pred; stores intermediate activations |
  | Best Pred | No final classifier; early exit possible | ~1.7% accuracy drop vs. Fusion |
- **Failure signatures**: Representational degradation (if layer-wise accuracy decreases with depth), channel explosion on large datasets (if memory scales with class count), goodness distribution distortion (if pooling changes goodness statistics significantly)
- **First 3 experiments**:
  1. Baseline validation on CIFAR-10: Train VGG8 with α=1, RMS pooling, Fusion Pred. Target: ~90.5% test accuracy
  2. Ablation on partitioning factor α: Sweep α ∈ {0, 0.5, 1, 1.5, 2} on CIFAR-100. Expect peak at α=1
  3. Pooling method comparison: Train VGG8 with RMS vs. average vs. max pooling on CIFAR-100

## Open Questions the Paper Calls Out

### Open Question 1
Can ASGE be effectively extended to non-convolutional architectures such as Transformers and RNNs? Currently, ASGE is only applied to CNNs, and its extension to other architectures remains to be explored. The spatial goodness extraction mechanism relies on partitioning 2D feature maps into patches, which is inherently tied to convolutional spatial structure.

### Open Question 2
Can the convergence speed of ASGE be improved to match end-to-end backpropagation training? As a greedy, layer-wise approach, it generally requires longer training time to converge than end-to-end BP. The layer-wise training paradigm prevents joint optimization across layers.

### Open Question 3
Would learnable projection matrices improve performance over the fixed random projections used for supervision? The method uses fixed randomly initialized projection weights that remain fixed throughout training, avoiding backpropagation through auxiliary classifiers. This design choice trades optimization flexibility for BP-free training.

## Limitations
- **Architecture dependency**: ImageNet result relies on unspecified ResNet18-CH×4 architecture, creating reproducibility concerns
- **Performance gaps**: Substantial accuracy gaps remain vs. backpropagation on harder datasets (65.34% vs 75.45% BP baseline on CIFAR-100, 51.58% vs 78.8% BP baseline on ImageNet)
- **Computational overhead**: Framework adds complexity through patch-based goodness extraction and multi-layer fusion, with unclear overhead for large models

## Confidence

**High Confidence**: CIFAR-10/FashionMNIST/MNIST results (90.62%, 93.41%, 99.65%) demonstrate consistent ASGE performance across multiple datasets with established benchmarks.

**Medium Confidence**: ImageNet result is methodologically sound but depends on an unspecified architecture variant. The claim of being first on ImageNet lacks comprehensive citation comparison.

**Low Confidence**: Performance gaps vs. backpropagation on harder datasets suggest fundamental limitations. The random projection assumption hasn't been validated for the specific goodness distributions ASGE produces.

## Next Checks

1. **Architecture Transparency**: Request explicit ResNet18-CH×4 specifications (exact channel counts, layer dimensions) to enable ImageNet reproduction and community benchmarking.

2. **Cross-Dataset Scaling Test**: Evaluate ASGE on a dataset with 1,000+ classes (e.g., iNaturalist) to empirically validate the channel decoupling mechanism beyond ImageNet's 1,000 classes.

3. **Random Projection Analysis**: Conduct ablation comparing fixed random projections vs. learned projections on CIFAR-100 to quantify the performance cost of true backpropagation-free training.