---
ver: rpa2
title: A Law of Data Reconstruction for Random Features (and Beyond)
arxiv_id: '2509.22214'
source_url: https://arxiv.org/abs/2509.22214
tags:
- have
- training
- probability
- data
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the feasibility of reconstructing a training\
  \ dataset from a trained model, focusing on the number of parameters p required\
  \ for this task. The authors introduce a \"law of data reconstruction\" that establishes\
  \ the threshold p \u2248 dn for successful reconstruction, where d is the data dimensionality\
  \ and n is the number of samples."
---

# A Law of Data Reconstruction for Random Features (and Beyond)

## Quick Facts
- arXiv ID: 2509.22214
- Source URL: https://arxiv.org/abs/2509.22214
- Reference count: 40
- The paper establishes a threshold p ≈ dn for successful reconstruction of training data from model parameters, where d is data dimensionality and n is the number of samples.

## Executive Summary
This paper investigates the feasibility of reconstructing training datasets from trained models, introducing a "law of data reconstruction" that establishes the critical threshold p ≈ dn for successful reconstruction. The authors prove that when the number of parameters p significantly exceeds the product of data dimensionality and sample size, the subspace spanned by training samples in feature space contains sufficient information to identify individual samples in input space. Through theoretical analysis based on random features regression and empirical validation across multiple architectures, the paper demonstrates that exceeding this threshold enables complete recovery of training data, with reconstructed images being perceptually indistinguishable from originals.

## Method Summary
The authors analyze data reconstruction feasibility by establishing a theoretical threshold for the number of parameters required to uniquely identify training samples from their projections in feature space. They develop an optimization algorithm that minimizes reconstruction loss by solving an ℓ₂-ℓ₁ minimization problem, leveraging the observation that when p ≫ dn, the random features of training samples are spanned by the features of reconstructed data. The algorithm is validated on random features models and extended empirically to two-layer fully-connected networks and deep residual networks, demonstrating successful reconstruction when the parameter threshold is exceeded.

## Key Results
- Theoretical proof that p ≈ dn is the critical threshold for successful data reconstruction
- Complete recovery of training datasets when parameter count exceeds dn threshold
- Reconstruction of perceptually indistinguishable images across multiple network architectures
- Identification and resolution of sign ambiguity issue with ReLU activations through mixed-parity Hermite coefficients

## Why This Works (Mechanism)
The reconstruction works because when p ≫ dn, the high-dimensional feature space contains sufficient information to uniquely identify each training sample through their feature projections. The random features create a rich enough representation that the span of training samples' features contains enough constraints to recover the original inputs. The optimization algorithm exploits this property by minimizing the reconstruction loss, which effectively finds the input samples whose feature representations match the given model parameters.

## Foundational Learning

**Random Features Regression**: A kernel approximation method that maps inputs to a randomized feature space, creating a linear model in high dimensions. Needed to establish the theoretical framework for reconstruction analysis. Quick check: Verify that random features approximate the kernel function with high probability.

**ℓ₂-ℓ₁ Minimization**: Optimization technique combining L2 regularization for stability and L1 regularization for sparsity. Needed to solve the reconstruction problem while avoiding overfitting. Quick check: Confirm convergence properties and sensitivity to regularization parameters.

**Feature Space Geometry**: Understanding how training samples are distributed and related in high-dimensional feature space. Needed to analyze when reconstruction is possible. Quick check: Examine the condition number of feature matrices and their span properties.

## Architecture Onboarding

**Component Map**: Data samples -> Random Features -> Feature Space -> Optimization Algorithm -> Reconstructed Data

**Critical Path**: The key insight is that when p ≫ dn, the feature space contains sufficient constraints for unique identification of training samples. The critical path involves mapping data to features, analyzing the span properties, and solving the reconstruction optimization.

**Design Tradeoffs**: High parameter count enables reconstruction but increases computational cost and storage requirements. The choice of activation function affects reconstruction quality, with ReLU activations introducing sign ambiguity that requires additional handling.

**Failure Signatures**: Reconstruction fails when p ≤ dn, when random features do not adequately span the data manifold, or when regularization prevents the optimization from finding the correct solution. Sign ambiguity with ReLU activations can also cause failures.

**First Experiments**: 1) Test reconstruction on synthetic data with known ground truth to verify theoretical predictions. 2) Evaluate reconstruction success rate across different values of p relative to dn. 3) Compare reconstruction quality using different activation functions and random feature distributions.

## Open Questions the Paper Calls Out

The paper identifies the sign ambiguity issue with ReLU activations as an open problem, though it proposes using activations with mixed-parity Hermite coefficients as a potential solution. The generalizability of the theoretical results to deep networks and practical architectures remains an open question, as the current analysis is limited to random features models.

## Limitations

- Theoretical analysis is limited to random features models and may not directly extend to deep networks
- Sign ambiguity with ReLU activations requires additional handling and may affect practical implementations
- Reconstruction algorithm may become computationally challenging for very large datasets
- Assumes full access to model parameters and random features, which may not be available in practice

## Confidence

**Theoretical threshold p ≈ dn**: High
**Empirical reconstruction success**: High
**Sign ambiguity solution**: Medium
**Generalizability beyond random features**: Medium

## Next Checks

1. Test the reconstruction algorithm's performance on real-world datasets with varying levels of noise and regularization
2. Extend theoretical analysis to include practical considerations like batch normalization and dropout
3. Evaluate reconstruction success rates when only partial model information is available