---
ver: rpa2
title: Multi-LLM Collaborative Search for Complex Problem Solving
arxiv_id: '2502.18873'
source_url: https://arxiv.org/abs/2502.18873
tags:
- search
- reasoning
- mosa
- llms
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Mixture-of-Search-Agents (MoSA) paradigm,
  which uses multiple LLMs to enhance search-based reasoning by combining independent
  exploration with iterative refinement. The approach integrates diverse reasoning
  pathways and employs Monte Carlo Tree Search as a backbone, with multiple agents
  proposing and aggregating reasoning steps.
---

# Multi-LLM Collaborative Search for Complex Problem Solving

## Quick Facts
- arXiv ID: 2502.18873
- Source URL: https://arxiv.org/abs/2502.18873
- Reference count: 40
- Multi-LLM approach improves search-based reasoning accuracy by 1.71% on average across four benchmarks

## Executive Summary
This paper introduces the Mixture-of-Search-Agents (MoSA) paradigm that enhances complex reasoning by using multiple LLMs collaboratively within a Monte Carlo Tree Search framework. The approach combines independent exploration from diverse models with iterative refinement through neural aggregation. By having multiple agents generate and critique reasoning steps, MoSA addresses the limitations of single-model approaches that struggle with local optima and limited search diversity.

## Method Summary
MoSA uses Monte Carlo Tree Search as a backbone for reasoning, with multiple LLM agents taking on Proposer and Aggregator roles. Proposers generate diverse sub-questions and candidate sub-answers independently, while Aggregators synthesize these candidates into refined responses. The method employs a pool of instruction-tuned LLMs (Llama-3.1-8B, Qwen-2-7B, Ministral-8B, GLM-4-9B) and uses self-consistency as a reward function. Key hyperparameters include temperature=0.75, 8 rollouts, 4 sub-questions per node, and max depth=5.

## Key Results
- MoSA achieves an average accuracy improvement of 1.71% across four reasoning benchmarks
- Particularly excels in complex mathematical and commonsense reasoning tasks
- Outperforms both single-agent baselines and other multi-agent approaches
- Demonstrates synergistic effects between multi-agent collaboration and search-based reasoning

## Why This Works (Mechanism)

### Mechanism 1: Inherent Diversity from Multi-Model Distributions
Different LLMs trained on distinct corpora generate varied probability distributions for the same input. By sampling from this collective distribution, MoSA achieves diversity without relying on high-temperature sampling that can degrade quality. The core assumption is that diverse initial reasoning steps increase the probability of discovering correct solutions.

### Mechanism 2: Collaborative Refinement via Neural Aggregation
A neural aggregator synthesizes and refines responses from multiple models beyond simple majority voting. Each LLM is prompted to critique and compare all candidate sub-answers, with the intuition that models can learn from the partial correctness of others. The core assumption is that LLMs possess innate capability to critique and aggregate responses effectively.

### Mechanism 3: Synergistic Multi-Agent and Search Combination
The combination of multi-agent collaboration and search-based reasoning yields multiplicative performance gains. MCTS provides structured exploration while multi-agent collaboration enhances each search step with diverse, high-quality proposals and refined aggregations. The assumption is that performance improvements are not merely additive but synergistic.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS)**
  - Why needed: Provides structured framework for exploring problem space through tree of states and actions
  - Quick check: Can you explain the four steps of a standard MCTS iteration (Selection, Expansion, Simulation, Backpropagation)?

- **Self-Consistency**
  - Why needed: Baseline method for aggregating multiple answers and reward function approximation
  - Quick check: How does self-consistency determine the final answer in LLM reasoning?

- **Multi-Agent Collaboration**
  - Why needed: MoSA is fundamentally a multi-agent system where different LLMs take on roles
  - Quick check: What are the two primary roles an LLM agent can play in MoSA?

## Architecture Onboarding

- **Component map:** LLM Pool -> MCTS Engine -> MoSA Proposers -> MoSA Aggregators -> Reward Function
- **Critical path:** Input → MCTS Selection → MoSA Proposal → MoSA Aggregation → Action Formation → MCTS Expansion → Simulation/Backprop → Output
- **Design tradeoffs:** More LLMs increase diversity but also computational cost; neural aggregation is more powerful than voting but adds compute; rich action sets enable complex reasoning but increase branching
- **Failure signatures:** Degenerate search (collapse into single path), aggregation collapse (fails to improve answers), excessive compute (system too slow)
- **First 3 experiments:** 1) Reproduce single-LLM RAP baseline and RAP + MoSA (Proposers only) on GSM8K; 2) Compare MoSA with "Proposers only" vs. "Proposers & Aggregators" to quantify aggregation contribution; 3) Vary number of distinct LLMs in pool (1, 2, 3, 4) to confirm diversity-accuracy correlation

## Open Questions the Paper Calls Out

- Does marginal performance gain diminish or become negative when scaling beyond four distinct LLM agents?
- Why does expanded action space degrade performance on StrategyQA despite improving mathematical reasoning?
- How robust is the MoSA aggregator when the candidate pool contains no correct answers?

## Limitations
- Neural aggregation mechanism lacks direct experimental validation in literature
- Synergistic effects between MCTS and multi-agent collaboration not externally validated
- Substantial computational cost from running multiple LLMs through MCTS rollouts

## Confidence

**High Confidence**: Core methodology of using multiple LLMs with MCTS for reasoning tasks is technically sound and builds on established frameworks. Performance improvements over single-LLM baselines are reproducible.

**Medium Confidence**: Specific contribution of neural aggregation component is less certain. While paper demonstrates improved performance, mechanism's reliability and generalizability remains unclear.

**Low Confidence**: Claim of synergistic multi-agent + search effects exceeds what can be verified from presented data. Paper shows MoSA outperforming single-agent approaches but doesn't establish multiplicative gains.

## Next Checks

1. **Ablation Study**: Run MoSA with Proposers only (no neural aggregation) and compare accuracy against full MoSA on all four benchmarks to quantify exact aggregation contribution.

2. **Cross-Domain Testing**: Apply MoSA to reasoning tasks outside tested domains (logical puzzles, code generation) to assess whether multi-LLM benefits generalize.

3. **Computational Efficiency Analysis**: Measure wall-clock time and token costs for MoSA versus single-LLM baselines, examining how number of LLMs affects both accuracy and latency.