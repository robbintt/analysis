---
ver: rpa2
title: 'MKA: Leveraging Cross-Lingual Consensus for Model Abstention'
arxiv_id: '2503.23687'
source_url: https://arxiv.org/abs/2503.23687
tags:
- language
- confidence
- pipeline
- accuracy
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of reducing hallucinations in large
  language models (LLMs) by using their multilingual knowledge to calibrate confidence
  and enable abstention when uncertain. The proposed Multilingual Knowledge Abstention
  (MKA) pipeline translates questions into auxiliary languages, generates responses
  in those languages, translates answers back, and uses semantic similarity to determine
  confidence.
---

# MKA: Leveraging Cross-Lingual Consensus for Model Abstention

## Quick Facts
- arXiv ID: 2503.23687
- Source URL: https://arxiv.org/abs/2503.23687
- Reference count: 4
- Primary result: MKA improves accuracy up to 71.2% for Bengali and 15.5% for English by enabling models to abstain when uncertain

## Executive Summary
MKA addresses LLM hallucination by using cross-lingual consensus to calibrate confidence and enable abstention. The method translates questions into auxiliary languages, generates responses in those languages, translates answers back, and uses semantic similarity to determine confidence. Experiments with Aya Expanse 8B, Gemma 2 variants, and Qwen 2.5 7B across six target languages show significant accuracy improvements, particularly for low-resource languages like Bengali. Performance depends critically on auxiliary language quality and translation fidelity, with degradation observed for low-resource languages like Yoruba.

## Method Summary
MKA is a post-inference pipeline that leverages cross-lingual knowledge to improve LLM confidence calibration. It translates questions into multiple auxiliary languages, generates responses in each language using the same model, translates responses back to the target language, and computes semantic similarity between responses. The method uses centroid polling to select the answer with highest average cosine similarity, then calculates confidence based on weighted similarities. If confidence exceeds a threshold, the model answers; otherwise, it abstains. The pipeline uses NLLB-200 for translation and paraphrase-multilingual-mpnet-base-v2 for embeddings.

## Key Results
- Accuracy improvements up to 71.2% for Bengali questions
- 15.5% improvement for English questions
- Performance degrades for low-resource languages (Yoruba: -24.6%) due to poor translation quality
- Composite accuracy ($AC_{comp}$) and effective accuracy ($AC_{eff}$) metrics show consistent gains across models and languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual prompting surfaces latent knowledge that monolingual prompting cannot access
- Mechanism: LLMs store knowledge in language-specific patterns; prompting in multiple languages and aggregating responses creates an ensemble effect where genuine knowledge persists across translations while hallucinations tend to diverge
- Core assumption: Models cannot implicitly leverage multilingual knowledge when monolingually prompted (assumption: knowledge representations are language-partitioned)
- Evidence anchors:
  - [abstract]: "translates questions into auxiliary languages, generates responses in those languages, translates answers back, and uses semantic similarity to determine confidence"
  - [Page 1]: "Etxaniz et al. (2023) find that language models cannot utilize cross-lingual knowledge implicitly: prompting in one language doesn't necessarily utilize the model's knowledge in other languages"
  - [corpus]: CausalAbstain paper (2506.00519) addresses similar multilingual abstention but through causal reasoning—suggesting cross-lingual knowledge access is an active research area, not a settled mechanism
- Break condition: When translation quality is poor (e.g., Yoruba with sp-BLEU 26.6/13.8), noise from translation artifacts overwhelms signal

### Mechanism 2
- Claim: Semantic consensus across translated responses correlates with answer correctness
- Mechanism: Correct answers produce semantically similar back-translations; incorrect/hallucinated answers show higher variance. Centroid polling identifies the answer with highest average cosine similarity to others.
- Core assumption: Agreement across languages signals epistemic confidence rather than shared bias
- Evidence anchors:
  - [Page 2]: "We poll all answers... to select the answer with the highest average cosine similarity across all answers"
  - [Page 4-5]: Bengali shows 71.2% improvement; Yoruba shows -24.6% degradation—correlated with translation quality
  - [corpus]: Best-of-L paper (2509.15811) explores cross-lingual reward modeling for reasoning, suggesting complementarity across languages is plausible but not proven for abstention
- Break condition: When all auxiliary languages share the same knowledge gap or bias, consensus will not help

### Mechanism 3
- Claim: Confidence-weighted similarity thresholds enable calibrated abstention
- Mechanism: Answers with similarity > 0.8 to the selected answer receive 1.5x weight in confidence calculation; if final confidence exceeds cutoff, model answers; otherwise abstains
- Core assumption: The 0.8 threshold and 1.5x weight are heuristics without theoretical justification
- Evidence anchors:
  - [Page 2]: "for answers that have a similarity greater than 0.8, we multiply their confidence weight by 1.5 (assumption: few very similar answers provide more confidence)"
  - [Page 4]: "different models achieve optimal performance at different confidence cutoffs" (0.58-0.70 range)
  - [corpus]: Weak direct evidence—corpus papers don't validate this specific weighting scheme
- Break condition: Optimal cutoff varies by model and language; no universal threshold exists

## Foundational Learning

- **Sentence embeddings and cosine similarity**:
  - Why needed here: Core to both centroid polling (answer selection) and confidence calculation; uses `paraphrase-multilingual-mpnet-base-v2`
  - Quick check question: Can you explain why character n-grams are used for polling but sentence embeddings for confidence?

- **Machine translation fidelity and sp-BLEU**:
  - Why needed here: Pipeline performance degrades when translation quality is poor; understanding NLLB-200 limitations is critical
  - Quick check question: Given Yoruba sp-BLEU of 26.6 (Yor→Eng), would you expect MKA to help or hurt for Yoruba questions?

- **Abstention trade-offs (coverage vs. accuracy)**:
  - Why needed here: The paper introduces composite metrics (ACcomp, ACeff) because naive accuracy doesn't capture abstention value
  - Quick check question: Why is coverage multiplied into effective accuracy, and what failure mode does this prevent?

## Architecture Onboarding

- **Component map**:
  NLLB-200 (int8 quantized) -> Target LLM (Aya 8B / Gemma 2 / Qwen 2.5 7B) -> paraphrase-multilingual-mpnet-base-v2 -> Decision layer

- **Critical path**:
  1. Question + choices → translate to all auxiliary languages (parallel)
  2. Each translated prompt → LLM inference → raw response
  3. Each response → back-translate to target language
  4. All back-translations → centroid polling → selected answer
  5. Selected answer vs. all others → confidence score
  6. Confidence vs. cutoff → abstain or output

- **Design tradeoffs**:
  - Latency: O(auxiliary_languages × (translation + inference)) — 6 languages = ~12x translation + 6x inference overhead
  - Auxiliary language selection: High-resource (better translation) vs. language family (potential knowledge diversity) — paper uses resource-level groupings
  - Embedding model choice: Must support all auxiliary and target languages; mpnet-base-v2 covers 50+ languages

- **Failure signatures**:
  - Low consensus on high-confidence questions: Check translation quality (sp-BLEU scores available in NLLB paper)
  - Model answers incorrectly with high confidence: May indicate shared bias across languages
  - Instruction volatility (observed with Qwen-2.5 7B): Prompting template sensitivity — standardize and test

- **First 3 experiments**:
  1. **Reproduce baseline vs. MKA gap** on English with high-resource auxiliary set (n=200, seed=97) — expect ~15% improvement per Table 2
  2. **Ablate auxiliary language count**: Test with 2, 4, 6 languages to measure confidence calibration quality vs. latency
  3. **Stress-test on poor-translation pairs**: Run MKA on Yoruba target with different auxiliary sets to isolate translation-induced degradation from genuine multilingual benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can cross-lingual consensus be leveraged for abstention using internal model representations rather than external translation?
- Basis in paper: [explicit] The authors explicitly identify this as a direction for future work to address weaknesses like translation artifacts and reliance on disjointed post-inference techniques.
- Why unresolved: The current MKA pipeline relies on external machine translation systems (NLLB) and sentence embeddings, which introduces noise and computational overhead.
- What evidence would resolve it: A study demonstrating a method that utilizes internal activations or latent multilingual representations to achieve confidence calibration comparable to or better than the translation-based MKA pipeline.

### Open Question 2
- Question: Does replacing cosine similarity with a large language model (LLM) evaluator improve the accuracy of the MKA pipeline?
- Basis in paper: [explicit] The authors state in the Limitations section that relying on semantic distance via cosine similarity is a limitation that "may be improved by using commercial LLMs as evaluator."
- Why unresolved: Cosine similarity on embeddings may fail to capture nuanced semantic equivalence or may be sensitive to translation artifacts, potentially lowering calibration accuracy.
- What evidence would resolve it: Comparative experiments evaluating the composite accuracy ($AC_{comp}$) and effective accuracy ($AC_{eff}$) of the pipeline when the similarity scoring function is swapped for an LLM-based evaluator.

### Open Question 3
- Question: What is the quantitative relationship between machine translation quality and the success of the MKA pipeline?
- Basis in paper: [inferred] The paper notes that the performance drop in Yoruba "might be explained by the poor translation performance," whereas high-resource languages benefited, suggesting the pipeline's success is contingent on translation fidelity.
- Why unresolved: It remains unclear if there is a specific translation quality threshold (e.g., sp-BLEU score) required for the consensus mechanism to be effective rather than detrimental.
- What evidence would resolve it: An analysis correlating the translation system's BLEU/BERTScore metrics for various language pairs against the $\Delta$ accuracy improvement provided by the MKA pipeline.

## Limitations

- **Translation Quality Dependency**: MKA effectiveness is strongly correlated with auxiliary language translation fidelity, creating fundamental constraints for low-resource languages.
- **No Theoretical Justification for Hyperparameters**: Confidence thresholds (0.58-0.70) and similarity weights (1.5x) are heuristic choices without theoretical grounding.
- **Resource Overhead**: Requires 2×n translations plus n LLM inferences, creating significant computational and latency costs that scale with auxiliary language count.

## Confidence

**High Confidence**: Cross-lingual consensus improves abstention quality (consistent +11.5% accuracy gain across models and languages)
**Medium Confidence**: Specific design choices work well but lack systematic validation (auxiliary language selection, threshold values, centroid polling)
**Low Confidence**: Claims about why cross-lingual prompting surfaces latent knowledge are speculative (mechanism cited but not proven)

## Next Checks

1. **Ablation on Auxiliary Language Quality**: Systematically test MKA with auxiliary languages of varying translation quality (high-resource vs. low-resource) while holding model and target language constant to quantify the translation quality threshold below which MKA degrades performance.

2. **Hyperparameter Sensitivity Analysis**: Conduct a grid search over confidence thresholds (0.5-0.9) and similarity weights (1.0-2.0) across multiple model/language combinations to identify whether current parameters are near-optimal or arbitrary.

3. **Generalization to Non-Subjective Tasks**: Test MKA on factual QA, code generation, or reasoning tasks where hallucinations manifest differently than in multiple-choice settings, to evaluate whether cross-lingual consensus generalizes beyond the current evaluation domain.