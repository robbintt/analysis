---
ver: rpa2
title: BlueLM-2.5-3B Technical Report
arxiv_id: '2507.05934'
source_url: https://arxiv.org/abs/2507.05934
tags:
- data
- training
- arxiv
- reasoning
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BlueLM-2.5-3B is a compact multimodal large language model with
  2.9 billion parameters that supports both thinking and non-thinking modes while
  allowing explicit control over reasoning token budget. It achieves this through
  diversified data curation, key data resampling, hybrid heterogeneous reinforcement
  learning, and high-performance training infrastructure.
---

# BlueLM-2.5-3B Technical Report

## Quick Facts
- arXiv ID: 2507.05934
- Source URL: https://arxiv.org/abs/2507.05934
- Reference count: 40
- BlueLM-2.5-3B: 2.9B parameter multimodal model matching Qwen3-4B on text, trailing Kimi-VL-A3B-16B by 5% on multimodal tasks

## Executive Summary
BlueLM-2.5-3B is a compact multimodal large language model with 2.9 billion parameters that supports both thinking and non-thinking modes while allowing explicit control over reasoning token budget. It achieves this through diversified data curation, key data resampling, hybrid heterogeneous reinforcement learning, and high-performance training infrastructure. In thinking mode, it matches Qwen3-4B on text benchmarks and trails Kimi-VL-A3B-16B by only 5% on multimodal tasks. In non-thinking mode, it outperforms Qwen2.5-VL-3B on most multimodal benchmarks. The model uses 23% less total training data than comparable models while achieving superior reasoning capabilities.

## Method Summary
BlueLM-2.5-3B employs a hybrid training approach combining supervised fine-tuning with heterogeneous reinforcement learning. The model uses diversified data curation with key data resampling to optimize training efficiency. A Bi-mode Adaptive Thinking (BAT) module enables explicit control over reasoning token budget, allowing seamless switching between thinking and non-thinking modes. The heterogeneous reinforcement learning framework combines Group Relative Policy Optimization (GRL) and Contrastive Optimization with Value Regularization (COVR) to enhance both reasoning and general capabilities. The training infrastructure leverages distributed computing with 4-8 A100-80GB GPUs for efficient large-scale training.

## Key Results
- Matches Qwen3-4B performance on text benchmarks in thinking mode
- Outperforms Qwen2.5-VL-3B on most multimodal benchmarks in non-thinking mode
- Trails Kimi-VL-A3B-16B by only 5% on multimodal tasks while using 23% less training data

## Why This Works (Mechanism)
The model's success stems from its hybrid heterogeneous reinforcement learning approach that combines GRL and COVR to simultaneously optimize reasoning and general capabilities. The Bi-mode Adaptive Thinking module enables efficient reasoning token allocation, while diversified data curation with key data resampling ensures optimal use of limited training data. The compact 2.9B parameter architecture achieves strong performance through efficient knowledge distillation and targeted reinforcement learning rather than brute-force scaling.

## Foundational Learning
- **Heterogeneous Reinforcement Learning**: Combines multiple RL algorithms (GRL + COVR) to address different optimization objectives - needed for balancing reasoning depth with general performance; quick check: verify gradient updates from both components during training
- **Data Curation with Resampling**: Strategically selects and repeats high-value training examples - needed to maximize learning efficiency with limited data; quick check: analyze training loss curves for convergence patterns
- **Bi-mode Adaptive Thinking**: Controls reasoning token allocation between thinking and non-thinking modes - needed for computational efficiency and task-appropriate reasoning; quick check: measure reasoning token usage across different task categories
- **Knowledge Distillation**: Transfers capabilities from larger models to compact architecture - needed to achieve strong performance with fewer parameters; quick check: compare performance against teacher model on representative tasks
- **Multimodal Integration**: Processes text and visual inputs through unified architecture - needed for handling diverse input types; quick check: test modality-specific versus cross-modal performance
- **Parameter-Efficient Scaling**: Achieves strong performance with 2.9B parameters versus larger models - needed for edge deployment and reduced computational costs; quick check: benchmark inference speed and memory usage

## Architecture Onboarding

**Component Map:**
Input Processing -> Multimodal Encoder -> BAT Module -> Reasoning Controller -> Task-Specific Heads -> Output Generation

**Critical Path:**
Data Curation → Supervised Fine-tuning → Heterogeneous RL (GRL+COVR) → BAT Integration → Performance Optimization

**Design Tradeoffs:**
- **Parameter Count vs Performance**: 2.9B parameters chosen over larger models for edge deployment capability, accepting minor performance trade-offs
- **Data Efficiency vs Coverage**: 23% less training data used for efficiency, potentially limiting domain coverage
- **Thinking vs Non-thinking Modes**: Dual-mode operation adds complexity but enables computational efficiency and task-appropriate reasoning

**Failure Signatures:**
- Degradation in reasoning depth when reasoning token budget is too constrained
- Performance drops on specialized domains not well-represented in curated training data
- Mode switching failures when task requirements are ambiguous

**First 3 Experiments:**
1. Benchmark performance comparison across thinking and non-thinking modes on representative text and multimodal tasks
2. Reasoning token budget ablation study to identify optimal allocation for different task categories
3. Data efficiency validation by comparing learning curves with Qwen2.5-VL-3B using identical training schedules

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Single dataset (Chinchilla-1.4T) may introduce domain bias and limit generalizability
- 23% reduction in training data could mask potential coverage gaps in certain domains or reasoning patterns
- Heterogeneous reinforcement learning complexity may affect reproducibility, particularly BAT module implementation details

## Confidence
- **High confidence**: Parameter count (2.9B), model architecture, hardware requirements (A100-80GB GPUs), training infrastructure claims
- **Medium confidence**: Benchmark comparisons (5% gap vs Kimi-VL-A3B-16B), data efficiency claim (23% reduction)
- **Medium confidence**: Reasoning capabilities in thinking vs non-thinking modes, BAT module behavior

## Next Checks
1. Reproduce training data efficiency claim by comparing training steps and dataset composition against Qwen2.5-VL-3B and Kimi-VL-A3B-16B
2. Verify 5% performance gap on multimodal benchmarks through independent evaluation using same test sets and prompts
3. Test BAT module's reasoning token budget control across multiple reasoning-intensive tasks to confirm consistent behavior between thinking and non-thinking modes