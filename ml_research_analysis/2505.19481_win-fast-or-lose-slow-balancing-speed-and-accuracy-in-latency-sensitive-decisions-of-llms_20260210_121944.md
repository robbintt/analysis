---
ver: rpa2
title: 'Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions
  of LLMs'
arxiv_id: '2505.19481'
source_url: https://arxiv.org/abs/2505.19481
tags:
- latency
- quality
- inference
- trading
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the first systematic study of latency\u2013\
  quality trade-offs in real-time LLM agent decision tasks, where both response speed\
  \ and output quality determine performance. The authors develop two new benchmarks:\
  \ HFTBench, a high-frequency trading simulator, and StreetFighter, a competitive\
  \ gaming environment."
---

# Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs

## Quick Facts
- arXiv ID: 2505.19481
- Source URL: https://arxiv.org/abs/2505.19481
- Authors: Hao Kang; Qingru Zhang; Han Cai; Weiyuan Xu; Tushar Krishna; Yilun Du; Tsachy Weissman
- Reference count: 8
- Primary result: FPX adaptive mixed-precision framework achieves up to 80% win rate improvement in StreetFighter and 26.52% higher daily yield in trading over fixed-precision baselines.

## Executive Summary
This work introduces the first systematic study of latency-quality trade-offs in real-time LLM agent decision tasks, where both response speed and output quality determine performance. The authors develop two new benchmarks: HFTBench, a high-frequency trading simulator, and StreetFighter, a competitive gaming environment. They propose FPX, an adaptive mixed-precision inference framework that dynamically selects model size and layer-wise quantization (FP8/FP4) to balance latency and accuracy. FPX achieves up to 80% improvement in win rate on Street Fighter and up to 26.52% higher daily yield in trading over fixed-precision baselines. The results highlight the need for latency-aware evaluation and deployment strategies in real-world LLM applications.

## Method Summary
FPX is an adaptive mixed-precision inference framework that dynamically selects between FP8 and FP4 quantization for different layers based on their sensitivity to quantization error. The method first calibrates quantization error per layer using FP16 inference on Wikitext-2, then selects the lowest-error layers for FP4 assignment while keeping others at FP8. The fraction of FP4 layers (γ) is controlled to navigate the latency-accuracy Pareto frontier. Experiments use Qwen2.5 models (1.5B-14B) on RTX 5090, with 14B models using 2-GPU model parallelism. The framework is evaluated on two new benchmarks: HFTBench (high-frequency trading with Polygon.io data) and StreetFighter (competitive gaming with DIAMBRA integration).

## Key Results
- FPX achieves 80% win rate improvement in StreetFighter competitive gaming environment
- FPX delivers 26.52% higher daily yield in high-frequency trading compared to fixed-precision baselines
- Optimal γ=0.3 for StreetFighter (3B model) and γ=0.2 for HFTBench (14B model)

## Why This Works (Mechanism)

### Mechanism 1
Selective layer-wise quantization minimizes quality degradation while achieving latency gains. FPX computes per-layer relative error ε_l = ||A_fp16 - A_fp4||_2 / ||A_fp16||_2 during offline calibration, then assigns FP4 only to layers with lowest error while keeping sensitive layers at FP8. Core assumption: Transformer linear layers have uniform latency impact per precision change, allowing optimization focus on error minimization rather than latency modeling.

### Mechanism 2
Task-specific latency-quality optima exist; faster models can outperform higher-quality slower models in time-sensitive environments. In dynamic environments, reward depends on both action quality and execution timing—delayed actions become stale as the environment evolves during inference. Core assumption: The environment changes meaningfully during the inference latency window (Δt), making action a_{t+Δt} evaluated against E_{t+Δt} rather than E_t.

### Mechanism 3
Hybrid FP8/FP4 precision at the operator level enables fine-grained, continuous latency control. FPX allows γ ∈ [0,1] to specify the fraction of layers compressed to FP4, providing a continuous trade-off curve rather than discrete model/precision choices. Core assumption: Hardware supports efficient mixed-precision kernel dispatch without significant overhead from precision switching.

## Foundational Learning

- Concept: Floating-point quantization (FP8/FP4)
  - Why needed here: Understanding how bitwidth reduction maps to latency gains and quality loss; FPX relies on hardware-native FP8/FP4 kernels.
  - Quick check question: Given FP16 baseline latency of 600ms, estimate FP8 and FP4 latency if FP8 gives 2× speedup and FP4 gives 4× speedup.

- Concept: Latency-sensitive vs. latency-tolerant agent tasks
  - Why needed here: Distinguishing when latency optimization matters; FPX is designed for tasks where r = f(quality, latency).
  - Quick check question: Classify autonomous driving, code generation, and competitive gaming as latency-sensitive or latency-tolerant, and explain why.

- Concept: Pareto frontier optimization
  - Why needed here: FPX navigates the latency-accuracy Pareto curve; understanding trade-offs is essential for configuration.
  - Quick check question: If γ=0.2 gives 26.52% yield and γ=0.4 gives 12.93% yield, what does this tell you about the optimal operating point?

## Architecture Onboarding

- Component map: Calibration module -> Precision assignment module -> Runtime dispatcher
- Critical path: Calibration (offline, one-time) → Precision assignment (per γ configuration) → Runtime mixed-precision inference
- Design tradeoffs:
  - Layer-level vs. finer granularity: FPX operates at layer level for compatibility; token-level precision could improve trade-offs but requires kernel support.
  - Calibration dataset choice: Wikitext-2 is general; domain-specific calibration may better estimate ε_l for target tasks.
  - γ granularity: Discretized to 0.1 steps in experiments; finer steps may find better optima but increase search cost.
- Failure signatures:
  - γ > 0.6 causes complete performance collapse (Table 2 shows "–" for HFTBench at γ=0.6+).
  - Mismatched calibration distribution vs. task inputs leads to underestimated ε_l for sensitive layers.
  - Hardware without native FP8/FP4 support causes fallback to slow emulation paths.
- First 3 experiments:
  1. Reproduce calibration on Qwen2.5-7B with Wikitext-2, verify ε_l distribution and identify top-3 most/least sensitive layers.
  2. Run StreetFighter benchmark with γ ∈ {0.0, 0.2, 0.3, 0.4} on 3B model, plot latency vs. win rate curve to validate reported optimum at γ=0.3.
  3. Profile per-layer latency on RTX 5090 to verify uniformity assumption; if non-uniform, quantify impact on precision assignment optimality.

## Open Questions the Paper Calls Out

### Open Question 1
Can token-level or operator-level mixed precision control improve latency-quality trade-offs beyond layer-level assignment? The authors state that "more fine-grained schemes, such as token-level precision control, may unlock better trade-offs, but require significantly more complex implementation and kernel support. We left this optimization for future works."

### Open Question 2
Does task-specific calibration data improve precision assignment compared to general-purpose datasets like Wikitext-2? Section 4.2 describes offline calibration using Wikitext-2, but HFTBench and StreetFighter have domain-specific distributions that Wikitext-2 may not represent.

### Open Question 3
How well does FPX generalize across diverse model architectures and families beyond Qwen2.5? Section 5.1 states experiments use only Qwen2.5 models "to ensure a fair comparison and reduce the complexity of the search space," limiting conclusions to this family.

## Limitations

- Hardware dependency: FP8/FP4 kernels require Blackwell GPUs or equivalent support, limiting reproducibility
- Model family generalization: Results only validated on Qwen2.5 models; effectiveness on other architectures unknown
- Calibration representativeness: Wikitext-2 calibration may not capture quantization sensitivity for domain-specific tasks like trading and gaming

## Confidence

**High Confidence**: The core mechanism of layer-wise quantization error computation and selection is technically sound and well-explained. The experimental methodology for measuring latency and quality metrics is clearly specified.

**Medium Confidence**: The reported performance improvements are impressive but depend on specific hardware configurations that may not be widely available. The assumption of uniform latency impact per layer-precision change is reasonable but not rigorously validated.

**Low Confidence**: The exact implementation details of FP4 kernels and their availability across different hardware platforms. The calibration dataset's representativeness for diverse downstream tasks.

## Next Checks

1. **Hardware Independence Verification**: Implement a simulation of FPX using only FP8 and FP16 operations to quantify the maximum achievable gains independent of FP4 kernel availability. Compare the simulated results against the claimed FP4 benefits to establish a lower bound on performance improvements.

2. **Layer Latency Uniformity Test**: Profile per-layer execution times for both FP8 and FP16 across multiple model sizes and layer types. Quantify the deviation from uniformity assumption and measure how this affects the precision assignment optimality. This will determine if the error-only optimization approach remains valid when latency varies by layer.

3. **Cross-Architecture Generalization**: Apply FPX to at least two different model architectures (e.g., Llama and Mistral families) and evaluate performance on StreetFighter and HFTBench. This will test whether the layer sensitivity patterns observed in Qwen2.5 generalize across architectures and whether the calibration approach transfers effectively.