---
ver: rpa2
title: Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests
  instead
arxiv_id: '2507.23009'
source_url: https://arxiv.org/abs/2507.23009
tags:
- measurement
- personality
- llms
- human
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that interpreting LLM performance on human-designed
  psychological and educational tests as evidence of human-like traits (e.g., intelligence,
  personality) is epistemologically unsound. Human tests are theory-driven, population-specific
  measurement instruments that rely on validated latent constructs, calibrated for
  human populations.
---

# Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead

## Quick Facts
- **arXiv ID**: 2507.23009
- **Source URL**: https://arxiv.org/abs/2507.23009
- **Reference count**: 40
- **Primary result**: Applying human psychometric tests to LLMs is epistemologically unsound due to population-specific validity; AI-specific evaluation frameworks are needed.

## Executive Summary
This paper argues that interpreting LLM performance on human-designed psychological and educational tests as evidence of human-like traits (e.g., intelligence, personality) is fundamentally flawed. Human tests are theory-driven measurement instruments calibrated for specific human populations and validated through rigorous psychometric methods. The authors demonstrate that applying these tests to LLMs without empirical validation risks misinterpreting what is being measured. They propose developing principled, AI-specific evaluation frameworks that leverage the unique experimental advantages of machine learning systems, including direct causal interventions and full access to output probabilities.

## Method Summary
The authors conducted a proof-of-concept experiment applying the Big Five Inventory-2 (BFI-2) personality test to GPT-4. They administered 60 items sequentially using a conversational prompt format, collecting 100 responses per item with temperature set to 1.0. The analysis used Principal Component Analysis (PCA) with Varimax rotation to examine the factor structure of LLM responses. The experiment compared factor loadings for specific items (e.g., "Is inventive...") against human baseline data from Soto & John (2017). The methodology aimed to demonstrate measurement non-invariance by showing that items that load highly onto human personality factors (e.g., Open-mindedness) show weak loadings when applied to LLMs.

## Key Results
- Human psychometric tests rely on factor loadings calibrated for specific human populations that do not transfer to LLMs
- LLMs scoring "average" on personality tests often do so through question balancing artifacts rather than genuine trait measurement
- The paper proposes AI-specific evaluation frameworks using measurement invariance testing and nomological networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Applying human psychometric tests to LLMs constitutes an "ontological error" because validity is population-specific, not universal.
- **Mechanism**: Psychometric instruments (e.g., Big Five, IQ tests) rely on factor loadings—statistical correlations between test items and latent traits calibrated on specific human populations. LLMs do not belong to the target population and may not share the underlying causal structure (embodiment, feelings). Therefore, high scores on test items do not logically imply the presence of the latent human trait.
- **Core assumption**: The statistical link between a test question and a latent trait (validity) does not automatically transfer across distinct classes of cognitive systems (biological vs. statistical models).
- **Evidence anchors**: [abstract]: "...human psychological and educational tests are theory-driven measurement instruments, calibrated to a specific human population." [section 3.2]: "...the factor loading structure of the items that has been validated in the cycle (c)→(d)→(e) is a priori not the same for LLMs."

### Mechanism 2
- **Claim**: AI evaluation requires measurement invariance testing to verify that instruments measure constructs equivalently across different model groups.
- **Mechanism**: Before comparing "intelligence" scores between Model A and Model B, one must verify that the measurement model holds. If factor loadings differ significantly between groups—meaning questions tap into different underlying abilities for different models—the scores are incomparable.
- **Core assumption**: Constructs like "reasoning" manifest differently across architectures or fine-tuning regimes.
- **Evidence anchors**: [section 4.2]: "Measurement invariance is necessary to establish that we measure what we aim to measure for different populations." [section 4.2]: "...we cannot claim measurement invariance of a benchmark like MMLU between models, when they are fine-tuned to different degrees on different data."

### Mechanism 3
- **Claim**: Validating AI-specific constructs requires Nomological Networks and falsifiable hypotheses rather than simple benchmark correlations.
- **Mechanism**: A construct (e.g., "Machine Intelligence") is valid only if it fits within a network of theoretical predictions (e.g., "Intelligence should correlate with efficiency"). Validity is strengthened by "risky" tests that could refute the theory, rather than "strange coincidences" (e.g., random answers averaging to human mean scores).
- **Core assumption**: Theoretical constructs for AI must have predictive power beyond the immediate test items.
- **Evidence anchors**: [section 4.1]: "...comparisons that would be expected under many vague theories cannot meaningfully support scientific claims." [section 4.3]: "Cronbach and Meehl proposes to embed such claims in a network of scientific evidence, called Nomological Networks."

## Foundational Learning

- **Concept**: Latent Variable Modeling
  - **Why needed here**: The paper argues that "intelligence" or "personality" are latent (hidden) variables inferred from observables (answers). You cannot understand the critique without grasping that test items are merely proxies for the hidden variable, not the variable itself.
  - **Quick check question**: If an LLM answers "I am outgoing" (Item A) and "I am quiet" (Item B) both as "Strongly Agree," does this indicate high Extraversion or a failure of the mapping function?

- **Concept**: Factor Loadings ($\lambda_i$)
  - **Why needed here**: This is the statistical "glue" the paper claims is broken. Factor loadings define how much "signal" of a trait is in an answer.
  - **Quick check question**: In the paper's GPT-4 experiment, did the item "I am inventive" load highly onto the Open-mindedness factor for the LLM like it did for humans?

- **Concept**: Confirmatory Factor Analysis (CFA)
  - **Why needed here**: The paper proposes CFA as the technical method to test for measurement invariance.
  - **Quick check question**: You run CFA on two groups (Humans vs. LLMs). If the model fit indices collapse when you constrain the loadings to be equal, what have you proven about the test?

## Architecture Onboarding

- **Component map**: Subject Domain (LLM's behavior/capabilities) -> Theoretical Abstraction (AI-specific constructs, e.g., "Context Retention") -> Measurement Instrument (Benchmark/Test items) -> Validation Layer (Invariance checks & Nomological Networks)

- **Critical path**:
  1. Define a construct relevant to AI behavior (not blindly copying human constructs)
  2. Design items or interventions that theoretically correlate with this construct
  3. Verify Measurement Invariance: Do these items correlate with the construct consistently across different model types?
  4. Embed in a Nomological Network: Does the score predict external behaviors?

- **Design tradeoffs**:
  - Proxy Validity vs. Experimental Power: Using human tests provides easy anthropomorphic interpretations but lacks scientific validity. Building AI-specific tests from scratch allows for causal interventions and valid measurement but requires massive foundational theoretical work.
  - Rankings vs. Measurement: Leaderboards are useful for ranking models (ordinal comparison), but the paper argues they fail at measurement (interval/ratio comparison of traits) without invariance.

- **Failure signatures**:
  - "Strange Coincidences": An LLM scores "average" on a personality test simply by answering neutrally or repetitively due to question balancing (True-keyed vs. False-keyed).
  - Sensitivity to Artifacts: Scores fluctuate wildly with minor prompt changes (e.g., answer ordering), indicating the test measures sensitivity to distribution rather than the latent trait.

- **First 3 experiments**:
  1. Replicate the Validity Failure: Run the Big Five Inventory on a target LLM and perform PCA. Check if the factor structure matches human norms (likely won't).
  2. Invariance Testing: Fine-tune a model on legal data and another on code. Test both on MMLU. Verify if the "Law" subset measures the same underlying "general intelligence" factor for both using CFA.
  3. Construct a Counterfactual Test: Design a test where you intervene directly on the model (e.g., alter system prompt). If the "personality" score doesn't change in response to a theoretically relevant intervention, the construct lacks validity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Should machine learning models be evaluated as statistical individuals, populations, or a new category entirely, and how does this choice impact the calibration of measurement instruments?
- **Basis in paper**: [explicit] The authors ask, "Should we evaluate ML models as individuals, populations or something else?" and note that "large parts of measurement theory... require a well-defined target population."
- **Why unresolved**: There is no consensus on the statistical unit; treating models as individuals ignores their ability to represent distributions via temperature/personas, while treating them as populations makes defining a "representative sample" difficult given rapid model iteration.
- **What evidence would resolve it**: A theoretical framework that defines the statistical unit for LLMs, successfully demonstrated by calibrating a measurement instrument that remains valid across different contexts (e.g., prompting strategies).

### Open Question 2
- **Question**: What are meaningful, ML-specific constructs that can validly describe differences in model behavior without relying on anthropocentric traits like "personality" or "intelligence"?
- **Basis in paper**: [explicit] The paper explicitly asks, "What are meaningful, ML-specific constructs?" and argues that "instead of trying to use human constructs that might not work well for LLMs, we should define new constructs that distinguish LLMs in meaningful ways."
- **Why unresolved**: Current constructs like "Open-mindedness" rely on human embodiment and social context. It is unclear what the latent variables are that actually drive variance in LLM outputs, other than simple metrics like model size or compute.
- **What evidence would resolve it**: The derivation of new latent constructs (e.g., distinct behavioral dimensions) that explain variance in model performance better than human psychometric constructs, validated through a nomological network.

### Open Question 3
- **Question**: How can the unique capacity for causal intervention in ML systems (e.g., weight manipulation, counterfactuals) be formalized to validate measurement models?
- **Basis in paper**: [explicit] The authors ask "How to leverage the unique context of ML?" and state, "We can directly probe causal relationships through counterfactual interventions... This experimental power, brings the potential to overcome limitations in psychological and educational testing."
- **Why unresolved**: While ML researchers can alter internal states and data in ways impossible with human subjects, there is no standard framework for using these interventions to establish the causal link required for validity between a construct and a measurement outcome.
- **What evidence would resolve it**: A validation protocol where specific causal interventions (e.g., fine-tuning on a specific skill) predictably alter the measurement score in accordance with the theoretical definition of the latent construct.

## Limitations

- The primary limitation is the assumption that LLM internal representations and cognitive processes fundamentally differ from human cognition in ways that invalidate psychometric tests, which remains largely theoretical
- The paper's experimental validation (Big Five on GPT-4) provides only a single case study, and the absence of human baseline replication in the same experimental conditions creates uncertainty about the magnitude of divergence
- The broader assertion that LLMs cannot possess human-like traits (intelligence, personality) due to ontological differences requires deeper investigation of LLM internal mechanisms

## Confidence

- **High Confidence**: The epistemological argument that psychometric validity is population-specific and cannot be assumed across cognitive systems
- **Medium Confidence**: The specific claim that current human tests fail on LLMs, based on the PCA analysis showing low factor loadings for Open-mindedness items
- **Low Confidence**: The broader assertion that LLMs cannot possess human-like traits (intelligence, personality) due to ontological differences

## Next Checks

1. **Cross-model Invariance Testing**: Apply the Big Five Inventory to multiple LLMs (GPT-4, Claude, LLaMA) and verify whether measurement invariance fails consistently across architectures, or if some models show better alignment with human factor structures.

2. **Causal Intervention Validation**: Design experiments where model parameters or system prompts are systematically varied (e.g., altering temperature, changing persona instructions) to test whether "personality" scores respond in theoretically predictable ways. A valid construct should show measurable changes under relevant interventions.

3. **Nomological Network Construction**: Build a preliminary AI-specific construct (e.g., "Context Retention") and test whether it correlates with theoretically related behaviors (memory span, chain-of-thought consistency) across multiple tasks and model families.