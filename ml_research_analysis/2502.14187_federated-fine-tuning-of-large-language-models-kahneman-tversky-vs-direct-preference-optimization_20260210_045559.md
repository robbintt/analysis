---
ver: rpa2
title: 'Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct
  Preference Optimization'
arxiv_id: '2502.14187'
source_url: https://arxiv.org/abs/2502.14187
tags:
- fine-tuning
- data
- federated
- learning
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares Kahneman-Tversky Optimization (KTO) with Direct
  Preference Optimization (DPO) for fine-tuning large language models in federated
  learning settings. KTO is evaluated against DPO using Alpaca-7B as the base model,
  with experiments conducted on a realistic dataset and assessed using MT-Bench-1,
  Vicuna, and AdvBench benchmarks.
---

# Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct Preference Optimization

## Quick Facts
- arXiv ID: 2502.14187
- Source URL: https://arxiv.org/abs/2502.14187
- Reference count: 20
- KTO consistently outperforms DPO across benchmarks in federated fine-tuning

## Executive Summary
This paper compares Kahneman-Tversky Optimization (KTO) with Direct Preference Optimization (DPO) for fine-tuning large language models in federated learning settings. KTO is evaluated against DPO using Alpaca-7B as the base model, with experiments conducted on a realistic dataset and assessed using MT-Bench-1, Vicuna, and AdvBench benchmarks. The study introduces a redistributed dataset setup, where KTO's ability to handle single-response feedback gives it an advantage over DPO, which requires paired responses. Results show that KTO consistently outperforms DPO across all benchmarks, with KTOR (redistributed KTO) maintaining superior performance even in scenarios where DPO cannot be applied. The findings highlight KTO as a robust and scalable fine-tuning method for federated learning, particularly in privacy-preserving and heterogeneous environments.

## Method Summary
The paper conducts experiments using Alpaca-7B as the base model, comparing KTO and DPO in federated learning settings. A redistributed dataset setup is introduced where KTO can handle single-response feedback while DPO requires paired responses. Both methods are evaluated using MT-Bench-1, Vicuna, and AdvBench benchmarks. The experiments focus on how each optimization method performs under realistic federated conditions, particularly examining their ability to process different types of preference feedback.

## Key Results
- KTO consistently outperforms DPO across all benchmark tests
- KTOR (redistributed KTO) maintains superior performance in scenarios where DPO cannot be applied
- KTO demonstrates better handling of single-response feedback compared to DPO

## Why This Works (Mechanism)
KTO's advantage stems from its ability to work with single-response feedback, unlike DPO which requires paired responses. This makes KTO more suitable for federated learning environments where complete preference pairs may not be available. The optimization framework better handles the heterogeneity and privacy constraints inherent in federated settings by not requiring explicit comparison data between responses.

## Foundational Learning
- Federated Learning: Distributed model training across multiple clients while preserving privacy - needed for understanding the deployment context
- Preference Optimization: Methods for aligning language models with human preferences - needed for comparing KTO and DPO
- Reinforcement Learning from Human Feedback (RLHF): Framework for incorporating human preferences into model training - needed for understanding optimization objectives

## Architecture Onboarding
- Component Map: Base Model (Alpaca-7B) -> Optimization Layer (KTO/DPO) -> Federated Aggregation -> Benchmark Evaluation
- Critical Path: Data Distribution → Local Fine-tuning → Federated Aggregation → Performance Evaluation
- Design Tradeoffs: Single-response flexibility (KTO) vs. paired-response requirements (DPO)
- Failure Signatures: KTO may struggle with highly correlated single responses; DPO fails entirely without paired data
- First Experiments: 1) Test KTO with various base model scales, 2) Evaluate KTO in heterogeneous data distributions, 3) Compare KTO against additional preference optimization methods

## Open Questions the Paper Calls Out
None

## Limitations
- Focus exclusively on Alpaca-7B base model limits generalizability to other architectures
- Redistributed dataset may not capture full diversity of real-world federated environments
- Evaluation relies on three specific benchmarks that may not comprehensively represent all performance aspects
- Does not explore computational efficiency or communication overhead critical in federated learning

## Confidence
- Core claim (KTO outperforms DPO in federated fine-tuning): High
- Generalizability to other model scales: Medium
- Practical applicability in real-world federated systems: Low

## Next Checks
1. Replicate experiments with larger models (e.g., LLaMA-33B or beyond) to assess scalability and performance trends
2. Test KTO in heterogeneous federated environments with varying data distributions and feedback mechanisms to evaluate robustness
3. Conduct ablation studies on KTO's hyperparameters and compare against additional preference optimization methods to isolate performance drivers