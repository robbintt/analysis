---
ver: rpa2
title: 'Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations
  from Text'
arxiv_id: '2507.19969'
source_url: https://arxiv.org/abs/2507.19969
tags:
- data
- answer
- code
- visualization
- chart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Text2Vis, a comprehensive benchmark for evaluating
  LLMs on text-to-visualization tasks, covering over 20 chart types and complex data
  science queries. It includes 1,985 samples with data tables, natural language queries,
  short answers, visualization code, and annotated charts, supporting diverse tasks
  like trend analysis, correlation, outlier detection, and predictive analytics.
---

# Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text

## Quick Facts
- arXiv ID: 2507.19969
- Source URL: https://arxiv.org/abs/2507.19969
- Authors: Mizanur Rahman; Md Tahmid Rahman Laskar; Shafiq Joty; Enamul Hoque
- Reference count: 40
- Primary result: Introduces Text2Vis benchmark with 1,985 samples and achieves 42% pass rate using actor-critic refinement

## Executive Summary
Text2Vis introduces a comprehensive benchmark for evaluating LLMs on text-to-visualization tasks, covering over 20 chart types and complex data science queries. The benchmark includes 1,985 samples with data tables, natural language queries, short answers, visualization code, and annotated charts. The authors propose a cross-modal actor-critic agentic framework that jointly refines textual answers and visualization code, improving GPT-4o's pass rate from 26% to 42% over direct inference. They also introduce an automated LLM-based evaluation framework for scalable assessment without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy.

## Method Summary
The Text2Vis benchmark provides 1,985 samples containing data tables, natural language queries, short answers, visualization code, and annotated charts. The method employs two inference approaches: direct zero-shot/RAG prompting and an actor-critic agentic framework. The actor generates initial ⟨answer, code⟩ pairs, while the critic provides structured multi-modal feedback (answer correctness, code syntax/semantics, visual clarity). The actor refines outputs based on this feedback in one iteration. Evaluation uses GPT-4o as an automated judge with deterministic parameters (temperature=0.1) to score outputs on answer match, readability, and chart correctness.

## Key Results
- Actor-critic framework improves GPT-4o pass rate from 26% to 42% on Text2Vis benchmark
- Automated LLM-based evaluation correlates with human judgments (Pearson r=0.85-0.89, Cohen's Kappa=0.78)
- Multi-modal feedback synergy: Combined Answer+Code feedback outperforms single modalities by 8-14 percentage points
- Error analysis reveals distinct failure modes: syntax errors, logical reasoning errors, data import issues, instruction-following failures

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Actor-critic Refinement Loop
Iterative refinement with structured multi-modal feedback improves both answer accuracy and visualization quality beyond single-pass inference. Actor generates initial ⟨answer, code⟩ → Critic evaluates across three modalities → Actor incorporates feedback to refine output in one iteration. Evidence shows answer match increased from 42% to 53% and chart correctness from 3.15 to 4.02 with combined feedback.

### Mechanism 2: LLM-Based Automated Evaluation as Proxy Judge
LLM-based evaluation closely aligns with human judgments, enabling scalable benchmarking without manual annotation. GPT-4o evaluates outputs against structured rubrics with deterministic parameters. Human evaluation on 236 samples showed high correlation (r=0.85-0.89) and repeatability (97.5% across 5 runs).

### Mechanism 3: Multi-Modal Feedback Synergy
Combining answer, code, and visual feedback outperforms any single feedback modality alone. Each modality captures orthogonal error types that compound when unaddressed. Table 4c shows combined Answer+Code feedback (42% pass) significantly outperforms single modalities (28-34%).

## Foundational Learning

- **Concept: Text-to-Visualization as Multi-Output Generation**
  - Why needed here: Unlike NL2SQL, Text2Vis requires simultaneous generation of precise numerical answer AND executable visualization code AND visual quality
  - Quick check question: Can you explain why generating a correct answer doesn't guarantee a correct visualization, and vice versa?

- **Concept: Reflective/Actor-Critic LLM Workflows**
  - Why needed here: The agentic framework relies on separation of generation (actor) and evaluation (critic) with structured feedback passing
  - Quick check question: What is the trade-off between multiple refinement iterations vs. single-pass inference in terms of latency and improvement gains?

- **Concept: Multi-Modal Error Taxonomy**
  - Why needed here: Error analysis reveals distinct failure modes requiring different feedback strategies
  - Quick check question: If code executes successfully but the chart is misleading, which feedback modality would catch this?

## Architecture Onboarding

- **Component map:** Table + Query → Actor → ⟨Answer, Code⟩ → Execution → Critic → Feedback → Refined Output → Evaluator

- **Critical path:**
  1. Input: Table (CSV/JSON) + natural language query
  2. Actor generates JSON {answer, visualization_code}
  3. Execute code → produce chart image
  4. Critic analyzes: answer vs. ground truth, code errors, visual clarity
  5. Actor refines with feedback → final output
  6. Evaluator scores against 4 metrics

- **Design tradeoffs:**
  - Single vs. multiple refinement iterations: Paper uses 1 round for efficiency; additional rounds may yield diminishing returns
  - Self-critique vs. cross-model critique: Cross-model provides diversity but increases API cost
  - LLM judge vs. human evaluation: Automated scales to 1,985 samples in 5 min/$2 vs. ~33 human hours

- **Failure signatures:**
  - Syntax/runtime errors: Missing imports, shape mismatches, indentation issues
  - Data import issues: `df` undefined, date format mismatches, web retrieval failures
  - Logical errors: Incorrect metric selection, multi-step calculation mistakes
  - Instruction-following failures: Ignoring provided data, calling `pd.read_csv()` instead

- **First 3 experiments:**
  1. Baseline establishment: Run zero-shot direct inference on 50 samples with GPT-4o; measure all 4 metrics to confirm ~26% pass rate
  2. Ablation on feedback modality: Compare Answer-only, Code-only, Visual-only, and combined feedback on same subset; expect combined to outperform single modalities by 8-14 percentage points
  3. Judge consistency check: Run GPT-4o evaluation on 20 samples twice with identical prompts; verify <2% variance in pass rates to confirm evaluation stability

## Open Questions the Paper Calls Out

- **Open Question 1:** How does incorporating dynamic planning and adaptive tool selection into the actor-critic framework impact the success rate of autonomous visualization generation? The paper suggests future work could enable models to autonomously decide when to revise, retrieve external information, or switch visualization libraries.

- **Open Question 2:** Can the Text2Vis benchmark and evaluation framework be effectively extended to support the direct generation of interactive, web-based visualization grammars like Vega-Lite or D3.js? Current benchmark focuses exclusively on Python libraries (Matplotlib/Seaborn).

- **Open Question 3:** To what extent do automated LLM-based evaluators exhibit systematic biases when assessing the aesthetics and interpretability of visualizations compared to human experts? While overall alignment is validated, specific systematic biases regarding visual style or complexity remain unexplored.

## Limitations

- **Data serialization format** for embedding tables in prompts is unspecified, which could affect model performance
- **RAG implementation details** lack critical parameters like embedding dimension, top-k selection, and context window specifications
- **Visual feedback mechanism** for processing generated chart images and incorporating them into the actor-critic loop is not detailed

## Confidence

- **Cross-modal actor-critic improvement (26%→42%):** High - Results are clearly presented with controlled ablation studies
- **LLM judge correlation with human evaluation (r=0.85-0.89):** High - Multiple validation methods reported with statistical significance
- **Multi-modal feedback synergy:** High - Systematic ablation experiments support the claim
- **Scalability of automated evaluation:** High - Cost and runtime estimates are provided and reasonable

## Next Checks

1. **Serialization format validation:** Test direct inference with different table formats (JSON, CSV, markdown) on a subset of 50 samples to identify optimal serialization

2. **Ablation replication:** Replicate actor-critic ablation study on 100 samples to verify the 8-14 percentage point improvement differential

3. **Judge consistency verification:** Run automated evaluation on 50 samples five times with identical parameters to confirm <2% variance in pass rates