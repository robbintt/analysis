---
ver: rpa2
title: An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid
  Literature
arxiv_id: '2509.15292'
source_url: https://arxiv.org/abs/2509.15292
tags:
- papers
- literature
- similarity
- semantic
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an automated literature review pipeline that
  uses semantic similarity to efficiently retrieve and filter relevant academic papers.
  The system generates keywords from a paper's title and abstract, fetches papers
  from arXiv, and ranks them using transformer-based embeddings and cosine similarity.
---

# An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature

## Quick Facts
- arXiv ID: 2509.15292
- Source URL: https://arxiv.org/abs/2509.15292
- Authors: Abhiyan Dhakal; Kausik Paudel; Sanjog Sigdel
- Reference count: 25
- Primary result: Automated literature review pipeline using semantic similarity with statistical thresholding achieves scalable retrieval with minimal manual effort

## Executive Summary
This paper introduces an automated literature review pipeline that leverages semantic similarity to efficiently retrieve and filter relevant academic papers. The system generates keywords from a paper's title and abstract, fetches papers from arXiv, and ranks them using transformer-based embeddings and cosine similarity. Three embedding models—TF-IDF, all-MiniLM-L6-v2, and Specter2—were evaluated. The pipeline offers a scalable, low-overhead alternative to traditional systematic literature reviews, though limitations include the lack of ground-truth relevance labels and domain generalization constraints.

## Method Summary
The pipeline begins by generating 5-10 keywords from the input paper's title and abstract using an LLM (gemini-2.0-flash). These keywords are used to fetch relevant papers from arXiv (max 20 per keyword), with deduplication based on metadata. Papers are embedded using all-MiniLM-L6-v2, and cosine similarity scores are computed against the input embedding. A statistical thresholding approach (Q3 + 0.5×IQR) filters papers, treating relevance as a statistical outlier. The system then extracts structured sections (Intro, Methods, etc.) using regex patterns, summarizes each paper via LLM, and outputs structured JSON summaries with BibTeX citations.

## Key Results
- TF-IDF model retrieved 19 papers with threshold 0.204
- all-MiniLM-L6-v2 retrieved 20 papers with threshold 0.659
- Specter2 achieved high similarity scores but required stricter thresholding due to score saturation
- Statistical thresholding effectively filters relevant papers while maintaining scalability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The pipeline retrieves relevant papers by mapping semantic similarity to a statistical outlier threshold.
- **Mechanism:** The system embeds the input text and candidate papers using `all-MiniLM-L6-v2`. It calculates cosine similarity and retains papers falling above the upper whisker of the similarity distribution ($Q3 + 0.5 \times IQR$), treating relevance as a statistical deviation rather than a fixed score.
- **Core assumption:** High semantic relevance correlates with being a statistical "outlier" in the similarity score distribution of the fetched batch.
- **Evidence anchors:**
  - [abstract]: "...ranks them based on their semantic closeness... statistical thresholding approach is then applied..."
  - [section]: III.C mentions adopting a "conservative upper bound of Q3 + 0.5×IQR" to ensure high precision.
  - [corpus]: Corpus evidence is weak for this specific statistical method; neighbors focus on general semantic search.
- **Break condition:** If the fetched batch of papers is uniformly relevant (high density), the IQR method may discard high-quality papers (false negatives), or conversely, fail to filter irrelevant papers in a noisy batch.

### Mechanism 2
- **Claim:** LLM-driven keyword expansion bridges the vocabulary gap between a paper's abstract and the repository's metadata.
- **Mechanism:** Instead of relying solely on the input text, an LLM (`gemini-2.0-flash`) generates 5–10 keywords. These keywords act as diverse probes into the arXiv database, mitigating the lexical mismatch that occurs when authors use different terminology for similar concepts.
- **Core assumption:** The LLM can infer meaningful search terms that exist in the target repository's vocabulary better than simple extraction from the source text.
- **Evidence anchors:**
  - [abstract]: "...generates relevant keywords, fetches relevant papers..."
  - [section]: III.A states keywords are generated to "capture both explicit and semantically related terms."
- **Break condition:** If the LLM hallucinates overly specific or nonexistent terms, the arXiv API return set may be empty or irrelevant.

### Mechanism 3
- **Claim:** Structured section extraction improves the fidelity of automated literature synthesis.
- **Mechanism:** The pipeline uses regex to slice PDFs into standard sections (Intro, Methods, etc.) before summarization. This forces the LLM to map summaries to a fixed schema (`problem_statement`, `methodology`, etc.), ensuring the final review compares like-for-like elements across papers.
- **Core assumption:** Scientific PDFs consistently follow standard structural patterns detectable by regex (e.g., `\bintroduction\b`).
- **Evidence anchors:**
  - [section]: III.D defines regex patterns to "identify and delineate different sections."
  - [section]: III.E describes mapping extracted text to a structured JSON summary.
- **Break condition:** If PDFs are scanned images or use non-standard headings (e.g., "Proposed Approach" instead of "Methodology"), the regex fails, leading to empty or malformed inputs for the summarizer.

## Foundational Learning

- **Concept: Cosine Similarity**
  - **Why needed here:** This is the core metric used to rank papers. Understanding that it measures the angle between embedding vectors (orientation) rather than magnitude is crucial for tuning thresholds.
  - **Quick check question:** If two papers have identical content but one is much longer (larger magnitude), how does cosine similarity handle them compared to Euclidean distance?

- **Concept: Interquartile Range (IQR) for Outlier Detection**
  - **Why needed here:** The pipeline uses IQR to set adaptive thresholds. You must understand that this method is non-parametric (distribution-free) to appreciate why the authors chose it over fixed thresholds for varying query results.
  - **Quick check question:** Why might a "Q3 + 0.5 * IQR" threshold fail to retrieve *any* papers if the initial fetch results in a uniform distribution of low scores?

- **Concept: Transformer Embeddings (SBERT vs. Specter2)**
  - **Why needed here:** The paper evaluates general-purpose (`all-MiniLM-L6-v2`) vs. domain-specific (`Specter2`) embeddings. Knowing that domain-specific models often saturate scores (high baseline similarity) explains why the authors faced "stricter thresholding" issues.
  - **Quick check question:** Why would a model trained specifically on scientific citations (Specter2) produce a negatively skewed similarity distribution compared to a general model?

## Architecture Onboarding

- **Component map:** Input -> Keyword Generator -> Retrieval -> Filter -> Processing -> Output
- **Critical path:** The **Retrieval -> Filter** loop. If the keyword generation fails to fetch a broad enough candidate set, the statistical thresholding has insufficient data to separate signal from noise.
- **Design tradeoffs:**
  - **TF-IDF vs. Transformers:** TF-IDF is fast but misses semantics; Transformers capture meaning but are computationally heavier and prone to "score saturation" (Specter2).
  - **Fixed vs. Adaptive Threshold:** The paper chooses adaptive (IQR-based) to handle varying query quality, but acknowledges it lacks semantic guarantee (Limitation V.C).
  - **Regex vs. Heuristic Parsing:** Regex is rigid but transparent; an ML-based section splitter would be more robust but adds complexity.
- **Failure signatures:**
  - **Specter2 Saturation:** Retrieving too many false positives because almost all scientific papers seem "similar" (scores clustered near 0.9).
  - **Regex Miss:** The summarizer receives empty strings for "Methodology" because the PDF used "System Design" instead.
  - **LLM Hallucination:** Keywords generated are valid words but return zero results in arXiv.
- **First 3 experiments:**
  1. **Threshold Sensitivity Analysis:** Run a known set of relevant/irrelevant papers through the pipeline and plot Precision/Recall curves for the IQR multiplier (e.g., $0.5 \times IQR$ vs $1.0 \times IQR$).
  2. **Section Extraction Accuracy:** Test the regex patterns against a sample of 50 PDFs from different domains to measure the percentage of successfully extracted sections vs. missed sections.
  3. **Model Generalization:** Compare `all-MiniLM-L6-v2` against `Specter2` on a highly specialized domain (e.g., quantum physics) to verify if Specter2's "saturation" issue is domain-dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive thresholding strategy dynamically adjust to score distributions to improve retrieval precision compared to the static $Q3 + 0.5 \times IQR$ heuristic?
- Basis in paper: [explicit] The authors state future work will "implement adaptive thresholding" and note the current statistical heuristic "does not guarantee semantic relevance."
- Why unresolved: The current implementation relies on a fixed statistical cutoff which may include irrelevant papers in the upper tail or exclude relevant papers in sparse corpora.
- What evidence would resolve it: A comparative study measuring precision and recall against a ground-truth dataset using the proposed adaptive method versus the static baseline.

### Open Question 2
- Question: To what extent does the pipeline's automated ranking and summarization correlate with human expert assessments of relevance?
- Basis in paper: [explicit] The paper lists "human-in-the-loop evaluation" as a key area for future work to provide qualitative insights and a ground-truth basis.
- Why unresolved: The current evaluation relies solely on quantitative metrics (cosine similarity distributions) without qualitative assessment or expert verification of the retrieved papers.
- What evidence would resolve it: A user study involving domain experts rating the relevance of the retrieved papers and the accuracy of the generated summaries.

### Open Question 3
- Question: Does integrating citation network data (e.g., co-citation patterns or PageRank) enhance retrieval accuracy beyond semantic similarity alone?
- Basis in paper: [explicit] The abstract and conclusion explicitly propose "citation network integration" with sources like Semantic Scholar to enable network-based relevance scoring.
- Why unresolved: The current system retrieves papers based solely on textual semantic similarity and keywords, ignoring the structural relationships between papers that often indicate relevance.
- What evidence would resolve it: An ablation study comparing the semantic-only pipeline against a hybrid model that incorporates citation graph features.

## Limitations

- Ground-truth relevance labels absent: The evaluation relies on corpus overlap and retrieval counts rather than manual relevance judgments, making precision/recall estimates speculative.
- Domain generalization constraints: The study does not test the pipeline outside arXiv's STEM corpus, leaving open whether statistical thresholding and embeddings generalize to social sciences or humanities.
- Model-specific thresholds: The IQR method works for one embedding model but lacks guarantees for others (e.g., Specter2 saturation); no universal rule for choosing Q3 + k×IQR is provided.

## Confidence

- **High confidence**: The statistical thresholding mechanism (IQR outlier detection) is mathematically sound and directly traceable to the paper's methodology.
- **Medium confidence**: Keyword generation via LLM improves retrieval diversity; however, no ablation study shows how much it outperforms simple text extraction.
- **Low confidence**: Claims about "scalability" and "low overhead" are not empirically supported—no runtime benchmarks or large-scale tests are reported.

## Next Checks

1. **Manual relevance annotation**: Have two independent domain experts label a sample of retrieved papers as relevant/non-relevant and compute precision/recall for each embedding model.
2. **Cross-domain evaluation**: Run the pipeline on a non-STEM dataset (e.g., arXiv's quantitative finance papers) and compare retrieval quality to the original test set.
3. **Threshold sensitivity analysis**: Systematically vary the IQR multiplier (0.5 to 2.0) and plot the precision-recall tradeoff curve for each embedding model.