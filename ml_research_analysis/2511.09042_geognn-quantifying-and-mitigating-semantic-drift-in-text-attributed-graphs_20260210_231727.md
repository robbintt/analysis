---
ver: rpa2
title: 'GeoGNN: Quantifying and Mitigating Semantic Drift in Text-Attributed Graphs'
arxiv_id: '2511.09042'
source_url: https://arxiv.org/abs/2511.09042
tags:
- semantic
- manifold
- drift
- aggregation
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of semantic drift in graph neural
  networks (GNNs) operating on text-attributed graphs, where linear aggregation of
  PLM-based embeddings distorts the intrinsic non-linear geometry of the semantic
  manifold, degrading performance. To quantify this drift, the authors introduce a
  local PCA-based metric measuring reconstruction error from PLM-derived tangent subspaces.
---

# GeoGNN: Quantifying and Mitigating Semantic Drift in Text-Attributed Graphs

## Quick Facts
- **arXiv ID**: 2511.09042
- **Source URL**: https://arxiv.org/abs/2511.09042
- **Reference count**: 40
- **Primary result**: GeoGNN improves node classification accuracy by 2-3% and link prediction Hit@10 by 4-6% over strong baselines by performing manifold-aware geodesic aggregation on text embeddings.

## Executive Summary
This paper addresses semantic drift in graph neural networks (GNNs) operating on text-attributed graphs, where linear aggregation of PLM embeddings distorts the intrinsic non-linear geometry of the semantic manifold. The authors introduce a local PCA-based metric to quantify this drift by measuring reconstruction error from PLM-derived tangent subspaces. They propose GeoGNN, which performs manifold-aware aggregation along geodesics using log-exp mappings on the unit sphere, preserving semantic structure. Experiments on four benchmark datasets with multiple PLMs show GeoGNN consistently outperforms strong baselines.

## Method Summary
GeoGNN addresses semantic drift by aggregating node information along geodesics on a spherical manifold rather than through linear operations. The method projects frozen PLM embeddings onto the unit sphere, then uses log maps to move neighbor embeddings to the tangent space of the target node, performs weighted aggregation there using geodesic proximity-based attention, and applies exp maps to return to the sphere. A local PCA-based reconstruction error metric quantifies how much standard linear aggregation distorts the underlying semantic manifold.

## Key Results
- GeoGNN achieves 2-3% higher node classification accuracy and 4-6% higher link prediction Hit@10 compared to strong baselines
- The local PCA drift metric successfully quantifies semantic distortion from linear aggregation
- Ablation studies confirm geodesic aggregation is critical for performance gains
- GeoGNN shows improved robustness across different PLM embeddings and datasets

## Why This Works (Mechanism)

### Mechanism 1: Semantic Drift Quantification via Local PCA
- **Claim**: Linear aggregation pulls embeddings off the intrinsic PLM manifold, creating semantic drift quantified by local reconstruction error.
- **Mechanism**: Estimates local tangent subspace $T_i$ for each node using PCA on its k-nearest PLM neighbors, projects aggregated embedding onto this subspace, and measures reconstruction error.
- **Core assumption**: Local geometry of PLM's semantic manifold can be approximated by linear tangent subspace from immediate neighbors.
- **Evidence anchors**: Abstract describes PCA metric as first quantitative framework; Section 2.3 details reconstruction error calculation.
- **Break condition**: Fails if PLM manifold is highly non-smooth or discontinuous locally, making linear PCA subspaces ineffective approximations.

### Mechanism 2: Geodesic Aggregation via Log-Exp Maps
- **Claim**: Aggregating along geodesics preserves semantic fidelity during message passing.
- **Mechanism**: Uses Riemannian operations on unit sphere—Log map moves neighbors to tangent space, weighted aggregation occurs there, Exp map projects result back to sphere.
- **Core assumption**: Unit sphere is faithful proxy for true (unknown) semantic manifold of text encoder.
- **Evidence anchors**: Section 3.1 defines Log, Exp maps and tangent space aggregation; Figure 4 visualizes workflow.
- **Break condition**: Fails if underlying semantic relationships are better modeled by Euclidean or Hyperbolic geometry.

### Mechanism 3: Geometry-Grounded Attention
- **Claim**: Attention weights from geodesic proximity (cosine similarity) are more semantically faithful than learned dot-product attention.
- **Mechanism**: Computes attention coefficients based on $\cos \theta_{ij}$ (distance on sphere) scaled by temperature $\tau$, eliminating need for learned query/key projections.
- **Core assumption**: Semantic similarity in PLM space maps directly to angular distance on normalized sphere.
- **Evidence anchors**: Section 3.1 uses cosine similarity as geodesic proximity measure; Figure 7 shows performance drops without geometric weighting.
- **Break condition**: Fails if downstream task requires semantic opposites to interact, as proximity-based attention might suppress distant node signals.

## Foundational Learning

- **Concept: Riemannian Manifolds (Sphere)**
  - **Why needed here**: The method treats text embeddings as points on curved surface rather than flat grid, requiring operations respecting curvature.
  - **Quick check question**: Can you explain why averaging two points on a sphere using standard vector addition $(a+b)/2$ does *not* yield a point on the sphere's surface?

- **Concept: Logarithmic and Exponential Maps**
  - **Why needed here**: Mathematical tools to switch between curved manifold and flat tangent space where standard aggregation is valid.
  - **Quick check question**: In the context of GeoGNN, does the $\text{Log}$ map move data to the manifold or away from it?

- **Concept: Semantic Drift**
  - **Why needed here**: Core problem definition—understanding that standard GNNs "flatten" nuance is critical to validating contribution.
  - **Quick check question**: If you aggregate embeddings by summing them, does the resulting vector have the same magnitude as inputs? How does this relate to the manifold?

## Architecture Onboarding

- **Component map**: Frozen PLM Embeddings -> Linear Projection -> L2 Normalization -> Log Map -> Tangent Space Aggregation -> Exp Map -> Output Classifier
- **Critical path**: The `Log Map -> Aggregation -> Exp Map` triplet. If normalization step is skipped or L2 norm is unstable, log/exp maps produce NaNs or undefined gradients.
- **Design tradeoffs**:
  - Spherical vs. True Manifold: Uses spherical manifold as computational proxy for unknown true semantic manifold
  - Frozen Encoder: PLM is frozen to isolate GNN's geometric contribution, reducing memory but preventing fine-tuning
- **Failure signatures**:
  - Instability: Exploding gradients if temperature $\tau$ is too small
  - Overshooting: If geodesic step size $\alpha$ is too large, Exp map moves representation past intended neighborhood
  - Performance Collapse: If dataset is homophilic but PLM embeddings are randomly oriented
- **First 3 experiments**:
  1. Drift Visualization: Replicate PCA drift metric on validation set to confirm GCN layers increase reconstruction error while GeoGNN maintains it
  2. Ablation on Normalization: Remove L2 normalization and observe if model fails to converge or performance drops
  3. Hyperparameter Sensitivity: Test temperature $\tau$ and step size $\alpha$ on small subset to find stable operating range

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can adaptive curvature modeling improve performance on heterogeneous text-attributed graphs with varying local manifold geometries?
- **Basis in paper**: Conclusion states framework "establishes a foundation for future research on geometry-aware text–graph reasoning and adaptive curvature modeling for heterogeneous Web data."
- **Why unresolved**: GeoGNN uses fixed spherical manifold throughout all layers and nodes, which may not capture heterogeneous curvature patterns across different graph regions or semantic subspaces.
- **What evidence would resolve it**: Comparing fixed-curvature GeoGNN against variant with learned/per-region curvature parameters on graphs with heterogeneous node types.

### Open Question 2
- **Question**: How robust is geodesic aggregation when PLM embeddings reside on non-spherical or mixed-geometry manifolds?
- **Basis in paper**: Method projects all embeddings onto unit hypersphere as "stable and geometry-preserving proxy," but acknowledges PLM manifolds are "highly non-linear and curved" without guaranteeing sphericity.
- **Why unresolved**: Spherical assumption may distort intrinsic geometry if PLM latent spaces contain hyperbolic regions or anisotropic curvature.
- **What evidence would resolve it**: Quantifying reconstruction error when approximating known non-spherical manifolds, or comparing performance against hyperbolic/mixed-geometry aggregation baselines.

### Open Question 3
- **Question**: Does joint fine-tuning of PLM encoder alongside geodesic aggregation yield complementary benefits, or does it destabilize manifold geometry?
- **Basis in paper**: All experiments freeze PLM to "ensure that performance differences arise solely from graph aggregation mechanisms."
- **Why unresolved**: Fine-tuning could shift underlying manifold, potentially invalidating geometric assumptions used during aggregation, or could jointly improve representations.
- **What evidence would resolve it**: Experiments comparing frozen vs. jointly fine-tuned PLM configurations with GeoGNN, measuring semantic drift before and after fine-tuning.

### Open Question 4
- **Question**: How does semantic drift evolve in deeper GNN architectures beyond 2-3 layers, and can geodesic aggregation delay over-smoothing?
- **Basis in paper**: Hyperparameter analysis shows accuracy peaks at 2-3 layers then declines due to over-smoothing, with geodesic drift remaining "consistently low across all layers."
- **Why unresolved**: Experiments only go up to ~6 layers, and interaction between geometric drift and over-smoothing at extreme depths is not isolated.
- **What evidence would resolve it**: Systematic depth scaling experiments (10-50+ layers) comparing drift accumulation and task performance between GeoGNN and linear aggregators.

## Limitations

- The spherical manifold assumption is a computational proxy for unknown true semantic manifold, which may not perfectly capture intrinsic geometry
- Effectiveness of PCA-based drift metric depends on local smoothness assumptions that may not hold in all PLM spaces
- Some hyperparameter choices (layer depth, head count, exact optimal τ/α per dataset) are not fully specified
- Frozen PLM embeddings limit adaptability to downstream tasks

## Confidence

- **High confidence**: Core claim that linear aggregation distorts semantic manifold geometry, and GeoGNN's geodesic aggregation mitigates this distortion (supported by ablation studies and consistent performance gains)
- **Medium confidence**: Exact quantitative impact of semantic drift (PCA metric) and its universality across all PLM families (limited corpus validation of specific metric)
- **Medium confidence**: Superiority of spherical over hyperbolic/Euclidean geometry for all text-attributed graph tasks (geometry choice is justified but not exhaustively compared)

## Next Checks

1. **Drift metric validation**: Replicate local PCA drift measurement on validation set to confirm standard GCN layers increase reconstruction error while GeoGNN maintains it
2. **Ablation of spherical constraint**: Remove L2 normalization to test if performance drops, validating necessity of manifold assumption
3. **Attention mechanism ablation**: Remove geometric attention (cosine similarity weighting) and retrain to quantify its specific contribution to performance gains