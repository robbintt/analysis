---
ver: rpa2
title: 'Finding Common Ground: Using Large Language Models to Detect Agreement in
  Multi-Agent Decision Conferences'
arxiv_id: '2507.08440'
source_url: https://arxiv.org/abs/2507.08440
tags:
- decision
- llms
- stance
- evaluation
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores using large language models (LLMs) to simulate\
  \ decision conferences\u2014structured meetings where diverse experts collaborate\
  \ to reach consensus on complex issues. We introduce a novel LLM-based multi-agent\
  \ system designed to model these conferences, focusing on detecting agreement among\
  \ participant agents."
---

# Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences

## Quick Facts
- arXiv ID: 2507.08440
- Source URL: https://arxiv.org/abs/2507.08440
- Reference count: 40
- Key outcome: LLM-based multi-agent systems can reliably detect agreement in structured debates, with smaller models matching larger ones on stance classification

## Executive Summary
This work introduces an LLM-based multi-agent system to simulate decision conferences—structured expert meetings for reaching consensus on complex issues. The system uses stance detection (pro/con/neutral) and stance polarity detection (positive/negative/neutral) to identify agreement among participant agents. Six LLMs were evaluated across two tasks: stance detection and stance polarity detection. A judge agent improves debate efficiency by preventing premature topic transitions, ensuring comprehensive coverage. Results show smaller models like Gemma 2 9B can match ChatGPT 4 performance, demonstrating the potential for cost-effective agreement detection in group decision-making simulations.

## Method Summary
The system uses zero-shot prompting to evaluate six LLMs on stance detection and stance polarity detection tasks using benchmark datasets (VAST and Claim Stance Classification). A multi-agent architecture includes moderator, participant, judge, and evaluator agents operating through a custom speaker selection function. The judge agent evaluates agreement after each debate round, signaling whether to continue debate or advance stages. Subjective evaluation uses LLM-as-a-judge approach on a real drug policy decision conference example, comparing outcomes with and without the judge agent.

## Key Results
- Smaller open-source models (Gemma 2 9B, LLaMA 3 70B) perform comparably to ChatGPT 4 on agreement detection tasks
- Judge agent prevents premature topic transitions, ensuring complete coverage of discussion clusters
- LLM-based multi-agent system produces outcomes similar to real-world decision conferences while improving debate efficiency

## Why This Works (Mechanism)

### Mechanism 1
Agreement detection operates through stance detection (pro/con/neutral) and stance polarity detection (positive/negative/neutral). The judge agent classifies positions and sentiment, inferring agreement when stance alignment and positive polarity converge. Core assumption: stance and polarity are sufficient proxies for agreement. Evidence: VAST dataset shows near-zero F1 for neutral class, indicating potential reliability issues.

### Mechanism 2
Dedicated judge agent prevents premature topic transitions by acting as a gatekeeper. After each debate round, the judge evaluates agreement status and signals the moderator to either continue debate or advance stages. Core assumption: extended debate rounds correlate with better outcome quality. Evidence: Without judge agent, public implications cluster was overlooked; with judge agent, previously missed topics were addressed.

### Mechanism 3
Smaller open-source models can match proprietary models through zero-shot prompting due to sufficient argumentation patterns in pre-training. Core assumption: benchmark performance transfers to real-world contexts. Evidence: LLaMA 3 70B, ChatGPT 4, and Gemma 2 9B were top performers, with open models matching or exceeding ChatGPT 4.

## Foundational Learning

- Concept: **Stance Detection vs. Sentiment Analysis**
  - Why needed here: Distinguishes position on proposition (pro/con/neutral) from emotional tone (positive/negative/neutral). Confusing these leads to incorrect agreement inferences.
  - Quick check question: Given "This policy has excellent intentions but will fail in practice," what is the stance and what is the sentiment? (Stance: con; Sentiment: mixed/positive on intentions, negative on outcome)

- Concept: **LLM-as-a-Judge Evaluation Paradigm**
  - Why needed here: System uses ChatGPT 4 to evaluate other LLMs' agreement detection performance. Understanding limitations is critical—the judge may share biases with evaluated models.
  - Quick check question: What are two failure modes when using LLM-as-a-judge? (Answer: positional bias in pairwise comparison; shared blind spots between judge and evaluated model)

- Concept: **Decision Conference Structure (Phillips Framework)**
  - Why needed here: Simulation models real decision conferences with four stages (need recognition → preparation → conference → follow-up) and three debate phases. Understanding structure is necessary for meaningful modifications.
  - Quick check question: In a real decision conference, who owns the evolving model, and why does this matter? (Answer: Participants own it through real-time collaborative editing; this creates commitment and surfaces dissent early)

## Architecture Onboarding

- Component map: Moderator -> Participant Agents (2+) <-> Judge Agent <-> Evaluator Agent
- Critical path: Moderator receives topic → participants debate → judge evaluates → if "Debate," loop back; if "Agreement," advance stage → Repeat through issue discussion → model building → results exploration → Evaluator scores each completed debate segment
- Design tradeoffs:
  - Decentralized vs. centralized communication: Paper chose decentralized for realism; centralized offers more control
  - Number of participants: Two suffice for evaluating judge accuracy; more increase complexity
  - Judge model selection: Smaller models offer cost/latency benefits; larger models provide more reliable formatting adherence
- Failure signatures:
  - Premature termination: Debate ends with incomplete coverage
  - Format drift: Judge outputs non-parseable responses, breaking speaker selection logic
  - Neutral stance collapse: Poor performance on neutral classification
  - Knowledge imbalance: Agents possess domain knowledge exceeding typical human experts
- First 3 experiments:
  1. Baseline validation: Run drug policy decision conference simulation with judge agent disabled, compare topic coverage against real conference transcript
  2. Model swap stress test: Replace ChatGPT 4 as judge with Gemma 2 9B, measure format adherence rate and agreement detection accuracy
  3. Participant scaling: Add third participant agent with distinct persona, observe debate rounds before agreement and whether new perspective clusters emerge

## Open Questions the Paper Calls Out

- Can LLM-based agreement detection maintain consistent accuracy across diverse topics and specialized domain decision conferences beyond the drug policy scenario tested? (Limited to one real-world conference example)
- How does system performance scale when expanding beyond two participant agents to larger groups with more diverse and complex personas? (Only two participants used to assess judge agent capabilities)
- Can techniques like retrieval-augmented generation (RAG) or knowledge graphs effectively constrain LLM agent knowledge to produce more natural debate dynamics? (Current system doesn't evaluate contribution accuracy or relevance)

## Limitations

- Zero-shot evaluation may not transfer to domain-specific terminology or complex argumentative structures
- LLM-as-a-judge introduces potential positional bias and shared blind spots with evaluated models
- Focus on two participants limits generalizability to larger groups with factional dynamics

## Confidence

- High confidence: LLMs can reliably detect agreement in structured debates; smaller models can match larger ones on stance classification; judge agent prevents incomplete topic coverage
- Medium confidence: Zero-shot prompting suffices for real-world decision conferences; format adherence issues won't break deployed systems
- Low confidence: Detected agreement equals substantive consensus; two-participant architecture generalizes to larger groups; benchmark performance predicts all real-world scenarios

## Next Checks

1. **Ground truth alignment**: Compare LLM judge agreement detections against human-labeled ground truth on 50 debate segments from real decision conferences. Measure Cohen's kappa to assess reliability.

2. **Format drift stress test**: Run 100 simulated debates with judge agents across Gemma 2 9B, LLaMA 3 70B, and ChatGPT 4. Calculate percentage of rounds where judge outputs non-parseable responses and implement recovery mechanism.

3. **Domain transfer evaluation**: Test stance detection pipeline on three domain-specific datasets (legal, medical, policy) not used in training. Measure performance drop compared to VAST/Claim datasets.