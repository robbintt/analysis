---
ver: rpa2
title: Neural Contextual Bandits Under Delayed Feedback Constraints
arxiv_id: '2504.12086'
source_url: https://arxiv.org/abs/2504.12086
tags:
- delayed
- reward
- neural
- algorithm
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of delayed feedback in neural
  contextual bandits, a common issue in applications like recommendation systems and
  clinical trials where reward feedback is not immediately available. The authors
  propose a new algorithm, Delayed NeuralUCB, which adapts the NeuralUCB algorithm
  to handle random, unknown delays in reward observations.
---

# Neural Contextual Bandits Under Delayed Feedback Constraints

## Quick Facts
- arXiv ID: 2504.12086
- Source URL: https://arxiv.org/abs/2504.12086
- Authors: Mohammadali Moghimi; Sharu Theresa Jose; Shana Moothedath
- Reference count: 25
- Primary result: Delayed NeuralUCB algorithm addresses delayed feedback in neural contextual bandits with regret bound scaling as O(˜d√T log T + ˜d^(3/2)D+ log(T)^(3/2))

## Executive Summary
This paper addresses the challenge of delayed feedback in neural contextual bandits, a common issue in applications like recommendation systems and clinical trials where reward feedback is not immediately available. The authors propose a new algorithm, Delayed NeuralUCB, which adapts the NeuralUCB algorithm to handle random, unknown delays in reward observations. The key innovation is modifying the update mechanism to account for missing rewards and adjusting the exploration parameter to reflect the reduced information.

The authors derive a regret bound that scales as O(˜d√T log T + ˜d^(3/2)D+ log(T)^(3/2)), where ˜d is the effective dimension of the neural tangent kernel and D+ depends on the expected delay. Experiments on MNIST and Mushroom datasets with various delay distributions demonstrate that the proposed algorithms effectively manage different delay scenarios and perform comparably to standard algorithms under no delays.

## Method Summary
The paper proposes Delayed NeuralUCB, an extension of the NeuralUCB algorithm that handles delayed feedback in contextual bandit settings. The algorithm modifies the standard NeuralUCB update mechanism to account for missing rewards by tracking pending rewards and updating the model only when rewards become available. The exploration parameter is adjusted to reflect the reduced information due to delays. The theoretical analysis establishes a regret bound that explicitly captures the penalty from delayed information, showing that the regret grows with both the time horizon T and the expected delay D+.

## Key Results
- Proposed Delayed NeuralUCB algorithm effectively handles random, unknown delays in neural contextual bandits
- Regret bound scales as O(˜d√T log T + ˜d^(3/2)D+ log(T)^(3/2)), where the D+ term explicitly captures delay penalty
- Experiments on MNIST and Mushroom datasets show comparable performance to standard algorithms under no delays across various delay distributions
- Algorithm successfully manages different delay scenarios while maintaining theoretical guarantees

## Why This Works (Mechanism)
The algorithm works by maintaining a queue of pending rewards and updating the neural network only when rewards become available. The key insight is that when feedback is delayed, the algorithm has less information about the true rewards, which reduces the effective sample size. To compensate, the exploration parameter is scaled by a factor that accounts for the missing information. The regret bound shows that the penalty from delays grows as D^(3/2), which is sublinear in T, making the algorithm practical for applications with moderate delays.

## Foundational Learning

**Neural Tangent Kernel (NTK)**: The infinite-width limit of neural networks where the training dynamics become linear. Needed to analyze the behavior of wide neural networks in the bandit setting. Quick check: Verify that the effective dimension ˜d remains bounded during training.

**Contextual Bandits**: Online learning framework where at each round, an agent selects an action based on context and receives a reward. Needed to model recommendation and clinical trial scenarios. Quick check: Ensure the context distribution is stationary.

**Regret Analysis**: Framework for quantifying the performance of online learning algorithms. Needed to establish theoretical guarantees. Quick check: Verify that the confidence bounds properly account for the delayed feedback.

## Architecture Onboarding

**Component Map**: Context -> Neural Network -> Action Selection -> Reward Queue -> Update Mechanism -> Context

**Critical Path**: The algorithm maintains a queue of pending rewards. When a reward becomes available after delay, it updates the neural network using the stored context-action pair. The updated model then informs future action selections.

**Design Tradeoffs**: The algorithm trades off between immediate updates (which would be incorrect with delayed feedback) and delayed updates (which provide accurate information but reduce exploration). The exploration parameter adjustment balances this tradeoff.

**Failure Signatures**: 
- If delays are too long (D+ approaching T), the algorithm may fail to learn effectively
- If the effective dimension ˜d grows too quickly, the regret bound may become vacuous
- If the delay distribution has heavy tails, the expected delay D+ may not capture the true penalty

**First Experiments**:
1. Test on a synthetic dataset with known optimal policy and varying delay distributions
2. Compare against a naive approach that updates with placeholder rewards during delays
3. Evaluate sensitivity to delay distribution parameters (mean, variance, tail behavior)

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The regret bound assumes the effective dimension ˜d remains constant, but neural networks can exhibit changing effective dimensions during training
- The analysis focuses on stochastic delays but doesn't address adversarial or heavy-tailed delay distributions
- Limited exploration of how the algorithm performs with very long delays where D+ becomes comparable to T
- The experiments use relatively small-scale problems compared to real-world applications

## Confidence
- Theoretical framework and regret analysis: High
- Algorithm design and modifications: High
- Experimental methodology: Medium (limited to specific datasets)
- Scalability to real-world applications: Low

## Next Checks
1. Test the algorithm on larger-scale recommendation datasets with millions of users and items to evaluate practical scalability
2. Implement stress tests with extreme delay scenarios (D+ approaching T) to identify breaking points
3. Compare against online learning algorithms that handle missing data rather than specifically bandit algorithms