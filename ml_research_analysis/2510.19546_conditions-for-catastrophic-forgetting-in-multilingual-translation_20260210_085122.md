---
ver: rpa2
title: Conditions for Catastrophic Forgetting in Multilingual Translation
arxiv_id: '2510.19546'
source_url: https://arxiv.org/abs/2510.19546
tags:
- language
- forgetting
- unseen
- fine-tuning
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates catastrophic forgetting
  in multilingual translation models. Through controlled experiments across various
  model architectures, data scales, and fine-tuning approaches, the authors find that
  the relative scale between model parameters and fine-tuning data volume is a primary
  determinant of forgetting.
---

# Conditions for Catastrophic Forgetting in Multilingual Translation

## Quick Facts
- arXiv ID: 2510.19546
- Source URL: https://arxiv.org/abs/2510.19546
- Authors: Danni Liu; Jan Niehues
- Reference count: 25
- Key outcome: Relative model-data scale is primary determinant of catastrophic forgetting; instruction-following models retain knowledge better than translation-specific models; cross-lingual alignment mitigates forgetting on unseen pairs

## Executive Summary
This paper systematically investigates catastrophic forgetting in multilingual translation models through controlled experiments across various model architectures, data scales, and fine-tuning approaches. The authors find that the relative scale between model parameters and fine-tuning data volume is a primary determinant of forgetting, with larger models showing greater resistance. They demonstrate that instruction-following models retain multilingual knowledge better than translation-specific models, and that cross-lingual alignment can mitigate forgetting on unseen language pairs while facilitating positive transfer. Parameter-efficient fine-tuning (LoRA) offers no clear advantage over full fine-tuning in mitigating forgetting.

## Method Summary
The study uses controlled fine-tuning experiments on base models including M2M-124-0.2B/0.6B (token-based control) and Qwen2.5-0.5B/7B-Instruct & Llama-3-8B-Instruct (instruction-based control). Two datasets are employed: SMALL (ALMA: 117K sentence pairs, cs/de/is/ru/zh↔en) and LARGE (WMT21: 54M sentence pairs, jv/ms/tl↔en), with subsampled versions at 12K/120K/1.2M per pair. Both full fine-tuning and LoRA (r=8, α=16) are tested. Cross-lingual alignment methods (adversarial, similarity-only, contrastive) are applied to encoder/middle layers. Evaluation uses COMET-22 as primary metric, with BLEU and language accuracy via LangID on three unseen pair types: unseen pair, unseen source, unseen target.

## Key Results
- Relative scale between model size and fine-tuning data volume is primary determinant of catastrophic forgetting
- Instruction-following ability is more critical for retaining multilingual knowledge than underlying architecture
- Cross-lingual alignment can mitigate forgetting on unseen pairs while facilitating positive transfer to unseen target languages
- LoRA provides no significant advantage over full fine-tuning for mitigating forgetting in translation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The relative scale between model size and fine-tuning data volume is a primary determinant of catastrophic forgetting in multilingual models.
- Mechanism: Larger models possess greater parameter capacity, which buffers against parameter overwriting during intensive fine-tuning. When fine-tuning data volume is high relative to model capacity, the optimization process more aggressively overwrites pre-trained multilingual knowledge, leading to forgetting of unseen languages.
- Core assumption: Model parameters encode language knowledge in a distributed manner that can be partially overwritten during gradient updates on new data.
- Evidence anchors: [abstract] "...we reveal that the relative scale between model and data size is a primary determinant of forgetting."; [section 4.1] Demonstrates that larger model variants consistently exhibit greater resistance to catastrophic forgetting. Figure 4 shows progressive forgetting as fine-tuning data scales from 12K to 1,200K sentences.

### Mechanism 2
- Claim: A model's instruction-following ability is more critical for retaining multilingual knowledge than its underlying architecture (encoder-decoder vs. decoder-only).
- Mechanism: Instruction-following models use natural language prompts for language control, which provides a more generalizable mechanism for specifying target languages compared to specialized language tokens. This reduces overfitting to spurious source-target associations learned during fine-tuning.
- Core assumption: Natural language instructions create more robust semantic representations for language control than specialized token-based mechanisms.
- Evidence anchors: [abstract] "...a model's instruction-following ability is more critical for retaining multilingual knowledge than its architecture."; [section 4.2] Controlled experiments show Qwen2.5-0.5B with instruction-based control outperforms token-based control on unseen target languages (Table 3).

### Mechanism 3
- Claim: Cross-lingual alignment methods can mitigate forgetting on unseen language pairs by encouraging language-invariant representations.
- Mechanism: Alignment methods (e.g., contrastive loss) encourage the model to learn representations that disentangle semantic content from language-specific features. This counteracts the spurious source-target associations learned during fine-tuning, which is the primary cause of forgetting in the "unseen pair" scenario where source and target languages are seen separately but never together.
- Core assumption: Unseen pair forgetting is primarily caused by spurious correlations (e.g., associating German inputs with English outputs after training on German-English), which can be broken by enforcing language-invariant representations.
- Evidence anchors: [abstract] "...cross-lingual alignment can mitigate forgetting while also facilitating positive transfer to unseen target languages."; [section 5] Figure 5a shows cross-lingual alignment methods reverse a -7.5 COMET loss on unseen pairs into a gain of over 10 COMET.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: This is the central phenomenon the entire paper investigates. Without understanding that neural networks can lose previously learned capabilities when fine-tuned on new data, the paper's research questions and findings are unintelligible.
  - Quick check question: Can you explain, in one sentence, why fine-tuning a model on a new task might make it worse at a task it was previously good at?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - Why needed here: The paper explicitly tests the common assumption that LoRA mitigates catastrophic forgetting compared to full fine-tuning. Understanding the basic principle of PEFT is necessary to interpret this finding and its counter-intuitive result.
  - Quick check question: What is the core idea behind Low-Rank Adaptation (LoRA) as a fine-tuning method?

- Concept: **Cross-Lingual Transfer / Alignment**
  - Why needed here: The paper's mitigation strategy (Section 5) is based on cross-lingual alignment. To understand why this helps, one must grasp the concept of aligning representations across languages to facilitate knowledge transfer and reduce interference.
  - Quick check question: What does it mean to "align" representations of two sentences from different languages in a neural model?

## Architecture Onboarding

- Component map: Base Models (M2M-124, Qwen2.5, Llama3) -> Fine-Tuning Methods (Full, LoRA) -> Language Control (Token-based, Instruction-based) -> Mitigation (Cross-lingual alignment losses) -> Evaluation (COMET, BLEU, LangID)

- Critical path:
  1. **Scale Assessment**: Before fine-tuning, evaluate the ratio of model parameters to fine-tuning data tokens
  2. **Model Selection**: Prefer instruction-following models (e.g., Qwen, Llama) over translation-specific ones (e.g., M2M) for better retention on unseen targets
  3. **Mitigation Application**: If using non-English-centric data or prone to unseen pair forgetting, implement cross-lingual alignment (e.g., contrastive loss) during fine-tuning

- Design tradeoffs:
  - **Model Scale vs. Compute Cost**: Larger models forget less but are more expensive to train and serve
  - **Architecture vs. Instruction Capability**: A decoder-only model with strong instruction-following may be preferable to an encoder-decoder model without it
  - **LoRA vs. Full Fine-Tuning**: This paper finds no significant advantage for LoRA in mitigating forgetting for translation tasks

- Failure signatures:
  - **Severe BLEU drop to near zero**: Indicates complete forgetting, typical of translation-specific models on unseen target languages with token-based control
  - **Incorrect language generation**: The model outputs in the wrong language (e.g., English when translating German-Czech)
  - **High COMET with low BLEU**: May indicate the model is generating fluent but incorrect language

- First 3 experiments:
  1. **Baseline Assessment**: Fine-tune your chosen base model on a subset of your target data. Evaluate on both fine-tuned language pairs and a held-out set of unseen pairs
  2. **Language Control Ablation**: If using an instruction-following model, compare performance on unseen target languages using standard English instructions vs. in-language instructions
  3. **Alignment Integration**: Integrate a contrastive cross-lingual alignment loss into the fine-tuning process for the same baseline setup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed catastrophic forgetting patterns generalize to non-English-centric fine-tuning scenarios, where pivot languages other than English dominate the training data?
- Basis in paper: [explicit] The limitations section states: "Extension to non-English pivot scenarios would provide additional validation of our findings' generalizability."
- Why unresolved: All experiments use English-centric language pairs, reflecting real-world data availability, but this constrains conclusions about whether the relative scale and instruction-following findings hold when fine-tuning on, for example, Chinese-centric or multilingual non-English data.
- What evidence would resolve it: Controlled experiments using the same model architectures and data scales but with non-English pivot languages (e.g., Chinese-centric or Spanish-centric datasets), measuring forgetting on unseen pairs and comparing patterns to the English-centric findings.

### Open Question 2
- Question: How do catastrophic forgetting dynamics change in even larger models (e.g., 70B+ parameters) when fine-tuned on proportionally larger datasets?
- Basis in paper: [explicit] The limitations note: "The dynamics of forgetting in even larger models remain to be investigated" due to computational constraints limiting exploration to larger size ranges.
- Why unresolved: The paper establishes that relative scale between model size and fine-tuning data volume matters, but the relationship may not be linear. Larger models may exhibit qualitatively different forgetting behaviors that cannot be extrapolated from 0.5B–8B parameter models.
- What evidence would resolve it: Experiments scaling both model size (e.g., 70B, 175B) and fine-tuning data volumes proportionally, comparing forgetting rates against the smaller model baselines to determine if the relative scale principle holds or if new dynamics emerge.

### Open Question 3
- Question: Do similar catastrophic forgetting patterns emerge across other multilingual tasks (e.g., summarization, classification, question answering), or are the findings specific to machine translation's structured output space?
- Basis in paper: [explicit] The limitations state: "Whether similar patterns emerge across other multilingual tasks remains an open question beyond the current scope."
- Why unresolved: Machine translation provides a structured testbed with clear evaluation metrics and directional language pairs, but multilingual models are deployed across diverse tasks. The instruction-following advantage and relative scale effects may manifest differently in tasks with less constrained outputs.
- What evidence would resolve it: Systematic experiments applying the same controlled fine-tuning methodology (varying model scale, data volume, instruction-following vs. non-instruction models) to multilingual summarization, classification, and QA tasks, measuring forgetting on unseen languages.

## Limitations

- Data Dependency and Generalization: Findings are derived from two specific datasets (ALMA and WMT21) with limited language diversity, primarily Indo-European languages
- Mitigation Scope: Focus on cross-lingual alignment as primary mitigation strategy, with alternative methods like elastic weight consolidation or memory replay not explored
- Model and Task Specificity: Experiments conducted primarily on translation tasks using specific model architectures, limiting generalizability to other multilingual tasks

## Confidence

- **High Confidence**: Relative model-data scale is primary determinant of catastrophic forgetting - well-supported by systematic experiments across multiple data scales and model sizes
- **Medium Confidence**: Instruction-following ability is more critical than architecture for retaining multilingual knowledge - supported by controlled experiments but relies on limited model comparisons
- **Low Confidence**: Cross-lingual alignment consistently mitigates forgetting while facilitating positive transfer - demonstrated on single dataset with specific alignment methods

## Next Checks

1. **Cross-Lingual Alignment Generalization Test**: Validate effectiveness of contrastive alignment across multiple datasets (ALMA, WMT21, and additional multilingual datasets) and diverse language families, including non-Indo-European languages

2. **Instruction Template Robustness Analysis**: Systematically evaluate sensitivity of in-language instruction performance to template variations and translation quality using multiple instruction templates for same language pairs

3. **Alternative Mitigation Method Comparison**: Implement and compare elastic weight consolidation and memory replay methods against proposed cross-lingual alignment approach to determine whether they address different types of forgetting