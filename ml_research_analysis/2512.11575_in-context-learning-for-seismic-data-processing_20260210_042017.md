---
ver: rpa2
title: In-Context Learning for Seismic Data Processing
arxiv_id: '2512.11575'
source_url: https://arxiv.org/abs/2512.11575
tags:
- data
- seismic
- learning
- contextseisnet
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ContextSeisNet, an in-context learning approach
  for seismic demultiple processing that addresses the challenge of spatially inconsistent
  results across neighboring seismic gathers. The method conditions predictions on
  a support set of spatially related example pairs from neighboring CDP gathers and
  their corresponding labels, enabling task-specific processing behavior at inference
  time without retraining.
---

# In-Context Learning for Seismic Data Processing

## Quick Facts
- arXiv ID: 2512.11575
- Source URL: https://arxiv.org/abs/2512.11575
- Reference count: 33
- Primary result: ContextSeisNet achieves superior lateral consistency in seismic demultiple processing using 90% less training data than conventional U-Net approaches

## Executive Summary
This paper introduces ContextSeisNet, an innovative in-context learning approach for seismic demultiple processing that addresses the persistent challenge of spatially inconsistent results across neighboring seismic gathers. The method conditions predictions on a support set of spatially related example pairs from neighboring CDP gathers, enabling task-specific processing behavior at inference time without retraining. By leveraging CrossBlocks to perform cross-convolutions between query images and support examples, ContextSeisNet exploits spatial correlations among neighboring gathers while maintaining flexibility through a prompting strategy that can integrate traditional processing methods with deep learning capabilities.

## Method Summary
ContextSeisNet is built on the UniverSeg architecture with modifications including batch normalization after second convolutions in CrossBlocks and LeakyReLU activations. The model processes seismic data by sampling S+1 gather-label pairs per iteration from a support set V containing spatially related examples. Training employs L1 loss with AdamW optimizer, OneCycle schedule, and gradient clipping. Data augmentation includes random white noise addition, per-image normalization, and random label replacement with inputs for regularization. The approach demonstrates substantial data efficiency, achieving comparable field data performance while training on only 10,500 gathers compared to traditional U-Net baselines trained on 105,000 gathers.

## Key Results
- ContextSeisNet achieves superior lateral consistency compared to both traditional Radon demultiple and U-Net baselines on field data
- Model delivers improved near-offset performance and more complete multiple removal
- ContextSeisNet achieves comparable field data performance despite being trained on 90% less data than conventional U-Net approaches

## Why This Works (Mechanism)
ContextSeisNet works by conditioning seismic demultiple processing predictions on spatially related example pairs from neighboring CDP gathers. The CrossBlocks architecture performs cross-convolutions between query images and support features, allowing the model to exploit spatial correlations while maintaining task-specific behavior at inference time. This in-context learning approach enables the network to adapt its processing behavior based on the support set, effectively transferring knowledge from spatially consistent regions to improve lateral consistency across seismic lines.

## Foundational Learning

**Cross-Convolution Operations**
- Why needed: Enables interaction between query and support features for spatially consistent predictions
- Quick check: Verify feature maps maintain spatial resolution after cross-convolution

**In-Context Learning**
- Why needed: Allows task-specific processing without retraining by conditioning on support examples
- Quick check: Test model response to different prompt types (e.g., Radon vs. identity mappings)

**Spatial Correlation in Seismic Data**
- Why needed: Neighboring CDP gathers contain correlated geological information crucial for lateral consistency
- Quick check: Measure PSNR degradation when prompts are spaced too closely or too far apart

## Architecture Onboarding

**Component Map**
CrossBlocks -> Batch Normalization -> LeakyReLU -> Support Set Sampling -> Prompt Spacing Optimization

**Critical Path**
1. Cross-convolution between query and support features
2. Batch normalization for training stability
3. Support set sampling strategy (S+1 pairs per iteration)
4. Prompt spacing optimization (10 CDP spacing)

**Design Tradeoffs**
- Model size vs. computational efficiency (4M parameters for smallest variant)
- Prompt spacing vs. lateral consistency (10 CDP spacing optimal)
- Training data volume vs. data efficiency (90% reduction achieved)

**Failure Signatures**
- Training instability without batch normalization in CrossBlocks
- Poor lateral consistency if prompts are too closely spaced
- Over-reliance on learned transformation vs. support set

**3 First Experiments**
1. Implement CrossBlocks with batch normalization and LeakyReLU activations
2. Test model response to different prompt types (Radon vs. identity mappings)
3. Systematically vary support set size S to identify optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Exact network specifications for different model variants remain unspecified
- Training epochs and convergence criteria are not provided
- Data augmentation hyperparameters lack precise definition

## Confidence

**High confidence in the core innovation**: combining in-context learning with seismic processing through CrossBlocks architecture

**Medium confidence in data efficiency claims**: methodology appears sound but exact training duration and convergence metrics are unspecified

**Low confidence in exact reproducibility**: critical hyperparameters for network architecture, data augmentation, and training schedule are missing

## Next Checks

1. **Network Architecture Validation**: Implement CrossBlocks with batch normalization and LeakyReLU activations as specified, then systematically vary support set size S and prompt spacing to identify optimal configurations for lateral consistency

2. **Synthetic Data Generation**: Develop convolutional modeling code to generate spatially correlated synthetic gathers with varying reflection coefficients and NMO velocities, then verify the 10,500/4,500 train/test split maintains representative geological variations

3. **Prompt Strategy Testing**: Conduct controlled experiments varying the number and spatial distribution of inference prompts (testing spacings of 5, 10, and 20 CDPs) to quantify the trade-off between lateral consistency and computational overhead, validating the claimed optimal spacing of 10 CDPs