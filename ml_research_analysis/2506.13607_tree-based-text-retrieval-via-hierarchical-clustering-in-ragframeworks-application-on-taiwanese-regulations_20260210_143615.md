---
ver: rpa2
title: 'Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks: Application
  on Taiwanese Regulations'
arxiv_id: '2506.13607'
source_url: https://arxiv.org/abs/2506.13607
tags:
- page
- retrieval
- tree
- node
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks: Application on Taiwanese Regulations

## Quick Facts
- **arXiv ID:** 2506.13607
- **Source URL:** https://arxiv.org/abs/2506.13607
- **Reference count:** 1
- **Primary result:** Introduces hierarchical clustering-based retrieval method that eliminates need to predefine k, showing improved expert evaluation over fixed-k baselines for Taiwanese legal QA.

## Executive Summary
This paper proposes a tree-based retrieval method for RAG systems that uses hierarchical clustering to organize document vectors, eliminating the need to predefine a retrieval cutoff k. Applied to Taiwanese legal documents, the method builds a dendrogram where each node represents a cluster and uses best-first search to find the most semantically relevant subtree for a query. Results show improved expert evaluation scores compared to fixed-k baselines, particularly when combined with query extraction to decompose legal questions into structured elements.

## Method Summary
The method constructs a retrieval tree by performing single-linkage hierarchical clustering on document embeddings using cosine similarity, iteratively merging clusters until a single root remains. Each non-leaf node stores a mean vector representing its children. For retrieval, a best-first search navigates the tree comparing the query vector to node representatives, returning all leaf documents under the best-matching node. An optional query extraction module uses an LLM to decompose legal questions into structured elements (core facts, legal issues, parties, statutes) before retrieval.

## Key Results
- Tree+QE achieved the highest expert score of 0.4853 with statistical significance (p < 0.05)
- The hierarchical method eliminates need to define k, adaptively selecting relevant content
- Outperformed fixed-k baselines on Taiwanese legal QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical clustering organizes document vectors into a tree structure, enabling retrieval of semantically grouped content without a fixed top-k cutoff.
- **Mechanism:** Document vectors are iteratively merged based on cosine similarity using single-linkage agglomerative clustering until a single root cluster forms. Each non-leaf node stores a representative vector (mean of its children). During retrieval, a best-first search navigates the tree to find the node most similar to the query, then returns all leaf texts under that node, yielding a variable number of documents based on query semantics.
- **Core assumption:** The mean vector of child nodes adequately represents the semantic content of a parent cluster for similarity comparison, and the single-linkage criterion produces coherent semantic groups.
- **Evidence anchors:**
  - [abstract] "hierarchical clustering-based retrieval method that eliminates the need to predefine k... adaptively selecting semantically relevant content."
  - [section 3.1.2, Algorithm 1] Formalizes the tree construction via iterative merging and representative vector computation.
  - [corpus] T-Retriever (arXiv:2601.04945) and CFT-RAG (arXiv:2501.15098) confirm tree-based hierarchical RAG is an active direction, though with different construction methods.
- **Break condition:** Fails if the embedding model poorly captures document semantics, or if vector averaging dilutes cluster-specific meaning, or if single-linkage causes "chaining" artifacts (linking documents that are only transitively similar).

### Mechanism 2
- **Claim:** The BFS-based search algorithm dynamically determines retrieval scope by identifying the most relevant cluster node, effectively adapting "k" based on the query.
- **Mechanism:** A query vector is compared against node representative vectors using a best-first search with a min-distance tracker. The algorithm explores the tree, updating the "best node" whenever a closer match is found, and returns all leaf content under that node—resulting in a variable number of retrieved documents.
- **Core assumption:** A single "best node" can sufficiently capture the relevant information scope for a given query.
- **Evidence anchors:**
  - [section 3.2, Algorithm 2] Details the BFS search with min-distance tracking.
  - [section 3.2, Figure 3] Illustrates variable retrieval scopes (subtree vs. single leaf) depending on query similarity.
  - [corpus] Limited external validation for this specific dynamic-k approach; primarily supported by this paper's results.
- **Break condition:** Fails when relevant information is split across multiple distinct clusters not co-located under a single relevant parent node, or when a generic query matches a high-level node, returning an overly broad document set.

### Mechanism 3
- **Claim:** Query extraction (Tree+QE) augments the original query with structured legal elements, improving retrieval precision for domain-specific questions.
- **Mechanism:** An LLM (GPT-4o-mini) processes the user query through a structured prompt (Appendix A.1.3) to extract core facts, legal issues, involved parties, and cited statutes. This decomposed, structured text is embedded and used for retrieval instead of the raw query.
- **Core assumption:** The LLM can reliably decompose a legal question without hallucinating non-existent elements, and this decomposition produces an embedding that better matches relevant document clusters.
- **Evidence anchors:**
  - [section 4.2] Describes using query extraction to "provide a more professional and complete description of the user's intent."
  - [section 5, Table 2] Shows Tree+QE achieving the highest expert score (0.4853) with statistical significance (p < 0.05).
  - [corpus] No direct corpus evidence for this specific extraction technique; appears novel to this work.
- **Break condition:** Degrades if the extraction prompt causes the LLM to misinterpret the query or introduce irrelevant legal concepts, sending retrieval down the wrong branch.

## Foundational Learning

- **Concept: Hierarchical Agglomerative Clustering (HAC)**
  - **Why needed here:** This is the core algorithm for building the retrieval tree. Understanding single-linkage merging, distance metrics (cosine similarity), and dendrogram formation is essential to grasp how the document index is structured and where it might fail.
  - **Quick check question:** Given clusters {A, B} and {C, D}, how would single-linkage compute the inter-cluster distance using cosine distance between individual points?

- **Concept: Best-First Search (BeFS)**
  - **Why needed here:** The retrieval algorithm uses BeFS (not BFS or DFS) to traverse the cluster tree. Understanding its priority queue behavior and how it differs from exhaustive traversal helps predict efficiency and failure modes.
  - **Quick check question:** In a balanced binary tree with N leaf nodes, what is the worst-case number of node comparisons the search algorithm makes?

- **Concept: RAG Trade-offs (Precision vs. Recall in k)**
  - **Why needed here:** The paper explicitly frames its contribution against the "top-k selection problem." A solid grasp of why increasing k can degrade answer quality (noise injection) is required to appreciate the adaptive-k solution.
  - **Quick check question:** In a standard RAG pipeline, why might increasing k from 5 to 20 harm the final generated answer even though more "relevant" documents are included?

## Architecture Onboarding

- **Component map:** Raw legal text → Chunks (2000 chars, 400 overlap) → Embeddings → Tree Builder (HAC) → Retrieval Engine (BeFS) → Generator (GPT-4o-mini) → Answer
- **Critical path:** The **index construction phase** is the computational bottleneck at O(N²) for the pairwise distance calculations. This is a one-time cost but becomes prohibitive for corpora exceeding ~50-100K documents without approximation techniques.
- **Design tradeoffs:**
  - **Fixed k vs. Adaptive Scope:** Trades predictability and simplicity of fixed k for context-dependent retrieval scope. Risk: may retrieve too much (ambiguous query → high-level node) or too little (specific query → single leaf).
  - **Tree+QE vs. Raw Query:** Trades latency/cost (extra LLM call) for improved precision. Justified in high-stakes domains like law where retrieval quality is paramount.
  - **Single-linkage clustering:** Trades computational simplicity for potential "chaining" artifacts where clusters become elongated and less semantically coherent.
- **Failure signatures:**
  - **Over-retrieval:** Generic/ambiguous query matches a high-level node, returning an entire large branch (effectively re-introducing the "large k" noise problem).
  - **Under-retrieval:** Query semantics align with a single leaf node, missing broader context that would improve answer completeness.
  - **Clustering pathology:** Single-linkage chaining produces clusters where documents are only transitively similar, causing semantically incoherent retrieval sets.
  - **QE hallucination:** Query extraction introduces legal concepts not in the original query, sending retrieval down an irrelevant branch.
- **First 3 experiments:**
  1. **Baseline reproduction:** Implement Algorithms 1 & 2 on a small public legal QA dataset. Compare retrieval sets against a fixed-k=5 baseline to characterize scope variance.
  2. **QE ablation:** Run the full pipeline with and without Query Extraction on held-out queries. Measure: (a) retrieval overlap with ground-truth relevant docs, (b) expert-rated answer quality.
  3. **Scope distribution analysis:** Log the number of documents retrieved per query across a diverse test set. Identify failure modes: queries retrieving >50 docs (over-retrieval) or =1 doc (potential under-retrieval). Manually inspect edge cases.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the hierarchical clustering retrieval method perform when applied to other specialized domains (e.g., medicine, finance) and languages with significantly different linguistic structures?
  - **Basis in paper:** [explicit] Section 7.2 states the need to verify the adaptability of the proposed method to other domains like medicine or finance, as well as other languages, to ensure generalizability beyond Taiwanese legal texts.
  - **Why unresolved:** The current experiments are strictly limited to a Taiwanese legal dataset, leaving the method's efficacy in other contexts unproven.
  - **What evidence would resolve it:** Benchmark results from experiments run on diverse datasets (e.g., biomedical literature, financial regulations) in non-Chinese languages.

- **Open Question 2:** Can the tree construction algorithm be optimized to incorporate explicit legal logic or citation relationships rather than relying solely on semantic similarity?
  - **Basis in paper:** [explicit] Section 7.1 suggests that future work should focus on optimizing the tree structure to better reflect legal logic and relationships, as the current single-aggregation method based on similarity may miss structural nuances.
  - **Why unresolved:** The current methodology relies exclusively on semantic vectors for hierarchical clustering, potentially ignoring logical connections between statutes.
  - **What evidence would resolve it:** A modified retrieval framework that utilizes citation graphs or logical connectives during tree construction, showing improved retrieval accuracy over the semantic-only baseline.

- **Open Question 3:** How does the retrieval method handle queries where the correct legal answer is effectively "no" (no applicable regulation) or requires integrating distant concepts not closely related semantically?
  - **Basis in paper:** [explicit] Section 6 highlights a limitation where the system assumes the query has a corresponding answer, and Section 7.2 raises the challenge of handling "negative" queries or complex integrations.
  - **Why unresolved:** The hierarchical approach searches for the "most relevant" node, which may force a retrieval result even when no valid legal basis exists, or fail to connect unrelated clusters.
  - **What evidence would resolve it:** Evaluation results using a dataset specifically designed with "negative" test cases (queries with no legal basis) to measure the model's ability to refuse answering or correctly identify gaps.

## Limitations
- The O(N²) complexity for hierarchical clustering is a severe bottleneck for large corpora, making the method impractical beyond ~50-100K documents without approximation
- Claims about adaptability and elimination of the "k" problem are primarily theoretical, with limited evaluation on diverse domains or failure mode analysis
- Single-linkage clustering can produce chaining artifacts where semantically distant documents are linked through transitive similarity

## Confidence
- **High Confidence:** The core mechanism of using hierarchical clustering to build a retrieval tree is technically sound and well-documented (Algorithms 1 & 2). The BFS search logic is correctly described.
- **Medium Confidence:** The Tree+QE approach shows measurable improvement in expert evaluation, but the LLM-based query extraction introduces variability that wasn't fully characterized.
- **Low Confidence:** Claims about the method's adaptability and elimination of the "k" problem are primarily theoretical. The paper doesn't thoroughly analyze failure modes where adaptive scope causes over-retrieval or under-retrieval.

## Next Checks
1. **Clustering coherence validation:** Run silhouette analysis or manual cluster inspection on the constructed tree to quantify semantic coherence and detect chaining artifacts across different tree depths.
2. **Scope distribution analysis:** Log the number of documents retrieved per query across a diverse test set. Identify failure modes: queries retrieving >50 docs (over-retrieval) or =1 doc (potential under-retrieval). Manually inspect edge cases.
3. **Generalization test:** Apply the pipeline to a non-legal corpus (e.g., scientific papers or news articles) with different chunking parameters. Compare retrieval quality degradation to identify domain-specific dependencies.