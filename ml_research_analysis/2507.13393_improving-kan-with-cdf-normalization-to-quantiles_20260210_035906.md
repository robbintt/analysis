---
ver: rpa2
title: Improving KAN with CDF normalization to quantiles
arxiv_id: '2507.13393'
source_url: https://arxiv.org/abs/2507.13393
tags:
- normalization
- distribution
- polynomials
- cdfkal
- legendre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes replacing the MinMax rescaling used in Legendre-KAN
  with CDF normalization to quantiles, aiming to improve representation and generalization.
  CDF normalization transforms inputs via their cumulative distribution function to
  a nearly uniform distribution, which better matches the assumptions of orthonormal
  polynomial bases like Legendre polynomials.
---

# Improving KAN with CDF normalization to quantiles

## Quick Facts
- arXiv ID: 2507.13393
- Source URL: https://arxiv.org/abs/2507.13393
- Authors: Jakub Strawa; Jarek Duda
- Reference count: 12
- One-line primary result: CDF normalization improves KAN generalization by 0.5-2 percentage points on MNIST

## Executive Summary
This paper proposes replacing MinMax rescaling in Kolmogorov-Arnold Networks (KANs) with cumulative distribution function (CDF) normalization to quantiles, achieving better representation and generalization. The approach transforms input features via their CDF to a nearly uniform distribution, better matching the orthogonality assumptions of Legendre polynomial bases. Experiments on MNIST demonstrate consistent accuracy improvements of 0.5-2 percentage points over the original MinMax approach, with the best variant using trainable LayerNorm before the CDF transform converging approximately twice as fast. The method draws inspiration from hierarchical correlation reconstruction theory, where coefficients represent mixed moments and local joint distributions.

## Method Summary
The method replaces standard MinMax scaling in KANs with a two-step normalization: first applying LayerNorm (optionally with fixed affine parameters), then transforming via the Gaussian CDF to map inputs to [0,1]. This creates a uniform input distribution that better matches the orthogonality assumptions of Legendre polynomials. The network uses 3 layers of CDFKal layers (polynomial basis functions up to degree d) with optional SiLU residual connections. Four variants are tested: baseline KAL_NET with MinMax scaling, CDFKAL_NET_FIXEDNORM with fixed LayerNorm parameters, CDFKAL_NET with learnable parameters, and CDFKAL_SILU with SiLU activations. Training uses Adam optimizer with learning rate 1e-3 for 20 epochs on a 20,000-sample MNIST subset.

## Key Results
- CDF-normalized KAN variants achieve 0.5-2 percentage points higher test accuracy than MinMax baseline on MNIST
- Best variant (trainable LayerNorm + CDF) converges approximately twice as fast in epochs
- Removing learnable affine parameters reduces training time by 5-15% without sacrificing accuracy
- Improvements are most pronounced for lower polynomial degrees (3-7), with overfitting occurring at higher degrees (>8)

## Why This Works (Mechanism)

### Mechanism 1: Orthogonality of Legendre Polynomials
- **Claim:** CDF normalization improves representation by low-degree Legendre polynomials by ensuring uniform input distribution
- **Mechanism:** Legendre polynomials form an orthonormal basis on [0,1] only when inputs are uniformly distributed. Real-world data often follows non-uniform distributions. CDF normalization transforms any continuous distribution to uniform, allowing lower-degree polynomials to better approximate underlying functions with improved coefficient balance and reduced overfitting
- **Core assumption:** Input data is continuous and not already uniform
- **Evidence anchors:** Abstract states CDF improves generalization by transforming to uniform distribution; Section II explains improved description with low-degree polynomials

### Mechanism 2: Redundant Learnable Parameters
- **Claim:** Removing trainable affine parameters from LayerNorm reduces training time without accuracy loss
- **Mechanism:** Standard LayerNorm's learnable scale (γ) and shift (β) parameters become redundant after CDF normalization's powerful data-driven rescaling. Fixing these parameters reduces computational cost per update and simplifies the optimization landscape
- **Core assumption:** CDF normalization provides sufficient normalization, making additional learned affine transformations unnecessary
- **Evidence anchors:** Abstract mentions 5-15% training time reduction; Section VI.D shows fixed parameters shave 5-10% off wall-clock time

### Mechanism 3: Accelerated Convergence
- **Claim:** CDF normalization accelerates model convergence by 2× in epochs
- **Mechanism:** Uniform input distribution prevents exploding/vanishing gradients common in polynomial networks with varying input scales. Well-scaled gradients enable more effective optimization steps from training outset
- **Core assumption:** Improved input scaling translates to stable gradient flow during backpropagation
- **Evidence anchors:** Abstract states CDF yields faster convergence (2× faster in epochs); Section VI.D demonstrates 2× epoch speedup

## Foundational Learning

- **Concept: Orthonormal Basis & Uniform Distribution**
  - **Why needed here:** Explains why CDF normalization is beneficial - Legendre polynomials are orthonormal on [0,1] only with uniform inputs
  - **Quick check question:** Why does transforming an input feature to a uniform distribution make it better suited for approximation by Legendre polynomials?

- **Concept: Kolmogorov-Arnold Networks (KANs)**
  - **Why needed here:** This is the architecture being improved - KANs learn univariate functions on edges using polynomial basis functions instead of fixed activation functions on nodes
  - **Quick check question:** What is the fundamental difference between how a KAN and a standard Multi-Layer Perceptron (MLP) process information?

- **Concept: MinMax vs. CDF Normalization**
  - **Why needed here:** Highlights why simple linear rescaling is insufficient for non-uniform data
  - **Quick check question:** For a dataset with many outliers, what is a key disadvantage of MinMax scaling compared to CDF normalization?

## Architecture Onboarding

- **Component map:** Input Layer -> Normalization Module (LayerNorm + Gaussian CDF) -> Legendre Polynomial Layer (CDFKal) -> Optional SiLU -> Stack

- **Critical path:**
  1. Correct implementation of Gaussian CDF normalization is paramount
  2. Fixing LayerNorm affine parameters (γ=1, β=0) to realize training speedup
  3. Properly scaling Legendre polynomials to [0,1] domain for orthonormality

- **Design tradeoffs:**
  - Polynomial Degree: Lower (3-7) improves generalization and speed but may reduce capacity; CDF normalization most effective at lower degrees
  - Learnable vs. Fixed LayerNorm: Fixed is faster to train; learnable may achieve slightly higher final accuracy but adds parameters
  - With vs. Without SiLU: Adding SiLU increases computational cost but provides non-linearity without degrading performance

- **Failure signatures:**
  - Slow convergence/poor accuracy: Incorrect CDF implementation or distribution mismatch
  - Overfitting: Polynomial degree >8 leads to overfitting, negating regularization benefits

- **First 3 experiments:**
  1. Implement simple Legendre-KAN on MNIST comparing MinMax, standardization only, and standardization + Gaussian CDF for accuracy and convergence
  2. Compare CDF-normalized model with learnable vs. fixed LayerNorm affine parameters for wall-clock training time and accuracy
  3. Train CDF-normalized model across polynomial degrees (3,5,7,9,11) to identify optimal range before overfitting occurs

## Open Questions the Paper Calls Out

- **Open Question 1:** Does CDF normalization maintain performance advantages on high-diversity datasets (e.g., ImageNet) and non-vision modalities like text or tabular data? (Section VIII lists this as future work; experiments were restricted to MNIST)
- **Open Question 2:** Does improved coefficient balance from CDF normalization enhance model robustness against distribution shift or adversarial noise? (Section VIII calls for studying behavior under distribution shift, class imbalance, and noisy inputs; study focused on standard accuracy)
- **Open Question 3:** Can uncalibrated HCR density parametrization (which permits negative densities) be used for reliable uncertainty estimation without expensive numerical calibration? (Section IV acknowledges "issue of sometimes getting negative density" but assumes approximation error is "practically negligible")
- **Open Question 4:** Can theoretical HCR extensions like propagating probability distributions or changing propagation directions be implemented effectively in deep networks? (Section IV and VIII mention these capabilities; paper implements KAN equivalent but not proposed multidirectional or distributional propagation)

## Limitations

- The method assumes Gaussian CDF is appropriate for the data distribution; performance may degrade with non-Gaussian distributions
- Benefits are primarily demonstrated on MNIST, a relatively simple image dataset, limiting generalizability claims
- Higher polynomial degrees (>8) lead to overfitting despite CDF normalization, suggesting fundamental capacity-generalization tradeoffs

## Confidence

- Mechanism of uniformization improving low-degree polynomial representation: **High** (well-established orthonormality theory)
- Training time reduction from fixed LayerNorm parameters: **Medium** (empirically demonstrated but hyperparameter-dependent)
- Generalization gains across diverse datasets: **Low** (only MNIST tested, Gaussian CDF assumption limits applicability)

## Next Checks

1. Test CDF normalization on non-image datasets (e.g., UCI datasets) with non-Gaussian distributions to verify the Gaussian CDF assumption doesn't break the approach
2. Systematically sweep network width and batch size to quantify their impact on the reported 2× epoch speedup and determine if width affects the fixed-vs-learnable LayerNorm tradeoff
3. Compare against alternative normalization schemes (e.g., PowerTransform, quantile normalization to exact uniform) to isolate the specific benefit of the Gaussian CDF transform versus general distribution standardization