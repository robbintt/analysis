---
ver: rpa2
title: A Comparative Analysis of Transformer Models in Social Bot Detection
arxiv_id: '2509.14936'
source_url: https://arxiv.org/abs/2509.14936
tags:
- detection
- social
- data
- decoder
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared encoder and decoder transformer architectures
  for social bot detection, using the TwiBot-22 dataset. Encoder models (BERT, RoBERTa,
  DistilBERT) consistently outperformed decoder models (Llama3.1, Mistral, Qwen2.5)
  across all metrics, with RoBERTa achieving 77.0% accuracy and 0.776 F1-score.
---

# A Comparative Analysis of Transformer Models in Social Bot Detection

## Quick Facts
- arXiv ID: 2509.14936
- Source URL: https://arxiv.org/abs/2509.14936
- Authors: Rohan Veit; Michael Lones
- Reference count: 23
- Primary result: Encoder transformers (BERT, RoBERTa, DistilBERT) consistently outperformed decoder models (Llama3.1, Mistral, Qwen2.5) for social bot detection on TwiBot-22 dataset

## Executive Summary
This study systematically compares encoder and decoder transformer architectures for social bot detection using the TwiBot-22 dataset. Encoder models demonstrated superior performance across all metrics, with RoBERTa achieving 77.0% accuracy and 0.776 F1-score, while decoder models struggled with robustness despite showing adaptability through prompt engineering. The research reveals that bidirectional self-attention in encoders provides structural advantages for classification tasks, while decoder models face challenges with null responses and feature set sensitivity. These findings suggest encoder-based classifiers are more effective when supervised learning is available, while decoder models offer customization potential with lower implementation barriers.

## Method Summary
The study used the TwiBot-22 dataset, extracting the first 10% of user messages and creating 5 train/test partitions of 1,000 users each. Two feature sets were evaluated: "Partial" (account metadata only) and "Enriched" (metadata + 3 recent tweets). Three encoder models (BERT, RoBERTa, DistilBERT) were fine-tuned using HuggingFace Transformers with cross-entropy loss, while three decoder models (Llama3.1, Mistral, Qwen2.5) were evaluated via Ollama inference with structured output constraints and prompt engineering. Performance was measured using accuracy, F1-score, precision, recall, and null response rate across 5-fold cross-validation.

## Key Results
- Encoder models achieved 75.9-77.0% accuracy versus 29.9-56.9% for decoder models
- RoBERTa achieved the highest performance at 77.0% accuracy and 0.776 F1-score
- Decoder models showed null response rates up to 45.3%, compared to 0% for encoders
- Prompt engineering improved decoder accuracy by 9.7% but did not close the performance gap
- Adding enriched text features degraded decoder performance more than encoder performance

## Why This Works (Mechanism)

### Mechanism 1: Encoder Bidirectional Attention vs Decoder Unidirectional Attention
Encoder models use bidirectional self-attention allowing each token to attend to all others in both directions, producing richer contextual representations for classification when combined with trained classification heads. Decoder models use unidirectional attention optimized for next-token prediction, requiring prompt engineering to indirectly steer outputs without gradient-based task adaptation. This structural difference makes encoders better suited to exploit supervised learning signals from labeled bot/human examples.

### Mechanism 2: Feature Set Sensitivity
Decoder models process longer input sequences through causal attention, which dilutes task-relevant metadata signals when mixed with noisy tweet content. Encoder models, fine-tuned with gradient descent, can learn to weight features appropriately through backpropagation. The paper notes this aligns with prior findings that adding extra text to user metadata reduced performance across Mistral, Llama2, and ChatGPT.

### Mechanism 3: Fine-Tuning vs In-Context Learning
Few-shot prompting provides in-context learning examples that help decoder models infer classification patterns without parameter updates, improving task alignment but remaining limited by frozen weights and prompt context reliance. Fine-tuned encoders directly optimize classification boundaries through gradient descent on task-specific data, achieving better performance despite lacking the flexibility of prompt engineering.

## Foundational Learning

- **Bidirectional vs. Unidirectional Self-Attention**: Understanding why encoder models outperform decoder models requires grasping how attention directionality affects representation quality. Quick check: Given "This account posts spam," can a decoder model's final hidden state for "account" attend to "spam"? Can an encoder model's representation for "account" do so?

- **Fine-Tuning vs. In-Context Learning**: The paper contrasts supervised fine-tuning (encoders) with prompt engineering (decoders); understanding optimization differences explains performance gaps. Quick check: When you add few-shot examples to a decoder prompt, do any model parameters change? When you fine-tune an encoder with cross-entropy loss, what specifically is being updated?

- **Null Response Rate and Structured Output Constraints**: Decoder models produced null responses up to 45% of the time, indicating a robustness issue critical for deployment decisions. Quick check: If a production bot detection system requires a binary decision for every user, what engineering safeguards would you need to handle null responses from decoder models?

## Architecture Onboarding

- **Component map**: TwiBot-22 JSON → Pydantic schemas (User, Tweet objects) → 5-fold train/test partitions (1,000 users) → Encoder pipeline (HuggingFace tokenizer → BertModel encoder → dropout → linear classifier → CrossEntropyLoss → Adam) OR Decoder pipeline (Ollama inference → PydanticAI agent → structured output schema) → Evaluation (accuracy, precision, recall, F1-score, null response rate)

- **Critical path**: 1) Data preprocessing: Extract first 10% of user messages, validate with Pydantic schemas 2) For encoders: Tokenize text, forward pass through encoder + classifier head, compute loss, backpropagate 3) For decoders: Format user metadata + tweets into prompt, invoke Ollama, parse structured response 4) Evaluation: Aggregate metrics across 5 folds, compute mean and standard deviation

- **Design tradeoffs**: Encoder: Higher accuracy (77%) and zero null responses, but requires labeled training data and GPU compute for fine-tuning; less adaptable without retraining. Decoder: Lower accuracy (52-66%) and non-zero null responses (up to 45%), but requires no training data; easily swapped for larger models; more responsive to prompt engineering. Feature set: Partial (metadata only) more stable across architectures; enriched (metadata + tweets) degrades decoders, minimal effect on encoders.

- **Failure signatures**: High null response rate (>5%): Decoder model failing to produce valid structured output; check prompt formatting, schema complexity, model token limits. Accuracy drops with enriched features: Signal-to-noise ratio in tweet content too low; decoder attention mechanisms over-weighting irrelevant tokens. High variance across folds: Dataset too small (1,000 users) or class imbalance; consider stratified sampling or larger partitions. Encoder overfitting: Training loss decreases but validation accuracy plateaus; increase dropout, reduce epochs, or augment data.

- **First 3 experiments**: 1) Baseline comparison: Run all six models (3 encoder, 3 decoder) on partial feature set (metadata only) with 5-fold CV; record all metrics plus null response rates 2) Feature sensitivity test: Repeat experiment 1 with enriched feature set (metadata + 3 recent tweets); compare performance deltas to identify noise-sensitive models 3) Tuning intervention: Apply prompt engineering (4 few-shot examples + task specification) to best decoder; apply hyperparameter grid search to best encoder; compare tuned vs. untuned performance gains

## Open Questions the Paper Calls Out

- **Graph-based features**: Would incorporating the graph-based relational features available in TwiBot-22 (e.g., follower networks, user interactions) improve decoder performance more than encoder performance? The authors state this as future work but did not utilize the full graph structure.

- **Encoder-decoder architectures**: How would encoder-decoder transformer architectures (e.g., T5, BART) perform relative to encoder-only and decoder-only models? The authors propose evaluating these but only tested encoder-only and decoder-only architectures.

- **Dataset generalizability**: Do the observed performance differences between encoder and decoder architectures generalize across diverse social bot datasets and platforms beyond Twitter/X? The authors acknowledge single-dataset evaluation limits generality.

- **Null response mechanisms**: What mechanisms cause decoder models to produce null responses at higher rates, and can prompt engineering or architectural changes reduce this failure mode? The authors report the phenomenon but do not investigate the underlying causes.

## Limitations

- **Dataset size and generalizability**: The study uses only 1,000 users per fold (5,000 total), which limits statistical power and may not capture the full distribution of bot behaviors in the larger TwiBot-22 dataset containing 295,000+ users.

- **Prompt engineering specificity**: While prompt engineering improved decoder performance by 9.7%, the exact prompt templates are not provided, making exact replication impossible and the improvement dependent on specific example selection.

- **Null response handling**: The study reports high null response rates for decoder models (up to 45%) but does not specify how these cases were handled in final accuracy calculations, potentially inflating decoder performance metrics.

- **Feature content assumptions**: The paper assumes tweet content universally introduces noise that degrades decoder performance, but this may be incorrect if tweet content contains strong bot indicators like coordinated messaging patterns.

## Confidence

**High Confidence**: Encoder models consistently outperform decoder models on accuracy and F1-score (77.0% vs 29.9-56.9%); decoder models exhibit significantly higher null response rates (0% vs up to 45%); adding enriched features degrades decoder performance more than encoder performance.

**Medium Confidence**: Prompt engineering improves decoder accuracy by 9.7%; encoder models show stable performance across feature set variations while decoders degrade; bidirectional attention mechanism of encoders provides structural advantages for classification.

**Low Confidence**: Decoder models are more adaptable through task-specific alignment (practical deployment challenges from null responses not fully addressed); specific hyperparameter values selected through grid search (exact values not disclosed); assumption that tweet content noise universally degrades decoder performance.

## Next Checks

1. **Dataset size sensitivity analysis**: Replicate experiments using progressively larger subsets of TwiBot-22 (5,000 → 25,000 → 100,000 users) to determine if the encoder-decoder performance gap persists with more representative samples and sufficient statistical power.

2. **Null response rate impact assessment**: Run decoder experiments while explicitly tracking how null responses affect overall accuracy by calculating "accuracy including null responses" (treating nulls as incorrect) versus "accuracy excluding null responses" to understand true deployment impact.

3. **Feature content analysis**: Conduct ablation studies on tweet content to identify whether specific linguistic patterns or metadata signals in tweets correlate with bot behavior, validating or challenging the assumption that tweet content is universally noisy for decoder models.