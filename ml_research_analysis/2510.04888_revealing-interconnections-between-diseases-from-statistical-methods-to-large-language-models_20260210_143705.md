---
ver: rpa2
title: 'Revealing Interconnections between Diseases: from Statistical Methods to Large
  Language Models'
arxiv_id: '2510.04888'
source_url: https://arxiv.org/abs/2510.04888
tags:
- methods
- disease
- data
- interconnections
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically evaluates seven approaches for uncovering
  disease interconnections using two data sources: real clinical data (ICD-10 code
  sequences from MIMIC-IV EHRs) and structured disease descriptions. Methods compared
  include statistical co-occurrence analysis, masked language modeling, domain-specific
  BERT variants (Med-BERT, BioClinicalBERT), general-purpose BERT with document retrieval,
  and four LLMs (Mistral, DeepSeek, Qwen, YandexGPT).'
---

# Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models

## Quick Facts
- **arXiv ID**: 2510.04888
- **Source URL**: https://arxiv.org/abs/2510.04888
- **Reference count**: 34
- **Primary result**: LLM-based approaches produce disease interconnections with significantly lower diversity than statistical and embedding methods, suggesting limited potential for discovering novel disease relationships

## Executive Summary
This study systematically evaluates seven approaches for uncovering disease interconnections using two data sources: real clinical data (ICD-10 code sequences from MIMIC-IV EHRs) and structured disease descriptions. Methods compared include statistical co-occurrence analysis, masked language modeling, domain-specific BERT variants, general-purpose BERT with document retrieval, and four LLMs (Mistral, DeepSeek, Qwen, YandexGPT). The analysis reveals that LLM-based approaches produce interconnections with the lowest diversity of ICD code connections compared to other methods. In the absence of ground truth databases, the study constructs a valuable medical disease ontology from consistently identified interconnections, serving as a foundational resource for future clinical research and AI applications in healthcare.

## Method Summary
The study compares ten methods across two data sources: MIMIC-IV ICD-10 sequences (223,291 patients, 1,754 unique 3-character codes) and structured disease descriptions. Statistical methods include Fisher's exact test with FDR correction and Jaccard co-occurrence. Machine learning approaches include a custom MLM model (3-layer transformer, 128-dim embeddings), domain-specific BERT variants (Med-BERT, BioClinicalBERT), general BERT with document retrieval, and four LLMs (Mistral, DeepSeek, Qwen, YandexGPT). Methods produce pairwise interconnection matrices that are thresholded at the 0.95-quantile to construct graphs, then analyzed through Spearman correlations, degree distributions, and manual literature validation.

## Key Results
- LLM-based approaches (DeepSeek, Qwen, YandexGPT) produce disease interconnections with the lowest diversity, showing narrower degree distributions and more conservative associations
- Statistical co-occurrence methods identify 138 significant disease pairs using Fisher's exact test with FDR correction
- The constructed consensus ontology aggregates interconnections from multiple methods to create a valuable medical disease knowledge base
- LLMs demonstrate limitations in discovering novel disease relationships compared to statistical and embedding-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Statistical Co-occurrence Quantifies Grounded Clinical Patterns
- Claim: Co-occurrence statistics (Fisher's exact test, Jaccard) identify disease pairs that appear together in real patient histories more often than expected by chance.
- Mechanism: For each ICD pair (i,j), the method counts patients with both diagnoses, then tests whether this co-occurrence significantly exceeds independence. High scores reflect genuine clinical multimorbidity patterns (e.g., shared risk factors like metabolic syndrome).
- Core assumption: The MIMIC-IV dataset captures representative disease co-occurrences, not just ICU-specific artifacts.
- Evidence anchors:
  - [abstract] "sequences of ICD-10 codes from MIMIC-IV EHRs"
  - [section 4.2.3] Lung and prostate cancer share five frequent comorbidities (E11, E78, I10, I25, Z87) driven by shared risk factors.
  - [corpus] Related work on multimorbidity patterns confirms metabolic syndrome links (weak direct evidence for this specific method).
- Break condition: If MIMIC-IV is heavily biased toward ICU populations, co-occurrence patterns may not generalize to broader clinical settings.

### Mechanism 2: MLM Learns Sequential Diagnostic Structure
- Claim: Masked language modeling on ICD sequences learns embeddings that encode disease relationships beyond simple co-occurrence.
- Mechanism: The model masks 15% of tokens in patient ICD sequences and trains to predict them using surrounding context. Learned embeddings reflect which diagnoses tend to appear in similar sequential positions across patients.
- Core assumption: The sequential ordering of ICD codes contains meaningful signal about disease progression, not just coding artifacts.
- Evidence anchors:
  - [section 3.2.1] "15% of tokens are selected for prediction; of these, 80% are replaced with [MASK]"
  - [section 4.2.2] "MLM embeddings exhibit the weakest separation" in t-SNE, though non-random distribution suggests learned structure.
  - [corpus] No direct corpus support for this specific MLM approach.
- Break condition: If ICD sequencing is inconsistent (e.g., chronic conditions coded repetitively regardless of timing), learned sequential patterns may be noise.

### Mechanism 3: LLMs Exhibit Low-Diversity Connectivity from Training Data Bias
- Claim: LLMs (DeepSeek, Qwen, YandexGPT) produce disease interconnections with significantly lower diversity than other methods.
- Mechanism: When prompted to predict related ICD codes, LLMs retrieve associations from their pretraining corpus. These associations tend toward common, well-documented relationships, producing conservative, homogeneous outputs (low node degree variance).
- Core assumption: The LLM's medical knowledge reflects training corpus distributions, which favor frequently discussed disease pairs.
- Evidence anchors:
  - [abstract] "LLM-based approach produces interconnections with the lowest diversity of ICD code connections"
  - [section 4.2.3] YandexGPT and Qwen show narrowest degree ranges "close to zero, suggesting these LLM-based approaches may be more conservative"
  - [corpus] Related work on LLM diagnostic reasoning shows similar conservatism in rare disease contexts (MIMIC-RD, DeepRare).
- Break condition: Prompt engineering or fine-tuning on domain-specific EHR data could increase diversity; current results reflect base model behavior.

## Foundational Learning

- **Co-occurrence Statistics (Fisher's Exact Test, Jaccard Index)**
  - Why needed here: These form the baseline for evaluating whether more complex models add value beyond raw frequency patterns.
  - Quick check question: Can you explain why Fisher's exact test is one-sided ("greater") and how FDR correction affects the 138 significant associations?

- **Transformer Embeddings and Cosine Similarity**
  - Why needed here: All BERT variants and the MLM model produce embeddings compared via cosine similarity to construct interconnection matrices.
  - Quick check question: Why might BioClinicalBERT produce "largely opposite results" to Med-BERT (high similarity where others are low)?

- **Graph Construction from Similarity Matrices**
  - Why needed here: The study converts matrices to graphs by thresholding at the 0.95-quantile, then analyzes degree distributions and connected components.
  - Quick check question: What information is lost when thresholding at the 0.95-quantile, and why is this acceptable for consensus ontology construction?

## Architecture Onboarding

- **Component map:**
  MIMIC-IV data -> Preprocessing (ICD conversion, truncation, deduplication) -> 10 methods -> Interconnection matrices -> Graph construction (0.95-quantile threshold) -> Analysis (correlations, degree distributions, PR AUC)

- **Critical path:**
  1. Preprocess ICD codes (convert ICD-9 to ICD-10, truncate to 3-character categories, deduplicate per admission)
  2. Run all 10 methods to produce interconnection matrices
  3. Threshold matrices at 0.95-quantile → construct graphs
  4. Aggregate across methods to build consensus ontology

- **Design tradeoffs:**
  - Truncating ICD codes to 3-character level reduces dimensionality but loses subcategory specificity (e.g., C50.911 → C50)
  - Thresholding at the 0.95-quantile retains only high-confidence connections but discards potentially novel weak signals
  - LLM prompt forces multiple predictions per query (avoiding O(N²) API calls) but may miss rare associations

- **Failure signatures:**
  - LLMs returning malformed JSON (observed with models <14B parameters) → increase model size or add post-processing
  - BioClinicalBERT showing inverse patterns to Med-BERT → may reflect different pretraining objectives, not necessarily error
  - Jaccard similarity inflating scores for rare diseases → exclude from ground truth comparisons

- **First 3 experiments:**
  1. Replicate Fisher's exact test on the preprocessed MIMIC-IV data to verify the 138 significant associations match the paper's baseline.
  2. Train the MLM model (3-layer transformer, 128-dim embeddings) and confirm test accuracy (~0.30) and t-SNE clustering patterns.
  3. Query one LLM (DeepSeek-V3) with the provided prompt for 10 diverse ICD codes, then compare returned connections against Med-BERT cosine similarities to validate the low-diversity claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the disease interconnections identified by different modeling approaches improve performance on downstream clinical prediction tasks?
- Basis in paper: [explicit] The authors state they "will examine how the identified interconnections and model embeddings affect downstream tasks, such as cancer prediction and mortality risk assessment."
- Why unresolved: The current study focused solely on deriving interconnections and comparing methodologies, without applying these structures to predictive modeling tasks.
- What evidence would resolve it: Integrating the generated interconnection matrices as features or priors in predictive models to see if they increase accuracy in tasks like mortality prediction.

### Open Question 2
- Question: Do interconnections found in statistical baselines but missing in deep learning models represent MIMIC-IV noise or novel clinical relationships?
- Basis in paper: [explicit] The authors list investigating "the presence of noise in the MIMIC dataset in more detail" as a future direction, noting they "cannot conclude that MIMIC-IV contains noisy data."
- Why unresolved: The analysis could not definitively determine if discrepancies between Fisher's exact test and Med-BERT were artifacts or valid, underreported associations.
- What evidence would resolve it: Clinical expert review of the specific discordant pairs or validation against an independent, external EHR dataset.

### Open Question 3
- Question: How can the quality of LLM-generated disease interconnections be evaluated without existing ground truth data?
- Basis in paper: [explicit] The conclusion lists plans to "incorporate an unsupervised quality assessment of LLMs."
- Why unresolved: The study relied on relative comparisons (e.g., Spearman correlations) and proxy indicators because a true "ground truth" database for disease links does not exist.
- What evidence would resolve it: The development of intrinsic evaluation metrics or proxy tasks that correlate with clinical validity without requiring manual labeling.

## Limitations

- Absence of ground truth disease interconnection data forces reliance on proxy validation methods and manual literature review
- Truncation of ICD codes to 3-character categories reduces clinical specificity and may obscure important subcategory relationships
- LLM-based results are highly sensitive to prompt engineering and model parameter sizes, with smaller models producing unreliable outputs
- The study does not account for temporal disease progression or patient demographic factors that could influence interconnection patterns

## Confidence

- **High confidence**: Statistical co-occurrence methods producing consistent multimorbidity patterns (Fisher's exact test with FDR correction)
- **Medium confidence**: BERT-based embedding approaches showing reasonable but varying similarity patterns across models
- **Low confidence**: LLM-based interconnections due to conservative associations and potential training data bias

## Next Checks

1. Replicate the Fisher's exact test analysis on the full MIMIC-IV dataset to verify the 138 significant associations and assess whether co-occurrence patterns persist across different time periods or patient subpopulations.

2. Conduct systematic prompt engineering experiments with the LLMs to determine whether diversity in disease interconnections can be improved through targeted queries or fine-tuning on EHR data.

3. Perform temporal validation by comparing interconnection patterns across different patient admission time windows to assess whether observed associations reflect persistent clinical patterns or temporal artifacts.