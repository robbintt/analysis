---
ver: rpa2
title: 'VisualPRM: An Effective Process Reward Model for Multimodal Reasoning'
arxiv_id: '2503.10291'
source_url: https://arxiv.org/abs/2503.10291
tags:
- arxiv
- step
- correct
- reasoning
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisualPRM, an 8B parameter Process Reward
  Model (PRM) designed to enhance multimodal reasoning in Multimodal Large Language
  Models (MLLMs) through Best-of-N evaluation strategies. The model improves reasoning
  performance across three types of MLLMs and four model scales, achieving a 5.9-point
  improvement on InternVL2.5-78B across seven benchmarks.
---

# VisualPRM: An Effective Process Reward Model for Multimodal Reasoning

## Quick Facts
- arXiv ID: 2503.10291
- Source URL: https://arxiv.org/abs/2503.10291
- Authors: Weiyun Wang; Zhangwei Gao; Lianjie Chen; Zhe Chen; Jinguo Zhu; Xiangyu Zhao; Yangzhou Liu; Yue Cao; Shenglong Ye; Xizhou Zhu; Lewei Lu; Haodong Duan; Yu Qiao; Jifeng Dai; Wenhai Wang
- Reference count: 40
- Primary result: 8B PRM achieves 5.9-point improvement on InternVL2.5-78B across seven benchmarks using Best-of-N evaluation

## Executive Summary
This paper introduces VisualPRM, a 8B parameter Process Reward Model (PRM) designed to enhance multimodal reasoning in Multimodal Large Language Models (MLLMs) through Best-of-N evaluation strategies. The model improves reasoning performance across three types of MLLMs and four model scales, achieving a 5.9-point improvement on InternVL2.5-78B across seven benchmarks. To enable PRM training, the authors construct VisualPRM400K, a dataset of 400K multimodal process supervision samples, and propose VisualProcessBench, a benchmark with 2,866 samples and 26,950 human-annotated step-wise correctness labels for evaluating PRMs.

## Method Summary
VisualPRM is trained using an automated data pipeline that generates process supervision from MMPR v1.1 questions and InternVL2.5 solutions. The pipeline uses Monte Carlo sampling (16 completions per step) to estimate step quality, creating binary training labels where a step is correct if its expected accuracy exceeds zero. The 8B PRM uses a value-based formulation in multi-turn dialogue format, predicting binary correctness per step. During inference, the model aggregates step scores via averaging to rank and select the best candidate from N generated responses. The model is evaluated across seven multimodal reasoning benchmarks using Best-of-N strategies.

## Key Results
- VisualPRM outperforms Outcome Reward Models and Self-Consistency in BoN evaluation, achieving 5.9-point improvement on InternVL2.5-78B
- Improvements generalize across three MLLM types and four model scales (8B, 26B, 38B, 78B)
- VisualPRM400K dataset constructed with 400K samples and 2M steps using MC sampling
- VisualProcessBench benchmark introduced with 2,866 samples and 26,950 human-annotated step-wise labels

## Why This Works (Mechanism)

### Mechanism 1: Monte Carlo Sampling for Process Supervision
The paper constructs VisualPRM400K using Monte Carlo sampling to generate step-wise supervision without human annotation. For each reasoning step, 16 continuations are sampled and their accuracy is computed to determine step correctness. This enables scalable data collection while maintaining reasonable label quality, though the 10% negative class imbalance suggests some noise in the automatic labeling process.

### Mechanism 2: Value-Based PRM Formulation
VisualPRM uses a value-based formulation that predicts binary correctness per step in multi-turn dialogue format. During inference, step scores are averaged to produce response rankings. This approach outperforms advantage-based alternatives and outcome-based ORMs, particularly as N grows in Best-of-N evaluation where the PRM gap widens to 4.3 points at N=128.

### Mechanism 3: Cross-Model Transfer Capability
The PRM operates as an external critic during inference, enabling cross-model and cross-scale transfer without policy modification. Any policy model generates N candidates, the PRM scores and selects the best, requiring no weight updates to the policy model. This allows VisualPRM to improve reasoning performance across different MLLM families and scales.

## Foundational Learning

- **Process Reward Models vs. Outcome Reward Models**: PRMs evaluate intermediate reasoning steps while ORMs score only final answers. PRMs better distinguish candidate quality when N grows, with the gap widening to 4.3 points at N=128 versus ORM's plateau. Quick check: Given a 10-step solution with errors at steps 3 and 7 but correct final answer, would an ORM or PRM give a higher score?

- **Monte Carlo Estimation for Process Labels**: The training pipeline uses MC sampling to approximate step quality without human labels, explaining both scalability (400K samples) and limitations (10% negative class imbalance, noise sensitivity). Quick check: If a step has mci = 0.5 from 16 samples, what's the uncertainty range? How might this affect training?

- **Test-Time Scaling (Best-of-N Evaluation)**: VisualPRM's value proposition is enabling TTS for MLLMs. The mechanism requires understanding how critic quality bounds performance gains—Figure 1 shows InternVL2.5-8B as critic gives marginal improvement vs. VisualPRM's 8-9 point gains. Quick check: If a policy model achieves 40% Pass@1 and the PRM has 70% accuracy at identifying correct responses, what's the theoretical upper bound for Best-of-8?

## Architecture Onboarding

- **Component map**: MMPR v1.1 (questions) -> InternVL2.5 (solutions) -> MC sampling (16 completions/step) -> VisualPRM400K -> 8B PRM (value-based, multi-turn) -> BoN inference (averaged step scores) -> VisualProcessBench evaluation

- **Critical path**: Data quality hinges on MC sampling parameters (16 continuations, max 12 steps) -> Model training uses all steps (no early stop) -> Inference aggregation via averaging -> Benchmark evaluation uses macro F1 to handle class imbalance

- **Design tradeoffs**: Value-based more robust to MC noise vs. advantage-based requiring accurate Δ-mc estimation; averaging ensembles multiple step judgments vs. max vulnerable to spurious high early scores; supervising all steps improves over early-stop approach; mci > 0 threshold outperforms higher thresholds

- **Failure signatures**: Policy model generates similar candidates (PRM cannot distinguish); high early-step scores (max aggregation will fail); PRM always predicts positive (check training balance); ORM outperforming PRM at high N (suggests overfitting)

- **First 3 experiments**: 1) Reproduce VisualProcessBench F1 scores for PRM variant to establish baseline error detection capability (62.0 macro F1 reported) 2) Ablate N in Best-of-N with fixed policy model to verify scaling behavior matches Figure 4 3) Cross-model transfer test: train PRM on one model family, evaluate on another to measure generalization

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding its approach. It questions whether an advantage-based PRM could outperform the value-based approach if trained on noise-free, human-annotated data, noting that current comparisons are confounded by data quality. The paper also questions whether VisualPRM can be effectively utilized as a dense reward signal for reinforcement learning rather than solely for inference-time Best-of-N selection. Additionally, it questions whether the heuristic of merging steps beyond 12 significantly impairs the model's ability to localize fine-grained errors in complex reasoning chains.

## Limitations

- Monte Carlo-based data generation introduces label noise, with only 10% negative class imbalance suggesting potential quality issues
- The step-merging threshold of 12 steps may reduce granularity and impair fine-grained error localization in complex reasoning chains
- Cross-architecture generalization remains untested, with results primarily showing within-family improvements
- The model struggles to distinguish between very similar candidates, limiting effectiveness when policy models generate high-quality but diverse responses

## Confidence

- **High**: BoN evaluation superiority over ORM and SC with p < 0.05 via bootstrap tests
- **Medium**: MC sampling data quality and label reliability with 10% negative class imbalance
- **Low**: Generalization across fundamentally different MLLM architectures beyond reported within-family improvements

## Next Checks

1. **Replicate VisualProcessBench performance**: Train VisualPRM and measure macro F1 on the 2,866-sample benchmark to verify the reported 62.0 F1 score and establish baseline error detection capability

2. **Validate MC label quality**: Generate MC samples with varying continuation counts (8, 16, 32) and measure impact on PRM training stability and downstream BoN performance

3. **Test cross-architecture transfer**: Train PRM on InternVL2.5 family and evaluate on completely different MLLM architectures (e.g., Gemini, GPT-4V) to measure true generalization limits beyond reported within-family improvements