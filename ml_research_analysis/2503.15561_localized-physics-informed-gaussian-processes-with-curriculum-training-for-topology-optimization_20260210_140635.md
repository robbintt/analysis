---
ver: rpa2
title: Localized Physics-informed Gaussian Processes with Curriculum Training for
  Topology Optimization
arxiv_id: '2503.15561'
source_url: https://arxiv.org/abs/2503.15561
tags:
- function
- optimization
- comsol
- design
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel topology optimization framework,
  LC-SMTO, which combines physics-informed Gaussian processes with localized learning
  and curriculum training. The method addresses limitations of existing approaches
  by employing a customized deep neural network (PGCAN) for localized feature learning
  and a distance-based weighting scheme to improve accuracy around interfaces.
---

# Localized Physics-informed Gaussian Processes with Curriculum Training for Topology Optimization

## Quick Facts
- arXiv ID: 2503.15561
- Source URL: https://arxiv.org/abs/2503.15561
- Reference count: 28
- Primary result: LC-SMTO achieves lower dissipated power and more symmetric topologies than SMTO and COMSOL on 2D Stokes flow benchmarks

## Executive Summary
This paper introduces LC-SMTO, a topology optimization framework that combines physics-informed Gaussian processes with localized learning via PGCAN and curriculum training. The method addresses limitations of existing approaches by employing a customized deep neural network for localized feature learning and a distance-based weighting scheme to improve accuracy around interfaces. Additionally, curriculum training is used to gradually enforce volume constraints, enhancing optimization stability and convergence. The framework is validated on three benchmark problems involving dissipated power minimization in Stokes flow, demonstrating superior performance compared to both the previous SMTO method and COMSOL in terms of objective function values, solution symmetry, and interface sharpness.

## Method Summary
LC-SMTO extends the SMTO framework by replacing the fully-connected neural network with a Physics-informed Gaussian Process with Customized Attention Network (PGCAN). The method uses a grid-based encoder with trainable vertex parameters to enable localized feature learning, combined with a distance-based weighting scheme that up-weights collocation points near solid-fluid interfaces. A curriculum training approach gradually enforces volume constraints, starting with a single-phase material and progressively tightening the constraint. The physics is modeled using Brinkman equations for Stokes flow, and the loss function balances objective function minimization, PDE residuals, and volume constraints with adaptive weights.

## Key Results
- LC-SMTO consistently achieves lower dissipated power than both SMTO and COMSOL across all three benchmarks
- The method produces more symmetric topologies, particularly for the Rugby and Double Pipe problems
- Interface sharpness is significantly improved, with residuals near interfaces reduced by approximately one order of magnitude
- Computational costs remain comparable to SMTO while providing superior solution quality

## Why This Works (Mechanism)

### Mechanism 1: Physics-Informed GP with Shared Mean Function
By endowing state and design variables with GP priors that share a neural mean function, boundary conditions are automatically satisfied while maintaining correlation between design and physics. The conditional mean adds a kernel-weighted correction term to the DNN output that is zero at boundary points by construction, satisfying BCs exactly regardless of mean function parameters.

### Mechanism 2: Localized Learning via PGCAN
A grid-based encoder with trainable vertex parameters enables learning of sharp interfaces and high-gradient regions that standard fully-connected networks fail to capture. The encoder creates localized feature vectors by parameterizing the domain via a structured grid where parameters of a vertex are optimized based on their surrounding cells and not the entire design domain.

### Mechanism 3: Distance-Based Residual Weighting + Curriculum Training
A two-part training strategy—up-weighting collocation points near interfaces and gradually enforcing volume constraints—improves convergence stability and solution quality. Weight function w(x) assigns higher weights to points on the zero-level set (interface), creating smooth interpolation between regions. Curriculum training schedules the volume constraint as V_scheduled(i) decreasing from 1.0 to V_target in blocks, allowing the model to initially learn the PDE solution over a single-phase material.

## Foundational Learning

- **Gaussian Process Regression with Mean Function**
  - Why needed: The entire framework builds on understanding that a GP is fully specified by mean and covariance functions, and conditioning on data yields a posterior GP.
  - Quick check: Can you explain why the conditional mean z_i(x*; θ) in Equation (6) interpolates boundary data exactly?

- **Physics-Informed Neural Networks (PINNs)**
  - Why needed: LC-SMTO extends PINN concepts by adding GP structure; understanding residual-based loss helps grasp why PDE residuals appear in Equation (3).
  - Quick check: How does the multi-component loss L(ζ) balance objective, constraints, and PDE residuals differently than a standard PINN?

- **Level Set Methods for Topology Optimization**
  - Why needed: The design variable ρ(x) is obtained from level set function ψ(x) via Heaviside function; understanding this implicit interface representation is crucial.
  - Quick check: Why does using a level set function (Equation 1e) reduce "gray areas" compared to density-based methods?

## Architecture Onboarding

**Component Map:**
Input: Spatial coordinates x
  ↓
PGCAN Encoder:
  - Initialize grid features F_0 ~ U[-10⁻⁵, 10⁻⁵]
  - Convolve with kernel (3×128×9×9)
  - Interpolate to query points → (f₁, f₂)
  ↓
PGCAN Decoder:
  - 3 hidden layers (64 neurons each)
  - Attention mechanism for feature weighting
  → Mean function m(x; θ)
  ↓
GP Conditioning Layer:
  - For each variable i: z_i = m_i + k_i(X*, X_i)k_i⁻¹(u_i - m_i(X_i))
  - Ensures BC satisfaction automatically
  ↓
Outputs: [u₁(x), u₂(x), ..., ψ(x)]
  ↓
Post-processing: ρ(x) = h(ψ(x)) [Heaviside]
  ↓
Loss Computation:
  - Objective L_o (dissipated power)
  - Weighted PDE residuals R_i² with w(x)
  - Volume constraint C₁²
  → L(ζ) = L_o + μ_p[α·(residuals + constraints)]
  ↓
Curriculum Schedule:
  - V_scheduled(i) updates every block_size iterations
  - Volume constraint gradually enforced

**Critical Path:**
1. Boundary sampling (n_bc = 25 per side) must cover all BCs before optimization starts
2. Collocation point distribution (n_cp = 10k–30k) determines residual accuracy; points must include interface regions
3. Mean function → GP conditioning → Loss → Adam update repeats for 20k epochs
4. Localized weighting activates after 9k epochs
5. Curriculum schedule completes at iteration i_t = 4k

**Design Tradeoffs:**
| Choice | Paper Setting | Alternative | Tradeoff |
|--------|---------------|-------------|----------|
| Kernel length-scale | Fixed at 10³ | Optimize φ_i | Fixed reduces optimization complexity but may underfit |
| Grid size in PGCAN | 3×128×9×9 | Larger/smaller | Larger captures finer features but increases parameters |
| Block size for curriculum | b_s = 20 | Continuous schedule | Block-wise stabilizes but may overshoot intermediate targets |
| Weight ratio w_h/w_l | 2.0/0.9 ≈ 2.2 | Higher ratio | Stronger interface focus risks ignoring bulk physics |

**Failure Signatures:**
- Gray areas in topology: Projection not fully binary; check Heaviside implementation
- Asymmetric solutions (e.g., Rugby problem): May indicate insufficient exploration; increase random restarts
- High residuals at interfaces: Localized weighting not activating or δ too small
- Loss oscillation after 7k epochs: Curriculum schedule may be too aggressive; increase i_t
- Covariance matrix singularity: Boundary sampling too sparse; increase n_bc

**First 3 Experiments:**
1. Validate GP conditioning: Create simple 1D problem with known BCs; verify z(x*) = u_BC exactly at boundary points regardless of m(x; θ) initialization
2. Ablate localized weighting: Run Rugby problem with w(x) = 1 everywhere; compare residual maps and final J to full LC-SMTO (expect ~10× higher interface residuals)
3. Compare curriculum schedules: Test continuous polynomial schedule vs. block-wise; monitor volume loss C₁² convergence and topology symmetry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does employing the weak form of governing equations, rather than the strong form, affect the stability and accuracy of the LC-SMTO framework?
- Basis in paper: [explicit] The authors state, "We plan to investigate this promising extension in our future works" regarding the use of the weak form (Page 15).
- Why unresolved: The current implementation relies on the strong form, which imposes strict smoothness requirements that the weak form would relax, potentially altering performance.
- What evidence would resolve it: A comparative study implementing the weak form loss on the same Stokes flow benchmarks to evaluate changes in convergence rates and solution stability.

### Open Question 2
- Question: Can the LC-SMTO framework maintain its computational efficiency and accuracy when scaled to complex three-dimensional topology optimization problems?
- Basis in paper: [inferred] All validation benchmarks (Rugby, Obstacle, Double Pipe) are restricted to 2D domains (Section 3), and the method relies on covariance matrix inversion which scales poorly with data dimensionality.
- Why unresolved: While PGCAN handles localized features, the memory overhead of the GP kernel and the grid-based encoding may become prohibitive in 3D.
- What evidence would resolve it: Applying the method to a 3D optimization problem and reporting memory usage and training time relative to the 2D cases.

### Open Question 3
- Question: Is the localized weighting mechanism effective for capturing discontinuities in solid mechanics, such as stress concentrations, or is it specific to fluid interfaces?
- Basis in paper: [inferred] The paper mentions PGCAN helps with "stress concentration regions in compliance minimization problems" (Section 2.3), but experimental validation is limited to Stokes flow.
- Why unresolved: The distance-based weighting w(x) was tuned for fluid-solid interfaces (ψ(x) ≈ 0); its utility for the high-gradient fields found in structural mechanics is unproven.
- What evidence would resolve it: Testing the framework on a compliance minimization benchmark and analyzing the residual reduction in high-stress regions.

## Limitations

- **Adaptive Penalty Scheme Uncertainty**: The exact adaptive updates for weights α and penalty μ_p reference prior work but are not explicitly detailed in the paper.
- **2D Restriction**: All validation benchmarks are restricted to 2D domains, with no evidence of performance on 3D problems or different physics.
- **Curriculum Schedule Sensitivity**: The block-wise curriculum shows good results but doesn't explore alternative schedules or parameter sensitivity.

## Confidence

**High Confidence Claims:**
- The PGCAN architecture provides localized feature learning advantages over fully-connected networks
- Distance-based weighting reduces PDE residuals near interfaces by approximately one order of magnitude
- Curriculum training improves optimization stability and helps avoid local minima

**Medium Confidence Claims:**
- The overall framework consistently produces lower dissipated power than SMTO and COMSOL across all three benchmarks
- The combination of localized weighting and curriculum training is essential for achieving symmetric solutions

**Low Confidence Claims:**
- The exact adaptive penalty update mechanism and its impact on convergence
- Generalization to problems beyond the three specific Stokes flow benchmarks
- Computational cost comparison is fair given the different underlying implementations (GP vs. FEM)

## Next Checks

1. **Ablation Study on Weighting Mechanism**: Run the Rugby problem with w(x) = 1 everywhere and compare residual maps and final objective values to the full LC-SMTO. This would quantify the exact contribution of the localized weighting to solution quality.

2. **Curriculum Schedule Sensitivity Analysis**: Test alternative curriculum schedules (continuous vs. block-wise, different i_t values) on the Obstacle problem. Monitor volume constraint violation C₁² and final J to identify optimal scheduling parameters.

3. **Boundary Condition Sampling Robustness**: Systematically vary n_bc from 10 to 50 per boundary side on the Double Pipe problem. Measure BC interpolation accuracy and final objective J to determine minimum sampling requirements for stable optimization.