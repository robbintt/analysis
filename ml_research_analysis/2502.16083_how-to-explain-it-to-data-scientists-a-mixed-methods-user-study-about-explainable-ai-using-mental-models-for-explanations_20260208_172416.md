---
ver: rpa2
title: How to explain it to data scientists? A mixed-methods user study about explainable
  AI, using mental models for explanations
arxiv_id: '2502.16083'
source_url: https://arxiv.org/abs/2502.16083
tags:
- explanation
- data
- mental
- content
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a mixed-methods user study on explainable
  AI (XAI) for data scientists, employing mental models to identify specific explanation
  needs. The research investigated two user tasks: generating ML models and maintaining
  them.'
---

# How to explain it to data scientists? A mixed-methods user study about explainable AI, using mental models for explanations

## Quick Facts
- arXiv ID: 2502.16083
- Source URL: https://arxiv.org/abs/2502.16083
- Reference count: 40
- Primary result: Mixed-methods study identifying data scientists' explanation needs across application, system, and AI domains using mental models

## Executive Summary
This study investigates explainable AI (XAI) for data scientists through a mixed-methods approach, employing mental models to identify specific explanation needs for machine learning model generation and maintenance tasks. The research develops standardized explanation questions and demonstrates that refining mental models through user feedback significantly improves their quality. Key findings include the need for explanation content spanning three domains (application, system, AI), organization as causal stories, and the effectiveness of standardized questions for ensuring complete coverage of explanation needs.

## Method Summary
The study employed a mixed-methods approach with 12 participants for qualitative interviews and 12 for quantitative validation, all experienced data scientists with 3+ years industrial ML experience. Researchers used a 6-step mental model creation process: raw data collection through interviews, initial mental model creation, qualitative validation, model revision, UI mockup creation, and quantitative validation. The study utilized standardized explanation questions (EQ1-EQ4) for task analysis and open coding for qualitative data analysis, followed by quantitative selection and ranking of explanation elements across five explanation intents.

## Key Results
- Explanation content spans application, system, and AI domains to address data scientists' multi-domain reasoning needs
- Content should be organized as sequential causal chains to support reasoning about cause-effect relationships
- Standardized explanation questions ensure complete coverage of explanation needs across different user tasks
- Five key explanation intents identified: reason, comparison, accuracy, prediction, and trust
- Mental model refinement through user feedback significantly improves explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data scientists require explanation content from three domains (application, system, AI) to make confident decisions about ML model generation and maintenance.
- Mechanism: When uncertainty arises from AI outcomes, data scientists map the outcome to three domains simultaneously—application context, system constraints, and AI mechanisms. This multi-domain mapping enables causal reasoning about why an outcome occurred and whether it will generalize.
- Core assumption: Data scientists possess sufficient cross-domain knowledge to integrate information from application and system domains.
- Evidence anchors: [abstract] "F1: Explanation content for data scientists comes from the application domain, system domain, and AI domain." [Section 4.5, Finding 1] All explanation elements contain content from all three domains.

### Mechanism 2
- Claim: Organizing complex explanation content as a sequential causal chain improves decision confidence by providing a narrative structure for reasoning.
- Mechanism: The causal chain (Context → Symptom → Causal factors → Immediate actions → Responsive actions → Projected objectives) creates a forward-chaining mental model where each element provides evidence for the next.
- Core assumption: The system can reliably identify causal relationships between symptoms and root causes.
- Evidence anchors: [abstract] "F4: Explanation content should be organized as a causal story." [Section 4.4] Participant P12 requested organization in a causal chain.

### Mechanism 3
- Claim: Using standardized explanation questions (EQ1-EQ4) during task analysis ensures complete coverage of explanation needs across different user tasks.
- Mechanism: Predefined questions about understanding how/why outcomes were determined, validity assessment, and future effectiveness force systematic elicitation of explanation content.
- Core assumption: The four standardized questions comprehensively capture all relevant explanation intents for data scientists.
- Evidence anchors: [abstract] "F5: Standardized explanation questions ensure complete coverage of explanation needs (novelty claim)." [Section 3.2] Lists EQ1-EQ4 used for both UT2 and UT3.

## Foundational Learning

- **Mental Models in HCI**
  - Why needed here: The paper's core methodology relies on eliciting, validating, and refining mental models of explanations before building systems.
  - Quick check question: Can you explain the difference between a user's mental model of a system and the designer's conceptual model?

- **Task Analysis for Explanation Elicitation**
  - Why needed here: The paper uses goal/task decomposition to identify where uncertainty-addressing explanations are needed (UT2, UT3) versus where they aren't (UT1).
  - Quick check question: For a given user workflow, can you identify which decision points create uncertainty that requires explanation?

- **Mixed-Methods Validation (Qualitative → Quantitative)**
  - Why needed here: The study uses qualitative interviews (n=12) to refine mental models, then quantitative validation (n=12) to test which content serves different intents.
  - Quick check question: Why is sample size of 12 justified for qualitative saturation but potentially limited for quantitative generalization?

## Architecture Onboarding

- **Component map:**
  - Inputs Layer (E1): Task definition, environment/context, constraints (dev/ops/maintenance phases), objectives/benchmarks
  - Historical View (E2, E7-E8): Training data, model artifacts, performance metrics, historical cases
  - Simulation Layer (E3, E9): Production-like environment, simulation cases, projected outcomes
  - Synthesis Layer (E4, E10): Combined comparisons, ranked outputs, strengths/weaknesses analysis
  - Causal Chain (E11): Narrative structure linking context → symptom → cause → action → outcome

- **Critical path:**
  1. Define standardized explanation questions (EQ1-EQ4) for each user task
  2. Elicit initial mental model content through task analysis
  3. Conduct qualitative validation with target users (look for "what's missing" feedback)
  4. Revise model by adding domain-specific content (application/system/AI)
  5. Structure revised content as hierarchical/sequential elements
  6. Quantitatively validate which content serves which explanation intents

- **Design tradeoffs:**
  - Completeness vs. cognitive load: Including all three domains increases content but risks overwhelming users; hierarchical organization mitigates this
  - Causal narrative vs. modular data: Causal chains support reasoning but may obscure alternative hypotheses; ranked lists with evidence provide flexibility
  - Historical evidence vs. simulation: Historical cases provide real-world grounding; simulation enables testing edge cases and future scenarios

- **Failure signatures:**
  - Users asking "why did the system choose this?" after viewing explanations → causal chain incomplete or ranking mechanism opaque
  - Users ignoring explanations entirely → content not matched to explanation intent
  - Users unable to predict future behavior → missing efficacy principles and simulation-based projections
  - Domain experts rejecting AI recommendations → application/system context missing from explanations

- **First 3 experiments:**
  1. **Content-domain mapping test:** Present explanations with/without application and system domain content to data scientists; measure decision confidence and time-to-decision for model selection tasks.
  2. **Causal chain ablation study:** Remove sequential narrative structure and present equivalent content as unordered data points; test whether users can still identify root causes and appropriate responsive actions.
  3. **Explanation intent matching:** For each intent (reason, comparison, accuracy, prediction, trust), measure whether the top-selected content elements from the study actually improve performance on intent-specific tasks versus generic explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the identified explanation content significantly improve data scientists' task performance and efficiency in behavioral usability tests?
- Basis in paper: [explicit] Section 6.2 explicitly states that a behavioral study is needed to "evaluate whether the identified explanation content is indeed effective, efficient, and satisfactory."
- Why unresolved: The current findings are based on interview feedback (self-report) rather than observed user behavior during actual system usage.
- What evidence would resolve it: Results from a controlled usability study measuring task completion times and error rates using a prototype based on the mental models.

### Open Question 2
- Question: How can shared mental models for explanations be derived to bridge communication gaps between data scientists and other cross-functional user roles?
- Basis in paper: [explicit] Section 6.2 suggests "Research on shared mental models for explainability could derive connected explanations that simplify communication between different user roles."
- Why unresolved: The study isolated the data scientist's perspective and did not investigate how their models overlap with or differ from other stakeholders.
- What evidence would resolve it: A comparative study mapping mental models of different roles (e.g., domain experts vs. data scientists) to identify distinct versus shared explanation requirements.

### Open Question 3
- Question: What are the most effective methods for presenting confidence levels within the proposed hierarchical and sequential explanation structures?
- Basis in paper: [explicit] Section 6.2 identifies the "presentation of a confidence level" as a specific topic for future work.
- Why unresolved: While the mental models define content types (e.g., quantitative information), they do not specify how system confidence or uncertainty should be communicated to the user.
- What evidence would resolve it: Design experiments evaluating user understanding and trust calibration when using different confidence visualization techniques within the framework.

## Limitations

- Homogeneous participant pool (all 12 participants have 3+ years industrial ML experience and strong technical backgrounds)
- Same participants used for both qualitative and quantitative phases, introducing potential bias
- Focus exclusively on two specific user tasks (model generation and maintenance) without empirical validation for generalization to all five user tasks

## Confidence

- **High confidence**: Mental model refinement process and identification of multi-domain content needs
- **Medium confidence**: Standardized explanation questions approach and specific content elements (E5, E7, E8, E10, E11)
- **Low confidence**: Claims about generalizability across all five user tasks and assertion that this is the first work to address all five explanation intents for data scientists

## Next Checks

1. **Cross-experience validation**: Test whether the identified explanation content and structure serve the needs of data scientists with varying levels of experience (0-2 years vs 3+ years) and different educational backgrounds.

2. **Real-time task validation**: Conduct think-aloud protocols during actual model generation and maintenance tasks to validate whether the retrospective mental models capture real-time explanation needs.

3. **Intent coverage validation**: For each of the five explanation intents (reason, comparison, accuracy, prediction, trust), measure whether the top-ranked content elements from this study actually improve task performance compared to baseline explanations in controlled experiments.