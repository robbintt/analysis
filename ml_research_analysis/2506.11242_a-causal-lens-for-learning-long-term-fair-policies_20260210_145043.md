---
ver: rpa2
title: A Causal Lens for Learning Long-term Fair Policies
arxiv_id: '2506.11242'
source_url: https://arxiv.org/abs/2506.11242
tags:
- fairness
- qualification
- policy
- gain
- long-term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies long-term fairness in sequential decision-making
  using a causal lens. It formulates fairness as the equality of expected qualification
  gains across groups, then decomposes this disparity into three components: direct
  policy effect (DPE), indirect policy effect (IPE), and spurious policy effect (SPE).'
---

# A Causal Lens for Learning Long-term Fair Policies

## Quick Facts
- arXiv ID: 2506.11242
- Source URL: https://arxiv.org/abs/2506.11242
- Reference count: 14
- This paper studies long-term fairness in sequential decision-making using a causal lens. It formulates fairness as the equality of expected qualification gains across groups, then decomposes this disparity into three components: direct policy effect (DPE), indirect policy effect (IPE), and spurious policy effect (SPE). The paper reveals a connection between DPE and benefit fairness, showing they may align under certain conditions. An optimization approach incorporating both qualification gain parity and benefit fairness is proposed and evaluated on a loan approval simulation environment. The method achieves better long-term fairness compared to baselines while maintaining competitive utility, with PPO-C and PPO-Cb variants showing reduced qualification disparities and more balanced lending rates.

## Executive Summary
This paper addresses long-term fairness in sequential decision-making by developing a causal framework for analyzing and optimizing qualification gain disparities across groups. The authors introduce a novel decomposition of qualification gain disparity into three causal components (direct, indirect, and spurious policy effects) and demonstrate a theoretical connection between the direct policy effect and benefit fairness. They propose a reinforcement learning approach that incorporates both qualification gain parity and benefit fairness constraints, showing empirically that this method reduces long-term fairness disparities while maintaining competitive utility in a loan approval simulation.

## Method Summary
The method formulates qualification gain parity as the primary long-term fairness metric and decomposes its disparity into direct policy effect (immediate impact), indirect policy effect (delayed impact via environment transitions), and spurious policy effect (baseline differences). The optimization objective combines standard PPO utility with KL penalty, quadratic qualification gain disparity penalty, and benefit fairness regularization inspired by Gini coefficient. The policy network outputs treatment probabilities conditioned on both state and sensitive attribute. The approach is evaluated on a loan approval simulation environment with three settings varying initial credit score distributions and repayment probabilities.

## Key Results
- PPO-C and PPO-Cb variants outperform baselines on qualification gain disparity across three simulation settings
- The method achieves better long-term fairness while maintaining competitive utility compared to standard PPO
- DPE decreases as benefit fairness enforcement increases, supporting the theoretical connection between these metrics
- PPO-Cb variant shows reduced qualification disparities and more balanced lending rates than PPO-C

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Qualification gain disparity can be causally decomposed into three distinct sources of inequality.
- Mechanism: Using hypothetical policies (baseline π₀ and virtual π^PS), the expected qualification gain V^π_do(s)(x) is decomposed into: (1) Direct Policy Effect (DPE)—immediate impact of policy decisions on qualification gain; (2) Indirect Policy Effect (IPE)—delayed impact via environment transitions; (3) Spurious Policy Effect (SPE)—disparity from baseline environment differences unrelated to policy.
- Core assumption: The qualification gain function g_s(x, x') is additive over time intervals, and causal interventions on policy and sensitive attributes are well-defined within the SCM framework.
- Evidence anchors:
  - [Section 3.4]: "V_do(π,s)(x) = [V_do(π,s)(x) - V_do(π^PS,s)(x)] + [V_do(π^PS,s)(x) - V_do(π_0,s)(x)] + V_do(π_0,s)(x)" with labels for direct impact, delayed impact, and spurious effect.
  - [Abstract]: "we decompose this metric into three components that represent the direct impact, the delayed impact, as well as the spurious effect"
  - [Corpus]: Related work on causal fairness frameworks exists (e.g., "A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination"), but this specific tripartite decomposition for RL is novel.
- Break condition: If the qualification gain function lacks additivity, or if the transition dynamics cannot be modeled as causal mechanisms separable from policy effects, the decomposition loses interpretability.

### Mechanism 2
- Claim: The Direct Policy Effect (DPE) is closely connected to benefit fairness, and they may align under specific conditions.
- Mechanism: Proposition 1 shows that V_do(π,s)(x) - V_do(π^PS,s)(x) = Σ η_π,s(x→x') π(d₁|x,s) Δ(x,s), where Δ is the benefit (CATE). DPE reformulates as the expected benefit-weighted treatment rate difference between groups. When state visitation distributions are similar across groups and benefits are group-independent, benefit fairness reduces DPE.
- Core assumption: Benefit Δ(x,s) can be meaningfully defined as the expected qualification gain difference between treatment and non-treatment; state visitation distributions reflect access to the decision process.
- Evidence anchors:
  - [Section 3.5]: "if the benefit Δ is independent of the sensitive feature S, then achieving benefit fairness aligns with decreasing DPE"
  - [Figure 6]: Shows DPE decreases as benefit fairness enforcement (β_Λ) increases, supporting the theoretical connection.
  - [Corpus]: Benefit fairness concept from Plecko & Bareinboim (2023) is cited; corpus lacks direct replications of this specific alignment result.
- Break condition: If benefit distributions differ substantially across groups, enforcing benefit fairness may introduce or maintain DPE disparities rather than reduce them.

### Mechanism 3
- Claim: Constraining qualification gain parity during policy optimization reduces long-term fairness disparities while maintaining competitive utility.
- Mechanism: The objective J(θ) = L_UTIL - β_KL L_KL - β_C Ĉ_π² - β_Λ Λ integrates: (1) standard PPO utility term with KL penalty, (2) quadratic penalty on qualification gain disparity Ĉ_π, and (3) benefit fairness regularizer Λ inspired by Gini coefficient. Gradients of Ĉ_π are computed via importance sampling over group-separated timesteps.
- Core assumption: The policy gradient theorem applies to the qualification-based value function; empirical estimates of Ĉ_π and Λ have manageable variance during minibatch SGD.
- Evidence anchors:
  - [Section 3.3]: Full objective function in Eq. (2) and Eq. (5) with penalty coefficients.
  - [Section 5.2, Figure 4]: PPO-C and PPO-Cb outperform baselines on qualification gain disparity across three settings.
  - [Corpus]: "Fairness Aware Reinforcement Learning via Proximal Policy Optimization" describes related PPO-fairness integration but with different constraint formulations.
- Break condition: If variance in Ĉ_π estimation is too high (due to stochastic environments or sparse state coverage), gradient noise may destabilize training or fail to reduce disparity meaningfully.

## Foundational Learning

- Concept: Structural Causal Models (SCM) and path-specific effects
  - Why needed here: The decomposition relies on soft/hard interventions and path-specific causal effects to separate policy influence from environment dynamics.
  - Quick check question: Can you explain how a soft intervention (do(π)) differs from a hard intervention on sensitive attribute S, and why both are used in this framework?

- Concept: Markov Decision Processes and Bellman equations
  - Why needed here: Qualification gain is formulated as a value function satisfying Bellman recursion; policy gradients require understanding state/action value functions and visitation distributions.
  - Quick check question: Given V(x) = Σ_d π(d|x) Q(x,d), derive how adding a constraint C_π(θ) modifies the policy gradient.

- Concept: Fairness notions (benefit fairness, demographic parity, equal opportunity)
  - Why needed here: The paper positions long-term fairness (qualification gain parity) against short-term notions; understanding benefit fairness (Definition 2) is essential for the DPE connection.
  - Quick check question: How does benefit fairness differ from demographic parity in its treatment of heterogeneous treatment effects?

## Architecture Onboarding

- Component map:
  - Causal graph module: Represents state x_t → decision d_t → next state x_{t+1} transitions with sensitive attribute S influencing initial distribution, transitions, and policy
  - Policy network: Neural network parameterizing π_θ(d|x,s); outputs treatment probability
  - Value function estimators: V_do(π,s)(x) for qualification gain; separate estimators for utility (standard reward) and fairness (qualification-based) terms
  - Hypothetical policy evaluator: Computes V_do(π_0,s) and V_do(π^PS,s) for decomposition (used in analysis, not training)
  - Constraint calculator: Estimates Ĉ_π via importance sampling; computes Λ for benefit fairness
  - Optimizer: PPO-style minibatch SGD with composite objective

- Critical path:
  1. Define qualification gain function g_s(x,x') for your domain (must be additive)
  2. Implement policy network with sensitive attribute conditioning
  3. Compute benefit estimates Δ(x,s) from transition data (requires counterfactual estimation)
  4. Integrate Ĉ_π and Λ terms into PPO objective; tune β_C and β_Λ

- Design tradeoffs:
  - **Utility vs. fairness**: Higher β_C reduces disparity but may lower cumulative reward (Figure 3 shows PPO-Cb utility lower than PPO-C in some settings)
  - **Short-term vs. long-term fairness**: Enforcing benefit fairness alone may not reduce IPE; the paper shows IPE is less sensitive to benefit fairness enforcement
  - **Constraint strength vs. variance**: Stronger constraints (large β_C, β_Λ) reduce disparity but increase gradient variance and training instability

- Failure signatures:
  - **High variance in Ĉ_π**: Manifests as unstable or oscillating disparity metrics; mitigated by increasing batch size or β_KL
  - **Conflicting objectives**: Utility drops sharply without fairness improvement; indicates β_C or β_Λ set too high relative to reward scale
  - **DPE/IPE misalignment**: DPE decreases but IPE remains high; suggests environment dynamics dominate delayed effects—may require environment-level intervention

- First 3 experiments:
  1. **Reproduce loan approval simulation (Setting 1)** with PPO-C; verify that Ĉ_π decreases over training while utility remains competitive with baseline PPO
  2. **Ablation on β_Λ**: Train PPO-Cb with β_Λ ∈ {0, 0.5, 2}; plot DPE vs. benefit fairness metric to confirm the theoretical relationship (replicate Figure 6 pattern)
  3. **Decomposition analysis**: At convergence, compute DPE, IPE, SPE for trained policy; verify that DPE varies more during training than IPE (as reported in Figure 5)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the qualification gain function be designed to ensure that benefit \(\Delta\) remains independent of the sensitive attribute S, while accounting for transition probabilities that depend on S?
- Basis in paper: [explicit] The authors state in their Remark: "This insight motivates us to introduce an additional consideration into the design qualification gain function when designing practical decision-making systems, which is to ensure that the benefit \(\Delta\) remains independent of S, even though the transition probability may be dependent on S."
- Why unresolved: The paper proposes this as a design consideration but does not provide a systematic methodology for constructing such functions or theoretical guarantees for when independence can be achieved.
- What evidence would resolve it: A formal characterization of conditions under which qualification gain functions yield S-independent \(\Delta\), along with constructive algorithms for designing such functions given arbitrary transition dynamics.

### Open Question 2
- Question: Under what precise conditions do benefit fairness and qualification gain parity objectives align versus conflict?
- Basis in paper: [inferred] The paper shows that when state visitation distributions differ between groups, "achieving benefit fairness becomes incompatible with eliminating the DPE," yet also demonstrates empirically that PPO-Cb reduces DPE. The boundary conditions remain unclear.
- Why unresolved: The theoretical analysis identifies cases of both alignment and conflict, but lacks a complete characterization of the parameter space where each occurs.
- What evidence would resolve it: Theoretical bounds specifying the relationship between state visitation distribution divergence, benefit distribution properties, and the resulting DPE under benefit fairness constraints.

### Open Question 3
- Question: How does the causal decomposition framework generalize beyond binary sensitive attributes to multi-valued or continuous protected attributes?
- Basis in paper: [inferred] The paper explicitly states it assumes "the sensitive feature S is assumed to be a binary variable in this paper for ease of representation."
- Why unresolved: Real-world fairness applications often involve multiple intersecting protected characteristics (e.g., race, gender, age) or continuous attributes, and the decomposition's theoretical properties may not directly extend.
- What evidence would resolve it: Extensions of the DPE, IPE, and SPE decomposition definitions to multi-group settings, with proofs showing whether the connection to benefit fairness and the optimization approach remain tractable.

## Limitations

- The method's performance on real-world sequential decision-making problems remains untested as the loan approval environment is highly stylized
- Unknown hyperparameters and network architecture create significant barriers to direct replication
- High variance in qualification gain estimates may limit practical applicability in noisy real-world domains
- The paper does not address potential feedback loops between qualification gain and policy decisions beyond the immediate transition

## Confidence

- Causal decomposition mechanism: **High** (theoretically sound, well-defined in SCM framework)
- DPE-benefit fairness alignment: **Medium** (supported by proposition and Figure 6, but dependent on benefit distribution assumptions)
- Constraint optimization effectiveness: **Medium** (empirical results show disparity reduction, but architecture and hyperparameters unspecified)

## Next Checks

1. Implement a small-scale loan simulation with specified transition dynamics and qualification gain function; verify that PPO-C reduces C_π(θ) while maintaining utility comparable to PPO baseline
2. Conduct ablation study on β_Λ; plot DPE against benefit fairness metric Λ across multiple runs to confirm the theoretical relationship shown in Figure 6
3. Compute DPE, IPE, and SPE for a converged policy; verify that DPE shows larger variation during training than IPE, as reported in the results