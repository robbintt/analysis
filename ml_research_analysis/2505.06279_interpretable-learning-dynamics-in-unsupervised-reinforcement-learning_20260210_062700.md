---
ver: rpa2
title: Interpretable Learning Dynamics in Unsupervised Reinforcement Learning
arxiv_id: '2505.06279'
source_url: https://arxiv.org/abs/2505.06279
tags:
- agents
- attention
- learning
- agent
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces an interpretability framework to analyze\
  \ how unsupervised reinforcement learning agents perceive and represent their environments.\
  \ The study compares five agents\u2014DQN, ICM, RND, PPO, and Transformer-RND\u2014\
  using gradient-based attention visualization (Grad-CAM, LRP), exploration metrics,\
  \ and VAE-based latent space analysis."
---

# Interpretable Learning Dynamics in Unsupervised Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.06279
- Source URL: https://arxiv.org/abs/2505.06279
- Authors: Shashwat Pandey
- Reference count: 19
- Primary result: Introduces interpretability framework comparing 5 RL agents using attention visualization, exploration metrics, and latent space analysis

## Executive Summary
This work introduces an interpretability framework to analyze how unsupervised reinforcement learning agents perceive and represent their environments. The study compares five agents—DQN, ICM, RND, PPO, and Transformer-RND—using gradient-based attention visualization (Grad-CAM, LRP), exploration metrics, and VAE-based latent space analysis. Novel metrics including attention diversity and attention change rate quantify spatial breadth and temporal dynamics of visual focus. Results show that curiosity-driven agents (ICM, RND) exhibit broader, more structured attention and richer exploration compared to value-based DQN. Transformer-RND achieves the best balance of wide attention, high exploration coverage, and compact, clustered latent representations. DQN displays diffuse attention and poorly structured embeddings, correlating with lower performance. VAE reconstructions confirm that curiosity-driven agents produce higher-fidelity internal representations. The proposed framework offers diagnostic tools for understanding agent perception, highlighting the role of architectural and training signal choices in shaping interpretable behavior.

## Method Summary
The framework analyzes five RL agents (DQN, ICM, RND, PPO, Transformer-RND) trained on CoinRun from Procgen benchmark using 64×64 RGB frames across 10 procedurally generated levels. Agents are evaluated using gradient-based attention visualization (Grad-CAM, LRP), exploration metrics (attention diversity, attention change rate, coverage, entropy), and VAE-based latent space analysis with clustering metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz scores). Training runs 1M steps with γ=0.99, batch sizes 64-128, and agent-specific configurations including 4-layer Transformer with 8 heads for Transformer-RND. VAE uses 32-dimensional latent space with 3 convolutional encoder layers.

## Key Results
- Curiosity-driven agents (ICM, RND) show broader, more structured attention and higher exploration coverage than value-based DQN
- Transformer-RND achieves optimal balance: wide attention, high exploration, and compact clustered latent representations
- DQN exhibits diffuse attention and poorly structured embeddings correlating with lower performance
- VAE reconstructions confirm curiosity-driven agents produce higher-fidelity internal representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intrinsic motivation signals (curiosity) drive broader spatial attention and higher exploration coverage compared to pure value-based learning.
- **Mechanism:** Curiosity-driven agents generate intrinsic rewards based on prediction error (novelty), forcing the visual encoder to attend to diverse features rather than overfitting to sparse extrinsic reward triggers.
- **Core assumption:** Visual attention maps accurately reflect decision-making features rather than gradient artifacts.
- **Evidence anchors:** ICM and RND show higher entropy and coverage metrics than DQN; intrinsic goals drive adaptive policies.

### Mechanism 2
- **Claim:** Transformer inductive biases facilitate more structured latent representations than CNN-only architectures.
- **Mechanism:** Transformer-RND uses self-attention over spatial tokens for global context mixing, encouraging distinct clusterable state embeddings rather than entangled local features.
- **Core assumption:** Clustering quality and VAE latent space compactness proxy semantic quality of agent's internal model.
- **Evidence anchors:** Transformer-RND achieves compact, clustered latent representations; self-attention supports more effective abstraction.

### Mechanism 3
- **Claim:** On-policy learning loops correlate with higher "attention change rates" (perceptual plasticity).
- **Mechanism:** On-policy methods constantly discard old data, forcing network adaptation to current state distribution, manifesting as high temporal variance in attention maps.
- **Core assumption:** Attention map changes indicate useful adaptation rather than unstable oscillation.
- **Evidence anchors:** On-policy agents exhibit greater perceptual adaptability due to tight feedback loops.

## Foundational Learning

- **Concept: Intrinsic Motivation (RND/ICM)**
  - **Why needed here:** Essential to interpret why attention maps differ between curiosity-driven and reward-focused agents.
  - **Quick check question:** If an agent visits the same state repeatedly, does the intrinsic reward for that state increase or decrease in RND? (Decreases due to prediction error minimization)

- **Concept: Gradient-based Attribution (Grad-CAM/LRP)**
  - **Why needed here:** Framework relies on these tools to visualize "attention" as proxy for what network is looking at.
  - **Quick check question:** Does Grad-CAM provide pixel-perfect localization or coarse, class-discriminative localization? (Coarse, class-discriminative)

- **Concept: Representation Learning (VAE & Latent Clustering)**
  - **Why needed here:** VAEs audit agent's mind; good reconstructions indicate essential feature capture.
  - **Quick check question:** What does a high Silhouette score in UMAP projection indicate about agent's understanding of different game levels? (Well-separated, cohesive clusters indicating good abstraction)

## Architecture Onboarding

- **Component map:** Input (64×64 RGB Frame) → Encoder (CNN or CNN+Transformer) → Head (Actor+Critic) → Intrinsic Reward Module (ICM/RND) or Value Learning (DQN/PPO) → Auditor (External VAE + Grad-CAM/LRP)

- **Critical path:** The interaction between Intrinsic Reward Module and Encoder is critical. The intrinsic reward signal shapes gradient updates of the Encoder. If this path is broken (e.g., intrinsic reward is zero), curiosity-agents' attention maps would revert to DQN-like behavior.

- **Design tradeoffs:** CNN vs Transformer encoders (local vs global feature extraction), on-policy vs off-policy learning (data freshness vs sample efficiency), intrinsic vs extrinsic motivation (exploration breadth vs goal-directed behavior).

- **Failure signatures:** DQN flat reward curves in sparse reward environments; GPU OOM with Transformer-RND and 8 parallel envs; ICM intrinsic reward collapse mid-training.

- **First experiments:** 1) Train DQN baseline and verify it achieves minimal non-zero rewards in CoinRun; 2) Monitor ICM intrinsic reward curves for sudden drops indicating collapse; 3) Compare attention diversity metrics between curiosity-driven and value-based agents.

## Open Questions the Paper Calls Out
None

## Limitations
- Gradient-based attribution methods provide coarse localization rather than pixel-perfect attention maps
- Clustering quality may not directly translate to semantic understanding of agent behavior
- Study focuses on single procedurally generated environment (CoinRun), limiting generalizability
- VAE reconstructions serve as proxy validation but don't directly measure functional understanding

## Confidence

**High Confidence**: Claims about attention diversity and exploration coverage differences between curiosity-driven and value-based agents (directly computed metrics showing clear patterns).

**Medium Confidence**: Claims about Transformer-RND producing more structured latent representations (clustering metrics support but causal link needs validation).

**Medium Confidence**: Claims about on-policy learning correlating with higher attention change rates (temporal dynamics observable but interpretation needs investigation).

## Next Checks

1. **Ablation study**: Remove intrinsic rewards from ICM/RND agents to test whether attention patterns revert to DQN-like behavior, confirming causal role of curiosity signals.

2. **Cross-environment testing**: Evaluate same agents on different Procgen environments to assess whether observed attention and exploration patterns generalize beyond CoinRun.

3. **Behavioral correlation analysis**: Directly link attention patterns to in-game behaviors (e.g., jumping, running) rather than relying solely on exploration metrics to validate that attention maps reflect functional understanding.