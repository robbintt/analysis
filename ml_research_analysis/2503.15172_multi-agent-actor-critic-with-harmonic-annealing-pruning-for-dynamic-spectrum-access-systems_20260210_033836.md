---
ver: rpa2
title: Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum
  Access Systems
arxiv_id: '2503.15172'
source_url: https://arxiv.org/abs/2503.15172
tags:
- pruning
- networks
- access
- each
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel multi-agent actor-critic framework
  with gradual neural network pruning for distributed dynamic spectrum access (DSA).
  The method integrates magnitude-based unstructured pruning into the independent
  actor global critic paradigm and introduces a harmonic annealing sparsity scheduler
  that enables periodic weight regrowth.
---

# Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems

## Quick Facts
- **arXiv ID**: 2503.15172
- **Source URL**: https://arxiv.org/abs/2503.15172
- **Reference count**: 29
- **Primary result**: Proposed harmonic annealing pruning achieves 17% reward improvement over dense IAGC-PPO at 95% sparsity in DSA systems

## Executive Summary
This paper introduces a novel multi-agent actor-critic framework that integrates gradual neural network pruning into distributed dynamic spectrum access (DSA). The approach uses magnitude-based unstructured pruning applied only to actor networks while maintaining dense critics during training. A key innovation is the harmonic annealing sparsity scheduler, which combines cosine annealing with periodic sinusoidal oscillation to enable weight regrowth. Experiments with 10 secondary users and 5 orthogonal channels demonstrate that this framework outperforms conventional DSA methods, MADRL baselines, and state-of-the-art pruning techniques, achieving superior performance at high sparsities (≥90%).

## Method Summary
The framework implements independent actor global critic (IAGC) PPO with LSTM-based actors and critics for DSA systems. Actors maintain independent policies using local observations while sharing a centralized critic during training. The method applies magnitude-based unstructured pruning exclusively to actor networks every 5 iterations using a harmonic annealing scheduler that combines cosine annealing with sinusoidal oscillation (period 200 iterations). This enables periodic weight regrowth while targeting 95% sparsity. The approach preserves critic quality during training while achieving inference-time compression, making it suitable for resource-constrained edge devices.

## Key Results
- Harmonic annealing scheduler achieves best-case rewards of 1826.86 (Setup A) and 1223.41 (Setup B) at 95% sparsity
- Outperforms dense IAGC PPO baseline (1556.83 and 1115.74 respectively) by 17% and 10%
- Maintains strong performance while significantly reducing model size compared to conventional DSA and MADRL baselines
- Superior performance at high sparsities (≥90%) compared to linear and polynomial pruning schedulers

## Why This Works (Mechanism)

### Mechanism 1: Harmonic Annealing Enables Weight Regrowth for Policy Exploration
The harmonic annealing scheduler combines cosine annealing with periodic sinusoidal oscillation, creating oscillating sparsity levels that allow previously pruned weights to be reactivated. This dynamic prune-and-regrow mechanism acts as implicit regularization while expanding the policy search space. The oscillation prevents premature loss of useful network pathways that monotonic pruning permanently destroys.

### Mechanism 2: Actor-Only Pruning Preserves Critic Value Estimation Quality
Pruning only actor networks while maintaining dense critics stabilizes training and preserves performance. Value function estimation is more sensitive to parameter reduction than policy learning. By restricting pruning to deployable actor networks, the framework maintains robust centralized value estimation during training while achieving inference-time compression.

### Mechanism 3: Recurrent IAGC Captures Temporal Dependencies with Agent Heterogeneity
LSTM-based actors process action-observation sequences to infer peer access patterns and channel quality over time. The shared critic aggregates global state information for coordinated value estimation during centralized training. This enables heterogeneous agent policies while leveraging centralized value estimation for partially observable DSA.

## Foundational Learning

- **Concept: Centralized Training with Decentralized Execution (CTDE)**
  - Why needed: IAGC framework relies on CTDE—agents share a critic during training but execute policies independently using only local observations
  - Quick check: Can you explain why the critic sees all agent observations during training but actors only see their own during deployment?

- **Concept: Proximal Policy Optimization (PPO) with Clipping**
  - Why needed: Framework uses clipped PPO for stable policy updates in multi-agent setting; understanding clip objective is essential for debugging training instability
  - Quick check: What does the clipping parameter ε=0.2 prevent during policy updates?

- **Concept: Magnitude-Based Unstructured Pruning**
  - Why needed: Pruning step removes weights with smallest absolute values; this differs from structured pruning and requires sparse matrix handling during inference
  - Quick check: Why might magnitude-based pruning fail if applied before network weights have converged to meaningful values?

## Architecture Onboarding

- **Component map:** N LSTM actor networks → softmax action distributions; 1 global LSTM critic network → scalar value estimate; pruning scheduler module → computes sparsity per iteration; DSA environment simulator → generates collisions, SNR observations, rewards

- **Critical path:** 1) Trajectory sampling (Episode → actions → collisions → observations → rewards); 2) PPO update (bootstrapped sequential updates through trajectories); 3) Pruning step (every 5 iterations, apply magnitude-based pruning to actors only)

- **Design tradeoffs:** Pruning interval (5 iterations) balances compression speed vs. weight recovery; sparsity target (95%) maximizes compression but risks performance; delayed pruning start allows initial convergence

- **Failure signatures:** Reward collapse during early training indicates pruning starts before policy stabilizes; high variance across seeds suggests scheduler sensitivity; performance matching random policy indicates incorrect initialization at high sparsities

- **First 3 experiments:** 1) Baseline validation: Run dense IAGC-PPO without pruning; verify ~1556 (Setup A) / ~1115 (Setup B) reward convergence; 2) Scheduler ablation: Compare linear, polynomial, and harmonic schedulers at 95% target sparsity; 3) Primary user robustness: Add PU occupancy with πk=0.2; verify harmonic scheduler maintains advantage in Setup B conditions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed framework scale to DSA systems with a significantly larger number of secondary users?
- **Basis**: Authors state in Section IV.C that "Studies with larger N values will be presented in a future journal version of this work."
- **Why unresolved**: Current experiments are restricted to N=10 secondary users and K=5 channels
- **Evidence needed**: Performance benchmarks on testbeds with N≫10

### Open Question 2
- **Question**: Can the harmonic annealing pruning method generalize effectively to other multi-agent applications beyond channel access?
- **Basis**: Section V lists future work to include "extensions of our approach to other applications, such as games and transmit power control"
- **Why unresolved**: Method has only been validated on the specific DSA channel selection task
- **Evidence needed**: Comparative results in transmit power control scenarios or multi-agent game environments

### Open Question 3
- **Question**: What is the impact of meta-learning initialization on the performance and stability of the sparse training process?
- **Basis**: Authors note they "plan to investigate meta-learning techniques to bias training towards promising initialization conditions"
- **Why unresolved**: Current approach starts from scratch, leading to high variance across seeds
- **Evidence needed**: Ablation studies showing reduced variance or improved best-case rewards with meta-learned initial weights

### Open Question 4
- **Question**: How robust is the harmonic annealing scheduler to variations in pruning hyperparameters?
- **Basis**: Section IV.A mentions that "Further investigations on the pruning hyperparameters as well as sensitivity studies will be provided in the journal version"
- **Why unresolved**: Fixed hyperparameters were used without exploring parameter space
- **Evidence needed**: Sensitivity analysis charts showing reward changes when varying pruning interval or scheduler constants

## Limitations
- Sparse training evidence gap: Claims rest on post-training evaluation rather than demonstrating performance during actual sparse training iterations
- MADRL pruning novelty: Primary innovation is harmonic scheduler, but ablation studies only compare against linear and polynomial schedules
- Seed sensitivity concern: Best-case performance shows 17% improvement but high variance across schedulers with only 10 seeds

## Confidence
- **High confidence**: Framework architecture (IAGC-PPO with LSTM actors/critics) is well-specified and reproducible; DSA environment setup and evaluation methodology are clearly defined
- **Medium confidence**: Harmonic annealing scheduler design and implementation are explicit, but superiority claims depend on unexplored hyperparameter sensitivity
- **Low confidence**: Claims about practical deployment benefits rest on post-hoc compression ratios rather than demonstrating end-to-end sparse training performance

## Next Checks
1. **Statistical validation**: Re-run experiments with 30+ seeds to establish confidence intervals for reward differences between harmonic and baseline schedulers
2. **Training stability analysis**: Plot per-iteration rewards during sparse training to verify harmonic annealing maintains performance throughout pruning progression
3. **Generalization test**: Evaluate framework on DSA scenarios with different numbers of secondary users (N=5, N=15) and channels (K=3, K=7) to assess scalability