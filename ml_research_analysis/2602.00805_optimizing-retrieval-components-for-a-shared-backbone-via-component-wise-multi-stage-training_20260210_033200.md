---
ver: rpa2
title: Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage
  Training
arxiv_id: '2602.00805'
source_url: https://arxiv.org/abs/2602.00805
tags:
- retrieval
- legal
- training
- stage
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of optimizing shared retrieval
  backbones in production legal AI systems, where a single model serves multiple applications.
  The proposed solution employs a multi-stage training framework with progressively
  harder supervision, applied to both embedding-based retrievers and rerankers.
---

# Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training

## Quick Facts
- arXiv ID: 2602.00805
- Source URL: https://arxiv.org/abs/2602.00805
- Reference count: 24
- Primary result: Multi-stage training with component-wise selection (Stage 3 embeddings + Stage 2 rerankers) improves end-to-end retrieval metrics and achieves 54.6% preference rate in production with only 6.7% latency overhead.

## Executive Summary
This work addresses the challenge of optimizing shared retrieval backbones in production legal AI systems, where a single model serves multiple applications. The proposed solution employs a multi-stage training framework with progressively harder supervision, applied to both embedding-based retrievers and rerankers. Component-wise evaluation reveals that embeddings benefit from later-stage training for improved recall, while rerankers exhibit stage-dependent trade-offs in ranking quality. A mixed-stage configuration combining Stage 3 embeddings with Stage 2 rerankers achieves end-to-end performance improvements: 0.935 MRR and 0.680 nDCG on CSAID, and 0.720 MRR and 0.691 nDCG on STARD, compared to 0.855 MRR and 0.615 nDCG on CSAID, and 0.673 MRR and 0.652 nDCG on STARD for the baseline. The improved backbone is deployed in production, showing a 54.6% preference rate in offline A/B tests with only a 6.7% relative increase in average query latency.

## Method Summary
The paper proposes a multi-stage optimization framework for dense retrievers and rerankers, drawing inspiration from curriculum learning by progressively introducing training signals of increasing difficulty. The framework consists of three stages: Stage 1 establishes broad semantic alignment via large-scale weak supervision; Stage 2 refines discrimination through auto-mined hard samples; Stage 3 calibrates robustness using dynamically refreshed challenging negatives. Both embedding and reranking models undergo this three-stage training, but the key innovation is component-wise evaluation and selection—different stages yield optimal performance for different components, requiring a mixed-stage configuration rather than a single checkpoint.

## Key Results
- Stage 3 embeddings achieve the strongest recall across all K values, enabling smaller retrieval budgets without coverage loss
- Reranker performance exhibits non-monotonic improvements across training stages, with Stage 2 yielding optimal ranking quality
- Mixed-stage configuration (Stage 3 embedding + Stage 2 reranker) achieves 0.935 MRR and 0.680 nDCG on CSAID, improving from baseline 0.855 MRR and 0.615 nDCG
- Production deployment shows 54.6% user preference with only 6.7% relative latency increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive curriculum-style training improves retrieval quality by introducing supervision of increasing difficulty across stages.
- Mechanism: Stage 1 establishes broad semantic alignment via large-scale weak supervision; Stage 2 refines discrimination through auto-mined hard samples; Stage 3 calibrates robustness using dynamically refreshed challenging negatives. Each stage targets a different aspect of retrieval capability.
- Core assumption: The paper assumes curriculum-style progression transfers benefits to dense retrieval, drawing analogy from curriculum learning (Bengio et al., 2009), though this is not formally proven in the paper.
- Evidence anchors:
  - [abstract] "multi-stage optimization framework for dense retrievers and rerankers... progressively refining embedding and reranking models using increasingly difficult supervision"
  - [section 2] "drawing inspiration from curriculum learning by progressively introducing training signals of increasing difficulty"
  - [corpus] Related work on domain-adaptive retrieval training (Luo et al., 2025, cited as [7]) supports staged refinement approaches.
- Break condition: If Stage 1 fails to establish adequate semantic coverage, later stages cannot effectively refine discrimination—the foundation must be sufficient before hard-sample training helps.

### Mechanism 2
- Claim: Embedding and reranking components exhibit stage-dependent trade-offs, requiring component-wise model selection rather than uniform checkpoint selection.
- Mechanism: Embeddings primarily affect candidate coverage (Recall@K) and benefit monotonically from later stages. Rerankers are sensitive to fine-grained relevance and may degrade on certain metrics at later stages due to robustness calibration trade-offs.
- Core assumption: Assumes the functional roles of components (coverage vs. ranking) are separable and can be optimized independently.
- Evidence anchors:
  - [abstract] "different retrieval components exhibit stage-dependent trade-offs... component-wise, mixed-stage configuration rather than relying on a single uniformly optimal checkpoint"
  - [section 3.2] "Stage 3 yields the strongest recall across all reported K values" for embeddings; "reranking performance does not improve monotonically across training stages"
  - [corpus] "More Than Efficiency" paper discusses domain adaptation trade-offs in dense retrieval, though stage-dependent component selection specifically is not well-covered in neighbors.
- Break condition: If downstream applications require tightly coupled embedding-reranker behavior (e.g., specific ranking patterns), independent component selection may produce mismatched relevance distributions.

### Mechanism 3
- Claim: Later-stage embeddings achieve target recall with smaller retrieval budgets, enabling latency reduction without coverage loss.
- Mechanism: Improved embedding discrimination reduces the candidate set size needed to achieve the same recall threshold. Fewer candidates means reduced reranking computation and end-to-end latency.
- Core assumption: Assumes retrieval budget is the primary latency driver and that downstream rerankers maintain quality on smaller candidate pools.
- Evidence anchors:
  - [section 3.2] "later-stage embedding models can reach comparable recall targets with substantially smaller retrieval budgets"
  - [section 3.5] "6.7% latency overhead" despite stacking multiple LoRA adapters—overhead is modest relative to quality gains
  - [corpus] DRAMA paper notes inference cost challenges in dense retrievers but does not directly address recall-budget trade-offs.
- Break condition: If reranking quality degrades sharply on smaller candidate sets (distribution shift), latency gains may not translate to end-to-end improvements.

## Foundational Learning

- Concept: **Dense Retrieval Architecture**
  - Why needed here: The paper optimizes a two-stage retrieval pipeline (embedding → reranking); understanding this decomposition is prerequisite to component-wise optimization.
  - Quick check question: Can you explain why embeddings optimize for recall while rerankers optimize for ranking quality?

- Concept: **Curriculum Learning**
  - Why needed here: The multi-stage training framework explicitly draws from curriculum learning principles—training signals progress from easy/broad to hard/refined.
  - Quick check question: What would happen if you trained on hard negatives before establishing broad semantic alignment?

- Concept: **Recall-Budget Trade-offs**
  - Why needed here: The paper's system-level contribution hinges on understanding how improved embeddings enable smaller retrieval budgets while maintaining coverage.
  - Quick check question: Given a recall target of 0.85, how would you determine the minimum retrieval budget for a Stage 3 vs. Stage 1 embedding model?

## Architecture Onboarding

- Component map:
  - Embedding Model -> Reranker Model -> Shared Backbone -> Multiple Applications
  - Qwen3-Embedding-4B base → Stages 1-3 → Outputs dense vectors
  - Qwen3-Reranker-4B base → Stages 1-3 → Scores candidate pairs

- Critical path:
  1. Establish baseline with pretrained embedding/reranker on target domain
  2. Execute Stage 1 training with domain-specific weak supervision
  3. Mine hard samples from model failures for Stage 2
  4. Generate dynamic challenging negatives for Stage 3
  5. Evaluate each component at each stage on held-out benchmarks
  6. Select Stage 3 embedding + Stage 2 reranker (or empirically optimal combination)

- Design tradeoffs:
  - **Recall vs. Latency**: Larger retrieval budgets improve coverage but increase reranking cost
  - **Coverage vs. Ranking**: Embedding improvements may not translate to ranking gains; reranker improvements may not improve recall
  - **Generalization vs. Specialization**: Later stages improve in-domain performance but may trade off generalization
  - **Deployment Complexity vs. Performance**: Mixed-stage configuration requires maintaining multiple checkpoints

- Failure signatures:
  - **Stage 2 overfitting**: Hard-sample mining causes degradation on easy queries (reranker MRR drops on STARD while improving on CSAID)
  - **Embedding-reranker mismatch**: Selecting Stage 3 for both components may yield suboptimal end-to-end performance
  - **Robustness degradation**: Stage 3 calibration may slightly reduce fine-grained relevance accuracy
  - **LoRA adapter stacking overhead**: Latency increases if too many adapters are active (paper reports 6.7% overhead)

- First 3 experiments:
  1. **Baseline domain assessment**: Evaluate pretrained embedding/reranker on your domain's held-out queries; compute Recall@K (embedding) and MRR/nDCG (reranker) to establish gaps.
  2. **Stage-wise ablation**: Train embedding and reranker models through all three stages; plot recall-budget curves and ranking metrics at each stage to identify component-specific optimal checkpoints.
  3. **End-to-end configuration search**: Combine embedding stages (1, 2, 3) with reranker stages (1, 2, 3) in a 3×3 grid; evaluate end-to-end retrieval on representative queries to validate mixed-stage hypothesis for your domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What systematic framework can guide optimal stage selection for each retrieval component, replacing empirical trial-and-error?
- Basis in paper: [explicit] The authors state: "later stages may improve certain system metrics while degrading others, revealing explicit trade-offs" and that "practical deployment benefits from selecting and combining models based on system-level trade-offs rather than relying on a single globally optimal checkpoint." The Stage 3 embedding + Stage 2 reranker configuration was determined empirically without principled selection criteria.
- Why unresolved: The paper demonstrates stage-dependent behaviors but provides no predictive methodology for identifying optimal stage-component pairings before exhaustive evaluation.
- What evidence would resolve it: A framework based on data characteristics, training dynamics, or validation metrics that predicts optimal stage assignments, validated across multiple domains and component types.

### Open Question 2
- Question: Do the multi-stage training findings generalize to non-legal domains or non-Chinese languages?
- Basis in paper: [inferred] All experiments use two Chinese legal benchmarks (CSAID, STARD). The authors note legal retrieval involves "domain-specific terminology, subtle semantic distinctions between legally related concepts, and informal expressions"—characteristics that may make findings domain-specific.
- Why unresolved: The optimal configuration (Stage 3 embeddings + Stage 2 rerankers) may depend on legal document structure or Chinese language properties not present in other domains.
- What evidence would resolve it: Evaluation on benchmarks from other domains (biomedical, technical documentation) and languages, showing whether the same stage configuration remains optimal.

### Open Question 3
- Question: Would larger-scale online evaluation confirm the 54.6% preference improvement across diverse query types and applications?
- Basis in paper: [explicit] The authors note: "This evaluation serves as a deployment sanity check rather than a hypothesis-testing benchmark." Only 389 queries from two datasets were tested.
- Why unresolved: Small sample size may not capture variability across query difficulty, user intent, or application contexts; preference signal could differ for edge cases.
- What evidence would resolve it: A powered online A/B test with thousands of queries across all supported applications, with stratified analysis by query characteristics.

### Open Question 4
- Question: Can jointly optimizing embeddings and rerankers across stages yield better end-to-end performance than independent component-wise training?
- Basis in paper: [inferred] The paper applies multi-stage training to both components identically but selects them independently; no joint optimization or interaction analysis during training is explored despite their complementary roles (coverage vs. fine-grained ranking).
- Why unresolved: Joint training could enable embeddings to produce representations better aligned with reranker discrimination needs.
- What evidence would resolve it: Experiments comparing independent vs. joint multi-stage training on end-to-end metrics (MRR, nDCG).

## Limitations
- Domain-specific findings may not generalize beyond Chinese legal benchmarks (CSAID, STARD)
- Multi-stage training pipeline, particularly hard negative mining strategies, is not fully specified for reproduction
- Small-scale online evaluation (389 queries) limits confidence in production performance claims
- Stage-dependent trade-offs require empirical trial-and-error rather than principled selection framework

## Confidence
- **High**: The core claim that embeddings and rerankers exhibit stage-dependent trade-offs requiring component-wise optimization (supported by clear ablation results showing non-monotonic metric improvements).
- **Medium**: The claim that Stage 3 embeddings enable smaller retrieval budgets without coverage loss (supported by recall-budget curves but dependent on reranker quality on smaller candidate sets).
- **Low**: The claim that the mixed-stage configuration is optimal for all industrial retrieval applications (validated only on two Chinese legal benchmarks).

## Next Checks
1. **Cross-Domain Generalization**: Apply the multi-stage training framework to a non-legal retrieval task (e.g., biomedical or e-commerce) and evaluate whether the Stage 3 embedding + Stage 2 reranker configuration remains optimal or requires task-specific adjustment.
2. **Negative Mining Ablation**: Systematically vary the hardness and diversity of negatives in Stage 2 and Stage 3 training to quantify their impact on final retrieval quality and identify potential overfitting thresholds.
3. **Latency-Performance Trade-off**: Measure end-to-end latency and retrieval quality across different retrieval budget sizes for each stage checkpoint to validate the recall-budget trade-off hypothesis and identify practical deployment boundaries.