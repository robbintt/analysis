---
ver: rpa2
title: 'The Feature Understandability Scale for Human-Centred Explainable AI: Assessing
  Tabular Feature Importance'
arxiv_id: '2510.07050'
source_url: https://arxiv.org/abs/2510.07050
tags:
- scale
- feature
- were
- items
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first psychometrically validated scale
  for measuring user understanding of tabular features in supervised classification
  problems. The Feature Understandability Scale was developed using expert interviews,
  focus groups, and pilot studies, then validated through exploratory and confirmatory
  factor analysis.
---

# The Feature Understandability Scale for Human-Centred Explainable AI: Assessing Tabular Feature Importance

## Quick Facts
- **arXiv ID:** 2510.07050
- **Source URL:** https://arxiv.org/abs/2510.07050
- **Reference count:** 40
- **Primary result:** First psychometrically validated scale for measuring user understanding of tabular features in supervised classification

## Executive Summary
This paper introduces the Feature Understandability Scale, a psychometrically validated instrument for measuring user comprehension of tabular features in supervised classification problems. The scale was developed through expert interviews, focus groups, and pilot studies, then validated using exploratory and confirmatory factor analysis on 1,440 participants rating 9 features across 3 domains. The resulting scales comprise 8 items for numerical features and 9 items for categorical features, measuring two distinct latent constructs: understanding/measurement and feature-outcome relations. The tool enables quantification of feature understandability, potentially allowing system designers to prioritize features based on user comprehension in interpretability-by-design approaches.

## Method Summary
The study collected 2,160 ratings from 1,440 participants (US, UK, Ireland, Canada, Australia, New Zealand) via Prolific, with each participant rating 3 of 9 features (3 numerical, 3 categorical, across medical, loan, and hiring domains). Data was split into equal halves for exploratory factor analysis (EFA) and confirmatory factor analysis (CFA). EFA used Maximum Likelihood extraction with Promax rotation in R to identify factor structure, while CFA employed Satorra-Bentler robust estimation to handle skewed data distributions. The scale was refined by removing items with loadings < 0.32 or cross-loadings > 0.32, resulting in two distinct 2-factor scales.

## Key Results
- The final scales showed excellent internal consistency with McDonald's ω > 0.85
- Good model fit achieved (CFI > 0.97, RMSEA < 0.08, SRMR ≤ 0.03)
- Two-factor structure confirmed as superior to one-factor model (χ²_Δ(1)=292.65, p ≤ 0.0001)
- Numerical scale: 8 items (5 for Understanding & Measurement, 3 for Feature-Outcome Relation)
- Categorical scale: 9 items (6 for Understanding & Measurement, 3 for Feature-Outcome Relation)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If user comprehension of tabular features is treated as a two-dimensional construct (Measurement vs. Outcome Relation), the assessment of "understandability" achieves higher structural validity than a unidimensional approach.
- **Mechanism:** The scale separates the cognitive processing of *what* a feature is (Understanding & Measurement) from the causal reasoning of *how* it affects the result (Feature-Outcome Relation). By measuring these distinct latent variables, the scale captures the variance in user comprehension that a single "understanding" score would miss.
- **Core assumption:** Users conceptually distinguish between the definition of a variable and its functional role in a decision process.
- **Evidence anchors:**
  - [abstract] "The scale comprises two two-factor scales... measuring understanding/measurement and feature-outcome relations."
  - [section 4.5] Confirmatory Factor Analysis (CFA) showed the two-factor model was significantly better than the one-factor model (χ²_Δ(1)=292.65, p ≤ 0.0001).
  - [corpus] ContextualSHAP and related papers focus on explanation generation; this mechanism focuses on the prerequisite user state, a distinction emphasized in the paper's "interpretability-by-design" argument.
- **Break condition:** If users conflated the definition of a feature with its impact (e.g., understanding "credit score" implies understanding exactly how it changes a loan decision), the two-factor structure would collapse into one.

### Mechanism 2
- **Claim:** Quantifying subjective feature understanding via a psychometric scale allows for the algorithmic ranking of features, potentially enabling "interpretability-by-design" in ML pipelines.
- **Mechanism:** The scale converts the subjective, qualitative experience of "understanding" into a quantitative score. This normalization allows system designers to filter or weight features based on user comprehension scores before model training or explanation generation, theoretically aligning model inputs with human mental models.
- **Core assumption:** A feature's psychometric "understandability" score predicts the efficacy of an explanation containing that feature.
- **Evidence anchors:**
  - [abstract] "This tool enables quantification of feature understandability, facilitating interpretability-by-design approaches... allowing system designers to prioritize features based on user comprehension."
  - [section 1] "It is in the best interest of the system designer to try to pre-select understandable features for producing a global explanation."
  - [corpus] Weak direct evidence; corpus neighbors focus on post-hoc explanation methods (SHAP, LIME) rather than pre-training feature selection based on psychometrics.
- **Break condition:** If high understandability scores do not correlate with improved user performance or trust when interacting with the actual ML system, the ranking mechanism loses functional utility.

### Mechanism 3
- **Claim:** If scale validation accounts for variance in both participants *and* features (rather than participants alone), the resulting instrument is robust against domain-specific idiosyncrasies.
- **Mechanism:** Unlike traditional psychometrics where only subjects vary, this validation introduced variability across 9 different features (medical, loan, hiring). By ensuring the scale holds true across these varying stimuli during Exploratory Factor Analysis (EFA), the tool avoids overfitting to a specific type of data concept.
- **Core assumption:** The latent structure of "understanding" is stable across different semantic domains (e.g., understanding "BMI" uses the same cognitive factors as understanding "Recruitment Strategy").
- **Evidence anchors:**
  - [section 3.3] "This study differs from traditional scale validation... it is important to not only consider the ratio of items to participants, but also that of items to features."
  - [section 3.3] Selected 9 features across three domains and collected 2160 ratings to satisfy item-feature-participant ratios.
  - [corpus] No direct comparison found in corpus; standard XAI evaluation typically focuses on model-proxy metrics rather than psychometric item-response theory.
- **Break condition:** If the factor structure shifts significantly when moving to a highly technical domain (e.g., genomics) not represented in the validation set, the scale would require re-validation.

## Foundational Learning

- **Concept: Latent Variables & Factor Analysis**
  - **Why needed here:** The paper constructs a "scale" to measure something that cannot be observed directly (understanding). You must understand how Factor Analysis groups survey questions (items) into underlying dimensions (factors) to interpret the results.
  - **Quick check question:** If all survey items correlated perfectly with each other, how many "factors" would the scale likely have?

- **Concept: Construct Validity**
  - **Why needed here:** The scale claims to measure "Understandability." You need to understand how the authors validate that their specific 8-9 questions actually capture that specific concept rather than "familiarity" or "trust."
  - **Quick check question:** The paper contrasts "Understandability" with "Explainability." Why is validating the distinction between these two concepts critical for the paper's contribution?

- **Concept: Ordinal vs. Numerical Data Treatment**
  - **Why needed here:** The paper produces two distinct scales—one for numerical features (e.g., income) and one for categorical features (e.g., loan type). Understanding how data types affect user cognition is key to applying the right scale.
  - **Quick check question:** Which factor is identical in both the Numerical and Categorical scales, and why might the "Measurement" factor differ between them?

## Architecture Onboarding

- **Component map:** Feature -> Scale Selection (Numerical/Categorical) -> 8 or 9 Likert items -> Understanding & Measurement + Feature-Outcome Relation scores -> Aggregate Understandability Score

- **Critical path:**
  1. Identify target user demographic (e.g., loan applicants)
  2. Select features used in the ML model
  3. Deploy relevant scale (Numerical/Categorical) to a representative sample for each feature
  4. Analyze scores to rank features
  5. (Proposed) Feed ranking into ML pipeline to prioritize high-score features for the final model or explanation layer

- **Design tradeoffs:**
  - *Breadth vs. Depth:* The scale was validated on general population samples (Prolific). Assumption: It may require re-calibration for specialized expert audiences (e.g., doctors)
  - *Accuracy vs. Comprehension:* As noted in Section 5, selecting features based purely on understandability scores may trade off against model accuracy if the most predictive features are complex

- **Failure signatures:**
  - **Cross-loading:** If a survey item scores >0.32 on both factors, it indicates ambiguous phrasing (checked during EFA)
  - **Low Fit Indices:** If CFI < 0.90 or RMSEA > 0.10 in new domains, the scale structure may be broken
  - **Skewed Distributions:** The paper notes robust estimation was needed for skewed data; future use must handle non-normal response distributions

- **First 3 experiments:**
  1. **Baseline Validation:** Run the scale on your specific dataset's features with a small pilot group (N=30) to ensure the questions make sense in your specific domain context
  2. **Prediction Check:** Correlate the scale's "Feature-Outcome Relation" score for a feature with users' actual ability to predict outcomes given that feature's value
  3. **A/B Testing:** Generate explanations using Top-N understandable features vs. Top-N important features (standard XAI) and measure user trust/decision latency

## Open Questions the Paper Calls Out

- **Open Question 1:** Does integrating the Feature Understandability Scale into the machine learning pipeline improve the user's comprehension of model explanations?
  - **Basis in paper:** [explicit] The authors state that future research should "integrate it into the ML pipeline... and evaluating the quality of the explanations through a user study."
  - **Why unresolved:** The current work validated the scale's psychometric structure but did not test its efficacy as an intervention tool for interpretability-by-design in a live system.
  - **What evidence would resolve it:** A controlled user study measuring explanation quality and user comprehension for models trained on high-scoring features versus standard features.

- **Open Question 2:** Does prioritizing high-understandability features during model training result in a loss of predictive accuracy?
  - **Basis in paper:** [explicit] The paper notes that favouring understandable features during training "may present a trade-off with accuracy, as the more understandable features are not necessarily the most informative ones."
  - **Why unresolved:** It is currently unknown if highly understandable features are sufficient proxies for complex features or if their systematic selection degrades model performance.
  - **What evidence would resolve it:** A comparative analysis of predictive performance (e.g., AUC, F1 score) between models constrained to high-understandability features and unconstrained baseline models.

- **Open Question 3:** Is the two-factor scale structure invariant across demographic variables such as education level or gender?
  - **Basis in paper:** [explicit] The authors suggest future work should "stratify the sample by gender, ethnicity or level of education to test whether the current scale structure is an artefact of sample fluctuations."
  - **Why unresolved:** The validation relied on a general population sample, leaving open the possibility that the "Understanding" and "Feature-Outcome" factors function differently across specific subgroups.
  - **What evidence would resolve it:** Multi-group Confirmatory Factor Analysis (CFA) results demonstrating measurement invariance across key demographic strata.

## Limitations
- The scale was validated on general population samples from Prolific, which may not generalize to specialized domains without re-validation
- The validation focused on 9 specific features across 3 domains, leaving uncertainty about performance with highly technical or domain-specific features
- The mechanism linking understandability scores to improved explanation efficacy remains largely theoretical, with limited empirical validation of the "interpretability-by-design" application

## Confidence
- **High confidence:** The psychometric validity of the scale structure (two-factor model confirmed through CFA with excellent fit indices)
- **Medium confidence:** The cross-domain stability of the factor structure, as the validation covered diverse feature types but limited domains
- **Low confidence:** The practical utility of the scale for feature selection in ML pipelines, as this application remains largely theoretical without empirical testing

## Next Checks
1. **Domain Transfer Test:** Validate the scale on a new domain (e.g., medical records or technical IoT sensor data) to assess whether the two-factor structure holds without modification
2. **Explanation Efficacy Study:** Conduct an A/B test where explanations are generated using features ranked by understandability scores versus traditional importance metrics, measuring user trust and decision accuracy
3. **Expert Audience Calibration:** Test whether the scale requires adjustment for expert audiences by comparing understandability scores between general population and domain experts for the same features