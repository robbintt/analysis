---
ver: rpa2
title: 'Large language models for artificial general intelligence (AGI): A survey
  of foundational principles and approaches'
arxiv_id: '2501.03151'
source_url: https://arxiv.org/abs/2501.03151
tags:
- arxiv
- language
- knowledge
- preprint
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey comprehensively explores foundational principles for\
  \ achieving artificial general intelligence (AGI) with large language models (LLMs).\
  \ The authors identify four critical concepts\u2014embodiment, symbol grounding,\
  \ causality, and memory\u2014as essential building blocks for robust, generalist\
  \ cognitive capabilities."
---

# Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches

## Quick Facts
- arXiv ID: 2501.03151
- Source URL: https://arxiv.org/abs/2501.03151
- Authors: Alhassan Mumuni; Fuseini Mumuni
- Reference count: 40
- Primary result: Identifies embodiment, symbol grounding, causality, and memory as essential building blocks for AGI-capable LLMs

## Executive Summary
This survey comprehensively explores foundational principles for achieving artificial general intelligence (AGI) with large language models (LLMs). The authors identify four critical concepts—embodiment, symbol grounding, causality, and memory—as essential building blocks for robust, generalist cognitive capabilities. Through detailed analysis of current research, they demonstrate how integrating these principles can overcome key limitations of existing LLMs, such as superficial understanding and brittle reasoning. The paper presents a unified conceptual framework showing how these interrelated concepts work together to create more sophisticated AI systems capable of genuine context understanding, flexible knowledge application, and robust decision-making.

## Method Summary
The paper conducts a comprehensive survey of existing research on integrating cognitive principles with LLMs to achieve AGI. It analyzes current approaches to embodiment (physical/virtual bodies with sensors and actuators), symbol grounding (connecting abstract tokens to real-world entities), causality (modeling cause-and-effect relationships), and memory (sensory, working, and long-term storage). The methodology involves synthesizing findings from various domains including robotics, knowledge representation, causal inference, and cognitive science. The authors propose a conceptual framework integrating these four principles, though the paper does not provide empirical validation of the complete system.

## Key Results
- Current LLMs are limited to Level 1 of Pearl's Causal Hierarchy (Association/Seeing) and cannot perform intervention or counterfactual reasoning
- Integrating Knowledge Graphs or RAG systems can reduce hallucinations by grounding abstract tokens to verified real-world entities
- Embodied agents with sensors and actuators can develop intrinsic goal-awareness rather than just following instructions
- A hierarchical memory system (sensory → working → long-term) is essential for overcoming catastrophic forgetting and enabling flexible knowledge application

## Why This Works (Mechanism)

### Mechanism 1: Embodiment Enables Situated Goal-Awareness
If an LLM is provided with a physical or virtual "body" (sensors and actuators), it may develop intrinsic goal-awareness and situatedness, which are prerequisites for autonomy. The paper posits that agency arises when an intelligent system can influence the world and receive feedback. By moving from static text processing to active interaction (embodiment), the system transitions from mission-awareness (following specific instructions) to goal-awareness (pursuing intrinsic high-level objectives). Intelligence requires a feedback loop where the system infers the state of the environment and its own capabilities (self-awareness) to guide actions, rather than relying solely on pre-trained statistical correlations.

### Mechanism 2: Neuro-Symbolic Grounding Reduces Hallucination
Connecting abstract LLM tokens to structured external knowledge (Knowledge Graphs or Ontologies) creates a "semantic bridge" that mitigates hallucinations and superficial understanding. Pure LLMs operate on statistical vector embeddings which can drift from reality. By integrating Knowledge Graphs (KGs) or using Retrieval-Augmented Generation (RAG), the system maps internal symbols to explicit, verified real-world entities and relationships. This forces the generative process to align with grounded facts. Meaningful representation requires referents; symbols (words) must be tied to physical or conceptual entities to support robust reasoning.

### Mechanism 3: Causal World Models Enable Counterfactual Reasoning
To achieve robust generalization, LLMs must model the world using structural causal relationships rather than just surface-level correlations. The paper surveys integrating Structural Causal Models (SCMs) or physics engines. This allows the system to answer "what if" (intervention) and "why" (counterfactual) questions. By explicitly modeling cause-and-effect (e.g., force equals mass times acceleration), the agent can predict outcomes in novel situations not seen in training data. Biological intelligence relies on "intuitive physics" and "intuitive psychology" to navigate the world; machines must replicate this structural understanding to be generalist.

## Foundational Learning

- **Concept: Pearl's Causal Hierarchy (The "Ladder of Causality")**
  - Why needed here: The paper explicitly frames the gap between current LLMs and AGI as the inability to move from Level 1 (Association/Seeing) to Level 2 (Intervention/Doing) and Level 3 (Counterfactual/Imagining). You cannot understand the architecture without understanding these levels.
  - Quick check question: Can you distinguish between a question asking "What is the probability of X given Y?" (Level 1) and "What would happen to X if I change Y?" (Level 2)?

- **Concept: Memory Taxonomy (Sensory, Working, Long-term)**
  - Why needed here: The architecture relies on a specific separation of memory. Sensory memory buffers input; Working memory (context window) handles immediate reasoning; Long-term memory (Vector DBs/KGs) stores accumulated knowledge. Confusing these leads to brittle designs.
  - Quick check question: Where does the paper suggest storing "episodic experiences" for long-term reuse versus "current task steps"?

- **Concept: Neuro-Symbolic AI**
  - Why needed here: The proposed solution is rarely "pure" deep learning. It involves blending neural networks (LLMs) with symbolic structures (Logic, KGs). Understanding this hybrid approach is essential for implementing the grounding mechanism.
  - Quick check question: How does a neural network (the LLM) interface with a symbolic Knowledge Graph to perform reasoning?

## Architecture Onboarding

- **Component map:**
  1. Embodied Interface: Sensors (Visual/Audio) + Actuators (Robot/Virtual Avatar) for environment interaction
  2. Cognitive Core: Multimodal LLM (Transformer) for processing semantic information
  3. Grounding Module: Knowledge Graphs or Vector Databases (RAG) to map tokens to reality
  4. Causal Engine: Physics simulator or Structural Causal Model (SCM) for predicting intervention outcomes
  5. Memory System: Hierarchical storage (Context Window for working memory; Vector DB for episodic/semantic long-term memory)

- **Critical path:**
  1. Perception: Environment data enters via Sensors to Sensory Memory
  2. Grounding: Data is contextualized by the Grounding Module (retrieving facts from Long-term Memory)
  3. Reasoning: The Cognitive Core uses Causal Models to simulate outcomes and update Working Memory
  4. Action: A plan is executed via Actuators

- **Design tradeoffs:**
  - *Symbolic vs. Implicit Grounding:* Knowledge Graphs are explicit and transparent but labor-intensive to curate. Vector embeddings are scalable and cheap but opaque and prone to hallucinations.
  - *Static vs. Interactive Training:* Training on static datasets (Ego4D) is safe but limits adaptation. Training in simulated worlds (Sim-to-Real) allows active exploration but suffers from the "reality gap."

- **Failure signatures:**
  - *Stochastic Parroting:* The system generates text that looks coherent but lacks semantic grounding (Hallucination)
  - *Catastrophic Forgetting:* The model overwrites previous knowledge when learning new tasks (Memory failure)
  - *Correlation Trap:* The system assumes correlation implies causation (e.g., "Roosters cause the sun to rise"), failing at counterfactual reasoning

- **First 3 experiments:**
  1. Baseline Grounding: Evaluate the raw LLM against a Knowledge Graph-augmented version on a hallucination benchmark to quantify the reduction in factual errors
  2. Sim-to-Real Transfer: Train a navigation agent in a 3D simulator (like Habitat or AI2-THOR) and measure the performance drop when deployed in a physical environment (Reality Gap analysis)
  3. Causal Intervention Test: Prompt the LLM with a physical scenario (e.g., "What happens if I remove this tablecloth quickly?"). Test if it predicts the outcome based on physics (causality) vs. statistical likelihood of text sequences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What fundamentally new paradigms are required to unify embodiment, symbol grounding, causality, and memory within a single LLM framework?
- Basis in paper: Section 8 argues that while individual principles are promising, "continued progress toward AGI will require fundamentally new paradigms for designing LLMs that implement all these principles in a unified fashion."
- Why unresolved: Current approaches often treat these cognitive principles in isolation or integrate them superficially, failing to model them as a unified set of interrelated and complementary primitives.
- What evidence would resolve it: The creation of an LLM-based cognitive architecture that natively processes sensorimotor inputs, grounds semantics, reasons causally, and retains long-term memories simultaneously without compartmentalization.

### Open Question 2
- Question: How can AGI be effectively evaluated given the fundamental differences between human and machine intelligence design?
- Basis in paper: Section 8 states that "evaluating and ascertaining when this is achieved is a challenging problem" because "fundamental differences in how human and machine intelligence are designed and function" render direct comparisons misleading.
- Why unresolved: Biological intelligence is evolved for survival and is inherently fuzzy, whereas machine intelligence is optimized for specific tasks, making standard benchmarks insufficient for measuring general adaptability.
- What evidence would resolve it: The establishment of evaluation metrics that assess autonomy, adaptability, and generalization across open-ended environments rather than performance on static, pre-defined tasks.

### Open Question 3
- Question: How can LLMs be modified to distinguish between superficial statistical correlations and genuine causal relationships?
- Basis in paper: Section 5.2.1 notes that LLMs trained purely on data "are not inherently aware of physical laws" and "learn hidden patterns... constrained by the fact that not all observed connections have cause-and-effect relationships."
- Why unresolved: Standard deep learning methods rely on observational data which captures statistical dependencies but fails to encode the underlying mechanisms necessary for causal intervention or counterfactual reasoning.
- What evidence would resolve it: Models demonstrating the ability to accurately predict the outcomes of novel interventions and answer counterfactual queries without relying on spurious correlations found in training data.

## Limitations

- The paper is a survey that proposes a conceptual framework without empirical validation of the integrated approach
- Many claims are drawn from separate studies rather than validated as a unified system
- The specific architectural integration lacks detailed implementation specifications
- The "reality gap" between simulated and real-world performance is acknowledged but not quantified for the proposed framework

## Confidence

- **High Confidence**: The identification of the four foundational principles (embodiment, symbol grounding, causality, memory) as critical gaps in current LLMs
- **Medium Confidence**: The proposed conceptual framework showing how these principles interconnect
- **Low Confidence**: Claims about achieving human-level intelligence through this framework

## Next Checks

1. **Integrated System Evaluation**: Build and test a unified agent that combines all four principles (embodiment + grounding + causality + memory) in a controlled environment, measuring performance against state-of-the-art single-purpose systems on generalization benchmarks.

2. **Component Ablation Study**: Systematically remove each principle from the integrated system to quantify their individual contributions to robust reasoning and action, determining which components are truly essential versus beneficial.

3. **Real-World Transfer Assessment**: Deploy an embodied agent trained in simulation to a physical environment, measuring the "reality gap" and identifying which architectural components most effectively bridge this gap.