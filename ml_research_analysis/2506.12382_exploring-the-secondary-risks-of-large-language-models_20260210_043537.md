---
ver: rpa2
title: Exploring the Secondary Risks of Large Language Models
arxiv_id: '2506.12382'
source_url: https://arxiv.org/abs/2506.12382
tags:
- risks
- arxiv
- secondary
- response
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces secondary risks as a novel class of non-adversarial\
  \ LLM failures that emerge during benign interactions, often evading standard safety\
  \ mechanisms. To enable systematic evaluation, the authors define two risk primitives\u2014\
  excessive response and speculative advice\u2014based on information-theoretic length\
  \ bounds and logical precondition inference."
---

# Exploring the Secondary Risks of Large Language Models

## Quick Facts
- arXiv ID: 2506.12382
- Source URL: https://arxiv.org/abs/2506.12382
- Reference count: 40
- Key outcome: Introduces secondary risks as a novel class of non-adversarial LLM failures that evade standard safety mechanisms, with SecLens achieving up to 75.82% attack success rate.

## Executive Summary
This work introduces secondary risks as a novel class of non-adversarial LLM failures that emerge during benign interactions, often evading standard safety mechanisms. To enable systematic evaluation, the authors define two risk primitives—excessive response and speculative advice—based on information-theoretic length bounds and logical precondition inference. They propose SecLens, a black-box, multi-objective search framework that elicits these risks by optimizing task relevance, risk activation, and linguistic plausibility. Extensive experiments on 16 models show that secondary risks are prevalent, transferable across model families, and modality-independent, with SecLens achieving up to 75.82% attack success rate—significantly outperforming baselines. These results underscore the urgent need for enhanced safety mechanisms to address subtle yet harmful LLM behaviors in real-world deployments.

## Method Summary
SecLens is a black-box, multi-objective evolutionary search framework that discovers prompts triggering secondary risks (excessive response and speculative advice) in LLMs. It initializes with few-shot seed prompts, then iteratively applies semantics-guided crossover and mutation operators to evolve populations toward prompts that balance task relevance, risk activation, and stealth. The fitness function weights these objectives (1, 0.2, 0.1) and uses GPT-4o as a black-box evaluator. The search terminates when risk thresholds are met or generation budgets are exhausted. Evaluation is performed on the SecRiskBench benchmark across 16 models, with transferability and modality-extension tests on MLLMs.

## Key Results
- SecLens achieves up to 75.82% attack success rate on secondary risks, significantly outperforming baselines
- Secondary risks are transferable across model families with 35-45% success rates
- Modality extension shows secondary risks persist in MLLMs with only moderate ASR degradation
- Evolutionary search with few-shot guidance converges 2-3× faster than random initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excessive responses arise from post-training patterns where models append unsolicited content to otherwise correct trajectories.
- Mechanism: RLHF training data contains Long-Response Bias, causing models to imitate lengthy completions. During PPO optimization, models learn that longer outputs correlate with higher cumulative rewards, incentivizing content generation beyond task requirements. This creates conditions for harmful or misleading appendices to benign completions.
- Core assumption: The relationship between training data verbosity patterns and excessive response behavior is causal rather than correlational.
- Evidence anchors:
  - [section] "During the RLHF phase, part of the training data contains a Long-Response Bias... reward functions used in RL sometimes incentivize LLMs to generate longer replies... models tend to produce excessive content during PPO to 'accumulate rewards'" (Section 5).
  - [section] "Excessive response occurs when H(Y|X) increases unnecessarily, adding information that is irrelevant or harmful" (Appendix I).
  - [corpus] Weak direct corpus evidence; related work on verbosity bias (Saito et al. 2023) confirms length preferences but does not establish the safety-risk pathway.
- Break condition: If length-constrained generation (e.g., max_tokens=128) eliminates excessive-response risks, the mechanism would primarily reflect verbosity preference rather than semantic drift.

### Mechanism 2
- Claim: Speculative advice emerges when mutual information between input and output shifts away from user intent toward model-inferred but unstated goals.
- Mechanism: Current alignment mechanisms operate shallowly, failing to constrain model inference in ambiguous or open-ended contexts. When prompts lack explicit constraints, models infer latent user needs and generate advice misaligned with stated requests. Information-theoretically, this manifests as I(X;Y) deviating from I(X;G_expected).
- Core assumption: Speculative advice reflects a fundamental limitation of current alignment methods rather than insufficient training data coverage.
- Evidence anchors:
  - [section] "Speculative advice occurs when I(Y;X) deviates, causing the output to drift away from the user's intent" (Appendix I).
  - [section] "Many existing alignment mechanisms are shallow and not sufficiently powerful... value alignment of existing LLMs is not always strong enough to prevent speculative or risky suggestions" (Section 5).
  - [corpus] SafeThinker (arxiv 2601.16506) provides supporting evidence that shallow safety alignment renders models vulnerable to disguised attacks, though focused on adversarial rather than benign settings.
- Break condition: If enhanced multi-objective RLHF or explicit intent-clarification mechanisms eliminate speculative advice without degrading utility, the mechanism would be remediable through improved training rather than architectural change.

### Mechanism 3
- Claim: SecLens discovers secondary-risk prompts efficiently by balancing three competing objectives in a black-box evolutionary search.
- Mechanism: Population-based search initializes with few-shot risk-prone examples, then iteratively applies semantics-guided crossover (aligning semantic roles via dependency parsing) and mutation (masked-language-model sampling conditioned on high-fitness prompts). The multi-objective fitness function F(x) = w_risk·R(·) + w_task·TASKSCORE - w_stealth·DETECTSCORE drives search toward prompts that are risky, task-compliant, and stealthy simultaneously.
- Core assumption: The three objectives are sufficiently decoupled that Pareto-optimal solutions exist where all constraints are satisficed; semantic variation operators preserve linguistic plausibility while exploring novel risk pathways.
- Evidence anchors:
  - [section] "SecLens formulates prompt discovery as a multi-objective optimization problem, balancing task relevance, risk behavior activation, and linguistic plausibility" (Section 3.3).
  - [section] "Few-shot contextual guidance yields 2-3× faster convergence versus random initialization" (Section 4.2, ablation study).
  - [corpus] Weak corpus connections; related jailbreak search methods (GCG, AutoDAN) optimize for single objectives and do not generalize to secondary-risk discovery.
- Break condition: If the Pareto front is empty for certain model-task combinations (i.e., no prompt satisfies all three constraints), the search would converge to degenerate solutions or fail to find valid triggers.

## Foundational Learning

- Concept: **RLHF and Preference Alignment**
  - Why needed here: Understanding why secondary risks evade standard safety mechanisms requires grasping how RLHF shapes model behavior and where it fails.
  - Quick check question: Can you explain why reward modeling might incentivize longer outputs without improving safety alignment?

- Concept: **Information-Theoretic Entropy and Mutual Information**
  - Why needed here: The paper formalizes secondary risks using H(Y|X) (conditional entropy for excessive response) and I(X;Y) (mutual information for speculative advice).
  - Quick check question: Given an LLM input-output pair, how would you determine whether output entropy exceeds task requirements?

- Concept: **Multi-Objective Evolutionary Optimization (Pareto Optimality)**
  - Why needed here: SecLens operates as a multi-objective evolutionary algorithm; interpreting results requires understanding Pareto fronts and objective tradeoffs.
  - Quick check question: If risk and stealth objectives conflict, what does a Pareto-optimal solution represent?

## Architecture Onboarding

- Component map: Seed initialization -> Population evaluation (black-box model query) -> Fitness computation -> Elite selection -> Semantic variation -> Loop until convergence
- Critical path: Seed initialization → Population evaluation (black-box model query) → Fitness computation → Elite selection → Semantic variation → Loop until convergence
- Design tradeoffs:
  - **Weight configuration**: Higher w_risk discovers more risks but may produce degenerate prompts; w_task=0.2 assumes task completion is easy for benign prompts (validated by ablation in Appendix A).
  - **Crossover vs. mutation**: Both needed; ablation shows 5-6% ASR drop when either is removed (Table 14).
  - **Few-shot guidance**: 3-shot yields 2-3× faster convergence than 0-shot, but requires curated seed data.
- Failure signatures:
  - **Semantic drift**: Optimized prompts diverge from original task intent (monitor via CLIP/SBERT similarity; threshold ~0.88-0.92).
  - **Degenerate solutions**: Prompts score high on risk but fail task compliance or detectability (check TASKSCORE and DETECTSCORE separately).
  - **Stagnation**: Population converges before reaching risk threshold (increase mutation rate or population diversity).
- First 3 experiments:
  1. **Baseline comparison on single model**: Run SecLens vs. Random/Tuning/MCTS on GPT-4o for both risk types; verify ASR improvements and log query efficiency (target: <40 queries/prompt).
  2. **Transferability test**: Optimize prompts on Claude-3.7, evaluate transfer success rates on 5 unseen models (GPT-4-turbo, Gemini 2.0-pro, Deepseek-v3, Llama-3.3-70b, Qwen2.5-32b); expect 35-45% transfer ASR.
  3. **Modality extension**: Convert text prompts to image-text pairs (CLIP similarity >85% + manual verification), run on MLLMs (Llama-OV-72b, Pixtral-12b); compare ASR degradation vs. text-only.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific role do post-training biases (such as "Long-Response Bias" in RLHF) play in causally inducing excessive response risks?
- Basis in paper: [explicit] The Discussion section hypothesizes that "suboptimal patterns" in RLHF reward functions may incentivize models to generate lengthy or verbose content, potentially "paving the way" for these risks.
- Why unresolved: The paper observes the phenomenon and correlates it with known RLHF issues (verbosity bias), but does not conduct ablation studies on training data or reward models to confirm the causal link.
- What evidence would resolve it: Experiments where LLMs are trained with length-controlled or debiased reward models, followed by evaluation with SecLens to see if excessive response rates decrease.

### Open Question 2
- Question: What specific safety alignment strategies can effectively mitigate secondary risks without refusing benign requests?
- Basis in paper: [explicit] The Abstract and Conclusion state there is an "urgent need for enhanced safety mechanisms" and "strengthened safety mechanisms," noting that current alignment is "imperfect."
- Why unresolved: The paper focuses on *exposing* vulnerabilities using SecLens and SecRiskBench rather than proposing or validating technical defenses.
- What evidence would resolve it: A proposed defense method (e.g., a specialized detector or fine-tuning regimen) that significantly lowers the Attack Success Rate (ASR) on SecRiskBench while maintaining standard utility metrics.

### Open Question 3
- Question: Are "excessive response" and "speculative advice" truly exhaustive primitives, or do other forms of non-adversarial semantic drift exist?
- Basis in paper: [inferred] The authors claim to capture "core failure patterns" via two primitives derived from "illustrative examples" and empirical audits, but do not offer a theoretical proof that this taxonomy is complete.
- Why unresolved: The paper relies on observed patterns in existing models; as models evolve, they may develop new failure modes that do not fit neatly into "excessive" (surplus info) or "speculative" (intent shift) categories.
- What evidence would resolve it: Discovery of a widespread, harmful LLM failure triggered by benign prompts that cannot be mathematically classified as an information-length violation (excessive) or a trajectory shift (speculative).

## Limitations
- The exact relationship between RLHF training data verbosity patterns and excessive response behavior is inferred but not directly validated through model introspection or ablation studies.
- The transferability of secondary risks across model families (35-45% ASR) is promising but relies on surface-level prompt similarity rather than understanding whether underlying vulnerabilities are truly shared.
- The stealth detection mechanism (DETECTSCORE) is implemented as a black-box safety filter score, but the specific detection criteria and their robustness against adversarial prompt engineering are not detailed.

## Confidence
- **High Confidence**: The formal definition of secondary risks (excessive response and speculative advice) based on information-theoretic principles (H(Y|X) and I(X;Y)). The experimental setup and evaluation methodology are well-specified.
- **Medium Confidence**: The SecLens algorithm's effectiveness in discovering secondary-risk prompts, given the multi-objective optimization framework and empirical results. However, the generalizability of these results to real-world deployment scenarios is less certain.
- **Low Confidence**: The transferability of secondary risks across model families and modalities. While results suggest transferability, the underlying mechanisms and practical implications require further investigation.

## Next Checks
1. **Model Introspection Study**: Conduct ablation studies to validate the causal relationship between RLHF training data verbosity patterns and excessive response behavior. This could involve training models with controlled length biases and measuring secondary-risk emergence.
2. **Transferability Validation**: Test the transferability of secondary risks across a broader range of model families and tasks, including zero-shot and few-shot transfer scenarios. Investigate whether surface-level prompt similarity is sufficient for successful transfer.
3. **Stealth Robustness Analysis**: Evaluate the robustness of the stealth detection mechanism (DETECTSCORE) against adversarial prompt engineering techniques. This could involve testing whether optimized prompts can evade detection by evolving their surface-level characteristics while maintaining risk behavior.