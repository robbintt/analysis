---
ver: rpa2
title: Normalizing Flows are Capable Visuomotor Policy Learning Models
arxiv_id: '2509.21073'
source_url: https://arxiv.org/abs/2509.21073
tags:
- diffusion
- policy
- arxiv
- nf-p
- normalizing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Normalizing Flows Policy (NF-P) is introduced as a competitive
  alternative to Diffusion Policy for visuomotor policy learning in robotics. NF-P
  leverages normalizing flows to provide both fast single-pass inference and uncertainty
  quantification, addressing key limitations of diffusion models such as slow sampling
  and lack of confidence measures.
---

# Normalizing Flows are Capable Visuomotor Policy Learning Models

## Quick Facts
- arXiv ID: 2509.21073
- Source URL: https://arxiv.org/abs/2509.21073
- Reference count: 31
- Primary result: NF-P achieves comparable or superior performance to Diffusion Policy while offering 30× faster inference and uncertainty quantification.

## Executive Summary
Normalizing Flows Policy (NF-P) is introduced as a competitive alternative to Diffusion Policy for visuomotor policy learning in robotics. NF-P leverages normalizing flows to provide both fast single-pass inference and uncertainty quantification, addressing key limitations of diffusion models such as slow sampling and lack of confidence measures. The method uses a ResNet18 encoder, action sequence prediction with temporal stride, and likelihood-based sampling (via gradient ascent or multi-sampling) to improve action quality. In experiments on four simulated robotic tasks using the RoboTwin 2.0 framework, NF-P achieved comparable or superior performance to Diffusion Policy, particularly with limited training data (e.g., 80% success rate on Lift Pot with 50 episodes vs. 37% for Diffusion Policy). Inference time was reduced by up to 30×, from 701 ms to under 20 ms. Ablation studies confirmed the importance of action sequencing, stride, and sampling strategies. NF-P is presented as a scalable, efficient, and interpretable model for general-purpose robotics, with potential for further integration with other policy models.

## Method Summary
NF-P uses a ResNet18 encoder to process visual observations and a 10-layer Neural Spline Flow (NSF) to model the conditional distribution of action sequences given observations. The flow is conditioned on both visual embeddings and robot state information. To address stalling issues common in flow-based policies, the method predicts temporally strided action sequences (e.g., actions at t, t+4, t+8, etc.) rather than consecutive steps. During inference, NF-P generates actions through a single inverse pass of the flow, with two refinement strategies: selecting the highest-likelihood sample from a batch of 128, or optimizing a single sample via gradient ascent on the likelihood. The model is trained using negative log-likelihood loss with a three-stage learning rate and batch size schedule.

## Key Results
- NF-P achieved 80% success rate on Lift Pot task with only 50 training episodes, compared to 37% for Diffusion Policy.
- Inference speed was reduced by up to 30×, from 701 ms (Diffusion Policy) to under 20 ms (NF-P).
- NF-P multi (sampling-based) achieved 78% success on Lift Pot with 50 episodes, outperforming Diffusion Policy's 37%.
- Ablation study showed stride s=4 achieved 51% success vs. stride s=1 at 37% for Beat Block Hammer task.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Normalizing Flows enable significantly faster inference than Diffusion Policy by replacing iterative denoising with a single bijective pass.
- **Mechanism:** The model learns an invertible transformation $T$ between the action distribution and a Gaussian latent space. At inference, a noise vector $u$ is sampled and transformed into an action sequence $x$ via a single forward pass through $T^{-1}$ (the inverse flow), rather than requiring sequential denoising steps.
- **Core assumption:** The complex, multimodal action distribution required for visuomotor tasks can be mathematically represented as a continuous, invertible deformation of a Gaussian distribution.
- **Evidence anchors:**
  - [abstract] "providing... a highly efficient inference process."
  - [section IV.C] "NF-P to generate only high-probability actions... leveraging the NF's ability to evaluate p(a|o)."
  - [corpus] "Normalizing Flows are Capable Models for RL" (neighbor paper) supports NFs as a viable alternative to diffusion in RL, but specific inference speed claims are isolated to this paper's experiments.
- **Break condition:** If the action space topology is disjoint or highly complex (non-smooth manifolds), the flow may fail to model the distribution accurately, degrading performance despite fast inference.

### Mechanism 2
- **Claim:** Exact log-likelihood computation allows for "likelihood-based output optimization," filtering out low-quality actions that diffusion models might generate.
- **Mechanism:** Unlike diffusion models, NFs admit explicit density estimation via the change of variables formula. The paper exploits this by either (a) selecting the best action from a multi-sample batch (NF-P multi) or (b) refining a sample via gradient ascent on the likelihood (NF-P opt).
- **Core assumption:** Samples with higher probability density under the learned model correlate with higher task success rates (i.e., the model has successfully captured the expert distribution).
- **Evidence anchors:**
  - [section IV.C] "we leverage the NF's ability to evaluate p(a|o)... generating a batch of 128 actions and selecting the one with highest probability."
  - [section V.B] "NF-P multi offers comparable performance to NF-P opt... [despite] major difference in inference times."
  - [corpus] Evidence for this specific optimization technique in robotics is weak in the provided corpus; this appears to be a domain-specific adaptation introduced in this work.
- **Break condition:** If the training data is poor or the model is miscalibrated, high likelihood may not correspond to expert behavior (high density $\neq$ high reward).

### Mechanism 3
- **Claim:** Applying a temporal stride to training data reduces "stalling" behavior common in flow-based policies.
- **Mechanism:** Instead of learning dense sequences ($t, t+1, \dots$), the model learns the distribution of temporally strided actions ($t, t+s, \dots$). This forces the model to predict larger state transitions, preventing the policy from getting stuck in "safe" local minima where it predicts small, ineffectual actions.
- **Core assumption:** Stalling is caused by the model converging to low-velocity predictions, and skipping timesteps encourages sufficient momentum in the generated trajectory.
- **Evidence anchors:**
  - [section IV.B] "normalizing flow model seems prone to stalling... mitigate this issue, we introduce a stride."
  - [section V.B] Ablation study shows stride $s=4$ achieves 51% success vs. stride $s=1$ at 37% (Table II).
  - [corpus] No direct mention of the "stride" mechanism for stalling in the provided corpus neighbors.
- **Break condition:** If the stride is too large ($s=8$), the model loses temporal resolution and produces erratic motions (ablation shows performance drop from 51% to 46%).

## Foundational Learning

- **Concept:** Normalizing Flows (Bijective Mappings)
  - **Why needed here:** This is the core engine of the architecture. Unlike standard neural networks, you must understand that the network $T$ must be strictly invertible and have a tractable Jacobian determinant to calculate likelihoods.
  - **Quick check question:** Can you explain why we cannot use a standard ResNet block directly for a Normalizing Flow without specific architectural constraints (like coupling layers)?

- **Concept:** Multimodal Action Distributions
  - **Why needed here:** Robotic tasks often have multiple valid solutions (e.g., left hand vs. right hand). Standard regression (MSE) averages these modes, leading to invalid actions. You must understand why generative models (Flows/Diffusion) are used to sample distinct modes.
  - **Quick check question:** Why does Mean Squared Error (MSE) loss fail when training a policy on a dataset where a robot can grasp a pot from either the left or right side?

- **Concept:** Visuomotor Policy Learning
  - **Why needed here:** You need to understand how high-dimensional image inputs (compressed by ResNet18) are conditioned into the policy alongside robot states to predict continuous action sequences.
  - **Quick check question:** How does the "Action Sequence Prediction" approach differ from single-step autoregressive prediction, and why does it improve temporal consistency?

## Architecture Onboarding

- **Component map:** Observation (Image + State) -> ResNet18 Encoder -> MLP Conditioning Network -> 10 Neural Spline Flow Coupling Layers -> 8-step Action Sequence Output

- **Critical path:**
  1.  **Input:** Observation $o_t$ (Image + State).
  2.  **Encoding:** ResNet18 processes image.
  3.  **Sampling:** Draw noise $u \sim \mathcal{N}(0, \sigma I)$.
  4.  **Inverse Pass:** Compute action $a = T^{-1}(u, o_t)$.
  5.  **Refinement (Optional):** Select max-likelihood sample from batch or run gradient ascent.

- **Design tradeoffs:**
  - **NF-P Opt vs. NF-P Multi:** *Opt* (gradient ascent) yields high-quality actions but negates the speed benefit (455ms vs 18ms). *Multi* (sampling 128 actions) retains near-instantaneous inference (19ms) while improving quality.
  - **Stride Size ($s$):** A balance between preventing stalling (high $s$) and maintaining smooth motion (low $s$). Paper settles on $s=4$.

- **Failure signatures:**
  - **Stalling:** Robot freezes or moves too slowly. *Likely cause:* Stride $s$ is too small or sampling variance $\sigma$ is too low.
  - **Erratic Motion:** Robot jerks or acts randomly. *Likely cause:* Sampling variance $\sigma$ is too high (sampling low-likelihood tails) or stride is too large.

- **First 3 experiments:**
  1.  **Overfit Sanity Check:** Train on a single demonstration (e.g., Lift Pot). Verify the NF-P can perfectly reconstruct the action sequence with near-zero loss.
  2.  **Inference Benchmark:** Compare inference latency of a single NF forward pass against a standard Diffusion Policy (DDIM) with 10-20 steps on the target hardware.
  3.  **Stride Ablation:** Train three policies on the "Beat Block Hammer" task with strides $s=1, 4, 8$ and observe the frequency of "stalling" vs. "erratic" behaviors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superior inference speed and data efficiency of NF-P transfer to physical robotic hardware?
- Basis: [inferred] All reported experiments were conducted exclusively within the RoboTwin 2.0 simulation framework; no physical robot evaluations were performed.
- Why unresolved: Simulators often fail to capture actuation noise, latency jitter, and visual domain shifts present in real-world deployment, which affects visuomotor policies.
- What evidence would resolve it: Benchmarks on physical robotic platforms (e.g., ALOHA) comparing success rates and real-time latency against Diffusion Policy.

### Open Question 2
- Question: How does training-time noise injection interact with the proposed gradient ascent sampling method?
- Basis: [explicit] Section VII states, "It would be an interesting direction to investigate how adding noise behaves in conjunction with our gradient ascent likelihood-optimization."
- Why unresolved: The authors hypothesize that noise removal may be equivalent to a step of gradient ascent, but this synergy remains empirically untested.
- What evidence would resolve it: Ablation studies comparing model performance when trained with varying noise levels while utilizing gradient ascent sampling strategies.

### Open Question 3
- Question: Can NF-P be integrated as a confidence estimator or auxiliary module for other policy architectures?
- Basis: [explicit] Section VII suggests, "It is possible that NFs could be applied in conjunction with for example DP or a transformer-based model to further improve performance."
- Why unresolved: The coupling mechanisms and trade-offs between a generative flow model and other architectures (like Transformers) in a hybrid setup are currently undefined.
- What evidence would resolve it: Experiments where an NF model guides or rejects samples from a primary Transformer or Diffusion policy to improve overall robustness.

## Limitations
- All experiments conducted in simulation (RoboTwin 2.0) with no real-robot validation presented.
- Theoretical understanding of why stride s=4 is optimal remains unclear.
- NF-P opt (gradient-based) is too slow for practical deployment despite higher performance.

## Confidence
- **High Confidence**: NF-P inference speed advantage over Diffusion Policy (up to 30× faster, 701ms vs. 18-20ms) is directly measured and well-supported by experimental results.
- **Medium Confidence**: Performance parity claim (NF-P vs. Diffusion Policy) is based on 4 tasks in simulation; real-world generalization is untested.
- **Medium Confidence**: Likelihood-based sampling improves action quality over naive sampling, but the ablation study does not isolate the contribution of the multi-sample strategy from the flow architecture itself.
- **Low Confidence**: The claim that NF-P provides meaningful uncertainty quantification for safety-critical decisions is not validated; the paper only shows likelihood scores, not their correlation with actual task risk.

## Next Checks
1. **Real-robot validation**: Deploy NF-P on a physical robot for at least one of the simulated tasks (e.g., Lift Pot) to verify sim-to-real transfer and measure actual inference latency on embedded hardware.
2. **Uncertainty calibration**: Test whether high-likelihood actions from NF-P actually correspond to lower failure rates under varying environmental conditions (e.g., perturbed object positions).
3. **Generalization test**: Evaluate NF-P on a held-out task or dataset (e.g., a different object manipulation task not in RoboTwin 2.0) to assess scalability beyond the 4 presented tasks.