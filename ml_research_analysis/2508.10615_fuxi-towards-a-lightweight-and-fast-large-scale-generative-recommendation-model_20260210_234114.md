---
ver: rpa2
title: "FuXi-\u03B2: Towards a Lightweight and Fast Large-Scale Generative Recommendation\
  \ Model"
arxiv_id: '2508.10615'
source_url: https://arxiv.org/abs/2508.10615
tags:
- fuxi
- attention
- arxiv
- recommendation
- relative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces FuXi-\u03B2, a lightweight and fast large-scale\
  \ generative recommendation model designed to address the latency and computational\
  \ costs associated with scaling recommendation systems. The authors identify two\
  \ bottlenecks in existing models: inefficient relative temporal attention bias operations\
  \ and redundant query-key attention maps."
---

# FuXi-β: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model

## Quick Facts
- **arXiv ID**: 2508.10615
- **Source URL**: https://arxiv.org/abs/2508.10615
- **Reference count**: 40
- **Primary result**: Achieves 27-47% improvements in NDCG@10 on large-scale datasets while reducing training time by 13-27% compared to FuXi-α

## Executive Summary
FuXi-β addresses computational bottlenecks in large-scale generative recommendation models by introducing two key innovations: Functional Relative Attention Bias (FRAB) and Attention-Free Token Mixer (AFTM). The authors identify that standard relative temporal attention bias operations are computationally expensive due to indexing operations, and that query-key attention maps may be redundant for sequential recommendation tasks. By replacing bucketed indexing with a parametric power function for temporal modeling and removing query-key dot products while preserving temporal and positional bias information, FuXi-β achieves significant performance improvements on industrial datasets while maintaining scalability according to scaling laws.

## Method Summary
FuXi-β implements a novel self-attention mechanism that replaces the standard query-key dot product with a simplified attention-free token mixer. The model uses FRAB to compute temporal attention bias via a power function $f(x) = a(1+x)^{-b}$ instead of time-consuming bucketed indexing, and AFTM removes the query-key attention map entirely, relying instead on positional and temporal bias terms to weight value vectors. The architecture stacks multiple FuXi-β blocks containing RMSNorm, AFTM, and a multi-stage feed-forward network with SwiGLU activation. Training uses sampled softmax loss with autoregressive next-item prediction on fixed-length user interaction sequences.

## Key Results
- FuXi-β achieves 27-47% improvements in NDCG@10 on large-scale industrial datasets compared to baseline models
- The model reduces training time by 13-27% compared to FuXi-α while maintaining or improving performance
- FuXi-β adheres to scaling laws, showing consistent performance improvements as layer count increases
- On MovieLens datasets, FuXi-β slightly underperforms FuXi-α but still outperforms other baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Replacing indexing-based Relative Attention Bias (RAB) with a functional form (FRAB) reduces computational overhead while preserving temporal modeling capability.
- **Mechanism**: Instead of looking up learnable embeddings via non-contiguous memory access based on bucketed time intervals, FRAB uses a parametric power function $f(x) = a(1+x)^{-b}$ to directly compute attention bias values based on time differences.
- **Core assumption**: The relationship between temporal distance and attention weight can be effectively approximated by a monotonic power function rather than a discrete lookup table.
- **Evidence anchors**: [abstract] "Functional Relative Attention Bias (FRAB) to replace time-consuming indexing operations"; [section 5.2] "avoids the time-consuming operations of the original relative attention bias... only utilizes special function and arithmetic operations"
- **Break condition**: If temporal patterns require non-monotonic or highly irregular cyclical relationships that a simple power function cannot capture (e.g., complex seasonality), this functional approximation may fail.

### Mechanism 2
- **Claim**: The standard Query-Key (QK) dot-product attention map is redundant and potentially detrimental in sequential recommendation tasks, and removing it improves performance.
- **Mechanism**: The authors empirically observe that QK attention maps might introduce noise or overfitting. By removing QK projections and using only the bias terms (positional and temporal) to weigh the Value vectors, the model simplifies the "Token Mixer" into an efficient, attention-free operation.
- **Core assumption**: Item-to-item similarity (via QK dot products) is less important than the sequential/temporal structure (via bias terms) for predicting the next item in this specific generative architecture.
- **Evidence anchors**: [abstract] "redundant query-key attention maps... using the query-key attention map might degrade the model's performance"; [section 6.4.3] "removing the query-key attention map... achieves the best performance on both datasets"
- **Break condition**: If a recommendation task relies heavily on the semantic content of items (e.g., finding a complementary product regardless of sequence position) rather than just sequential patterns, removing QK interactions would likely degrade performance.

### Mechanism 3
- **Claim**: Decoupling the "Token Mixer" from standard matrix multiplication allows for consistent scaling with lower latency.
- **Mechanism**: By reducing the self-attention complexity from $O(2n^2d)$ to $O(n^2d)$ (by removing Q-K dot product) and optimizing the bias calculation, the model reduces the total training time (13-27% reduction in experiments) while still adhering to scaling laws (performance improves with layers).
- **Core assumption**: The efficiency gains from simplified arithmetic (FRAB) and reduced matrix multiplication (AFTM) outweigh the representational capacity lost by removing standard self-attention.
- **Evidence anchors**: [abstract] "FuXi-$\beta$... achieves significant acceleration compared to FuXi-$\alpha$, while also adhering to the scaling law"; [section 5.6] "reduce approximately half of the computational load in the self-attention layer"
- **Break condition**: Scaling breaks if the parameter count (layers/dimensions) increases to the point where the remaining operations (Value projection and MFFN) become the new primary bottlenecks, or if the reduced model capacity limits the effective use of larger datasets.

## Foundational Learning

- **Concept**: **Relative Attention Bias (RAB)**
  - **Why needed here**: Standard Transformer position embeddings are often absolute. RAB allows the model to weigh the importance of items based on their distance from each other (time/position) in the sequence, which is critical for user behavior modeling.
  - **Quick check question**: How does the model know the difference between an item viewed 1 minute ago vs. 1 hour ago without RAB/FRAB?

- **Concept**: **Token Mixing**
  - **Why needed here**: In Transformers, this is the mechanism (usually Self-Attention) that allows information to flow between different items in a sequence. FuXi-$\beta$ radically changes this by removing the Query-Key interaction.
  - **Quick check question**: If we remove the Query-Key dot product, what mechanism allows the model to "mix" information from previous items into the current item's representation?

- **Concept**: **Scaling Laws in Recommendation**
  - **Why needed here**: The paper argues for the validity of its lightweight model by showing that performance predictably improves as the model gets larger (more layers), proving it is not just a "small model" trick but a scalable architecture.
  - **Quick check question**: Does the NDCG@10 score continue to increase as we stack more FuXi-$\beta$ blocks, or does it plateau/degrade?

## Architecture Onboarding

- **Component map**: Embedding Layer -> FuXi-$\beta$ Block (Stacked $L$ times) -> Prediction Layer
- **Critical path**: The critical innovation path is inside the **AFTM**. Specifically, the calculation of $B_t$ (Temporal Bias) via the function $a(1+x)^{-b}$. This function must be differentiable and efficient. If this function defaults to zero or fails to distinguish time intervals, the model collapses to a position-only model.
- **Design tradeoffs**:
  - **Accuracy vs. Complexity (Public Data)**: On smaller public datasets (MovieLens), the simplified model performs slightly worse than the heavier baseline (FuXi-$\alpha$).
  - **Accuracy vs. Complexity (Industrial Data)**: On large industrial data, the simplified model performs significantly *better* (27-47% lift), likely due to regularization effects preventing overfitting on noisy data.
  - **Function Selection**: Using a Power function ($f_{pow}$) vs Exponential ($f_{exp}$). The paper argues Power is better for longer distances.
- **Failure signatures**:
  - **Training instability**: If the power function $b$ becomes large, gradient explosion might occur.
  - **Performance Plateau**: If using linear or periodic functions (sin) for temporal bias where monotonic decay is required (Table 6 shows sin performs worse).
  - **Inference Latency**: While faster, if the sequence length ($n$) is massive, the $n^2$ bias matrices ($B$) still consume memory, though less than full QK attention.
- **First 3 experiments**:
  1. **Ablate the QK Map**: Take a standard baseline (FuXi-$\alpha$ or HSTU), remove the Query-Key matrix multiplication, and measure the change in NDCG and speed. (Expect: Speed up, NDCG stable or up).
  2. **Functional Fit Test**: Replace the FRAB power function with alternatives (Linear, Log, Sin) and plot attention weights against time distance to verify monotonic decay requirements (replicate Table 6/Figure 3).
  3. **Scaling Check**: Train FuXi-$\beta$ with varying layer counts (2, 4, 8, 16) on a large dataset and plot NDCG to verify the "Scaling Law" curve is positive (replicate Figure 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the FuXi-β framework be effectively adapted for complex multi-task or cross-domain recommendation scenarios?
- Basis in paper: [explicit] In the Conclusion, the authors state, "In the future, we plan to further reduce the complexity of the model and aim to address more complex recommendation tasks."
- Why unresolved: The current study focuses exclusively on standard sequential recommendation tasks using public and industrial datasets, without evaluating multi-task learning or cross-domain transfer capabilities.
- What evidence would resolve it: Experiments applying FuXi-β to multi-task benchmarks (e.g., predicting click-through rate and conversion rate simultaneously) demonstrating it maintains efficiency without performance degradation.

### Open Question 2
- Question: Does the removal of the query-key attention map universally improve performance across diverse data densities, or is it specific to the sparse interaction data used in this study?
- Basis in paper: [inferred] The authors empirically found that removing the query-key attention map improved performance, but they note this contradicts standard Transformer wisdom. The result is inferred to be sensitive to the "complex and challenging data distribution" of industrial datasets.
- Why unresolved: The paper validates this on specific music and movie datasets, but the theoretical boundary where query-key interactions become redundant or harmful (e.g., in dense information domains) remains undefined.
- What evidence would resolve it: A comparative analysis on datasets with varying interaction densities showing the performance delta of the query-key map relative to data sparsity.

### Open Question 3
- Question: What specific modifications are required to successfully apply the AFTM module to architectures like HSTU without suffering from the performance drop observed in the paper?
- Basis in paper: [explicit] In Section 6.4.4, the authors note that applying the framework to HSTU (creating HSTU-β) resulted in a performance decline, attributing it to HSTU's "weakened implicit feature interaction."
- Why unresolved: The paper demonstrates the framework works for FuXi-α but fails to fully explain or resolve why the simplification harms the specific structural dynamics of HSTU.
- What evidence would resolve it: An architectural analysis of HSTU-β identifying the specific feature interaction layers that collapse when the query-key map is removed, followed by a successful architectural correction.

## Limitations

- **Dataset scope**: Results are validated primarily on MovieLens and one industrial dataset, without systematic testing across diverse recommendation domains or varying sequence characteristics.
- **Bias mechanism details**: The exact form of positional bias within AFTM remains ambiguous—it's unclear whether bucketed RAB is still used or if a functional form is applied there too.
- **Hyperparameter opacity**: Critical training configurations (learning rate, batch size, negative sample count) are not explicitly stated, making exact reproduction challenging.

## Confidence

**High confidence**: The core architectural innovation (FRAB + AFTM) is clearly specified and the empirical results on NDCG@10 improvements (27-47% on industrial data) are directly supported by reported experiments. The scaling law adherence claim is substantiated by layer-varying experiments.

**Medium confidence**: The mechanism explanation for why removing QK attention improves performance is well-reasoned but relies on observed empirical patterns rather than analytical proof. The generalization of latency improvements across different hardware/sequence lengths is implied but not directly validated.

**Low confidence**: The claim about outperforming all baselines on "all metrics" is questionable—Table 2 shows FuXi-β performs slightly worse than FuXi-α on public datasets for some metrics (HR@10/50, MRR). The "significant acceleration" claim is relative to FuXi-α specifically, not an absolute benchmark.

## Next Checks

1. **Cross-domain replication**: Implement FuXi-β on a non-MovieLens dataset (e.g., Amazon Books or YooChoose) to verify the NDCG@10 improvements and latency gains hold across different recommendation domains with varying sequence characteristics.

2. **Bias composition verification**: Run controlled experiments isolating positional vs. temporal bias contributions by systematically disabling each component in the AFTM, then measuring performance drops to quantify their relative importance.

3. **Extreme scaling test**: Train FuXi-β with very large layer counts (16-32) on the largest available dataset to empirically verify the scaling law prediction continues positively without degradation, and measure where computational bottlenecks actually emerge.