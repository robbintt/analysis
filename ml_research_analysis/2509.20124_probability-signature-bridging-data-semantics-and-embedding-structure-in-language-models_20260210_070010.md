---
ver: rpa2
title: 'Probability Signature: Bridging Data Semantics and Embedding Structure in
  Language Models'
arxiv_id: '2509.20124'
source_url: https://arxiv.org/abs/2509.20124
tags:
- embedding
- next
- figure
- structure
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how the embedding structures in language
  models are formed, focusing on the relationship between data distribution and semantic
  patterns. The authors propose a set of probability signatures that capture semantic
  relationships among tokens, derived from label distribution, input distribution,
  and co-occurrence patterns.
---

# Probability Signature: Bridging Data Semantics and Embedding Structure in Language Models

## Quick Facts
- **arXiv ID**: 2509.20124
- **Source URL**: https://arxiv.org/abs/2509.20124
- **Reference count**: 37
- **Primary result**: Proposes probability signatures that capture semantic relationships among tokens, demonstrating they significantly influence embedding structure formation in language models.

## Executive Summary
This work investigates how embedding structures in language models emerge from data distribution patterns. The authors introduce probability signatures—derived from label distribution, input distribution, and co-occurrence patterns—that capture semantic relationships among tokens. Through controlled experiments on synthetic tasks and theoretical analysis of gradient flow dynamics, they demonstrate that these signatures drive embedding formation. When extended to LLMs trained on subsets of the Pile corpus, the results show that probability signatures faithfully align with embedding structures, particularly capturing strong pairwise similarities among embeddings.

## Method Summary
The study combines synthetic experiments with theoretical analysis. Synthetic tasks involve composite addition operations using linear models and feedforward networks, with controlled data distributions. The authors compute probability signatures from corpus statistics and train models with small initialization to analyze gradient flow dynamics. For LLMs, they train Qwen2.5 architectures on Pile subsets and measure alignment between embedding cosine similarity and probability signature similarity. The theoretical framework derives gradient flow equations showing how probability signatures act as forcing terms that systematically organize embeddings.

## Key Results
- Probability signatures (ϕ^y, ϕ^X, ϕ^{X|y}) significantly influence embedding structure formation through gradient flow dynamics
- Linear models capture ϕ^y-driven structure while feedforward networks are needed for ϕ^{X|y}-driven structure
- In trained LLMs, probability signatures faithfully capture strong pairwise embedding similarities, with alignment strengthening in high-similarity regions

## Why This Works (Mechanism)

### Mechanism 1
Embedding structure formation is driven by probability signatures through gradient flow dynamics. The gradient of the embedding vector decomposes into terms weighted by probability signatures—specifically, the label distribution given input (ϕ^y_x), co-occurrence patterns (ϕ^X_x), and conditional co-occurrence (ϕ^{X|y}_x). These signatures act as forcing terms that systematically organize embeddings. Core assumption: Training dynamics can be approximated in the infinite-data limit (N→∞) with small initialization scales. Evidence anchors: [abstract] states signatures "significantly influence embedding structures"; [Section 5.2, Corollary 1] shows dynamics are "primarily impacted by probability signatures." Break condition: When probability signatures are identical across tokens, gradient flow lacks differential signal, causing embedding collapse.

### Mechanism 2
Different probability signatures dominate embedding formation depending on task structure and model architecture. In linear models, ϕ^y_x (label distribution) acts as the leading term while ϕ^X_x (co-occurrence) is attenuated. In feedforward networks with nonlinearity, ϕ^{X|y}_x (conditional co-occurrence) emerges as the dominant driver through tensor T multiplication in the gradient. Core assumption: Activation functions can be approximated via Weierstrass expansion for small initialization. Evidence anchors: [Section 5.2] shows ϕ^y_α as leading term; [Section 5.2, Corollary 2] shows ordered embedding structure "primarily relies on probability signature ϕ^{X|y}_α." Break condition: Without nonlinear activation, the ϕ^{X|y}_x term cannot propagate effectively; modular addition fails in linear models but succeeds in FFN.

### Mechanism 3
Probability signatures faithfully capture strong pairwise embedding similarities in trained LLMs. For next-token prediction, the gradient flow dW^E_s/dt ≈ r^in_s W^{U,T} ϕ^next_s. The alignment between cos(W^E) and cos(ϕ^next) strengthens in high-similarity regions, meaning tokens with similar next-token distributions develop similar embeddings. Core assumption: Self-attention contributions are secondary to embedding/unembedding dynamics in early training. Evidence anchors: [Section 6, Corollary 4] shows "distributions of its next token and previous token significantly impact its embedding"; [Figure 5 C-E] shows correlation increases with embedding similarity magnitude. Break condition: Estimation from incomplete or mismatched corpora may fail.

## Foundational Learning

- **Gradient Flow Dynamics**: The paper's core theoretical contribution derives embedding dynamics from continuous-time gradient descent; understanding dW/dt formulations is essential. Quick check: Can you explain why small initialization changes the relative importance of different terms in the gradient expansion?

- **Cross-Entropy Loss Decomposition**: Proposition 1 and 2 derive from ∂ℓ/∂W decompositions; the softmax approximation (p ≈ 1/d_vob · 1 + 1/d_vob · f) is used repeatedly. Quick check: How does the softmax Jacobian affect gradient propagation when logits are small?

- **Conditional Probability as Geometric Structure**: Probability signatures are conditional distributions (e.g., P(y=ν|x∈X)) converted to vector form; their cosine similarity predicts embedding similarity. Quick check: Why would two tokens with identical next-token distributions converge to similar embeddings under gradient flow?

## Architecture Onboarding

- **Component map**:
  - Embedding Matrix (W^E): d × d_vocab; column x is the embedding vector for token x
  - Unembedding Matrix (W^U): d_vocab × d; row ν is the output projection for predicting token ν
  - Hidden Mapping (G): Architecture-specific transformation (linear sum, FFN with ReLU, or Transformer layers)
  - Probability Signatures: ϕ^y, ϕ^X, ϕ^{X|y}, φ^X computed from training corpus statistics

- **Critical path**:
  1. Compute probability signatures from corpus (counts → conditional distributions)
  2. Initialize W^E, W^U with small variance (d^-0.8 scaling used in paper)
  3. Training produces embedding structure aligned with signatures
  4. Validate via cosine similarity correlation: Corr(cos(W^E), cos(ϕ^next))

- **Design tradeoffs**:
  - Linear vs. FFN: Linear models capture ϕ^y-driven structure quickly; FFN required for ϕ^{X|y}-driven structure (nonlinear conditional patterns)
  - Signature selection: ϕ^next captures output-side statistics; φ^pre captures input-side; tied embeddings require combined estimate (ϕ̃ = ϕ^next + φ^pre)
  - Corpus subset choice: Different domains (mathematics vs. Wikipedia) produce different signature-embedding alignments

- **Failure signatures**:
  - Embedding collapse: All vectors align in same direction when probability signatures are uniform/identical across tokens
  - Slow structure formation: When ϕ^X dominates over ϕ^y (as in ˜f_add), ordered structure requires more training steps
  - Signature-embedding mismatch: When estimating from wrong corpus, predicted structure diverges from actual

- **First 3 experiments**:
  1. Replicate f_add task with linear model; verify R_order(W^E_A) → -1 and correlate with cos(ϕ^y_A) structure
  2. Train FFN on f_mod; confirm ϕ^{X|y}_α alignment via PCA projection comparison (Figure 3C style)
  3. Train small Transformer on Pile subset; compute R_cos(W^E, ϕ^next) and verify stronger alignment in high-similarity regions (per Figure 5C)

## Open Questions the Paper Calls Out

### Open Question 1
How does the self-attention mechanism in full Transformers interact with probability signatures to shape embedding structures, compared to the feedforward and linear models analyzed? Basis: [explicit] The Conclusion states the intent to "incorporate the self-attention mechanism into our analysis... which is essential for capturing more subtle and complex relationships among embeddings that remain beyond the reach of our current methods." Why unresolved: Current theoretical derivations treat the network backbone G(·) as a linear or feedforward mapping, or approximate attention effects as negligible. What evidence would resolve it: Theoretical gradient flow analysis including attention matrix terms, or empirical studies demonstrating attention heads alter embedding alignment in ways not predicted by ϕ_{next} or ϕ_{pre}.

### Open Question 2
Does the dependence of embedding structure on probability signatures hold under standard parameter initialization schemes, or is it specific to the small initialization scale used in the study? Basis: [inferred] The theoretical analysis (Appendix D) and experimental setup (Section 5.1) rely on small initialization (W_{i,j} ~ N(0, d^{-0.8})) to approximate softmax functions and activation derivatives, which may not reflect standard training dynamics. Why unresolved: Standard initialization often leads to different training phases, and the "probability signature" driving force might be overshadowed by other optimization dynamics if weights are larger. What evidence would resolve it: Experiments replicating the addition tasks using standard variance scalers (e.g., d^{-1}) to observe if the R_{order} alignment with probability signatures persists or degrades.

### Open Question 3
Can probability signatures estimated from proxy text corpora reliably predict the embedding structure of pre-trained models whose exact training data is unknown or distinct? Basis: [explicit] Section 6 notes that the estimation method "may not generalize across all open-source base models, as it is sensitive to both the initialization of the pre-trained model and the true training dataset." Why unresolved: The alignment between ϕ̃ and W^E was demonstrated for models the authors trained themselves; for black-box LLMs, the mismatch between the proxy data distribution and the true training distribution creates uncertainty. What evidence would resolve it: A systematic benchmark evaluating the correlation between estimated signatures and embeddings across diverse pre-trained models using various proxy datasets to quantify the sensitivity to data mismatch.

## Limitations

- Theoretical approximations rely on infinite data limits, small initialization regimes, and second-order Taylor expansions that may not hold in practice
- Corpus domain specificity raises questions about generalizability beyond mathematics, Wikipedia, and coding domains
- Validation relies primarily on cosine similarity correlations rather than causal analysis or ablation studies

## Confidence

**High Confidence**: The basic experimental results showing hierarchical structure formation in synthetic tasks (f_add vs f_mod) are reproducible and align with theoretical predictions. The observation that linear models capture ϕ^y-driven structure while FFNs are needed for ϕ^{X|y}-driven structure is well-supported by controlled experiments.

**Medium Confidence**: The theoretical analysis of gradient flow dynamics is mathematically sound but depends on approximations that may not hold in practice. The extension to LLMs shows promising alignment patterns, but the relationship between theoretical predictions and actual training dynamics is not directly validated.

**Low Confidence**: The claim that probability signatures "faithfully capture" embedding structures in LLMs is based on correlation metrics rather than causal analysis. The paper doesn't demonstrate that embeddings wouldn't form without probability signatures, or that the signatures are the primary rather than merely correlated driver.

## Next Checks

**Check 1: Dynamic Trajectory Validation** - Instead of just comparing final embeddings to probability signatures, track the actual gradient flow during training. Measure how closely dW^E/dt trajectories follow theoretical predictions based on probability signatures. This would directly validate whether the signatures drive the dynamics as claimed.

**Check 2: Cross-Domain Generalization** - Test the probability signature-embedding alignment across diverse domains including social media text, news articles, and multilingual corpora. This would assess whether the observed relationships are robust or domain-specific artifacts of the Pile subsets used.

**Check 3: Ablation Studies on Signature Components** - Systematically remove or corrupt individual probability signature components (ϕ^y, ϕ^X, ϕ^{X|y}) and observe the impact on embedding structure formation. This would establish causal relationships rather than just correlations, addressing the key uncertainty about whether signatures are drivers or merely correlated features.