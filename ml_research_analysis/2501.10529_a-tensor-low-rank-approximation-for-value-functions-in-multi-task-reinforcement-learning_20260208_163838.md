---
ver: rpa2
title: A Tensor Low-Rank Approximation for Value Functions in Multi-Task Reinforcement
  Learning
arxiv_id: '2501.10529'
source_url: https://arxiv.org/abs/2501.10529
tags:
- learning
- tensor
- tasks
- low-rank
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a multi-task reinforcement learning approach
  based on low-rank tensor approximation of Q-functions across tasks. The key idea
  is to model the collection of Q-functions as a single tensor and enforce a low-rank
  structure to capture task similarities without explicitly prescribing them.
---

# A Tensor Low-Rank Approximation for Value Functions in Multi-Task Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.10529
- Source URL: https://arxiv.org/abs/2501.10529
- Reference count: 31
- Multi-task RL method using low-rank tensor approximation of Q-functions across tasks, achieving improved sample efficiency

## Executive Summary
This paper introduces S-TLR-Q, a multi-task reinforcement learning method that models the collection of Q-functions across tasks as a low-rank tensor. By enforcing a PARAFAC decomposition on the Q-tensor, the method enables information sharing across related tasks without requiring explicit similarity specifications. The approach uses stochastic optimization to learn tensor factors jointly across all tasks, capturing latent task similarities in the data.

The method is evaluated on inverted pendulum and wireless communication systems, where it consistently outperforms baselines that learn tasks separately or share a common representation. Specifically, S-TLR-Q achieves higher cumulative rewards while requiring fewer samples to converge, demonstrating improved sample efficiency in data-scarce regimes.

## Method Summary
The method represents the Q-functions for M tasks as a 3-mode tensor Q ∈ ℝ^{|S|×|A|×M} and enforces a low-rank PARAFAC decomposition with rank K. Each task's Q-function is expressed as a linear combination of K shared basis matrices, with task-specific weights. The factors Q₁ (state embeddings), Q₂ (action embeddings), and Q₃ (task coefficients) are learned jointly using stochastic semi-gradient updates based on sampled transitions. The method uses ε-greedy exploration and updates all factor matrices simultaneously when processing a transition from any task, allowing cross-task gradient flow.

## Key Results
- S-TLR-Q consistently outperforms baselines (independent learning and shared representation) in cumulative reward and sample efficiency
- The method achieves comparable final performance to independent learning but converges notably faster, especially in early training
- Parameter reduction from O(|S||A|M) to O((|S|+|A|+M)K) improves sample efficiency in data-scarce regimes

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Tensor Structure Implicitly Encodes Task Similarity
- Claim: Imposing a low-rank PARAFAC decomposition on the Q-tensor enables information sharing across related tasks without requiring explicit similarity specifications.
- Mechanism: The Q-functions for M tasks are collected into a 3-mode tensor Q ∈ ℝ^{|S|×|A|×M}. Under PARAFAC decomposition with rank K, each task's Q-matrix becomes a weighted combination of K shared basis matrices Q̃^k = [Q₁]_{:,k}([Q₂]_{:,k})^⊤, with task-specific weights from factor matrix Q₃. When tasks share structural similarities, a small K suffices, forcing the model to discover and exploit these latent commonalities.
- Core assumption: Related RL tasks exhibit low-dimensional structure in their joint Q-function space—i.e., their optimal value functions lie near a low-rank manifold.
- Evidence anchors:
  - [abstract] "the low-rank structure enforces the notion of similarity, without the need to explicitly prescribe which tasks are similar, but inferring this information from a reduced amount of data"
  - [Section II.A, Eq. 7] Formal decomposition showing Q̄_m = Σ_k [Q₃]_{m,k} Q̃^k, where each task is a linear combination of K shared components
  - [corpus] "Accelerating Multi-Task Temporal Difference Learning under Low-Rank Representation" explicitly studies multi-task RL where value functions lie in a low-rank subspace, supporting this assumption

### Mechanism 2: Joint Factor Optimization Enables Cross-Task Gradient Flow
- Claim: Stochastic semi-gradient updates on shared factor matrices propagate information from one task's samples to all other tasks' Q-estimates.
- Mechanism: When transition σ_t^m from task m is sampled, S-TLR-Q updates all three factor matrices (Q₁, Q₂, Q₃) via Eqs. 6a–6c. Critically, Q₁ (state embeddings) and Q₂ (action embeddings) are shared across all tasks. Thus, a gradient step using task m's data modifies the basis matrices Q̃^k that constitute every task's Q-function. Task m's learning signal automatically transfers to tasks sharing the same state-action basis structure.
- Core assumption: The semi-gradient approximation (treating the max_a Q(s', a) target as constant) remains stable under coupled multi-task updates.
- Evidence anchors:
  - [Section III, Eqs. 9a–9c] Update rules showing shared Q₁ and Q₂ are updated using single-task transitions
  - [Section III text] "the updates associated with trajectories of task m impact the value functions of all the other tasks, accelerating convergence"
  - [corpus] Weak direct evidence; related tensor-RL works focus on single-task settings

### Mechanism 3: Parameter Reduction Lowers Sample Complexity
- Claim: The low-rank constraint reduces learnable parameters from O(|S||A|M) to O((|S|+|A|+M)K), improving sample efficiency in data-scarce regimes.
- Mechanism: The full Q-tensor has |S|·|A|·M entries. Factorized form with rank K requires only |S|·K + |A|·K + M·K parameters. When K ≪ min(|S|, |A|, M), this represents a dramatic compression. Fewer parameters mean fewer samples needed to estimate them reliably.
- Core assumption: The true Q-tensor is approximately low-rank (or can be well-approximated by a low-rank tensor without significant policy degradation).
- Evidence anchors:
  - [Section II.A] Explicit parameter count reduction stated: "shifts from |S||A|M to (|S|+|A|+M)K"
  - [Section IV, Fig. 2] S-TLR-Q achieves comparable final performance to independent learning (LR-Q) but converges notably faster, especially in early training
  - [corpus] "Tensor-Efficient High-Dimensional Q-learning" similarly leverages tensor decomposition for high-dimensional RL, corroborating the general principle

## Foundational Learning

- **Q-Learning and the Bellman Optimality Equation**
  - Why needed here: S-TLR-Q is a variant of Q-learning; understanding the temporal-difference update rl + γ max_a Q(s', a) − Q(s, a) is essential to follow Eqs. 5b and 6a–6c.
  - Quick check question: Can you explain why Q-learning uses a "semi-gradient" (treating the target as constant) rather than a full gradient?

- **PARAFAC (CP) Tensor Decomposition**
  - Why needed here: The entire method hinges on representing the Q-tensor as a sum of K rank-1 outer products. Without this, the factor matrices Q₁, Q₂, Q₃ and Eq. 7 will be opaque.
  - Quick check question: Given a 3-mode tensor of shape (100, 50, 10) with rank-5 PARAFAC decomposition, how many total parameters are in the factor matrices?

- **Multi-Task Learning and Negative Transfer**
  - Why needed here: The method assumes tasks benefit from shared structure, but practitioners must recognize when this assumption fails and tasks should be learned independently.
  - Quick check question: What would you expect to happen if you applied S-TLR-Q to two tasks with completely conflicting optimal policies (e.g., "maximize speed" vs. "minimize speed")?

## Architecture Onboarding

- **Component map:**
  Q-tensor construction -> Factor matrices (Q₁, Q₂, Q₃) -> Forward pass (Eq. 7) -> Semi-gradient updater (Eqs. 6a–6c) -> ε-greedy exploration

- **Critical path:**
  1. Initialize Q₁, Q₂, Q₃ uniformly random (Algorithm 1, line 1)
  2. Sample transition σ_t^m = (s_t, a_t, r_t, s'_t) from task m
  3. Compute TD error using reconstructed Q̄_m from current factors
  4. Apply semi-gradient updates to Q₁, Q₂ via Eqs. 6a–6b (shared across tasks)
  5. Update Q₃ row for task m via Eq. 6c
  6. Repeat until convergence across all tasks

- **Design tradeoffs:**
  - **Rank K selection**: Lower K → stronger sharing, faster convergence, risk of underfitting; higher K → more expressive, but reduced sample efficiency gains. Paper uses K as a hyperparameter; no automatic rank selection is provided.
  - **Task weights λ_m**: Control relative importance of each task's loss. Default is uniform; may need adjustment for imbalanced task difficulty.
  - **Shared vs. separate representations**: C-LR-Q (fully shared) converges fast but to suboptimal solutions; LR-Q (fully separate) is optimal but sample-inefficient. S-TLR-Q sits between via low-rank factorization.

- **Failure signatures:**
  - S-TLR-Q converges but final performance plateaus below LR-Q: Likely rank K too low (underfitting). Increase K.
  - Training is unstable or diverges: Check learning rate schedules; coupled multi-task updates may require smaller η(n) than single-task Q-learning.
  - No sample efficiency gain over LR-Q: Tasks may not share low-rank structure; verify that tasks are genuinely related.

- **First 3 experiments:**
  1. **Reproduce pendulum results (Section IV)** with M=4 tasks, K=2–4. Plot cumulative reward vs. episodes for S-TLR-Q, LR-Q, and C-LR-Q to validate the claimed sample efficiency gap.
  2. **Ablation on rank K**: Run S-TLR-Q with K ∈ {1, 2, 4, 8, 16} on the pendulum setup. Identify the smallest K achieving parity with LR-Q final performance.
  3. **Negative transfer test**: Construct an adversarial multi-task setup with intentionally conflicting rewards (e.g., pendulums with opposite goal angles). Compare S-TLR-Q vs. LR-Q to characterize failure modes when low-rank assumption is violated.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity and computational load of S-TLR-Q scale when explicitly modeling higher-order tensors for multi-dimensional states and actions ($D_S + D_A + 1$) rather than collapsing them into three modes?
- Basis in paper: [explicit] Remark 1 states that while the current work uses a 3-mode tensor for simplicity, "higher-order modeling will be explored in the journal version."
- Why unresolved: The paper derives gradients and algorithms only for the 3-mode case. It remains unclear if the multilinear structure remains as computationally tractable when handling high-dimensional state/action vectors as separate modes.
- What evidence would resolve it: Derivation of update rules for higher-order tensors and experiments comparing the 3-mode approximation against the full $D_S + D_A + 1$ tensor formulation in complex environments.

### Open Question 2
- Question: Under what conditions does the S-TLR-Q algorithm guarantee convergence to a stationary point, given the non-convexity of the low-rank tensor decomposition combined with the semi-gradient update?
- Basis in paper: [inferred] Section II.B notes the optimization is complex because the decomposition is not convex, and Section III uses a semi-gradient scheme for the max operator without providing theoretical convergence proofs.
- Why unresolved: The paper relies on empirical demonstration of "good practical performance," but the interaction between stochastic semi-gradient descent and the non-convex tensor manifold is theoretically ambiguous.
- What evidence would resolve it: A formal convergence analysis proving convergence to a local optimum or stationary point, defining necessary assumptions on the learning rate and data sampling.

### Open Question 3
- Question: How robust is the S-TLR-Q method to the selection of the hyperparameter $K$ (tensor rank), and does a mis-specified rank lead to catastrophic forgetting or negative transfer?
- Basis in paper: [inferred] Section II.A identifies $K$ as the only hyperparameter controlling the degrees of freedom, but the experiments fix $K$ without analyzing sensitivity or the consequences of rank mismatch.
- Why unresolved: If the true Q-tensor rank is higher than $K$, the model may be too restrictive; if lower, it may overfit. The paper does not explore the trade-offs involved in rank selection.
- What evidence would resolve it: Ablation studies showing performance curves across a wide range of $K$ values on tasks with varying degrees of inter-task similarity.

## Limitations
- Experiments limited to relatively simple tabular environments without evaluation on continuous control or image-based tasks
- Theoretical analysis focuses on representation benefits but does not formally address negative transfer risks when tasks lack shared structure
- Several key implementation details unspecified (rank K, learning rates, exploration schedules) that may critically affect empirical results

## Confidence
- **High**: Core mathematical formulation and theoretical framework are sound and well-established
- **Medium**: Empirical claims require reproduction with unspecified hyperparameters and implementation details
- **Low**: No direct evidence on performance with high-dimensional state/action spaces or when tasks lack low-rank structure

## Next Checks
1. Implement S-TLR-Q with multiple rank K values (2, 4, 8) on the pendulum setup to identify the minimal K achieving full LR-Q performance
2. Test S-TLR-Q on an intentionally dissimilar task pair (opposite control objectives) to quantify negative transfer effects
3. Compare sample efficiency on progressively larger state/action spaces to verify the claimed parameter reduction benefits scale with problem size