---
ver: rpa2
title: 'Probing the Geometry of Truth: Consistency and Generalization of Truth Directions
  in LLMs Across Logical Transformations and Question Answering Tasks'
arxiv_id: '2506.00823'
source_url: https://arxiv.org/abs/2506.00823
tags:
- probes
- truthfulness
- truth
- probe
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the consistency and generalization of truth
  directions in large language models (LLMs) across logical transformations and question-answering
  tasks. The authors find that not all LLMs exhibit consistent truth directions, with
  stronger representations observed in more capable models, particularly in the context
  of logical negation.
---

# Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks

## Quick Facts
- arXiv ID: 2506.00823
- Source URL: https://arxiv.org/abs/2506.00823
- Reference count: 40
- Primary result: Truth directions in LLMs generalize across logical transformations and QA tasks, with stronger consistency in more capable models.

## Executive Summary
This paper investigates whether large language models encode truthfulness as a linear direction in their activation space and how consistently this representation generalizes across logical transformations and question-answering tasks. The authors find that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly when handling logical negation. Truthfulness probes trained on declarative atomic statements demonstrate effective generalization to logical transformations, question-answering tasks, in-context learning, and external knowledge sources. The study shows that simple probing techniques are sufficient to identify truth directions when they are distinctly represented within the model, advancing understanding of internal representations of LLM beliefs.

## Method Summary
The study extracts hidden states from LLMs at specified decoder layers and trains linear classifiers (SVM, logistic regression, mass-mean) to separate true/false representations using a hyperplane. The normal vector to this hyperplane corresponds to the "truth direction." Layer selection is performed via between-class/within-class variance ratio analysis. Probes are trained on 70% of factual statements from 6 curated topics and tested on held-out data, logical transformations (negation, conjunction, disjunction), and various QA tasks including MMLU, TriviaQA, and BoolQ. The methodology focuses on identifying whether truth representations are semantically encoded rather than syntactically dependent.

## Key Results
- Truth directions exist as linear features in capable LLMs and can be detected via simple probes
- Probes trained on atomic statements generalize to logical transformations, question-answering tasks, and in-context learning
- Capability scaling correlates with truth direction consistency, with Llama-3.1-70B showing consistent directions across all tested topics

## Why This Works (Mechanism)

### Mechanism 1
Truthfulness is represented as a linear direction in the activation space of capable LLMs, identifiable via simple probes. Linear classifiers separate true/false representations using a hyperplane, with the normal vector corresponding to the "truth direction." Evidence shows this structure emerges from pretraining, not probe construction, with random models showing AUROC ≈ 0.5 versus pretrained ≈ 1.0. The core assumption is that truth values map to distinct, linearly separable regions in residual stream space.

### Mechanism 2
Truth directions generalize across syntactic forms and logical transformations. Probes trained on atomic declarative statements successfully classify truthfulness in QA formats and compound logical statements, suggesting the representation encodes semantic truth rather than surface syntax. The core assumption is that the model internally represents semantic content consistently across linguistic transformations, though disjunction shows weaker generalization than conjunction.

### Mechanism 3
Consistent truth direction emergence correlates with model capability. Stronger models show consistent truth directions across all tested topics, while weaker models show domain-specific or absent representations. Capability enables stable internal representations that survive syntactic negation. The core assumption is that capability scaling improves internal representation coherence, not just output quality.

## Foundational Learning

- **Linear separability in high-dimensional spaces**: Understanding why simple linear probes can distinguish complex semantic properties requires grasping how high-dimensional representations can encode information along specific directions. Quick check: Can you explain why a hyperplane in 4096-dimensional space might separate "true" from "false" representations even when the boundary seems complex?

- **Probing classifiers**: This paper's methodology centers on training external classifiers on frozen internal activations to detect properties the model may not express behaviorally. Quick check: What does it mean when a probe achieves high accuracy but the model's outputs don't reflect the probed property?

- **Logical negation as representation test**: The paper uses negation generalization as a key test of whether models encode truth semantically vs. syntactically. Quick check: Why is generalization from "Paris is in France" (true) to "Paris is not in France" (false) a stronger test than same-syntax transfer?

## Architecture Onboarding

- **Component map**: Factual statements -> Activation extraction at optimal decoder layer -> Probe training (SVM/LR/MLP/Mass-mean) -> Generalization testing across logical transformations and QA tasks

- **Critical path**: 1) Identify optimal layer via variance ratio analysis 2) Extract activations from atomic statement datasets 3) Train probe on 70% split 4) Test generalization: negation → conjunction/disjunction → QA → contextual tasks 5) Deploy for selective QA if calibration adequate

- **Design tradeoffs**: SVM + Platt scaling achieves best calibration but requires CV overhead; MLP shows higher variance; mass-mean requires no training but underperforms on weaker models; layer depth balances semantic abstraction against noise.

- **Failure signatures**: AUROC ≈ 0.5 across topics indicates model lacks consistent truth direction; high AUROC but ECE > 0.3 indicates probe overconfidence; disjunction << conjunction performance suggests logical operator asymmetry.

- **First 3 experiments**: 1) Layer variance diagnostic: Plot between/within-class variance ratio across all layers for your target model using 2-3 topics 2) Negation generalization test: Train on affirmative statements, test on negated versions 3) Selective QA pilot: Sample 20 answers per question from TriviaQA, apply probe, filter predictions with P[Φ=1] > 0.5

## Open Questions the Paper Calls Out

- **Does causal intervention along the identified truth direction effectively steer LLM outputs toward greater truthfulness?** The causality of truthfulness probes is unclear, and it's unknown whether LLMs utilize the implicit truthfulness classification results for predictions. Experiments applying vector-based interventions could resolve this.

- **Do frontier or proprietary models (e.g., GPT-4 class) exhibit more consistent and distinct truth directions compared to the open-weight models tested?** The hypothesis that highly capable LLMs will establish consistent internal concepts of truthfulness remains untested on more advanced models. Replicating consistency experiments on state-of-the-art models would resolve this.

- **Do probes trained on atomic statements generalize effectively to complex, long-form generations or multi-step instruction following?** The current investigation was restricted to short-form QA, and the geometry of truth in short, factual statements may differ from extended narratives or complex reasoning chains. Testing on long-form hallucination detection datasets would resolve this.

## Limitations
- Reliance on 6 curated topics and specific LLM families (Llama-2/3.1) creates uncertainty about generalizability to broader domains and model architectures
- Asymmetric performance on disjunction (lower than conjunction) suggests incomplete logical reasoning
- Calibration degradation with few-shot exemplars indicates potential probe over-reliance on surface patterns rather than semantic understanding

## Confidence
- **High**: Truth directions exist as linear features in capable LLMs and can be detected via probes
- **Medium**: Generalization across logical transformations and QA formats
- **Medium**: Capability scaling correlates with truth direction consistency

## Next Checks
1. Cross-architecture validation: Test truth direction probes on GPT-4/Claude models to verify if linear separability pattern holds beyond Llama family
2. Domain expansion: Apply probes to non-factual domains (ethics, causality) to test whether truth direction geometry generalizes beyond curated fact statements
3. Probe architecture ablation: Compare linear probes against nonlinear (MLP) and geometry-aware (cone-based) methods to quantify whether linear assumptions limit detection of more complex truth representations