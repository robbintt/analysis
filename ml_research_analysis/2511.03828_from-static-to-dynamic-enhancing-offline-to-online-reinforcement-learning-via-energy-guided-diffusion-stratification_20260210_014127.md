---
ver: rpa2
title: 'From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning
  via Energy-Guided Diffusion Stratification'
arxiv_id: '2511.03828'
source_url: https://arxiv.org/abs/2511.03828
tags:
- offline
- online
- learning
- steps
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes StratDiff, a method for improving offline-to-online
  reinforcement learning by leveraging energy-guided diffusion modeling. The key idea
  is to use a diffusion model to approximate the offline behavior policy and compute
  the KL divergence between generated and actual actions to stratify samples into
  offline-like and online-like subsets.
---

# From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification

## Quick Facts
- **arXiv ID:** 2511.03828
- **Source URL:** https://arxiv.org/abs/2511.03828
- **Reference count:** 40
- **Primary result:** Achieves higher normalized scores and more stable training curves than existing offline-to-online methods (Cal-QL, IQL, EDIS) on D4RL benchmarks via energy-guided diffusion-based sample stratification.

## Executive Summary
This paper proposes StratDiff, a method that improves offline-to-online reinforcement learning by leveraging a diffusion model to approximate the offline behavior policy and stratifying samples based on their KL divergence from generated actions. The key innovation is using energy-guided diffusion sampling to compute a quality-aware alignment score, which allows the algorithm to distinguish between "offline-like" and "online-like" samples. Offline-like samples receive conservative updates to prevent distributional shift, while online-like samples receive standard TD updates to encourage exploration. Evaluated on D4RL tasks using Cal-QL and IQL backbones, StratDiff consistently outperforms baselines and shows improved stability during fine-tuning.

## Method Summary
StratDiff addresses the offline-to-online RL challenge by first pre-training a diffusion behavior model and an energy function network on the offline dataset. During online training, it uses energy-guided diffusion sampling to generate "ideal" offline actions, computes the KL divergence between these and actual actions to stratify samples into offline-like and online-like subsets, and applies distinct learning objectives: conservative offline objectives (with regularizers) to offline-like samples and exploratory online objectives (without regularizers) to online-like samples. The method is evaluated on D4RL benchmarks (MuJoCo locomotion and AntMaze) using Cal-QL and IQL as base algorithms, with hyperparameters including a fixed stratification ratio of 0.5 and task-specific guidance scales.

## Key Results
- StratDiff consistently outperforms Cal-QL, IQL, and EDIS baselines on D4RL benchmarks with higher normalized scores
- Ablation studies confirm the importance of energy guidance, showing performance drops when removed
- The method demonstrates more stable training curves during the offline-to-online transition
- Performance gains are particularly notable on tasks like hopper-medium and walker2d-medium-expert

## Why This Works (Mechanism)

### Mechanism 1: Behavior Policy Approximation via Diffusion
- Claim: A state-conditional diffusion model can more accurately approximate the action distribution of the offline behavior policy than standard offline RL policies.
- Mechanism: The model learns to denoise actions conditioned on states, effectively capturing the multimodal nature and variance of the behavior policy $\mu$ present in the static dataset $D$.
- Core assumption: The offline dataset contains sufficient samples to reconstruct the behavior policy's action distribution via generative modeling.
- Evidence anchors:
  - [Section 3.1 / Figure 2]: "Fig. 2(c) and (d) show that the diffusion model can closely reconstruct the dataset actions... the generated actions differ from those in the dataset, revealing a natural divergence [in standard policies]."
  - [Section 3.2]: "the diffusion policy is assumed to approximate the action distribution in the offline dataset."
  - [Corpus]: Related work like "FlowQ" and "Energy-Weighted Flow Matching" supports the viability of generative models for policy representation in offline RL.
- Break condition: If the offline dataset is extremely sparse or consists of disjoint, contradictory policies, the diffusion model may fail to converge to a meaningful representation.

### Mechanism 2: Energy-Guided Stratification
- Claim: Incorporating a Q-function-based energy guidance term into the diffusion process allows for the generation of high-value "offline-like" actions, enabling effective sample stratification via KL divergence.
- Mechanism: The energy function $E_t(s, a_t)$ modifies the sampling distribution to favor actions with higher Q-values (Eq. 8). The KL divergence between this "ideal" offline action and the actual sample action measures the sample's deviation from optimal offline behavior.
- Core assumption: The Q-function provides a reliable energy signal that correlates with action quality within the offline data manifold.
- Evidence anchors:
  - [Abstract]: "StratDiff... refines this knowledge through energy-based functions to improve policy imitation... The KL divergence... is used to stratify the training batch."
  - [Figure 4 / Section 4.3]: "Ablation results... showing the performance drop when removing energy function... confirms that simply relying on behavioral similarity is insufficient."
  - [Corpus]: "Energy-Guided Diffusion Sampling" (Liu et al., 2024) is cited as a baseline, reinforcing the utility of energy guidance in this domain.
- Break condition: If the Q-function is poorly calibrated or the guidance scale $\beta$ is set incorrectly (too high/low), the generated actions may be out-of-distribution or suboptimal, leading to erroneous stratification.

### Mechanism 3: Distinct Objective Optimization
- Claim: Applying conservative objectives to "offline-like" samples and exploratory objectives to "online-like" samples stabilizes the offline-to-online transition.
- Mechanism: Samples close to the offline distribution (low KL) are updated with constraints (e.g., Cal-QL regularizer) to prevent divergence. Samples far from the offline distribution (high KL) are updated with standard TD learning to encourage exploration and value maximization.
- Core assumption: Samples with high KL divergence indicate regions where the policy has legitimately explored beyond the offline data and should be treated with standard online updates.
- Evidence anchors:
  - [Section 3.3 / Eq. 11]: "Offline-like samples in $b_{off}$, we apply the original Cal-QL loss with the conservative regularizer. For online-like samples in $b_{on}$, we drop the regularizer..."
  - [Table 1]: StratDiff consistently outperforms base methods (Cal-QL, IQL) and EDIS on D4RL benchmarks, suggesting the stratified updates are effective.
  - [Corpus]: "Behavior-Adaptive Q-Learning" explores similar themes of adapting Q-learning based on behavioral alignment.
- Break condition: If the stratification ratio $\rho$ is fixed (as in this work, $\rho=0.5$) but the optimal ratio shifts dynamically during training, the model may apply the wrong objective to a significant portion of the batch.

## Foundational Learning

- Concept: **Diffusion Models for Policy Representation**
  - Why needed here: The core of StratDiff relies on using a diffusion model not just to generate actions, but to define a "reference distribution" for the offline policy.
  - Quick check question: Can you explain how a denoising score matching objective allows a model to learn a data distribution $p(a|s)$?

- Concept: **Energy-Based Models (EBMs) and Guidance**
  - Why needed here: The method uses an energy function (derived from Q-values) to bias the diffusion sampling. Understanding how $p(x) \propto e^{-E(x)}$ works is crucial for Section 3.2.
  - Quick check question: How does adding a gradient term $\nabla E(x)$ to the score function $\nabla \log p(x)$ change the sampling distribution?

- Concept: **Distributional Shift in Offline-to-Online RL**
  - Why needed here: The problem StratDiff solves is the mismatch between the static offline data and the evolving online policy.
  - Quick check question: Why does standard off-policy RL often fail or perform poorly when fine-tuning directly from a static offline dataset?

## Architecture Onboarding

- Component map: Diffusion Behavior Model ($\epsilon_\theta$) -> Energy Network ($f_\phi$) -> Base RL Agent (Cal-QL/IQL) -> Stratifier
- Critical path:
  1.  **Pre-training:** Train Base RL Agent and Diffusion Behavior Model on offline dataset $D$.
  2.  **Energy Training:** Train Energy Network $f_\phi$ using contrastive loss (Eq. 7).
  3.  **Online Loop:**
      - Collect online data into $D_{online}$.
      - Sample batch $b$ from $D_{offline} \cup D_{online}$.
      - **Stratify:** Generate actions $\hat{a}$ using energy-guided diffusion; compute KL vs actual $a$; split batch.
      - **Update:** Apply distinct losses to $b_{off}$ and $b_{on}$.
- Design tradeoffs:
  - **Fixed Ratio $\rho$:** Setting $\rho=0.5$ is simple but may be suboptimal if the online phase rapidly generates high-quality data that should dominate training.
  - **Guidance Scale:** Requires careful tuning; the paper uses empirically derived values (e.g., 3.0 to 10.0) which adds a hyperparameter burden.
  - **UTD Ratio:** High UTD (e.g., 10) benefits StratDiff significantly (Appendix D), suggesting higher computational cost for best performance.
- Failure signatures:
  - **Performance Collapse:** If the energy guidance scale is too high, the generated actions may become "adversarial" or unrealistic, causing the stratification logic to misclassify all data as "online-like" (or vice versa).
  - **Stagnation:** If the diffusion model overfits to a poor offline policy, it may force the agent to remain overly conservative, preventing online improvement.
- First 3 experiments:
  1.  **Benchmark Reproduction:** Run StratDiff (Cal-QL backbone) on `halfcheetah-medium-v2` and compare the normalized score against vanilla Cal-QL to verify the ~8-10 point gain claimed in Table 1.
  2.  **Ablation on Energy:** Disable the energy network (use pure diffusion sampling for stratification) to replicate the performance drop shown in Figure 4. This validates the "quality alignment" hypothesis.
  3.  **Stratification Dynamic Analysis:** Track the "Exchange Count" (Appendix F) to see if the model naturally reduces the exchange of offline/online samples over time, which indicates the policy is successfully moving away from the offline distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does StratDiff perform when the offline dataset is composed of trajectories from multiple behavior policies with significantly divergent optimality levels?
- Basis in paper: [explicit] The authors state in the Limitations section that "the diffusion model may fail to capture a consistent behavioral pattern" in scenarios with multi-policy datasets.
- Why unresolved: The current experiments utilize D4RL benchmarks which generally represent specific policy qualities (e.g., medium, expert, or mixed-replay) but do not explicitly test for severe inconsistency or multi-modality in the behavior policy itself, which could destabilize the stratification logic.
- What evidence would resolve it: Empirical evaluations on specifically constructed heterogeneous datasets (e.g., a 50/50 mix of random and expert data) demonstrating whether the energy-guided diffusion can successfully stratify samples without conflating distinct behavioral modes.

### Open Question 2
- Question: Can the fixed stratification ratio ($\rho=0.5$) be replaced with an adaptive mechanism to improve robustness without requiring manual tuning?
- Basis in paper: [explicit] The paper notes that the "stratification ratio $\rho$... is fixed to 0.5 in our experiments" and acknowledges that "it may need adjustment in different environments... which could impact the robustness."
- Why unresolved: A static ratio assumes the volume of offline-like data remains constant or equally relevant throughout training, which may not hold as the policy drifts further from the offline distribution or if the replay buffer composition changes dynamically.
- What evidence would resolve it: An ablation study introducing a dynamic threshold or a schedule for $\rho$ (e.g., based on running uncertainty or KL divergence statistics) that outperforms the fixed ratio across all standard benchmarks.

### Open Question 3
- Question: How sensitive is the performance to the specific choice of the guidance scale, and can this hyperparameter be automated to ensure stability?
- Basis in paper: [explicit] The authors identify that "the energy-guided sampling process is sensitive to the choice of the guidance scale" and note that "improper settings... can significantly affect performance."
- Why unresolved: While the paper uses empirical settings from prior work (CEP), there is no analysis of how deviations from these scales impact the "offline-like" vs. "online-like" classification accuracy or the resulting stability of the fine-tuning process.
- What evidence would resolve it: A sensitivity analysis showing performance variance across a range of guidance scales, or the introduction of a theoretical framework to automatically select the scale based on the gradient norms of the energy function.

## Limitations
- The fixed stratification ratio $\rho=0.5$ may be suboptimal as the online policy evolves, potentially applying conservative updates to genuinely novel, high-value states
- The method is sensitive to the quality of the offline dataset and the calibration of the guidance scale $s$, with poor choices leading to out-of-distribution actions or overly conservative policies
- The implementation of the KL divergence metric for stratification is ambiguous, creating potential implementation risk

## Confidence

- **High Confidence:** The general framework of using diffusion models for behavior policy approximation and the theoretical justification for energy-guided sampling are well-supported by the literature and experimental results.
- **Medium Confidence:** The specific implementation details of the KL divergence calculation and the fixed $\rho=0.5$ ratio are inferred from context but lack explicit specification, introducing implementation risk.
- **Low Confidence:** The dynamic behavior of the stratification process over long training horizons and the method's robustness to extremely poor offline datasets are not thoroughly characterized.

## Next Checks

1. **Implement a Controlled Ablation:** Run StratDiff (Cal-QL) on `halfcheetah-medium-v2` with the energy network disabled to verify the performance drop shown in Figure 4, confirming the energy guidance's contribution.
2. **Analyze the KL Metric Implementation:** Track and visualize the distribution of alignment scores and the resulting offline/online split ratio over training to ensure the stratification logic is functioning as intended and not biased by the KL calculation method.
3. **Stress-Test the Guidance Scale:** Systematically vary the energy guidance scale $s$ on a single task (e.g., hopper-medium) to find the threshold where performance collapses, providing bounds for hyperparameter tuning and validating the sensitivity warnings.