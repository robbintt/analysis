---
ver: rpa2
title: Investigating LLM Capabilities on Long Context Comprehension for Medical Question
  Answering
arxiv_id: '2510.18691'
source_url: https://arxiv.org/abs/2510.18691
tags:
- context
- arxiv
- medical
- performance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first comprehensive evaluation of LLM capabilities
  for long-context medical question answering across three task formulations (MCQA,
  extractive, open-ended) using human-validated datasets derived from MIMIC-III and
  MIMIC-IV electronic health records. It systematically compares full-context prompting
  against retrieval-augmented generation (RAG) across varying note inclusion strategies,
  model sizes, and reasoning capabilities.
---

# Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering

## Quick Facts
- arXiv ID: 2510.18691
- Source URL: https://arxiv.org/abs/2510.18691
- Authors: Feras AlMannaa; Talia Tseriotou; Jenny Chim; Maria Liakata
- Reference count: 40
- Key outcome: First comprehensive evaluation of LLM long-context medical QA across three task formulations, showing hybrid RAG with dense+sparse retrieval outperforms full-context prompting, with performance degrading as context length increases

## Executive Summary
This work presents the first comprehensive evaluation of LLM capabilities for long-context medical question answering across three task formulations (MCQA, extractive, open-ended) using human-validated datasets derived from MIMIC-III and MIMIC-IV electronic health records. It systematically compares full-context prompting against retrieval-augmented generation (RAG) across varying note inclusion strategies, model sizes, and reasoning capabilities. The study introduces a hybrid RAG pipeline combining dense and sparse retrieval with late-interaction reranking, demonstrating its superiority over full-context approaches particularly for single-document tasks and specific fact retrieval. While larger models show improved performance especially on reasoning-intensive multi-note tasks, all models exhibit significant performance degradation with increased context length.

## Method Summary
The study evaluates LLM performance on medical question answering using hybrid RAG (BM25 + Qwen3-Embedding-8B + Reason-ModernColBERT reranking with RRF fusion) versus full-context prompting across four inclusion settings (Exclude All, Exclude Relevant, Include All, Include Related) and four context bins (0-8K, 8-16K, 16-32K, 32-128K tokens). Models tested include Qwen2.5-7B/32B-Instruct, QwQ:32B, and HuatuoGPT-o1-7B on four datasets (CliniQG4QA, RadQA, EHR-DS-QA, EHRNoteQA). Evaluation uses METEOR, Clinical BioBERT BERTScore, domain-adapted NLI, and LLM-as-a-judge (Correctness/Completeness/Faithfulness), with 512-token chunks and FP8 quantization on H100 GPUs.

## Key Results
- Context length increase degrades LLM performance across all models and metrics, with significant drops at 8-16K and 16-32K ranges
- Hybrid RAG pipeline with dense+sparse retrieval and late-interaction reranking outperforms full-context approaches, especially for single-document tasks
- Larger models (32B) show improved performance on reasoning-intensive multi-note tasks, but 7B models exhibit less memorization in exclude settings
- RAG excels at specific fact retrieval (66.7% accuracy) while full-context prompting performs better for temporal reasoning (70.0% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval (dense + sparse) with late-interaction reranking improves medical QA over single-strategy retrieval
- Mechanism: Dense embeddings capture semantic similarity while sparse lexical matching (BM25) catches domain-specific terminology and exact phrase matches. Reciprocal Rank Fusion combines ranked lists, and late-interaction reranking (MaxSim) ensures query tokens are supported by document tokens at fine granularity.
- Core assumption: Medical QA queries contain both semantically-rich and lexically-precise components that benefit from complementary retrieval strategies.
- Evidence anchors: [abstract] "hybrid RAG pipeline combining dense and sparse retrieval with late-interaction reranking, demonstrating its superiority over full-context approaches particularly for single-document tasks and specific fact retrieval"; [section 3.2] Hybrid approach with RRF fusion and MaxSim reranking; Table 3 shows RAG HIR outperforming Include All for most settings.

### Mechanism 2
- Claim: Context length increase degrades LLM performance even with extended context windows
- Mechanism: Attention dispersion across more tokens reduces signal-to-noise for relevant information. Models struggle with long-range dependencies requiring multi-hop reasoning across temporally-separated notes.
- Core assumption: Attention mechanisms don't scale proportionally with context size; relevant evidence may be "lost in the middle."
- Evidence anchors: [abstract] "all models exhibit significant performance degradation with increased context length"; [section 5.1, Figure 3] Performance decreases across metrics with increased context size on open-ended QA, particularly for multi-note reasoning (EHRNoteQA).

### Mechanism 3
- Claim: RAG outperforms full-context for specific fact retrieval while full-context excels at holistic document understanding
- Mechanism: RAG isolates relevant chunks, reducing distractors for targeted queries. Full-context preserves document structure and cross-sectional relationships needed for synthesis tasks (discharge diagnoses, temporal sequences).
- Core assumption: Task type determines optimal context delivery; facts benefit from narrowing, synthesis from breadth.
- Evidence anchors: [abstract] "RAG excels at specific fact retrieval and synthesizing scattered information, while full-context prompting performs better for questions requiring holistic document understanding"; [section 5.2, Table 5] RAG favored for Specific Entity Extraction (66.7%), FC favored for Temporal & Longitudinal Reasoning (70.0%).

## Foundational Learning

- Concept: **Reciprocal Rank Fusion (RRF)**
  - Why needed here: Combines ranked outputs from heterogeneous retrievers (dense semantic, sparse lexical) without requiring score normalization across different similarity scales.
  - Quick check question: Given rank positions [1, 3] and [2, 5] from two retrievers with k_RRF=60, which document has higher combined RRF score?

- Concept: **Late Interaction (ColBERT-style MaxSim)**
  - Why needed here: Standard retrieval treats queries and documents as single vectors; late interaction preserves token-level granularity to ensure each query term finds supporting evidence.
  - Quick check question: Why does cosine similarity between whole-query and whole-document embeddings fail for multi-part medical questions?

- Concept: **Context Length Scaling vs. Reasoning Depth**
  - Why needed here: Extended context windows (128K+) don't guarantee proportional reasoning capability; understanding why helps set realistic expectations for longitudinal EHR processing.
  - Quick check question: A model with 128K context window processes 100K tokens of patient notes. What two factors likely cause performance degradation despite sufficient capacity?

## Architecture Onboarding

- Component map:
  Document Store -> Sparse Retriever (BM25) -> Dense Retriever (Qwen3-Embedding-8B) -> Fusion Layer (RRF) -> Reranker (Reason-ModernColBERT) -> LLM Generator (Qwen2.5-32B or QwQ:32B) -> Evaluator (Multi-metric)

- Critical path:
  1. Chunk notes → 2. Index (sparse + dense) → 3. Query both retrievers → 4. RRF fusion → 5. Rerank top-k → 6. Temporal ordering → 7. LLM generation

- Design tradeoffs:
  - **Chunk size (512 vs 1024)**: Smaller chunks improve retrieval precision but fragment cross-sentence context; larger chunks preserve context but introduce noise
  - **RAG vs FC routing**: No clear automatic discriminator; Table 5 shows task-category heuristics (SEE→RAG, TLR→FC) but requires query classification
  - **Model size**: 32B models outperform 7B on reasoning tasks but 7B shows less memorization in exclude settings

- Failure signatures:
  - **Temporal inversion**: Retrieved chunks reordered by score instead of timestamp; answers report events out of clinical sequence
  - **Missing negative findings**: RAG retrieves only positive matches; "absence of complications" not captured if explicitly stated
  - **Mid-range context dip**: 8-16K contexts show performance drops (Figure 3), likely from data noise or ambiguity amplification

- First 3 experiments:
  1. **Establish baseline**: Run Include All (full context) on EHRNoteQA with Qwen2.5-32B across all context bins (0-8K, 8-16K, 16-32K, 32-128K); plot degradation curve
  2. **RAG ablation**: Compare RAG-5, RAG-10, RAG-15 chunks on single-note task (EHR-DS-QA) vs multi-note task (EHRNoteQA); identify optimal k per task type
  3. **Retrieval strategy comparison**: Ablate dense-only, sparse-only, hybrid (no rerank), hybrid+rerank on 50-sample subset; measure LLM Correctness and NLI Entailment delta

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation metrics (LLM-as-a-judge, NLI-based faithfulness) show substantial inconsistencies across models and tasks, raising questions about reliability for medical QA
- Chunk-based retrieval inherently fragments cross-sentence medical concepts, potentially breaking semantic continuity critical for understanding medical narratives
- Temporal reasoning performance remains particularly weak across all models, with current evaluation methods struggling to capture clinical significance of temporal ordering

## Confidence

**High Confidence Claims:**
- Context length negatively impacts LLM performance across all evaluated models
- RAG outperforms full-context approaches for specific fact retrieval tasks
- Hybrid retrieval combining dense and sparse methods with late-interaction reranking provides superior performance

**Medium Confidence Claims:**
- Task-specific routing (RAG for SEE, full-context for TLR) provides optimal performance
- Larger models show meaningful improvements on reasoning-intensive multi-note tasks
- The hybrid RAG pipeline is superior to single-strategy retrieval

**Low Confidence Claims:**
- The proposed hybrid RAG pipeline represents the optimal architecture for medical QA
- Current evaluation metrics adequately capture medical QA quality
- Model performance generalizes across diverse medical domains

## Next Checks

1. **Metric Validation Study**: Conduct inter-annotator agreement analysis on a subset of 100 samples using human experts to establish ground truth for Correctness, Completeness, and Faithfulness. Compare LLM-as-a-judge scores against expert consensus to quantify reliability and identify systematic biases in current evaluation framework.

2. **Temporal Reasoning Benchmark**: Develop a controlled evaluation set specifically targeting temporal reasoning capabilities by creating synthetic patient records with clear chronological dependencies. Test whether chunk-based retrieval inherently breaks temporal reasoning or if specific preprocessing can preserve temporal coherence.

3. **Cross-Domain Generalization Test**: Apply the hybrid RAG pipeline to medical QA datasets from different domains (e.g., radiology reports from different institutions, outpatient clinical notes) to assess whether the retrieval strategy and performance patterns generalize beyond MIMIC critical care data, particularly examining how domain shift affects dense embedding effectiveness.