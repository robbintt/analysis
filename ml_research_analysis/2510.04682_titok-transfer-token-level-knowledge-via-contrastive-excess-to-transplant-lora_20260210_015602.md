---
ver: rpa2
title: 'TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant
  LoRA'
arxiv_id: '2510.04682'
source_url: https://arxiv.org/abs/2510.04682
tags:
- data
- source
- tasks
- transfer
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of transferring LoRA adapters\
  \ between different large language models (LLMs), which is currently difficult because\
  \ LoRA parameters are tied to their original model backbones. The proposed TiTok\
  \ framework solves this by transferring LoRA knowledge at the token level through\
  \ contrastive excess scores\u2014measuring the difference between a source model\
  \ with and without LoRA to identify the most informative tokens."
---

# TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA

## Quick Facts
- arXiv ID: 2510.04682
- Source URL: https://arxiv.org/abs/2510.04682
- Authors: Chanjoo Jung; Jaehyung Kim
- Reference count: 34
- Key outcome: TiTok achieves +7.96% improvement over vanilla models, +6.0% over knowledge distillation, and +4.4% over TransLoRA for LoRA adapter transfer across diverse model scenarios

## Executive Summary
TiTok addresses the challenge of transferring LoRA adapters between different large language models by transferring knowledge at the token level through contrastive excess scores. The framework measures the difference between a source model with and without LoRA to identify the most informative tokens, then performs selective training on synthetic data generated by the source expert model. This approach eliminates the need for additional models or extensive training overhead while maintaining effectiveness across multiple transfer settings including same model, cross-family, cross-size, and cross-version scenarios.

## Method Summary
The TiTok framework transfers LoRA adapters by first computing contrastive excess scores between a source model (M_s) and its LoRA-equipped version (M_s + A_s) on generated synthetic data. This score identifies tokens where the LoRA adapter injects task-specific knowledge by measuring the knowledge discrepancy between the amateur and expert versions. The method then trains the target model on synthetic data, but only for tokens with high excess scores, making the transfer process both efficient and selective. The approach works across various transfer scenarios including same-model adaptation, cross-family transfers (different architectures), cross-size transfers (different model capacities), and cross-version transfers (different model versions), demonstrating consistent performance improvements over existing baselines.

## Key Results
- TiTok achieves average performance gains of +4-8% compared to baselines across all transfer scenarios
- Demonstrates +7.96% improvement over vanilla models, +6.0% over knowledge distillation, and +4.4% over TransLoRA
- Shows consistent effectiveness even when using external data from different domains
- Maintains robust performance across reasoning tasks, personalization tasks, and generation tasks

## Why This Works (Mechanism)
TiTok leverages the insight that LoRA adapters encode task-specific knowledge that manifests as systematic differences in model predictions at the token level. By contrasting outputs from a base model versus a LoRA-equipped model, the framework identifies tokens where the adapter contributes meaningful information. This contrastive excess score serves as a proxy for knowledge importance, allowing selective transfer of only the most relevant tokens. The synthetic data generation approach ensures that the target model receives high-quality supervision signals without requiring access to the original training data or additional model architectures.

## Foundational Learning
- **LoRA adapters**: Parameter-efficient fine-tuning method using low-rank decomposition to adapt large models with minimal parameter changes. Why needed: TiTok builds on LoRA's efficiency premise but extends it to cross-model transfer. Quick check: Verify understanding of rank decomposition and adapter architecture.
- **Contrastive excess scoring**: Token-level difference measurement between base and LoRA-enhanced model predictions. Why needed: Forms the core mechanism for identifying transferable knowledge. Quick check: Understand how score differences translate to knowledge importance.
- **Synthetic data generation**: Creating training data by prompting source expert models to produce task-relevant outputs. Why needed: Enables transfer without original training data access. Quick check: Recognize the role of synthetic data in knowledge distillation.
- **Token-level transfer**: Selective training on individual tokens rather than full sequences or model parameters. Why needed: Improves efficiency by focusing on high-value knowledge. Quick check: Grasp the efficiency benefits of selective token training.
- **Knowledge transplantation**: Moving learned capabilities between models without full retraining. Why needed: Addresses the practical challenge of adapter reusability across models. Quick check: Understand the distinction between transplantation vs. fine-tuning.

## Architecture Onboarding

**Component Map**: Source model (M_s) -> Synthetic data generation -> Contrastive excess scoring -> Token selection (k% threshold) -> Target model training

**Critical Path**: The most time-sensitive components are synthetic data generation and contrastive excess scoring, as they must complete before token selection and target model training can begin. The excess scoring depends entirely on having generated synthetic data from the source model.

**Design Tradeoffs**: The framework balances between comprehensive knowledge transfer (high k% selection) and efficiency (low k% selection). Higher k% captures more knowledge but requires more training resources, while lower k% improves efficiency but risks missing important information. The fixed threshold approach provides stability but lacks adaptability to different task characteristics.

**Failure Signatures**: Poor performance may result from incorrectly calibrated excess scores (failing to identify truly important tokens), low-quality synthetic data from the source model, or inappropriate k% thresholds for the task type. Cross-family transfers may show reduced effectiveness due to architectural differences that affect excess score reliability.

**First Experiments**:
1. Same-model LoRA transfer: Apply TiTok within the same model family to establish baseline effectiveness
2. Cross-family transfer test: Transfer between different model architectures (e.g., Llama to Mistral) to evaluate architectural compatibility
3. Cross-size transfer validation: Transfer from smaller to larger models to test scalability of the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive or data-driven token selection thresholds improve TiTok's efficiency beyond the fixed k% ratio used currently?
- Basis in paper: The authors state in the Limitations section: "TITOK currently applies a fixed threshold to determine which tokens are retained. While this simple design is effective and stable across tasks, future work could explore more adaptive or data-driven thresholding strategies to further enhance efficiency without compromising robustness."
- Why unresolved: The current approach manually sets k% (typically 70%, but 30% for some weak-to-strong transfers), requiring task-specific tuning without a principled automatic mechanism.
- What evidence would resolve it: Experiments comparing fixed k% against learned or adaptive thresholding strategies across multiple transfer settings, showing whether automated approaches can match or exceed hand-tuned values while reducing manual tuning overhead.

### Open Question 2
- Question: Why does the optimal token selection ratio differ between reasoning tasks (where lower k% helps weak-to-strong transfer) and personalization tasks (where higher k% is better)?
- Basis in paper: Figure 3 shows that for BBH under weak-to-strong transfer (Llama2 7B → Llama3 8B), lower k% improves performance by filtering noisy regions, while News Headline generation under the same transfer benefits from higher k%. The authors suggest personalization tasks "benefit more broadly from the source model's outputs, even when the source model is relatively weaker."
- Why unresolved: The underlying mechanism explaining this task-dependent behavior is not fully characterized—whether it relates to the nature of knowledge (lexical/stylistic vs. reasoning patterns) or task-specific tolerance for noisy supervision remains unclear.
- What evidence would resolve it: Systematic analysis across a broader range of task types (reasoning, generation, classification) with controlled experiments varying source-target capability gaps, combined with probing studies on what knowledge gets transferred at different k% values.

### Open Question 3
- Question: How does TiTok scale to much larger models (e.g., 70B+ parameters) and does the contrastive excess signal remain discriminative as model capacity increases?
- Basis in paper: All experiments use 3B-8B models (Mistral-7B, Llama3-8B, Llama3-3B, Llama2-7B). The method's reliance on detecting token-level differences between base and LoRA-enhanced models may behave differently when base models already possess stronger capabilities.
- Why unresolved: The contrastive excess assumes a meaningful gap between amateur (M_s) and expert (M_s + A_s) predictions; with larger base models, this gap could diminish or change in character.
- What evidence would resolve it: Experiments transferring LoRA adapters on larger model families (e.g., Llama-70B, Mixtral-8x7B) with analysis of excess score distributions and transfer effectiveness compared to smaller-scale results.

### Open Question 4
- Question: What are the theoretical guarantees or failure modes of the contrastive excess measure in identifying truly task-relevant tokens?
- Basis in paper: Equation 2 defines excess score as S(y_i) = L_e(y_i) - L_a(y_i), which "quantifies the knowledge discrepancy incurred by equipment of LoRA." The paper relies on the intuition that "tokens with higher S(y_i) correspond to positions where the LoRA adapter injects task-specific knowledge," but does not formally characterize when this assumption might fail.
- Why unresolved: High excess scores could potentially arise from spurious correlations, dataset artifacts, or LoRA overfitting rather than genuine task knowledge, which could mislead the filtering mechanism.
- What evidence would resolve it: Probing experiments correlating excess scores with ground-truth token importance metrics (e.g., influence functions, gradient-based attribution), plus failure case analysis where high-excess tokens lead to negative transfer.

## Limitations
- The method assumes a meaningful gap between base and LoRA-enhanced model predictions, which may diminish with larger, more capable base models
- Performance degrades in more distant cross-family transfers where architectural similarities are limited
- The synthetic data generation process depends on source model quality and may propagate errors or biases
- Computational overhead, while reduced, is not negligible and may limit scalability for very large models

## Confidence
- **Major Claim Cluster 1: TiTok effectively transfers LoRA knowledge across different LLM architectures** - Medium Confidence. Experimental results show consistent improvements, but evaluation focuses on specific model pairs and tasks.
- **Major Claim Cluster 2: Token-level contrastive excess scoring identifies optimal training data** - Medium Confidence. Methodology shows theoretical soundness and empirical support, but sensitivity to different thresholds remains unexplored.
- **Major Claim Cluster 3: TiTok outperforms existing LoRA transfer methods (TransLoRA, knowledge distillation)** - High Confidence. Comparative experiments across multiple settings provide robust evidence for performance improvements.

## Next Checks
1. **Cross-task generalization study**: Evaluate TiTok's effectiveness across a broader range of NLP tasks beyond those tested, including long-form generation, structured output tasks, and multimodal applications to assess domain transferability.

2. **Ablation analysis of contrastive excess scoring**: Systematically vary the token selection threshold and scoring methodology to determine sensitivity and identify optimal parameters across different model pairs and task types.

3. **Resource efficiency benchmarking**: Conduct detailed measurements of computational overhead, including memory usage during contrastive excess calculation and synthetic data generation, compared to alternative approaches across varying model scales.