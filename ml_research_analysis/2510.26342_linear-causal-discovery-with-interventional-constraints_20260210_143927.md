---
ver: rpa2
title: Linear Causal Discovery with Interventional Constraints
arxiv_id: '2510.26342'
source_url: https://arxiv.org/abs/2510.26342
tags:
- causal
- constraints
- interventional
- discovery
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces interventional constraints as a novel way
  to incorporate high-level causal knowledge into causal discovery. Unlike traditional
  path constraints that only regulate causal structures, interventional constraints
  enforce inequality conditions on total causal effects between variable pairs.
---

# Linear Causal Discovery with Interventional Constraints

## Quick Facts
- **arXiv ID:** 2510.26342
- **Source URL:** https://arxiv.org/abs/2510.26342
- **Reference count:** 40
- **Primary result:** Interventional constraints improve causal structure recovery and sign consistency over path constraints in linear SEMs.

## Executive Summary
This paper introduces interventional constraints as a novel approach to incorporate high-level causal knowledge into causal discovery. Unlike traditional path constraints that only regulate causal structures, interventional constraints enforce inequality conditions on total causal effects between variable pairs. The authors propose a metric to quantify total causal effects in linear models and formulate the discovery problem as constrained optimization, solved via a two-stage approach combining L-BFGS-B and SLSQP. Experiments on synthetic and real-world data (including the Sachs dataset) show that interventional constraints outperform path constraints, achieving better Structural Hamming Distance (SHD), Structural Intervention Distance (SID), and sign consistency metrics. Incorporating interventional constraints not only improves model accuracy and consistency with known causal effects but also facilitates the discovery of new causal relationships.

## Method Summary
The method learns a weighted DAG from observational data while enforcing inequality constraints on total causal effects between specified variable pairs. It minimizes an objective combining data fit and sparsity subject to acyclicity and interventional constraints. The total causal effect matrix T = (I-W)^{-1} - I captures both direct and indirect effects. Optimization is performed in two stages: L-BFGS-B first finds a feasible acyclic initialization, then SLSQP refines it to satisfy interventional constraints. Post-processing with threshold ω=0.3 checks if sparsification violates constraints, triggering dynamic adjustment of constraint bounds.

## Key Results
- Interventional constraints outperform path constraints on SHD, SID, and sign consistency metrics
- The method achieves better alignment with known causal effects in the Sachs dataset
- Incorporating interventional constraints enables discovery of new causal relationships beyond purely observational approaches

## Why This Works (Mechanism)

### Mechanism 1: Total Effect Regularization via Inequality Constraints
The method computes a Total Causal Effect matrix T = (I - W)^{-1} - I, which captures both direct and indirect effects. It enforces constraints of the form δ_{ij}(T_{ij} - δ_{ij}) > 0, forcing the optimization to find edge weights W such that the aggregated path effects respect the known causal sign (e.g., positive activation), even if it requires adjusting multiple indirect edges to satisfy the sum.

### Mechanism 2: Two-Stage Decoupling of Acyclicity and Causal Effects
Stage 1 uses L-BFGS-B to satisfy the acyclicity constraint (h(W)=0) and minimize loss, ignoring interventional constraints. This provides a feasible starting point W^(1). Stage 2 uses SLSQP to refine W^(1) to satisfy the interventional inequalities.

### Mechanism 3: Post-Thresholding Constraint Repair
After optimization, small weights are set to zero (W* = W_{est} ◦ 1_{|W_{est}|>ω}). This might sever a causal path, changing the sign of the total effect T. The algorithm detects this violation and increases the threshold δ_{ij} to force the optimizer to retain stronger connections in the next iteration.

## Foundational Learning

- **Concept:** Linear Structural Equation Models (SEM) & Total Effects
  - **Why needed here:** The method relies on the matrix identity T = (I - W)^{-1} - I to calculate effects. Without understanding that matrix inversion captures infinite path summation in linear systems, the "Total Effect" metric appears arbitrary.
  - **Quick check question:** If X → Y has weight 0.5 and Y → Z has weight 0.5, what is the total effect of X on Z? (Answer: 0.25).

- **Concept:** Constrained Optimization (KKT Conditions)
  - **Why needed here:** The paper uses SLSQP and mentions KKT points. Understanding that constraints like "Sign must be positive" define a "feasible set" in the loss landscape is crucial for debugging why the optimizer fails.
  - **Quick check question:** Why does adding a non-convex constraint (like the inverse of W) make finding the global minimum harder? (Answer: It creates local minima where the gradient is zero but the solution is suboptimal).

- **Concept:** Structural vs. Parametric Identifiability
  - **Why needed here:** Standard causal discovery often finds "Markov Equivalence Classes" (skeletons). This method claims to resolve directions using parametric signs. You need to distinguish between finding an edge (structure) and finding its weight/sign (parameter).
  - **Quick check question:** Can two different causal graphs produce the same observational data correlations? (Answer: Yes, if they are in the same Markov Equivalence Class).

## Architecture Onboarding

- **Component map:** Data Input X ∈ R^{n×d} -> Stage 1 (L-BFGS-B) -> W^(1) -> Stage 2 (SLSQP) -> W_{est} -> Thresholding & Repair -> Final W*

- **Critical path:** The definition of the Constraint Set (I). If the user defines a causal constraint that is physically impossible given the data, Stage 2 will struggle to converge.

- **Design tradeoffs:**
  - Accuracy vs. Speed: Stage 2 is significantly slower (O(m·d³) vs Stage 1)
  - Sparsity vs. Constraints: Enforcing a causal path for a weak signal forces the model to be less sparse, potentially overfitting noise

- **Failure signatures:**
  - The "Flip-Flop" Loop: The optimizer satisfies a constraint, thresholding removes the edge, the constraint is violated, optimization adds it back
  - Ghost Edges: The model adds strong direct edges to satisfy a total effect constraint, ignoring the true indirect path mechanism

- **First 3 experiments:**
  1. Sanity Check (Synthetic): Generate data from a known linear DAG. Run NOTEARS (Stage 1) vs. Lin-CDIC. Verify that if NOTEARS learns a negative weight for a positive causal effect, Lin-CDIC corrects the sign.
  2. Robustness to Noise (Sachs-like): Take the Sachs dataset. Inject incorrect constraints (e.g., claim "PIP3 inhibits Akt"). Observe if the model structure collapses or if it simply forces that specific edge negative while maintaining the rest.
  3. Threshold Sensitivity (ε): Run the optimization with ε ∈ {0.25, 0.5, 0.75, 1.0}. Verify if larger ε (aggressive constraint tightening) leads to denser graphs with potentially more False Positives.

## Open Questions the Paper Calls Out

1. **Question:** How can interventional constraints be formalized and optimized for nonlinear causal models?
   - **Basis in paper:** [explicit] The authors identify extending the method to nonlinear settings as a primary direction for future research in the Discussion and the Introduction.
   - **Why unresolved:** The current metric for total causal effect, T=(I-W)^{-1}-I, relies on linear matrix properties. Nonlinear settings require complex parameterizations and path-specific derivative calculations.
   - **What evidence would resolve it:** A formulation where total causal effects are estimated via interventional distributions or path-specific derivatives within a differentiable nonlinear architecture.

2. **Question:** Can interventional constraints be integrated into cyclic Structural Causal Models (SCMs)?
   - **Basis in paper:** [explicit] The Discussion identifies incorporating constraints into cyclic SCMs to handle dynamic systems with feedback loops as a future direction.
   - **Why unresolved:** The current optimization relies on the strict acyclicity constraint h(W) = 0. Real-world biological systems often contain feedback loops, which this assumption prohibits the model from capturing.
   - **What evidence would resolve it:** A modified optimization framework that relaxes the DAG constraint while retaining interventional inequalities, validated on data generated from cyclic processes.

3. **Question:** How can interventional constraints be extended to handle hidden confounders?
   - **Basis in paper:** [explicit] The Discussion proposes extending the method using differentiable algebraic constraints for classes like ancestral or bow-free ADMGs.
   - **Why unresolved:** The current method assumes Causal Sufficiency (no unmeasured confounders). If this assumption is violated, the estimated total causal effects and the resulting constrained structure may be biased.
   - **What evidence would resolve it:** An algorithm that unifies interventional inequality constraints with constraint-based characterizations of mixed graphs, proving identifiability in the presence of latent variables.

## Limitations

- The method assumes linear additive data generating process; performance degrades if true relationships are non-linear or contain interaction effects
- Two-stage optimization introduces local optima risk, particularly when Stage 1 produces an incompatible initialization for Stage 2
- Computational complexity scales poorly with constraint count (O(m·d³)), limiting applicability to datasets with many known causal pairs

## Confidence

**High Confidence:** The core mechanism of using total causal effect matrices (T = (I-W)⁻¹ - I) to encode interventional constraints is mathematically sound and well-supported by linear SEM theory. The two-stage optimization approach is a reasonable solution to the non-convexity introduced by these constraints.

**Medium Confidence:** The experimental results showing improved SHD and SID metrics over baseline methods are promising, but the Sachs dataset results depend heavily on the correctness of assumed constraints. The post-thresholding repair mechanism works in theory but may not be robust to noise or weak signals.

**Low Confidence:** The scalability claims for datasets with many interventional constraints remain untested. The method's behavior with incorrect or contradictory constraints is not fully characterized.

## Next Checks

1. **Constraint Robustness Test:** Run the algorithm on the Sachs dataset with intentionally incorrect constraints (e.g., claim "PIP3 inhibits Akt" when the true effect is positive). Measure whether the algorithm gracefully degrades or catastrophically fails.

2. **Scalability Benchmark:** Evaluate performance on synthetic datasets with increasing constraint counts (m = 5, 10, 20, 50) while holding d constant. Measure runtime, constraint satisfaction rates, and structural accuracy to identify the practical limit for constraint integration.

3. **Noise Sensitivity Analysis:** Generate synthetic linear DAGs with varying noise levels (SNR = 10, 5, 2, 1). For each, test the algorithm's ability to correctly identify known causal signs and measure how constraint satisfaction degrades as signal-to-noise ratio decreases.