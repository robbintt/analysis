---
ver: rpa2
title: Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization
arxiv_id: '2508.02002'
source_url: https://arxiv.org/abs/2508.02002
tags:
- bidding
- action
- learning
- value
- auto-bidding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automated bidding optimization
  in online advertising, where advertisers have diverse goals and real-world constraints
  like budget and return on investment. The proposed GRAD framework uses a generative
  foundation model with an Action-Mixture-of-Experts (MoE) module for exploration
  and a Causal Transformer-based value estimator for constraint-aware optimization.
---

# Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization

## Quick Facts
- **arXiv ID**: 2508.02002
- **Source URL**: https://arxiv.org/abs/2508.02002
- **Reference count**: 40
- **Primary result**: GRAD framework achieves 2.18% GMV and 10.68% ROI improvements over baselines in large-scale ad bidding optimization.

## Executive Summary
GRAD introduces a generative foundation model for automated bidding optimization that addresses diverse advertiser goals and real-world constraints. The framework combines a Causal Transformer for trajectory generation with an Action-Mixture-of-Experts module for constrained exploration and a Causal Transformer-based value estimator for constraint-aware optimization. The model demonstrates significant improvements in both offline experiments on AuctionNet and online A/B tests on Meituan's advertising platform, showing particular strength in maintaining constraint satisfaction while optimizing multiple objectives simultaneously.

## Method Summary
GRAD is a generative foundation model for automated bidding optimization that uses a Causal Transformer backbone to process sequential state-action-RTG embeddings. The ActionMoE module generates constrained exploratory actions through element-wise perturbation of previous actions, processed through shared and routed experts with residual fusion. A value estimator predicts time-adjusted rewards with constraint-aware factors. The model is trained end-to-end with a composite loss including policy, value, balance, and diversity terms, optimized using AdamW with batch size 128 for 400K steps.

## Key Results
- Achieves 2.18% increase in Gross Merchandise Value (GMV) compared to baseline methods
- Improves Return on Investment (ROI) by 10.68% in online A/B tests on Meituan's platform
- Outperforms Decision Transformer (329 vs 321) and IQL baselines on AuctionNet with 100% budget

## Why This Works (Mechanism)

### Mechanism 1: Trust-Region Action Exploration via MoE
ActionMoE enables constrained exploration beyond historical action distributions while preserving feasibility through element-wise perturbation (f ∈ [0.8, 1.2)) of previous actions. Each candidate passes through shared expert (baseline preservation) and top-1 routed expert (targeted exploration), fused via learnable weighted residuals. The method assumes constraint functions are L_k-Lipschitz and baseline actions maintain feasibility margin η > 0, ensuring perturbations within ε ≤ η/(L·||a_{t-1}||) remain feasible.

### Mechanism 2: Time-Adjusted Value Estimation for Distribution Shift
The model explicitly models temporal decay, cost penalties, and budget efficiency in the reward signal to mitigate offline-online distribution mismatch. The reward R_t = Γ(t)·Ω(t)·Π(t)·g_t + Θ_noise uses Γ(t)=e^t for temporal decay, Ω(t) penalizes CPC violations, and Π(t) tracks remaining budget proportion. Training uses temporal-decoupled MSE with increasing weights for late-stage stabilization.

### Mechanism 3: Causal Transformer for Conditional Trajectory Generation
The model uses causal attention over state-action-RTG sequences for direct generation of bidding trajectories conditioned on advertiser preferences. Input embeds RTG, state, and previous action with positional encoding through N stacked decoder-only blocks, with dual heads for policy (tanh-MLP) and value estimation using MSE loss between predicted and ground-truth actions.

## Foundational Learning

- **Offline Reinforcement Learning**: GRAD trains on static historical bidding logs without online environment interaction. Quick check: Why does the paper use behavior cloning (MSE loss) rather than Q-learning for the policy head?

- **Mixture-of-Experts (MoE) Routing**: ActionMoE balances shared expert (exploitation stability) with routed experts (exploration diversity). Quick check: What happens to gradient variance if all experts are activated simultaneously instead of top-1 routing?

- **Constrained Markov Decision Process (CMDP)**: Bidding optimization involves hard constraints (budget) and soft constraints (CPC/ROI targets). Quick check: How does GRAD differentiate between hard constraint enforcement versus soft constraint penalty in the reward design?

## Architecture Onboarding

- **Component map**: RTG/state/action embeddings → Causal Transformer (8 layers, 16 heads, 512 hidden) → Policy head + Value head; ActionMoE branch: a_{t-1} → perturbation → shared + top-1 routed expert → residual fusion → weighted ensemble

- **Critical path**: 1) Input construction: Embed g_t (RTG), s_t (16-dim state), a_{t-1} (1-dim action) with positional encoding; 2) Transformer forward: 8 stacked causal attention blocks; 3) ActionMoE branch: Perturb a_{t-1} → route through shared + top-1 expert → residual fusion → weighted action ensemble; 4) Value branch: MLP prediction of R_t with time-adjustment factors; 5) Loss computation: Joint optimization of all four objectives

- **Design tradeoffs**: Expert count 6 optimal (4 under-explores, 8 causes gradient dilution); Top-1 vs. top-k routing: Top-1 reduces latency for online inference but limits expert diversity; Shared expert essential for stability but may limit exploration capacity

- **Failure signatures**: CPC_CR degradation: Value estimator not properly penalizing constraint violations; Gradient spikes: MoE load imbalance; Action collapse: Diversity loss too weak or perturbation range too narrow; Late-stage performance drop: Time-adjustment weights insufficient

- **First 3 experiments**: 1) Ablation on expert count: Run AuctionNet 100% budget with M∈{4,6,8} experts; expect peak at 6 with degradation at 8; 2) Constraint satisfaction stress test: Set budget to 50% and verify CPC_CR remains above baseline; 3) Offline-online gap measurement: Train on historical data, evaluate on sparse AuctionNet-Sparse variant; expect Value Estimator to show larger contribution in sparse settings

## Open Questions the Paper Calls Out

- **Cross-domain transfer capability**: Can GRAD function as a true "foundation model" by transferring learned policies to distinct advertising ecosystems (e.g., from feed recommendation to search bidding) without significant performance degradation? The paper evaluates solely on Meituan's food delivery platform and AuctionNet dataset, leaving cross-domain generalization unverified.

- **Hard constraint safety**: Does the reliance on soft penalty terms in the Value Estimator suffice for safety-critical scenarios requiring strict adherence to hard budget constraints? The theoretical analysis relies on specific feasibility margins that may not hold during extreme market volatility, risking budget overdraft.

- **Top-1 routing limitations**: Does the Top-1 routing strategy in ActionMoE limit the discovery of optimal multimodal bidding strategies in sparse reward environments? While Top-1 reduces variance, it might discard potentially beneficial exploratory actions from non-maximally activated experts in high-dimensional action spaces.

## Limitations
- Theoretical feasibility guarantees depend on precise Lipschitz constants for constraint functions that are not empirically verified on real auction data
- Optimal expert count of 6 is determined empirically on AuctionNet but may not generalize to other bidding domains
- Time-adjustment factors in the value estimator are heuristic and lack rigorous sensitivity analysis to market volatility conditions

## Confidence
- **High confidence**: AuctionNet offline experiment results (329 vs 321 DT vs IQL), Meituan online A/B test improvements (2.18% GMV, 10.68% ROI)
- **Medium confidence**: Theoretical feasibility proofs for ActionMoE perturbation bounds, ablation study results showing individual component contributions
- **Low confidence**: Generalization of the 6-expert configuration to other domains, robustness of time-adjustment heuristics under extreme market conditions

## Next Checks
1. Test ActionMoE perturbation bounds empirically by measuring constraint violation rates across different expert counts and perturbation scales on AuctionNet-Sparse
2. Conduct ablation studies with varying λ_b and λ_d weights to determine their optimal balance for constraint satisfaction vs. exploration
3. Evaluate performance degradation when RTG conditioning signals are replaced with random noise to confirm they are genuinely providing useful advertiser-specific guidance rather than serving as learned positional encodings