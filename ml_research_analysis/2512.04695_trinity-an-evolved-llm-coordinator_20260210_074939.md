---
ver: rpa2
title: 'TRINITY: An Evolved LLM Coordinator'
arxiv_id: '2512.04695'
source_url: https://arxiv.org/abs/2512.04695
tags:
- agent
- coordinator
- selection
- head
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TRINITY, a lightweight coordinator that\
  \ orchestrates multiple large language models (LLMs) without modifying their weights.\
  \ The coordinator, consisting of a compact language model (0.6B parameters) and\
  \ a small head (10K parameters), assigns one of three roles\u2014Thinker, Worker,\
  \ or Verifier\u2014to selected LLMs across multiple turns."
---

# TRINITY: An Evolved LLM Coordinator

## Quick Facts
- **arXiv ID**: 2512.04695
- **Source URL**: https://arxiv.org/abs/2512.04695
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art 86.2% on LiveCodeBench with a lightweight coordinator orchestrating 7 LLMs without modifying model weights

## Executive Summary
TRINITY introduces a novel lightweight coordinator that orchestrates multiple large language models (LLMs) across multiple turns without modifying their weights. The coordinator consists of a compact language model (0.6B parameters) and a small head (10K parameters) that assigns one of three roles—Thinker, Worker, or Verifier—to selected LLMs. Optimized via a separable Covariance Matrix Adaptation Evolution Strategy (sep-CMA-ES), TRINITY sets a new record of 86.2% on LiveCodeBench while consistently outperforming individual models and existing multi-agent methods across coding, math, reasoning, and domain knowledge tasks.

## Method Summary
TRINITY employs a 0.6B parameter Qwen3 model as a coordinator that processes the full conversation transcript and outputs hidden states from its penultimate token position. These hidden states are projected by a 10K parameter linear head to logits for selecting among 7 LLMs and assigning one of three roles (Thinker, Worker, Verifier). The system operates in turns, with the Verifier role determining when to terminate. Training uses sep-CMA-ES to optimize the coordinator's parameters under a binary correctness reward, exploiting the block-ε-separability of the coordination objective to efficiently handle the high-dimensional parameter space.

## Key Results
- Sets new state-of-the-art record of 86.2% on LiveCodeBench V6
- Consistently outperforms individual models and existing multi-agent methods across coding, math, reasoning, and domain knowledge tasks
- Demonstrates robust generalization to unseen tasks including AIME2025, BigCodeBench, MT-Bench, and GPQA-D
- Maintains strong performance with minimal parameter overhead (10K parameters beyond the coordinator backbone)

## Why This Works (Mechanism)

### Mechanism 1: Hidden-State Representations as Coordination Signal
The penultimate-token hidden states from the small language model encode sufficient semantic context for a lightweight head to make effective agent-role assignments. The SLM processes the full conversation transcript; its hidden states capture task-type information in a near-linearly separable manifold. A 10K-parameter linear head then projects these representations directly to agent and role logits. The core assumption is that task-relevant context is preserved in the SLM's hidden states without requiring the coordinator to generate explicit reasoning text. Evidence shows linear SVM achieves 100% accuracy on task-type classification from hidden states, and switching to the final token instead of penultimate causes >10 point drop on LiveCodeBench.

### Mechanism 2: Tri-Role Protocol for Structured Decomposition
Partitioning responsibilities into Thinker, Worker, and Verifier roles enables specialized invocation without requiring the coordinator to learn complex skills itself. The coordinator assigns roles dynamically across turns. Thinker decomposes problems into subgoals; Worker executes steps; Verifier validates completeness. Termination occurs only when Verifier accepts or turn budget exhausts. The core assumption is that each LLM in the pool can effectively fulfill its assigned role when appropriately prompted. Evidence shows removing Thinker role causes 6.0 point drop on MATH500, and the formal role definitions with contracts between coordinator and selected LLM structure the coordination process.

### Mechanism 3: sep-CMA-ES Exploits Block-ε-Separability
Under high dimensionality (~10K parameters) and tight evaluation budgets (1.5k–40k calls), separable CMA-ES outperforms gradient-based RL by exploiting weak inter-block parameter correlations. sep-CMA-ES maintains only a diagonal covariance matrix, sampling perturbations around a parent policy and recombining based on fitness-weighted averaging. This matches the block-diagonal structure of the optimization landscape where inter-logit correlations are negligible. The core assumption is that the coordination objective exhibits block-ε-separability—i.e., most informative signal is intra-block. Evidence shows sep-CMA-ES achieves 0.615 on LiveCodeBench vs. 0.253 for REINFORCE, 0.374 for random search.

## Foundational Learning

- **Concept: Covariance Matrix Adaptation Evolution Strategy (CMA-ES)**
  - Why needed here: The paper uses sep-CMA-ES as its core optimizer; understanding rank-based selection, step-size adaptation, and covariance learning is essential to grasp why it outperforms RL.
  - Quick check question: How does CMA-ES differ from gradient descent in handling noisy, black-box objectives?

- **Concept: Hidden-State Representations in Transformers**
  - Why needed here: The coordinator relies on penultimate-token hidden states from the SLM; knowing how self-attention builds contextual representations explains the separability results.
  - Quick check question: Why might the penultimate token's hidden state carry more contextual information than the final EOS token?

- **Concept: Multi-Agent Coordination Protocols**
  - Why needed here: The tri-role system (Thinker/Worker/Verifier) structures agent interactions; understanding role-based decomposition helps evaluate generalizability.
  - Quick check question: What failure modes emerge if the Verifier role is removed or if roles are assigned randomly?

## Architecture Onboarding

- **Component map:**
  - User query → Coordinator SLM (forward pass to penultimate token)
  - Hidden state → Lightweight head → Agent/role logits → Softmax → Selection
  - Role-specific prompt + full transcript → Selected LLM → Response
  - Response appended to transcript; repeat until Verifier accepts or max turns (5)
  - Final response returned

- **Critical path:**
  1. User query → Coordinator SLM (forward pass to penultimate token)
  2. Hidden state → Lightweight head → Agent/role logits → Softmax → Selection
  3. Role-specific prompt + full transcript → Selected LLM → Response
  4. Response appended to transcript; repeat until Verifier accepts or max turns (5)
  5. Final response returned

- **Design tradeoffs:**
  - Linear vs. low-rank vs. block-diagonal heads: Linear (10K params) is most robust; block-diagonal-10 (1K params) achieves ~90% of performance with higher independence
  - sep-CMA-ES vs. REINFORCE: sep-CMA-ES requires no gradient computation but needs multiple rollouts per iteration; REINFORCE fails under binary rewards and weak parameter correlations
  - Penultimate vs. final token: Penultimate provides stable distribution context; final (EOS) is semantically sparse and causes performance collapse

- **Failure signatures:**
  - Near-uniform agent selection distribution → REINFORCE not learning
  - Performance drops >10 points on specific tasks → Check if hidden-state separability degraded
  - Random search collapses to single-agent selection → Insufficient replication or poor sampling range

- **First 3 experiments:**
  1. Reproduce ablation: Train coordinator with linear head only (no SVF) on LiveCodeBench; expect ~5 point drop per Table 2
  2. Test separability: Extract hidden states from coordinator on held-out task (e.g., GPQA-D); train linear SVM to predict task type. Accuracy < 0.8 suggests generalization risk
  3. Budget sensitivity: Vary atomic evaluation budget (1k, 5k, 20k) for sep-CMA-ES vs. random search; plot performance vs. budget to validate Proposition 1 scaling

## Open Questions the Paper Calls Out
- Can TRINITY effectively extend its coordination framework to include grounded execution agents, such as code interpreters and external APIs?
- Does the efficiency of sep-CMA-ES over Reinforcement Learning (RL) persist if the optimization landscape violates the assumed block-ε-separability?
- Is the fixed tri-role taxonomy (Thinker, Worker, Verifier) optimal, or does it constrain the system compared to a fully learned action space?

## Limitations
- Effectiveness depends critically on the separability of task types in the SLM's hidden-state space; current analysis only covers LiveCodeBench, MATH500, and MMLU
- Does not report total compute cost of training the coordinator relative to end-task performance gains
- Fixed agent pool at 7 models; scalability to larger pools is untested and may degrade sep-CMA-ES's diagonal covariance approximation

## Confidence
- **High confidence** in empirical superiority of TRINITY over individual models and existing methods (86.2% LiveCodeBench, consistent gains across all tasks)
- **Medium confidence** in claim that hidden-state representations alone suffice for effective coordination; SVM separability is strong but causal link to performance is indirect
- **Medium confidence** in sep-CMA-ES's advantage over gradient-based RL under high dimensionality; theoretical advantage well-justified but empirical comparison covers narrow set of optimizers

## Next Checks
1. **Out-of-distribution robustness test**: Run the trained coordinator on a multilingual or multimodal task suite (e.g., XSUM, WikiLingua) and measure task-type classification accuracy and end-task performance; accuracy < 0.7 on new task types would indicate a critical generalization gap
2. **Compute cost accounting**: Report total FLOPs or wall-clock time for coordinator training (including all sep-CMA-ES iterations) and compare to compute cost of fine-tuning a single large model to same accuracy; 10x cost increase would challenge practicality
3. **Scalability probe**: Expand agent pool to 20+ models (including low-resource open models) and re-train coordinator; if performance degrades or sep-CMA-ES fails to converge, this would highlight fundamental scalability limit