---
ver: rpa2
title: Success in Humanoid Reinforcement Learning under Partial Observation
arxiv_id: '2507.18883'
source_url: https://arxiv.org/abs/2507.18883
tags:
- learning
- training
- remove
- state
- observation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first successful instance of learning humanoid
  locomotion policies under partial observability in the Gymnasium Humanoid-v4 environment.
  The proposed method uses a novel parallel history encoder that processes fixed-length
  sequences of past observations, enabling model-free reinforcement learning without
  sequential memory architectures like LSTM or Mamba.
---

# Success in Humanoid Reinforcement Learning under Partial Observation

## Quick Facts
- arXiv ID: 2507.18883
- Source URL: https://arxiv.org/abs/2507.18883
- Authors: Wuhao Wang; Zhiyong Chen
- Reference count: 25
- Key outcome: First successful humanoid locomotion under partial observability using parallel history encoder; achieves comparable performance to full-observation baselines while using only one-third to two-thirds of state dimensions.

## Executive Summary
This paper presents the first successful instance of learning humanoid locomotion policies under partial observability in the Gymnasium Humanoid-v4 environment. The proposed method uses a novel parallel history encoder that processes fixed-length sequences of past observations, enabling model-free reinforcement learning without sequential memory architectures like LSTM or Mamba. Experiments show that the approach achieves performance comparable to state-of-the-art full-observation baselines despite using only one-third to two-thirds of the original state dimensions.

## Method Summary
The method processes a fixed-length sequence of past observations in parallel, treating each timestep's observation as an independent channel rather than using sequential recurrence. This parallel history encoder integrates with TD3-based model-free reinforcement learning, processing stacked observation histories to learn spatial-temporal patterns directly. The approach removes semantic attribute groups (velocity, mass/inertia, force) while retaining position information, and trains policies that achieve high performance even with significantly reduced observation dimensions.

## Key Results
- Achieves humanoid locomotion performance comparable to full-observation TD3 baselines using only 34-73% of original state dimensions
- Performance surpasses full-observation TD3 in certain partial observability configurations
- Single policy trained without mass/inertia observations generalizes to ±50% body mass variations
- Velocity observations are identified as most critical for performance, while force information is least essential

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parallel processing of fixed-length observation histories enables effective humanoid locomotion under partial observability.
- **Mechanism:** The encoder stacks a window of past observations and processes them in parallel—treating each timestep's observation as an independent channel—rather than using sequential recurrence. This allows the network to learn spatial-temporal patterns (e.g., velocity trends) directly from the stacked history.
- **Core assumption:** The agent's belief state can be sufficiently approximated from a short, fixed window of recent observations.
- **Evidence anchors:** [abstract] "...novel parallel history encoder that processes a fixed-length sequence of past observations in parallel..." [Introduction] "By treating each time step in the history as equally important, our network allows the agent to flexibly combine information across the observation window."
- **Break condition:** If the task requires memory beyond the fixed window (e.g., long-horizon planning), performance will degrade.

### Mechanism 2
- **Claim:** Position and velocity observations are more critical than mass/inertia or force for reconstructing latent state in this task.
- **Mechanism:** Position provides absolute body configuration, and velocity enables differentiation of motion states. Mass/inertia and force can often be inferred from kinematics or are less directly tied to the reward function (forward velocity + staying upright).
- **Core assumption:** The reward function depends more on kinematic outcomes than on explicit force or inertia values.
- **Evidence anchors:** [Results] "Among the observation attributes, velocity is the most critical for performance, while mass and inertia notably influence convergence speed. In contrast, force information appears to be the least essential." [Results] Table 1 and Table 3 show Remove MF (only position+velocity) retains fast convergence and high final performance.
- **Break condition:** If contact forces or inertia become central to the task (e.g., carrying heavy objects), performance may drop.

### Mechanism 3
- **Claim:** Training without mass/inertia observations induces robustness to body mass variations at test time.
- **Mechanism:** The policy learns to control locomotion without relying on explicit mass values, effectively learning mass-agnostic dynamics. This generalizes to unseen mass configurations within the tested ±50% range.
- **Core assumption:** Mass variations within the tested range do not fundamentally change the optimal control policy structure.
- **Evidence anchors:** [Results] "As successful training can still be achieved when mass/inertia and force inputs are removed (Remove MF), it suggests that the learned policy is adaptive to variations in body mass..." [Results] Table 4 shows a single policy generalizes across ±50% mass changes for all body parts.
- **Break condition:** If mass variations exceed ±50% or change body proportions, the policy may fail.

## Foundational Learning

**Concept: Partially Observable Markov Decision Process (POMDP)**
- Why needed here: The agent cannot access the full state (e.g., missing velocity or mass), so it must act on histories. Understanding POMDPs explains why history encoding matters.
- Quick check question: Can you explain why a single observation might be insufficient for decision-making in this environment?

**Concept: History-based State Augmentation**
- Why needed here: The method stacks past observations to create an augmented state. This is the core alternative to recurrent memory.
- Quick check question: What is the difference between feeding a sequence to an LSTM vs. stacking it as channels for a parallel encoder?

**Concept: Model-Free RL (TD3)**
- Why needed here: The encoder integrates with TD3. Understanding actor-critic methods and off-policy learning is required to implement or extend this work.
- Quick check question: How does TD3 use Q-learning and target policy smoothing to stabilize training?

## Architecture Onboarding

**Component map:** Environment Wrapper -> History Buffer -> Parallel History Encoder -> Latent Vector -> TD3 Actor -> Action

**Critical path:** History Buffer → Parallel Encoder → Latent Vector → TD3 Actor → Action. Ensure the history window size and encoder architecture are tuned before adjusting TD3 hyperparameters.

**Design tradeoffs:**
- Longer history windows capture more temporal context but increase computational cost and may slow training
- Removing velocity (Remove V) is hardest; retaining it (Remove MF) is easiest. Prioritize velocity observation if sensor choice is flexible
- Parallel encoders are simpler than LSTMs but may miss long-term dependencies

**Failure signatures:**
- Training collapse or near-zero rewards: Likely due to removing position (which the paper advises against) or incorrect history stacking
- Slow convergence with Remove VM or Remove VF: Expected; may require >3M steps or larger encoders
- High variance across seeds in two-attribute removal: Indicates the task is under-constrained; consider increasing history length

**First 3 experiments:**
1. **Baseline Reproduction:** Train on full observation with standard TD3 (1M steps). Confirm you match the reported TD3 baseline (~4547 max reward)
2. **Remove MF:** Train with mass/inertia and force masked (history length = paper's default). Verify fast convergence and high reward (>5000)
3. **Remove V:** Train with velocity masked. Expect slower learning and lower final performance (~4400–5000). Test varying history lengths to see if performance recovers

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- Unknown encoder architecture details (layers, hidden dimensions, activation functions) prevent exact reproduction
- Mass robustness claims only validated within ±50% range, leaving boundaries untested
- Method effectiveness in other robotic domains (quadruped, manipulation) remains unvalidated
- No ablation studies on history window length to determine optimal temporal context

## Confidence
- **High Confidence:** The core claim that parallel history encoding enables humanoid learning under partial observability is well-supported by consistent performance across multiple partial observation configurations
- **Medium Confidence:** The assertion that position and velocity observations are more critical than mass/inertia or force attributes is supported by empirical ranking but lacks theoretical justification
- **Low Confidence:** The claim that the method "surpasses" full-observation TD3 in certain settings is based on single metrics without statistical significance testing

## Next Checks
1. Implement systematic ablation study varying history window length K (2, 4, 8 steps) to determine minimum sufficient temporal context
2. Conduct statistical significance tests comparing method against full-observation TD3 across all six configurations with 10+ random seeds
3. Test learned policies with mass variations beyond ±50% (±100%, ±200%) to establish true boundaries of mass-robustness property