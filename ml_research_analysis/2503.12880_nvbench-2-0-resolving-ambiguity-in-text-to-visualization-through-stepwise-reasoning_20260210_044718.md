---
ver: rpa2
title: 'nvBench 2.0: Resolving Ambiguity in Text-to-Visualization through Stepwise
  Reasoning'
arxiv_id: '2503.12880'
source_url: https://arxiv.org/abs/2503.12880
tags:
- data
- visualization
- ambiguity
- query
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: nvBench 2.0 is a benchmark for evaluating Text-to-Visualization
  systems on ambiguous natural language queries. It includes 7,878 queries and 24,076
  visualizations derived from 780 tables across 153 domains.
---

# nvBench 2.0: Resolving Ambiguity in Text-to-Visualization through Stepwise Reasoning

## Quick Facts
- arXiv ID: 2503.12880
- Source URL: https://arxiv.org/abs/2503.12880
- Reference count: 40
- nvBench 2.0 includes 7,878 queries and 24,076 visualizations across 153 domains

## Executive Summary
nvBench 2.0 is a benchmark for evaluating Text-to-Visualization systems on ambiguous natural language queries. It addresses the challenge where a single query can map to multiple valid visualizations by providing 7,878 queries and 24,076 visualizations derived from 780 tables across 153 domains. The benchmark uses a controlled ambiguity-injection pipeline that generates queries with multiple valid interpretations, each traceable through step-wise reasoning paths. This enables systematic evaluation of how well systems can resolve ambiguity and select appropriate visualizations from multiple valid options.

## Method Summary
The nvBench 2.0 benchmark employs a controlled ambiguity-injection pipeline that systematically introduces multiple valid interpretations into natural language queries. Starting with 780 tables across 153 domains, the pipeline generates 7,878 queries with step-wise reasoning paths that trace each ambiguity resolution. Each query can have multiple valid visualizations, creating a dataset of 24,076 visualizations. The benchmark introduces Step-Text2Vis, a model trained on this dataset using step-wise preference optimization, which learns to resolve ambiguities through incremental reasoning steps rather than single-step decisions.

## Key Results
- Step-Text2Vis achieves F1@3 of 81.50% and F1@5 of 80.88%, outperforming GPT-4o by over 20%
- The model demonstrates superior performance on ambiguous query resolution compared to baseline Text-to-Visualization systems
- Step-wise reasoning approach shows significant improvements in handling multiple valid visualization interpretations

## Why This Works (Mechanism)
The approach works by explicitly modeling ambiguity resolution as a stepwise process rather than a single decision. By providing multiple valid interpretations for each query with traceable reasoning paths, the system can learn to disambiguate through incremental steps. The preference optimization training method reinforces correct ambiguity resolution patterns, enabling the model to better handle the inherent uncertainty in natural language visualization requests.

## Foundational Learning

**Ambiguity Injection Pipeline** - Why needed: To create controlled, systematic ambiguity for benchmarking. Quick check: Verify that injected ambiguities represent real-world query patterns through human evaluation.

**Step-wise Reasoning Paths** - Why needed: To trace how ambiguities are resolved incrementally. Quick check: Ensure each reasoning step logically follows from the previous one and leads to a valid visualization.

**Preference Optimization** - Why needed: To train models on selecting optimal visualizations from multiple valid options. Quick check: Validate that the optimization correctly identifies preferred interpretations over alternatives.

## Architecture Onboarding

**Component Map**: Query Input -> Ambiguity Injection -> Step-wise Reasoning Paths -> Visualization Generation -> Preference Optimization -> Step-Text2Vis Model

**Critical Path**: The ambiguity injection and step-wise reasoning components form the critical path, as they directly determine the quality and complexity of training examples for the model.

**Design Tradeoffs**: The controlled injection approach prioritizes systematic coverage of ambiguity types over capturing naturally occurring ambiguities, trading breadth for controlled experimental conditions.

**Failure Signatures**: Poor performance on queries with domain-specific terminology, failure to recognize implicit contextual information, and inability to handle nested or compound ambiguities.

**3 First Experiments**:
1. Test model performance on unambiguous queries to establish baseline capabilities
2. Evaluate performance on single-ambiguity queries before progressing to multi-ambiguity cases
3. Compare step-wise reasoning outputs against human-annotated reasoning paths

## Open Questions the Paper Calls Out
The paper acknowledges that the controlled ambiguity-injection pipeline may not fully capture the breadth of real-world ambiguity in natural language queries, particularly for complex analytical tasks. The evaluation methodology focuses on precision metrics (F1@3, F1@5) that reward exact matches but may not adequately capture semantic similarity between visualizations.

## Limitations
- The controlled ambiguity-injection pipeline may not fully represent real-world ambiguity patterns
- Evaluation focuses on exact match precision metrics that may not capture semantic similarity
- Performance improvements are demonstrated primarily against GPT-4o rather than a broader range of state-of-the-art systems

## Confidence
- High confidence in dataset construction methodology and existence of ambiguity in Text-to-Visualization queries
- Medium confidence in benchmark's representation of real-world ambiguity patterns
- Medium confidence in claimed performance improvements over baselines
- Medium confidence in step-wise reasoning approach's practical utility

## Next Checks
1. Test Step-Text2Vis on real user queries from diverse domains outside the benchmark to assess practical performance
2. Compare visualization similarity metrics beyond exact matches to evaluate semantic equivalence
3. Validate the ambiguity-injection pipeline by having human annotators assess whether injected ambiguities match natural ambiguity patterns