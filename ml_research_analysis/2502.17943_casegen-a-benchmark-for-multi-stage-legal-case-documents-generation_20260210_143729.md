---
ver: rpa2
title: 'CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation'
arxiv_id: '2502.17943'
source_url: https://arxiv.org/abs/2502.17943
tags:
- legal
- case
- defense
- llms
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CaseGen, the first comprehensive benchmark
  for multi-stage legal case documents generation in the Chinese legal domain. CaseGen
  addresses the gap in evaluating LLMs' ability to generate complex legal documents
  by providing 500 real-world case samples annotated by legal experts, covering seven
  essential case sections.
---

# CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation

## Quick Facts
- arXiv ID: 2502.17943
- Source URL: https://arxiv.org/abs/2502.17943
- Reference count: 38
- Primary result: Current LLMs score below 6/10 on legal case document generation tasks

## Executive Summary
CaseGen introduces the first comprehensive benchmark for multi-stage legal case document generation in the Chinese legal domain. The benchmark provides 500 real-world case samples annotated by legal experts, covering seven essential case sections and supporting four key generation tasks. Using an LLM-as-a-judge evaluation framework with human validation, the authors find that current LLMs struggle significantly with legal document generation, with legal-specific models performing worse than general models. The study reveals critical challenges in handling complex legal reasoning while demonstrating the benchmark's effectiveness through high consistency with human expert evaluations.

## Method Summary
CaseGen is a Chinese legal case document generation benchmark consisting of 500 annotated cases with seven essential sections. The benchmark supports four tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results. Each task uses authentic documents as input to prevent error accumulation. Evaluation employs GPT-4o as an LLM judge with Chain-of-Thought reasoning, comparing generated outputs against ground-truth references using task-specific criteria. Human validation was performed on 50 samples, achieving Spearman's ρ=0.75 correlation with the LLM judge.

## Key Results
- Current LLMs score below 6 out of 10 across all legal document generation tasks
- Legal-specific models (LexiLaw, ChatLaw) underperform general models like Qwen2.5-72B-Instruct
- High consistency (Spearman's ρ=0.75) between LLM judge and human expert evaluations
- Error propagation concerns addressed by using authentic documents as inputs rather than model-generated outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage task decomposition enables more precise evaluation of LLM capabilities than end-to-end generation.
- Mechanism: By separating legal document generation into four distinct tasks, each with its own evaluation criteria, the benchmark isolates specific failure modes rather than conflation in a single score.
- Core assumption: Legal document sections have distinct writing logics that can be meaningfully evaluated independently.
- Evidence anchors:
  - [abstract] "It supports four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results."
  - [section 3] "Each section of a legal case document follows distinct writing logic and evaluation criteria... generating a complete case document in one step fails to properly evaluate the LLM's performance."

### Mechanism 2
- Claim: LLM-as-a-judge with chain-of-thought reasoning provides human-validated evaluation at scale.
- Mechanism: GPT-4o compares generated outputs against ground-truth references using task-specific criteria, reasoning step-by-step before assigning scores. Human validation confirmed Spearman's ρ=0.75 correlation.
- Core assumption: LLM judges can reliably assess legal quality dimensions when provided with references and explicit criteria.
- Evidence anchors:
  - [abstract] "LLM-as-a-judge evaluation framework with human validation... high consistency (Spearman's ρ=0.75) with human expert evaluations."
  - [section 4.4] "LLM judge first compares the generated output with the reference answer, then assigns scores for each evaluation dimension."

### Mechanism 3
- Claim: Legal-specific LLMs underperform general models due to base model limitations and potential reasoning degradation from domain adaptation.
- Mechanism: Legal models like LexiLaw (2048 token limit) suffer information loss on long documents; continuous legal training may diminish reasoning capabilities inherited from base models.
- Core assumption: Domain-specific training does not automatically improve task performance if base capabilities are compromised.
- Evidence anchors:
  - [section 5.2] "Lexilaw has a maximum input length of only 2048 tokens... continuous training on legal corpora may reduce the reasoning abilities inherited from the original base model."
  - [table 2] LexiLaw and ChatLaw score 1.17-3.41 across tasks vs. Qwen2.5-72B scoring 4.46-6.19.

## Foundational Learning

- Concept: Chinese legal case document structure (Procedure, Trial Fact, Reasoning, Judgment, Tail)
  - Why needed here: Each section demands different evaluation criteria; understanding this structure is prerequisite to interpreting benchmark results.
  - Quick check question: Can you name which section prioritizes "complete evidence chain and accurate timeline" vs. which focuses on "identifying key issues and applying legal rules"?

- Concept: Pointwise vs. pairwise LLM evaluation
  - Why needed here: CaseGen uses pointwise scoring (1-10 scale per sample) rather than comparing outputs pairwise; understanding this distinction matters for interpreting scores.
  - Quick check question: Would a score of 5/10 indicate the model lost a head-to-head comparison, or that it met basic requirements with average quality?

- Concept: Hallucination risk in legal generation
  - Why needed here: The paper explicitly notes probability-based LLMs "cannot guarantee correctness and interpretability," which motivates the multi-stage, reference-based design.
  - Quick check question: Why does providing ground-truth references to LLM judges partially mitigate—but not eliminate—hallucination concerns?

## Architecture Onboarding

- Component map: Prosecution + Evidence → Defense Statement → Trial Facts → Legal Reasoning → Judgment Results
- Critical path: Evidence + Prosecution → Defense Statement → Trial Facts → Legal Reasoning → Judgment Results. Note: Each task uses authentic documents as input (not model-generated outputs) to prevent error accumulation.
- Design tradeoffs:
  - Preventing error propagation vs. realistic pipeline evaluation (authors chose former)
  - Human annotation cost vs. coverage (500 samples, ~$5,475 for evidence annotation)
  - Single-jurisdiction focus vs. cross-cultural validity (Chinese law only)
- Failure signatures:
  - Legal-specific models scoring <3 typically indicate context truncation or degraded reasoning
  - Low consistency between LLM judge and human scores (below ρ=0.5) would signal evaluation framework failure
  - BERTScore outperforming LLM judge would indicate semantic similarity alone insufficient for legal evaluation
- First 3 experiments:
  1. Replicate baseline evaluation using Qwen2.5-72B-Instruct on all 4 tasks; verify scores align with reported 4.97/4.58/6.19/4.46 range.
  2. Ablate the CoT reasoning component in LLM judge; measure correlation drop with human annotations on 50-sample subset.
  3. Test whether providing model-generated (vs. ground-truth) inputs causes score degradation, quantifying error propagation risk.

## Open Questions the Paper Calls Out

- What specific training strategies effectively mitigate the loss of general reasoning capabilities in legal-specific LLMs during domain adaptation?
- Does the multi-stage generation framework proposed in CaseGen transfer effectively to Common Law jurisdictions with different document structures?
- Can hybrid evaluation metrics combining LLM-as-a-judge with traditional lexical overlap measures provide robustness against adversarial attacks?

## Limitations

- Single-jurisdiction focus (Chinese law) limits generalizability to other legal systems
- Limited context windows for legal-specific models may artificially depress their performance
- Resource constraints limited human annotation to 50 samples for validation

## Confidence

- **High Confidence**: Benchmark construction methodology, task definitions, and overall finding that current LLMs struggle with legal document generation
- **Medium Confidence**: The specific ranking of model performances and the observation that legal-specific models underperform general models
- **Medium Confidence**: The LLM-as-a-judge framework's effectiveness, though the 0.75 correlation suggests reasonable but not perfect alignment with human judgment

## Next Checks

1. Test error propagation by evaluating whether model-generated outputs from upstream tasks degrade downstream task performance when using generated rather than ground-truth inputs
2. Validate cross-jurisdiction applicability by testing whether models that perform well on CaseGen also show comparable performance on legal document generation tasks in other legal systems
3. Conduct ablation study comparing CoT reasoning versus direct scoring in the LLM judge to quantify the contribution of explicit reasoning steps to evaluation reliability