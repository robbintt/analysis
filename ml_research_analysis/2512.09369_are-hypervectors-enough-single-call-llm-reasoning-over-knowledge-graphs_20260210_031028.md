---
ver: rpa2
title: Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs
arxiv_id: '2512.09369'
source_url: https://arxiv.org/abs/2512.09369
tags:
- paths
- reasoning
- path
- pathhd
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PathHD is a lightweight, encoder-free framework for knowledge graph
  question answering that uses hyperdimensional computing to rank relation paths without
  neural models. It encodes paths as block-diagonal GHRR hypervectors, scores candidates
  with cosine similarity and Top-K pruning, and then uses a single LLM call for final
  adjudication, producing answers with cited supporting paths.
---

# Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs
## Quick Facts
- arXiv ID: 2512.09369
- Source URL: https://arxiv.org/abs/2512.09369
- Reference count: 40
- Primary result: Matches or exceeds state-of-the-art Hits@1 and F1 on KG QA while reducing latency by 40-60% and GPU memory by 3-5x via encoder-free retrieval

## Executive Summary
PathHD is a lightweight, encoder-free framework for knowledge graph question answering that leverages hyperdimensional computing to rank relation paths without neural models. It encodes paths as block-diagonal GHRR hypervectors, scores candidates with cosine similarity and Top-K pruning, and uses a single LLM call for final answer adjudication, producing answers with cited supporting paths. On WebQSP, CWQ, and GrailQA, PathHD matches or exceeds state-of-the-art Hits@1 and F1 scores while reducing latency by 40-60% and GPU memory by 3-5x, thanks to its encoder-free retrieval. It also delivers interpretable, path-grounded rationales, addressing key efficiency, accuracy, and transparency challenges in KG-LLM reasoning.

## Method Summary
PathHD uses hyperdimensional computing to represent and rank relation paths in knowledge graphs without neural encoders. It encodes candidate paths as block-diagonal GHRR hypervectors, then scores and prunes them using cosine similarity and Top-K filtering. The framework relies on a symbolic KG solver to generate candidate paths for each query, which are then encoded and ranked. After retrieving the top paths, a single LLM call adjudicates the final answer, returning both the answer and cited supporting paths. This approach achieves high efficiency by eliminating neural retrieval models and provides interpretable reasoning via path citations.

## Key Results
- Matches or exceeds state-of-the-art Hits@1 and F1 scores on WebQSP, CWQ, and GrailQA
- Reduces inference latency by 40-60% compared to neural retrievers
- Cuts GPU memory usage by 3-5x due to encoder-free retrieval

## Why This Works (Mechanism)
PathHD's efficiency and accuracy stem from encoding relation paths as block-diagonal GHRR hypervectors, which are computationally lightweight and support fast cosine similarity-based ranking. By avoiding neural encoders, it sidesteps the heavy computation and memory requirements of traditional neural retrievers. The single LLM call for final adjudication minimizes overhead while maintaining answer quality. The block-diagonal encoding preserves path structure and enables interpretable, path-grounded rationales by directly linking answers to cited KG paths.

## Foundational Learning
- **Hyperdimensional Computing (HDC)**: Needed to represent complex symbolic structures (paths) as fixed-length vectors for efficient similarity search. Quick check: Verify that path encodings preserve semantic similarity under cosine distance.
- **Block-Diagonal GHRR Encoding**: Needed to maintain path structure while enabling fast similarity computation. Quick check: Confirm that block-diagonal structure improves retrieval accuracy over naive concatenation.
- **Cosine Similarity Ranking**: Needed for efficient candidate path scoring without neural models. Quick check: Ensure ranking correlates with answer correctness on validation sets.
- **Top-K Pruning**: Needed to limit candidate paths to a manageable number for LLM adjudication. Quick check: Test retrieval quality at different K values.
- **Symbolic KG Solvers**: Needed to generate candidate paths for queries. Quick check: Measure solver coverage and accuracy on benchmark queries.
- **Single LLM Call**: Needed to minimize latency and complexity in answer generation. Quick check: Compare answer quality and consistency with multi-step LLM pipelines.

## Architecture Onboarding
**Component Map**: KG Solver -> GHRR Path Encoder -> Cosine Similarity Scorer -> Top-K Pruner -> LLM Adjudicator
**Critical Path**: Query -> Symbolic Solver (candidate paths) -> GHRR Encoding (paths) -> Cosine Scoring (top paths) -> Single LLM Call (final answer)
**Design Tradeoffs**: Encoder-free retrieval boosts speed and reduces memory, but relies on high-quality symbolic path annotations; single LLM call is efficient but may miss nuanced multi-step reasoning.
**Failure Signatures**: Degraded accuracy if symbolic solver misses relevant paths; inconsistent answers if path annotations are noisy or incomplete; performance drop if block-diagonal encoding fails to scale with graph size.
**First Experiments**: 1) Test symbolic solver coverage and accuracy on a held-out KG. 2) Benchmark GHRR encoding and retrieval speed/latency. 3) Validate answer quality and path grounding on a small query set.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance hinges on high-quality symbolic path annotations; incomplete annotations degrade accuracy.
- Relies on existing symbolic solvers for candidate path generation; if solver misses paths, retrieval suffers.
- Block-diagonal GHRR encoding may not scale efficiently to very large KGs or long reasoning chains.

## Confidence
- Latency and memory savings: High
- Path grounding and interpretability: Medium
- Accuracy on benchmark datasets: High

## Next Checks
1. Test PathHD on a dataset with incomplete or noisy symbolic path annotations to assess robustness to missing candidate paths.
2. Benchmark scalability on a large-scale KG (e.g., >10M triples) to evaluate whether block-diagonal GHRR encoding maintains performance and latency gains.
3. Conduct ablation studies isolating the contribution of GHRR path encoding from the symbolic solver to quantify each component's impact on accuracy and efficiency.