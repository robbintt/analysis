---
ver: rpa2
title: 'Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to
  Increase Machine Learning Accuracy and Explainability'
arxiv_id: '2507.02922'
source_url: https://arxiv.org/abs/2507.02922
tags:
- data
- learning
- machine
- knowledge
- conceptual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using domain knowledge from conceptual models
  to improve machine learning (ML) performance and transparency. The authors developed
  the Conceptual Modeling for Machine Learning (CMML) method, which uses Extended
  Entity-Relationship (EER) constructs to guide data preparation for ML.
---

# Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to Increase Machine Learning Accuracy and Explainability

## Quick Facts
- **arXiv ID**: 2507.02922
- **Source URL**: https://arxiv.org/abs/2507.02922
- **Reference count**: 40
- **Primary result**: CMML improves ML performance and transparency by using conceptual models to guide data preparation

## Executive Summary
This paper proposes the Conceptual Modeling for Machine Learning (CMML) method, which integrates domain knowledge from Extended Entity-Relationship (EER) conceptual models into machine learning data preparation. The method consists of five guidelines for feature engineering: labeling, deriving, imputing, summarizing entities, and creating multiple training datasets. Evaluated in two foster care case management scenarios, CMML demonstrated significant improvements in model performance (23.8% increase in explained variance and 24-point RMSE reduction) and transparency, with a focus group of data scientists confirming its structured approach to preventing misrepresentation of domain knowledge.

## Method Summary
The CMML method uses EER constructs to systematically guide data preparation for machine learning. It transforms raw datasets (DS0) into prepared training datasets (TDSn) by applying five guidelines: (1) feature labeling with entity names, (2) deriving new features from existing ones, (3) domain-aware imputation of missing values, (4) entity summarization to resolve one-to-many relationships, and (5) creating multiple datasets for entity subtypes. The method was evaluated using two foster care case studies with algorithms including LightGBM, Random Forest, GBM, Deep Learning, and AutoML, comparing baseline models trained on raw data against models trained on CMML-prepared datasets.

## Key Results
- In the first case (predicting length of stay), CMML-transformed datasets showed a 23.8% increase in explained variance and a 24-point reduction in RMSE compared to baseline models
- In the second case (predicting psychotropic medication prescriptions), CMML improved recall from 59.28% to 84% with a statistically significant increase in F-measure
- A focus group with 15 data scientists confirmed CMML's utility for providing a structured approach to data preparation and improving transparency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving entity-relationship semantics during data preparation improves ML model performance.
- Mechanism: CMML uses conceptual models (EER diagrams) to systematically guide feature engineering (labeling, deriving, imputing, summarizing), reducing information loss and aligning data structure with the target prediction task.
- Core assumption: The conceptual model accurately reflects the underlying domain structure relevant to the prediction task.
- Evidence anchors:
  - [abstract] "We develop and demonstrate a method... based on conceptual modeling constructs and principles."
  - [section 4.1.1, Table 4] CMML-transformed datasets (TDS1) showed an average increase of 23.8% in explained variance and a 24-point reduction in RMSE compared to the baseline.
  - [corpus] Weak direct corpus link; the CMML acronym in one neighbor paper refers to "cutting mechanics-based machine learning," not conceptual modeling.
- Break condition: If the conceptual model is outdated, incomplete, or inconsistent with the actual data generation process, applying CMML could introduce noise and degrade performance.

### Mechanism 2
- Claim: Systematizing data preparation via explicit guidelines improves process transparency and model interpretability.
- Mechanism: By mandating feature labeling with entity names (Guideline 1) and basing transformations on explicit EER constructs, CMML provides an auditable trail for data lineage, making feature roles more traceable.
- Core assumption: Data scientists will adhere to the guidelines and that the naming conventions are consistently applied.
- Evidence anchors:
  - [abstract] "These results demonstrate the value of CMML for improving machine learning outcomes," where outcomes include process transparency.
  - [section 4.2] Focus group participants noted CMML provides a structured approach, improves transparency, and can prevent misrepresentation of domain knowledge.
  - [corpus] Related work (e.g., Neuro-Conceptual AI, PassAI) aims for explainability but via different mechanisms like neuro-symbolic integration, not specifically through EER-based data prep.
- Break condition: If feature engineering is highly automated (e.g., AutoML with opaque transformations) and does not expose hooks for entity-aware labeling, the transparency benefits are diminished.

### Mechanism 3
- Claim: Entity summarization based on cardinality constraints reduces training case duplication and improves model accuracy.
- Mechanism: Guideline 4 ("Summarize Entity") resolves granularity mismatches in one-to-many relationships by aggregating predictor entities to match the target-bearing entity's level, preventing over-representation and redundant signals.
- Core assumption: The aggregation functions (e.g., average, count) chosen for summarization appropriately capture the predictive information from the "many" side.
- Evidence anchors:
  - [abstract] The method includes guidelines for "summarizing entities."
  - [section 3.4.2, Figure 5-7] Illustrates how duplicating customer data across multiple orders can over-represent information, which is resolved by creating summarized features like `ORDER_Count` and `Order_Average`.
  - [corpus] No direct corpus evidence on this specific EER-to-ML summarization mechanism.
- Break condition: If key predictive signals are lost during aggregation (e.g., summarizing a complex sequence of events into a simple average), model performance may suffer.

## Foundational Learning

- Concept: **Extended Entity-Relationship (EER) Modeling**
  - Why needed here: CMML's five guidelines are built directly on EER constructs (entity types, attributes, relationships, cardinalities, generalization/specialization). Understanding these is a prerequisite to applying the method.
  - Quick check question: Given a scenario with `Customers` and `Orders`, can you identify the cardinality of their relationship and what "target-bearing entity" means for a prediction task?

- Concept: **Feature Engineering for Tabular Data**
  - Why needed here: CMML operationalizes domain knowledge into feature engineering actions (deriving, imputing, summarizing). A baseline understanding of these techniques is required to execute the guidelines.
  - Quick check question: What is the difference between a raw feature and a derived feature, and when would you choose one over the other?

- Concept: **Supervised Machine Learning Workflow (CRISP-DM)**
  - Why needed here: CMML is a method to be applied within the data preparation phase of a standard ML workflow. Knowing where it fits prevents misapplication (e.g., not for unsupervised learning without adaptation).
  - Quick check question: At which phase of the CRISP-DM cycle would you apply the CMML guidelines, and what artifacts (conceptual model, raw data) do you need before starting?

## Architecture Onboarding

- Component map: Raw Data (DS0) + Conceptual Model (EER) -> CMML Transformation Engine (applies relevant guidelines from G1-G5) -> Prepared Training Datasets (TDSn) -> ML Model Training & Evaluation

- Critical path:
  1. Input Acquisition: Secure the raw dataset and a corresponding EER conceptual model.
  2. Target Identification: Identify the target attribute and its host entity (target-bearing entity) in the model.
  3. Guideline Application: Systematically check and apply guidelines G1 (labeling), G2 (deriving), G3 (imputing), G4 (summarizing), G5 (multiple datasets) as relevant to the data structure.
  4. Output Generation: Produce one or more transformed training datasets (TDSn).

- Design tradeoffs: The primary tradeoff is upfront investment in data preparation for long-term gains in performance and transparency. Another is precision vs. recall (as seen in Case 2), where improved recall for a critical class might slightly reduce overall precision.

- Failure signatures:
  - Data Leakage: Incorrectly applying summarization (G4) using future information (e.g., an average calculated over a future time window).
  - Semantic Mismatch: Applying a guideline based on an outdated conceptual model, leading to nonsensical derived features or inappropriate imputations.
  - Over-fragmentation: Creating too many subtype datasets (G5) with insufficient samples, leading to low-confidence models.

- First 3 experiments:
  1. Baseline vs. CMML Benchmark: On a held-out dataset, train a baseline model on raw data (DS0) and a second model on CMML-prepared data (TDS1). Compare performance metrics (e.g., RMSE, F1-score) to quantify improvement.
  2. Ablation Study: Apply CMML guidelines incrementally (e.g., first G1+G4, then G1+G4+G2) to determine which transformations contribute most to performance gains in your specific domain.
  3. Transparency Audit: Use a feature importance tool (e.g., SHAP) on both baseline and CMML models. Compare the interpretability of the top featuresâ€”are CMML features more traceable and meaningful to domain experts?

## Open Questions the Paper Calls Out

- Question: How can frameworks be developed so that conceptual models both inform machine learning processes and dynamically evolve based on patterns discovered through ML?
  - Basis in paper: [explicit] Section 5.2 states the need to "develop frameworks where conceptual models both inform machine learning processes and evolve based on patterns discovered through ML."
  - Why unresolved: The current CMML method is unidirectional, using static domain knowledge to prepare data, but it lacks mechanisms to update the conceptual model based on new insights generated by the machine learning model.
  - What evidence would resolve it: A prototype implementation where machine learning outputs (e.g., feature importance or latent representations) automatically trigger updates to the Extended Entity-Relationship (EER) constructs.

- Question: To what extent can automated techniques for extracting conceptual models from existing databases enable organizations without formal models to benefit from the CMML method?
  - Basis in paper: [explicit] Section 5.2 proposes to "develop automated techniques for extracting conceptual models from existing databases, enabling organizations without formal models to benefit..."
  - Why unresolved: The paper assumes a conceptual model is available, which is often not the case in practice. It is unclear if "reverse-engineered" models provide the same semantic quality and ML performance improvements as manually created ones.
  - What evidence would resolve it: Comparative studies showing that ML models trained on datasets prepared via automatically extracted conceptual models yield performance metrics comparable to those using manually designed models.

- Question: How can the CMML guidelines be effectively integrated into the pre-processing routines of Automated Machine Learning (AutoML) tools?
  - Basis in paper: [explicit] Section 5.2 identifies the need to "effectively incorporate CMML and possibly additional methods based on conceptual modeling into the routines used to perform automatic machine learning."
  - Why unresolved: Current AutoML tools treat performance and transparency as tradeoffs and lack domain knowledge. The methodological guidelines of CMML have not yet been translated into algorithmic constraints or automated pipeline steps.
  - What evidence would resolve it: An AutoML system that accepts a conceptual model file as a constraint input and automatically generates training datasets adhering to CMML summarization and labeling guidelines.

- Question: Under what specific boundary conditions does the use of conceptual modeling in data preparation lead to degraded machine learning model performance?
  - Basis in paper: [inferred] Section 5 notes that "there can be cases where adding domain knowledge expressed in a conceptual model leads to degraded ML model performance," such as when models are outdated or inconsistent with data.
  - Why unresolved: The paper demonstrates positive outcomes but acknowledges that enforcing domain rules might conflict with the statistical reality of the data (e.g., real-world deviations from business rules), yet does not empirically test these failure modes.
  - What evidence would resolve it: An empirical evaluation applying CMML to datasets where the conceptual model is intentionally misaligned with the actual data distribution, measuring the resulting drop in accuracy or F-measure.

## Limitations
- The evaluation is based on only two real-world scenarios in a specific domain (foster care), limiting generalizability
- The study does not explore CMML's effectiveness on different problem types (e.g., time series, image data) or with different conceptual modeling notations
- The paper does not provide a systematic error analysis of where and why CMML might fail when the conceptual model is inaccurate or incomplete

## Confidence
- **High confidence**: The CMML framework's five guidelines are clearly defined and logically consistent with EER modeling principles
- **Medium confidence**: The performance improvements (23.8% increase in explained variance, 24-point RMSE reduction) are well-documented for the specific use cases but may not generalize to all domains
- **Medium confidence**: The transparency benefits identified in the focus group are valuable but subjective and not systematically measured

## Next Checks
1. **Cross-domain validation**: Apply CMML to datasets from different domains (e.g., healthcare, finance, manufacturing) to test generalizability of performance improvements
2. **Model-agnostic evaluation**: Test whether CMML benefits extend beyond the algorithms used in this study by applying it to simpler models (e.g., logistic regression, decision trees) and comparing results
3. **Model decomposition analysis**: Use SHAP or similar tools to quantify how much of the performance improvement comes from specific CMML guidelines (e.g., is summarization or feature labeling more impactful?) and whether the features generated are more interpretable than baseline features