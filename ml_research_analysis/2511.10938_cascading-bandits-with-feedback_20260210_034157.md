---
ver: rpa2
title: Cascading Bandits With Feedback
arxiv_id: '2511.10938'
source_url: https://arxiv.org/abs/2511.10938
tags:
- regret
- logt
- arms
- algorithm
- cascade
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes cascading bandits with feedback for edge inference,\
  \ where each arm corresponds to an ML model with an associated accuracy and error\
  \ probability. The study examines four decision-making policies\u2014Explore-then-Commit,\
  \ Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling\u2014\
  providing sharp theoretical regret guarantees for each."
---

# Cascading Bandits With Feedback

## Quick Facts
- arXiv ID: 2511.10938
- Source URL: https://arxiv.org/abs/2511.10938
- Reference count: 40
- Primary result: Adaptive policies LCB and Thompson Sampling achieve O(1) regret while commit strategies incur O(log T) regret

## Executive Summary
This paper analyzes cascading bandits with feedback for edge inference, where each arm corresponds to an ML model with associated accuracy and error probability. The study examines four decision-making policies—Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling—providing sharp theoretical regret guarantees for each. The key finding is that policies which commit to a fixed ordering after exploration incur O(log T) regret, while adaptive policies LCB and Thompson Sampling achieve constant O(1) regret. Simulations validate these theoretical results, demonstrating that Thompson Sampling and LCB outperform Explore-then-Commit and Action Elimination in terms of regret minimization.

## Method Summary
The paper studies a K-arm cascade bandit problem where each arm (ML model) outputs 1 with probability μ_i and user feedback Y_i(t) ∈ {0,1} indicates satisfaction. Error probability p_i = P(Y_i = 0 | arm i triggered). Four policies are analyzed: (1) Explore-then-Commit with N = ⌈16logT/(Δ²ᵢμᵢ)⌉ pulls per arm; (2) Action Elimination with round-robin rotation and confidence interval overlap checks; (3) LCB ordering by L_i(t) = p̂ᵢ - √(2logt/Sᵢ); (4) Thompson Sampling with Beta(αᵢ,βᵢ) posteriors, ordering by sampled θᵢ. Regret is defined as R(T) = Σ(r_{L*} - r_{Lt}), where optimal ordering L* sorts arms by increasing p_i. Simulations use K=5 arms with μ = [0.85, 0.9, 0.95, 0.92, 0.87], p = [0.1, 0.25, 0.4, 0.55, 0.7], T = 5×10⁴, averaged over 20 runs.

## Key Results
- Adaptive policies (LCB and Thompson Sampling) achieve constant O(1) regret
- Commit strategies (Explore-then-Commit, Action Elimination) incur logarithmic O(log T) regret
- Optimal static ordering depends exclusively on error probabilities p_i, independent of means μ_i
- Thompson Sampling and LCB outperform commit strategies in regret minimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive policies (LCB and Thompson Sampling) achieve constant O(1) regret
- Mechanism: These algorithms continuously update cascade ordering based on incoming feedback, allowing indefinite correction of suboptimal ordering choices and preventing regret accumulation
- Core assumption: Error probabilities p_i are stationary and feedback accurately reflects ground truth satisfaction
- Evidence: Theorem 6 and Theorem 7 formally bound regret for LCB and TS as O(1); abstract confirms adaptive policies achieve constant regret

### Mechanism 2
- Claim: Commit strategies incur logarithmic regret Ω(log T)
- Mechanism: Fixed exploration followed by commitment means non-zero probability of selecting wrong ordering, leading to persistent suboptimal performance
- Core assumption: Exploration budget is finite and may be insufficient to distinguish arms with small gaps Δ_i
- Evidence: Theorem 3 and Theorem 5 prove Ω(log T) lower bound for commit strategies; abstract notes suboptimal regret from fixed ordering

### Mechanism 3
- Claim: Optimal ordering depends only on p_i, not μ_i
- Mechanism: Mathematical derivation shows expected reward depends on difference in error probabilities when swapping adjacent arms, cancelling out μ terms
- Core assumption: Cascade structure where model only reached if all preceding models output zero
- Evidence: Theorem 1 states optimal ordering by increasing p_i; Remark 1 explicitly notes ordering independence from μ_i

## Foundational Learning

- Concept: Combinatorial Multi-Armed Bandits (CMAB)
  - Why needed: Understanding that arms are permutations of models and reward depends on combinations
  - Quick check: Can you explain why pulling an arm reveals information about only one specific model, making this partial monitoring?

- Concept: Bayesian Posterior Updates (Beta-Bernoulli)
  - Why needed: Mathematical engine of Thompson Sampling algorithm
  - Quick check: If arm has Beta(1,1) prior and receives feedback 0, what are new parameters and how does this shift expected error probability?

- Concept: Concentration Inequalities (Hoeffding's Inequality)
  - Why needed: Underpins confidence bounds in LCB and Action Elimination
  - Quick check: Why does confidence interval width shrink as observations S_i(t) increase, and how does this relate to algorithm locking in decisions?

## Architecture Onboarding

- Component map: Environment -> Learner (Agent) -> Cascade Controller -> Scoring Modules
- Critical path: Receive query → Traverse ordered list L_t → Find first I_t where X_{I_t}=1 → Present result to User → Receive feedback Y_{I_t} → Update statistics for arm I_t only
- Design tradeoffs: TS is computationally lighter and empirically robust vs. LCB's deterministic confidence guarantees; Adaptive algorithms require constant re-sorting vs. simpler commit strategies
- Failure signatures: Linear/logarithmic regret growth (incorrect update), starvation (low μ arms), high "unserved" rate (poor ordering)
- First 3 experiments: (1) Run LCB/TS for T=10⁵ steps with synthetic data and plot cumulative regret to verify plateauing; (2) Vary error probabilities p to measure ordering stabilization time; (3) Implement Explore-then-Commit with insufficient exploration budget to demonstrate linear-log regret divergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do regret guarantees change if objective incorporates explicit latency or computational costs per arm?
- Basis: Introduction motivates with "faster inference" and "reducing latency" but problem setup defines reward solely as binary success probability
- Why unresolved: Theoretical analysis derives optimal ordering based exclusively on error probabilities without cost terms
- What evidence would resolve it: Modified regret analysis including cost c_i for each arm, resulting in new optimal ordering and bounds

### Open Question 2
- Question: Can constant O(1) regret bounds be maintained in non-stationary environments with drifting p_i and μ_i?
- Basis: Problem setup assumes i.i.d. Bernoulli variables with static parameters, limiting applicability to dynamic scenarios
- Why unresolved: O(1) regret proofs rely on convergence to fixed ground-truth values
- What evidence would resolve it: Regret bounds under non-stationary bandit model showing robustness of adaptive policies

### Open Question 3
- Question: Does independence of optimal ordering from μ_i hold under more general reward functions or correlated arm structures?
- Basis: Remark 1 notes optimal ordering independence from μ_i, derived from specific multiplicative reward structure
- Why unresolved: Property derived from specific formulation, unclear if general to cascade models
- What evidence would resolve it: Theoretical proof extending result to correlated outputs or counter-example showing μ_i dependence

## Limitations
- O(1) regret claim relies on stationary error probabilities; concept drift could invalidate bounds
- Theoretical separation between static and adaptive policies derived for specific scaling regimes; practical gap may be smaller
- Claim of strictly suboptimal regret for commit strategies assumes specific conditions on exploration budget and gap sizes

## Confidence
- **High**: Core mechanism of adaptive policies achieving O(1) regret through continuous updating (Theorems 6-7, simulations)
- **Medium**: Claim that optimal ordering depends only on p_i (Theorem 1), significance depends on real deployment patterns
- **Low**: Assertion that commit strategies incur strictly suboptimal regret (Theorems 3-5), practical performance may be closer with sufficient resources

## Next Checks
1. Implement non-stationary variant where p_i(t) drifts slowly and measure whether adaptive policies maintain O(1) regret
2. Profile computational overhead of Thompson Sampling and LCB on resource-constrained edge devices
3. Conduct ablation studies varying gap Δ_min between error probabilities to determine convergence thresholds