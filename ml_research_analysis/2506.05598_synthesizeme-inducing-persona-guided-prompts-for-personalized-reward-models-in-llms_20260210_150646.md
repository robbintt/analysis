---
ver: rpa2
title: SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models
  in LLMs
arxiv_id: '2506.05598'
source_url: https://arxiv.org/abs/2506.05598
tags:
- user
- preferences
- persona
- synthesizeme
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SynthesizeMe is a method for personalized reward modeling that
  induces synthetic user personas from interaction data to improve LLM preference
  prediction. The method reasons over user preferences, generates synthetic personas,
  and filters informative prior interactions to create personalized prompts.
---

# SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs

## Quick Facts
- arXiv ID: 2506.05598
- Source URL: https://arxiv.org/abs/2506.05598
- Reference count: 40
- Method improves personalized LLM-as-a-judge accuracy by up to 4.4% using synthetic personas

## Executive Summary
SynthesizeMe introduces a method for creating personalized reward models by inducing synthetic user personas from interaction data. The approach generates persona-guided prompts that help Large Language Models better predict individual user preferences. Tested on PersonalRewardBench with 854 users from Chatbot Arena and PRISM, the method demonstrates improved accuracy in personalized preference prediction while producing interpretable personas that reflect actual user preferences.

## Method Summary
SynthesizeMe reasons over user preferences from interaction data to generate synthetic personas, which are then used to create personalized prompts for reward modeling. The method filters informative prior interactions and combines them with the synthetic persona to form a comprehensive prompt. This personalized prompt is used to guide LLM-based reward models in predicting user preferences more accurately than generic approaches.

## Key Results
- Achieved up to 4.4% improvement in personalized LLM-as-a-judge accuracy
- Demonstrated scalability across different model sizes
- Showed successful transfer across model families
- Produced interpretable personas that reflect actual user preferences

## Why This Works (Mechanism)
The method works by creating a bridge between raw interaction data and personalized preference modeling through synthetic personas. By distilling user preferences into interpretable persona descriptions and combining them with carefully selected prior interactions, the system provides context that helps reward models understand individual user preferences more accurately than using either raw data or generic models alone.

## Foundational Learning

**Personalized Reward Modeling**: Why needed - Different users have varying preferences that generic models cannot capture; Quick check - Compare preference prediction accuracy between personalized and generic models

**Synthetic Persona Generation**: Why needed - Raw interaction data is too voluminous and noisy for direct use; Quick check - Verify personas accurately reflect user preference patterns

**Informative Prior Interaction Filtering**: Why needed - Not all past interactions are equally relevant for predicting future preferences; Quick check - Ensure selected interactions represent meaningful preference signals

## Architecture Onboarding

**Component Map**: User Interaction Data -> Synthetic Persona Generator -> Informative Interaction Filter -> Persona-Guided Prompt Generator -> Personalized Reward Model

**Critical Path**: The most important sequence is: interaction data → persona generation → interaction filtering → prompt creation → reward prediction

**Design Tradeoffs**: The system balances between persona specificity (too generic loses personalization benefits) and generality (too specific limits scalability). The filtering mechanism must balance comprehensiveness against noise reduction.

**Failure Signatures**: 
- Poor persona quality leads to inaccurate preference predictions
- Over-filtering removes valuable preference signals
- Under-filtering introduces noise that confuses the reward model
- Persona-prompt misalignment reduces effectiveness

**First Experiments**:
1. Compare preference prediction accuracy with and without persona-guided prompts
2. Test different persona generation approaches to optimize quality
3. Evaluate the impact of interaction filtering thresholds on prediction accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Extent of persona capture of genuine user preferences versus surface-level patterns is uncertain
- Filtering mechanism for informative interactions lacks transparency in selection criteria
- Limited scope of cross-model-family transfer experiments reduces confidence in generalizability

## Confidence

**High Confidence**: Personalized Reward Modeling Framework - Core methodology and demonstrated accuracy improvements are well-documented and reproducible

**Medium Confidence**: Scalability and Transferability - Results show promise but are based on limited model combinations

**Medium Confidence**: Persona Interpretability - Qualitative examples are convincing but lack systematic evaluation

## Next Checks

1. **Cross-Context Generalization Test**: Evaluate performance on users from different domains not represented in training data to assess true generalization capabilities

2. **Longitudinal User Preference Tracking**: Study how well synthetically induced personas adapt to users whose preferences evolve over time, measuring both short-term adaptation speed and long-term stability

3. **Ablation Study with Human Evaluation**: Conduct human studies where participants rate the alignment between synthetically generated personas and their actual preferences, comparing this to baseline methods without persona induction