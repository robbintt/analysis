---
ver: rpa2
title: 'CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for
  Learning with Noisy Labels'
arxiv_id: '2507.11807'
source_url: https://arxiv.org/abs/2507.11807
tags:
- learning
- noisy
- labels
- noise
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLID-MU tackles noisy label learning without requiring a clean
  meta-dataset by introducing an unsupervised Cross-Layer Information Divergence (CLID)
  metric. CLID measures divergence between feature and output layer data distributions,
  capturing model performance independent of label quality.
---

# CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2507.11807
- Source URL: https://arxiv.org/abs/2507.11807
- Reference count: 40
- Primary result: Unsupervised meta-update strategy using CLID achieves 89.07% CIFAR-10N accuracy without clean data

## Executive Summary
CLID-MU introduces a meta-learning strategy for training with noisy labels that doesn't require clean data. The method uses Cross-Layer Information Divergence (CLID) to measure divergence between feature and output layer distributions, providing an unsupervised metric for assessing model performance. By leveraging this metric to guide meta-updates, CLID-MU outperforms state-of-the-art methods on CIFAR-10/100 with synthetic and real-world noise, achieving 89.07% accuracy on CIFAR-10N and 72.93% on Clothing1M without needing a clean validation set.

## Method Summary
CLID-MU addresses noisy label learning by introducing CLID, a metric that measures divergence between feature and output layer distributions without requiring clean labels. The method operates through a two-phase process: first, a supervised training phase using the noisy dataset with regular loss, then a meta-update phase where CLID guides which samples should be emphasized or downweighted. The divergence between feature and output distributions serves as a proxy for model confidence and data quality, enabling the model to identify potentially mislabeled samples. This unsupervised approach eliminates the need for clean meta-datasets while maintaining strong performance across different noise types and levels.

## Key Results
- Achieves 89.07% accuracy on CIFAR-10N and 67.53% on CIFAR-100N
- Outperforms state-of-the-art meta-learning methods (WNet, VRI) and semi-supervised learning frameworks (FlexMatch, FixMatch)
- Reaches 72.93% accuracy on Clothing1M without requiring clean data
- Demonstrates stable performance across different hyperparameter settings

## Why This Works (Mechanism)
CLID-MU works by using the divergence between feature and output layer distributions as an indicator of data reliability. When labels are noisy, the model's feature representations and output predictions become misaligned, creating higher divergence. By measuring this divergence through CLID, the method can identify samples where the model is uncertain or where label noise is likely present. The meta-update strategy then uses this information to adjust training focus, emphasizing reliable samples while being cautious with potentially noisy ones. This creates a self-correcting training loop that progressively improves the model's ability to distinguish between clean and noisy samples.

## Foundational Learning
- **Information Divergence**: Measures the difference between probability distributions - needed to quantify misalignment between feature and output layers, quick check: verify KL divergence calculation is stable
- **Meta-learning**: Learning to learn by optimizing a meta-objective - needed to create adaptive training strategies, quick check: confirm meta-updates improve validation performance
- **Feature extraction**: Mapping inputs to meaningful representations - needed as the basis for CLID calculation, quick check: ensure features capture semantic information
- **Distribution alignment**: Comparing distributions across network layers - needed to detect label noise effects, quick check: verify divergence increases with label corruption

## Architecture Onboarding
**Component map**: Input -> Feature Extractor -> CLID Calculator -> Meta-Updater -> Model Parameters
**Critical path**: Forward pass through network → Calculate CLID between feature/output distributions → Use CLID to weight samples in meta-update → Update model parameters
**Design tradeoffs**: CLID uses unsupervised divergence instead of clean labels for meta-updates (robustness vs. potential noise sensitivity), focuses on cross-layer divergence rather than single-layer metrics (comprehensive vs. computational cost)
**Failure signatures**: Over-reliance on CLID may amplify errors if divergence doesn't correlate with label quality; high computational overhead from calculating divergence at each meta-update
**3 first experiments**: 1) Verify CLID correlates with sample reliability on controlled noise, 2) Compare CLID-MU performance vs baseline on CIFAR-10 with varying noise rates, 3) Ablation study removing CLID meta-updates to confirm their contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation relies heavily on synthetic noise models which may not fully capture real-world label noise patterns
- Limited testing on diverse real-world noisy datasets beyond Clothing1M raises generalizability concerns
- Computational overhead from calculating cross-layer divergence at each meta-update step

## Confidence
- High confidence in CLID's effectiveness as a noise-robust metric, supported by ablation studies and comparisons with established baselines
- Medium confidence in the overall method's superiority, given strong performance on CIFAR benchmarks but limited testing on diverse real-world datasets
- Low confidence in the claim of being "state-of-the-art" across all scenarios, as the comparison set may not be exhaustive and real-world noise patterns can vary significantly

## Next Checks
1. Test CLID-MU on additional real-world noisy label datasets beyond Clothing1M to assess generalizability
2. Conduct experiments with more diverse synthetic noise models, including instance-dependent noise, to evaluate robustness across different noise types
3. Perform ablation studies specifically focusing on the impact of CLID's design choices (e.g., layer selection, divergence calculation method) on overall performance