---
ver: rpa2
title: Stability-based Generalization Bounds for Variational Inference
arxiv_id: '2502.12353'
source_url: https://arxiv.org/abs/2502.12353
tags:
- bounds
- bound
- generalization
- error
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops stability-based generalization bounds for variational
  inference and other approximate Bayesian algorithms optimized using stochastic gradient
  descent. Unlike prior work that used PAC-Bayes analysis, this approach uses stability
  arguments based on parameter differences between perturbed datasets.
---

# Stability-based Generalization Bounds for Variational Inference

## Quick Facts
- arXiv ID: 2502.12353
- Source URL: https://arxiv.org/abs/2502.12353
- Reference count: 23
- One-line primary result: Develops stability-based generalization bounds for VI algorithms using parameter differences on perturbed datasets

## Executive Summary
This paper introduces stability-based generalization bounds for variational inference (VI) algorithms optimized via stochastic gradient descent. Unlike prior PAC-Bayes approaches, the analysis uses stability arguments based on parameter differences between posteriors trained on slightly perturbed datasets. Two types of bounds are derived: one for bounded loss functions using total variation distance and another for Lipschitz loss functions using Wasserstein distance. Experiments on CIFAR10 demonstrate the bounds are non-vacuous in practice and effectively differentiate between generalization performance under various training conditions.

## Method Summary
The method computes generalization bounds by tracking how VI parameters evolve under dataset perturbations. For bounded losses, the bound uses total variation distance between posteriors on perturbed datasets, while for Lipschitz losses, it uses Wasserstein distance. Both reduce to differences between posterior parameters. The core technical result adapts stability analysis from SGD to VI, showing that parameter differences accumulate along the optimization trajectory and can be bounded by sums of gradient differences. The bounds are computed by running parallel training trajectories with different batch sequences and estimating expansion rates empirically.

## Key Results
- Non-vacuous generalization bounds for VI on CIFAR10, correctly ranking scenarios with/without data augmentation
- Bounds are tighter than PAC-Bayes bounds in tested scenarios and effectively explain performance differences between ELBO and DLM algorithms
- Empirical validation shows stability bounds can detect overfitting to random labels (bounds become vacuous)
- Cumulative expansion rates remain small in practice, validating the theoretical assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalization error of VI algorithms can be bounded by parameter differences on perturbed datasets.
- Mechanism: Stability analysis shows that if an algorithm's output distribution is insensitive to single-point data perturbations, the train-test gap is controlled. For VI, this sensitivity is measured via TV distance (bounded loss) or Wasserstein distance (Lipschitz loss), both reducible to parameter differences between posteriors QS,ϵ and Q̄S,ϵ.
- Core assumption: Algorithm A is order-independent (satisfied by random permutation at training start).
- Evidence anchors:
  - [abstract] "the generalization error is bounded by expected parameter differences on a perturbed dataset"
  - [section 4.1] Lemma 3 and Lemma 6 derive the core bounds; Lemma 4 and Lemma 7 reduce to parameter differences for Gaussian posteriors
  - [corpus] Related work on stability for SGLD (Li et al. 2020, Banerjee et al. 2022) uses similar arguments but doesn't apply to VI due to distribution structure differences
- Break condition: When the posterior family is not Gaussian (or lacks closed-form distance expressions), the parameter-difference reduction may not hold tractably.

### Mechanism 2
- Claim: Parameter differences accumulate along the SGD trajectory and can be bounded by sums of gradient differences.
- Mechanism: Following Hardt et al. (2016) and Zhou et al. (2019), each update step has bounded expansion ηt. Tracking how a single data point perturbation propagates yields Theorem 9: E[‖θT - θ̄T‖] ≤ (1/n) Σt [Πi>t ηi] αt E[Δt], where Δt = ‖∇F(θt-1, z̄, ϵt) - ∇F(θt-1, z, ϵt)‖.
- Core assumption: Update rule Gt is ηt-expansive; gradients are L-Lipschitz and β-bounded for asymptotic O(log T / n) bound.
- Evidence anchors:
  - [section 4.2] Theorem 9 provides the exact bound; Corollary 10 gives asymptotic form
  - [section 5] Figure 1 shows cumulative expansion rates remain small in practice, much smaller than the log T worst case
  - [corpus] PAC-Bayes bounds (Germain et al. 2016, Dziugaite & Roy 2017) scale as 1/√n vs. stability's 1/n—stability can be tighter for large n but grows with T
- Break condition: For adaptive optimizers (Adam), characterizing parameter differences is "significantly more challenging" (Section 6).

### Mechanism 3
- Claim: Stability bounds are data-dependent and algorithm-specific, enabling discrimination between generalizing and overfitting scenarios.
- Mechanism: Since the bound involves gradient differences on actual data points, it captures whether training dynamics are stable on the specific dataset. Random labels or lack of augmentation produce higher gradient differences across perturbations, inflating the bound.
- Core assumption: Assumption 1 (order-independence) plus Lipschitz or bounded loss structure.
- Evidence anchors:
  - [section 5] Figure 2 shows bounds are non-vacuous for normal training but vacuous for 50% random labels; bounds correctly rank data augmentation scenarios
  - [section 5] Figure 3 shows ELBO has lower bound than DLM, correlating with better generalization
  - [corpus] Weak direct corpus evidence on this specific mechanism; related work (Zhang et al. 2017) motivates the need for data-dependent bounds
- Break condition: If gradient differences cannot be computed or estimated during training, the bound becomes impractical.

## Foundational Learning

- Concept: **Variational Inference and ELBO**
  - Why needed here: The paper analyzes VI algorithms that optimize ELBO or variants (DLM) via SGD; understanding how posteriors are parameterized and optimized is essential.
  - Quick check question: Can you explain why minimizing KL(Q‖p) is equivalent to maximizing ELBO?

- Concept: **Total Variation and Wasserstein Distances**
  - Why needed here: These divergences measure distribution sensitivity for bounded vs. Lipschitz losses; the bounds reduce to parameter differences via properties of these distances.
  - Quick check question: For two Gaussians N(m, σ²) and N(m̄, σ̄²), can you sketch why W₂ distance is ‖m - m̄‖₂ + ‖σ - σ̄‖₂?

- Concept: **Stability of Stochastic Optimization (Hardt et al.)**
  - Why needed here: The core technical result adapts SGD stability analysis to VI; the expansive map and trajectory arguments are directly reused.
  - Quick check question: Why does uniform stability imply generalization, and how does batch sampling affect the 1/n factor?

## Architecture Onboarding

- Component map:
  - **Objective layer**: ELBO (Eq. 1) or DLM (Eq. 2) with KL regularizer
  - **Posterior family**: Gaussian with mean m and diagonal variance σ² (lower-bounded by σ₀)
  - **Optimizer**: SGD with momentum; learning rate schedule αt
  - **Bound computation**: Track expansion rates ηt, compute gradient differences Δt on sampled (z, z̄) pairs
  - **Comparison baseline**: PAC-Bayes bounds (Eqs. 26-29) with prior or Q₀

- Critical path:
  1. Initialize Q₀ = N(m₀, σ₀²I) and collect training data S
  2. Run SGD on ELBO/DLM objective for T steps, storing trajectory {θt}
  3. Estimate ηt by running parallel trajectories with same batches
  4. Sample 50 (z, z̄) pairs from train/test; compute Δt = ‖∇F(θt-1, z̄, ϵt) - ∇F(θt-1, z, ϵt)‖ over 10 random seeds
  5. Aggregate via Theorem 9 into final bound; compare to actual generalization gap

- Design tradeoffs:
  - **Batch size vs. bound tightness**: Larger batches reduce T (fewer steps), helping stability bound, but may slow convergence
  - **σ₀ floor vs. bound quality**: Smaller σ₀ tightens Lemma 4 bound but may hurt VI optimization
  - **ELBO vs. DLM**: ELBO shows better stability (lower ηt, lower Δt) but DLM has different training dynamics; paper suggests stability explains ELBO's superiority in BNNs
  - **PAC-Bayes prior choice**: Using Q₀ (initialization) can tighten PAC-Bayes but still looser than stability in tested scenarios

- Failure signatures:
  - **Vacuous bound on random labels**: High gradient differences cause bound to exceed 1.0 (for 0-1 loss with C=1)
  - **Bound explodes with T**: If ηt ≈ 1 consistently, cumulative product grows; indicates unstable optimization
  - **DLM underperforms ELBO**: Paper traces this to higher expansion rates and gradient differences (Figure 3)
  - **Adam incompatibility**: Theorem 9 relies on SGD structure; adaptive methods lack clean η-expansive characterization

- First 3 experiments:
  1. **Replicate ELBO with/without augmentation on CIFAR10**: Train the CNN from the paper, compute bound (11) with C=1, verify non-vacuous (bound < 1) and correct ranking of scenarios
  2. **Measure cumulative expansion rate**: Run 10 independent trajectories with fixed batch sequences, compute ηt, confirm it stays below log T asymptotic (Figure 1)
  3. **Compare ELBO vs. DLM on log loss**: Use bound (20) (omitting K), verify DLM has higher bound correlating with worse generalization; compute PAC-Bayes (28) and (29) as baselines to confirm they don't discriminate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the stability analysis be extended to adaptive optimization methods like Adam?
- Basis in paper: [explicit] The conclusion states that characterizing parameter differences is "significantly more challenging" for advanced optimizers like Adam, limiting the current scope to SGD.
- Why unresolved: The definition of expansion rate and the proof of Theorem 9 rely on the specific update structure of SGD, which differs significantly in adaptive methods.
- What evidence would resolve it: A theoretical derivation of the expansiveness of Adam updates or an empirical demonstration of non-vacuous bounds when VI is trained with Adam.

### Open Question 2
- Question: Can the derived bounds be generalized to non-Gaussian variational families?
- Basis in paper: [inferred] Lemmas 4 and 7 rely heavily on specific properties of Gaussian distributions (e.g., Pinsker's inequality application, closed-form Wasserstein-2 distance) to connect stability to parameter differences.
- Why unresolved: The mathematical link between the generalization gap and parameter updates may not hold or may become intractable for complex approximate posteriors like mixture models or normalizing flows.
- What evidence would resolve it: Deriving analogous parameter difference bounds for non-Gaussian families or proving that the Gaussian assumption is a strict requirement for the stability argument.

### Open Question 3
- Question: Can this stability framework be applied to alternative approximate Bayesian objectives like PAC2 variational learning?
- Basis in paper: [explicit] The conclusion suggests the general applicability of the approach implies it could be extended to algorithms such as PAC2 variational learning (Masegosa, 2020).
- Why unresolved: While the framework is general, the specific stability properties (expansiveness) of the gradients for the PAC2 objective have not been verified.
- What evidence would resolve it: Theoretical analysis verifying the gradient difference properties for the PAC2 objective, followed by empirical validation of the resulting generalization bounds.

## Limitations

- The bounds rely on Gaussian variational posteriors with closed-form distance expressions, limiting applicability to more complex posterior families
- Cumulative expansion rates may become loose in practice if optimization updates are unstable, potentially inflating the bounds
- Experiments are limited to CIFAR10 dataset and two VI objectives (ELBO, DLM), with only one model class (CNNs)

## Confidence

- Mechanism 1 (stability → generalization gap): High - follows established Hardt et al. framework with clear mathematical derivation
- Mechanism 2 (parameter differences → gradient sums): Medium - technically correct but cumulative expansion may be loose in practice
- Mechanism 3 (data-dependent discrimination): Medium - empirical results support this but sample size is limited to one dataset and two algorithms

## Next Checks

1. Test bounds on non-Gaussian variational families (e.g., mixture posteriors or implicit distributions) to validate the parameter-difference reduction mechanism
2. Evaluate bounds on larger architectures (ResNet, WideResNet) and other datasets (SVHN, CIFAR100) to assess scalability and robustness
3. Compare stability bounds against other data-dependent generalization measures (e.g., sharpness-based bounds) to validate the claimed advantages