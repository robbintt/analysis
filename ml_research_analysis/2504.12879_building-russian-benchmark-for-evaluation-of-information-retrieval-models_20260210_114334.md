---
ver: rpa2
title: Building Russian Benchmark for Evaluation of Information Retrieval Models
arxiv_id: '2504.12879'
source_url: https://arxiv.org/abs/2504.12879
tags:
- datasets
- retrieval
- russian
- dataset
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RusBEIR, a benchmark for zero-shot evaluation
  of information retrieval models in Russian. It comprises 17 datasets from various
  domains, including adapted, translated, and newly created datasets.
---

# Building Russian Benchmark for Evaluation of Information Retrieval Models

## Quick Facts
- **arXiv ID:** 2504.12879
- **Source URL:** https://arxiv.org/abs/2504.12879
- **Reference count:** 35
- **Primary result:** Introduction of RusBEIR, a benchmark for zero-shot evaluation of information retrieval models in Russian

## Executive Summary
This paper introduces RusBEIR, a comprehensive benchmark for evaluating information retrieval models in Russian. The benchmark comprises 17 datasets spanning various domains and includes adapted, translated, and newly created datasets. The authors evaluate both traditional lexical models (BM25) and neural models (mE5, BGE-M3, RoSBERTa, LaBSE) with and without reranking capabilities. The benchmark is open-source and available on GitHub, providing a valuable resource for the Russian NLP community.

## Method Summary
The RusBEIR benchmark was constructed by collecting 17 diverse datasets from multiple domains, including adapted datasets from existing English benchmarks, translated datasets, and newly created datasets specifically for Russian. Models were evaluated in zero-shot settings, with BM25 receiving Russian-specific preprocessing including lemmatization and stopword removal. Neural models were tested with various max input lengths. The evaluation covered both lexical and neural approaches, with some neural models incorporating reranking capabilities.

## Key Results
- BM25 serves as a strong baseline, particularly for full-document retrieval tasks
- mE5-large and BGE-M3 achieved the best overall performance among neural models
- BM25 combined with BGE reranker yielded top results across multiple datasets
- The benchmark successfully evaluates model performance across diverse domains and task types

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of different dataset creation methodologies and domains. By including adapted, translated, and original datasets, it captures various aspects of Russian information retrieval challenges. The combination of traditional BM25 with modern neural approaches provides a complete evaluation spectrum, while the zero-shot setting ensures models are tested on their generalization capabilities rather than fine-tuned performance.

## Foundational Learning
- **Zero-shot evaluation**: Testing models without task-specific fine-tuning to assess generalization
  - Why needed: Ensures evaluation of model's inherent capabilities rather than adaptation skills
  - Quick check: Models perform well across diverse datasets without task-specific training
- **Dataset adaptation methods**: Understanding differences between adapted, translated, and original datasets
  - Why needed: Different creation methods may introduce systematic biases
  - Quick check: Performance varies systematically across dataset types
- **Russian morphological preprocessing**: Lemmatization and stopword removal specific to Russian
  - Why needed: Russian morphology requires specialized preprocessing for optimal retrieval
  - Quick check: BM25 performance improves with proper morphological preprocessing

## Architecture Onboarding

**Component Map:**
- Datasets (17) -> Preprocessing (lemmatization, stopword removal) -> Retrieval Models (BM25, mE5, BGE-M3, RoSBERTa, LaBSE) -> Reranking (optional) -> Evaluation Metrics

**Critical Path:**
Data preparation → Model inference → Reranking (if applicable) → Metric calculation

**Design Tradeoffs:**
- Zero-shot vs. fine-tuned evaluation: Zero-shot ensures fair comparison but may underestimate model capabilities
- Neural vs. lexical models: Neural models offer semantic understanding but require more resources
- Reranking integration: Improves results but adds computational overhead

**Failure Signatures:**
- Poor performance on translated datasets may indicate semantic loss during translation
- Inconsistent results across domains suggest limited generalization
- BM25 underperformance may indicate insufficient morphological preprocessing

**First Experiments:**
1. Evaluate BM25 with and without Russian morphological preprocessing
2. Compare neural model performance with varying max input lengths
3. Test reranking impact on both BM25 and neural model outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot evaluation may not capture domain-specific performance
- Translated datasets may introduce semantic shifts affecting model performance
- Limited analysis of biases introduced by different dataset creation methodologies

## Confidence
- **High confidence** in benchmark construction methodology
- **Medium confidence** in relative model performance comparisons
- **Medium confidence** in cross-dataset generalization claims

## Next Checks
1. Evaluate RusBEIR with additional neural models specifically trained on Russian data
2. Conduct detailed error analysis comparing model performance across dataset creation methodologies
3. Test robustness of results by varying preprocessing parameters, particularly for BM25