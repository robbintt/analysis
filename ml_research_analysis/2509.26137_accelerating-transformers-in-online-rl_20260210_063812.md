---
ver: rpa2
title: Accelerating Transformers in Online RL
arxiv_id: '2509.26137'
source_url: https://arxiv.org/abs/2509.26137
tags:
- transformer
- training
- stage
- algorithm
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to accelerate transformer training
  in online reinforcement learning by using a simpler, more stable model (the "Accelerator")
  to generate trajectories for pretraining the transformer. The Accelerator interacts
  with the environment independently while simultaneously training the transformer
  through behavior cloning.
---

# Accelerating Transformers in Online RL

## Quick Facts
- arXiv ID: 2509.26137
- Source URL: https://arxiv.org/abs/2509.26137
- Reference count: 40
- One-line primary result: This paper proposes a method to accelerate transformer training in online reinforcement learning by using a simpler, more stable model (the "Accelerator") to generate trajectories for pretraining the transformer.

## Executive Summary
This paper addresses the challenge of stabilizing transformer-based policies in online reinforcement learning. The authors propose a two-stage approach where a simpler, more stable model (the "Accelerator") generates trajectories for pretraining the transformer via behavior cloning. In the second stage, the pretrained transformer fine-tunes in a fully online setting. Experiments on MuJoCo and ManiSkill environments show that this approach enables stable transformer training, reduces training time by up to a factor of two, and decreases the required replay buffer size to 10-20 thousand, significantly lowering computational demands.

## Method Summary
The proposed method involves a two-stage training process. In Stage 1, an "Accelerator" model (MLP or LSTM) interacts with the environment and generates trajectories. Simultaneously, the transformer is trained via behavior cloning on these trajectories using MSE loss. In Stage 2, the transformer fine-tunes online using the accelerator's critic within the standard TD3/SAC framework. The approach is designed to overcome the optimization instability inherent in online RL updates from scratch by providing a stable initialization via supervised learning.

## Key Results
- Stable transformer training achieved through behavior cloning on accelerator trajectories
- Training time reduced by up to a factor of two compared to standard approaches
- Replay buffer size decreased to 10-20 thousand from typical 1 million, lowering computational demands
- Successful application to both state-based and image-based tasks in MDP and POMDP settings

## Why This Works (Mechanism)

### Mechanism 1: Stability Transfer via Supervised Warm-Start
If a Transformer is pretrained via Behavior Cloning (BC) on trajectories from a stable "Accelerator" model, it avoids the optimization instability inherent in online RL updates from scratch. The "Accelerator" (e.g., an MLP or LSTM) absorbs the high-variance exploration and optimization noise of the early training phase. The Transformer learns a policy mapping via stable supervised loss (MSE) rather than noisy policy gradients. This initializes the Transformer in a stable region of the loss landscape before it attempts its own online updates.

### Mechanism 2: Sample Efficiency via Curriculum Buffering
Reducing the replay buffer size from standard scales (1M) to 10-20k is viable because the Accelerator provides a high-value "curriculum" that substitutes for massive random exploration. Standard online RL requires large buffers to buffer random exploration until useful signals are found. In this framework, the "Trajectory Buffer" ($\mathcal{T}$) captures the Accelerator's current skills immediately. When the Transformer enters Stage 2 (online fine-tuning), it only needs to refine existing skills, requiring significantly less diverse experience in the Replay Buffer ($\mathcal{B}$).

### Mechanism 3: Decoupled Optimization Landscapes
Using the pre-trained Accelerator's critic to guide the Transformer's actor during Stage 2 stabilizes the bootstrapping process. The Transformer actor $\pi_\theta$ is updated using gradients from the Accelerator critic $Q_\phi$. Since $Q_\phi$ was trained on stable Accelerator transitions and is not updated by the Transformer's potentially unstable gradients, it provides a stationary target for the Transformer to climb.

## Foundational Learning

### Concept: Behavior Cloning (BC)
**Why needed here**: This is the engine of Stage 1. Understanding that the Transformer is not "learning to RL" initially, but merely mimicking a teacher (supervised learning), is critical to debugging Stage 1 failures.
**Quick check question**: If the Accelerator achieves 100 reward but the Transformer (Stage 1) stays at 10, is the BC loss decreasing? (If loss is low but reward is low, check state representation).

### Concept: Actor-Critic Methods (TD3/SAC)
**Why needed here**: The framework relies on the interaction between the Actor (Transformer) and Critic (Accelerator). Understanding the policy gradient update is necessary to implement Stage 2 correctly.
**Quick check question**: In Stage 2, are we updating the Critic's weights? (No, the paper proposes keeping the Accelerator critic fixed or updating it slowly via standard RL, but the code suggests standard off-policy updates on the critic using new Transformer dataâ€”verify this distinction in Algorithm 4).

### Concept: POMDPs (Partially Observable MDPs)
**Why needed here**: The choice of Accelerator depends on the environment type. Using an MLP accelerator for a POMDP will fail regardless of the framework.
**Quick check question**: For a memory-requiring task (e.g., POMDP Hopper), what architecture must the Accelerator be? (LSTM/RNN, as stated in Section 5.4).

## Architecture Onboarding

### Component map
Accelerator Actor ($\pi_\phi$) -> Trajectory Buffer ($\mathcal{T}$) -> Transformer (Stage 1 BC) -> Replay Buffer ($\mathcal{B}$) -> Transformer (Stage 2 Fine-tuning)

### Critical path
1. **Init**: Initialize Accelerator & Transformer
2. **Stage 1**: Accelerator interacts with Env -> Fill $\mathcal{T}$ -> Transformer trains on $\mathcal{T}$ via BC. (Do NOT update Transformer via RL yet)
3. **Switch**: Monitor Accelerator performance. When it plateaus or reaches a threshold, stop Stage 1
4. **Stage 2**: Transformer interacts with Env -> Fill $\mathcal{B}$ -> Update Transformer Actor via Accelerator Critic

### Design tradeoffs
- **Buffer Size vs. Speed**: The paper claims success with 10-20k buffers. Start small (20k) for speed; increase only if fine-tuning is unstable
- **Gradient Ascent**: The paper explores "Additional Gradient Ascent" on the Critic (Alg 3) but finds it **inconsistent** (Section 5.4, RQ3). Default to standard BC unless specific tasks show clear benefit
- **Context Length**: Must be matched to the environment's temporal dependencies (e.g., 3 for MuJoCo, 5 for SAC experiments)

### Failure signatures
- **Stage 1 Divergence**: Transformer reward tracks Accelerator but with high variance. *Fix*: Increase BC dataset size or reduce Transformer learning rate
- **Stage 2 Collapse**: Reward drops immediately upon switching to online. *Fix*: The Transformer is not "ready"; extend Stage 1 pretraining or reduce Stage 2 exploration noise
- **POMDP Failure**: MLP Accelerator works, Transformer fails. *Fix*: Ensure Transformer input includes the sequence context, not just current state

### First 3 experiments
1. **Sanity Check (MDP)**: Run Accelerator (MLP) on a simple MuJoCo task (e.g., HalfCheetah). Verify Transformer can overfit (BC) to the Accelerator's data in Stage 1 without online updates
2. **Buffer Ablation**: Run full pipeline on ManiSkill with Buffer sizes [10k, 50k, 1M]. Verify the performance drop is negligible at 20k vs 1M to confirm efficiency gains
3. **POMDP Transfer**: Run pipeline on a POMDP environment (e.g., POMDP-Hopper) using an LSTM Accelerator. Verify if the Transformer successfully captures the memory requirement implied by the LSTM's sequences

## Open Questions the Paper Calls Out

### Open Question 1
**How can the transition from the acceleration stage to the online fine-tuning stage be automated to remove the need for manual monitoring?**
The authors state in the discussion that "the absence of an automatic switching mechanism... requires continuous involvement and monitoring" of the pretraining phase. The current framework relies on manual termination or simple heuristics like plateauing, lacking a formal condition for optimal switching.

### Open Question 2
**Under what conditions does the additional gradient ascent over the accelerator's critic function provide consistent benefits?**
The key outcome notes "additional gradient ascent providing inconsistent benefits," and Section 5.4 concludes that each case must be evaluated individually as it does not guarantee improvement. The experimental results show success in PullCube but failure in PushCube, indicating a lack of theoretical understanding or predictive criteria for this technique.

### Open Question 3
**Does a substantial performance gap between the accelerator and the transformer's potential limit the final asymptotic performance?**
The paper notes the "accelerator may have weaker performance" but "must be more stable." However, it is not established if a very low-skill accelerator bottlenecks the transformer's capabilities during the behavior cloning stage. The paper demonstrates the transformer follows the accelerator's curve but does not explore scenarios where the accelerator is significantly sub-optimal yet stable.

## Limitations
- Limited testing on complex, sparse-reward environments where the Accelerator might struggle
- Manual transition criteria between training stages rather than automated detection
- Lack of ablation studies isolating the contribution of each component mechanism

## Confidence
- **Stage 1 Stability Transfer**: Medium - Experimental results support the claim but lack ablation studies
- **Sample Efficiency Gains**: Medium - Demonstrated on specific tasks but not extensively validated across diverse environments
- **POMDP Extension**: Medium - Works for ManiSkill but limited testing on more challenging POMDP scenarios

## Next Checks
1. **Ablation Study**: Run experiments isolating the effect of the fixed critic vs. standard critic updates in Stage 2 to verify the claimed stabilization benefit
2. **Transition Criteria**: Implement an automatic transition criterion (e.g., accelerator reward plateau over N episodes) and test if manual vs. automatic switching affects final performance
3. **POMDP Stress Test**: Test the approach on a more challenging POMDP task (e.g., Ant with partial observability) to verify the LSTM Accelerator and Transformer memory handling claims