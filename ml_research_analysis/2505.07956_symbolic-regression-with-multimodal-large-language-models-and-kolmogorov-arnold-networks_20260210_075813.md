---
ver: rpa2
title: Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold
  Networks
arxiv_id: '2505.07956'
source_url: https://arxiv.org/abs/2505.07956
tags:
- function
- functions
- symbolic
- regression
- llm-lex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel symbolic regression approach using
  vision-capable large language models (LLMs) combined with Kolmogorov-Arnold Networks
  (KANs). The method generates plots of univariate functions, prompts LLMs to propose
  ansatzes, and optimizes parameters through genetic algorithms.
---

# Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks

## Quick Facts
- arXiv ID: 2505.07956
- Source URL: https://arxiv.org/abs/2505.07956
- Authors: Thomas R. Harvey; Fabian Ruehle; Kit Fraser-Taliente; James Halverson
- Reference count: 19
- Primary result: Novel symbolic regression approach combining vision-capable LLMs with KANs successfully identifies exact expressions for many test functions

## Executive Summary
This paper introduces a novel symbolic regression approach using vision-capable large language models (LLMs) combined with Kolmogorov-Arnold Networks (KANs). The method generates plots of univariate functions, prompts LLMs to propose ansatzes, and optimizes parameters through genetic algorithms. For multivariate functions, it trains KANs and applies symbolic regression to each edge function. The combined expressions are then simplified using additional LLM processing. The approach successfully identifies exact expressions for many test functions, often outperforming traditional methods like Mathematica's FindFormula. The method is implemented in a publicly available Python package called LLM-LEx for univariate functions and KAN-LEx for multivariate functions.

## Method Summary
The symbolic regression method uses vision-capable LLMs to propose functional ansatzes based on plot images of target functions. For univariate cases, the LLM receives two parent functions and a plot image, then proposes a new ansatz that is optimized using a genetic algorithm with population size 25 over 10 generations. Parameters are fitted using SciPy optimizers. For multivariate functions, the approach trains KANs, prunes the architecture iteratively, applies symbolic regression to each edge function, and combines the resulting expressions. The final expressions are simplified using additional LLM processing with sympy integration.

## Key Results
- Successfully identifies exact expressions for many benchmark functions, including polynomials, exponentials, and trigonometric combinations
- Often outperforms traditional methods like Mathematica's FindFormula on test functions
- The LLM-based method shows particular success in proposing interpretable functional forms that capture the underlying structure of the data
- The KAN extension enables effective multivariate symbolic regression through edge function decomposition

## Why This Works (Mechanism)
The method leverages the LLM's ability to recognize visual patterns in function plots and generate interpretable mathematical expressions. By combining this with genetic algorithm optimization, the approach can efficiently search the space of possible functional forms. The KAN extension allows decomposition of multivariate functions into univariate components that can be individually analyzed using the same LLM-based approach, making the method scalable to higher dimensions.

## Foundational Learning
- **Symbolic Regression**: Mathematical method for discovering functional relationships from data; needed because traditional regression assumes fixed functional forms
- **Genetic Algorithms**: Optimization technique inspired by natural selection; needed for efficiently exploring space of possible functional forms
- **Kolmogorov-Arnold Networks**: Neural network architecture with sum-of-univariate-functions structure; needed for decomposing multivariate functions
- **Vision-Capable LLMs**: Large language models that can process images; needed for interpreting function plots and proposing mathematical expressions
- **Genetic Algorithm Optimization**: Population-based search method; needed for parameter fitting when traditional methods fail
- **Symbolic Expression Simplification**: Process of reducing complex expressions to simpler equivalent forms; needed for producing interpretable results

## Architecture Onboarding

**Component Map**: Plot Image Generation -> LLM Prompt -> Ansatz Proposal -> Genetic Algorithm Optimization -> Parameter Fitting -> Expression Simplification

**Critical Path**: The core workflow involves generating plot images from data points, prompting the LLM with parent functions and the image, receiving an ansatz proposal, optimizing parameters via genetic algorithm, and simplifying the final expression.

**Design Tradeoffs**: The method trades computational efficiency for interpretability and accuracy. Using LLMs for ansatz proposal is computationally expensive but produces more meaningful expressions than purely numerical approaches.

**Failure Signatures**: 
- LLM returns unparseable or syntactically invalid Python expressions
- Genetic algorithm overfitting to noise at high noise levels (ϵ≥0.03)
- Aliasing in highly oscillatory functions with sparse sampling
- Expression simplification fails to reduce complexity

**Three First Experiments**:
1. Test univariate regression on simple polynomial functions (x³, x⁴) with 100 evenly spaced points
2. Evaluate multivariate regression on product functions (sin(x)cos(y)) using KAN decomposition
3. Assess noise sensitivity by testing functions with added Gaussian noise at varying levels (ϵ=0.01, 0.03, 0.05)

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades significantly at high noise levels (ϵ ≥ 0.03), with overfitting to noise rather than identifying true functional forms
- Heavy computational resource requirements for both LLM inference and genetic algorithm optimization
- Method's scalability to higher-dimensional problems and truly noisy real-world data remains largely unproven

## Confidence

**High Confidence**: Basic methodology and benchmark results showing exact identification of simple functions are reproducible and demonstrate the core approach works as intended.

**Medium Confidence**: KAN extension for multivariate functions shows promise but has limited validation. The pruning strategy and edge function selection criteria need more rigorous testing.

**Low Confidence**: The method's performance on truly noisy real-world data and its sensitivity to hyperparameters like population size and temperature settings need systematic investigation.

## Next Checks
1. **Noise Sensitivity Analysis**: Systematically test the method across noise levels from 0.001 to 0.1 using benchmark functions, measuring both accuracy and computational cost to establish practical noise limits.

2. **Model Comparison Study**: Compare results using different vision-capable LLMs (gpt-4o, llama3.3, codestral) with identical settings to quantify the impact of model choice on ansatz quality and overall success rate.

3. **Real-World Data Testing**: Apply the method to actual scientific datasets (e.g., physics simulations, biological measurements) with known underlying relationships to assess performance beyond synthetic benchmarks.