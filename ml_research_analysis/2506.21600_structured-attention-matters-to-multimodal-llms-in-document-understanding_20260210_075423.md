---
ver: rpa2
title: Structured Attention Matters to Multimodal LLMs in Document Understanding
arxiv_id: '2506.21600'
source_url: https://arxiv.org/abs/2506.21600
tags:
- text
- structured
- mllms
- attention
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how input format influences multimodal
  large language models (MLLMs) performance in document understanding. The authors
  discovered that unstructured OCR text often degrades MLLM performance due to attention
  dispersion and loss of structural information.
---

# Structured Attention Matters to Multimodal LLMs in Document Understanding

## Quick Facts
- **arXiv ID**: 2506.21600
- **Source URL**: https://arxiv.org/abs/2506.21600
- **Reference count**: 10
- **Primary result**: Structured LaTeX-like text encoding improves MLLM document understanding accuracy by up to 11.8% compared to unstructured OCR text

## Executive Summary
This paper investigates how input format influences multimodal large language models (MLLMs) performance in document understanding. The authors discovered that unstructured OCR text often degrades MLLM performance due to attention dispersion and loss of structural information. To address this, they propose a novel structure-preserving approach that encodes document elements using the LaTeX paradigm, maintaining hierarchical organization and spatial relationships. Their attention analysis reveals that structured text induces structured attention patterns on both textual and visual content, directing models to focus on semantically meaningful regions while reducing attention waste.

## Method Summary
The authors propose a structure-preserving encoding approach for MLLMs that represents documents using LaTeX-like formatting. This method maintains the hierarchical structure of documents by encoding sections, paragraphs, and sentences with appropriate delimiters and relationships. The approach combines this structured text with visual features from document images, creating a unified representation that preserves both spatial layout and semantic organization. The encoding captures document elements' hierarchical relationships through systematic use of LaTeX syntax, allowing MLLMs to better understand the document's logical structure and spatial arrangement without requiring architectural modifications or additional training.

## Key Results
- Structured text encoding with images significantly improves MLLM accuracy compared to using images alone or images with unstructured OCR text
- Accuracy improvements of up to 11.8% on certain datasets and models when using structured text
- Attention analysis shows structured text induces focused attention patterns on semantically meaningful regions while reducing attention waste
- Experiments conducted on four document understanding benchmarks: MMLongBench, LongDocUrl, PaperTab, and FetaTab

## Why This Works (Mechanism)
The mechanism behind improved performance stems from structured attention patterns induced by LaTeX-like encoding. When documents are represented with hierarchical structure, MLLMs can better parse the logical relationships between different document elements. This structured representation helps the model focus attention on semantically relevant regions rather than dispersing it across unstructured text. The LaTeX paradigm preserves both spatial relationships and semantic hierarchy, enabling more efficient information processing. The attention analysis reveals that structured text guides the model to attend to meaningful content areas while reducing attention waste on irrelevant regions, ultimately improving comprehension and question-answering performance.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**: Neural architectures that process both visual and textual inputs to generate responses. Why needed: Form the baseline models being evaluated and improved through structured input formats.

**OCR Text Formats**: Different ways of representing extracted document text, particularly unstructured vs. structured representations. Why needed: The paper directly compares performance impacts of these different text formats on MLLM comprehension.

**Document Structure Encoding**: Methods for preserving hierarchical relationships in text using delimiters and formatting conventions. Why needed: The proposed solution relies on LaTeX-like encoding to maintain document structure that would otherwise be lost in plain text.

**Attention Mechanisms**: Neural network components that determine which parts of input to focus on during processing. Why needed: The paper's core argument centers on how structured text induces more effective attention patterns in MLLMs.

**Document Understanding Benchmarks**: Standardized datasets for evaluating document comprehension, including MMLongBench, LongDocUrl, PaperTab, and FetaTab. Why needed: These benchmarks provide the empirical foundation for measuring performance improvements.

## Architecture Onboarding

**Component Map**: Document Image → Visual Feature Extractor → Visual Features + Structured Text → MLLM Encoder → Structured Attention Patterns → Enhanced Document Understanding

**Critical Path**: The most important processing sequence is the combination of visual features with structured text input, which then flows through the MLLM encoder to produce the final output. The structured text encoding step is critical as it determines how effectively the model can parse document relationships.

**Design Tradeoffs**: The approach trades increased input complexity (LaTeX-like encoding) for improved comprehension accuracy. This avoids architectural modifications but requires careful text preprocessing. The method is model-agnostic but may not capture all document types equally well.

**Failure Signatures**: Poor performance would manifest when documents have non-standard layouts that don't map well to LaTeX conventions, or when the visual-textual alignment breaks down. Attention dispersion patterns would reappear, indicating the structured encoding isn't being effectively utilized.

**First 3 Experiments**:
1. Replicate baseline experiments comparing unstructured OCR text vs. structured text on MMLongBench to verify core performance improvements
2. Conduct attention visualization analysis on a sample document to confirm structured attention patterns
3. Test ablation studies on different structure levels (sections vs. paragraphs vs. sentences) to identify optimal granularity

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on MLLMs without direct comparison to traditional OCR-free document understanding approaches or specialized document foundation models
- Structural information preservation relies on LaTeX-like encoding, which may not capture all document types or layouts equally well, particularly non-standard or creative document designs
- Attention pattern analysis is based on qualitative visualizations rather than quantitative metrics that could more rigorously demonstrate "structured attention" effects

## Confidence

**High confidence** in the empirical observation that structured text improves MLLM performance over unstructured OCR text

**Medium confidence** in the mechanism explanation (structured attention patterns) due to reliance on visual attention analysis rather than quantitative metrics

**Medium confidence** in the generality of results across different document types and MLLM architectures

## Next Checks
1. Conduct systematic ablation studies across a broader range of MLLM architectures (including both encoder-decoder and decoder-only models) to verify that structured input benefits generalize beyond the tested models

2. Compare performance against specialized document understanding models that process images without OCR to isolate the contribution of structured text encoding versus other factors

3. Develop quantitative metrics for measuring "structured attention" patterns to complement the qualitative visualizations and provide more rigorous validation of the proposed mechanism