---
ver: rpa2
title: LLM-Assisted Topic Reduction for BERTopic on Social Media Data
arxiv_id: '2509.19365'
source_url: https://arxiv.org/abs/2509.19365
tags:
- topic
- topics
- bertopic
- reduction
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses excessive topic redundancy in BERTopic when
  applied to social media data. It proposes combining BERTopic with LLM-assisted topic
  reduction, where semantically similar topics are iteratively merged using LLM prompts.
---

# LLM-Assisted Topic Reduction for BERTopic on Social Media Data

## Quick Facts
- **arXiv ID:** 2509.19365
- **Source URL:** https://arxiv.org/abs/2509.19365
- **Reference count:** 25
- **Primary result:** LLM-assisted topic reduction outperforms baseline HDBSCAN parameter tuning on coherence and diversity for BERTopic on social media data.

## Executive Summary
This paper addresses excessive topic redundancy in BERTopic when applied to noisy social media data by introducing an LLM-assisted iterative merging approach. The method combines BERTopic's efficient topic generation with LLMs' semantic reasoning to reduce overlapping topics while maintaining quality. Experiments on three Twitter datasets using four LLMs show improved topic diversity and coherence compared to increasing HDBSCAN minimum cluster size, with the added benefit of significantly lower computational cost than end-to-end LLM topic modeling approaches.

## Method Summary
The approach first generates an initial set of topics using BERTopic with standard parameters, then represents each topic by its top 10 c-TF-IDF keywords. These representations are provided as input to an LLM, which iteratively identifies and merges the two most semantically similar topics. This process continues until reaching a user-defined target topic count. The method uses semantic reasoning to identify overlap beyond surface-level keyword matching, offering finer control than single-shot clustering while avoiding overwhelming the LLM with large input lists.

## Key Results
- BERTopic+LLM consistently outperformed baseline (increased HDBSCAN `min_cluster_size`) on topic diversity across all datasets
- GPT-4o-mini and Qwen3-30b-a3b achieved the best coherence scores, with GPT-4o-mini also providing cost efficiency
- Llama-3 frequently refused to process the Cyberbullying dataset due to safety filters, limiting its applicability on sensitive content

## Why This Works (Mechanism)

### Mechanism 1
LLMs can identify semantic overlap between topics that keyword-based methods miss. The LLM receives topic representations (either top-10 c-TF-IDF keywords or LLM-generated labels) and is prompted to identify the two most semantically similar topics from the set. This enables merging based on conceptual similarity rather than surface-level keyword overlap. Core assumption: LLMs possess sufficient semantic reasoning to consistently identify meaningful topic relationships across diverse domains. Evidence: The method "iteratively identifies and merges semantically similar topics" using semantic reasoning "beyond surface-level keyword overlap."

### Mechanism 2
Iterative agglomerative merging preserves topic quality better than single-pass reduction. Rather than asking the LLM to cluster all topics at once, the method repeatedly identifies and merges only the two most similar topics, then updates representations. This continues until reaching a user-defined target. Core assumption: Incremental merging prevents overwhelming the LLM's context window and allows representations to stabilize between iterations. Evidence: "Iterative agglomerative merging offers finer control over the number of topics and helps prevent overwhelming the LLM with very large input lists."

### Mechanism 3
Hybrid BERTopic-LLM pipelines achieve better efficiency-accuracy tradeoffs than end-to-end LLM topic modeling. BERTopic handles the computationally expensive step of clustering thousands of documents using efficient embedding-based methods. LLMs are invoked only on the reduced space of topic representations (typically dozens to hundreds), avoiding per-document LLM processing. Core assumption: BERTopic's initial topic generation quality is sufficient for LLM-assisted refinement to improve upon. Evidence: The approach addresses "high computational costs that make [end-to-end LLM approaches] impractical for large-scale social media datasets."

## Foundational Learning

- **Concept: HDBSCAN clustering parameters (minimum cluster size, cluster selection method)**
  - **Why needed here:** These parameters directly control topic granularity and are the baseline comparison point. Understanding them is essential for interpreting why LLM-assisted reduction outperforms simply increasing `minimum_cluster_size`.
  - **Quick check question:** If you increase `minimum_cluster_size` from 100 to 300 on a noisy Twitter dataset, what happens to the number of topics and why might this reduce topic diversity?

- **Concept: c-TF-IDF (class-based Term Frequency-Inverse Document Frequency)**
  - **Why needed here:** This is how BERTopic extracts representative keywords for each topic, which become the LLM's input. Understanding c-TF-IDF helps explain why keyword-based representations sometimes fail to capture semantic overlap.
  - **Quick check question:** How does c-TF-IDF differ from standard document-level TF-IDF, and why is it better suited for extracting topic keywords?

- **Concept: Topic coherence (NPMI) vs. topic diversity metrics**
  - **Why needed here:** The paper evaluates success using both metrics, which often trade off against each other. The Cyberbullying dataset showed improved diversity but degraded coherence—a pattern requiring both concepts to interpret.
  - **Quick check question:** If a topic model achieves high diversity but low NPMI coherence, what does this suggest about the semantic quality of the extracted topics?

## Architecture Onboarding

- **Component map:** Raw Documents → SBERT Embeddings (all-MiniLM-L6-v2) → UMAP Dimensionality Reduction → HDBSCAN Clustering (tunable: min_cluster_size, cluster_selection_method) → c-TF-IDF Topic Keywords (top 10 per topic) → LLM Prompting (topic representations in) → Topic Pair Selection (two most similar) → Merge & Update Representations → [Loop until target topic count reached]

- **Critical path:** HDBSCAN parameter selection → initial topic count → LLM prompt format (keywords vs. labels) → iterative merging loop. Errors in HDBSCAN configuration propagate through the entire pipeline.

- **Design tradeoffs:**
  - **Prompt format:** Top-10 keywords are cheaper (no extra LLM call) but LLM-generated labels may capture semantics better—the paper shows mixed results with no clear winner.
  - **Model selection:** Larger models (Qwen-30B) show strong coherence but smaller models (GPT-4o-mini) often match performance at lower cost. Open-weight models (Llama, Gemma) enable local deployment but refuse sensitive content more frequently.
  - **Initial topic count:** More initial topics gives LLM more merging flexibility but increases iterations and cost.

- **Failure signatures:**
  - **LLM refusals:** Llama-3 failed frequently on Cyberbullying data due to safety filters—check logs for empty responses or refusals before interpreting results as model inadequacy.
  - **Coherence collapse:** On sensitive/noisy datasets (Cyberbullying), LLM methods underperformed baseline on coherence despite improving diversity—this signals domain mismatch rather than method failure.
  - **Representation drift:** If topic labels degrade after multiple merges, consider re-generating from merged document sets rather than combining keyword lists.

- **First 3 experiments:**
  1. **Baseline replication:** Run BERTopic with default settings on a clean dataset (e.g., Trump Tweets), then reduce by increasing `minimum_cluster_size` to establish baseline coherence/diversity curves.
  2. **Single-LLM comparison:** Apply GPT-4o-mini with top-10 keyword prompts to reduce to 2-3 target topic counts, measuring both metrics and API cost per iteration.
  3. **Sensitivity stress test:** Test Llama-3 and GPT-4o-mini on a dataset with known sensitive content, logging refusal rates and comparing coherence/diversity tradeoffs to identify safe operational boundaries.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do LLM-reduced topics align more closely with human judgment than baseline methods, despite mixed automated coherence scores on sensitive data?
  - **Basis in paper:** [explicit] "Future work... incorporating human evaluation may offer a more nuanced assessment of topic quality compared to reliance on automated metrics alone."
  - **Why unresolved:** The study relied entirely on automated metrics (NPMI and diversity), which the authors suggest may not fully capture nuances, particularly for the cyberbullying dataset where LLMs struggled.
  - **What evidence would resolve it:** A user study correlating automated metric scores with human annotator ratings of topic interpretability and coherence.

- **Open Question 2:** Can LLM-assisted topic reduction be effectively applied to highly sensitive content domains where models currently exhibit high refusal rates or semantic inconsistency?
  - **Basis in paper:** [inferred] The authors note that Llama-3 frequently refused to respond to the Cyberbullying dataset and that "LLMs struggle to maintain semantic consistency in this domain," limiting the method's applicability.
  - **Why unresolved:** Current safety alignment in standard LLMs prevents processing harmful text, causing pipeline failures or reduced quality that current prompt engineering did not overcome.
  - **What evidence would resolve it:** Demonstrating a method (e.g., specialized prompting or domain-specific models) that maintains topic coherence on sensitive datasets without triggering safety refusal mechanisms.

- **Open Question 3:** To what extent do advanced prompting strategies improve the accuracy of iterative topic merging compared to the static approach used in this study?
  - **Basis in paper:** [explicit] "Future work could explore more advanced prompting strategies to enhance the identification and merging of overlapping topics, such as dynamic or context-aware prompting."
  - **Why unresolved:** The current implementation used a standard prompt to identify the two most similar topics iteratively; the impact of prompt complexity on merging precision remains untested.
  - **What evidence would resolve it:** Comparative analysis measuring the reduction in redundant topics and coherence scores when using dynamic context injection versus the static prompt.

## Limitations
- Method depends heavily on BERTopic's initial topic quality, which can be unstable on highly noisy datasets
- Performance degrades notably on sensitive content due to LLM safety filters (especially Llama-3 on Cyberbullying data)
- The coherence-diversity tradeoff is dataset-dependent: improved diversity often comes at the cost of NPMI coherence

## Confidence

- **High confidence:** BERTopic+LLM outperforms HDBSCAN parameter tuning on coherence for clean datasets (Trump, Covid19); computational efficiency advantage over end-to-end LLM pipelines
- **Medium confidence:** LLM-generated labels vs. keyword inputs show no clear winner; domain sensitivity (refusals, coherence drops) is real but mechanism-specific
- **Low confidence:** Exact merging mechanics (document reassignment, c-TF-IDF recalculation) and full prompt templates are not specified

## Next Checks

1. **Prompt robustness test:** Run GPT-4o-mini and Llama-3 on the same clean dataset with explicit JSON-format constraints; measure parsing success rates and compare coherence/diversity outcomes.

2. **Merging mechanics audit:** Log document counts and c-TF-IDF keyword changes after each merge iteration; check for representation drift or topic collapse.

3. **Safety filter boundary mapping:** Systematically test Llama-3 refusal rates across datasets with controlled keyword filtering; compare with GPT-4o-mini's refusal profile to quantify operational limits.