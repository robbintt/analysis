---
ver: rpa2
title: 'R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts'
arxiv_id: '2502.20395'
source_url: https://arxiv.org/abs/2502.20395
tags:
- r2-t2
- routing
- multimodal
- arxiv
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts\
  \ improves routing in multimodal MoE models by dynamically adjusting routing weights\
  \ during inference based on reference samples. The method uses neighborhood-based\
  \ optimization\u2014specifically, gradient descent over similar successful tasks\u2014\
  to refine routing without retraining."
---

# R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts

## Quick Facts
- arXiv ID: 2502.20395
- Source URL: https://arxiv.org/abs/2502.20395
- Reference count: 38
- Improves routing in multimodal MoE models by dynamically adjusting routing weights during inference based on reference samples, achieving up to 6.9% accuracy gains.

## Executive Summary
R2-T2 introduces a test-time re-routing method for multimodal Mixture-of-Experts models that optimizes routing weights without retraining. The approach retrieves k nearest neighbors from a reference set of correctly predicted samples and uses neighborhood gradient descent to adjust routing weights toward configurations that succeeded on similar inputs. Tested on MoAI and MoVA models across 8 benchmarks, R2-T2 consistently improves performance, reaching near-oracle levels with minimal overhead. The method addresses expert selection imbalance, particularly over-reliance on ILANG, by shifting weights to more task-appropriate experts.

## Method Summary
R2-T2 operates by first building a reference dataset of correctly predicted samples with stored routing weights and embeddings. During inference, for each test sample, it retrieves k nearest neighbors from this reference set using embedding similarity. It then performs neighborhood gradient descent (NGD) to optimize routing weights by minimizing a kernel-weighted average of neighbor losses. The method includes three strategies: NGD, Kernel Regression, and Mode Finding, with NGD achieving the best results. The optimization uses cosine annealing learning rates over 10 steps, updating routing weights to improve predictions without modifying model parameters.

## Key Results
- R2-T2 improves accuracy on all 8 tested benchmarks, with up to 6.9% gains (71.5 points on MME-P for MoVA-7B)
- Matches or exceeds performance of larger VLMs while using smaller 7B models
- Reaches near-oracle performance, capturing 70-80% of potential improvement
- Corrects mispredictions by shifting from over-relied ILANG experts to more suitable ones like IAUX or LAUX

## Why This Works (Mechanism)

### Mechanism 1: Reference-Guided Routing Transfer
Routing weights from correctly predicted similar samples provide actionable optimization signals for test samples, conditional on task similarity in embedding space. Given a test sample x, R2-T2 retrieves k nearest neighbors from a reference set of correctly predicted samples. The routing weights r are then adjusted toward the configurations that succeeded on similar inputs, exploiting the assumption that expert utility correlates with task structure.

### Mechanism 2: Neighborhood Gradient Descent (NGD) Surrogate Loss
A kernel-weighted average of neighbor losses serves as a practical surrogate for the oracle loss, enabling gradient-based optimization without ground truth labels. NGD computes L(r) = Σ K(xᵢ, x) × ℓ[f(xᵢ, r), yᵢ] / Σ K(xᵢ, x) over neighbors, then updates r ← r − λ∇ᵣL(r) for multiple steps. This transfers gradient information from labeled neighbors to the unlabeled test sample.

### Mechanism 3: Expert Selection Correction via Over-reliance Mitigation
The original router over-relies on a single expert (ILANG), and R2-T2 corrects this by redistributing weight toward task-appropriate experts. Analysis shows the base router excessively favors ILANG (language-aligned visual expert). Re-routing shifts top-1 expert from ILANG to alternatives (IAUX for spatial reasoning, LIMG for fine-grained visual details, LAUX for external knowledge), improving accuracy.

## Foundational Learning

- **Mixture-of-Experts Routing**: Understanding how routers produce weights and aggregate expert outputs is essential since R2-T2 operates on routing weight vectors r. Quick check: Can you explain why a softmax gating function might produce sub-optimal weights for out-of-distribution inputs?

- **k-Nearest Neighbors in Embedding Space**: R2-T2 depends on finding semantically similar reference samples. The choice of embedding model and kernel function directly affects neighbor quality. Quick check: Why might Gaussian kernels outperform linear kernels for similarity in high-dimensional embedding spaces?

- **Test-Time Adaptation Objectives**: R2-T2 is training-free but optimizes r at test time. Understanding the trade-off between oracle loss (unavailable) and surrogate loss (practical) clarifies the design. Quick check: What is the risk of using a surrogate loss that poorly approximates the true objective?

## Architecture Onboarding

- **Component map**: Reference set -> Embedding model (NV-Embed-V2) -> kNN search -> Gradient computation -> Update routing weights
- **Critical path**: 1. Embed test question → 2. kNN search in reference set → 3. Compute neighbor losses → 4. Accumulate weighted gradients → 5. Update r → 6. Inference with refined weights
- **Design tradeoffs**: Accuracy vs. FLOPs (R2-T2 requires 6–7× more FLOPs than base model), k choice (k=5 balances context and noise), step count (10 steps captures most gains)
- **Failure signatures**: Performance degradation when reference set lacks task-relevant samples, oscillating predictions when gradient steps are too large, no improvement when original routing is already near-optimal
- **First 3 experiments**: 1. Run oracle baseline on held-out samples to quantify routing sub-optimality gap (should see ≥10% improvement). 2. Compare NV-Embed-V2 vs Sentence-BERT (expect ~3% accuracy difference). 3. Visualize k=5 retrieved samples for failure cases to verify semantic relevance.

## Open Questions the Paper Calls Out

### Open Question 1
Can R2-T2 be effectively applied to standard sparse Mixture-of-Experts (MoE) architectures that route between hidden-layer experts (e.g., MLPs) rather than distinct input encoders? The methodology is demonstrated exclusively on multimodal models (MoAI, MoVA) that use MoE at the input level. Applying this to token-level routing in LLMs would require defining a new, effective embedding space for individual tokens or sequences.

### Open Question 2
Is the 6-7x computational overhead (in FLOPs) of the NGD strategy acceptable in real-world deployment scenarios compared to simply using a larger dense model? The paper demonstrates accuracy gains over larger models but does not compare wall-clock latency or energy consumption against those larger models.

### Open Question 3
How does R2-T2 performance degrade when the reference set lacks high-density clusters of correctly predicted samples for out-of-distribution tasks? The method implicitly assumes that for a given test sample, there exist similar neighbors in the reference set. If the base model is weak on a specific domain, the reference set will contain few or no correct samples to guide the gradient descent.

## Limitations
- **Architectural dependence**: Effectiveness hinges on ILANG over-reliance being a common failure mode; may not generalize to models with balanced routing
- **Reference set quality**: Performance depends on having diverse, representative reference sets; small or biased sets could limit effectiveness
- **Embedding model sensitivity**: Method's success relies on embeddings capturing relevant task similarity dimensions; poor embeddings undermine neighbor retrieval

## Confidence
- **High confidence**: Claims about performance improvements on tested benchmarks and basic NGD mechanism functioning as described
- **Medium confidence**: Generalization claims to other MoE architectures and assertion that ILANG over-reliance is primary failure mode
- **Low confidence**: Claims about computational overhead without accounting for hardware-specific optimizations and exact mechanism for extracting routing weights

## Next Checks
1. Apply R2-T2 to a different MoE architecture (e.g., LLaVA-NeXT) and verify whether ILANG over-reliance remains dominant and provides similar gains
2. Systematically vary reference set size (100, 500, 5000, 10000 samples) and measure impact on accuracy and neighbor quality
3. Replace NV-Embed-V2 with alternative embedding models (Sentence-BERT, CLIP) and measure impact on neighbor retrieval quality and overall performance