---
ver: rpa2
title: Large Language Models Often Say One Thing and Do Another
arxiv_id: '2503.07003'
source_url: https://arxiv.org/abs/2503.07003
tags:
- uni00000044
- uni00000048
- uni00000057
- uni00000010
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Words and Deeds Consistency Test (WDCT)
  to evaluate whether large language models (LLMs) exhibit consistency between their
  stated opinions/values and their actions in specific situations. The benchmark pairs
  aligned word and deed questions across four domains (opinion, non-ethical value,
  ethical value, theory) to quantitatively measure inconsistency.
---

# Large Language Models Often Say One Thing and Do Another

## Quick Facts
- arXiv ID: 2503.07003
- Source URL: https://arxiv.org/abs/2503.07003
- Reference count: 15
- Key outcome: 12 LLMs show widespread inconsistency between stated opinions/values and actual actions across four domains, with average consistency scores around 50-76%

## Executive Summary
This paper introduces the Words and Deeds Consistency Test (WDCT) to evaluate whether large language models exhibit consistency between their stated opinions/values and their actions in specific situations. The benchmark pairs aligned word and deed questions across four domains (opinion, non-ethical value, ethical value, theory) to quantitatively measure inconsistency. Evaluation of 12 LLMs shows widespread inconsistency between words and deeds, with average consistency scores around 50-76% across models and domains. Separate alignment on either words or deeds through supervised fine-tuning and direct preference optimization poorly influences the other aspect, supporting the hypothesis that knowledge guiding word vs. deed choices exists in separate spaces. Common knowledge generalization methods like explicit reasoning and data augmentation show limited effectiveness in improving consistency, highlighting the need for architectural-level solutions to this systemic problem.

## Method Summary
The authors developed the Words and Deeds Consistency Test (WDCT) benchmark to systematically evaluate LLM consistency between stated beliefs and actual behavior. The benchmark pairs word questions (asking for opinions/values) with deed questions (asking how the model would act in specific situations) across four domains. The study evaluated 12 popular LLMs using this benchmark and measured consistency scores. The researchers also tested various alignment approaches including supervised fine-tuning and direct preference optimization to see if separately aligning on words or deeds would improve overall consistency. Additionally, they explored knowledge generalization methods like explicit reasoning and data augmentation to determine if these could improve consistency between words and deeds.

## Key Results
- WDCT benchmark reveals widespread inconsistency between LLMs' stated opinions/values and their actual actions
- Average consistency scores across 12 models range from 50-76%, with no model achieving perfect consistency
- Separate alignment on words or deeds through SFT and DPO poorly influences the other aspect
- Data augmentation and explicit reasoning methods show limited effectiveness in improving consistency
- The underlying knowledge guiding word vs. deed choices appears to exist in separate representational spaces

## Why This Works (Mechanism)
The paper demonstrates that LLMs can maintain separate knowledge spaces for declarative beliefs ("words") and behavioral policies ("deeds"), leading to inconsistency when these aspects are not properly aligned during training. Standard alignment techniques that optimize for either words or deeds independently fail to synchronize these aspects, resulting in models that may express certain values but act contrary to them in specific situations.

## Foundational Learning
- **Word-deed inconsistency problem**: Why needed - Understanding that LLMs can express beliefs that don't align with their actions is crucial for evaluating real-world deployment risks. Quick check - Does the model's stated ethical position match its recommended actions in test scenarios?
- **Separate representational spaces**: Why needed - Recognizing that word knowledge and deed knowledge may be stored differently helps explain why standard alignment fails. Quick check - Can the model maintain consistency when asked the same question in different formats?
- **Alignment method limitations**: Why needed - Understanding why SFT and DPO alone are insufficient prevents false confidence in model alignment. Quick check - Does improving performance on word questions improve deed question performance?
- **Benchmark construction methodology**: Why needed - Knowing how to construct paired word-deed questions enables replication and extension of the research. Quick check - Are the word and deed questions truly testing the same underlying value or opinion?
- **Consistency measurement**: Why needed - Having quantitative metrics for measuring consistency enables systematic evaluation and comparison. Quick check - What consistency score threshold indicates acceptable alignment?

## Architecture Onboarding
- **Component map**: Input question → Question type classification (word/deed) → Separate knowledge retrieval spaces → Response generation → Consistency evaluation
- **Critical path**: Question understanding → Knowledge space selection → Response generation → Cross-validation with paired question
- **Design tradeoffs**: The architecture trades off between specialized knowledge spaces for words vs. deeds versus unified representation spaces that might improve consistency but potentially reduce specialized reasoning capabilities
- **Failure signatures**: Inconsistency scores below 70%, high variance in responses to paired word-deed questions, responses that contradict stated values in action-oriented queries
- **3 first experiments**: 1) Test consistency on additional domains beyond the four evaluated, 2) Evaluate newer LLMs not included in the original study, 3) Test whether joint alignment on paired word-deed datasets improves consistency compared to separate alignment

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can specific architectural modifications to LLMs enforce a unified representation space where declarative beliefs ("words") and behavioral policies ("deeds") are stored and retrieved jointly?
- Basis in paper: [explicit] The authors conclude that data augmentation is insufficient and state, "it does not solve the fundamental problem, which we believe requires solutions at the model architecture level and further exploration."
- Why unresolved: The study demonstrated that current alignment techniques (SFT, DPO) fail to synchronize these aspects, but did not propose or test architectural changes to the transformer model itself.
- What evidence would resolve it: A study evaluating modified transformer architectures (e.g., with explicit belief-action modules) that show significantly higher consistency scores on the WDCT benchmark compared to standard models.

### Open Question 2
- Question: Does joint alignment on strictly paired word-deed datasets prevent the degradation of consistency observed when aligning these aspects separately?
- Basis in paper: [inferred] The authors show that "separate alignment on words or deeds leads to poor and unpredictable effects on the other aspect," suggesting that standard independent alignment pipelines are the cause of the issue.
- Why unresolved: The experiments aligned words or deeds in opposite directions to test generalization, but did not test a training regime where the model is optimized for consistency on paired data simultaneously.
- What evidence would resolve it: Experiments showing that RLHF or DPO performed on synchronized word-deed pairs results in higher consistency and less divergence than the standard alignment methods tested in the paper.

### Open Question 3
- Question: Are the internal representations of "words" and "deeds" located in distinct, non-overlapping regions of the model's parameter space, and can mechanistic interpretability techniques isolate these circuits?
- Basis in paper: [explicit] The authors hypothesize that "the underlying knowledge guiding LLMs' choices of words or deeds is not contained within a unified space," but treat this as a theoretical explanation for behavioral results rather than a proven mechanistic fact.
- Why unresolved: The paper relies on black-box evaluation metrics; it does not analyze internal neuron activations or attention heads to prove that the knowledge is physically separated within the network.
- What evidence would resolve it: Probing experiments or causal tracing that visualizes distinct activation patterns for word-based versus deed-based queries, confirming the lack of a shared representational basis.

## Limitations
- The study's scope is limited to four specific domains (opinion, non-ethical value, ethical value, theory) which may not generalize to all contexts where alignment matters
- The inconsistency rates depend heavily on specific question construction methodology and may vary with different prompt engineering approaches
- The paper tested a limited set of alignment and consistency improvement methods, leaving open the possibility that other techniques might perform better
- The hypothesis about separate representational spaces is based on behavioral evidence rather than direct mechanistic analysis of model internals

## Confidence
- High confidence in the empirical observation of word-deed inconsistency across multiple models and domains
- Medium confidence in the conclusion that separate alignment spaces exist for words and deeds
- Medium confidence in the effectiveness of tested consistency improvement methods
- Low confidence in specific architectural-level solutions that would address the problem

## Next Checks
1. Replicate the WDCT benchmark across a broader range of domains and real-world scenarios to assess generalizability of the inconsistency findings
2. Test additional alignment and consistency improvement methods beyond those evaluated, including newer fine-tuning approaches and different prompting strategies
3. Conduct human evaluation studies to validate whether the identified inconsistencies align with human judgments of appropriate behavior in the tested scenarios