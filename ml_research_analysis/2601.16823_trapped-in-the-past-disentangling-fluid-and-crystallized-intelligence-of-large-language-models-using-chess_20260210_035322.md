---
ver: rpa2
title: Trapped in the past? Disentangling fluid and crystallized intelligence of large
  language models using chess
arxiv_id: '2601.16823'
source_url: https://arxiv.org/abs/2601.16823
tags:
- positions
- reasoning
- intelligence
- chess
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether Large Language Models (LLMs) rely
  on memorization or reasoning when solving chess problems. Using chess as a controlled
  testbed, the authors construct three categories of positions based on their likelihood
  of appearing in training data: within-distribution (common positions), near-distribution
  (structurally similar but unseen), and out-of-distribution (novel positions).'
---

# Trapped in the past? Disentangling fluid and crystallized intelligence of large language models using chess

## Quick Facts
- arXiv ID: 2601.16823
- Source URL: https://arxiv.org/abs/2601.16823
- Authors: Leonard S. Pleiss; Maximilian Schiffer; Robert K. von Weizsäcker
- Reference count: 18
- Primary result: LLMs rely on memorized patterns for chess, failing on novel positions despite reasoning augmentation

## Executive Summary
This study investigates whether Large Language Models rely on memorization or reasoning when solving chess problems. Using chess as a controlled testbed, the authors construct three categories of positions based on their likelihood of appearing in training data: within-distribution (common positions), near-distribution (structurally similar but unseen), and out-of-distribution (novel positions). They evaluate GPT-3.5, GPT-4, and GPT-5 on these positions using centipawn loss and illegal move rates as performance metrics. Results show a clear gradient: performance degrades as positions become less likely to have been seen during training. In out-of-distribution positions, models perform no better than random play, even with reasoning enabled. Newer model generations show consistent but diminishing improvements, with the smallest gains in out-of-distribution tasks.

## Method Summary
The study generates 1,500 chess positions across three categories: within-distribution (WD, ≥1,000 database occurrences), near-distribution (ND, 10 random moves from start), and out-of-distribution (OOD, random piece placement). Models (GPT-3.5, GPT-4o, GPT-5) are queried via OpenAI API with standardized JSON prompts requesting moves in SAN/UCI format plus reasoning. Performance is evaluated using Stockfish 17.1 at depth 30, calculating centipawn loss (CPL) and illegal move rates. The framework tests whether models can generalize beyond memorized patterns by measuring performance degradation as training data likelihood decreases.

## Key Results
- Performance consistently degrades as positions become less likely to have been seen during training
- In out-of-distribution positions, models perform no better than random play, even with reasoning enabled
- Newer model generations show consistent but diminishing improvements, with smallest gains in out-of-distribution tasks
- Reasoning augmentation helps but provides diminishing marginal returns for out-of-distribution generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance degrades systematically as the likelihood of encountering a position during training decreases.
- Mechanism: The combinatorial explosion of chess positions (~10^120 possible games) creates a natural distribution shift where early-game positions are frequently represented in training corpora while mid-game positions become exponentially rare. Models rely on pattern matching to recalled positions rather than first-principles reasoning, causing performance collapse when recognizable patterns are absent.
- Core assumption: Position frequency in the Lichess Masters database approximates likelihood of appearance in LLM training data.
- Evidence anchors:
  - [abstract]: "performance consistently degrades as fluid intelligence demands increase. Notably, in out-of-distribution tasks, performance collapses to random levels."
  - [Section 4, Results]: "The Average Centipawn Loss (ACPL) in ND positions is 4.75× higher than in WD positions and 7.79× higher in OOD ones."
  - [corpus]: Related paper "Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation" similarly questions whether LLMs possess genuine fluid intelligence for novel situations, but uses different methodology.
- Break condition: If training corpora contained systematically different chess positions than public databases (e.g., synthetic game data), the proximity approximation would fail.

### Mechanism 2
- Claim: Chain-of-thought reasoning amplifies crystallized intelligence but provides diminishing marginal returns for out-of-distribution generalization.
- Mechanism: Reasoning tokens help models locate and retrieve relevant memorized patterns more effectively, but cannot synthesize novel solutions when no proximate training examples exist. The model allocates more computational resources to OOD tasks (16,952 tokens vs 3,426 for WD) yet achieves worse outcomes per token.
- Core assumption: Assumption: Chain-of-thought in GPT-5 operates primarily as pattern-retrieval optimization rather than symbolic inference.
- Evidence anchors:
  - [abstract]: "reasoning-augmented inference improves performance, its marginal benefit per token decreases with distributional proximity."
  - [Section 4, Figure 4]: "performance gains through reasoning per token decreases sharply with decreasing training data likelihood... dropping by 88.56% [for OOD vs WD]."
  - [corpus]: Weak corpus evidence on this specific mechanism; related work focuses on multi-agent reasoning rather than marginal utility of reasoning tokens.
- Break condition: If reasoning mechanisms were fundamentally redesigned to include explicit symbolic manipulation or external tool use, marginal returns might not degrade.

### Mechanism 3
- Claim: Scaling model size yields progressively smaller improvements in fluid intelligence relative to crystallized intelligence.
- Mechanism: Larger models expand the memorized pattern library and improve interpolation within the training manifold, but do not develop qualitatively new mechanisms for extrapolation. The improvement rate from GPT-4 to GPT-5 dropped 69.36% for ND and 36.94% for OOD positions compared to the previous generation transition.
- Core assumption: Assumption: The observed deceleration reflects architectural constraints rather than temporarily exhausted training data or suboptimal training regimes.
- Evidence anchors:
  - [abstract]: "While newer models improve, progress slows significantly for tasks outside the training distribution."
  - [Section 4, Figure 2]: Projected improvement rates for OOD would drop to 2.3% within two generations if trends continue.
  - [corpus]: ARC-AGI-2 paper reports LLMs solve fewer than 3% of tasks, consistent with persistent fluid intelligence limitations across scaling.
- Break condition: If architectural innovations (not mere scale) were introduced—such as explicit symbolic reasoning modules—this trend would not necessarily hold.

## Foundational Learning

- **Concept: Within-distribution vs. Out-of-distribution Generalization**
  - Why needed here: The entire experimental framework hinges on distinguishing memorized performance (WD) from adaptive reasoning (OOD).
  - Quick check question: Can you explain why a model could achieve high accuracy on a benchmark while having zero genuine reasoning capability?

- **Concept: Centipawn Loss as Decision Regret**
  - Why needed here: This is the primary quantitative metric for move quality; understanding it is essential for interpreting all results.
  - Quick check question: If a model has an average centipawn loss of 300, does this indicate minor inaccuracies or game-losing blunders?

- **Concept: Chain-of-Thought Reasoning Marginal Utility**
  - Why needed here: The paper's key finding is that reasoning tokens have diminishing returns outside the training distribution—this challenges assumptions about reasoning as a general capability amplifier.
  - Quick check question: If doubling reasoning tokens improves WD performance by 20% but OOD performance by only 2%, what does this suggest about the mechanism?

## Architecture Onboarding

- **Component map:**
  - Position Generator → creates WD (BFS from opening, n≥1000 database occurrences), ND (10 random legal moves from start, excluded from database), OOD (random placement of 10 pieces per side)
  - FEN Encoder → standardized position representation input to models
  - Model Interface → OpenAI API calls with temperature=0, standardized JSON prompts
  - Stockfish 17.1 Evaluator → depth-30 search producing centipawn evaluations
  - Metric Calculator → CPL (max(0, eval_before - eval_after)), illegal move rate

- **Critical path:**
  1. Generate positions with verified distribution proximity
  2. Query models with identical prompts
  3. Validate move syntax (SAN/UCI)
  4. Evaluate with engine at fixed depth
  5. Aggregate by condition and model generation

- **Design tradeoffs:**
  - Random piece placement for OOD creates positions that are tactically valid but strategically unnatural—this isolates fluid reasoning but may underestimate models trained on human-game distributions
  - Using only GPT lineage controls for architectural confounders but limits generalizability to other model families
  - Engine evaluation is not ground truth but serves as sufficiently high-resolution proxy for move quality

- **Failure signatures:**
  - High illegal move rate (>30% for GPT-5 OOD) indicates syntactic rule-following breakdown, not just strategic weakness
  - Performance no better than random legal move policy indicates complete absence of fluid reasoning
  - Increasing token expenditure with decreasing performance indicates inefficient search in unfamiliar solution spaces

- **First 3 experiments:**
  1. Replicate with a different formal system (e.g., legal contract generation with novel clause combinations) to test if the WD→OOD gradient generalizes beyond chess.
  2. Introduce a "familiarity oracle" by fine-tuning a small model on a known corpus, then evaluate whether the same degradation pattern emerges with precisely known training distribution.
  3. Test whether providing explicit rules (e.g., chess rule documentation in context) reduces OOD illegal move rates, which would distinguish rule-ignorance from rule-application failure.

## Open Questions the Paper Calls Out

None

## Limitations
- Distribution proximity approximation: Position frequency in Lichess database may not accurately reflect LLM training data exposure
- Chess as general intelligence testbed: Unclear whether chess-specific limitations transfer to other domains
- Reasoning mechanism opacity: Cannot determine if chain-of-thought represents genuine symbolic manipulation or extended pattern retrieval
- Temporal validity: Results collected August-October 2025 may not generalize to future model versions

## Confidence
- **High confidence** (4+ independent evidence anchors):
  - Performance degrades systematically with decreasing training data proximity
  - Chain-of-thought reasoning provides diminishing marginal returns for OOD tasks
  - Newer model generations show progressively smaller improvements in fluid intelligence

- **Medium confidence** (2-3 independent evidence anchors):
  - Current LLMs are fundamentally limited in systematic generalization
  - Centipawn loss differences reflect genuine reasoning limitations rather than syntactic rule-following failures

- **Low confidence** (1-2 evidence anchors):
  - The observed deceleration will continue to 2.3% improvement within two generations
  - Chess-specific findings generalize to other formal systems

## Next Checks
1. **Cross-domain generalization test**: Replicate the WD→ND→OOD framework using legal contract generation with novel clause combinations. If models show similar degradation patterns when moving from familiar contract templates to structurally novel agreements, this would strengthen claims about fundamental fluid intelligence limitations rather than chess-specific effects.

2. **Controlled familiarity experiment**: Fine-tune a small model on a precisely known corpus (e.g., a specific book corpus), then evaluate its performance on held-out positions using the same WD→ND→OOD framework. This would test whether the gradient persists when training distribution is perfectly known, isolating whether the effect is about familiarity or something more fundamental.

3. **Rule-documentation intervention**: For OOD positions, provide explicit chess rule documentation in the model context before move generation. If this reduces illegal move rates significantly while maintaining high CPL, it would distinguish between rule-ignorance and rule-application failure, clarifying whether the limitation is knowledge-based or reasoning-based.