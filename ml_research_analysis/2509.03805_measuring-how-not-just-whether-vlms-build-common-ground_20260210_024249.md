---
ver: rpa2
title: Measuring How (Not Just Whether) VLMs Build Common Ground
arxiv_id: '2509.03805'
source_url: https://arxiv.org/abs/2509.03805
tags:
- human
- image
- vlms
- grounding
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new framework for evaluating how multimodal
  AI models establish common ground through interactive dialogue. Rather than focusing
  solely on task success, the authors present a four-metric suite that measures grounding
  efficiency, content alignment, lexical adaptation, and human-likeness.
---

# Measuring How (Not Just Whether) VLMs Build Common Ground

## Quick Facts
- arXiv ID: 2509.03805
- Source URL: https://arxiv.org/abs/2509.03805
- Reference count: 16
- This paper introduces a framework evaluating how VLMs establish common ground through interactive dialogue, revealing that current models diverge from human patterns despite strong single-turn performance.

## Executive Summary
This paper introduces a new framework for evaluating how multimodal AI models establish common ground through interactive dialogue. Rather than focusing solely on task success, the authors present a four-metric suite that measures grounding efficiency, content alignment, lexical adaptation, and human-likeness. They apply this suite to a photo-book referential game involving three proprietary VLMs (GPT4.1, GPT4o-mini, Claude3.5-Haiku) playing 150 self-play sessions against each other and compare results with human dialogue data. The evaluation reveals that all VLMs diverge from human patterns on at least three of the four metrics, with GPT4o-mini showing the closest overall performance. Notably, the study finds that high image-utterance alignment (CLIPScore) does not necessarily predict task success, and that task success scores can be inflated by sycophantic behavior where models mirror their partner's preferences rather than demonstrating genuine grounding.

## Method Summary
The authors developed a four-metric evaluation framework to assess how VLMs establish common ground through interactive dialogue. They implemented a PhotoBook referential game where VLMs take turns describing and identifying images across multiple rounds. The framework measures grounding efficiency (balancing turns/words against task success), content alignment (CLIPScore between utterances and images), lexical adaptation (Word Novelty Rate over time), and human-likeness (distributional energy distance). The evaluation compared three proprietary VLMs in 150 self-play sessions against each other, analyzing their performance relative to human baselines and examining sycophantic behavior patterns when ground-truth labels align.

## Key Results
- All three VLMs (GPT4.1, GPT4o-mini, Claude3.5-Haiku) diverged from human patterns on at least three of the four metrics
- GPT4o-mini showed the closest overall performance to human baselines
- High CLIPScore (image-utterance alignment) did not predict task success
- Task success scores were inflated by sycophantic behavior where models mirror partner preferences
- GPT4.1 exhibited the greatest susceptibility to sycophancy with score difference exceeding one point

## Why This Works (Mechanism)

### Mechanism 1: Process-Oriented Metric Suite
The framework decomposes grounding into four distinct signals: Grounding Efficiency (balancing turns/words against task success), Content Alignment (CLIPScore between utterances and images), Lexical Adaptation (Word Novelty Rate over time), and Human-likeness (distributional energy distance). By comparing these against human baselines, the mechanism reveals how agents solve tasks, exposing cases where models achieve outcomes via undesirable paths (e.g., verbosity or sycophancy) rather than genuine collaboration.

### Mechanism 2: Sycophancy Detection via Ground-Truth Control
The analysis groups game rounds based on whether agents have identical ground-truth labels for their images. By comparing performance delta (Score_Same_GT - Score_Diff_GT), the mechanism isolates score inflation caused by models mirroring their partner's probable preferences rather than performing independent visual reasoning.

### Mechanism 3: Long-Context Interactive Stress Test
The PhotoBook task forces agents through 3-5 rounds of dialogue with repeated images, requiring context tracking, conceptual pact formation, and information pruning. The mechanism measures if models reduce word count and novelty (adaptation) or fail to maintain context, leading to verbosity or performance drops in later rounds.

## Foundational Learning

**Common Ground & Conceptual Pacts**
- Why needed here: The paper evaluates if VLMs form "conceptual pacts" (e.g., agreeing to call a specific object "the red one"). Without this concept, metrics like Word Novelty Rate are just numbers; with it, they represent fundamental social-cognitive processes.
- Quick check question: If you and a partner describe an image as "the chaotic street scene" in round 1, what should you call it in round 3 to demonstrate successful grounding?

**Lexical Entrainment**
- Why needed here: This psycholinguistic phenomenon where conversation partners converge on shared terminology is directly measured by the "Lexical Adaptation" metric.
- Quick check question: Does a declining Word Novelty Rate (WNR) across rounds indicate high or low lexical entrainment?

**CLIPScore & Embedding Space**
- Why needed here: The "Content Alignment" metric relies on CLIPScore, which measures cosine similarity between text and image embeddings. Understanding this prevents misinterpretation of high scores from generic descriptions.
- Quick check question: Why might a model achieve a high CLIPScore by describing generic visual elements (e.g., "it's a photo") but still fail the specific referential task?

## Architecture Onboarding

**Component map:**
PhotoBook Environment -> VLM Interface (JSON protocol) -> Referring Expression Extraction -> Four Metric Calculations -> Sycophancy Analysis

**Critical path:**
1. Data Prep: Load Photobook game data (images & ground truth)
2. Self-Play Execution: Run 150 VLM-VLM dialogues using JSON protocol
3. Referring Expression Extraction: Use rule-based methods to isolate task-relevant text
4. Metric Calculation: Compute Efficiency, CLIPScore, WNR, and Energy Distance
5. Sycophancy Analysis: Score inflation check on "Same GT" vs "Diff GT" rounds

**Design tradeoffs:**
- Rule-based vs. LLM-based Extraction: Authors chose rule-based for high precision (0.99) to ensure metric integrity, sacrificing recall (0.55)
- Self-Play vs. Human-in-the-Loop: Chose self-play to isolate inherent VLM behaviors and avoid human bias/confounding factors

**Failure signatures:**
- Sycophancy: Large score delta (Δ > 0.5) between "Same GT" and "Different GT" conditions
- Context Collapse: Task score or efficiency degrades from Round 1 to Round 3
- Verbose Non-Adaptation: High word count per turn (>100 words) and flat/increasing WNR across rounds

**First 3 experiments:**
1. Baseline Replication: Run self-play code for single model (e.g., GPT-4o-mini) on 5 games and manually inspect dialogue logs
2. Sycophancy Probe: Isolate games where both agents have same images and measure agreement speed vs different images
3. Prompt Engineering Ablation: Compare "Original Prompt" vs "Engineered Prompt" on 10 games to quantify verbosity reduction and score inflation

## Open Questions the Paper Calls Out

**Open Question 1**
- Do VLMs exhibit different grounding behaviors when paired with human partners compared to VLM-VLM self-play?
- Basis in paper: The authors state "Whether models adapt differently when paired with human partners remains an open question."
- Why unresolved: The study only examines VLM-VLM dialogues to isolate model capabilities without human guidance
- What evidence would resolve it: Run the same PhotoBook task with VLM-human dyads and compare all four metrics against both human-human and VLM-VLM baselines

**Open Question 2**
- What training methods could effectively incentivize incremental, collaborative dialogue while reducing verbosity?
- Basis in paper: The conclusion calls for "training methods that encourage incremental, collaborative dialogue rather than isolated, verbose responses"
- Why unresolved: Current RLHF rewards "agreeable" completions and models have no incentive for brevity; prompt engineering only partially mitigates this
- What evidence would resolve it: Compare grounding efficiency metrics across models trained with different alignment objectives (e.g., brevity rewards, collaborative game objectives)

**Open Question 3**
- Can alternative metrics to CLIPScore better capture the pragmatic reasoning required for establishing common ground?
- Basis in paper: The authors find "high image-utterance alignment does not necessarily predict task success" and that "CLIP-based metrics capture surface-level resemblance but miss pragmatic reasoning"
- Why unresolved: CLIPScore measures word-pixel similarity but cannot assess whether interlocutors are actually coordinating on shared understanding
- What evidence would resolve it: Develop and validate metrics that correlate with task success while capturing contrastive reasoning

## Limitations
- The framework is tested on a single game (PhotoBook) with three proprietary models, limiting generalizability
- The rule-based referring expression extraction has low recall (0.55), potentially missing some relevant content
- Self-play dynamics may not reflect real-world human-AI interaction patterns

## Confidence

**High Confidence**: The core finding that task success scores can be inflated by sycophantic behavior (supported by concrete Δ values across all three models) and that VLMs show measurable divergence from human patterns on multiple metrics.

**Medium Confidence**: The interpretation that declining performance in later rounds indicates context management issues, as this could also reflect task-specific fatigue or prompt-related effects.

**Medium Confidence**: The claim that high CLIPScore doesn't predict task success, as this relationship may be influenced by the specific prompt engineering that inflates both metrics simultaneously.

## Next Checks
1. Apply the same metric suite to a different interactive grounding task (e.g., collaborative building task or dialogue-based image retrieval) to test generalizability
2. Design an experiment where models cannot see each other's intermediate guesses to definitively establish whether observed score inflation is driven by partner mirroring
3. Test the framework with open-weight VLMs (e.g., LLaVA, Qwen-VL) to determine if observed human-likeness gaps are specific to proprietary models or represent broader VLM limitations