---
ver: rpa2
title: 'FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems
  with Adaptive and Robust Adversarial Training'
arxiv_id: '2511.22872'
source_url: https://arxiv.org/abs/2511.22872
tags:
- attribute
- user
- unlearning
- training
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of attribute unlearning in user-level
  federated recommender systems, where sensitive user attributes embedded in model
  parameters are vulnerable to inference attacks. Unlike group-level settings, user-level
  federated learning restricts cross-user information sharing, making existing unlearning
  methods infeasible.
---

# FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training

## Quick Facts
- arXiv ID: 2511.22872
- Source URL: https://arxiv.org/abs/2511.22872
- Reference count: 15
- Primary result: Achieves up to 26.42% reduction in balanced accuracy for attribute unlearning while maintaining superior recommendation performance in user-level federated learning

## Executive Summary
FedAU2 addresses the critical challenge of attribute unlearning in user-level federated recommender systems, where sensitive user attributes embedded in model parameters are vulnerable to inference attacks. Unlike group-level federated learning, user-level settings restrict cross-user information sharing, making existing unlearning methods infeasible. The paper proposes an integrated approach combining adaptive adversarial training with gradient masking to effectively remove sensitive attribute information while preserving recommendation utility.

The method introduces SUT (Self-Adaptive Unlearning Training) that dynamically adjusts perturbation budgets based on local optimization behavior, and DSVAE (Dual-Stochastic Variational Autoencoder) that masks sensitive attribute information in gradients to prevent reconstruction attacks. Extensive experiments on three real-world datasets across representative federated recommendation models demonstrate that FedAU2 significantly outperforms existing baselines, achieving superior privacy preservation while maintaining recommendation performance.

## Method Summary
FedAU2 tackles attribute unlearning in user-level federated recommender systems by addressing the fundamental challenge that sensitive user attributes embedded in model parameters can be exploited by inference attacks. The method operates in a constrained environment where cross-user information sharing is restricted, making traditional unlearning approaches ineffective. The proposed solution integrates two key components: an adaptive adversarial training strategy that dynamically adjusts perturbation budgets based on local optimization behavior, and a dual-stochastic variational autoencoder that masks sensitive attribute information in gradients to prevent reconstruction attacks. This dual approach enables effective attribute unlearning while maintaining recommendation utility in real-world federated learning scenarios.

## Key Results
- Achieves up to 26.42% reduction in balanced accuracy for attribute unlearning compared to baseline methods
- Maintains superior recommendation performance across three real-world datasets (Douban Movie, Amazon Books, Yelp2018)
- Effectively balances privacy preservation and utility in user-level federated learning settings
- Outperforms existing unlearning methods in both privacy protection and recommendation quality metrics

## Why This Works (Mechanism)
The method works by combining adaptive adversarial training with gradient masking through a dual-stochastic variational autoencoder. The adaptive component (SUT) dynamically adjusts perturbation budgets based on local optimization behavior, allowing the system to respond to varying optimization landscapes across different users and model updates. The gradient masking component (DSVAE) introduces stochasticity into the gradient representation, making it significantly harder for adversaries to reconstruct sensitive attribute information while preserving the essential information needed for model training. This dual approach addresses both the privacy vulnerability at the gradient level and the optimization challenges inherent in federated learning.

## Foundational Learning
- **User-level federated learning**: Individual users train models locally without sharing raw data - needed because group-level methods assume data aggregation that's not possible in user-level settings
- **Attribute inference attacks**: Attackers can reconstruct sensitive user attributes from model parameters or gradients - needed to understand the privacy threat that FedAU2 addresses
- **Adversarial training**: Deliberately adding perturbations to improve model robustness - needed as the foundation for the adaptive perturbation strategy
- **Variational autoencoders**: Probabilistic generative models that learn latent representations - needed for the gradient masking component that prevents attribute reconstruction
- **Gradient masking**: Modifying gradients to hide sensitive information while preserving utility - needed to prevent attackers from inferring user attributes from gradient updates
- **Dynamic perturbation budgeting**: Adjusting the amount of perturbation based on optimization state - needed to balance privacy protection with model convergence

## Architecture Onboarding

**Component Map**: User Devices -> Local Model Training -> SUT Perturbation -> DSVAE Gradient Masking -> Server Aggregation -> Global Model Update

**Critical Path**: User local training → SUT adaptive perturbation → DSVAE gradient masking → server aggregation → global model update → distribution to users

**Design Tradeoffs**: The method trades increased computational complexity and communication overhead for improved privacy protection. The dual-stochastic approach provides stronger privacy guarantees but requires more sophisticated training procedures compared to simpler gradient perturbation methods.

**Failure Signatures**: If the adaptive perturbation budget is set too high, model convergence may be severely impacted; if too low, attribute information may remain vulnerable to inference attacks. Ineffective gradient masking could result in successful attribute reconstruction despite the adversarial training component.

**First Experiments**:
1. Test FedAU2 on a small-scale federated recommendation task with synthetic sensitive attributes to verify the basic unlearning mechanism
2. Evaluate the impact of different perturbation budget ranges on both privacy protection and recommendation performance
3. Assess the effectiveness of the DSVAE component by attempting attribute reconstruction attacks on masked versus unmasked gradients

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on three specific datasets (Douban Movie, Amazon Books, Yelp2018) and representative federated recommendation models, potentially limiting generalizability
- Performance gains measured against baseline unlearning methods, without extensive comparison to state-of-the-art privacy-preserving techniques using different mechanisms
- Adaptive perturbation budget relies on gradient norm and loss change as proxy metrics, which may not capture all aspects of optimization behavior across diverse model architectures
- Computational overhead introduced by dual-stochastic training process is not thoroughly analyzed, leaving scalability questions in large-scale federated settings

## Confidence
- High confidence in the core claims regarding attribute unlearning effectiveness and recommendation performance maintenance
- Medium confidence in the generalizability across different recommendation domains and model architectures
- Low confidence in the scalability analysis and computational overhead characterization

## Next Checks
1. Evaluate FedAU2 on additional recommendation domains (e.g., news, music, e-commerce) and diverse model architectures beyond the tested baselines
2. Conduct a comprehensive scalability analysis measuring communication overhead, computation time, and memory usage as the number of users and model parameters scale
3. Compare FedAU2 against state-of-the-art privacy-preserving recommendation techniques that use different approaches (e.g., differential privacy, homomorphic encryption) to establish relative effectiveness across the privacy-utility spectrum