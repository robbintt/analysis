---
ver: rpa2
title: 'Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation
  Control'
arxiv_id: '2501.03847'
source_url: https://arxiv.org/abs/2501.03847
tags:
- video
- control
- arxiv
- videos
- tracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diffusion as Shader (DaS), a novel 3D-aware
  video diffusion method that supports multiple video control tasks within a unified
  architecture. Unlike prior methods limited to 2D control signals, DaS leverages
  3D tracking videos as control inputs, making the video diffusion process inherently
  3D-aware.
---

# Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control

## Quick Facts
- **arXiv ID:** 2501.03847
- **Source URL:** https://arxiv.org/abs/2501.03847
- **Reference count:** 21
- **Primary result:** Novel 3D-aware video diffusion method supporting multiple video control tasks within a unified architecture using 3D tracking videos.

## Executive Summary
This paper introduces Diffusion as Shader (DaS), a 3D-aware video diffusion method that leverages 3D tracking videos as control inputs to achieve versatile video generation control. Unlike prior methods limited to 2D control signals, DaS makes the video diffusion process inherently 3D-aware by using 3D tracking videos that link frames and enhance temporal consistency. The model demonstrates strong control capabilities across diverse tasks including mesh-to-video generation, camera control, motion transfer, and object manipulation through simple manipulation of the 3D tracking videos.

## Method Summary
DaS fine-tunes a pre-trained video diffusion transformer (CogVideoX) by adding a trainable "condition DiT" branch that processes 3D tracking videos. The condition DiT is a copy of the first 18 blocks of the denoising DiT, which processes the 3D tracking video's latent representation. Its output features are injected into the frozen main denoising DiT via zero-initialized linear layers. This approach leverages the pre-trained model's knowledge while efficiently teaching it to attend to the new 3D tracking signal, requiring minimal fine-tuning (<10k videos, 3 days on 8 GPUs).

## Key Results
- Achieves strong control capabilities across diverse tasks with just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos
- Outperforms baseline methods on camera control and motion transfer tasks, achieving rotation errors of 5.97 degrees and translation errors of 27.85 degrees for small camera movements
- For motion transfer, achieves a text-video alignment score of 32.6 and a temporal consistency score of 0.971

## Why This Works (Mechanism)

### Mechanism 1: 3D Tracking Video as a Unifying Control Signal
The core innovation uses a 3D tracking video as the primary control input, enabling a single model to perform diverse video generation tasks without task-specific architectures. The 3D tracking video represents dynamic 3D scene structure and motion as a sequence of RGB frames, where each pixel's color is derived from the initial 3D coordinates (X, Y, Z) of a tracked 3D point. By manipulating these 3D points, the user generates a modified 3D tracking video that is processed by the trainable condition DiT, whose output features are injected into the frozen main denoising DiT to generate the final video conditioned on this structured 3D motion information.

### Mechanism 2: Temporal Consistency via Persistent 3D Point Colors
Each 3D point's color, derived from its initial coordinates, serves as a persistent "anchor" or identity across all frames in the tracking video. This persistent coloring enforces temporal consistency in the generated video by providing a strong signal to the model that this region should have the same appearance when the point reappears after occlusion. This is superior to per-frame depth maps, which provide structural cues but no explicit frame-to-frame correspondence.

### Mechanism 3: Data-Efficient Fine-tuning with a Frozen Backbone
The model achieves data efficiency by freezing the main denoising DiT backbone and only fine-tuning a smaller condition DiT branch. The main denoising DiT is a large, pre-trained model with strong general video generation capabilities. The condition DiT processes the 3D tracking video's latent representation, and its output features are injected into the frozen denoising DiT via zero-initialized linear layers. This approach leverages the pre-trained model's vast knowledge while efficiently teaching it to attend to the new 3D tracking signal.

## Foundational Learning

- **Concept: 3D Tracking Videos / Point Clouds**
  - **Why needed here:** This is the core input and control signal for DaS. Understanding how 2D images relate to 3D space and how 3D points are tracked over time is fundamental.
  - **Quick check question:** Given a single RGB image and its estimated depth map, can you describe how you would create a colored 3D point cloud of the scene?

- **Concept: Diffusion Transformers (DiT)**
  - **Why needed here:** The entire architecture is built upon a DiT (specifically, CogVideoX). Knowing how transformers process video data is critical for understanding the model's operation.
  - **Quick check question:** In a video diffusion transformer, what are the main steps to process a video clip into a sequence of tokens for the transformer layers?

- **Concept: ControlNet / Adapter-based Fine-tuning**
  - **Why needed here:** DaS uses a ControlNet-like approach where a trainable copy of part of the base model is used to inject a new control signal.
  - **Quick check question:** What is the purpose of using zero-initialized layers when connecting a new adapter network to a pre-trained model?

## Architecture Onboarding

- **Component map:**
  VAE Encoder (Frozen) -> Condition DiT (Trainable) -> Zero-initialized Linear Layers -> Denoising DiT (Frozen) -> VAE Decoder (Frozen)

- **Critical path:**
  The inference flow starts with generating a 3D tracking video from a source. This video is encoded by the VAE Encoder. Separately, the input image is padded and encoded by the same VAE Encoder. The Condition DiT processes the tracking video's latent, and its outputs are injected into the Denoising DiT, which iteratively denoises an initial latent. The VAE Decoder then converts the final latent to the generated video.

- **Design tradeoffs:**
  - Dense vs. Sparse Tracking: Using 4900 tracking points per frame offers a balance between tracking detail and inference speed
  - Depth vs. 3D Tracking as Control: The authors chose 3D tracking videos over depth maps because the former provides temporal correspondence, which is crucial for consistency
  - Freezing vs. Fine-tuning the Backbone: Freezing the entire denoising DiT ensures the model's general generation capabilities are preserved, making training fast and data-efficient

- **Failure signatures:**
  - Incompatible Tracking Video: If the 3D structure implied by the tracking video contradicts the content of the input image, the model generates a scene transition to a new scene compatible with the tracking video
  - Uncontrolled Regions: Areas in the video without corresponding 3D tracking points are not constrained by the control signal, leading to potentially hallucinated or uncontrollable content
  - Loss of Object Identity: If tracking is lost or incorrect, the persistent color "anchors" are disrupted, potentially causing flickering or texture swapping upon reappearance

- **First 3 experiments:**
  1. Verify Basic Control: Generate a 3D tracking video for a simple camera movement from a single input image and evaluate if the generated video reflects the camera movement
  2. Ablation Study (Tracking vs. Depth): Implement a variant that uses depth maps instead of 3D tracking videos as the control signal and compare temporal consistency
  3. Test a Motion Transfer Scenario: Use a different image as input and a source video's 3D tracking video as control signal to evaluate motion transfer

## Open Questions the Paper Calls Out

- **How can a generative model be designed to synthesize 3D tracking videos autonomously?**
  The authors identify the need to learn to generate 3D tracking videos with a new diffusion model, removing the current dependency on pre-existing animated meshes or source videos.

- **What architectural or training modifications are necessary to ensure content control in regions lacking explicit 3D tracking points?**
  The paper identifies that regions without 3D tracking points may produce unnatural results and calls for methods to handle these "blind spots."

- **How can the model resolve geometric conflicts between an input image and an incompatible 3D tracking video?**
  The authors note that incompatible inputs often lead to scene transitions and seek techniques to preserve input image structure while following motion trajectories.

## Limitations

- **Generalizability to Unconstrained Inputs:** Performance on noisy, inaccurate, or incomplete 3D tracking videos from real-world applications remains uncertain
- **Performance Ceiling:** Claims of outperforming state-of-the-art are relative to specific baselines and may not hold against absolute best-performing models on each individual task
- **Complexity of Control Signal Generation:** Generating accurate 3D tracking videos is computationally expensive and the accuracy of tracking tools for complex, dynamic scenes is not fully explored

## Confidence

- **High Confidence:** The core mechanism of using a trainable condition DiT branch to inject 3D tracking video features into a frozen denoising DiT is well-defined and supported by experimental results
- **Medium Confidence:** Claims regarding the specific advantages of 3D tracking videos over depth maps and strong performance on camera control and motion transfer tasks are supported but rely on limited benchmarks
- **Medium Confidence:** The assertion of data-efficient training is credible given the architectural design but absolute efficiency gains compared to training from scratch are not quantified

## Next Checks

1. **Robustness Test:** Evaluate DaS on a dataset with synthetically degraded 3D tracking videos to assess performance under realistic conditions
2. **Baseline Comparison:** Conduct a direct controlled experiment comparing DaS to a version using depth maps instead of 3D tracking videos as the control signal
3. **Real-World Application:** Apply DaS to a challenging, real-world video control task and qualitatively assess quality, consistency, and failure modes