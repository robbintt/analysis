---
ver: rpa2
title: 'Slimmable NAM: Neural Amp Models with adjustable runtime computational cost'
arxiv_id: '2511.07470'
source_url: https://arxiv.org/abs/2511.07470
tags:
- neural
- slimmable
- network
- computational
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Slimmable Neural Amp Models (Slimmable NAMs),
  which allow musicians to adjust the computational cost of neural amp models in real-time
  without additional training. The core method involves training a single neural network
  that can dynamically adjust its width (number of channels) during inference, enabling
  users to trade off between accuracy and computational efficiency via a simple GUI
  slider in an audio plugin.
---

# Slimmable NAM: Neural Amp Models with adjustable runtime computational cost

## Quick Facts
- arXiv ID: 2511.07470
- Source URL: https://arxiv.org/abs/2511.07470
- Authors: Steven Atkinson
- Reference count: 12
- One-line primary result: Neural amp models with real-time adjustable computational cost via width-truncation, enabling runtime trade-offs between accuracy and efficiency.

## Executive Summary
This paper introduces Slimmable Neural Amp Models (Slimmable NAMs), which allow musicians to adjust the computational cost of neural amp models in real-time without additional training. The core method involves training a single neural network that can dynamically adjust its width (number of channels) during inference, enabling users to trade off between accuracy and computational efficiency via a simple GUI slider in an audio plugin. The approach is demonstrated using a stacked WaveNet architecture trained on recordings of various guitar amplifiers (clean, crunch, rhythm, and lead tones). Results show that Slimmable NAMs provide a Pareto-optimal trade-off between real-time factor (higher is better) and error-signal ratio (lower is better) compared to other models. The system enables practical, real-time use of neural amp models even on computationally constrained systems, while maintaining high accuracy when full computational resources are available. The method requires negligible computational overhead and can be easily integrated into existing workflows, making neural amp modeling more accessible to musicians without specialized ML hardware.

## Method Summary
The method trains a single WaveNet-based neural network to operate effectively at any runtime width between 1 and its maximum channel count. During training, the network is randomly slimmed each mini-batch by truncating convolutional weights and biases to a sampled width. Gradients from the slimmed sub-network update the shared master weights, ensuring all widths are optimized simultaneously. At inference time, users can select a width that balances computational cost (Real-Time Factor) against model accuracy (Error-Signal Ratio) through a simple GUI slider. The approach removes the need for resource-intensive post-training optimization like distillation or pruning by shifting the complexity trade-off from design-time/training-time to runtime decision-making.

## Key Results
- Slimmable NAMs provide a Pareto-optimal trade-off between Real-Time Factor and Error-Signal Ratio compared to baseline models
- The method enables real-time neural amp modeling on computationally constrained systems while maintaining high accuracy when full resources are available
- Training a single slimmable model eliminates the need for multiple specialized models or post-training optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A WaveNet-based neural network can be trained via stochastic width sampling to operate effectively at any runtime width between 1 and its maximum channel count.
- **Mechanism:** During training, the network is randomly slimmed each mini-batch by truncating convolutional weights ($W \in \mathbb{R}^{c \times c \times k}$) and biases ($b \in \mathbb{R}^c$) to a sampled width $1 \leq c' \leq c$. Gradients from the slimmed sub-network update the shared master weights, ensuring all widths are optimized simultaneously.
- **Core assumption:** The random sampling of widths is representative enough to prevent the optimization from overfitting to a specific sub-network capacity.
- **Break condition:** Fails if the training data is insufficient to cover the variance introduced by training multiple sub-networks, or if the optimizer cannot reconcile conflicting gradients from different widths.

### Mechanism 2
- **Claim:** Truncating channel width provides a direct, user-controllable mechanism for trading computational cost (Real-Time Factor) for model accuracy (Error-Signal Ratio) at inference time.
- **Mechanism:** Reducing width $c'$ lowers the number of multiply-add operations in the convolutional layers. This is a structural change that directly reduces the model's capacity and computational load, enabling a tunable performance profile.
- **Core assumption:** The relationship between width, accuracy, and compute follows a predictable, Pareto-optimal curve that users can navigate meaningfully.
- **Break condition:** Fails if the smallest usable model ($c' = 1$) still exceeds available compute, or if the accuracy degradation at acceptable compute levels is audibly prohibitive.

### Mechanism 3
- **Claim:** This approach removes the need for resource-intensive post-training optimization (e.g., distillation, pruning) by shifting the complexity trade-off from a design-time/training-time task to a runtime decision.
- **Mechanism:** A single trained model asset replaces what would otherwise be a set of fixed-size models. This eliminates the need for a user to perform or wait for model recompilation or distillation when their resource constraints change.
- **Core assumption:** Users prioritize workflow flexibility and accessibility over the theoretical peak performance of a specialized, fixed-width model.
- **Break condition:** Fails if the performance of the slimmable model at a specific width is consistently and significantly inferior to a dedicated model trained at that exact width, making the flexibility not worth the quality cost.

## Foundational Learning

- **Concept: WaveNet Architecture for Audio Modeling**
  - **Why needed here:** This is the foundational architecture being modified. Understanding its dilated convolutional structure and how it processes audio features is critical to understanding what is being truncated.
  - **Quick check question:** Can you explain the role of the receptive field in a WaveNet and how the number of channels ($c$) affects its capacity and computational cost?

- **Concept: Real-Time Factor (RTF) & Error-Signal Ratio (ESR)**
  - **Why needed here:** These are the primary metrics used to quantify the trade-off in this paper. RTF measures efficiency (higher is better), and ESR measures accuracy (lower is better). You cannot interpret the results without them.
  - **Quick check question:** If a user reports their audio is glitching, which metric would you check first, and what change to the model's width would you advise?

- **Concept: Model Distillation & Pruning**
  - **Why needed here:** The paper positions itself as a superior alternative for end-users. Understanding the compute and workflow costs of these baseline techniques is necessary to appreciate the problem being solved.
  - **Quick check question:** Why is requiring a musician to run model distillation a significant barrier to adoption compared to a runtime slider?

## Architecture Onboarding

- **Component map:** Input -> Input Projection -> Slimmable Conv Layers (with truncation) -> Output Projection -> Output

- **Critical path:** The implementation-critical path is the training loop modification. The standard supervised training loop must be altered to: 1) Sample a random width $c'$. 2) Truncate the active model weights to $c'$. 3) Execute the forward pass with the slimmed sub-network. 4) Backpropagate gradients and update the *full* master weights (zeroing gradients for unused weights).

- **Design tradeoffs:**
  - **Pareto Efficiency:** The model provides a frontier of ESR vs. RTF points. The user must choose a point on this curve.
  - **"Slimmability Tax":** A model trained to be slimmable may have slightly lower peak accuracy at its full width compared to a standard model trained only at full width, as its weights are a compromise across all widths.
  - **Training Complexity:** Training is potentially less sample-efficient and may take longer to converge than training a single fixed-width model.

- **Failure signatures:**
  - **Performance Collapse at Low Widths:** The ESR may increase non-linearly and sharply below a certain width threshold.
  - **Non-Monotonicity:** The relationship between width and ESR should be roughly monotonic. Inversions suggest training instability.
  - **Audio Artifacts at Boundaries:** Rapidly changing the width at runtime could, in theory, introduce artifacts if state (e.g., dilated convolution buffers) is not handled correctly.

- **First 3 experiments:**
  1. **Baseline Comparison:** Train a slimmable model and a standard WaveNet model to the same full width $c$. Compare their final ESR at full width to quantify the "slimmability tax."
  2. **Trade-off Curve Characterization:** For a trained slimmable model, plot ESR vs. RTF for widths $c' \in \{1, \lfloor c/4\rfloor, \lfloor c/2\rfloor, \lfloor 3c/4\rfloor, c\}$. Verify the Pareto front shape.
  3. **Real-Time Usability Test:** Integrate the model into the referenced plugin and measure the actual CPU load and audio latency when switching between widths during active playback.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the slimmable training approach be effectively adapted to non-WaveNet architectures used in Neural Amp Modeling (e.g., LSTMs or recurrent networks) without destabilizing training?
- **Basis in paper:** [explicit] The author states the method can be "adapted to NAM's other open-source architectures in a similar way" but only demonstrates it on the stacked WaveNet architecture.
- **Why unresolved:** The paper provides no experimental validation on architectures other than the convolution-based WaveNet.
- **What evidence would resolve it:** Successful training and benchmarking of slimmable LSTMs or other NAM architectures with comparable accuracy-efficiency trade-offs.

### Open Question 2
- **Question:** Does a slimmable model at a reduced width consistently outperform a model trained natively at that specific smaller width?
- **Basis in paper:** [inferred] The results compare the slimmable model against baselines like "Nano," but do not explicitly compare the slimmed model against a version of the "New" architecture trained from scratch at the reduced width.
- **Why unresolved:** It is unclear if the "once-for-all" training compromises the accuracy of the smaller sub-networks compared to dedicated small models.
- **What evidence would resolve it:** Ablation studies comparing the ESR of a slimmed model (width $c'$) vs. a statically trained model (width $c'$).

### Open Question 3
- **Question:** Does the proposed single-module slimmable architecture generalize well enough to replace the standard stacked architecture as the community default?
- **Basis in paper:** [explicit] The text notes the architecture is "under investigation as the next-generation default model architecture."
- **Why unresolved:** The paper demonstrates the method works, but broader community validation across diverse gear and playing styles is required for standardization.
- **What evidence would resolve it:** Widespread adoption metrics or competitive benchmarking on a standard dataset (like the "NAM" challenge datasets) showing superior Pareto efficiency.

## Limitations

- The paper does not report the actual peak performance degradation ("slimmability tax") at full width compared to a standard WaveNet trained only at maximum width, making the true cost of runtime flexibility unclear.
- No formal user study or perceptual evaluation is presented to confirm that the ESR/RTF trade-off aligns with what musicians actually find acceptable in practice.
- The training procedure relies on stochastic width sampling, but no analysis is provided on whether the sampling distribution or frequency significantly impacts convergence or the quality of the resulting trade-off curve.

## Confidence

- **High Confidence:** The core training mechanism (stochastic width sampling with weight truncation) is clearly described and represents a straightforward, well-defined modification to standard supervised training.
- **Medium Confidence:** The claim that Slimmable NAMs provide a Pareto-optimal trade-off is supported by the results, but without baseline comparisons to alternative approaches (distillation, pruning) at runtime, the practical superiority is inferred rather than proven.
- **Low Confidence:** The assertion that this approach "removes the need" for post-training optimization is somewhat overstated. It removes the *user's* need to perform it, but the model still requires a resource-intensive training process; the optimization burden has simply been shifted to the training pipeline.

## Next Checks

1. **Baseline Comparison Experiment:** Train and evaluate a standard WaveNet model at the same maximum width as the slimmable model. Compare their ESR at full width to quantify the "slimmability tax."
2. **Perceptual Validation:** Conduct a blind listening test with musicians to assess whether the ESR/RTF trade-off curve identified by the model corresponds to perceptible changes in audio quality across different computational settings.
3. **Runtime Integration Stress Test:** Integrate the slimmable model into a real-time plugin environment and measure actual CPU load, audio latency, and the presence of any audible artifacts when dynamically changing the width during active playback.