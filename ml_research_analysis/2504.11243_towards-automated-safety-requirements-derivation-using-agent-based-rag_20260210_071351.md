---
ver: rpa2
title: Towards Automated Safety Requirements Derivation Using Agent-based RAG
arxiv_id: '2504.11243'
source_url: https://arxiv.org/abs/2504.11243
tags:
- safety
- context
- answer
- pipeline
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an agent-based retrieval-augmented generation
  (RAG) approach for automated derivation of safety requirements in self-driving vehicles.
  The key idea is to use document agents that preprocess and summarize source material
  before retrieval, improving the relevance of context provided to the LLM.
---

# Towards Automated Safety Requirements Derivation Using Agent-based RAG

## Quick Facts
- arXiv ID: 2504.11243
- Source URL: https://arxiv.org/abs/2504.11243
- Reference count: 8
- One-line primary result: Agent-based RAG improves retrieval precision and context relevance for safety requirement derivation in self-driving vehicles.

## Executive Summary
This paper presents an agent-based retrieval-augmented generation (RAG) approach for automated derivation of safety requirements in self-driving vehicles. The key innovation is using document agents that preprocess and summarize source material before retrieval, improving the relevance of context provided to the LLM. The approach is evaluated on a dataset of safety requirement questions derived from the Apollo automated driving perception system, comparing against a default RAG method and LLM-only baseline. Results show that the agent-based RAG significantly improves retrieval precision and context relevance metrics compared to default RAG, while maintaining similar answer similarity scores.

## Method Summary
The method uses a two-level agent hierarchy: a top-level document agent selects the three most relevant documents from a pool including Apollo documentation and automotive safety standards, then delegates to individual document agents. Each document agent chooses between a vector query engine (for factual lookups) and a summary query engine (for conceptual questions) based on query characteristics. The summary query engine uses a hierarchical summarization approach (Summary Index) to compress long documents into queryable abstractions. The system is implemented using LlamaIndex with FAISS for vector similarity search and GPT-3.5-turbo for both generation and judging.

## Key Results
- Agent-based RAG significantly improves Retrieval Precision (RP) compared to default RAG
- Context relevance metrics show better quality of retrieved information for safety-critical applications
- Answer similarity scores remain comparable between agent-based RAG and default RAG
- The approach effectively prevents the "stuck on one document" problem common in default RAG systems

## Why This Works (Mechanism)

### Mechanism 1
- Agent-based query engine selection improves retrieval precision by matching retrieval strategy to query type
- Document agents choose between vector query engines (for factual lookups) and summary query engines (for conceptual/synthesis questions) based on query characteristics
- Core assumption: Queries can be reliably classified as fact-seeking vs. summary-seeking by the agent framework
- Break condition: If queries are ambiguous or hybrid (requiring both facts and synthesis), single-engine selection may miss relevant context

### Mechanism 2
- Hierarchical summarization (Summary Index) compresses long documents into queryable abstractions, reducing irrelevant context retrieval
- A tree-like structure stores summaries at multiple granularities (lower-level details, higher-level abstractions)
- Core assumption: The summarization process preserves query-relevant information and doesn't introduce distortion
- Break condition: If source documents are already concise or if queries require specific verbatim excerpts, summarization may lose critical details

### Mechanism 3
- Two-level agent hierarchy improves multi-document information fusion over default single-stage retrieval
- Top-level agent first identifies the 3 most relevant documents via descriptions, then delegates to each document's agent to extract refined context
- Core assumption: Document descriptions are sufficiently discriminative for coarse selection, and document agents can independently judge relevance
- Break condition: If the correct answer requires documents outside the top-3, or if document descriptions are misleading, the hierarchy will filter out relevant sources early

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The entire paper builds on RAG as the baseline for LLM domain knowledge integration
  - Quick check question: Can you explain why RAG is preferred over fine-tuning when domain documents are frequently updated?

- Concept: Vector embeddings and similarity search
  - Why needed here: The Vector Store Index uses embedding similarity for chunk retrieval; understanding its limitations motivates the agent-based approach
  - Quick check question: Why might semantic similarity fail to identify context relevant to complex, multi-part queries?

- Concept: Automotive safety standards (ISO 26262, ISO 21448/SOTIF)
  - Why needed here: The document pool includes these standards, and the task (safety requirement derivation) follows their prescribed workflow (HARA, ASIL, safety goals)
  - Quick check question: What is the difference between functional safety (ISO 26262) and SOTIF (ISO 21448)?

## Architecture Onboarding

- Component map: Document Ingestion Pipeline -> Top-level Document Agent -> Document Agents -> LLM Generator
- Critical path: User query → Top-level Document Agent (select top-3 docs) → Document Agents (select query engine type, retrieve context) → Context fusion → LLM generation
- Design tradeoffs:
  - Latency vs. precision: Two-stage agent calls add inference overhead compared to single-stage default RAG
  - Summary fidelity vs. compression: Aggressive summarization may lose domain-specific terminology important for safety requirements
  - Top-k document selection: Choosing top-3 balances breadth with focus; higher k increases fusion but risks noise
- Failure signatures:
  - Low Retrieval Precision (RP): Agent may be selecting wrong query engine type or document descriptions are inadequate
  - High RP but low Answer Consistency (AC): Retrieved context is relevant but LLM isn't effectively using it; check prompt engineering
  - "Stuck" on one document: Top-level agent not diversifying; verify document tool descriptions are discriminative
- First 3 experiments:
  1. Ablation on query engine selection: Force all queries to use only vector or only summary engines to isolate the contribution of dynamic selection
  2. Vary top-k documents: Test top-1, top-3, top-5 to measure sensitivity of answer quality to document breadth
  3. Dataset complexity scaling: Replicate on a more complex dataset where generic LLM knowledge cannot achieve high NASS without RAG

## Open Questions the Paper Calls Out

- Does the improved retrieval precision of agent-based RAG translate to significantly higher answer quality when applied to more complex safety datasets or smaller language models?
- Do human safety experts rate the output of agent-based RAG higher than standard RAG despite similar automated similarity scores?
- Can alternative automated evaluation metrics be developed to effectively capture the nuanced quality of safety requirements better than standard text similarity scores?

## Limitations
- Evaluation dataset is relatively small (58 QA pairs) and derived from a single AD system (Apollo), limiting generalizability
- NASS metric's discriminative power is questionable for this dataset, as generic LLM knowledge appears to achieve high scores even without RAG
- Hierarchical summarization mechanism lacks empirical validation through ablation studies or comparison to simpler summarization baselines

## Confidence
- High confidence: Retrieval Precision (RP) improvements and the "stuck on one document" failure mode of default RAG are well-demonstrated
- Medium confidence: The agent-based approach's advantage in fusing multi-document information is supported but could benefit from more granular analysis
- Low confidence: The claim that hierarchical summarization is superior to flat chunking lacks direct comparison, and the NASS metric's reliability for measuring true RAG value is questionable

## Next Checks
1. Conduct an ablation study isolating the contribution of dynamic query engine selection by forcing all queries to use only vector or only summary engines
2. Expand evaluation to a more complex dataset where generic LLM knowledge cannot achieve high NASS without RAG
3. Compare hierarchical summarization (Summary Index) against simpler summarization baselines to quantify its specific contribution to retrieval quality