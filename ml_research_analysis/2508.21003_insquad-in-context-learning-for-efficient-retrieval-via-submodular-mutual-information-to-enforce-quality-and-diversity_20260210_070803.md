---
ver: rpa2
title: 'InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual
  Information to Enforce Quality and Diversity'
arxiv_id: '2508.21003'
source_url: https://arxiv.org/abs/2508.21003
tags:
- learning
- diversity
- in-context
- retrieval
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving In-Context Learning
  (ICL) by enhancing the quality and diversity of in-context exemplars through a unified
  combinatorial approach. The authors introduce InSQuaD, which leverages Submodular
  Mutual Information (SMI) functions to jointly model quality, diversity, and order
  in exemplar selection.
---

# InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity

## Quick Facts
- arXiv ID: 2508.21003
- Source URL: https://arxiv.org/abs/2508.21003
- Reference count: 40
- Key outcome: Joint quality-diversity exemplar selection for ICL via Submodular Mutual Information (SMI) functions, achieving up to 21.6% improvement on classification tasks

## Executive Summary
This paper addresses the challenge of improving In-Context Learning (ICL) by enhancing the quality and diversity of in-context exemplars through a unified combinatorial approach. The authors introduce InSQuaD, which leverages Submodular Mutual Information (SMI) functions to jointly model quality, diversity, and order in exemplar selection. InSQuaD comprises two key components: InSQuaD-RETRIEVE, which performs targeted exemplar annotation and retrieval using SMI-based selection, and InSQuaD-LEARN, which trains a retrieval model to inherently capture quality and diversity through a novel likelihood-based loss derived from SMI functions. The method is evaluated on nine benchmark datasets, demonstrating significant improvements over existing baselines, with up to 21.6% gains on classification tasks, 16.4% on multi-choice tasks, and up to 7% on generation-based tasks. Additionally, InSQuaD reduces inference times compared to iterative selection methods, making it a practical and effective solution for ICL tasks.

## Method Summary
InSQuaD introduces a unified framework for ICL exemplar selection using Submodular Mutual Information (SMI) functions. The method operates in two phases: InSQuaD-LEARN trains a retrieval model (SBERT/MPNET) using a likelihood-based loss that enforces both quality and diversity through SMI functions; InSQuaD-RETRIEVE then uses this trained model to perform greedy SMI maximization for exemplar annotation and retrieval. The approach leverages synthetic paraphrase augmentation to create diversity training signals and uses a novel loss function that balances query-relevance and paraphrase-deduplication through a calibrated λ parameter.

## Key Results
- Up to 21.6% improvement on classification tasks compared to baselines
- Up to 16.4% improvement on multi-choice tasks
- Up to 7% improvement on generation-based tasks
- Reduced inference times compared to iterative selection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Submodular Mutual Information (SMI) functions jointly optimize quality, diversity, and ordering in exemplar selection through a single unified objective.
- Mechanism: SMI functions compute If(A; Q) measuring shared information between candidate set A and query Q. Greedy maximization iteratively selects elements maximizing marginal gain: v* = argmax If(S ∪ {v}; Q) - If(S; Q). The diminishing returns property of submodularity ensures diversity (redundant elements yield lower gains), while query-relevance ensures quality. Greedy selection naturally orders exemplars by information contribution.
- Core assumption: Ground set embeddings capture semantic similarity relevant to downstream ICL tasks.
- Evidence anchors:
  - [abstract] "unified selection strategy based on SMIs which mines relevant yet diverse in-context examples encapsulating the notions of quality and diversity"
  - [Section III-C1] "InSQuAD drifts away from existing works by introducing a unified single stage formulation... maximizing the SMI between qtest and selected exemplars in C"
  - [corpus] Related work (KITE, Submodular Context Partitioning) confirms submodular approaches for ICL are actively explored, though citation counts are low (new direction).
- Break condition: If retrieval model R produces poor embeddings (e.g., domain mismatch), SMI optimization amplifies rather than corrects embedding errors.

### Mechanism 2
- Claim: Training retrieval models with likelihood-based SMI losses explicitly encodes quality and diversity into embedding space, eliminating the need for multi-stage retrieval heuristics.
- Mechanism: Submodular Point Process formulation: Pθ(S) = If(S; Q) / ΣS' If(S'; Q). The loss L = log(If(S-; Q)) - log(If(S+; Q)) maximizes likelihood of relevant set S+ over distractors S-. Combined loss LInSQuAD = exp((1-λ)Lq + λLd) jointly optimizes quality (query-relevance via Lq) and diversity (paraphrase-deduplication via Ld).
- Core assumption: Distractor sets S- and paraphrase sets Sp adequately represent failure modes in downstream ICL.
- Evidence anchors:
  - [abstract] "learns the parameters of an SMI function to enforce both quality and diversity in the retrieval model through a novel likelihood-based loss"
  - [Section III-C2] "InSQuaD-LEARN addresses this gap by introducing a family of likelihood-based loss functions (Table I) to train a retrieval model that learns the parameters of an SMI function"
  - [corpus] Weak corpus evidence for this specific training paradigm; related work focuses on selection rather than embedding learning.
- Break condition: If λ calibration is incorrect (Table IV shows task-specific optimal λ varies), model may over-emphasize diversity at quality's expense or vice versa.

### Mechanism 3
- Claim: Synthetic paraphrase augmentation enables the retrieval model to learn de-duplication by creating explicit diversity training signals absent in standard QA datasets.
- Mechanism: For each document in HotpotQA, GPT-3.5 generates paraphrases Sp. During training, Ld minimizes If(S-; Sp) (distractor-paraphrase overlap) while maximizing If(S+; Sp) (relevant-paraphrase alignment). This teaches the model that semantically equivalent documents should not both be retrieved.
- Core assumption: GPT-3.5 paraphrases adequately represent semantic equivalence without introducing systematic artifacts.
- Evidence anchors:
  - [Section III-C2] "popular multi-hop Question Answering (QA) datasets do not encapsulate paraphrases, thereby hindering models trained on them to model de-duplication"
  - [Section III-C2] "We leverage GPT-3.5 Turbo to generate paraphrases for each corresponding document in Vraw"
  - [corpus] No corpus evidence for this augmentation strategy; appears novel to this work.
- Break condition: If paraphrases introduce distribution shift or artifacts, model may learn spurious de-duplication patterns not generalizing to real ICL scenarios.

## Foundational Learning

- Concept: Submodular Functions (diminishing returns property)
  - Why needed here: Core mathematical foundation for SMI functions. Understanding f(A∪{v}) - f(A) ≥ f(B∪{v}) - f(B) for A ⊆ B explains why greedy selection provides (1-e^-1) approximation guarantees.
  - Quick check question: Given sets A ⊆ B and element v, what does f(A∪{v}) - f(A) ≥ f(B∪{v}) - f(B) imply about adding v to smaller vs. larger sets?

- Concept: Facility-Location, Graph-Cut, and Log-Determinant functions
  - Why needed here: These are the three SMI instantiations tested (Table I). Each computes similarity kernels differently—FL uses max-pooling, GC uses sum aggregation, LD uses determinant operations on similarity matrices.
  - Quick check question: Which submodular function variant (FL/GC/LD) would you expect to be most sensitive to outlier similarities in the embedding space?

- Concept: Submodular Point Processes
  - Why needed here: Provides probabilistic framework for deriving likelihood-based loss. Understanding P(S) ∝ If(S; Q) connects combinatorial selection to differentiable training objectives.
  - Quick check question: How does normalizing If(S; Q) by the partition function ΣS' If(S'; Q) enable gradient-based optimization?

## Architecture Onboarding

- Component map:
  - **InSQuaD-LEARN** (offline training): Takes HotpotQA + synthetic paraphrases → trains SBERT/MNNet embeddings using LInSQuaD loss → outputs fine-tuned retrieval model R(·, θ)
  - **InSQuaD-RETRIEVE** (inference): Takes unlabeled corpus V → Exemplar Annotation (maximize f(Vshortlisted) for diversity) → human labeling → Exemplar Retrieval (maximize If(C; qtest) for quality+diversity) → ordered exemplars C for LLM prompt
  - **Ground set**: Document pool V (unlabeled), annotation budget B, selection budget k

- Critical path:
  1. Training data curation: Augment HotpotQA with GPT-3.5 paraphrases → create (qi, S+i, S-i, Spi) tuples
  2. InSQuaD-LEARN training: Fine-tune SBERT with LInSQuaD (λ=0.5 default) for 7 epochs
  3. Exemplar Annotation (one-time): Run InSQuaD-RETRIEVE on V with budget B → Vshortlisted → human annotators
  4. InSQuaD-RETRIEVE inference: For each qtest, maximize If(C; qtest) over Vlabeled → return ordered C

- Design tradeoffs:
  - **FL vs. GC vs. LD**: Table II shows InSQuaD-GC achieves best average rank (3.6) but InSQuAD-LD has lower inference cost (Figure 4). GC provides robustness, LD provides speed.
  - **Annotation budget B**: Figure 3(c) shows B∈{18, 100} yields similar performance—increasing B doesn't scale accuracy. Start with smaller budgets.
  - **λ calibration**: Table IV shows optimal λ varies by task (MRPC prefers λ=0, DBpedia prefers λ=1). Requires task-specific tuning; no universal setting.

- Failure signatures:
  - **Performance plateau despite more data**: Check if retrieval model R was trained on domain-matched data. Section VI notes only HotpotQA was used, limiting generalization.
  - **Excessive redundancy in retrieved exemplars**: λ likely too low (quality over-emphasized). Increase λ ∈ [0.25, 0.75] per Table IV.
  - **Slow inference**: InSQuAD-LD kernel computation is costliest. Switch to InSQuAD-GC or InSQuAD-FL for faster retrieval.
  - **Zero-shot-like performance**: Embeddings may be misaligned. Verify R training completed (check loss curves, not just epochs).

- First 3 experiments:
  1. **Baseline validation**: Run InSQuaD-RETRIEVE (NT) with pretrained SBERT on single dataset (e.g., MRPC). Compare vs. Random selection. Target: >6% improvement per Table II (NT results). If failing, debug embedding quality first.
  2. **Training ablation**: Train InSQuaD-LEARN with λ∈{0, 0.5, 1.0} on HotpotQA-paraphrase data. Evaluate on held-out task. Confirm λ>0 improves 7/9 tasks (Table IV pattern). This validates the diversity training signal.
  3. **Budget sensitivity**: Run annotation with B∈{18, 50, 100} on same task. Confirm Figure 3(c) finding—performance should plateau, not scale linearly. If scaling occurs, ground set V may lack diversity (collect more unlabeled data).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the InSQuAD-LEARN training paradigm generalize to other multi-hop Question Answering datasets besides HotpotQA?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that the current model "adopts only the HotpotQA dataset... leaving other multi-hop QA datasets to be experimented with as future research."
- Why unresolved: The experimental validation was restricted to a single dataset (HotpotQA), selected primarily for its popularity, leaving the robustness of the learned retrieval model across different data distributions unknown.
- What evidence would resolve it: Performance benchmarks demonstrating that the retrieval model maintains high-quality and diverse exemplar selection when trained on alternative multi-hop datasets like MuSiQue or ConcurrentQA.

### Open Question 2
- Question: How can selection biases during exemplar annotation and retrieval be quantitatively addressed within the InSQuAD framework?
- Basis in paper: [explicit] The authors list "addressing selection biases during exemplar annotation and retrieval" as a specific direction for future research in the conclusion.
- Why unresolved: While the method optimizes for quality and diversity via submodular functions, it does not explicitly model or correct for underlying biases present in the unlabeled corpus or the synthetic paraphrase generation process.
- What evidence would resolve it: The integration of a bias-aware loss term or a fairness constraint into the Submodular Mutual Information function, resulting in a more balanced representation of demographic or topical attributes in the selected exemplars.

### Open Question 3
- Question: Can the quality-diversity trade-off hyperparameter $\lambda$ be adapted automatically rather than requiring manual task-specific calibration?
- Basis in paper: [inferred] In the ablations (Section V), the authors note that the "optimal value of $\lambda$ varies based on the downstream task," necessitating user calibration during deployment.
- Why unresolved: The current framework relies on a static hyperparameter to balance the loss terms $L_q$ and $L_d$, which hinders fully automated deployment across diverse tasks.
- What evidence would resolve it: A meta-learning algorithm or an adaptive mechanism that dynamically adjusts $\lambda$ based on the statistical properties of the incoming query or the validation performance of the in-context learner.

## Limitations
- The method requires a one-time exemplar annotation step with human-labeled budget B, limiting applicability to truly zero-resource scenarios.
- Training data augmentation relies on synthetic paraphrases generated by GPT-3.5 Turbo, but exact generation prompts and quality control measures are not specified.
- The SMI-based training assumes quality and diversity can be jointly optimized through a single objective, but optimal λ calibration varies significantly across tasks, suggesting no universal parameterization exists.

## Confidence
- **High confidence**: Core mechanism of SMI functions demonstrably optimizes quality and diversity through greedy selection with (1-e^-1) approximation guarantees.
- **Medium confidence**: Likelihood-based loss formulation is theoretically sound, but lack of ablation studies on different SMI instantiations during training creates uncertainty about which variant works best for embedding learning.
- **Low confidence**: Cross-domain generalization is uncertain since the retriever is trained only on HotpotQA data, with no evaluation on out-of-domain exemplar retrieval scenarios.

## Next Checks
1. **Cross-dataset embedding transfer**: Train InSQuaD-LEARN on HotpotQA-paraphrase data, then evaluate exemplar retrieval on a completely different domain (e.g., biomedical or code translation) without fine-tuning. This tests whether SMI-learned embeddings generalize beyond their training distribution.

2. **Annotation budget scaling**: Systematically vary B from 5 to 200 examples and measure the marginal improvement in downstream ICL accuracy. This quantifies whether the reported B=18 is truly sufficient or if performance plateaus earlier, which would impact practical deployment costs.

3. **SMI variant ablation during training**: Train separate models using each SMI variant (FL/GC/LD) as the training objective and compare their retrieval performance. This reveals whether the choice of SMI function for embedding learning matters as much as it does for greedy selection.