---
ver: rpa2
title: 'ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through
  Multimodal Risk Detection'
arxiv_id: '2511.18780'
source_url: https://arxiv.org/abs/2511.18780
tags:
- unsafe
- text
- image
- safe
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ConceptGuard, a unified framework for proactive
  safety in multimodal text-and-image-to-video generation. The framework addresses
  the challenge of detecting and mitigating harmful content that can emerge from the
  interaction of image and text inputs, where existing methods often focus on only
  one modality or act as post-generation auditors.
---

# ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection

## Quick Facts
- arXiv ID: 2511.18780
- Source URL: https://arxiv.org/abs/2511.18780
- Reference count: 40
- Key result: Achieves 0.976 detection accuracy on ConceptRisk dataset and reduces harmfulness rates from 90.0% to 10.0% in practical tests

## Executive Summary
This paper presents ConceptGuard, a unified framework for proactive safety in multimodal text-and-image-to-video generation. The framework addresses the challenge of detecting and mitigating harmful content that can emerge from the interaction of image and text inputs, where existing methods often focus on only one modality or act as post-generation auditors. ConceptGuard operates in two stages: first, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; second, a semantic suppression mechanism intervenes during generation to steer the model away from unsafe concepts.

## Method Summary
ConceptGuard uses a two-stage approach for multimodal safety in text-and-image-to-video generation. The first stage employs a contrastive detection module that fuses image and text features using bidirectional cross-attention and a gating network to dynamically weight modalities, projecting them into a structured concept space for risk scoring. The second stage uses semantic suppression through embedding-level projection: when risk exceeds threshold, detected concepts define a projection matrix that removes harmful semantics from early diffusion steps while preserving benign content. The framework is trained on ConceptRisk, a dataset of 8,000 samples across 200 unsafe concepts, using symmetric contrastive loss.

## Key Results
- Detection accuracy of 0.976 on ConceptRisk dataset and 0.960 on T2VSafetyBench-TI2V
- Reduces harmfulness rates from 90.0% to 10.0% in practical safety intervention tests
- Outperforms baselines that rely on fixed concept sets or lack multimodal perception

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Fusion with Risk-Aware Gating
- Claim: Adaptive weighting of image and text modalities enables detection of risks originating from either modality or their combination.
- Mechanism: CLIP features are projected into a shared space, processed through bidirectional cross-attention, then a gating network dynamically adjusts modality contributions. This allows the model to amplify visual signals when images carry risk (even with safe text) and vice versa.
- Core assumption: Unsafe semantics distribute differently across modalities depending on input context; a fixed fusion ratio would miss single-modality risks.
- Evidence anchors:
  - [abstract] "contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space"
  - [Section 4.2] "In the critical 'Safe Text + Unsafe Image' scenario, our model achieves an accuracy of 0.944... In contrast, existing methods like LatentGuard+CLIPScore (0.773)... perform significantly worse"
  - [corpus] Related work on LVLM safety (VisuoAlign) confirms visual inputs introduce new attack surfaces requiring multimodal defenses

### Mechanism 2: Concept-Guided Contrastive Training
- Claim: Structuring the embedding space around explicit concept prototypes enables fine-grained risk identification rather than binary safe/unsafe classification.
- Mechanism: Each unsafe concept (e.g., "shooting," "bribery") is embedded as a prototype vector. The contrastive loss aligns unsafe inputs with their correct concept while pushing them away from other concepts and safe variants. This creates separable clusters in the learned space.
- Core assumption: Risks can be decomposed into discrete, namable concepts; the 200-concept taxonomy is sufficiently comprehensive for target use cases.
- Evidence anchors:
  - [abstract] "projecting fused image-text inputs into a structured concept space"
  - [Section 3.1] "We define four safety-critical categories... For each, we curated 50 representative concepts... yielding 200 core unsafe concepts"
  - [Section 4.2, Figure 3] t-SNE visualization shows "distinct, compact clusters" for learned embeddings vs. "significant class confusion" in baseline CLIP
  - [corpus] ConceptGuard (Neuro-Symbolic) paper similarly uses sparse interpretable concepts for LLM jailbreak detection—concept-based approaches show promise across safety domains

### Mechanism 3: Embedding-Level Projection Suppression
- Claim: Orthogonal projection of risk-bearing token embeddings away from unsafe subspaces suppresses harmful semantics without surface-level prompt modification.
- Mechanism: When risk exceeds threshold θ, top-k detected concepts define a projection matrix P_risk. Tokens whose embeddings have high overlap with this subspace (low orthogonal magnitude) are identified as "risk-bearing" and projected: t_safe = (I - P_risk)t. This is applied only during early diffusion steps to steer initial generation.
- Core assumption: Unsafe semantics are linearly represented in embedding space and can be removed while preserving benign intent; early-step intervention is sufficient.
- Evidence anchors:
  - [abstract] "semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning"
  - [Section 3.3] "This projection is applied only during the initial N steps of the diffusion process (e.g., N=13) to steer the generation away from harmful content early on"
  - [Section 4.3, Table 4] Full method reduces harmfulness rate from 90.0% to 10.0%; text-only suppression ablation (image editing only) achieves only 62.0%
  - [corpus] SAFREE uses similar projection for text-only suppression but achieves 80.0% harmfulness rate, suggesting multimodal suppression is critical

## Foundational Learning

- **Concept: Cross-Attention in Multimodal Fusion**
  - Why needed here: The detection module uses bidirectional cross-attention between image and text representations before gating. Understanding how queries/keys/values flow between modalities is essential for debugging fusion failures.
  - Quick check question: Given an unsafe image + safe text input, which attention direction (text→image or image→text) would carry the risk signal?

- **Concept: Orthogonal Projection for Concept Removal**
  - Why needed here: Stage 2 suppression uses (I - P_risk) to project embeddings away from unsafe subspaces. This is linear algebra—understanding why orthogonal projection removes component aligned with P_risk is essential.
  - Quick check question: If a token embedding t has 90% of its magnitude in the unsafe subspace defined by P_risk, what happens to its norm after projection? What does this imply for semantic preservation?

- **Concept: Contrastive Learning with Symmetric Loss**
  - Why needed here: The detection module is trained with a symmetric contrastive loss (I,T→C and C→I,T). This structure differs from standard classification and affects how the model generalizes.
  - Quick check question: Why include both directions in the loss? What failure mode might occur if only I,T→C were used?

## Architecture Onboarding

- **Component map:**
  - CLIP encoder (frozen) -> projection layers (trainable, dm=256) -> cross-attention (4 heads) -> gating MLP -> fused representation -> concept scoring head (attention-based)
  - Risk threshold check -> subspace construction from top-k concepts -> risk-bearing token identification -> embedding projection (early N=13 steps) + Flux.1 Kontext image editing

- **Critical path:**
  1. Detection accuracy determines suppression effectiveness—false negatives pass through unmitigated; false positives trigger unnecessary intervention
  2. Concept identification quality gates subspace construction—incorrect top-k concepts create wrong projection matrix
  3. Early-step suppression timing (N=13) balances safety vs. fidelity—too few steps may miss semantic influence; too many may degrade output quality

- **Design tradeoffs:**
  - **Fixed concept taxonomy vs. open vocabulary:** 200 concepts enable structured training but may miss novel risks; corpus shows related work exploring sparser/symbolic concept approaches
  - **Proactive vs. reactive:** Pre-generation intervention prevents harm but cannot catch risks the detector misses; post-hoc systems like SafeWatch catch more but allow harmful content creation
  - **Text + visual suppression vs. single modality:** Full method (10% harmfulness) vs. image-only (62%) vs. text-only SAFREE (80%)—dual intervention is costly but necessary

- **Failure signatures:**
  - High false negatives on adversarial prompts (Syn/Adv variants): Check if contrastive training sufficiently covers adversarial augmentation
  - Over-suppression of benign content: Check threshold θ and sensitivity parameter α—too strict removes valid semantics
  - Visual-only risks missed: Check gating network weights—is image pathway being underweighted?
  - Temporal re-emergence of harm (Frame 1 safe, later frames unsafe): Text suppression is failing—verify risk-bearing token identification

- **First 3 experiments:**
  1. **Reproduce detection accuracy on held-out concepts:** Train on 160 concepts, test on 40 unseen concepts to verify generalization claim (0.960 on T2VSafetyBench-TI2V suggests strong but not perfect OOD performance)
  2. **Ablate gating mechanism:** Set equal weights (ω_img = ω_txt = 0.5) and measure UI+ST accuracy—paper reports drop to 0.939 without gating
  3. **Stress-test suppression timing:** Vary N (early steps) from 5 to 25 and measure harmfulness rate vs. fidelity (e.g., CLIP score alignment with original prompt intent)—finds optimal balance point for target deployment context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ConceptGuard generalize to detect and mitigate risk concepts that fall entirely outside the fixed 200-concept taxonomy used during training?
- Basis in paper: [inferred] Section 3.1 explicitly defines a "Taxonomy of Unsafe Concepts" with 200 core entries, and Section 3.2 describes the detection module as scoring inputs against these "predefined unsafe concept" embeddings.
- Why unresolved: While the paper claims the method handles "unforeseen" risks, the contrastive training objective explicitly aligns inputs with the fixed set of concept embeddings. The framework's ability to perform open-vocabulary detection without retraining the concept bank remains unverified.
- What evidence would resolve it: Evaluation results on a test set containing harmful prompts from categories semantically distinct from the 200 training concepts (e.g., novel forms of harassment or emerging illegal acts).

### Open Question 2
- Question: To what extent does the orthogonal projection used for semantic suppression inadvertently remove benign semantic content that is linearly correlated with unsafe concepts in the embedding space?
- Basis in paper: [inferred] Section 3.3 (Eq. 8) projects token embeddings onto the orthogonal complement of the unsafe subspace. The authors claim this preserves "content fidelity," but the method relies on the assumption that safe and unsafe semantics are linearly separable in CLIP space.
- Why unresolved: In high-dimensional embedding spaces, benign attributes (e.g., "excitement" or "speed") may be geometrically entangled with unsafe concepts (e.g., "recklessness"). The paper evaluates safety and general fidelity, but does not isolate the potential "semantic erosion" of specific benign attributes.
- What evidence would resolve it: A quantitative analysis (e.g., using CLIP-score or human evaluation) measuring the preservation rate of specific benign attributes (e.g., "smiling," "running") in videos where correlated unsafe attributes (e.g., "leering," "fleeing") are suppressed.

### Open Question 3
- Question: Does the framework's intervention strategy transfer effectively to video generation architectures that do not rely on diffusion-based iterative denoising?
- Basis in paper: [inferred] Section 3.3 explicitly states that the suppression mechanism is applied "only during the initial N steps of the diffusion process," tailoring the intervention to the noise-to-video trajectory.
- Why unresolved: The Semantic Risk Suppression relies on modifying guidance during the early denoising steps. It is unclear how this approach would be implemented in autoregressive models (which generate tokens sequentially) or flow-matching models, where the conditioning mechanisms and temporal dynamics differ fundamentally.
- What evidence would resolve it: Application of the ConceptGuard detection and suppression logic to a non-diffusion video generation architecture (e.g., an autoregressive transformer model) with corresponding safety benchmarks.

### Open Question 4
- Question: How does the computational overhead of the two-stage pipeline, specifically the Flux.1 Kontext image editing module, impact the practical latency of real-time or high-throughput video generation?
- Basis in paper: [inferred] Section 3.3 describes using Flux.1 Kontext for image editing prior to video generation, and Section 4.3 confirms this step is active during the "Full Method."
- Why unresolved: The paper focuses on efficacy (harmfulness rate) and accuracy but does not report inference times. Running a large image editing model (Flux.1) followed by a video generation model (CogVideoX) suggests a significant cumulative latency that may limit deployment feasibility.
- What evidence would resolve it: A breakdown of end-to-end inference latency, specifically isolating the time cost of the Stage 1 detection and Stage 2 image editing components versus the base video generation time.

## Limitations

- ConceptGuard's performance is bounded by the comprehensiveness and representativeness of its 200-concept safety framework, potentially missing novel risk categories not captured in training
- The orthogonal projection suppression mechanism assumes linear separability of harmful semantics in embedding space—complex or deeply embedded risks may not be fully mitigated
- The framework's reliance on diffusion-based architectures limits its applicability to non-diffusion video generation methods

## Confidence

**High Confidence**: Detection accuracy results (0.976 on ConceptRisk, 0.960 on T2VSafetyBench-TI2V) and harmfulness reduction metrics (90.0% to 10.0%) are well-supported by comprehensive experiments and ablation studies.

**Medium Confidence**: The claim that cross-modal fusion with risk-aware gating is essential for detecting single-modality risks is supported by specific accuracy numbers (0.944 for safe text + unsafe image) but could benefit from additional stress tests with more diverse risk scenarios.

**Low Confidence**: The long-term effectiveness against evolving adversarial attacks remains uncertain, and the semantic preservation claim during projection suppression lacks rigorous evaluation of content quality degradation.

## Next Checks

1. **OOD Generalization Stress Test**: Evaluate ConceptGuard on a dataset containing risks from 40+ unseen concept categories to verify the 0.960 accuracy claim holds when concepts are truly out-of-distribution.

2. **Adversarial Robustness Benchmark**: Test the system against a battery of adversarial inputs specifically crafted to evade multimodal detection, including gradient-based attacks and concept-substitution strategies.

3. **Long-term Temporal Consistency Analysis**: Track safety and content quality across all diffusion steps (not just initial N=13) to verify that suppressed concepts don't re-emerge in later frames and that early suppression doesn't cause cascading quality degradation.