---
ver: rpa2
title: Best Group Identification in Multi-Objective Bandits
arxiv_id: '2505.17869'
source_url: https://arxiv.org/abs/2505.17869
tags:
- group
- algorithm
- each
- groups
- arms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the best group identification (BGI) problem
  in multi-objective multi-armed bandits, where the objective is to identify optimal
  groups of arms based on their efficiency vectors with high probability while minimizing
  sample complexity. The authors propose two key formulations: group Pareto set identification
  (GPSI) and linear best group identification (LBGI).'
---

# Best Group Identification in Multi-Objective Bandits

## Quick Facts
- **arXiv ID**: 2505.17869
- **Source URL**: https://arxiv.org/abs/2505.17869
- **Reference count**: 40
- **Primary result**: Introduces Best Group Identification (BGI) in multi-objective bandits, proposing GPSI and LBGI formulations with provable (ε,δ)-PAC guarantees and near-optimal sample complexity.

## Executive Summary
This paper introduces the Best Group Identification (BGI) problem in multi-objective multi-armed bandits, where the objective is to identify optimal groups of arms based on their efficiency vectors with high probability while minimizing sample complexity. The authors propose two key formulations: group Pareto set identification (GPSI) and linear best group identification (LBGI). For GPSI, they present the Triple Elimination (TE) algorithm and prove it is (ε,δ)-PAC with near-optimal sample complexity in certain problem instances. For LBGI, they introduce the Equal Effect Confidence Bound (EECB) algorithm with proven δ-correctness and near-optimal sample complexity for non-optimal groups. Both algorithms are elimination-based and extend single-objective techniques to the multi-objective setting.

## Method Summary
The paper proposes two algorithms for BGI. The Triple Elimination (TE) algorithm for GPSI maintains active sets of groups, dimensions, and arms, eliminating sub-optimal elements based on statistical domination tests. The Equal Effect Confidence Bound (EECB) algorithm for LBGI selects sampling dimensions based on a weighted uncertainty criterion (w_d · β) to efficiently identify the group maximizing the weighted sum of rewards. Both algorithms operate in the fixed-confidence setting, using confidence bounds to determine when sufficient evidence has been gathered to identify the optimal group(s) with high probability.

## Key Results
- The TE algorithm for GPSI achieves near-optimal sample complexity by hierarchically eliminating groups, dimensions, and arms based on statistical tests.
- The EECB algorithm for LBGI focuses sampling on the dimension with the highest product of weight and uncertainty, ensuring efficient identification of the optimal group.
- Numerical experiments demonstrate that TE significantly outperforms naive baselines across varying numbers of groups and arms per group.
- Both algorithms provide theoretical guarantees on correctness and sample complexity in the multi-objective setting.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Triangular Elimination
The Triple Elimination (TE) algorithm minimizes sample complexity by discarding groups, dimensions, and arms in a nested manner rather than uniformly sampling all arms. The algorithm maintains active sets for groups (G), dimensions (D_i), and arms (A_{i,d}). It eliminates a group G_i if its efficiency vector is statistically dominated (m(·) ≥ 2β), eliminates a dimension if the efficiency gap between groups is resolved (|Ĥ_j^d - Ĥ_i^d| ≥ 4β + ε), and eliminates arms that are statistically sub-optimal within their group. Core assumption: reward distributions are 1-sub-Gaussian with distinct gap Δ_i between optimal and sub-optimal groups.

### Mechanism 2: Dimension-Wise Equal Effect Balancing
For Linear Best Group Identification (LBGI), sampling efficiency is maximized by focusing rounds on the dimension with the highest product of weight and uncertainty (w_d · β). The EECB algorithm selects dimension d(r) = argmax w_d · β(n_d(r), δ). By pulling active arms in this dimension, it reduces the confidence radius β for the "weakest" dimension. This ensures that the weighted sum Ĥ^T w converges efficiently without wasting samples on already well-estimated or low-weight dimensions. Core assumption: weight vector w is known and there is a unique optimal group.

### Mechanism 3: Efficiency Vector Grouping
Defining group performance via an "efficiency vector" (the max mean per dimension) allows the agent to ignore intra-group variance and focus purely on inter-group domination. Instead of treating each arm independently, the agent estimates Ĥ_i where H_i^d = max_j μ_{i,j}^d. This effectively reduces the problem size from N × K arms to N groups, provided the maximum arm in each dimension can be identified and eliminated quickly. Core assumption: the "best" performance of a group is defined by its peak potential in each dimension, not the average of its arms.

## Foundational Learning

- **Concept: Pareto Dominance & Efficiency Frontiers**
  - Why needed here: The GPSI problem relies entirely on identifying the Pareto set. You cannot implement the TE algorithm without understanding that a vector u is dominated by v if u_d ≤ v_d for all d.
  - Quick check question: Given two groups with efficiency vectors H_1 = (0.8, 0.2) and H_2 = (0.5, 0.5), are either Pareto dominated?

- **Concept: Fixed-Confidence (δ-PAC) Setting**
  - Why needed here: The algorithms stop based on a probability threshold δ, not a fixed time horizon. The derivation of the confidence bound β(r, δ) is the engine of the elimination logic.
  - Quick check question: If we set δ = 0.5 (high error tolerance), should the stopping time τ increase or decrease compared to δ = 0.01?

- **Concept: Sub-Gaussian Random Variables**
  - Why needed here: The paper assumes 1-sub-Gaussian rewards to derive the concentration inequalities (Hoeffding's bound) used in β(r, δ).
  - Quick check question: Why is the sub-Gaussian assumption critical for guaranteeing that the empirical mean ĥ converges to μ at a rate of O(1/√r)?

## Architecture Onboarding

- **Component map:**
  Input: Tensor μ (unknown), Groups N, Arms K, Dimensions D → TE Engine (GPSI): Active Sets (G, D_i, A_{i,d}) → Sampling Rule → Update ĥ, H → 3-Phase Elimination → Output: Estimated optimal set Ĥ_τ
  OR → EECB Engine (LBGI): Active Groups G → Dimension Selector (w_d · β) → Sampling → Group/Arm Elimination → Output: Estimated optimal group

- **Critical path:**
  1. Initialization of active sets (all active)
  2. The loop: Pull active arms → Update empirical efficiency vector Ĥ
  3. Check confidence radius β(r, δ)
  4. Eliminate: Arms ≺ Group Max, Dimensions (Resolved), Groups (Dominated)
  5. Halt when active sets stabilize or thresholds met

- **Design tradeoffs:**
  - TE vs. EECB: Use TE for unweighted multi-objective goals (Pareto); use EECB if specific weights w are known. TE has higher overhead due to dimension tracking.
  - Conservative vs. Aggressive β: The paper defines a specific β(r, δ) for theoretical guarantees. In practice (Section 5), smaller values may work but risk correctness.

- **Failure signatures:**
  - Non-termination: Δ_i,j is too small; β never shrinks enough to resolve domination
  - False Positives: If variance exceeds the sub-Gaussian assumption, the confidence bound fails, leading to incorrect elimination of optimal groups

- **First 3 experiments:**
  1. **Baseline Validation**: Implement the UniS (Uniform Sampling) baseline and the TE algorithm on synthetic data with N=5, K=6, D=3 to verify the sample complexity gap shown in Figure 1.
  2. **Gap Sensitivity**: Run TE while varying Δ_min (the smallest gap) to observe the theoretical scaling of sample complexity (should scale roughly as 1/Δ^2).
  3. **LBGI Weight Stress Test**: Run EECB with highly skewed weights (e.g., w=[0.9, 0.05, 0.05]) to confirm that the algorithm correctly prioritizes the highest-weight dimension for sampling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the Linear Best Group Identification (LBGI) problem be extended to scenarios where the weight vector w is only partially specified or uncertain?
- **Basis in paper**: [explicit] The conclusion explicitly states, "An interesting direction for future work is to extend the LBGI problem to scenarios where the weight vector w is not fully known, but only partially specified."
- **Why unresolved**: The proposed EECB algorithm relies on the weight vector being fully known to determine the sampling dimension and elimination criteria.
- **What evidence would resolve it**: Development of a new algorithm that provides sample complexity guarantees under partial weight information.

### Open Question 2
- **Question**: Can the Best Group Identification problem be solved in the fixed-budget setting, where the number of rounds is limited?
- **Basis in paper**: [inferred] The paper focuses exclusively on the fixed-confidence setting, while Appendix A notes that the related Pareto Set Identification problem has been studied in the fixed-budget setting, implying this variant is open for BGI.
- **Why unresolved**: The theoretical analysis and stopping rules of the TE and EECB algorithms depend on confidence bounds that require a variable horizon based on δ.
- **What evidence would resolve it**: Derivation of bounds on the error probability for a fixed horizon T and a corresponding algorithm.

### Open Question 3
- **Question**: Can the BGI framework be generalized to partition the space of arm mean vectors into arbitrary non-overlapping regions?
- **Basis in paper**: [explicit] Appendix A states that "Studying this general problem [partitioning space of arm mean vectors into regions] in the multi-objective setting can be seen as an important future direction of this work."
- **Why unresolved**: The current paper restricts its analysis to specific optimality criteria (Pareto optimality and linear maximization) rather than general region identification.
- **What evidence would resolve it**: A unified theoretical analysis and algorithmic framework that handles generic partitioning rules.

## Limitations

- **Confidence radius scaling**: The paper uses a specific β(r,δ) that guarantees theoretical bounds, but Section 5 notes smaller values work empirically. The exact tradeoff between conservatism and efficiency remains unclear, particularly in high-dimensional settings.
- **Gap assumptions**: Both algorithms assume distinct separation (Δ_i > 0) between optimal and sub-optimal groups. The practical performance degradation when gaps are small or when variance patterns are non-standard is not fully characterized.
- **Multi-objective interpretation**: The efficiency vector approach (max-per-dimension) may not align with practical multi-objective goals where trade-offs matter more than dominance. The GPSI formulation is conservative—it identifies all Pareto-optimal groups rather than selecting one based on preferences.

## Confidence

- **High confidence**: The algorithmic framework for both TE and EECB is clearly specified with provable correctness. The core elimination logic and stopping conditions are sound under stated assumptions.
- **Medium confidence**: The sample complexity bounds are theoretically derived but rely on specific gap conditions. Empirical validation is limited to synthetic experiments without real-world benchmarks.
- **Low confidence**: The practical implications of the efficiency vector definition and its sensitivity to outliers or high-variance arms within groups.

## Next Checks

1. **Gap Scaling Experiment**: Systematically vary Δ_min (from 0.1 down to 0.01) and measure how sample complexity scales for both TE and EECB. This validates the theoretical 1/Δ^2 scaling and reveals practical break points.

2. **Variance Sensitivity Test**: Replace the 1-sub-Gaussian assumption with heavier-tailed distributions (e.g., t-distribution with low degrees of freedom) to assess algorithm robustness when concentration inequalities no longer hold tightly.

3. **Real-World Benchmark**: Apply TE and EECB to a multi-objective recommendation or portfolio optimization problem where efficiency vectors and Pareto optimality have clear practical meaning, comparing against both naive baselines and problem-specific heuristics.