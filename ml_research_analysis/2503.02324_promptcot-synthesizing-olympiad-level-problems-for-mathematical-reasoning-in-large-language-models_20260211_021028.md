---
ver: rpa2
title: 'PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning
  in Large Language Models'
arxiv_id: '2503.02324'
source_url: https://arxiv.org/abs/2503.02324
tags:
- problem
- problems
- concepts
- rationale
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PromptCoT, a novel approach for automatically
  generating high-quality Olympiad-level math problems to address the scarcity of
  sufficiently challenging problems for training mathematical reasoning in large language
  models. The method synthesizes complex problems by leveraging mathematical concepts
  and a rationale that emulates experienced problem designers' thought processes,
  theoretically maximizing both the likelihood of rationale generation and problem
  generation conditioned on the rationale and concepts.
---

# PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2503.02324
- **Source URL**: https://arxiv.org/abs/2503.02324
- **Reference count**: 40
- **Primary result**: Novel approach generates Olympiad-level math problems via concept-rationale synthesis, achieving 0.4%-4.8% gains on MATH-500 and 6.7%-20% gains on AIME2024

## Executive Summary
This paper introduces PromptCoT, a method for automatically generating high-quality Olympiad-level mathematics problems to address the scarcity of challenging training data for mathematical reasoning in large language models. The approach synthesizes problems by leveraging mathematical concepts and a rationale that emulates experienced problem designers' thought processes, theoretically maximizing both the likelihood of rationale generation and problem generation conditioned on the rationale and concepts. Evaluated on standard benchmarks including GSM8K, MATH-500, and AIME2024, PromptCoT consistently outperforms existing problem generation methods, achieving significant performance improvements and demonstrating superior data scalability.

## Method Summary
PromptCoT generates Olympiad-level problems through a three-stage pipeline: (1) Extract k=5 mathematical concepts from seed AoPS problems using an LLM, (2) Generate rationales by inferring the design thought process from concepts-problem pairs, and (3) Fine-tune a problem generator to map concepts to (rationale, problem) pairs. The method employs rejection sampling with dual LLM evaluators requiring unanimous "perfect" ratings to ensure quality. Generated problems are paired with solutions from teacher models and used to fine-tune student models, with the approach uniquely enabling base models to surpass their Instruct versions.

## Key Results
- Achieves 0.4%-4.8% absolute gains on MATH-500 and 6.7%-20% absolute gains on AIME2024 over existing methods
- Demonstrates superior data scalability, maintaining high performance as dataset size increases
- Enables Qwen2.5-Math base models to surpass their Instruct versions, with a 7B model achieving performance comparable to state-of-the-art 32B models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating Olympiad-level problems is tractable when decoupling generation into rationale and problem, rather than direct generation
- **Mechanism:** By introducing latent variable $z$ (rationale), the system maximizes $p(z|c)$ and $p(x|z,c)$ effectively, acting as a density amplifier bridging abstract concepts and complex problem structures
- **Core assumption:** The logic used to design a problem is learnable and distinct from the logic used to solve it
- **Evidence anchors:** [abstract] "optimal rationale should maximize both the likelihood of rationale generation given the associated concepts and the likelihood of problem generation conditioned on both"
- **Break condition:** If rationale generator produces generic or hallucinated design logic that doesn't entail the target problem, the chain is broken

### Mechanism 2
- **Claim:** The system leverages "Concepts" as structured constraints to ground synthesis, preventing drift seen in unconstrained generation
- **Mechanism:** Conditions on specific mathematical concepts (e.g., "Prime Numbers," "Congruence Theorems") to force generation to adhere to valid mathematical curriculum
- **Core assumption:** Foundational concepts extracted by LLM from seed prompts are sufficient descriptors to define the problem space
- **Evidence anchors:** [abstract] "synthesizes complex problems by leveraging mathematical concepts and a rationale"
- **Break condition:** If concept extraction is noisy or too broad, generated problems may lack coherence or fail to target specific reasoning bottlenecks

### Mechanism 3
- **Claim:** Superior data scalability is achieved through iterative rejection sampling loop maintaining quality while expanding volume
- **Mechanism:** Generates high volume of candidates but iteratively filters using dual LLM evaluators, acting as noise filter preventing model collapse during scaling
- **Core assumption:** Evaluator models have sufficient capability to reliably grade Olympiad-level difficulty and solvability
- **Evidence anchors:** [abstract] "exhibits superior data scalability, consistently maintaining high performance as the dataset size increases"
- **Break condition:** If evaluators fail to detect subtle mathematical errors, training set becomes contaminated

## Foundational Learning

- **Concept: Variational Inference (ELBO)**
  - **Why needed here:** Paper frames finding optimal rationale $z$ as maximizing Evidence Lower Bound (ELBO). Understanding $q(z|c,x) \propto p(x|z,c)p(z|c)$ is necessary to grasp why rationale is generated as it is
  - **Quick check question:** How does maximizing the ELBO help approximate the intractable posterior of a problem's design logic?

- **Concept: Knowledge Distillation**
  - **Why needed here:** Ultimate goal is transferring reasoning capabilities from large "Teacher" to smaller "Student" using synthesized data
  - **Quick check question:** Why might training a student on synthetic Olympiad problems yield better results than training on teacher's original training set?

- **Concept: Chain-of-Thought (CoT) for Synthesis**
  - **Why needed here:** Unlike standard CoT which solves a problem, PromptCoT uses CoT to construct a problem. This inversion is the core novelty
  - **Quick check question:** How does the prompt structure for "Rationale Generation" differ from a standard "Solve this problem" prompt?

## Architecture Onboarding

- **Component map:** Seed Bank -> Concept Extractor -> Rationale Generator -> Problem Generator -> Evaluators -> Student Trainer
- **Critical path:** The Rationale Generation step is most sensitive. If reverse-engineered thought process doesn't logically imply the problem, resulting training data will be syntactically correct but semantically hollow
- **Design tradeoffs:**
  - Generator Size: Uses Llama-3.1-8B for generation, which is efficient but limits ceiling of problem complexity
  - Rejection Threshold: Requiring "unanimous perfect" ratings ensures high precision but may drastically reduce dataset size
- **Failure signatures:**
  - Low Rejection Rate: Generator hallucinates math concepts (e.g., false theorems)
  - Plateauing Performance: Generated problems are valid but lack diversity (mode collapse)
  - Solvable but Uninteresting: Problems are mathematically correct but don't require multi-step reasoning
- **First 3 experiments:**
  1. Run Concept Extractor on 50 seed problems and manually verify if extracted concepts accurately reflect necessary skills
  2. Train two problem generators—one with rationale condition ($p(x|z,c)$) and one without ($p(x|c)$)—and compare "solvable" rate of generated outputs
  3. Measure accuracy of Qwen2.5-Math-72B-Instruct on generated problems vs. AIME problems to verify difficulty distribution matches target

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can PromptCoT effectively generate problems at the International Mathematical Olympiad (IMO) level of difficulty if implemented with larger foundation models?
- **Basis in paper:** [explicit] The "Limitations" section states the current generator (Llama-3.1-8B) struggles with IMO-level complexity and suggests future work explore larger pre-trained models
- **Why unresolved:** Paper evaluates performance on AIME/AMC benchmarks but identifies generation of "exceptionally challenging" IMO problems as current capability gap due to model scale
- **Evidence:** Evaluation results from PromptCoT implementation using a 70B+ parameter generation model on IMO-specific benchmarks

### Open Question 2
- **Question:** Does superior data scalability of PromptCoT persist when synthesis scale is expanded from thousands to millions of problems to match pretraining corpora?
- **Basis in paper:** [explicit] The "Limitations" section notes that 905K problems is modest compared to pretraining data and calls for extending scalability to align with LLM training data scales
- **Why unresolved:** Current experiments demonstrate scalability up to 800K problems, but unverified if this trend continues to billion-token scale required for pre-training
- **Evidence:** Scaling law analysis of model performance when trained on datasets generated by PromptCoT ranging from 1 million to 100 million samples

### Open Question 3
- **Question:** Can rationale-guided synthesis framework be effectively adapted to low-resource domains outside of mathematics, such as physics or logic?
- **Basis in paper:** [explicit] Section 2.1 claims the method "can be readily adapted to other domains" by replacing seed prompts, but experiments are restricted to mathematical reasoning
- **Why unresolved:** Specific "concept extraction" and "rationale generation" instructions are tailored for math; transferability to non-mathematical reasoning is asserted but not demonstrated
- **Evidence:** Performance metrics of models fine-tuned on PromptCoT-generated datasets for non-math benchmarks (e.g., physical reasoning or formal logic)

## Limitations

- The method appears capped at "difficulty below IMO" due to the 8B generator size limitation, suggesting the approach may not scale to most challenging Olympiad problems without architectural changes
- The "unanimous perfect" rating requirement for rejection sampling, while ensuring high quality, may severely limit dataset size and diversity, potentially creating mode collapse
- Reliance on LLMs as both generators and evaluators creates a closed-loop system where errors can compound

## Confidence

- **High Confidence**: The basic framework of using concept-rationale-problem triples for generation is well-supported by experimental results across multiple benchmarks
- **Medium Confidence**: Claims about superior data scalability are supported by ablation studies, but long-term scaling properties beyond tested dataset sizes remain uncertain
- **Medium Confidence**: Distillation results showing base models outperforming instruct versions are compelling, but comparison doesn't account for potential differences in pretraining data between model variants

## Next Checks

1. **Evaluator Calibration Test**: Independently evaluate the same generated problems using multiple different LLM families (not just Llama/Qwen) to verify the "unanimous perfect" ratings aren't model-specific artifacts

2. **Diversity Analysis**: Measure the concept coverage and problem type distribution in generated datasets versus the seed AoPS problems to quantify potential mode collapse from the strict rejection criteria

3. **Difficulty Scaling Experiment**: Systematically test whether increasing the generator model size (beyond 8B) produces proportionally more difficult problems, or if the approach hits a ceiling regardless of model scale