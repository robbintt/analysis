---
ver: rpa2
title: 'VIDEOP2R: Video Understanding from Perception to Reasoning'
arxiv_id: '2511.11113'
source_url: https://arxiv.org/abs/2511.11113
tags:
- reasoning
- video
- answer
- perception
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VIDEOP2R, a process-aware video reasoning
  framework that models perception and reasoning as distinct processes to improve
  video understanding. The key innovation is a two-stage reinforcement fine-tuning
  approach: an SFT stage that generates a process-aware CoT dataset (VIDEOP2R-CoT-162K)
  with separate perception and reasoning annotations, followed by an RL stage using
  a novel process-aware GRPO (PA-GRPO) that assigns separate rewards for perception
  and reasoning.'
---

# VIDEOP2R: Video Understanding from Perception to Reasoning

## Quick Facts
- arXiv ID: 2511.11113
- Source URL: https://arxiv.org/abs/2511.11113
- Authors: Yifan Jiang; Yueying Wang; Rui Zhao; Toufiq Parag; Zhimin Chen; Zhenyu Liao; Jayakrishnan Unnikrishnan
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on six out of seven video reasoning benchmarks, outperforming previous RFT methods by 1.3% on average accuracy

## Executive Summary
This paper introduces VIDEOP2R, a process-aware video reasoning framework that models perception and reasoning as distinct processes to improve video understanding. The key innovation is a two-stage reinforcement fine-tuning approach: an SFT stage that generates a process-aware CoT dataset (VIDEOP2R-CoT-162K) with separate perception and reasoning annotations, followed by an RL stage using a novel process-aware GRPO (PA-GRPO) that assigns separate rewards for perception and reasoning. This design addresses the challenge of credit assignment in video reasoning where perception errors can propagate to reasoning errors. VIDEOP2R achieves state-of-the-art performance on six out of seven video reasoning benchmarks, outperforming previous RFT methods by 1.3% on average accuracy.

## Method Summary
VIDEOP2R uses a two-stage training approach with Qwen2.5-VL-7B as base model. First, an SFT stage trains on VideoP2R-CoT-162K, a process-aware CoT dataset generated through a three-step pipeline: generate CoT traces, verify answer correctness, then filter using Claude 3.7 to judge observation sufficiency. Second, an RL stage applies PA-GRPO to original 260K VQA pairs, computing separate rewards for perception (LLM-judged sufficiency) and reasoning (task-specific metrics) within the CoT structure. The method uses fixed length targets ([128,320] for perception, [320,512] for reasoning) and processes 16-32 frames at 128×28×28 resolution. Training uses 8×A800 GPUs with batch size 8 (SFT) or 56 (RL), while inference uses 32 frames at 256×28×28 resolution.

## Key Results
- Achieves state-of-the-art performance on six out of seven video reasoning benchmarks
- Outperforms previous RFT methods by 1.3% on average accuracy
- Demonstrates consistent gains of 1.9%-9.1% over the base model across benchmarks
- Ablation studies confirm the effectiveness of process-aware modeling and PA-GRPO
- Perception outputs generated by VIDEOP2R are shown to be information-sufficient for downstream reasoning

## Why This Works (Mechanism)

### Mechanism 1
Separate reward signals for perception and reasoning enable more precise credit assignment than process-agnostic single-reward approaches. PA-GRPO decomposes the trajectory into perception tokens (within `<observation>` segments) and reasoning tokens (within `<think>` and `<answer>` segments). Each receives its own reward: perception accuracy is LLM-judged for sufficiency; reasoning accuracy uses task-specific metrics (exact match, ROUGE, etc.). These rewards are normalized within their respective groups before computing advantages, preventing scale mismatch between processes.

### Mechanism 2
Process-aware CoT data with observation sufficiency verification creates training examples where perception is demonstrably adequate for downstream reasoning. The three-step pipeline generates CoT traces, verifies answer correctness, then uses Claude 3.7 to judge whether the `<observation>` segment alone contains sufficient evidence to answer the question. Samples failing this cross-modal validation are filtered out, yielding 162K high-quality samples from 260K raw pairs.

### Mechanism 3
Two-stage training (SFT warm-start then RL refinement) is necessary; single-stage training is insufficient. SFT establishes the process-separated output template and provides strong initialization. RL then pushes beyond supervised boundaries via reward-driven exploration. Ablation shows SFT-only improves +2.7%, RL-only +3.1%, but combined yields +5.8% average improvement.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: PA-GRPO extends GRPO by adding process-aware rewards. Understanding GRPO's group-based advantage normalization is prerequisite.
  - Quick check question: Given G sampled responses with rewards r₁...r_G, how does GRPO compute the advantage for response i?

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The entire framework relies on structured CoT outputs with explicit perception/reasoning segments.
  - Quick check question: What is the purpose of explicit reasoning traces versus direct answer prediction?

- **Credit Assignment in RL**
  - Why needed here: The core contribution is improving credit assignment by separating perception and reasoning rewards.
  - Quick check question: Why might a single scalar reward at trajectory end cause poor credit assignment in multi-stage reasoning?

## Architecture Onboarding

- Component map: Base model (Qwen2.5-VL-7B) → SFT on VideoP2R-CoT-162K → VideoP2R-SFT → PA-GRPO training on 260K QA pairs → VideoP2R (final)
- Critical path: SFT data quality (observation sufficiency) → SFT template adherence → PA-GRPO perception reward calibration → reasoning quality improvement
- Design tradeoffs:
  - Fixed length targets ([128,320] for perception, [320,512] for reasoning) may truncate complex spatial descriptions (VSI-Bench failure mode)
  - Uniform 16-frame sampling risks missing key frames; inference uses 32 frames
  - Claude 3.7 as judge introduces external dependency; Llama3.1-8B achieves 82% agreement but lower downstream performance
- Failure signatures:
  - Key-frame missing: Uniform sampling omits question-critical frames → perception incomplete → reasoning fails
  - Overly detailed configurations: Spatial reasoning questions exceed length budget → observation compressed → reasoning errors
  - Think-answer mismatch: Single-reward GRPO models show 16%+ mismatch rates; PA-GRPO reduces to ~7%
- First 3 experiments:
  1. Reproduce SFT-only baseline: Train Qwen2.5-VL-7B on VideoP2R-CoT-162K, verify process-separated output format compliance (>95% format reward on held-out samples)
  2. Ablate perception reward: Run PA-GRPO without R_P, confirm performance drops below SFT baseline on VideoMME (per Table 2)
  3. Verify sufficiency judge reliability: Sample 200 observations, compare LLM judge against human labels, target >90% agreement before full training run

## Open Questions the Paper Calls Out

### Open Question 1
How can a dynamic length reward mechanism be designed to adaptively adjust target sequence lengths based on task complexity and visual content? The current static thresholds work for general tasks but fail when questions demand extensive spatial detail (e.g., tracking multiple object positions across video frames). A dynamic reward system that improves VSI-Bench performance without degrading other benchmarks would demonstrate adaptive behavior across question types.

### Open Question 2
Can adaptive frame sampling strategies mitigate key-frame missing failures without substantially increasing computational costs? Uniform sampling with 16-32 frames may miss temporally sparse but critical visual evidence, yet more frames increase inference cost quadratically. A sampling method achieving higher accuracy on motion-critical benchmarks (e.g., TempCompass) while maintaining comparable inference latency would resolve this.

### Open Question 3
How can domain-specific factual knowledge be effectively integrated into the VIDEOP2R framework to improve performance on knowledge-intensive benchmarks like MMVU? The current framework relies on visual perception and reasoning but lacks mechanisms for grounding external knowledge during inference. Demonstrated improvements on MMVU through knowledge-enhanced training or retrieval-augmented generation without sacrificing perception-reasoning separation would resolve this.

## Limitations
- The sufficiency judgment by LLM introduces potential bias and may systematically miss certain types of perception failures, particularly in complex spatial reasoning tasks
- Fixed length constraints for perception and reasoning segments may be suboptimal for different benchmark types, potentially explaining the VSI-Bench performance gap
- The two-stage training approach assumes SFT provides adequate template structure before RL refinement, but if the supervised data distribution differs significantly from test distribution, this warm-start could introduce bias

## Confidence
- High confidence: The overall performance improvements across 6/7 benchmarks and the ablation study showing combined SFT+RL superiority over single-stage training
- Medium confidence: The claim that separate rewards enable more precise credit assignment, as this mechanism is theoretically sound but the specific reward normalization approach could have alternative formulations
- Medium confidence: The sufficiency judgment approach using text-only LLM evaluation, as it shows reasonable agreement but hasn't been validated against human judgments on the specific filtered dataset

## Next Checks
1. Conduct human evaluation of the 162K filtered dataset to verify that LLM sufficiency judgments accurately identify perception traces that contain adequate visual evidence, targeting >95% agreement on a random sample
2. Test the model with adaptive frame sampling strategies (rather than uniform 16-frame sampling) on VSI-Bench to determine if key-frame preservation improves spatial reasoning performance
3. Run ablation experiments with different perception/reasoning length constraints on each benchmark to identify optimal segment lengths for different reasoning types, particularly for spatial vs temporal reasoning tasks