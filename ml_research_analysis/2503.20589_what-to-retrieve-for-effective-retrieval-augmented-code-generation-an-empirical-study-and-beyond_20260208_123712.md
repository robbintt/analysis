---
ver: rpa2
title: What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical
  Study and Beyond
arxiv_id: '2503.20589'
source_url: https://arxiv.org/abs/2503.20589
tags:
- code
- generation
- information
- arxiv
- apis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts an empirical study to analyze the effectiveness
  of different information sources in retrieval-augmented code generation. The study
  reveals that contextual information and API information significantly improve model
  performance, while retrieved similar code often introduces noise and can degrade
  results by up to 15%.
---

# What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical Study and Beyond

## Quick Facts
- **arXiv ID:** 2503.20589
- **Source URL:** https://arxiv.org/abs/2503.20589
- **Reference count:** 40
- **Key outcome:** Contextual information and API information improve repository-level code generation, while retrieved similar code often introduces noise and degrades results by up to 15%.

## Executive Summary
This paper conducts an empirical study to analyze the effectiveness of different information sources in retrieval-augmented code generation. The study reveals that contextual information and API information significantly improve model performance, while retrieved similar code often introduces noise and can degrade results by up to 15%. Based on these findings, the authors propose AllianceCoder, which employs chain-of-thought prompting to decompose user queries into implementation steps and retrieves APIs via semantic description matching. AllianceCoder achieves state-of-the-art performance, improving Pass@1 by up to 20% over existing approaches. The study provides valuable insights into effective retrieval strategies for repository-level code generation.

## Method Summary
The study evaluates three types of retrieval information—context, similar code, and APIs—on repository-level code generation tasks. AllianceCoder is proposed as a solution that first decomposes user queries into implementation steps using chain-of-thought prompting, then generates semantic descriptions for required APIs, and retrieves them from the repository using cosine similarity. The system combines retrieved APIs with current file context and the original query to generate code. Experiments are conducted on Python benchmarks CoderEval and RepoExec, measuring performance using Pass@1, Pass@3, and Pass@5 metrics.

## Key Results
- Contextual information consistently improves code generation performance.
- API information, when retrieved via semantic description matching, significantly enhances results.
- Retrieved similar code introduces noise that can degrade performance by up to 15%.
- AllianceCoder achieves up to 20% improvement in Pass@1 over existing approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting repository APIs to natural language descriptions before retrieval improves semantic matching accuracy.
- Mechanism: A persistent alignment gap exists between code and natural language modalities. By using an LLM to generate descriptions for each API and encoding those descriptions (rather than raw code) into vectors, retrieval becomes a text-to-text similarity problem rather than text-to-code. This reduces mismatch between user intent expressed in natural language and the target API implementation.
- Core assumption: The LLM used for description generation produces accurate, semantically meaningful summaries of API functionality.
- Evidence anchors:
  - [abstract] "retrieves APIs via semantic description matching"
  - [section 6.2.1] "a semantic gap exists between programming languages and natural language, which can result in low cosine similarity between code snippets and their corresponding descriptions"
  - [corpus] SelfRACG (arXiv:2507.19033) identifies a "content gap" in code retrieval where consecutive fragments diverge due to logical progression, supporting the need for better semantic bridging
- Break condition: If repository APIs are poorly documented or cryptically named, LLM-generated descriptions may hallucinate functionality, degrading retrieval precision.

### Mechanism 2
- Claim: Chain-of-thought query decomposition into implementation steps enables more targeted API retrieval.
- Mechanism: Rather than directly matching a high-level user query to APIs, AllianceCoder first decomposes the query into concrete implementation steps. For each step, the LLM generates descriptions of potential APIs needed. This creates multiple focused retrieval queries instead of one broad query, increasing the likelihood of matching the correct APIs.
- Core assumption: The decomposed steps and generated API descriptions accurately reflect the actual APIs required for implementation.
- Evidence anchors:
  - [abstract] "employs chain-of-thought prompting to decompose user queries into implementation steps and retrieves APIs via semantic description matching"
  - [section 6.3.1] "we first provide examples and instruct LLMs to decompose the overall code task into a sequence of concrete implementation steps"
  - [corpus] Dynamic and Parametric RAG (arXiv:2506.06704) critiques static retrieve-then-generate pipelines, aligning with AllianceCoder's adaptive, query-aware retrieval approach
- Break condition: If the LLM's step decomposition is logically flawed or omits critical implementation phases, subsequent API retrieval will miss essential dependencies.

### Mechanism 3
- Claim: Retrieving similar code snippets introduces noise that degrades repository-level code generation performance.
- Mechanism: Unlike code completion (where partial context constrains the output), code generation requires synthesizing entire functions. Similar code retrieved from the repository may be functionally dissimilar to the target, providing misleading patterns. The LLM may then hallucinate incorrect logic based on irrelevant retrieved examples.
- Core assumption: The repository lacks guaranteed functionally similar implementations for arbitrary generation tasks.
- Evidence anchors:
  - [abstract] "retrieved similar code often introduces noise, degrading results by up to 15%"
  - [section 5.1] "retrieved relevant code contributes very little to performance improvement and can even degrade performance when combined with other types of supplementary information"
  - [corpus] CoRaCMG (arXiv:2509.18337) emphasizes contextual retrieval for commit messages, implicitly supporting the broader principle that not all retrieved context is beneficial
- Break condition: In repositories with highly repetitive patterns or near-duplicate functions, similar code retrieval may become beneficial rather than harmful.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: AllianceCoder is fundamentally a RAG framework specialized for code. Understanding the retriever-generator split is essential to grasp why retrieval quality directly impacts generation quality.
  - Quick check question: Can you explain why RAG helps with long-context limitations in LLMs?

- Concept: Semantic Gap in Code-Text Alignment
  - Why needed here: The core innovation is bridging this gap via description generation. Without understanding the problem, the solution appears unnecessary.
  - Quick check question: Why would a vector embedding of `def parse_json(s):` have low cosine similarity with the query "load configuration from file"?

- Concept: Pass@k Metric
  - Why needed here: All performance claims are evaluated using Pass@1, Pass@3, and Pass@5. Understanding this metric is required to interpret experimental results.
  - Quick check question: If a model has Pass@1=30% and Pass@5=50%, what does this imply about its output variance?

## Architecture Onboarding

- Component map:
  - Repository API Processing: LLM generates descriptions for all APIs → UniXcoder encodes descriptions → vectors stored
  - Query Processing: User query → CoT decomposition into steps → API description generation per step → description extension/refinement → encoding
  - Context-Integrated Code Generation: Vector similarity retrieval (cosine) → top API per description → concatenate APIs + context + query → LLM generates code

- Critical path:
  1. Offline: Repository API description generation and encoding (one-time cost per repo)
  2. Runtime: Query decomposition → API description generation → retrieval → code generation

- Design tradeoffs:
  - Precision vs. Recall in API retrieval: AllianceCoder retrieves only the single highest-similarity API per description, trading coverage for precision. Table 6 shows this leads to predicting more APIs than needed in 50-74% of cases.
  - Prompt length vs. information density: Section 5.2 demonstrates that naively combining context and APIs can fail test cases due to excessive input length, requiring careful prompt ordering.
  - Offline vs. online computation: API descriptions are generated offline; query-side descriptions are generated online, adding latency.

- Failure signatures:
  - Hallucinated API descriptions from the query-processing LLM lead to irrelevant retrievals.
  - Over-prediction of APIs (Table 6: 51-74% "Higher" than actual) may introduce noise into the prompt.
  - Repositories with minimal internal APIs provide little retrieval benefit; performance approaches baseline ContextLLM.

- First 3 experiments:
  1. Ablation on retrieval source: Compare AllianceCoder with three variants—(a) retrieve APIs directly from code vectors without description generation, (b) retrieve similar code instead of APIs, (c) retrieve APIs + similar code. Expect (a) and (b) to underperform, confirming the semantic gap and noise mechanisms.
  2. Prompt length sensitivity: Systematically vary the position and length of retrieved API content in the prompt. Measure where performance drops due to attention dilution, validating the Finding 2 observation.
  3. Cross-LLM generalization: Run AllianceCoder's pipeline with different backbone LLMs (e.g., CodeLlama, DeepSeek-Coder) for both description generation and final code generation. Assess whether description quality or generation capability is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AllianceCoder perform in repository-level code generation for programming languages other than Python?
- Basis in paper: [explicit] Section 9 states the current evaluation was limited to Python and notes, "In the future, we will conduct experiments on more programming languages."
- Why unresolved: The empirical study and AllianceCoder evaluation were restricted to the Python-based CoderEval and RepoExec benchmarks.
- What evidence would resolve it: Evaluation results on multi-lingual benchmarks or repository-level datasets in languages like Java or C++.

### Open Question 2
- Question: How can the framework be made robust against LLM hallucinations during the API description generation phase?
- Basis in paper: [explicit] Section 9 identifies "edge cases (e.g., generated hallucinated descriptions)" as a threat that "require[s] further investigation."
- Why unresolved: AllianceCoder relies on LLMs to generate accurate natural language descriptions for retrieval; hallucinated descriptions lead to retrieval failure, and no automated correction mechanism was proposed.
- What evidence would resolve it: An error analysis of retrieval failures caused by hallucinated descriptions and a proposed filtering or verification step.

### Open Question 3
- Question: What is the true performance of AllianceCoder on dynamically evolving benchmarks free from pre-training data leakage?
- Basis in paper: [explicit] Section 9 identifies potential data leakage as an external threat and suggests that "dynamically evolving benchmarks could be investigated to mitigate this threat."
- Why unresolved: Current benchmarks rely on open-source repositories that may have been included in the pre-training data of the evaluated LLMs (GPT, Gemini).
- What evidence would resolve it: Evaluation on repositories created or updated after the LLMs' knowledge cutoff dates to ensure zero overlap.

## Limitations

- Performance improvements are highly dependent on repository API density, with minimal benefit in repositories lacking documented APIs.
- The system relies on LLM-generated API descriptions, making it vulnerable to hallucination and semantic misrepresentation.
- Current evaluation is limited to Python, leaving multi-language generalization effectiveness uncertain.

## Confidence

**High confidence**: The finding that contextual information improves code generation performance is well-established across multiple RAG studies. The empirical observation that retrieved similar code introduces noise is directly measurable through controlled ablation experiments.

**Medium confidence**: The mechanism that description-based retrieval improves semantic matching assumes the description generation LLM produces faithful representations. While theoretically sound, this depends on the quality of the underlying LLM's reasoning about code functionality, which varies across model versions.

**Low confidence**: The claimed 20% Pass@1 improvement requires careful interpretation. The metric comparison involves different backbone LLMs (GPT-4o Mini vs. Gemini 1.5 Flash) and different temperature settings, introducing confounding variables beyond the retrieval strategy itself.

## Next Checks

1. **Cross-repository generalization**: Apply AllianceCoder to repositories with varying API density (e.g., Django vs. pure algorithmic libraries) and measure whether the Pass@1 improvement scales proportionally with API availability, confirming the mechanism's dependency on repository structure.

2. **Description quality audit**: For a sample of retrieved APIs, manually evaluate whether the LLM-generated descriptions accurately capture API functionality. Compare semantic similarity scores between (query description, API description) pairs versus (query, API code) pairs to quantify the semantic gap reduction.

3. **Ablation on API over-prediction**: Since Table 6 shows AllianceCoder predicts more APIs than needed in 51-74% of cases, conduct an ablation study where retrieved APIs are filtered based on relevance scoring before being included in the generation prompt. Measure whether this reduces noise without sacrificing precision.