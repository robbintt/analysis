---
ver: rpa2
title: 'MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution
  to Automate Cross-App Instructions'
arxiv_id: '2502.16796'
source_url: https://arxiv.org/abs/2502.16796
tags:
- task
- execution
- arxiv
- mobilesteward
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution to Automate Cross-App Instructions

## Quick Facts
- **arXiv ID:** 2502.16796
- **Source URL:** https://arxiv.org/abs/2502.16796
- **Reference count:** 40
- **Primary result:** A multi-agent system that automates complex cross-app instructions on Android via centralized planning, specialized app agents, and self-evolving memory.

## Executive Summary
MobileSteward is a multi-agent framework designed to automate complex, cross-application tasks on Android smartphones. It uses a centralized StewardAgent to decompose instructions into a scheduling graph of app-specific sub-tasks, which are then assigned to specialized StaffAgents. Each StaffAgent leverages memory of past successful executions to perform actions within its assigned app. The system incorporates a self-evolution mechanism that updates both planning and execution memories based on successful task completions, enabling continuous improvement without weight updates.

## Method Summary
MobileSteward employs a two-tier agent architecture: a StewardAgent for centralized planning and evaluation, and multiple StaffAgents, each specialized for a specific app. The StewardAgent decomposes user instructions into a Directed Acyclic Graph (DAG) of sub-tasks, guided by a Staff Expertise Memory. StaffAgents execute these sub-tasks using retrieved Task Guidelines from their memory. The system includes a self-evolution loop where successful execution traces are analyzed to update both the StewardAgent's planning memory and the StaffAgents' execution memories. The framework was evaluated on the CAPBench benchmark of 500 cross-app instructions using an Android emulator, comparing against baselines like AppAgent and MobileAgent-v2.

## Key Results
- **Success Rate (CAPBench):** MobileSteward achieved a 0.86 success rate, outperforming AppAgent (0.72) and MobileAgent-v2 (0.74).
- **Dynamic Recruitment Accuracy:** StewardAgent achieved a 0.72 success rate on 2-app tasks and 0.67 on 3-app tasks.
- **Self-Evolution Impact:** Ablation studies showed that self-evolution significantly improved performance, with a clear performance gap when using open-source models versus GPT-4o for planning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A centralized StewardAgent decomposing complex cross-app instructions into app-oriented tasks reduces scheduling errors compared to monolithic planning.
- **Mechanism:** The StewardAgent uses a Staff Expertise Memory to identify relevant apps and dependencies. It then constructs a Directed Acyclic Graph (DAG) called a "scheduling graph" where nodes are app-specific tasks and edges represent information flow, enabling structured topological execution.
- **Core assumption:** Complex user instructions contain implicit or explicit dependencies between applications that can be accurately modeled as a DAG based on information flow.
- **Evidence anchors:**
  - [abstract] "...Dynamic Recruitment generates a scheduling graph guided by information flow to explicitly associate tasks among apps."
  - [section] "...StewardAgent decomposes the instruction into sub-tasks on the specific apps based on Staff Expertise Memory... and then constructs the scheduling graph SG among them..."
  - [corpus] Corpus evidence for this specific DAG-based decomposition is weak; however, related work like MADRA [75688] uses multi-agent debate for risk-aware planning, suggesting structured multi-agent coordination is a recognized strategy for complex tasks.
- **Break condition:** The mechanism fails if user instructions are highly ambiguous or if the required task logic involves circular dependencies that cannot be represented as a DAG.

### Mechanism 2
- **Claim:** Assigning tasks to specialized, app-specific StaffAgents improves execution accuracy by leveraging focused expertise and targeted demonstrations.
- **Mechanism:** Each StaffAgent is role-prompted to be an expert in a single application (e.g., Clock, Contacts). During execution, it retrieves relevant "Task Guidelines"—successful action sequences—from a dedicated memory to inform its plan and next-action prediction.
- **Core assumption:** Application interfaces and workflows are sufficiently heterogeneous that generalist agents struggle, and successful execution strategies can be effectively summarized as reusable textual guidelines.
- **Evidence anchors:**
  - [abstract] "Assigned Execution assigns the task to app-oriented StaffAgents, each equipped with app-specialized expertise to address the diversity between apps."
  - [section] "StaffAgent first extracts the successful execution demonstrations related to the assigned task Tj from memory MG to make a task plan pj."
  - [corpus] Agent-SAMA [102772] notes that existing reactive agents lack state-awareness; specialized agents with explicit memories for state/demonstrations address this limitation.
- **Break condition:** Performance degrades if the Task Guideline Memory is empty, leading to a cold-start problem, or if the BM25 retrieval fails to surface relevant demonstrations for a novel task.

### Mechanism 3
- **Claim:** A memory-based self-evolution loop allows the system to continuously refine its coordination and execution strategies without weight updates.
- **Mechanism:** Upon successful task completion, the StewardAgent analyzes the execution trace to extract two types of knowledge: "staff expertise" (what a StaffAgent can do) to update its scheduling memory, and "task guidelines" (how a task is done) to update the StaffAgent's execution memory.
- **Core assumption:** High-quality, generalizable procedural knowledge can be reliably extracted from successful action traces via an LLM and is beneficial for future, potentially different tasks.
- **Evidence anchors:**
  - [abstract] "We develop a Memory-based Self-evolution mechanism, which summarizes the experience from successful execution, to improve the performance of MobileSteward."
  - [section] "...StewardAgent will summarize the execution process, extract the staff expertise and task guidelines, and update the memory."
  - [corpus] MOBIMEM [4967] and Mobile-Agent-E [90381] both explicitly champion self-evolution and experience-driven learning for mobile agents, supporting the core premise that post-deployment adaptation is critical.
- **Break condition:** The mechanism risks performance degradation if the LLM "summarizes" noisy, overly specific, or hallucinated guidelines, thereby corrupting the memory for future tasks.

## Foundational Learning

- **Concept: Role-Playing with Large Language Models**
  - **Why needed here:** The entire MobileSteward architecture is constructed via prompt engineering, defining the distinct personas and responsibilities of the "Steward" (coordinator) and "Staff" (executor) agents.
  - **Quick check question:** Can you design two system prompts for an LLM where one persona only decomposes tasks and another only executes them, and have them work together on a simple logical puzzle?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** StaffAgents use BM25 retrieval to find relevant "Task Guidelines" from a knowledge base of past successes. Understanding how retrieval informs generation is key to this module.
  - **Quick check question:** Given a user's current goal and a database of past successful solutions, how would you use a similarity search to retrieve the most relevant example to guide the user?

- **Concept: Directed Acyclic Graphs (DAGs)**
  - **Why needed here:** The "Dynamic Recruitment" module outputs a scheduling graph that is a DAG. Understanding topological sorting is necessary to follow the task execution order.
  - **Quick check question:** If Task A produces information needed for Task B, and Task B produces information needed for Task C, how would you represent and order these tasks in a graph?

## Architecture Onboarding

- **Component map:**
  - **StewardAgent:** The coordinator equipped with **Staff Expertise Memory**. It performs `Schedule` (Dynamic Recruitment), `Evaluate` execution, `Reflect` on errors, `Extract` knowledge, and `Update` memories.
  - **StaffAgent(s):** The executors. Each is bound to a specific app and equipped with **Task Guideline Memory**. They `Plan` using retrieved guidelines, `Predict` actions, and `Summary` their steps.
  - **Environment:** An Android simulator providing multimodal state (screenshots + XML hierarchy) which agents perceive and act upon via a defined action space (tap, swipe, type, etc.).

- **Critical path:**
  1. **Dynamic Recruitment:** A user instruction is parsed by the StewardAgent into a DAG of app-specific sub-tasks.
  2. **Assigned Execution:** Sub-tasks are assigned to appropriate StaffAgents in topological order. StaffAgents execute actions using their memory until the sub-task is complete or fails.
  3. **Adjusted Evaluation:** StewardAgent reviews the outcome. On success, it triggers the `Update` process. On failure, it provides `Reflect` tips for a retry.
  4. **Self-Evolution:** Extracted knowledge from successful runs updates both Steward and Staff memories, closing the learning loop.

- **Design tradeoffs:**
  - **Centralized vs. Distributed Planning:** A centralized Steward simplifies coordination but creates a potential bottleneck and single point of failure.
  - **Object-Oriented vs. Procedure-Oriented:** The paper argues for "object-oriented" (one agent per app) over "procedure-oriented" (one agent per pipeline stage), trading general agent flexibility for deep, specialized app proficiency.
  - **Memory Retrieval Method:** Using BM25 for retrieving task guidelines is computationally efficient but may lack the semantic nuance of dense embedding-based retrieval, potentially missing relevant analogies.

- **Failure signatures:**
  - **Recruitment Loop:** The StewardAgent repeatedly fails to generate a valid DAG or assigns tasks to the wrong StaffAgent.
  - **Execution Drift:** A StaffAgent wanders through an app, taking irrelevant actions because its retrieved guidelines are poor or its planning is flawed.
  - **Memory Corruption:** The `Extract` module begins populating the memories with irrelevant or hallucinated "expertise," causing a cascade of scheduling or execution failures in subsequent tasks.

- **First 3 experiments:**
  1. **Sandboxed Single-App Execution:** Run MobileSteward on 10 tasks from the SAPBench (single-app benchmark) with self-evolution **disabled**. This validates the core `Assigned Execution` loop and StaffAgent competence without the complexity of cross-app scheduling.
  2. **Self-Evolution Ablation:** Run MobileSteward on 5 cross-app tasks from CAPBench, first with empty memories and then allowing it to evolve over repeated trials. Plot the success rate to verify the contribution of the `Memory-based Self-evolution`.
  3. **Dynamic Recruitment Trace:** For a complex 3-app instruction, log the output of the StewardAgent's `Schedule` function. Manually verify if the generated DAG correctly captures the logical dependencies (e.g., "Find date" -> "Create event"). This isolates the performance of the `Dynamic Recruitment` module.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MobileSteward framework maintain high scheduling accuracy when deployed on smaller, on-device open-source models to ensure user privacy?
- **Basis in paper:** [inferred] Table 5 reveals a significant performance gap in Dynamic Recruitment; while GPT-4o achieves 0.72 success on 2-app tasks, open-source models like Qwen-VL (9.6B) drop to 0.28.
- **Why unresolved:** The paper demonstrates the feasibility of the architecture but highlights a strong dependency on the reasoning capabilities of large proprietary models.
- **What evidence would resolve it:** Successful implementation of the StewardAgent logic on models under 10B parameters with comparable scheduling graph accuracy.

### Open Question 2
- **Question:** How does the retrieval efficiency and relevance of the self-evolving memory degrade as the Task Guideline Memory accumulates thousands of successful execution trajectories?
- **Basis in paper:** [inferred] Section 3.7 describes a Memory-based Self-evolution mechanism that continuously updates without pruning, while Section 3.5 notes the use of BM25 for retrieval.
- **Why unresolved:** The study evaluates performance on a finite benchmark (CAPBench), leaving the long-term scalability and noise sensitivity of the memory retrieval system unexplored.
- **What evidence would resolve it:** Analysis of retrieval latency and success rates on a longitudinal dataset where the memory size exceeds the initial benchmark scale by an order of magnitude.

### Open Question 3
- **Question:** How can the Dynamic Recruitment module be made more robust to ambiguous natural language instructions that do not map cleanly to a specific app's expertise?
- **Basis in paper:** [inferred] Section 4.5.6 reports a decline in App Rate during online user experiments, explicitly attributing it to "ambiguity in user instructions."
- **Why unresolved:** The current framework assumes a relatively clear mapping from instruction to app-specific tasks, a condition that falters with real-world user inputs.
- **What evidence would resolve it:** Integration of a clarification dialogue loop or improved intent disambiguation that maintains App Rate parity between offline benchmarks and online user studies.

## Limitations

- **Reproducibility Barrier:** The lack of a publicly available CAPBench dataset, exact prompt templates, and the specific list of 14 target apps creates significant barriers to faithful replication.
- **Long-Term Memory Scalability:** The paper does not address how the self-evolving memory system will perform as it accumulates a large number of task guidelines, leaving open questions about retrieval efficiency and potential memory corruption over time.
- **Open-Source Model Performance Gap:** A significant performance gap exists between proprietary models like GPT-4o and open-source alternatives for the critical Dynamic Recruitment module, raising concerns about deployment costs and privacy.

## Confidence

- **High Confidence:** The mechanism of using a centralized StewardAgent with app-specific StaffAgents for cross-app task decomposition and execution is well-supported by the experimental results and aligns with established multi-agent coordination principles.
- **Medium Confidence:** The self-evolution mechanism's effectiveness is supported by the ablation study, but the lack of long-term performance tracking and explicit measures to prevent memory corruption limits definitive conclusions about its reliability.
- **Low Confidence:** Claims about the superiority of the "object-oriented" (one agent per app) approach over "procedure-oriented" (one agent per pipeline stage) are stated but not rigorously tested against a procedure-oriented baseline within the paper's experiments.

## Next Checks

1. **Cold Start and Memory Degradation Study:** Implement a version of MobileSteward with empty memories and run it on a progressively expanding set of CAPBench tasks over 50 trials. Track not just success rate but also the quality of the extracted Task Guidelines over time to detect signs of memory corruption or catastrophic forgetting.
2. **Error Propagation Analysis:** For a set of failed cross-app instructions, log the StewardAgent's decision at each DAG node. Analyze whether failures stem from incorrect initial decomposition (StewardAgent error) or from StaffAgent execution errors that cascade. This isolates whether the bottleneck is in planning or execution.
3. **Semantic Retrieval vs. BM25 Comparison:** Replace the BM25-based Task Guideline retrieval with a semantic embedding-based retrieval system (e.g., using sentence transformers). Run both systems on the same set of tasks and compare the success rates and step efficiency to determine if semantic search provides a meaningful performance improvement for this application.