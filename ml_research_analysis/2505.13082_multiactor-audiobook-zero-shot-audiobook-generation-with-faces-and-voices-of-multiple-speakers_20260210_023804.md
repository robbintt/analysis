---
ver: rpa2
title: 'MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices
  of Multiple Speakers'
arxiv_id: '2505.13082'
source_url: https://arxiv.org/abs/2505.13082
tags:
- each
- speaker
- generation
- story
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MultiActor-Audiobook is a zero-shot system for generating emotionally
  expressive audiobooks without requiring manual annotation or costly model training.
  It introduces two key processes: Multimodal Speaker Persona Generation (MSP) and
  LLM-based Script Instruction Generation (LSI).'
---

# MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers

## Quick Facts
- arXiv ID: 2505.13082
- Source URL: https://arxiv.org/abs/2505.13082
- Reference count: 0
- Zero-shot system for audiobook generation using AI-generated faces and voices of multiple speakers without manual annotation or costly model training.

## Executive Summary
MultiActor-Audiobook is a zero-shot system for generating emotionally expressive audiobooks without requiring manual annotation or costly model training. It introduces two key processes: Multimodal Speaker Persona Generation (MSP) and LLM-based Script Instruction Generation (LSI). MSP extracts speaker identities from text, generates AI faces, and synthesizes matching voices to ensure speaker consistency. LSI uses LLM to annotate emotional and prosodic instructions for each sentence based on context and speaker personas. Evaluations against commercial baselines show competitive performance: human MOS scores for character consistency reach 2.9/3.9 (ours vs 4.2/3.7 for ElevenLabs), while MLLM evaluations show an average 0.225-point improvement over baselines. Quantitative analysis reveals highest speaker embedding similarity (51.334) and pitch turning points (146,885.1), indicating strong voice consistency and emotional expressiveness.

## Method Summary
The system uses GPT-4o to extract speaker identities and generate portrait captions, then creates photorealistic faces using Stable Diffusion. These faces and captions are converted to voice samples via FleSpeech's Masked Generation. For each sentence, GPT-4o identifies the speaker and generates emotional instructions based on full story context. The final audio is synthesized using FleSpeech's unified multimodal prompt encoder, which processes text, audio, and visual inputs to produce speaker-consistent, emotionally expressive speech. No model training is required—all components operate in zero-shot mode.

## Key Results
- Human MOS scores for character consistency: 2.9/3.9 (MultiActor-Audiobook vs 4.2/3.7 for ElevenLabs)
- MLLM evaluation shows 0.225-point improvement over baselines
- Highest speaker embedding similarity (51.334) among all methods
- Highest pitch turning points (146,885.1), indicating strong emotional fluency

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Identity Anchoring (Face→Voice Consistency)
Generating AI faces from character descriptions, then mapping them to voice samples via a pretrained face-to-voice model creates stable acoustic "anchors" that persist across all of a character's dialogue lines. The MSP pipeline extracts character traits via LLM, synthesizes a photorealistic face via Stable Diffusion, then passes face+caption to FleSpeech's Masked Generation to produce a short voice sample. This sample conditions the TTS decoder for all subsequent lines by that speaker. If generated faces diverge from FleSpeech's training distribution (e.g., non-photorealistic or highly stylized), the voice conditioning may produce inconsistent or artifacts-heavy audio.

### Mechanism 2: Context-Conditioned Prosodic Instruction Synthesis
Providing an LLM with full story context, the target sentence, and speaker persona enables it to synthesize emotionally and situationally appropriate prosodic instructions (tone, pitch, pacing) that a prompt-TTS model can render. The LSI pipeline uses GPT-4o to generate a single concise instruction per sentence, conditioned on prior emotional state to avoid abrupt shifts. These instructions are fed as text prompts to FleSpeech alongside the target text. If narrative context is ambiguous or extremely long, the LLM may issue generic or contradictory instructions, producing flat prosody.

### Mechanism 3: Unified Multimodal Prompt Encoding for TTS
A single encoder that jointly processes text, audio, and visual inputs via query-based MLPs and diffusion can synthesize speech that simultaneously respects speaker identity, emotional instruction, and lexical content. FleSpeech's unified multimodal prompt encoder maps face images, face captions, audio samples, text descriptions, and target text into a shared conditioning space. Fixed per-character multimodal anchors are reused across sentences; only target text and text description change. If any modality is noisy or misaligned (e.g., face caption contradicts face image, or audio sample is too short), the encoder may produce confused conditioning, degrading either identity or expressiveness.

## Foundational Learning

- **Zero-Shot Prompt-TTS**: Why needed: MultiActor-Audiobook relies on prompt-TTS (FleSpeech) to render speech from textual instructions and audio/visual references without fine-tuning. Quick check: Can you explain how a prompt-TTS model differs from a conventional TTS model in terms of conditioning inputs?

- **Cross-Modal Representation Learning**: Why needed: The face-to-voice mapping assumes a learned correspondence between visual and acoustic features; understanding multimodal encoders is critical to debug failures. Quick check: What is the role of a unified encoder in aligning face, voice, and text representations?

- **LLM Prompt Engineering for Structured Output**: Why needed: MSP and LSI both require carefully designed prompts to extract structured attributes (speaker lists, captions) and prosodic instructions. Quick check: How would you constrain an LLM to output a concise prosodic instruction while avoiding generic descriptions like "read naturally"?

## Architecture Onboarding

- **Component map**: LLM (GPT-4o) extracts speakers and generates captions → Stable Diffusion generates photorealistic faces → FleSpeech Masked Generation synthesizes voice samples → LLM assigns speakers and generates instructions → FleSpeech unified encoder synthesizes final audio.

- **Critical path**: 1) Input full story → LLM extracts speaker list + per-speaker captions. 2) Each caption → Stable Diffusion → face image (filter non-faces). 3) Each (face, caption) → FleSpeech Masked Generation → audio sample (persona anchor). 4) For each sentence: LLM identifies speaker ID + generates prosodic instruction. 5) For each sentence: FleSpeech takes (face, caption, audio sample, instruction, target text) → synthesized audio. 6) Concatenate sentence-level audio → final audiobook.

- **Design tradeoffs**: Photorealistic vs. stylized faces (FleSpeech trained on TED-like faces; stylized or cartoon faces may degrade voice quality); zero-shot vs. fine-tuned TTS (zero-shot enables deployment without data, but raw audio quality lags behind trained commercial systems); full-context LLM inference vs. sliding window (full context improves emotional coherence but increases cost).

- **Failure signatures**: Speaker drift (if MSP fails or face/audio anchors are weak, speaker embedding similarity drops; check audio sample quality and face-image realism); flat prosody (if LSI prompts are too generic or context is truncated, MOS-E degrades; inspect generated instructions for vagueness); voice artifacts (if AI-generated faces deviate from FleSpeech's training distribution, outputs may sound robotic or mismatched).

- **First 3 experiments**: 1) Ablate MSP: Run w/o MSP (mask face, caption, audio sample) on a single story; compare speaker embedding similarity and MOS-S to full system. 2) Ablate LSI: Run w/o LSI (remove prosodic instructions) on the same story; compare pitch turning points and MOS-E. 3) Face-realism sensitivity test: Replace photorealistic faces with stylized or noisy variants; measure speaker similarity and MOS-Q.

## Open Questions the Paper Calls Out

- **Fine-tuning for AI-generated faces**: Can fine-tuning the backbone TTS model (FleSpeech) on AI-generated faces rather than TED lecturer faces significantly improve voice quality and speaker consistency? The authors identify domain mismatch as a key bottleneck but do not test alternative training data.

- **Human vs. MLLM evaluation discrepancy**: Why do human and MLLM evaluators diverge in their rankings (humans prefer ElevenLabs; MLLM prefers MultiActor-Audiobook), and which evaluation better predicts real user satisfaction? The paper reports both evaluations without investigating the causes or implications of the disagreement.

- **Scalability to longer narratives**: How does performance scale with longer narratives containing more speakers, given the computational costs and the small evaluation sample (12 stories)? Limited evaluation scale leaves unclear whether the approach generalizes to full-length novels with 10+ speakers.

## Limitations

- The zero-shot face-to-voice mapping via FleSpeech's Masked Generation lacks explicit validation for AI-generated faces outside its training distribution, risking voice quality degradation for non-photorealistic characters.
- LLM-based prosodic instructions are generated without acoustic supervision, relying entirely on the LLM's ability to infer emotional prosody from narrative context, which may produce inconsistent results for ambiguous or highly complex stories.
- Speaker embedding similarity (51.334) and pitch turning points (146,885.1) are presented as benchmarks but lack confidence intervals or statistical significance tests, making it unclear whether observed differences from baselines are meaningful.

## Confidence

- **High Confidence**: The core architecture (MSP + LSI + FleSpeech) is technically sound and the evaluation metrics (MOS, MLLM, speaker embedding similarity) are standard and appropriate.
- **Medium Confidence**: The face-to-voice conditioning mechanism works within FleSpeech's training distribution but may degrade for stylized faces; the LLM's ability to generate consistent emotional instructions is plausible but untested for edge cases.
- **Low Confidence**: The quantitative improvements over commercial baselines (e.g., 0.225-point MLLM gain) are modest and lack statistical validation, making it unclear if the system meaningfully outperforms established systems.

## Next Checks

1. **Ablate MSP**: Run w/o MSP (mask face, caption, audio sample) on a single story; compare speaker embedding similarity and MOS-S to full system to quantify identity anchoring contribution.
2. **Ablate LSI**: Run w/o LSI (remove prosodic instructions) on the same story; compare pitch turning points and MOS-E to assess expressiveness impact.
3. **Face-realism sensitivity test**: Replace photorealistic faces with stylized or noisy variants; measure speaker similarity and MOS-Q to characterize the break condition for face-to-voice mapping.