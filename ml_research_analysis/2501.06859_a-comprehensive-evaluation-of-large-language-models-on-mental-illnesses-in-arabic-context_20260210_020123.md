---
ver: rpa2
title: A Comprehensive Evaluation of Large Language Models on Mental Illnesses in
  Arabic Context
arxiv_id: '2501.06859'
source_url: https://arxiv.org/abs/2501.06859
tags:
- arabic
- datasets
- performance
- dataset
- depression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluated 8 large language models on
  Arabic mental health datasets, examining prompt design, language configuration,
  and few-shot prompting effects. Results showed that prompt engineering significantly
  influenced performance, with structured prompts outperforming less structured ones
  by 14.5% on multi-class datasets.
---

# A Comprehensive Evaluation of Large Language Models on Mental Illnesses in Arabic Context

## Quick Facts
- arXiv ID: 2501.06859
- Source URL: https://arxiv.org/abs/2501.06859
- Reference count: 40
- Primary result: Large language models show significant performance variation on Arabic mental health datasets based on prompt design, language configuration, and few-shot learning

## Executive Summary
This study comprehensively evaluates eight large language models (LLMs) on Arabic mental health datasets, focusing on prompt engineering, language configuration, and few-shot learning effects. The research systematically examines how different prompt structures, language settings (Arabic vs English), and the inclusion of few-shot examples impact model performance across various mental health classification tasks. Results demonstrate that prompt optimization significantly influences performance, with structured prompts outperforming less structured ones by 14.5% on multi-class datasets.

The study identifies Phi-3.5 MoE as the top performer for balanced accuracy in binary classification tasks, while Mistral NeMo excels in mean absolute error for severity prediction. Few-shot prompting consistently improves performance, with GPT-4o Mini achieving a 1.58x accuracy boost on multi-class classification. Interestingly, while English configuration outperformed Arabic by 6% on average, translating Arabic datasets to English while maintaining Arabic prompts showed Arabic could perform better (17.3% difference on binary tasks). These findings highlight the importance of prompt optimization, multilingual analysis, and few-shot learning for developing culturally sensitive LLM-based mental health tools for Arabic-speaking populations.

## Method Summary
The study evaluates eight large language models (LLaMA 3 70B, Mistral Large, Mistral NeMo, Qwen 2.5 72B, Mixtral 8x7B, GPT-4o Mini, GPT-4o, and Phi-3.5 MoE) on three Arabic mental health datasets covering depression, anxiety, and bipolar disorder. The evaluation examines three key factors: prompt engineering (comparing unstructured, single-aspect, and multi-aspect structured prompts), language configuration (Arabic vs English model settings), and few-shot prompting (with 0, 1, and 3 examples). Performance metrics include accuracy, balanced accuracy, F1-score, and mean absolute error across binary classification, multi-class classification, and severity prediction tasks.

## Key Results
- Structured prompts outperformed less structured ones by 14.5% on multi-class datasets
- Phi-3.5 MoE excelled in balanced accuracy (particularly for binary classification)
- Mistral NeMo showed superior performance in mean absolute error for severity prediction tasks
- Few-shot prompting consistently improved performance, with GPT-4o Mini achieving a 1.58x accuracy boost on multi-class classification
- English configuration outperformed Arabic by 6% on average, though Arabic could perform better with translated datasets (17.3% difference on binary tasks)

## Why This Works (Mechanism)
The performance differences stem from how LLMs process structured instructions versus free-form text. Structured prompts provide clear task boundaries and expected output formats, reducing ambiguity in model interpretation. Language configuration affects tokenization and embedding representations, with English settings potentially benefiting from better pre-training coverage. Few-shot learning helps models understand task-specific patterns without requiring full fine-tuning, leveraging their existing knowledge to adapt to new contexts.

## Foundational Learning

**Mental Health Classification**: Why needed - to evaluate model performance on clinically relevant tasks; Quick check - verify dataset annotations align with clinical diagnostic criteria

**Prompt Engineering**: Why needed - to optimize model performance through instruction design; Quick check - compare model outputs across different prompt structures

**Few-shot Learning**: Why needed - to demonstrate model adaptation without fine-tuning; Quick check - measure performance improvement with increasing number of examples

**Language Configuration**: Why needed - to understand multilingual model behavior; Quick check - compare performance across different language settings on identical tasks

## Architecture Onboarding

**Component Map**: Arabic Mental Health Datasets -> Prompt Engineering -> Language Configuration -> Model Inference -> Performance Metrics

**Critical Path**: Dataset → Prompt Generation → Model Configuration (Language Setting) → Inference → Metric Calculation → Analysis

**Design Tradeoffs**: The study balances between comprehensive evaluation (8 models, 3 datasets, multiple configurations) and practical constraints, potentially limiting exploration of the full prompt space or additional language settings.

**Failure Signatures**: Poor performance may result from inadequate prompt structure, mismatched language configuration, insufficient few-shot examples, or dataset limitations in representing diverse mental health expressions.

**First Experiments**:
1. Replicate baseline results using unstructured prompts with Arabic language configuration
2. Test single-aspect structured prompts with English configuration
3. Evaluate few-shot prompting impact using 3 examples with top-performing models

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted on specific Arabic mental health datasets that may not represent full diversity of mental health expressions
- Limited to Reddit-derived data, potentially missing clinical or conversational contexts
- Prompt engineering approaches limited to specific structures without exploring full space of variations
- Language configuration experiments compared only English and Arabic settings without examining code-switching scenarios

## Confidence

**High Confidence**: Prompt engineering significantly affects model performance (14.5% improvement on multi-class datasets)

**Medium Confidence**: Relative performance rankings between models are context-dependent and may vary with different dataset compositions

**Medium Confidence**: English configuration outperforming Arabic by 6% on average requires further validation due to complex interplay between language settings and dataset characteristics

## Next Checks

1. **Cross-dataset validation**: Test the same prompt engineering approaches on additional Arabic mental health datasets from different sources (clinical records, social media platforms beyond Reddit, or conversational data)

2. **Extended language configuration study**: Systematically evaluate performance across multiple language settings (Arabic, English, code-switched) on the same dataset to isolate language effects from dataset translation artifacts

3. **Prompt space exploration**: Conduct comprehensive analysis of prompt engineering by testing broader range of prompt structures and using automated prompt optimization techniques to determine if identified "best" prompts are truly optimal