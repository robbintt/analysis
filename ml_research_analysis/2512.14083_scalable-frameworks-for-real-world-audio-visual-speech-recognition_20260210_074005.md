---
ver: rpa2
title: Scalable Frameworks for Real-World Audio-Visual Speech Recognition
arxiv_id: '2512.14083'
source_url: https://arxiv.org/abs/2512.14083
tags:
- speech
- audio
- audio-visual
- recognition
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This dissertation proposes scalable frameworks for real-world audio-visual
  speech recognition (AVSR), addressing the challenge of significant performance degradation
  in noisy and visually interfered environments. The research introduces a systematic,
  hierarchical approach to overcome these limitations at the representation, architecture,
  and system levels.
---

# Scalable Frameworks for Real-World Audio-Visual Speech Recognition

## Quick Facts
- **arXiv ID:** 2512.14083
- **Source URL:** https://arxiv.org/abs/2512.14083
- **Reference count:** 0
- **Primary result:** Proposes scalable frameworks for AVSR addressing noise and visual interference at representation, architecture, and system levels.

## Executive Summary
This dissertation presents a comprehensive, hierarchical approach to robust audio-visual speech recognition (AVSR) for real-world deployment. The research introduces three interconnected frameworks: CAV2vec for robust self-supervised audio-visual representation learning, MoHAVE for scalable MoE-based architecture, and DualHyp for LLM-powered compositional error correction. These contributions collectively address the significant performance degradation AVSR systems face in noisy and visually interfered environments.

The work systematically tackles scalability and robustness challenges by learning noise-resistant representations without specialized noise modules, dynamically allocating computational resources through hierarchical expert routing, and leveraging LLM reasoning capabilities for final recognition accuracy. The frameworks are evaluated on standard benchmarks (LRS2/LRS3) with synthetic noise and occlusion, demonstrating substantial improvements in word error rate compared to state-of-the-art baselines.

## Method Summary
The dissertation proposes a systematic, hierarchical approach to robust AVSR through three interconnected frameworks. At the representation level, CAV2vec employs corrupted prediction tasks within a self-distillation framework to learn robust audio-visual features inherently resistant to real-world corruptions. For architecture-level scalability, MoHAVE utilizes a sparse Mixture-of-Experts architecture with hierarchical gating that dynamically allocates computational resources based on input characteristics. At the system level, DualHyp uses dual modality-specific hypotheses from separate ASR and VSR models, presented to an LLM for intelligent composition through temporal reliability masks.

## Key Results
- CAV2vec learns robust audio-visual features through corrupted prediction tasks in self-distillation, achieving superior performance under noise and visual occlusion without specialized noise modules.
- MoHAVE achieves efficient scaling through hierarchical MoE routing with inter-modal and intra-modal expert groups, guided by load biasing loss for optimal resource allocation.
- DualHyp framework achieves state-of-the-art robustness by leveraging LLM reasoning capabilities to compose separate ASR and VSR hypotheses with temporal reliability masks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Corrupted prediction tasks in a self-distillation framework can learn audio-visual representations that generalize to unseen corruptions.**
- Mechanism: The student model is trained to predict clean representation targets (generated by an EMA-updated teacher) from explicitly corrupted audio or visual inputs. A unimodal multi-task strategy (e.g., predicting clean audio targets from corrupted video) forces the model to learn cross-modal alignment, reducing the dispersion in the representation space caused by noise.
- Core assumption: The model can learn a robust, disentangled latent space by reconstructing clean signals from corrupted ones, and this learned robustness transfers to downstream tasks without specialized noise modules.
- Evidence anchors:
  - [abstract] "CAV2vec employs corrupted prediction tasks within a self-distillation framework, learning robust audio-visual features inherently resistant to diverse real-world corruptions."
  - [section] Section 3.4.2 defines the unimodal corrupted prediction (ACP/VCP) tasks: "predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios."
  - [corpus] Related work *Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation* (corpus paper 4) details this exact method, providing supporting evidence.
- Break condition: Performance degrades significantly when evaluated on corruption types whose statistical characteristics (e.g., spatial patterns in visual occlusion) fall completely outside the distribution of the synthetic corruptions used during pre-training.

### Mechanism 2
- Claim: **A hierarchical Mixture-of-Experts (MoE) architecture with modality-specific expert groups enables efficient scaling of model capacity while dynamically adapting to input characteristics.**
- Mechanism: An inter-modal router assigns weights to audio and visual expert groups. Within each group, an intra-modal router selects top-k experts. This hierarchical gating is guided by a load biasing loss, which encourages the model to route audio-only inputs to audio experts and video-only inputs to visual experts. The overall architecture scales capacity by increasing experts while maintaining sparse activation.
- Core assumption: Modality-specific expert specialization, combined with learned dynamic routing, leads to more effective and efficient processing of multimodal inputs than dense models or uniform MoE routing.
- Evidence anchors:
  - [abstract] "MoHAVE utilizes a sparse Mixture-of-Experts (MoE) architecture with a hierarchical gating mechanism, dynamically allocating computational resources based on input characteristics."
  - [section] Section 4.4 describes the hierarchical gating and Equation 4.14 details the load biasing loss \( L_S \) that guides group specialization.
  - [corpus] *MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition* (corpus paper 1) is this work, providing direct evidence. Related work *Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion* (corpus paper 6) supports the value of dynamic gating for robustness.
- Break condition: The inter-modal router fails to learn meaningful specialization, leading to under- or over-utilization of expert groups (e.g., always routing to the audio group even when audio is severely corrupted), or the load balancing loss dominates and prevents adaptive routing.

### Mechanism 3
- Claim: **Providing a Large Language Model (LLM) with separate N-best hypothesis lists from ASR and VSR models, augmented with temporal reliability masks, enables superior generative error correction.**
- Mechanism: The DualHyp framework delays modality fusion to the language space. Independent ASR and VSR models generate hypothesis sets. An LLM acts as a compositional reasoner to merge these texts. The RelPrompt module provides time-aligned "Clean/Noisy/Mixed" reliability tokens for each modality, grounding the LLM's correction process in signal quality.
- Core assumption: LLMs possess sufficient compositional reasoning and world knowledge to effectively arbitrate between and correct separate, potentially conflicting textual hypotheses, and explicit reliability signals improve this process.
- Evidence anchors:
  - [abstract] "The DualHyp framework uses dual modality-specific hypotheses...presented to an LLM for intelligent composition."
  - [section] Section 5.4 describes RelPrompt: providing reliability token sequences \( m_a, m_v \) to guide the LLM (Eq. 5.3).
  - [corpus] Evidence for the LLM-based error correction paradigm is supported by the general trend in the field, though corpus evidence for this specific dual-hypothesis method with reliability prompting is weak or missing among the provided neighbors.
- Break condition: The quality of the upstream hypotheses (especially from the weaker VSR modality) is too poor to provide useful signal, causing the LLM to "hallucinate" or be misled by plausible but incorrect hypotheses. This is noted as a limitation in Section 5.7.

## Foundational Learning

- **Concept:** **Self-Supervised Learning (SSL) with Masked Prediction**
  - Why needed here: CAV2vec builds upon the SSL paradigm (like HuBERT/data2vec) where models learn representations by predicting masked parts of the input. Understanding this pretext task is essential to grasp how corrupted prediction extends it.
  - Quick check question: How does predicting a masked frame from its context help a model learn useful features without labels?

- **Concept:** **Mixture-of-Experts (MoE) and Sparsity**
  - Why needed here: MoHAVE is an MoE model. You must understand how sparse gating allows a model to have a large parameter count (total experts) while only activating a small subset (low compute) per input token.
  - Quick check question: In a sparsely-gated MoE layer, what is computed to decide which expert(s) process a given token, and how does this affect FLOPs during inference?

- **Concept:** **Generative Error Correction (GER) with LLMs**
  - Why needed here: DualHyp applies GER to AVSR. This requires understanding how an LLM can be prompted with an ASR's N-best list and instructed to generate a corrected transcription.
  - Quick check question: In a standard GER pipeline, what is the input to the LLM and what is its expected output? What information might be lost by not providing acoustic features directly?

## Architecture Onboarding

- **Component map:**
  - **Representation (CAV2vec):** Audio/Visual Feature Extractors → [Corruption Augmentation] → Multimodal Transformer Encoder (Student) || EMA Teacher → Pretraining Losses (Masked + Corrupted Prediction + MLM).
  - **Architecture (MoHAVE):** Pretrained Encoder (e.g., from CAV2vec/AV-HuBERT) → [Multimodal Features] → MoE-based Decoder with: Inter-modal Router → (Audio Expert Group | Visual Expert Group) → Intra-modal Routers → Sparse Expert FFNs.
  - **System (DualHyp):** Separate ASR Model + Separate VSR Model → (Beam Search) → ASR N-best + VSR N-best. Reliability Predictors → (Audio/Video Reliability Masks). All inputs → LLM (fine-tuned with LoRA) → Final Transcript.

- **Critical path:**
  1.  **Pretraining:** Train CAV2vec encoder on large unlabeled audio-visual data with corrupted prediction tasks.
  2.  **AVSR Fine-tuning:** Initialize the encoder from CAV2vec. Replace the decoder with the MoHAVE MoE structure. Fine-tune on labeled audio-visual speech data with noise augmentation. Apply load biasing loss to train hierarchical gating.
  3.  **Error Correction Setup:** Use the fine-tuned MoHAVE model (or separate SOTA ASR/VSR models) as frozen "hypothesis generators". Train lightweight reliability predictors. Fine-tune an LLM (e.g., TinyLlama, Llama-3.2) on the DualHyp dataset (ASR+VSR hypotheses + ground truth) with LoRA.

- **Design tradeoffs:**
  - **CAV2vec:** Robustness vs. clean-performance. High corruption ratios during pretraining improve noise robustness but may slightly reduce accuracy in clean conditions (Section 3.6.3).
  - **MoHAVE:** Capacity vs. Complexity. Scaling to more experts increases model capacity and potential robustness but complicates training (routing instability) and may not yield gains without proper load balancing and biasing.
  - **DualHyp:** Accuracy vs. Latency & Complexity. Achieves state-of-the-art robustness by leveraging powerful LLMs, but introduces significant latency (parallel ASR/VSR + sequential LLM step) and architectural complexity compared to a single end-to-end AVSR model.

- **Failure signatures:**
  - **CAV2vec:** "Over-correction" where the model generates text that is semantically plausible but not spoken (LLM hallucination), or failure on unseen corruption patterns not simulated during pre-training.
  - **MoHAVE:** "Expert collapse" where routing concentrates on a few experts, or "modality blindness" where the inter-modal router ignores visual experts even in severe audio noise (check load analysis in Fig. 4.5-4.6).
  - **DualHyp:** "Hallucination cascade" where the LLM generates content unrelated to either hypothesis set, or "over-reliance" on a persistently noisy modality despite RelPrompt signals (analyze qualitative examples in Tables 5.11-5.13).

- **First 3 experiments:**
  1.  **Validate CAV2vec Pre-training:** Re-implement the unimodal corrupted prediction task on top of a pre-trained AV-HuBERT encoder. Fine-tune on a small subset of LRS3 and evaluate on a standard noisy benchmark (e.g., MUSAN noise) to confirm the robustness gain over the baseline.
  2.  **Ablate MoHAVE Routing:** Take a small MoHAVE model (e.g., Base). Compare three variants: (a) standard dense decoder, (b) MoE with flat routing, (c) MoHAVE with hierarchical routing. Evaluate under varying SNR levels (e.g., -10dB to 10dB babble noise) to isolate the contribution of hierarchical gating and load biasing.
  3.  **DualHyp Pilot:** Use a small LLM (e.g., TinyLlama). Create a toy DualHyp dataset by generating 5-best hypotheses from a small ASR and VSR model on LRS2 with simple corruption. Train the LLM with and without RelPrompt masks. Measure WER reduction over the ASR-only baseline to verify the core compositional correction capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AVSR architectures be extended to incorporate non-verbal visual cues, such as eyebrow movements, eye gaze, and micro-expressions, to capture paralinguistic information alongside lip movements?
- Basis: [explicit] Chapter 6.3 explicitly lists "Expanding the Visual Modality beyond Lip Reading" as a future direction, stating that current models focus almost exclusively on lips and should transition toward interpreting facial dynamics for deeper context-aware understanding.
- Why unresolved: Current visual front-ends (like the modified ResNet-18 used in CAV2vec and MoHAVE) are trained to extract features specifically aligned with phonetic content from the mouth region, ignoring the rest of the face.
- What evidence would resolve it: Demonstrating that integrating facial dynamics into the visual encoder improves performance on datasets annotated with emotional states or intent, or improves robustness in conversational dialogue systems.

### Open Question 2
- Question: Can the DualHyp framework be adapted to support streaming inference and on-device deployment without the latency introduced by the sequential LLM correction step?
- Basis: [explicit] Chapter 5.7 lists computational latency and the inability to perform streaming AVSR as primary limitations of the DualHyp system, and Chapter 6.3 calls for "On-Device Deployment and Efficiency" as a major research challenge.
- Why unresolved: The current DualHyp design requires the LLM to wait for the entire N-best list of hypotheses from the ASR/VSR heads before generating a correction, creating an unavoidable bottleneck for real-time applications.
- What evidence would resolve it: The development of a chunk-based or incremental DualHyp mechanism that achieves a Real-Time Factor (RTF) < 1.0 on edge devices while maintaining the error correction benefits of the full-sequence model.

### Open Question 3
- Question: What specific linguistic or phonetic features drive the language-dependent differences in expert group allocation within the MoHAVE architecture?
- Basis: [inferred] Section 4.6.3 observes that Arabic tokens route more frequently to visual experts while Spanish tokens rely more on audio experts, but notes that the precise relation to linguistic characteristics remains a subject for future work.
- Why unresolved: The hierarchical gating mechanism learns to distribute load based on input characteristics, but the correlation between these distribution patterns and specific linguistic structures (e.g., viseme distinctness vs. acoustic density) is not yet mapped.
- What evidence would resolve it: A detailed analysis correlating specific phoneme classes or viseme-to-phoneme mappings with the activation frequencies of audio versus visual expert groups across a diverse multilingual corpus.

## Limitations
- CAV2vec may exhibit "over-correction" generating semantically plausible but incorrect text, and performance degrades on corruption types outside training distribution.
- MoHAVE's hierarchical routing complexity may lead to expert collapse or modality blindness if load biasing loss is not properly balanced.
- DualHyp introduces significant latency from parallel ASR/VSR processing and sequential LLM correction, and is vulnerable to hallucination when VSR hypotheses are unreliable.

## Confidence
- **CAV2vec:** High confidence - well-grounded in established SSL paradigms with corrupted prediction tasks validated in related work.
- **MoHAVE:** Medium confidence - conceptually sound with promising results, but certain implementation details (inter-modal router configuration) are underspecified.
- **DualHyp:** Low to Medium confidence - novel integration shows potential but effectiveness heavily depends on upstream hypothesis quality and faces LLM hallucination risk.

## Next Checks
1. Implement a minimal CAV2vec pretraining pipeline with corrupted prediction on AV-HuBERT and validate robustness gains on a noisy benchmark.
2. Conduct an ablation study comparing MoHAVE's hierarchical routing against flat MoE and dense decoders under varying noise conditions to isolate the contribution of the gating mechanism.
3. Build a small-scale DualHyp prototype using lightweight LLM and verify the effectiveness of reliability prompts in reducing hallucination compared to baseline error correction.