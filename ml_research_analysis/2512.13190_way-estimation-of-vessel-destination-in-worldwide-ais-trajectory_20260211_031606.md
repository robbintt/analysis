---
ver: rpa2
title: 'WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory'
arxiv_id: '2512.13190'
source_url: https://arxiv.org/abs/2512.13190
tags:
- trajectory
- data
- destination
- each
- port
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of vessel destination estimation
  using global AIS trajectory data. The key challenge is to predict port destinations
  days to weeks in advance despite irregular data intervals and reliability issues.
---

# WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory

## Quick Facts
- arXiv ID: 2512.13190
- Source URL: https://arxiv.org/abs/2512.13190
- Reference count: 40
- Primary result: 80.44% accuracy in vessel destination estimation

## Executive Summary
This paper addresses the challenge of predicting vessel destinations days to weeks in advance using global AIS trajectory data, despite irregular transmission intervals and reliability issues. The authors propose WAY, a novel approach that restructures long port-to-port trajectories into nested spatial grid sequences to mitigate spatio-temporal bias while preserving local navigational details. WAY employs a multi-channel deep learning architecture with spatial encoding, channel attention, and self-attention mechanisms, along with a specialized Gradient Dropout technique to handle the many-to-many training setup with single destination labels. Experiments on 5 years of AIS data demonstrate significant improvements over existing methods, achieving 80.44% accuracy in destination estimation and strong performance in ETA estimation when extended to multitask learning.

## Method Summary
WAY processes AIS trajectories by first segmenting them into uniform 1×1° spatial grid units, preserving local navigational patterns while decoupling progression from irregular time intervals. The model uses a 4-channel representation: spatial encoding via sinusoidal functions for spherical coordinates, local patterns via GRU-processed subsequences, departure port embedding, and ship type embedding. These channels are aggregated using multi-head channel attention, then processed through stacked CASP blocks containing masked self-attention for sequential processing. A specialized Gradient Dropout technique balances gradient contributions across variable-length trajectories during training. The model is trained with cross-entropy loss and evaluated on classification accuracy at different trajectory quartiles.

## Key Results
- WAY achieves 80.44% accuracy in vessel destination estimation, outperforming LSTM (77.28%), GRU (75.52%), and transformer baselines (76.43%)
- Multi-head channel attention achieves 79.45% accuracy, outperforming concatenation (73.59%) and cross-attention (75.61%)
- Gradient Dropout improves all baseline models by an average of 1.01% accuracy
- The model demonstrates strong ETA estimation performance when extended to multitask learning (MAE of 2.90 days)

## Why This Works (Mechanism)

### Mechanism 1: Nested Spatial Grid Representation
Restructuring message-level AIS sequences into spatial grid units with preserved local details mitigates spatio-temporal bias from irregular transmission intervals. By segmenting trajectories into uniform spatial grid units G_k = {x_i,...,x_i+j; g_k}, the model decouples progression from time-density, better capturing vessel intent through spatial progression rather than raw temporal sequences. This approach preserves essential maneuvering information while aggregating multiple observations within the same grid.

### Mechanism 2: Multi-Channel Representation with Channel Attention Aggregation
Separating trajectory features into semantically distinct channels (spatial, local patterns, departure context, ship type) and learning adaptive channel weights improves destination inference. The 4-channel vector sequence allows the model to emphasize informative channels at different trajectory stages through multi-head channel attention with squeeze-excitation weighting. This dynamic feature importance capture proves crucial, as removing the local pattern channel drops accuracy by 14.75%.

### Mechanism 3: Gradient Dropout for Length-Balanced Many-to-Many Training
Stochastically blocking gradient contributions proportional to sequence length prevents long trajectories from dominating parameter updates. For each instance k, sampling ratio δk is computed inversely to log-scaled length, balancing feedback across the batch despite many-to-many training with single destination labels. This technique ensures that training signals from each trajectory step are approximately equally valuable, improving generalization across variable-length sequences.

## Foundational Learning

- **Concept: Spatial Encoding for Spherical Coordinates**
  - Why needed here: Standard positional encoding assumes linear sequences; maritime coordinates exist on a sphere where distance relationships are non-linear
  - Quick check question: Can you explain why standard sinusoidal positional encoding from Transformers would fail to preserve latitude-dependent distance relationships?

- **Concept: Attention Mechanisms (Self-Attention and Channel Attention)**
  - Why needed here: WAY uses self-attention for temporal progression and channel attention for feature aggregation—understanding both is essential for debugging the CASP blocks
  - Quick check question: Given Q, K, V matrices, what does the attention weight α represent, and how does masking in the decoder prevent future information leakage?

- **Concept: Many-to-Many Sequence Learning with Single Labels**
  - Why needed here: The training setup produces predictions at each grid step but has only one ground-truth destination, creating a gradient accumulation problem
  - Quick check question: If you have 100 gradient contributions from a trajectory of length 100 and 10 from length 10, why might the model overfit to longer trajectories?

## Architecture Onboarding

- **Component map:** AIS message sequence → nested grid structure with Poisson-sampled subsequences → 4 parallel paths (Spatial Encoding, Local Pattern GRU, Departure/Ship Type embeddings + Time Encoding) → CASP Blocks (MCA → MSA → Channel concatenation → SFF) → Linear classifier over 3,243 port classes

- **Critical path:** Local Pattern extraction via GRU → Time Encoding addition → MCA aggregation → MSA sequential processing → classification head. If any component fails, the multi-channel fusion breaks.

- **Design tradeoffs:**
  - Grid size: 1×1°² chosen as default; smaller preserves detail but increases sequence length
  - Hidden dimension d=128 with 4 heads (dk=64) balances capacity vs. overfitting on 130K trajectories
  - Channel replacement strategy: SE channel is repeatedly replaced across CASP layers to accumulate enriched representations while other channels maintain identity

- **Failure signatures:**
  - Accuracy plateaus at ~60%: Likely channel attention not learning meaningful weights; check MCA output distributions
  - Early-quartile accuracy low but late-quartile high: Self-attention may not be propagating early information forward; check masking implementation
  - Training loss unstable across batches: Gradient Dropout may be too aggressive; verify δk computation

- **First 3 experiments:**
  1. **Ablate each channel:** Remove Local Pattern, Departure, and Ship Type one at a time to confirm contribution levels (baseline: Table IV shows Local Pattern most critical with 14.75% drop when removed)
  2. **Test GD on baseline models:** Apply Gradient Dropout to LSTM/GRU/Transformer to verify generalization benefit (baseline: Table III shows +0.72% to +1.68% gains)
  3. **Vary grid resolution:** Compare 0.5×0.5°², 1×1°², and 2×2°² to find optimal spatial granularity before detail loss or sequence explosion

## Open Questions the Paper Calls Out

### Open Question 1
How can a robust annotation framework be designed to generate consistent arrival time labels, and to what extent does label consistency improve the accuracy of multi-task ETA estimation? The authors state that "a different annotation framework needs to be designed for consistent arrival time labels" to achieve accurate estimation within both destination and arrival time. Current arrival time labels are inconsistent because they include waiting times caused by external factors like port congestion, which degraded the ETA estimation performance (MAE of 2.90 days). Evidence would come from comparing WAY's ETA performance using current labels versus a new dataset where arrival times are normalized or annotated to exclude non-voyage waiting periods.

### Open Question 2
How can dynamic external factors, such as real-time port congestion and shipping due dates, be integrated into the WAY architecture to refine arrival time predictions? Section VI notes that "external factors like port congestion and due date of shipping result in waiting time, which leads to inconsistent arrival time," suggesting these factors are currently missing from the model. The current model relies on kinematic features and historical semantics but lacks the input mechanisms to account for queueing delays at destination ports. Evidence would come from an augmented version of WAY that processes port congestion indices as auxiliary inputs, demonstrating a statistically significant reduction in ETA Mean Absolute Error.

### Open Question 3
Is the proposed fixed trigonometric Spatial Encoding (SE) mechanism more effective for global trajectory representation than standard learnable positional embeddings? Section IV.A.1 introduces a specific mathematical formula for Spatial Encoding to represent spherical coordinates, but the paper does not provide an ablation study comparing this fixed encoding against learnable alternatives. While the fixed encoding reduces parameter count, it assumes a specific inductive bias that may not capture irregular traffic patterns as effectively as a learned representation. Evidence would come from ablation experiments comparing the convergence rate and final accuracy of the fixed SE module against a standard learnable embedding layer of equivalent dimension.

## Limitations
- The spatial grid discretization at 1×1°² represents a tradeoff between preserving local navigational detail and managing sequence length that is not thoroughly explored across different maritime environments
- The model's performance advantage over baselines is demonstrated, but the specific contribution of the nested spatial grid representation versus the multi-channel architecture is not isolated through systematic ablation
- WAY achieves strong accuracy but the relative contribution of each architectural component to this improvement is not fully quantified

## Confidence

- **High confidence:** Multi-channel representation with channel attention consistently improves performance across different sequence lengths and baselines; Gradient Dropout effectively balances gradient contributions from variable-length trajectories
- **Medium confidence:** Nested spatial grid restructuring mitigates spatio-temporal bias as claimed, though the specific grid size selection (1×1°²) is based on empirical observation rather than theoretical justification
- **Medium confidence:** WAY achieves 80.44% accuracy and outperforms baselines, but the relative contribution of each architectural component to this improvement is not fully quantified

## Next Checks

1. **Ablate Gradient Dropout** on the complete WAY model across different batch sizes and sequence length distributions to verify the claimed 1.01% average improvement is consistent and not dependent on specific training configurations

2. **Vary spatial grid resolution** systematically (0.5×0.5°², 1×1°², 2×2°²) across diverse maritime regions (coastal vs. open ocean) to quantify the tradeoff between local detail preservation and sequence length explosion, and identify optimal grid sizes for different vessel types

3. **Validate port identification pipeline** using alternative port databases (e.g., public World Port Index vs. proprietary SeaVantage) and different matching thresholds to assess the robustness of the Damerau-Levenshtein + DBSCAN port annotation process across different geographic regions and port densities