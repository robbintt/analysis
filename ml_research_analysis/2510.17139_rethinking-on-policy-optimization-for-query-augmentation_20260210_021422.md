---
ver: rpa2
title: Rethinking On-policy Optimization for Query Augmentation
arxiv_id: '2510.17139'
source_url: https://arxiv.org/abs/2510.17139
tags:
- query
- retrieval
- dense
- retriever
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically compares two paradigms for query augmentation
  in information retrieval: simple prompt-based expansion and reinforcement learning
  (RL)-based query rewriting. Through extensive experiments across evidence-seeking,
  ad hoc, and tool retrieval tasks, the authors find that training-free prompt-based
  methods often match or exceed the performance of more complex RL-based approaches,
  especially when using powerful LLMs.'
---

# Rethinking On-policy Optimization for Query Augmentation

## Quick Facts
- arXiv ID: 2510.17139
- Source URL: https://arxiv.org/abs/2510.17139
- Reference count: 22
- Primary result: On-policy pseudo-document query expansion (OPQE) consistently outperforms both prompt-based and RL-based query augmentation methods, especially for dense retrieval tasks.

## Executive Summary
This paper systematically compares prompt-based query expansion (SPQE) with reinforcement learning (RL)-based query rewriting for information retrieval. Through extensive experiments across evidence-seeking, ad hoc, and tool retrieval tasks, the authors find that training-free prompt-based methods often match or exceed the performance of more complex RL-based approaches, especially when using powerful LLMs. Motivated by this, they propose On-policy Pseudo-document Query Expansion (OPQE), a hybrid method that trains an LLM to generate pseudo-documents optimized for retrieval performance. OPQE combines the generative flexibility of prompting with the targeted optimization of RL, consistently outperforming both standalone prompting and RL-based rewriting. For example, on ad hoc retrieval with a dense retriever, OPQE achieves a state-of-the-art average NDCG@10 of 58.1, surpassing prior RL methods.

## Method Summary
The paper compares three query augmentation paradigms: prompt-based (SPQE), RL-based (query rewriting), and hybrid (OPQE). SPQE generates pseudo-documents from queries using a prompt template, concatenating the result with the original query. RL-based methods train an LLM policy via PPO to maximize retrieval rewards. OPQE combines these by prompting the LLM to generate pseudo-documents, then using on-policy RL to optimize the generation process for retrieval performance. The method uses Qwen2.5-{3B,7B}-Instruct as the base model, veRL for PPO implementation, and evaluates on multiple retrieval tasks using both sparse (BM25) and dense (E5, Contriever) retrievers.

## Key Results
- SPQE with GPT-4o-mini achieves 78.2 average on evidence-seeking (sparse), outperforming +7B RL's 75.6
- OPQE-7B achieves 58.1 average NDCG@10 on ad hoc (dense), surpassing +7B RL (57.5) and SPQE (56.6)
- RL improved sparse retrieval on tool retrieval (29.7→29.9 NDCG@10) but degraded dense retriever performance (29.9→29.0 NDCG@10)
- Pseudo-document generation transforms asymmetric retrieval into symmetric matching, better aligning with dense retriever training objectives

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-document generation transforms asymmetric retrieval into symmetric matching
Generating a hypothetical document that answers the query, then concatenating it with the original query, improves retrieval more than rewriting the query itself. The pseudo-document provides richer lexical and semantic signals. For sparse retrievers like BM25, it mitigates vocabulary mismatch by introducing relevant terms. For dense retrievers, it creates a document-like query that aligns with their training on document-document similarity tasks.

### Mechanism 2: On-policy RL with retrieval rewards enables retriever-specific adaptation
Training an LLM policy to maximize retrieval metrics (NDCG, Recall, Completeness) via PPO adapts query augmentation to the specific retriever-reward combination. The policy generates augmented queries, receives reward signals from the retriever's performance, and updates via PPO with KL regularization to maintain fluency. This directly optimizes the end metric rather than mimicking supervised rewrites.

### Mechanism 3: OPQE hybridizes pseudo-document structure with RL optimization for superior dense retrieval
Combining pseudo-document generation (via prompt template) with on-policy RL fine-tuning achieves the best overall performance, especially for dense retrievers. The policy is prompted to generate a pseudo-document, which provides a strong initialization leveraging the LLM's knowledge. RL then fine-tunes this generation process to maximize retrieval rewards, optimizing the content within the effective pseudo-document structure.

## Foundational Learning

- **Dense vs Sparse Retrieval**: Why needed here - The paper evaluates on both BM25 (sparse, term-matching) and neural retrievers like E5/Contriever (dense, embedding-based). Query augmentation strategies that help one may hurt the other. Quick check - Can you explain why adding more terms generally helps BM25 but might not help a dense retriever trained on query-document pairs?

- **On-policy RL with PPO and KL Regularization**: Why needed here - The core RL method uses PPO to optimize the retrieval reward with a KL penalty to prevent the policy from drifting too far from the reference model. Understanding this trade-off (reward vs. fluency) is critical for tuning β. Quick check - What happens to generation quality if β (KL coefficient) is set too low during PPO training?

- **Prompt-based Query Expansion (HyDE/Query2Doc family)**: Why needed here - SPQE is a simplified zero-shot version of HyDE. Understanding why generating a pseudo-document and concatenating it with the query works—rather than replacing the query—is essential for grasping OPQE's design. Quick check - Why does the paper concatenate (q, dH) rather than using dH alone as the augmented query?

## Architecture Onboarding

- **Component map**: Qwen2.5-3B/7B-Instruct policy -> PPO trainer (veRL) -> Retriever environment (BM25/E5/Contriever) -> Reward function -> Updated policy
- **Critical path**: 1) Initialize policy from instruction-tuned LLM 2) Define prompt template based on method 3) For each training query: generate augmented query/pseudo-doc, retrieve, compute reward, update policy via PPO 4) Evaluate on held-out benchmarks
- **Design tradeoffs**: SPQE (zero-shot, no training cost, requires strong LLM) vs RL (retriever-specific, expensive training) vs OPQE (hybrid, best dense performance, retains training cost)
- **Failure signatures**: RL degrades dense retriever performance (keyword-heavy outputs), reward hacking (high reward, low metric), poor generalization, training instability
- **First 3 experiments**: 1) Replicate SPQE baseline on NQ to match ~80.7 sparse / ~85.2 dense 2) Train RL query rewriting on FEVER training set, compare against SPQE on FEVER dev set 3) Compare OPQE vs RL vs SPQE on ad hoc retrieval with MS MARCO/DL19/20

## Open Questions the Paper Calls Out

### Open Question 1
Can group-based on-policy RL algorithms (e.g., GRPO) overcome the efficiency bottleneck in retrieval-based RL training while maintaining or improving query augmentation performance? Basis - "We did not include group-based on-policy RL algorithms such as GRPO... We leave the investigation of other on-policy algorithms to future work."

### Open Question 2
Can more nuanced reward function designs improve training stability and yield stronger query augmentation performance beyond simple retrieval metrics? Basis - "refining RL task formulations, for example, by designing more nuanced reward functions, may help stabilize training and yield stronger performance."

### Open Question 3
Do the relative strengths of prompting-based vs. RL-based query augmentation generalize to larger models (>7B parameters) and different on-policy algorithms? Basis - "Our observations and conclusions may be subject to change with more performant base models and different on-policy algorithms."

### Open Question 4
How do prompting-based and RL-based query augmentation methods perform in multi-hop or deep research retrieval scenarios beyond single-round retrieval? Basis - "exploring query augmentation prompt design and RL training beyond the classic single-round retrieval paradigm, such as in multi-hop or deep research settings, remains an underexplored but important direction."

## Limitations
- RL optimization is sensitive to reward design and hyperparameters, with clear evidence of degradation on dense retrievers for tool retrieval tasks
- Computational expense of RL training (8-16 A100 GPUs) and requirement for relevance labels create practical barriers to adoption
- Paper did not explore larger model scales beyond 7B due to compute constraints, leaving scalability questions open

## Confidence
- **High confidence**: SPQE effectiveness as a zero-shot baseline (consistent performance across multiple datasets and retrievers)
- **Medium confidence**: OPQE superiority for dense retrieval (strong results on ad hoc but limited testing on other dense-heavy tasks)
- **Low confidence**: RL-based query rewriting as a standalone approach (clear evidence of failure modes on dense retrievers, particularly in tool retrieval)

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary the KL coefficient β in OPQE training across a range (e.g., 0.01, 0.1, 1.0) and measure the trade-off between reward optimization and retrieval performance on both sparse and dense retrievers to identify stable operating points.

2. **Cross-retriever generalization test**: Train OPQE policies on one retriever (e.g., BM25) and evaluate zero-shot on a different retriever type (e.g., dense) to quantify the degree of retriever-specific adaptation and inform whether retriever-agnostic training is feasible.

3. **Scaling study**: Evaluate OPQE with incrementally larger models (e.g., Qwen2.5-14B, 32B) on a representative subset of tasks to determine if performance gains continue beyond 7B and establish the practical limits of the approach.