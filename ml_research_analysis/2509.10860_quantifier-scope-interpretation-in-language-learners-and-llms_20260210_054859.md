---
ver: rpa2
title: Quantifier Scope Interpretation in Language Learners and LLMs
arxiv_id: '2509.10860'
source_url: https://arxiv.org/abs/2509.10860
tags:
- chinese
- llms
- scope
- english
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) interpret
  quantifier scope in English and Chinese. Using a truth-value judgment task, it examines
  whether LLMs prefer surface scope (SS) or inverse scope (IS) readings for doubly
  quantified sentences.
---

# Quantifier Scope Interpretation in Language Learners and LLMs

## Quick Facts
- arXiv ID: 2509.10860
- Source URL: https://arxiv.org/abs/2509.10860
- Reference count: 17
- Primary result: Most LLMs prefer surface scope readings like humans, with variability across model type, scale, and linguistic context.

## Executive Summary
This study investigates how large language models (LLMs) interpret quantifier scope in English and Chinese, examining whether they prefer surface scope (SS) or inverse scope (IS) readings for doubly quantified sentences. Using a truth-value judgment task, the research finds that most LLMs, like humans, prefer SS readings, especially in existential quantifier (UE) structures. Only a subset of models, particularly BERT-family, differentiate between English and Chinese in their IS preferences, aligning with human-like patterns. Human similarity (HS) scores reveal that LLMs generally approximate human language use, with GPT and Llama models outperforming BERT. Model architecture, scale, and pre-training language background significantly influence this alignment. Overall, LLMs show potential for human-like quantifier scope interpretation, but with variability depending on model type and linguistic context.

## Method Summary
The study uses a truth-value judgment task (TVJT) to assess LLM interpretation of quantifier scope in doubly quantified sentences across English and Chinese. 60 experimental target sentences per language (30 UE: existential-universal, 30 EU: universal-existential) are paired with two story contexts (SS-favoring and IS-favoring), yielding 120 trials per language. Human baseline data from Fang (2023) across 4 participant groups (L1/L2 English/Chinese) provides the comparison standard. Surprisal scores (lower = preferred interpretation) are computed via conditional probability for autoregressive models (GPT, LLaMA) or pseudo-log-likelihood scoring for masked models (BERT). Binary preferences (SS=1 if lower surprisal) and Human Similarity (HS) Scores via Jensen-Shannon divergence comparing LLM response distributions to human TVJT ratings form the primary metrics. Analysis uses mixed-effects models with sum-coded predictors and random intercepts for items.

## Key Results
- Most LLMs prefer SS readings for UE structures, matching human preferences.
- Only BERT-family models show cross-linguistic IS preference differences between English and Chinese.
- GPT and Llama models achieve higher HS scores than BERT, indicating better overall human alignment.
- Model architecture, scale, and pre-training language background significantly influence HS scores.

## Why This Works (Mechanism)

### Mechanism 1
Surprisal-based probability scoring serves as a proxy for interpretation preference in doubly quantified sentences. Conditional probability of sentence acceptance in context is computed via token-level probabilities (autoregressive) or pseudo-log-likelihood scoring (masked models). Lower surprisal indicates more expected (preferred) interpretation. Processing cost correlates with interpretive likelihood; surprisal approximates cognitive load. This proxy may fail if surprisal does not correlate with human acceptability judgments.

### Mechanism 2
Processing Scope Economy (PSE) principle explains SS preference in both humans and LLMs. IS derivation involves covert movement at Logical Form, creating mismatch between surface syntax and semantics, incurring higher cognitive/computational cost. SS requires no such operation. LLMs internalize processing constraints analogous to human cognition during pre-training. PSE-like alignment is not universal, as LLM legal interpretation diverges from human judgments in some domains.

### Mechanism 3
Pre-training language composition creates L1-transfer-like biases in LLMs, producing L2-like cross-linguistic behavior. Models trained on English-dominant corpora encode English scope patterns; Chinese-trained models still show residual English bias due to translated content in training data. Pre-training corpora contain structural biases transferable to downstream tasks; translated texts preserve source-language patterns. Chinese-native-trained models showing robust Chinese-specific scope rigidity without English contamination would weaken the transfer claim.

## Foundational Learning

- **Quantifier Scope Ambiguity (Surface vs. Inverse Scope)**: The entire paper hinges on distinguishing SS ("every child climbed a different tree") from IS ("all children climbed the same tree") interpretations. Quick check: Given "A child climbed every tree," does SS mean one child climbed all trees, or different children climbed different trees?

- **Truth-Value Judgment Task (TVJT)**: This is the experimental paradigm used to elicit human and model scope preferences via context-sentence fit ratings. Quick check: If a story describes three children each climbing a different tree, and the sentence is "Every child climbed a tree," which scope reading does this context support?

- **Surprisal and Log-Likelihood Scoring**: LLM interpretation preferences are quantified via surprisal (negative log-probability); lower surprisal = preferred reading. Quick check: If SS surprisal = 1.6 and IS surprisal = 1.8, which interpretation does the model prefer?

## Architecture Onboarding

- **Component map**: Autoregressive models (GPT-2, Llama) -> compute sentence probability via chain rule -> higher HS scores, poor cross-linguistic contrast. Masked models (BERT) -> compute pseudo-log-likelihood by sequential token masking -> better cross-linguistic contrasts, lower HS scores. Training language variants -> GPT2En vs. GPT2Ch; LlamaEn vs. LlamaCh -> mixed evidence for Chinese scope rigidity.

- **Critical path**: Input: DQ sentence + context (SS-favoring or IS-favoring story) -> Compute surprisal for sentence-in-context pair -> Compare SS-context vs. IS-context surprisal; assign binary preference -> Aggregate preferences across items; compare to human TVJT ratings via JS divergence -> HS score

- **Design tradeoffs**: GPT/Llama vs. BERT: Autoregressive models yield higher HS scores but fail to capture cross-linguistic contrasts; BERT captures contrasts but has lower surface alignment. Scale vs. inverse scaling: Larger models (BERT-large) do not uniformly outperform smaller ones; inverse scaling possible. Surprisal vs. prompt-based probing: Surprisal provides continuous likelihood; prompting can elicit explicit acceptability judgments but introduces prompt-sensitivity.

- **Failure signatures**: EU inversion: LLMs prefer IS for EU structures where humans prefer SS (opposite of expected PSE pattern). Cross-linguistic collapse: Models fail to distinguish English IS-availability from Chinese IS-rigidity (except BERT-family). L2-like alignment: Models align better with L2 speakers than L1 speakers for Chinese, indicating training-data bias.

- **First 3 experiments**: 1. Baseline replication: Run UE/EU stimuli through GPT-2En and BERT-base; verify SS preference for UE, document EU inversion pattern. 2. Cross-linguistic probe: Compare IS surprisal for English vs. Chinese in BERT-large; confirm significant language effect (b=1.1, p=.0499) per Table 2. 3. HS score validation: Compute JS divergence between model surprisal distributions and human TVJT ratings for L1-English group; confirm GPT > Llama > BERT ordering per Figure 4.

## Open Questions the Paper Calls Out

### Open Question 1
Do surprisal-based interpretations of quantifier scope in LLMs converge with explicit acceptability judgments obtained through prompt-based probing? The authors explicitly state in the Limitations that the study "relies exclusively on surprisal values" and that future research should "incorporate additional metrics, such as prompt-based probing," to validate findings. Surprisal measures the relative predictability of a token sequence, which is distinct from the binary or scalar grammaticality judgments humans provide in truth-value tasks. A replication using identical stimuli where LLMs are prompted to provide explicit Likert-scale ratings (e.g., 1-7) for surface vs. inverse scope interpretations would resolve this.

### Open Question 2
Why do autoregressive models (GPT, Llama) prefer Inverse Scope (IS) for Existential-Universal (EU) constructions, whereas humans strongly prefer Surface Scope (SS)? The Discussion highlights that LLM results for EU sentences were "unexpected," showing an IS preference that contradicts human data and the Single Reference Principle, whereas BERT models aligned better with human preferences. The paper observes the divergence but does not identify if the cause is the autoregressive architecture's handling of incremental parsing or a failure to capture semantic entailment. Targeted diagnostic tests on autoregressive models to determine if they construct single-referent representations for singular indefinites at the sentence onset would resolve this.

### Open Question 3
Does the tendency of Chinese-trained LLMs to align with L1-English/L2-Chinese participants stem from "translated English" content in the pre-training data? The authors hypothesize that "translated English content in the training data" may cause models like LlamaCh to exhibit English-dominant patterns, leaving the "systematic examination for future research." It is unclear if the "L2-like" behavior observed in LLMs is a data contamination issue or an inherent "transfer" effect within the model's representations. Training comparable models on strictly curated, non-translated monolingual Chinese corpora and measuring if the English-like scope preferences persist would resolve this.

## Limitations
- Stimuli construction is a significant limitation, with only 4 example sentences provided out of 60 per language needed for full replication.
- Human response distributions required for HS score computation are aggregated rather than item-level, potentially affecting precision of cross-model comparisons.
- Reliance on surprisal as a proxy for interpretive preference introduces uncertainty about correlation with human acceptability judgments.

## Confidence

- **High Confidence**: The general finding that LLMs prefer SS readings for UE structures, and that autoregressive models (GPT/Llama) show higher human similarity than BERT models.
- **Medium Confidence**: The claim that pre-training language composition creates L1-transfer-like biases in LLMs, and that BERT models uniquely capture cross-linguistic contrasts.
- **Low Confidence**: The universality of Processing Scope Economy (PSE) as the mechanism explaining SS preference, given the EU inversion pattern where LLMs show opposite preferences to humans.

## Next Checks
1. **Stimuli Validation**: Reconstruct the full 60-sentence stimulus set per language using the provided templates and verify semantic parallelism between English and Chinese versions.
2. **Surprisal Correlation Test**: Run a subset of items through both surprisal scoring and explicit prompt-based acceptability judgments to validate that lower surprisal corresponds to higher human acceptability.
3. **Cross-linguistic Contrast Verification**: Test BERT-large on the cross-linguistic contrast between English and Chinese IS availability to confirm the reported significant language effect (b=1.1, p=.0499).