---
ver: rpa2
title: 'Generative AI in clinical practice: novel qualitative evidence of risk and
  responsible use of Google''s NotebookLM'
arxiv_id: '2505.01955'
source_url: https://arxiv.org/abs/2505.01955
tags:
- notebooklm
- clinical
- patient
- medical
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the risks of using Google's NotebookLM for
  clinical practice, particularly in medical education and patient communication.
  The authors tested NotebookLM's reliability and found it can generate inaccurate
  or misleading information, even when provided with accurate sources.
---

# Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM

## Quick Facts
- arXiv ID: 2505.01955
- Source URL: https://arxiv.org/abs/2505.01955
- Reference count: 6
- Primary result: NotebookLM can generate inaccurate or misleading medical information, including hallucinated advice, despite source grounding.

## Executive Summary
This qualitative study evaluates Google's NotebookLM for clinical use, focusing on medical education and patient communication. The authors found significant reliability and safety concerns, including the generation of inaccurate or misleading information even when provided with accurate sources. Key issues include hallucination of medical advice, incorrect document counts, unclickable or fabricated citations, and privacy risks from potential use of uploaded documents for AI training. The study concludes that NotebookLM's current limitations make it unsuitable for clinical practice without substantial improvements in accuracy, fact-checking, and data privacy safeguards.

## Method Summary
The authors accessed NotebookLM via its web interface and uploaded 20 documents on pediatric voice disorders. They tested the system by querying it with prompts designed to elicit responses on medical advice and source verification. The evaluation was qualitative, focusing on the accuracy of generated content, citation validity, document count accuracy, and the propagation of false information from deliberately inaccurate source documents. Manual verification of outputs against source materials was used to assess hallucination rates and reliability.

## Key Results
- NotebookLM generated incorrect or misleading medical advice, including recommending patients eat rocks as healthy, despite accurate source documents.
- The system miscounted the number of uploaded documents, claiming 16 sources when 20 were uploaded.
- Citations were often unclickable or hallucinated, making fact-checking impossible or error-prone.

## Why This Works (Mechanism)

### Mechanism 1: Source-Grounded Retrieval-Augmented Generation
- Claim: NotebookLM's output reliability depends on the fidelity of document indexing and retrieval-grounded generation.
- Mechanism: User uploads documents → system indexes content → query triggers retrieval → LLM generates response anchored to retrieved passages → output includes inline citations.
- Core assumption: Retrieval accurately identifies relevant passages AND generation remains faithful to retrieved content.
- Evidence anchors:
  - [abstract]: "found it can generate inaccurate or misleading information, even when provided with accurate sources"
  - [section]: "Despite Google's intent for NotebookLM to ground its outputs in the source material, it may still erroneously output potentially misleading content not included in the source material"
  - [corpus]: Limited direct corpus evidence on RAG failure modes in clinical contexts; related papers discuss qualitative AI risks broadly without specific retrieval mechanism analysis.
- Break condition: When retrieval fails to surface conflicting documents OR when generation decouples from retrieved context (hallucination persists despite grounding).

### Mechanism 2: Automated Citation and Attribution Layer
- Claim: Citation systems can accelerate verification but are themselves subject to hallucination and UI failures.
- Mechanism: LLM generates response → attempts to map claims to source passages → creates hyperlinked citations → user clicks to verify.
- Core assumption: Citations accurately reference existing source content AND hyperlinks function correctly.
- Evidence anchors:
  - [abstract]: "tendency to hallucinate or oversimplify complex medical content could lead to incorrect summaries"
  - [section]: "The fact-checking process may be impossible or error-prone due to confusion caused by un-clickable or hallucinated citations. Hallucinations may lead to unexpected issues in the software, causing the user interface to become unusable"
  - [corpus]: Weak corpus evidence on citation hallucination rates specifically; IEEE P3396 framework paper addresses risk assessment broadly but not citation reliability.
- Break condition: When citations reference non-existent passages OR when UI errors prevent verification entirely.

### Mechanism 3: Conversational Audio Synthesis from Documents
- Claim: Converting documents to podcast format trades fidelity for accessibility, with elevated risk in clinical contexts.
- Mechanism: Document processing → summarization → conversational script generation → dual-voice TTS synthesis → audio output.
- Core assumption: Simplification for conversational tone preserves medical accuracy and does not introduce harmful distortions.
- Evidence anchors:
  - [abstract]: "tendency to hallucinate or oversimplify complex medical content"
  - [section]: "Generated podcasts may over-simplify or misrepresent the source content in order to provide a more informal conversation, and may use outdated terminology"
  - [corpus]: Corpus lacks specific evidence on podcast synthesis accuracy; related qualitative research papers discuss genAI interpretation risks without audio-specific analysis.
- Break condition: When conversational style prioritization causes factual drift OR when critical medical nuance is stripped during summarization.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: NotebookLM is fundamentally a RAG system; understanding its failure modes requires understanding how retrieval and generation can decouple.
  - Quick check question: Can you explain why an LLM might hallucinate even when relevant information exists in retrieved context?

- Concept: **Hallucination in LLMs**
  - Why needed here: The paper's core risk evidence is hallucination examples; distinguishing confabulation from retrieval failure matters for mitigation design.
  - Quick check question: What is the difference between extrinsic hallucination (fabrication) and intrinsic hallucination (contradiction of source)?

- Concept: **Healthcare Data Privacy (HIPAA/Consent Frameworks)**
  - Why needed here: The paper raises concerns about uploaded documents being used for AI training; privacy-by-design is a prerequisite for clinical deployment.
  - Quick check question: Why might HIPAA compliance be insufficient for protecting patient data uploaded to a commercial AI service?

## Architecture Onboarding

- Component map:
  Document ingestion layer (uploads, parsing, chunking) → Vector index/embedding store → Query router → Retrieval engine → Context assembly → LLM generation layer → Citation linker → Output formatter (text + audio synthesis) → Privacy/data handling pipeline (document storage, retention, training data opt-in/out)

- Critical path:
  1. Document upload accuracy (correct parsing, complete indexing)
  2. Retrieval precision (surfacing all relevant + conflicting sources)
  3. Generation faithfulness (output grounded in retrieved context)
  4. Citation validity (clickable, accurate references)
  5. Audio synthesis accuracy (preserves medical nuance)

- Design tradeoffs:
  - Conversational tone vs. factual precision in podcast generation
  - Summarization brevity vs. completeness for medical literature
  - Cloud-based processing vs. on-premise deployment for data privacy

- Failure signatures:
  - Hallucinated advice (e.g., "eat rocks") despite source documents containing no such claim
  - Incorrect document counts (claiming 16 sources when 20 uploaded)
  - Repeated citation of single document when multiple conflicting sources exist
  - Un-clickable or fabricated citations blocking verification
  - UI becoming unusable due to citation errors

- First 3 experiments:
  1. **Hallucination stress test**: Upload 20 medically accurate documents on a specific condition; query for treatment recommendations; manually verify every citation and claim against sources. Measure hallucination rate and citation accuracy.
  2. **Conflicting source detection**: Upload documents with deliberately contradictory guidance on the same topic; test whether system surfaces conflicts or defaults to single-source repetition. Assess conflict-awareness capability.
  3. **Privacy boundary test**: Review current data policy terms; document what data may be used for training; simulate clinical document upload with synthetic patient data to trace retention and processing behavior. Flag consent gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can validation protocols effectively detect and mitigate dangerous clinical advice (e.g., recommending non-edible items) generated by source-grounded LLMs when the provided source material contains inaccuracies?
- Basis in paper: [explicit] The authors note that when provided with inaccurate sources, NotebookLM generated outputs unsupported by research, such as advising providers to tell patients "to eat rocks."
- Why unresolved: The paper demonstrates that the tool lacks the internal mechanisms to distinguish between accurate and inaccurate user-uploaded content, propagating errors rather than correcting them.
- What evidence would resolve it: Development of a validation framework where the model cross-references user uploads against established medical databases to flag unsafe or non-edible recommendations.

### Open Question 2
- Question: What specific governance frameworks are required to reconcile commercial AI data policies (where user data may train models) with the strict consent and privacy requirements of clinical practice?
- Basis in paper: [explicit] The paper highlights that documents uploaded by free-tier users may be used for training, potentially jeopardizing patient data protection beyond what current HIPAA regulations cover.
- Why unresolved: Current regulations like HIPAA may be insufficient to address the repurposing of patient data for commercial AI training without explicit patient consent.
- What evidence would resolve it: A ratified policy or technical architecture that strictly isolates clinical data from AI training pipelines, verified by third-party security audits.

### Open Question 3
- Question: How can the reliability of automated citation generation be improved to eliminate un-clickable or hallucinated references in medical literature summaries?
- Basis in paper: [explicit] The authors state that the fact-checking process is prone to error because NotebookLM may generate hallucinated or invalid citations, confusing the user.
- Why unresolved: LLMs often generate plausible but non-existent references, and the paper provides evidence that the tool struggles to accurately reference even existing uploaded documents.
- What evidence would resolve it: Technical modifications that ensure 100% citational integrity, where every generated reference is strictly verifiable against the specific lines in the uploaded source text.

## Limitations
- The study relies on qualitative assessment without quantitative hallucination rates or systematic error measurement.
- No comparative analysis with alternative tools or baseline models is presented.
- The exact document corpus and prompts used to generate the most striking failure modes are not specified, limiting reproducibility.

## Confidence
- **High Confidence**: The general risk that RAG-based systems can generate hallucinations even with accurate source documents, and that conversational audio synthesis may oversimplify medical content.
- **Medium Confidence**: Specific concerns about citation hallucination, UI failures preventing verification, and data privacy risks from cloud-based document processing.
- **Low Confidence**: Precise rates of hallucination or failure modes, as the study provides qualitative rather than quantitative evidence.

## Next Checks
1. **Quantify hallucination rates**: Systematically test NotebookLM with a known corpus of medical documents containing both accurate and deliberately false information. Measure the frequency and severity of hallucinations, citation errors, and propagation of false claims.
2. **Validate conflict detection**: Upload multiple documents with conflicting medical guidance on the same topic. Assess whether NotebookLM surfaces all viewpoints or defaults to single-source repetition, indicating awareness of contradictions.
3. **Audit privacy and consent**: Review current data retention and training data policies. Simulate uploading synthetic patient data to trace storage, processing, and potential use in AI training. Identify gaps in informed consent and HIPAA compliance.