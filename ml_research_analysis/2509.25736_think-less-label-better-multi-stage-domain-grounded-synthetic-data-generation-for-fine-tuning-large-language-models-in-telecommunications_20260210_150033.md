---
ver: rpa2
title: 'Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation
  for Fine-Tuning Large Language Models in Telecommunications'
arxiv_id: '2509.25736'
source_url: https://arxiv.org/abs/2509.25736
tags:
- synthetic
- data
- generation
- arxiv
- troubleshooting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fully automated, multi-stage pipeline for
  generating high-quality synthetic QA pairs grounded in domain-specific knowledge
  graphs, aimed at reducing the need for manual labeling in fine-tuning LLMs for specialized
  tasks like telecom troubleshooting. The approach combines retrieval-augmented generation
  with base generation and refinement stages, followed by customized RAGAS-based filtering
  to ensure technical accuracy and contextual relevance.
---

# Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications

## Quick Facts
- **arXiv ID:** 2509.25736
- **Source URL:** https://arxiv.org/abs/2509.25736
- **Reference count:** 22
- **Primary result:** Multi-stage pipeline generates telecom troubleshooting QA pairs with 33.6% ratio and 75% indistinguishability rate, reducing manual labeling needs.

## Executive Summary
This paper presents a fully automated pipeline for generating high-quality synthetic QA pairs grounded in domain-specific knowledge graphs, aimed at reducing manual labeling requirements for fine-tuning LLMs on specialized tasks. The approach combines retrieval-augmented generation with base generation and refinement stages, followed by customized RAGAS-based filtering to ensure technical accuracy and contextual relevance. Applied to telecom RAN troubleshooting, the pipeline achieves significant generation efficiency while maintaining domain specificity, demonstrating a scalable solution for building instruction and reinforcement datasets in technical domains.

## Method Summary
The method employs a three-stage pipeline: first, a retrieval model (Qwen3-8B) extracts top-3 context chunks from a domain knowledge graph indexed via HippoRAG; second, a base generator (Qwen3-14B-Base) creates diverse candidate QA pairs using few-shot prompts from 50 manually curated seed examples; third, a refiner (Qwen3-8B/Qwen3-32B) enhances answers with retrieved context while disabling "thinking mode." Custom RAGAS metrics (ResponseGroundedness, AspectCritic, Tele-Specificity, ResponseRelevancy) filter outputs with thresholds >0.75, =1, >0.75, and >0.5 respectively. The pipeline runs on 7× RTX A6000 GPUs, completing in ~65 minutes with 33.6% generation ratio and 75% indistinguishability rate.

## Key Results
- Achieved 33.6% generation ratio through hybrid base generation and refinement stages
- 75% of generated QA pairs were indistinguishable from human-curated seed data
- Generated 410 QA pairs across 41 telecom troubleshooting topics (10 per topic target)
- Question diversity maintained through pairwise cosine similarity measurements

## Why This Works (Mechanism)
The pipeline's effectiveness stems from combining retrieval-augmented generation with iterative refinement and rigorous filtering. By grounding generation in domain-specific knowledge graphs and using multiple evaluation metrics, it ensures technical accuracy while maintaining diversity. The three-stage approach allows for broad candidate generation followed by context-aware refinement, with RAGAS filtering serving as quality control.

## Foundational Learning
- **HippoRAG retrieval**: Required for extracting relevant context chunks from knowledge graphs; quick check: verify retrieval accuracy on held-out queries
- **RAGAS metrics**: Custom evaluation framework for technical QA quality; quick check: validate metric consistency across different domain samples
- **Multi-stage generation**: Base generation followed by refinement improves quality; quick check: compare single-stage vs multi-stage outputs
- **Few-shot prompting**: Seed examples guide generation; quick check: measure impact of varying seed quantity on diversity
- **Tele-Specificity scoring**: Domain-aware evaluation ensures technical accuracy; quick check: test with domain experts on sample outputs
- **GPU-based serving**: vLLM deployment enables efficient inference; quick check: benchmark throughput vs traditional serving methods

## Architecture Onboarding
- **Component map**: Document corpus → HippoRAG indexing → Retrieval (Qwen3-8B) → Base generation (Qwen3-14B) → Refinement (Qwen3-8B/Qwen3-32B) → RAGAS filtering → Output QA pairs
- **Critical path**: Retrieval → Base generation → Refinement → Filtering (RAGAS metrics determine final output)
- **Design tradeoffs**: Balance between generation diversity (favoring base generation) and accuracy (favoring refinement with retrieved context)
- **Failure signatures**: Low generation ratio indicates overly strict filtering; hallucinated terms suggest Tele-Specificity metric issues
- **First experiments**:
  1. Test HippoRAG retrieval accuracy on held-out telecom troubleshooting queries
  2. Compare single-stage vs multi-stage generation quality using RAGAS metrics
  3. Measure impact of varying RAGAS threshold values on generation ratio and quality

## Open Questions the Paper Calls Out
### Open Question 1
Does synthetic QA data generated by this pipeline actually improve downstream LLM performance on telecom troubleshooting tasks when used for reinforcement fine-tuning?
- **Basis in paper:** The paper demonstrates the pipeline can generate high-quality synthetic data (33.6% generation ratio, 75% IR) but does not report results from actually fine-tuning a model with this data and measuring task performance.
- **Why unresolved:** The evaluation focuses on data quality metrics (RAGAS scores, indistinguishability rate) rather than end-to-end fine-tuning experiments that would demonstrate practical utility.
- **What evidence would resolve it:** Fine-tune an LLM on the generated dataset and benchmark against baseline models on held-out telecom troubleshooting tasks using domain-specific evaluation metrics.

### Open Question 2
What is the minimum quantity and quality of expert-curated seed data required for the pipeline to produce viable synthetic datasets?
- **Basis in paper:** The authors use 50 manually verified seed QA pairs but note that "the limited number of expert-curated seed QA pairs may constrain question diversity."
- **Why unresolved:** No ablation study is conducted on seed data size, leaving unclear whether fewer examples suffice or whether more would significantly improve output diversity.
- **What evidence would resolve it:** Systematic experiments varying seed data size (e.g., 10, 25, 50, 100 samples) while measuring synthetic data quality and diversity.

### Open Question 3
How robust are the empirically determined RAGAS thresholds across different technical domains beyond telecom RAN troubleshooting?
- **Basis in paper:** The thresholds (Groundedness >0.75, Specificity >0.75, Relevancy >0.5) are established through comparison between synthetic and seed datasets in one specific domain.
- **Why unresolved:** TeleQuAD validation suggests some generalization, but threshold calibration for different domains (medicine, legal, industrial) remains unexplored.
- **What evidence would resolve it:** Apply the pipeline with identical thresholds to multiple specialized domains and analyze whether recalibration is necessary for comparable quality.

## Limitations
- Exact prompt templates for generation and evaluation are not disclosed, potentially impacting reproducibility
- Seed dataset of 50 SME-verified QA pairs is not released, preventing independent validation
- HippoRAG indexing parameters are assumed to follow standard configurations without detailed specification

## Confidence
- **High Confidence:** Multi-stage pipeline architecture and overall experimental setup are clearly described and reproducible
- **Medium Confidence:** Generation ratio (33.6%) and indistinguishability rate (75%) are specific but exact computation methods are not fully detailed
- **Low Confidence:** Claims of "rigorous domain specificity" depend on Tele-Specificity metric robustness, which lacks independent validation

## Next Checks
1. Reconstruct and test the full pipeline using publicly available telecom troubleshooting documents, measuring generation ratio and indistinguishability rate with specified RAGAS thresholds
2. Conduct ablation studies to quantify the impact of three-stage pipeline versus simpler retrieval-augmented generation approaches on answer quality and diversity
3. Evaluate method transferability to a different technical domain (e.g., healthcare or cybersecurity) using same pipeline and filtering criteria