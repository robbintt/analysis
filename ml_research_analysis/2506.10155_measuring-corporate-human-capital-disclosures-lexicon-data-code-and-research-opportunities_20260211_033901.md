---
ver: rpa2
title: 'Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research
  Opportunities'
arxiv_id: '2506.10155'
source_url: https://arxiv.org/abs/2506.10155
tags:
- disclosures
- words
- human
- disclosure
- keywords
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a comprehensive lexicon of human capital-related
  keywords to measure corporate human capital disclosures. Using the word2vec algorithm
  trained on nearly 4,000 firms'' SEC-mandated human capital disclosures, the authors
  create a list of 1,285 keywords classified into five subcategories: Diversity, Equity,
  and Inclusion; Health and Safety; Labor Relations and Culture; Compensation and
  Benefits; and Demographics and Others.'
---

# Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities

## Quick Facts
- **arXiv ID:** 2506.10155
- **Source URL:** https://arxiv.org/abs/2506.10155
- **Reference count:** 29
- **Primary result:** Comprehensive human capital lexicon (1,285 keywords) trained on SEC-mandated disclosures, validated via BERT fine-tuning achieving 97.46% accuracy

## Executive Summary
This paper develops a comprehensive lexicon of human capital-related keywords to measure corporate human capital disclosures. Using the word2vec algorithm trained on nearly 4,000 firms' SEC-mandated human capital disclosures, the authors create a list of 1,285 keywords classified into five subcategories: Diversity, Equity, and Inclusion; Health and Safety; Labor Relations and Culture; Compensation and Benefits; and Demographics and Others. The lexicon is validated through applications to proxy statements and fine-tuning a BERT model, demonstrating construct validity and strong classification performance (97.46% accuracy on HC vs. non-HC classification). Researchers can use this lexicon and accompanying code to measure human capital disclosures and pursue related research questions.

## Method Summary
The authors train word2vec on hand-collected human capital disclosures from 3,953 firms' 10-K Item 1 sections (Nov 2020–Nov 2021), then expand seed words from SASB and SHRM frameworks using cosine similarity thresholds. Manual screening produces 1,285 final keywords across five categories. They also fine-tune BERT on 83,961 human capital sentences versus twice as many non-HC sentences to create a context-aware classifier. The approach is validated by applying the lexicon to proxy statements and testing BERT's out-of-sample performance.

## Key Results
- 1,285 human capital keywords classified into five subcategories: DEI, Health & Safety, Labor Relations & Culture, Compensation & Benefits, and Demographics & Others
- BERT fine-tuning achieves 97.46% accuracy on human capital vs. non-human capital classification
- Lexicon successfully identifies temporal trends in proxy statements aligned with regulatory events
- High construct validity demonstrated through application to different disclosure venues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training word2vec exclusively on confirmed HC disclosures produces a lexicon with high construct validity.
- Mechanism: Word2vec learns word associations from co-occurrence patterns. When the training corpus is restricted to unambiguously HC-related text (the mandated HC sections from 10-Ks), the resulting embeddings capture HC-specific semantics rather than the broader, noisier meanings found in general corporate text.
- Core assumption: Terms used within HC disclosure sections have more consistent, domain-specific meanings than the same terms used elsewhere in corporate filings.
- Evidence anchors:
  - [abstract]: "We use a machine learning algorithm (word2vec) trained on a confirmed set of HC disclosures to develop a comprehensive list of HC-related keywords."
  - [section]: "Training our model on this corpus of unambiguously HC-related documents consisting of almost two million words ensures that our lexicon has high construct validity."
  - [corpus]: Related work on corporate disclosure analysis (e.g., climate disclosure scoring via LLMs) relies on fine-tuning or prompting, not domain-restricted pre-training; limited direct corpus evidence for this specific mechanism.
- Break condition: If the HC disclosure corpus contains significant non-HC content, or if HC terminology varies fundamentally across industries in ways not captured by the training set.

### Mechanism 2
- Claim: Seed words grounded in standard-setter frameworks (SASB, SHRM) can be expanded via cosine similarity to capture comprehensive HC terminology.
- Mechanism: Expert-identified seed words initialize the semantic space. Word2Vec identifies candidate terms with cosine similarity ≥ 0.5 (or ≤ -0.5 for antonyms) to seed words. Manual screening then removes false positives and assigns categories based on highest average similarity to category seed words.
- Core assumption: A cosine similarity threshold of |0.5| meaningfully separates HC-related terms from unrelated terms, and manual review can reliably filter remaining noise.
- Evidence anchors:
  - [abstract]: "1,285 keywords classified into five subcategories."
  - [section]: "Prior studies show that using a cosine similarity threshold of 0.5 yields stronger results."
  - [corpus]: Weak corpus evidence for the specific 0.5 threshold; no comparative threshold studies found in related disclosure measurement literature.
- Break condition: If HC language evolves rapidly post-2021, or if the threshold is too permissive (noise) or too restrictive (missed valid terms).

### Mechanism 3
- Claim: Fine-tuning BERT on labeled HC/non-HC sentences produces a classifier with strong out-of-sample performance.
- Mechanism: Pre-trained BERT captures contextual word meanings. Fine-tuning on 83,961 HC sentences plus an equal number of non-HC sentences adapts the model to binary HC classification, enabling context-aware detection that keyword approaches miss.
- Core assumption: The binary HC vs. non-HC distinction is meaningful and the 10-K-derived training data generalizes to other document types (10-Qs, proxy statements).
- Evidence anchors:
  - [abstract]: "97.46% accuracy on HC vs. non-HC classification."
  - [section]: "At the probability threshold of 86 percent, the corresponding precision, recall, and F1 score are 85.16 percent, 85.35 percent, and 85.26 percent, respectively, for the HC class" (on out-of-sample 10-Q data).
  - [corpus]: Related financial disclosure work (FinGEAR, climate disclosure LLMs) uses fine-tuning but typically for retrieval or scoring, not binary classification validation.
- Break condition: If HC content patterns differ substantially between 10-Ks and target documents, or if the labeled dataset contains systematic annotation errors.

## Foundational Learning

- Concept: **Word2Vec and Word Embeddings**
  - Why needed here: Core algorithm for lexicon expansion; maps words to dense vectors where semantic similarity corresponds to geometric proximity.
  - Quick check question: Given a trained word2vec model, how would you find the five words most semantically similar to "inclusion"?

- Concept: **Cosine Similarity**
  - Why needed here: Determines which candidate terms are "close enough" to seed words to warrant inclusion in the lexicon.
  - Quick check question: If a seed word has embedding [1, 0] and a candidate term has embedding [0.6, 0.8], what is their cosine similarity? Should this candidate be kept at a 0.5 threshold?

- Concept: **BERT Fine-Tuning for Classification**
  - Why needed here: Alternative to keyword-based detection; requires understanding transfer learning, classification heads, and evaluation metrics.
  - Quick check question: Why does fine-tuning require labeled data? What would happen if the training set were 95% non-HC sentences?

## Architecture Onboarding

- Component map: Data Collection -> Seed Word Definition -> Word2Vec Training -> Similarity Filtering -> Manual Screening -> Category Assignment -> BERT Fine-Tuning
- Critical path:
  1. Seed word quality → defines scope of entire lexicon
  2. Corpus purity (HC-only) → determines embedding semantic focus
  3. Manual screening rigor → prevents false positives in production use
- Design tradeoffs:
  - Keyword vs. ML-based: Keywords are transparent and replicable but context-insensitive; BERT captures context but is opaque and requires labeled data
  - Cosine threshold: Higher = more precision, lower recall; lower = more coverage, more noise
  - Binary vs. multi-class classification: Binary is simpler but loses topic granularity within HC
- Failure signatures:
  - Lexicon includes clearly non-HC terms → corpus contamination or threshold too low
  - Important HC concepts missing → seed words incomplete or threshold too high
  - BERT accuracy drops on new document types → training data not representative
  - Categories semantically overlap → seed word categories not sufficiently distinct
- First 3 experiments:
  1. Lexicon trend validation: Apply lexicon to proxy statements (1994–2022); check if keyword spikes align with known regulatory events (2007 compensation rules, 2010 Proxy Disclosure Enhancements). Expected: visible jumps in Compensation & Benefits and Demographics keywords at regulatory inflection points.
  2. Threshold sensitivity test: Re-run expansion at thresholds 0.4, 0.5, 0.6, 0.7; manually evaluate precision/recall of top 100 candidates at each level. Expected: monotonic increase in precision, decrease in recall as threshold rises.
  3. Out-of-domain BERT stress test: After fine-tuning on 10-K HC sentences, evaluate on 10-Qs and earnings call transcripts. If F1 drops below 0.75, indicates domain shift requiring additional labeled data or domain-adaptive pre-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific dimensions of human capital management (HCM) practices contribute most to firm value creation and innovation?
- Basis in paper: [Explicit] The authors state that researchers can use the lexicon to "study which employee treatment practices contribute more to value creation or to other important performance outcomes such as innovation."
- Why unresolved: Prior literature relied on external ratings or limited keywords, failing to capture the multi-dimensional nature of HCM across large samples.
- What evidence would resolve it: Large-sample empirical analysis correlating granular lexicon-based measures (e.g., DEI vs. Health & Safety) with financial performance and patent output.

### Open Question 2
- Question: What are the determinants and consequences of a firm's choice of disclosure channel (e.g., 10-K vs. sustainability reports) for human capital information?
- Basis in paper: [Explicit] The paper notes a "notable dearth of large-sample empirical evidence related to the determinants and consequences of companies’ choice of HC disclosure channel(s)."
- Why unresolved: Companies now disseminate HC information through various channels (websites, stand-alone reports, calls), but the strategic motivation and impact of these specific choices are not well understood.
- What evidence would resolve it: A comprehensive study comparing HC disclosure characteristics and market reactions across multiple communication venues simultaneously.

### Open Question 3
- Question: To what extent do optimistic or forward-looking HC disclosures constitute "social washing" rather than reflecting genuine performance?
- Basis in paper: [Explicit] The authors highlight that there is "little evidence related to the extent, determinants, and consequences of social washing in HC disclosures."
- Why unresolved: It is difficult to distinguish between substantive reporting and impression management without granular tools to measure disclosure attributes like tone or specificity against real-world outcomes.
- What evidence would resolve it: Analyzing the disconnect between disclosure tone (using the lexicon) and actual HCM performance metrics (e.g., employee turnover, satisfaction scores).

## Limitations

- **Temporal Validity**: The lexicon is trained on 2020-2021 data and may not capture evolving HC terminology and regulatory requirements.
- **Context Insensitivity**: The keyword-based lexicon cannot distinguish between HC and non-HC uses of the same term without additional context.
- **Corpus Purity Assumption**: The mechanism assumes hand-collected HC disclosures are entirely HC-related, which may not hold if Item 1 sections contain mixed content.

## Confidence

- **High Confidence**: BERT classification performance metrics (accuracy, F1) on the test set; the mechanical steps of word2vec training and similarity filtering.
- **Medium Confidence**: The validity of the 0.5 cosine similarity threshold for lexicon expansion; the generalizability of the 10-K-trained BERT model to proxy statements and other document types.
- **Low Confidence**: The long-term robustness of the lexicon to evolving HC disclosure language; the completeness of the seed word list in capturing all relevant HC concepts.

## Next Checks

1. **Temporal Drift Test**: Apply the lexicon to a stratified sample of 10-Ks from 2015, 2018, 2021, and 2023. Measure keyword frequency changes and identify any terms that shift in meaning or drop in relevance over time.

2. **Context Sensitivity Audit**: Manually review 100 sentences flagged as HC-positive by the keyword lexicon but HC-negative by the BERT model (and vice versa). Categorize the types of errors (e.g., keyword ambiguity, domain-specific phrasing) to assess the relative strengths and weaknesses of each approach.

3. **Seed Word Gap Analysis**: Use the fine-tuned BERT model to identify the top 50 sentences with the highest HC probability that contain zero lexicon keywords. Extract and categorize the HC concepts in these sentences to identify potential gaps in the seed word list.