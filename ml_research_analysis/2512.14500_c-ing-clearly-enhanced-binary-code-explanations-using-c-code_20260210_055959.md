---
ver: rpa2
title: 'C-ing Clearly: Enhanced Binary Code Explanations using C code'
arxiv_id: '2512.14500'
source_url: https://arxiv.org/abs/2512.14500
tags:
- code
- assembly
- c-ing
- clearly
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C-ing Clearly, a method to improve Large
  Language Models' understanding of assembly code by leveraging corresponding C source
  code during training. The approach generates synthetic datasets of {assembly, report}
  pairs using prompts that include both assembly and C code, helping the model use
  C as a high-resource anchor to better reason about assembly.
---

# C-ing Clearly: Enhanced Binary Code Explanations using C code

## Quick Facts
- **arXiv ID**: 2512.14500
- **Source URL**: https://arxiv.org/abs/2512.14500
- **Reference count**: 32
- **Primary result**: C-ing Clearly improves binary code summarization (66.8% win rate) and vulnerability detection (F1 score 8.66 vs 3.08) by leveraging C code during synthetic data generation

## Executive Summary
This paper introduces C-ing Clearly, a method to improve Large Language Models' understanding of assembly code by leveraging corresponding C source code during training. The approach generates synthetic datasets of {assembly, report} pairs using prompts that include both assembly and C code, helping the model use C as a high-resource anchor to better reason about assembly. The method is applied to two tasks: binary code summarization and vulnerability detection, with Llama-3.1-8B-Instruct fine-tuned on datasets constructed from C/C++ code with known vulnerabilities. Results show that C-ing Clearly consistently outperforms baselines across model families and sizes.

## Method Summary
C-ing Clearly generates synthetic training data by having a large LLM (Llama-3.1-Nemotron-70B) produce reports about assembly code while being provided the corresponding C source code as context. This C code serves as a "high-resource proxy" that anchors the model's understanding of the assembly. The synthetic {assembly, report} pairs are then used to fine-tune a smaller target model (Llama-3.1-8B-Instruct). The method employs two prompting strategies: single-turn for general summarization and multi-turn (chain-of-thought) for vulnerability detection. The approach uses datasets from DiverseVul (~8.5K vulnerable functions) and VDISC (~9K non-vulnerable samples), with assembly generated via `gcc -S source.c`.

## Key Results
- Binary Code Summarization: C-ing Clearly wins 66.8% of pairwise comparisons vs baseline
- Vulnerability Detection: Achieves F1 score of 8.66 compared to 3.08 for baseline
- Consistent improvements across multiple model families and sizes
- Multi-turn prompting yields best performance for vulnerability detection tasks

## Why This Works (Mechanism)

### Mechanism 1: High-Resource Proxy Grounding
Providing high-level source code (C) alongside low-level assembly during synthetic data generation creates higher-quality training signals than assembly alone. The generator LLM uses its strong internal representations of C to resolve ambiguities and infer semantic intent in the assembly, producing factually grounded reports.

### Mechanism 2: Chain-of-Thought via Multi-Turn Prompting
Decomposing analysis into distinct steps (C analysis, cross-reference, assembly analysis) improves reasoning capability for vulnerability detection. This forces the model to generate intermediate reasoning states before attempting the harder task of mapping to assembly.

### Mechanism 3: Semantic Feature Enhancement
Training on semantically rich, anchored reports improves the latent representation of assembly code, not just textual output quality. The model's hidden states are forced to encode higher-level semantic features rather than just syntactic assembly patterns.

## Foundational Learning

- **High-Resource vs. Low-Resource Languages in LLMs**: Understanding why C is well-understood by LLMs while assembly is not, due to training data abundance and tokenization efficiency differences.
- **Supervised Fine-Tuning (SFT) vs. In-Context Learning**: Grasping how a large model (In-Context) teaches a smaller model (SFT) through synthetic data generation.
- **LLM-as-a-Judge**: Recognizing why static metrics are insufficient for evaluating binary code summaries when semantic equivalence matters more than syntactic similarity.

## Architecture Onboarding

- **Component map**: Compiler (GCC) -> Teacher (Llama-3.1-Nemotron-70B) -> Student (Llama-3.1-8B-Instruct) -> Judge (GPT-4o)
- **Critical path**: 1) Data Compilation: Align C functions with Assembly, 2) Synthetic Generation: Run Teacher using C-ing Clearly prompts, 3) Rejection Sampling: Filter generated reports, 4) Fine-Tuning: Train Student model
- **Design tradeoffs**: Single-turn vs. Multi-turn (faster vs. better reasoning), Report Verbosity (concise vs. detailed)
- **Failure signatures**: C leakage in student outputs, low CWE match rate requiring rejection sampling
- **First 3 experiments**: 1) Baseline Validation: Compare semantic accuracy of "No C-ing" vs "C-ing Clearly", 2) Vulnerability Reasoning Check: Verify multi-turn pipeline on known buffer overflow, 3) Embedding Probe: Visualize shift in assembly feature space

## Open Questions the Paper Calls Out

- Does the method retain effectiveness when applied to heavily optimized assembly code (e.g., -O2/-O3)?
- How does the method generalize to other hardware architectures like ARM or RISC-V?
- Can high-level languages other than C serve as effective anchors for binary code explanation?
- What is the trade-off mechanism causing Multi-turn to outperform single-turn for vulnerability detection but underperform for summarization?

## Limitations

- Only tested on x86-64 architecture without covering other instruction sets
- Results may not generalize to heavily optimized assembly code
- Potential bias introduced by using GPT-4o as an LLM judge
- Security concerns around synthetic data containing generation artifacts

## Confidence

- **High Confidence**: Core methodology and consistent empirical improvements across model families
- **Medium Confidence**: Mechanism explanations and alternative interpretations
- **Low Confidence**: Long-term stability and generalizability to other architectures

## Next Checks

1. Run controlled experiments comparing C-ing Clearly performance across different optimization levels (-O0 through -O3) to test the break condition where control flow diverges from C source.

2. Re-run the BCS evaluation using a different LLM judge (e.g., Claude-3.5-Sonnet) to assess the robustness of the pairwise win rate metric.

3. Conduct t-SNE or UMAP visualization of assembly embeddings from C-ing Clearly models versus baselines to empirically verify the claimed shift toward more discriminative feature spaces.