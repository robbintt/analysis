---
ver: rpa2
title: Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using
  Generalizability Theory
arxiv_id: '2507.19980'
source_url: https://arxiv.org/abs/2507.19980
tags:
- raters
- human
- scoring
- language
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied generalizability theory to evaluate the reliability
  of large language models (LLMs) in scoring AP Chinese Language and Culture Exam
  essays compared to human raters. Two human raters and seven AI raters independently
  scored 120 essays across two task types (story narration and email response) using
  holistic and analytic scoring for task completion, delivery, and language use.
---

# Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory

## Quick Facts
- arXiv ID: 2507.19980
- Source URL: https://arxiv.org/abs/2507.19980
- Reference count: 0
- Primary result: Human raters produced more reliable scores than LLMs, but hybrid human-AI scoring improved overall reliability, especially with at least one human rater included

## Executive Summary
This study applied generalizability theory to evaluate the reliability of large language models (LLMs) in scoring AP Chinese Language and Culture Exam essays compared to human raters. Two human raters and seven AI raters independently scored 120 essays across two task types (story narration and email response) using holistic and analytic scoring for task completion, delivery, and language use. Results showed that human raters produced more reliable scores than LLMs, but composite scoring incorporating both human and AI raters improved overall reliability, particularly when at least one human rater was included. LLMs demonstrated reasonable consistency for story narration tasks but were less reliable for email response tasks and analytic scoring domains requiring nuanced judgment.

## Method Summary
The study scored 120 AP Chinese essays (30 students × 4 essays each) using two human raters and seven AI models (ChatGPT-3.5, 4.0, 4o, o1; Gemini 1.5, 2.0; Claude 3.5 Sonnet). Essays were scored on holistic and analytic domains (task completion, delivery, language use) using 0-6 scales. AI raters used few-shot prompting with task-specific protocols including rubrics and benchmark samples. Multivariate generalizability theory (G-theory) with p×t×r designs estimated variance components, and D-studies simulated reliability under different rater/task configurations.

## Key Results
- Human raters achieved higher generalizability coefficients (0.805-0.890) than AI raters (0.612-0.830)
- Composite scoring improved reliability, with 1&1 (1 human, 1 AI) outperforming 1&2 due to proportional weighting issues
- Story narration tasks showed higher AI reliability than email response tasks
- Task completion scoring yielded higher reliability than delivery and language use domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid human-AI scoring improves reliability over single-source scoring under specific conditions
- Mechanism: Combining human and AI raters reduces overall error variance by averaging across different judgment patterns. The composite approach works because humans and AI make different types of errors—humans vary more in stringency but less in ranking consistency, while AI shows lower stringency variability but higher person-rater interaction variance. When weighted appropriately, these error patterns partially cancel.
- Core assumption: Human and AI error variance sources are not perfectly correlated, enabling variance reduction through aggregation.
- Evidence anchors:
  - [abstract] "Composite scoring that incorporates both human and AI raters improved reliability, particularly when at least one human rater was included"
  - [section page 23] "For both SN and ER, the 1 & 2 combination exhibited higher reliability than the 0 & 3 combination—note that the total number of raters is the same between the two conditions"
  - [corpus] Related papers confirm hybrid approaches: "Agreement Between Large Language Models and Human Raters in Essay Scoring" synthesizes mixed reliability findings; "Machine-Assisted Grading" notes LLMs enable rapid evaluation but doesn't establish hybrid superiority
- Break condition: Over-weighting AI raters in the composite can reduce reliability. The study found that "1 & 2" condition (1 human, 2 AI) performed worse than "1 & 1" because proportional weighting gave AI raters twice the weight (0.67 vs 0.33), amplifying AI's higher interaction variance.

### Mechanism 2
- Claim: Task structure moderates AI scoring reliability—more structured tasks yield higher AI-human agreement
- Mechanism: Story narration (SN) tasks with visual prompts constrain the response space and provide explicit, objective evaluation criteria (addressing all four pictures = task completion). This reduces the "pragmatic judgment" demands on AI systems. Email response (ER) tasks require understanding real-world context, politeness registers, and nuanced communication appropriateness—all areas where LLMs show higher variance.
- Core assumption: AI systems perform better on criterion-referenced scoring (checking content presence) than on pragmatic appropriateness judgments requiring cultural/situational interpretation.
- Evidence anchors:
  - [abstract] "LLMs demonstrated reasonable consistency for story narration tasks but were less reliable for email response tasks and analytic scoring domains requiring nuanced judgment"
  - [section page 18] "The difference in generalizability coefficients between SN and ER for human raters with 2 raters and 2 tasks was .036; by contrast, for AI raters, it was .132"
  - [corpus] Weak corpus support—neighbor papers discuss AI scoring but don't specifically address task-type moderation effects
- Break condition: When tasks require cultural knowledge, register sensitivity, or pragmatic inference beyond surface-level content, AI reliability degrades more sharply than human reliability.

### Mechanism 3
- Claim: Scoring dimension objectivity predicts AI reliability—more objective criteria produce higher AI consistency
- Mechanism: Task completion (TC) scoring requires checking discrete content elements (did the student address all four pictures?), which maps well to LLM pattern-matching capabilities. Delivery and language use require holistic judgments about appropriateness, fluency, and sophistication—assessments requiring fine-grained linguistic intuition and cultural knowledge that LLMs process less consistently.
- Core assumption: LLMs trained on general corpora can reliably detect content presence but lack stable internal representations for gradations of linguistic sophistication and cultural appropriateness in L2 writing.
- Evidence anchors:
  - [section page 19] "Task completion is expected to exhibit relatively higher reliability compared to the other two domain scores" because "variance associated with the object of measurement was highest for TC"
  - [section page 26] "AI raters showed greater variability when scoring these subtle aspects of language, which suggests they might struggle with making complex linguistic judgments"
  - [corpus] "Comparison of Scoring Rationales Between Large Language Models and Human Raters" examines scoring rationale alignment but corpus lacks direct evidence on dimension-specific reliability
- Break condition: Even "objective" criteria may fail when LLMs interpret task requirements idiosyncratically; TC reliability advantage assumes aligned understanding of what constitutes "completion."

## Foundational Learning

- Concept: **Generalizability Theory (G-Theory)**
  - Why needed here: Unlike classical test theory's single error term, G-theory decomposes variance into components (raters, tasks, persons, interactions), enabling identification of which measurement facets contribute most to unreliability. This is essential for designing efficient hybrid scoring systems.
  - Quick check question: If you observe high person × task interaction variance but low rater variance, should you add more raters or more task prompts to improve reliability?

- Concept: **Variance Component Interpretation**
  - Why needed here: The paper reports variance components like σ²_p (person/universe score), σ²_r (rater stringency), σ²_pr (person-rater interaction). Understanding which components are large tells you where measurement error originates and how to allocate resources.
  - Quick check question: In Table 1, AI raters show lower σ²_r than humans but higher σ²_pr. What does this mean for how AI raters differ from humans in their scoring behavior?

- Concept: **Decision (D) Study Design**
  - Why needed here: G-studies estimate variance components from observed data; D-studies use these estimates to predict reliability under different design configurations (number of raters, tasks). This enables optimization without costly empirical testing.
  - Quick check question: The paper finds diminishing returns after 2 raters for human scoring. What cost-benefit analysis would justify using 3+ AI raters despite smaller marginal reliability gains?

## Architecture Onboarding

- Component map:
  Prompt Engineering Module -> AI Rater Pool -> Variance Decomposition Engine -> D-Study Simulator -> Composite Scoring Aggregator

- Critical path:
  1. Define scoring rubric and collect benchmark responses across score levels
  2. Design task-specific prompts incorporating rubrics and benchmarks
  3. Execute scoring with multiple AI models and human raters in parallel
  4. Estimate G-study variance components from observed scores
  5. Run D-study simulations to determine optimal rater/task configuration
  6. Implement composite scoring with weights derived from reliability optimization

- Design tradeoffs:
  - **More AI raters vs. quality AI raters**: Adding low-quality AI raters can reduce reliability due to proportional weighting amplifying noise. Consider variance-weighted composites instead of count-based weighting.
  - **Task coverage vs. reliability**: SN tasks yield higher reliability but may underrepresent pragmatic communication skills assessed by ER tasks. Domain coverage requires accepting some reliability loss.
  - **Prompt complexity vs. generalization**: More detailed rubrics may improve AI accuracy on specific tasks but reduce generalization to new prompts. The study used task-specific training documents—evaluate whether this scales.
  - **Fine-tuning vs. few-shot prompting**: Fine-tuning could improve reliability but requires computational resources and large annotated datasets. The paper opted for few-shot; ROI for fine-tuning remains unknown.

- Failure signatures:
  - **Composite reliability lower than single-source**: Indicates over-weighting of noisy raters. Check variance components—AI σ²_pr and σ²_ptr should be compared to humans before weighting decisions.
  - **Large SN-ER reliability gap for AI**: Expected, but gaps >0.15 suggest prompt design issues for ER tasks. May need more few-shot examples of email appropriateness.
  - **Negative variance estimates**: Sampling error artifact; the paper replaced with zero. Frequent negatives suggest sample size insufficient for stable estimation (n=30 may be marginal).
  - **Rater × task interaction dominates**: Suggests raters interpret tasks inconsistently. May indicate ambiguous rubric language requiring clarification.

- First 3 experiments:
  1. **Weighting scheme comparison**: Test proportional weighting vs. reliability-optimized weighting (inverse-variance weighting) for human-AI composites. Hypothesis: Inverse-variance weighting will prevent the reliability decrease observed when adding low-quality AI raters.
  2. **Cross-domain generalization**: Train AI raters on SN prompts only, test on ER prompts (and vice versa). Measure reliability degradation. This quantifies prompt dependency and informs whether task-specific training is necessary.
  3. **Minimum viable human integration**: Systematically test 0-human, 1-human, and 2-human configurations with varying AI counts. Identify the reliability frontier—where adding more AI raters provides no marginal benefit for each human count level. This directly addresses the "1 & 1 vs. 1 & 2" anomaly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would fine-tuning LLMs specifically for AP Chinese scoring tasks, rather than using structured prompts, reduce the reliability gap between AI and human raters?
- Basis in paper: [explicit] "Future research could explore whether fine-tuning or reinforcement learning might help reduce the reliability gap between AI and human raters."
- Why unresolved: This study used zero-shot/few-shot prompting without fine-tuning, leaving the potential gains from task-specific model training unexplored.
- What evidence would resolve it: A controlled comparison of reliability coefficients between fine-tuned LLMs and prompt-based LLMs scoring the same AP Chinese essays.

### Open Question 2
- Question: What is the optimal weighting scheme for composite human-AI scoring that maximizes reliability, beyond the proportional weighting used in this study?
- Basis in paper: [inferred] The 1&2 condition (1 human, 2 AI) yielded lower reliability than 1&1, which the authors attribute to proportional weighting giving more weight to less reliable AI scores.
- Why unresolved: The study acknowledges that "alternative weighting schemes could be used" but does not systematically explore them.
- What evidence would resolve it: D-study simulations comparing different fixed and optimized weighting schemes for human-AI composites.

### Open Question 3
- Question: How does LLM scoring reliability compare across different second-language writing assessments and genres beyond AP Chinese tasks?
- Basis in paper: [explicit] "Broader comparative research across diverse language assessments and content areas is essential to better understand how well LLM-based scoring models work."
- Why unresolved: The study focused solely on AP Chinese, which has unique cultural and linguistic expectations that may not generalize.
- What evidence would resolve it: Replication studies applying the same G-theory methodology to other L2 writing assessments (e.g., TOEFL, IELTS, AP Spanish).

### Open Question 4
- Question: How do learner characteristics such as proficiency level, linguistic background, and task familiarity moderate LLM scoring reliability?
- Basis in paper: [explicit] "Factors such as students' linguistic backgrounds, familiarity with task types, and individual writing strategies were not explicitly addressed... Future investigations could also consider how LLMs perform across varying learner profiles."
- Why unresolved: The study did not examine whether LLM reliability varies for subgroups of writers.
- What evidence would resolve it: Moderation analyses examining whether G-coefficients differ across proficiency levels, native language backgrounds, or prior experience with task types.

## Limitations
- Small sample size (30 students) may produce unstable variance component estimates
- Few-shot prompting approach lacks comparison with fine-tuning methods
- Proportional weighting scheme may not optimize composite reliability
- Task-specific prompt design may limit generalization to other assessment contexts

## Confidence

- **High Confidence**: Hybrid human-AI scoring reliably improves assessment reliability when at least one human rater is included, and story narration tasks produce higher AI reliability than email response tasks. These findings are supported by clear variance decomposition patterns and align with established G-theory principles.
- **Medium Confidence**: Scoring dimension objectivity predicts AI reliability (TC > delivery > language use). While the variance component data supports this ordering, the corpus lacks direct validation studies comparing these dimensions across multiple AI systems.
- **Low Confidence**: The specific rater weighting thresholds (e.g., why "1 & 2" performs worse than "1 & 1") are not fully explained by the variance decomposition. The interaction between weighting schemes and variance components requires further empirical validation.

## Next Checks

1. **Weighting Scheme Optimization**: Compare proportional weighting against inverse-variance weighting for human-AI composites to determine if reliability can be improved by accounting for AI raters' higher interaction variance.

2. **Cross-Domain Generalization Test**: Train AI raters on one task type (SN or ER) and evaluate their performance on the other to quantify task-specific prompt dependencies and inform whether universal prompt engineering is feasible.

3. **Rater Configuration Frontier Mapping**: Systematically test all combinations of 0-2 human raters with 1-4 AI raters to identify the exact reliability gains from each additional rater, clarifying the diminishing returns threshold for both human and AI raters.