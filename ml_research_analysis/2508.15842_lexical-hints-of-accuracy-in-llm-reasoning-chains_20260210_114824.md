---
ver: rpa2
title: Lexical Hints of Accuracy in LLM Reasoning Chains
arxiv_id: '2508.15842'
source_url: https://arxiv.org/abs/2508.15842
tags:
- accuracy
- arxiv
- reasoning
- words
- omni-math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether lexical cues in LLM Chain-of-Thought
  (CoT) traces can reliably predict reasoning accuracy. The authors analyze three
  feature classes: CoT length, intra-CoT sentiment volatility, and uncertainty-related
  lexical markers.'
---

# Lexical Hints of Accuracy in LLM Reasoning Chains

## Quick Facts
- arXiv ID: 2508.15842
- Source URL: https://arxiv.org/abs/2508.15842
- Reference count: 40
- Primary result: Uncertainty-related words in CoT traces are strongest predictors of incorrect responses, achieving MCC scores of 0.215-0.305

## Executive Summary
This paper investigates whether lexical cues in LLM Chain-of-Thought (CoT) traces can predict reasoning accuracy. Analyzing CoT length, sentiment volatility, and uncertainty markers across DeepSeek-R1 and Claude 3.7 Sonnet on Omni-MATH and Humanity's Last Exam benchmarks, the authors find that uncertainty-related words (e.g., "guess," "stuck," "hard") are the most reliable indicators of incorrect responses. CoT length is informative only on intermediate-difficulty tasks, while sentiment volatility provides a weaker but complementary signal. A lightweight rule using the top five uncertainty words outperforms self-reported confidence, suggesting practical applications for post-hoc calibration without requiring model weights.

## Method Summary
The study collects CoT traces from two models on two benchmarks, then extracts three feature classes: CoT length, sentiment volatility (measured via o3-mini), and presence of 25 predefined hedging/uncertainty words. Features are used to train a feed-forward neural network (2 hidden layers: 32→16 units) with binary cross-entropy loss and class weighting. A simpler rule-based classifier uses the top five "harmful" uncertainty words. Performance is evaluated using Matthews Correlation Coefficient on balanced test sets, compared against self-reported confidence baselines.

## Key Results
- Uncertainty-related words in CoT are the strongest predictors of incorrect responses (odds reduction up to 40%)
- CoT length correlates with accuracy only on intermediate-difficulty benchmarks (Omni-MATH, ~70% accuracy)
- A 5-word heuristic rule achieves MCC 0.215 (HLE) and 0.305 (Omni-MATH), outperforming self-reported confidence
- Sentiment volatility shows weak but complementary signal only on intermediate-difficulty tasks

## Why This Works (Mechanism)

### Mechanism 1: Lexical Uncertainty Markers Predict Incorrect Responses
When models encounter problems near or beyond their capability boundary, they linguistically signal cognitive difficulty using hedging expressions and uncertainty markers. Words like "guess," "stuck," and "hard" correlate with reduced accuracy odds by up to 40%. However, CoT faithfulness research (Chen et al. 2025) suggests these markers may reflect stylistic artifacts rather than true internal states.

### Mechanism 2: CoT Length Signal Depends on Task Difficulty Distribution
Longer reasoning chains indicate struggle with harder subproblems only within a model's demonstrated capabilities. On Omni-MATH (~70% accuracy), accuracy declines with CoT length, but on HLE (~9% accuracy), the signal disappears as all responses are similarly likely to fail. This suggests a parabolic or context-dependent relationship rather than monotonic.

### Mechanism 3: Sentiment Volatility Provides Weak Complementary Calibration
Intra-CoT sentiment shifts show a parabolic relationship with accuracy on intermediate-difficulty tasks. CoTs with consistent or slightly uplifting sentiment trajectories correlate with higher accuracy, while large volatility may indicate backtracking or unstable reasoning. This signal is weak and highly sensitive to the evaluator used (o3-mini).

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**: Explicit reasoning trace generated before the final answer, not post-hoc justification. Why needed: The entire analysis depends on understanding CoT as a pre-answer reasoning trace. Quick check: Can you explain why CoT length might correlate with difficulty but not always with correctness?

- **Calibration Error**: Mismatch between reported confidence and actual accuracy. Why needed: The paper's motivation is that self-reported confidence is poorly calibrated (81-88% error on HLE). Quick check: If a model reports 90% confidence but achieves only 10% accuracy, what is the calibration error?

- **Hedging Language**: Words expressing uncertainty (modal verbs, uncertainty adverbs). Why needed: The strongest signal comes from detecting hedging words that indicate lower confidence. Quick check: Which words in "I might possibly guess the answer is 42" would count as hedging markers?

## Architecture Onboarding

- **Component map**: CoT Collection -> Feature Extraction -> Classification -> Evaluation
- **Critical path**: Define hedging lexicon and harmful word list -> Extract CoT from target model -> Check for top 5 harmful words as lightweight heuristic -> Train neural classifier on 25-word features if higher precision needed
- **Design tradeoffs**: Lightweight rule (5 words) vs. neural classifier: Rule achieves MCC 0.215-0.305; neural achieves MCC 0.229-0.354 with more complexity. Cross-benchmark generalization: Word-based features generalize better than CoT length. Language limitation: Current lexicon is English-only.
- **Failure signatures**: High false positive rate on math-heavy tasks; signal degradation if models suppress hedging language; no signal on extremely easy or hard benchmarks.
- **First 3 experiments**: 1) Baseline validation: Replicate 5-word heuristic on your target model/benchmark. 2) Domain transfer test: Train classifier on Omni-MATH, test on your domain. 3) Ablation study: Compare word-only vs. word+hedging vs. word+length features.

## Open Questions the Paper Calls Out

### Open Question 1
Do lexical uncertainty markers maintain their predictive power in non-English languages with richer morphology or different hedging conventions? The current analysis relies on an English-focused lexicon, and the authors note findings might not hold up in languages with richer morphology.

### Open Question 2
Does suppression of hedging and uncertainty language in Chain-of-Thought traces causally improve reasoning accuracy, or does it merely mask the model's internal confusion? The paper establishes correlation but cannot determine if markers are symptoms or contributors to failure.

### Open Question 3
How does Reinforcement Learning from Human Feedback (RLHF) alter the expression and reliability of lexical uncertainty signals compared to Supervised Fine-Tuning (SFT)? The observed optimism bias and predictive power of uncertainty words may be artifacts of alignment training.

## Limitations

- Lexical signal generalization: Fixed lexicon of 25 uncertainty words primarily English-specific; cross-linguistic transfer untested
- Causal vs. correlational interpretation: Relationship could be spurious due to training data bias or stylistic artifacts
- Benchmark specificity: Findings tied to specific benchmarks with distinct difficulty profiles; may not transfer to other domains

## Confidence

- **High Confidence**: Uncertainty-related words predict incorrect responses (MCC 0.215–0.354 across models/benchmarks)
- **Medium Confidence**: CoT length signal depends on task difficulty (supported by comparative accuracy trends)
- **Low Confidence**: Sentiment volatility provides weak complementary signal (weak/no corpus evidence; highly sensitive to evaluator choice)

## Next Checks

1. **Cross-Domain Transfer Test**: Apply the 5-word heuristic to a non-math domain (e.g., commonsense QA or code generation) and measure MCC drop relative to Omni-MATH/HLE baselines.

2. **Adversarial Prompting Stress Test**: Systematically prompt models to suppress hedging language and measure degradation in uncertainty word signal accuracy.

3. **Evaluator Calibration Check**: Replace o3-mini with a different LLM or human annotators for sentiment volatility scoring; compare MCC stability across evaluators.