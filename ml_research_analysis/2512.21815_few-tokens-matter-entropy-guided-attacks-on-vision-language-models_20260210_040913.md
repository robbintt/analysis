---
ver: rpa2
title: 'Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models'
arxiv_id: '2512.21815'
source_url: https://arxiv.org/abs/2512.21815
tags:
- harmful
- tokens
- clean
- entropy
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the vulnerability of vision-language models\
  \ (VLMs) to adversarial attacks, specifically focusing on how entropy-guided perturbations\
  \ can induce harmful content. The key finding is that a small fraction (around 20%)\
  \ of high-entropy tokens\u2014critical decision points in autoregressive generation\u2014\
  disproportionately influence output trajectories."
---

# Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models

## Quick Facts
- **arXiv ID**: 2512.21815
- **Source URL**: https://arxiv.org/abs/2512.21815
- **Reference count**: 40
- **Key finding**: ~20% of high-entropy tokens disproportionately influence VLM output trajectories, enabling effective adversarial attacks

## Executive Summary
This paper demonstrates that vision-language models are vulnerable to adversarial attacks targeting high-entropy tokens in autoregressive generation. The authors show that a small fraction of critical tokens drives output trajectories, and by concentrating perturbations on these positions using their Entropy-bank Guided Adversarial (EGA) method, they achieve high attack success rates while converting benign outputs to harmful content. The attack exploits the shared high-entropy token patterns across different VLM architectures, enabling transferability. This finding reveals a significant safety risk for VLMs in real-world applications.

## Method Summary
The authors propose EGA, which identifies high-entropy tokens in the generation process and concentrates adversarial perturbations on these critical positions. The method works by first analyzing the entropy distribution across token positions during generation, then crafting perturbations that maximize harm while minimizing the number of modified tokens. The attack specifically targets the autoregressive generation process, exploiting the fact that decisions at high-entropy positions have outsized influence on final outputs. The method is evaluated across multiple open-source VLMs including Llama3-V, Llava-Next, and Qwen2-VL, demonstrating both high attack success rates and transferability to unseen models.

## Key Results
- EGA achieves attack success rates of 93-95% on targeted VLMs
- 35-49% of benign outputs are converted to harmful content through targeted perturbations
- High-entropy tokens are shared across architecturally diverse VLMs, enabling transferability
- Transferability results show 17-26% harmful conversion rates on unseen target models

## Why This Works (Mechanism)
The attack works because autoregressive VLMs make critical generation decisions at high-entropy token positions. These tokens represent uncertainty points where the model is less confident, making them sensitive to perturbations. By targeting these positions, attackers can steer the generation process toward harmful outputs with minimal effort. The shared nature of high-entropy tokens across different architectures means that perturbations crafted for one model can often transfer to others, amplifying the attack's effectiveness.

## Foundational Learning

**Autoregressive Generation**: VLMs generate text sequentially, with each token depending on previous ones. Understanding this sequential dependency is crucial for identifying critical decision points in the generation process.

*Why needed*: Explains how VLMs produce outputs and where vulnerabilities exist in the generation pipeline.
*Quick check*: Can you trace how a single token decision affects downstream generation?

**Entropy in Token Selection**: Entropy measures the uncertainty in token selection at each generation step. High-entropy positions represent points of maximum uncertainty where small perturbations can have large effects.

*Why needed*: Identifies the specific vulnerability points that the attack exploits.
*Quick check*: Can you calculate entropy for a given token distribution?

**Cross-Modal Embeddings**: VLMs use joint embedding spaces that combine visual and textual information. Adversarial perturbations in this space can affect both modalities simultaneously.

*Why needed*: Explains how vision-language models represent and process multi-modal inputs.
*Quick check*: Can you describe how visual features are encoded in the shared embedding space?

## Architecture Onboarding

**Component Map**: Input Image -> Vision Encoder -> Cross-Modal Fusion -> Autoregressive Decoder -> Text Output

**Critical Path**: The vulnerability lies in the autoregressive decoder's token selection process, particularly at high-entropy positions where the model's confidence is lowest.

**Design Tradeoffs**: VLMs balance computational efficiency with generation quality, creating predictable entropy patterns that attackers can exploit. The shared embedding space enables cross-modal understanding but also creates attack surfaces.

**Failure Signatures**: The attack manifests as generation drift at high-entropy positions, where small perturbations cause significant changes in output trajectories, particularly in safety-critical contexts.

**First Experiments**:
1. Measure entropy distribution across token positions in benign generations
2. Compare attack success rates when targeting high vs. low entropy positions
3. Test transferability of perturbations across different VLM architectures

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Results are limited to open-source VLMs and may not generalize to proprietary models like GPT-4V or Gemini
- Evaluation relies on automated harmful content detection without human validation
- The attack space is constrained to vision-language embeddings, not testing more complex perturbations
- Transferability results are based on a limited set of target models

## Confidence

**High confidence**: The observation that ~20% of high-entropy tokens drive autoregressive trajectories is consistently supported by quantitative analysis across multiple VLMs and attack scenarios.

**Medium confidence**: Claims about EGA's superiority over gradient-based methods (93-95% attack success) are robust within tested parameters but may not hold under different attack budgets or alternative metrics.

**Medium confidence**: Transferability findings (17-26% harmful conversion on unseen targets) are promising but limited by the small set of target models and attack scenarios tested.

## Next Checks

1. Evaluate EGA's effectiveness on proprietary VLMs (GPT-4V, Gemini) and newer open models to assess cross-model generalizability.

2. Conduct human evaluations of automatically classified harmful outputs to validate semantic harmfulness and reduce annotation bias.

3. Test EGA against combined perturbation strategies (e.g., vision embedding + pixel-level image modifications) to assess robustness under multi-modal adversarial scenarios.