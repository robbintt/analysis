---
ver: rpa2
title: Predicting the Order of Upcoming Tokens Improves Language Modeling
arxiv_id: '2508.19228'
source_url: https://arxiv.org/abs/2508.19228
tags:
- loss
- token
- training
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Token Order Prediction (TOP) as a more efficient
  auxiliary training objective for language models compared to Multi-Token Prediction
  (MTP). TOP predicts the order of upcoming tokens using a learning-to-rank loss rather
  than exactly predicting future tokens, requiring only a single additional unembedding
  layer instead of multiple transformer layers.
---

# Predicting the Order of Upcoming Tokens Improves Language Modeling

## Quick Facts
- **arXiv ID**: 2508.19228
- **Source URL**: https://arxiv.org/abs/2508.19228
- **Reference count**: 3
- **Primary result**: Token Order Prediction (TOP) achieves better efficiency and performance than Multi-Token Prediction (MTP) by predicting token order rather than exact tokens

## Executive Summary
This paper introduces Token Order Prediction (TOP) as an auxiliary training objective for language models that predicts the order of upcoming tokens rather than predicting the tokens themselves. The method uses a learning-to-rank loss and requires only a single additional unembedding layer, making it more efficient than Multi-Token Prediction (MTP) which requires multiple transformer layers. The authors pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives, evaluating them on eight standard NLP benchmarks. Results show that TOP consistently outperforms both NTP and MTP baselines, with the 7B TOP model surpassing both baselines on general tasks. The performance gains increase with model scale, suggesting TOP's potential value for larger language models.

## Method Summary
The Token Order Prediction (TOP) method reformulates the auxiliary training objective from predicting exact future tokens to predicting their relative order. Instead of the model generating multiple candidate tokens for future positions (as in MTP), TOP uses a learning-to-rank loss to predict which of several candidate tokens will appear first in the upcoming sequence. This requires only a single unembedding layer rather than multiple transformer layers, significantly reducing computational overhead. The approach maintains the benefits of looking ahead multiple tokens while being more parameter-efficient and potentially easier to optimize.

## Key Results
- TOP outperforms both NTP and MTP baselines across all tested model sizes (340M, 1.8B, and 7B parameters)
- The 7B TOP model surpasses both 7B NTP and MTP baselines on general NLP tasks
- TOP demonstrates improved performance as parameter count increases, suggesting better scaling properties
- TOP achieves superior efficiency by requiring only one unembedding layer versus multiple transformer layers for MTP

## Why This Works (Mechanism)
TOP works by shifting the prediction task from exact token generation to relative ordering. This change simplifies the learning problem - the model only needs to learn which token comes first rather than generating the exact token itself. The learning-to-rank loss is generally easier to optimize than exact generation losses, and the single unembedding layer reduces computational overhead while maintaining the benefits of looking ahead multiple tokens. This combination of reduced complexity and maintained predictive power leads to better performance and efficiency.

## Foundational Learning
- **Learning-to-rank loss**: A loss function that trains models to correctly order items rather than predict their exact values; needed because TOP predicts token order instead of tokens themselves, and quick check is whether ranking accuracy correlates with downstream task performance
- **Unembedding layer**: The inverse operation of an embedding layer, mapping hidden states back to vocabulary space; needed for decoding predictions into tokens, and quick check is whether the single unembedding layer provides sufficient capacity for the ranking task
- **Auxiliary training objectives**: Additional training targets beyond the primary next-token prediction; needed to improve model capabilities without changing the main architecture, and quick check is whether TOP's gains come from better representation learning or task-specific adaptation
- **Transformer efficiency**: The relationship between architectural depth and computational cost; needed to understand TOP's efficiency advantage, and quick check is whether the single unembedding layer truly captures all necessary information