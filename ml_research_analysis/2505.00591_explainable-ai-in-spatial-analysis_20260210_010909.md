---
ver: rpa2
title: Explainable AI in Spatial Analysis
arxiv_id: '2505.00591'
source_url: https://arxiv.org/abs/2505.00591
tags:
- spatial
- data
- feature
- values
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter presents a comprehensive framework for integrating
  eXplainable AI (XAI) into spatial analysis, addressing the black-box nature of machine
  learning models in geospatial applications. The core method, GeoShapley, extends
  Shapley value-based approaches to capture both intrinsic location effects and spatially
  varying interactions between features and geographic context.
---

# Explainable AI in Spatial Analysis

## Quick Facts
- arXiv ID: 2505.00591
- Source URL: https://arxiv.org/abs/2505.00591
- Reference count: 0
- Primary result: GeoShapley achieves 0.933 out-of-sample R² in county-level voting prediction while providing interpretable spatial explanations

## Executive Summary
This chapter introduces GeoShapley, a framework that extends Shapley value-based explainability to spatial analysis by treating geographic coordinates as joint features. The method decomposes machine learning predictions into intrinsic location effects and spatially varying interactions, enabling interpretation of black-box models in geospatial contexts. Applied to 2020 US Presidential election data, GeoShapley achieves predictive performance matching traditional methods while providing spatially explicit explanations that separate location-invariant primary effects from location-varying contextual influences.

## Method Summary
The method trains a machine learning model (XGBoost via FLAML AutoML) on spatial tabular data including coordinates, then applies GeoShapley to calculate Shapley values that capture both feature effects and spatial context. The framework decomposes predictions into four components: global base value, intrinsic location effect, global feature effects, and spatially varying interaction effects. To create interpretable spatial coefficients comparable to traditional methods, the combined primary and interaction effects are smoothed using GWR regression. The approach maintains local accuracy while providing spatially explicit explanations through maps of intrinsic location effects and spatially varying coefficients.

## Key Results
- XGBoost model achieves 0.933 out-of-sample R² for county-level Democratic vote share prediction
- GeoShapley successfully separates intrinsic location effects from spatially varying feature interactions
- Spatially varying coefficients produced via GWR smoothing closely match patterns from Multi-scale Geographically Weighted Regression
- Percentage of Black residents and educational attainment show strongest contributions to voting patterns
- Method reveals non-linear relationships between socio-demographic variables and voting behavior

## Why This Works (Mechanism)

### Mechanism 1: Additive Decomposition of Spatial Heterogeneity
- **Claim:** GeoShapley isolates spatial effects from feature effects in machine learning predictions, mimicking the output structure of spatial regression models.
- **Mechanism:** The method extends Shapley values by treating geographic coordinates as a unified "player" in the game theory model. It decomposes a prediction into four additive components: a global base value, an intrinsic location effect, global feature effects, and spatially varying interaction effects.
- **Core assumption:** The underlying machine learning model has successfully learned location-dependent patterns during training.
- **Break condition:** If the ML model ignores coordinate inputs, the spatial terms remain near zero, reducing results to standard non-spatial SHAP values.

### Mechanism 2: Bridging Inference via Spatial Smoothing
- **Claim:** GeoShapley values can be converted into interpretable spatial coefficients comparable to those from Geographically Weighted Regression (GWR).
- **Mechanism:** While Shapley values represent marginal contributions, the method applies a spatial smoother (specifically GWR) to regress the combined GeoShapley effects against feature values. This transforms local contribution values into "spatially varying coefficients."
- **Core assumption:** The relationship between the Shapley contribution and the feature value is locally linear enough to be meaningfully summarized by a smoothed coefficient.
- **Break condition:** If the relationship between a feature and the prediction is highly non-linear or non-monotonic, a single smoothed coefficient may misrepresent the local effect.

### Mechanism 3: Model-Agnostic Local Fidelity
- **Claim:** Complex "black-box" models can be explained locally without sacrificing their non-linear predictive power.
- **Mechanism:** SHAP calculates the marginal contribution of a feature by averaging its impact across all possible coalitions of features, satisfying the property of "local accuracy."
- **Core assumption:** The model's internal logic, while complex, is consistent and deterministic.
- **Break condition:** If features are perfectly collinear, the Shapley value distribution between them becomes unstable or arbitrary.

## Foundational Learning

- **Concept: Shapley Values (Game Theory)**
  - **Why needed here:** This is the mathematical core of the chapter. Understanding that it allocates "payout" (prediction) fairly among "players" (features) based on marginal contribution is required to interpret the results.
  - **Quick check question:** If a model predicts house prices using only 'Size' and 'Location', does a Shapley value of $50k for 'Size' mean the price increases by $50k for every square foot?
  - *Answer: No, it means 'Size' contributed $50k to the difference between this specific prediction and the average prediction.*

- **Concept: Spatial Heterogeneity vs. Spatial Autocorrelation**
  - **Why needed here:** The chapter distinguishes between global non-linear effects (heterogeneity in features) and spatially varying relationships (spatial heterogeneity). You must distinguish "why values are high" from "why relationships vary."
  - **Quick check question:** Does a high intrinsic location effect imply that nearby features are similar, or that there is a unique "contextual" boost at that location?
  - *Answer: It implies a unique contextual boost (spatial fixed effect) unexplained by other variables.*

- **Concept: Partial Dependence Plots (PDP)**
  - **Why needed here:** Page 7-8 uses PDPs to visualize "primary effects." You need to know that PDPs show the average marginal effect of a feature to understand the global non-linear behavior.
  - **Quick check question:** If a PDP for 'Income' vs. 'Voting' is flat initially and then rises steeply, what does that tell us about the model's learned behavior?
  - *Answer: It indicates a threshold effect or non-linearity where voting patterns change significantly only after income reaches a certain level.*

## Architecture Onboarding

- **Component map:** Tabular spatial data (features + coordinates) -> XGBoost (FLAML) -> GeoShapley values (geoshapley package) -> GWR smoothing (mgwr) -> Visualization
- **Critical path:** Model Validation -> GeoShapley Calculation -> Visualization
- **Design tradeoffs:** Use ML+GeoShapley for prediction accuracy and complex non-linearities; use MGWR for strict statistical inference. Interpretation accuracy requires careful qualification of GWR smoothing approach.
- **Failure signatures:** Hallucinated Spatial Effects from overfitting; Misinterpreting Shapley values as regression coefficients; Unstable values with highly correlated features.
- **First 3 experiments:**
  1. Reproduce the XGBoost voting model to verify 0.933 R² and intrinsic location map.
  2. Train identical models with and without coordinate inputs to quantify spatial context contribution.
  3. Introduce synthetic highly correlated features to test stability of GeoShapley value distribution.

## Open Questions the Paper Calls Out

- **Question 1:** How can spatial contributions from GeoShapley be visualized using formal methods that preserve game-theoretic properties, rather than relying on exploratory spatial smoothers like GWR?
  - **Basis:** The authors state that using GWR as a spatial smoother is "exploratory and deviate from the true GeoShapley values which adhere to game-theory properties."
  - **Why unresolved:** Current visualization approaches sacrifice theoretical guarantees for interpretability.
  - **What evidence would resolve it:** Development of a visualization framework that maintains local accuracy, missingness, and consistency properties while producing spatially coherent coefficient maps.

- **Question 2:** How can causal Shapley value methods be extended to capture spatial causality and distinguish direct from indirect spatial effects in machine learning models?
  - **Basis:** The authors note that "extending and applying these causal ML methods to geospatial tasks would be a valuable direction for future research."
  - **Why unresolved:** Standard Shapley values capture marginal contributions but do not distinguish causal pathways.
  - **What evidence would resolve it:** A spatially-aware causal Shapley framework validated on simulated spatial data with known causal structures.

- **Question 3:** What architectural modifications and training strategies are needed to develop robust and transferable GeoAI models specifically designed for spatial tabular data regression and classification tasks?
  - **Basis:** The authors state that "GeoAI models for traditional tabular data regression and classification problems are still in their infancy."
  - **Why unresolved:** Most GeoAI advances focus on image-based tasks; spatial tabular data presents unique challenges.
  - **What evidence would resolve it:** Benchmarks demonstrating that purpose-built spatial tabular GeoAI models outperform standard ML models while maintaining interpretability.

## Limitations

- GeoShapley performance critically depends on the underlying ML model's ability to learn spatial patterns
- Conversion of GeoShapley values to spatially varying coefficients via GWR smoothing is exploratory rather than strictly derived from game theory
- Computational complexity of exact Shapley value calculation requires kernel approximations, which may introduce estimation variance
- Stability of GeoShapley values under correlated features is not empirically tested

## Confidence

- **High Confidence:** Predictive performance claim (R² = 0.933) is well-supported by empirical results and matches MGWR benchmark. Decomposition framework is mathematically sound.
- **Medium Confidence:** Claim that GeoShapley can bridge ML and traditional spatial statistics is supported but relies on assumptions about GWR smoothing producing meaningful coefficients.
- **Low Confidence:** Stability of GeoShapley values under correlated features is not empirically tested.

## Next Checks

1. **Ablation Test:** Train identical models with and without coordinate inputs to quantify the marginal contribution of spatial context to predictive accuracy.
2. **Correlation Stress Test:** Introduce synthetic highly correlated features to evaluate stability of GeoShapley value distribution between correlated predictors.
3. **Out-of-Sample Spatial Transfer:** Test whether GeoShapley values generalize to new geographic regions not seen during training, assessing true spatial extrapolation capability.