---
ver: rpa2
title: Learning with the $p$-adics
arxiv_id: '2512.22692'
source_url: https://arxiv.org/abs/2512.22692
tags:
- p-adic
- which
- have
- learning
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the potential of p-adic numbers as an alternative
  to real numbers for machine learning. The key contributions are: p-adic classification:
  The paper develops linear and nonlinear binary classifiers in the p-adic domain,
  showing that some problems which cannot be solved by real linear classifiers (such
  as XOR and parity problems) are easy for p-adic linear classifiers, and vice versa.'
---

# Learning with the $p$-adics

## Quick Facts
- arXiv ID: 2512.22692
- Source URL: https://arxiv.org/abs/2512.22692
- Reference count: 14
- Primary result: p-adic numbers offer an alternative mathematical framework for machine learning that can solve certain problems (like XOR) that real-valued classifiers cannot, while having complementary limitations

## Executive Summary
This paper explores the potential of p-adic numbers as an alternative to real numbers for machine learning applications. The author develops fundamental machine learning operations—classification, regression, and representation learning—in the p-adic domain, demonstrating that p-adic classifiers can solve problems that real-valued linear classifiers cannot (such as XOR and parity problems), while having complementary limitations. The paper establishes theoretical foundations for p-adic learning, including properties of p-adic classifiers and regressors, and demonstrates applications to semantic networks and Boolean function learning.

## Method Summary
The paper develops p-adic machine learning by reformulating fundamental operations in the p-adic number system. For classification, it introduces linear and nonlinear binary classifiers using p-adic distance metrics, showing that p-adic balls (rather than real intervals) define decision regions. For regression, it formulates linear regression where optimal solutions pass through exactly d+1 training points. The hierarchical structure of p-adic numbers is leveraged for representation learning, enabling compact representations of semantic networks that cannot be achieved with real numbers. The theoretical analysis establishes when p-adic classifiers can solve specific problems like Boolean functions, congruence problems, and counting problems.

## Key Results
- p-adic linear classifiers can solve XOR and parity problems that real linear classifiers cannot
- Optimal p-adic regressors pass through exactly d+1 training points, unlike real-valued regression
- p-adic representations enable compact encoding of semantic networks that cannot be represented with real numbers
- The paper establishes fundamental properties characterizing when problems are linearly separable in the p-adic domain

## Why This Works (Mechanism)
p-adic numbers have a hierarchical, ultrametric structure fundamentally different from real numbers. While real numbers use absolute value for distance, p-adic numbers use p-adic absolute value, where numbers are "close" if their difference is divisible by high powers of p. This creates a tree-like structure where numbers are organized into nested balls of decreasing radius. This ultrametric property enables certain geometric separability properties that don't exist in Euclidean space—specifically, it allows linear separation of problems that require nonlinear boundaries in real space (like XOR). The hierarchical nature also enables efficient hierarchical representations, as information can be encoded at different levels of granularity simultaneously.

## Foundational Learning

1. **p-adic numbers and p-adic absolute value** - Why needed: Fundamental to understanding the mathematical framework; Quick check: Verify that |x|_p = p^(-v_p(x)) where v_p(x) is the p-adic valuation

2. **Ultrametric spaces and non-Archimedean geometry** - Why needed: Explains why p-adic distance behaves differently than Euclidean distance; Quick check: Confirm that the strong triangle inequality |x+y|_p ≤ max(|x|_p, |y|_p) holds

3. **Balls in ultrametric spaces** - Why needed: Decision boundaries in p-adic classifiers are defined by p-adic balls; Quick check: Verify that any two balls are either disjoint or one contains the other

4. **Congruence and modular arithmetic** - Why needed: p-adic numbers naturally encode modular relationships; Quick check: Confirm that two numbers are in the same p-adic ball of radius p^(-k) iff they are congruent mod p^k

5. **Quillian semantic networks** - Why needed: Example application demonstrating representation learning capabilities; Quick check: Verify that semantic similarity can be encoded through shared hierarchical paths

## Architecture Onboarding

**Component map**: Input -> p-adic preprocessing -> Classifier/Regressor -> p-adic decision region -> Output

**Critical path**: Data → p-adic encoding → Distance computation in p-adic space → Decision boundary application → Prediction

**Design tradeoffs**: p-adic vs real arithmetic (complementary capabilities), computational complexity of p-adic operations vs benefits, choice of prime p affecting granularity

**Failure signatures**: Inability to separate problems requiring Euclidean geometry, performance degradation when problem structure doesn't align with p-adic hierarchy, numerical precision issues with large p-adic expansions

**First experiments**:
1. Implement p-adic linear classifier on XOR problem and compare with real linear classifier
2. Test p-adic regressor on synthetic data where optimal solution passes through d+1 points
3. Encode a simple semantic network using p-adic representations and verify compactness

## Open Questions the Paper Calls Out
The paper identifies several open problems: developing gradient-based optimization methods for p-adic learning, exploring multi-layer p-adic networks, investigating adelic predictors that use all primes simultaneously, and extending p-adic learning to more complex architectures and real-world datasets.

## Limitations
- Theoretical analysis primarily focuses on binary classification with limited exploration of multi-class scenarios
- Limited empirical validation on practical machine learning benchmarks and real-world datasets
- Computational complexity and implementation efficiency of p-adic operations compared to standard approaches remains unclear
- Assumes familiarity with p-adic number theory, potentially limiting accessibility

## Confidence

*High confidence*: Theoretical foundations of p-adic classifiers and regressors (Propositions 2-7)
*Medium confidence*: Claims about problem-solving capabilities (XOR, parity) - limited empirical validation
*Low confidence*: Practical utility claims and computational efficiency - insufficient real-world testing

## Next Checks

1. Implement and benchmark p-adic classifiers on standard machine learning datasets (MNIST, CIFAR-10) to compare performance against real-valued counterparts.

2. Develop and test gradient-based optimization methods for p-adic neural networks to enable end-to-end learning.

3. Create a comprehensive computational study comparing p-adic and real-valued operations, measuring both accuracy and runtime efficiency across various hardware platforms.