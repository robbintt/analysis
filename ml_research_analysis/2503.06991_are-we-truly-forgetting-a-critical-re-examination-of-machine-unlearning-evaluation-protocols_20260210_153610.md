---
ver: rpa2
title: Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation
  Protocols
arxiv_id: '2503.06991'
source_url: https://arxiv.org/abs/2503.06991
tags:
- unlearning
- evaluation
- forgetting
- classes
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically re-examines machine unlearning evaluation
  protocols, arguing that existing logit-based metrics under small-scale scenarios
  can lead to a false sense of security. The authors propose a unified benchmark that
  incorporates representation-based metrics, such as Centered Kernel Alignment (CKA)
  and k-Nearest Neighbors (k-NN), to evaluate the feature representations of unlearned
  models under large-scale scenarios like ImageNet-1K.
---

# Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation Protocols

## Quick Facts
- **arXiv ID:** 2503.06991
- **Source URL:** https://arxiv.org/abs/2503.06991
- **Reference count:** 18
- **Primary result:** Existing logit-based metrics for machine unlearning can create false security, as state-of-the-art methods often only modify classifiers while maintaining representational similarity to original models.

## Executive Summary
This paper critically re-examines machine unlearning evaluation protocols, arguing that existing logit-based metrics under small-scale scenarios can lead to a false sense of security. The authors propose a unified benchmark that incorporates representation-based metrics, such as Centered Kernel Alignment (CKA) and k-Nearest Neighbors (k-NN), to evaluate the feature representations of unlearned models under large-scale scenarios like ImageNet-1K. Their analysis reveals that state-of-the-art unlearning approaches often only modify the classifier while maintaining representational similarity to the original model, failing to truly eliminate targeted data from the model's representation perspective. Additionally, they introduce a novel evaluation scenario, Top Class-wise Forgetting, where forgetting classes are semantically similar to downstream task classes, requiring substantial divergence of feature representations.

## Method Summary
The authors propose a unified benchmark that combines logit-based and representation-based metrics to evaluate machine unlearning effectiveness. They introduce CKA and k-NN accuracy as representation-based metrics to measure the similarity between feature representations of unlearned and original models. The benchmark also includes a novel Top Class-wise Forgetting scenario where semantically similar classes to downstream tasks are forgotten, creating a more rigorous test. The Harmonic Mean of Logit-based and Representation-based scores (H-LR) provides a unified assessment metric.

## Key Results
- Logit-based metrics (AGL) can show high success while representation-based metrics (CKA, k-NN) reveal models maintain representational similarity to original models
- State-of-the-art unlearning methods (GA, SCRUB, PL) primarily modify classifiers while preserving feature representations
- Top Class-wise Forgetting scenario exposes significant limitations in current methods when semantically similar classes are forgotten
- Representation-based evaluation reveals catastrophic failure in large-scale settings (ImageNet-1K) despite good logit-based performance on small datasets (CIFAR-10)

## Why This Works (Mechanism)
The paper demonstrates that machine unlearning methods can achieve high logit-based success by only modifying the final classifier layer while leaving the feature extractor (encoder/backbone) largely unchanged. This creates a false sense of security because the model still retains the representational knowledge of the forgotten data, which can be exploited through feature-space attacks or by retraining the classifier. Representation-based metrics like CKA and k-NN accuracy directly measure this representational persistence, revealing the true effectiveness of unlearning methods.

## Foundational Learning
- **Logit-based metrics (FA, RA, AGL):** Measure forgetting by comparing classifier outputs between original and unlearned models. Needed to establish baseline performance; quick check: AGL close to 1 indicates high forgetting.
- **Representation-based metrics (CKA, k-NN):** Measure similarity of feature representations between models. Needed because classifiers can be modified independently of feature extractors; quick check: High CKA between θu and θo indicates representational persistence.
- **Centered Kernel Alignment (CKA):** A similarity metric for comparing neural network representations. Needed to quantify representational similarity; quick check: CKA values range from 0 (completely different) to 1 (identical representations).
- **Harmonic Mean (H-LR):** Combines logit-based and representation-based scores into unified metric. Needed to prevent methods from optimizing only one aspect; quick check: H-LR penalizes methods excelling in only one metric type.
- **Top Class-wise Forgetting:** Evaluates unlearning when forgotten classes are semantically similar to downstream tasks. Needed to create more challenging, realistic scenarios; quick check: Should show lower performance than random class forgetting.
- **Random Class-wise Forgetting:** Standard evaluation where classes are forgotten randomly. Needed as baseline comparison; quick check: Should be easier than top class-wise forgetting.

## Architecture Onboarding

- **Component map:**
    - Original Model (θo) -> Retrained Model (θr) -> Unlearned Model (θu)
    - Encoder (Backbone) -> Classifier (Output Layer/Head)
    - Logit-based Metrics (FA, RA, AGL) -> Representation-based Metrics (CKA, k-NN, AGR) -> H-LR Score

- **Critical path:**
    1. Start with a pre-trained model (θo) and define a forget set (Df)
    2. Apply an unlearning algorithm (e.g., GA, SCRUB, PL) to produce θu
    3. **Do not** rely solely on logit-based metrics (AGL)
    4. **Must** extract feature representations from the encoder of θu on diverse downstream datasets
    5. Compute representation-based metrics: CKA to measure similarity to θr (vs. θo) and k-NN accuracy to test transferability
    6. Combine into AGR and the final H-LR score for a truthful assessment

- **Design tradeoffs:**
    - **Logit-based vs. Representation-based Evaluation:** Logit-based metrics (AGL) are fast and intuitive but can be fooled by classifier-only modifications (shortcut learning). Representation-based metrics (AGR, H-LR) are more computationally expensive (require CKA, k-NN on extra datasets) but are necessary for a truthful evaluation in large-scale scenarios.
    - **Using Retain Set (Dr) vs. Not:** Methods using Dr (e.g., SCRUB, DUCK) are more computationally intensive but tend to preserve retain performance better. Methods without Dr (e.g., GA, PL) are faster but risk over-forgetting or representation collapse.
    - **Random vs. Top Class-wise Forgetting:** Random is standard but can mask representational persistence. Top Class-wise is more rigorous and semantically meaningful but requires defining downstream task similarity.

- **Failure signatures:**
    - **High Logit Success, Low Representational Change:** An unlearned model (θu) shows high AGL (close to θr) but low CKA similarity to θr and high CKA similarity to the original model (θo). This is the "classifier-only" shortcut.
    - **Representational Collapse:** Metrics like k-NN accuracy and CKA to both θr and θo plummet. The feature space is destroyed. Common in unbounded loss methods (GA, SCRUB in large-scale settings).
    - **Failure to Scale:** A method performing well on CIFAR-10 (high AGL) shows catastrophic failure (low AGR) on ImageNet-1K, especially as the number of forgotten classes (N) increases.

- **First 3 experiments:**
    1. Replicate the "False Security" finding: Take a standard unlearning method (e.g., Pseudo Labeling - PL) on ImageNet-1K (Random-100 classes). Measure its AGL score. You will see it's high (e.g., ~0.94), suggesting success. Now, measure its CKA(θu, θo) on a downstream dataset like Office-Home. You will find it's also high, demonstrating it has NOT truly unlearned the representations.
    2. Test for "Shortcut Learning": Take the unlearned model from experiment #1. Freeze its encoder and *only* retrain its final classifier layer on the forget set to maximize loss (a proxy for what the unlearning algorithm may have done internally). Compare the logit-based performance of this "final-layer-only" model to the fully unlearned model. If they are nearly identical, it confirms the shortcut.
    3. Implement the "Top Class-wise" Stress Test: Choose a downstream dataset (e.g., CUB-200). Identify the top 100 most semantically similar classes in ImageNet-1K. Apply the same unlearning method from experiment #1 for these specific classes. Now, measure the AGR score on the CUB dataset. Observe if the AGR score drops significantly compared to the Random-100 scenario, exposing the method's weakness when semantic alternatives are removed.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of representation-based evaluation may limit adoption in resource-constrained settings
- Unified benchmark focuses on vision tasks; transferability to other domains (NLP, tabular data) requires further investigation
- Cannot definitively prove complete representational erasure is theoretically achievable or practically necessary

## Confidence
- **High Confidence:** The observation that existing logit-based metrics can create false security by only measuring classifier modifications while ignoring representational similarity
- **Medium Confidence:** The claim that representation-based metrics are necessary for rigorous evaluation in large-scale scenarios
- **Medium Confidence:** The assertion that Top Class-wise Forgetting represents a more challenging and semantically meaningful evaluation scenario

## Next Checks
1. Apply the unified benchmark to NLP models (e.g., BERT) to verify whether representation-based metrics similarly expose limitations in text-based unlearning methods
2. Investigate theoretical bounds on how much representational similarity can be reduced while maintaining model utility
3. Develop and validate lighter-weight approximations of CKA and k-NN that maintain evaluation rigor while reducing computational overhead