---
ver: rpa2
title: 'Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based
  Approach'
arxiv_id: '2510.17854'
source_url: https://arxiv.org/abs/2510.17854
tags:
- image
- images
- embedding
- ai-generated
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting AI-generated images
  amid rapid advancements in generative AI models like DALL-E and Stable Diffusion.
  The authors propose EmbedAIDetect, a training-free system that uses image embeddings
  from Vision Transformer models and vector similarity search to distinguish AI-generated
  images from human-created ones.
---

# Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach

## Quick Facts
- **arXiv ID:** 2510.17854
- **Source URL:** https://arxiv.org/abs/2510.17854
- **Authors:** Jitendra Sharma; Arthur Carvalho; Suman Bhunia
- **Reference count:** 25
- **Primary result:** EmbedAIDetect achieves 99.51% classification accuracy (CLIP model) and high robustness (DINOv2) for AI-generated vs. human-created image detection.

## Executive Summary
This paper presents EmbedAIDetect, a training-free system for detecting AI-generated images using embedding similarity and blockchain-based verification. The approach leverages Vision Transformer models (CLIP, DINOv2) to extract image embeddings, classifies images via cosine similarity search in a vector database, and optionally stores embedding hashes on Ethereum for tamper-resistant provenance. Experiments show high accuracy (99.51% for CLIP) and robustness to common perturbations, while blockchain integration provides verifiable, non-repudiable records despite scalability and cost constraints.

## Method Summary
EmbedAIDetect extracts image embeddings using ViT-based models (CLIP, DINOv2, Google ViT, ResNet-50, AIMv2), stores them in a vector database (Pinecone/ChromaDB), and classifies images by comparing cosine similarity between AI and human image clusters. Classification uses a simple nearest-neighbor rule: if an image is closer to the AI cluster, it is classified as AI-generated. The system optionally integrates with Ethereum smart contracts to store 256-bit embedding hashes for tamper-resistant verification. The approach is training-free, relying on pre-trained models and vector similarity. Experiments use 9,000 AI-generated (Stable Diffusion 3.5 Medium, 512×512) and 6,074 human art images, evaluating accuracy, robustness to perturbations, and blockchain scalability.

## Key Results
- CLIP achieves 99.51% classification accuracy, the highest among tested models.
- DINOv2 is most robust to perturbations (blurring, resizing, patch overlays) with minimal accuracy drop.
- Blockchain integration (Ethereum) enables tamper-resistant verification but faces scalability and cost limitations; uint256 hash storage is more gas-efficient than string storage.

## Why This Works (Mechanism)
The system exploits the distinct statistical and stylistic signatures embedded in AI-generated images, which differ from human-created art at the feature level captured by Vision Transformers. These differences are preserved in the embedding space, enabling high-accuracy similarity-based classification without retraining. Blockchain integration adds a layer of trust and verifiability, as embedding hashes are immutable and non-repudiable once recorded on-chain.

## Foundational Learning
- **Vision Transformer embeddings**: Extract high-level semantic features from images; needed for capturing stylistic differences between AI and human images; quick check: verify embedding extraction pipeline and normalization.
- **Cosine similarity for classification**: Measures angular distance in embedding space to determine image origin; needed for training-free clustering-based detection; quick check: confirm correct distance metric and cluster separation.
- **Vector database storage (Pinecone/ChromaDB)**: Enables efficient nearest-neighbor search over large embedding sets; needed for scalable classification; quick check: ensure embeddings stored in separate AI/human indexes.
- **Blockchain-based verification**: Stores immutable hashes of embeddings for provenance; needed for tamper-resistant trust; quick check: validate hash storage and retrieval on testnet.
- **Image perturbation robustness**: Tests detection under common image modifications; needed to ensure real-world applicability; quick check: apply perturbations as specified and measure similarity drop.
- **Gas usage and scalability**: Evaluates blockchain deployment feasibility; needed for practical deployment; quick check: measure gas costs for different data types (uint256 vs string).

## Architecture Onboarding

**Component Map**
Embedding Extraction (ViT models) -> Vector Database (Pinecone/ChromaDB) -> Classification (Cosine Similarity) -> Optional: Blockchain Verification (Ethereum Smart Contracts)

**Critical Path**
Extract embeddings → Store in vector database → Classify via cosine similarity → (Optional) Store/verify hash on blockchain

**Design Tradeoffs**
- **Accuracy vs. Robustness**: CLIP is more accurate but less robust to perturbations than DINOv2.
- **Scalability vs. Trust**: Blockchain provides trust but is costly and slower; vector databases are fast and scalable but lack immutability.
- **Training-free vs. Custom Models**: Avoids retraining but relies on pre-trained model generalization; custom models may improve accuracy but require labeled data.

**Failure Signatures**
- Low accuracy: Label leakage between train/test sets; embeddings not properly normalized; AI and human clusters overlap in embedding space.
- Robustness mismatch: Perturbations applied inconsistently; querying perturbed images against wrong (non-original) embedding set.
- Blockchain failures: Nonce conflicts; high gas variance; smart contract deployment errors.

**3 First Experiments**
1. **Core Classification**: Extract CLIP embeddings for AI and human images, store in Pinecone, run 4:1 train/test split, compute accuracy/precision/recall.
2. **Perturbation Robustness**: Apply white patches (128×128), resize (128×128), and blur (20-80%) to AI images, query against originals, report match accuracy per model.
3. **Blockchain Integration**: Deploy HashStorageAI/HashStorageHuman on Sepolia, store 256-bit embedding hashes for 200 images, verify on-chain, measure gas usage.

## Open Questions the Paper Calls Out
- **IPFS Integration**: How can IPFS be integrated with the current blockchain framework to create a scalable, decentralized image verification system? The current implementation faces significant scalability and cost limitations when storing data directly on Ethereum, leading to nonce conflicts and high gas fees; IPFS was proposed but not implemented or tested.
- **Standardization and Shared Embeddings**: What protocols or architectural changes are necessary to establish a standardized, industry-wide shared database of vector embeddings? Different embedding models produce varying representations, and there is currently no unified standard, limiting the "universality" of detection.
- **Adversarial Robustness**: How does detection accuracy and embedding robustness hold up against sophisticated adversarial modifications and unseen generative architectures? While common perturbations were tested, the paper acknowledges the need to test against more sophisticated image modifications and rapidly evolving generative models beyond Stable Diffusion and StyleGAN.

## Limitations
- **Dataset Scope**: Experiments use only Stable Diffusion 3.5 Medium at 512×512; results may not generalize to other generative models or resolutions.
- **Parameter Transparency**: Exact prompts, generation parameters, and seed settings for AI images are not fully disclosed, limiting reproducibility.
- **Blockchain Scalability**: Mainnet deployment would incur higher costs and latency; current validation is limited to Sepolia testnet.

## Confidence
- **High Confidence**: General framework of embedding-based similarity search; use of cosine similarity and vector databases; conceptual blockchain integration for verification.
- **Medium Confidence**: Reported robustness of DINOv2 to perturbations; comparative performance of CLIP vs. DINOv2; blockchain scalability and gas usage (limited to Sepolia).
- **Low Confidence**: Generalization of accuracy to other models or broader image sets; exact reproduction of embedding extraction; mainnet blockchain deployment feasibility.

## Next Checks
1. **Dataset and Preprocessing Audit**: Recreate AI and human image datasets as closely as possible; verify embedding extraction, normalization, and train/test splits to prevent label leakage.
2. **Perturbation Robustness Validation**: Replicate robustness experiments with specified perturbations; compare model performance and test with alternative perturbation types.
3. **Blockchain Integration Verification**: Deploy smart contracts on Sepolia; conduct end-to-end verification with a subset of embeddings; measure gas usage and attempt mainnet simulation for cost assessment.