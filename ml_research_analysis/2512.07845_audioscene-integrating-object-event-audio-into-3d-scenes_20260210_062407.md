---
ver: rpa2
title: 'AudioScene: Integrating Object-Event Audio into 3D Scenes'
arxiv_id: '2512.07845'
source_url: https://arxiv.org/abs/2512.07845
tags:
- audio
- object
- datasets
- events
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two novel audio-spatial datasets, Audio-ScanNet
  and Audio-RoboTHOR, which integrate audio clips with 3D scenes to enable research
  on audio-conditioned tasks within 3D environments. The datasets were created by
  leveraging large language models (LLMs) like GPT-4 to generate object-event pairs
  and match them with existing audio datasets, supplemented with rigorous human verification
  for quality assurance.
---

# AudioScene: Integrating Object-Event Audio into 3D Scenes

## Quick Facts
- arXiv ID: 2512.07845
- Source URL: https://arxiv.org/abs/2512.07845
- Authors: Shuaihang Yuan; Congcong Wen; Muhammad Shafique; Anthony Tzes; Yi Fang
- Reference count: 40
- Primary result: Two novel audio-spatial datasets with 12,876 audio clips across 65 events and 36 object categories, enabling audio-conditioned 3D tasks

## Executive Summary
This paper introduces Audio-ScanNet and Audio-RoboTHOR, two novel datasets that integrate object-event audio with 3D scenes to enable audio-guided spatial reasoning tasks. The datasets were created by leveraging large language models (GPT-4) to generate object-event associations and match them with existing audio datasets, followed by rigorous human verification. The authors designed two benchmark tasks—audio-based 3D visual grounding and audio-based robotic zero-shot navigation—to evaluate the datasets. Results show that current audio-centric methods have significant limitations, with the proposed method improving mAP@0.25 by approximately 19% and mAP@0.5 by 42% compared to baselines. For navigation, the ESC model achieved the best performance with an SPL of 14.1 and an SR of 27.1.

## Method Summary
The paper proposes a pipeline for creating audio-spatial datasets by leveraging large language models to generate object-event pairs from 3D scene categories, matching these events to existing audio datasets (ESC, FSD50K, EPIC-SOUNDS, ReaLISED), and performing rigorous human verification. The authors design two benchmark tasks: audio-based 3D visual grounding and audio-based robotic zero-shot navigation. For grounding, they adapt baseline methods by replacing text encoders with audio encoders (wav2clip) and propose a method that chains audio event recognition, 3D object detection (VoteNet), and LLM inference (LLaMA) for probabilistic object-event linking. For navigation, they evaluate audio-conditioned models using SPL and SR metrics in the Audio-RoboTHOR environment.

## Key Results
- Proposed method improved mAP@0.25 by approximately 19% and mAP@0.5 by 42% compared to baseline methods for audio-based 3D object localization
- ESC model achieved the best performance for audio-based robotic navigation with an SPL of 14.1 and an SR of 27.1
- Current audio-centric methods show significant limitations, highlighting the need for specialized datasets and approaches
- The datasets contain 12,876 audio clips across 65 events and 36 object categories, integrated with 3D scene data

## Why This Works (Mechanism)

### Mechanism 1: LLM-Mediated Object-Event Association
Large language models can generate plausible object-event pairs that bridge semantic gaps between disparate audio and 3D scene datasets. GPT-4 receives object categories and generates candidate events (e.g., "alarm clock" → "ringing"), which are then matched to existing audio datasets through dual-matrix scoring. The core assumption is that LLM "common sense reasoning" produces event associations reflecting real-world audio causality. Limited direct corpus support suggests LLM-generated mappings may not capture acoustic complexity in real-world reverberation and noise conditions.

### Mechanism 2: Audio Embedding Substitution for Cross-Modal Transfer
Replacing text encoders with audio encoders (wav2clip) enables text-based 3D grounding models to process audio inputs with minimal architectural changes. Baseline methods originally using text encoders are adapted by substituting wav2clip audio encoder, producing V_audio = E_audio(A). The core assumption is that audio and text embeddings occupy sufficiently similar semantic space for direct substitution. Experiments show low baseline performance (mAP@0.25: 0.084-0.360), suggesting the substitution is non-trivial and may miss spatial audio cues.

### Mechanism 3: Event-Object Probabilistic Inference via LLM
A language model can probabilistically link recognized audio events to spatially localized objects, improving grounding accuracy. The proposed method chains: (1) wav2clip recognizes event from audio → (2) VoteNet detects objects in 3D scene → (3) LLaMA processes event text + detection results to infer most likely source object. This explicit reasoning step outperforms direct audio-feature fusion. The core assumption is that explicit symbolic reasoning over event-object relationships compensates for weak audio-visual feature alignment. Results show mAP@0.25=0.429 vs. best baseline 0.360 (≈19% improvement).

## Foundational Learning

- **Concept: Audio Embeddings (wav2clip)**
  - Why needed here: All baseline adaptations and the proposed method rely on wav2clip to convert raw audio into 512-dimensional feature vectors compatible with downstream models
  - Quick check question: Can you explain how wav2clip aligns audio representations with CLIP's joint embedding space?

- **Concept: 3D Object Detection (VoteNet)**
  - Why needed here: The proposed grounding method uses VoteNet to generate object proposals from point clouds before LLaMA inference
  - Quick check question: How does Hough voting in VoteNet handle cluttered indoor scenes with occlusions?

- **Concept: Zero-Shot Navigation Metrics (SPL, SR)**
  - Why needed here: Benchmark Task 2 evaluates audio-conditioned navigation using Success Rate (SR) and Success weighted by Path Length (SPL)
  - Quick check question: Why might SPL be lower than SR even when the agent reaches the target?

## Architecture Onboarding

- **Component map:**
  [Scene Data] ──┐
                 ├──> [Category Selection] ──> [LLM-OEG (GPT-4)] ──> [Events]
  [Audio Data] ──┘                                                           │
                                                                             ▼
  [Events] <──[Human Verification]──> [Dual-Matrix Scoring (GPT-4 + Human)]──>[Mapping]
                                                                             │
                                                                             ▼
  [Object-Audio Pairs] ──> [Spatial Backmapping] ──> [Audio-ScanNet / Audio-RoboTHOR]

- **Critical path:**
  1. Extract 36 common object categories from ScanNet + RoboTHOR
  2. Generate events via GPT-4 prompts (LLM-OEG)
  3. Match events to ESC/FSD50K/EPIC-SOUNDS/ReaLISED using dual-matrix scoring
  4. Index audio clips back to object spatial coordinates
  5. Validate via grounding and navigation benchmarks

- **Design tradeoffs:**
  - LLM automation vs. manual verification: GPT-4 reduces annotation burden but requires human verification for plausibility
  - Audio diversity vs. label consistency: Four source datasets provide 12,876 clips but use different event taxonomies; GPT-4 consolidates semantically similar events
  - Direct substitution vs. explicit reasoning: Baselines use direct audio-for-text substitution; proposed method adds LLaMA inference layer at cost of complexity

- **Failure signatures:**
  - Low mAP@0.5 on grounding baselines (0.026-0.274) suggests audio embeddings lack spatial precision
  - MDETR navigation achieves SR=0.0, indicating complete failure on audio-conditioned task
  - CLIP-Ref (SR=1.8%) suggests reference-based matching doesn't transfer to audio domain

- **First 3 experiments:**
  1. Reproduce baseline adaptation: Take 3DVG-Transformer, replace text encoder with wav2clip, evaluate on Audio-ScanNet grounding task. Expected: low mAP, confirms paper's baseline numbers.
  2. Ablate LLaMA inference: Remove LLaMA from proposed pipeline, directly match wav2clip events to VoteNet detections. Expected: performance drops toward MVT baseline levels.
  3. Test event-object ambiguity: Construct scene with multiple same-category objects (e.g., 3 doors), play "door slam" audio. Measure whether LLaMA correctly identifies specific source or random-guesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the LLM-based event generation be made fully automated while maintaining quality, or is manual verification indispensable for high-accuracy object-event mapping?
- Basis in paper: The authors state their approach "offers greater scalability compared to purely manual annotation" but acknowledge "ensuring high-quality data demands a combination of the strong reasoning capabilities of large language models... and rigorous manual verification."
- Why unresolved: The paper does not quantify the error rate of GPT-4 alone versus GPT-4 with human verification, nor does it explore active learning or automated quality filtering alternatives
- What evidence would resolve it: Ablation experiments comparing fully automated LLM event generation against human-verified outputs, measuring inter-annotator agreement and downstream task performance

### Open Question 2
- Question: How would incorporating spatial acoustics (room impulse responses, distance attenuation, occlusion) into the audio representation improve grounding and navigation performance?
- Basis in paper: The paper maps audio clips to object coordinates without simulating how sound propagates in 3D spaces, yet real-world audio-spatial reasoning requires understanding acoustic properties of environments
- Why unresolved: The datasets treat audio as scene-agnostic, detached from geometric and material properties that affect sound in physical spaces
- What evidence would resolve it: Experiments augmenting the dataset with spatial audio rendering (e.g., using SoundSpaces-style acoustic simulation) and comparing task performance against the current non-spatial audio setup

### Open Question 3
- Question: Can current methods handle polyphonic audio scenes with multiple simultaneous sound sources, or does performance degrade significantly beyond single-event scenarios?
- Basis in paper: The dataset design assumes one-to-one mapping between audio events and objects, with no evaluation of multi-source or overlapping sounds, despite this being common in realistic environments
- Why unresolved: The benchmark tasks only evaluate single-audio grounding and navigation, leaving multi-source scenarios unexplored
- What evidence would resolve it: Extending the dataset with mixed audio clips containing multiple simultaneous events and evaluating whether existing models can disambiguate and localize multiple sources

### Open Question 4
- Question: Does the modular pipeline (audio-to-event classification → object detection → LLaMA inference) introduce error accumulation that an end-to-end learned model could avoid?
- Basis in paper: The proposed grounding method chains separate components (wav2clip, VoteNet, LLaMA), while baseline adaptations simply replace text encoders with audio encoders—neither approach is truly optimized for the audio-to-3D task
- Why unresolved: No comparison exists between modular pipelines and jointly trained audio-visual-3D models that learn unified representations
- What evidence would resolve it: Designing and training an end-to-end model that directly maps audio and point clouds to object locations, comparing against the modular approach on the same benchmark

## Limitations
- LLM-generated object-event associations may introduce domain shift between synthetic associations and real-world acoustic causality
- Audio embeddings may inadequately represent spatial audio cues due to direct substitution approach
- Benchmark tasks may not fully capture complexity of real-world audio-guided navigation scenarios

## Confidence
- **High confidence:** Dataset creation methodology and basic evaluation framework
- **Medium confidence:** Performance improvements of proposed method over baselines (limited baseline diversity)
- **Low confidence:** Generalization claims beyond controlled benchmark settings

## Next Checks
1. Conduct cross-dataset validation by testing Audio-ScanNet models on real-world audio scenes with varying acoustic conditions (reverberation, noise levels)
2. Perform ablation studies on the LLM-OEG module to quantify the contribution of human verification vs. pure LLM generation
3. Test proposed method on scenes with high object-event ambiguity (multiple plausible sound sources) to evaluate disambiguation capabilities