---
ver: rpa2
title: 'Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent
  decomposition training algorithm'
arxiv_id: '2512.19440'
source_url: https://arxiv.org/abs/2512.19440
tags:
- data
- formulation
- points
- sparsity
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a sparsity-inducing formulation for training
  binary kernel logistic regression (KLR) that improves the trade-off between model
  sparsity and testing accuracy compared to existing approaches. The authors extend
  the KLR training formulation by introducing a slack variable that adjusts the contribution
  of easy-to-classify data points, thereby promoting sparsity in the dual variables.
---

# Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm

## Quick Facts
- arXiv ID: 2512.19440
- Source URL: https://arxiv.org/abs/2512.19440
- Authors: Antonio Consolo; Andrea Manno; Edoardo Amaldi
- Reference count: 40
- Key outcome: Proposes sparsity-inducing formulation for binary kernel logistic regression with convergent decomposition algorithm, achieving competitive trade-off between sparsity and testing accuracy

## Executive Summary
This paper introduces a novel sparsity-inducing formulation for binary kernel logistic regression (KLR) that improves the trade-off between model sparsity and testing accuracy. The authors extend the standard KLR training formulation by introducing a slack variable that adjusts the contribution of easy-to-classify data points, thereby promoting sparsity in the dual variables. To efficiently solve the resulting optimization problem, they develop a Sequential Minimal Optimization (SMO)-type decomposition algorithm that exploits second-order information and establish its global convergence.

The proposed approach is evaluated on 12 benchmark datasets, demonstrating competitive testing accuracy and sparsity compared to existing methods including Import Vector Machine (IVM), ℓ1/2-KLR regularization, and Support Vector Machine (SVM). The experiments show that the method achieves an average testing accuracy of 92.5% with 42.2% sparsity, outperforming SVM's 90.5% accuracy with 26.8% sparsity. Additionally, the authors demonstrate that hyperparameter tuning can be simplified by setting the sparsity parameter to a small fraction of the regularization parameter without significantly compromising performance.

## Method Summary
The paper proposes a sparsity-inducing formulation for binary kernel logistic regression by introducing a slack variable that modifies the contribution of easy-to-classify data points. This adjustment promotes sparsity in the dual variables during optimization. To solve the resulting optimization problem efficiently, the authors develop a Sequential Minimal Optimization (SMO)-type decomposition algorithm that exploits second-order information. The algorithm selects working sets based on second-order criteria, which reduces CPU training time by approximately 18% compared to first-order selection methods. The authors establish global convergence of their algorithm and demonstrate its effectiveness through experiments on 12 benchmark datasets.

## Key Results
- Achieves average testing accuracy of 92.5% with 42.2% sparsity across 12 benchmark datasets
- Outperforms SVM (90.5% accuracy, 26.8% sparsity) in the trade-off between accuracy and sparsity
- Second-order working set selection reduces CPU training time by approximately 18% compared to first-order variants
- Simplifies hyperparameter tuning by setting sparsity parameter to small fraction of regularization parameter

## Why This Works (Mechanism)
The proposed sparsity-inducing formulation works by introducing a slack variable that reduces the influence of easy-to-classify data points during training. This mechanism effectively creates a soft thresholding effect on the dual variables, promoting sparsity while maintaining classification accuracy. The slack variable acts as a regularizer that selectively penalizes the contribution of well-classified points, allowing the model to focus on more challenging instances. The SMO-type decomposition algorithm efficiently handles the modified optimization problem by exploiting second-order information, which improves convergence speed and computational efficiency compared to first-order methods.

## Foundational Learning
- Kernel logistic regression fundamentals: Understanding how kernels transform input space to enable non-linear decision boundaries
  - Why needed: Forms the basis for the proposed sparsity-inducing extension
  - Quick check: Verify understanding of kernel trick and its application in logistic regression

- Sequential Minimal Optimization (SMO) algorithm: Knowledge of the decomposition method for solving SVM optimization problems
  - Why needed: The proposed algorithm builds upon and extends SMO principles
  - Quick check: Understand how SMO selects working sets and optimizes dual variables

- Sparsity-inducing regularization techniques: Familiarity with methods like ℓ1 regularization and their effects on model complexity
  - Why needed: The proposed method introduces a novel approach to achieving sparsity
  - Quick check: Compare and contrast different sparsity-inducing mechanisms

## Architecture Onboarding

Component map:
KLR formulation -> Sparsity-inducing slack variable -> SMO-type decomposition algorithm -> Second-order working set selection -> Global convergence proof

Critical path:
1. Define kernel logistic regression objective with sparsity-inducing slack variable
2. Derive SMO-type decomposition algorithm for the modified optimization problem
3. Establish second-order working set selection criterion
4. Prove global convergence of the algorithm
5. Validate performance through experiments

Design tradeoffs:
- Sparsity vs accuracy: The slack variable introduces a tunable parameter to balance these competing objectives
- First-order vs second-order working set selection: Second-order information improves convergence but increases per-iteration complexity
- Computational efficiency vs convergence guarantees: The algorithm trades some computational overhead for proven global convergence

Failure signatures:
- Poor sparsity performance: May indicate inappropriate choice of sparsity parameter or slack variable formulation
- Slow convergence: Could suggest issues with working set selection or second-order information computation
- Suboptimal accuracy: Might result from excessive sparsity promotion or inadequate regularization

First experiments:
1. Validate the effect of the sparsity parameter on model sparsity and accuracy using a simple dataset
2. Compare convergence speed of first-order vs second-order working set selection on a medium-sized dataset
3. Test the sensitivity of the proposed method to different kernel choices and hyperparameters

## Open Questions the Paper Calls Out
None

## Limitations
- The sparsity metric (percentage of exactly zero dual variables) may not fully capture practical model complexity, as small non-zero values can still contribute to model complexity
- Experimental comparison focuses primarily on testing accuracy and sparsity but lacks thorough examination of computational efficiency in terms of memory usage and prediction time
- Claims about sparsity-accuracy trade-off are based on a relatively small set of 12 benchmark datasets, which may not represent all potential applications

## Confidence
- High confidence in the mathematical formulation and convergence proof of the proposed decomposition algorithm
- Medium confidence in the experimental results showing competitive performance against existing methods, given the limited dataset diversity
- Medium confidence in the claimed simplification of hyperparameter tuning based on the small fraction relationship between sparsity and regularization parameters

## Next Checks
1. Conduct experiments on a larger and more diverse set of datasets, including real-world applications with different data characteristics, to verify the generalizability of the sparsity-accuracy trade-off claims.
2. Perform detailed computational complexity analysis comparing memory usage and prediction time for the proposed sparse models against existing methods, in addition to training time comparisons.
3. Investigate the practical significance of the sparsity achieved by the proposed method by analyzing the distribution of non-zero dual variables and their contributions to the final model predictions.