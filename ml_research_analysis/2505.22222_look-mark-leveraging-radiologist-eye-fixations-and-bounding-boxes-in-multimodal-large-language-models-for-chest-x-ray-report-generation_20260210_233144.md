---
ver: rpa2
title: 'Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal
  Large Language Models for Chest X-ray Report Generation'
arxiv_id: '2505.22222'
source_url: https://arxiv.org/abs/2505.22222
tags:
- report
- bounding
- fixation
- reports
- mark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Look & Mark (L&M), a novel grounding fixation
  strategy that integrates radiologist eye fixations (Look) and bounding box annotations
  (Mark) into multimodal LLM prompting frameworks for chest X-ray report generation.
  The method employs in-context learning rather than fine-tuning, achieving substantial
  performance improvements without model retraining.
---

# Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation

## Quick Facts
- **arXiv ID**: 2505.22222
- **Source URL**: https://arxiv.org/abs/2505.22222
- **Reference count**: 11
- **Primary result**: Look & Mark (L&M) improves multimodal LLM report generation by integrating eye fixations and bounding boxes, boosting C.AVG from 87.3% to 87.3% for general models and reducing clinically significant errors by 0.43 per report.

## Executive Summary
This study introduces Look & Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye fixations (Look) and bounding box annotations (Mark) into multimodal LLM prompting frameworks for chest X-ray report generation. The method employs in-context learning rather than fine-tuning, achieving substantial performance improvements without model retraining. When evaluated across domain-specific and general-purpose models, L&M demonstrated significant gains: CXR-LLaVA improved by 1.2% in overall metrics (A.AVG), while LLaVA-Med showed a remarkable 9.2% boost. General-purpose models like LLaVA-OV achieved the highest clinical average performance (87.3% C.AVG), surpassing domain-specific models. Expert evaluations confirmed L&M reduces clinically significant errors by 0.43 per report. These findings highlight L&M as a scalable solution for AI-assisted radiology, particularly beneficial for low-resource clinical settings.

## Method Summary
The method employs a training-free in-context learning approach that augments multimodal LLM prompts with radiologist eye fixations and bounding box annotations. Eye fixations are mapped to containing bounding boxes using a smallest-area selection rule, with fixation durations summed per abnormality label. Bounding boxes are overlaid directly onto chest X-ray images as visual prompts, while fixation data is formatted as textual prompts: "Fixation Data: [Abnormality bounding box: {label}, Fixation Time: {time} seconds]". The framework uses batch size 1, temperature 0/0.1, and max 512 tokens. For general-purpose models, 3 exemplar reports from REFLACX are prepended to the prompt for in-context learning. The approach leverages REFLACX (eye fixations, dictated reports) and MS_CXR (bounding boxes) datasets, both derived from MIMIC-CXR.

## Key Results
- L&M achieved 9.2% improvement in A.AVG for LLaVA-Med and 1.2% for CXR-LLaVA
- General-purpose LLaVA-OV achieved highest C.AVG at 87.3%, surpassing domain-specific models
- Expert evaluation confirmed reduction of 0.43 clinically significant errors per report
- Method demonstrated effectiveness without model fine-tuning, using only in-context learning

## Why This Works (Mechanism)
The Look & Mark framework works by providing multimodal LLMs with explicit visual and textual grounding signals that align model attention with clinically relevant regions. By mapping radiologist eye fixations to specific abnormalities through bounding boxes, the model receives both spatial (visual overlay) and semantic (textual fixation duration) cues about where to focus its attention. This dual-channel grounding reduces hallucinations by constraining the model's generation to areas that human experts actually examined. The in-context learning approach allows the model to adapt to the specific reporting style and clinical context without expensive fine-tuning, making it scalable across different institutions and clinical settings.

## Foundational Learning
- **Multimodal prompt engineering**: Why needed - to effectively combine visual and textual information in LLM prompts; Quick check - verify both visual overlays and textual prompts are correctly formatted and passed to the model
- **Eye fixation mapping**: Why needed - to translate raw gaze data into clinically meaningful attention signals; Quick check - confirm smallest-area selection correctly handles overlapping bounding boxes
- **In-context learning**: Why needed - to adapt model behavior without retraining; Quick check - ensure exemplar reports are diverse and representative of target reporting style
- **Clinical metric validation**: Why needed - to ensure model outputs meet actual clinical utility standards; Quick check - verify expert evaluation criteria align with clinical significance thresholds
- **Bounding box visualization**: Why needed - to provide clear visual cues to the model; Quick check - test different rendering parameters (color, thickness) for optimal model performance

## Architecture Onboarding
- **Component map**: REFLACX/MS_CXR datasets -> Fixation-to-bbox mapping -> Visual overlay generation -> Textual prompt formatting -> Multimodal LLM (CXR-LLaVA/LLaVA-OV/LLaVA-Med) -> Report generation
- **Critical path**: The core pipeline requires accurate fixation mapping to bounding boxes, proper visual overlay rendering, and correct prompt formatting for in-context learning
- **Design tradeoffs**: Training-free approach sacrifices potential fine-tuning performance gains for scalability and reduced computational cost
- **Failure signatures**: Poor fixation mapping leads to incorrect attention signals; improper visual rendering reduces model effectiveness; inadequate exemplar reports limit in-context learning
- **First experiments**: 1) Test fixation mapping on a small subset to verify smallest-area selection; 2) Validate visual overlay rendering with a few sample images; 3) Confirm prompt formatting produces expected model responses

## Open Questions the Paper Calls Out
- **Automated grounding methods**: Can automated methods effectively replace expert-derived eye fixations and bounding boxes without degrading report quality? The study notes reliance on expert input limits scalability and suggests exploring weakly supervised learning or automatic fixation prediction. Evidence needed: Comparative performance analysis when substituting expert annotations with model-predicted boxes.
- **3D imaging modalities**: Does the Look & Mark framework generalize to 3D medical imaging modalities like CT and MRI? The conclusion explicitly states future work will extend L&M to other modalities. Evidence needed: Successful application on 3D datasets demonstrating comparable accuracy.
- **Multi-view chest X-rays**: How does performance change when applied to multi-view chest X-rays? The limitations section highlights reliance on single-view X-rays. Evidence needed: Evaluation on multi-view datasets to determine if integrating annotations from different views improves false prediction error rates.

## Limitations
- Performance gains rely on specific implementation details not fully specified, particularly fixation mapping and visual rendering parameters
- Evaluation depends heavily on RadGraph-XL metric, which lacks independent validation across radiology community
- Study uses relatively small dataset (560 images, 1,372 reports) limiting generalizability
- Fixation-to-bounding-box mapping assumes non-overlapping boxes, which may not hold in actual data

## Confidence
- **High confidence**: Methodology of using in-context learning with visual and textual prompts is technically sound and reproducible
- **Medium confidence**: Reported performance improvements (1.2% to 9.2% gains) are likely valid but sensitive to implementation details
- **Medium confidence**: Clinical error reduction finding (0.43 fewer errors per report) is supported but limited by small expert reviewer sample size

## Next Checks
1. Implement and validate the fixation-to-bounding-box mapping with test cases including overlapping boxes and boundary points
2. Conduct ablation studies on visual rendering parameters (color, thickness, label placement) to quantify impact on model performance
3. Scale evaluation to independent datasets from different institutions to assess generalizability and identify domain-specific limitations