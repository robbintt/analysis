---
ver: rpa2
title: 'Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug
  Repurposing'
arxiv_id: '2511.12472'
source_url: https://arxiv.org/abs/2511.12472
tags:
- serendipity
- knowledge
- graph
- type
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SerenQA, a framework to assess LLMs\u2019\
  \ ability to discover serendipitous insights in knowledge graph question answering.\
  \ It defines a novel graph-based RNS metric combining relevance, novelty, and surprise,\
  \ and constructs a drug repurposing benchmark with expert annotations."
---

# Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing

## Quick Facts
- arXiv ID: 2511.12472
- Source URL: https://arxiv.org/abs/2511.12472
- Reference count: 40
- Primary result: Introduces SerenQA framework evaluating LLMs' ability to discover serendipitous insights in KGQA, showing LLMs struggle with serendipity despite strong retrieval performance.

## Executive Summary
This paper introduces SerenQA, a framework to assess LLMs' ability to discover serendipitous insights in knowledge graph question answering. It defines a novel graph-based RNS metric combining relevance, novelty, and surprise, and constructs a drug repurposing benchmark with expert annotations. The evaluation pipeline decomposes tasks into knowledge retrieval, subgraph reasoning, and serendipity exploration. Experiments show that while large LLMs perform well on retrieval, they struggle significantly with serendipity discovery—highlighting a key gap in current LLM capabilities for scientific innovation.

## Method Summary
The framework constructs a three-phase pipeline: (1) Knowledge Retrieval translating natural language to Cypher queries for explicit answer extraction, (2) Subgraph Reasoning summarizing retrieved subgraphs, and (3) Serendipity Exploration using LLM-guided beam search to propose serendipitous entities. The RNS metric quantifies serendipity via relevance (GCN embedding distance), novelty (mutual information), and surprise (Jensen-Shannon divergence). The benchmark uses Clinical Knowledge Graph (15M nodes, 200M edges) with 1,529 expert-annotated queries, partitioned into existing and serendipity answer sets.

## Key Results
- Retrieval performance: F1 ~78% for 1-hop queries, dropping to <10% for 3+ hops
- All models achieve SerenHit <0.10, indicating poor serendipity discovery despite finding relevant entity types
- Removing subgraph summaries improves exploration performance, suggesting hallucinations propagate and hinder serendipity
- No single model excels across all metrics, suggesting multi-agent or MoE architectures may be beneficial

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Serendipity can be quantified via a graph-based RNS metric combining relevance, novelty, and surprise.
- Mechanism: RNS decomposes serendipity into three components: (1) Relevance via normalized Euclidean distance between GCN embeddings of existing vs. serendipity answer sets; (2) Novelty via mutual information between sets; (3) Surprise via Jensen-Shannon divergence over entity probability distributions. These are weighted (α, β, γ) and summed.
- Core assumption: Graph embeddings and probability distributions derived from the KG meaningfully capture semantic relatedness and unexpectedness as perceived by domain experts.
- Evidence anchors: Section 3.1 formally defines R(Ae, As), N(Ae, As), S(Ae, As) with GCN embeddings, MI, and JSD formulas; "SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise."

### Mechanism 2
- Claim: A three-phase pipeline (retrieval → reasoning → exploration) isolates and evaluates distinct LLM capabilities for serendipity discovery.
- Mechanism: Phase 1 translates NL to Cypher, extracting explicit answers (Ae). Phase 2 summarizes retrieved subgraphs. Phase 3 uses LLM-guided beam search (width w, max depth 3) to propose serendipitous entities (As) via multi-hop traversal and LLM node/relation selection.
- Core assumption: Decomposing the task preserves the integrated skill needed for serendipity; beam search with LLM guidance approximates exploratory scientific reasoning.
- Evidence anchors: Section 5 describes the modular pipeline and beam search process with LLM prompts for relation/node selection; "A structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration."

### Mechanism 3
- Claim: Multi-hop conditional probability matrices enable scalable, information-theoretic serendipity computation across large graphs.
- Mechanism: Constructs k-hop (k=3) transition matrix Pk = Σ(α_h · P₁^h) with distance-weighted coefficients. Marginal probabilities P(i) are approximated via PageRank-style damped iteration: P_{t+1} = λP₃^T P_t + (1-λ)P₀. Used for MI and JSD in RNS.
- Core assumption: 3-hop reachability captures most serendipitous connections (paper states 99% in their data); PageRank approximation converges to meaningful steady-state distributions.
- Evidence anchors: Section 3.2 and Appendix C detail Pk construction, row-stochasticity proof, and O(V² log V) convergence analysis.

## Foundational Learning

- Concept: **Knowledge Graph Question Answering (KGQA)**
  - Why needed here: The entire framework operates over KGQA, requiring understanding of structured queries (Cypher), multi-hop traversal, and answer set construction.
  - Quick check question: Given a drug-disease KG, can you write a 2-hop Cypher query to find drugs targeting proteins associated with a specific disease?

- Concept: **Information-Theoretic Divergence Measures (MI, KL, JSD)**
  - Why needed here: Novelty (MI) and Surprise (JSD) components of RNS rely on these measures to quantify distributional differences between answer sets.
  - Quick check question: Why is JSD preferred over raw KL divergence for comparing entity distributions when symmetry is desired?

- Concept: **Graph Neural Networks (GCN Embeddings)**
  - Why needed here: The Relevance component of RNS uses GCN embeddings to measure semantic distance between entities in existing vs. serendipity answer sets.
  - Quick check question: How does a GCN differ from a standard MLP in incorporating graph topology during node embedding generation?

## Architecture Onboarding

- Component map:
  RNS Metric Module -> Benchmark Dataset -> Evaluation Pipeline (T1, T2, T3)

- Critical path:
  1. Load ClinicalKG subgraph (~15M nodes, ~200M edges) into Redis-backed storage.
  2. Compute P₃ via Strassen-optimized matrix multiplication (parallelized across cluster).
  3. Generate GCN embeddings for all entities.
  4. For each query: retrieve Ae → summarize subgraph → run beam search for As → compute RNS against ground truth.

- Design tradeoffs:
  - **T2 Summarization vs. Exploration**: Including summaries degrades T3 performance (hallucinations propagate); skipping summaries improves SerenHit but reduces interpretability.
  - **Partition Strategy**: LLM-ensemble is scalable but noisy; expert annotation is gold-standard but costly; RNS-guided is automated but metric-dependent.
  - **Beam Width/Depth**: Paper uses w=30, max depth=3; deeper/wider search increases compute exponentially with marginal SerenHit gains (<0.10 even for 70B models).

- Failure signatures:
  - **Retrieval collapse**: F1 drops from ~78% (1-hop) to <10% (3+ hops) — indicates Cypher generation failure for complex queries.
  - **SerenHit near-zero**: All models achieve <0.10 SerenHit — exploration finds relevant entity types but rarely exact serendipitous entities.
  - **Summary hallucination**: T2 Faithfulness scores of 2-3/5 for some models — beam search inherits incorrect context.

- First 3 experiments:
  1. **Baseline retrieval sanity check**: Run T1 on 100 one-hop queries; verify F1 > 70% with frontier models (GPT-4o, DeepSeek-V3). If failed, debug Cypher generation prompts.
  2. **RNS metric validation on synthetic partitions**: Manually construct Ae/As pairs with known RNS properties; verify computed scores match expected ordering (e.g., higher novelty → higher N).
  3. **Ablation on summary inclusion**: Run T3 with and without T2 summaries on 50 queries; quantify SerenHit delta to confirm tradeoff before scaling to full benchmark.

## Open Questions the Paper Calls Out

- **Multi-agent/MoE architectures**: Can composite systems overcome the "No Single Winner" problem to achieve balanced performance across retrieval, reasoning, and exploration? Section 6.3 suggests multi-agent systems or MoE strategies may be beneficial.
- **Generalization to other domains**: Does the RNS metric generalize effectively to scientific domains outside of drug repurposing? Section 3.2 claims adaptability, but experiments were limited to CKG.
- **Subgraph reasoning modification**: How can the subgraph reasoning phase be modified to support exploration rather than hinder it due to hallucinations? Section 6.2 reports that removing summaries improved exploration performance.

## Limitations
- Serendipity quantification relies on graph-based metrics that lack external validation against actual scientific breakthroughs or expert perception of true serendipity.
- Heavy computational requirements (O(V^2.807) for P3 matrix computation) create scalability concerns for real-world deployment.
- Beam search approach shows very low SerenHit scores (<0.10) even for largest models, suggesting fundamental limitations in current LLM architectures for serendipitous discovery.

## Confidence
- **High confidence**: Three-phase pipeline architecture is well-specified and retrieval component shows reasonable performance (F1 ~78% for 1-hop queries). Mathematical framework for RNS metrics is internally consistent.
- **Medium confidence**: Subgraph reasoning and exploration components are implemented as described, but their effectiveness is limited by LLM hallucination and poor serendipity discovery rates. Benchmark construction methodology is sound but may contain bias.
- **Low confidence**: That the current approach can enable meaningful serendipitous discovery in practice, given very low SerenHit scores and computational constraints.

## Next Checks
1. **RNS metric validation**: Manually construct synthetic Ae/As pairs with known RNS properties and verify that computed scores correctly rank them by perceived serendipity.
2. **Ablation study on graph sparsity**: Test RNS computation on progressively sparser subgraphs to determine the minimum edge density required for stable probability estimation and meaningful serendipity scores.
3. **Cross-model consistency check**: Run the full pipeline on the same queries across multiple LLM families (frontier vs. specialized) to determine if serendipity discovery is model-architecture dependent or a fundamental LLM limitation.