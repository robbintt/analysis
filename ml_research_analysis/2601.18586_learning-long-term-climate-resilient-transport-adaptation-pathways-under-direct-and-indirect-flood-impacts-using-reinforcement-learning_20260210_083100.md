---
ver: rpa2
title: Learning long term climate-resilient transport adaptation pathways under direct
  and indirect flood impacts using reinforcement learning
arxiv_id: '2601.18586'
source_url: https://arxiv.org/abs/2601.18586
tags:
- climate
- transport
- adaptation
- impacts
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing long-term, climate-resilient
  adaptation strategies for urban transportation systems, particularly in the face
  of deep uncertainty about future rainfall and flood risks. It proposes a reinforcement
  learning (RL)-based decision-support framework that couples an integrated assessment
  model (IAM) with RL to learn adaptive, multi-decade investment pathways.
---

# Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning

## Quick Facts
- arXiv ID: 2601.18586
- Source URL: https://arxiv.org/abs/2601.18586
- Reference count: 13
- Primary result: RL-based adaptation policy achieves 22% cost reduction in Copenhagen flood resilience case study

## Executive Summary
This paper proposes a reinforcement learning (RL) framework for long-term climate-resilient adaptation of urban transportation systems facing flood risks. The approach couples an integrated assessment model with RL to learn adaptive investment pathways across decades, integrating climate projections, hazard modeling, transport simulation, and impact valuation. A case study in Copenhagen demonstrates that the learned RL policy significantly outperforms no-adaptation and random-action baselines by balancing upfront infrastructure investments against avoided flood damages.

## Method Summary
The framework treats urban flood adaptation as a Graph Markov Decision Process where an RL agent selects interventions for traffic zones to minimize climate-induced transport disruption from 2024-2100. The method uses Proximal Policy Optimization (PPO) with a custom Graph Neural Network policy wrapper to handle the spatial structure of the transport network. The Copenhagen case study employs 29 traffic zones, 84k trips, and rainfall projections from RCP2.6/4.5/8.5 scenarios, with training performed over 4.5M steps across 10 parallel environments.

## Key Results
- RL policy achieved 22% reduction in cumulative total costs compared to baselines
- Policies trained under intermediate climate scenarios (RCP4.5) showed most robust performance across scenarios
- Severe scenario-trained policies incurred higher costs but better mitigated extreme outcomes
- Framework successfully balances upfront adaptation investments against avoided flood impacts

## Why This Works (Mechanism)
The framework succeeds by framing adaptation as a sequential decision problem where the agent learns to balance immediate costs against long-term risk reduction. The Graph Neural Network policy naturally captures spatial dependencies between zones, enabling coordinated interventions. PPO's stability allows learning across the 76-year planning horizon despite sparse rewards and high-dimensional state spaces. The modular architecture permits stress-testing policies across climate scenarios while maintaining computational tractability through parallel environment execution.

## Foundational Learning
- **Graph Neural Networks for spatial policy representation**: Needed to capture interdependencies between neighboring traffic zones; check by verifying message-passing layers preserve spatial relationships
- **Action masking in RL**: Required to enforce immutable intervention states; check by confirming gradients don't flow through invalid actions
- **Multi-scale cost aggregation**: Necessary to combine infrastructure damage ($10^7-$10^9$) with travel delays; check by monitoring value function explained variance
- **Climate scenario stress-testing**: Enables robustness analysis across uncertainty; check by comparing policy performance across RCP scenarios
- **Integrated assessment coupling**: Links flood hazards to transport impacts; check by validating depth-disruption curves against empirical data
- **Sequential decision horizon planning**: Handles 76-year adaptation pathways; check by examining policy stability across time steps

## Architecture Onboarding

**Component Map**: Climate Scenarios -> Flood Modeling -> Transport Simulation -> Cost Valuation -> RL Environment -> GNN Policy -> PPO Trainer

**Critical Path**: The RL environment receives climate inputs, runs flood-transport simulations, computes rewards, and updates the GNN policy through PPO. The GNN policy's ability to handle action masking and spatial dependencies is critical for performance.

**Design Tradeoffs**: The framework sacrifices fine-grained spatial resolution (29 zones) for computational tractability, enabling parallel scenario testing. Fixed population/land-use assumptions simplify the IAM coupling but limit dynamic adaptation representation.

**Failure Signatures**: Training instability manifests as exploding gradients when action masking is incorrectly implemented. Poor performance indicates reward scale mismatch or inadequate exploration of intervention combinations.

**First Experiments**:
1. Verify action masking implementation by checking no illegal actions are selected during training
2. Test reward normalization by training with scaled cost components and monitoring value function stability
3. Validate GNN architecture by comparing spatial policy outputs against zone independence baselines

## Open Questions the Paper Calls Out
- How can probabilistic climate ensembles and belief updating be integrated into the RL-based adaptation framework, and how does this affect the robustness of learned pathways compared to discrete RCP scenario training?
- How can multi-objective formulations incorporating equity, accessibility, and wellbeing be integrated into the reward function without compromising optimization tractability?
- Can surrogate models or emulators for the flood-transport simulation pipeline achieve sufficient fidelity to enable scaling to larger urban areas and richer intervention portfolios?
- How do learned RL adaptation pathways compare against conventional robust planning methods such as Dynamic Adaptive Policy Pathways (DAPP) or robust optimization under deep uncertainty?

## Limitations
- Simplified IAM-transport coupling assumes fixed population and land-use patterns throughout simulation
- Results are sensitive to specific network topology and unspecified intervention parameters
- Validation only against simple baselines rather than alternative RL algorithms or ensemble methods
- Cross-scenario robustness analysis doesn't account for compounding uncertainties in projections

## Confidence
- RL policy performance vs. baselines: Medium
- Cross-scenario robustness findings: Medium
- Adaptation pathway recommendations: Low

## Next Checks
1. Conduct sensitivity analysis by varying the discount factor Î³ across [0.95, 0.99] to assess impact on long-term policy behavior
2. Implement and compare alternative RL algorithms (SAC, TD3) to verify PPO's superiority in this domain
3. Test policy transferability by evaluating Copenhagen-trained policies on synthetic transport networks with different topological characteristics