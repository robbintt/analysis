---
ver: rpa2
title: 'Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking
  Study'
arxiv_id: '2504.12503'
source_url: https://arxiv.org/abs/2504.12503
tags:
- learning
- forgetting
- data
- continual
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces continual learning (CL) to engineering regression\
  \ tasks, focusing on surrogate modeling for computationally intensive simulations.\
  \ Three CL scenarios are proposed\u2014bin incremental, input incremental, and multi-target\
  \ incremental\u2014to address the dynamic nature of engineering datasets."
---

# Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study

## Quick Facts
- arXiv ID: 2504.12503
- Source URL: https://arxiv.org/abs/2504.12503
- Reference count: 40
- Key outcome: Experience Replay achieves performance close to joint retraining while reducing training time by nearly half in 3D engineering regression tasks

## Executive Summary
This paper introduces continual learning to engineering regression problems, focusing on surrogate modeling for computationally intensive simulations. Three CL scenarios—bin incremental, input incremental, and multi-target incremental—are proposed to address the dynamic nature of engineering datasets. The study benchmarks Experience Replay, Elastic Weight Consolidation (EWC), and Gradient Episodic Memory (GEM) across five 3D engineering datasets, evaluating their ability to mitigate catastrophic forgetting. Results demonstrate that Experience Replay consistently outperforms other methods, achieving near-joint retraining accuracy while significantly reducing computational overhead.

## Method Summary
The study evaluates continual learning strategies for 3D engineering regression using Avalanche library implementations. Models include Regression PointNet (for point clouds) and modified ResNet50 (for parametric data). Data is split into 4 sequential experiences using bin-incremental (target distribution) or input-incremental scenarios. Three CL strategies are compared: Experience Replay (data replay with 20% memory buffer), EWC (Fisher Information regularization), and GEM (gradient constraint optimization). Performance is measured using Mean Absolute Error (MAE), Mean Percent Error (MPE), and Forgetting Ratio against joint retraining baseline.

## Key Results
- Experience Replay consistently achieves lowest MPE across all benchmarks while maintaining forgetting ratios below 1.0
- GEM shows best forgetting ratio performance but plateaus in incremental MAE improvement
- EWC underperforms significantly, closely matching naive sequential training across all datasets
- Experience Replay reduces training time to approximately 50% of joint retraining while maintaining comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1: Experience Replay (Data-Driven Retention)
Replaying stored samples from previous experiences maintains performance on old tasks while adapting to new data. A memory buffer stores a randomly selected subset of past samples (up to 20% storage limit). During training on new experiences, these stored samples are interleaved with incoming data, forcing the model to simultaneously optimize for both old and new distributions.

### Mechanism 2: Gradient-Based Constraint (GEM)
Constraining gradient updates prevents interference with past knowledge by solving a quadratic optimization problem. Stores "patterns per experience" from past tasks. Before updating weights, GEM projects the current gradient onto a feasible region where the loss on past patterns does not increase.

### Mechanism 3: Parameter Importance Regularization (EWC)
Penalizing changes to important parameters preserves past knowledge without storing data. Computes the diagonal Fisher Information Matrix after each experience to identify parameters critical to past tasks. A regularization term is added to the loss, penalizing changes to high-importance weights.

## Foundational Learning

- **Catastrophic Forgetting**: Why needed: The fundamental problem this paper addresses—without intervention, sequential training causes models to "forget" previously learned mappings. Quick check: If you train a model on low-drag cars (batch 1) then high-drag cars (batch 4), why would the final model fail to predict low-drag values correctly?

- **Surrogate Modeling**: Why needed: The application context—replacing computationally expensive simulations (CFD, FEA) with fast ML predictions, where retraining from scratch negates efficiency gains. Quick check: Why is joint retraining described as "infeasible" for engineering workflows?

- **Stability-Plasticity Dilemma**: Why needed: Explains the tradeoff observed in results—GEM shows strong stability (low forgetting) but weak plasticity (poor adaptability), while EWC shows high plasticity but near-zero stability. Quick check: Why does Replay balance stability and plasticity better than GEM despite both being rehearsal-based methods?

## Architecture Onboarding

- **Component map**: Point cloud/parametric encoder → regression head → experience splitter → memory buffer → CL strategy wrapper → evaluation loop
- **Critical path**: Select CL scenario → choose data representation → initialize memory buffer → run incremental training with evaluation
- **Design tradeoffs**: Accuracy vs. storage (ER requires 20% data storage); computation vs. forgetting mitigation (GEM approaches joint training runtime); scenario selection (bin vs. input incremental)
- **Failure signatures**: EWC failure (forgetting ratio >3.0, MPE within 1-2 percentage points of Naive); GEM failure (incremental MAE plateaus despite low forgetting); Naive failure (sharp error spikes on early experiences)
- **First 3 experiments**: 1) Establish baseline with Naive and Joint training on smallest dataset; 2) Memory ablation testing ER with 5%, 10%, 15%, 20% memory; 3) Scenario comparison on same dataset to understand distribution shift patterns

## Open Questions the Paper Calls Out

### Open Question 1
Can dynamic architectural strategies effectively mitigate forgetting in multi-target incremental engineering scenarios? The authors define this scenario but did not evaluate architectural strategies.

### Open Question 2
How can regularization-based methods be adapted to perform competitively with Experience Replay in engineering regression? Standard Fisher Information-based importance weighting may not capture parameter dependencies required for high-dimensional regression.

### Open Question 3
How does incremental learning impact prediction uncertainty and physical constraint satisfaction in engineering surrogates? Current metrics fail to quantify calibration or physics law violations as the model updates.

## Limitations
- Study focuses exclusively on 3D engineering regression, limiting generalizability
- Experience Replay's 20% memory requirement may be prohibitive for larger datasets
- EWC's poor performance raises questions about Fisher-based regularization suitability for regression
- GEM's computational cost constrains practical applicability
- Only 4 experiences per scenario may not capture long-term CL dynamics

## Confidence
- High confidence: Experience Replay consistently outperforms other methods; catastrophic forgetting is successfully mitigated
- Medium confidence: GEM's lower forgetting ratios translate to meaningful performance gains; EWC's ineffectiveness in regression
- Low confidence: Whether 20% memory buffer is optimal; whether results generalize beyond 4-experience scenarios

## Next Checks
1. Test Experience Replay with progressively smaller memory buffers (5%, 10%, 15%) to identify minimum storage requirements
2. Implement alternative parameter importance metrics to replace Fisher Information in EWC for regression tasks
3. Extend experiments to 8-12 experiences per scenario to evaluate long-term CL performance