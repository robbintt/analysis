---
ver: rpa2
title: Memory and Bandwidth are All You Need for Fully Sharded Data Parallel
arxiv_id: '2504.03655'
source_url: https://arxiv.org/abs/2504.03655
tags:
- training
- gpus
- size
- gbps
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study thoroughly investigates Fully Sharded Data Parallel
  (FSDP) training of large transformer models across diverse hardware configurations.
  Through extensive simulations and empirical tests on clusters with 200Gbps and 100Gbps
  interconnects using up to 512 GPUs, the research reveals that network bandwidth
  and GPU memory are critical bottlenecks for training efficiency.
---

# Memory and Bandwidth are All You Need for Fully Sharded Data Parallel

## Quick Facts
- arXiv ID: 2504.03655
- Source URL: https://arxiv.org/abs/2504.03655
- Reference count: 40
- Primary result: Network bandwidth and GPU memory are critical bottlenecks for FSDP training of large transformer models, with doubling bandwidth improving efficiency by up to 9% for 7B and 13B models.

## Executive Summary
This study investigates Fully Sharded Data Parallel (FSDP) training of large transformer models across diverse hardware configurations, focusing on how network bandwidth and GPU memory impact training efficiency. Through extensive simulations and empirical tests on clusters with 200Gbps and 100Gbps interconnects using up to 512 GPUs, the research demonstrates that these two factors are the primary bottlenecks for training performance. The findings reveal that models with larger parameters and longer sequences show varying performance, with bandwidth limitations significantly impacting throughput and model FLOPs utilization. The study provides actionable guidelines for optimizing FSDP configurations and emphasizes the importance of network infrastructure in scaling large transformer architectures efficiently.

## Method Summary
The researchers conducted extensive simulations and empirical tests on GPU clusters with different interconnect bandwidths (200Gbps and 100Gbps) using up to 512 GPUs. They tested various transformer model sizes, including 7B and 13B parameter models, across different sequence lengths to understand how network bandwidth and GPU memory constraints affect training efficiency. The study measured key performance metrics including training throughput, model FLOPs utilization (MFU), and the impact of different FSDP configurations. The empirical approach involved systematic variation of hardware configurations and model parameters to isolate the effects of bandwidth and memory on training performance.

## Key Results
- Network bandwidth and GPU memory are identified as the primary bottlenecks for FSDP training efficiency
- Doubling network bandwidth can improve training efficiency by up to 9% for 7B and 13B parameter models
- Larger models and longer sequences show more pronounced sensitivity to bandwidth limitations

## Why This Works (Mechanism)
The study demonstrates that FSDP's communication patterns create significant network traffic during parameter synchronization across GPUs, making bandwidth a critical constraint. The mechanism works because FSDP requires all-reduce operations for gradient synchronization, which scales with the number of parameters and GPUs involved. When network bandwidth is insufficient, these operations become the bottleneck, reducing overall training throughput. Additionally, GPU memory limitations force trade-offs in batch sizes and activation checkpointing strategies, further constraining performance. The interplay between these two factors creates a compounding effect where both bandwidth and memory constraints must be addressed simultaneously for optimal training efficiency.

## Foundational Learning
- FSDP communication patterns: Understanding how parameter synchronization works across GPUs is crucial for identifying bottlenecks in distributed training. Quick check: Review the all-reduce operation implementation in distributed training frameworks.
- Network topology impact: The physical and logical arrangement of GPUs in a cluster significantly affects communication efficiency. Quick check: Map out the network topology of your training cluster.
- Memory hierarchy management: Efficient use of GPU memory through techniques like activation checkpointing is essential for scaling models. Quick check: Evaluate your current activation checkpointing strategy.
- Performance metrics (MFU): Model FLOPs utilization helps quantify how effectively computational resources are being used. Quick check: Calculate MFU for your current training setup.
- Scaling laws: Understanding how performance scales with model size and cluster size is critical for capacity planning. Quick check: Plot scaling curves for your models across different cluster sizes.

## Architecture Onboarding

Component map: Model parameters -> FSDP sharding -> GPU memory allocation -> Network communication -> All-reduce synchronization

Critical path: Forward pass computation -> Backward pass computation -> Gradient accumulation -> All-reduce synchronization -> Parameter update

Design tradeoffs: The study highlights the balance between model size, batch size, and communication efficiency. Larger models require more memory but may achieve better compute utilization, while smaller batches reduce memory pressure but increase communication overhead. The optimal configuration depends on the specific hardware constraints and model characteristics.

Failure signatures: When network bandwidth is insufficient, training throughput drops dramatically and MFU decreases significantly. Memory bottlenecks manifest as reduced batch sizes or activation checkpointing overhead. Both conditions lead to underutilization of GPU compute resources and longer training times.

First experiments:
1. Measure baseline training throughput with current FSDP configuration and cluster setup
2. Test training performance with doubled network bandwidth (if available) to quantify bandwidth impact
3. Evaluate different activation checkpointing strategies to understand memory-accuracy tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on interconnect bandwidth and GPU memory, potentially overlooking other system-level optimizations like communication scheduling algorithms
- Limited investigation of alternative communication strategies beyond standard all-reduce operations
- The findings may not fully generalize to all large transformer models, as the study primarily focuses on 7B and 13B parameter models

## Confidence
- High confidence in the core finding that network bandwidth and GPU memory are primary bottlenecks for FSDP training
- Medium confidence in the specific performance numbers and throughput measurements due to hardware dependency
- Medium confidence in generalizability to all large transformer models based on the study's model size focus

## Next Checks
1. Replicate the experiments on different GPU architectures (e.g., Hopper vs. Ampere) to verify findings across hardware generations
2. Test additional model sizes (e.g., 175B parameter models) to understand if scaling relationships extend to larger models
3. Evaluate the impact of alternative communication patterns (e.g., hierarchical all-reduce, ring-based communication) on identified bottlenecks