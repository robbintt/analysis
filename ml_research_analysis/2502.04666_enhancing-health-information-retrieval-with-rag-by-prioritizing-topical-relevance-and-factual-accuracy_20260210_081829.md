---
ver: rpa2
title: Enhancing Health Information Retrieval with RAG by Prioritizing Topical Relevance
  and Factual Accuracy
arxiv_id: '2502.04666'
source_url: https://arxiv.org/abs/2502.04666
tags:
- health
- information
- retrieval
- arxiv
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Retrieval-Augmented Generation (RAG) model
  to enhance Health Information Retrieval by addressing both topical relevance and
  factual accuracy. The model leverages scientific literature from PubMed Central
  (PMC) to retrieve relevant passages, generates a contextually relevant text (GenText)
  using Large Language Models (LLMs), and ranks documents based on topicality and
  factual accuracy.
---

# Enhancing Health Information Retrieval with RAG by Prioritizing Topical Relevance and Factual Accuracy

## Quick Facts
- arXiv ID: 2502.04666
- Source URL: https://arxiv.org/abs/2502.04666
- Authors: Rishabh Uapadhyay; Marco Viviani
- Reference count: 40
- Primary Result: CAM MAP and CAMNDCG scores reach up to 0.1602 and 0.2723, respectively, on CLEF eHealth and TREC Health Misinformation datasets

## Executive Summary
This paper introduces a Retrieval-Augmented Generation (RAG) model designed to enhance health information retrieval by simultaneously optimizing for topical relevance and factual accuracy. The model retrieves relevant scientific literature from PubMed Central (PMC), generates contextually relevant text using Large Language Models (LLMs), and ranks documents based on both topicality and factual accuracy. Factual accuracy is assessed through stance detection and semantic similarity with the generated text. Experimental results demonstrate significant improvements over baseline models, with CAM MAP and CAMNDCG scores reaching up to 0.1602 and 0.2723, respectively. The generated text (GenText) provides explainability by referencing scientific sources, aiding users in understanding the reasoning behind the retrieval.

## Method Summary
The proposed RAG model enhances health information retrieval by addressing both topical relevance and factual accuracy. It leverages scientific literature from PubMed Central (PMC) to retrieve relevant passages, generates a contextually relevant text (GenText) using Large Language Models (LLMs), and ranks documents based on topicality and factual accuracy. Factual accuracy is assessed through stance detection and semantic similarity with GenText. The model is evaluated on CLEF eHealth and TREC Health Misinformation datasets, demonstrating significant improvements over baseline models. GenText also provides explainability by referencing scientific sources, aiding users in understanding the reasoning behind the retrieval.

## Key Results
- CAM MAP and CAMNDCG scores reach up to 0.1602 and 0.2723, respectively, on CLEF eHealth and TREC Health Misinformation datasets.
- Significant improvements over baseline models in both topical relevance and factual accuracy.
- GenText provides explainability by referencing scientific sources, aiding users in understanding the reasoning behind the retrieval.

## Why This Works (Mechanism)
The model works by integrating topical relevance and factual accuracy into a unified RAG framework. It retrieves relevant scientific literature from PMC, generates contextually relevant text (GenText) using LLMs, and ranks documents based on both topicality and factual accuracy. Factual accuracy is assessed through stance detection and semantic similarity with GenText, ensuring that the retrieved information is both relevant and accurate. The use of scientific literature as a knowledge source enhances the credibility of the retrieved information, while GenText provides explainability by referencing these sources.

## Foundational Learning
- **Retrieval-Augmented Generation (RAG):** A framework that combines information retrieval with text generation to enhance the quality and relevance of generated content. *Why needed:* To leverage external knowledge sources for generating accurate and relevant text. *Quick check:* Ensure the retrieval component effectively identifies relevant passages from PMC.
- **Topical Relevance:** The degree to which retrieved documents are related to the user's query. *Why needed:* To ensure that the retrieved information is pertinent to the user's needs. *Quick check:* Verify that the ranking algorithm prioritizes documents with high topical relevance.
- **Factual Accuracy:** The correctness of the information retrieved and generated. *Why needed:* To ensure that the information provided is reliable and trustworthy. *Quick check:* Assess the accuracy of stance detection and semantic similarity metrics.

## Architecture Onboarding

### Component Map
PMC Retrieval -> GenText Generation -> Topicality Ranking -> Factual Accuracy Assessment -> Document Ranking

### Critical Path
The critical path involves retrieving relevant passages from PMC, generating GenText using LLMs, and ranking documents based on topicality and factual accuracy. Factual accuracy is assessed through stance detection and semantic similarity with GenText, ensuring that the retrieved information is both relevant and accurate.

### Design Tradeoffs
- **Knowledge Source:** Using PubMed Central as the sole knowledge source may limit coverage but ensures credibility.
- **Explainability:** GenText provides explainability but may increase computational cost.
- **Accuracy Metrics:** Stance detection and semantic similarity are effective but may be sensitive to the quality of underlying models.

### Failure Signatures
- **Low Topical Relevance:** Poor retrieval performance or ineffective ranking algorithm.
- **Inaccurate Factual Assessment:** Errors in stance detection or semantic similarity metrics.
- **Scalability Issues:** High computational cost for generating GenText and performing accuracy assessments.

### 3 First Experiments
1. Evaluate the retrieval performance of the PMC retrieval component.
2. Assess the accuracy of the stance detection and semantic similarity metrics.
3. Test the scalability of the model with increased query volumes.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on relatively small datasets (CLEF eHealth and TREC Health Misinformation), which may not fully represent real-world scenarios.
- The reliance on PubMed Central as the sole knowledge source may limit coverage of non-PMC health literature and emerging research.
- The computational cost of generating GenText and performing stance detection for each query could impact scalability.

## Confidence

### Confidence Labels
- **High:** The core methodology of using RAG for health information retrieval with topical relevance and factual accuracy is well-established and clearly described.
- **Medium:** The experimental results show improvements over baselines, but the small dataset size and specific metrics limit generalizability.
- **Low:** The long-term effectiveness and robustness of the stance detection and semantic similarity components in diverse real-world scenarios remain uncertain.

## Next Checks
1. Test the model on a larger, more diverse health information retrieval dataset to assess generalizability.
2. Evaluate the computational efficiency and scalability of the model with increased query volumes.
3. Conduct a user study to assess the perceived explainability and usefulness of GenText references in real-world applications.