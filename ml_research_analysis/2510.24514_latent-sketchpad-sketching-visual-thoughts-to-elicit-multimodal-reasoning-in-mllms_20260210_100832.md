---
ver: rpa2
title: 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning
  in MLLMs'
arxiv_id: '2510.24514'
source_url: https://arxiv.org/abs/2510.24514
tags:
- visual
- reasoning
- latent
- sketchpad
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latent Sketchpad addresses the challenge of multimodal reasoning
  in MLLMs, which struggle with complex tasks requiring visual planning and imagination.
  Inspired by human sketching for problem-solving, it introduces a framework that
  equips MLLMs with an internal visual scratchpad, enabling them to interleave textual
  reasoning with the generation of visual latents.
---

# Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs

## Quick Facts
- arXiv ID: 2510.24514
- Source URL: https://arxiv.org/abs/2510.24514
- Reference count: 40
- Key outcome: Latent Sketchpad enables MLLMs to interleave textual reasoning with visual latent generation, achieving comparable or superior reasoning performance while providing interpretable visual traces.

## Executive Summary
Latent Sketchpad addresses the challenge of multimodal reasoning in MLLMs by introducing an internal visual scratchpad that generates interpretable sketches during autoregressive reasoning. The framework equips MLLMs with a Context-Aware Vision Head that produces context-aware visual representations, which are then rendered into sketches by a pretrained Sketch Decoder. This approach enables complex visual reasoning tasks like maze planning while maintaining the backbone model's reasoning capabilities. Experiments on the new MazePlanning dataset demonstrate that Latent Sketchpad delivers comparable or superior performance to its backbone models while providing interpretable visual traces of the reasoning process.

## Method Summary
Latent Sketchpad builds on connector-based MLLMs (Gemma3 and Qwen2.5-VL) by adding a Context-Aware Vision Head that generates visual latents during autoregressive reasoning. The Vision Head uses 2 cross-attention layers and 8 self-attention layers to produce continuous visual latents conditioned on both global context (previous image latents) and local context (partial latents within current image). These latents are trained with L1 regression loss while the backbone remains frozen. A pretrained Sketch Decoder (12 encoder + 12 decoder transformer layers) renders these latents into interpretable sketches using an AlignerNet to project to SDXL-VAE latent space. The framework is trained on the MazePlanning dataset with maze sizes 3×4 to 5×5, achieving success rates comparable to or exceeding baseline models.

## Key Results
- Achieves 100% Success Rate on training mazes (3×4 to 5×5) compared to 90.2% for Qwen2.5-VL
- Maintains 88.4% Success Rate on 6×6 out-of-distribution mazes vs 83.0% for Qwen2.5-VL
- Improves Layout Consistency Rate from 58.3% to 98.6% for visual trace quality
- Shows effective cross-model transfer from Gemma3 to Qwen2.5-VL with only Vision Head trained

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Context-aware visual latent generation enables coherent, cumulative visual reasoning across multiple steps.
- **Mechanism:** The Vision Head maintains both global context (all preceding image latents) and local context (partial latents within current image), applying causal cross-attention to retrieve visual cues from prior context and causal self-attention for intra-image coherence. This dual-context design propagates spatial information forward through reasoning chain.
- **Core assumption:** Visual details attenuate during long-range reasoning in standard MLLM hidden states; explicit context injection recovers this.
- **Evidence anchors:**
  - [abstract] "a Context-Aware Vision Head autoregressively produces visual representations"
  - [Section 2.2] "the Vision Head explicitly perform visual generation by leveraging both: 1) Global Context: the latents of all preceding images... 2) Local Context: the partial latents already produced within the current image"
  - [corpus] Monet (2511.21395) similarly argues that reasoning in latent visual space extends beyond images and language.

### Mechanism 2
- **Claim:** Operating in continuous latent space (rather than discrete visual tokens) preserves spatial-semantic fidelity for reasoning.
- **Mechanism:** The Vision Head predicts continuous visual latents supervised via L1 regression against pretrained vision encoder features. These latents remain in continuous space during reasoning and are only optionally decoded to pixels by separate Sketch Decoder for interpretability.
- **Core assumption:** Discrete tokenization introduces information loss or optimization bias toward realism over reasoning-relevant abstractions.
- **Evidence anchors:**
  - [abstract] "repurpose them to support generative visual thought without compromising reasoning ability"
  - [Section 2.2] "latent-level regression loss between the predicted context-enriched latent l* and the target latent lXk, which is obtained from pretrained visual features"
  - [Section 4.4] "L1 loss consistently outperforms cosine similarity... directly minimizing element-wise distance in latent space better preserves the spatial and semantic fidelity"
  - [corpus] LaViT (2601.10129) aligns latent visual thoughts for multimodal reasoning.

### Mechanism 3
- **Claim:** Freezing backbone and training only Vision Head preserves pretrained reasoning capacity while adding visual generation.
- **Mechanism:** The Vision Head is trained from scratch with MLLM backbone frozen, isolating visual latent generation learning from backbone's language-vision alignment. This enables plug-and-play deployment across different MLLMs without retraining full model.
- **Core assumption:** Backbone's visual features are already sufficiently rich; only projection/generation head is needed.
- **Evidence anchors:**
  - [abstract] "building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process"
  - [Section 2.2] "The Vision Head is trained from scratch using the regression loss Lreg, while keeping all parameters of the MLLM frozen"
  - [Table 1] GPT-4o + LS shows +3.8% average SR improvement with only Vision Head trained on Qwen2.5-VL, then applied to GPT-4o.

## Foundational Learning

- **Concept: Autoregressive generation with cross-attention**
  - **Why needed here:** Vision Head generates visual latents token-by-token conditioned on both text context and prior visual latents via causal cross-attention.
  - **Quick check question:** Can you explain how causal masking prevents attending to future tokens in both self-attention and cross-attention?

- **Concept: VAE latent spaces and reconstruction**
  - **Why needed here:** Sketch Decoder aligns ViT features to SDXL-VAE latent space, requiring understanding of VAE encoders/decoders and posterior distributions.
  - **Quick check question:** What does the VAE encoder produce that the AlignerNet must predict—deterministic codes or a distribution?

- **Concept: Connector-based MLLM architecture**
  - **Why needed here:** Latent Sketchpad builds on connector-based MLLMs where vision encoder outputs are projected to LLM embedding space; understanding this data flow is essential for modification.
  - **Quick check question:** In a connector-based MLLM, where do the visual embeddings get concatenated with text embeddings, and what module must be adapted for new visual tasks?

## Architecture Onboarding

- **Component map:** Image → Vision Encoder → Connector → Backbone → Vision Head → Visual Latents → (optional) Sketch Decoder → Sketch

- **Critical path:**
  1. Input image → Vision Encoder → Connector → Backbone embeddings
  2. Backbone generates text until `<start_of_image>`
  3. Vision Head attends to global context (previous image latents) + local context (current partial latents) → produces visual latent token
  4. Latent projected back to embedding space → continues autoregression
  5. After `nv` tokens, `<end_of_image>` → resume text
  6. Optionally: visual latents → Sketch Decoder → interpretable sketch

- **Design tradeoffs:**
  - **L1 vs. cosine loss:** L1 preserves spatial fidelity better (Table 4); cosine may lose fine-grained structure
  - **Frozen vs. adapted connector:** Freezing preserves plug-and-play but severely impairs spatial understanding; adaptation required for downstream tasks
  - **Continuous latents vs. discrete tokens:** Continuous preserves reasoning fidelity; discrete shows layout instability but may be more deployment-flexible

- **Failure signatures:**
  - **Layout drift/teleportation:** paths through walls or sudden jumps (Figure 11) → check global context attention integrity
  - **Uninterpretable outputs:** chaotic images (Figure 9a) → modality alignment missing; connector or AlignerNet not trained
  - **OOD degradation:** gradual loss of position tracking in larger mazes → insufficient training data for encoder dimensionality

- **First 3 experiments:**
  1. **Ablate context sources:** Train Vision Head with (a) global context only, (b) local context only, (c) both; measure Layout Consistency Rate and Success Rate to quantify each context's contribution.
  2. **Loss function comparison:** Train identical Vision Heads with L1, cosine, and combined losses; evaluate visual reconstruction SSIM and downstream reasoning SR to validate L1 superiority claim.
  3. **Cross-backbone transfer:** Train Vision Head on Gemma3, apply to Qwen2.5-VL with frozen backbone; measure whether gains transfer or if connector dimension mismatch causes failure.

## Open Questions the Paper Calls Out
None

## Limitations
- **Dataset Generalization:** MAZEPLANNING dataset was constructed using GPT-4o with unspecified prompts, raising concerns about whether model learns reasoning strategies or memorizes synthetic patterns.
- **Architecture Specificity:** Paper lacks critical implementation specifics for Vision Head architecture including layer dimensions and exact mechanism for combining global/local contexts.
- **Task Scope:** Demonstrated effectiveness only on maze planning; performance on non-connector architectures or fundamentally different visual reasoning tasks remains unknown.

## Confidence
- **High Confidence:** Core mechanism of generating visual latents during autoregressive reasoning is technically sound; ablation studies provide robust evidence for L1 loss superiority; Sketch Decoder architecture and training procedure are well-specified.
- **Medium Confidence:** MazePlanning dataset construction and task design are appropriate; claim of "coherent, cumulative visual reasoning" is supported by qualitative examples but lacks quantitative measures of reasoning chain coherence.
- **Low Confidence:** Assertion that continuous latent space inherently preserves reasoning fidelity over discrete tokens lacks ablation studies testing different tokenization strategies; plug-and-play transfer claim weakly supported with only one transfer example.

## Next Checks
1. **Context Ablation Study:** Systematically train Vision Heads with global context only, local context only, and both contexts disabled to quantify each context source's contribution to Layout Consistency Rate and Success Rate.
2. **Cross-Task Generalization:** Apply pretrained Vision Head and Sketch Decoder to fundamentally different visual reasoning task (e.g., visual arithmetic or spatial puzzle solving) without additional fine-tuning to test transfer beyond maze-specific patterns.
3. **Discrete vs. Continuous Ablation:** Replace continuous latent generation with discrete visual tokens while keeping all other components identical to directly test whether continuous space advantage is intrinsic or incidental.