---
ver: rpa2
title: 'Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic
  Model Checking'
arxiv_id: '2508.00500'
source_url: https://arxiv.org/abs/2508.00500
tags:
- safety
- agent
- runtime
- enforcement
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pro2Guard introduces a proactive runtime enforcement framework
  for LLM agents that anticipates safety violations by learning probabilistic models
  of agent behavior and intervening before violations occur. The core innovation is
  constructing a Discrete-Time Markov Chain from execution traces, then estimating
  the probability of reaching unsafe states at runtime to trigger early interventions
  when risk exceeds a threshold.
---

# Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking

## Quick Facts
- arXiv ID: 2508.00500
- Source URL: https://arxiv.org/abs/2508.00500
- Reference count: 36
- Primary result: Pro2Guard achieves 100% prediction accuracy for traffic violations with up to 38.66 seconds advance warning using probabilistic model checking of learned agent behavior

## Executive Summary
Pro2Guard introduces a proactive runtime enforcement framework that anticipates safety violations in LLM agents by learning probabilistic models of agent behavior and intervening before violations occur. The system constructs a Discrete-Time Markov Chain from execution traces, then estimates the probability of reaching unsafe states at runtime to trigger early interventions when risk exceeds a threshold. Evaluated across autonomous vehicles and embodied agents, Pro2Guard reduces manual rule-writing effort, provides interpretable probabilistic explanations, and adds only 5-30ms overhead for embodied agents and 100ms for AV scenarios.

## Method Summary
Pro2Guard learns a probabilistic model of agent behavior from execution traces and uses it to predict future safety violations before they occur. The framework defines domain-specific predicates from safety specifications, collects execution traces, and learns a Discrete-Time Markov Chain with valid-transition-aware Laplace smoothing. At runtime, it computes the probability of reaching unsafe states given the current trajectory and intervenes (either stopping or prompting reflection) when this probability exceeds a threshold. The system reduces manual rule-writing burden while maintaining safety guarantees through probabilistic prediction rather than static guardrails.

## Key Results
- 100% prediction accuracy for traffic law violations and collisions in AV scenarios with up to 38.66 seconds advance warning
- Enforces safety on 93.6% of unsafe embodied agent tasks while maintaining 80.4% task completion rate
- Runtime overhead of 5-30ms for embodied agents and 100ms for AV scenarios

## Why This Works (Mechanism)

### Mechanism 1: Valid-Transition Aware Abstraction
The framework maps infinite concrete states to finite symbolic predicates, learning behavioral probabilities from sparse data while maintaining semantic consistency. By adding probability mass only to semantically feasible transitions using valid-transition Laplace smoothing, the model prevents learning logically impossible state changes while handling sparse training traces.

### Mechanism 2: Probabilistic Reachability via Monitor Synchronization
By composing the learned DTMC with deterministic safety monitors, the system quantifies future risk of temporal logic violations. The synchronous product of monitor automata and DTMC allows runtime calculation of violation probabilities, enabling early intervention when risk exceeds thresholds.

### Mechanism 3: Reflection-Based Prompt Augmentation
Instead of hard-stopping, the framework augments agent prompts with interpretable risk context (state + probability), leveraging LLM reasoning to generate safer plans. This balances safety and task completion by enabling "reflection" strategies that correct course without complete intervention.

## Foundational Learning

- **Discrete-Time Markov Chains (DTMCs)**: Core engine for modeling state transitions and probability flow. Quick check: How does Laplace smoothing affect transition probabilities that were never observed in training traces?
- **Computation Tree Logic (CTL) & Temporal Properties**: Safety rules often involve temporal conditions (e.g., "if trigger, then response within K steps"). Quick check: How does the framework convert bounded liveness properties into safety property checks?
- **Probably Approximately Correct (PAC) Learning**: Framework claims statistical reliability through PAC bounds. Quick check: If state space expands, how does required sample size change to maintain same PAC guarantees?

## Architecture Onboarding

- **Component map**: Trace Collector -> Abstraction Layer -> Model Learner -> Runtime Monitor -> Intervention Module
- **Critical path**: Runtime Monitor's inference step is the latency bottleneck, requiring probability calculations to stay under decision latency budgets (<30ms for embodied, <100ms for AV)
- **Design tradeoffs**: Threshold θ balances recall vs. precision; intervention mode (stop vs. reflect) trades safety vs. task completion; state granularity affects inference speed vs. risk model fidelity
- **Failure signatures**: Lagging warnings (coarse abstraction), thrashing (over-sensitive threshold), silent violations (distribution shift)
- **First 3 experiments**: 1) Predicate sensitivity analysis varying abstraction predicates, 2) Threshold calibration sweeping θ to find Pareto frontier, 3) Advance warning timing measuring time between risk trigger and actual violation

## Open Questions the Paper Calls Out

- **Automated abstraction**: Can domain-specific predicate definition be automated or transferred across domains without manual expert input?
- **Non-stationary environments**: How does Pro2Guard perform when transition dynamics change during deployment?
- **Optimal intervention strategy**: What intervention approach maximizes both safety and task completion?

## Limitations

- Performance depends heavily on quality and coverage of domain predicates, which are not fully specified for AV and embodied agent domains
- Claims 100% prediction accuracy but lacks false positive rate reporting and distribution shift evaluation
- Reflection-based intervention effectiveness contingent on underlying LLM's reasoning capabilities

## Confidence

- **High Confidence**: DTMC learning with valid-transition smoothing is well-founded (Sections 3.2-4.1)
- **Medium Confidence**: Advance warning claims lack baseline comparisons and uncertainty quantification
- **Medium Confidence**: Task completion vs. safety tradeoff demonstrated but LLM specifications unclear

## Next Checks

1. Evaluate Pro2Guard on traffic scenarios with weather/lighting conditions not present in training data to assess robustness
2. Systematically vary number and granularity of predicates to find optimal state space complexity vs. risk discrimination accuracy
3. Conduct user studies to assess whether LLM-generated reflection responses actually produce safer outcomes compared to stopping interventions