---
ver: rpa2
title: 'BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting
  in Bengali'
arxiv_id: '2601.01778'
source_url: https://arxiv.org/abs/2601.01778
tags:
- bengali
- transcription
- system
- word
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BanglaIPA is the first end-to-end IPA transcription system for
  Bengali that supports both standard language and six regional dialects, along with
  context-dependent numeral handling. It uses a contextual rewriting step via LLM
  to convert numerals into word forms, followed by a Transformer-based model with
  a novel State Alignment algorithm for efficient handling of out-of-vocabulary characters.
---

# BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali

## Quick Facts
- arXiv ID: 2601.01778
- Source URL: https://arxiv.org/abs/2601.01778
- Reference count: 20
- BanglaIPA achieves 11.4% mean WER on Bengali IPA transcription, improving upon baseline MT5/UMT5 models by 58.4–78.7%

## Executive Summary
BanglaIPA is the first end-to-end IPA transcription system for Bengali that handles both standard language and six regional dialects while supporting context-dependent numeral handling. The system uses a two-stage approach: first, a large language model rewrites numerals into their contextually appropriate word forms to resolve pronunciation ambiguity, then a Transformer-based model with a novel State Alignment algorithm generates IPA transcriptions. Evaluated on the DUAL-IPA dataset, BanglaIPA achieves a mean word error rate of 11.4%, significantly outperforming baseline models while demonstrating consistent performance across dialects.

## Method Summary
BanglaIPA uses a contextual rewriting step via GPT-4.1-nano to convert numerals into word forms, followed by a Transformer-based model with a novel State Alignment (STAT) algorithm for handling out-of-vocabulary characters. The STAT algorithm segments words based on character set membership, applying model-based transcription only to valid Bengali segments while preserving foreign characters unchanged. The system is trained from scratch on dialect-diverse data rather than fine-tuning multilingual models, using an 8.6M parameter encoder-decoder architecture with character-level input synthesis.

## Key Results
- Mean WER of 11.4% across all dialects, improving upon MT5 and UMT5 baselines by 58.4–78.7%
- Lowest WER of 10.4% on Rangpur dialect with consistent performance across all six dialects (10.7%-14.3%)
- LLM-based rewriting reduces numeral transcription errors from 27% to 1.3% on an 18-sentence test set

## Why This Works (Mechanism)

### Mechanism 1
LLM-based contextual rewriting of numerals substantially reduces pronunciation ambiguity errors before IPA generation. A large language model receives full sentence context and rewrites numeric expressions into their contextually appropriate word forms, disambiguating cases where identical digits have different pronunciations (e.g., "1 dollar" vs. "1st place"). The LLM can reliably infer correct linguistic interpretation from surrounding context, with hallucination errors remaining acceptably low.

### Mechanism 2
The State Alignment (STAT) algorithm enables robust handling of out-of-vocabulary characters by selectively applying model-based transcription only to valid Bengali subword segments. Each word is segmented using a predefined Bengali character set, with segments containing valid characters receiving state=true (model-based IPA generation) and segments with foreign characters/symbols receiving state=false (passed through unchanged). This prevents the model from attempting to transcribe unseen character classes.

### Mechanism 3
Training a lightweight Transformer from scratch on dialect-diverse data yields more consistent cross-dialect performance than fine-tuning pretrained multilingual models. Rather than fine-tuning MT5/UMT5, the authors train a small (8.6M parameter) encoder-decoder Transformer directly on the DUAL-IPA dataset covering standard Bengali and six regional dialects. The character-based vocabulary with word-level alignment allows the model to learn dialect-specific phonological patterns without interference from unrelated multilingual pretraining.

## Foundational Learning

- **Concept: Grapheme-to-Phoneme (G2P) Conversion**
  - Why needed here: The core task is converting Bengali orthography (graphemes) to IPA symbols (phonemes). Understanding that G2P is inherently ambiguous—multiple pronunciations can map to the same spelling—is essential for grasping why contextual rewriting matters.
  - Quick check question: Given the English word "read," how would you determine whether to transcribe it as /riːd/ or /red/ without sentence context?

- **Concept: Subword Tokenization and Alignment**
  - Why needed here: The STAT algorithm operates at the subword level, segmenting words based on character set membership. Understanding how tokenization boundaries affect model inputs clarifies why the algorithm uses character-level splitting for IPA generation.
  - Quick check question: If a Bengali word contains an embedded English acronym (e.g., "WHO" within a sentence), how should tokenization handle the boundary between scripts?

- **Concept: Word Error Rate (WER) as Evaluation Metric**
  - Why needed here: All performance claims use WER. Understanding that WER counts insertions, deletions, and substitutions at the word level—normalized by reference length—helps interpret the 11.4% mean WER and baseline comparisons.
  - Quick check question: If a system transcribes "the cat sat" as "the cat sits," is the WER 25% or 33%, and why does edit distance matter?

## Architecture Onboarding

- **Component map:** Input text -> Contextual Rewriting -> De-duplication -> STAT segmentation -> (for state=true segments) Space insertion -> Vectorization -> Transformer inference -> Space removal -> Merge with state=false segments -> Dictionary update -> Full sequence rebuild

- **Critical path:** Input text → Contextual Rewriting → De-duplication → STAT segmentation → (for state=true segments) Space insertion → Vectorization → Transformer inference → Space removal → Merge with state=false segments → Dictionary update → Full sequence rebuild

- **Design tradeoffs:**
  - LLM dependency vs. accuracy: GPT-4.1-nano adds latency and API cost but achieves 27%→1.3% numeral error reduction; a future small language model replacement is mentioned
  - Scratch training vs. fine-tuning: Training from scratch yields consistent dialect performance but requires dialect-diverse training data; limits zero-shot generalization to unseen dialects
  - Dictionary caching vs. memory: Precomputed word-IPA mappings improve inference efficiency but require memory proportional to vocabulary size

- **Failure signatures:**
  1. LLM hallucination in numeral rewriting: Incorrect word forms propagate through entire pipeline; detectable via validation against expected word-form patterns
  2. Character set gaps in STAT: If predefined Bengali character set excludes valid characters, they will be incorrectly passed through
  3. Dialect mismatch at inference: Input from unrepresented dialects may produce elevated WER; monitor per-region error rates

- **First 3 experiments:**
  1. Unit test the STAT algorithm: Create synthetic words mixing Bengali characters, Latin characters, and symbols; verify state assignments match expectations and merged output correctly reconstructs the intended IPA sequence
  2. Ablate the contextual rewriting module: Process the 18-sentence numeral test set through the pipeline with and without LLM rewriting; confirm the 27%→1.3% WER reduction is reproducible
  3. Cross-dialect generalization test: Hold out one regional dialect entirely from training, evaluate WER on held-out dialect, and compare against in-distribution dialects to quantify generalization gap

## Open Questions the Paper Calls Out
- Can a distilled small language model (SLM) effectively replace GPT-4.1-nano for contextual numeral rewriting while maintaining the 1.3% error rate and reducing computational overhead?
- How does BanglaIPA’s performance degrade when processing regional Bengali dialects excluded from the six represented in the DUAL-IPA training set?
- To what degree do the "occasional hallucinated outputs" from the LLM rewriting stage propagate as errors in the final IPA transcription?

## Limitations
- Reliance on future LLM (GPT-4.1-nano) introduces significant reproducibility concerns
- Performance on dialects outside the six represented regions remains untested
- Exact Transformer architecture parameters beyond parameter count remain unspecified

## Confidence
- **High Confidence:** The 27%→1.3% numeral error reduction achieved through LLM rewriting is well-supported by direct comparison within the 18-sentence test set. The consistent cross-dialect WER performance (10.7%-14.3%) versus baseline variance (27.8%-106.4%) is empirically demonstrated.
- **Medium Confidence:** The claim that scratch training outperforms fine-tuning for dialect consistency is supported by comparative results but lacks ablation studies isolating the architectural contribution from data quality effects.
- **Low Confidence:** The claim about future LLM replacement feasibility lacks technical specification of model size, architecture, or performance targets for the proposed smaller model.

## Next Checks
1. STAT Algorithm Boundary Testing: Construct test cases mixing Bengali characters, Latin script, digits, and symbols in various configurations to verify that the state assignments and merging behavior correctly preserve non-Bengali content while transcribing Bengali segments.
2. Numeral Rewriting Ablation: Process the 18-sentence numeral test set with and without the LLM rewriting stage, measuring WER change to confirm the reported 27%→1.3% improvement.
3. Dialect Generalization Evaluation: Hold out one regional dialect from training data, evaluate WER on the held-out dialect, and compare against in-distribution dialects to quantify the model's ability to generalize to unseen phonological patterns.