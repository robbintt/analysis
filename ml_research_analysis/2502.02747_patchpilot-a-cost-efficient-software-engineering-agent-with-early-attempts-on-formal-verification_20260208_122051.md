---
ver: rpa2
title: 'PatchPilot: A Cost-Efficient Software Engineering Agent with Early Attempts
  on Formal Verification'
arxiv_id: '2502.02747'
source_url: https://arxiv.org/abs/2502.02747
tags:
- patchpilot
- patch
- issue
- code
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PatchPilot is a rule-based software patching framework that balances
  performance, stability, and cost-efficiency. It uses a five-component workflow:
  reproduction, hierarchical localization, two-phase generation (planning + generation),
  validation with both PoC and functionality tests, and iterative refinement.'
---

# PatchPilot: A Cost-Efficient Software Engineering Agent with Early Attempts on Formal Verification

## Quick Facts
- arXiv ID: 2502.02747
- Source URL: https://arxiv.org/abs/2502.02747
- Authors: Hongwei Li; Yuheng Tang; Shiqi Wang; Wenbo Guo
- Reference count: 40
- Primary result: Rule-based patching framework achieving 45.33% resolved rate on SWE-bench at under $1 per instance

## Executive Summary
PatchPilot is a rule-based software patching framework that balances performance, stability, and cost-efficiency. It uses a five-component workflow: reproduction, hierarchical localization, two-phase generation (planning + generation), validation with both PoC and functionality tests, and iterative refinement. PatchPilot outperforms all open-source methods on SWE-bench benchmarks while maintaining low cost and higher stability than agent-based approaches. The refinement component is particularly effective, improving resolved rates by enabling iterative improvements based on validation feedback rather than regenerating patches from scratch.

## Method Summary
PatchPilot employs a five-component rule-based workflow for automated software patching. The reproduction component uses self-reflection to generate PoC tests (max 7 iterations) and identifies covered files plus 3 benign test files. Hierarchical localization progressively narrows from file to function to line level using specialized search tools, with a review step expanding context by ±15 lines. Two-phase generation first creates detailed plans (max 3 steps) then executes with diverse prompt strategies. Validation runs both PoC and functionality tests, ranking patches using a formula based on passing test ratios. Iterative refinement uses validation feedback to improve patches, re-running localization if no progress occurs. The system processes N=4 patches per batch with Nmax=12 total attempts.

## Key Results
- Resolved rate of 45.33% on SWE-bench Lite, outperforming all open-source baselines
- Cost efficiency under $1 per instance while maintaining competitive performance
- Iterative refinement improves resolved rates by 3.66% (41.67% to 45.33%)
- Higher stability than agent-based approaches through rule-based planning constraints

## Why This Works (Mechanism)

### Mechanism 1: Rule-Based Planning Reduces Aggregate LLM Randomness
Pre-defining the patching workflow limits LLM decision points, reducing variance across runs. Agent-based planning lets LLMs dynamically determine when and which components to call, compounding probabilistic uncertainty across each decision. Rule-based planning constrains the LLM to content-generation roles within a fixed scaffold, limiting failure propagation.

### Mechanism 2: Iterative Refinement Enables Progressive Fix Quality
Refining partially-correct patches based on validation feedback is more effective than regenerating from scratch. The refinement component preserves partial correctness while targeting specific failures, contrasting with regeneration which discards all progress. Validation feedback explicitly guides the LLM to correct only failed test cases.

### Mechanism 3: Two-Phase Generation with Context Retrieval Improves Patch Completeness
Separating planning from generation, combined with expanded context retrieval, produces more complete patches. The planning phase forces explicit multi-step reasoning before code emission, reducing the tendency to produce incomplete single-location fixes. The localization review step retrieves related code snippets beyond the minimal root cause, providing necessary context for cross-location modifications.

## Foundational Learning

- **Concept**: Hierarchical Code Localization (File → Function → Line)
  - Why needed here: Direct line-level localization overloads LLM reasoning; hierarchical filtering reduces search space progressively.
  - Quick check question: Can you explain why starting at file-level before function-level reduces false positives in large codebases?

- **Concept**: Chain-of-Thought Prompting for Code Tasks
  - Why needed here: Two-phase generation (planning + execution) is an instance of CoT applied to patch synthesis.
  - Quick check question: How does explicit planning before generation differ from implicit reasoning in standard prompting?

- **Concept**: Test-Driven Validation with PoC and Functionality Tests
  - Why needed here: Patches must fix the issue (PoC test) without breaking existing behavior (functionality tests); both are required to avoid false positives.
  - Quick check question: Why would a patch that passes a PoC test still be rejected by the validation component?

## Architecture Onboarding

- **Component map**: Reproduction -> Localization -> Generation -> Validation -> (if failed) Refinement -> Generation loop until qualified patch or N_max reached
- **Critical path**: Reproduction → Localization → Generation → Validation → (if failed) Refinement → Generation loop until qualified patch or N_max reached
- **Design tradeoffs**: Rule-based vs agent-based: Stability + cost vs potential for higher peak performance; Refinement vs regeneration: Preserves partial progress but may trap in local optima; Diverse prompt strategies (minimal/comprehensive/standard): Increases coverage but raises token costs
- **Failure signatures**: Localization returns insufficient context → patches incomplete; PoC-only validation passes → functionality broken (see ablation: PoC-only drops to 37%); Refinement without progress → triggers re-localization; if still stuck, patch fails
- **First 3 experiments**:
  1. Run ablation removing only the refinement component on a 50-instance subset; measure resolved rate delta and cost savings.
  2. Compare stability: Run PatchPilot vs OpenHands on 20 instances × 5 runs each; compute standard deviation of resolved rates.
  3. Test localization variants: Hierarchical with tools vs simple LLM query; measure precision@5 for file-level localization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid architecture combining agent-based and rule-based planning achieve higher resolved rates while preserving the stability and cost-efficiency of rule-based systems?
- Basis in paper: Conclusion: "First, we can explore combining agent-based planning and rule-based planning, which potentially can reach a higher resolved rate..."
- Why unresolved: Agent-based methods achieve high performance but lack stability; rule-based methods are stable but rigid. The optimal integration strategy is undefined.
- What evidence would resolve it: A system matching OpenHands' resolved rate while maintaining PatchPilot's low standard deviation and sub-$1 cost.

### Open Question 2
- Question: Would fine-tuning specialized patching models mitigate the tendency of general LLMs to propose overly simple or logic-breaking patches?
- Basis in paper: Conclusion: "...failure instances are caused by the fact that LLMs tend to give overly simple patches. It is worth investigating fine-tuning specialized patching models..."
- Why unresolved: General models often fail to understand complex program logic, proposing surface-level fixes that break functionality.
- What evidence would resolve it: A specialized model demonstrating higher semantic correctness on complex logic bugs compared to GPT-4o or Claude-3.5-Sonnet.

### Open Question 3
- Question: How can formal verification components be advanced to handle intricate data structures and dynamic function calls in Python patching?
- Basis in paper: Section 5: "...current limitations of Python-based formal verifiers, which struggle with intricate data structures and deeply nested or dynamic function calls."
- Why unresolved: Current tools could only verify 11 out of 300 instances due to Python's dynamic nature.
- What evidence would resolve it: A verification pipeline successfully applying logical contracts to a significant majority (e.g., >50%) of SWE-bench instances.

## Limitations

- **Prompt and Tool Implementation Gaps**: Critical component prompts and tool implementations are only partially specified, preventing faithful reproduction.
- **Cost Metric Ambiguity**: The $1 per instance claim lacks transparent token accounting and cost breakdown details.
- **Limited Benchmark Scope**: Testing confined to Python repositories only, with unknown generalizability to other languages.

## Confidence

- **High Confidence**: Resolved Rate improvements over baselines (45.33% vs 41.67% without refinement) are well-supported by ablation studies and SWE-bench Lite/Verified results.
- **Medium Confidence**: Cost-efficiency claims (under $1/instance) and stability advantages are mechanism-plausible but lack complete empirical validation.
- **Low Confidence**: Claims about superiority across all open-source methods require scrutiny due to potential implementation differences in baselines.

## Next Checks

1. **Stability Verification**: Run PatchPilot vs OpenHands on 20 SWE-bench instances with 5 repeated trials each. Measure resolved rate standard deviation to empirically validate stability claims.

2. **Localization Effectiveness Test**: Implement both PatchPilot's hierarchical localization (with tools) and a simple LLM-based direct localization. Compare file-level precision@5 on 50 validation instances to quantify the contribution of the hierarchical approach.

3. **Cost Accounting Validation**: Implement token counting for all pipeline components (reproduction, localization, generation, validation, refinement) on 20 instances. Verify the under-$1 per instance claim and identify which components dominate costs.