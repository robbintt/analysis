---
ver: rpa2
title: Optimizer Dynamics at the Edge of Stability with Differential Privacy
arxiv_id: '2512.19019'
source_url: https://arxiv.org/abs/2512.19019
tags:
- sharpness
- privacy
- training
- stability
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how differential privacy (DP) modifies optimization
  dynamics in deep learning by comparing Gradient Descent and Adam against their DP
  variants. DP introduces per-example gradient clipping and Gaussian noise, which
  alter curvature and stability patterns observed in non-private training.
---

# Optimizer Dynamics at the Edge of Stability with Differential Privacy

## Quick Facts
- arXiv ID: 2512.19019
- Source URL: https://arxiv.org/abs/2512.19019
- Reference count: 29
- Primary result: Differential privacy modifies optimization dynamics, inducing privacy-dependent stability regimes distinct from classical Edge of Stability behavior

## Executive Summary
This work investigates how differential privacy (DP) modifies optimization dynamics in deep learning by comparing Gradient Descent and Adam against their DP variants. DP introduces per-example gradient clipping and Gaussian noise, which alter curvature and stability patterns observed in non-private training. The study tracks sharpness (maximum Hessian eigenvalue) and loss evolution across learning rates and privacy budgets. Results show that DP substantially reshapes stability dynamics: for DP-GD, larger privacy budgets yield higher stabilized sharpness, while smaller learning rates often exhibit slow convergence without reaching classical Edge of Stability thresholds. For DP-Adam, both raw and preconditioned sharpness stabilize at finite values below non-private Adaptive Edge of Stability thresholds, with sharpness scaling inversely with learning rate compared to standard Adam.

## Method Summary
The paper compares optimization dynamics of standard GD and Adam with their DP counterparts on a two-hidden-layer MLP trained on CIFAR-10. DP is implemented via Opacus with ghost clipping and RDP accounting. Sharpness is estimated using power iteration on the full Hessian at each training step, with preconditioned sharpness computed for Adam by extracting the preconditioner matrix from optimizer state. Experiments sweep learning rates and privacy budgets to characterize how DP modifies stability thresholds and sharpness evolution.

## Key Results
- DP-GD exhibits privacy-dependent sharpness stabilization below classical 2/η thresholds, with larger ε yielding higher stabilized sharpness
- DP-Adam shows coupled raw and preconditioned sharpness stabilization, differing from standard Adam's Adaptive Edge of Stability behavior
- Sharpness stabilization depends jointly on learning rate and privacy budget, requiring co-tuning rather than sequential optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient clipping bounds per-example gradient norms, altering the effective update direction and limiting curvature growth along the optimization trajectory.
- **Mechanism:** The clipping operation $\tilde{g}_i = g_i \cdot \min(1, C/\|g_i\|_2)$ rescales gradients that exceed the threshold $C$. When gradients are clipped, updates no longer follow the true gradient direction, potentially steering the optimizer toward regions with different sharpness characteristics than non-private training would reach.
- **Core assumption:** Assumption: Clipping uniformly rescales large gradients rather than selectively affecting specific directions.
- **Evidence anchors:**
  - [section 2.3] "For each data point, the per-sample gradient $g_i = \nabla_\theta \ell(\theta; x_i, y_i)$ is clipped to norm $C > 0$"
  - [section 5.1] "DP can delay sharpness growth, shift or suppress stability thresholds, or create long-lived flat-plateau phases"
  - [corpus] Limited direct evidence; related work "Stabilization of Perturbed Loss Function" mentions clipping effects on convergence but without explicit curvature analysis.
- **Break condition:** If clipping threshold $C$ is set larger than typical gradient magnitudes, clipping becomes inactive and this mechanism collapses to standard GD.

### Mechanism 2
- **Claim:** Gaussian noise injection increases effective curvature variability and can halt progressive sharpening before classical EoS thresholds are reached.
- **Mechanism:** Noise $\mathcal{N}(0, \sigma^2 C^2 I)$ perturbs each update independently. Higher noise (lower $\varepsilon$) creates more stochasticity in the trajectory, preventing the optimizer from sustaining the coherent descent needed to enter high-curvature regions. This produces stabilized sharpness below the $2/\eta$ threshold.
- **Core assumption:** Assumption: Noise magnitude scales with privacy budget such that smaller $\varepsilon$ implies substantially larger $\sigma$.
- **Evidence anchors:**
  - [abstract] "larger privacy budgets yield higher stabilized sharpness, while smaller learning rates often exhibit slow convergence without reaching classical Edge of Stability thresholds"
  - [section 4.3] "Higher privacy budgets again consistently lead to higher sharpness, preserving monotonic scaling with $\varepsilon$"
  - [corpus] "Stabilization of Perturbed Loss Function" proposes eliminating gradient noise entirely, implicitly confirming noise as a destabilizing factor.
- **Break condition:** If noise scale is too small relative to gradient magnitudes, sharpness dynamics approximate non-private behavior.

### Mechanism 3
- **Claim:** For DP-Adam, clipping and noise couple raw and preconditioned sharpness evolution, causing joint stabilization rather than the decoupled dynamics seen in standard Adam.
- **Mechanism:** Standard Adam operates at AEoS where preconditioned sharpness $\lambda_1(P^{-1}H)$ stabilizes while raw sharpness $\lambda_1(H)$ continues growing. DP modifications appear to synchronize these: once preconditioned sharpness plateaus, raw sharpness also flattens. The noise may prevent the gradient accumulation patterns that normally allow raw sharpness to increase independently.
- **Core assumption:** Assumption: The Adam preconditioner $P_t$ responds to privatized gradients similarly enough to maintain meaningful preconditioned sharpness as a stability metric.
- **Evidence anchors:**
  - [section 4.4] "Unlike the non-private case, raw sharpness no longer continues to grow beyond the breakeven point"
  - [section 5.1] "This coupled flattening suggests a DP-modified stability regime in which Adam remains in low-curvature regions for extended periods"
  - [corpus] "DP-MicroAdam" addresses DP training with adaptive optimizers but focuses on compression rather than curvature dynamics.
- **Break condition:** If Adam's second-moment accumulator becomes dominated by noise variance rather than signal, the preconditioner loses meaning and AEoS framework may not apply.

## Foundational Learning

- **Concept:** Edge of Stability (EoS) and the $2/\eta$ threshold
  - **Why needed here:** The paper frames all DP effects relative to classical EoS behavior. Without understanding that standard GD operates near sharpness $\approx 2/\eta$, the significance of DP-induced deviations is unclear.
  - **Quick check question:** If learning rate $\eta = 0.1$, what sharpness value defines the classical stability threshold? (Answer: $2/0.1 = 20$)

- **Concept:** Preconditioned sharpness and Adaptive Edge of Stability
  - **Why needed here:** Adam's behavior is analyzed through preconditioned sharpness $\lambda_1(P^{-1}H)$, not raw sharpness. The paper shows DP decouples this relationship.
  - **Quick check question:** In standard Adam at AEoS, does raw sharpness stabilize or continue growing while preconditioned sharpness plateaus? (Answer: Raw sharpness continues growing)

- **Concept:** Differential privacy composition and privacy budget $\varepsilon$
  - **Why needed here:** The paper sweeps $\varepsilon$ as a core hyperparameter. Understanding that smaller $\varepsilon$ = stronger privacy = more noise is essential for interpreting results.
  - **Quick check question:** If $\varepsilon = 16$ vs. $\varepsilon = 64$, which configuration provides stronger privacy guarantees? (Answer: $\varepsilon = 16$)

## Architecture Onboarding

- **Component map:** Per-example gradient computation → clipping module (norm threshold $C$) → aggregation → Gaussian noise addition ($\sigma$ scaled by $C$) → optimizer step (DP-GD or DP-Adam) → sharpness monitoring via power iteration on Hessian
- **Critical path:** Learning rate $\eta$ and privacy budget $\varepsilon$ must be co-tuned. The paper shows sharpness stabilization depends jointly on both—fixing one while sweeping the other yields incomplete optimization understanding.
- **Design tradeoffs:**
  - Larger $\eta$ → faster convergence but sharper solutions (inverts for DP-Adam vs. standard Adam)
  - Larger $\varepsilon$ (weaker privacy) → higher stabilized sharpness, closer to non-private thresholds
  - Smaller $\eta$ + small $\varepsilon$ → may never reach any stability threshold within practical epoch budgets
- **Failure signatures:**
  - Sharpness plateaus far below expected threshold with monotonic loss but minimal improvement → likely over-noised for the chosen learning rate
  - Sharpness exceeds threshold but with high variance across runs → noise scale may be marginal relative to gradient magnitudes
  - Preconditioned sharpness grows indefinitely without plateau → DP noise may have corrupted Adam's second-moment estimates
- **First 3 experiments:**
  1. **Baseline reproduction:** Train non-private GD and Adam on your target architecture, plot sharpness vs. epoch to confirm EoS/AEoS behavior before adding DP complexity.
  2. **Privacy sweep at fixed LR:** For a single learning rate, sweep $\varepsilon \in \{8, 16, 32, 64\}$ and track both sharpness trajectories and final test accuracy to characterize the privacy-sharpness-utility frontier.
  3. **Joint ($\eta$, $\varepsilon$) grid:** Run a small grid (e.g., 3 LRs × 3 $\varepsilon$ values) to verify whether the paper's finding—that sharpness depends jointly on both—transfers to your model/dataset. Log epoch-to-threshold (if reached) and final sharpness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do gradient clipping and noise injection independently alter sharpness evolution and stability thresholds in differentially private optimization?
- **Basis in paper:** [explicit] Section 6 states future work should "isolate the separate effects of clipping and noise on sharpness and stability."
- **Why unresolved:** The study analyzes the combined DP mechanism (clipping + noise), making it difficult to distinguish whether observed sharpness suppression is due to the direction distortion from clipping or the regularization effect of noise.
- **What evidence would resolve it:** Ablation studies training models with only clipping or only noise added to the optimizer, comparing sharpness trajectories against the combined DP and non-private baselines.

### Open Question 2
- **Question:** Do differentially private optimizers with small learning rates eventually reach a modified stability threshold given sufficient training time, or does the flattening represent a permanent equilibrium?
- **Basis in paper:** [explicit] Section 6 proposes to "explore longer training to determine whether smaller LRs eventually approach DP-modified thresholds."
- **Why unresolved:** The experiments were compute-limited to 5,000 epochs; small learning rates showed slow convergence and progressive sharpening without clearly reaching a stability plateau within this horizon.
- **What evidence would resolve it:** Extending training duration for small learning rates until full convergence to observe if sharpness stabilizes at a finite value or continues rising to a non-private-like threshold.

### Open Question 3
- **Question:** Do the observed DP-modified stability regimes persist when scaling to modern architectures like deep convolutional networks or transformers?
- **Basis in paper:** [explicit] Section 6 explicitly lists plans to "test scaling to larger models and datasets."
- **Why unresolved:** The empirical findings rely on a specific two-hidden-layer fully connected network; it is unknown if the "coupled flattening" of raw and preconditioned sharpness is an artifact of this shallow architecture.
- **What evidence would resolve it:** Replicating the sharpness and loss tracking experiments on standard deep learning benchmarks (e.g., ResNets or Transformers on ImageNet) under similar privacy constraints.

## Limitations

- Narrow experimental scope limited to one architecture (two-hidden-layer MLP) and one dataset (CIFAR-10 subset), raising generalizability concerns
- Unspecified power iteration budget for Hessian eigenvalue estimation may affect sharpness measurement consistency
- Mechanistic relationship between DP noise and Adam's second-moment estimator remains unclear despite observed effects

## Confidence

- **High confidence**: DP-GD exhibits privacy-dependent sharpness stabilization below classical EoS thresholds; larger ε yields higher stabilized sharpness
- **Medium confidence**: DP-Adam shows coupled raw and preconditioned sharpness stabilization, differing from standard Adam's AEoS behavior
- **Medium confidence**: Sharpness stabilization depends jointly on learning rate and privacy budget, requiring co-tuning rather than sequential optimization

## Next Checks

1. **Architecture scaling test**: Reproduce sharpness dynamics for DP-GD and DP-Adam on ResNet-18 trained on CIFAR-10 to verify whether privacy-induced stability modifications generalize beyond MLPs
2. **Dataset diversity validation**: Test on a regression task (e.g., UCI Energy Efficiency) to determine if privacy effects on sharpness are consistent across loss landscape geometries
3. **Mechanism isolation**: Compare DP-Adam with and without second-moment updates (i.e., using clipped/noisy gradients but standard SGD updates) to disentangle noise effects from preconditioner corruption