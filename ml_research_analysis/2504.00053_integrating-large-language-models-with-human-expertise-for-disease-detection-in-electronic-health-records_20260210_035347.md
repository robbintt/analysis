---
ver: rpa2
title: Integrating Large Language Models with Human Expertise for Disease Detection
  in Electronic Health Records
arxiv_id: '2504.00053'
source_url: https://arxiv.org/abs/2504.00053
tags:
- clinical
- diabetes
- mmol
- hypertension
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduced a pipeline integrating generative large language
  models (LLMs) with clinical expert knowledge to detect multiple diseases from electronic
  health records (EHRs) without manual data labeling. Using prompts based on clinical
  diagnosis, treatment, and guidelines, the pipeline processed clinical notes to identify
  acute myocardial infarction, diabetes, and hypertension.
---

# Integrating Large Language Models with Human Expertise for Disease Detection in Electronic Health Records

## Quick Facts
- **arXiv ID:** 2504.00053
- **Source URL:** https://arxiv.org/abs/2504.00053
- **Reference count:** 40
- **Primary result:** LLM-based pipeline detects AMI, diabetes, and hypertension from EHR notes with 88-94% sensitivity, outperforming ICD-10 methods

## Executive Summary
This study introduces a pipeline that integrates generative large language models with clinical expert knowledge to detect multiple diseases from electronic health records without manual data labeling. The method processes clinical notes using prompts based on clinical diagnosis, treatment, and guidelines to identify acute myocardial infarction, diabetes, and hypertension. The pipeline achieves high sensitivity (88-94%) and negative predictive value across all conditions, demonstrating superior performance to traditional ICD-10 coding methods, particularly for diabetes detection. Results show the method's potential for scalable, real-time disease surveillance in healthcare systems.

## Method Summary
The pipeline employs a four-component architecture to detect diseases from unstructured EHR clinical notes. First, it preprocesses clinical notes using prompt-based document type filtering and keyword-based sentence extraction, retaining only relevant document types above the 25th percentile information relevance score. Second, it applies inferential reasoning prompts combined with information extraction prompts to the filtered notes. Third, it runs Mistral-7B-OpenOrca inference with specific temperature and sampling parameters. Finally, it applies rule-based post-processing that compares extracted lab values to clinical thresholds (e.g., glucose ≥11.1 mmol/L for diabetes). The method was validated on 3,088 patients with 551,095 clinical notes from the CREATE database in Alberta, Canada, using reference labels from the APPROACH clinical registry.

## Key Results
- Achieved 88-94% sensitivity and high negative predictive value for AMI, diabetes, and hypertension detection
- Outperformed traditional ICD-10 methods, particularly showing significant improvement for diabetes (82% vs 73% sensitivity)
- Demonstrated potential for real-time disease surveillance through monthly trend alignment with clinical diagnoses

## Why This Works (Mechanism)
The pipeline works by leveraging LLMs' natural language understanding capabilities guided by clinical expertise through carefully designed prompts. Rather than relying on manual feature engineering or traditional rule-based systems, the approach uses clinical domain knowledge encoded in prompt templates to direct the LLM's reasoning process. The two-stage prompting strategy first establishes diagnostic criteria through inferential reasoning, then extracts specific evidence from clinical notes that supports or refutes these criteria. This hybrid approach combines the flexibility and reasoning capabilities of LLMs with the precision of clinical thresholds in post-processing, enabling accurate disease detection without requiring labeled training data.

## Foundational Learning
**Clinical threshold-based reasoning**: Understanding how specific lab values and clinical criteria define disease states (why needed: to design effective post-processing rules; quick check: verify glucose ≥11.1 mmol/L correctly identifies diabetes cases)
**Prompt engineering for clinical inference**: Crafting prompts that guide LLMs to apply diagnostic reasoning rather than simple pattern matching (why needed: to extract clinically meaningful evidence; quick check: test prompts on known positive and negative cases)
**Document relevance scoring**: Using LLM-generated scores to filter relevant clinical note types (why needed: to focus computational resources on informative documents; quick check: compare performance with and without filtering)
**Patient-level aggregation logic**: Combining multiple document-level responses into coherent patient diagnoses (why needed: to handle inconsistent or conflicting evidence across notes; quick check: audit aggregation decisions on edge cases)

## Architecture Onboarding

**Component map**: Preprocessing (document filtering → sentence extraction) -> LLM Inference (reasoning prompts → extraction prompts) -> Post-processing (threshold comparison) -> Patient aggregation

**Critical path**: Document type filtering → keyword sentence extraction → inferential reasoning prompts → information extraction prompts → lab value comparison → patient-level diagnosis

**Design tradeoffs**: The pipeline trades computational efficiency for accuracy by filtering document types based on relevance scores rather than processing all notes, but this requires careful threshold selection. The two-prompt approach balances general reasoning with specific evidence extraction but adds complexity to the inference process.

**Failure signatures**: Low specificity for hypertension (32%) indicates the pipeline may be too sensitive to isolated elevated blood pressure readings without requiring multiple measurements per clinical guidelines. Numerical threshold comparisons may fail when LLM struggles with explicit comparisons like "≥11.1 mmol/L", suggesting the need for rule-based numeric handling rather than LLM inference.

**3 first experiments**: 1) Test the document type filtering approach by varying the percentile threshold (15th, 25th, 35th, 50th) and measuring impact on detection performance; 2) Evaluate the two-prompt strategy by comparing single-prompt vs. dual-prompt performance on the same dataset; 3) Assess the numerical threshold comparison by implementing explicit rule-based numeric comparisons versus LLM-based extraction.

## Open Questions the Paper Calls Out

**Open Question 1**: Does fine-tuning the LLM on domain-specific biomedical data improve disease detection performance compared to the zero-shot prompt-based approach used in this study? The authors note that while fine-tuning might distinguish borderline cases, recent studies show that fine-tuning LLMs on biomedical data without appropriate strategies does not always lead to improved outcomes. A comparative experiment evaluating the sensitivity and specificity of a fine-tuned LLM versus the prompt-based pipeline on the same dataset would resolve this question.

**Open Question 2**: Can the pipeline effectively generalize to non-cardiac cohorts and external databases in different geographic regions? The authors state the pipeline was validated on a cardiac disease cohort in Calgary and has not been evaluated in external databases. The current study used a cohort with high disease prevalence (cardiac-specific), potentially inflating performance metrics compared to general populations. Validation studies applying the identical pipeline to general medicine cohorts or EHR systems from different health authorities would provide evidence.

**Open Question 3**: Can the pipeline be refined to distinguish between active diagnoses, patient history, and "highly suspected" conditions to reduce false positives? The authors report low specificity (e.g., 32% for hypertension) and note that 40-50% of false positives were "highly suspected conditions" and 15% were "history of the conditions." The current prompts and inferential reasoning process appear to conflate historical mentions or risk factors with active disease status. Development and testing of temporal-specific prompts or post-processing rules that categorize extracted evidence into "active," "historical," or "suspected" would address this issue.

## Limitations
- Patient-level aggregation mechanism for handling multiple document responses per condition is unspecified, creating ambiguity in the inference pipeline
- Document type filtering using a 25th percentile threshold lacks validation methodology or sensitivity analysis
- Hypertension detection shows particularly poor specificity (32%), suggesting fundamental limitations in capturing clinical diagnostic criteria through LLM inference alone
- The approach's performance depends heavily on keyword list quality and clinical threshold definitions, which may not generalize across healthcare systems

## Confidence
- **High confidence**: The pipeline architecture (4-component structure) and overall methodology are clearly described
- **Medium confidence**: Performance metrics for AMI and diabetes detection are robust, but hypertension results show concerning limitations
- **Low confidence**: Reproducibility is limited by unspecified implementation details in aggregation logic and prompt formatting

## Next Checks
1. Audit the document-level response aggregation mechanism by implementing multiple candidate strategies and comparing sensitivity/specificity trade-offs against the reported results
2. Test the numerical threshold comparison logic separately from LLM inference by implementing explicit numeric comparison rules to isolate LLM vs. rule-based performance contributions
3. Validate the document type filtering approach by systematically varying the percentile threshold (15th, 25th, 35th, 50th) and measuring impact on detection performance and computational efficiency