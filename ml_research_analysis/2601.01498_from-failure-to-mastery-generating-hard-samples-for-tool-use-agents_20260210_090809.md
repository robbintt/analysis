---
ver: rpa2
title: 'From Failure to Mastery: Generating Hard Samples for Tool-use Agents'
arxiv_id: '2601.01498'
source_url: https://arxiv.org/abs/2601.01498
tags:
- tool
- hard
- data
- arxiv
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HardGen, an automated pipeline for generating
  challenging training samples for tool-use agents. The key innovation is a failure-driven
  approach that constructs an API Graph from model failure cases, synthesizes hard
  traces, abstracts them into advanced tools, and uses feedback-guided refinement
  to produce verifiable multi-step reasoning trajectories.
---

# From Failure to Mastery: Generating Hard Samples for Tool-use Agents

## Quick Facts
- arXiv ID: 2601.01498
- Source URL: https://arxiv.org/abs/2601.01498
- Reference count: 40
- Primary result: 4B model trained with HardGen data achieves 79.14% overall accuracy on BFCLv3, surpassing GPT-5.2, Gemini-3-Pro, and Claude-Opus-4.5

## Executive Summary
HardGen introduces an automated pipeline for generating challenging training samples for tool-use agents by focusing on model failure cases. The key innovation is a failure-driven approach that constructs an API Graph from model failure cases, synthesizes hard traces, abstracts them into advanced tools, and uses feedback-guided refinement to produce verifiable multi-step reasoning trajectories. Evaluated on the BFCLv3 benchmark, a 4B model trained with HardGen data achieves 79.14% overall accuracy, surpassing leading open-source and closed-source models including GPT-5.2, Gemini-3-Pro, and Claude-Opus-4.5. The method demonstrates strong generalization across model scales and architectures, with consistent gains in both single-turn and multi-turn scenarios.

## Method Summary
HardGen operates in three phases: (1) self-evaluation to identify failure APIs and construct a dynamic API Graph, (2) trace sampling and tool abstraction to generate advanced tools and hard queries, and (3) feedback-guided refinement using a Reasoner-Verifier loop to produce high-quality CoTs. The pipeline targets implicit logical dependencies by requiring models to autonomously decompose complex tasks without explicit step-by-step instructions. Training is performed via SFT with LLaMA-Factory or RL with Verl on 27K synthesized trajectories. The approach specifically addresses the gap in current datasets that fail to capture the reasoning challenges of multi-step tool use with prerequisite dependencies.

## Key Results
- 4B model trained with HardGen achieves 79.14% accuracy on BFCLv3, outperforming GPT-5.2, Gemini-3-Pro, and Claude-Opus-4.5
- Hard queries yield +8.75 multi-turn improvement on Qwen3-4B and +7.90 on Llama-3-3B
- Feedback refinement provides +10-12% accuracy gains across 30B-671B models
- HardGen consistently improves both single-turn and multi-turn performance across model scales

## Why This Works (Mechanism)

### Mechanism 1: Failure-Driven Sampling Bias
Targeting model weaknesses yields harder, more valuable training data than random API sampling. A self-evaluation phase identifies "failure APIs" where models err, constructs a dynamic API Graph encoding dependencies, then samples traces that specifically exercise these weak points.

### Mechanism 2: Implicit Logical Bridging via Tool Abstraction
Abstracting multi-step traces into single "advanced tools" enables queries that force models to infer intermediate steps. Given an execution trace (e.g., get_zipcode → buy_tickets), a Tool Maker synthesizes a unified advanced tool (buy_tickets_adv). Hard queries then request the end goal without specifying prerequisites.

### Mechanism 3: Closed-Loop CoT Verification
Iterative error diagnosis and corrective feedback refine reasoning chains more effectively than single-pass generation. A Verifier analyzes incorrect function calls against ground truth, generates targeted hints, and re-prompts the Reasoner. This repeats up to Kmax iterations.

## Foundational Learning

- **Tool Dependency Graphs**
  - Why needed here: HardGen's API Graph encodes prerequisite relationships (Ti → Tj means Tj requires Ti). Understanding directed acyclic graphs and topological ordering is essential for legality-constrained sampling.
  - Quick check question: Given tools A, B, C where B requires A and C requires B, what valid execution orders exist?

- **Chain-of-Thought Reasoning**
  - Why needed here: Phase III generates complex CoTs that must be verifiable. Understanding CoT structure helps diagnose where refinement fails.
  - Quick check question: What distinguishes a CoT that enables verification from one that merely rationalizes a guess?

- **Reinforcement Learning Reward Design**
  - Why needed here: HardGen-4B-RL uses binary rewards based on format + correctness. Understanding reward sparsity and credit assignment is critical for reproducing RL results.
  - Quick check question: If a model produces correct tool calls but malformed reasoning text, what should the reward be under the paper's formulation?

## Architecture Onboarding

- **Component map:**
  - Phase I: Self-evaluation Query Set → Failure API Set → API Graph → Sampler
  - Phase II: Hard Traces → Tool Maker → Advanced Tools → Hard-query Generator
  - Phase III: Hard Queries + Tools → Reasoner ↔ Verifier (loop) → High-quality CoTs
  - Training: SFT (LLaMA-Factory) or RL (Verl) on 27K trajectories

- **Critical path:**
  1. Deploy 2,095-tool environment and run self-evaluation to identify failure APIs
  2. Construct/update API Graph with dependencies and parameter constraints
  3. Sample traces, abstract into advanced tools, generate hard queries
  4. Run Reasoner-Verifier refinement (Kmax=3 recommended)
  5. Train target model on curated dataset

- **Design tradeoffs:**
  - Kmax=3 balances quality vs. compute; higher values yield diminishing returns
  - Generator backbone choice: Qwen3-30B-A3B-Thinking selected for 89% correctness at K=3 with efficient MoE inference
  - Binary reward is simple but may not capture partial credit; more nuanced rewards are unexplored

- **Failure signatures:**
  - Low multi-turn scores with high single-turn scores → hard queries not forcing implicit bridging
  - High rejection rate in refinement loop → verifier hints too vague or tools poorly documented
  - SFT outperforms RL → reward signal too sparse or base model underpowered

- **First 3 experiments:**
  1. Replicate Phase I on a smaller tool set (50-100 APIs) to validate failure API identification correlates with downstream hardness.
  2. Ablate advanced tool abstraction: train on queries generated with vs. without Tadv, measure multi-turn delta.
  3. Sweep Kmax ∈ {1, 2, 3, 5} on a 1K-trajectory subset to confirm paper's claim of diminishing returns beyond 3.

## Open Questions the Paper Calls Out

- Can HardGen generalize effectively to entirely new API ecosystems or highly specialized domains beyond the 2,095-tool environment tested?
- How can HardGen's verification pipeline be adapted for proprietary APIs or tools with complex external dependencies that lack executable sandbox environments?
- Does the failure-driven API Graph constructed from one model architecture transfer effectively to models with fundamentally different failure modes?
- How does Tool Maker's abstraction quality degrade for highly irregular or domain-specific tool chains, and can this be automatically detected?

## Limitations

- Performance claims hinge on quality and generality of failure-driven sampling; failure APIs may reflect model-specific idiosyncrasies rather than general reasoning gaps
- Dependency extraction method for API Graph construction is underspecified
- No direct ablation studies comparing failure-driven sampling to random or heuristic-based sampling
- Binary RL reward may not capture partial credit, limiting fine-grained optimization

## Confidence

- High confidence: SFT-trained model performance (79.14% on BFCLv3) and overall accuracy improvements across benchmarks
- Medium confidence: Claims about failure-driven sampling superiority (no direct comparison to alternative sampling methods)
- Medium confidence: Advanced tool abstraction mechanism (performance gains shown, but no analysis of abstraction granularity or information leakage)
- Low confidence: RL results (binary reward, no partial credit analysis, limited ablation)

## Next Checks

1. Replicate Phase I on a smaller, diverse tool set (50-100 APIs) and compare failure API identification across multiple model architectures to test generalizability.
2. Ablate the advanced tool abstraction: generate and evaluate two training datasets—one with T_adv included, one without—measuring multi-turn reasoning deltas to isolate abstraction impact.
3. Sweep Kmax in the Reasoner-Verifier loop (Kmax ∈ {1, 2, 3, 5}) on a fixed subset of 1K trajectories to confirm diminishing returns and optimal refinement depth.