---
ver: rpa2
title: 'The Persistence of Cultural Memory: Investigating Multimodal Iconicity in
  Diffusion Models'
arxiv_id: '2511.11435'
source_url: https://arxiv.org/abs/2511.11435
tags:
- cultural
- reference
- references
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework to evaluate how diffusion models
  handle multimodal iconicity, the culturally shared correspondence between textual
  concepts and visual representations. The framework disentangles recognition (whether
  a model identifies a cultural reference) from realization (how it depicts it, through
  replication or reinterpretation), using CLIP-based similarity for alignment and
  patch-level DINOv3 analysis for visual reuse.
---

# The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models

## Quick Facts
- **arXiv ID**: 2511.11435
- **Source URL**: https://arxiv.org/abs/2511.11435
- **Reference count**: 40
- **Primary result**: Framework distinguishes recognition vs. realization in diffusion models, showing alignment depends on textual uniqueness and cultural popularity more than training frequency.

## Executive Summary
This work introduces a framework to evaluate how diffusion models handle multimodal iconicity—the culturally shared correspondence between textual concepts and visual representations. The framework disentangles recognition (whether a model identifies a cultural reference) from realization (how it depicts it, through replication or reinterpretation), using CLIP-based similarity for alignment and patch-level DINOv3 analysis for visual reuse. Evaluated across five diffusion models and 767 Wikidata-derived cultural references, the approach distinguishes transformation from replication more effectively than existing similarity-based metrics. Results show that recognition and visual reuse are largely decoupled, with models achieving alignment through either close replication or independent reinterpretation.

## Method Summary
The study builds a dataset of 767 cultural references from Wikidata (374 static artworks/albums, 393 dynamic films/series), filtered to those with >20 sitelinks. Static references use canonical Wikidata images; dynamic references use top 50 Google Image results filtered by CLIP similarity >0.7. For each reference, diffusion models generate images using only the title as prompt. The framework computes CRA (Cultural Reference Alignment) as the proportion of generations with CLIP similarity >0.7 to reference, VR (Visual Reuse) as the fraction of 4×4 patches with DINOv3 similarity >0.6 to any reference patch, VI (Visual Independence) as 1 - VR, and CRT (Cultural Reference Transformation) as CRA × VI. The approach uses CLIP ViT-B/32 for recognition and DINOv3 for patch-level analysis, enabling distinction between close replication and independent reinterpretation.

## Key Results
- Recognition and visual reuse are largely decoupled across models, with alignment achieved through either replication or reinterpretation
- Models retain recognizability under lexical changes, especially with richer visual descriptions, often generating more transformed outputs under perturbations
- Training data frequency alone does not predict alignment; textual uniqueness, reference popularity, and creation date strongly correlate with multimodal iconicity
- The framework distinguishes transformation from replication more effectively than existing similarity-based metrics

## Why This Works (Mechanism)
The framework works by systematically separating two distinct aspects of cultural alignment: whether a model recognizes a reference (recognition) and how it depicts it (realization). By using CLIP for semantic alignment and DINOv3 for patch-level visual analysis, the approach can detect when models achieve recognizability through different mechanisms—either by closely copying reference images or by generating independent reinterpretations that capture the same cultural concept. This dual analysis reveals that models can be culturally aligned without being visually derivative.

## Foundational Learning
- **Multimodal iconicity**: The culturally shared correspondence between textual concepts and visual representations - needed to frame the study of how models capture cultural knowledge across modalities; quick check: verify examples like "Mona Lisa" having consistent visual associations
- **Recognition vs. realization**: Distinguishing whether a model identifies a reference from how it depicts it - needed to separate semantic understanding from visual generation strategy; quick check: examine cases where CRA is high but VR is low
- **CLIP similarity thresholds**: Using cosine similarity >0.7 as alignment criterion - needed for consistent recognition measurement; quick check: verify that threshold achieves high recall and low false positive rate
- **DINOv3 patch analysis**: Comparing 4×4 patch features for visual reuse detection - needed to quantify degree of visual copying vs. transformation; quick check: ensure position-independent comparison across all reference patches
- **Cultural reference transformation (CRT)**: CRA × VI metric combining recognition and independence - needed to capture overall multimodal iconicity; quick check: verify CRT values reflect intuitive notions of cultural alignment
- **Prompt perturbation robustness**: Testing model response to lexical changes - needed to assess stability of cultural alignment under different textual formulations; quick check: compare CRA under title-only vs. descriptive prompts

## Architecture Onboarding
**Component map**: Wikidata query -> Reference dataset -> Image generation -> CLIP analysis -> DINOv3 patch analysis -> Metric computation -> Correlation analysis

**Critical path**: The core pipeline flows from reference selection through generation to metric computation, with CLIP and DINOv3 analysis as parallel but complementary analyses that feed into final CRT scores.

**Design tradeoffs**: The framework trades computational intensity (patch-level DINOv3 analysis) for more nuanced understanding of visual reuse vs. transformation, avoiding simplistic similarity thresholds that conflate replication with reinterpretation.

**Failure signatures**: High CRA with high VR indicates close replication; high CRA with low VR indicates transformation; low CRA suggests poor recognition; threshold choices for CLIP (0.7) and DINOv3 (0.6) critically affect results.

**First experiments**:
1. Verify threshold sensitivity by plotting CRA distributions at 0.6, 0.7, 0.8 CLIP similarity thresholds
2. Test position-independence by comparing patch matching when restricting to same-position vs. all patches
3. Validate dynamic reference filtering by examining pairwise CLIP similarities among retrieved Google Images

## Open Questions the Paper Calls Out
- How do specific dataset properties, such as text uniqueness, causally shape multimodal iconicity? The current study relies on observational correlations rather than controlled interventions within the training data.
- To what extent do training data factors predict cultural alignment in models without public training sets? Proprietary models like Imagen 4 don't provide training data access, making findings for other models "indicative rather than definitive."
- How does the framework apply to non-Western or underrepresented cultural references? The current results reflect Wikidata's "Western and Anglophone visibility biases" and may not generalize to regionally specific cultural knowledge.

## Limitations
- DINOv3 patch similarity threshold (0.6) sensitivity not validated through comprehensive analysis, limiting confidence in absolute VR/VI values
- Dynamic reference dataset construction using Google Images introduces potential bias through filtering process and lacks public availability
- Focus on English-language Wikidata references may not capture diverse cultural representations or generalize beyond Western cultural contexts

## Confidence
| Claim Cluster | Confidence |
|---------------|------------|
| Dataset representativeness | Medium - Selection criteria clear but may not capture diverse cultural representations |
| Metric validity | High - CRA, VR, VI, CRT metrics are logically constructed and internally consistent |
| Model comparison conclusions | Medium - Relative ordering appears robust but absolute values depend on unvalidated thresholds |
| Training data correlation findings | High - Statistical analysis is rigorous and findings are internally consistent |

## Next Checks
1. Conduct sensitivity analysis by varying DINOv3 patch similarity threshold (0.5, 0.6, 0.7) and CLIP similarity threshold (0.6, 0.7, 0.8) to determine metric robustness
2. Reconstruct dynamic reference dataset using different image search engine or time period to assess systematic bias in reference image selection
3. Test subset of references with multiple prompt formulations (including artist names, contexts, detailed descriptions) to verify lexical perturbation conclusions generalize beyond single-title prompts