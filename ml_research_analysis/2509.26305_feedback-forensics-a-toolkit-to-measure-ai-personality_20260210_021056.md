---
ver: rpa2
title: 'Feedback Forensics: A Toolkit to Measure AI Personality'
arxiv_id: '2509.26305'
source_url: https://arxiv.org/abs/2509.26305
tags:
- more
- personality
- traits
- response
- uses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Feedback Forensics is an open-source toolkit that measures AI personality
  traits using relative pairwise annotations. The method compares personality annotations
  to human feedback or target model responses to compute strength metrics indicating
  how much a trait is encouraged or exhibited.
---

# Feedback Forensics: A Toolkit to Measure AI Personality

## Quick Facts
- **arXiv ID:** 2509.26305
- **Source URL:** https://arxiv.org/abs/2509.26305
- **Reference count:** 40
- **Primary result:** Open-source toolkit measures AI personality traits through relative pairwise annotations, finding structured formatting, verbosity, and factual correctness are most encouraged by human feedback

## Executive Summary
Feedback Forensics introduces an open-source toolkit that quantifies AI personality traits using relative pairwise annotations compared against human feedback or target model responses. The method computes strength metrics indicating how much each trait is encouraged or exhibited by different models. Analysis of 10k samples from major datasets revealed that structured formatting, verbosity, and factual correctness are most encouraged by human feedback, while conciseness and avoidant tone are discouraged. The toolkit successfully identified significant personality differences across six popular models, including notable markdown formatting usage in Gemini-2.5-Pro and Mistral-Medium-3.1, and higher conciseness in GPT-5.

## Method Summary
The toolkit employs relative pairwise annotation comparisons where personality traits are evaluated by comparing two model responses rather than scoring them individually. This approach compares personality annotations to either human feedback or target model responses to compute strength metrics. The method uses six personality traits: verbosity, factual correctness, conciseness, structured formatting, enthusiastic tone, and avoidant tone. By analyzing these traits across multiple datasets and models, the toolkit provides relative measures of how much each trait is encouraged by human feedback or exhibited by specific models.

## Key Results
- Human feedback most strongly encourages structured formatting, verbosity, and factual correctness while discouraging conciseness and avoidant tone
- Gemini-2.5-Pro and Mistral-Medium-3.1 showed notable markdown formatting usage compared to other models
- GPT-5 demonstrated higher conciseness than other models in the analysis
- Chatbot Arena version of Llama-4-Maverick was 0.97 strength more verbose and 0.96 more enthusiastic than the public release
- GPT-5-mini and Gemini-2.5-Flash achieved 92% choice agreement with human annotations in validation experiments

## Why This Works (Mechanism)
The relative pairwise annotation approach works by comparing two model responses directly rather than requiring absolute scoring, which reduces annotation complexity and improves consistency. By establishing baselines through human feedback and then measuring how closely model responses align with these preferences, the method creates interpretable strength metrics that indicate personality trait prevalence.

## Foundational Learning
- **Relative pairwise comparison**: Why needed - eliminates need for absolute trait scoring; Quick check - two responses compared yield consistent trait rankings
- **Strength metrics**: Why needed - provides quantitative measure of personality trait prevalence; Quick check - metric values correlate with observable behavioral differences
- **Six personality traits**: Why needed - covers key dimensions of conversational AI behavior; Quick check - trait definitions map clearly to user preferences
- **Human feedback baselines**: Why needed - establishes ground truth for what traits are preferred; Quick check - baseline rankings align with user satisfaction data

## Architecture Onboarding
**Component map:** Human feedback -> Trait annotations -> Model responses -> Strength metric computation -> Personality profile
**Critical path:** Pairwise comparison → Trait strength calculation → Model personality aggregation
**Design tradeoffs:** Relative comparison reduces annotation burden but limits absolute trait measurement
**Failure signatures:** Inconsistent annotations across annotators, model responses that don't trigger clear trait expressions
**3 first experiments:**
1. Run pairwise comparisons on 100 samples from Chatbot Arena dataset
2. Compute strength metrics for two contrasting models (e.g., Gemini-2.5-Pro vs GPT-5)
3. Validate human-AI agreement on a subset of annotations

## Open Questions the Paper Calls Out
None

## Limitations
- Results are relative rather than absolute measures of personality
- Validation limited to two model pairs may not generalize across all traits and contexts
- Strength metrics depend on assumptions about human feedback representing desired personality characteristics
- Analysis of 10k samples may miss nuanced expressions in longer conversations

## Confidence
- **High**: Relative differences in personality traits between models are well-supported
- **Medium**: Strength metric interpretations require further validation across contexts
- **Medium**: Human-AI annotation agreement results need broader testing

## Next Checks
1. Test toolkit performance on conversational datasets with varying lengths and complexity
2. Conduct cross-cultural validation studies for personality trait measurements
3. Perform longitudinal analysis to track personality trait evolution during model updates