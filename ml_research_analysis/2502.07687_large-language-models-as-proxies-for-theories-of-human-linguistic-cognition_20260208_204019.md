---
ver: rpa2
title: Large Language Models as Proxies for Theories of Human Linguistic Cognition
arxiv_id: '2502.07687'
source_url: https://arxiv.org/abs/2502.07687
tags:
- llms
- theory
- language
- linguistic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether current large language models (LLMs)
  can serve as proxies for theories of human linguistic cognition (HLC), specifically
  those that are linguistically-neutral but differ from typical generative approaches.
  The authors evaluate this "Proxy View" by testing whether LLMs can approximate key
  linguistic patterns that humans acquire from limited data, and whether they show
  learning asymmetries that might explain typological patterns across languages.
---

# Large Language Models as Proxies for Theories of Human Linguistic Cognition

## Quick Facts
- arXiv ID: 2502.07687
- Source URL: https://arxiv.org/abs/2502.07687
- Reference count: 11
- Key outcome: Current LLMs fail to approximate key aspects of human linguistic knowledge from developmentally-realistic corpora and do not demonstrate learning patterns that would explain typological asymmetries.

## Executive Summary
This paper examines whether current large language models can serve as proxies for theories of human linguistic cognition, specifically those that are linguistically-neutral but differ from typical generative approaches. The authors evaluate this "Proxy View" by testing whether LLMs can approximate key linguistic patterns that humans acquire from limited data, and whether they show learning asymmetries that might explain typological patterns across languages. The study tests eight LLMs of varying sizes on three phenomena: across-the-board movement, parasitic gaps, and that-trace effects. Using a lenient success criterion, the authors found that all models except the largest failed to prefer grammatical over ungrammatical sentence continuations for these phenomena, even after extensive training. The primary conclusion is that current LLMs provide no support for the Proxy View.

## Method Summary
The study tests eight LLMs ranging from 10 million to 9 trillion tokens on three linguistic phenomena using minimal pair probability comparisons. For the acquisition experiments, models were evaluated on whether they preferred grammatical continuations over ungrammatical ones in carefully constructed minimal pairs. For the typology experiments, models were trained on perturbed versions of English, Italian, Russian, and Hebrew, and validation perplexity was tracked to assess learning ease. The authors used a lenient success criterion where any positive probability difference between grammatical and ungrammatical options counted as success. Typology experiments involved training fresh Transformer models on perturbed corpora and measuring convergence rates.

## Key Results
- All models except LLaMA failed to prefer grammatical over ungrammatical sentence continuations for across-the-board movement, parasitic gaps, and that-trace effects
- Some perturbed (unattested) languages were easier to learn than their attested baseline counterparts
- Even after training on 9 trillion tokens (~821,250 years of linguistic exposure), models showed limited grammatical knowledge acquisition
- Learning asymmetries that would explain typological patterns were not consistently demonstrated across perturbations

## Why This Works (Mechanism)

### Mechanism 1
LLMs can potentially serve as proxies for evaluating whether linguistically-neutral theories of human linguistic cognition would acquire specific patterns from limited corpora. Train models on corpora of varying sizes, then probe grammatical knowledge through minimal pair probability comparisons—if the model prefers grammatical continuations, this suggests (but does not prove) that the target theory might also succeed. Core assumption: LLM learning dynamics approximate those of the unstated linguistically-neutral theory. Break condition: When models fail to show grammatical preferences even after massive training.

### Mechanism 2
Perplexity-based learning curves can indicate whether attested linguistic patterns are easier to acquire than typologically-unattested perturbations. Train transformers on baseline and perturbed versions of languages; compare validation perplexity trajectories—lower/faster convergence suggests easier learning. Core assumption: Ease of learning by LLMs reflects learning asymmetries that could explain typological distributions. Break condition: When perturbed (unattested) languages are easier to learn than baseline languages.

### Mechanism 3
Scale-insensitive failure patterns can diagnose architectural limitations vs. data insufficiency. Test models across 6+ orders of magnitude in training data; if even the largest models fail specific phenomena, this suggests representational/architectural constraints rather than mere data poverty. Core assumption: Failure at extreme scales implies the architecture cannot represent the target linguistic knowledge. Break condition: If the largest model succeeds where others fail, this suggests data scale matters more than architecture.

## Foundational Learning

- Concept: **Poverty of the Stimulus (APS)**
  - Why needed here: The entire experimental design tests whether knowledge can be acquired from limited corpora; understanding APS arguments is prerequisite to interpreting results.
  - Quick check question: Can you explain why yes-no question formation (auxiliary fronting) is classically used as an APS example?

- Concept: **Competence vs. Performance**
  - Why needed here: LLMs conflate grammaticality with probability; humans distinguish them. This distinction is central to why LLMs fail as direct theories of HLC.
  - Quick check question: Why is center-embedding difficulty attributed to performance rather than competence?

- Concept: **Wh-movement and Island Constraints**
  - Why needed here: Three test phenomena (ATB, parasitic gaps, that-trace) all involve complex wh-movement interactions; understanding basic wh-extraction is prerequisite.
  - Quick check question: In "Which book did you say Mary read?", where is the base position of "which book"?

## Architecture Onboarding

- Component map:
  Smallest models (CHILDES LSTM/Transformer): 8.6M tokens (~10 months human equivalent)
  Developmental-scale models (BabyLM 10M/100M): 1-9 years equivalent
  Medium models (BERT, GPT-2): 320-730 years equivalent
  Large model (LLaMA-3.2-3b): ~9T tokens (~821,250 years equivalent)
  Evaluation: Minimal pair probability comparison on critical regions

- Critical path:
  1. Select linguistic phenomenon and generate minimal pairs via CFG templates
  2. Extract probability for grammatical vs. ungrammatical continuation at critical region
  3. Compute accuracy (proportion where grammatical > ungrammatical probability)
  4. For typology experiments: train fresh models on perturbed corpora, track validation perplexity

- Design tradeoffs:
  Lenient success criterion (any positive probability difference) vs. stricter thresholds
  Unidirectional vs. bidirectional models require different probability extraction methods
  Template-generated sentences maximize control but reduce naturalness

- Failure signatures:
  Near-chance or below-chance accuracy on minimal pairs despite massive training
  Perturbed languages showing lower perplexity than natural baselines
  No clear learning curve separation between grammatical and ungrammatical conditions

- First 3 experiments:
  1. Replicate ATB/PG minimal pair test on a new model checkpoint to verify behavioral consistency
  2. Train a small transformer on partial-reverse English to confirm perplexity divergence pattern
  3. Test the critical region probability extraction method on simple subject-verb agreement pairs as a sanity check

## Open Questions the Paper Calls Out

### Open Question 1
What are the concrete specifics of the linguistically-neutral target theory H3 that the Proxy View envisions? The authors repeatedly state that proponents of the Proxy View need to provide concrete details about their target theories. This remains unresolved because proponents suggest LLMs support linguistically-neutral theories without making those theories explicit.

### Open Question 2
Can learning asymmetries demonstrated by LLMs be situated within cultural evolution models to properly evaluate typological patterns? Section 3.6 states that such a model is the proper context for using LLM learning ease to evaluate typological asymmetries, but this is left for future work. This remains unresolved because current perplexity-based metrics cannot capture how small learning asymmetries might amplify over generations.

### Open Question 3
Can mechanistic interpretability research uncover a notion of grammatical correctness within LLMs that is distinct from likelihood? Footnote 2 notes that interpretability research has the potential to uncover correctness notions within LLMs. This remains unresolved because current LLMs provide only probability distributions, not acceptability judgments.

### Open Question 4
Would LLMs show different performance on more linguistically sophisticated treatments of ATB movement, parasitic gaps, and that-trace effects? The authors acknowledge their treatments are simplified and that any attempt to make positive claims would need to address the full complexities discussed in linguistic literature. This remains unresolved because the paper's test cases use simplified paradigms.

## Limitations
- The abstract nature of the "Proxy View" being critiqued makes empirical disconfirmation somewhat indirect
- The lenient success criterion may overestimate model capabilities by accepting minimal probability differences
- Template-generated minimal pairs may not capture the full complexity of natural language use
- Easier learning of perturbed languages doesn't definitively rule out all possible learning-based explanations for typological patterns

## Confidence

**High Confidence**: The conclusion that current LLMs fail to acquire specific linguistic patterns from developmentally-realistic corpora is well-supported by consistent failure across multiple models and phenomena.

**Medium Confidence**: The broader claim that LLMs provide "no support" for the Proxy View is reasonable but depends on how strictly one interprets what constitutes adequate proxy behavior.

**Low Confidence**: The assertion that these specific failure patterns definitively rule out all linguistically-neutral theories of HLC is too strong.

## Next Checks

1. Replicate the minimal pair experiments with stricter success criteria (≥0.01 probability difference) to determine if the lenient threshold was masking continued model failures.

2. Test additional linguistically-neutral architectures (e.g., recurrent neural networks, symbolic systems) on the same phenomena to distinguish whether the failures are specific to transformers or reflect broader challenges for any learning-based approach.

3. Conduct human psycholinguistic experiments on the minimal pairs to establish baseline human performance, enabling direct comparison between human and model behavior rather than just grammaticality judgments.