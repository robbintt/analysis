---
ver: rpa2
title: 'DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous
  Travel Planning Agents'
arxiv_id: '2509.21842'
source_url: https://arxiv.org/abs/2509.21842
tags:
- tool
- travel
- agent
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepTravel, an end-to-end agentic reinforcement
  learning framework for autonomous travel planning agents. The framework addresses
  the challenges of dynamic travel environments and open-ended task verification by
  constructing a robust sandbox environment that caches real-world data, developing
  a hierarchical reward modeling system with trajectory and turn-level verifiers,
  and introducing a reply-augmented reinforcement learning method with experience
  replay.
---

# DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents

## Quick Facts
- arXiv ID: 2509.21842
- Source URL: https://arxiv.org/abs/2509.21842
- Reference count: 40
- Primary result: Small-size LLM (Qwen3-32B) significantly outperforms state-of-the-art reasoning LLMs (OpenAI-o1/o3, DeepSeek-R1) on travel planning tasks, achieving up to 73.21% final pass rates on complex scenarios.

## Executive Summary
This paper presents DeepTravel, an end-to-end agentic reinforcement learning framework for autonomous travel planning agents. The framework addresses the challenges of dynamic travel environments and open-ended task verification by constructing a robust sandbox environment that caches real-world data, developing a hierarchical reward modeling system with trajectory and turn-level verifiers, and introducing a reply-augmented reinforcement learning method with experience replay. The proposed approach enables small-size LLMs (e.g., Qwen3-32B) to significantly outperform state-of-the-art reasoning LLMs (e.g., OpenAI-o1/o3 and DeepSeek-R1) on travel planning tasks. In comprehensive evaluations using both online real-world user data and offline synthetic data, DeepTravel achieves final pass rates up to 73.21% on complex travel planning scenarios, demonstrating its effectiveness in building autonomous travel planning agents capable of tool integration and multi-turn reasoning.

## Method Summary
DeepTravel implements a three-stage approach: (1) Sandbox Environment Construction: Caches real-world API responses (flight, train, hotel, POI, route, web search) with daily refresh to provide stable, repeatable tool responses for RL training; (2) Hierarchical Reward Modeling: Two-stage verification system where trajectory-level verifier checks spatiotemporal feasibility (completeness, intent understanding, logic, constraints, requirements, backup plan) and turn-level verifier validates consistency between agent claims and tool responses at each reasoning step; (3) Reply-Augmented RL: Experience replay mechanism that stores failed queries in a buffer and periodically re-samples them after training steps, enabling progressive capability emergence. The system uses GRPO-based optimization with SFT cold-start on 1K distilled trajectories from DeepSeek-R1.

## Key Results
- Qwen3-32B trained with DeepTravel achieves 62.77% final pass rate on online real-world queries, outperforming OpenAI-o1/o3 and DeepSeek-R1
- Experience replay mechanism contributes 5-10% improvement on hard problems, validating progressive learning from failures
- Sandbox-trained agents show 4-6 interaction turns with stable tool-call accuracy, compared to unstable performance with live APIs
- Hierarchical reward modeling achieves 73.21% final pass rate on complex offline synthetic queries

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Reward Modeling with Two-Stage Verification
Separating coarse-grained trajectory validation from fine-grained turn-level verification improves both training efficiency and final itinerary quality. The trajectory-level verifier first checks spatiotemporal feasibility (logical sequence, geographic plausibility, constraint satisfaction). Only trajectories passing this check proceed to turn-level verification, which validates consistency between the agent's claims and actual tool responses at each reasoning step. The joint reward is set to 1 only if both stages pass. Travel planning errors can be cleanly separated into "structural/feasibility" (trajectory-level) and "factual/hallucination" (turn-level) categories, and early filtering of structurally invalid plans saves computation without missing learnable failures.

### Mechanism 2: Sandbox Caching for Stable Environment Simulation
Caching real-world API responses with timestamp-based refresh enables repeatable RL training despite dynamic real-world data. The system maintains a daily-refreshed database that stores flight, train, and hotel search results on-demand. When a query is issued, cached results are returned instead of live API calls, enabling the agent to re-encounter identical tool responses when replaying failed trajectories. Cached data from previous API calls remains sufficiently representative of real-world conditions for training purposes, and the staleness window (daily refresh) balances realism with stability.

### Mechanism 3: Experience Replay from Failure Buffer
Periodically replaying previously failed queries enables progressive capability emergence that standard on-policy RL misses. During rollout, if all sampled trajectories for a query fail verification, the query is stored in an experience buffer. After fixed training steps, these failed queries are re-sampled with the improved policy, giving the agent opportunities to solve previously impossible problems. Failed queries contain learnable patterns that the current policy cannot yet solve but will become tractable after sufficient training; the replay interval is appropriately tuned.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The RL component builds on GRPO's group-based advantage estimation (Ai = ri - avg(r)/std(r)) rather than single-trajectory baselines. Can you explain why computing advantages relative to a group of rollouts for the same query reduces variance compared to single-sample estimation?

- **Tool-Integrated Reasoning with Loss Masking**: The training objective masks environment observation tokens (API content) from loss calculation, ensuring gradients flow only through agent-generated tokens. What would happen if tool response tokens were included in the loss? How would this affect credit assignment?

- **SFT Cold-Start for Format Acquisition**: Before RL, the agent must learn the reasoning format (thought/action/observation tags) through supervised fine-tuning on verified trajectories distilled from DeepSeek-R1. Why might direct RL from a base model fail to learn structured tool-calling format without SFT pre-training?

## Architecture Onboarding

- **Component map**: User Query → [TP Agent πθ] → [Sandbox Environment] → [Hierarchical Reward Model] → [Experience Buffer B] → [Policy Optimizer]
- **Critical path**: Sandbox → Reward Model → Policy Update. The sandbox must return stable responses; the reward model must provide reliable binary signals; policy updates must not collapse entropy prematurely.
- **Design tradeoffs**:
  - Sandbox freshness vs. stability: Daily refresh balances realism with reproducibility. Shorter intervals increase realism but reduce stability for replay learning.
  - Trajectory vs. turn-level verifier strictness: Over-strict trajectory filtering saves compute but may discard repairable plans. The paper sets trajectory-level as coarse filter (six rubrics) and turn-level as fine-grained (two rubrics per turn).
  - Rollout size n: Set to 8. Larger n improves advantage estimation but increases compute. The paper filters samples where std(r) ≤ 0.1 (too easy/hard).
- **Failure signatures**:
  - Reward hacking: Agent generates very short responses or skips tool calls. Monitor average response length and interaction turns (Figure 4 shows expected 4-6 turns).
  - Sandbox staleness: Tool-call accuracy plateaus or drops. Monitor verifier success rate separately from policy reward.
  - Buffer explosion: Experience buffer grows unboundedly. Monitor sample keep rate; if near 0%, queries may be too hard.
  - Entropy collapse: Policy becomes deterministic. Paper notes non-decreasing entropy is expected for agentic RL due to dynamic tool responses; monotonic decrease may indicate insufficient exploration.
- **First 3 experiments**:
  1. Ablate replay mechanism: Train DeepTravel-8B without experience replay (w/o ER). Expect ~5-10% drop on hard problems per Table 4 (51.01→54.25 offline without constraint). This validates that progressive learning from failures matters.
  2. Compare sandbox vs. live APIs: Train identical policy with real APIs vs. cached sandbox. Expect unstable tool-call accuracy and no reward improvement with live APIs per section 4.4 analysis. This validates environment stability requirement.
  3. Scale study on model size: Compare DeepTravel-8B vs. DeepTravel-32B on the same training data. Expect 32B to achieve higher final pass rates (62.77% vs. 49.75% online per Table 2) but with 4× GPU requirements. This establishes compute-performance tradeoffs.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the hierarchical reward modeling system be replaced with a more flexible, automated reward model that does not require hand-crafted rubrics? The current system requires human-generated rubrics for both trajectory-level and turn-level verification, making domain adaptation labor-intensive.

- **Open Question 2**: What mechanisms drive the non-decreasing entropy phenomenon during agentic RL training, and is it fundamental to tool-integrated reasoning tasks? Understanding entropy dynamics is critical for training stability and sample efficiency in agentic RL.

- **Open Question 3**: How well does sandbox-trained agent performance transfer to real-world API environments with higher instability and inconsistency? The sim-to-real gap suggests deployment challenges; the degree of transfer degradation remains unquantified.

- **Open Question 4**: How do verifier errors or biases in the DeepSeek-R1-based reward model propagate through RL training? RL agents may exploit verifier blind spots rather than improving actual planning quality, creating hidden failure modes.

## Limitations

- The caching mechanism's staleness window and its impact on generalization remain unclear, with no systematic analysis of how cache freshness affects performance across different travel seasons.
- The hierarchical reward model's rubrics are summarized but not fully specified, making it difficult to assess potential biases in verification or reproducibility.
- The experience replay hyperparameters (buffer size, frequency γ) are not detailed, which could significantly affect the progressive learning claims.

## Confidence

- **High Confidence**: The sandbox environment's stability benefits for RL training are well-supported by comparative results showing superior performance over live API usage.
- **Medium Confidence**: The hierarchical reward modeling approach's efficiency gains are plausible given the separation of concerns, but the rubrics' comprehensiveness cannot be fully evaluated from the paper.
- **Medium Confidence**: The experience replay mechanism's contribution to solving previously failed queries is supported by progressive improvement trends, though the specific conditions for its effectiveness are not fully characterized.

## Next Checks

1. **Cache Staleness Sensitivity Analysis**: Systematically vary the caching refresh interval (daily, weekly, monthly) and measure impact on final pass rates and tool-call accuracy across different travel seasons.
2. **Reward Model Rubric Validation**: Implement and test the complete trajectory and turn-level verifiers on a held-out validation set to assess inter-rater reliability and coverage of failure modes.
3. **Replay Buffer Size and Frequency Sweep**: Conduct hyperparameter sweeps on experience replay buffer capacity and replay frequency γ to identify optimal settings and failure conditions.