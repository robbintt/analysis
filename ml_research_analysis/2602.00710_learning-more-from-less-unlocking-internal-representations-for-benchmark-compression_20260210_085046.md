---
ver: rpa2
title: 'Learning More from Less: Unlocking Internal Representations for Benchmark
  Compression'
arxiv_id: '2602.00710'
source_url: https://arxiv.org/abs/2602.00710
tags:
- benchmark
- source
- qwen2
- item
- gp-irt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REPCORE introduces a benchmark compression framework that leverages
  internal hidden states to overcome source model scarcity. Unlike prior methods that
  rely on sparse output labels, REPCORE aligns heterogeneous hidden states into a
  unified latent space, enabling robust coreset selection and performance extrapolation
  even with as few as ten source models.
---

# Learning More from Less: Unlocking Internal Representations for Benchmark Compression

## Quick Facts
- arXiv ID: 2602.00710
- Source URL: https://arxiv.org/abs/2602.00710
- Reference count: 40
- Primary result: REPCORE achieves Spearman ρ up to 0.913 and MAE as low as 0.045 using hidden-state alignment for benchmark compression

## Executive Summary
REPCORE introduces a novel benchmark compression framework that leverages internal hidden states to overcome source model scarcity. Unlike prior methods that rely on sparse output labels, REPCORE aligns heterogeneous hidden states into a unified latent space, enabling robust coreset selection and performance extrapolation even with as few as ten source models. Experiments on five benchmarks (BBH, GSM8K, SEED-Bench-2-Plus, ARC-Challenge, and MMLU-Pro) with over 200 models show consistent improvements over output-based baselines in ranking correlation and mean absolute error. Spectral analysis reveals that aligned representations encode separable components reflecting both broad response tendencies and task-specific reasoning patterns.

## Method Summary
REPCORE operates through a three-phase pipeline: (1) Extract hidden states from source models and align them via model-specific linear projections followed by a shared MLP bottleneck, trained to predict correctness; (2) Create consensus embeddings by averaging aligned states across sources, normalize to unit hypersphere, and cluster to select K representative anchors closest to centroids; (3) Fit a Ridge regressor using average source correctness on the coreset to extrapolate full-benchmark performance. The framework compresses high-dimensional hidden states (4096+ dims) into 32-dim aligned embeddings while preserving correctness-predictive signals, then uses these compact representations for coreset selection and performance estimation.

## Key Results
- Achieves Spearman correlation up to 0.913 and MAE as low as 0.045 across five benchmarks
- Maintains performance with as few as ten source models, overcoming scarcity limitations
- Spectral analysis shows aligned embeddings encode both difficulty (PC1) and task-specific reasoning patterns (PC2/PC3)
- Ablation studies confirm continuous embeddings are essential, with 32-dim bottleneck capturing sufficient structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous hidden states encode denser item structure than discrete correctness labels, enabling better geometry recovery under source scarcity.
- Mechanism: Hidden states from the final layer's last output token capture reasoning trajectories and decision signals; when projected through a bottleneck (32-dim), they preserve fine-grained inter-item relations that binary 0/1 labels collapse.
- Core assumption: Models encode uncertainty and task-specific reasoning in hidden states, not just in final outputs.
- Evidence anchors: Abstract states discrete correctness labels are "lossy"; Section 2.2 validates hidden states encode reasoning trajectory; weak corpus support from neighbor paper on hidden states for evaluation.
- Break condition: If hidden states are highly model-specific without shared structure, alignment fails to recover meaningful geometry.

### Mechanism 2
- Claim: Model-specific projections followed by a shared MLP align heterogeneous hidden spaces into a unified latent space without introducing systematic bias.
- Mechanism: Each source model m has an independent linear projection Proj_m, followed by a source-agnostic MLP f_θ, creating an information bottleneck trained to predict correctness via cross-entropy loss.
- Core assumption: A low-dimensional bottleneck (d_z=32) is sufficient to capture task-relevant signals across architectures.
- Evidence anchors: Abstract mentions aligning heterogeneous hidden states; Section 2.2 describes the pipeline as an information bottleneck; no direct corpus precedent for this specific approach.
- Break condition: If source models share architectural biases, aligned space may inherit systematic distortions.

### Mechanism 3
- Claim: Difficulty-stratified spectral analysis confirms aligned embeddings encode separable components for both broad response tendencies and task-specific reasoning patterns.
- Mechanism: PCA on aligned embeddings reveals PC1 correlates with item difficulty (ρ≈0.46), while even after controlling for difficulty, fine-grained task type shows strong clustering effect (ε²≈0.54).
- Core assumption: The latent manifold preserves hierarchical cognitive structure beyond scalar difficulty.
- Evidence anchors: Section 4.2 shows hierarchical structural persistence; Section 4.3 demonstrates PC2/PC3 enhance estimation fidelity; neighbor paper on hidden states encoding difficulty supports the concept.
- Break condition: If task structure is not linearly separable in the bottleneck space, higher-order PCs provide no incremental signal.

## Foundational Learning

- Concept: **Item Response Theory (IRT)**
  - Why needed here: Prior benchmark compression methods use IRT to estimate latent difficulty/discrimination from sparse response patterns; understanding this clarifies why REPCORE's hidden-state approach is a paradigm shift.
  - Quick check question: Can you explain why binary response matrices become unreliable with only 10 source models?

- Concept: **Information Bottleneck**
  - Why needed here: REPCORE compresses high-dimensional hidden states (often 4096+ dims) into 32-dim aligned embeddings while preserving correctness-predictive signals.
  - Quick check question: What is the trade-off between bottleneck capacity and generalization in representation learning?

- Concept: **Consensus Clustering / Centroid Selection**
  - Why needed here: Coreset selection aggregates embeddings across source models, then clusters to find K representative anchors closest to centroids.
  - Quick check question: Why normalize embeddings to unit hypersphere before clustering?

## Architecture Onboarding

- Component map: Hidden State Extraction → Per-Model Linear Proj → Shared MLP → Linear Classifier (training) → Consensus Embedding (average across sources) → L2-Normalize → K-Means Clustering → Select K anchors (nearest to centroids) → Ridge Regressor (fit on coreset, extrapolate to full benchmark)

- Critical path: Hidden state extraction → alignment training → consensus embedding → coreset selection → Ridge extrapolation. Any failure in alignment (low classifier AUC) propagates to poor coreset.

- Design tradeoffs:
  - d_z=32 bottleneck: Empirically captures 99% variance for most models; larger dims risk overfitting.
  - Ridge regression with single scalar feature (source-averaged correctness): Trades expressiveness for robustness under small coreset sizes.
  - Consensus embedding by averaging: Simple but assumes source models provide complementary views; single-family sources degrade performance.

- Failure signatures:
  - Low classifier AUC on held-out splits → aligned embeddings not capturing validity signals.
  - High MAE variance across runs → unstable coreset selection; check source diversity.
  - GP-IRT matching MAE but lower Agreement → baseline shrinking predictions toward mean.

- First 3 experiments:
  1. **Reproduction check**: Run REPCORE on BBH with |S|=10, K=30; verify Spearman ρ ≈0.87 and MAE ≈0.057. Compare against discrete K-Means baseline to confirm continuous embeddings matter.
  2. **Ablation: bottleneck dimension**: Vary d_z ∈ {16, 32, 64, 128}; plot Spearman correlation vs. dimension. Expect plateau around 32.
  3. **Source composition stress test**: Construct source pool from single model family (e.g., Qwen only) vs. diverse; measure performance gap. Should match Table 5 findings (~0.02-0.05 Spearman drop).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can REPCORE's alignment mechanism be adapted for closed-source models where hidden states are inaccessible?
- Basis in paper: [explicit] "For future directions, we aim to extend this paradigm to closed-source models, thereby establishing a unified standard for open and proprietary systems."
- Why unresolved: The current framework requires extracting hidden states from source models, which is impossible for proprietary APIs.
- What evidence would resolve it: Demonstration of proxy-based alignment (e.g., using output distributions or intermediate accessible signals) that achieves comparable coreset quality without internal states.

### Open Question 2
- Question: Can aligned representations learned on one benchmark transfer to coreset selection for entirely different domains without retraining?
- Basis in paper: [explicit] "Furthermore, exploring the cross-domain transferability of aligned representations promises to enable efficient coreset selection for specialized domains without task-specific retraining."
- Why unresolved: Current experiments train and evaluate within the same benchmark; cross-domain transfer remains untested.
- What evidence would resolve it: Evaluation showing that embeddings trained on general benchmarks (e.g., MMLU-Pro) produce effective coresets for specialized domains (e.g., medical or legal QA) without task-specific fine-tuning.

### Open Question 3
- Question: What is the optimal bottleneck dimension (dz) for balancing representational capacity and generalization across diverse model architectures?
- Basis in paper: [inferred] The paper sets dz=32 empirically based on observed effective ranks, but does not systematically ablate this choice across architectures or benchmarks.
- Why unresolved: Different model families may have varying intrinsic dimensionality; a fixed dz may under- or over-capacity for certain architectures.
- What evidence would resolve it: Systematic ablation varying dz ∈ {16, 32, 64, 128} across heterogeneous model pools, measuring both reconstruction fidelity and extrapolation accuracy.

## Limitations
- Hidden state extraction requirement limits applicability to closed-source models
- Alignment architecture details (MLP depth, activation functions) remain underspecified
- Claims about encoding "reasoning trajectories" are interpretive rather than directly validated

## Confidence
- **High confidence**: Continuous embeddings outperform discrete labels in geometry preservation; consensus averaging across diverse sources improves coreset quality; spectral analysis shows separable difficulty and task-type components.
- **Medium confidence**: 32-dim bottleneck captures sufficient information; Ridge regression with scalar correctness feature is optimal; temporal robustness generalizes to unseen future distributions.
- **Low confidence**: Claims about encoding "reasoning trajectories" are interpretive rather than directly validated; the specific alignment architecture details needed for exact replication are incomplete.

## Next Checks
1. **Architectural sensitivity analysis**: Systematically vary MLP depth (1-3 layers), activation functions (ReLU, GeLU, Swish), and bottleneck dimensions (16-128) to identify the minimal architecture achieving ≥0.9 classifier AUC and ≥0.85 Spearman correlation.
2. **Source diversity stress test**: Construct extreme cases—single-family sources (all Qwen variants), single-capability sources (all 7B models), and maximally diverse sources (mix of 1B-70B, different families). Quantify performance degradation and identify diversity thresholds.
3. **Distribution shift robustness**: Hold out not just time periods but also capability ranges (e.g., test on only 1B-7B models when training on 13B-70B). Measure performance drop to assess whether alignment captures model-agnostic task structure or capability-specific biases.