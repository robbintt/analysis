---
ver: rpa2
title: 'SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with
  Reinforcement Learning'
arxiv_id: '2512.13159'
source_url: https://arxiv.org/abs/2512.13159
tags:
- clarification
- user
- reward
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpeakRL, a reinforcement learning method
  that enables language models to proactively ask clarification questions in multi-turn
  dialogues. By rewarding the model for asking targeted questions using structured
  tokens and an LLM-as-judge reward, SpeakRL improves task success from 25.63% to
  46.17% on MultiWOZ 2.4, reduces average dialogue turns from 8.12 to 5.82, and outperforms
  larger proprietary models despite using a smaller open-source model.
---

# SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.13159
- Source URL: https://arxiv.org/abs/2512.13159
- Reference count: 22
- Primary result: Task success increased from 25.63% to 46.17% on MultiWOZ 2.4 using reinforcement learning for clarification questions

## Executive Summary
This paper introduces SpeakRL, a reinforcement learning method that enables language models to proactively ask clarification questions in multi-turn dialogues. By rewarding the model for asking targeted questions using structured tokens and an LLM-as-judge reward, SpeakRL improves task success from 25.63% to 46.17% on MultiWOZ 2.4, reduces average dialogue turns from 8.12 to 5.82, and outperforms larger proprietary models despite using a smaller open-source model. The approach demonstrates that effective clarification behavior directly leads to better task completion and more efficient interactions.

## Method Summary
SpeakRL implements a reinforcement learning framework where language models are trained to generate clarification questions during dialogue. The system uses structured tokens to guide question formulation across five dimensions (where, what, when, who, and semantic relevance) and employs an LLM-as-judge to provide reward signals based on question quality and task relevance. The model learns to ask targeted questions that reduce ambiguity and improve task completion rates in multi-turn dialogues, particularly in task-oriented scenarios like restaurant reservations and travel bookings.

## Key Results
- Task success rate increased from 25.63% to 46.17% on MultiWOZ 2.4 benchmark
- Average dialogue turns reduced from 8.12 to 5.82, indicating more efficient interactions
- Outperformed GPT-3.5 and GPT-4 in specific metrics despite using a smaller open-source model

## Why This Works (Mechanism)
The reinforcement learning approach works by creating a feedback loop where the model receives rewards for asking effective clarification questions that lead to successful task completion. The structured token system ensures questions cover essential dimensions (where, what, when, who, semantic relevance), while the LLM-as-judge provides nuanced evaluation of question quality. This combination allows the model to learn when and how to ask questions that most effectively reduce ambiguity and move the conversation toward successful task completion.

## Foundational Learning
- **Reinforcement Learning in Dialogue Systems**: Needed to enable proactive question generation; quick check: model learns to maximize reward through trial-and-error dialogue interactions
- **LLM-as-Judge Reward Design**: Required for nuanced evaluation of clarification quality; quick check: reward correlates with human judgment of question effectiveness
- **Multi-Turn Dialogue Management**: Essential for tracking conversation context and history; quick check: model maintains coherence across dialogue turns
- **Task-Oriented Dialogue Optimization**: Critical for measuring practical success; quick check: improvements translate to actual task completion rates
- **Structured Token Generation**: Enables systematic coverage of clarification dimensions; quick check: questions address all five key aspects
- **Ablation Study Methodology**: Important for validating component contributions; quick check: full reward structure outperforms individual components

## Architecture Onboarding

**Component Map:**
User Input -> Context Encoder -> Clarification Decision -> Structured Token Generator -> LLM-as-Judge -> Reward Signal -> Policy Update -> Response Generator

**Critical Path:**
The most critical path is: Context Encoder → Clarification Decision → Structured Token Generator → LLM-as-Judge → Reward Signal. This path determines whether the model asks effective questions and receives appropriate feedback for learning.

**Design Tradeoffs:**
The framework balances exploration (trying different question types) against exploitation (using known effective questions). Using an LLM-as-judge provides nuanced feedback but introduces potential circularity. The structured token approach ensures systematic coverage but may limit natural question generation.

**Failure Signatures:**
- Model asks irrelevant or redundant questions (reward signal too weak)
- Model fails to ask clarifying questions when needed (exploration too limited)
- Questions don't lead to task completion despite being well-formed (reward signal misaligned)
- Model asks too many questions, creating inefficient dialogues (reward weighting unbalanced)

**3 First Experiments:**
1. Run baseline dialogue without clarification questions to establish performance floor
2. Test individual structured token components (where, what, when, who, semantic) separately to measure their individual impact
3. Compare reward signals from LLM-as-judge versus human evaluations on a small sample

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single MultiWOZ 2.4 benchmark, raising generalizability concerns
- Potential circularity in using LLM-as-judge for both reward generation and evaluation
- Sensitivity of reward weightings across different dialogue scenarios remains unclear
- Cost-effectiveness claims are assumptions rather than empirically validated

## Confidence
**High confidence:** Performance improvements on MultiWOZ 2.4 benchmark, reduction in dialogue turns, effectiveness of structured clarification tokens
**Medium confidence:** Generalizability to other dialogue tasks, cost-effectiveness claims, superiority over proprietary models
**Low confidence:** Real-world deployment readiness, sensitivity to reward weightings, scalability to more complex dialogue scenarios

## Next Checks
1. Evaluate SpeakRL on multiple dialogue benchmarks beyond MultiWOZ 2.4, including task-oriented and open-domain conversations, to assess generalizability across different dialogue types and complexity levels.
2. Conduct a cost-benefit analysis comparing SpeakRL's computational requirements and inference costs against the claimed cost-effectiveness, including both training and deployment phases.
3. Test the approach with human evaluations in realistic deployment scenarios to validate that the LLM-as-judge reward correlates with actual user satisfaction and task completion quality in real-world settings.