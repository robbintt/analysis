---
ver: rpa2
title: Observation Noise and Initialization in Wide Neural Networks
arxiv_id: '2502.01556'
source_url: https://arxiv.org/abs/2502.01556
tags:
- network
- neural
- gradient
- training
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the Neural Tangent Kernel (NTK) framework to
  incorporate observation noise and arbitrary prior means. The authors introduce a
  regularizer in the training loss that corresponds to adding observation noise in
  the NTK-Gaussian Process (GP) posterior, addressing the limitation of standard NTK-GP
  assuming noiseless targets.
---

# Observation Noise and Initialization in Wide Neural Networks

## Quick Facts
- arXiv ID: 2502.01556
- Source URL: https://arxiv.org/abs/2502.01556
- Reference count: 40
- Key outcome: This paper extends the Neural Tangent Kernel (NTK) framework to incorporate observation noise and arbitrary prior means through a regularizer in the training loss and a "shifted network" approach, respectively.

## Executive Summary
This paper bridges a critical gap between neural network training and Gaussian Process (GP) inference by extending the Neural Tangent Kernel (NTK) framework to handle real-world noisy data and arbitrary prior mean functions. The authors demonstrate that by adding a specific regularizer to the training loss, wide neural networks can be trained to match the posterior mean of a noisy NTK-GP, effectively recovering the benefits of Bayesian uncertainty quantification without requiring expensive kernel matrix inversion. Additionally, they introduce a "shifted network" method that allows incorporating arbitrary prior mean functions through a single training run, eliminating the need for ensembles or multiple initializations.

## Method Summary
The authors extend the NTK framework by introducing a regularizer in the training loss that corresponds to adding observation noise in the NTK-Gaussian Process (GP) posterior. This modification addresses the limitation of standard NTK-GP assuming noiseless targets and enables handling of real-world noisy data. They prove that this regularization preserves the linearization of wide neural networks during training, ensuring the trained network converges to the NTK-GP posterior mean. The second major contribution is the "shifted network" approach, which allows incorporating arbitrary prior mean functions by modifying the network architecture to output predictions relative to the prior mean. This eliminates the need for ensembles or multiple training runs to incorporate prior knowledge, as the network can be trained once with the shifted architecture to achieve the desired prior mean.

## Key Results
- The observation noise regularizer preserves network linearization during training, ensuring convergence to NTK-GP posterior mean for noisy data
- The shifted network approach enables arbitrary prior mean functions through single training runs, eliminating the need for ensembles
- As network width increases, the trained network converges to its linearized counterpart, validating the theoretical framework
- Using a learned prior mean improves data efficiency in transfer learning settings compared to standard initialization

## Why This Works (Mechanism)
The mechanism works because wide neural networks become increasingly linear during gradient descent due to the NTK remaining approximately constant. By adding observation noise regularization, the training dynamics preserve this linearization property while simultaneously recovering the Bayesian posterior mean under noise. The shifted network approach works by decoupling the prior mean from the network's learned function, allowing the network to focus on learning the residual while the prior mean handles the baseline prediction. This separation enables efficient incorporation of prior knowledge without requiring multiple training runs or ensemble methods.

## Foundational Learning
- Neural Tangent Kernel (NTK): The kernel that describes how neural network outputs change during training in the infinite-width limit; needed to understand the linearization property that makes the theoretical analysis tractable.
- Gaussian Process (GP) Posterior: The Bayesian framework for regression that provides uncertainty estimates; needed to understand what the NTK framework approximates and how observation noise affects inference.
- Gradient Flow: The continuous-time limit of gradient descent; needed to analyze the training dynamics and prove convergence properties in the infinite-width regime.
- Network Linearization: The property that wide networks become approximately linear during training; needed to establish the connection between neural network training and kernel methods.
- Prior Mean Functions: The expected output of a GP before observing data; needed to understand how to incorporate domain knowledge into the model through the shifted network approach.
- Quick check: Verify that the NTK remains constant during training in the infinite-width limit by computing the second derivative of the loss with respect to parameters.

## Architecture Onboarding

Component Map: Network parameters (θ) -> NTK matrix (K) -> GP posterior mean (μ) -> Regularized loss -> Gradient update -> New parameters (θ₁) -> Shifted network architecture -> Prior mean function gradient -> Training dynamics

Critical Path: Initialize parameters → Compute NTK → Add observation noise regularizer → Train with gradient descent → Network converges to GP posterior mean → Incorporate prior mean via shifted network → Single training run achieves desired prior

Design Tradeoffs: The observation noise regularizer adds computational overhead but enables handling of real-world noisy data; the shifted network approach requires storing initial parameters but eliminates the need for multiple training runs; wider networks provide better theoretical guarantees but increase computational cost.

Failure Signatures: If the network width is insufficient, the linearization approximation breaks down and convergence to GP posterior mean is not achieved; if observation noise is mis-specified, the uncertainty estimates become unreliable; if the prior mean gradient is not computed correctly, the shifted network approach fails to incorporate the intended prior knowledge.

First Experiments:
1. Train a wide network on synthetic noisy data with known observation noise variance and verify that the trained network's predictions match the analytical NTK-GP posterior mean.
2. Implement the shifted network approach with a simple prior mean function (e.g., linear regression) and verify that the network learns the residual while maintaining the prior mean.
3. Test the observation noise regularization on a real-world dataset with heteroscedastic noise and evaluate the quality of uncertainty estimates compared to standard NTK-GP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can algorithms be developed to efficiently compute the full posterior covariance, rather than just the mean, for wide neural networks in this framework?
- Basis in paper: [explicit] The Conclusion states future work "would require looking into algorithms to compute the posterior covariance," noting that current methods only recover the mean efficiently.
- Why unresolved: The paper proves the trained network output converges to the NTK-GP posterior mean, but the resulting covariance (detailed in Appendix I) is a mixture of NTK and NNGP contributions that does not correspond to the posterior covariance of any single GP.
- What evidence would resolve it: A derivation showing that a specific modification to the training dynamics or an ensemble method converges to the exact Bayesian posterior covariance without requiring kernel matrix inversion.

### Open Question 2
- Question: Can this NTK-based training framework be adapted to train Gaussian Processes using standard, interpretable kernels such as RBF or Matérn?
- Basis in paper: [explicit] The Conclusion suggests future work could "explore the benefits of leveraging the NTK to efficiently train GPs with interpretable and widely used kernels, such as RBF or Matérn."
- Why unresolved: The current equivalence is strictly between neural network training and the Neural Tangent Kernel (NTK), which is determined by the architecture rather than being a user-chosen, interpretable kernel.
- What evidence would resolve it: A theoretical mapping or architectural modification that forces the network's effective kernel to match a target kernel (e.g., RBF) during the gradient descent process.

### Open Question 3
- Question: Can the requirement to store initial parameters be eliminated while retaining the ability to enforce arbitrary prior means?
- Basis in paper: [inferred] Section 5.1 notes that the "shifted network" approach requires "storing the initial parameters $\theta_0$, effectively doubling the memory requirements, which can be prohibitive for large networks."
- Why unresolved: The method relies on explicitly calculating the difference between current and initial predictions ($f(x, \theta) - f(x, \theta_0)$) to shift the prior mean, necessitating the retention of the initialization state.
- What evidence would resolve it: A training scheme or architectural constraint that achieves the deterministic convergence of the shifted network without requiring explicit access to $\theta_0$ during the backward pass or storage.

## Limitations
- Theoretical results assume sufficiently wide networks and rely on linearization approximations that may not hold in practical, finite-width settings.
- The shifted network approach requires computing the prior mean's gradient with respect to network parameters, which could be computationally expensive for complex mean functions.
- Empirical validation focuses on relatively simple architectures and datasets, leaving open questions about performance on deeper networks or more challenging real-world problems.

## Confidence
High: The theoretical connection between observation noise regularization and NTK-GP posterior uncertainty is well-established through the proof that this preserves network linearization.
Medium: The empirical demonstrations showing convergence to linearized dynamics as width increases, while supportive, use limited experimental conditions.
Low: The practical impact of the observation noise regularization in real-world noisy datasets beyond synthetic experiments remains to be thoroughly established.

## Next Checks
1. Test the observation noise regularization on larger-scale real-world datasets with heteroscedastic noise patterns to evaluate practical performance
2. Benchmark the computational efficiency and memory requirements of the shifted network approach against ensemble methods across varying network widths and dataset sizes
3. Extend empirical validation to deeper architectures (ResNets, Transformers) and more complex tasks (image classification, language modeling) to assess generalizability of the theoretical claims