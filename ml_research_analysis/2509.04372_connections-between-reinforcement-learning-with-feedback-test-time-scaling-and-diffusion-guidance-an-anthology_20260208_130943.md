---
ver: rpa2
title: 'Connections between reinforcement learning with feedback,test-time scaling,
  and diffusion guidance: An anthology'
arxiv_id: '2509.04372'
source_url: https://arxiv.org/abs/2509.04372
tags:
- diffusion
- pdata
- scaling
- arxiv
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes formal connections between several widely
  used post-training techniques for large language models. The authors show that reinforcement
  learning with human feedback (RLHF), reinforcement learning with internal feedback
  (RLIF), and soft best-of-N sampling are asymptotically equivalent under certain
  parameter conditions, with test-time scaling approaches converging to RLHF/RLIF
  solutions as the number of candidates increases.
---

# Connections between reinforcement learning with feedback,test-time scaling, and diffusion guidance: An anthology

## Quick Facts
- **arXiv ID**: 2509.04372
- **Source URL**: https://arxiv.org/abs/2509.04372
- **Reference count**: 2
- **Primary result**: RLHF, RLIF, and soft best-of-N sampling are asymptotically equivalent under certain conditions

## Executive Summary
This paper establishes formal connections between several widely used post-training techniques for large language models. The authors show that reinforcement learning with human feedback (RLHF), reinforcement learning with internal feedback (RLIF), and soft best-of-N sampling are asymptotically equivalent under certain parameter conditions, with test-time scaling approaches converging to RLHF/RLIF solutions as the number of candidates increases. They also extend these insights to diffusion models, demonstrating that soft best-of-N sampling can achieve classifier-free guidance asymptotically. Building on these connections, the authors propose an "RL-free alignment" approach that sidesteps explicit reinforcement learning by using maximum likelihood estimation with importance sampling, approximating the exponentially-tilted distributions used in RLHF and RLIF.

## Method Summary
The paper proposes an "RL-free alignment" method that reformulates alignment as weighted maximum likelihood estimation. Given a pre-trained reference policy π_ref and a reward function r, the approach generates samples from π_ref and assigns importance weights proportional to exp(r/β). The model is then trained to maximize the weighted log-likelihood of these samples, effectively learning the exponentially-tilted target distribution without explicit reinforcement learning. The method is further extended to diffusion models using a similar importance resampling framework.

## Key Results
- RLHF and RLIF can be reformulated as minimizing KL divergence to exponentially tilted reference distributions
- Soft Best-of-N sampling converges asymptotically to the optimal RLHF distribution as N approaches infinity
- The proposed "RL-free alignment" method achieves similar results to RLHF/RLIF without explicit reinforcement learning optimization
- These connections extend to diffusion models, showing soft best-of-N can achieve classifier-free guidance asymptotically

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RLHF and RLIF can be reformulated as minimizing the KL divergence to an exponentially tilted reference distribution.
- **Mechanism:** The optimization objectives for both RLHF and RLIF—typically expressed as reward maximization with KL penalties—algebraically rearrange into a form that seeks the closest probability distribution to a "tilted" version of the reference policy. In RLHF, this tilt is determined by the exponential of the reward (exp(r/β)); in RLIF, it is determined by the policy's own confidence (entropy).
- **Core assumption:** The learned reward function accurately reflects the true underlying preference model (Bradley-Terry model), and for exact equivalence between RLHF and RLIF, the reference policy must already be aligned with human preferences (π_ref = π_hp).
- **Evidence anchors:**
  - [abstract] "clarify some intimate connections and equivalences between reinforcement learning with human feedback, reinforcement learning with internal feedback"
  - [section 2.2] Demonstrates the equivalence arg max_θ (...) = arg min_θ KL(π_θ || π_hp^(1/β_hf) π_ref) (Eq. 9) and the similar form for RLIF (Eq. 10).
  - [corpus] Corpus signals confirm active research interest in connecting policy gradients and guidance, though specific algebraic equivalence proofs are unique to this paper.
- **Break condition:** If the reference policy is not aligned (π_ref ≠ π_hp), the mapping between RLHF and RLIF parameter conditions (β_hf = Lβ_if - 1) no longer holds.

### Mechanism 2
- **Claim:** Soft Best-of-N sampling (test-time scaling) converges asymptotically to the optimal policy distribution of RLHF as the number of candidates N → ∞.
- **Mechanism:** By sampling N responses from the reference policy and selecting one with probability proportional to the tilted preference π_hp(o)^(1/β_tts), the resulting distribution over selected outputs approaches the exponentially tilted target distribution. This effectively implements rejection sampling or importance resampling to approximate the RLHF objective without training.
- **Core assumption:** The reward function is bounded (0 ≤ r(q,o) ≤ r_max), ensuring the weights do not explode, and N is sufficiently large to reduce the variance of the selection distribution.
- **Evidence anchors:**
  - [abstract] "test-time scaling approaches converging to RLHF/RLIF solutions as the number of candidates increases"
  - [section 2.2] Proves P(o_best = o | q) → (π_ref(o)π_hp(o)^(1/β_tts))/(Σ...) (Eq. 15) and notes equivalence to Eq. 9 when β_tts = β_hf.
  - [corpus] "Extending Test-Time Scaling" and "Policy Gradient Guidance" support the general viability of scaling compute at test time to improve policy performance.
- **Break condition:** If N is small, the selection distribution has high variance and may deviate significantly from the smooth optimal RLHF distribution.

### Mechanism 3
- **Claim:** Explicit reinforcement learning optimization can be sidestepped by using Importance Sampling (IS) to convert the alignment problem into a weighted Maximum Likelihood Estimation (MLE) problem.
- **Mechanism:** Instead of optimizing a policy via PPO-style gradients to maximize reward, the proposed "RL-free" method draws samples from the reference distribution π_ref and assigns importance weights proportional to exp(r(o)/β). By maximizing the weighted log-likelihood of these samples, the model is trained to mimic the exponentially tilted target distribution directly.
- **Core assumption:** The reference policy π_ref has sufficient support (coverage) over high-reward regions so that the importance weights are not dominated by noise or extreme values (low variance IS).
- **Evidence anchors:**
  - [abstract] "sidesteps the need for explicit reinforcement learning techniques" via "maximum likelihood estimation with importance sampling."
  - [section 2.3] "arg max_θ E_{o ~ Z^(-1)exp(r/β)π_ref} [log π_θ(o|q)]" (Eq. 20).
  - [corpus] "Maximum Likelihood Reinforcement Learning" is a direct neighbor, validating the conceptual shift toward likelihood-based methods for control.
- **Break condition:** If the reward landscape is sparse or the reference policy is poor, importance weights will have high variance, leading to unstable or ineffective training.

## Foundational Learning

**Concept: KL Divergence Regularization**
- **Why needed here:** The entire framework relies on the mathematical identity that maximizing reward with a KL penalty is equivalent to minimizing the KL divergence to a specific "tilted" distribution. Without this, the connection between RLHF and Best-of-N selection is invisible.
- **Quick check question:** How does changing the temperature β affect the "sharpness" of the tilted target distribution π_target ∝ π_ref exp(r/β)?

**Concept: Importance Sampling (IS)**
- **Why needed here:** This is the engine of the proposed "RL-free" architecture. It explains how to estimate properties of a desired distribution (high reward) using samples from a different, available distribution (reference model).
- **Quick check question:** In the context of the paper, what does the weight w(o) = exp(r(o)/β) represent geometrically relative to the reference distribution?

**Concept: Exponential Tilting (Gibbs Distribution)**
- **Why needed here:** This is the mathematical bridge connecting the reward function (energy) to the policy distribution (probability). It appears in the RLHF objective (Eq. 9), the Best-of-N limit (Eq. 15), and the diffusion guidance (Eq. 31).
- **Quick check question:** Why does the paper claim RLIF is equivalent to RLHF only when the reference policy is already "aligned" with human preferences?

## Architecture Onboarding

**Component map:** Reference Policy (π_ref) -> Reward/Utility Scorer -> Weighting Engine (exp(score/β)) -> MLE Optimizer

**Critical path:** The correct implementation of the Importance Weight calculation (Eq. 19/38). If the weights are not normalized or the temperature β is not tuned, the gradient estimates will be biased or explode.

**Design tradeoffs:**
- **Stability vs. Efficiency:** RL-free MLE is more stable than PPO (less exploration noise) but relies heavily on the quality of π_ref.
- **Compute vs. Accuracy:** Test-time scaling (Mechanism 2) requires zero training but massive inference compute (N candidates). RL-free alignment (Mechanism 3) requires standard training compute but relies on IS variance.

**Failure signatures:**
- **Weight Collapse:** Training loss diverges or stalls because exp(r/β) produces extreme outliers; implies β is too low or reward scale is uncalibrated.
- **Reward Hacking:** The model learns to generate outputs that maximize the proxy reward r but fall outside the support of π_ref, resulting in out-of-distribution garbage.

**First 3 experiments:**
1. **Sanity Check (Toy Task):** Implement Soft Best-of-N on a small bandit task. Plot the distribution of selected samples against the analytical tilted distribution to verify Eq. 15 convergence as N increases.
2. **RL-free vs. RLHF (Alignment):** Fine-tune a small LLM (e.g., GPT-2) on a sentiment task. Compare "RL-free MLE" (weighted SFT) against standard PPO/DPO. Monitor KL divergence and reward achievement.
3. **Diffusion Resampling (Visual):** Generate images using the Soft Best-of-N approach for diffusion (Eq. 35). Visually confirm if increasing N sharpens the class-conditional generation (e.g., "generating a specific dog breed") similar to increasing classifier-free guidance scale.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: What are the finite-sample convergence rates for soft best-of-N sampling, and how does the reward range (r_max) affect the number of candidates needed to closely approximate the RLHF solution?
- Basis in paper: [inferred] The paper establishes that soft best-of-N is asymptotically equivalent to RLHF as N → ∞ (Page 4) and provides concentration bounds dependent on exp(r_max) (Page 7), but does not quantify the error for practical, finite N.
- Why unresolved: The proofs rely on asymptotic limits and high-probability bounds that may not be tight for the smaller N values used in practice.
- What evidence would resolve it: Explicit error bounds or empirical scaling laws relating the KL divergence between the best-of-N policy and the optimal RLHF policy to N and β.

**Open Question 2**
- Question: How robust is the equivalence between RLHF and RLIF when the reference policy π_ref is not perfectly aligned with human preferences?
- Basis in paper: [explicit] The paper notes that RLIF is equivalent to RLHF specifically in the "special case where... π_ref = π_hp" (Page 4).
- Why unresolved: The theoretical equivalence relies on this alignment assumption to equate the reward tilting of RLHF with the entropy/certainty tilting of RLIF. In practice, pre-trained reference models are rarely perfectly aligned.
- What evidence would resolve it: A theoretical derivation of the performance gap (e.g., KL divergence or reward deficit) between RLHF and RLIF as a function of the divergence between π_ref and π_hp.

**Open Question 3**
- Question: Does the "RL-free alignment" method suffer from high variance due to the importance weights exp(r/β), limiting its practical applicability compared to explicit RL?
- Basis in paper: [inferred] The method utilizes importance sampling to adjust the data distribution (Page 5), a technique known to exhibit high variance when the proposal distribution (reference) differs significantly from the target (exponentially tilted) distribution.
- Why unresolved: While the paper proposes the MLE objective, it does not analyze the variance of the gradient estimator or compare its sample efficiency against standard RL techniques like PPO.
- What evidence would resolve it: Analysis of the gradient variance for the proposed objective and empirical comparisons of sample complexity against RLHF baselines.

## Limitations
- The equivalence between RLHF and RLIF requires the reference policy to already be aligned with human preferences, a condition rarely satisfied in practice
- RL-free alignment is fundamentally constrained by the coverage and quality of the reference policy; poor reference policies propagate errors
- The scalability claims for test-time approaches may not hold in high-stakes domains due to exponential computational requirements

## Confidence
- **High confidence**: The mathematical equivalence proofs connecting RLHF, RLIF, and soft best-of-N sampling
- **Medium confidence**: The RL-free alignment implementation details and their practical performance relative to traditional RL methods
- **Low confidence**: The scalability claims for test-time approaches in high-stakes domains

## Next Checks
1. **Importance Weight Calibration**: Systematically vary the temperature parameter β and measure the effective sample size (ESS) of the importance weights during RL-free training. Plot ESS against final reward achievement to identify optimal β ranges that balance variance and performance.

2. **Asymptotic Convergence Verification**: Implement soft best-of-N sampling across multiple N values (10, 100, 1000) on a controlled sentiment classification task. Measure the KL divergence between the empirical selection distribution and the theoretical tilted distribution to empirically validate the convergence claims.

3. **Reference Policy Sensitivity**: Compare RL-free alignment performance using reference policies of varying quality (pre-trained vs. SFT-tuned vs. RLHF-tuned) on the same alignment task. Quantify the performance degradation when moving away from an aligned reference to understand the practical limits of this approach.