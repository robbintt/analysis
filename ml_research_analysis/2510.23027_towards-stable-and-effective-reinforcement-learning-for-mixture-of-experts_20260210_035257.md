---
ver: rpa2
title: Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts
arxiv_id: '2510.23027'
source_url: https://arxiv.org/abs/2510.23027
tags:
- training
- routing
- grpo
- router
- gmpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability problem in applying reinforcement
  learning (RL) with verifiable rewards (RLVR) to Mixture-of-Experts (MoE) language
  models. The core issue is "router drift," where expert routing probabilities change
  substantially across policy updates, amplifying off-policy mismatch and leading
  to reward collapse.
---

# Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts

## Quick Facts
- arXiv ID: 2510.23027
- Source URL: https://arxiv.org/abs/2510.23027
- Reference count: 5
- Primary result: RSPO achieves 77.1 Pass@1 on math and 85.2 on code benchmarks, improving over GMPO/GSPO baselines

## Executive Summary
This paper addresses the instability problem in applying reinforcement learning with verifiable rewards (RLVR) to Mixture-of-Experts (MoE) language models. The core issue is "router drift," where expert routing probabilities change substantially across policy updates, amplifying off-policy mismatch and leading to reward collapse. The proposed method, Router-Shift Policy Optimization (RSPO), computes a per-token router-shift ratio from previously activated experts, applies stop-gradient and lower-bound flooring, and uses this as a trust weight to rescale importance ratios before clipping/aggregation. Experiments on Qwen3-30B-A3B demonstrate that RSPO achieves Pass@1 accuracy of 77.1 on math benchmarks (improving over GMPO/GSPO at 76.4) and 85.2 on code benchmarks (substantially outperforming GMPO at 82.5), while maintaining more stable training dynamics and higher reward trajectories compared to GRPO.

## Method Summary
RSPO addresses MoE instability in RLVR by computing a router-shift ratio for each token that measures how much the routing probabilities have changed on the experts activated in the previous policy. This ratio is derived by aggregating layer-wise routing deviations on the old activated experts, applying exponential transformation, and then applying stop-gradient and flooring. The resulting weight is used to rescale importance ratios before clipping, effectively down-weighting tokens with severe routing deviations. This soft adjustment reduces the influence of tokens with routing-induced off-policy mismatch while preserving router adaptivity. The method is implemented as a modification to GMPO, requiring caching of old routing statistics during rollout and adding ~1.5GiB memory overhead per device.

## Key Results
- RSPO achieves 77.1 Pass@1 accuracy on math benchmarks, improving over GMPO/GSPO at 76.4
- RSPO achieves 85.2 Pass@1 accuracy on code benchmarks, substantially outperforming GMPO at 82.5
- RSPO maintains more stable training dynamics with higher reward trajectories compared to GRPO
- Training throughput reduction of 20.8% vs GMPO under same configuration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Router drift across policy updates amplifies off-policy mismatch, causing volatile importance-ratio signals and bursty clipping that precede reward collapse.
- **Mechanism:** When the MoE router changes expert assignments between θ_old and θ, the same token can have substantially different routing probabilities. This distributional shift compounds the standard off-policy mismatch, increasing variance in importance ratios and triggering frequent clipping activations.
- **Core assumption:** Router drift is a primary driver of MoE-specific instability, not merely a correlated symptom.
- **Evidence anchors:**
  - [abstract] "router drift, where expert routing probabilities change substantially across policy updates, amplifying off-policy mismatch and leading to reward collapse"
  - [Section 3.3] "routing stability degrades over training and coincides with increasingly volatile off-policy mismatch signals and bursty clipping activity"
  - [corpus] Weak direct evidence; related work (StableMoE) identifies routing fluctuation as instability source but in supervised settings, not RLVR.

### Mechanism 2
- **Claim:** Softly down-weighting tokens with severe routing deviations via a router-shift ratio stabilizes optimization while preserving router adaptivity.
- **Mechanism:** RSPO computes γ_i,t = exp(-Δ_i,t) where Δ aggregates layer-wise routing deviation on old activated experts. This bounded coefficient (0,1] rescales importance ratios before clipping: w̃_i,t = w_i,t · γ̃_i,t. Tokens with large routing shifts contribute less to policy updates.
- **Core assumption:** The relationship between routing deviation and gradient reliability is approximately monotonic—greater drift implies less trustworthy gradients.
- **Evidence anchors:**
  - [Section 4.2] "tokens with severe routing deviations contribute less to the policy update, mitigating routing-induced off-policy mismatch while preserving router adaptivity"
  - [Figure 5] RSPO maintains stable router-shift ratios and reduces volatility in importance-ratio signals compared to GRPO
  - [corpus] GSPO similarly reduces variance via geometric aggregation, suggesting variance reduction is a viable stabilization pathway.

### Mechanism 3
- **Claim:** Stop-gradient on the router-shift weight prevents gradient coupling that would amplify variance under non-smooth top-K routing.
- **Mechanism:** Treating γ̃ as a detached sample weight prevents ∂logγ/∂θ from flowing through. Without stop-gradient, the router-shift penalty couples with the geometric objective and clipping, creating competing gradient signals through discrete routing decisions.
- **Core assumption:** Router-shift weights should act as trust signals, not optimization objectives themselves.
- **Evidence anchors:**
  - [Section 5.3] "Allowing gradients to flow through the router-shift weight leads to rapid instability in our small-scale setting"
  - [Appendix B, Figure 6] Backpropagating through γ triggers early collapse
  - [corpus] No direct corpus evidence; this appears to be a novel finding in the RLVR-MoE context.

## Foundational Learning

- **Concept:** Importance sampling ratios in policy gradient methods
  - **Why needed here:** RSPO modifies importance ratios before clipping. Understanding why ratios matter—estimating expectation under π_θ using samples from π_θ_old—is essential to grasp what rescaling accomplishes.
  - **Quick check question:** Given w = π_θ(a|s) / π_θ_old(a|s), what happens to gradient variance when w becomes large?

- **Concept:** MoE routing mechanisms (top-K selection, expert distributions)
  - **Why needed here:** The router-shift ratio measures probability mass changes on old activated experts. You need to understand what routers output (distributions over experts) and how top-K creates discrete selections.
  - **Quick check question:** For a layer with 128 experts and K=8, what does r_φ^(ℓ)(e|c_i,t) represent and how is it used during forward pass?

- **Concept:** Off-policy mismatch and clipping in PPO/GRPO
  - **Why needed here:** The paper diagnoses router drift as amplifying off-policy mismatch. Understanding baseline PPO clipping (and why it exists) clarifies what RSPO adds.
  - **Quick check question:** Why does PPO clip importance ratios at 1 ± ε rather than using unclipped surrogate objectives?

## Architecture Onboarding

- **Component map:** Rollout phase (sample responses using π_θ_old; cache old top-K expert indices and routing probabilities per token per layer) -> Router-shift computation (compute layer-wise deviation d^(ℓ)_i,t comparing current vs. old router probabilities on old activated experts; aggregate to Δ_i,t; derive γ_i,t = exp(-Δ_i,t)) -> Processing (apply floor γ_min and stop-gradient to get γ̃_i,t) -> Loss computation (rescale importance ratios w̃_i,t = w_i,t · γ̃_i,t before feeding to base algorithm's clipping/aggregation) -> Backward (standard gradient flow through rescaled ratios; no gradients through γ̃)

- **Critical path:** Caching old routing statistics during rollout is the latency-sensitive step. Compute router-shift ratios on-device before loss calculation to avoid synchronization overhead.

- **Design tradeoffs:**
  - **Memory vs. stability:** Caching old routing probabilities and indices adds ~1.5 GiB per device for Qwen3-30B-A3B (0.75 GiB each for FP16 probabilities and 16-bit indices)
  - **Throughput vs. stability:** 20.8% throughput reduction vs. GMPO under same configuration
  - **Floor value:** Lower γ_min preserves more learning signal but risks instability; γ_min=0.8 worked across both small and large settings

- **Failure signatures:**
  - Rising ClipFrac_γ (fraction of tokens below γ_min) indicates accumulating routing drift
  - Spiking ppo_kl or pg_clipfrac alongside router-shift degradation signals impending collapse
  - Sharp entropy decay suggests policy collapsing to deterministic outputs

- **First 3 experiments:**
  1. **Diagnostic replication:** Train GRPO on Qwen2.5-MoE (Countdown task) with router-shift logging disabled. Observe if collapse occurs around step 1500-2000 as in Figure 1. Then enable RSPO logging to confirm correlation between router-shift degradation and optimization instability.
  2. **Ablation on γ_min:** Sweep {0.2, 0.5, 0.8} on a held-out validation set. Confirm that γ_min=0.2 over-suppresses learning signal and γ_min ∈ {0.5, 0.8} yields comparable stability.
  3. **Stop-gradient validation:** Train two runs on small-scale setting—one with stop-gradient on γ̃, one without. Verify that backpropagating through router-shift weight causes early collapse as reported in Appendix B.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the router-shift weight be made differentiable without inducing training instability?
- Basis in paper: [explicit] Appendix B states that backpropagating through the router-shift ratio triggers early collapse, necessitating a stop-gradient operation.
- Why unresolved: Treating the weight as a detached constant prevents the router from directly optimizing the stability signal, potentially limiting the convergence ceiling.
- What evidence would resolve it: A regularization technique or smooth approximation that allows gradient flow through γ while maintaining stable reward curves.

### Open Question 2
- Question: Does RSPO generalize to "soft" routing mechanisms or Expert Choice architectures?
- Basis in paper: [inferred] The method relies on identifying "old activated experts" (Sec 3.2), assuming discrete Top-K token-to-expert assignment.
- Why unresolved: Soft MoE or Expert Choice routing involves continuous mixing or expert-driven selection, where the concept of a discrete "activated set" to track drift against may not apply directly.
- What evidence would resolve it: Theoretical adaptation of the router-shift ratio for continuous routing weights and empirical tests on non-Top-K MoE models.

### Open Question 3
- Question: Can the memory and communication overhead of RSPO be reduced for trillion-parameter scale training?
- Basis in paper: [explicit] Section 5.6 notes a 20.8% reduction in training throughput and ~1.5GiB memory overhead per device due to caching routing traces.
- Why unresolved: As model size and sequence length increase, the linear scaling of caching old top-K probabilities may become a prohibitive bottleneck.
- What evidence would resolve it: An implementation using CPU offloading, gradient checkpointing, or approximate drift metrics that mitigates the throughput penalty without sacrificing stability.

## Limitations

- Evaluation relies on binary rule-based rewards (math correctness, code compilation), representing a narrow slice of real-world RLVR applications
- Stability improvements demonstrated on Qwen3-30B-A3B need validation across different MoE configurations and model families
- The choice of γ_min=0.8 as a universal floor across experiments may not be optimal for all regimes

## Confidence

**High Confidence:** The core mechanism of router-shift ratio computation and its role in stabilizing importance sampling in MoE RLVR. The theoretical motivation connecting router drift to amplified off-policy mismatch is sound and supported by the experimental observations in Figures 1 and 5.

**Medium Confidence:** The specific choice of γ_min=0.8 and the claim that RSPO consistently improves over GMPO/GSPO across both math and code benchmarks. While results are positive, the evaluation is limited to two specific tasks and one MoE architecture.

**Low Confidence:** The claim that stop-gradient on router-shift weights is strictly necessary for stability. The evidence comes from a small-scale ablation, and there may be alternative gradient flow patterns or regularization approaches that could achieve similar or better results.

## Next Checks

1. **Cross-architecture validation:** Apply RSPO to a different MoE architecture (e.g., different expert count, K value, or router type) on the same math/code benchmarks to test generalizability beyond Qwen3-30B-A3B's specific configuration.

2. **Reward landscape diversity:** Evaluate RSPO on RLVR tasks with continuous or multi-dimensional reward signals (e.g., human preference data with scalar rewards) rather than binary correctness checks to assess performance in more realistic RLVR scenarios.

3. **Adaptive thresholding study:** Replace the fixed γ_min=0.8 with an adaptive threshold based on router statistics (e.g., mean/std of router-shift ratios) and measure impact on both stability and final task performance to determine if learned thresholds outperform hand-tuned values.