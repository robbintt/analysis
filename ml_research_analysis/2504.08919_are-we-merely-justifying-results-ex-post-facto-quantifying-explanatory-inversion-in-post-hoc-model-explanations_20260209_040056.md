---
ver: rpa2
title: Are We Merely Justifying Results ex Post Facto? Quantifying Explanatory Inversion
  in Post-Hoc Model Explanations
arxiv_id: '2504.08919'
source_url: https://arxiv.org/abs/2504.08919
tags:
- inversion
- spurious
- explanation
- explanations
- explanatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the concept of explanatory inversion, where
  post-hoc explanations rely on model outputs rather than faithfully reflecting input-output
  relationships. To quantify this phenomenon, the authors propose the Inversion Quantification
  (IQ) framework, which evaluates explanations using two dimensions: reliance on outputs
  (R) and explanation faithfulness (F).'
---

# Are We Merely Justifying Results ex Post Facto? Quantifying Explanatory Inversion in Post-Hoc Model Explanations

## Quick Facts
- **arXiv ID:** 2504.08919
- **Source URL:** https://arxiv.org/abs/2504.08919
- **Reference count:** 40
- **Primary result:** Introduces Inversion Quantification (IQ) framework and shows widely used methods like LIME and SHAP are prone to explanatory inversion, which can be mitigated by Reproduce-by-Poking (RBP).

## Executive Summary
This paper introduces the concept of "explanatory inversion" in post-hoc model explanations, where explanations rely more on model outputs than on faithful input-output relationships. The authors propose the Inversion Quantification (IQ) framework to measure this phenomenon through two dimensions: reliance on outputs (R) and explanation faithfulness (F). They demonstrate that common explanation methods like LIME and SHAP exhibit significant inversion, especially under spurious correlations, across tabular, image, and text domains. To address this, they propose Reproduce-by-Poking (RBP), a model-agnostic enhancement that uses forward perturbation checks to reduce inversion by 1.8% on average.

## Method Summary
The paper proposes the Inversion Quantification (IQ) framework to measure explanatory inversion through two metrics: Reliance (R) measuring correlation between attribution changes and output changes under perturbation, and Faithfulness (F) measuring alignment between attributions and actual feature effects on output. The framework is validated across synthetic tabular, image, and text datasets with common explanation methods (SHAP, LIME, Integrated Gradients, Occlusion). To mitigate inversion, the authors introduce Reproduce-by-Poking (RBP), which applies multiple perturbations to each feature and refines attributions by penalizing those that are unstable under small input variations while predictions remain constant.

## Key Results
- IQ framework reveals significant explanatory inversion across LIME, SHAP, IG, and Occlusion methods (average IS reduction of 1.8% via RBP)
- Spurious feature injection during inference consistently increases R, decreases F, and raises inversion scores across all tested explanation methods
- RBP reduces inversion scores by approximately 0.95% for tabular, 2.19% for image, and 2.39% for text data
- Baseline explanation methods show higher inversion scores under stronger spurious correlations (ψ=0.8)

## Why This Works (Mechanism)

### Mechanism 1: Inversion Quantification (IQ) Framework
Post-hoc explanations can exhibit "explanatory inversion," where attributions depend more on model outputs than the true input-output relationship, quantifiable through a two-dimensional metric. IQ combines (1) **Reliance on Outputs (R)**: correlation between attribution changes and output changes under perturbation; (2) **Explanation Faithfulness (F)**: alignment between attributions and actual feature effects on output. These form the Inversion Score: IS = [(R^p + (1-F)^p)/2]^(1/p). Higher IS indicates greater inversion. Attributions that faithfully reflect the forward decision process should (a) correlate with output changes when features are perturbed (high R) and (b) align with actual causal effects on output (high F).

### Mechanism 2: Spurious Feature Injection as Diagnostic Probe
Injecting features that correlate with output only at inference time reveals whether explanation methods incorrectly attribute importance to non-causal features. Define spurious feature as x_spur = ψM(x) + ε, injected only during testing (not training). Since the model never learned to use x_spur, any attribution to it indicates the explanation method is responding to output correlation rather than causal mechanism. Measure impact via ΔIS = IS_spur - IS_base. Features uninformative during training but correlated with output at test time should receive near-zero attribution if explanations are faithful.

### Mechanism 3: Reproduce-by-Poking (RBP) as Mitigation
Forward perturbation checks can reduce inversion by penalizing attributions that are unstable under small input variations while predictions remain constant. For each feature j, apply multiple perturbations generating x_pert,j. Compute attribution deviation δ^(j) = mean(|a_pert - a_base|). Refine: ã^(j) = a^(j) / (1 + δ^(j) · λ). Features with high instability (likely spurious) get penalized. True causal features should have stable attributions under small perturbations that preserve predictions; unstable attributions indicate output-driven rationalization.

## Foundational Learning

- **Concept: Shapley Values and Feature Attribution**
  - **Why needed here:** SHAP (one of the tested methods) uses game-theoretic Shapley values to distribute "credit" for the prediction among features. Understanding this helps interpret what attributions represent and why they might fail.
  - **Quick check question:** If features x1 and x2 are perfectly correlated with output y, but only x1 is causally responsible, will Shapley values distinguish them?

- **Concept: Local Surrogate Models (LIME)**
  - **Why needed here:** LIME approximates complex model behavior locally with interpretable linear models. The weights of this surrogate become attributions. This abstraction layer can introduce inversion.
  - **Quick check question:** If the surrogate model is trained to match outputs (not input-output dynamics), what failure mode might emerge?

- **Concept: Faithfulness vs. Plausibility in Explanations**
  - **Why needed here:** The paper distinguishes explanations that "look reasonable" (plausible) from those that accurately reflect model reasoning (faithful). IS specifically targets faithfulness failures via output dependency.
  - **Quick check question:** An explanation highlights pixels that humans find meaningful but the model never used. Is this a faithfulness or plausibility failure?

## Architecture Onboarding

- **Component map:**
  Input x → Model M(x) → Output y
       ↓
  Explanation Method E(x, M) → Baseline Attributions a
       ↓
  RBP Module:
    ├── Perturbation Generator → {x_pert,1, x_pert,2, ...}
    ├── Attribution Deviation Calculator → δ^(j) for each feature
    └── Refinement Step → ã^(j) = a^(j) / (1 + δ^(j)·λ)
       ↓
  Refined Attributions ã
       ↓
  IQ Evaluation (R, F, IS)

- **Critical path:**
  1. Implement baseline explanation method (start with SHAP or LIME via existing libraries: `shap`, `lime`, `captum`)
  2. Add perturbation sampling (Gaussian noise, default σ=5% of feature magnitude)
  3. Compute per-feature deviation δ across n_pert=3 perturbations
  4. Apply refinement with λ=0.1
  5. Evaluate using IQ metrics (R requires correlation computation; F requires ablation tests)

- **Design tradeoffs:**
  - **Perturbation count (n_pert):** Higher values increase stability of δ estimates but linearly increase compute. Paper shows IS is relatively stable across n_pert=1-9.
  - **Perturbation magnitude:** Paper tests 1-9% noise; IS remains stable. Larger perturbations risk changing predictions, violating the "M(x_pert) = M(x)" assumption.
  - **λ scaling factor:** Higher λ more aggressively penalizes unstable features but may over-suppress valid attributions. Default λ=0.1 is conservative.

- **Failure signatures:**
  - **RBP provides no improvement:** Check if baseline attributions are already stable (δ ≈ 0). This occurs with simple linear models (see Appendix F).
  - **Refined attributions are all near zero:** λ may be too large relative to δ values; normalize δ or reduce λ.
  - **IS increases after RBP:** Perturbations may be changing predictions (M(x_pert) ≠ M(x)), causing δ to capture legitimate sensitivity rather than instability.

- **First 3 experiments:**
  1. **Sanity check on synthetic tabular data:** Create y = x1·sin(x2) with dummy features x3-x6. Verify baseline explanations correctly upweight x1, x2. Inject spurious x3 = 0.8y + noise. Measure ΔIS. Confirm RBP reduces ΔIS.
  2. **Image classification with distractor:** Train CNN on shape classification. Add bright pixel to class-1 images at test time. Verify attribution shifts to pixel. Apply RBP and confirm ã for pixel region decreases.
  3. **Ablation on perturbation parameters:** On tabular data, vary n_pert ∈ {1, 3, 5, 10} and magnitude ∈ {1%, 5%, 10%}. Plot IS vs. parameters to verify robustness claims (Figure 5c-d replication).

## Open Questions the Paper Calls Out

- **Open Question 1:** Are domain-specific explanation methods, such as Grad-CAM or attention-flow mechanisms, susceptible to explanatory inversion to the same degree as general methods like LIME and SHAP? The authors explicitly note they focused on general methods applicable across domains and leave exploration of domain-specific methods as future work.

- **Open Question 2:** How can the Reproduce-by-Poking (RBP) method be adapted for multi-modal models to mitigate inversion without incurring prohibitive computational costs? The Conclusion notes that future research must explore expanding RBP to handle multi-modal models while maintaining scalability and computational efficiency.

- **Open Question 3:** Can the perturbation logic in RBP be theoretically or empirically improved to achieve a mitigation rate significantly higher than the average 1.8% reduction reported? The paper reports that RBP reduces inversion by 1.8% on average, suggesting that while effective, the majority of the inversion remains unmitigated by this specific enhancement.

## Limitations
- The IQ framework's effectiveness depends on perturbation magnitude and choice of reference points, which may vary across domains
- RBP mitigation shows modest improvements (1.8% average reduction) with unverified performance on real-world datasets with complex feature interactions
- Spurious feature injection approach may not fully capture the spectrum of inversion scenarios encountered in practice

## Confidence
- **High Confidence:** The existence of explanatory inversion (IS > 0) across multiple explanation methods and domains is well-supported by empirical evidence
- **Medium Confidence:** The RBP mitigation strategy's effectiveness is demonstrated on synthetic datasets, but its generalization to real-world scenarios requires further validation
- **Medium Confidence:** The IQ framework's ability to capture all forms of inversion is plausible but may miss subtler dependencies not captured by the R and F metrics

## Next Checks
1. **Domain Transfer Test:** Apply RBP to a real-world tabular dataset (e.g., UCI Adult) and measure IS reduction. Compare with synthetic results to assess generalization.
2. **Robustness to Perturbation Parameters:** Systematically vary perturbation magnitude (1-10%) and count (1-10) on synthetic data to identify thresholds where IQ metrics become unstable or RBP effectiveness plateaus.
3. **Alternative Mitigation Strategies:** Implement a variant of RBP using Shapley-value-based stability (e.g., variance across coalition sizes) and compare its IS reduction performance against the perturbation-based approach.