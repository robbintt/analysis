---
ver: rpa2
title: 'Beyond the convexity assumption: Realistic tabular data generation under quantifier-free
  real linear constraints'
arxiv_id: '2502.18237'
source_url: https://arxiv.org/abs/2502.18237
tags:
- constraints
- data
- each
- dataset
- dgms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of synthetic tabular data generation
  under user-defined constraints. The core method is Disjunctive Refinement Layer
  (DRL), a novel layer that enforces constraint compliance in deep generative models
  by compiling quantifier-free linear real arithmetic (QFLRA) formulas into a neural
  layer.
---

# Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints

## Quick Facts
- **arXiv ID:** 2502.18237
- **Source URL:** https://arxiv.org/abs/2502.18237
- **Reference count:** 40
- **Primary result:** Disjunctive Refinement Layer (DRL) eliminates constraint violations entirely for DGMs that frequently violated constraints, reducing violations from over 50% to 0% in many cases

## Executive Summary
This paper introduces Disjunctive Refinement Layer (DRL), a novel neural layer that enforces hard constraints on deep generative models for tabular data. DRL is the first method capable of handling non-convex and disconnected constraint spaces by compiling quantifier-free linear real arithmetic (QFLRA) formulas into a sequential dependency structure. The method guarantees zero constraint violations while improving downstream machine learning efficacy metrics by up to 21.4% in F1-score and 20.9% in AUC compared to baseline approaches.

## Method Summary
DRL operates through a two-phase approach: an offline compilation phase using modified Fourier-Motzkin elimination (specifically Cutting Planes resolution) to transform QFLRA constraints into a hierarchical structure, followed by an online application phase that sequentially projects generated samples onto valid intervals. The layer is added after the generator/decoder of standard DGMs like TVAE or CTGAN, processing features in a user-defined order to ensure each variable satisfies constraints given the previously processed variables. This approach eliminates the need for rejection sampling and provides faster constraint enforcement than linear-constrained alternatives.

## Key Results
- Eliminates constraint violations entirely (CVR = 0%) for QFLRA constraints where baseline methods showed over 50% violations
- Improves downstream machine learning efficacy by up to 21.4% in F1-score and 20.9% in AUC
- Outperforms linearly-constrained approaches on complex non-convex constraints
- Provides faster sampling than rejection sampling methods, which fail when violation rates approach 100%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Complex, non-convex logical constraints (QFLRA) can be compiled into a sequential dependency structure suitable for neural network integration.
- **Mechanism:** The layer utilizes a modified Fourier-Motzkin elimination procedure—specifically "Cutting Planes (CP) resolution"—to project constraints backward through the variable ordering ($x_D \to x_1$). This transforms a global constraint satisfaction problem into a sequence of local, single-variable interval checks dependent on previously computed features.
- **Core assumption:** The set of constraints $\Pi$ is satisfiable, and a valid variable ordering exists that respects user preferences for which features to modify first.
- **Break condition:** The constraint set is logically contradictory (unsatisfiable), triggering the `UNSAT FLAG` in Algorithm 1.

### Mechanism 2
- **Claim:** A generated sample can be refined to satisfy constraints by iteratively projecting variables onto the nearest valid interval defined by the "compiled" constraints of preceding variables.
- **Mechanism:** The layer processes features $x_1$ to $x_D$. For each feature $x_i$, it substitutes the values of previously refined features ($DRL(\tilde{x})_1 \dots DRL(\tilde{x})_{i-1}$) into the compiled constraints to determine the specific valid intervals (left/right boundaries) for $x_i$. If the raw generated $\tilde{x}_i$ falls outside these boundaries, it is moved to the closest boundary (Euclidean distance).
- **Core assumption:** The "Compile" phase (Mechanism 1) successfully guarantees that if preceding variables satisfy their constraints, a valid interval always exists for the current variable (Lemma 3.3).
- **Break condition:** The user-defined variable ordering forces a change in a feature that the model cannot accommodate without violating dependencies, though the theorem guarantees satisfaction if the compile phase succeeds.

### Mechanism 3
- **Claim:** Enforcing hard constraints via projection improves downstream machine learning efficacy compared to unconstrained generation or rejection sampling.
- **Mechanism:** By guaranteeing that synthetic samples lie within the valid logical space (often disconnected/non-convex), the layer prevents the generative model from wasting capacity on impossible regions and provides higher-quality training data for downstream classifiers/regressors. Rejection sampling fails when violation rates approach 100%.
- **Core assumption:** The "real" data distribution aligns with the provided background knowledge (constraints), and violation in synthetic data is a generative failure rather than a discovery of new valid states.

## Foundational Learning

- **Concept:** **Fourier-Motzkin Elimination**
  - **Why needed here:** DRL generalizes this method to handle disjunctions (ORs). You must understand how standard elimination projects inequalities onto a lower-dimensional space to grasp how the paper extends this to logical formulas.
  - **Quick check question:** Given a system of inequalities involving $x$ and $y$, can you manually eliminate $y$ to find the valid range of $x$?

- **Concept:** **Quantifier-Free Linear Real Arithmetic (QFLRA)**
  - **Why needed here:** This defines the input language. Unlike simple convex ranges (e.g., $0 < x < 10$), QFLRA allows disconnected spaces (e.g., $x < 0 \lor x > 10$), which is the core challenge this architecture solves.
  - **Quick check question:** How does the solution space of $(x > 10 \lor x < 5) \land (x \neq 7)$ differ geometrically from $(x > 5 \land x < 10)$?

- **Concept:** **Deep Generative Models (DGMs) for Tabular Data**
  - **Why needed here:** DRL is a "layer" added on top of models like TVAE or CTGAN. Understanding that these models output continuous vectors (often unbounded) explains why a refinement layer is necessary to clamp outputs to logical reality.
  - **Quick check question:** In a VAE, where does the "reparameterization trick" output a sample, and where would you insert a constraint layer?

## Architecture Onboarding

- **Component map:** Input (noise/encoder) -> Base DGM (generator/decoder) -> DRL (Compile phase: offline) -> DRL (Apply phase: online) -> Output (constraint-satisfied sample)
- **Critical path:** The variable ordering ($x_1; \dots; x_D$) in the **Apply** phase. This determines the dependency hierarchy; changing the order changes which features are moved to satisfy constraints and which remain "immutable" relative to later features.
- **Design tradeoffs:**
  - **Expressivity vs. Speed:** The number of constraints and disjunctions can cause exponential growth in the compilation step (though application remains fast).
  - **Ordering vs. Optimality:** The result is optimal *with respect to the variable ordering*, but not necessarily globally optimal (minimizing Euclidean distance across all dimensions simultaneously).
- **Failure signatures:**
  - **High CVR with Linear Layer:** If you use the baseline "Linear Layer" (neighbor paper) on QFLRA constraints, you will see high violation rates (e.g., 100% on House dataset) because it cannot handle disjunctions.
  - **Timeout:** Rejection sampling will hang or fail entirely if the base DGM learns a distribution that consistently violates constraints (CVR $\approx 100\%$).
- **First 3 experiments:**
  1. **Sanity Check (Single Variable):** Implement Eq (6) for a single variable with a disjoint constraint (e.g., $x < -5 \lor x > 5$). Verify that inputs at $0$ are pushed to $-5$ or $5$.
  2. **Compile Test:** Run the CP resolution (Algorithm 1, Compile function) on the Example 1 formula (Page 4). Verify you derive the implied constraints like $x_1 \le x_4$.
  3. **Integration:** Wrap a simple `torch.nn.Linear` layer (as a dummy generator) with the DRL Apply function. Generate random tensors and verify 0% Constraint Violation Rate (CVR) against a simple QFLRA rule.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the computational complexity of the CP resolution compilation step scale with the dimensionality and constraint density compared to standard linear programming methods?
- **Basis in paper:** [inferred] from Section 3.2, which states that the variable elimination procedure can generate a "non-polynomial number of constraints" in the intermediate sets $\Pi_{i-1}$.
- **Why unresolved:** The experiments are limited to datasets with 20-64 features, which may not reveal the exponential blow-up in constraint numbers associated with Fourier-Motzkin style elimination in very high-dimensional settings.
- **What evidence would resolve it:** A theoretical analysis or empirical benchmarking of the compile time and memory usage of DRL on synthetic datasets with significantly higher feature counts (e.g., $D > 500$) and dense constraint graphs.

### Open Question 2
- **Question:** Can the CP resolution rule and Disjunctive Refinement Layer be generalized to enforce non-linear (e.g., polynomial or convex quadratic) constraints?
- **Basis in paper:** [explicit] The paper explicitly restricts the problem scope to "Quantifier-Free Linear Real Arithmetic" (QFLRA) in Section 2, leaving non-linear relationships unaddressed.
- **Why unresolved:** The mathematical foundation of the CP resolution rule (Eq. 7) relies on the properties of linear inequalities to derive cutting planes; non-linear interactions introduce curved boundaries that standard linear elimination cannot resolve.
- **What evidence would resolve it:** A theoretical extension of the DRL compilation algorithm that accommodates non-linear boundaries or an empirical demonstration using an approximation scheme (e.g., piecewise linear approximations) compared against non-linear solvers.

### Open Question 3
- **Question:** To what extent does the arbitrary selection of variable ordering bias the joint probability distribution of the generated synthetic data?
- **Basis in paper:** [inferred] from Section 3.2 and Theorem 3.6, which note that the "optimality" of the refinement depends on the user-selected variable ordering ($x_1; \dots; x_D$).
- **Why unresolved:** While the method guarantees constraint satisfaction, the paper does not analyze how different orderings might distort the correlations or conditional dependencies between features compared to the true data distribution.
- **What evidence would resolve it:** An ablation study measuring statistical divergence (e.g., Jensen-Shannon divergence) between real data and synthetic data generated using reversed or randomized variable orderings.

## Limitations
- The compilation step's scalability to complex, high-dimensional constraint systems remains untested, with potential exponential growth in constraint numbers.
- The assumption that constraint violations in DGMs represent model failures may not hold in all domains where constraints are based on incomplete or evolving domain knowledge.
- The paper lacks discussion of memory or time limits for the pre-processing compilation step, despite demonstrating feasibility on five datasets with 12-18 constraints each.

## Confidence

**High Confidence:** The mechanism for handling non-convex and disconnected constraint spaces through sequential projection is well-supported by the mathematical framework and empirical results showing 0% CVR for QFLRA constraints.

**Medium Confidence:** The downstream efficacy improvements (up to 21.4% F1-score increase) are demonstrated but depend on the assumption that synthetic data distributions must match real distributions for effective training.

**Low Confidence:** The computational complexity analysis of the compilation phase is insufficient. While application remains fast, the paper lacks discussion of memory or time limits for the pre-processing step.

## Next Checks

1. **Scalability Test:** Apply DRL to a synthetic dataset with 50+ constraints involving multiple disjunctions to measure compilation time and memory usage.
2. **Constraint Validity Audit:** For datasets where constraints were manually annotated (CCS, House), verify that the generated synthetic samples satisfy all domain-expert specified logical relationships.
3. **Alternative Ordering Study:** Systematically evaluate how different variable orderings affect the final synthetic data distribution and downstream ML efficacy metrics beyond the three strategies (Rnd, Corr, KDE) tested.