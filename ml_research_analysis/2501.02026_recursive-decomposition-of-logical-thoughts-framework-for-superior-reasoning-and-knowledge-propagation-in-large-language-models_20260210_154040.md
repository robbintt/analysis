---
ver: rpa2
title: 'Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning
  and Knowledge Propagation in Large Language Models'
arxiv_id: '2501.02026'
source_url: https://arxiv.org/abs/2501.02026
tags:
- thought
- thoughts
- reasoning
- rdolt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RDoLT, a novel prompting framework designed
  to enhance the reasoning capabilities of large language models (LLMs) by recursively
  decomposing complex tasks into progressively structured sub-tasks, employing a multi-dimensional
  scoring system, and integrating a Knowledge Propagation Module to leverage both
  selected and rejected thoughts. RDoLT consistently outperforms existing methods
  such as Chain-of-Thought, CoT with Self-Consistency, and Least2Most across multiple
  benchmarks, achieving state-of-the-art results including a 90.98% accuracy on GSM8K
  and notable gains in SVAMP, MultiArith, and Gaokao 2023 Math.
---

# Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models

## Quick Facts
- arXiv ID: 2501.02026
- Source URL: https://arxiv.org/abs/2501.02026
- Reference count: 26
- Outperforms state-of-the-art reasoning methods across multiple benchmarks

## Executive Summary
RDoLT introduces a novel prompting framework that enhances large language model reasoning through recursive decomposition of complex tasks into progressively structured sub-tasks. The framework employs a multi-dimensional scoring system and a Knowledge Propagation Module that tracks both selected and rejected thoughts. Tested across five benchmarks (GSM8K, SVAMP, MultiArith, LastLetterConcatenation, Gaokao 2023 Math), RDoLT achieves state-of-the-art results including 90.98% accuracy on GSM8K, consistently outperforming Chain-of-Thought, CoT with Self-Consistency, and Least2Most methods.

## Method Summary
The framework recursively breaks down reasoning tasks into three complexity tiers (Easy, Intermediate, Final), generates multiple candidate thoughts per level, scores them across four dimensions (Logical Validity, Coherence, Simplicity, Adaptiveness), and propagates both selected and rejected thoughts forward via a Knowledge Propagation Module. This structured approach reduces cognitive load, filters low-quality reasoning paths, and preserves potentially useful information that might otherwise be discarded. The method was evaluated across multiple model sizes (ChatGPT-4o, Llama-3 8B, Qwen-2 7B, Gemma-2 9B/27B) and demonstrated consistent improvements over baseline methods.

## Key Results
- Achieves 90.98% accuracy on GSM8K benchmark, surpassing previous state-of-the-art
- Demonstrates consistent performance gains across SVAMP (11.29% improvement), MultiArith (2.99% improvement), and Gaokao 2023 Math benchmarks
- Shows optimal performance with 3 thoughts per step (53.33% success rate) compared to 5 (49.75%) or 7 thoughts (38.89%)
- Threshold sensitivity peaks at 30-35, with performance degrading at higher thresholds (≥40) due to over-processing

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical task decomposition into three fixed complexity tiers reduces cognitive load compared to single-pass reasoning. The decomposition function `fdecomp(R, θk)` transforms reasoning tasks into sub-tasks with progressive complexity, while `tk+1,i = ffeedback(tk,i, θk+1)` ensures outputs inform subsequent levels. Core assumption: problems can be meaningfully segmented into three tiers. Evidence: 3-tier structure outperforms alternatives; CARD extension suggests fixed tiers may not be optimal for all problems.

### Mechanism 2
Multi-dimensional scoring (Logical Validity, Coherence, Simplicity, Adaptiveness) filters low-quality thoughts before propagation, reducing error compounding. Each thought receives composite score `S(tki) = Svalid + Scohere + Ssimple + Sadapt`, with thoughts exceeding threshold τ selected. Core assumption: four dimensions are sufficient proxies for reasoning quality. Evidence: optimal thresholds vary (30-35); performance drops at τ≥40. Related work suggests model-generated scores can be unreliable without external validation.

### Mechanism 3
Propagating rejected thoughts alongside selected ones preserves potentially useful reasoning paths for later stages. The KPM maintains sets `Sk_selected` and `Sk_non-selected` at each level k, propagating both to subsequent levels k+1, k+2, ..., n. Core assumption: rejected thoughts contain recoverable value justifying memory overhead. Evidence: incorporating both selected and non-selected thoughts yields wider accuracy range; computational overhead noted as limitation. Context length limits may be exceeded with full KPM history.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) Prompting
  - **Why needed here:** RDoLT builds on CoT's step-by-step decomposition but adds scoring and propagation layers. Understanding CoT's error propagation problem motivates RDoLT's design.
  - **Quick check question:** Can you explain why a single incorrect intermediate step in CoT leads to incorrect final answers?

- **Concept:** Self-Consistency Voting
  - **Why needed here:** RDoLT compares against CoT-SC's majority voting. The paper critiques CoT-SC for discarding rare-but-correct paths—a gap KPM addresses.
  - **Quick check question:** Why might majority voting fail when only two reasoning paths are generated?

- **Concept:** Threshold-Based Filtering
  - **Why needed here:** RDoLT's scoring system requires setting thresholds (τ) for thought selection. Section 5.1 shows performance is sensitive to threshold choice.
  - **Quick check question:** What happens to success rate when threshold is set too high (≥40) versus too low (≤25)?

## Architecture Onboarding

- **Component map:** Task Decomposer -> Thought Generator -> Scoring Module -> Knowledge Propagation Module -> Final Answer Derivation
- **Critical path:** 1. Decompose task → 2. Generate thoughts for Easy level → 3. Score thoughts → 4. KPM selects/rejects → 5. Propagate to Intermediate → 6. Repeat through Final → 7. Derive final answer from highest-scoring Final thought
- **Design tradeoffs:** 3 thoughts/step outperforms 5 and 7 (53.33% vs 49.75% vs 38.89%); τ=30-35 optimal; RDoLT benefits stronger models more; 8192 tokens needed for full KPM history
- **Failure signatures:** All thoughts rejected at a level triggers regeneration; long chains exceed context window; LLM scorer produces unreliable scores for specialized domains; 3-tier decomposition not suited to all problems
- **First 3 experiments:**
  1. Implement RDoLT on GSM8K with n=3 thoughts, τ=35, using Llama-3 8B; compare against CoT and CoT-SC baselines; target ~72% accuracy
  2. Run RDoLT with KPM disabled versus full KPM; measure accuracy delta and context token usage; expect smaller accuracy but lower overhead without KPM
  3. Test τ ∈ {25, 30, 35, 40} on SVAMP benchmark; plot accuracy vs threshold to validate peak at 30-35; observe regeneration frequency

## Open Questions the Paper Calls Out

### Open Question 1
How does the RDoLT framework perform in highly specialized domains such as legal reasoning or medical diagnostics? The paper states generalizability to domain-specific tasks has not been fully explored and may require further fine-tuning. Current evaluation is limited to general reasoning and mathematical benchmarks, leaving specialized domain efficacy unknown.

### Open Question 2
Can the computational overhead of the Knowledge Propagation Module be optimized for resource-constrained environments? The paper identifies KPM's computational overhead as a factor that may limit scalability, particularly in resource-constrained environments, but focuses primarily on accuracy improvements rather than efficiency optimization.

### Open Question 3
How does framework reliability change when using LLM self-scoring versus external human evaluation? The paper notes that while LLM scoring reduces complexity, relying on the same LLM to generate and evaluate thoughts introduces potential bias that could reinforce hallucinations or inconsistency.

## Limitations
- Three-tier decomposition may not generalize to problems with ambiguous hierarchical structure or unpredictable complexity
- Scoring dimensions lack external validation; LLM self-scoring may introduce bias, especially in specialized domains
- Computational overhead of KPM (maintaining selected and rejected thoughts) may become prohibitive at scale or with longer reasoning chains

## Confidence

**High Confidence:** Recursive decomposition mechanism and empirical observation that n=3 thoughts per step outperforms alternatives are well-supported by ablation studies. Substantial accuracy gains on GSM8K, SVAMP, and MultiArith are reproducible.

**Medium Confidence:** Efficacy of four-dimensional scoring system is less certain; limited evidence that these specific metrics are optimal or that LLM self-scoring is reliable. Claim that rejected-thought propagation adds value is supported by accuracy ranges but lacks direct comparison to simpler alternatives.

**Low Confidence:** Generalizability beyond mathematical reasoning tasks is not established. Computational overhead claims are qualitative without concrete metrics on latency or memory usage. Framework's robustness to different decomposition strategies is untested.

## Next Checks

1. **Domain Generalization Test:** Apply RDoLT to non-mathematical benchmarks like StrategyQA, CommonsenseQA, or multi-hop reasoning datasets. Measure whether three-tier decomposition and KPM provide similar accuracy improvements outside arithmetic domains.

2. **Threshold Stability Analysis:** Conduct systematic study across 10+ diverse problem types to determine if optimal threshold (τ=30-35) is consistent or requires task-specific tuning. Include analysis of regeneration frequency and context usage at different thresholds.

3. **KPM Overhead Measurement:** Quantify computational cost by measuring (a) average prompt length with full KPM history, (b) token generation time per thought, and (c) accuracy degradation when context window is constrained (e.g., 4096 tokens). Compare metrics against baseline CoT methods to assess practical scalability.