---
ver: rpa2
title: 'LExT: Towards Evaluating Trustworthiness of Natural Language Explanations'
arxiv_id: '2504.06227'
source_url: https://arxiv.org/abs/2504.06227
tags:
- explanation
- explanations
- faithfulness
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LExT, a comprehensive framework for evaluating
  the trustworthiness of natural language explanations generated by large language
  models (LLMs). The framework quantifies both Plausibility and Faithfulness of explanations
  using novel metrics like Context Relevancy, QAG score, Counterfactual Stability,
  and Contextual Faithfulness.
---

# LExT: Towards Evaluating Trustworthiness of Natural Language Explanations

## Quick Facts
- arXiv ID: 2504.06227
- Source URL: https://arxiv.org/abs/2504.06227
- Reference count: 40
- Primary result: LExT framework quantifies LLM explanation trustworthiness through Plausibility and Faithfulness metrics, revealing significant performance differences across six models on medical datasets

## Executive Summary
This paper introduces LExT (Learning Explanation Trustworthiness), a comprehensive framework for evaluating the trustworthiness of natural language explanations generated by large language models. The framework quantifies both Plausibility and Faithfulness of explanations using novel metrics like Context Relevancy, QAG score, Counterfactual Stability, and Contextual Faithfulness. LExT integrates these metrics into a unified trustworthiness score that helps assess whether explanations are both convincing and faithful to the underlying reasoning.

Experiments on two medical datasets show significant performance differences across six models, with general-purpose models like Llama3 and Gemma outperforming domain-specific models in Plausibility and Faithfulness. The results highlight the importance of using tailored evaluation frameworks for trustworthy AI in high-stakes domains, particularly in healthcare where explanation quality directly impacts decision-making.

## Method Summary
LExT evaluates explanation trustworthiness through two complementary dimensions: Plausibility (how convincing the explanation appears) and Faithfulness (whether the explanation accurately reflects the model's reasoning). The framework uses four core metrics: Context Relevancy measures explanation alignment with input context using BERT-based similarity scoring; QAG score evaluates plausibility by measuring how well explanations answer generated questions about the input; Counterfactual Stability tests faithfulness by assessing explanation consistency when key evidence is removed; and Contextual Faithfulness quantifies alignment between explanations and input-output pairs.

The framework aggregates these metrics into a unified trustworthiness score through a weighted combination that can be customized based on application requirements. This allows practitioners to balance Plausibility and Faithfulness according to domain needs, with healthcare applications potentially prioritizing Faithfulness for safety-critical decisions.

## Key Results
- General-purpose models (Llama3, Gemma) significantly outperformed domain-specific medical models in both Plausibility and Faithfulness metrics
- Explanation quality varied substantially across models, with trustworthiness scores ranging from 0.45 to 0.82 on the evaluated datasets
- The unified trustworthiness score effectively differentiated between models, showing that higher Plausibility doesn't always correlate with higher Faithfulness
- Medical domain-specific models showed surprisingly lower performance, suggesting that specialization alone doesn't guarantee better explanation quality

## Why This Works (Mechanism)
The framework works by decomposing explanation trustworthiness into measurable components that capture different aspects of explanation quality. By separating Plausibility from Faithfulness, LExT can identify cases where explanations sound convincing but may not reflect actual reasoning, or where explanations are faithful but difficult to understand. The combination of similarity-based metrics (Context Relevancy, Contextual Faithfulness) with reasoning-based metrics (QAG score, Counterfactual Stability) provides complementary perspectives on explanation quality.

The use of automated metrics enables scalable evaluation across multiple models and datasets, while the unified scoring system allows for practical comparison and ranking. The framework's design recognizes that explanation quality is multidimensional and that different applications may require different balances between convincingness and faithfulness.

## Foundational Learning

**BERT-based similarity scoring** - Why needed: Provides quantitative measure of semantic alignment between explanations and input contexts. Quick check: Verify that similarity scores correlate with human judgments of explanation relevance.

**Question-answer generation for plausibility** - Why needed: Tests whether explanations actually address the key aspects of the input they claim to explain. Quick check: Ensure generated questions cover diverse aspects of the input and aren't trivially answerable.

**Counterfactual evidence removal** - Why needed: Identifies whether explanations rely on specific evidence or can maintain consistency without it. Quick check: Confirm that removing explicit evidence should substantially change faithful explanations.

**Unified trustworthiness scoring** - Why needed: Combines multiple quality dimensions into a single comparable metric. Quick check: Validate that the weighted combination reflects domain-specific priorities for explanation quality.

## Architecture Onboarding

**Component map:** Input -> Context Relevancy & Contextual Faithfulness (BERT) -> QAG Score (Question Generator + QA Model) -> Counterfactual Stability (Evidence Removal + Retest) -> Metric Aggregation -> Unified Trustworthiness Score

**Critical path:** Input context and explanation flow through all four metrics in parallel, then combine into final score. The QAG and Counterfactual Stability components are most computationally intensive as they require multiple LLM calls.

**Design tradeoffs:** The framework trades computational efficiency for comprehensive evaluation - using multiple automated metrics provides better coverage but increases evaluation time. The frozen BERT model ensures consistency but may not capture domain-specific terminology effectively.

**Failure signatures:** Low Context Relevancy with high QAG score suggests plausible but potentially fabricated explanations. High Contextual Faithfulness with low Counterfactual Stability indicates explanations that align with inputs but may over-rely on specific evidence. Low scores across all metrics suggest fundamental issues with explanation quality.

**First experiments to run:**
1. Evaluate explanation quality on a held-out validation set to verify metric stability
2. Compare automated metrics against human evaluations to validate metric relevance
3. Test framework on non-medical datasets to assess domain generalizability

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the results raise several implicit questions about the relationship between model architecture, training data, and explanation quality, particularly why domain-specific models underperform general-purpose models in explanation tasks.

## Limitations

- **Limited generalizability:** Framework evaluated only on two medical datasets, may not extend to other healthcare domains or non-medical applications
- **Metric reliability concerns:** BERT-based similarity metrics may not capture medical terminology nuances, and Counterfactual Stability assumptions may not hold for complex reasoning patterns
- **Dataset specificity:** Results based on limited dataset types, performance patterns may not reflect broader healthcare applications

## Confidence

- **Plausibility and Faithfulness Metric Performance:** Medium - Metrics show clear differentiation but reliability depends on underlying similarity models
- **Generalizability of Results:** Low - Findings based on limited medical datasets, uncertain for other domains
- **Model Comparison Conclusions:** Medium - Performance differences are consistent but may be influenced by dataset-specific factors

## Next Checks

1. Evaluate LExT metrics on non-medical datasets to assess generalizability across domains and verify if the same patterns hold for general-purpose explanations.

2. Compare LExT's automated metrics against human evaluations of explanation quality to validate that the framework captures what humans consider trustworthy explanations.

3. Test the framework with additional medical datasets from different specialties (e.g., radiology, pathology) to determine if the observed model performance differences are consistent across healthcare subdomains.