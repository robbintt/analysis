---
ver: rpa2
title: 'Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable
  AI'
arxiv_id: '2506.15408'
source_url: https://arxiv.org/abs/2506.15408
tags:
- explanation
- metrics
- arxiv
- explanations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a comprehensive framework for the functionality-grounded
  evaluation of XAI methods, termed VXAI. Through a systematic literature review of
  362 sources, it aggregates over 400 individual metrics into 41 functionally similar
  metric groups.
---

# Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI

## Quick Facts
- arXiv ID: 2506.15408
- Source URL: https://arxiv.org/abs/2506.15408
- Reference count: 40
- Primary result: Introduces a comprehensive framework organizing over 400 XAI metrics into 41 functionally similar groups across three dimensions

## Executive Summary
This paper presents VXAI, a comprehensive framework for evaluating Explainable AI (XAI) methods through functionality-grounded metrics. The authors conducted a systematic literature review of 362 sources, aggregating over 400 individual metrics into 41 functionally similar metric groups. These metrics are organized along three dimensions: explanation type, evaluation contextuality, and desiderata. The framework provides the most structured and extensible overview of XAI evaluation metrics to date, enabling systematic metric selection and cross-method comparison. The work reveals that fidelity is the most frequently addressed desideratum, while metrics for parsimony and plausibility are predominantly Explanans-Centric or Model Observation-based.

## Method Summary
The authors conducted a systematic literature review using six search strings across four databases (arXiv, ACM, IEEE, Scopus), yielding 362 sources. From these sources, they extracted 412 metrics and 36 evaluation methods, which were then aggregated into 41 functionally similar metric groups. The framework organizes these metrics across three dimensions: explanation type (Feature Attribution, Concept Explanation, Example Explanation, White-Box Surrogate, Natural Language Explanation), evaluation contextuality (Explanans-Centric, Model Observation, Data Observation, Task Observation, A Priori Constrained), and desiderata (Parsimony, Plausibility, Coverage, Fidelity, Continuity, Consistency, Efficiency). The authors analyzed metric frequencies across these dimensions to identify patterns in current XAI evaluation practices.

## Key Results
- Aggregated over 400 individual XAI metrics into 41 functionally similar metric groups
- Fidelity is the most frequently addressed desideratum across evaluation contexts
- Metrics for Parsimony and Plausibility are predominantly Explanans-Centric or Model Observation-based
- Natural Language Explanations are underrepresented in dedicated metrics
- The framework provides a systematic structure for metric selection and cross-method comparison

## Why This Works (Mechanism)
The framework works by providing a structured taxonomy that connects evaluation metrics to their functional purposes and contexts. By organizing metrics along three dimensions (explanation type, contextuality, and desiderata), it creates a systematic approach to selecting appropriate evaluation methods. The aggregation of similar metrics into functionally equivalent groups reduces redundancy and clarifies the landscape of available evaluation approaches. This structure enables researchers to understand which metrics are most appropriate for their specific XAI method and evaluation goals.

## Foundational Learning
- Explanation Types (why needed: to categorize different forms of AI explanations; quick check: verify methods fit into one of five categories)
- Evaluation Contextuality (why needed: to understand when and how metrics can be applied; quick check: confirm metrics align with context constraints)
- Desiderata (why needed: to identify what properties metrics should measure; quick check: validate desiderata cover all quality aspects)
- Metric Aggregation (why needed: to reduce redundancy and clarify functional similarities; quick check: ensure grouped metrics truly serve similar purposes)
- Systematic Review Methodology (why needed: to ensure comprehensive and unbiased literature coverage; quick check: verify search strings capture relevant literature)

## Architecture Onboarding
- Component Map: Search strings -> Literature sources (362) -> Extracted metrics (412) -> Aggregated metric groups (41) -> Three-dimensional framework
- Critical Path: Literature review → Metric extraction → Functional aggregation → Dimensional organization → Pattern analysis
- Design Tradeoffs: Comprehensive coverage vs. manual categorization effort; granularity vs. usability; specificity vs. extensibility
- Failure Signatures: Incomplete literature coverage → missing metric categories; subjective grouping → inconsistent classifications; context misalignment → inappropriate metric selection
- First 3 Experiments:
  1. Apply framework to a new XAI method to test categorization accuracy
  2. Compare metric outputs across multiple XAI methods using the same dataset
  3. Test inter-rater reliability on metric grouping process

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Under what conditions do different functionality-grounded XAI metrics agree or contradict, and what are the associated trade-offs?
- Basis in paper: The authors explicitly state, "Future work should investigate how different metrics behave in practice, under which conditions they agree or contradict, and what trade-offs they impose."
- Why unresolved: This work focused exclusively on surveying and categorizing existing literature rather than conducting empirical benchmarking.
- Evidence to resolve it: Empirical studies that systematically compare metric outputs across diverse models, datasets, and tasks to map their behavioral correlations.

### Open Question 2
- Question: What constitute practical thresholds for "good" explanation quality, and how can multiple metric scores be meaningfully aggregated?
- Basis in paper: The paper identifies a lack of consensus on evaluation protocols and notes that "Establishing practical thresholds... and exploring aggregation strategies remain open and important questions."
- Why unresolved: While many metrics exist, few offer clear benchmarks, and methods for combining metrics (e.g., weighted sums) lack standardization or broad acceptance.
- Evidence to resolve it: Empiratively validated threshold values for specific metrics or a robust framework for creating composite evaluation scores.

### Open Question 3
- Question: How can functionality-grounded metrics be effectively adapted and validated for underexplored explanation types, such as Natural Language Explanations (NLEs)?
- Basis in paper: The authors highlight that NLEs are "underrepresented in dedicated metrics" and explicitly call for future work to "adapt and extend these metrics to underexplored explanation types."
- Why unresolved: The review found limited metrics specifically designed for NLEs, often labeling existing ones as "potentially applicable" without published validation.
- Evidence to resolve it: Validation studies demonstrating that metrics transferred from other explanation types reliably capture the quality of natural language outputs.

## Limitations
- Selection bias may have been introduced through search string design and database coverage
- Aggregation of 400+ metrics into 41 groups relies on subjective functional similarity judgments
- Limited empirical validation of metric applicability across all XAI methods
- The framework structure, while internally consistent, requires independent verification in applied settings

## Confidence
- High: Framework structure and metric categorization
- Medium: Relative frequency analysis of desiderata across evaluation contexts
- Low: Claims about metric applicability across all XAI methods without empirical validation

## Next Checks
1. Conduct an independent replication of the metric grouping process using a subset of the literature to assess inter-rater reliability.
2. Apply the VXAI framework to a diverse set of XAI methods (e.g., image classification, NLP, tabular data) to test its cross-domain utility and identify any missing metric categories.
3. Design and execute a controlled experiment comparing multiple metrics from different VXAI groups on the same XAI method to evaluate their discriminative power and complementarity.