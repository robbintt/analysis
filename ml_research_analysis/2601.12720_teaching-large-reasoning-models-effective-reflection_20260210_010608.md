---
ver: rpa2
title: Teaching Large Reasoning Models Effective Reflection
arxiv_id: '2601.12720'
source_url: https://arxiv.org/abs/2601.12720
tags:
- scft
- reflection
- critique
- reasoning
- effective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of ineffective self-reflection in
  large reasoning models (LRMs), where reflections often fail to improve answers and
  increase computation cost. To address this, the authors propose Self-Critique Fine-Tuning
  (SCFT), a training framework that uses self-generated critiques filtered via rejection
  sampling to fine-tune the model for better reflective reasoning.
---

# Teaching Large Reasoning Models Effective Reflection

## Quick Facts
- arXiv ID: 2601.12720
- Source URL: https://arxiv.org/abs/2601.12720
- Reference count: 37
- Key result: SCFT improves reasoning accuracy by 4.8%-6.0% on AIME benchmarks; RLERR further enhances performance with reflection quality gains.

## Executive Summary
This paper tackles the challenge of ineffective self-reflection in large reasoning models (LRMs), where reflections often fail to improve answers and increase computation cost. The authors propose Self-Critique Fine-Tuning (SCFT), a training framework that uses self-generated critiques filtered via rejection sampling to fine-tune the model for better reflective reasoning. Building on SCFT, they introduce Reinforcement Learning with Effective Reflection Rewards (RLERR), which incorporates a hierarchical reward system based on reflection quality principles to further optimize the model's reasoning policy. Experiments on AIME2024 and AIME2025 show SCFT improves accuracy by 4.8%–6.0% and RLERR pushes performance further, achieving state-of-the-art results among models of similar size. Both reasoning accuracy and reflection quality significantly improve.

## Method Summary
The authors introduce Self-Critique Fine-Tuning (SCFT), a framework that fine-tunes LRMs to improve self-reflection during reasoning. SCFT uses a self-critique generator to produce critiques of the model's answers, which are filtered via rejection sampling to retain only high-quality critiques. These critiques are then used as training signals to fine-tune the model, enabling more effective reflection. Building on SCFT, Reinforcement Learning with Effective Reflection Rewards (RLERR) is introduced, employing a hierarchical reward system that evaluates reflection quality and reasoning correctness to optimize the model's reasoning policy. Both methods are evaluated on AIME2024 and AIME2025 benchmarks, demonstrating significant gains in reasoning accuracy and reflection quality.

## Key Results
- SCFT improves reasoning accuracy by 4.8%–6.0% on AIME benchmarks.
- RLERR further enhances performance, achieving state-of-the-art results among models of similar size.
- Both reasoning accuracy and reflection quality significantly improve with the proposed methods.

## Why This Works (Mechanism)
SCFT and RLERR improve reflection in LRMs by explicitly training the model to generate and utilize high-quality self-critiques. SCFT uses rejection sampling to filter critiques, ensuring only effective reflections are used for fine-tuning. RLERR then refines the model's reasoning policy using rewards that assess both the correctness of the answer and the quality of the reflection. This two-stage approach ensures that the model learns to reflect in a way that meaningfully improves its reasoning process, rather than just increasing computation cost.

## Foundational Learning
- **Self-Critique Generation**: Why needed: To enable models to identify and correct errors in their own reasoning. Quick check: Does the critique accurately identify the error?
- **Rejection Sampling**: Why needed: To filter out ineffective or low-quality critiques that do not improve reasoning. Quick check: Are the retained critiques of high quality and relevance?
- **Hierarchical Reward System**: Why needed: To balance the trade-off between reflection quality and reasoning correctness. Quick check: Do rewards effectively guide the model toward better reflection?

## Architecture Onboarding

### Component Map
Self-Critique Generator -> Rejection Sampler -> Fine-Tuning Module -> RLERR Reward System -> LRM

### Critical Path
1. Self-critique generation by the model.
2. Rejection sampling to filter high-quality critiques.
3. Fine-tuning using filtered critiques.
4. RLERR optimization with hierarchical rewards.

### Design Tradeoffs
- Rejection sampling ensures high-quality critiques but may discard useful but imperfect reflections.
- RLERR balances reflection quality and correctness but may overfit to the reward structure.

### Failure Signatures
- Poor reflection quality: Model generates critiques that do not identify errors.
- Over-reliance on reflection: Model spends excessive steps on reflection without improving accuracy.
- Reward misalignment: Model optimizes for rewards without improving reasoning.

### First 3 Experiments
1. Ablation study: Remove rejection sampling to quantify its impact on reflection quality.
2. Domain transfer: Evaluate SCFT and RLERR on non-mathematical reasoning tasks.
3. Human evaluation: Validate automated reflection quality scores with human judgment.

## Open Questions the Paper Calls Out
None

## Limitations
- Results are narrowly focused on AIME mathematics benchmarks, limiting generalizability to other domains.
- Reliance on automated reflection quality scoring (G-Eval) may not fully align with human judgment.
- Effectiveness of rejection sampling depends on the initial quality of the critique generator.

## Confidence
- Performance claims: Medium (experimental design and scope)
- Reflection quality claims: Low (reliance on automated metrics and limited domain testing)
- Efficiency claims: Low (small relative gains, limited task diversity)

## Next Checks
1. Replicate results on non-mathematical reasoning tasks (e.g., logical puzzles, scientific reasoning) to test domain transfer.
2. Conduct extensive human evaluation of reflection quality to validate automated scoring.
3. Perform ablation studies removing the rejection sampling step to quantify its contribution to downstream performance.