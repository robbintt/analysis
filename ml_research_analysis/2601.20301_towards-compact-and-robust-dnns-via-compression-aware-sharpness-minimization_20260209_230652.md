---
ver: rpa2
title: Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization
arxiv_id: '2601.20301'
source_url: https://arxiv.org/abs/2601.20301
tags:
- pruning
- robustness
- c-sam
- loss
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Compression-aware Sharpness Minimization
  (C-SAM), a novel framework that integrates DNN pruning with robustness optimization.
  Unlike existing approaches that optimize robustness and pruning separately, C-SAM
  shifts sharpness-aware learning from parameter perturbations to mask perturbations,
  explicitly optimizing pruning masks to achieve structural flatness and semantic
  robustness.
---

# Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization

## Quick Facts
- arXiv ID: 2601.20301
- Source URL: https://arxiv.org/abs/2601.20301
- Reference count: 40
- Key outcome: Achieves 42% higher certified robustness than strong baselines while maintaining task accuracy comparable to unpruned models

## Executive Summary
This paper introduces Compression-aware Sharpness Minimization (C-SAM), a novel framework that integrates DNN pruning with robustness optimization by shifting sharpness-aware learning from parameter perturbations to mask perturbations. Unlike existing approaches that optimize robustness and pruning separately, C-SAM explicitly optimizes pruning masks to achieve structural flatness and semantic robustness, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations.

## Method Summary
C-SAM employs a three-stage pipeline: (1) pre-train full-precision model on augmented dataset for 50 epochs, (2) freeze backbone and optimize soft pruning masks using composite loss comprising stability, ratio, and consistency terms for 100 epochs, and (3) binarize masks and fine-tune pruned architecture for 50 epochs. The method shifts SAM's philosophy of flat minima to the mask space by injecting uniform noise into soft masks and optimizing for prediction consistency across perturbed mask instances, directly addressing the gap between continuous weight-space sharpness and discrete structural changes induced by pruning.

## Key Results
- Achieves up to 42% higher certified robustness than strong baselines across multiple datasets
- Maintains task accuracy comparable to unpruned models after compression
- Demonstrates effectiveness on diverse architectures including ResNet-18, GoogLeNet, and MobileNet-V2
- Validated on three datasets (CelebA-HQ, Flowers-102, CIFAR-10-C) with semantic mutations via diffusion models

## Why This Works (Mechanism)

### Mechanism 1: Structural Flatness via Mask-Space Perturbations
C-SAM shifts sharpness-aware perturbations from model parameters to pruning masks, promoting robustness under discrete structural changes that pruning induces. By injecting uniform noise into soft masks (C_ξ = clip(C + ξ, 0,1), ξ ~ U(-μ, μ)) and optimizing for prediction consistency across perturbed mask instances, it enforces flatness in the structural topology space rather than weight magnitude space. This is novel because traditional SAM variants focus on weight-space perturbations and do not address mask-space sharpness minimization.

### Mechanism 2: Stability Loss (L_stab) Suppresses Compression-Induced Variance
L_stab = E_{x,ξ}[||p_{C_ξ}(x) - p̄(x)||²_2] minimizes prediction variance across stochastic mask instances, tightening the theoretical upper bound on semantic prediction discrepancy. This directly reduces Terms (A) and (C) in the prediction discrepancy decomposition, which represent variance from mask perturbations before and after semantic transformation. VASSO addresses variance suppression in SAM weight-space but does not address mask-space variance.

### Mechanism 3: Margin-Aware Ratio Loss (L_ratio) Enforces Semantic Consistency
L_ratio penalizes the ratio of prediction discrepancy to classification margin using softplus penalty when ratio exceeds safety threshold η: L_ratio = E[softplus(r - η)]. The theoretical sufficiency condition states that if ||p - p_T||_∞ < d, predictions remain invariant, where d is the top-2 class margin. This provides certified robustness guarantee by bounding prediction discrepancy relative to classification margins.

## Foundational Learning

**Concept: Sharpness-Aware Minimization (SAM)**
- Why needed here: C-SAM inherits SAM's philosophy (flat minima → better generalization) but departs from its implementation (weight perturbation → mask perturbation)
- Quick check question: Why does SAM's guarantee of flatness in weight space NOT ensure robustness after pruning removes weights entirely?

**Concept: Soft vs. Binary Pruning Masks**
- Why needed here: C-SAM optimizes continuous masks C ∈ [0,1]^N but deploys binary masks m̂ ∈ {0,1}^N; understanding this gap and L_consist's role is essential
- Quick check question: If soft masks produce robust predictions but binary masks do not, where does the failure occur and which loss term addresses it?

**Concept: Certified Robustness and PCA Metric**
- Why needed here: The paper optimizes for Probabilistically Certified Accuracy (PCA), not standard accuracy; this measures the fraction of samples where prediction flip probability is bounded below ε under semantic transformations
- Quick check question: What does "PCA = 74% with ε = 10^{-3}" actually guarantee about model behavior on perturbed inputs?

## Architecture Onboarding

**Component map:**
- Stage 1 (Pre-training): Train full-precision θ on augmented dataset S' with cross-entropy (50 epochs)
- Stage 2 (Robust mask search): Freeze θ, optimize soft masks C via composite loss L = λ_stab·L_stab + λ_ratio·L_ratio + λ_consist·L_consis + λ_1·L_1 (100 epochs)
- Stage 3 (Post-training): Binarize masks m̂ via top-k thresholding, fine-tune θ on pruned architecture (50 epochs)

**Critical path:**
1. Percentile-scaled mask initialization (Equation 5, τ=30) → random initialization fails (Table 5)
2. Noise injection for stochastic mask sampling (μ=0.5 optimal per Table 6)
3. Parallel computation of L_stab (two independent mask samples), L_ratio (mask pair for original vs. transformed input), L_consist (soft mask vs. binarized via STE)
4. Mask-only gradient updates with frozen backbone
5. Binarization + weight fine-tuning

**Design tradeoffs:**
- Noise magnitude μ (0.5 optimal): Higher → more exploration but optimization instability; μ=0.8 drops PCA from 74% to 66%
- Safety threshold η (1.0 optimal): Lower → stricter constraint but impairs representational capacity; η=0.95 drops PCA to 67%
- Initialization percentile τ (30 optimal): Controls how many weights start as "important"; τ=0 (full-scaled) yields PCA=0%, τ=40 drops to 63%
- KL vs. MSE for L_consis: KL divergence on full distributions outperforms MSE by 9% PCA (Table 4)

**Failure signatures:**
- Mask collapse (all → 0 or 1 early): Check initialization percentile; full-scaled initialization (τ=0) produces PCA=0%
- PCA degrades from L1 to L2 augmentation while accuracy stable: Model overfitting to specific transformation magnitudes; check diffusion perturbation settings
- Large accuracy/PCA gap between soft mask and binary deployment: L_consist insufficient; verify STE gradient flow
- Training instability with high noise: Reduce μ below 0.5; check gradient norms

**First 3 experiments:**
1. **Baseline replication**: Run C-SAM on CelebA-HQ with ResNet-18, pr=50%, default hyperparameters (μ=0.5, η=1.0, τ=30, λ_stab=5, λ_ratio=1, λ_consist=1). Target: ~74% PCA per Table 1
2. **Component ablation**: Remove L_stab, L_ratio, L_consist individually (retrain each variant). Expect ~6-8% PCA drop per ablation (Table 4). This validates each mechanism's contribution
3. **Structured pruning transfer**: Apply C-SAM to structured pruning on ResNet-18 (channel-level), compare against DepGraph and HESSO baselines. Target: Match or exceed Table 3 results (~64-66% PCA on CelebA-HQ L2)

## Open Questions the Paper Calls Out

### Open Question 1
How does C-SAM perform when handling multi-attribute semantic variations rather than single-attribute shifts? The conclusion explicitly lists exploring "C-SAM's interplay with multi-attribute semantic variations" as future work. This remains unresolved because current experiments utilize diffusion models to vary single attributes (e.g., "smiling"), leaving the dynamics of compound semantic shifts untested.

### Open Question 2
Can the computational overhead of the certification and mask optimization process be reduced for large-scale training? The authors state future work includes efforts to "further reduce its computational overhead for optimizing certified robustness." This is unresolved because the method requires sampling stochastic masks and transforming inputs via generative models, which is costlier than standard pruning techniques.

### Open Question 3
How robust is the certified robustness if the generative model used for semantic augmentation fails to capture the true data manifold? The framework relies on generators (e.g., DIFF-AE) to approximate the transformation space $S_T(x)$, but the paper does not analyze sensitivity to generator quality. This remains unresolved because if the generator produces artifacts or off-manifold images, the "semantic" robustness optimized by C-SAM may not transfer to real-world input variations.

### Open Question 4
Can the structural flatness optimization generalize to non-CNN architectures, specifically Vision Transformers? Experiments are limited to CNNs (ResNet, GoogLeNet, MobileNet-V2). This is unresolved because the mask broadcasting operator $\mathcal{T}$ is defined for weights and channels; Transformers have distinct structural dependencies (attention heads) that may require a different flatness definition.

## Limitations

- Mask-space flatness theoretical guarantees are asserted rather than empirically validated across diverse architectures
- Performance heavily depends on hyperparameters (μ=0.5, τ=30, η=1.0) that were tuned on specific datasets
- Certified robustness claims rely on semantic transformations generated by diffusion models with unclear prevalence in practical deployment scenarios

## Confidence

- **High confidence**: Task accuracy claims and relative performance improvements over baselines (Table 1,3) - standard metrics with reproducible evaluation protocols
- **Medium confidence**: PCA certification methodology and its absolute values - certification framework is sound but depends on specific diffusion model implementations
- **Low confidence**: Theoretical claims about structural flatness transferring from mask-space to weight-space after binarization - critical mechanism lacks empirical validation beyond the paper's own results

## Next Checks

1. **Mechanism isolation**: Implement a variant that applies traditional SAM (weight perturbations) to the same pruned architectures and compare PCA results to determine if mask-space perturbations provide unique benefits beyond pruning alone

2. **Hyperparameter robustness**: Systematically vary μ (0.3-0.8), τ (20-40), and η (0.9-1.1) across all three datasets to quantify sensitivity and identify potential failure modes

3. **Cross-dataset generalization**: Apply the exact C-SAM configuration to a fourth dataset (e.g., Stanford Cars or Food-101) with different semantic properties to test whether the framework's success transfers beyond face and flower imagery