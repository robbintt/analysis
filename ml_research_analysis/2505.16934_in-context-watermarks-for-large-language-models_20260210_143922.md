---
ver: rpa2
title: In-Context Watermarks for Large Language Models
arxiv_id: '2505.16934'
source_url: https://arxiv.org/abs/2505.16934
tags:
- text
- watermarking
- arxiv
- llms
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces In-Context Watermarking (ICW), a novel method\
  \ for embedding watermarks into LLM-generated text solely through prompt engineering,\
  \ without requiring access to the model's decoding process. Four ICW strategies\
  \ are explored\u2014Unicode, Initials, Lexical, and Acrostics\u2014each with tailored\
  \ detection schemes."
---

# In-Context Watermarks for Large Language Models

## Quick Facts
- arXiv ID: 2505.16934
- Source URL: https://arxiv.org/abs/2505.16934
- Reference count: 40
- Primary result: Novel prompt-based watermarking achieves ROC-AUC up to 0.999 without model access

## Executive Summary
This paper introduces In-Context Watermarking (ICW), a novel method for embedding watermarks into LLM-generated text solely through prompt engineering, without requiring access to the model's decoding process. Four ICW strategies are explored—Unicode, Initials, Lexical, and Acrostics—each with tailored detection schemes. Experiments show ICW achieves strong detection performance (ROC-AUC up to 0.999) on advanced LLMs like GPT-o3-mini, maintains robustness under paraphrasing and editing attacks, and produces high-quality text comparable to unwatermarked outputs. The approach is effective in both Direct Text Stamp and Indirect Prompt Injection settings, demonstrating scalability for broader content attribution applications as LLMs advance.

## Method Summary
ICW embeds watermarks by instructing LLMs to follow specific generation constraints through carefully crafted prompts. The method requires only access to the LLM's API, not its internal decoding process. Four strategies are implemented: Unicode (inserting invisible characters), Initials (favoring words starting with certain letters), Lexical (using green word lists), and Acrostics (structuring sentences to spell secret keys). Detection uses statistical hypothesis testing comparing watermarked text against baseline distributions. The approach works in two settings: Direct Text Stamp (watermarking instruction in system prompt) and Indirect Prompt Injection (covertly embedding instructions in documents). Detection exploits statistical anomalies created by instruction-following, with ROC-AUC scores reaching 0.999 for capable models.

## Key Results
- ICW achieves ROC-AUC up to 0.999 on GPT-o3-mini with strong instruction-following capability
- Text quality remains comparable to unwatermarked outputs (perplexity within 5% range)
- Acrostics ICW shows highest robustness against paraphrasing attacks (ROC-AUC 0.965)
- Method scales effectively with model capability, showing performance improvements as LLMs advance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IF an LLM has sufficiently strong instruction-following capability, THEN watermarking instructions can bias output distributions to create statistically detectable patterns without modifying the decoding process.
- Mechanism: The watermarking instruction specifies constraints (e.g., prefer words starting with certain letters, structure sentences as acrostics). The LLM, following these instructions, shifts its generation to satisfy constraints, creating statistical anomalies relative to natural text. Detection exploits these shifts via hypothesis testing.
- Core assumption: LLMs can reliably follow complex, multi-constraint instructions over long outputs AND the instruction-following behavior generalizes across query types.
- Evidence anchors:
  - [abstract]: "ICW... embeds watermarks into generated text solely through prompt engineering, leveraging LLMs' in-context learning and instruction-following abilities"
  - [Section 4.2.1, Table 2]: GPT-4o-mini achieves ROC-AUC 0.572 for Initials ICW vs 0.999 for GPT-o3-mini—same method, different capability levels
  - [corpus]: Related work on "Position: LLM Watermarking Should Align Stakeholders' Incentives" notes deployment gaps from incentive misalignment; ICW addresses this by moving control to third parties
- Break condition: If LLM instruction-following reliability degrades (e.g., for complex tasks, long contexts, or adversarial queries), watermark strength drops unpredictably.

### Mechanism 2
- Claim: IF a watermark creates measurable statistical deviations from baseline text distributions, THEN a detector with knowledge of the watermarking scheme can discriminate watermarked from unwatermarked text using statistical tests.
- Mechanism: Each ICW strategy defines a detector D(y|k, τ). For Unicode ICW, D = count(invisible_unicode)/N. For Initials/Lexical ICW, D computes a z-statistic comparing green token/letter frequency against baseline. For Acrostics, D uses Levenshtein distance between sentence-initial letters and the secret key sequence.
- Core assumption: Baseline distributions (human text or unwatermarked LLM text) are well-characterized AND watermark-induced deviations exceed natural variance.
- Evidence anchors:
  - [Section 3.2.2]: "We detect the watermark by computing the z-statistic of the suspect y, i.e., D(y|kc, τc) := (|y|G − γ|y|)/√(γ(1−γ)|y|)"
  - [Section 3.2.4]: "To detect the existence of a watermark, we use the Levenshtein distance d(ℓ, ζ) to measure the closeness between ℓ and ζ"
  - [corpus]: Corpus neighbors focus on watermark detection limits but don't directly validate ICW's specific detection statistics—external validation limited
- Break condition: If paraphrasing or editing alters the specific statistical property being measured (e.g., removes Unicode characters, changes word initials), detection fails.

### Mechanism 3
- Claim: IF ICW effectiveness scales with model capability, THEN as LLMs improve in instruction-following and long-context retrieval, ICW methods become more practical AND more robust.
- Mechanism: More capable models (GPT-o3-mini vs GPT-4o-mini) better retrieve constraints from long prompts (Lexical ICW requires processing a 2,171-word green list), maintain acrostic patterns over longer outputs, and bias word choice more consistently. This creates stronger statistical signals.
- Core assumption: Model capability improvements will continue to enhance instruction-following without introducing new failure modes.
- Evidence anchors:
  - [Section 4.2.1]: "ICW effectiveness highly depends on the capabilities of the underlying LLMs and is expected to improve as models advance"
  - [Section 4.2.1, Table 2]: Lexical ICW ROC-AUC: 0.910 (GPT-4o-mini) → 0.995 (GPT-o3-mini) in DTS setting
  - [Section 5]: "As LLMs become more capable, ICWs will become correspondingly more powerful"
  - [corpus]: Weak external validation—corpus papers don't address capability scaling of prompt-based watermarking
- Break condition: If future models develop adversarial robustness features that resist instruction-based manipulation, or if capability gains plateau, ICW effectiveness may not continue improving.

## Foundational Learning

- **In-Context Learning**:
  - Why needed here: ICW entirely depends on LLMs modifying behavior based on prompt context without weight updates. Understanding that prompts can shift output distributions is foundational.
  - Quick check question: Can you explain why telling an LLM to "maximize words starting with A-M" would shift output statistics, even though the model wasn't trained for this?

- **Statistical Hypothesis Testing (z-statistics, ROC-AUC)**:
  - Why needed here: Detection uses z-scores to measure deviation from baseline distributions, and ROC-AUC to evaluate detector quality. Without this, you can't assess whether a watermark is detectable or just noise.
  - Quick check question: If green words appear in 15% of human text and 35% of watermarked text, how would increasing sample size affect detection confidence?

- **Prompt Injection Attacks**:
  - Why needed here: The IPI setting reverses the threat model of prompt injection—covertly embedding instructions in documents to trigger watermarking. Understanding injection vectors (hidden text, Unicode tricks) is essential for deployment.
  - Quick check question: How might you hide watermarking instructions in a PDF that a reviewer wouldn't notice but an LLM would process?

## Architecture Onboarding

- **Component map**: Watermarking instruction generator -> LLM (black-box) -> Detector D(y|k, τ) -> Binary decision
- **Critical path**: Instruction design → LLM instruction-following → Statistical pattern emergence → Detection test → Threshold comparison → Binary decision (watermarked/unwatermarked)
- **Design tradeoffs**:
  - Unicode ICW: Lowest LLM requirements, perfect detectability, but fragile to paraphrasing and fails for printed/scanned text
  - Initials ICW: High detectability with strong models, robust to paraphrasing, but conspicuous if adversary knows the scheme (green letters can be inferred)
  - Lexical ICW: Good robustness/quality balance, but requires long-context retrieval (2,171-word green list stresses LLM memory)
  - Acrostics ICW: Highest robustness (only needs sentence-initial letters preserved), but requires strong instruction-following for long outputs
  - [Section 3.1, Table 1]: Visual summary of tradeoffs across methods

- **Failure signatures**:
  - Low ROC-AUC with capable model → Instruction likely ignored; simplify or restructure prompt
  - Detection works but text quality drops → Constraints too restrictive; relax green list or allow skip rules
  - Paraphrasing destroys watermark → Switch from Unicode to sentence-level (Acrostics) or letter-level (Initials) schemes
  - IPI setting fails → Instruction may not be parsed from document; test injection method visibility

- **First 3 experiments**:
  1. **Baseline capability test**: Run Unicode ICW on your target LLM. If ROC-AUC < 0.95, the model lacks basic instruction-following; other methods will fail. Use: prompt from Appendix A.1, 500 samples, measure ROC-AUC.
  2. **Robustness degradation test**: Apply Acrostics ICW, then paraphrase outputs with a separate LLM. Measure ROC-AUC drop. If >20% drop, paraphrasing attacks are a threat; consider detection timing or multi-scheme combinations.
  3. **Text quality tradeoff scan**: For Lexical ICW, vary γ (proportion of green words) at 0.1, 0.2, 0.4. Measure both ROC-AUC and perplexity/LLM-judge scores. Identify the Pareto-optimal point for your use case. [Section C.3, Table 8] shows this analysis for GPT-o3-mini.

## Open Questions the Paper Calls Out

- **Token-level constraints**: Can advanced prompt engineering or alignment fine-tuning enable reliable Token-wise Lexical ICW?
- **IPI security**: How can ICW instructions be robustly obfuscated against adversarial detection and removal in the Indirect Prompt Injection setting?
- **Alignment integration**: To what extent does incorporating ICW-specific data into the model alignment phase improve performance on complex watermarking constraints?

## Limitations

- **Model capability dependence**: Effectiveness varies significantly across LLMs and depends on continued capability scaling
- **Statistical assumptions**: Detection relies on baseline distributions being well-characterized and watermark-induced deviations exceeding natural variance
- **Known-secret vulnerability**: Initials ICW is vulnerable to inversion attacks if adversaries know the green/red letter sets

## Confidence

**High Confidence** (Strong evidence, multiple independent validations):
- ICW works on current capable models (GPT-o3-mini) with near-perfect detection
- Detection statistics are mathematically sound and properly validated
- Text quality remains comparable to unwatermarked outputs when properly tuned

**Medium Confidence** (Reasonable evidence but with limitations):
- ICW effectiveness scales with model capability
- Method works across diverse query types in DTS setting
- Quality preservation is maintained across different ICW strategies

**Low Confidence** (Limited evidence or speculative claims):
- Future model improvements will continue enhancing ICW effectiveness
- IPI deployment will be practical and secure in real-world settings
- Watermark robustness against sophisticated attacks

## Next Checks

1. **Capability Transfer Test**: Implement ICW across a range of LLMs (GPT-4, Claude, Llama) and plot ROC-AUC vs. measured instruction-following capability (using standardized benchmarks). This would validate whether ICW effectiveness truly scales with capability and identify capability thresholds for practical deployment.

2. **Adversarial Attack Battery**: Design and test a comprehensive attack suite including: adversarial prompt injection to break instruction-following, multi-stage generation (outline→draft→refine) to disrupt patterns, chain-of-thought manipulation to obscure statistical signals, and model-level watermark removal techniques. Measure detection degradation for each attack type.

3. **Real-World Deployment Simulation**: Create a realistic IPI scenario using actual academic papers with embedded instructions, process them through document pipelines (PDF→text conversion, copy-paste operations), and test watermark detection across different LLMs and query types. Include adversarial attempts to detect and remove hidden instructions.