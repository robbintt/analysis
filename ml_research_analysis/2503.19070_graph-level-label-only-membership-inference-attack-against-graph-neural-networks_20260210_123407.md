---
ver: rpa2
title: Graph-Level Label-Only Membership Inference Attack against Graph Neural Networks
arxiv_id: '2503.19070'
source_url: https://arxiv.org/abs/2503.19070
tags:
- attack
- graph
- membership
- target
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first label-only membership inference
  attack (GLO-MIA) against graph neural networks in graph classification tasks. The
  attack exploits the intuition that model predictions on training data are more stable
  than on testing data by generating perturbed versions of target graphs and measuring
  prediction consistency.
---

# Graph-Level Label-Only Membership Inference Attack against Graph Neural Networks
## Quick Facts
- arXiv ID: 2503.19070
- Source URL: https://arxiv.org/abs/2503.19070
- Reference count: 26
- First label-only membership inference attack against graph neural networks achieving up to 0.825 attack accuracy

## Executive Summary
This paper introduces GLO-MIA, the first label-only membership inference attack targeting graph neural networks in graph classification tasks. Unlike previous attacks requiring confidence scores, GLO-MIA exploits the stability of model predictions on training data by generating perturbed versions of graphs and measuring prediction consistency. The attack only requires prediction labels, making it more practical and privacy-threatening since it works against systems that hide confidence scores.

The method uses a shadow model to determine optimal perturbation magnitude and threshold selection, demonstrating that small perturbations to effective graph features can reliably distinguish between training and testing data. Experiments across three datasets (DD, ENZYMES, PROTEINS_full) and four GNN architectures (GCN, GAT, GraphSAGE, GIN) show that GLO-MIA outperforms baseline label-only attacks by 8.5% and closely matches probability-based MIAs despite only using prediction labels.

## Method Summary
GLO-MIA operates by exploiting the stability difference between training and testing data predictions. The attack generates perturbed versions of target graphs by applying small random perturbations to effective features, based on the observation that training data predictions are more consistent across perturbations than testing data predictions. A shadow model is trained to determine the optimal perturbation magnitude and threshold for membership inference. The attack measures prediction consistency across multiple perturbed versions of each graph, using this consistency metric to infer whether a graph was in the target model's training set.

## Key Results
- Achieves attack accuracy up to 0.825 on graph classification tasks
- Outperforms baseline label-only attacks by 8.5% improvement
- Closely matches probability-based MIAs despite only using prediction labels
- Attack performance sensitive to perturbation magnitude and improves with more perturbed graphs

## Why This Works (Mechanism)
The attack exploits the fundamental difference in how GNNs behave on training versus testing data. Training data tends to produce more stable predictions across perturbations because the model has already seen similar patterns during training. By measuring prediction consistency across multiple perturbed versions of the same graph, GLO-MIA can distinguish between data that was used for training and data that wasn't. The shadow model approach allows automatic optimization of the perturbation parameters to maximize this distinction without requiring confidence scores.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Deep learning models designed to operate on graph-structured data, aggregating information from neighboring nodes to learn node and graph representations. Needed because the attack targets GNN vulnerabilities in graph classification tasks.
- **Membership Inference Attacks (MIAs)**: Privacy attacks that determine whether a specific data sample was part of a model's training set. Quick check: MIAs can compromise data privacy even when model outputs don't reveal sensitive information directly.
- **Label-only attacks**: A subset of MIAs that only use prediction labels rather than confidence scores or probabilities. Quick check: Label-only attacks are more practical as many systems hide confidence scores for privacy reasons.
- **Shadow models**: Auxiliary models trained to mimic the target model's behavior for attack parameter optimization. Quick check: Shadow models enable attacks to adapt to specific target model characteristics without direct access.

## Architecture Onboarding
Component map: Target GNN -> Shadow Model -> Perturbation Generator -> Consistency Measurer -> Membership Classifier
Critical path: Perturbation generation → Prediction consistency measurement → Membership inference decision
Design tradeoffs: Larger perturbations increase distinction but may break meaningful graph structure; smaller perturbations preserve structure but reduce attack effectiveness
Failure signatures: Low attack accuracy indicates either insufficient perturbation magnitude or threshold selection errors
First experiments:
1. Test prediction consistency difference between training and testing data on a simple GNN
2. Measure attack accuracy sensitivity to perturbation magnitude
3. Compare label-only attack performance against probability-based attacks

## Open Questions the Paper Calls Out
None

## Limitations
- Attack performance depends on finding the right perturbation magnitude
- Requires multiple perturbed versions of each graph, increasing computational cost
- May be less effective on GNNs with strong regularization or data augmentation

## Confidence
High: The attack is well-justified, methodologically sound, and achieves significant improvements over baselines. The results are consistent across multiple datasets and GNN architectures, with clear comparisons to existing approaches.

## Next Checks
1. Validate attack effectiveness on additional GNN architectures beyond the four tested
2. Test robustness against different types of graph perturbations (edge deletion, feature masking)
3. Evaluate transferability of the attack across different GNN architectures