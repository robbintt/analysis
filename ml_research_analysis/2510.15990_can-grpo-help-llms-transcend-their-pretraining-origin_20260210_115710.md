---
ver: rpa2
title: Can GRPO Help LLMs Transcend Their Pretraining Origin?
arxiv_id: '2510.15990'
source_url: https://arxiv.org/abs/2510.15990
tags:
- grpo
- trav
- data
- generalization
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Group Relative Policy Optimization
  (GRPO) can improve Large Language Models (LLMs) beyond their pretraining capabilities.
  The authors theoretically prove that GRPO is a conservative reweighting mechanism
  bounded by the base model's distribution, unable to discover novel solutions.
---

# Can GRPO Help LLMs Transcend Their Pretraining Origin?

## Quick Facts
- **arXiv ID**: 2510.15990
- **Source URL**: https://arxiv.org/abs/2510.15990
- **Reference count**: 24
- **Primary result**: GRPO improves OOD generalization only when target tasks align with pretrained biases; otherwise it cannot transcend pretraining origin.

## Executive Summary
This paper investigates whether Group Relative Policy Optimization (GRPO) can improve Large Language Models beyond their pretraining capabilities. Through theoretical analysis and controlled synthetic experiments, the authors demonstrate that GRPO is fundamentally a conservative reweighting mechanism bounded by the base model's distribution. They prove that if the pretrained model assigns zero probability to correct answers for a task, GRPO cannot discover those solutions. Experimental validation across four generalization axes (reasoning depth, input length, token representation, and compositional reasoning) confirms that GRPO only improves out-of-distribution performance when the target task aligns with the model's pretraining biases, and gains diminish as in-distribution tasks approach perfect performance.

## Method Summary
The authors use a controlled synthetic setup with a 4-layer Llama-style decoder (~45M parameters). They pretrain models on mixed ID/OOD data (0-50% OOD ratio), then apply supervised fine-tuning (SFT) on ID data only, followed by GRPO fine-tuning. The critical design is evaluating both ID and OOD performance after each stage to isolate GRPO's effect. Four generalization axes are tested: reasoning depth (2-step vs 3-step tasks), input length (varying sequence lengths), token representation (original vs alternative tokens), and compositional reasoning (traversal+shift vs individual operations). Performance is measured on both ID and OOD test sets to determine whether GRPO can improve OOD generalization beyond the pretraining ceiling.

## Key Results
- GRPO only improves OOD generalization when the target task aligns with pretraining distribution; without pretraining exposure, GRPO fails to improve performance even after fine-tuning
- Gains on ID tasks diminish as performance saturates near 100%
- Base model must have non-zero pretrained correct mass on a task for GRPO to provide any improvement
- Token mixing (ID prompts with OOD tokens) causes catastrophic failure that GRPO cannot recover

## Why This Works (Mechanism)

### Mechanism 1: Exponential-Tilting Reweighting
GRPO's optimal policy is a conservative reweighting of the base model, not a mechanism for discovering novel solutions. The objective `Jβ(π) = E[R(x,y)] − β·KL(π||q)` yields optimal policy `π*(y|x) ∝ q(y|x)·exp(β⁻¹R(x,y))`, which amplifies correct responses but preserves the support of `q`. If `q(y|x)=0`, then `π*(y|x)=0`.

### Mechanism 2: Pretrained Correct Mass Determines Gain Ceiling
Marginal improvement from GRPO scales linearly with the base model's existing probability mass on correct answers. `π*_β(C_x|x) − Q(x) ≤ (e^(β⁻¹)−1)·Q(x)`. In low-mass regimes, gains are bounded by the tiny initial mass; GRPO cannot bootstrap from negligible probability.

### Mechanism 3: Alignment Gap Sweet Spot
GRPO is most effective when the base model is partially aligned with the target task but not yet saturated. Marginal gains increase with Q below a threshold, then diminish toward zero as `Q→1`. Zero alignment yields zero gain; full saturation yields zero marginal improvement.

## Foundational Learning

- **Concept: KL-Regularized Policy Optimization**
  - Why needed here: GRPO's objective is fundamentally KL-constrained; understanding the tradeoff between reward maximization and distributional deviation is essential
  - Quick check question: If β increases, does the optimal policy move closer to or further from the base model `q`?

- **Concept: Out-of-Distribution (OOD) Generalization Axes**
  - Why needed here: The paper evaluates generalization across four specific axes; distinguishing these is critical for interpreting results
  - Quick check question: Which axis tests whether the model can combine known operations in novel sequences?

- **Concept: Pretrained Correct Mass Q(x)**
  - Why needed here: Q(x) is the central quantity determining GRPO's effectiveness; it quantifies how much "correctness" the base model already allocates
  - Quick check question: If Q(x)=0.001 for a task, what is the approximate maximum marginal gain GRPO can provide (assuming typical β)?

## Architecture Onboarding

- **Component map**: Pretraining corpus (67M samples) -> SFT stage (ID-only, 2K samples) -> GRPO stage (ID/OOD data, 1K samples) -> 4-layer Llama-style decoder (~45M params, RoPE, RMSNorm)

- **Critical path**:
  1. Define ID/OOD split for target generalization axis
  2. Pretrain with controlled OOD ratio (0–50%)
  3. SFT on ID-only
  4. Run GRPO with ID or OOD prompts
  5. Evaluate both ID and OOD test sets

- **Design tradeoffs**:
  - Higher OOD pretraining ratio improves OOD base performance but may reduce ID accuracy if total compute fixed
  - GRPO on OOD data can further boost OOD, but only if pretraining already established non-zero capability
  - Saturated tasks (near 100% ID accuracy) show minimal GRPO gains

- **Failure signatures**:
  - Base model achieves 0% on OOD → GRPO will not help regardless of iterations
  - Base model near 100% on ID → GRPO shows diminishing returns
  - Token mixing (e.g., ID inputs with 1–3 OOD tokens) collapses performance even after GRPO

- **First 3 experiments**:
  1. **Reasoning depth probe**: Pretrain on 1–2 step tasks (ID) with varying 3-step (OOD) ratios. Apply GRPO with 2-step ID data. Observe whether 3-step accuracy improves only when pretraining included 3-step data.
  2. **Token representation probe**: Pretrain on original token set with 0–50% alternative token set. Apply GRPO on OOD tokens. Verify that 0% alternative tokens → 0% OOD accuracy even after GRPO.
  3. **Composition probe**: Pretrain on traversal-only and shift-only tasks. Test whether GRPO enables traversal+shift composition without any composite pretraining data (expected: no improvement).

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data domain raises questions about generalizability to real-world tasks with richer reward structures
- 4-layer, ~45M parameter architecture may not scale to larger models with emergent capabilities
- Binary sequence-level correctness assumption may not hold for real-world RLHF with nuanced rewards
- SFT stage uses only ID data, potentially masking how pretraining+OOD-SFT+GRPO would perform

## Confidence

**High Confidence**:
- GRPO is fundamentally a conservative reweighting mechanism bounded by base model support
- Pretrained correct mass Q(x) determines the ceiling of possible GRPO gains
- Zero base model capability on a task implies zero GRPO improvement potential
- GRPO effectiveness depends on alignment between pretraining distribution and target task

**Medium Confidence**:
- The saturation effect where GRPO gains diminish as ID performance approaches 100%
- Token mixing failure mode being unrecoverable through GRPO
- The four-axis generalization framework effectively captures OOD challenge dimensions

**Low Confidence**:
- How these dynamics would manifest in larger models or with non-binary rewards
- Whether dense reward supervision could partially overcome the Q(x)=0 floor constraint
- The interaction effects of multiple GRPO objectives or alternative KL regularization strengths

## Next Checks
1. **Real-World Task Validation**: Replicate findings using a real reasoning dataset (e.g., GSM8K or MATH) with controlled pretraining exposure ratios to test if conservative reweighting persists with naturalistic data.

2. **Dense Reward Experiment**: Modify setup to use step-level or intermediate rewards rather than binary sequence outcomes to test whether the Q(x)=0 floor constraint is fundamental to GRPO's mechanism.

3. **Architecture Scaling Study**: Run analogous experiments on a 16-layer or 32-layer model to determine whether conservative reweighting behavior intensifies, diminishes, or remains constant as model capacity increases.