---
ver: rpa2
title: 'Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc
  Unlearning'
arxiv_id: '2506.14515'
source_url: https://arxiv.org/abs/2506.14515
tags:
- forgetting
- forget
- famr
- unlearning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Forget-Aligned Model Reconstruction (FAMR),
  a post-hoc unlearning framework that efficiently removes specific information (classes,
  samples, or styles) from trained image classifiers without full retraining. FAMR
  combines a uniform-prediction loss on the forget set with an L2 anchor penalty to
  constrain deviations from original model parameters, enabling certified forgetting
  with minimal computational overhead.
---

# Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning

## Quick Facts
- arXiv ID: 2506.14515
- Source URL: https://arxiv.org/abs/2506.14515
- Authors: Prabhav Sanga; Jaskaran Singh; Arun K. Dubey
- Reference count: 10
- Key outcome: FAMR achieves certified forgetting with minimal retraining by combining uniform-prediction loss with L2 anchoring

## Executive Summary
This paper introduces Forget-Aligned Model Reconstruction (FAMR), a post-hoc unlearning framework that efficiently removes specific information (classes, samples, or styles) from trained image classifiers without full retraining. FAMR combines a uniform-prediction loss on the forget set with an L2 anchor penalty to constrain deviations from original model parameters, enabling certified forgetting with minimal computational overhead. Theoretical analysis links FAMR to influence-function approximations and establishes bounds on parameter and output deviation. Experiments on CIFAR-100 and ImageNet-100 using Vision Transformer variants show that FAMR drives forgotten class accuracy to near-zero while preserving high performance on retained classes.

## Method Summary
FAMR implements post-hoc unlearning by optimizing the original model parameters to minimize a loss function that drives predictions toward uniform distribution on forget-set samples while constraining parameter deviation from the original model via an L2 penalty. The framework works by fine-tuning a pretrained ViT on the target dataset, then applying FAMR optimization for 10 epochs with learning rate 10⁻⁴ and anchor strength λ=0.1. The forget set can be defined by class indices, sample IDs, or style attributes, with the uniform prediction loss generalized to style forgetting using Gram matrix matching. Theoretical analysis shows FAMR approximates influence-function-based retraining, with bounds on parameter and output deviation.

## Key Results
- FAMR drives forgotten class accuracy to near-zero (0% on ViT-B/L) while maintaining high retained class accuracy
- Larger Vision Transformer models (ViT-B, ViT-L) demonstrate superior forgetting effectiveness compared to smaller variants
- Theoretical analysis establishes that FAMR's solution approximates influence-function-based retraining with bounded deviation
- Framework achieves certified forgetting with minimal computational overhead compared to full retraining

## Why This Works (Mechanism)

### Mechanism 1: Uniform-Prediction Loss for Forgetting
- Claim: Driving predictions toward uniform distribution on forget set removes learned class information
- Mechanism: Minimizes KL divergence between model output p_θ(y|x) and uniform distribution u over C classes. As loss decreases, softmax outputs approach equal probability across all classes, maximally increasing uncertainty on forgotten samples.
- Core assumption: Uniform predictions indicate genuine removal of learned representations rather than superficial output manipulation.
- Evidence anchors: [abstract]: "FAMR combines a uniform-prediction loss on the forget set with an L2 anchor penalty to constrain deviations from original model parameters"; [section 2.2.1]: "L_KL_forget(θ) = Σ_(x,y)∈T KL(u ∥ p_θ(y|x)) where u = [1/C, ..., 1/C] is the uniform distribution over C classes"
- Break condition: If intermediate-layer representations still cluster forgotten-class samples distinctly despite uniform outputs, forgetting is superficial.

### Mechanism 2: L2 Anchoring for Behavioral Preservation
- Claim: Constraining parameter deviation from original model preserves retained-class performance
- Mechanism: The λ/2 · ||θ - θ₀||² penalty adds strong convexity to the objective. At convergence, the stationarity condition ∇L_forget(θ*) + λ(θ* - θ₀) = 0 ensures forgetting gradients are exactly balanced by anchoring forces.
- Core assumption: Parameter proximity to original weights correlates with behavioral preservation on non-forgotten data.
- Evidence anchors: [abstract]: "anchoring model parameters to their original values via an ℓ2 penalty" enables "certified forgetting with minimal computational overhead"; [section 3.1]: "the anchor term λ/2||θ-θ₀||² is strongly convex, the full objective J(θ) is locally strongly convex around θ₀"
- Break condition: If retained-class accuracy drops >5% despite anchoring, λ may be underspecified relative to forgetting-loss magnitude.

### Mechanism 3: Influence-Function Approximation to Ideal Retraining
- Claim: FAMR's solution approximates weights obtained by retraining from scratch on D\T
- Mechanism: Theoretical analysis shows FAMR satisfies (H + λI)(θ* - θ₀) = -Σ∇ℓ(x,y;θ₀), with bounded deviation ||θ* - w*|| = O(λ/(λ²_min(H)) · ||∇ℓ||). As λ → 0, FAMR converges to influence-function-based retraining estimate.
- Core assumption: The Hessian H has well-conditioned eigenvalues and first-order influence approximations hold for deep networks.
- Evidence anchors: [abstract]: "Theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation"; [section 3.2]: "FAMR's update solves: (H + λI)(θ* - θ₀) = -Σ∇ℓ(x,y;θ₀)... Hence, as λ → 0, θ* → w*"
- Break condition: If Hessian has near-zero eigenvalues (common in overparameterized ViTs), the approximation bound becomes vacuous.

## Foundational Learning

- Concept: **KL Divergence and Entropy**
  - Why needed here: FAMR minimizes KL(u || p_θ) to drive uncertainty; understanding that KL is asymmetric and that uniform distributions maximize entropy is essential for interpreting results.
  - Quick check question: For a 10-class problem, if model outputs [0.1, 0.1, ..., 0.1] vs [0.91, 0.01, ..., 0.01] on a forget sample, which has higher entropy and which better satisfies FAMR's objective?

- Concept: **Strong Convexity and Convergence Rates**
  - Why needed here: The L2 anchor provides strong convexity guaranteeing unique local minimum and linear convergence; the bound η ≤ 1/(L+λ) determines stable learning rates.
  - Quick check question: If λ increases from 0.1 to 1.0, how does this affect both the maximum stable learning rate and the final distance ||θ* - θ₀||?

- Concept: **Lipschitz Continuity for Output Bounds**
  - Why needed here: The certified output divergence bound ||f_θ*(x) - f_w*(x)|| ≤ L_f · ||θ* - w*|| relies on Lipschitz assumptions to translate parameter bounds to behavioral guarantees.
  - Quick check question: Why might Lipschitz constants be difficult to estimate for Vision Transformers, and how does this affect the practicality of the certification?

## Architecture Onboarding

- Component map: Original Model (θ₀, frozen reference) -> FAMR Optimization Loop (T iterations) -> Unlearned Model θ*

- Critical path:
  1. Load pretrained ViT and store frozen reference weights θ₀
  2. Define forget set T by class index, sample IDs, or style attributes
  3. Run FAMR for 10 epochs (paper default), monitoring For-Acc and Ret-Acc
  4. Verify For-Acc approaches 0% while Ret-Acc stays within 2-4% of baseline

- Design tradeoffs:
  - **λ (anchor strength)**: Paper uses 0.1. Higher λ → better retention but slower/incomplete forgetting; lower λ → faster forgetting but risk of collateral damage.
  - **Model scale**: ViT-B/L achieve 0% For-Acc; ViT-Ti/S leave 1-2% residual. Larger models forget more completely, possibly due to redundant capacity.
  - **Learning rate η**: Must satisfy η ≤ 1/(L+λ). Paper uses 10⁻⁴. Too high causes oscillation; too low requires more epochs.

- Failure signatures:
  - Ret-Acc drops >5%: λ too small or forget set overlaps semantically with retained classes
  - For-Acc stalls at 5-10%: λ too large; try decreasing by 10×
  - High run-to-run variance: optimization instability; reduce η or increase batch size
  - Entropy plateaus below log(C): uniform loss not converging; check gradient flow on forget set

- First 3 experiments:
  1. **Single-class forgetting sanity check**: Forget class 0 on CIFAR-100 ViT-S with λ=0.1, η=10⁻⁴, 10 epochs. Target: For-Acc <2%, Ret-Acc within 3% of baseline.
  2. **λ sensitivity sweep**: Run λ ∈ {0.01, 0.05, 0.1, 0.5, 1.0} on same task. Plot For-Acc vs Ret-Acc tradeoff curve; identify Pareto-optimal region.
  3. **Architecture scaling validation**: Compare For-Acc convergence curves across ViT-Ti/S/B/L on ImageNet-100. Verify paper's claim that larger models achieve more complete forgetting (Tables 1-2 show ViT-B/L at 0% For-Acc vs 0.9-2.1% for smaller).

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- Theoretical analysis assumes well-conditioned Hessian, but deep networks often have ill-conditioned or rank-deficient Hessians
- Lack of representation-level validation leaves open possibility of "shallow forgetting" where only output logits are manipulated
- Claims about why larger models forget more effectively lack theoretical justification and may reflect capacity for superficial forgetting rather than genuine information removal

## Confidence
- **High confidence**: FAMR's empirical effectiveness in driving forgotten-class accuracy to near-zero while preserving retained-class performance
- **Medium confidence**: Theoretical link between FAMR and influence-function approximations
- **Low confidence**: Claims about why larger models forget more effectively and whether forgetting is genuine rather than superficial

## Next Checks
1. **Representation-level forgetting validation**: Apply PCA/t-SNE to intermediate-layer activations of forgotten vs. retained classes before and after FAMR. Verify that forgotten-class clusters dissolve, not just output logits become uniform.
2. **Hessian conditioning analysis**: Compute eigenvalue spectrum of the Hessian H during FAMR optimization. Quantify how many eigenvalues are near-zero and how this affects the theoretical approximation bounds.
3. **Shallow forgetting stress test**: For a forgotten class, freeze all layers except the final classifier and apply FAMR. Measure whether For-Acc still approaches zero—if yes, forgetting may be superficial; if no, deep-layer optimization is necessary for genuine forgetting.