---
ver: rpa2
title: 'Beyond Pattern Recognition: Probing Mental Representations of LMs'
arxiv_id: '2502.16717'
source_url: https://arxiv.org/abs/2502.16717
tags:
- lamps
- charlene
- quantity
- mental
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates whether language models (LMs) truly perform\
  \ dynamic, incremental reasoning or merely recognize patterns. Using the MATHWORLD\
  \ dataset, it compares traditional Chain-of-Thought (CoT) prompting against step-by-step\
  \ mental modeling approaches\u2014both text-only (TextMM) and vision-based (VisMM)."
---

# Beyond Pattern Recognition: Probing Mental Representations of LMs

## Quick Facts
- arXiv ID: 2502.16717
- Source URL: https://arxiv.org/abs/2502.16717
- Authors: Moritz Miller; Kumar Shridhar
- Reference count: 36
- Key outcome: LMs excel at pattern recognition (CoT) but struggle with dynamic mental modeling, though visual grounding and distillation can help.

## Executive Summary
This study investigates whether language models perform genuine dynamic reasoning or merely pattern recognition by comparing Chain-of-Thought (CoT) prompting against step-by-step mental modeling approaches. Using the MATHWORLD dataset of linear transfer problems, researchers found that while CoT achieves near-perfect accuracy, incremental mental modeling (TextMM) shows significant performance degradation, suggesting LMs cannot maintain evolving internal representations. Vision-based mental modeling (VisMM) outperforms text-only approaches, and interestingly, smaller multimodal models sometimes outperform larger ones due to hallucination issues. The findings raise questions about the depth of LMs' reasoning processes.

## Method Summary
The study evaluates mental modeling by presenting problems incrementally rather than as complete contexts. MATHWORLD provides 2,400 linear transfer problems (400 per depth level 1-6). Three conditions are tested: full CoT prompting, incremental TextMM updating JSON state objects, and VisMM providing prior JSON as images with new text. Models include Llama3.2-3B/11B/90B, Llama3.3-70B, and distilled variants. Inference uses vLLM at temperature=0, max_tokens=2048. Performance is measured as accuracy across depths, comparing whether models maintain correct state through incremental updates.

## Key Results
- CoT achieves near-perfect accuracy across all depths and models
- TextMM shows significant accuracy drops with increasing depth (e.g., Llama3.2-3B: 0.48 → 0.05 from depth 1 to 6)
- VisMM consistently outperforms TextMM for both Llama 11B and 90B
- Distilled models (DeepSeek-R1-Distill-Llama-70B) maintain stable accuracy across depths
- Larger multimodal models (Llama 90B) sometimes underperform smaller ones due to visual hallucination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental mental modeling exposes a fundamental gap between pattern recognition and dynamic state updating in LMs.
- Mechanism: When information arrives step-by-step, the model must maintain and modify an internal representation (JSON object) across steps. This requires holding prior state, integrating new information, and updating—not just retrieving a pattern from pre-training.
- Core assumption: Human-like reasoning involves continuously updating an internal world model, which can be approximated by sequential JSON updates.
- Evidence anchors: "they struggle significantly when required to incrementally construct and refine mental representations" (abstract); "when required to incrementally construct and refine a mental model, all models–except for the distilled Llama70B–exhibit a notable drop in accuracy" (section 4).
- Break condition: If models succeed at CoT but fail when identical information is presented incrementally, pattern-matching-on-full-context dominates over dynamic representation construction.

### Mechanism 2
- Claim: Visual grounding improves mental model maintenance by offloading state to an external, explicit representation.
- Mechanism: In VisMM, the model receives an image of the current state at each step. The image serves as an external working memory, reducing the burden on the model's internal context to hold prior quantities.
- Core assumption: Multimodal models can more reliably read state from images than maintain it in textual working memory across steps.
- Evidence anchors: "VisMM consistently outperforms TextMM for both Llama 11B and 90B" (section 4); "Llama 11B demonstrates significantly better performance with VisMM compared to Llama 90B. Manual inspection suggests that Llama 90B overfits the visual task, often hallucinating an image rather than utilizing the provided one."
- Break condition: If larger models hallucinate images (as Llama 90B does), visual grounding may not scale reliably and could introduce new failure modes.

### Mechanism 3
- Claim: Distillation from stronger reasoning models transfers mental modeling capability more effectively than scale alone.
- Mechanism: The DeepSeek-R1-Distill-Llama-70B maintains near-perfect accuracy across all depths in TextMM, unlike standard models. Distillation may encode reasoning traces that explicitly teach incremental state updating.
- Core assumption: Distillation captures not just outputs but reasoning patterns that generalize to incremental tasks.
- Evidence anchors: "Only distilled Llama 70B maintains stable accuracy across varying complexity. This suggests that model distillation from a stronger source enhances the ability to perform mental modeling" (section 4).
- Break condition: If distillation effects are task-specific and do not transfer to other incremental reasoning domains, the mechanism may be narrow pattern transfer rather than general mental modeling.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The paper compares mental modeling against CoT as a baseline. CoT prompts the model to generate intermediate steps, but this paper questions whether those steps reflect genuine reasoning.
  - Quick check question: Can you explain why CoT might produce high accuracy without genuine dynamic mental modeling?

- Concept: **Mental Models (Cognitive Science)**
  - Why needed here: The paper draws on Johnson-Laird's theory that humans build and update internal representations incrementally. Understanding this provides the theoretical motivation for the experimental design.
  - Quick check question: What is the difference between pattern recognition and mental model updating in human reasoning?

- Concept: **Multimodal Grounding**
  - Why needed here: VisMM outperforms TextMM, suggesting that visual representations help models maintain state. Understanding cross-modal alignment is essential to interpret these results.
  - Quick check question: Why might an image of a state be easier for a model to process than a textual description of the same state?

## Architecture Onboarding

- Component map: Full problem text (CoT) -> incremental sentences (TextMM/VisMM) -> JSON state object {agent, quantity, entity, attribute, unit} -> updated JSON -> final answer extraction

- Critical path:
  1. Parse initial sentence → generate JSON (Step 0)
  2. For each subsequent step: feed prior JSON + new sentence → model outputs updated JSON
  3. Extract final quantity from last JSON → compare to ground truth

- Design tradeoffs:
  - TextMM vs. VisMM: TextMM requires model to maintain state internally; VisMM offloads state to image but introduces risk of hallucination (observed in Llama 90B)
  - Model size: Larger models may overfit visual tasks (Llama 90B underperforms 11B on VisMM)
  - Distillation: Distilled models show better mental modeling but require access to strong teacher model

- Failure signatures:
  - **TextMM degradation**: Accuracy drops sharply with depth (e.g., Llama3.2-3B: 0.48 → 0.05 from depth 1 to 6)
  - **VisMM hallucination**: Larger models generate fake image URLs instead of using provided images (Llama 90B: "This is the image:" + identical URL across prompts)
  - **CoT/TextMM inconsistency**: Models may answer correctly in CoT but fail when same information arrives incrementally (e.g., Llama3.2-11B CoT: 0.90+ across depths; TextMM: 0.22 → 0.02)

- First 3 experiments:
  1. Replicate TextMM vs. VisMM comparison on a small subset of MATHWORLD to validate setup and confirm the performance gap.
  2. Test a distilled model (e.g., DeepSeek-R1-Distill-Qwen-32B) on TextMM to verify distillation effect on mental modeling accuracy.
  3. Introduce a hybrid condition where the model receives both text and image at each step to test whether cross-modal redundancy improves or degrades performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can improving the alignment between text-based (TextMM) and vision-based (VisMM) mental representations enhance TextMM performance, given that VisMM errors fully encompass TextMM errors?
- Basis in paper: The authors state this "raises important questions about the alignment of mental representations between TextMM and VisMM—specifically, whether improving this alignment could enhance TextMM's performance. We leave this exploration for future work."
- Why unresolved: The Oracle experiment showed that whenever TextMM was correct, VisMM was also correct, but the reverse did not hold, suggesting misaligned internal representations between modalities.
- What evidence would resolve it: Training interventions that explicitly align text and vision embeddings for mental modeling tasks, followed by evaluation of whether TextMM accuracy improves toward VisMM levels.

### Open Question 2
- Question: Do language models employ alternative forms of mental representation that are not captured by JSON object structures?
- Basis in paper: The limitations section states "it remains plausible that the model employs an alternative form of mental representation that is not readily interpretable."
- Why unresolved: The JSON-based evaluation may not capture how models internally represent problems; poor performance could reflect measurement limitations rather than genuine inability to form mental models.
- What evidence would resolve it: Probing studies using representation analysis techniques (e.g., probing classifiers, attention pattern analysis) to identify whether models maintain coherent internal states even when JSON outputs are incorrect.

### Open Question 3
- Question: Can mental modeling capabilities generalize beyond linear "transfer" problems to more complex reasoning domains?
- Basis in paper: The authors acknowledge their study is "inherently constrained" by focusing exclusively on transfer problems, which represent only a subset of the MATHWORLD dataset.
- Why unresolved: Transfer problems involve simple zero-sum exchanges; real-world reasoning often requires handling conditional logic, temporal constraints, or multi-agent interactions not tested here.
- What evidence would resolve it: Replicating the mental modeling paradigm on diverse reasoning tasks (e.g., commonsense reasoning, causal inference, spatial reasoning) to assess whether the TextMM-VisMM gap persists.

## Limitations

- The study uses only linear "transfer" problems, which may not generalize to more complex reasoning tasks involving multiplication, division, or non-linear relationships
- The mental modeling approach relies on specific JSON templates and formatting instructions, and different prompt engineering could potentially improve TextMM performance
- While VisMM outperforms TextMM, hallucination issues in larger models raise questions about whether VisMM truly measures improved mental modeling or shifts failure modes to visual hallucination

## Confidence

**High Confidence**: The empirical finding that CoT achieves near-perfect accuracy while TextMM shows significant degradation across depths is robust and well-supported by the data.

**Medium Confidence**: The interpretation that this performance gap indicates LMs rely on pattern recognition rather than dynamic mental modeling is reasonable but not definitively proven.

**Low Confidence**: The claim that visual grounding substantially improves mental modeling capability is tentative, given the hallucination problems observed in larger models.

## Next Checks

1. Test the same mental modeling approach on MATHWORLD problems involving multiplication, division, or compound interest calculations to validate whether the pattern recognition vs. mental modeling distinction extends beyond linear transfer problems.

2. Implement a controlled experiment where models receive both text and image representations of state at each step (redundant cross-modal input) to determine whether hallucination issues can be mitigated through redundancy.

3. Create an ablation study where the distilled model's training is modified to exclude reasoning traces to determine whether the advantage comes from reasoning pattern transfer or improved general capabilities.