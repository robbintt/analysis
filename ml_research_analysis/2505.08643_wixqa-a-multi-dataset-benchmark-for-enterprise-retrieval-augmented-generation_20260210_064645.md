---
ver: rpa2
title: 'WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation'
arxiv_id: '2505.08643'
source_url: https://arxiv.org/abs/2505.08643
tags:
- answer
- datasets
- answers
- information
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'WixQA is a multi-dataset benchmark suite for evaluating Retrieval-Augmented
  Generation (RAG) systems in enterprise settings. It includes three KB-grounded QA
  datasets derived from Wix.com customer support: WixQA-ExpertWritten (200 real user
  queries with expert-authored, multi-step answers), WixQA-Simulated (200 expert-validated
  QA pairs distilled from user dialogues), and WixQA-Synthetic (6,221 LLM-generated
  QA pairs systematically derived from Wix articles).'
---

# WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.08643
- Source URL: https://arxiv.org/abs/2505.08643
- Reference count: 40
- Primary result: A multi-dataset benchmark suite for evaluating Retrieval-Augmented Generation (RAG) systems in enterprise settings.

## Executive Summary
WixQA is a multi-dataset benchmark suite designed to evaluate Retrieval-Augmented Generation (RAG) systems in enterprise contexts. It comprises three knowledge base-grounded question-answering datasets derived from Wix.com's customer support: ExpertWritten (200 real user queries with expert-authored, multi-step answers), Simulated (200 expert-validated QA pairs distilled from user dialogues), and Synthetic (6,221 LLM-generated QA pairs systematically derived from Wix articles). The benchmark addresses the need for domain-specific, multi-document QA evaluation in enterprise environments, where users often require procedural guidance and complex information synthesis. WixQA uniquely features multi-article dependencies in ExpertWritten and Simulated datasets, requiring retrieval and synthesis from multiple sources. A comprehensive baseline evaluation using BM25 and E5 dense retrieval with multiple generation models shows that dense retrieval consistently improves context recall for complex queries, though overall performance indicates significant room for advancement in enterprise RAG systems.

## Method Summary
WixQA consists of three knowledge base-grounded QA datasets: ExpertWritten (200 real user queries with expert-authored, multi-step answers), Simulated (200 expert-validated QA pairs distilled from user dialogues), and Synthetic (6,221 LLM-generated QA pairs systematically derived from Wix articles). The benchmark also includes a 6,221-article Wix Help Center knowledge base corpus. Retrieval is performed using BM25 and E5 (e5-large-v2) retrievers with top-k=5, while generation is handled by Claude 3.7, Gemini 2.0 Flash, GPT-4o, and GPT-4o Mini. Evaluation metrics include F1, BLEU, ROUGE-1, ROUGE-2, Context Recall (LLM-judge), and Factuality (LLM-judge), with GPT-4o serving as the judge for the last two metrics. The datasets and KB are publicly available under MIT license on Hugging Face (Wix/WixQA).

## Key Results
- Dense retrieval (E5) consistently improves context recall for complex queries compared to BM25, particularly for multi-article dependencies (0.81 vs 0.73 on ExpertWritten).
- Dataset difficulty varies significantly: Synthetic dataset achieves high scores (0.95 Context Recall) while ExpertWritten (0.81) and Simulated (0.67) datasets present greater challenges.
- Overall performance indicates substantial room for advancement in enterprise RAG systems, especially for procedural answers requiring multi-document synthesis.

## Why This Works (Mechanism)
WixQA addresses the gap in enterprise RAG evaluation by providing domain-specific datasets that capture the complexity of real-world customer support scenarios. The benchmark's multi-article dependencies reflect the practical need to synthesize information across multiple knowledge base articles, a common requirement in procedural enterprise queries. By including real user queries (ExpertWritten), distilled dialogue data (Simulated), and systematically generated synthetic pairs (Synthetic), WixQA offers a comprehensive evaluation framework that spans different levels of query complexity and answer length.

## Foundational Learning
- **Enterprise RAG systems**: Why needed - Traditional RAG benchmarks focus on general knowledge, but enterprise systems require domain-specific, procedural reasoning. Quick check - Are the queries in WixQA focused on procedural guidance rather than factual questions?
- **Multi-document synthesis**: Why needed - Enterprise answers often require combining information from multiple sources. Quick check - What percentage of queries in ExpertWritten require information from multiple articles?
- **LLM-based evaluation**: Why needed - Traditional n-gram metrics may not capture semantic correctness of procedural answers. Quick check - How do LLM-judge scores compare to traditional metrics like F1 and BLEU?
- **Dense vs sparse retrieval**: Why needed - Different retrieval methods perform differently on complex, multi-hop queries. Quick check - Which retrieval method performs better on multi-article dependent queries?
- **Synthetic data generation**: Why needed - Real expert data is expensive to obtain, but synthetic data must be systematically validated. Quick check - How was the synthetic dataset validated against the knowledge base?
- **Benchmark difficulty calibration**: Why needed - Different datasets should represent varying levels of challenge for model development. Quick check - How do performance scores differ across the three datasets?

## Architecture Onboarding

**Component Map**: Knowledge Base -> Retriever (BM25/E5) -> Generator (Claude/Gemini/GPT) -> LLM Judge

**Critical Path**: Query input → Retriever selects top-k articles → Generator produces answer → LLM judge evaluates Context Recall and Factuality

**Design Tradeoffs**: 
- Multi-article dependencies increase realism but also evaluation complexity
- Synthetic data provides scale but may not capture all real-world nuances
- LLM judges offer semantic evaluation but introduce potential bias and variability

**Failure Signatures**:
- Low Context Recall on ExpertWritten/Simulated indicates difficulty with multi-article queries
- Discrepancy between n-gram metrics and Factuality scores suggests procedural answers may be semantically correct despite lexical differences

**First 3 Experiments**:
1. Run baseline evaluation using BM25 retriever on all three datasets and compare to E5 performance
2. Analyze the correlation between Context Recall scores and multi-article dependency percentage
3. Test the sensitivity of evaluation results to different LLM judges for Context Recall and Factuality

## Open Questions the Paper Calls Out
- How can retrieval architectures be optimized to better handle multi-hop dependencies in enterprise RAG, where answers require synthesizing information across distinct knowledge base articles?
- To what extent do LLM-based evaluation metrics (e.g., GPT-4o judging Factuality) align with human expert judgment when assessing long-form, procedural enterprise answers?
- What specific adaptation strategies are required to close the performance gap between high-performing synthetic data and lower-performing, expert-curated data in enterprise RAG?

## Limitations
- Small sample sizes for ExpertWritten and Simulated datasets (200 samples each) limit statistical confidence in findings
- Use of GPT-4o as LLM judge introduces potential variability and bias in evaluation metrics
- Claim about multi-article dependencies being a unique feature requires careful interpretation given modest percentages (27% in ExpertWritten, 14% in Simulated)
- Exact generation prompts for baseline models are unspecified, potentially affecting reproducibility

## Confidence
- **High confidence**: Benchmark construction methodology, dataset characteristics, and public availability are well-documented and verifiable
- **Medium confidence**: Baseline evaluation results, dependent on unspecified implementation details and single-run evaluation
- **Medium confidence**: Claim about multi-article dependencies being a unique feature, given modest percentages observed

## Next Checks
1. Re-run baseline evaluation using specified retrieval parameters (BM25 defaults, E5 e5-large-v2) and document any performance variations from reported scores
2. Conduct sensitivity analysis by testing different LLM judges for Context Recall and Factuality to quantify judge-specific bias
3. Perform statistical power analysis on ExpertWritten and Simulated datasets to determine if observed retrieval differences are significant given sample sizes