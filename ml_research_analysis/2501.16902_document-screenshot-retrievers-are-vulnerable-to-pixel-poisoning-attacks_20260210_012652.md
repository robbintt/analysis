---
ver: rpa2
title: Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks
arxiv_id: '2501.16902'
source_url: https://arxiv.org/abs/2501.16902
tags:
- attack
- queries
- document
- attacks
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces three pixel-based poisoning attack methods\u2014\
  Direct Optimisation, Noise Optimisation, and Mask Direct Optimisation\u2014for compromising\
  \ vision-language model-based document screenshot retrievers such as DSE and ColPali.\
  \ By leveraging gradients to manipulate pixel values, these methods successfully\
  \ poison retrieval rankings, achieving top-10 success rates of 41.9% for DSE and\
  \ 26.4% for ColPali on unseen in-domain queries when injecting a single adversarial\
  \ screenshot."
---

# Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks

## Quick Facts
- **arXiv ID**: 2501.16902
- **Source URL**: https://arxiv.org/abs/2501.16902
- **Reference count**: 38
- **Primary result**: Pixel poisoning attacks can successfully manipulate retrieval rankings in VLM-based document screenshot retrievers like DSE and ColPali

## Executive Summary
This paper introduces three pixel-based poisoning attack methods—Direct Optimisation, Noise Optimisation, and Mask Direct Optimisation—for compromising vision-language model-based document screenshot retrievers such as DSE and ColPali. By leveraging gradients to manipulate pixel values, these methods successfully poison retrieval rankings, achieving top-10 success rates of 41.9% for DSE and 26.4% for ColPali on unseen in-domain queries when injecting a single adversarial screenshot. Success rates increase significantly when targeting predefined query groups, reaching complete success. The attacks generalise across out-of-domain datasets, though effectiveness varies. This highlights substantial vulnerabilities in VLM-based retrievers to corpus poisoning and SEO manipulation.

## Method Summary
The paper proposes three gradient-based pixel poisoning methods for document screenshot retrievers. Direct Optimisation uses FGSM to directly modify pixel values via gradients. Noise Optimisation instead optimizes an additive noise matrix, producing less visually disruptive artifacts. Mask Direct Optimisation confines modifications to a white margin around a resized document, preserving original content visibility. All methods require white-box access to retriever weights and iteratively optimize pixel values to maximize similarity between poisoned documents and target queries.

## Key Results
- Direct Optimisation achieves 41.9% success@10 on DSE and 26.4% success@10 on ColPali for in-domain queries with single injection
- Complete success@5 (100%) achieved for predefined query groups with 2-3% mask area in Mask Direct Optimisation
- Effectiveness varies dramatically across out-of-domain datasets, ranging from 41.9% to 0.8% for DSE and 26.4% to 0.0% for ColPali

## Why This Works (Mechanism)

### Mechanism 1: Gradient-to-Pixel Direct Optimization
- Claim: Pixel values in document screenshots can be directly manipulated via gradients to maximize embedding similarity with target queries.
- Mechanism: The Direct Optimisation method computes gradients of a loss function with respect to image pixels, then iteratively updates pixels in the direction of the gradient sign (FGSM-based). Unlike text-based attacks where gradients cannot propagate through discrete tokenization, VLMs operate on continuous pixel representations enabling end-to-end gradient flow.
- Core assumption: The attacker has white-box access to retriever weights; the model's differentiable pipeline allows gradient computation back to pixel inputs.
- Evidence anchors:
  - [abstract] "By leveraging gradients to manipulate pixel values, these methods successfully poison retrieval rankings"
  - [Section 1] "the pixel values of document screenshots can be directly manipulated using gradients to deceive the model"
  - [corpus] Related work on Joint-GCG confirms gradient-based poisoning transfers to RAG systems, though specific VLM pixel attacks remain underexplored
- Break condition: If retriever is only accessible via API (no gradient access), or if input preprocessing blocks gradient flow (e.g., non-differentiable augmentation), this mechanism fails.

### Mechanism 2: Noise Pattern Additive Injection
- Claim: A learned additive noise matrix, when applied to original pixels, can increase retrieval ranking while better preserving visual fidelity.
- Mechanism: Instead of modifying the image directly, the method initializes a zero noise matrix n and optimizes it via gradient descent. The final adversarial image is x + n. This produces blur-like artifacts rather than concentrated distortions, which users may attribute to poor image quality rather than tampering.
- Core assumption: The noise pattern's magnitude can be constrained sufficiently to remain imperceptible while still achieving meaningful embedding shifts.
- Evidence anchors:
  - [Section 3.2] "Visually, this attack is less noticeable than Direct Optimisation... produces a blurry appearance, resembling poor image quality"
  - [Section 5.2] For ColPali, noise optimisation achieves only 8.92% success@10 vs 26.4% for direct optimisation
  - [corpus] Limited direct evidence for noise-based attacks on document retrievers; most corpus poisoning work focuses on text perturbations
- Break condition: If the retriever applies denoising or compression preprocessing, or if noise magnitude is clamped aggressively for fidelity, attack effectiveness degrades substantially.

### Mechanism 3: Mask-Constraint Content Preservation
- Claim: Confining pixel modifications to a mask margin preserves original content visibility while still enabling ranking manipulation.
- Mechanism: The original image is resized smaller and placed within a white margin; only margin pixels are optimized. This ensures all original information remains readable, aligning with SEO scenarios where document usability matters.
- Core assumption: Users will accept documents with visible margins/borders; the mask area provides sufficient pixel capacity for embedding manipulation.
- Evidence anchors:
  - [Section 3.3] "only the pixels within this mask margin are updated, leaving the original image completely untouched"
  - [Section 5.1] With only 2-3% mask area, complete success@5 is achieved for predefined queries
  - [corpus] Mask-based attacks are uncommon in computer vision; this paper introduces them specifically for the IR setting
- Break condition: If mask area is too small (<0.5%), gradient updates lack capacity to shift embeddings meaningfully. If defenders inspect for unnatural margins, attacks become detectable.

## Foundational Learning

- Concept: **Dense Retrieval with Embedding Similarity**
  - Why needed here: Understanding that DSE and ColPali encode documents and queries into vector embeddings, then rank by cosine similarity. Attacks work by shifting document embeddings toward query-adjacent regions.
  - Quick check question: Can you explain why modifying pixels affects ranking even though the query is text?

- Concept: **FGSM and Iterative Gradient Attacks**
  - Why needed here: All three attack methods build on Fast Gradient Sign Method principles. Understanding FGSM helps explain why small perturbations in gradient direction cause large embedding shifts.
  - Quick check question: What is the role of the sign() function in the gradient update equation?

- Concept: **Attack Effectiveness vs. Fidelity Trade-off**
  - Why needed here: The paper explicitly parameterizes this trade-off via gradient percentage p and mask area a. SEO requires high fidelity; corpus poisoning may not.
  - Quick check question: Which attack method would you choose for a black-hat SEO campaign and why?

## Architecture Onboarding

- Component map: Target Retriever (DSE/ColPali) -> Attack Generator -> Corpus Injection -> Evaluation Pipeline

- Critical path:
  1. Sample seed document (ideally low-relevance to maximize attack challenge)
  2. Select attack method and fidelity parameters (p for Direct/Noise, a for Mask)
  3. Run gradient optimization for ~3,000 steps (~10-20 min on H100)
  4. Inject adversarial document(s) into corpus
  5. Run retrieval and measure ranking position

- Design tradeoffs:
  - **DSE vs ColPali vulnerability**: DSE more vulnerable for in-domain attacks (41.9% vs 26.4% success@10); ColPali's multi-vector mechanism provides some robustness
  - **Query knowledge**: Predefined queries → complete success; in-domain unseen → partial; out-of-domain → highly variable
  - **Fidelity parameters**: Lower p/a improves visual quality but reduces effectiveness. For in-domain attacks, high effectiveness requires low fidelity

- Failure signatures:
  - Success@10 drops below 10% when: (a) out-of-domain dataset has large distribution shift (e.g., French vs English), (b) noise optimisation used on ColPali, (c) gradient percentage p < 0.5% for in-domain setting
  - Zero success when mask area a = 0% or retriever access is API-only

- First 3 experiments:
  1. **Baseline reproduction**: Run Direct Optimisation (p=100%) on DSE with Wiki-SS-NQ training queries, evaluate on test queries. Verify ~41.9% success@10
  2. **Fidelity ablation**: For predefined query group, sweep p from 0.1% to 5%. Plot success@5 vs p to identify minimal viable perturbation
  3. **Cross-model transfer**: Generate adversarial image using DSE gradients, test retrieval success on ColPali. Assess whether attacks transfer between architectures (not directly tested in paper—assumption: limited transfer expected)

## Open Questions the Paper Calls Out

- Can effective black-box attack methods be developed for document screenshot retrievers that do not require access to the model's weights?
- What defense mechanisms can effectively mitigate pixel poisoning attacks without significantly degrading the retrieval system's performance?
- Does the high success rate of multi-injection attacks (m > 1) generalize across a diverse distribution of seed documents?

## Limitations

- White-box access requirement: All methods require direct access to retriever weights for gradient computation, limiting real-world applicability
- Distribution shift robustness: Effectiveness varies dramatically across datasets (from 41.9% to 0.8% for DSE), with no systematic analysis of failure modes
- Visual fidelity validation: Limited perceptual analysis of generated artifacts; no user studies confirm undetectability of attacks

## Confidence

- **High confidence**: The fundamental vulnerability exists - VLMs operating on continuous pixel inputs are susceptible to gradient-based poisoning attacks. The mathematical framework and core methodology are sound.
- **Medium confidence**: Attack effectiveness numbers are reproducible under stated conditions (white-box access, specific datasets, targeted queries). However, real-world effectiveness likely varies significantly based on deployment specifics.
- **Low confidence**: Claims about SEO applicability and cross-dataset generalization lack sufficient empirical support. The paper presents success cases but doesn't adequately address failure modes or defensive scenarios.

## Next Checks

1. **API black-box adaptation**: Implement a query-based black-box attack that approximates gradients through limited retrieval feedback (e.g., observing ranking changes after small perturbations) and measure effectiveness compared to white-box results.

2. **Large-scale fidelity assessment**: Conduct a perceptual study with human evaluators rating adversarial document screenshots across fidelity levels (p=0.1% to 5%) to determine which perturbations remain undetectable in realistic document workflows.

3. **Cross-retriever transferability evaluation**: Systematically test whether adversarial documents generated against one retriever architecture (DSE) maintain effectiveness against different architectures (ColPali) and whether this varies by attack method (Direct vs Noise vs Mask).