---
ver: rpa2
title: Generalization of Diffusion Models Arises with a Balanced Representation Space
arxiv_id: '2512.20963'
source_url: https://arxiv.org/abs/2512.20963
tags:
- usion
- training
- samples
- data
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes memorization and generalization in diffusion
  models through the lens of representation learning. The authors prove that memorization
  occurs when network weights store individual training samples, producing spiky,
  localized representations, while generalization arises when weights capture local
  data statistics, yielding balanced, semantically rich representations.
---

# Generalization of Diffusion Models Arises with a Balanced Representation Space

## Quick Facts
- arXiv ID: 2512.20963
- Source URL: https://arxiv.org/abs/2512.20963
- Authors: Zekai Zhang; Xiao Li; Xiang Li; Lianghe Shi; Meng Wu; Molei Tao; Qing Qu
- Reference count: 40
- Primary result: Memorization in diffusion models occurs when network weights store individual training samples producing spiky, localized representations, while generalization arises when weights capture local data statistics yielding balanced, semantically rich representations.

## Executive Summary
This paper analyzes memorization and generalization in diffusion models through the lens of representation learning. The authors prove that memorization occurs when network weights store individual training samples, producing spiky, localized representations, while generalization arises when weights capture local data statistics, yielding balanced, semantically rich representations. Using a two-layer ReLU denoising autoencoder, they establish a block-wise structure for local minimizers that depends on the separability of training data clusters. They validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge. Based on these insights, they propose a representation-based method for detecting memorization (achieving high AUC and TPR) and a training-free editing technique that leverages representation steering. Their results highlight that learning good, balanced representations is central to novel and meaningful generative modeling.

## Method Summary
The authors use a two-layer ReLU denoising autoencoder as a simplified model of diffusion models to derive closed-form solutions for memorization versus generalization. They establish theoretical conditions under which memorization (storing raw training data in weights) versus generalization (capturing data statistics) occurs, based on data separability and sample size. The method involves training DAEs under controlled conditions to verify the block-wise weight structure, detecting memorization through representation spikiness (standard deviation), and editing images via representation steering by adding average concept vectors. The approach is validated on both synthetic MoG data and real diffusion models including EDM, DiT, and Stable Diffusion.

## Key Results
- Established theoretical conditions for memorization (storing individual samples in weights) versus generalization (capturing data statistics) in two-layer ReLU denoising autoencoders
- Proposed spikiness detection method achieving 90% AUC and 92% TPR on memorization detection task
- Demonstrated training-free representation steering that enables smooth editing of generalized samples while failing on memorized ones
- Validated that the theoretical block-wise weight structure persists in real-world diffusion models including EDM, DiT, and Stable Diffusion

## Why This Works (Mechanism)

### Mechanism 1: Weight-Storage Memorization via Over-parameterization
If the model is over-parameterized relative to local data density, it memorizes by storing training samples directly in the weight matrix. When training samples are locally sparse, the optimal minimizer $W^*$ aligns its columns with individual data vectors $x_i$ rather than statistical aggregates. The ReLU activation then acts as a selector. Due to the separability of data, the representation becomes nearly one-hot (spiky), firing only for the specific stored sample. This occurs when training data is $(\alpha, \beta)$-separable and weight decay $\lambda$ is small. The memorization solution ceases to be optimal if the number of samples per cluster increases significantly such that $n_k \gg p_k$.

### Mechanism 2: Statistical Generalization via Balanced Representations
If the model is under-parameterized relative to local data density, it generalizes by compressing data statistics into balanced representations. With abundant samples, the network cannot store them individually. Instead, weights form blocks $W_{X_k}$ that approximate the principal components of the cluster's Gram matrix ($X_k X_k^T$). The representation $h(x)$ spreads energy across multiple neurons (low spikiness), encoding semantic features rather than identity. This requires data clusters to be non-degenerate (sufficient samples to estimate covariance). If data duplication creates rank-1 clusters within a large dataset, the model locally reverts to memorization (Hybrid Regime).

### Mechanism 3: Representation Geometry Predicts Steerability
The standard deviation (Std) of internal representations serves as a proxy for memorization and predicts controllability. Memorized samples produce high-variance, sparse codes (spiky), while generalized samples produce low-variance, dense codes (balanced). Adding a steering vector to a balanced code creates smooth, meaningful edits. Adding it to a spiky code causes brittle, threshold-like changes or fails. This assumes the geometric properties of the two-layer ReLU DAE hold for deep models (U-Nets/DiTs). If the decoder $g_\theta$ is highly non-linear or discontinuous, linear steering in representation space may fail even for balanced codes.

## Foundational Learning

- **Concept: Denoising Autoencoder (DAE) as a Score Estimator**
  - **Why needed here:** The paper models diffusion explicitly as a two-layer ReLU DAE to derive closed-form solutions for memorization vs. generalization
  - **Quick check question:** How does minimizing the $\ell_2$ denoising loss relate to estimating the score function $\nabla \log p(x_t)$?

- **Concept: $(\alpha, \beta)$-Separability**
  - **Why needed here:** This mathematical constraint allows the authors to decouple the loss into independent "blocks" and prove that weights store data vectors
  - **Quick check question:** If cluster means are not linearly separable (high $\beta$), does the block-wise theory still strictly hold? (See Appendix A.1 for robustness)

- **Concept: Spikiness vs. Entropy in Activations**
  - **Why needed here:** This is the operational metric used to detect memorization without access to training data
  - **Quick check question:** Why does a "spiky" representation (high max, low density) imply that a specific weight column is being selected?

## Architecture Onboarding

- **Component map:** Input (Noisy sample $x_t$) -> Encoder ($W_1^T x_t$) -> Non-linearity (ReLU) -> Representation ($h(x_t) = [W_1^T x_t]_+$) -> Decoder ($W_2 h(x_t)$) -> Detection (Compute Std of $h(x_t)$)

- **Critical path:** The transition from sparse data $\to$ sparse columns in $W$ $\to$ sparse activations in $h$ $\to$ failure of steering

- **Design tradeoffs:**
  - **High Capacity ($p \gg n$):** High fidelity on specific samples (Memorization). Risk: Privacy leakage, rigidity
  - **Low Capacity ($p \ll n$):** Smooth generation and editing (Generalization). Risk: Loss of fine detail or "underfitting" rare concepts

- **Failure signatures:**
  - **High Std in $h(x)$:** Memorization detected
  - **Editing Failure:** Large changes in steering strength $a$ result in no change or sudden jumps (brittle behavior) rather than smooth transitions
  - **Jacobian Rank:** Jacobian around the sample is extremely low-rank (dominated by the sample vector)

- **First 3 experiments:**
  1. **Verify Block Storage:** Train a 2-layer ReLU DAE on 5 isolated CelebA images. Visualize $W_1$ columns to confirm they look like the raw images (Section 3.1)
  2. **Spikiness Detection:** Extract intermediate activations from a pre-trained Stable Diffusion model for known memorized prompts vs. general prompts. Plot the Std distributions (Section 4.1)
  3. **Steering Test:** Apply a "style" steering vector (e.g., oil painting) to both a memorized image and a generalized image. Observe the difference in smoothness (Section 4.2)

## Open Questions the Paper Calls Out

### Open Question 1
Do the theoretical block-wise weight structures and "spiky" representation dynamics persist and propagate similarly in deep (multi-layer) architectures or Transformer-based diffusion models? The authors focused on a simplified setting to provide foundational intuition, explicitly acknowledging that real-world deep models may require extensions beyond the two-layer ReLU setting. The proof relies on the closed-form solution of a two-layer ReLU network, which does not easily scale to the nested non-linearities of deep U-Nets or DiTs. Theoretical analysis of multi-layer DAEs or empirical layer-wise probing showing that deep layers exhibit the same separability-dependent block structure would resolve this.

### Open Question 2
How does the characterization of the denoiser solution change when the noise level $\sigma$ varies dynamically across diffusion timesteps rather than remaining fixed? The main theoretical results are derived for a fixed noise level $\sigma$, whereas standard diffusion models train and sample using a schedule of varying noise levels. While the paper empirically notes that the learned structure remains stable across timesteps, the theoretical interaction between the time-dependent noise schedule and the stability of the block-wise local minimizer is not rigorously derived. A theoretical extension proving that the block-wise structure holds uniformly across a continuous noise schedule or a bound on the perturbation of the solution as $\sigma$ changes would resolve this.

### Open Question 3
Can the theoretical bounds on cluster separability ($\alpha, \beta$) and sample size ($n_k$) be used to predict the exact onset of memorization for specific sub-populations in a hybrid regime? Corollary 3.4 characterizes a hybrid regime where memorization and generalization coexist, but the paper focuses on detecting memorization post-hoc rather than predicting it a priori from data statistics. The theoretical conditions describe the possibility of the hybrid structure but do not provide a precise quantitative threshold for when a specific data cluster transitions from generalization to memorization during training. Experiments correlating the theoretical "separability" metrics of training subsets with the observed memorization rate would validate if the theory can serve as a predictive tool for dataset curation.

## Limitations

- The empirical verification of representation steering's effectiveness on real-world diffusion models is limited, with the corpus providing insufficient evidence that two-layer ReLU DAE properties fully transfer to deep architectures
- The detection threshold (THRES) for memorization is not specified and would require calibration on held-out validation sets
- The proposed methods depend on accessing intermediate representations from pretrained models, which may not be straightforward with all model architectures

## Confidence

- **High Confidence:** The theoretical derivation of weight-storage memorization and statistical generalization regimes for the two-layer ReLU DAE (Section 3)
- **Medium Confidence:** The spikiness metric as a reliable proxy for memorization detection across different model architectures
- **Medium Confidence:** The representation steering mechanism for editing memorized samples, though empirical validation is limited
- **Low Confidence:** The complete transferability of two-layer DAE theory to deep diffusion models without additional empirical verification

## Next Checks

1. **Block Structure Verification:** Train a two-layer ReLU DAE on a controlled dataset (e.g., CelebA with exactly 5-10 images) and verify that weight columns either store individual training samples (memorization) or capture principal components of data statistics (generalization) as predicted by Corollaries 3.2 and 3.3

2. **Spikiness Distribution Analysis:** Extract intermediate representations from a pre-trained diffusion model (e.g., Stable Diffusion) for a balanced dataset containing both known memorized samples and generalized samples. Compute and compare the standard deviation distributions to validate the spikiness detection method

3. **Steering Robustness Test:** Apply the representation steering technique to both confirmed memorized and generalized samples from the same model. Measure edit smoothness by computing the Jacobian of output changes with respect to steering strength, expecting brittle behavior (low Jacobian rank) for memorized samples versus smooth, continuous changes for generalized samples