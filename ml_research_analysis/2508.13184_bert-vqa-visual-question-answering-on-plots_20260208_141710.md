---
ver: rpa2
title: 'BERT-VQA: Visual Question Answering on Plots'
arxiv_id: '2508.13184'
source_url: https://arxiv.org/abs/2508.13184
tags:
- question
- questions
- visual
- plots
- visualbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates visual question answering on plots using
  a BERT-based model. The authors compare their proposed modified VisualBERT model
  (with and without joint fusion) against a baseline LSTM + CNN + shallow classifier.
---

# BERT-VQA: Visual Question Answering on Plots

## Quick Facts
- arXiv ID: 2508.13184
- Source URL: https://arxiv.org/abs/2508.13184
- Authors: Tai Vu; Robert Yang
- Reference count: 7
- Primary result: VisualBERT-based models underperform a simple LSTM+CNN baseline on binary plot question answering

## Executive Summary
This paper investigates visual question answering on statistical plots using a BERT-based architecture. The authors compare their modified VisualBERT model (with and without joint fusion) against a baseline LSTM + CNN + shallow classifier. Both models use the same ResNet 101 image encoder and are evaluated on a subset of the PlotQA dataset containing "Yes/No" questions. The primary finding is that the VisualBERT-based models (80.70% and 58.15% accuracy) perform worse than the baseline (83.66% accuracy), contradicting the hypothesis that the cross-modality module in VisualBERT is essential for this task. The authors conclude that for plot question answering, the simpler concatenation-based approach outperforms the more sophisticated VisualBERT architecture, suggesting that advanced attention mechanisms may not be necessary for this specific multimodal task.

## Method Summary
The paper evaluates three models on a binary question subset of the PlotQA dataset. The baseline uses ResNet-101 for image encoding, GloVe embeddings with LSTM for text encoding, and simple concatenation for fusion before a shallow classifier. Two VisualBERT variants are tested: one with Faster R-CNN for region extraction and cross-attention, and another that adds joint fusion by concatenating global CNN features. All models share the same ResNet-101 backbone. The VisualBERT models were trained for 100 epochs while the baseline trained for 8 epochs. The subset contains 87,962 training questions on 10,000 images, 43,770 validation questions on 5,000 images, and 43,918 test questions on 5,000 images.

## Key Results
- Baseline (LSTM+CNN+concatenation) achieved 83.66% accuracy on binary plot questions
- Modified VisualBERT achieved 80.70% accuracy, underperforming the baseline
- Modified VisualBERT with joint fusion achieved only 58.15% accuracy
- VisualBERT's cross-attention mechanism did not improve alignment of plot components with question phrases compared to simple concatenation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple concatenation-based fusion outperforms transformer-based cross-attention for plot VQA on binary questions.
- Mechanism: The baseline extracts a single image embedding via ResNet-101 and a question embedding via LSTM (final hidden state), concatenates them, and passes through a shallow classifier. This direct fusion avoids the optimization complexity of learning attention alignments.
- Core assumption: Plot understanding for Yes/No questions may not require fine-grained token-region alignments; global image and question representations may suffice.
- Evidence anchors:
  - [abstract] "The final outcome disproved our core hypothesis that the cross-modality module in VisualBERT is essential in aligning plot components with question phrases."
  - [section 6.1] "the baseline's accuracy of 83.66%, versus Modified VisualBERT getting an accuracy of 80.70%"
  - [corpus] Weak direct corpus support; neighbor papers focus on VLM code generation and map QA rather than fusion mechanism comparisons for plot VQA.
- Break condition: If questions require multi-hop reasoning across multiple plot regions (e.g., comparing non-adjacent bars), simple concatenation may fail to capture necessary fine-grained alignments.

### Mechanism 2
- Claim: Object detection-based region features can help when plot elements are small or localized, but add optimization complexity.
- Mechanism: Faster R-CNN extracts bounding boxes from the plot; ResNet-101 encodes each region. These regional features are combined with question tokens and processed through VisualBERT's transformer layers for alignment.
- Core assumption: Plot QA requires identifying and reasoning about specific visual elements (lines, bars, legend keys), which benefits from explicit region proposals.
- Evidence anchors:
  - [section 4.2] "we implemented a model architecture that follows the VisualBERT architecture... used a pretrained Faster R-CNN to extract bounding boxes"
  - [section 6.2.1] Example 25: "the line only covers a small portion of the image, so VisualBERT with an object detector is able to extract a bounding box that contain the line"
  - [corpus] No direct corpus evidence on region-based plot VQA mechanisms.
- Break condition: If object detector fails to identify correct bounding boxes (e.g., overlapping elements, unusual plot styles), downstream reasoning degrades substantially.

### Mechanism 3
- Claim: Adding whole-image CNN features to VisualBERT outputs (joint fusion) degrades performance significantly.
- Mechanism: Concatenate global CNN features with VisualBERT's cross-modal representations before classification, aiming to provide "big picture" context.
- Core assumption: Assumption: Global image context complements region-level features for plot understanding.
- Evidence anchors:
  - [section 4.3] "we predicted that the CNN features of the entire input images would provide 'big picture' information"
  - [section 6.1] "the addition of joint fusion leads to a significant drop in the accuracy score (by about 20%)... the 'big picture' information from the entire input plots actually confuses the classifier"
  - [corpus] No corpus evidence on joint fusion for plot VQA.
- Break condition: When global and region features contain conflicting signals, the classifier cannot disambiguate which source to trust.

## Foundational Learning

- Concept: Multimodal fusion strategies (early vs. late fusion, concatenation vs. attention-based)
  - Why needed here: Understanding why simple concatenation outperformed cross-attention requires grasping trade-offs between fusion complexity and task requirements.
  - Quick check question: For a binary classification task on structured images, would you expect attention-based fusion to always help? Why or why not?

- Concept: Pretrained vision encoders and domain mismatch
  - Why needed here: ResNet-101 was pretrained on natural images (COCO, Visual Genome), not plots; this affects feature quality.
  - Quick check question: What risks arise when using a natural-image pretrained encoder for synthetic plot images?

- Concept: Object detection for structured documents
  - Why needed here: Faster R-CNN extracts regions but may not capture plot-specific structures (axes, legends, data points) reliably.
  - Quick check question: How would you evaluate whether an object detector is producing meaningful regions for a bar chart?

## Architecture Onboarding

- Component map:
  Image encoder (ResNet-101) -> Text encoder (LSTM or BERT) -> Fusion (concatenation or transformer) -> Classifier (shallow MLP)

- Critical path:
  1. Image → ResNet-101 → visual embedding(s)
  2. Question → text embedding (LSTM or BERT)
  3. Fusion (concatenation or transformer layers)
  4. Classifier → Yes/No prediction

- Design tradeoffs:
  - Baseline: Fast training, fewer parameters, but limited fine-grained alignment capability
  - VisualBERT: Slower training (~100 epochs vs 8), more parameters, enables region-text alignment but underperformed on this task
  - Joint fusion: Adds global context but introduces signal confusion

- Failure signatures:
  - VisualBERT underperforming baseline: Suggests cross-attention optimization is difficult for plot domain with limited fine-tuning data
  - Joint fusion collapse (~58% accuracy): Indicates feature conflict between global and regional signals
  - Both models struggle with dense plots (many bars/lines): Error analysis shows false positives on complex examples

- First 3 experiments:
  1. Reproduce baseline vs VisualBERT comparison on the same PlotQA subset to confirm the ~3% accuracy gap.
  2. Fine-tune ResNet-101 on plot images (not natural images) to test whether domain-specific visual features close the performance gap.
  3. Ablate the object detector: replace Faster R-CNN with grid-based features to isolate whether region extraction quality is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can feature attribution techniques explain why VisualBERT's cross-attention layers fail to align bounding regions with text phrases more effectively than simple concatenation?
- Basis in paper: [explicit] The authors state, "In the future, we might... use feature attribution techniques to investigate how the model aligns the bounding regions with text phrases via attention mechanisms and whether these alignments are reasonable."
- Why unresolved: The paper reports final accuracy metrics but lacks analysis of the internal attention states, leaving the specific failure mode of the cross-modality module opaque.
- What evidence would resolve it: Visualization of attention maps comparing successful baseline feature interaction against misaligned VisualBERT attention heads.

### Open Question 2
- Question: Does fine-tuning the ResNet-101 image encoder on plot-specific data improve the extraction of bounding boxes and visual encodings?
- Basis in paper: [explicit] The authors note their ResNet 101 was "pretrained on datasets that contain natural scenes" and suggest fine-tuning "on a plot dataset... [to] extract more accurate bounding boxes."
- Why unresolved: The experiments utilized frozen weights pretrained on natural images (COCO/Visual Genome), creating a domain mismatch with the abstract, text-heavy nature of statistical plots.
- What evidence would resolve it: Performance metrics of the VisualBERT model when the visual backbone is specifically fine-tuned on the PlotQA training set.

### Open Question 3
- Question: Can the proposed architecture be successfully adapted to handle numerical and open-vocabulary questions without losing efficiency?
- Basis in paper: [explicit] The authors identify the limitation that "findings... are strictly applicable only to the domain of answering 'Yes/No' questions" and express interest in "exploring model architectures that are able to answer diverse sets of plot-based questions."
- Why unresolved: The current study restricted the scope to binary classification; the model's ability to handle regression or generative tasks required for other question types is untested.
- What evidence would resolve it: Evaluation of the model on the full PlotQA dataset (including numerical reasoning questions) using a regression or sequence generation head.

## Limitations

- Architecture details underspecified: Critical hyperparameters (learning rates, batch sizes, optimizer choices, layer dimensions for the shallow classifier, LSTM architecture specifics) are not provided, making exact reproduction difficult.
- Domain adaptation gap unaddressed: All models use ResNet-101 pretrained on natural images rather than plots, with no exploration of whether fine-tuning the visual encoder on plot-specific data would improve performance.
- Subset sampling methodology unclear: The paper doesn't specify whether sampling was random or stratified by question type, which could affect generalization and performance comparisons.

## Confidence

- High confidence: The core finding that VisualBERT underperforms the simpler baseline (83.66% vs 80.70%) is clearly stated and directly comparable with well-defined experimental setup.
- Medium confidence: The mechanism explanation for why VisualBERT underperforms is plausible but not systematically tested, as the paper doesn't explore whether domain adaptation, object detector quality, or optimization difficulty with cross-attention is the primary bottleneck.
- Low confidence: The joint fusion failure explanation is reported but lacks ablation studies or feature analysis to determine whether the failure stems from conflicting signals, optimization issues, or architectural incompatibility.

## Next Checks

1. Reproduce baseline vs VisualBERT comparison: Implement both models with identical ResNet-101 backbones and train on the exact same balanced subset of PlotQA binary questions. Verify the ~3% accuracy gap and test whether domain-specific fine-tuning of ResNet-101 on plot images closes this gap.

2. Ablate the object detector: Replace Faster R-CNN with grid-based features (e.g., 7x7 spatial features from ResNet-101) in the VisualBERT model to isolate whether region extraction quality or cross-attention optimization is the primary bottleneck.

3. Test fine-grained alignment needs: Create a controlled experiment with questions requiring multi-hop reasoning across multiple plot regions (e.g., "Is the third bar higher than the first bar?") to determine whether the baseline's concatenation approach fails on questions needing fine-grained spatial reasoning.