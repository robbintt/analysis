---
ver: rpa2
title: 'Measuring the Measures: Discriminative Capacity of Representational Similarity
  Metrics Across Model Families'
arxiv_id: '2509.04622'
source_url: https://arxiv.org/abs/2509.04622
tags:
- metrics
- similarity
- representational
- families
- unsup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a quantitative framework to evaluate representational
  similarity metrics by measuring their ability to discriminate between different
  model families across architectures (CNNs, Vision Transformers, Swin Transformers,
  ConvNeXt) and training regimes (supervised vs. self-supervised).
---

# Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families

## Quick Facts
- **arXiv ID**: 2509.04622
- **Source URL**: https://arxiv.org/abs/2509.04622
- **Reference count**: 29
- **Primary result**: Geometry-preserving metrics like RSA achieve the strongest family separation, challenging assumptions about loose metrics capturing representational differences.

## Executive Summary
This paper introduces a quantitative framework to evaluate representational similarity metrics by measuring their ability to discriminate between different model families across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures—d-prime, silhouette coefficients, and ROC-AUC—the study systematically assesses the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes alignment, and soft matching. The results show that separability systematically increases as metrics impose more stringent alignment constraints, with RSA achieving the strongest overall separability (d-prime = 3.95, silhouette = 0.56, ROC-AUC = 0.9257), followed by soft-matching (d-prime = 3.59, silhouette = 0.30, ROC-AUC = 0.8989). This hierarchy demonstrates that geometry-preserving metrics provide the most reliable signal for distinguishing model families, challenging the assumption that looser metrics better capture representational differences.

## Method Summary
The study evaluates four representational similarity metrics (RSA, Soft Matching, Procrustes, Linear Predictivity) on 35 vision models across 6 families using activations from the penultimate layer of ImageNet-1K validation data. For each metric, pairwise similarity scores are computed across all model pairs, then evaluated using three separability measures: d-prime (signal-to-noise ratio between within-family and between-family distributions), silhouette coefficients (clustering quality), and ROC-AUC (classification accuracy). The analysis reveals a systematic hierarchy where metrics with more stringent alignment constraints (RSA requiring orthogonal transformations, Soft Matching allowing soft permutations, Procrustes enforcing global rotations, Linear Predictivity permitting arbitrary linear maps) show progressively higher discriminative capacity between model families.

## Key Results
- RSA achieves the highest separability across all three measures (d-prime = 3.95, silhouette = 0.56, ROC-AUC = 0.9257)
- Soft Matching outperforms both Procrustes (d-prime = 2.96) and Linear Predictivity (d-prime = 2.09) among mapping-based approaches
- Separability systematically increases as metrics impose more stringent alignment constraints
- Model families are generally well-separated, though CNNs and ConvNeXt show higher confusion rates

## Why This Works (Mechanism)

### Mechanism 1
Metrics imposing more stringent alignment constraints achieve systematically higher discriminability between model families. Constraints act as filters that suppress incidental variation (e.g., random weight initialization, minor hyperparameter differences) while preserving computational signatures linked to architecture and training regime. RSA's geometry-only comparison discards unit-level idiosyncrasies; SoftMatch's doubly-stochastic constraint limits matching flexibility; Procrustes enforces orthogonality; Linear Predictivity permits arbitrary linear transformations, absorbing family-specific structure.

### Mechanism 2
Relational structure comparison (RSA) provides the most reliable signal for distinguishing model families because it captures organization independent of specific features. RSA computes Representational Dissimilarity Matrices (RDMs)—pairwise dissimilarities among all stimuli—then correlates RDMs across models. This is invariant to orthogonal transformations and compares how models organize representation spaces rather than what specific units encode.

### Mechanism 3
Among mapping-based approaches, Soft Matching achieves highest separability because it relaxes rigid permutations while preserving unit-level correspondence structure. Soft Matching solves an optimal transport problem over the transportation polytope, finding soft permutation weights that match units across representations with different dimensions. This preserves sensitivity to neuron-level tuning differences (which Procrustes's global rotation and Linear Predictivity's arbitrary linear map can obscure) while allowing dimension mismatch.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**
  - Why needed here: RSA is the top-performing metric; understanding its RDM-based comparison is essential for interpreting why it achieves highest separability.
  - Quick check question: Given two model activations (1000 stimuli × 512 units), what is the shape of each RDM and what does Spearman correlation between RDMs measure?

- **Signal Detection Theory (d-prime)**
  - Why needed here: d-prime is the primary separability metric; understanding its formula is necessary to interpret discriminability results.
  - Quick check question: If within-family similarity mean=0.8 (σ=0.1) and between-family mean=0.4 (σ=0.15), compute d-prime. What does d'>2 conventionally indicate?

- **Optimal Transport / Transportation Polytope**
  - Why needed here: Soft Matching uses optimal transport for soft permutations; understanding doubly-stochastic constraints is necessary to differentiate it from rigid permutation or unconstrained linear maps.
  - Quick check question: For a 3×4 transportation matrix P, what two sets of constraints define the transportation polytope? Why does this relaxation help when comparing models with different hidden dimensions?

## Architecture Onboarding

- **Component map**: 35 vision models → activations (penultimate layer) → 4 similarity metrics → pairwise similarity matrices → 3 separability measures → ranked metric hierarchy

- **Critical path**:
  1. Extract and cache activations for all 35 models (compute-heavy, do once)
  2. Implement four similarity metrics with correct normalization (mean-centering, dimension handling)
  3. Compute pairwise similarity matrices
  4. For each family, compute intra-family and inter-family similarity distributions
  5. Calculate d-prime, silhouette, ROC-AUC from distributions

- **Design tradeoffs**:
  - Layer selection: Penultimate layer chosen for consistency; earlier layers may show different discriminability patterns (unexplored)
  - Stimulus set: ImageNet-1K validation set balances coverage and compute; smaller sets may reduce discriminability signal
  - Family granularity: ConvNeXt and Swin treated as distinct from CNNs/ViT; merging would change separability baseline

- **Failure signatures**:
  - Negative silhouette scores: Models more similar to other families than their own (possible mislabeling or convergent representations)
  - d-prime near 0: Intra- and inter-family distributions overlap substantially; metric insensitive to family differences
  - ROC-AUC near 0.5: Metric provides no discriminative signal beyond chance

- **First 3 experiments**:
  1. Reproduce the d-prime hierarchy on a subset (e.g., 5 CNNs, 5 ViTs) to validate implementation; verify RSA > SoftMatch > Procrustes > Linear Predictivity ordering.
  2. Ablate stimulus count: Compute d-prime using 10, 50, 100, 500 stimuli per class to characterize how much data separability requires.
  3. Layer-wise analysis: Extract activations from early, middle, and late layers to test whether discriminability increases, decreases, or varies non-monotonically with depth.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the high discriminability of geometry-preserving metrics like RSA mask meaningful functional convergences between distinct model families? The study equates high separability with metric quality, but it remains unclear if this sensitivity prevents the detection of functionally equivalent representations across different architectures.

- **Open Question 2**: Is the observed hierarchy of discriminative capacity consistent across early and intermediate network layers, or is it specific to the penultimate layer? Representation structures differ significantly by depth (e.g., local features vs. abstract semantics); metrics that excel at aligning high-level semantic spaces may fail on the statistical properties of early layers.

- **Open Question 3**: Do these findings generalize to model-to-brain comparisons, or are they specific to the alignment of artificial neural networks? Biological data involves noise, limited sampling, and different dimensionality constraints; it is uncertain if RSA remains superior to mapping-based methods when aligning models to noisy fMRI/MEG recordings.

## Limitations

- The analysis is restricted to penultimate layer activations, potentially missing representational dynamics at other depths
- Family labels represent coarse abstractions that may not capture meaningful computational distinctions
- Results may not generalize to generative models, multimodal systems, or language models with different representational geometries
- The study focuses on image classification models, limiting applicability to other domains

## Confidence

- **High**: The metric hierarchy (RSA > SoftMatch > Procrustes > Linear Predictivity) in terms of separability is consistently observed across all three evaluation measures and multiple family pairs. The monotonic relationship between alignment constraint stringency and discriminability appears robust within the tested domain.
- **Medium**: The interpretation that geometry-preserving metrics capture "reliable signals" for model family distinction assumes these differences reflect meaningful computational distinctions rather than arbitrary architectural conventions.
- **Low**: The claim that "looser metrics better capture representational differences" is challenged but not definitively falsified—the study shows constrained metrics achieve better family separation, but looser metrics may still capture other forms of representational variation.

## Next Checks

1. **Cross-layer validation**: Replicate the metric comparison using activations from early, middle, and late layers to determine whether the RSA dominance holds throughout the network or varies with depth.
2. **Out-of-domain testing**: Apply the same framework to a different model domain (e.g., language models or generative vision models) to test whether the constraint-separation relationship generalizes beyond supervised classification.
3. **Family granularity analysis**: Recompute separability using more granular model groupings (e.g., separating ResNet variants) to determine whether the metric hierarchy changes when family distinctions become finer-grained.