---
ver: rpa2
title: A.X K1 Technical Report
arxiv_id: '2601.09200'
source_url: https://arxiv.org/abs/2601.09200
tags:
- training
- reasoning
- data
- arxiv
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A.X K1 is a 519B-parameter Mixture-of-Experts (MoE) language model
  trained from scratch to balance high reasoning capability with practical inference
  efficiency. By leveraging scaling laws for MoE architectures and vocabulary size,
  it uses 33B active parameters and a 160K vocabulary to maximize performance under
  fixed computational constraints.
---

# A.X K1 Technical Report

## Quick Facts
- arXiv ID: 2601.09200
- Source URL: https://arxiv.org/abs/2601.09200
- Reference count: 12
- A.X K1 is a 519B-parameter MoE language model achieving 89.8% on AIME25 and 80.2% on KMMLU

## Executive Summary
A.X K1 is a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch to balance high reasoning capability with practical inference efficiency. By leveraging scaling laws for MoE architectures and vocabulary size, it uses 33B active parameters and a 160K vocabulary to maximize performance under fixed computational constraints. The model supports user-controllable switching between thinking and non-thinking modes via the Think-Fusion training recipe, enabling flexible trade-offs between reasoning depth and response latency. Evaluations show A.X K1 achieves competitive performance with leading open-source models, excelling particularly in Korean-language benchmarks (e.g., KMMLU: 80.2, CLIcK: 84.9) and high-reasoning math tasks (AIME25: 89.8). The work demonstrates the feasibility of end-to-end engineering for 500B+ parameter models and establishes a foundation for scaling toward frontier-class development.

## Method Summary
A.X K1 uses a 519B-parameter MoE architecture with 33B active parameters, 61 layers, and 160K vocabulary. The model employs Mixture-of-Experts routing with 192 routed experts per block plus 8 shared experts, using granularity G=7 for training stability. Training follows a three-stage curriculum: Stage 1 (7T tokens, stable LR) builds general knowledge; Stage 2 (1.66T tokens, decay) introduces STEM and reasoning data; Stage 3 (600B tokens) extends context from 4K to 32K. The model supports user-controllable thinking modes through Think-Fusion, which linearly interpolates specialized reasoning and instruction models, then fine-tunes on Mode-Overlap Dataset pairs to prevent mode confusion. Training uses FP8 (E4M3 forward, E5M2 backward) with selective activation recomputation and dual normalization for stability.

## Key Results
- A.X K1 achieves 89.8% on AIME25 and 80.2% on KMMLU
- Model supports user-controlled switching between thinking and non-thinking modes
- Competitively matches or exceeds performance of other open-weight models on reasoning and Korean language benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MoE scaling laws enable efficient capacity scaling under fixed compute budgets when granularity and vocabulary are jointly optimized.
- **Mechanism:** The architecture uses granularity G = (2 × d_model) / d_expert = 7, deliberately below the reported optimal range (8–12), because lower granularity improves training stability under imperfect expert load balancing. Vocabulary size (160K) is scaled 25% above compute-optimal estimates to accommodate the 10T-token training regime while maintaining hardware alignment (multiple of 128 for Tensor Core efficiency).
- **Core assumption:** Expert load balancing remains imperfect during training, making robustness more valuable than theoretical optimality.
- **Evidence anchors:**
  - [abstract] "leveraging scaling laws to optimize training configurations and vocabulary size under fixed computational budgets"
  - [section 2.2–2.3] Details granularity calculation, vocabulary scaling rationale, and references to Tian et al. (2025) and Tao et al. (2024)
  - [corpus] LLaDA-MoE and dots.llm1 similarly activate small parameter subsets from large total capacity, suggesting cross-validation of MoE efficiency principles
- **Break condition:** If expert routing achieves near-perfect load balance, higher granularity configurations may outperform; vocabulary latency overhead may dominate at smaller batch sizes.

### Mechanism 2
- **Claim:** Think-Fusion enables user-controllable mode switching by fusing specialized reasoning and instruction models, then realigning with Mode-Overlap data.
- **Mechanism:** Linear interpolation θ_init = α·θ_think + (1−α)·θ_non-think with α = 0.8 preserves reasoning priors while inheriting conversational versatility. Mode-Overlap Dataset (MOD) pairs identical prompts with both y_think (reasoning traces) and y_non-think (direct responses), training the merged model to condition behavior on control tokens (`` vs. `[:,:class:`]`) rather than defaulting to one mode.
- **Core assumption:** The parameter space can represent both reasoning-intensive and concise response distributions without catastrophic interference.
- **Evidence anchors:**
  - [abstract] "enabling user-controlled switching between thinking and non-thinking modes within a single unified model"
  - [section 4.3] Full description of Think-Fusion SFT, merging coefficient, and MOD construction
  - [corpus] Holistic Capability Preservation addresses similar multi-objective training challenges; direct validation of Think-Fusion specifically is absent
- **Break condition:** Mode confusion persists if MOD is undersampled or mixing ratio is miscalibrated; format-aware rewards in RL stage are required to enforce tag discipline.

### Mechanism 3
- **Claim:** Curriculum-based pre-training with WSD scheduling concentrates high-quality reasoning data during the decay phase for better generalization.
- **Mechanism:** Stage 1 (7T tokens, stable LR) builds general knowledge; Stage 2 (1.66T tokens, LR decay) introduces STEM, reasoning-intensive tasks, and synthetic data; Stage 3 (600B tokens) extends context progressively. Difficulty Classifier scores documents to schedule simpler samples earlier, domain classifier upsamples reasoning domains.
- **Core assumption:** High-complexity data is more valuable when the model has already acquired foundational representations.
- **Evidence anchors:**
  - [abstract] "curated by a multi-stage data processing pipeline"
  - [section 3.1–3.2] Describes data curation pipeline, curriculum learning, and WSD scheduler alignment
  - [corpus] Every FLOP Counts and TeleChat3-MoE employ multi-stage training; corpus lacks direct curriculum-learning ablations for MoE
- **Break condition:** If high-quality data is introduced too early, model may overfit to narrow distributions; if too late, compute is wasted on suboptimal representations.

## Foundational Learning

- **Mixture-of-Experts (MoE) routing and load balancing**
  - Why needed here: Understanding why granularity G=7 was chosen requires knowing how routing imbalances affect training stability.
  - Quick check question: What happens to gradient variance when one expert receives 90% of tokens?

- **Scaling laws (Chinchilla-type and extensions)**
  - Why needed here: Vocabulary size and model dimension trade-offs are derived from FLOP-optimal frontiers, not intuition.
  - Quick check question: If compute budget doubles, should you scale parameters or tokens more?

- **Reinforcement learning from human feedback (RLHF) basics**
  - Why needed here: The on-policy RL stage uses DAPO + GSPO; understanding reward shaping (R_correct + R_format) is essential.
  - Quick check question: Why would format penalties be necessary after SFT?

## Architecture Onboarding

- **Component map:** Input → Tokenizer (160K BBPE) → Embedding → 61 × [MLA (64 heads Q / 8 KV) → MoE Block (192 routed / 8 shared experts, dual RMSNorm)] → Output Head

- **Critical path:**
  1. Pre-train Stage 1 (7T tokens, 4K ctx) → Stage 2 (1.66T tokens, decay) → Stage 3 (600B tokens, 4K→32K ctx)
  2. Instruct SFT + Reasoning SFT (parallel from same pretrain checkpoint)
  3. Linear merge (α=0.8) → Think-Fusion SFT on MOD → On-policy RL (DAPO+GSPO)

- **Design tradeoffs:**
  - Granularity 7 vs. 8–12: favors stability over ideal routing efficiency
  - Vocabulary 160K vs. 132.5K optimal: improves tokenization efficiency but increases embedding memory
  - EP=8 cap: network-bound in cloud environments; higher EP yields diminishing returns
  - Selective activation recomputation (MLA up-projection, MLP only) balances memory vs. throughput

- **Failure signatures:**
  - Loss spikes early in training → resolved by dual normalization (pre/post MoE)
  - Mode confusion (malformed `` tags) → requires MOD fine-tuning and format-aware RL rewards
  - Context parallelism instability with sliding-window attention → excluded from final architecture

- **First 3 experiments:**
  1. **Dry-run with A.X K1 Light (20B-A3B):** Validate dual normalization effect on loss curve before full-scale training.
  2. **Granularity ablation (G=7 vs. G=10):** Measure training stability and expert utilization under imperfect load balancing.
  3. **Think-Fusion merge coefficient sweep (α ∈ {0.6, 0.7, 0.8, 0.9}):** Evaluate mode confusion rate and reasoning benchmark retention on a held-out MOD subset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can native multimodal capabilities be integrated into the 519B-parameter MoE architecture without degrading the specialized Korean language performance established during text-only pre-training?
- Basis in paper: [Explicit] Section 6 (Limitations) states, "A.X K1 is currently text-only... We plan to extend the model with native multimodal capabilities in future works."
- Why unresolved: Integrating vision encoders into a massive, sparse MoE often causes "catastrophic forgetting" of linguistic nuances or disrupts the expert routing balance optimized purely for text tokens.
- What evidence would resolve it: Evaluation results of a multimodal A.X K1 variant on the KMMLU and KoBALT benchmarks compared to the current text-only baseline to confirm no regression in Korean cultural/linguistic intelligence.

### Open Question 2
- Question: Is the empirically determined fusion coefficient ($\alpha=0.8$) universally optimal for hybrid reasoning models, or does it require adjustment based on the specific ratio of reasoning-to-instruction data?
- Basis in paper: [Explicit] Section 4.3 notes that the fusion recipe is "highly sensitive" and the $\alpha$ value was determined empirically to "strike a good balance" for this specific run.
- Why unresolved: The paper acknowledges that the mixing ratio varies across models and implies that the optimal $\alpha$ is currently found via "iterative train–evaluate cycles" rather than a theoretical principle.
- What evidence would resolve it: Ablation studies plotting model performance and "mode confusion" rates across a range of $\alpha$ values (e.g., 0.5 to 0.95) for models with different pre-training data mixtures.

### Open Question 3
- Question: What specific interaction between alternating sliding-window attention and large-scale context parallelism causes unreliable behavior during long-context pre-training?
- Basis in paper: [Explicit] Section 2.2 states that while sliding-window attention showed gains in small experiments, it "exhibited unreliable behavior with large-scale context parallelism" and was therefore excluded.
- Why unresolved: The authors identify the failure mode but do not isolate the root cause (e.g., specific communication overhead or gradient synchronization errors) that prevents its application in frontier-scale training.
- What evidence would resolve it: A diagnostic analysis of gradient norms and communication traces during a dry-run utilizing sliding-window attention with context parallel degrees $\geq 4$.

## Limitations

- **Load Balancing Assumptions**: The model's architecture assumes imperfect expert load balancing, justifying a granularity of 7 below the optimal 8-12 range. This assumption is supported by training stability observations but lacks quantitative validation comparing exact load distribution across different granularities.
- **Think-Fusion Generalization**: While Think-Fusion enables mode switching through linear interpolation and MOD fine-tuning, the mechanism's effectiveness depends heavily on the quality and diversity of Mode-Overlap Dataset pairs. The technical report doesn't provide ablation studies showing performance degradation when MOD quality varies.
- **Curriculum Learning Efficacy**: The three-stage curriculum with WSD scheduling claims improved reasoning generalization, but the report lacks comparative studies showing how different curriculum schedules affect final performance. The assumption that later-stage high-complexity data yields better generalization isn't empirically validated against alternative scheduling approaches.

## Confidence

- **High Confidence**: MoE scaling laws and vocabulary optimization methodology; basic Think-Fusion mechanism (linear interpolation for model merging); standard curriculum learning benefits
- **Medium Confidence**: Granularity choice of 7 being optimal for stability; Think-Fusion's ability to prevent mode confusion with current MOD approach; WSD scheduler's effectiveness in concentrating reasoning data
- **Low Confidence**: Exact reasoning performance improvements attributable to Think-Fusion vs. pretraining; long-term generalization benefits of the specific curriculum schedule; vocabulary scaling of 160K being optimal for this compute regime

## Next Checks

1. **Granularity and Load Balancing Study**: Run controlled experiments with G=7, G=10, and G=12 configurations under identical pretraining conditions, measuring both training stability metrics and final reasoning benchmark performance to quantify the stability-performance tradeoff.

2. **MOD Quality Ablation**: Create multiple versions of the Mode-Overlap Dataset with varying prompt diversity and quality levels, then measure mode confusion rates and reasoning retention across Think-Fusion models trained on each dataset variant.

3. **Curriculum Schedule Comparison**: Implement alternative curriculum schedules (e.g., reverse curriculum, mixed-difficulty throughout) and compare final performance on reasoning benchmarks to isolate the specific benefits of the proposed WSD-based approach.