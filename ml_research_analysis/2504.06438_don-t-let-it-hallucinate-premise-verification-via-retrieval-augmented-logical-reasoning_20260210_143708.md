---
ver: rpa2
title: 'Don''t Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical
  Reasoning'
arxiv_id: '2504.06438'
source_url: https://arxiv.org/abs/2504.06438
tags:
- 'false'
- premise
- logical
- query
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in large language
  models (LLMs) when responding to questions containing false premises. The authors
  propose a retrieval-augmented logical reasoning framework that transforms user queries
  into structured logical forms, uses retrieval-augmented generation (RAG) to verify
  premise validity against factual sources, and incorporates verification results
  into the LLM prompt to maintain factual consistency.
---

# Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning

## Quick Facts
- arXiv ID: 2504.06438
- Source URL: https://arxiv.org/abs/2504.06438
- Reference count: 16
- Primary result: Retrieval-augmented logical reasoning framework achieves 94.3% accuracy in hallucination mitigation by detecting and correcting false premises in user queries

## Executive Summary
This paper addresses hallucinations in large language models when responding to questions containing false premises. The authors propose a retrieval-augmented logical reasoning framework that transforms user queries into structured logical forms, uses retrieval-augmented generation (RAG) to verify premise validity against factual sources, and incorporates verification results into the LLM prompt to maintain factual consistency. Experiments on the KG-FPQ dataset demonstrate that this approach effectively reduces hallucinations, improves factual accuracy, and does not require access to model logits or large-scale fine-tuning.

## Method Summary
The proposed framework consists of three main stages: First, an LLM parses user queries into structured logical forms (predicates) to abstract away conversational elements. Second, a retriever (embedding-based, G-retriever, or LLM-based) searches a Knowledge Graph to find evidence related to the query's logical form. Third, an LLM verifier checks whether the query contradicts the retrieved evidence, and if a false premise is detected, the system explicitly instructs the LLM to acknowledge this before generating a response. The method achieves up to 94.3% accuracy in hallucination mitigation when integrated with GPT-3.5-Turbo.

## Key Results
- The framework achieves 94.3% accuracy in hallucination mitigation on KG-FPQ dataset
- Logical form extraction significantly improves retrieval F1 scores (48.78% to 73.97% for basic RAG)
- Explicit premise detection notes outperform generic warnings and reduce unnecessary cautiousness
- Multi-hop questions show dramatic improvement (48.3% to 73.3%) compared to single-hop (48.9% to 55.6%)

## Why This Works (Mechanism)

### Mechanism 1: Structured Logical Abstraction
Normalizing natural language queries into structured logical forms (predicates) appears to improve the precision of knowledge retrieval and reduces ambiguity. The system uses an LLM to parse a user query $q$ into a logical tuple $L(q) = \text{Predicate}(\text{Subject}, \text{Object})$. This strips away conversational flair, isolating the core assertion. This structured representation is then encoded to search the Knowledge Graph (KG), ensuring the retrieval step targets the specific relationship in question rather than semantic neighbors.

### Mechanism 2: Retrieval-Based Premise Verification
Validating premises against an external Knowledge Graph (KG) before generation prevents the LLM from "hallucinating along" with the user's false assumption. The system compares the logical form against a retrieved subgraph $G^*$. If the tuple in the query contradicts the facts in $G^*$ (checked via an LLM verifier), the query is flagged. This proactively intercepts errors before the generation phase begins.

### Mechanism 3: Explicit Negation Injection
Explicitly instructing the model that a premise is false is more effective for mitigation than generic warnings or relying on the model's self-correction. If a false premise is detected, the system modifies the user prompt by appending the specific constraint: *"Note: This question contains a false premise."* This steers the model's probability distribution toward rejection/correction rather than affirmation.

## Foundational Learning

**Knowledge Graph Triples (Subject, Predicate, Object)**
- Why needed here: The entire verification logic relies on mapping natural language to these structured triples. You cannot debug the "Logical Form" errors without understanding the underlying graph schema.
- Quick check question: Given the sentence "Paris is the capital of France," what are the Subject, Predicate, and Object?

**Semantic Search / Embedding Cosine Similarity**
- Why needed here: The "Embedding-based Retriever" uses cosine similarity between query embeddings and graph embeddings to find evidence. Understanding this helps explain why retrieval might fail on synonyms or rare entities.
- Quick check question: If two vectors are orthogonal (90 degrees), what is their cosine similarity, and what does that imply about their semantic relationship?

**Factuality vs. Faithfulness Hallucinations**
- Why needed here: This paper explicitly targets *factuality* (conflict with real-world facts) rather than *faithfulness* (conflict with provided context). Confusing the two leads to misapplying this architecture.
- Quick check question: If an LLM summarizes a document but invents a detail not found in the text, is this a factuality or faithfulness error?

## Architecture Onboarding

**Component map:**
Parser (GPT-4o-mini) -> Logical Form Extraction -> Retriever (Embedding/G-retriever/ToG) -> Verifier (GPT-3.5-Turbo) -> Augmentor (Premise Note) -> Generator (LLM)

**Critical path:** Query $\rightarrow$ Logical Form Extraction $\rightarrow$ Vector Search/PCST Retrieval $\rightarrow$ Verification Prompt $\rightarrow$ Final Generation

**Design tradeoffs:**
- G-retriever vs. Embedding Search: G-retriever (graph-based) performs better on multi-hop reasoning (73.3% accuracy) but is computationally heavier than simple embedding search (48.9% accuracy on multi-hop)
- Static KG vs. Web Search: The paper uses a static KG (KG-FPQ/KoPL), ensuring high precision for stable facts but lacking the real-time updates of systems like Perplexity AI

**Failure signatures:**
- Hallucination Snowballing: If the verifier fails, the generator often compounds the error (Majority Vote actually *hurt* performance in some baselines)
- Extraction Error: If the parser misidentifies the relation, the retrieval step fetches wrong evidence, leading to "correct reasoning on wrong data"

**First 3 experiments:**
1. Unit Test Parser: Pass 20 diverse queries to the Parser. Manually verify the logical form output matches the query intent.
2. Retrieval Hit Rate: Run the Retriever on a test set where ground truth triples are known. Measure Recall@1 to ensure the evidence exists before checking the verifier.
3. A/B Test Intervention: Compare "Direct Ask" vs. "Prompt + Explicit Note" on a controlled set of False Premise Questions (FPQs) to replicate the ~94% accuracy gain.

## Open Questions the Paper Calls Out

**Open Question 1:** Can logical form-based false premise detection be effectively extended to generative and open-ended question-answering tasks, beyond the discriminative Yes-No format evaluated in this study? The current evaluation is limited to discriminative tasks where answers are simply "Yes" or "No." Generative tasks require the model to produce substantive responses, which may introduce different hallucination patterns that the current detection mechanism does not address.

**Open Question 2:** How can the framework handle domains where comprehensive knowledge graphs are unavailable, or where facts require frequent real-time updates? The approach depends entirely on the existence and currency of a structured knowledge graph (KoPL subset of Wikidata). Many domains lack such resources, and static knowledge graphs become stale, limiting practical deployment.

**Open Question 3:** Why does the framework show substantially greater improvement on multi-hop questions than single-hop questions, and can this gap be reduced? Figure 4 shows single-hop accuracy improving only marginally (48.9% to 55.6%), while multi-hop improves dramatically (48.3% to 73.3%). The paper does not explain why logical form-based retrieval helps multi-hop reasoning more than simple factual lookups.

## Limitations

- KG Coverage Dependency: The system's effectiveness hinges on the completeness of the Knowledge Graph. If a false premise contains facts not represented in the KG, the retrieval step will fail to surface contradictory evidence.
- Generalization Beyond KG-FPQ: While achieving 94.3% accuracy on the KG-FPQ dataset, the paper doesn't demonstrate performance on other domains or knowledge sources.
- No Human Evaluation of Correction Quality: The paper measures whether false premises are detected, but doesn't evaluate whether the LLM's corrected responses are actually helpful or coherent.

## Confidence

**High Confidence:** The core retrieval-augmented premise verification mechanism works as described, given the experimental results on KG-FPQ and the logical soundness of the approach.

**Medium Confidence:** The claim that explicit negation injection outperforms generic warnings is well-supported, but the comparison with other sophisticated hallucination mitigation approaches is limited.

**Low Confidence:** The scalability and generalization claims beyond the art domain KG are not substantiated with cross-domain experiments or performance on open-domain datasets.

## Next Checks

1. Cross-Domain Performance Test: Evaluate the same framework on a non-art domain (e.g., science or general knowledge) using a different knowledge graph to assess whether the 94.3% accuracy is domain-specific or generalizable.

2. Ablation on G-retriever vs. Embedding Search: Systematically compare performance across different retriever types (embedding-based, G-retriever, ToG) on the same test set to quantify the computational vs. accuracy tradeoff, particularly for multi-hop reasoning questions.

3. Human Evaluation of Response Quality: Conduct a human study where evaluators rate both the correctness and helpfulness of LLM responses when false premises are detected and corrected, to ensure the system doesn't just detect errors but provides useful answers.