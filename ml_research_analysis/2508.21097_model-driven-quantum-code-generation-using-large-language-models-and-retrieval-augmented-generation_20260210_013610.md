---
ver: rpa2
title: Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented
  Generation
arxiv_id: '2508.21097'
source_url: https://arxiv.org/abs/2508.21097
tags:
- code
- quantum
- generation
- software
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes using Large Language Models (LLMs) with Retrieval-Augmented
  Generation (RAG) to generate quantum code from UML models. The key insight is that
  prompt engineering dramatically improves LLM performance: a specific, constraint-rich
  prompt increased CodeBLEU scores from 0.16 to 0.57 and Q-F-measure from 0.68 to
  0.99.'
---

# Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2508.21097
- Source URL: https://arxiv.org/abs/2508.21097
- Reference count: 40
- Primary result: Constraint-rich prompts dramatically improve LLM quantum code generation, achieving CodeBLEU 0.57 and Q-F-measure 0.99

## Executive Summary
This paper demonstrates that Large Language Models can generate executable Qiskit quantum code from UML model instances when guided by carefully engineered prompts. The key insight is that constraint-rich prompts encoding gate mapping strategies and syntax rules substantially outperform generic prompts, with CodeBLEU scores improving from 0.16 to 0.57 and Q-F-measure from 0.68 to 0.99. The approach was validated on seven UML model instances representing quantum software systems. Surprisingly, adding Retrieval-Augmented Generation with Qiskit repositories did not improve results, suggesting that repository-level context was not sufficiently relevant to the UML-to-code transformation task.

## Method Summary
The method generates Python code using IBM's Qiskit library from textual representations of UML model instances conforming to a quantum UML profile. The system uses OpenAI's GPT-4o LLM with two experimental conditions: generic prompts and constraint-rich specific prompts that incorporate gate mapping strategies and syntax constraints. A RAG pipeline retrieves code snippets from 8 public Qiskit GitHub repositories to augment the prompt. The approach is evaluated on 7 UML model instances using CodeBLEU for syntactic/semantic similarity and custom quantum-specific metrics (Q-Precision, Q-Recall, Q-F-measure) against reference implementations.

## Key Results
- Constraint-rich prompts increased CodeBLEU scores from 0.16 to 0.57 and Q-F-measure from 0.68 to 0.99
- Specific prompts achieved near-perfect quantum-specific metrics (Q-Precision, Q-Recall, Q-F-measure near 1.0)
- Adding RAG with Qiskit repositories did not improve results, suggesting insufficient relevance of retrieved context
- Generated code successfully executes quantum circuits that match reference implementations

## Why This Works (Mechanism)

### Mechanism 1
Constraint-rich prompt engineering substantially improves LLM quantum code generation quality. Specific prompts encode implementation requirements like gate mapping strategies, syntax rules, and behavioral constraints that narrow the LLM's output space toward syntactically and semantically correct Qiskit code. The constraints act as explicit guidance signals that reduce ambiguity in model-to-code translation. This works because the LLM has sufficient pre-trained knowledge of Qiskit APIs and quantum programming patterns; prompts primarily guide retrieval and composition of this knowledge rather than teaching new concepts.

### Mechanism 2
Generic repository-based RAG does not automatically improve quantum code generation from UML models. RAG effectiveness depends on semantic alignment between retrieved context and the generation task. General Qiskit repository code lacks direct correspondence to UML model structures, providing insufficient task-relevant signals to augment the LLM's existing knowledge. The retriever cannot surface code snippets that are structurally and semantically analogous to the target generation task when using generic repositories.

### Mechanism 3
UML model instances can serve as structured input specifications for LLM-based quantum code generation. Textual representations of UML models (conforming to a quantum-specific UML profile) encode structural and behavioral information that LLMs can parse and translate into executable Qiskit code. The model acts as an intermediate abstraction between requirements and implementation. This works because the UML profile and model instance format are sufficiently expressive to capture quantum circuit semantics, and the LLM can infer correct Qiskit implementations from these abstract specifications.

## Foundational Learning

- **Concept: UML Profiles for Domain-Specific Modeling**
  - **Why needed here:** The paper relies on a UML profile that extends standard UML with quantum-specific constructs. Understanding how profiles add domain stereotypes, tagged values, and constraints is essential to interpret the model instances used as LLM input.
  - **Quick check question:** Can you explain how a UML profile differs from a standard class diagram and name one quantum-specific stereotype that might appear in such a profile?

- **Concept: CodeBLEU Metric Components**
  - **Why needed here:** The paper uses CodeBLEU (not standard BLEU) to evaluate generated code. CodeBLEU combines n-gram matching, weighted n-grams, AST matching, and data-flow matchingâ€”understanding these components clarifies what "code quality"