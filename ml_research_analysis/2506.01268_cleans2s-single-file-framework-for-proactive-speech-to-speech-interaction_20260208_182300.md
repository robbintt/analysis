---
ver: rpa2
title: 'CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction'
arxiv_id: '2506.01268'
source_url: https://arxiv.org/abs/2506.01268
tags:
- interaction
- cleans2s
- user
- judgement
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CleanS2S introduces a unified single-file framework for human-like
  speech-to-speech interaction that advances conversational AI through proactive dialogue
  capabilities. The system integrates automatic speech recognition, large language
  models, and text-to-speech synthesis into a unified pipeline with real-time interruption
  handling, achieving low transition latency through full-duplex websocket connections
  and non-blocking I/O.
---

# CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction

## Quick Facts
- arXiv ID: 2506.01268
- Source URL: https://arxiv.org/abs/2506.01268
- Authors: Yudong Lu; Yazhe Niu; Shuai Hu; Haolin Wang
- Reference count: 9
- Key outcome: Unified speech-to-speech interaction framework with proactive dialogue capabilities, achieving 91% accuracy on response strategy selection

## Executive Summary
CleanS2S introduces a unified single-file framework for human-like speech-to-speech interaction that advances conversational AI through proactive dialogue capabilities. The system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I/O. The framework pioneers a proactive interaction mechanism that combines memory systems with a Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical and contextual data to inform interaction decisions, while the Action Judgement SFT module fine-tuned on collected dialogue data enables the system to make context-aware response selections.

## Method Summary
The framework implements a cascaded ASR-LLM-TTS pipeline using full-duplex websocket connections with non-blocking I/O for low-latency speech-to-speech interaction. The system employs VAD for continuous speech detection and a finite state machine tracking listening/processing/speaking phases. The core innovation is the Subjective Action Judgement module, which uses a memory system aggregating temporal signals, historical interactions, and factual anchors to inform five response strategies. Action Judgement SFT fine-tunes Llama-3.1-8B-Instruct on dialogue data collected from interviews and talk shows, annotated by pause duration and augmented with truncated speech segments as negative samples to prevent over-interruption.

## Key Results
- Action Judgement accuracy improved from 84% to 91% (Precision: 0.78, Recall: 0.83) through fine-tuning
- Framework enables five human-like response strategies: interruption, refusal, deflection, silence, and standard response
- Single-file implementation provides researchers with transparency and extensibility for interaction agents

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Full-duplex websocket architecture enables natural, interruptible speech-to-speech interaction with low transition latency.
- **Mechanism**: The system maintains bidirectional communication channels through non-blocking I/O, allowing simultaneous input monitoring and output generation. Voice Activity Detection (VAD) continuously scans for speech while a finite state machine tracks interaction phases (listening, processing, speaking). When interruption is detected—either voice-based or text-triggered—ongoing TTS playback and LLM inference threads are immediately halted, audio buffers purged, and the pipeline resets to listening state via priority queues managing concurrent tasks.
- **Core assumption**: Non-blocking I/O and lightweight threading can sufficiently minimize latency even during computationally intensive LLM operations to maintain natural conversation flow.
- **Evidence anchors**: [abstract] "achieving low transition latency through full-duplex websocket connections and non-blocking I/O"; [section 2.1.1] "To minimize latency, non-blocking I/O and lightweight threading ensure low transition latency, even during computationally intensive LLM operations."; [corpus] FLEXI benchmark (FMR 0.559) validates this architectural direction.

### Mechanism 2
- **Claim**: The Subjective Action Judgement module enables context-aware selection among five response strategies by combining memory-derived context with fine-tuned LLM decision-making.
- **Mechanism**: The memory module aggregates three information dimensions—temporal signals, historical interactions, and factual anchors—to generate structured conversational context. This context feeds into the Subjective Action Judgement module, which categorizes potential responses into three processing pathways: (1) model-dependent processing for rejection, perfunctory responses, and routine replies; (2) model-free processing for blocking; (3) special-case handling for user interruption. The module continuously assesses input streams against dialogue history to determine appropriate response timing and strategy.
- **Core assumption**: The five formalized response strategies (interruption, refusal, deflection, silence, standard response) sufficiently capture human-like conversational variance, and memory-contextualized decisions transfer effectively across conversation types.
- **Evidence anchors**: [abstract] "combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies"; [section 2.2.2] "The memory module serves as the central data hub within our system, facilitating information exchange between users and agents...integrates three critical information dimensions: temporal signals, historical interactions, and factual anchors"; [corpus] O-Mem (FMR 0.494) and Interpersonal Memory Matters (FMR 0.568) papers demonstrate related memory-driven proactive dialogue approaches.

### Mechanism 3
- **Claim**: Action Judgement SFT improves response strategy selection accuracy from 84% to 91% by training on temporally-annotated dialogue data with negative sampling.
- **Mechanism**: Training data is collected from one-on-one interviews and talk show videos, with dialogues extracted and annotated based on actual pause durations in real conversations. This enables the model to learn temporal interval assessments. Critically, truncated speech segments at arbitrary positions are introduced as negative samples to suppress indiscriminate interruption decisions. The fine-tuned LLM then generalizes judgement capabilities across diverse scenarios including boring topics, unreasonable demands, affronts, and opinion conflicts.
- **Core assumption**: Interview and talk show dialogue patterns transfer to general user-AI interactions, and pause-duration annotation captures sufficient signal for response timing decisions.
- **Evidence anchors**: [section 3.1, Table 2] Llama-3.1-8B-Instruct baseline accuracy 0.84 → 0.91 with Action Judgement SFT; Precision 0.70 → 0.78; Recall 0.70 → 0.83; [section 2.2.2] "we collect one-on-one interviews and talk show videos, extracting dialogues between participants as foundational text...annotated the data based on the actual duration of pauses in real conversations"; [corpus] Limited corpus evidence on SFT methods for proactive dialogue found in neighbor papers.

## Foundational Learning

- **Concept: Voice Activity Detection (VAD) and Full-Duplex Communication**
  - **Why needed here**: CleanS2S relies on VAD to continuously monitor speech input while simultaneously generating output. Understanding how VAD detects speech boundaries and how full-duplex websockets maintain bidirectional streams is essential for debugging interruption logic.
  - **Quick check question**: Can you explain why a half-duplex system would fail to support natural conversation interruption, and what VAD false positives would cause in this architecture?

- **Concept: Finite State Machines for Conversation Flow**
  - **Why needed here**: The system uses an FSM tracking listening→processing→speaking states with priority queues. Understanding state transitions is critical for implementing and debugging the interruption reset mechanism.
  - **Quick check question**: Draw the state transition diagram for the three interaction phases and identify where audio buffer purging occurs during an interruption event.

- **Concept: Supervised Fine-Tuning (SFT) with Negative Sampling**
  - **Why needed here**: Action Judgement SFT uses annotated dialogue data with truncated segments as negatives to prevent over-interruption. Understanding how negative samples shape decision boundaries is key to extending or modifying the training approach.
  - **Quick check question**: Why would training only on positive examples (successful interruptions) lead to poor deployment performance, and how do the arbitrary-position truncations address this?

## Architecture Onboarding

- **Component map**:
  WebSocket Receiver → ASR Model → Memory Module ↔ Chat Memory Storage → Subjective Action Judgement → Interrupt Logic ← VAD Monitor → LLM → TTS Model → WebSocket Sender; Preset Templates for Interruption

- **Critical path**:
  1. Audio input → ASR transcription (streaming)
  2. Partial/complete text → Memory module (context retrieval + temporal aggregation)
  3. Context + input → Subjective Action Judgement (strategy selection among 5 types)
  4a. If interrupt: preset template + continuation context → LLM
  4b. If standard/refusal/deflection: context + behavioral guidance → LLM
  4c. If silence: return to listening state
  4d. If blocking: access control, no model invocation
  5. LLM output → TTS → audio output (with concurrent VAD monitoring for user interruption)

- **Design tradeoffs**:
  - **Cascaded ASR-LLM-TTS vs. end-to-end SLM**: Cascaded offers modularity and component-level debugging but introduces error propagation; end-to-end (mentioned as future work) would reduce latency but sacrifice transparency.
  - **Single-file implementation**: Maximizes research transparency and reduces configuration overhead but may complicate large-scale deployment and team collaboration on independent modules.
  - **Five discrete strategies vs. continuous response space**: Formalized strategies enable clear evaluation but may oversimplify nuanced human responses; the paper does not claim completeness of this taxonomy.

- **Failure signatures**:
  - High latency (>500ms transitions): Check non-blocking I/O implementation, verify VAD response time, examine LLM inference blocking
  - Inappropriate interruption: Inspect Action Judgement model outputs, review negative sampling adequacy in training data
  - Memory context not retrieved: Verify memory module interface compliance with A-MEM/MemGPT protocols, check temporal signal integration
  - VAD false negatives causing missed interruptions: Tune VAD sensitivity thresholds, validate 16kHz audio input quality

- **First 3 experiments**:
  1. **Baseline latency profiling**: Deploy the single-file pipeline with default models (Whisper + Llama-3.1-8B + CosyVoice), measure end-to-end latency and interruption response time under varying load. Log state transitions and identify bottleneck component.
  2. **Action Judgement ablation**: Compare the fine-tuned Llama-3.1-8B-Instruct (+Action Judgement SFT) against baseline on a held-out dialogue set, measuring accuracy on each of the 5 strategy types separately to identify which strategies benefit most from SFT.
  3. **Memory integration test**: Run conversations with and without the memory module enabled, evaluating response appropriateness in multi-turn dialogues requiring historical context (e.g., referring to earlier preferences). Document cases where memory absence produces socially awkward responses.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the framework integrate end-to-end spoken language models while preserving the current system's granular control over proactive interventions?
- **Basis in paper**: [explicit] Section 3.2 suggests "integrating end-to-end spoken language models" to streamline the pipeline and reduce error propagation.
- **Why unresolved**: End-to-end models typically abstract away intermediate text states, potentially conflicting with the current architecture's reliance on modular hooks for the Subjective Action Judgement module.
- **What evidence would resolve it**: A prototype implementation using a spoken LLM that successfully replicates the "interruption" and "deflection" behaviors with comparable latency.

### Open Question 2
- **Question**: How does the system scale proactive interaction strategies to multi-speaker scenarios with dynamic turn-taking?
- **Basis in paper**: [explicit] Section 3.2 lists "expanding support for multi-speaker conversational modeling" as a future direction for facilitating speaker adaptation.
- **Why unresolved**: The current framework focuses on a single-user interaction model; handling overlapping speech or distinct user profiles within the single-file memory module remains undefined.
- **What evidence would resolve it**: Experiments demonstrating the memory module tracking distinct user histories and the judgement module resolving overlapping turn-taking conflicts.

### Open Question 3
- **Question**: Does the "Subjective Action Judgement" capability, trained on performative data, transfer effectively to natural, disfluent human speech?
- **Basis in paper**: [inferred] Section 2.2.2 describes training the SFT model using "interviews and talk show videos," which may lack the hesitations or interruptions common in casual speech.
- **Why unresolved**: The paper reports 91% accuracy on judgement tasks but does not analyze performance on "negative samples" involving real-world speech disfluencies or noise.
- **What evidence would resolve it**: Evaluation of the Action Judgement SFT on a dataset of casual, unscripted conversations containing false starts and hesitations.

### Open Question 4
- **Question**: Do the proactive response strategies actually enhance the subjective feeling of "naturalness" for human users?
- **Basis in paper**: [inferred] The Introduction claims the approach yields "more natural, human-aligned interactions," but the Results section (3.1) only validates the technical accuracy of the judgement module, not user experience.
- **Why unresolved**: High accuracy in deciding *when* to interrupt does not confirm that the user perceives the interruption as "human-like" rather than annoying or abrupt.
- **What evidence would resolve it**: A user study measuring preference and perceived naturalness between CleanS2S and a baseline reactive model in blind testing.

## Limitations
- Training methodology for Action Judgement SFT lacks critical implementation details including hyperparameters, dataset size, and complete annotation guidelines
- Five formalized response strategies may oversimplify human conversational nuance and lack validation for completeness across diverse interaction scenarios
- Single-file implementation, while promoting transparency, may not scale efficiently for production deployment

## Confidence

- **High confidence**: Full-duplex websocket architecture enabling low-latency interruption (supported by concrete technical specifications and alignment with FLEXI benchmark findings)
- **Medium confidence**: Subjective Action Judgement module effectively selecting among five response strategies (supported by accuracy metrics but limited ablation and generalization evidence)
- **Medium confidence**: Action Judgement SFT improving response accuracy from 84% to 91% (supported by Table 2 but lacks training detail reproducibility)
- **Low confidence**: Memory module meaningfully improves contextual decision-making (described functionally but insufficiently validated with empirical comparisons)

## Next Checks
1. Reproduce the Action Judgement SFT training using the paper's methodology (interview/talk show data + pause-duration annotation + truncated negatives) and measure accuracy on held-out data across all five response types individually
2. Implement an ablation study comparing the full pipeline with and without the memory module in multi-turn dialogues requiring historical context reference
3. Conduct latency profiling under realistic load conditions to verify that non-blocking I/O and websocket full-duplex communication maintain sub-300ms transition times