---
ver: rpa2
title: 'PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver
  Distraction Detection'
arxiv_id: '2508.10397'
source_url: https://arxiv.org/abs/2508.10397
tags:
- data
- driver
- pose
- image
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PQ-DAF addresses the challenge of driver distraction detection
  under data-scarce conditions by introducing a Pose-driven Quality-controlled Data
  Augmentation Framework. The method leverages a Progressive Conditional Diffusion
  Model to generate diverse, high-fidelity training samples guided by driver pose
  information extracted from DWpose.
---

# PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection

## Quick Facts
- arXiv ID: 2508.10397
- Source URL: https://arxiv.org/abs/2508.10397
- Reference count: 40
- Outperforms GuidedMixup by 12.22 percentage points on StateFarm and 5.03 percentage points on AUC-DDD datasets

## Executive Summary
PQ-DAF addresses the challenge of driver distraction detection under data-scarce conditions by introducing a Pose-driven Quality-controlled Data Augmentation Framework. The method leverages a Progressive Conditional Diffusion Model to generate diverse, high-fidelity training samples guided by driver pose information extracted from DWpose. To ensure data reliability, a CogVLM-based filtering mechanism evaluates semantic consistency of generated samples using confidence thresholds. Extensive experiments on StateFarm and AUC-DDD datasets demonstrate significant performance improvements across different few-shot settings.

## Method Summary
PQ-DAF operates through a three-stage pipeline: First, DWpose extracts driver pose information from all images in the dataset. Second, a Progressive Conditional Diffusion Model (PCDM) fine-tuned on the original training split generates synthetic images conditioned on the extracted poses. Third, a CogVLM-based filter evaluates the semantic consistency of generated samples using class-specific prompts and confidence thresholds, discarding low-quality samples before training the downstream classifier on the augmented dataset.

## Key Results
- ResNet50 accuracy increased from 36.67% to 54.00% (10-shot) and 64.67% to 88.00% (30-shot) on StateFarm
- Inceptionv4 improved from 19.33% to 34.00% on AUC-DDD dataset
- Outperforms existing augmentation methods like GuidedMixup by 12.22 percentage points on StateFarm and 5.03 percentage points on AUC-DDD

## Why This Works (Mechanism)
The framework succeeds by combining pose-guided generation with semantic filtering to create high-quality synthetic data that addresses domain shift and data scarcity. The Progressive Conditional Diffusion Model leverages pose information as structural guidance while the CogVLM filter ensures generated samples maintain semantic consistency with the target classes. This dual approach produces diverse, high-fidelity samples that improve classifier generalization without introducing harmful noise.

## Foundational Learning
- **Conditional Diffusion Models**: Generate data conditioned on auxiliary information (poses); needed for structured sample generation
- **Confidence-based Filtering**: Uses semantic consistency scores to validate synthetic data; needed to maintain data quality
- **Few-shot Learning**: Trains classifiers with limited labeled examples; needed for real-world driver distraction scenarios
- **Pose-guided Generation**: Uses skeletal information as generation conditioning; needed for realistic driver images
- **Semantic Consistency Evaluation**: Measures alignment between generated images and class descriptions; needed for reliable filtering

## Architecture Onboarding

**Component Map**: Image + Pose -> PCDMs -> Generated Samples -> CogVLM Filter -> Valid Samples -> Classifier

**Critical Path**: The filtering stage is critical - poor semantic filtering leads to degraded classifier performance regardless of generation quality

**Design Tradeoffs**: Balance between generation diversity (more synthetic samples) and quality (higher filtering thresholds). Higher thresholds improve quality but reduce sample quantity, while lower thresholds increase quantity but risk introducing noisy data

**Failure Signatures**: 
- Low sample retention rate (>90% rejection) indicates generation quality issues or overly strict thresholds
- Identity drift in generated drivers suggests imbalance in image vs pose branch weighting in guidance
- Performance degradation on lightweight models suggests synthetic domain bias exceeding model capacity

**First Experiments**:
1. Verify pose extraction quality using DWpose on sample images and render pose maps
2. Test CogVLM filtering with $\tau=0.8$ on pre-generated samples to measure retention rate
3. Train ResNet50 with 1:1 Real:Synthetic ratio and compare against baseline few-shot performance

## Open Questions the Paper Calls Out
- **Adaptation to Lightweight Models**: How to prevent performance degradation in architectures like MobileViT when increasing synthetic data ratios, given their limited capacity for handling synthetic domain bias
- **Multi-modal Scene Understanding**: Whether incorporating additional modalities like depth or scene text could improve semantic fidelity and extend applicability to complex driving scenarios beyond pose-only guidance
- **Pose Estimation Robustness**: How the pipeline performs when DWpose fails due to occlusion or poor lighting, and whether advanced pose estimators can mitigate this dependency

## Limitations
- Performance gains diminish for lightweight models (MobileViT) at higher synthetic ratios due to capacity limitations
- Framework remains dependent on pose estimation accuracy, with failures propagating through the generation pipeline
- Current implementation relies solely on pose information, potentially missing complex environmental interactions

## Confidence
- **High confidence**: Quantitative improvements on both datasets, superiority over GuidedMixup, standard few-shot experimental design
- **Medium confidence**: Effectiveness relies heavily on pose extraction quality and semantic filtering pipeline
- **Low confidence**: Exact architectural implementation details underspecified (Projection MLP, Pose Encoder, Indicator Map construction)

## Next Checks
1. Replicate 10-shot and 30-shot experiments on StateFarm and AUC-DDD with exact pose extraction and data splits
2. Implement CogVLM filtering with $\tau=0.8$ and verify retention rates match reported values (39.24% StateFarm, 42.56% AUC-DDD)
3. Compare against GuidedMixup using identical classifier training protocol and dataset conditions