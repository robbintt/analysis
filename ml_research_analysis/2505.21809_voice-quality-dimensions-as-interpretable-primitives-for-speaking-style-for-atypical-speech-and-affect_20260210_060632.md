---
ver: rpa2
title: Voice Quality Dimensions as Interpretable Primitives for Speaking Style for
  Atypical Speech and Affect
arxiv_id: '2505.21809'
source_url: https://arxiv.org/abs/2505.21809
tags:
- speech
- voice
- atypical
- dataset
- probes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops interpretable voice quality dimension (VQD)
  probes for atypical speech and affect detection. The authors train regression and
  binary classification probes on the Speech Accessibility Project dataset (11,184
  samples from 434 speakers) using frozen pre-trained embeddings as features.
---

# Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect

## Quick Facts
- arXiv ID: 2505.21809
- Source URL: https://arxiv.org/abs/2505.21809
- Reference count: 0
- Key outcome: Linear probes on frozen speech embeddings extract interpretable voice quality dimensions that transfer zero-shot to atypical speech detection and affect classification

## Executive Summary
This paper develops interpretable voice quality dimension (VQD) probes for atypical speech and affect detection. The authors train regression and binary classification probes on the Speech Accessibility Project dataset (11,184 samples from 434 speakers) using frozen pre-trained embeddings as features. Seven VQDs are modeled: intelligibility, imprecise consonants, harsh voice, naturalness, monoloudness, monopitch, and breathiness. Results show strong performance on held-out test data, with Spearman correlations ranging from 0.33 to 0.72 and AUCs from 0.68 to 0.91 across dimensions. Probes generalize well across speech elicitation categories and demonstrate zero-shot performance on out-of-domain datasets including English and Italian atypical speech, as well as affective speech from RAVDESS. The work demonstrates that VQDs serve as interpretable primitives that transfer across languages and tasks, and can explain model predictions for affect detection, revealing potential biases in affect models toward atypical speech.

## Method Summary
The method trains linear probes (Lasso regression and logistic regression) on frozen pre-trained speech embeddings to predict seven voice quality dimensions. The SAP dataset with 11,184 samples from 434 speakers across four pathologies provides training data. Silence is trimmed using wav2vec2 forced alignment before extracting embeddings from HuBERT Large, HuBERT Large ASR, RawNet3, and a proprietary CLAP model. Probes are trained with validation-set regularization selection and evaluated using Spearman correlation for regression tasks and AUC for binary classification tasks.

## Key Results
- Linear probes achieve Spearman correlations of 0.33-0.72 and AUCs of 0.68-0.91 across seven VQDs
- Probes generalize across speech categories within SAP and show zero-shot performance on external atypical speech datasets
- VQD-based probes transfer to affect detection and outperform domain-specific affect models on atypical speech
- HuBERT ASR features excel for pronunciation-related VQDs while CLAP features excel for voice-quality dimensions and cross-language transfer

## Why This Works (Mechanism)

### Mechanism 1: Linear Decodability of Voice Quality from Frozen SSL Embeddings
Pre-trained self-supervised speech embeddings linearly encode perceptual voice quality information that can be extracted without fine-tuning. Frozen HuBERT/CLAP embeddings capture acoustic properties from their pre-training objectives; linear probes project these high-dimensional representations onto interpretable VQD axes. The paper explicitly found linear probes outperformed neural network probes.

### Mechanism 2: Cross-Domain Transfer via Domain-Agnostic Acoustic Primitives
VQDs trained on atypical speech transfer zero-shot to unseen languages, pathologies, and affective speech because they capture acoustic primitives rather than domain-specific patterns. Perceptual dimensions like "harsh voice" or "breathiness" have consistent acoustic correlates across pathologies, languages, and emotional states.

### Mechanism 3: Pre-Training Objective Determines Embedding-VQD Alignment
Different pre-training objectives create embeddings with different VQD extraction strengths—ASR-tuned models excel at pronunciation-related VQDs; general audio models excel at voice-quality dimensions. Fine-tuning HuBERT for ASR emphasizes phonetic/articulatory features; CLAP's audio-caption alignment emphasizes holistic acoustic properties including tone and quality.

## Foundational Learning

- Concept: Linear Probing (Probing Classifiers)
  - Why needed here: The entire method relies on understanding that linear classifiers can extract interpretable information from frozen embeddings without modifying the backbone.
  - Quick check question: Why might a linear probe outperform a neural network probe for extracting VQD information?

- Concept: Self-Supervised Speech Representations (HuBERT, wav2vec2 family)
  - Why needed here: Understanding what acoustic/phonetic information SSL models encode determines whether they're suitable features for VQD extraction.
  - Quick check question: What type of information does a model trained via masked prediction of hidden units learn to encode?

- Concept: Perceptual Voice Quality Assessment (CAPE-V, GRBAS scales)
  - Why needed here: The VQDs are based on clinical perceptual evaluation frameworks; understanding their definition and annotation protocol is essential for interpreting results.
  - Quick check question: Why might breathiness show weaker model performance than imprecise consonants, based on how these dimensions are perceptually defined?

## Architecture Onboarding

- Component map: Raw Audio -> Silence Trimming -> Frozen Embedding Extractor (HuBERT / CLAP / RawNet3) -> Linear Probe (Lasso/LogReg) -> VQD Scores (7 dimensions)

- Critical path:
  1. Embedding extraction quality (choice of pre-trained model) most impacts downstream VQD performance
  2. Annotation quality and consistency directly bounds achievable probe performance
  3. Speech category diversity in training determines cross-category generalization

- Design tradeoffs:
  - HuBERT vs. CLAP vs. RawNet3: HuBERT-ASR best for pronunciation VQDs; CLAP best for cross-language transfer and voice-quality VQDs; RawNet3 smaller but underperforms
  - Training on all speech categories vs. single category: All-categories training improves average performance but spontaneous-speech-only training also performs well
  - Linear vs. nonlinear probes: Linear preferred for interpretability and generalization despite lower capacity

- Failure signatures:
  - Breathiness consistently weakest across all models (Spearman .23-.35) — may indicate embedding gap or annotation reliability issue
  - Probes trained only on novel sentences generalize poorly — read speech may not elicit representative voice quality
  - Affect models predict "sadness" for atypical speech — domain-specific models conflate atypicality with negative affect

- First 3 experiments:
  1. Replicate single-embedding linear probe baseline on SAP split for highest-performing VQDs (imprecise consonants, naturalness) to validate setup
  2. Ablate training speech categories: train on each category separately, evaluate on held-out categories to measure category generalization gap
  3. Zero-shot evaluation on held-out domain: apply trained probes to external atypical speech dataset (or simulate by holding out a speaker subset) to verify cross-domain transfer before attempting new datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary CLAP model prevents direct replication and independent verification of performance characteristics
- Subjective perceptual annotations for voice quality dimensions introduce inherent measurement variability
- Generalization claims based on limited out-of-domain datasets; robustness across diverse atypical speech populations remains uncertain

## Confidence
- High Confidence:
  - Linear probes outperform neural network probes for VQD extraction from frozen embeddings
  - VQD-based probes transfer zero-shot to affect detection and outperform domain-specific affect models on atypical speech
  - HuBERT ASR features perform best for pronunciation-related VQDs (intelligibility, imprecise consonants)
  - CLAP features perform best for cross-language transfer and voice-quality VQDs (harshness)

- Medium Confidence:
  - VQDs serve as domain-agnostic acoustic primitives applicable across languages and pathologies
  - Breathiness is inherently harder to model than other VQDs due to its perceptual nature
  - VQD predictions can reliably explain and audit affect model decisions

- Low Confidence:
  - Exact performance characteristics of the proprietary CLAP model
  - Generalizability to speaker populations not represented in SAP (e.g., stroke, traumatic brain injury)
  - The specific threshold values used for binary label conversion per VQD

## Next Checks
1. Conduct inter-annotator agreement analysis on a subset of SAP samples to quantify the measurement error bounds for each VQD, particularly for breathiness which shows consistently lower model performance.

2. Design a within-SAP experiment where probes trained on typical speech categories (digital commands, novel sentences) are evaluated on spontaneous speech from the same speakers, then compare to zero-shot transfer to external atypical speech datasets to isolate domain transfer effects from speaker adaptation effects.

3. Replace the proprietary CLAP model with publicly available audio representation models (e.g., YAMNet, PANNs) and evaluate whether similar cross-language transfer patterns emerge, helping to determine if CLAP's performance is model-specific or represents a broader trend in audio captioning models.