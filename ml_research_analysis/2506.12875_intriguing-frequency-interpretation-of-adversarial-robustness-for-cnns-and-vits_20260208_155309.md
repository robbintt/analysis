---
ver: rpa2
title: Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and
  ViTs
arxiv_id: '2506.12875'
source_url: https://arxiv.org/abs/2506.12875
tags:
- uni00000048
- uni00000044
- uni00000003
- uni00000055
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the frequency characteristics of adversarial
  examples in image classification tasks, focusing on how different frequency components
  affect the performance gap between adversarial and natural examples across Convolutional
  Neural Networks (ConvNets) and Vision Transformers (ViTs). The key findings reveal
  that as high-frequency components increase, the performance gap between adversarial
  and natural examples becomes more pronounced.
---

# Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and ViTs

## Quick Facts
- **arXiv ID:** 2506.12875
- **Source URL:** https://arxiv.org/abs/2506.12875
- **Reference count:** 36
- **Primary result:** Different frequency components affect adversarial vulnerability differently across CNNs and ViTs - CNNs vulnerable to mid/high frequencies while ViTs vulnerable to low/mid frequencies.

## Executive Summary
This paper investigates how frequency components of adversarial examples affect classification performance differently across Convolutional Neural Networks and Vision Transformers. The authors find that as high-frequency components increase, the performance gap between adversarial and natural examples widens significantly. Specifically, ConvNets show vulnerability to mid- and high-frequency adversarial components, while ViTs are more susceptible to low- and mid-frequency components. These findings suggest that different network architectures have distinct frequency preferences, and the differences in frequency components between adversarial and natural examples may directly influence model robustness. The study provides practical recommendations for developing frequency-based defense strategies tailored to specific architectures.

## Method Summary
The authors analyze adversarial examples by decomposing images into frequency components using 2D Discrete Fourier Transform, then applying low-pass filters with varying bandwidth to isolate frequency effects. They generate adversarial examples using standard attacks (PGD, FGSM, C&W, AutoAttack) on CIFAR-10/100 and Tiny ImageNet datasets, then evaluate classification accuracy on filtered versions across different frequency bandwidths. The methodology involves sweeping frequency scale from 0.0 to 1.4, measuring how accuracy changes for both adversarial and natural examples, and computing the performance gap between them. The approach also includes frequency swapping experiments to test causal relationships between specific frequency components and misclassification.

## Key Results
- ConvNets show peak adversarial accuracy around 40% at mid-frequency bandwidths, then decline to 0% as higher frequencies are included
- ViTs exhibit declining adversarial accuracy even at low and mid frequencies, indicating vulnerability to these frequency ranges
- The performance gap between adversarial and natural examples widens monotonically as higher frequency components are introduced
- Adversarially-trained models show more low- and mid-frequency components in their adversarial examples compared to standard models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Architecture-specific frequency sensitivity determines where adversarial perturbations are most effective
- **Mechanism:** ConvNets process information through localized convolutional kernels that naturally respond to mid- and high-frequency patterns (edges, textures). ViTs use patch-based tokenization and global self-attention, creating different inductive biases that propagate low- and mid-frequency perturbations more effectively across spatial positions
- **Core assumption:** Attack success correlates with how well perturbation frequencies align with the architecture's native signal processing characteristics
- **Evidence anchors:**
  - [abstract] "In Convolutional Neural Networks, mid- and high-frequency components of adversarial examples exhibit their attack capabilities, while in Transformers, low- and mid-frequency components of adversarial examples are particularly effective"
  - [section III-B] "For standard ConvNets, as higher frequency components are introduced, the performance of filtered adversarial examples initially increases, peaking at an accuracy around 40%, and subsequently declines to 0.0%"
  - [section III-C] "Compared to ConvNets, the performance of filtered adversarial examples on Transformers declines even at low and mid frequencies, indicating that these frequency ranges contribute to the attack utility against Transformers"
  - [corpus] Limited direct corroboration; corpus focuses on transferability and detection rather than frequency-architecture alignment
- **Break condition:** If a hybrid architecture shows vulnerability patterns that don't split along clean frequency bands, the mechanism may require refinement

### Mechanism 2
- **Claim:** The performance gap between adversarial and natural examples widens monotonically as higher frequency components are introduced
- **Mechanism:** Natural images and adversarial perturbations have fundamentally different frequency distributions. As low-pass filtering bandwidth expands, natural examples retain information critical for classification (eventually reaching clean accuracy), while adversarial examples reintroduce the perturbation frequency bands that were deliberately crafted to cause misclassification
- **Core assumption:** The frequency components where adversarial perturbations concentrate are separable from the components most important for natural classification
- **Evidence anchors:**
  - [abstract] "As the high-frequency components increase, the performance gap between adversarial and natural examples becomes increasingly pronounced"
  - [Fig. 1e] Shows performance gap starting near zero at low frequencies and expanding to the gap between generalization and robustness as frequency scale increases
  - [section III-B] "The low-frequency components of both adversarial and natural examples have a nearly identical impact on the model. However, as higher frequency components are incorporated, the performance gap between filtered adversarial and natural examples gradually increases"
  - [corpus] Wang et al. (2020) previously linked high-frequency components to CNN generalization, supporting the frequency-separability assumption
- **Break condition:** If adversarial examples crafted to concentrate perturbations primarily in low-frequency bands show reversed gap dynamics, the mechanism's generality is limited

### Mechanism 3
- **Claim:** Adversarial training redistributes model vulnerability toward lower frequencies
- **Mechanism:** Adversarial training exposes the model to perturbations during training, causing it to develop robustness in the frequency bands where standard attacks concentrate (mid-to-high). However, this creates a gradient shift—robust models become relatively more vulnerable to low- and mid-frequency perturbations because their decision boundaries have been "hardened" primarily in the high-frequency regime
- **Core assumption:** Adversarial training provides non-uniform robustness improvements across frequency bands
- **Evidence anchors:**
  - [abstract] "For adversarially-trained models, more attention should be given to improving robustness against low- and mid-frequency components in adversarial examples"
  - [Fig. 8] Visualization shows adversarial examples from robust models exhibit more low- and mid-frequency components than those from standard models
  - [section III-D] "Fig. 8(e) shows that adversarial examples generated by adversarially trained models exhibit more low- and mid-frequency components"
  - [corpus] "Defense That Attacks" paper notes robust models improve transferability of attacks—potentially consistent with shifted vulnerability patterns
- **Break condition:** If frequency analysis of attacks specifically optimized against adversarially-trained models shows they still concentrate in high frequencies, the redistribution mechanism may be an artifact

## Foundational Learning

- **Concept: Discrete Fourier Transform (DFT) for Images**
  - **Why needed here:** The entire methodology relies on decomposing images into frequency components via 2D DFT, applying low-pass filters, and reconstructing via inverse DFT to isolate frequency effects
  - **Quick check question:** If you apply a low-pass filter with bandwidth B that removes all components outside radius B/2, what happens to sharp edges in the reconstructed image?

- **Concept: Adversarial Perturbation as Constrained Optimization**
  - **Why needed here:** Understanding that adversarial examples are generated by maximizing loss within an ε-ball (ℓ∞ or ℓ2 constraint) explains why perturbations have characteristic frequency distributions—they're optimizing within pixel-space constraints that manifest in the frequency domain
  - **Quick check question:** Why would an ℓ∞-bounded perturbation (max per-pixel change) tend to produce different frequency characteristics than an ℓ2-bounded perturbation (Euclidean distance)?

- **Concept: CNN vs. ViT Inductive Biases**
  - **Why needed here:** The paper's central claim hinges on architectural differences. CNNs have translation equivariance and local connectivity (favoring texture/high-freq). ViTs have global attention and patch-based processing (enabling long-range dependencies that may favor lower frequencies)
  - **Quick check question:** How does patch size in a ViT affect the minimum spatial frequency that can be represented in a single token?

## Architecture Onboarding

- **Component map:** Load image → Apply 2D DFT → Create low-pass filter mask → Apply mask to frequency domain → Apply inverse DFT → Evaluate on model
- **Critical path:** Bandwidth sweep (B/M from 0.0 to ~1.4) → for each B, filter all test images → evaluate accuracy → plot accuracy curves for adversarial vs. natural examples. The key metric is the gap between these curves as B increases
- **Design tradeoffs:** Filter shape: Paper uses hard binary cutoff. Alternative: Gaussian taper reduces ringing artifacts but introduces gradual transitions that complicate interpretation. Dataset resolution: CIFAR-10 (32×32) vs. Tiny ImageNet (64×64) have different Nyquist frequencies; direct comparison of "B/M" values across datasets requires care. Attack strength: ε = 8/255 for ℓ∞ is standard, but stronger attacks may saturate frequency response differently
- **Failure signatures:** If accuracy curves for adversarial and natural examples are nearly identical across all B → perturbations may be concentrated in frequencies outside tested range, or model has uniform frequency sensitivity. If performance gap is largest at low frequencies → check for implementation error in filter application. If curves are non-monotonic with multiple peaks → may indicate frequency-specific features critical for particular classes
- **First 3 experiments:**
  1. Baseline reproduction: Implement low-pass filtering pipeline on CIFAR-10 with ResNet-18. Generate PGD adversarial examples, sweep B/M from 0.0 to 1.4, plot accuracy vs. B for both adversarial and natural examples. Expect peak adversarial accuracy ~40% at mid-B, declining to 0% at B/M = 1.0
  2. Architecture validation: Repeat with ViT-B/16 on same data. Expect adversarial accuracy decline starting at lower B values (low/mid-frequency vulnerability). Compare gap curve slopes
  3. Frequency swap sanity check: For merged adversarial examples (adversarial frequencies within B, natural frequencies outside B), verify that classification accuracy decreases monotonically as B increases—this confirms adversarial frequency components causally drive misclassification rather than correlating with other factors

## Open Questions the Paper Calls Out

- **Question:** Do these frequency sensitivity patterns (low/mid for ViTs vs. mid/high for CNNs) persist in high-resolution datasets (e.g., full ImageNet) where high-frequency components carry more semantic information?
- **Basis in paper:** [inferred] The experiments are conducted on CIFAR-10/100 and Tiny ImageNet (64x64). The authors note that "due to variations in image size... the frequency range... may differ," but do not test on standard high-resolution datasets to verify if the observed trends generalize
- **Why unresolved:** Low-resolution datasets force a different distribution of spectral energy than high-resolution images. It is unclear if ViTs retain their low-frequency vulnerability when image complexity and resolution increase significantly
- **What evidence would resolve it:** Replicating the filtering experiments on ImageNet-1k (224x224) and comparing the performance gap curves for both architectures

- **Question:** What specific architectural mechanisms (e.g., patch embedding, global attention) cause the distinct frequency preferences observed between ConvNets and Vision Transformers?
- **Basis in paper:** [inferred] The paper empirically identifies that ViTs are sensitive to low/mid frequencies while CNNs are sensitive to mid/high frequencies, but it does not isolate the root cause (e.g., whether it is the convolution operator vs. the self-attention mechanism)
- **Why unresolved:** The study establishes a correlation between architecture type and frequency vulnerability but does not perform ablation studies on hybrid architectures (e.g., ConvNeXt or ViTs with convolutions) to pinpoint the structural source of this sensitivity
- **What evidence would resolve it:** Ablation studies on hybrid models or by varying patch sizes/convolutional kernel sizes to observe shifts in frequency sensitivity

- **Question:** Can frequency-aware adversarial training, which specifically augments the model with the identified "effective" frequency components, improve robustness against adaptive attacks?
- **Basis in paper:** [inferred] The authors propose in Proposal 3 that "more attention should be given to improving robustness against low- and mid-frequency components" for robust models, but they do not implement or validate a specific defense mechanism
- **Why unresolved:** While the analysis suggests where to focus, it remains to be seen if a defense strategy based on these findings (e.g., training with filtered perturbations) can actually withstand adaptive attacks designed to circumvent frequency-based defenses
- **What evidence would resolve it:** Developing a training pipeline that augments data with the specific frequency bands identified and evaluating its robustness against strong adaptive attacks

## Limitations

- The findings are based on standard attack methods and may not generalize to attacks specifically optimized for robust models or different architectures
- The frequency-based defense recommendations assume that manipulating frequency components during inference can reliably distinguish adversarial from natural examples, but this may not hold against adaptive attacks
- The analysis does not explain the specific architectural mechanisms (e.g., convolution vs. self-attention) that cause the distinct frequency preferences between ConvNets and ViTs

## Confidence

- **High confidence:** The empirical observation that performance gaps widen with increasing high-frequency components is well-supported by the experimental results across multiple datasets and architectures
- **Medium confidence:** The mechanism explaining why CNNs versus ViTs show different frequency vulnerabilities is plausible but requires further validation, particularly for hybrid architectures
- **Medium confidence:** The claim that adversarial training redistributes vulnerability toward lower frequencies is supported by visualization evidence but needs direct testing with attacks optimized specifically against robust models

## Next Checks

1. Test whether attacks specifically optimized for adversarially-trained models still concentrate perturbations in mid-to-high frequencies, or if they adapt to the redistributed vulnerability patterns
2. Evaluate the frequency vulnerability claims on hybrid architectures (e.g., ConvNeXt, Vision MLPs) to determine if the clean separation between CNN and ViT frequency preferences holds
3. Assess the effectiveness of frequency-based defense strategies against adaptive attacks that explicitly modify their frequency distribution to evade detection