---
ver: rpa2
title: 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective'
arxiv_id: '2507.23632'
source_url: https://arxiv.org/abs/2507.23632
tags:
- attention
- softmax
- linear
- recurrent
- gate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work derives a recurrent neural network formulation for softmax
  attention using Taylor series expansion, revealing that linear attention is a first-order
  approximation of softmax. The recurrent form shows softmax attention as a weighted
  sum of infinite RNNs, each modeling higher-order multiplicative interactions between
  query and key dimensions.
---

# On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective

## Quick Facts
- **arXiv ID:** 2507.23632
- **Source URL:** https://arxiv.org/abs/2507.23632
- **Reference count:** 40
- **Primary result:** Replacing softmax's denominator with L2 norm achieves equivalent performance to standard softmax on Llama 2 next-token prediction

## Executive Summary
This paper presents a novel recurrent neural network perspective on softmax attention by deriving its recurrent formulation through Taylor series expansion. The key insight reveals that softmax attention can be expressed as an infinite sum of RNNs, each modeling progressively higher-order multiplicative interactions between query and key dimensions. Through extensive experiments on Llama 2 models, the authors demonstrate that the crucial component for softmax's effectiveness is vector normalization rather than exact exponential normalization—any vector norm performs equivalently to standard softmax while achieving the same performance.

## Method Summary
The method involves deriving a recurrent formulation of softmax attention by expanding the exponential function via Taylor series, decomposing it into multiple RNN-like computations. The authors implement two variants: a "Gate" variant that uses sequence-length normalization with gradient clipping for stability, and a "Norm" variant that replaces the softmax denominator with simple vector normalization (L2 norm). They train Llama 2 models (300M and 2B parameters) on next-token prediction tasks using The Pile, SlimPajama, and FineWeb datasets, comparing these variants against standard softmax and linear attention baselines.

## Key Results
- L2 norm normalization achieves equivalent performance to standard softmax attention on Llama 2 models
- Gate-based normalization performs slightly worse than norm-based approaches
- Higher-order Taylor terms (n=10) approximate softmax behavior, with n=1 representing linear attention
- The exponential function on inner products is essential, but normalization method is flexible

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Softmax attention is expressively superior to linear attention because it represents an infinite sum of recurrent networks with progressively higher-order hidden states.
- **Mechanism**: The exponential function in softmax, when expanded via Taylor series, decomposes into multiple RNN-like computations. Each term n operates on a hidden state H^n_t with dimension d^n, capturing n-th order multiplicative interactions between query and key dimensions.
- **Core assumption**: The Taylor series converges sufficiently and higher-order terms contribute meaningful signal despite 1/n! decay.
- **Evidence anchors**: [section 3.1] Full derivation showing O_t = G_t Σ(1/n!) (Q^⊗n_t) · H^n_t; [section 3.2] "Linear attention is a linear approximation of softmax attention when φ = ψ = Id"; [corpus] Related work on linear attention expressiveness.

### Mechanism 2
- **Claim**: The performance gap between softmax and linear attention stems from linear attention's inability to model higher-order multiplicative interactions between query-key dimension pairs.
- **Mechanism**: Linear attention applies functions φ(Q) and ψ(K) independently to each vector, restricting operations within the original d-dimensional space. The exponential on the inner product e^(Q·K^T) creates combinatorial multiplicative terms across all dimensions—n-th order terms represent n-way interactions that cannot be factored into independent transformations.
- **Core assumption**: These cross-dimensional interactions are task-relevant for language modeling.
- **Evidence anchors**: [section 3.2] "These terms in softmax attention allow it to model combinatorial interactions between inner product dimensions"; [figure 5] Progressive addition of Taylor terms shows smooth interpolation from linear to softmax performance.

### Mechanism 3
- **Claim**: The softmax denominator's critical function is normalization/stabilization, not exact probabilistic interpretation—any vector norm produces equivalent performance.
- **Mechanism**: The denominator G_t = 1/Σ e^(Q_t·K_s^T) stabilizes accumulated numerator values across sequence positions. When replaced with L2 norm, RMS norm, or LayerNorm, performance matches softmax. The exponential on Q·K is irreplaceable, but the normalization method is flexible.
- **Core assumption**: The norm approximation remains stable across sequence lengths.
- **Evidence anchors**: [section 4.5] "any type of normalization, with or without learnable parameters, works just as well as softmax"; [figure 6] Ablation showing ReLU kernel degrades performance regardless of normalization.

## Foundational Learning

- **Concept: Taylor Series Expansion**
  - Why needed here: Core mathematical tool enabling the recurrent decomposition. You must understand how e^x expands to infinite polynomials to grasp why softmax becomes multiple RNNs.
  - Quick check question: Why does the 1/n! coefficient matter for tractability of higher-order terms?

- **Concept: Kronecker Products (A ⊗ B)**
  - Why needed here: Mechanism for constructing higher-order interaction spaces. The n-th Kronecker power A^⊗n lives in d^n dimensions, encoding all n-way products of elements.
  - Quick check question: For Q ∈ ℝ^d, what is the dimensionality of Q^⊗3?

- **Concept: RNN Hidden State Accumulation**
  - Why needed here: The recurrent formulation H^n_t = H^n_{t-1} + (K^⊗n_t)^T · V_t shows how information aggregates. Each order n has its own hidden state.
  - Quick check question: Why can't a single hidden state capture all orders simultaneously?

## Architecture Onboarding

- **Component map**: Q/K projections → Multiple RNNs (one per Taylor order n) → Sum weighted by 1/n! → Apply normalization (L2 norm) → Output
- **Critical path**: 1) Compute query/key projections as standard; 2) For each Taylor order n ∈ {0, 1, ..., N_max}: maintain and update H^n_t; 3) At inference: compute Q^⊗n_t · H^n_t for each n, sum with 1/n! weights; 4) Apply chosen normalization (L2 norm on output vector)
- **Design tradeoffs**:
  - Approximation order vs. compute: n=1 (linear) → n=10 (near-softmax). Paper shows n=10 sufficient; n=3-5 may offer practical balance.
  - Hidden state size: Grows exponentially as d^n. For d=1024, n=2 → 1M elements; practical limits hit quickly.
  - Gate vs. Norm: Gates offer LSTM-like interpretability but introduce instability; norms are stable but less interpretable.
- **Failure signatures**:
  - Loss spikes with gates: Indicates unbounded accumulation—add sequence-length division
  - No improvement adding Taylor terms: Check inner product magnitude; if cosine attention bounded [0,1], higher orders decay too fast
  - Memory explosion: Hidden state dimension d^n exceeds capacity—reduce approximation order
- **First 3 experiments**:
  1. Validate Taylor approximation: Implement n∈{1, 2, 5, 10} variants on small Llama (300M params), plot loss convergence toward softmax baseline as n increases.
  2. Norm ablation: Compare L2, RMS, LayerNorm, and standard softmax denominator on same architecture. Hypothesis: < 1% performance difference.
  3. Linear attention + Taylor boost: Take existing linear attention (e.g., ReLU kernel), add n=2,3,4 terms incrementally. Test if performance improves but never reaches softmax.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the recurrent formulation of softmax attention be extended to theoretically explain the expressiveness of modern state-space models like Mamba and RWKV?
- Basis in paper: [explicit] Section 5 states, "The current formulation covers only linear attention and softmax attention, future work can expand it to more complicated recurrent architectures such as RWKV and state-space models like Mamba."
- Why unresolved: The current theoretical derivation maps softmax to an infinite series of RNNs but has not yet been applied to the specific structural constraints or matrices used in SSMs.
- What evidence would resolve it: A mathematical derivation showing that SSM architectures are approximating specific higher-order terms of the softmax recurrent formulation.

### Open Question 2
- Question: Why do linear attention variants with feature functions (like ReLU or Cosine) fail to match softmax performance even when higher-order Taylor expansion terms are added?
- Basis in paper: [explicit] Section 4.4 observes that linear variants with higher terms improve but never fully reach softmax performance. The authors state, "We hypothesize this performance gap exists because linear attention variants have functions on the independent vectors... We leave exploring this observation to future work."
- Why unresolved: The paper empirically demonstrates the performance gap but only hypothesizes that decomposable kernel functions limit the "reachable" vector space compared to the exponential function's non-decomposable interaction space.
- What evidence would resolve it: A theoretical analysis or ablation study proving that decomposable kernel functions limit the "reachable" vector space compared to the exponential function's non-decomposable interaction space.

### Open Question 3
- Question: Does the finding that a vector norm effectively replaces the softmax denominator generalize to bidirectional, non-causal tasks?
- Basis in paper: [explicit] Section 5 notes, "Only the causal next token prediction task was investigated... further investigation is necessary to ensure this generalization" to other domains like bidirectional modeling.
- Why unresolved: The recurrent formulation and the "norm vs. gate" ablation were only validated on autoregressive language modeling (Llama 2); bidirectional attention involves different summation dynamics.
- What evidence would resolve it: Experimental results from BERT-style bidirectional models or non-causal sequence tasks demonstrating that the norm-based denominator approximation retains performance parity with standard softmax.

## Limitations

- The empirical validation is limited to Llama 2 models trained for causal next-token prediction, leaving generalization to other architectures and tasks uncertain.
- The theoretical expressiveness gap between softmax and linear attention may not translate to practical task performance gains.
- Higher-order terms become computationally intractable due to exponential growth of hidden state dimensions (d^n).

## Confidence

- **High Confidence**: Softmax's denominator normalization is crucial for stability; L2 norm provides equivalent performance to exact softmax normalization
- **Medium Confidence**: Softmax represents an infinite sum of RNNs capturing higher-order interactions; Taylor approximation with n=10 sufficiently approximates softmax
- **Low Confidence**: Higher-order interactions are essential for language modeling performance; the theoretical expressiveness gap translates to practical task performance gains

## Next Checks

1. **Cross-Architecture Validation**: Implement the recurrent attention variants on BERT-base and T5-small architectures for masked language modeling and sequence-to-sequence translation. Compare whether L2 norm normalization still matches softmax performance and whether higher Taylor orders improve downstream metrics beyond next-token prediction.

2. **Hidden State Dimensionality Study**: Systematically measure memory consumption and compute time for Taylor orders n∈{1,2,3,4} on various model scales (128M, 350M, 1.3B parameters). Identify the practical ceiling where exponential state growth makes higher orders infeasible, and test whether knowledge distillation or parameter-efficient approximations can capture higher-order benefits without full state expansion.

3. **Inner Product Distribution Analysis**: For each dataset (The Pile, SlimPajama, FineWeb), measure the empirical distribution of query-key inner products across attention layers. Compute the effective contribution of n-th order terms using the decay factor (inner_product^n / n!). Determine whether datasets with larger inner products benefit more from higher-order terms, validating the theoretical mechanism that higher orders only matter when inner products are sufficiently large.