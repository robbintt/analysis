---
ver: rpa2
title: Sample Efficient Demonstration Selection for In-Context Learning
arxiv_id: '2506.08607'
source_url: https://arxiv.org/abs/2506.08607
tags:
- case
- selection
- arms
- answer
- exemplars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CASE proposes a sample-efficient exemplar selection method for
  in-context learning that formulates the task as a top-m arms identification problem
  in stochastic linear bandits. It maintains a shortlist of challenger arms and selectively
  samples from them to reduce the number of LLM evaluations.
---

# Sample Efficient Demonstration Selection for In-Context Learning

## Quick Facts
- arXiv ID: 2506.08607
- Source URL: https://arxiv.org/abs/2506.08607
- Reference count: 40
- CASE achieves up to 7× speedup in runtime and 7× fewer LLM calls (87% reduction) while maintaining or improving task performance

## Executive Summary
This paper proposes CASE, a sample-efficient method for exemplar selection in in-context learning that formulates the task as a top-m arms identification problem in stochastic linear bandits. Instead of exhaustively evaluating all possible exemplar subsets, CASE maintains a shortlist of challenger arms and selectively samples from them to reduce the number of LLM evaluations. The approach models exemplar subset rewards using a linear function of BERT-based similarity scores, enabling a stochastic linear bandit setting. CASE achieves significant efficiency gains—up to 7× speedup and 87% fewer LLM calls—while maintaining or improving task performance compared to state-of-the-art exemplar selection methods across multiple reasoning datasets.

## Method Summary
CASE frames exemplar selection as a top-m arm identification problem in stochastic linear bandits. The method represents each k-sized exemplar subset as a feature vector based on averaged BERT similarity scores between exemplars and validation examples. The reward (validation accuracy) is modeled as a linear function of these features. CASE maintains a shortlist of challenger arms (N_t) and iteratively samples m' new arms from the unexplored space, selecting top candidates to add to N_t. In each iteration, the algorithm compares the weakest arm in the current top-m set (U_t) against the strongest challenger from N_t, pulling only the more promising arm via an LLM call to get a reward. The process continues until a gap-index stopping criterion is met, indicating ε-optimal top-m identification. This selective exploration strategy dramatically reduces the number of LLM calls compared to exhaustive evaluation while maintaining solution quality.

## Key Results
- CASE achieves up to 7× speedup in runtime and 7× fewer LLM calls (87% reduction) compared to baseline methods
- Maintains or improves Exact Match and Cover-EM accuracy across GSM8K, AquaRat, TabMWP, FinQA, and StrategyQA datasets
- Exemplar subsets selected using CASE transfer effectively to larger models, achieving up to 97% of the performance of exemplars selected directly on the target model at 1/3 the cost

## Why This Works (Mechanism)

### Mechanism 1
Formulating exemplar selection as a linear bandit problem enables sample-efficient estimation of exemplar quality without exhaustively evaluating all subsets. Each k-sized exemplar subset (arm) is represented as a feature vector based on averaged BERT similarity scores between exemplars and validation examples. The reward (validation accuracy) is modeled as ρ(a) = α^T x_a, where α is learned via regularized least squares. This reduces the search space from evaluating combinatorial arms to learning a d-dimensional parameter vector. Core assumption: The true reward function is approximately linear in the similarity-based features; exemplar usefulness correlates with semantic similarity to validation data. Break condition: If exemplar interactions are highly non-linear (e.g., diverse exemplars together provide emergent reasoning), the linear surrogate underfits and top-m selection becomes unreliable.

### Mechanism 2
Maintaining a shortlist of challenger arms (N_t) instead of comparing against all remaining arms reduces gap-index computations from O(|S|) to O(m') per iteration. At each iteration, CASE samples m' arms uniformly from unexplored space, selects top-m' by current estimated reward, and merges with existing N_t. The most ambiguous arm from U_t (current top-m) is compared only against arms in N_t, not the full complement U^c_t. This prunes comparisons while preserving exploration of promising regions. Core assumption: Uniform sampling from unexplored arms with selection by current model is sufficient to discover true challengers; high-reward arms are discoverable through this filtered exploration. Break condition: If top-m' sampled arms consistently miss the true best challengers (e.g., reward landscape is highly multimodal), convergence stalls or identifies suboptimal arms.

### Mechanism 3
Gap-index based stopping criterion (B_t(s_t, b_t) ≤ ε) provides provable sample complexity bounds while ensuring ε-optimal top-m identification. The gap index B_t(i,j) = ρ̂_t(i) - ρ̂_t(j) + W_t(i,j) combines empirical reward difference with confidence width. The algorithm pulls the arm that most reduces uncertainty between the weakest top-m arm (b_t) and strongest challenger (s_t). Convergence occurs when the gap is sufficiently small with high confidence. Core assumption: Subgaussian noise in rewards; the event E (confidence bounds hold) occurs with probability ≥ 1-δ. Break condition: If validation accuracy is high-variance (noisy LLM outputs), confidence widths remain large and convergence requires many more pulls than theoretical bound suggests.

## Foundational Learning

- **Stochastic Multi-Armed Bandits**: Why needed here: CASE frames exemplar selection as a bandit problem where each subset is an arm with unknown reward; understanding explore-exploit tradeoffs is essential. Quick check question: Explain why uniform exploration is suboptimal compared to gap-index based selection for identifying top-m arms.

- **Linear Regression with Regularization**: Why needed here: The reward model ρ(a) = α^T x_a is learned via ridge regression (regularized least squares); parameter updates use incremental Sherman-Morrison formula. Quick check question: Derive the update for V̂_{t+1}^{-1} given a new sample (x_a, r) without full matrix inversion.

- **Concentration Inequalities (Subgaussian Tail Bounds)**: Why needed here: Confidence widths W_t(i,j) rely on subgaussian assumptions to bound estimation error; sample complexity guarantees depend on these. Quick check question: State how the confidence term C_{t,δ} scales with number of pulls and dimensionality.

## Architecture Onboarding

- **Component map**: Training exemplars X → Cluster into 5 groups → Sample k-subsets (arms) → Initialize U_0 (random m arms), N_0 (empty), α̂_0 ~ N(0,1) → Loop until B_t(s_t,b_t) ≤ ε: Swap arms between U_t and N_t if challenger score > top-m score, Sample m' new arms from unexplored space → M_t, Select top-m' from M_t ∪ N_{t-1} → N_t, Compute b_t (weakest in U_t) and s_t (strongest challenger in N_t), Pull selected arm via LLM call → reward r_t, Update V̂_t, α̂_t via least squares → Output: U_T (top-m exemplar subsets)

- **Critical path**: The LLM inference call (line 21 in Algorithm 1) dominates runtime; all other operations are O(n²) matrix updates. The number of iterations until convergence directly determines total cost.

- **Design tradeoffs**: m (top-m size): Larger m captures more diverse strategies but increases comparisons and may include lower-quality subsets. Paper uses m=10. m' (challenger shortlist size): Larger m' improves exploration robustness but increases per-iteration computation. Paper uses m'=5. ε (convergence threshold): Smaller ε guarantees better solutions but requires more LLM calls. Paper uses ε=0.1. Validation set size: Larger V improves reward estimation but costs more LLM calls per pull. Paper uses |V|=20.

- **Failure signatures**: Non-convergence: B_t never drops below ε—check if validation rewards are high-variance or if m' is too small to find true challengers. Poor transfer to test set: Exemplars overfit validation set—reduce |V| or add regularization. Runtime blowup: Per-iteration LLM calls exceed expected—verify that only one arm is pulled per iteration (not all of M_t).

- **First 3 experiments**: 1. Reproduce synthetic efficiency results (Figure 2): Implement CASE, LinGapE, LinGIFA with K=20, m=3, m'=3. Verify CASE requires ~20x fewer comparisons and ~5-12x less runtime while gap-index and regret converge to zero. 2. Ablate challenger shortlist size: Run CASE with m' ∈ {1, 3, 5, 10} on GSM8K subset. Plot LLM calls vs. final accuracy to find optimal efficiency-performance tradeoff. Expect m'=1 to fail (insufficient exploration) and m'=10 to be wasteful. 3. Test transfer across LLM scales: Select exemplars using Mistral-7b, evaluate on GPT-3.5-turbo (as in Figure 4). Compare performance vs. exemplars selected directly on GPT-3.5-turbo to quantify transfer gap and cost savings.

## Open Questions the Paper Calls Out

### Open Question 1
Question: What is the rigorous theoretical regret bound for the CASE algorithm, and how does it quantify the trade-off between selective exploration and estimation error?
Basis in paper: [explicit] The authors state, "While we postpone a rigorous derivation of the regret bound for CASE to a later study, we justify our assumption by using the SETC Algorithm..."
Why unresolved: The paper provides a sample complexity bound (Theorem 1) but explicitly defers the proof for the average regret bound of the selected arm sets ($U_T \ ∪ N_T$), currently relying on heuristic justification.
What evidence would resolve it: A formal proof establishing an upper bound for the average regret over $t$ timesteps specifically for the challenger arm sampling strategy.

### Open Question 2
Question: Can the challenger arm sampling technique be effectively adapted for adaptive retrieval in open-domain question answering?
Basis in paper: [explicit] "In the future, we also plan to extend and apply CASE to adaptive retrieval methods... and question answering."
Why unresolved: The current formulation addresses exemplar subset selection (MESS); adapting it to document retrieval requires handling a significantly larger and noisier search space with different relevance signals.
What evidence would resolve it: Successful application of the selective exploration framework to re-rank documents in a retrieval-augmented generation (RAG) pipeline, demonstrating sample efficiency over dense retrieval baselines.

### Open Question 3
Question: Does the linear reward model assumption fail on tasks where in-context learning dynamics are highly non-linear?
Basis in paper: [inferred] The method models the reward score as a "parameterized linear scoring function" (Section 3.2) based on sentence similarities, but LLM behaviors are often non-linear.
Why unresolved: While the linear assumption simplifies the stochastic bandit formulation, it may not capture complex synergy or interference between exemplars that a linear combination of similarities would miss.
What evidence would resolve it: An ablation study comparing CASE against a non-linear bandit or neural process baseline on tasks specifically designed to require synergistic exemplar reasoning.

## Limitations

- Theoretical guarantees assume idealized conditions (subgaussian noise, linear reward model) that may not hold in practice, potentially requiring more samples than bounds suggest
- Transfer experiments are limited to one downstream model (GPT-3.5-turbo) evaluated on exemplars selected for a different model (Mistral-7b), which may not generalize to other model pairs
- The method's performance depends critically on the quality of the challenger shortlist sampling strategy and may fail if top-m' sampled arms consistently miss true best challengers

## Confidence

- **High confidence**: The runtime efficiency improvements (7× speedup, 87% reduction in LLM calls) are directly measurable and well-supported by the synthetic experiments. The linear bandit formulation is mathematically sound and the gap-index stopping criterion has provable sample complexity bounds.
- **Medium confidence**: The task performance improvements across multiple datasets are credible but could be affected by random initialization and dataset-specific factors. The transfer learning results show promise but are limited in scope and don't establish robustness across different model pairs.
- **Low confidence**: The theoretical guarantees (Theorem 1, Lemma 1) assume idealized conditions that may not hold in practice. The choice of hyperparameters (m=10, m'=5, ε=0.1) appears reasonable but lacks systematic justification or sensitivity analysis.

## Next Checks

1. **Sensitivity analysis**: Systematically vary m, m', and ε parameters on GSM8K to quantify their impact on convergence speed, LLM calls, and final accuracy. This will reveal the robustness of CASE to hyperparameter choices and identify optimal settings for different dataset characteristics.

2. **Multi-model transfer study**: Evaluate exemplar transferability across three different LLM pairs (e.g., Mistral→GPT-3.5, Mistral→GPT-4, Llama→Claude). This will determine whether the transfer benefits are model-specific or represent a general phenomenon, and identify which model characteristics affect transferability most.

3. **Noisy reward robustness test**: Introduce varying levels of noise to the validation accuracy rewards (simulating unreliable LLM outputs) and measure the impact on CASE's convergence and solution quality. This will validate whether the subgaussian noise assumptions hold in practice and identify failure modes when they don't.