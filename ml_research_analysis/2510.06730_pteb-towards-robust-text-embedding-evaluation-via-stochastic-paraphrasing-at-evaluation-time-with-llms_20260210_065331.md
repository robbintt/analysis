---
ver: rpa2
title: 'PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing
  at Evaluation Time with LLMs'
arxiv_id: '2510.06730'
source_url: https://arxiv.org/abs/2510.06730
tags:
- pteb
- evaluation
- datasets
- table
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PTEB addresses benchmark saturation and overfitting in text embedding
  evaluation by dynamically generating meaning-preserving paraphrases at evaluation
  time using LLMs. The method uses an LLM judge to select a high-quality paraphraser
  based on semantic similarity ratings, then applies it to MTEB datasets to create
  novel test variants.
---

# PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs

## Quick Facts
- arXiv ID: 2510.06730
- Source URL: https://arxiv.org/abs/2510.06730
- Reference count: 26
- Primary result: Text embedding models consistently score lower (average drop of 3.89%) on dynamically generated paraphrased data compared to original static benchmarks

## Executive Summary
PTEB introduces a novel framework to address benchmark saturation and overfitting in text embedding evaluation by generating meaning-preserving paraphrases at evaluation time using LLMs. The method uses an LLM judge to select high-quality paraphrasers based on semantic similarity ratings, then applies them to MTEB datasets to create novel test variants. Across 7 MTEB tasks and 20 datasets in 25 languages, embedding models show consistent performance degradation on paraphrased data, revealing sensitivity to token-level changes while preserving semantics. The framework provides a contamination-resistant evaluation approach that shifts NLP benchmarking from static datasets to dynamic, eval-time paraphrasing.

## Method Summary
PTEB addresses text embedding evaluation robustness through a two-phase LLM-driven process. First, an LLM judge evaluates candidate paraphrasers on their ability to maintain semantic similarity while introducing surface-level variation on STS data. The best-performing paraphraser (Gemma3-27b) is then used to generate paraphrases for 20 MTEB datasets across 25 languages. The evaluation compares embedding model performance on original vs. paraphrased test sets, measuring performance drops as the primary metric. The approach ensures high-quality paraphrases through human evaluation (mean semantic similarity 4.05/5.0) and edit distance checks (>0.80), providing a statistically robust framework that reveals embedding models' sensitivity to token-level changes despite semantic preservation.

## Key Results
- Text embedding models consistently score lower on paraphrased data, with an average performance drop of 3.89% compared to original benchmarks
- Smaller embedding models (4B vs 8B parameters) show similar robustness levels, suggesting parameter count is not the primary factor in robustness
- Human evaluation confirms paraphrases maintain high semantic similarity (mean 4.05/5.0) while introducing sufficient surface-level variation (normalized Edit Distance >0.80)

## Why This Works (Mechanism)
The framework works by exposing embedding models to semantically equivalent but surface-level different inputs at evaluation time, revealing their sensitivity to token-level variations. By using LLMs to generate paraphrases dynamically during evaluation rather than relying on static test sets, PTEB prevents models from overfitting to specific phrasing patterns in benchmark datasets. The dual-LLM approach (judge + paraphraser) ensures high-quality paraphrases that preserve meaning while introducing meaningful variation, making the evaluation more robust to contamination and more reflective of real-world embedding performance.

## Foundational Learning

**LLM-based Semantic Similarity Evaluation**: Needed to automatically assess paraphrase quality at scale without human annotation. Quick check: Verify judge LLM's correlation with human similarity ratings on STS benchmarks exceeds 0.7.

**Dynamic Test Generation**: Required to create novel evaluation instances that prevent benchmark overfitting. Quick check: Ensure paraphrased datasets contain >80% unique n-grams compared to originals.

**Stochastic Sampling for Diversity**: Essential for generating varied paraphrases that test model robustness. Quick check: Measure paraphrase diversity using normalized edit distance (>0.80 target).

**Multilingual Paraphrasing**: Critical for evaluating embeddings across diverse linguistic contexts. Quick check: Verify paraphrased versions maintain semantic similarity (>4.0/5.0) across all 25 languages.

## Architecture Onboarding

**Component Map**: User Query -> Embedding Model -> Similarity Score (Original) vs User Query -> Paraphraser (Gemma3-27b) -> Embedding Model -> Similarity Score (Paraphrased)

**Critical Path**: Paraphrase Generation → Embedding Evaluation → Performance Comparison → Robustness Assessment

**Design Tradeoffs**: 
- *LLM Judge Selection*: Balances semantic fidelity vs. computational cost
- *Paraphraser Diversity*: Trades off surface variation vs. meaning preservation
- *Evaluation Granularity*: Weighs comprehensive testing vs. runtime efficiency

**Failure Signatures**:
- Low performance delta (<1%) suggests benchmark overfitting or inadequate paraphrasing
- High semantic similarity (>4.5/5.0) with low edit distance (<0.7) indicates insufficient surface variation
- Memory OOM errors during paraphrase generation point to insufficient GPU resources

**First Experiments**:
1. Run PTEB pipeline on single dataset (e.g., STS-B) to verify end-to-end functionality
2. Compare performance drops across embedding model families (sentence-transformers vs. Qwen vs. proprietary)
3. Measure paraphrase quality metrics (semantic similarity, edit distance) for sanity checking LLM outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Results may be specific to MTEB benchmark and may not generalize to other embedding tasks or domains
- Computational expense of eval-time paraphrasing could limit practical adoption for large-scale evaluations
- Performance drop measurements may partially reflect idiosyncrasies of the specific Gemma3-27b paraphrasing style rather than universal embedding model properties

## Confidence

**High Confidence**: The 3.89% average performance drop finding is well-supported by extensive experimentation across 20 datasets, 7 tasks, and 25 languages, with strong human evaluation validation of paraphrase quality.

**Medium Confidence**: Claims about smaller models showing similar robustness levels are based on limited comparisons and would benefit from broader testing across more model families.

## Next Checks

1. Reproduce the core 3.89% average performance drop by running the full PTEB pipeline on all 20 MTEB datasets using Gemma3-27b and default Ollama parameters across 6 seeds.

2. Substitute alternative LLMs (GPT-4o-mini, Claude-3.5-Sonnet) for judge and paraphraser roles to measure how performance drops vary with different model selections.

3. Measure wall-clock time and computational resources required to generate paraphrased versions of all 20 datasets, calculating cost per dataset using current API pricing.