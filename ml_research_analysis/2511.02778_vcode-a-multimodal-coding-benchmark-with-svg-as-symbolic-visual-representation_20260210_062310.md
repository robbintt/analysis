---
ver: rpa2
title: 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation'
arxiv_id: '2511.02778'
source_url: https://arxiv.org/abs/2511.02778
tags:
- image
- visual
- code
- answer
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces VCode, a benchmark that reframes multimodal\
  \ understanding as SVG code generation: given an image, a model must produce SVG\
  \ code that preserves symbolic meaning for downstream reasoning. To evaluate symbolic\
  \ fidelity, the authors propose CodeVQA, a novel protocol in which a policy model\
  \ answers questions over rendered SVGs\u2014correct answers indicate faithful symbolic\
  \ preservation."
---

# VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation

## Quick Facts
- arXiv ID: 2511.02778
- Source URL: https://arxiv.org/abs/2511.02778
- Reference count: 40
- Primary result: Introduces VCode benchmark reframing multimodal understanding as SVG code generation, with CodeVQA protocol evaluating symbolic fidelity through question-answering over rendered SVGs

## Executive Summary
This paper introduces VCode, a benchmark that reframes multimodal understanding as SVG code generation: given an image, a model must produce SVG code that preserves symbolic meaning for downstream reasoning. To evaluate symbolic fidelity, the authors propose CodeVQA, a novel protocol in which a policy model answers questions over rendered SVGs—correct answers indicate faithful symbolic preservation. Experiments show that frontier VLMs struggle to generate faithful SVGs, revealing a gap between language-centric and visual-centric coding. To address this, the authors introduce VCoder, an agentic framework that augments VLMs via (i) Thinking with Revision—iteratively analyzing discrepancies and refining SVG code—and (ii) Acting with Visual Tools—leveraging detectors and parsers to supply structured cues like objects, shapes, and text. Across benchmarks, VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show consistent performance drops for both humans and VLMs when reasoning over rendered SVGs versus raw images, suggesting the promise of symbolic visual representation. The benchmark and code are publicly available.

## Method Summary
The VCode benchmark evaluates SVG code generation from images, measuring both pixel-level similarity (SigLIP Score) and symbolic fidelity (CodeVQA—policy model accuracy answering questions from rendered SVGs). The VCoder framework augments VLMs with two mechanisms: (1) Thinking with Revision—iterative discrepancy analysis and code refinement using up to 2 revision rounds, and (2) Acting with Visual Tools—Florence-2 for detection, SAM-2 for segmentation masks (converted to sparse polygons), and OpenOCR for text regions. The system uses a headless browser to render SVGs for evaluation. Inference-only pipeline runs on RTX 4090 with 16K token limits, using GPT-4o-mini as the primary policy model with benchmark-specific evaluators.

## Key Results
- VLMs struggle with faithful SVG generation: Claude-4-Opus achieves only 54.0 CodeVQA vs. 77.5 on raw images
- VCoder improves performance by 12.3 points over top-performing baseline
- Visual tool augmentation (Florence-2, SAM-2, OpenOCR) yields significant gains in structured cue preservation
- Human studies confirm both humans and VLMs drop in performance when reasoning over rendered SVGs vs. raw images
- Thinking with Revision mechanism provides substantial gains after just 1-2 iteration rounds

## Why This Works (Mechanism)

### Mechanism 1: Augmented Perception via Structured Tool Cues
- **Claim:** Injecting explicit geometric and semantic metadata into the context window bridges the gap between a VLM's coarse visual understanding and the precise numerical requirements of SVG code.
- **Mechanism:** VCoder uses external tools (Florence-2 for detection, SAM-2 for segmentation, OpenOCR) to extract coordinates, masks, and text. These are formatted as structured "cues" (e.g., bounding boxes or polygon paths). The VLM shifts from *hallucinating* coordinates to *transcribing* provided structured data into SVG syntax.
- **Core assumption:** The VLM possesses strong instruction-following capabilities to map provided JSON-like metadata to valid SVG tags, even if it cannot generate the coordinates from raw pixels alone.
- **Evidence anchors:** Section 4.2 states detectors supply structured cues "beyond intrinsic model capacity," specifically noting SAM-2 helps with "irregular boundaries." Table 3 shows adding Location, Category, and Shape cues yields significant gain (43.3 avg) compared to baseline (37.5).

### Mechanism 2: Iterative Visual Feedback (Thinking with Revision)
- **Claim:** Using a "render-difference-refine" loop allows the model to correct semantic misalignments that are invisible in the code text but obvious in the rendered image.
- **Mechanism:** The system executes the initial SVG code to create an image $\hat{V}$. It feeds the original image $V$ and $\hat{V}$ back to the VLM, prompting it to generate a "difference signal" $\Delta$. This forces the model to critique its own output visually before generating revised code.
- **Core assumption:** The VLM can accurately compare the original and rendered images to articulate specific differences (e.g., "missing object," "wrong color") effectively.
- **Evidence anchors:** Section 4.1 describes the loop: "detect discrepancies... then update the code." Figure 5 shows GLM-4.5V and Claude-4-Opus gaining significant CodeVQA points after just 1-2 revision rounds.

### Mechanism 3: Semantic Verification via Executability
- **Claim:** Evaluating "Symbolic Fidelity" via CodeVQA (answering questions based on the rendered SVG) ensures the generated code preserves the *reasoning-relevant* structure of the image, rather than just pixel similarity.
- **Mechanism:** Unlike CLIPScore or pixel-loss, CodeVQA forces the generator to preserve "question-relevant" attributes (e.g., relative spatial positions, object counts). If the SVG renders an image where a policy model can still answer "What is to the left of the laptop?", the symbolic representation is deemed faithful.
- **Core assumption:** The policy model (evaluator) is robust enough to answer questions based on the rendered SVG, acting as a reliable proxy for human symbolic reasoning.
- **Evidence anchors:** Section 3.2 "Correct answers indicate faithful symbolic preservation." Section 5.3 human studies show both humans and VLMs drop in performance on rendered SVGs vs. raw images, validating that the task measures a non-trivial abstraction capability.

## Foundational Learning

- **Concept: Scalable Vector Graphics (SVG) DOM**
  - **Why needed here:** The benchmark treats SVG not as an image format, but as a *code generation target*. You must understand that SVG is XML-based, requiring precise numerical coordinates (`viewBox`, `cx`, `cy`) and strict tag nesting.
  - **Quick check question:** If an SVG has `viewBox="0 0 100 100"`, but you draw a rectangle at `x="110"`, will it appear in the rendered image?

- **Concept: Tool-Augmented Agents**
  - **Why needed here:** VCoder is not a single model; it is an agentic framework. You need to understand how a VLM acts as a controller that calls external APIs (Detectors, OCR) and processes their JSON outputs.
  - **Quick check question:** How does the VLM receive the output of the SAM-2 segmentation tool—as a raster image mask or as a list of polygon coordinates?

- **Concept: Test-Time Compute (Revision)**
  - **Why needed here:** The performance gains in this paper come significantly from "Thinking with Revision," effectively scaling compute at inference time rather than training time.
  - **Quick check question:** In the revision loop, does the model update its weights based on the error, or does it update the context window for the next prediction?

## Architecture Onboarding

- **Component map:** Raw Image + Question -> Florence-2 (Detection) + SAM-2 (Segmentation) + OpenOCR (Text) -> VLM (SVG Generation) -> Render Engine -> Policy Model (CodeVQA) -> Final Score

- **Critical path:**
  1. **Extraction:** Run tools on the raw image to generate a JSON of coordinates/text
  2. **Generation:** Prompt VLM with Image + Tool-JSON to write initial SVG code
  3. **Rendering:** Execute code to create a temporary image file
  4. **Revision:** Prompt VLM with [Original Image, Rendered Image, Old Code] -> Output [Discrepancy Analysis, New Code]
  5. **Evaluation:** Pass Final SVG to the Policy Model to answer the original question

- **Design tradeoffs:**
  - **Token Cost vs. Fidelity:** Table 2 shows high-performing models use >2K tokens. Effective SVGs are verbose. You must balance context window limits with the need for detailed path descriptions.
  - **Tool Reliability:** The system is only as good as the detectors. If OCR fails on rotated text, the SVG will fail.

- **Failure signatures:**
  - **"Blind Coding":** The model generates valid XML syntax but the coordinates are nonsense (e.g., a "sheep" drawn entirely outside the viewBox)
  - **Over-Segmentation:** Using complex path elements for simple shapes (drawing a circle with 100 tiny lines instead of `<circle>`), causing rendering lag
  - **Drift in Revision:** The model "fixes" one error but deletes an object in a subsequent revision round

- **First 3 experiments:**
  1. **Tool Ablation:** Run VCoder without SAM-2 shapes (only bounding boxes) to quantify the value of precise contours vs. coarse location
  2. **Round Limit:** Compare performance with 0, 1, and 2 revision rounds to find the point of diminishing returns for inference cost
  3. **Direct Prompting vs. Chain-of-Thought:** Compare "Just generate SVG" vs. "Think step-by-step then generate SVG" to see if explicit planning replaces the need for iterative revision

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can end-to-end trained vision–language coders achieve faithful symbolic representations without requiring iterative revision or external visual tools?
- **Basis in paper:** [explicit] "Future work can focus on developing end-to-end vision–language coders with scalable training data to enable more faithful symbolic representations."
- **Why unresolved:** VCoder relies on test-time scaling (revision loops) and external tools (detectors, OCR) rather than intrinsic model capabilities; current VLMs lack native visual-centric coding abilities.
- **What evidence would resolve it:** A model trained from scratch on large-scale image-to-SVG pairs that matches or exceeds VCoder's 54.0 CodeVQA score without any test-time augmentation.

### Open Question 2
- **Question:** What visual attributes are minimally necessary to preserve in SVG representations for downstream reasoning across different task types (spatial, knowledge, counting)?
- **Basis in paper:** [inferred] Models struggle differently across domains—spatial reasoning fails most on 3D relations (near-random scores), while knowledge tasks require semantic fidelity. The paper shows shape cues help spatial tasks without always improving SigLIP similarity.
- **Why unresolved:** The paper does not systematically ablate which attributes (shape fidelity, color accuracy, positional precision, text preservation) drive performance on each subtask.
- **What evidence would resolve it:** Controlled ablation studies varying attribute preservation independently while measuring CodeVQA scores per domain.

### Open Question 3
- **Question:** Does the superiority of Img2Text2SVG over direct Img2SVG indicate a fundamental limitation in current VLM architectures for visual-centric code generation?
- **Basis in paper:** [explicit] "Translating images into linguistic captions before coding (Img2Text2SVG) achieves the best performance, highlighting the benefit of language as an intermediate representation."
- **Why unresolved:** If language mediation outperforms direct visual-to-code pathways, this suggests VLMs may not truly integrate visual and symbolic reasoning—but whether this is architectural or a data scaling issue remains unclear.
- **What evidence would resolve it:** Comparing Img2Text2SVG vs. Img2SVG performance as model scale and visual-code training data increase; architectural interventions that bypass linguistic bottlenecks.

## Limitations
- The benchmark evaluates a narrow slice of multimodal understanding—symbolic fidelity via CodeVQA may not capture broader visual reasoning capabilities
- Reliance on external detectors (Florence-2, SAM-2, OpenOCR) introduces critical dependencies that can fail regardless of VLM quality
- The policy model (GPT-4o-mini) as proxy for human reasoning could be brittle, leading to false negatives where semantically correct SVGs are rejected
- The benchmark assumes static visual elements (objects, shapes, text) and may not generalize to complex visual reasoning tasks involving dynamic scenes or abstract concepts

## Confidence
- **High Confidence:** VLMs struggle to generate faithful SVGs (well-supported by quantitative results showing large performance gaps)
- **High Confidence:** Effectiveness of Thinking with Revision mechanism (validated through ablation studies)
- **Medium Confidence:** Acting with Visual Tools significantly improves performance (supported but exact tool contributions not fully isolated)
- **Medium Confidence:** Human study results are suggestive but limited in sample size and scope
- **Low Confidence:** Claim that symbolic visual representation is "promising" for downstream reasoning (speculative, only demonstrated on specific CodeVQA protocol)

## Next Checks
1. **Tool Ablation Study:** Run VCoder without each visual tool (Florence-2, SAM-2, OpenOCR) individually to quantify their marginal contribution and identify which tool is most critical for performance
2. **Evaluator Robustness Test:** Create adversarial SVG examples that are visually correct but semantically ambiguous to test whether the policy model can reliably distinguish faithful from unfaithful representations
3. **Cross-Dataset Generalization:** Evaluate VCoder on a held-out dataset not seen during development (e.g., from UniSVG or custom collection) to test whether the approach generalizes beyond the VCode benchmark