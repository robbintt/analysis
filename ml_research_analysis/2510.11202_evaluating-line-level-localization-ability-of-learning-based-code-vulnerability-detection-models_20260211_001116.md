---
ver: rpa2
title: Evaluating Line-level Localization Ability of Learning-based Code Vulnerability
  Detection Models
arxiv_id: '2510.11202'
source_url: https://arxiv.org/abs/2510.11202
tags:
- code
- vulnerability
- detection
- relevance
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating how well learning-based
  vulnerability detection models can localize vulnerable code lines, rather than just
  flagging entire functions. The authors propose Detection Alignment (DA), an explainability-based
  metric that quantifies agreement between model predictions and ground truth by measuring
  overlap between the most influential code lines (as identified by XAI methods) and
  actual vulnerable lines.
---

# Evaluating Line-level Localization Ability of Learning-based Code Vulnerability Detection Models

## Quick Facts
- arXiv ID: 2510.11202
- Source URL: https://arxiv.org/abs/2510.11202
- Reference count: 14
- Primary result: Learning-based vulnerability detection models achieve high F1 scores but consistently show poor line-level localization ability (DA scores often below 0.12), indicating reliance on spurious correlations rather than semantic vulnerability understanding

## Executive Summary
This paper introduces Detection Alignment (DA), a metric that evaluates whether vulnerability detection models can correctly identify the specific code lines that contain vulnerabilities, rather than just flagging entire functions. Using explainability methods to aggregate token-level relevance scores into line-level attributions, the authors compute a fuzzy Jaccard index between model-attended lines and ground truth vulnerable lines. Testing three transformer-based models (CodeBERT, LineVul, CodeT5) across three datasets (BigVul, Devign, PrimeVul) reveals a consistent pattern: models achieve high F1 scores but consistently fail to localize vulnerabilities to the correct lines, with DA scores typically below 0.12. This disconnect suggests models learn spurious correlations rather than genuine vulnerability patterns, highlighting the need for better evaluation metrics and training approaches that prioritize localization.

## Method Summary
The authors propose a model-agnostic evaluation framework that measures alignment between model predictions and ground truth localization using explainability methods. For each model prediction, token-level relevance scores are computed using attention weights, AttnLRP, or Integrated Gradients, then aggregated into line-level scores by summing across tokens in each line. The Detection Alignment (DA) metric computes the fuzzy Jaccard index between the resulting relevance distribution and ground truth vulnerable lines. The framework is applied to three transformer models (CodeBERT, LineVul, CodeT5-Small) finetuned on three vulnerability datasets (BigVul, Devign, PrimeVul) with standard hyperparameters (lr=2e-5, epochs=10, max_length=512), with results evaluated on the BigVul test set.

## Key Results
- Models achieve high F1 scores (up to 0.93) but consistently low DA scores (often below 0.12), indicating poor localization ability
- Across all models and datasets, there is a systematic disconnect between detection performance and localization accuracy
- Example analysis shows models often attend to related but non-vulnerable lines (e.g., focusing on `memcpy` when vulnerability is in `strcpy`)
- The 510-token truncation affects 426/1005 vulnerable samples in the test set, potentially impacting localization

## Why This Works (Mechanism)

### Mechanism 1
Token-level attribution methods (Attention, AttnLRP, IG) can be aggregated to quantify the contribution of specific source code lines to a model's vulnerability prediction. The model computes attention weights or gradient attributions for input tokens, which are summed across all tokens belonging to a specific line and normalized to $[0,1]$. This aggregates fine-grained model activations into human-interpretable line relevance scores. The core assumption is that the chosen XAI method faithfully represents the model's internal decision logic. Evidence comes from the paper's formulation of line relevance aggregation and the consistent low DA scores despite high F1. Break condition: If the XAI method is unfaithful or if tokens cannot be deterministically mapped back to source lines.

### Mechanism 2
Detection Alignment (DA) acts as a conditional validity check for model predictions by measuring the overlap between model-attended lines and ground-truth vulnerable lines. DA constructs two fuzzy sets (model relevance and ground truth) and computes their Jaccard index. A high score implies the model "looked at" the right lines; a low score implies it relied on spurious features or context. The core assumption is that ground truth is complete and accurate. Evidence includes the consistent DA scores across models and the discussion of shortcut learning. Break condition: If vulnerabilities are semantic and require understanding interactions between distant lines.

### Mechanism 3
Low DA scores reveal that high classification performance (F1) is often driven by dataset artifacts rather than semantic vulnerability understanding. By decoupling detection from localization, the metric exposes "shortcut learning" where models exploit spurious correlations. If a model predicts "vulnerable" with high confidence but attends to irrelevant lines, the DA score drops. The core assumption is that spurious correlations exist in training data. Evidence includes sample-wise analysis showing focus on `memcpy` instead of `strcpy`. Break condition: If the dataset is perfectly curated, low DA might indicate model capacity issues.

## Foundational Learning

- **Concept: Explainable AI (XAI) Attribution (Attention vs. Gradients)**
  - Why needed here: You cannot compute the "R" set (Relevant Lines) without understanding how to extract importance weights from a transformer. The paper uses Attention (raw), AttnLRP (backprop), and Integrated Gradients.
  - Quick check question: Does the gradient-based approach (IG) yield different DA scores than the attention-based approach in the results, and what does that imply about the model's layers?

- **Concept: Fuzzy Set Theory & Jaccard Index**
  - Why needed here: The DA metric is not a binary "hit/miss" but a weighted overlap. You need to understand how to handle the membership degree $\mu$ (relevance scores) in the intersection/union calculation.
  - Quick check question: Why is a fuzzy Jaccard index preferred over a strict binary intersection count for measuring alignment in this context?

- **Concept: Spurious Correlations & Shortcut Learning**
  - Why needed here: The central thesis is that high F1 $\neq$ security. Understanding how models exploit dataset biases is required to interpret why DA is low.
  - Quick check question: In the "memcpy" vs "strcpy" example, why does the model focus on the wrong function, and how does DA penalize this?

## Architecture Onboarding

- **Component map:** Input Parser -> Tokenizer -> Model Wrapper -> Attribution Engine -> Aggregator -> Comparator
- **Critical path:** The mapping of Tokens $\to$ Lines is the most fragile step. If the tokenizer splits a semantic identifier (e.g., `buffer_overflow` $\to$ `buffer`, `_`, `overflow`), you must sum the attentions of all sub-tokens to get the line score.
- **Design tradeoffs:**
  - XAI Method: Attention is fast but noisy; IG is robust but computationally expensive
  - Granularity: Line-level is intuitive but coarse; token-level is precise but harder to evaluate
  - Input Length: The paper truncates to 510 tokens, potentially missing vulnerable lines in long functions
- **Failure signatures:**
  - High F1, Low DA: Model is a "security theater" detectorâ€”likely exploiting style or common libraries
  - Zero DA on True Positives: The model guessed correctly based on context not part of vulnerable line set
  - Layer Mismatch: DA-A(1) differs from DA-A(M), indicating features processed hierarchically (syntax $\to$ semantics)
- **First 3 experiments:**
  1. Sanity Check: Implement DA using simple raw attention on CodeBERT on BigVul test set to verify low DA trend (<0.12)
  2. XAI Ablation: Compare DA-A(1) vs. DA-IG to see if gradient method yields higher alignment than raw attention
  3. Short vs. Long Functions: Filter for functions < 512 tokens vs. truncated functions to test truncation impact

## Open Questions the Paper Calls Out
1. Can the DA metric be integrated as a regularization loss during training to force models to learn semantic vulnerability patterns rather than spurious correlations? The authors explicitly state this as a key future direction, but the optimization dynamics and architectural changes required have not been investigated.

2. How does the 510-token input truncation affect the validity of alignment scores, and would models capable of processing full-context functions yield different results? The authors note this limitation and suggest evaluating detectors able to handle larger inputs as a necessary future step.

3. To what extent does the specific choice of explainability method bias the resulting Detection Alignment score? While multiple methods are tested, the authors acknowledge that evaluating XAI method quality is orthogonal to their work, and the low scores could partially stem from attribution method limitations.

## Limitations
- The paper assumes XAI methods faithfully represent model decision-making, though attention weights are known to be unreliable indicators of feature importance
- Ground truth vulnerable lines derived from patch diffs may not represent the minimal or most relevant subset of code, potentially penalizing models that attend to semantically important but non-annotated lines
- The 510-token truncation affects a significant portion of vulnerable samples, potentially missing relevant context or vulnerable lines

## Confidence
- **High Confidence:** The DA metric formulation (fuzzy Jaccard index) is mathematically sound and the experimental methodology is rigorous
- **Medium Confidence:** The interpretation that low DA scores indicate spurious correlation is reasonable but requires further validation
- **Low Confidence:** The specific attribution of model failures to dataset artifacts without controlling for model architecture differences

## Next Checks
1. **XAI Method Ablation:** Recompute DA scores using multiple attribution methods (Attention, LIME, SHAP) and compare consistency to test if different methods yield divergent scores for the same predictions.

2. **Controlled Spurious Correlation Test:** Construct synthetic datasets where vulnerabilities are artificially correlated with specific code patterns (e.g., always preceded by `printf`), train models, and measure whether DA scores drop while F1 remains high.

3. **Semantic vs. Syntactic Localization:** For a subset of samples, manually annotate not just vulnerable lines but also semantically relevant lines (e.g., variable declarations, control flow), then recompute DA using this expanded ground truth to test whether models attend to broader vulnerability context.