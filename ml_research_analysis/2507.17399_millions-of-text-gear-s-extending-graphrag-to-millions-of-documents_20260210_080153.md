---
ver: rpa2
title: 'Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents'
arxiv_id: '2507.17399'
source_url: https://arxiv.org/abs/2507.17399
tags:
- passages
- triples
- question
- what
- film
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates scaling graph-based retrieval-augmented
  generation (GraphRAG) to datasets with millions of documents, motivated by the high
  computational cost of traditional LLM-based triple extraction. The authors adapt
  the GeAR framework by introducing an online method that aligns passages with external
  Wikidata triples using sparse retrieval, enabling multi-step reasoning without explicit
  offline triple-passage associations.
---

# Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents

## Quick Facts
- arXiv ID: 2507.17399
- Source URL: https://arxiv.org/abs/2507.17399
- Reference count: 16
- Primary result: Scaled GraphRAG to millions of documents using runtime pseudo-alignment, achieving correctness 0.876 and faithfulness 0.529 on FineWeb-10BT

## Executive Summary
This paper presents a scalable GraphRAG architecture that extends graph-based retrieval-augmented generation to datasets with millions of documents by eliminating the expensive offline triple extraction step. The system adapts the GeAR framework through runtime pseudo-alignment, linking retrieved passages to Wikidata triples during query processing rather than pre-extracting triples from the corpus. In the SIGIR 2025 LiveRAG Challenge, this approach achieved strong correctness scores while highlighting the ongoing challenge of aligning unstructured text with structured graph data in shared semantic spaces.

## Method Summary
The authors extend GraphRAG to millions of documents by replacing offline triple extraction with runtime pseudo-alignment to Wikidata. The system performs hybrid retrieval (dense + sparse) on the corpus, extracts "proximal triples" from retrieved passages using a small LLM, then links these triples to Wikidata using sparse similarity. Graph expansion follows reasoning paths in the KG, and passages are re-retrieved and fused using Reciprocal Rank Fusion. An agentic loop with filtering manages noise and controls reasoning depth, enabling multi-hop reasoning without explicit offline triple-passage associations.

## Key Results
- Achieved correctness score of 0.876 and faithfulness score of 0.529 on FineWeb-10BT corpus in SIGIR 2025 LiveRAG Challenge
- Demonstrated feasibility of scaling GraphRAG to millions of documents without offline triple extraction
- Revealed semantic alignment challenges between text and graph data, with topic drift examples like "Pacific Geoducks" linking to "Pacific Oysters"

## Why This Works (Mechanism)

### Mechanism 1: Runtime Pseudo-Alignment of Graph and Corpus
The system scales GraphRAG by using a pre-existing static knowledge graph (Wikidata) instead of extracting triples from the FineWeb corpus beforehand. During retrieval, it pseudo-aligns retrieved passages to Wikidata triples through sparse similarity matching, enabling multi-step reasoning without offline triple-passage associations.

### Mechanism 2: Triple-Guided Multi-Hop Expansion
After initial triples are linked to Wikidata, the system performs diverse triple beam search to traverse the KG and find sequential triples. These expanded triples are converted back into text queries to retrieve distant passages, bridging evidence gaps that pure lexical search would miss.

### Mechanism 3: Agentic Memory and Filtering
The architecture maintains memory of triples and passages through an agentic loop, with a final filtering step using accumulated triples to re-rank and discard irrelevant passages. This corrects for the low precision of the pseudo-alignment strategy before generation.

## Foundational Learning

- **Reciprocal Rank Fusion (RRF)**: Needed to combine sparse and dense search results for initial passage retrieval; Quick check: How does RRF handle cases where a document is ranked high by dense search but low by sparse search?
- **Knowledge Graph Alignment / Linking**: Needed to understand the core innovation of pseudo-alignment; Quick check: What is the difference between linking a text span to an entity ID versus extracting a new triple from text?
- **Beam Search in Graphs**: Needed to understand diverse triple beam search for exploring reasoning chains; Quick check: Why would "diverse" beam search be preferred over standard beam search when traversing a knowledge graph for QA?

## Architecture Onboarding

- **Component map**: Hybrid Search on FineWeb -> Top-k Chunks -> Reader LLM -> Proximal Triples -> Sparse Linking -> Wikidata Triples -> Graph Expansion -> Expanded Triples -> Back-Retrieval -> RRF Fusion + LLM Filtering -> Final Answer
- **Critical path**: 1) Hybrid search retrieves initial passages; 2) LLM extracts proximal triples; 3) Sparse linking maps to Wikidata; 4) Graph expansion finds reasoning paths; 5) Back-retrieval gets distant passages; 6) RRF fusion and filtering produce final answer
- **Design tradeoffs**: Sparsity vs. precision - sparse retrieval chosen for efficiency despite misalignment issues; Model size - Falcon-3B-Instruct used for knowledge synchronization for speed but limits triple complexity extraction
- **Failure signatures**: Topic drift (e.g., "Hot tub" drifting to "Geology" due to word matching); Low faithfulness (~0.53) indicating unsupported answers; Noisy graph expansion retrieving irrelevant documents
- **First 3 experiments**: 1) Replace sparse linker with dense bi-encoder to measure topic drift reduction; 2) Measure hit rate of proximal triples finding matches in Wikidata; 3) Compare correctness for n=1 vs n=2 to validate graph expansion benefit

## Open Questions the Paper Calls Out

- **Open Question 1**: How can asymmetric semantic models be designed to operate within a shared semantic space to effectively align unstructured passages with structured graph triples? [explicit] The conclusion explicitly calls for improved asymmetric semantic models capable of operating within a shared semantic space for both graph data and text.
- **Open Question 2**: Can "proximal triples" extracted at runtime serve as reliable proxies for "real" triples in a knowledge graph index without compromising the integrity of reasoning chains? [inferred] The Discussion section states that the case study challenges a key assumption that proximal triples can reliably serve as proxies.
- **Open Question 3**: Does the filtering mechanism sufficiently mitigate the noise introduced by "loose online alignment" without discarding critical evidence for multi-hop reasoning? [inferred] Section 4.3 introduces filtering to handle noisy outputs, yet the Results section reports a faithfulness score of only 0.529.

## Limitations
- Sparse retrieval for triple-passage alignment introduces semantic drift and topic misalignment (e.g., "Pacific Geoducks" linking to "Pacific Oysters")
- Low faithfulness score (0.529) reveals significant reliability concerns with generated answers
- Architecture depends heavily on external knowledge graph completeness, potentially missing domain-specific knowledge

## Confidence
- **High Confidence**: Runtime pseudo-alignment mechanism is technically sound and well-documented
- **Medium Confidence**: Reported performance metrics are verifiable, but component contributions to scores are not isolated
- **Low Confidence**: Impact of specific hyperparameter choices on performance remains unclear without ablation studies

## Next Checks
1. **Alignment Quality Assessment**: Measure semantic drift rate by tracking how often sparse-retrieved Wikidata triples share the same topic as source passages, using automated metrics and human evaluation
2. **Knowledge Graph Coverage Analysis**: Quantify hit rate of triples extracted from FineWeb passages successfully finding semantically equivalent matches in Wikidata
3. **Faithfulness Improvement Experiment**: Implement and evaluate a dense cross-encoder for the alignment step to measure computational cost vs. faithfulness improvement tradeoff