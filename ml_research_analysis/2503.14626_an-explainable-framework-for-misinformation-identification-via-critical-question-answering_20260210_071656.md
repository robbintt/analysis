---
ver: rpa2
title: An Explainable Framework for Misinformation Identification via Critical Question
  Answering
arxiv_id: '2503.14626'
source_url: https://arxiv.org/abs/2503.14626
tags:
- argument
- critical
- argumentation
- scheme
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an explainable framework for misinformation
  detection using argumentation schemes and critical questions. The authors create
  and release NLAS-CQ, a corpus of 3,566 textbook-like argumentation schemes with
  4,687 critical question annotations.
---

# An Explainable Framework for Misinformation Identification via Critical Question Answering

## Quick Facts
- arXiv ID: 2503.14626
- Source URL: https://arxiv.org/abs/2503.14626
- Authors: Ramon Ruiz-Dolz; John Lawrence
- Reference count: 20
- Key outcome: Framework combines argumentation scheme classification with critical question answering, achieving 99.2% F1 on textbook arguments and 73.9% with context

## Executive Summary
This paper introduces an explainable framework for misinformation detection that uses argumentation schemes and critical questions to provide transparent explanations. The authors create NLAS-CQ, a corpus of 3,566 textbook-like argumentation schemes with 4,687 critical question annotations. Their framework combines argumentation scheme classification with critical question answering to provide transparent explanations for misinformation detection. Experimental results show strong performance in scheme classification (F1-scores above 99%) on textbook-like arguments, while the critical question answering task achieves F1-scores of 70.7% without and 73.9% with contextual information. The work addresses the explainability gap in existing misinformation detection systems and establishes a new benchmark for the critical question answering task.

## Method Summary
The framework implements a two-stage pipeline: first classifying arguments into predefined argumentation schemes using RoBERTa-large fine-tuning, then validating arguments by answering critical questions associated with each scheme. The ASC module maps natural language arguments to ~20 stereotyped reasoning patterns, while the CQA module answers binary questions about each argument's validity. Context can be provided to improve answer accuracy. The NLAS-CQ corpus contains textbook-like arguments and critical question answers, enabling fine-tuning on structured argumentation patterns.

## Key Results
- ASC module achieves 99.2% F1 on textbook-like arguments using RoBERTa-large fine-tuning
- CQA module achieves 70.7% F1 without context and 73.9% with context
- Performance drops significantly on real dialogue arguments (44.8% F1) due to enthymematic structures
- Subjective questions (Waste CQ2, Example CQ4) show near-random performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured argumentation patterns enable higher classification accuracy than free-form sequence modeling.
- Mechanism: The ASC module maps natural language arguments to predefined scheme templates by maximizing P(c|x), constraining the output space to ~20 theoretically grounded classes.
- Core assumption: Arguments follow recognizable structural patterns that persist across topics and stances.
- Evidence anchors:
  - [abstract] "combines classification with question answering to analyse arguments in search of misinformation"
  - [section] Table 2 shows RoBERTa-ASC achieving 99.2% F1 on textbook-like arguments
  - [corpus] Weak direct evidence; TraceRAG (FMR=0.47) uses LLM-based explainable detection but for malware, not argumentation
- Break condition: Performance degrades sharply on enthymematic (incomplete) dialogue arguments (F1 drops to 44.8% when grouped into 8 scheme classes).

### Mechanism 2
- Claim: Critical questions provide explainable validity assessment by surfacing specific failure modes.
- Mechanism: Once a scheme is identified, the framework retrieves its associated critical questions. The CQA module answers each binary question, exposing which premise or inference step is weak.
- Core assumption: Answers to critical questions correlate with argument validity; "bad" answers indicate potential misinformation.
- Evidence anchors:
  - [abstract] "provides the explanations in form of critical questions to the human user"
  - [section] "Being unable to provide a good answer for all the critical questions will result in a potential piece of misinformation"
  - [corpus] ESGBench (FMR=0.53) demonstrates explainable QA benchmarks but in ESG domain, not argumentation
- Break condition: Subjective questions (e.g., Argument from Waste CQ2 on cost-benefit reassessment) show near-random performance; correct answers do not prevail over incorrect ones.

### Mechanism 3
- Claim: Contextual information improves answer accuracy by providing evidence external to the argument.
- Mechanism: The CQA module conditions on both the argument and retrieved context: P(a|q, x, context). Context bridges gaps when arguments lack sufficient detail for answering questions.
- Core assumption: High-quality contextual information is available and retrievable.
- Evidence anchors:
  - [abstract] "70.7% without and 73.9% with contextual information"
  - [section] Figure 3 shows context improves negative-answer accuracy by >2% and affirmative by ~5%
  - [corpus] RAIDX (FMR=0.50) uses retrieval-augmented generation for detection but doesn't evaluate context contribution
- Break condition: Context retrieval itself is not addressed in this paper; assumes gold-standard context. Questions requiring external knowledge (e.g., Argument from Analogy CQ1 on relevant differences) remain error-prone even with context.

## Foundational Learning

- Concept: **Argumentation Schemes (Walton et al., 2008)**
  - Why needed here: The entire framework depends on mapping arguments to ~60 stereotyped reasoning patterns. Without understanding scheme structure (major premise, minor premise, conclusion, variables), you cannot interpret ASC outputs or implement CQ retrieval.
  - Quick check question: Given "Dr. Smith says vaccines are safe because she's an expert immunologist," which scheme applies and what are its three critical questions?

- Concept: **Enthymematic Arguments**
  - Why needed here: Real-world arguments omit premises, causing the 54-point F1 drop from textbook to dialogue data. Understanding enthymemes explains why structural pattern matching fails in practice.
  - Quick check question: Why does the "Argument from Waste" dialogue example ("We need to embed successes") fail to match its textbook template?

- Concept: **Binary Question Answering with BERT-family Models**
  - Why needed here: CQA is implemented as binary classification fine-tuning on RoBERTa-large. Understanding [CLS] token pooling, fine-tuning hyperparameters (lr=1e-5, weight_decay=0.1), and class imbalance informs debugging.
  - Quick check question: Why might a binary QA model show asymmetric errors (answering "yes" to 53.3% of negative ground-truth questions)?

## Architecture Onboarding

- Component map:
  Input Argument → [ASC: RoBERTa-large classifier] → Scheme ID → [Critical Question DB lookup] → CQ set {q1, q2, ..., qn} → (Argument + Context) + CQ → [CQA: RoBERTa-large binary QA] → Answers {a1, a2, ..., an} → Answer Analysis → Misinformation flag + Explanation

- Critical path: ASC accuracy is the gating factor. If scheme classification fails, wrong CQs are retrieved, making CQA outputs meaningless for that argument.

- Design tradeoffs:
  - Textbook vs. real arguments: Training on NLAS-CQ (textbook) gives 99% F1 but transfers poorly to dialogue (45% F1). Fine-tuning on small dialogue data (65 samples) recovers to 45% but requires labeled real-world data.
  - Generative vs. fine-tuned QA: GPT-4 requires no training but shows strong positive bias (53.3% false "yes" on negatives). Fine-tuned RoBERTa-CQA is more balanced but requires annotated CQ data.
  - Context dependency: +3.2% F1 gain from context, but requires retrieval pipeline not addressed in this paper.

- Failure signatures:
  - ASC: Enthymematic arguments missing premises (Table 3 validation drop)
  - CQA: Questions requiring external knowledge (Analogy CQ1, Sign CQ2) or subjective judgment (Waste CQ2, Example CQ4)
  - Generative models: Systematic "yes" bias (GPT-4: 53.3% of negative samples answered affirmatively)

- First 3 experiments:
  1. **ASC domain transfer test**: Train on NLAS-CQ, test on QT30 dialogue subset. Measure per-scheme accuracy to identify which scheme structures generalize worst.
  2. **CQA error stratification**: Run RoBERTa-CQA[CI] on test set, aggregate errors by question type. Confirm whether context-dependent questions (CQ1s across schemes) improve more than subjective questions.
  3. **Context ablation**: For 100 test samples, manually replace annotator-provided context with retrieved web search results. Measure F1 degradation to quantify retrieval quality requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be adapted to effectively handle the high subjectivity inherent in specific critical questions, such as Argument from Waste CQ2?
- Basis in paper: [explicit] The Conclusion explicitly lists "exploring effective ways of dealing with question subjectivity" as a direction for future work, and the Error Analysis identifies questions like Argument from Example CQ4 and Argument from Waste CQ2 as "highly subjective."
- Why unresolved: The current binary classification approach struggles when valid answers depend heavily on subjective interpretation rather than objective facts.
- What evidence would resolve it: A modified model architecture or training approach that achieves significantly higher F1-scores on the specific subjective questions identified in the error analysis.

### Open Question 2
- Question: To what extent does the integration of an automated information retrieval module enhance the performance of the Critical Question Answering (CQA) task?
- Basis in paper: [explicit] The Conclusion states that future work includes "improving the information retrieval process that complements our natural language input," noting that the current work assumes contextual information is pre-retrieved.
- Why unresolved: The authors verified the utility of contextual information manually but did not implement or test an automated retrieval system, leaving the pipeline's end-to-end robustness unconfirmed.
- What evidence would resolve it: Experimental results comparing CQA performance using manually provided context versus context retrieved automatically by an implemented IR module.

### Open Question 3
- Question: How can the Argumentation Scheme Classification (ASC) models be optimized to maintain high performance when processing incomplete, enthymematic arguments typical of natural human dialogue?
- Basis in paper: [inferred] The Limitations section highlights that while ASC performance is high on "textbook-like" arguments, the task becomes "significantly harder" with enthymematic arguments from real dialogues, as evidenced by the performance drop in Table 3.
- Why unresolved: The structural patterns the models rely on for classification are often missing or obscured in natural dialogue, and the current transfer learning approach only partially mitigates this.
- What evidence would resolve it: A training methodology or architecture that yields F1-scores for dialogue arguments comparable to the >99% scores achieved on the textbook-like NLAS-CQ corpus.

## Limitations
- **Domain transfer challenge**: Framework achieves 99.2% F1 on textbook arguments but drops to 44.8% on real dialogue data, suggesting severe generalization limits for real-world misinformation detection.
- **Subjective question performance**: Critical questions requiring value judgments show near-random performance, limiting the framework's ability to validate arguments in value-laden domains.
- **Context dependency**: The +3.2% F1 gain from context is contingent on retrieval quality, which is not evaluated and could erase gains in real-world applications.

## Confidence
- **High Confidence** (95%+): The framework's mechanism is sound - argumentation schemes provide structured classification, and critical questions offer transparent validity assessment. The textbook ASC results (99.2% F1) are robust.
- **Medium Confidence** (70-95%): The +3.2% F1 gain from context is real but contingent on retrieval quality. CQA performance on objective questions is reasonable but not exceptional.
- **Low Confidence** (below 70%): Generalization to real-world misinformation is limited by enthymematic argument handling. Subjective question performance suggests the framework cannot fully explain validity in value-laden domains.

## Next Checks
1. **Enthymeme Recovery Test**: Fine-tune ASC on the 65 dialogue samples in NLAS-CQ, then test on unseen dialogue. Measure per-scheme accuracy to identify which structures transfer best and quantify the minimum real-data requirement for robust detection.

2. **Subjective Question Isolation**: Filter CQA test set to include only subjective questions (Waste CQ2, Example CQ4). Measure performance separately to quantify the framework's limitations in value-based reasoning and determine if these questions should be excluded from validity assessment.

3. **Context Quality Sensitivity**: For 100 test samples, replace gold-standard context with top-5 search results from a commercial search API. Measure F1 degradation to establish the retrieval quality threshold required to maintain the 3.2% performance gain.