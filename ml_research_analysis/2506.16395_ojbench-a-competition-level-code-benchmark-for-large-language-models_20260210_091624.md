---
ver: rpa2
title: 'OJBench: A Competition Level Code Benchmark For Large Language Models'
arxiv_id: '2506.16395'
source_url: https://arxiv.org/abs/2506.16395
tags:
- code
- arxiv
- ojbench
- problems
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OJBench is a competition-level code reasoning benchmark introduced
  to evaluate the reasoning capabilities of large language models (LLMs) in complex
  algorithmic tasks. The benchmark comprises 232 problems from the National Olympiad
  in Informatics (NOI) and the International Collegiate Programming Contest (ICPC),
  categorized into Easy, Medium, and Hard difficulty levels.
---

# OJBench: A Competition Level Code Benchmark For Large Language Models

## Quick Facts
- arXiv ID: 2506.16395
- Source URL: https://arxiv.org/abs/2506.16395
- Reference count: 40
- OJBench evaluates LLMs on 232 competition-level programming problems from NOI and ICPC using Pass@k metrics

## Executive Summary
OJBench is a competition-level code reasoning benchmark introduced to evaluate the reasoning capabilities of large language models (LLMs) in complex algorithmic tasks. The benchmark comprises 232 problems from the National Olympiad in Informatics (NOI) and the International Collegiate Programming Contest (ICPC), categorized into Easy, Medium, and Hard difficulty levels. It supports dual-language assessment in Python and CPP, enabling a comprehensive evaluation of models' performance.

Experiments on 37 models, including reasoning-oriented and non-reasoning-oriented, open-source and closed-source models, reveal that even state-of-the-art reasoning-oriented models struggle with highly challenging competition-level problems. For instance, models like o4-mini and Gemini-2.5-pro-exp-03-25 achieve pass@1 rates of 33.30% and 38.91% in Python, respectively, highlighting the significant challenges models face in competitive-level code reasoning. Additionally, models using CPP generally outperform those using Python, aligning with human experience in competitive programming.

## Method Summary
OJBench evaluates LLMs on 232 competition-level programming problems using Pass@k metrics with 8 candidate solutions per problem. Problems span Easy/Medium/Hard difficulty from NOI (159 problems) and ICPC (73 problems). Models generate solutions in both Python and CPP, with reasoning models using max_tokens=64k. The benchmark uses complete official test suites (avg 31.81 tests per problem) and supports iterative refinement through error feedback. Evaluation is performed using official judge implementations to ensure accuracy.

## Key Results
- State-of-the-art reasoning-oriented models achieve only 33-39% pass@1 rates on Python implementations
- CPP implementations consistently outperform Python across all reasoning-oriented models
- Models can improve performance through error feedback refinement, particularly for compile errors, but struggle with time limit exceeded errors
- Easy problems show clear discrimination between reasoning and non-reasoning models, while Hard problems remain nearly unsolvable for non-reasoning models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended chain-of-thought reasoning improves competitive programming performance by enabling deeper algorithmic exploration before code generation.
- Mechanism: Reasoning-oriented models generate extended internal deliberation (up to 64k tokens) that allows iterative problem decomposition, hypothesis testing, and solution refinement before producing final code output.
- Core assumption: The correlation between reasoning-orientation and higher pass rates implies causal benefit from extended deliberation, though this may partially reflect training data quality or scale confounds.
- Evidence anchors:
  - [abstract] "reasoning-oriented models significantly outperform non-reasoning-oriented models"
  - [section 3.2] "models designed for reasoning tasks consistently outperform non-reasoning-oriented models, highlighting the significant advantages of post-training methods like reinforcement learning and distillation"
  - [corpus] Related work CodeElo confirms similar patterns with OpenAI o1/o3 models on competition-level tasks
- Break condition: If extended deliberation time budget is reduced below ~16k tokens for hard problems, performance degradation expected; if non-reasoning models receive equivalent inference compute via sampling, gap may narrow

### Mechanism 2
- Claim: CPP outperforms Python on competition-level problems due to runtime efficiency enabling solutions that meet strict time complexity constraints.
- Mechanism: Competition problems impose time limits; CPP's compiled nature and lower-level memory management allow more algorithms to pass within constraints, whereas equivalent Python implementations may timeout even with correct algorithmic logic.
- Core assumption: The CPP advantage reflects fundamental runtime characteristics rather than training data imbalance, though training corpus composition may contribute.
- Evidence anchors:
  - [abstract] "CPP generally yields better performance than Python for most reasoning-oriented models"
  - [section 4.2] "for advanced reasoning-oriented models, such as o4-mini, o1-20241217, and Gemini-2.5-pro-exp-03-25, using CPP yields significantly better performance... CPP is inherently a high-performance programming language"
  - [corpus] CodeElo paper notes similar CPP preference among human competitive programmers
- Break condition: If problems have relaxed time limits or primarily test correctness over efficiency, Python-CPP gap should narrow; if training data heavily favors Python solutions, some models may show reverse pattern

### Mechanism 3
- Claim: Error feedback enables iterative refinement that improves solution pass rates, particularly for surface-level errors rather than algorithmic inefficiencies.
- Mechanism: Models receive execution feedback (compile errors, wrong answers, timeouts) and regenerate solutions conditioned on error signals; compile errors are easily corrected while time-limit-exceeded errors require algorithmic redesign that models struggle to perform.
- Core assumption: The refinement improvement reflects genuine error correction capability rather than mere resampling luck.
- Evidence anchors:
  - [abstract] "models can leverage error feedback to refine their solutions, improving overall performance"
  - [section 4.3] "through each round of refinement, the model can continuously improve the pass rate... CE errors decreased most significantly... the model struggled to address TLE errors"
  - [corpus] ICPC-Eval paper explores similar feedback mechanisms in contest environments
- Break condition: If refinement rounds exceed 3-4 iterations without algorithmic hints, TLE correction plateaus; if feedback is limited to binary pass/fail without error type details, refinement effectiveness decreases

## Foundational Learning

- Concept: **Pass@k evaluation metric**
  - Why needed here: Understanding that single-attempt evaluation underestimates model capability; Pass@8 scores are 10-15 absolute points higher than Pass@1 across models
  - Quick check question: If a model achieves 25% Pass@1 and 40% Pass@8, what does this indicate about solution diversity?

- Concept: **Test case coverage validity**
  - Why needed here: OJBench uses complete official test suites (avg 31.81 tests per problem) because limited test cases produce false positives that overstate model performance
  - Quick check question: Why might a solution pass 5 test cases but fail the full test suite on competition problems?

- Concept: **Competition difficulty calibration**
  - Why needed here: Problems span Easy/Medium/Hard with distinct model performance profiles; Hard problems show near-zero pass rates for non-reasoning models, validating benchmark discrimination
  - Quick check question: If a model achieves 80% on Easy but 5% on Hard problems, what does this suggest about its reasoning capabilities?

## Architecture Onboarding

- Component map:
  - Problem collection from NOI (159) + ICPC (73) → Test case validation → Difficulty labeling → Translation (GPT-4o + manual) → Final 232 problems
  - Dual-language code execution (Python/CPP) → Full test suite judge → Pass@k calculation with n=8 samples
  - Initial solution → Execution feedback → Conditional regeneration → Re-evaluation

- Critical path:
  1. Ensure test case completeness (filter problems missing official judges)
  2. Set max_tokens=64k for reasoning models to allow extended deliberation
  3. Execute full test suite (not subset) to avoid false positives
  4. Support both Python and CPP to capture language-dependent performance

- Design tradeoffs:
  - **Breadth vs. depth**: 232 problems from top-tier competitions vs. broader coverage of easier problems; chosen for maximum discrimination of frontier models
  - **Language support**: Python most common for LLMs but CPP better for competition; dual-language evaluation increases cost but reveals capability differences
  - **Special judge exclusion**: Problems requiring custom validators were excluded, trading completeness for evaluation reliability

- Failure signatures:
  - **TLE-dominated failures**: Model produces algorithmically correct but inefficient solutions; indicates reasoning capable of correctness but not complexity optimization
  - **Solution-reasoning mismatch**: Case analysis shows model generates reasoning path that doesn't match implemented code (observed in Qwen3-235B-A22B)
  - **Repetitive deliberation**: Extended CoT contains problem restatement rather than deeper analysis without progress

- First 3 experiments:
  1. **Baseline establishment**: Run Pass@1 and Pass@8 evaluation on 3-5 models across difficulty tiers to confirm benchmark discriminates between reasoning and non-reasoning architectures
  2. **Language comparison**: Evaluate same model in both Python and CPP on full benchmark to quantify language-dependent performance gap and identify problem types where gap is largest
  3. **Refinement ceiling**: Implement 3-round error feedback loop on failed solutions, categorizing error types (CE/WA/TLE) to measure which error categories models can self-correct

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model refinement mechanisms be improved to successfully resolve Time Limit Exceeded (TLE) errors?
- Basis in paper: [explicit] Section 4.3 states models struggle to fix TLEs because they require designing efficient algorithms, which remains a significant challenge.
- Why unresolved: Current refinement strategies work for simple errors (e.g., Compile Errors) but fail for complexity optimization.
- What evidence would resolve it: A mechanism where models consistently optimize algorithmic complexity to pass timed test cases during refinement.

### Open Question 2
- Question: To what extent does training data composition dictate the performance gap between Python and CPP in reasoning models?
- Basis in paper: [inferred] Section 4.2 observes Qwen models (distilled on Python) favor Python while others favor CPP, attributing this to training data without verification.
- Why unresolved: The causal link between specific fine-tuning corpora and language proficiency is speculative.
- What evidence would resolve it: Ablation studies correlating training data language ratios with OJBench performance gaps.

### Open Question 3
- Question: How can the tendency of reasoning models to engage in repetitive restatement rather than deep analysis be mitigated?
- Basis in paper: [explicit] Section 4.4 notes models often repeat problem requirements instead of assessing feasibility or deepening the solution strategy.
- Why unresolved: It is unclear if this behavior is an artifact of Chain-of-Thought training or a fundamental limitation.
- What evidence would resolve it: Training methods that reduce repetition rates and increase feasibility assessment in traces.

## Limitations

- Benchmark evaluation infrastructure completeness: Repository and evaluation harness were noted as "coming soon" in the paper, creating uncertainty about exact implementation details for reproduction
- Test case availability: Full test suites and judge implementations were not available at time of writing, preventing immediate validation
- Special judge exclusion: Problems requiring custom validators were excluded, limiting benchmark coverage of competition problem types

## Confidence

- **High confidence**: CPP vs Python performance gap (CPP advantage is well-established in competitive programming); error feedback mechanism effectiveness for surface errors (compile errors, wrong answers); benchmark difficulty calibration (clear discrimination between reasoning and non-reasoning models across difficulty tiers)
- **Medium confidence**: Extended chain-of-thought reasoning benefit (mechanistic explanation supported but confounded by model scale/training data quality); TLE error correction limitations (observed but could improve with algorithmic hints); refinement ceiling patterns (based on limited iterations in experiments)

## Next Checks

1. Benchmark infrastructure verification: Once repository is available, validate that test cases are complete (average 31.81 per problem) and that the judge correctly implements pass@k metrics with full test suite execution
2. Language performance gap analysis: Replicate the CPP vs Python comparison on a subset of problems to quantify the runtime efficiency advantage and identify which problem types show the largest language-dependent gaps
3. Error feedback mechanism testing: Implement the 3-round refinement loop and track error-type distributions to confirm that compile errors decrease while TLE errors persist, validating the mechanism's effectiveness boundaries