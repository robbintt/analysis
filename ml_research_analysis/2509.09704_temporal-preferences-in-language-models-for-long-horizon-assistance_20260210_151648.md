---
ver: rpa2
title: Temporal Preferences in Language Models for Long-Horizon Assistance
arxiv_id: '2509.09704'
source_url: https://arxiv.org/abs/2509.09704
tags:
- decision
- human
- making
- time
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether language models exhibit future-oriented
  preferences and can be manipulated to adopt different temporal frames in intertemporal
  choice tasks. Using adapted human experimental protocols, the authors introduce
  the "Manipulability of Time Orientation" (MTO) metric to assess how much an LM's
  time preference shifts between future- and present-oriented prompts.
---

# Temporal Preferences in Language Models for Long-Horizon Assistance

## Quick Facts
- arXiv ID: 2509.09704
- Source URL: https://arxiv.org/abs/2509.09704
- Reference count: 0
- Primary result: Reasoning models show higher manipulability of time orientation (MTO) than non-reasoning models, adopting future-oriented preferences when prompted as AI decision-makers

## Executive Summary
This paper investigates whether language models exhibit future-oriented preferences and can be manipulated to adopt different temporal frames in intertemporal choice tasks. Using adapted human experimental protocols, the authors introduce the "Manipulability of Time Orientation" (MTO) metric to assess how much an LM's time preference shifts between future- and present-oriented prompts. Reasoning-focused models like DeepSeek-Reasoner and grok-3-mini demonstrated higher MTO scores, successfully shifting toward later options under future-oriented prompts. However, models struggled to personalize decisions across identities or geographies. Notably, models that correctly understood time orientation adopted a future-oriented stance when prompted as AI decision-makers. The study highlights the potential for AI assistants to align with heterogeneous, long-horizon goals while outlining research needs in personalized contextual calibration and socially aware deployment.

## Method Summary
The study employs an 8-item binary staircase protocol where models choose between smaller-sooner (SS) and larger-later (LL) rewards ($0.20-$0.13 in 1 week vs. fixed $0.20 in 4 weeks). Context prompts from 12 categories (identity, geography, crisis, role, manipulation, etc.) are prepended to elicit responses. The MTO metric calculates the difference between imputed discount factors under future-oriented vs. present-oriented prompts. ~500 API calls per model across ~5M total tokens test 12 language models including reasoning-focused (DeepSeek-Reasoner, grok-3-mini) and non-reasoning models (GPT-4o, Llama, Gemini).

## Key Results
- Reasoning models (DeepSeek-Reasoner, grok-3-mini) achieved MTO scores approaching the maximum benchmark (0.33), successfully shifting toward later options under future-oriented prompts
- Non-reasoning models exhibited either incorrect understanding of temporal orientation or failed to distinguish between future/present frames
- Models that correctly understood time orientation internalized future preferences when cast as AI decision-makers
- Present-bias amplification under crisis framing mirrored human urgency-gating behavior
- Models struggled to personalize decisions across identities or geographies, showing null effects for Iran/USA/Europe prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning-capable models exhibit higher temporal preference manipulability than non-reasoning models.
- **Mechanism:** Models with explicit reasoning architectures process temporal framing prompts through extended computation chains, enabling coherent preference shifts. Non-reasoning models either produce inconsistent outputs or fail to distinguish between future/present frames.
- **Core assumption:** Reasoning traces allow models to maintain coherent internal representations of abstract temporal concepts across prompt variations.
- **Evidence anchors:**
  - [abstract] "reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini) choose later options under future-oriented prompts"
  - [section 4, p.12] Figure 1 shows DeepSeek-reasoner and grok-3-mini approach the maximum MTO benchmark (0.33), while "models, including GPT-4o, exhibit an even incorrect understanding of future/present orientation"
  - [corpus] Weak direct support; related work on LM homogeneity (arxiv:2510.22954) suggests architectural constraints on output diversity but doesn't address reasoning-specific effects
- **Break condition:** If a non-reasoning model achieves comparable MTO through alternative mechanisms (e.g., specialized fine-tuning), the reasoning-specific claim weakens.

### Mechanism 2
- **Claim:** Models that correctly parse time-orientation prompts internalize future-oriented preferences when cast as AI decision-makers.
- **Mechanism:** When prompted with "act as an AI agent," models that understand temporal framing apply their learned future-oriented interpretation to their own decision role, rather than defaulting to present bias.
- **Core assumption:** The "AI decision-maker" persona activates a normative script for rational, long-horizon planning that overrides default short-term biases.
- **Evidence anchors:**
  - [abstract] "models that correctly reason about time orientation internalize a future orientation for themselves as AI decision makers"
  - [section 4.1, p.17] "A notable insight from this study is that LMs capable of understanding future/present orientation tend to internalize this perspective, thereby considering themselves future-oriented"
  - [corpus] No direct corpus support for AI self-concept and temporal preference linkage
- **Break condition:** If models with high MTO scores do not show future orientation under the AI persona, the internalization mechanism is unsupported.

### Mechanism 3
- **Claim:** Present-bias amplification under crisis framing mirrors human urgency-gating behavior.
- **Mechanism:** Crisis prompts trigger a learned association between uncertainty and immediate reward preference, consistent with the Urgency Gating Model from cognitive neuroscience.
- **Core assumption:** LMs have absorbed textual patterns linking crisis vocabulary to present-focused decision contexts from training data.
- **Evidence anchors:**
  - [section 4.1, p.14-15] "in a 'Crisis' scenario, where resources were scarce, the model tended to prioritize more immediate rewards, mirroring human behavior under pressure"
  - [section 4.1, p.16] Cites Cisek et al. (2009) and Thura et al. (2012) on the Urgency Gating Model
  - [corpus] No corpus papers directly address crisis framing effects on LM temporal preferences
- **Break condition:** If crisis framing produces inconsistent or opposite effects across models, the human-mirroring interpretation weakens.

## Foundational Learning

- **Concept: Intertemporal choice and delay discounting**
  - Why needed here: The paper's core methodology uses smaller-sooner vs. larger-later tradeoffs to infer discount rates; understanding how humans typically exhibit present bias is prerequisite to interpreting LM behavior as human-like or anomalous.
  - Quick check question: If a model chooses 19 cents now over 20 cents in 4 weeks but reverses at 15 cents now, what does this imply about its implied weekly discount rate?

- **Concept: Imputed discount factor calculation**
  - Why needed here: MTO metric depends on translating choice switch-points into discount factors; without this, the numerical comparison between conditions is opaque.
  - Quick check question: Given the 8-question staircase protocol, how would you compute the implied discount factor from the switch-point response?

- **Concept: Reasoning model architectures (chain-of-thought, extended inference)**
  - Why needed here: The paper's central finding distinguishes reasoning from non-reasoning models; understanding what "reasoning" means architecturally (e.g., DeepSeek-Reasoner's extended computation, grok-3-mini's lightweight reasoning traces) is essential for replication.
  - Quick check question: What is the operational difference between a model that "thinks step by step" via prompting versus one with a native reasoning architecture?

## Architecture Onboarding

- **Component map:** Prompt generator -> Choice elicitation (8-question protocol) -> Discount factor imputer -> MTO calculator -> Analysis layer
- **Critical path:**
  1. Validate prompt templates against Table 1 specifications
  2. Confirm API response parsing extracts only the numeric switch-point
  3. Verify discount factor formula matches paper's implied calculation (maximum MTO = 0.95 - 0.62 = 0.33)
  4. Cross-check that reasoning models (DeepSeek-Reasoner, grok-3-mini) show MTO > 0.15 threshold, non-reasoning models show lower or inverted responses
- **Design tradeoffs:**
  - Token budget vs. depth: DeepSeek-Reasoner required 48+ hours for ~500 requests; grok-3-mini was faster but may have lower reasoning depth
  - Prompt complexity vs. interpretability: Multi-factor prompts increase ecological validity but complicate causal attribution
  - Single-turn vs. multi-turn: Current design uses single elicitation; multi-turn could reveal consistency but increases cost
- **Failure signatures:**
  - Models returning identical responses across all conditions (e.g., gemma, grok-3 base) indicate prompt insensitivity
  - Models showing negative MTO (higher discount rate under future framing) indicate misinterpretation of temporal vocabulary (observed in GPT-4o)
  - Models refusing to choose or stating inability to express preferences indicate alignment/safety filter interference
- **First 3 experiments:**
  1. Replicate MTO calculation for DeepSeek-Reasoner and grok-3-mini on the 8-question protocol with only future/present manipulation prompts; verify MTO scores fall within paper's reported range.
  2. Ablate reasoning traces by comparing grok-3 (non-reasoning) vs. grok-3-mini (lightweight reasoning) under identical prompts; quantify MTO difference attributable to reasoning capability.
  3. Test geographic sensitivity by running Iran/USA/Europe prompts on high-MTO models; confirm paper's finding of null geographic effects and document response variance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs be effectively calibrated to reflect heterogeneous time preferences across diverse cultural and geographic identities?
- **Basis in paper:** [explicit] The authors state that models "struggled to personalize decisions across identities or geographies" and outline a research agenda for "personalized contextual calibration."
- **Why unresolved:** Current models displayed a lack of sensitivity to socio-political contexts, failing to distinguish preferences between users in Iran, the USA, and Europe.
- **What evidence would resolve it:** Successful fine-tuning or prompting strategies that result in statistically significant variance in discount rates when models are conditioned on specific cultural personas.

### Open Question 2
- **Question:** Do temporal preferences in LLMs remain consistent during multi-turn interactions in dynamic, real-world scenarios?
- **Basis in paper:** [explicit] The conclusion suggests that "investigating multi-turn interactions could provide deeper insights into the continuity and adaptability of future-oriented reasoning."
- **Why unresolved:** The current methodology relies on single-turn binary choices, which does not capture how an AI assistant maintains consistency throughout a prolonged planning session.
- **What evidence would resolve it:** A study measuring the stability of the "switching point" in discount rates over a sequence of interdependent decision tasks.

### Open Question 3
- **Question:** To what extent is the high "Manipulability of Time Orientation" (MTO) in reasoning models attributable to architecture versus training data?
- **Basis in paper:** [inferred] The paper notes reasoning-focused models succeed where others fail, but it remains unclear if this is due to the reasoning process itself or training on reasoning-rich content.
- **Why unresolved:** The correlation between "reasoning ability" and successful temporal manipulation is observed, but the causal mechanism is not isolated.
- **What evidence would resolve it:** Comparative analysis of models with reasoning disabled or ablated to determine if the reasoning trace is necessary for high MTO scores.

## Limitations

- The precise formula mapping switch points to discount factors is not specified, creating uncertainty in metric calculation
- The study relies on single-turn responses without replication across trials, limiting statistical robustness
- Observed null effects for identity and geography manipulations may reflect prompt design limitations rather than true model insensitivity
- Potential safety filter interference from models like Gemini and Qwen that refuse to express preferences

## Confidence

- **High confidence:** The core finding that reasoning-capable models (DeepSeek-Reasoner, grok-3-mini) exhibit manipulable future-oriented preferences while non-reasoning models do not
- **Medium confidence:** The claim that crisis framing amplifies present bias in ways that mirror human urgency-gating behavior
- **Low confidence:** The assertion that models which understand time orientation internalize future preferences when cast as AI decision-makers

## Next Checks

1. Replicate the core MTO calculation using DeepSeek-Reasoner and grok-3-mini with the 8-question protocol under future/present manipulation prompts to verify reported scores fall within the claimed range.
2. Compare grok-3 (non-reasoning) vs. grok-3-mini (lightweight reasoning) under identical prompts to quantify MTO differences attributable to reasoning capability.
3. Test geographic sensitivity by running Iran/USA/Europe prompts on high-MTO models to confirm null effects and document response variance.