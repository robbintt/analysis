---
ver: rpa2
title: 'HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation
  in Large Language Models'
arxiv_id: '2509.06596'
source_url: https://arxiv.org/abs/2509.06596
tags:
- attention
- evidence
- decoding
- value
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models

## Quick Facts
- arXiv ID: 2509.06596
- Source URL: https://arxiv.org/abs/2509.06596
- Reference count: 20
- Key outcome: Improved Exact Match and F1 scores across five QA benchmarks

## Executive Summary
HAVE introduces a decoding-time intervention to mitigate hallucination in LLMs by combining head-adaptive gating with value calibration and uncertainty-scaled fusion. The framework dynamically reweights attention heads based on context sensitivity and augments attention with value vector magnitudes to better approximate token contributions to the residual stream. HAVE operates in a single forward pass without model finetuning, achieving significant improvements in factual consistency across multiple QA benchmarks.

## Method Summary
HAVE consists of two main modules: Head-Adaptive Gating (HAG) computes instance-level soft reweighing of attention heads based on context-sensitivity scores, while Value Calibration (VC) masks sink tokens and weights attention by value vector magnitudes to approximate write-back contribution to the residual stream. These modules generate a utilization distribution that fuses with the model's output distribution via an uncertainty-scaled additive residual policy. The framework extracts per-layer per-head attention weights and value vectors from the last attention row during decoding, computes HAG weights, applies VC with sink correction, and combines the resulting utilization distribution with the model distribution using entropy-normalized scaling.

## Key Results
- Significant EM/F1 improvements across HotpotQA, SearchQA, SQuAD, Natural Questions, and NQ-Swap
- HAG and VC modules contribute positively when ablated separately
- Performance gains observed across LLaMA2-7B, LLaMA2-13B, and Mistral-7B models
- Robust to numerical collapse with fallback mechanisms for degenerate cases

## Why This Works (Mechanism)

### Mechanism 1: Head-Adaptive Gating (HAG)
Instance-level reweighting of attention heads improves evidence extraction compared to static or input-agnostic head importance. The mechanism computes context-sensitivity scores per head based on deduplicated attention over the visible context, then applies soft gating via softmax normalization combined with optional base priors and a small floor value to ensure all heads retain nonzero weights for stability. Core assumption: context-sensitive heads contribute more to faithful evidence extraction, and their importance varies across inputs.

### Mechanism 2: Value Calibration (VC)
Augmenting attention with value vector magnitude better approximates token contribution to the residual stream update than raw attention alone. The mechanism masks and renormalizes attention to suppress sink tokens, weights attention by the L2 norm of value vectors, normalizes within each head, and refines with a lightweight estimator producing a multiplicative mask per token. Core assumption: value norm correlates with write-back contribution to the residual stream; sink tokens bias raw attention.

### Mechanism 3: Uncertainty-Scaled Fusion Policy
Fusing model distribution with utilization distribution via uncertainty-scaled additive residual amplifies evidence when the model is uncertain and attenuates it when confident. The mechanism computes normalized entropy of the model distribution, then forms final distribution as additive combination where scaling factor depends on entropy. Core assumption: entropy-normalized scaling appropriately modulates evidence influence without destabilizing the output distribution.

## Foundational Learning

- **Transformer attention and the residual stream**: WHY NEEDED: HAVE reinterprets attention as insufficient for contribution estimation; value vectors write to the residual stream that ultimately determines logits. QUICK CHECK: Can you explain why a token with high attention weight might still have low influence on the next-token prediction?

- **Sink tokens and attention artifacts**: WHY NEEDED: VC explicitly masks sink tokens (special/whitespace) that dominate raw attention but contribute little semantically. QUICK CHECK: What are "attention sinks" and why do they emerge in transformer models?

- **Entropy-based uncertainty scaling**: WHY NEEDED: The fusion policy uses normalized entropy to modulate how strongly utilization evidence shifts the distribution. QUICK CHECK: Why might high entropy indicate that external evidence should be weighted more heavily?

## Architecture Onboarding

- **Component map**: Extract attention weights and value vectors -> Compute HAG weights from context-sensitivity scores -> Apply VC with sink mask and value-norm weighting -> Project to vocabulary and restrict to Top-R -> Fuse with model distribution via entropy-scaled additive rule

- **Critical path**: 1) Extract last-row attention weights and value vectors from all layers/heads at decoding step t; 2) Compute HAG weights w_ℓ,h from context-sensitivity scores; 3) Apply VC: mask sinks, weight by ‖V‖, normalize, aggregate with w_ℓ,h, project to vocabulary, restrict to Top-R; 4) Fuse with model distribution via entropy-scaled additive rule; 5) Trigger fallback to uniform-head Attn×V if degenerate

- **Design tradeoffs**: Soft gating vs. hard Top-K (preserves stability vs. sharper signal); Top-R size (larger incorporates more candidates but dilutes evidence vs. smaller is focused but may miss correct tokens); α fusion scale (moderate balances P_t and U_t vs. too large over-relies on evidence vs. too small yields negligible gains)

- **Failure signatures**: Near-zero utilization mass after Top-R restriction (check if context contains mostly sink tokens or if estimator mask is too aggressive); no improvement over baseline (verify HAG weights are varying across instances; if uniform, context-sensitivity scores may be degenerate); performance drop on short contexts (entropy scaling may underweight evidence when P_t is already sharp)

- **First 3 experiments**: 1) Sanity check: On HotpotQA subset, log HAG weight distributions and U_t mass before/after Top-R restriction to confirm non-degenerate signals; 2) Ablation: Run HAVE with HAG-only and VC-only on HotpotQA to verify both modules contribute positively; 3) Hyperparameter sweep: Vary α ∈ {0.5, 1.0, 2.0} and R ∈ {5, 10, 20} on HotpotQA to identify stable operating regions

## Open Questions the Paper Calls Out

- **Multi-turn dialogue extension**: Can HAVE be effectively extended to interactive or multi-turn dialogue settings where context relevance shifts dynamically across turns? The current framework is limited to single-turn QA benchmarks; dynamic dialogue requires managing evolving context windows and shifting attention patterns not present in evaluated datasets.

- **Synergy with retrieval selection**: What performance gains result from combining HAVE with upstream retrieval selection or downstream confidence calibration methods? The paper evaluates HAVE in isolation using fixed retrieval contexts; it does not analyze how instance-level gating interacts with retrieval pipeline quality or external calibration layers.

- **Generalization to non-QA tasks**: Does HAVE generalize to non-QA generation tasks, such as abstractive summarization or open-ended generation? While claiming to address "Hallucination Mitigation in Large Language Models" generally, experiments are strictly confined to five QA datasets; summarization requires synthesizing broader context which may alter how "value calibration" weights token contributions.

## Limitations

- Lightweight estimator architecture unspecified (features, training data, initialization not provided)
- Sink token definition ambiguous (exact token list or detection rule not standardized)
- Hyperparameter sensitivity high (exact values for α, R, η not provided, could substantially alter performance)
- Limited to QA benchmarks (generalization to other generation tasks untested)

## Confidence

**High Confidence**: General framework architecture is internally consistent; observed EM/F1 improvements represent valid measurements; failure conditions are logically derived.

**Medium Confidence**: HAG improves over input-agnostic head importance; value normalization approximates write-back contribution; uncertainty scaling appropriately modulates evidence influence.

**Low Confidence**: Lightweight estimator consistently improves evidence quality across domains; hyperparameter choices are optimal; performance gains generalize beyond QA benchmarks.

## Next Checks

1. **Estimator Architecture Audit**: Reconstruct the lightweight estimator by testing multiple feature sets and training on hallucination detection data. Measure impact on VC performance across domains.

2. **Sink Token Protocol Standardization**: Implement and compare three sink masking strategies (whitespace-only, whitespace+special tokens, whitespace+special+low-frequency tokens). Evaluate impact on VC performance per model family.

3. **Cross-Domain Generalization Test**: Apply HAVE to non-QA generation tasks prone to hallucination (summarization, creative writing with factual constraints). Compare performance degradation patterns to identify transfer beyond QA benchmarks.