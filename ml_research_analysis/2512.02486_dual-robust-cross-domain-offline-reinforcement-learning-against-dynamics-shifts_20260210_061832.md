---
ver: rpa2
title: Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts
arxiv_id: '2512.02486'
source_url: https://arxiv.org/abs/2512.02486
tags:
- uni00000013
- domain
- dynamics
- droco
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of dual (train-time and test-time)
  robustness in cross-domain offline RL, where policies must handle dynamics shifts
  in both training and deployment. The authors propose the Dual-RObust Cross-domain
  Offline RL (DROCO) algorithm, which introduces a novel robust cross-domain Bellman
  (RCB) operator to enhance test-time robustness while maintaining train-time robustness.
---

# Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts

## Quick Facts
- arXiv ID: 2512.02486
- Source URL: https://arxiv.org/abs/2512.02486
- Reference count: 40
- Primary result: DROCO achieves a total normalized score of 1105.2 across dynamics shift scenarios, surpassing second-best method by 14.0%

## Executive Summary
This paper addresses dual-robustness challenges in cross-domain offline RL where policies must handle dynamics shifts during both training and deployment. The authors propose DROCO (Dual-RObust Cross-domain Offline RL) algorithm that introduces a robust cross-domain Bellman (RCB) operator to enhance test-time robustness while maintaining train-time conservatism. DROCO incorporates dynamic value penalty and Huber loss to mitigate value overestimation or underestimation. Extensive experiments across kinematic and morphology shifts demonstrate significant performance gains over strong baselines, achieving 1105.2 normalized score versus 14.0 improvement over second-best method.

## Method Summary
DROCO tackles cross-domain offline RL by using an ensemble dynamics model trained on target data to approximate uncertainty sets for source transitions. The RCB operator applies an in-sample robust backup (infimum over ensemble predictions) for source transitions while using standard backup for target transitions. This asymmetry treats source data as worst-case reference, regularizing Q-values relative to target policy. Dynamic value penalty scales Q-target based on discrepancy between observed source next-state value and worst-case ensemble prediction. Huber loss reduces sensitivity to outliers in TD errors. The method is built on IQL backbone with Expectile regression for value updates and AWR for policy updates.

## Key Results
- DROCO achieves total normalized score of 1105.2 across all tested dynamics shift scenarios
- Outperforms second-best method (OTDF) by 14.0% margin
- Demonstrates enhanced robustness to test-time dynamics perturbations across kinematic, morphology, and min-Q perturbation types
- Shows consistent improvement over multiple perturbation intensity levels

## Why This Works (Mechanism)

### Mechanism 1: RCB Operator Asymmetry
Applying differentiated Bellman backup (RCB operator) to source-domain data may simultaneously constrain train-time value overestimation and provide bounded guarantee for test-time performance under perturbed dynamics. The RCB operator applies in-sample robust backup (infimum over state uncertainty set) for source transitions while using standard in-sample backup for target transitions. This asymmetry treats source data as worst-case reference, regularizing Q-values to remain bounded relative to target-domain optimal policy. Core assumption: state uncertainty set (parameterized by ε) covers target dynamics support, and Q-function is Lipschitz continuous.

### Mechanism 2: Ensemble-Based Uncertainty Approximation
Using ensemble dynamics model trained solely on target-domain data to sample from uncertainty set may provide tractable, adaptive approximation of RCB operator's infimum computation. Ensemble of N neural networks models Ptar via maximum likelihood. For source transition (s, a, s'src), ensemble predictions s'i ~ P̂ψi(s'|s, a) serve as samples within uncertainty set. Minimum Q-value over samples approximates infimum, reducing conservatism compared to fixed-radius perturbation. Core assumption: ensemble predictions are diverse and reasonably accurate (low total variation distance to Ptar), and ensemble size N is sufficient to capture relevant uncertainty.

### Mechanism 3: Dynamic Value Penalty with Huber Loss
Combining dynamic value penalty with Huber loss may counteract residual value estimation errors introduced by ensemble-based RCB approximation. Penalty term u(s, a, s') = I(s' from source) × [V(s') − inf_i V(s'i)] scales Q-target based on discrepancy between observed source next-state value and worst-case ensemble prediction. Huber loss transitions from L2 to L1 for large TD errors, reducing sensitivity to outliers. Core assumption: penalty coefficient β and Huber threshold δ can be tuned to balance over/underestimation for given domain, and value errors are finite and not systematically biased.

## Foundational Learning
- **Cross-domain offline RL**: Training policies on source domain data while deploying on target domain with different dynamics. Needed to handle real-world deployment scenarios where training and deployment conditions differ. Quick check: Can train on simulation data, deploy on physical robot.
- **Robust Bellman operator**: Bellman backup that accounts for transition uncertainty to prevent value overestimation. Needed to maintain conservative estimates when dynamics are uncertain. Quick check: Should produce bounded Q-values under uncertainty.
- **Ensemble dynamics modeling**: Training multiple neural networks to predict next states and capture epistemic uncertainty. Needed to estimate uncertainty sets for robust backup computation. Quick check: Should show prediction diversity across ensemble members.
- **Value penalty regularization**: Scaling Q-targets based on discrepancy between observed and predicted next-state values. Needed to correct residual estimation errors from approximation. Quick check: Should reduce overestimation in conservative estimates.
- **Huber loss for TD errors**: Loss function that transitions from L2 to L1 for large errors. Needed to reduce sensitivity to outliers in Bellman updates. Quick check: Should be less affected by extreme TD errors than MSE.
- **In-sample vs out-of-sample backups**: Different treatment of source vs target transitions based on data distribution. Needed to balance train-time conservatism with test-time robustness. Quick check: Should use robust backup only for source data.

## Architecture Onboarding

### Component Map
D4RL Target Data (10%) -> Ensemble Dynamics Model -> Uncertainty Set Sampling -> RCB Operator -> Q-Value Update
Expert Source Data (1M) -> RCB Operator with Penalty -> Q-Value Update
IQL Networks -> Expectile Regression (Value) -> AWR (Policy)

### Critical Path
1. Train ensemble dynamics model on target data
2. For each source transition, compute RCB target using ensemble samples
3. Apply dynamic penalty based on value discrepancy
4. Update Q-values using Huber loss
5. Update policy using AWR

### Design Tradeoffs
- Ensemble size N vs computational cost: Larger N provides better uncertainty coverage but increases inference time
- Penalty coefficient β vs conservatism: Higher β increases robustness but may hurt clean performance
- Huber threshold δ vs stability: Lower δ increases robustness to outliers but may reduce convergence stability

### Failure Signatures
- Value overestimation: Policy takes risky actions, performance degrades under perturbations
- Value underestimation: Policy becomes overly conservative, clean performance suffers
- Ensemble overfitting: Penalty signals become unreliable, causing unstable updates
- Poor uncertainty coverage: RCB operator fails to provide test-time guarantees

### First Experiments to Run
1. Train ensemble dynamics model on target data and visualize prediction diversity
2. Implement RCB operator with fixed penalty and evaluate on single domain
3. Conduct ablation study comparing RCB vs standard Bellman on source data

## Open Questions the Paper Calls Out

### Open Question 1: Adaptive Penalty Tuning
Can an adaptive mechanism be developed to automatically tune the penalty coefficient (β) and Huber transition threshold (δ) based on the specific intensity of the dynamics shift, rather than relying on task-specific manual tuning? Current method requires hyperparameter sweeping to find optimal settings, with authors only providing general guideline (β ≤ 1.0) rather than algorithmic solution for adaptation.

### Open Question 2: Inter-Domain Mapping Integration
How does integration of inter-domain mapping techniques affect theoretical guarantees of RCB operator when source and target domains possess distinct state-action representations? While DROCO is theoretically applicable to distinct state-action spaces via mapping techniques, this extension has not been empirically verified or theoretically analyzed in the paper.

### Open Question 3: Optimal Uncertainty Set Size
Is it possible to theoretically determine optimal uncertainty set size (ε) to perfectly balance trade-off between train-time and test-time robustness? Current work relies on ensemble model to approximate uncertainty set but does not provide method to analytically derive ε that maximizes area under robustness-performance curve.

## Limitations
- Limited ablation studies prevent isolating individual contributions of RCB operator, dynamic value penalty, and Huber loss
- Theoretical guarantees rely on assumption that uncertainty set covers target dynamics support, which may not hold for large shifts or small target datasets
- Ensemble dynamics model trained on only 10% of D4RL data raises concerns about overfitting and poor uncertainty estimation

## Confidence

**High**: Core methodology (DROCO algorithm with RCB operator, dynamic value penalty, and Huber loss) is clearly described and experimental results demonstrate effectiveness across multiple domains and perturbation types.

**Medium**: Theoretical analysis provides justification for RCB operator and ensemble-based approximation, but assumptions may not hold in all practical scenarios.

**Low**: Exact contribution of each component to overall performance improvement is not well-established due to limited ablation studies.

## Next Checks
1. Conduct comprehensive ablation study to isolate individual contributions of RCB operator, dynamic value penalty, and Huber loss
2. Investigate sensitivity of DROCO to choice of state uncertainty set (ε) across different dynamics shift magnitudes and target dataset sizes
3. Evaluate ensemble dynamics model performance on held-out validation set from target domain, monitoring validation loss and visualizing prediction diversity