---
ver: rpa2
title: Differentially Private Federated Learning With Time-Adaptive Privacy Spending
arxiv_id: '2502.18706'
source_url: https://arxiv.org/abs/2502.18706
tags:
- privacy
- clients
- rounds
- client
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel time-adaptive differentially private
  federated learning framework that allows clients to spend their privacy budgets
  non-uniformly across training rounds. The key idea is to let clients save privacy
  budgets in early rounds and spend more in later rounds when fine-grained features
  are learned, improving the privacy-utility tradeoff.
---

# Differentially Private Federated Learning With Time-Adaptive Privacy Spending

## Quick Facts
- **arXiv ID**: 2502.18706
- **Source URL**: https://arxiv.org/abs/2502.18706
- **Reference count**: 40
- **Primary result**: Time-adaptive privacy budget allocation improves DP-FL accuracy by 4.7-19.9% over baselines by letting clients save budget early and spend more later when fine-grained features are learned.

## Executive Summary
This paper introduces a novel time-adaptive differentially private federated learning framework that allows clients to spend their privacy budgets non-uniformly across training rounds. The key insight is that clients can save privacy budgets in early rounds and spend more in later rounds when fine-grained features are learned, improving the privacy-utility tradeoff. The framework introduces a "spend-as-you-go" method where clients transition from saving to spending mode based on their privacy budgets, not their data. Theoretical analysis proves that this approach reduces clipping bias and improves utility, especially for clients with stricter privacy constraints. Experiments on FMNIST, MNIST, Adult Income, and CIFAR10 datasets show that the proposed framework outperforms baselines including DP-FedAvg and IDP-FedAvg by 4.7-19.9% in global test accuracy while adhering to privacy constraints.

## Method Summary
The framework uses a two-phase approach: clients operate in "saving rounds" with reduced sampling rates (qn < q) early in training, then transition to "spending rounds" with full sampling rate (q) later. The SetPrivacyParams algorithm pre-computes per-round noise multipliers and clipping norms for each client based on their privacy budget and transition round. During training, clients with stricter privacy constraints (smaller clipping norms) are assigned lower sampling rates during saving rounds, reducing their expected clipping bias contribution. The method maintains rigorous differential privacy guarantees through recursive Rényi differential privacy accounting across all T rounds.

## Key Results
- Outperforms DP-FedAvg and IDP-FedAvg baselines by 4.7-19.9% in global test accuracy across four datasets
- Particularly effective for clients with stricter privacy constraints (ε ≤ 10), showing consistent improvement in high-privacy regimes
- Maintains exact privacy budget adherence with no overspending across all experimental configurations
- The time-adaptive approach is most beneficial when learning follows a coarse-to-fine pattern typical in deep neural networks

## Why This Works (Mechanism)

### Mechanism 1: Time-Adaptive Privacy Budget Allocation (Spend-as-you-go)
- Claim: Non-uniform privacy spending across training rounds improves the privacy-utility tradeoff compared to uniform spending.
- Mechanism: Clients operate in two phases—saving rounds (lower sampling rate qn < q) and spending rounds (full sampling rate q). Budget saved early enables higher signal-to-noise ratio in later rounds when fine-grained features are learned. The transition round Tn and saving-based sampling rate qn are determined solely by privacy budget, not data, enabling pre-training scheduling with zero additional privacy loss.
- Core assumption: Coarse-grained features learned in early rounds are more noise-tolerant; fine-grained features learned later benefit from higher SNR.
- Evidence anchors:
  - [abstract]: "clients to save their privacy budgets in early rounds and spend more in later rounds when fine-grained features are learned"
  - [section 3.1, Algorithm 1]: Full pseudocode for SetPrivacyParams showing recursive privacy accounting across rounds
  - [corpus]: Weak direct support—neighbor papers focus on DP-FL generally but not time-adaptive spending specifically
- Break condition: If learning dynamics don't follow the coarse-to-fine pattern (e.g., very shallow networks or early-exit architectures), the benefit diminishes.

### Mechanism 2: Privacy Amplification via Variable Sampling Rates
- Claim: Reducing sampling rate during saving rounds proportionally reduces RDP privacy expenditure.
- Mechanism: Per Lemma 4 and Equation (1), RDP privacy spend scales as (qn/q)² during saving rounds. When qn < q, the client spends only a fraction of the budget they would at full sampling rate, accumulating savings for later rounds.
- Core assumption: Poisson sampling independence holds across rounds; clients can be sampled at different rates without correlation leakage.
- Evidence anchors:
  - [section 4.1, Equation 1]: "ϵt_rdp,n = [budget_remaining/(T-t+1)] × (qt_n/q)²" shows quadratic scaling
  - [section 2]: References Sampled Gaussian Mechanism (Mironov et al., 2019) for q² amplification factor
  - [corpus]: DP-FL clustering paper (arXiv:2508.06183) uses client-level DP but doesn't address temporal budget allocation
- Break condition: If adaptive sampling rates leak information about client data (not done here—qn depends only on ϵn), privacy guarantees could be compromised.

### Mechanism 3: Optimal Sampling Rate-Clip Norm Matching
- Claim: Assigning lower sampling rates to clients with smaller clipping norms (stricter privacy) reduces expected clipping bias.
- Mechanism: Per Theorem 3, the bias upper bound is minimized when qt_n/ct_n^(ρ-1) is low for clients who contribute highly perturbed updates. Stricter clients (smaller ct_n) get lower qt_n during saving, preserving budget while preventing their noisy updates from degrading early-round learning.
- Core assumption: Privacy budget assignment is independent of data distribution (Assumption: privacy preferences and data are decoupled).
- Evidence anchors:
  - [section 4.2, Theorem 3]: Formal bound showing E[Error] ≤ (1/N²) × ΣE[||Δθ||^ρ] × Σ(qt_n/ct_n^(ρ-1))
  - [section 4.2]: "clients with stricter privacy budgets benefit from expending their privacy budgets more unevenly"
  - [corpus]: No direct evidence—neighbor papers don't address clipping bias optimization
- Break condition: If strict-privacy clients hold uniquely important data features, reducing their early contribution may harm coarse-feature learning.

## Foundational Learning

- Concept: **Rényi Differential Privacy (RDP)**
  - Why needed here: The framework uses RDP for privacy accounting because it composes additively across rounds, enabling clean recursive budget tracking in Algorithm 1.
  - Quick check question: Can you explain why RDP composition is simpler than (ε,δ)-DP composition for tracking privacy across T rounds?

- Concept: **Privacy Amplification by Subsampling**
  - Why needed here: Core to the saving mechanism—lower sampling rate qn reduces effective privacy spend by factor (qn/q)² per round.
  - Quick check question: If a client is sampled with probability 0.3 instead of 0.9, by what factor does their per-round RDP privacy spend decrease?

- Concept: **Clipping Bias in DP-SGD**
  - Why needed here: Theoretical analysis (Theorems 2-3) builds on clipping bias bounds from Das et al. (2023); understanding this is essential for grasping why optimal sampling matters.
  - Quick check question: When gradient norms exceed the clipping threshold, what type of error is introduced to the expected update?

## Architecture Onboarding

- Component map:
  - **SetPrivacyParams (Algorithm 1)** -> **IterativeTraining (Algorithm 2)** -> **ClientUpdate** -> **GetNoise/Compute_rdp**

- Critical path:
  1. Pre-training: Define privacy groups, assign Tn and qn values per group
  2. Run SetPrivacyParams to generate all per-round parameters (Lines 1-16, Alg 1)
  3. Training loop: Sample clients, local update with per-client (ct_n, σt_n), aggregate with compensation noise for unsampled clients
  4. Post-training: Convert RDP to (ε,δ)-DP for final reporting

- Design tradeoffs:
  - **Earlier Tn (transition round)**: More budget spent early → better early learning but lower final accuracy on fine-grained features
  - **Lower qn (saving rate)**: More budget saved → stronger late-round contribution but weaker early contribution
  - **Stricter εn**: Requires smaller ct_n → more clipping bias, benefits more from non-uniform spending (Theorem 3)
  - Assumption: Hyperparameter tuning of Tn, qn incurs additional privacy cost unless done without data access

- Failure signatures:
  - **Accuracy plateau in early rounds then sudden jump**: Expected behavior—budget conservation until Tn
  - **Final accuracy lower than DP-FedAvg baseline**: Check if Tn is too late (insufficient spending rounds) or qn too low (underutilized budget)
  - **Privacy accounting error (ε exceeded)**: Verify GetNoise uses remaining budget correctly; check RDP-to-DP conversion at correct α, δ
  - **Clipping bias dominates**: ct_n values too small relative to gradient norms; consider increasing average clipping norm c

- First 3 experiments:
  1. **Reproduce FMNIST baseline (Table 2)**: N=100, T=25, ε_groups=(10,20,30), Tn=13, qn=(0.5,0.6,0.7), q=0.9, c=250. Target: ~70% accuracy vs IDP-FedAvg ~65%.
  2. **Ablate transition round Tn**: Compare Tn=7, 13, 19 for middle-privacy group; measure final accuracy and privacy spent per round (Fig 2 style curves).
  3. **Test strict privacy setting**: Set ε_groups=(2,5,10); verify framework still outperforms IDP-FedAvg (expect smaller absolute gains but consistent relative improvement per Table 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal saving-to-spending transition round ($T_n$) be determined using privacy-preserving hyperparameter tuning?
- Basis in paper: [explicit] The authors state that while tuning transition rounds may improve utility, it currently leads to additional privacy loss and suggest future work should integrate "privacy-preserving hyperparameter tuning."
- Why unresolved: The current framework relies on fixed or heuristic transition points (e.g., midway through training), as dynamic tuning would leak information about the data.
- What evidence would resolve it: A modified algorithm that adjusts $T_n$ dynamically during training accompanied by a formal proof that the tuning mechanism itself satisfies differential privacy.

### Open Question 2
- Question: Can this time-adaptive framework be integrated with adaptive clipping methods while maintaining rigorous privacy accounting?
- Basis in paper: [explicit] The paper notes that while adaptive clipping is orthogonal to their approach, combining them "requires careful privacy analysis, which is beyond the scope of this paper."
- Why unresolved: Integrating time-varying sampling rates and noise multipliers with data-dependent clipping updates complicates the composition of privacy guarantees.
- What evidence would resolve it: A unified framework that incorporates both time-adaptive spending and adaptive clipping with a corresponding theoretical bound on privacy loss.

### Open Question 3
- Question: What constitutes the optimal set of saving-based sampling rates $\{q_1, \dots, q_N\}$?
- Basis in paper: [explicit] In Section 4.2, the authors note that the set of sampling rates is currently a hyperparameter and "The optimized choice of the set of sampling rates... is reserved for future work."
- Why unresolved: The current work optimizes the permutation (matching clients to rates) but takes the specific values of the rates as given inputs rather than optimizing the values themselves.
- What evidence would resolve it: A theoretical derivation or empirical strategy for selecting the specific values of the saving-based sampling rates based on client budgets and total training rounds.

## Limitations
- The optimal transition round (Tn) is currently a hyperparameter rather than being determined through privacy-preserving optimization
- The framework assumes privacy budgets are independent of data distribution, which may not hold in all real-world scenarios
- Per-layer clipping implementation details (how global clip norm is distributed across layers) are not fully specified

## Confidence
- **High Confidence**: The core time-adaptive mechanism and its theoretical privacy guarantees (Theorems 2-3) are well-founded
- **Medium Confidence**: The experimental results showing 4.7-19.9% improvement over baselines, though the specific hyperparameter sensitivity is not fully explored
- **Low Confidence**: The claim that the method works particularly well for clients with stricter privacy constraints (Theorem 3) - while theoretically supported, the empirical validation is limited to the specific privacy group configurations tested

## Next Checks
1. **Ablation on Transition Rounds**: Systematically vary Tn across privacy groups to identify optimal transition timing and measure its impact on final accuracy
2. **Privacy Budget Consumption Tracking**: Log and verify that each client's privacy budget is consumed exactly as planned (depletes at round T), ensuring no overspending
3. **Strict Privacy Setting Validation**: Test the framework under more stringent privacy budgets (e.g., ε_groups=(2,5,10)) to confirm consistent relative improvement over baselines in high-privacy regimes