---
ver: rpa2
title: Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks
  Structure Prediction
arxiv_id: '2601.09285'
source_url: https://arxiv.org/abs/2601.09285
tags:
- structure
- building
- structures
- rotation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Metal-organic frameworks (MOFs) are crystalline materials with
  diverse applications, but predicting their 3D structures is challenging due to their
  atomic complexity. Large language models (LLMs) have been effective in generating
  simpler crystals but struggle with MOFs' intricate 3D geometries.
---

# Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction

## Quick Facts
- **arXiv ID:** 2601.09285
- **Source URL:** https://arxiv.org/abs/2601.09285
- **Reference count:** 26
- **Primary result:** MOF-LLM achieves 35.78% match rate on unseen MOFs with 0.04s inference speed

## Executive Summary
Metal-organic frameworks (MOFs) are crystalline materials with diverse applications, but predicting their 3D structures is challenging due to their atomic complexity. Large language models (LLMs) have been effective in generating simpler crystals but struggle with MOFs' intricate 3D geometries. To address this, we propose MOF-LLM, the first LLM framework specifically designed for block-level MOF structure prediction. Our method uses spatial-aware continual pre-training (CPT) to improve LLMs' spatial reasoning, structural supervised fine-tuning (SFT) to enable block assembly, and matching-driven reinforcement learning (RL) to refine structure stability.

## Method Summary
MOF-LLM uses a three-stage training approach on a Qwen-3 8B backbone. First, spatial-aware CPT augments SMILES descriptions with molecular weight, PCA spans, and topology codes to inject geometric priors. Second, structural SFT trains the model to predict lattice parameters and per-block roto-translations using instruction-response pairs. Third, matching-driven RL (SAPO) refines structural stability by sampling candidate structures and rewarding those that match ground-truth structures within specified tolerances. The model treats MOFs as assemblies of rigid building blocks rather than individual atoms, significantly reducing sequence length while preserving chemical modularity.

## Key Results
- Achieves 35.78% match rate at stol=0.5 on held-out test set, outperforming state-of-the-art methods
- Maintains high inference speed of 0.04 seconds per structure for practical high-throughput discovery
- Ablation studies confirm spatial-aware CPT and matching-driven RL significantly improve performance

## Why This Works (Mechanism)

### Mechanism 1
Block-level representation enables LLMs to handle MOF structure prediction by reducing sequence length and exploiting chemical modularity. MOFs are decomposed into rigid building blocks (metal nodes, organic linkers). Instead of generating N atomic coordinates, the model predicts M block positions/orientations (M << N). Building blocks are encoded as SMILES strings, leveraging pre-trained chemical semantics. Core assumption: MOF building blocks can be treated as rigid bodies without significant conformational flexibility. Break condition: If target MOFs have highly flexible linkers requiring torsion modeling.

### Mechanism 2
Spatial-aware CPT injects geometric priors that improve LLM spatial reasoning for 3D assembly. CPT augments SMILES with explicit spatial descriptors: molecular weight, PCA spans (bounding box dimensions), and rotated principal axes before/after rotation. Topology codes provide global connectivity information. Core assumption: LLMs can learn 3D spatial relationships from 1D text descriptions when geometry is explicitly encoded. Break condition: If geometric descriptors are omitted or blocks have complex non-convex shapes poorly captured by PCA spans.

### Mechanism 3
Matching-driven RL refines structural stability beyond token-level SFT optimization. SAPO samples G candidate structures, computes rewards based on pymatgen StructureMatcher tolerance against ground truth. High-reward structures (τ≤0.5 with low RMSE) receive bonus signals. Group-normalized advantages provide relative learning signals even from mostly suboptimal samples. Core assumption: Matching-to-ground-truth correlates with thermodynamic stability and practical utility. Break condition: If reward hacking occurs or if ground-truth reference structures are themselves non-optimal.

## Foundational Learning

- **MOF block decomposition**: Understanding how metal nodes and organic linkers assemble into periodic structures is prerequisite for interpreting SMILES inputs and rotation/translation outputs. Quick check: Given a MOF unit cell with 200 atoms decomposed into 5 building blocks, what does the model actually predict?

- **Euler angles vs. rotation matrices**: The paper uses Euler angles (roll, pitch, yaw) as the rotation representation. Understanding the extrinsic x-y-z convention and gimbal lock limitations is essential for debugging prediction failures. Quick check: Why does the paper constrain pitch (ω) to [-π/2, π/2] while roll and yaw span [-π, π]?

- **Fractional coordinates in crystallography**: Translations are output as fractional coordinates (relative to lattice vectors), not Cartesian. This is critical for parsing model outputs correctly. Quick check: If lattice parameters are a=10Å, b=10Å, c=10Å and a block has fractional translation [0.5, 0.5, 0.5], where is its center?

## Architecture Onboarding

- **Component map:** Qwen-3 8B backbone → Spatial-aware CPT → Structural SFT → SAPO RL
- **Critical path:** 1) Preprocess MOF structures: decompose → canonical SMILES + compute PCA spans + extract topology codes; 2) CPT stage: full-parameter training on augmented descriptions (3 epochs, lr=5e-5); 3) SFT stage: instruction-response pairs for assembly (12K steps, lr=2e-5); 4) RL stage: SAPO with G=8 samples per prompt, matching-based rewards (2 epochs, lr=1e-6)
- **Design tradeoffs:** Euler angles chosen over axis-angle (2.73% higher MR) but have singularity issues; SMILES loses 3D geometry → compensated by explicit spatial descriptors; Rigid block assumption limits applicability to flexible MOFs
- **Failure signatures:** Parsing failures → reward=-1 (check output format compliance); High RMSE with low MR → rotation prediction issues; Valid structures but poor geometric properties → insufficient RL refinement
- **First 3 experiments:** 1) Reproduce ablation: compare SFT-only vs. SFT+CPT vs. SFT+CPT+RL on held-out test set; 2) Rotation representation sweep: Euler vs. axis-angle on subset with complex orientations; 3) Inference efficiency benchmark: single-sample latency vs. MOF-BFN vs. MOFFlow on same hardware

## Open Questions the Paper Calls Out

1. **Can the framework be extended to model conformational flexibility in building blocks rather than treating them as rigid bodies?** The authors state in the Limitations section: "Extending the framework to model conformational flexibility could further enhance practicality." This remains unresolved because current metal-oxo decomposition and block-level representation assume fixed geometries.

2. **Can advanced tokenization methods natively preserve 3D geometric information without flattening to 1D SMILES strings?** The authors note: "Flattening 3D blocks into 1D SMILES strings inherently simplifies geometric details. Future research could delve deeper into advanced tokenization methods that natively preserve 3D geometries." This is unresolved because SMILES representations discard explicit spatial geometry.

3. **Would integrating explicit Chain-of-Thought (CoT) reasoning improve spatial planning and interpretability in block assembly?** The authors state: "Integrating explicit Chain-of-Thought reasoning offers a promising direction for improving interpretability and complex spatial planning." This remains untested as the current autoregressive generation directly outputs roto-translations without intermediate reasoning steps.

## Limitations

- **Rigid block assumption:** Cannot handle flexible linkers or conformational changes within building blocks, limiting applicability to MOFs requiring flexible organic linkers
- **Geometric descriptor limitations:** PCA spans and molecular weight may not fully capture complex non-convex building block shapes
- **Reward function correlation:** StructureMatcher-based rewards may not perfectly correlate with thermodynamic stability or synthetic accessibility

## Confidence

**High Confidence:** Spatial-aware CPT mechanism and its effectiveness demonstrated through ablation studies; block-level decomposition approach validated through comparison with atom-level methods.

**Medium Confidence:** RL-based refinement shows substantial improvement over SFT alone, but reward function's correlation with actual structural stability remains indirect.

**Low Confidence:** Generalization capability to MOFs with flexible building blocks or those requiring conformational sampling is not demonstrated.

## Next Checks

1. **Stability Validation Beyond Matching:** Run molecular dynamics simulations or DFT calculations on high-MR MOF-LLM predictions to verify that good matching scores correlate with actual thermodynamic stability.

2. **Flexible Linker Test Case:** Construct benchmark set of MOFs containing flexible organic linkers and evaluate MOF-LLM's performance versus methods incorporating internal torsion angles.

3. **Geometric Descriptor Ablation:** Systematically test alternative geometric representations (3D conformer fingerprints, shape descriptors beyond PCA spans) to determine if current spatial-aware CPT can be improved.