---
ver: rpa2
title: Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label
  Visual Recognition
arxiv_id: '2511.20641'
source_url: https://arxiv.org/abs/2511.20641
tags:
- uni00000013
- uni00000011
- uni00000018
- classes
- tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-tailed multi-label visual recognition,
  where images contain multiple objects with highly imbalanced class distributions,
  leading to biased models favoring head classes. The proposed Correlation Adaptation
  Prompt Network (CAPNET) leverages pre-trained vision-language models (VLMs) like
  CLIP to mitigate these imbalances.
---

# Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition

## Quick Facts
- arXiv ID: 2511.20641
- Source URL: https://arxiv.org/abs/2511.20641
- Authors: Wei Tang; Zuo-Zheng Wang; Kun Zhang; Tong Wei; Min-Ling Zhang
- Reference count: 40
- Primary result: CAPNET achieves mAP scores of 93.03% on VOC-LT and 76.36% on COCO-LT, surpassing state-of-the-art methods by up to 10.17% for long-tailed multi-label visual recognition.

## Executive Summary
This paper addresses long-tailed multi-label visual recognition, where images contain multiple objects with highly imbalanced class distributions, leading to biased models favoring head classes. The proposed Correlation Adaptation Prompt Network (CAPNET) leverages pre-trained vision-language models (VLMs) like CLIP to mitigate these imbalances. The core method explicitly models label correlations using a graph convolutional network (GCN) derived from CLIP's textual encoder, replacing noisy caption-derived priors with robust semantic embeddings. CAPNET incorporates learnable soft prompts, a distribution-balanced Focal loss with class-aware re-weighting, and parameter-efficient fine-tuning (AdaptFormer) to prevent overfitting on tail classes. Extensive experiments on VOC-LT, COCO-LT, and NUS-WIDE datasets demonstrate significant performance gains, with CAPNET achieving mAP scores of 93.03% on VOC-LT and 76.36% on COCO-LT, surpassing state-of-the-art methods by up to 10.17%. These results validate CAPNET's effectiveness in handling long-tailed multi-label visual recognition, particularly for underrepresented tail classes.

## Method Summary
The Correlation Adaptation Prompt Network (CAPNET) addresses long-tailed multi-label visual recognition by leveraging pre-trained vision-language models, specifically CLIP. The method constructs a label correlation graph using CLIP's textual encoder, where each label is represented as a semantic embedding, and correlations are captured through a graph convolutional network (GCN). This approach replaces traditional noisy caption-based label correlations with more robust semantic relationships. CAPNET employs learnable soft prompts to adapt CLIP's frozen encoders for the target dataset, parameter-efficiently fine-tuning the model while preserving pre-trained knowledge. The model uses a distribution-balanced Focal loss with class-aware re-weighting to address class imbalance, focusing more on tail classes. The architecture integrates these components through a dual-branch design that combines image and label correlation features, enabling effective multi-label prediction in imbalanced scenarios.

## Key Results
- CAPNET achieves mAP scores of 93.03% on VOC-LT and 76.36% on COCO-LT, significantly outperforming state-of-the-art methods.
- The method surpasses existing approaches by up to 10.17% on VOC-LT and 5.07% on COCO-LT, demonstrating its effectiveness for tail class recognition.
- Extensive ablation studies confirm the importance of CLIP-based label correlation modeling, soft prompts, and distribution-balanced Focal loss in achieving superior performance.

## Why This Works (Mechanism)
CAPNET works by addressing the fundamental challenge of label correlation modeling in long-tailed multi-label recognition. Traditional methods rely on noisy caption-derived label correlations, which are unreliable due to dataset biases and inconsistent annotations. By leveraging CLIP's textual encoder, CAPNET constructs a robust label correlation graph using semantic embeddings, capturing true relationships between labels. The GCN-based correlation modeling propagates contextual information across related labels, helping tail classes benefit from shared semantic features with head classes. The learnable soft prompts enable fine-grained adaptation of the pre-trained VLMs without catastrophic forgetting, while the distribution-balanced Focal loss with class-aware re-weighting ensures balanced learning across the class spectrum. This combination of semantic correlation modeling, parameter-efficient adaptation, and imbalance-aware training effectively mitigates the long-tail problem in multi-label recognition.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Pre-trained models like CLIP that jointly learn visual and textual representations. Needed to provide rich semantic embeddings for label correlation modeling. Quick check: Verify CLIP's zero-shot performance on relevant datasets.
- **Graph Convolutional Networks (GCNs)**: Neural networks that operate on graph-structured data, propagating information between connected nodes. Needed to model label correlations as a graph where nodes are labels and edges represent semantic relationships. Quick check: Ensure the correlation graph is sparse and well-connected.
- **Multi-Label Classification**: Task where each input can belong to multiple classes simultaneously. Needed as the target problem, where images contain multiple objects with imbalanced distributions. Quick check: Confirm label cardinality and co-occurrence statistics in the dataset.
- **Class Imbalance and Long-Tailed Distributions**: Scenarios where a few classes (head) have abundant samples while many classes (tail) have few. Needed to understand the core challenge CAPNET addresses. Quick check: Analyze class frequency distribution and imbalance ratio.
- **Parameter-Efficient Fine-Tuning**: Techniques like prompt tuning that adapt pre-trained models with minimal parameter updates. Needed to prevent overfitting on tail classes while leveraging pre-trained knowledge. Quick check: Compare parameter count between full fine-tuning and prompt tuning.
- **Focal Loss and Distribution-Balanced Loss**: Modified cross-entropy losses that focus on hard examples and address class imbalance. Needed to ensure tail classes receive adequate training attention. Quick check: Verify loss weights and focusing parameters are properly tuned.

## Architecture Onboarding
- **Component Map**: Input Image → CLIP Visual Encoder → [Soft Prompts] → Image Features; Input Labels → CLIP Textual Encoder → Label Embeddings → GCN Correlation Graph → Label Correlation Features; Dual-Branch Fusion → Multi-Label Classifier → Output Predictions
- **Critical Path**: Image features and label correlation features are extracted in parallel, then fused through a dual-branch network. The correlation features, derived from the GCN, provide contextual information that enhances the image features for multi-label prediction.
- **Design Tradeoffs**: Using pre-trained VLMs reduces training data requirements but limits architectural flexibility. The GCN-based correlation modeling adds computational overhead but provides more accurate label relationships than caption-based methods. Soft prompts enable efficient adaptation but may not capture all dataset-specific nuances.
- **Failure Signatures**: Poor performance on tail classes may indicate insufficient correlation modeling or imbalanced loss weighting. Overfitting on head classes suggests the Focal loss parameters need adjustment. Degraded performance with novel labels points to limitations in the semantic embedding space.
- **First 3 Experiments to Run**:
  1. Evaluate zero-shot performance of CLIP on the target dataset to establish baseline.
  2. Test GCN correlation modeling with random label embeddings to assess the importance of semantic information.
  3. Compare performance with and without soft prompts to quantify parameter-efficient fine-tuning benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is primarily conducted on synthetic long-tailed versions of existing datasets (VOC-LT and COCO-LT), which may not fully represent real-world long-tailed scenarios. The performance on naturally long-tailed datasets remains unexplored.
- The method's computational overhead during inference, particularly due to the GCN-based correlation modeling and soft prompt learning, is not thoroughly analyzed.
- The ablation studies, while comprehensive, do not isolate the contribution of the CLIP-based label correlation modeling from other components like the Focal loss and soft prompts. This makes it difficult to determine which specific innovation drives the performance gains.

## Confidence
- **High confidence**: The reported performance improvements on VOC-LT and COCO-LT datasets are reliable based on the presented experimental setup.
- **Medium confidence**: The generalizability to naturally occurring long-tailed distributions needs further validation.
- **Medium confidence**: The efficiency claims regarding parameter-efficient fine-tuning require more rigorous analysis of computational overhead.

## Next Checks
1. Evaluate CAPNET on naturally long-tailed datasets (e.g., LVIS, Open Images) to assess real-world applicability.
2. Conduct controlled ablation studies to isolate the contribution of CLIP-based label correlation modeling versus other components.
3. Analyze inference time and memory requirements compared to baseline methods to quantify the practical efficiency trade-offs.