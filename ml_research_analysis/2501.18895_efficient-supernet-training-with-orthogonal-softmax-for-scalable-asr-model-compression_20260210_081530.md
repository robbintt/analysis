---
ver: rpa2
title: Efficient Supernet Training with Orthogonal Softmax for Scalable ASR Model
  Compression
arxiv_id: '2501.18895'
source_url: https://arxiv.org/abs/2501.18895
tags:
- training
- selection
- supernet
- each
- subnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently training multiple
  ASR models of varying sizes to meet diverse hardware constraints. The authors propose
  a novel method called OrthoSoftmax, which uses multiple orthogonal softmax functions
  to learn binary masks for identifying optimal subnets within a supernet, avoiding
  resource-intensive search.
---

# Efficient Supernet Training with Orthogonal Softmax for Scalable ASR Model Compression

## Quick Facts
- **arXiv ID**: 2501.18895
- **Source URL**: https://arxiv.org/abs/2501.18895
- **Reference count**: 40
- **Primary result**: FLOPs-aware component-wise selection achieves best performance with WERs comparable to individually trained models

## Executive Summary
This paper introduces OrthoSoftmax, a novel method for efficient supernet training in Automatic Speech Recognition (ASR) model compression. The approach addresses the challenge of training multiple ASR models of varying sizes to meet diverse hardware constraints by using multiple orthogonal softmax functions to learn binary masks for identifying optimal subnets within a supernet. This eliminates the need for resource-intensive search processes while enabling flexible and precise subnet selection based on various criteria and levels of granularity. The method demonstrates that FLOPs-aware component-wise selection achieves the best overall performance across different model sizes, with Word Error Rates (WERs) comparable to or slightly better than individually trained models.

## Method Summary
OrthoSoftmax employs multiple orthogonal softmax functions to simultaneously learn binary masks that identify optimal subnets within a supernet. Unlike traditional approaches that require resource-intensive search processes, OrthoSoftmax directly optimizes these masks during supernet training. The method enables selection at different granularities (component-wise or layer-wise) and based on different criteria (FLOPs-aware or parameter-aware), providing flexibility for various hardware constraints. By maintaining orthogonality between the softmax functions, the approach ensures that each subnet selection criterion learns distinct and complementary mask patterns, leading to more effective compression without significant performance degradation.

## Key Results
- FLOPs-aware component-wise selection achieves the best overall performance across different model sizes
- WERs for all model sizes are comparable to or slightly better than individually trained models
- Analysis reveals interesting patterns in component selection across different model sizes
- The method successfully compresses ASR models while maintaining competitive accuracy

## Why This Works (Mechanism)
OrthoSoftmax works by introducing multiple orthogonal softmax functions that learn complementary binary masks during supernet training. Each softmax function is responsible for optimizing subnets based on specific criteria (FLOPs or parameters) and at different granularities (component or layer level). The orthogonality constraint ensures that these functions don't interfere with each other, allowing them to capture distinct subnet characteristics. During training, the supernet learns to produce optimal subnets for various model sizes simultaneously, with the masks being directly optimized rather than discovered through post-hoc search. This joint optimization approach leads to more coherent subnet selection and better utilization of the supernet's representational capacity.

## Foundational Learning

**Supernet Training**
- *Why needed*: Enables training multiple model variants simultaneously, reducing overall training time compared to training each model separately
- *Quick check*: Verify that all subnet paths through the supernet are exercised during training to ensure proper weight sharing

**Orthogonal Softmax Functions**
- *Why needed*: Prevents interference between different subnet selection criteria, ensuring each learns distinct and complementary mask patterns
- *Quick check*: Confirm orthogonality constraint is properly enforced during training through correlation analysis of learned masks

**Binary Mask Learning**
- *Why needed*: Allows dynamic identification of which components to include/exclude for different model sizes without architectural changes
- *Quick check*: Validate that masks produce expected FLOPs/parameter reductions while maintaining model functionality

**Hardware-Aware Model Compression**
- *Why needed*: Ensures compressed models meet specific computational constraints while maintaining accuracy
- *Quick check*: Measure actual inference time and memory usage on target hardware to verify theoretical estimates

## Architecture Onboarding

**Component Map**
Supernet -> OrthoSoftmax Layers -> Multiple Binary Masks -> Subnet Selection

**Critical Path**
The critical path involves the supernet backbone architecture, where OrthoSoftmax layers are inserted to generate binary masks. These masks determine which components are active for each subnet configuration, directly affecting both model size and computational requirements.

**Design Tradeoffs**
- Granularity vs. flexibility: Component-wise selection offers more precise control but increases complexity compared to layer-wise selection
- Criterion specificity vs. generalization: FLOPs-aware selection optimizes for speed but may miss parameter-efficient configurations
- Training overhead vs. search efficiency: Multiple orthogonal softmax functions increase training complexity but eliminate expensive post-training search

**Failure Signatures**
- Poor orthogonality between softmax functions leading to redundant mask patterns
- Insufficient mask sparsity causing minimal actual compression
- Mask instability during training resulting in inconsistent subnet selection

**First Experiments**
1. Verify orthogonality constraint by measuring correlation between different softmax outputs
2. Test mask stability by checking consistency across multiple training runs
3. Validate FLOPs estimation accuracy by comparing predicted vs. measured values on target hardware

## Open Questions the Paper Calls Out

None

## Limitations
- Evaluation limited to two English ASR datasets (Librispeech and TED-LIUM-v2), potentially limiting generalizability to other languages or domains
- Computational overhead of training multiple orthogonal softmax layers and memory requirements during training not thoroughly analyzed
- Lacks ablation studies on the number of orthogonal components and their impact on final performance

## Confidence

**High Confidence**: The core methodology of using orthogonal softmax for mask learning is technically sound and well-justified. The comparative WER results showing parity or slight improvement over individually trained models are convincing.

**Medium Confidence**: The claims about hardware-aware selection efficiency are supported by results but would benefit from more detailed analysis of training overhead and memory consumption during the supernet training phase.

**Medium Confidence**: The insights about component selection patterns across model sizes are interesting but based on a limited set of model configurations, suggesting the need for broader exploration.

## Next Checks

1. Conduct experiments on additional ASR datasets representing different languages, domains, and acoustic conditions to assess cross-domain robustness and generalizability.

2. Perform detailed analysis of training computational overhead, including memory consumption and wall-clock time comparison between OrthoSoftmax and traditional search-based approaches.

3. Systematically vary the number of orthogonal components and evaluate the trade-off between selection granularity, training complexity, and final model performance to establish optimal configuration guidelines.