---
ver: rpa2
title: From Image Captioning to Visual Storytelling
arxiv_id: '2508.14045'
source_url: https://arxiv.org/abs/2508.14045
tags:
- visual
- storytelling
- story
- image
- bart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses visual storytelling by proposing a two-stage
  approach that treats the task as a superset of image captioning. The method first
  generates captions for individual images using a vision-to-language model, then
  transforms these captions into coherent narratives using a language-to-language
  model.
---

# From Image Captioning to Visual Storytelling

## Quick Facts
- **arXiv ID:** 2508.14045
- **Source URL:** https://arxiv.org/abs/2508.14045
- **Authors:** Admitos Passadakis; Yingjin Song; Albert Gatt
- **Reference count:** 21
- **One-line primary result:** Two-stage visual storytelling approach combining ClipCap with BART/T5 achieves competitive performance across multiple metrics including RoViST and ideality, with improved coherence and visual grounding compared to baselines.

## Executive Summary
This paper addresses visual storytelling by proposing a two-stage approach that treats the task as a superset of image captioning. The method first generates captions for individual images using a vision-to-language model, then transforms these captions into coherent narratives using a language-to-language model. The framework combines ClipCap with either T5 or BART as the storyteller, and is trained separately on different data subsets to enhance robustness. Evaluation on the VIST dataset shows that the proposed method achieves competitive performance across multiple metrics, including RoViST and ideality, which measures linguistic closeness to human-written stories. Human and LLM evaluations further demonstrate that the generated stories are more coherent, visually grounded, and human-like compared to several state-of-the-art baselines. The approach is lightweight, reproducible, and effective in balancing visual grounding with narrative quality.

## Method Summary
The method decomposes visual storytelling into two sequential stages: first, a vision-to-language model (ClipCap) generates descriptive captions for each image in a sequence; second, a language-to-language model (BART or T5) transforms these captions into a coherent narrative. The framework employs a "revolving training" strategy that cycles through three caption variants per image from the VIST dataset to enhance robustness. The captioner uses CLIP visual features mapped to a language model prefix, while the storyteller processes concatenated captions with [EOS] tokens as input to generate the final story. The approach is trained separately on different data subsets and achieves competitive performance while maintaining a lightweight architecture.

## Key Results
- The proposed two-stage framework achieves competitive performance across multiple metrics including RoViST, ideality, and standard captioning metrics
- Human and LLM evaluations show generated stories have improved coherence, visual grounding, and human-likeness compared to several state-of-the-art baselines
- The approach balances visual grounding with narrative quality while maintaining a lightweight, reproducible architecture
- Transformer mapper combined with BART shows better diversity and narrative quality compared to MLP mapper with T5

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If visual storytelling is treated as a sequential superset of image captioning, the optimization landscape simplifies by decoupling visual grounding from narrative coherence.
- **Mechanism:** The framework decomposes the end-to-end mapping (pixels → story) into two distinct sub-tasks. First, a "Captioner" (ClipCap) acts as a semantic extractor, freezing visual features into textual descriptions. Second, a "Storyteller" (BART/T5) acts as a style transfer engine, operating solely on text to enforce narrative flow without the burden of visual encoding.
- **Core assumption:** High-quality descriptive captions contain sufficient information to reconstruct the visual context necessary for a story; the text modality is an adequate lossless proxy for the image modality in the second stage.
- **Evidence anchors:**
  - [abstract] "...firstly employ a vision-to-language model for obtaining captions... and then, these captions are transformed into coherent narratives..."
  - [section 3.2] "This means that we firstly employ a vision-to-language model... and then, these captions are transformed... using language-to-language methods."
  - [corpus] Weak direct support in corpus; neighbors like *MM-StoryAgent* utilize multi-agent paradigms but often maintain tighter multimodal integration rather than a pure hand-off to text-only generation.
- **Break condition:** If the captioner generates generic or hallucinated descriptions (visual grounding fails), the storyteller will amplify these errors into narrative nonsense, as it has no access to the original image to correct course.

### Mechanism 2
- **Claim:** Pre-trained language-to-language transformers (BART/T5) can convert disjointed image captions into a coherent narrative by treating the task as a denoising or style-transfer problem.
- **Mechanism:** The storyteller model takes a concatenated sequence of captions separated by `[EOS]` tokens. By training on paired data (Captions → Human Stories), the model learns to insert transitional phrases ("linking words"), resolve coreference (linking "the man" in image 1 to "he" in image 2), and adjust the tense to simulate a story.
- **Core assumption:** The structural difference between a list of captions and a story can be learned via standard cross-entropy loss on sequence-to-sequence architectures without requiring explicit logical reasoning modules.
- **Evidence anchors:**
  - [section 3.2.2] "The reconstructed story... is given by the equation: $\hat{S}_i = D(\tilde{S}_i, E(\tilde{C}_i))$... conditioned on the previously generated content and the captions."
  - [section 6.3] "...our frameworks... exhibits improved logical flow, often utilizing linking words (highlighted in blue) that increase cohesion..."
  - [corpus] *StoryReasoning Dataset* suggests explicit grounding is needed for complex reasoning, implying this mechanism may struggle with deep causal logic between frames compared to surface-level coherence.
- **Break condition:** If the sequence of images implies a causal event not explicitly described in the captions (e.g., a sequence of frames implying a fall), the text-only model cannot "see" the action and may fail to infer the event.

### Mechanism 3
- **Claim:** Data augmentation via "Revolving Training" stabilizes the text-to-text generation by exposing the model to lexical variance while maintaining the target story.
- **Mechanism:** The VIST dataset provides 3 captions per image. Instead of picking one, the training revolves through T1, T2, T3 (caption sets). This forces the Storyteller to map multiple linguistic variations of the same visual concept to a single narrative output, acting as a regularizer.
- **Core assumption:** Variance in caption style does not necessitate variance in story output; the model should converge on the "story" concept regardless of which descriptive variant is input.
- **Evidence anchors:**
  - [section 3.3] "...we simply exploit the different type of descriptions that VIST dataset offers... train our captioner and storyteller on three slightly different sub-datasets..."
  - [section 4.2] "We train both versions... for 15 epochs, which means that for each of the three versions... they get 5 epochs."
  - [corpus] No direct evidence found in neighbors regarding this specific revolving strategy.
- **Break condition:** If the three captions for an image are semantically contradictory, the revolving training could introduce noise, leading the model to hallucinate details that fit one caption but not the image or the overall story.

## Foundational Learning

- **Concept: Prefix Tuning (ClipCap Architecture)**
  - **Why needed here:** The visual encoder (CLIP) outputs static embeddings. You must understand how a "Mapping Network" (MLP or Transformer) projects these visual vectors into the language model's (GPT-2) embedding space to serve as a "prefix" prompt.
  - **Quick check question:** How does the dimensionality of the CLIP visual embedding (512 or 640) affect the design of the mapping network compared to the language model's hidden size?

- **Concept: Sequence-to-Sequence (Seq2Seq) with Encoders**
  - **Why needed here:** The Storyteller (BART/T5) must process a long sequence of concatenated captions. You need to distinguish between the Encoder's role (digesting the context of 5 images) and the Decoder's role (autoregressive generation of the story).
  - **Quick check question:** In the equation $\hat{S}_i = D(\tilde{S}_i, E(\tilde{C}_i))$, what specific information does $E(\tilde{C}_i)$ (encoder memory) pass to the Decoder $D$ at each generation step?

- **Concept: VIST Dataset Structure (DII vs SIS)**
  - **Why needed here:** The method relies on specific data tiers. "Descriptions of Images in Isolation" (DII) are used for Stage 1, while "Stories for Images in Sequence" (SIS) are the targets for Stage 2. Confusing these sets will cause data leakage or training failure.
  - **Quick check question:** If you accidentally used SIS (Story) as input to the Captioner instead of DII, what specific task would you be training the Captioner to perform?

## Architecture Onboarding

- **Component map:** Input: Sequence of 5 Images (Xi) → Stage 1 (Captioner): CLIP ViT/ResNet → Mapping Network (Transformer/MLP) → GPT-2 Small → Intermediate: Concatenated Text Captions (tilde Ci) → Stage 2 (Storyteller): BART Large (Encoder-Decoder) OR T5 Base → Output: Story Text (hat Si)

- **Critical path:** The visual features flow from CLIP → Prefix Mapper → GPT-2. The output text is then tokenized specifically for BART/T5. The critical interaction is the concatenation of captions with `[EOS]` tokens, which signals the storyteller to treat the input as a sequence of distinct events rather than a single run-on sentence.

- **Design tradeoffs:**
  - **MLP vs. Transformer Mapper:** The Transformer mapper captures more complex visual features but is heavier. The paper notes Transformer mappers generally yield better performance (e.g., Transf.+BART outperforms MLP+BART on semantic metrics).
  - **BART vs. T5:** BART (Denoising Autoencoder) tends to produce more coherent narrative flow ("linking words"), while T5 in this specific setup showed higher rates of repetition/struggle with text degeneration in the analysis.

- **Failure signatures:**
  - **Visual Inconsistency:** If the captioner misses a key object, the story will ignore it (Red text in Fig 5).
  - **Text Degeneration:** The T5 model showed susceptibility to repetitive loops ("There were a lot of people there..." repeated).
  - **Loss of Detail:** The two-stage process compresses image pixels → text → story. Fine-grained visual details (e.g., colors, counts) are often lost in the first compression.

- **First 3 experiments:**
  1. **Module Verification:** Run the ClipCap module on a single image from the VIST validation set. Does the generated caption describe the objects accurately enough to serve as a story prompt?
  2. **Input Formatting:** Implement the caption concatenation logic (Eq 4a). Feed a dummy list of 5 strings into BART and verify the tokenizer is adding `[EOS]` correctly and not truncating the context window.
  3. **Ablation on Mapping:** Train two lightweight versions of the Captioner (one with MLP, one with Transformer) for 1 epoch on the T1 subset. Compare the training loss convergence speed to verify the implementation of the mapping network.

## Open Questions the Paper Calls Out

- **Question:** How can Retrieval-Augmented Generation (RAG) or external knowledge graphs be effectively integrated into the modular framework to enhance factual grounding and world knowledge?
  - **Basis in paper:** [explicit] The authors state that their system currently lacks the world-knowledge present in large multimodal LLMs and explicitly aim to explore RAG and knowledge graphs in future work.
  - **Why unresolved:** The current architecture relies on a "text-only" storyteller (BART/T5) disconnected from external databases or real-time information retrieval during inference.
  - **What evidence would resolve it:** A future implementation showing improved performance on benchmarks requiring commonsense reasoning or factual accuracy, along with ablation studies on the retrieval mechanism.

- **Question:** What architectural or loss function refinements are necessary to address the "occasional redundancy or simplistic sentence structure" identified in the generated stories?
  - **Basis in paper:** [explicit] The conclusion lists redundancy and simplistic structure as remaining challenges despite the model's overall competitive performance.
  - **Why unresolved:** The current training uses standard cross-entropy loss on the VIST dataset, which may not sufficiently penalize repetitive n-grams or encourage complex syntactic variations.
  - **What evidence would resolve it:** Demonstration of a modified training objective (e.g., reinforcement learning with a diversity reward) that yields higher distinct-n scores and human ratings for linguistic complexity.

- **Question:** To what extent does the sequential decomposition (Captioner → Storyteller) suffer from error propagation, where inaccuracies in the initial captions degrade the final narrative?
  - **Basis in paper:** [inferred] The "Storyteller" is a language-to-language model that relies solely on the text generated by the "Captioner," cutting off access to the original visual features.
  - **Why unresolved:** The paper evaluates the end-to-end performance but does not isolate the impact of the captioning module's specific errors on the downstream storytelling quality.
  - **What evidence would resolve it:** An ablation study comparing narrative quality when the storyteller is fed ground-truth captions versus model-generated captions, quantifying the performance drop attributable to the captioning stage.

## Limitations

- The two-stage pipeline design introduces error amplification risk where visual grounding errors from the captioner propagate to the storyteller
- The approach assumes captions provide lossless visual context, but fine-grained visual details are inevitably lost during image-to-text compression
- The revolving training strategy's effectiveness for caption variance is not extensively validated against other augmentation techniques
- The reported ideality metric, while novel, relies on sentence embedding similarity which may not fully capture narrative quality nuances

## Confidence

- **High Confidence:** The core decomposition framework (image captioning → story generation) is well-supported by experimental results showing competitive performance across multiple metrics and successful human/LLM evaluation demonstrating improved coherence and visual grounding compared to baselines.
- **Medium Confidence:** The revolving training strategy's effectiveness and the superiority of the Transformer mapper over MLP are supported but could benefit from more extensive ablation studies. The ideality metric's validity as a proxy for human story quality requires further validation.
- **Low Confidence:** Claims about the framework's ability to handle complex causal reasoning between frames are not well-supported, as the method relies on surface-level coherence through linking words rather than deep logical inference. The paper acknowledges this limitation when discussing StoryReasoning Dataset.

## Next Checks

1. **Caption Quality Validation:** Before training the storyteller, evaluate the captioner's output quality on a held-out validation set using standard captioning metrics (CIDEr, SPICE, BLEU). Target competitive captioning performance to ensure the storyteller receives adequate visual information.
2. **Error Amplification Analysis:** Analyze cases where the captioner generates incorrect or generic descriptions, then trace how these errors propagate through the storyteller. Identify failure modes where visual grounding is completely lost.
3. **Revolving Training Ablation:** Conduct controlled experiments comparing the revolving training strategy against standard single-caption training and other augmentation techniques to quantify its specific contribution to model robustness and performance.