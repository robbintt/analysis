---
ver: rpa2
title: 'Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated
  Role-Playing Game Sessions'
arxiv_id: '2503.20623'
source_url: https://arxiv.org/abs/2503.20623
tags:
- features
- language
- written
- sessions
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether LLM-generated text exhibits features
  of spoken or written language by analyzing automatically generated RPG sessions.
  The research compares LLM outputs with transcribed human RPG sessions, conversations,
  academic speeches, and books using lexical and syntactic metrics including D-value,
  concreteness, sentence length, subordinate clauses, and cohesion.
---

# Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions

## Quick Facts
- arXiv ID: 2503.20623
- Source URL: https://arxiv.org/abs/2503.20623
- Authors: Alessandro Maisto
- Reference count: 37
- Key outcome: LLM-generated text exhibits written-like lexical patterns (higher D-value, complexity) in RPG sessions despite spoken-genre context, with low cohesion and present-tense dominance compared to human transcripts.

## Executive Summary
This study investigates whether LLM-generated text exhibits features of spoken or written language by analyzing automatically generated RPG sessions. The research compares LLM outputs with transcribed human RPG sessions, conversations, academic speeches, and books using lexical and syntactic metrics including D-value, concreteness, sentence length, subordinate clauses, and cohesion. Results show that LLM-generated text exhibits a unique linguistic pattern: higher lexical diversity and complexity than human RPG sessions, longer sentences with lower root distance, extensive use of present tense, and lower textual cohesion. While LLMs produce more descriptive and syntactically complex language than human players, they lack the cohesive features typical of natural spoken discourse.

## Method Summary
The study generated 8 RPG sessions using 4 LLM instances (1 GM + 3 players) via Groq API with LLaMa3-8b-8192. System messages included adventure and character descriptions. Sessions followed a circular turn order (GM→Grog→Pike→Vax→GM) with temperature settings of 0.3-1.0 and token limits of 50-200. Linguistic features were extracted using Stanford CoreNLP and compared against reference corpora: 8 BNC conversations, 8 ELFA academic speeches, 8 Critical Role transcripts, and 8 fantasy novels. Metrics included D-value (lexical diversity), Lexical Range (LR1-LR3), concreteness, deictics ratio, emphatic particles, sentence length, subordinate/relative clause ratios, ROOT distance, verb tense ratios, and cohesion scores.

## Key Results
- LLM RPG sessions showed D-value of 19.8, closely matching books (19.9) and far exceeding human RPG sessions (8.2)
- LLM outputs exhibited present tense ratio of 0.72, significantly higher than speech corpora (~0.50)
- Cohesion scores for LLM-generated text were very low (0.007) compared to human RPG (0.105)
- LLM-generated sentences were longer with lower root distance, indicating more complex syntax than human speech

## Why This Works (Mechanism)

### Mechanism 1: Training Data Vocabulary Transfer
- Claim: LLMs produce written-like lexical patterns in spoken-genre contexts due to training data dominance.
- Mechanism: Written corpora in training data (books, academic text) transfer vocabulary density and complexity to outputs, even when task context (RPG sessions) typically demands spoken-register language.
- Core assumption: Training data composition biases lexical selection independent of prompt instructions.
- Evidence anchors:
  - [abstract] "LLMs generate language more characteristic of written text despite being trained on diverse data sources"
  - [section 3.3] LLM RPG D-value (19.8) closely matches books (19.9), far exceeds human RPG (8.2); LR3 (academic vocabulary) highest for LLM RPG (0.04) vs human RPG (0.03)
  - [corpus] Neighbor paper "RPGBENCH" evaluates LLMs as RPG engines but does not directly address training data influence on register.
- Break condition: If fine-tuning on spoken transcripts reduces lexical complexity to human RPG levels, training transfer is modifiable.

### Mechanism 2: Present-Tense Narrative Bias
- Claim: LLMs over-rely on present tense due to action-oriented generation without temporal context planning.
- Mechanism: Without explicit backstory scaffolding, models default to immediate-action description (present tense: 0.72 ratio) rather than alternating tenses for temporal context (human RPG mixes present/past).
- Core assumption: Temporal discourse planning requires explicit context or instruction not captured in standard prompts.
- Evidence anchors:
  - [section 3.3] "The LLMs almost exclusively used the present tense, occasionally replaced by participles, but rarely used the past tense"
  - [section 3.4] Present tense ratio 0.72 for LLM vs ~0.50 for speech corpora; human RPG intersperses past tense (0.08)
  - [corpus] Weak direct evidence; neighbor papers focus on gameplay mechanics, not tense distribution.
- Break condition: If prompts requiring backstory references increase past-tense usage proportionally, temporal planning is prompt-controllable.

### Mechanism 3: Cohesion Deficit from Local Generation
- Claim: LLM outputs lack discourse-level cohesion because generation optimizes local token coherence, not global connective structure.
- Mechanism: Causal and logical connectives require multi-sentence planning; autoregressive generation without explicit discourse modeling yields low cohesion scores (0.007 for LLM vs 0.105 for human RPG).
- Core assumption: Cohesion requires explicit discourse-level objectives not captured in next-token prediction.
- Evidence anchors:
  - [section 3.3] "The score obtained by LLM-generated texts was very low (0.007)"; lack of logical and causal connectives
  - [section 3.4] "LLMs exhibit low cohesion, with limited use of logical and causal connectives"
  - [corpus] No direct corpus evidence on cohesion mechanisms in LLMs.
- Break condition: If prompting for explicit connective categories or multi-turn planning improves cohesion scores, local generation is addressable.

## Foundational Learning

- **Concept: Spoken vs. Written Discourse Features (Ochs/Chafe)**
  - Why needed here: The entire analysis framework depends on distinguishing registers (deictics, subordinate clauses, first-person pronouns, cohesion devices).
  - Quick check question: Given a transcript with high concreteness, first-person pronouns, and low subordinate clauses, would you classify it as spoken or written register?

- **Concept: D-Value and Lexical Diversity Metrics**
  - Why needed here: Primary quantitative measure distinguishing LLM output from human RPG; less length-sensitive than Type-Token Ratio.
  - Quick check question: Why might D-value be more reliable than TTR for comparing texts of different lengths?

- **Concept: Dependency Parsing and Root Distance**
  - Why needed here: Explains syntactic complexity findings (LLMs produce longer sentences with centered roots vs. human speech with scattered roots).
  - Quick check question: What does "low distance from ROOT" indicate about sentence structure compared to high distance?

## Architecture Onboarding

- **Component map:**
  - 4 LLM instances via Groq API (LLaMa3-8b-8192): 1 Game Master + 3 Player Characters
  - System messages: Adventure description (GM), character descriptions (PCs)
  - Response loop: GM → Grog → Pike → Vax → GM (circular, max 200 interactions/player)
  - Temperature settings: 0.3-1.0 (GM), 0.3-0.7 (PCs); token limits: 50-200 per response

- **Critical path:**
  1. Define system prompts with character/adventure descriptions
  2. Initialize loop with "Please start the adventure!"
  3. Aggregate responses sequentially (concatenate prior responses as input)
  4. Extract linguistic features via CoreNLP + custom Java module
  5. Compare against reference corpora (BNC conversations, ELFA academic speech, Critical Role, fantasy books)

- **Design tradeoffs:**
  - Higher temperature → more linguistic variation but less consistent character voice
  - Token limits → shorter responses may underrepresent syntactic complexity
  - Single-model architecture (all instances same base model) → may homogenize DM vs PC language (observed: LLM-DM and LLM-PC remarkably similar)

- **Failure signatures:**
  - Repetitive NPC names (e.g., "Grimbold" across sessions) → insufficient prompt diversity or temperature too low
  - Low cohesion + present-tense dominance → missing discourse-level planning
  - PC dialogues too complex/descriptive → model not distinguishing spoken register

- **First 3 experiments:**
  1. **Ablate temperature systematically:** Run sessions at 0.3, 0.5, 0.7, 1.0 across all instances; measure D-value, cohesion, tense ratio to isolate temperature effects on register.
  2. **Add explicit cohesion prompting:** Include instructions like "Use causal and temporal connectives to link events"; compare cohesion scores (0.007 baseline) to prompted condition.
  3. **Hybrid human-LLM sessions:** Replace one PC instance with human player; measure whether human-LLM interaction shifts LLM's lexical/syntactic patterns toward spoken register.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning or reinforcement learning techniques train LLMs to generate narratives with oral discourse features and improved textual cohesion comparable to human RPG sessions?
- Basis in paper: [explicit] Conclusion states: "Further research is needed to...investigate reinforcement learning techniques for training LLMs to generate more human-like narratives, including affective analysis."
- Why unresolved: Current LLMs show low cohesion (0.007 vs 0.105 for human RPG) and lack causal/logical connectives. This study only analyzed base model outputs without intervention.
- What evidence would resolve it: Pre/post comparison of LLM outputs after cohesion-focused fine-tuning, measuring weighted cohesion scores and connective frequency improvements.

### Open Question 2
- Question: Do the identified linguistic patterns (present tense dominance, low cohesion, high lexical diversity) generalize across different LLM architectures and parameter scales beyond LLaMa3-8b?
- Basis in paper: [inferred] Study used only one model (LLaMa3-8b-8192) via Groq API. The authors note patterns "distinct from all other text categories" but cannot determine if this is architecture-specific.
- Why unresolved: Single model study cannot distinguish between universal LLM characteristics and LLaMa-specific behaviors.
- What evidence would resolve it: Replication across multiple model families (GPT, Claude, Mistral, etc.) and sizes, applying the same linguistic metrics.

### Open Question 3
- Question: What explains LLMs' near-exclusive use of present tense (0.72 ratio) compared to human RPG sessions' mixed tense usage, and can this be modified?
- Basis in paper: [explicit] Authors note this as a "key difference" and state further research is needed to "develop methods for improving...verb tense usage." They hypothesize it relates to focus on "immediate actions" vs. backstories.
- Why unresolved: Mechanism unclear whether this stems from training data, prompting, or architectural biases. No intervention was tested.
- What evidence would resolve it: Controlled experiments varying prompt instructions about temporal context; analysis of training corpus tense distributions.

## Limitations
- The study's conclusions rest on comparisons between LLM-generated RPG sessions and reference corpora that may not perfectly represent the linguistic phenomena under investigation.
- The LLM sessions were generated with temperature settings (0.3-1.0) that introduce variability not systematically controlled across experiments.
- The feature extraction relies on predefined dictionaries (deictics, emphatic particles, attributive adjectives) without validation of their completeness or applicability to the specific genre.

## Confidence

- **High confidence**: The finding that LLM-generated text shows higher lexical diversity (D-value 19.8) than human RPG sessions (8.2) is robust and directly measurable from the data.
- **Medium confidence**: The conclusion about LLM's present-tense bias (0.72 ratio vs ~0.50 for speech) is supported by the data but depends on the assumption that the reference corpora adequately represent natural spoken discourse.
- **Medium confidence**: The claim about low cohesion (0.007 vs 0.105 for human RPG) is well-supported by connective frequency counts but may oversimplify the complex nature of discourse coherence.
- **Low confidence**: The interpretation that LLMs generate "written-like" language is somewhat speculative, as the distinction between spoken and written registers involves many factors not fully explored in this study.

## Next Checks

1. **Replicate with controlled temperature settings**: Generate LLM sessions at fixed temperatures (0.3, 0.5, 0.7, 1.0) across all instances to isolate temperature effects on linguistic features, particularly D-value and cohesion scores.

2. **Validate cohesion metric**: Compare the connective-based cohesion scores against alternative metrics like Coh-Metrix or discourse coherence measures to ensure the 0.007 score meaningfully reflects discourse quality.

3. **Test prompt engineering effects**: Create explicit instructions for LLMs to use past tense and connective devices, then measure whether these targeted prompts significantly alter the linguistic patterns observed in the baseline sessions.