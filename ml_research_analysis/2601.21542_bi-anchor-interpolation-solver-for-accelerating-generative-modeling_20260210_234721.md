---
ver: rpa2
title: Bi-Anchor Interpolation Solver for Accelerating Generative Modeling
arxiv_id: '2601.21542'
source_url: https://arxiv.org/abs/2601.21542
tags:
- ba-solver
- solver
- nfes
- sidenet
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Bi-Anchor Interpolation Solver (BA-solver),
  a novel approach to accelerate sampling in Flow Matching (FM) models. Existing methods
  face a trade-off: training-free solvers require many sequential function evaluations
  (NFEs) for high fidelity, while training-based few-step methods are computationally
  expensive.'
---

# Bi-Anchor Interpolation Solver for Accelerating Generative Modeling

## Quick Facts
- arXiv ID: 2601.21542
- Source URL: https://arxiv.org/abs/2601.21542
- Reference count: 27
- One-line primary result: Achieves FID 1.96 at 7 NFE on ImageNet-256² using bi-anchor interpolation and a lightweight SideNet.

## Executive Summary
This paper introduces the Bi-Anchor Interpolation Solver (BA-solver), a novel approach to accelerate sampling in Flow Matching (FM) models. Existing methods face a trade-off: training-free solvers require many sequential function evaluations (NFEs) for high fidelity, while training-based few-step methods are computationally expensive. BA-solver bridges this gap by adding a lightweight SideNet (1-2% of backbone size) alongside a frozen backbone, granting it bidirectional temporal perception. It uses two anchor velocities—one from the backbone and one predicted—to efficiently approximate intermediate velocities for high-order numerical integration, enabling large interval sizes with minimal error. On ImageNet-256², BA-solver achieves FID 1.96 with just 7 NFEs, matching 100+ NFE Euler solver quality, and maintains high fidelity with as few as 5 NFEs. It is also parameter- and training-efficient, converging in just 250 iterations versus hundreds of thousands for other methods.

## Method Summary
BA-solver accelerates FM sampling by combining a frozen pre-trained backbone with a lightweight SideNet that predicts velocity deviations at temporal offsets. The SideNet, trained via chain-based simulation with velocity matching loss, learns to provide bidirectional temporal perception (lookahead and lookback). During inference, BA-solver uses bi-anchor interpolation: it performs a forward probe with the SideNet, queries the backbone for a terminal anchor velocity, and refines predictions backward using high-order quadrature (e.g., 4-point Gauss-Lobatto). A state reuse mechanism ensures exactly one backbone evaluation per interval, maintaining the exact-N NFE cost. This allows large integration steps while minimizing error accumulation.

## Key Results
- Achieves FID 1.96 at 7 NFE on ImageNet-256², matching 100+ NFE Euler solver quality.
- Maintains high fidelity with as few as 5 NFEs.
- Training-efficient: converges in just 250 iterations vs. hundreds of thousands for other methods.
- Parameter-efficient: SideNet is only 1-2% of backbone size (~6M params vs. 675M backbone).

## Why This Works (Mechanism)

### Mechanism 1: Bi-Anchor Error Correction
Using two anchor velocities (start and terminal) reduces interpolation error compared to single-anchor extrapolation, enabling larger integration steps. The SideNet predicts velocity deviations from both the current velocity $v_t$ (forward probe) and the estimated terminal velocity $v_{t-h}$ (backward refinement). By partitioning quadrature nodes based on proximity to either anchor, the maximum prediction offset is halved from $h$ to $h/2$. Since extrapolation error scales with offset distance (due to state drift that the SideNet cannot perceive), this reduces error accumulation. The theoretical local truncation error bound is reduced by approximately half compared to single-anchor interpolation (from $\frac{1}{2}LCh^2$ to $\frac{1}{4}LCh^2$).

### Mechanism 2: Parameter-Efficient Temporal Perception via Lightweight SideNet
A small auxiliary network (1-2% of backbone size) can learn to predict velocity deviations at temporal offsets, adding "lookahead" and "lookback" capabilities without modifying the frozen backbone. The SideNet $S_\phi$ is conditioned on the current state $x_t$, current velocity $v_t$, time $t$, and temporal offset $\Delta t$ to predict a velocity correction term: $\hat{v}_{t+\Delta t} = v_t + \Delta t \cdot S_\phi(x_t, v_t, t, \Delta t)$. The formulation ensures the prediction equals the backbone velocity when $\Delta t = 0$, providing a performance lower bound. Because gradients are stopped at the backbone during chain-based training, only the lightweight SideNet parameters are updated, allowing efficient learning of temporal dynamics.

### Mechanism 3: High-Order Integration with State Reuse
Combining bi-anchor velocity predictions with high-order quadrature rules (e.g., 4-point Gauss-Lobatto) and caching the terminal anchor for the next step achieves high-fidelity sampling with exact N NFEs. Within each interval, after computing the forward probe and backward refined velocities, BA-solver integrates them using a high-order quadrature rule (e.g., Gauss-Lobatto, which has algebraic precision of degree $2n-3=5$ for n=4 nodes, yielding theoretical LTE of $O(h^7)$). Crucially, the terminal anchor $v_{t-h}$ computed in the current interval is cached and reused as the starting anchor for the next interval (State Reuse mechanism). This ensures exactly one backbone evaluation per interval.

## Foundational Learning

- **ODE Solvers in Generative Models (Flow Matching/Diffusion)**: Why needed here: BA-solver fundamentally operates by modifying the numerical integration of the probability flow ODE. Understanding the tradeoffs between extrapolation (e.g., Euler) and interpolation (e.g., Heun) solvers, and concepts like Neural Function Evaluations (NFEs), local truncation error, and step size $h$ is essential to grasp *why* acceleration is needed and how BA-solver achieves it. Quick check question: Can you explain why a 1st-order Euler solver requires many more NFEs than a higher-order interpolation solver to achieve the same error tolerance, and what "NFE" represents in the context of a neural network-based generative model?

- **Parameter-Efficient Fine-Tuning (PEFT) and Side-Tuning**: Why needed here: The SideNet architecture is a form of side-tuning, where a small network is trained alongside a frozen backbone. Understanding that gradients are stopped at the backbone, reducing memory and compute costs, is key to appreciating the "training-efficient" claim. Quick check question: In a side-tuning setup where a lightweight SideNet is added to a large frozen backbone, how does the backward pass differ from full fine-tuning, and what are the implications for GPU memory usage during training?

- **Numerical Integration (Quadrature Rules)**: Why needed here: BA-solver replaces standard Euler/Heun updates with integration using high-order quadrature rules (Gauss-Legendre, Gauss-Lobatto). Understanding what quadrature nodes and weights are, and how their placement affects integration accuracy, is necessary to interpret the method's mechanics. Quick check question: What is the key difference between Gauss-Legendre and Gauss-Lobatto quadrature in terms of node placement, and why might including endpoints (as in Gauss-Lobatto) be beneficial when you already have anchor velocities at the interval boundaries?

## Architecture Onboarding

- **Component map**: Frozen Backbone ($v_\theta$) -> SideNet ($S_\phi$) -> Solver Logic (Algorithm 1) -> Training Loop (Algorithm 2)
- **Critical path**: The critical path for inference performance is: Backbone Forward Pass (1 NFE per interval) -> SideNet Batched Forward Pass (negligible cost) -> Quadrature Integration. The backbone forward pass dominates latency. The SideNet operations must be efficiently batched (predicting multiple offsets in parallel) to ensure they remain "negligible" as claimed.
- **Design tradeoffs**:
  - SideNet Size vs. Accuracy: A larger SideNet may capture more complex temporal dynamics but increases the "negligible" cost and may overfit with the very limited training iterations (250-500).
  - Number of Quadrature Nodes (K) vs. NFE: More nodes provide higher integration order but offer diminishing returns if the SideNet's own prediction error dominates. The paper finds K=2 (with 4-point Gauss-Lobatto including anchors) is a sweet spot.
  - Chain Length (Training) vs. Stability: Longer chains in training better simulate inference error accumulation but increase training graph complexity and memory. A chain length of 8 is used.
- **Failure signatures**:
  1. FID degrades sharply as NFE decreases (e.g., at 3 NFE): Suggests SideNet prediction error or quadrature error at large intervals $h$ is too high; may need larger SideNet, more training, or a lower-order fallback for very low NFE.
  2. Training instability / loss divergence: Check learning rate or chain length; the velocity matching target depends on the backbone's output at simulated states, which may become out-of-distribution if the solver simulation drifts too far.
  3. No speedup observed: Verify that SideNet predictions are truly batched and executed on the same device as the backbone. If run sequentially or on CPU, the "negligible" cost assumption breaks.
- **First 3 experiments**:
  1. Baseline Reproduction: Implement BA-solver with the specified SideNet architecture and training settings. Train for 250 iterations on a subset of ImageNet and measure FID vs. NFE compared to a frozen backbone using standard Euler solver. Verify the claimed FID improvement at 5-10 NFEs.
  2. Ablation: Bi-Anchor vs. Single-Anchor: Run the trained model with only the Forward Probe phase (using $v_t$ as the sole anchor) and compare FID. This validates the core mechanism's contribution (expected significant FID degradation per Table 3).
  3. Ablation: Quadrature Rule Comparison: Swap the 4-point Gauss-Lobatto rule for Simpson's rule or a different K value (as in Table 3) and measure the impact on FID at a fixed NFE (e.g., 7). This isolates the contribution of the numerical integration scheme.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Limited ablation on SideNet capacity scaling to confirm the claimed 1-2% is optimal.
- Reliance on a specific backbone (REPA-enhanced SiT) may limit generalizability to other FM architectures.
- Lack of comparison to other acceleration methods (e.g., score distillation, predictor-corrector) under identical backbones.

## Confidence
- **High**: Interpolation error reduction mechanism (bi-anchoring halves prediction offset), parameter efficiency (1-2% SideNet, exact-N NFE), FID improvement at low NFE (5-10).
- **Medium**: Training efficiency (250 iterations vs. hundreds of thousands), generalizability across FM backbones, scalability to higher resolutions.

## Next Checks
1. **Training Efficiency Validation**: Replicate training with varying SideNet sizes (e.g., 0.5%, 2%, 5% of backbone) to confirm that the claimed 1-2% is optimal and that convergence speed is not bottlenecked by SideNet capacity.
2. **Backbone Generalization Test**: Apply BA-solver to a different FM architecture (e.g., DiT or NCSN++) on a held-out dataset (e.g., LSUN Church) to assess whether the bi-anchoring mechanism and training efficiency hold without REPA/SiT-specific priors.
3. **Error Accumulation Analysis**: Run sampling with NFE < 5 (e.g., 3-4) and measure FID degradation curve. Compare against a theoretical error bound extrapolation to identify whether the breakdown is due to SideNet prediction error or quadrature instability at large h.