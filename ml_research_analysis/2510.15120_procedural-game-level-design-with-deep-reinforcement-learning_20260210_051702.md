---
ver: rpa2
title: Procedural Game Level Design with Deep Reinforcement Learning
arxiv_id: '2510.15120'
source_url: https://arxiv.org/abs/2510.15120
tags:
- agent
- learning
- terrain
- flower
- island
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a procedural content generation (PCG) framework
  that integrates two deep reinforcement learning (DRL) agents within Unity: a hummingbird
  agent that learns to collect flowers and an island agent that generates diverse,
  context-aware flower layouts. The agents are trained using the Proximal Policy Optimization
  (PPO) algorithm.'
---

# Procedural Game Level Design with Deep Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2510.15120
- **Source URL:** https://arxiv.org/abs/2510.15120
- **Reference count:** 22
- **Primary result:** Dual PPO agents (hummingbird collector + island generator) achieve 90.2% success rate with 12.4 flowers/episode in procedurally generated environments

## Executive Summary
This paper presents a co-adaptive procedural content generation framework using two deep reinforcement learning agents within Unity. The hummingbird agent learns to navigate and collect flowers, while the island agent generates context-aware flower layouts. Both agents are trained using Proximal Policy Optimization (PPO), with the island agent dynamically adjusting spawn parameters based on the hummingbird's performance feedback. The system demonstrates that environment and agent can evolve together to create engaging, procedurally generated content, with the hummingbird achieving 90.2% success rate across varied layouts.

## Method Summary
The framework implements dual PPO agents where a hummingbird solver collects nectar from procedurally placed flowers while an island generator adjusts spawn radius and congestion parameters. The hummingbird agent receives rich observations including raycasts, terrain normals, and relative positions, enabling robust navigation. The island agent observes obstacle positions, initial hummingbird state, and performance feedback to place flowers using a penalty function for invalid placements. Training occurs over 20M steps with 8 parallel environments, using custom reward shaping that balances nectar collection against collision penalties and parameter deviation costs.

## Key Results
- Hummingbird agent achieves 90.2% success rate in collecting all flowers across varied layouts
- Average of 12.4 flowers collected per episode with 1.35 reward per step
- Island agent's placement quality improves over training, demonstrating emergent behaviors
- Co-adaptive setup shows robust generalization across dynamically generated environments

## Why This Works (Mechanism)
The system works by creating a closed feedback loop where the environment generator (island) adapts to the agent's (hummingbird's) capabilities and performance. The island agent uses the hummingbird's success metrics as observations to inform its placement decisions, while the hummingbird learns to navigate whatever layouts the island generates. This co-evolutionary process naturally balances difficulty - the island creates challenging but solvable layouts because unsolvable ones result in poor hummingbird performance, which the island learns to avoid through its reward structure.

## Foundational Learning
- **Dual-agent PPO training:** Understanding how two agents can be trained simultaneously with interdependent objectives. Why needed: The core innovation is the co-adaptive training loop. Quick check: Verify both agents' reward curves show learning progress without catastrophic collapse.
- **Procedural content generation via RL:** Using RL agents to generate game content rather than traditional procedural methods. Why needed: Enables dynamic, context-aware level design that adapts to agent capabilities. Quick check: Compare generated layouts to hand-crafted ones for playability.
- **Rich observation spaces for navigation:** The hummingbird's 9-dimensional observation vector including terrain normals and relative positions. Why needed: Enables robust navigation across varied, dynamically generated terrain. Quick check: Test navigation success rate with reduced observation dimensions.

## Architecture Onboarding

**Component Map:**
Unity Environment -> Hummingbird Agent (PPO) -> Flower Collection Performance -> Island Agent (PPO) -> Flower Placement Parameters -> Unity Environment

**Critical Path:**
1. Environment initialization with obstacles and hummingbird spawn
2. Island agent places flowers based on current parameters
3. Hummingbird agent collects flowers using raycasts and terrain information
4. Episode ends, feedback (nectar, collisions) sent to island agent
5. Island updates placement strategy for next episode

**Design Tradeoffs:**
- **Feedback frequency:** Island updates per episode vs. per step - episodic feedback simplifies training but may slow adaptation
- **Observation richness:** 9 dimensions for hummingbird vs. minimal for island - balances learning complexity with performance
- **Reward shaping:** Custom rewards with penalty functions vs. pure success metrics - provides stability but may constrain creativity

**Failure Signatures:**
- Oscillating spawn parameters (r, c) indicate unstable feedback loop
- Hummingbird hovering without collecting suggests collision avoidance dominates nectar rewards
- Poor convergence indicates reward weight imbalance or inadequate exploration

**3 First Experiments:**
1. Train hummingbird alone with static layouts to establish baseline performance
2. Train island agent with random parameters to verify it can learn placement quality
3. Enable full co-adaptive training and monitor parameter convergence

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the co-adaptive framework scale when multiple solver agents (hummingbirds) interact within the same generated environment? The current system supports only a single hummingbird agent; extending to multiple agents introduces complexity regarding shared reward shaping strategies and coordination mechanisms.
- **Open Question 2:** Can the island agent effectively learn to generate content using more complex design actions, such as dynamic terrain deformation or moving obstacles? The current island agent's action space is restricted to two continuous parameters (radius and congestion) and does not possess the action space or mechanism to alter the environment's geometry or physics dynamically.
- **Open Question 3:** Can the island agent learn to generate high-quality layouts using only gameplay performance metrics, without relying on hand-crafted penalty functions? The current system depends on explicit heuristic penalties to guide the island agent, and it is unclear if the agent can infer physical constraints purely from the hummingbird's success/failure feedback.
- **Open Question 4:** What mechanisms can effectively mitigate feedback loop volatility and oscillatory dynamics during co-adaptive training? The current solution relies on simple clipping of parameters to prevent instability, suggesting a lack of a robust theoretical or algorithmic solution to the inherent instability of dual-agent co-evolution.

## Limitations
- **Reward function ambiguity:** Critical scalar weights (α, β, γ, δ) and target congestion (c*) are not specified, requiring empirical tuning
- **Network architecture unspecified:** Number of hidden layers and units per layer for PPO policy/value networks not mentioned
- **Training frequency uncertainty:** Unclear if island agent updates every step or once per episode, affecting feedback loop dynamics

## Confidence
- **High Confidence:** Dual-agent PPO framework architecture and training infrastructure (8 parallel environments, 20M steps) are clearly specified and reproducible
- **Medium Confidence:** Observation spaces and action specifications for both agents are detailed enough for implementation, though exact raycast configurations may require empirical tuning
- **Low Confidence:** Reward function parameters and neural network architecture details are critical missing pieces that significantly impact training outcomes

## Next Checks
1. Implement reward function sensitivity analysis by testing multiple weight combinations to identify stable training regimes
2. Verify Island Agent update frequency (per-step vs. per-episode) through ablation studies measuring training stability and convergence
3. Validate network architecture choices by comparing ML-Agents defaults against potential over/under-fitting scenarios using early stopping metrics