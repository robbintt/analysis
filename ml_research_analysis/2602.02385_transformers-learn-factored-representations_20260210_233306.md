---
ver: rpa2
title: Transformers learn factored representations
arxiv_id: '2602.02385'
source_url: https://arxiv.org/abs/2602.02385
tags:
- factored
- each
- factor
- factors
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper shows that transformers pretrained on next-token prediction
  learn to decompose their world into conditionally independent factors, representing
  each factor in orthogonal subspaces of the residual stream. The authors formalize
  two competing representational hypotheses: a joint representation requiring exponentially
  many dimensions versus a factored representation using only linearly many dimensions.'
---

# Transformers learn factored representations
## Quick Facts
- **arXiv ID**: 2602.02385
- **Source URL**: https://arxiv.org/abs/2602.02385
- **Reference count**: 40
- **Primary result**: Transformers learn to decompose world into conditionally independent factors represented in orthogonal subspaces

## Executive Summary
This paper demonstrates that transformers trained on next-token prediction naturally learn to decompose their world into conditionally independent factors, representing each factor in orthogonal subspaces of the residual stream. The authors formalize a fundamental tradeoff between joint representations (requiring exponentially many dimensions) versus factored representations (using only linearly many dimensions). Through theoretical analysis and synthetic experiments, they show transformers consistently learn factored representations when factors are conditionally independent, and critically exhibit an inductive bias toward factoring even when the data-generating process violates conditional independence, preferring the dimensionally efficient but lossy factored representation early in training.

## Method Summary
The authors develop a theoretical framework comparing joint versus factored representations under conditional independence assumptions. They construct synthetic experiments with controlled generative processes where multiple factors (discrete variables) generate observations through a known conditional probability distribution. The experiments employ dimensionality analysis using PCA, linear regression to ground-truth belief states, and orthogonality metrics to verify the geometric structure of learned representations. They systematically vary the degree of conditional independence between factors and track representation structure across training time to identify inductive biases.

## Key Results
- Transformers consistently learn factored representations in orthogonal subspaces when factors are conditionally independent
- Transformers exhibit inductive bias toward factoring even when data violates conditional independence assumptions
- Factored representations require only linearly many dimensions versus exponentially many for joint representations
- Early in training, transformers prefer the dimensionally efficient but lossy factored representation

## Why This Works (Mechanism)
Transformers trained on next-token prediction must learn to represent and reason about the underlying structure of their input data. When factors of variation are conditionally independent, representing them jointly requires exponentially many dimensions to capture all possible combinations, while representing them separately requires only linearly many dimensions. The transformer architecture, through its attention and residual connections, can learn to project different factors into orthogonal subspaces of the residual stream. This geometric separation allows the model to efficiently represent and manipulate each factor independently while maintaining the ability to recombine them for prediction.

## Foundational Learning
- **Conditional independence**: Understanding when knowing one variable provides no information about another given a third variable; needed to establish when factored representations are valid and efficient.
- **Exponential vs linear dimensionality**: Grasping how joint representations scale with the number of factors (exponentially) versus factored representations (linearly); critical for understanding the efficiency advantage.
- **Orthogonal subspaces**: Knowledge of how vectors can be separated into perpendicular directions in high-dimensional space; essential for understanding how transformers geometrically separate different factors.
- **Next-token prediction objective**: Understanding how predicting the next token requires representing and reasoning about the underlying generative factors; provides the learning signal for factorization.
- **Residual stream geometry**: Familiarity with how information flows and is represented in the transformer's residual stream; necessary to understand where and how factorization occurs.
- **Inductive bias**: Concept of systematic preferences in learning algorithms toward certain solution types; explains why transformers favor factored representations.

## Architecture Onboarding
**Component Map**: Input Tokens -> Embedding Layer -> Residual Stream -> Attention Layers -> Output Logits
**Critical Path**: The residual stream serves as the primary substrate where factored representations emerge through the interaction of attention mechanisms across layers.
**Design Tradeoffs**: The model balances representational efficiency (favoring factored representations) against expressiveness (joint representations can capture arbitrary dependencies).
**Failure Signatures**: When factors are strongly dependent, the model may produce inconsistent or lossy representations that fail to capture the true joint distribution.
**3 First Experiments**:
1. Vary the degree of conditional independence between factors and measure the emergence of orthogonal subspaces
2. Track the evolution of factored structure across training time to identify when the inductive bias manifests
3. Test whether factored representations transfer to downstream tasks or remain tied to the pretraining objective

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes conditional independence, which rarely holds perfectly in real-world data
- Synthetic experiments use simplified generative processes that may not capture natural language complexity
- Linear methods (PCA, regression) may not capture nonlinear representational structures in deeper layers
- Orthogonality metrics assume Euclidean geometry which may not reflect true transformer representation properties
- Early training bias claim needs more rigorous temporal analysis across model sizes

## Confidence
- **High Confidence**: The core theoretical result showing factored representations require fewer dimensions than joint representations under conditional independence (mathematical proof is sound)
- **Medium Confidence**: The experimental demonstration that transformers learn factored representations in synthetic settings (results are consistent but synthetic setup has limitations)
- **Low Confidence**: The claim about transformers' inductive bias toward factoring in non-independent cases (based on limited experiments and requires more extensive validation)

## Next Checks
1. Test the factored representation hypothesis on more complex generative processes with partial dependencies between factors to understand the robustness of the inductive bias
2. Apply the dimensionality and orthogonality analysis to real-world transformer models trained on natural language to verify if factored structure persists beyond synthetic settings
3. Investigate whether the factored representation pattern holds across different transformer architectures (decoder-only, encoder-only, encoder-decoder) and at different layers within models to understand the generality of the finding