---
ver: rpa2
title: 'Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced
  Hallucination in Large Language Models'
arxiv_id: '2505.00557'
source_url: https://arxiv.org/abs/2505.00557
tags:
- tarot
- hallucination
- elements
- table
- periodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates prompt-induced hallucination in large language
  models (LLMs) by introducing Hallucination-Inducing Prompts (HIPs) that fuse semantically
  distant concepts (e.g., periodic table and tarot) and Hallucination Quantifying
  Prompts (HQPs) to evaluate the resulting outputs. Controlled experiments across
  six LLMs revealed significant variation in hallucination susceptibility, with DeepSeek-R1
  and DeepSeek showing the highest scores (8.2 and 7.73 out of 10) while Gemini2.5Pro
  demonstrated the lowest (3.07 out of 10).
---

# Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models

## Quick Facts
- **arXiv ID**: 2505.00557
- **Source URL**: https://arxiv.org/abs/2505.00557
- **Authors**: Makoto Sato
- **Reference count**: 0
- **Primary result**: HIPs (Hallucination-Inducing Prompts) that fuse semantically distant concepts (e.g., periodic table and tarot) consistently produce more hallucinated responses than null-fusion controls across six tested LLMs.

## Executive Summary
This study investigates prompt-induced hallucination by introducing Hallucination-Inducing Prompts (HIPs) that fuse semantically distant concepts and Hallucination Quantifying Prompts (HQPs) to evaluate the resulting outputs. Controlled experiments across six LLMs revealed significant variation in hallucination susceptibility, with DeepSeek-R1 and DeepSeek showing the highest scores (8.2 and 7.73 out of 10) while Gemini2.5Pro demonstrated the lowest (3.07 out of 10). Reasoning-oriented models exhibited distinct profiles compared to general-purpose ones, with DeepSeek-R1 scoring significantly higher than its general-purpose counterpart (p ≈ 0.048). HIPs consistently produced more hallucinated responses than null-fusion controls, demonstrating that conceptual fusion without logical grounding acts as a key trigger for hallucination. The study establishes a reproducible framework for quantifying LLM hallucination vulnerability and highlights the importance of prompt design in both safe deployment and diagnostic probing of model behavior.

## Method Summary
The study employed a two-prompt framework: Hallucination-Inducing Prompts (HIPs) that fuse semantically distant concepts, and Hallucination Quantifying Prompts (HQPs) that score the resulting outputs on a 0-10 scale. Six LLMs were tested with HIPc (conceptually fused) and HIPn (non-fused) variants, along with TIPcs (logically fused) as controls. Responses were scored by GPT-o3 in disposable sessions to prevent context contamination. The experimental design controlled for token count (29-30 tokens, two sentences) and included null-fusion controls to isolate the effect of conceptual fusion.

## Key Results
- DeepSeek-R1 and DeepSeek showed highest hallucination susceptibility (8.2 and 7.73 out of 10)
- Gemini2.5Pro demonstrated the lowest hallucination scores (3.07 out of 10)
- HIPc consistently produced significantly more hallucinated responses than HIPn (p < 0.01)
- Reasoning-oriented DeepSeek-R1 scored significantly higher than general-purpose DeepSeek (p ≈ 0.048)
- Logically grounded fusion (TIPcs) produced lower hallucination scores than even non-fusion prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conceptual fusion without logical grounding triggers hallucination-like breakdowns in LLM outputs.
- Mechanism: When prompts synthetically fuse semantically distant domains (e.g., "periodic table" + "tarot divination"), models attempt semantic reconciliation but lack cognitive filtering to discard epistemically invalid blends—instead generating fluent but ungrounded outputs.
- Core assumption: LLMs lack the "intuitive filtering" humans use to reject illogical conceptual blends (assumed, not proven).
- Evidence anchors:
  - [abstract] "HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls"
  - [section] "HIPc and TIPcs serve as complementary experimental tools for isolating the specific structural conditions that induce hallucination-like behaviors"
  - [corpus] Related work on symbolic triggers (SymLoc) identifies similar failure modes with modifiers, negation, and named entities
- Break condition: Logically grounded fusion (TIPcs) produced lower hallucination scores than even non-fusion prompts, suggesting fusion type matters more than fusion presence.

### Mechanism 2
- Claim: Architecture-specific self-regulation mechanisms may govern hallucination resistance.
- Mechanism: Models like Gemini2.5Pro exhibit "epistemic conservatism"—stricter thresholds for speculative generation—possibly from architecture features or training, not just instruction tuning.
- Core assumption: The observed behavioral differences reflect internal filtration mechanisms rather than surface-level prompt-response patterns.
- Evidence anchors:
  - [section] "Gemini2.5Pro's consistently low hallucination scores may reflect a built-in mechanism of epistemic conservatism"
  - [section] DeepSeek-R1 scored significantly higher than DeepSeek (p ≈ 0.048), showing reasoning orientation alone doesn't guarantee hallucination suppression
  - [corpus] KDCM and structured reasoning approaches show promise for explicit reasoning structures that reduce hallucination
- Break condition: GPT-4o vs GPT-o3 showed no significant difference, indicating architectural claims don't generalize across all model families.

### Mechanism 3
- Claim: Prompt structure—not just content—determines hallucination susceptibility.
- Mechanism: The same two concepts ("periodic table" and "tarot") produced different hallucination rates depending on whether they were structurally fused (HIPc) vs. separately presented (HIPn).
- Core assumption: Token count, syntactic structure, and sentence number are adequately controlled (29-30 tokens, two sentences).
- Evidence anchors:
  - [section] "HIPn consistently exhibited significantly lower hallucination scores than those to HIPc (p < 0.01)"
  - [section] "hallucination is not simply triggered by the presence of multiple concepts in a prompt, but rather by the unnatural or forced fusion of semantically incompatible ideas"
  - [corpus] Conceptual Blending Theory (Fauconnier & Turner, 2002) is referenced as theoretical framing; corpus paper "The Way We Prompt" explores similar dynamics
- Break condition: The study tested only one fusion pair; generalization to other domain combinations remains unvalidated.

## Foundational Learning

- Concept: **Conceptual Blending Theory**
  - Why needed here: The paper uses this cognitive science framework to explain how humans integrate distant domains meaningfully—contrasting with LLM failures.
  - Quick check question: Can you explain why "periodic table + tarot" might produce novel insight in humans but hallucination in LLMs?

- Concept: **Epistemic Uncertainty in Generative Models**
  - Why needed here: Understanding how models handle ambiguity and conflicting information is central to interpreting HIP/HQP results.
  - Quick check question: What behavioral signal would indicate a model is "uncertain" vs. confidently hallucinating?

- Concept: **Null-Fusion Control Design**
  - Why needed here: The paper's causal claims rest on comparing HIPc (fused) vs. HIPn (non-fused) prompts with matched token counts.
  - Quick check question: Why is token-matching critical when comparing prompt-induced effects?

## Architecture Onboarding

- Component map: HIP -> Target LLM -> HQP -> GPT-o3 Evaluator -> Score Aggregation
- Critical path: Design HIP → Generate response via target model → Insert response into HQP → Score via evaluator model (GPT-o3) → Aggregate across 5 trials
- Design tradeoffs:
  - Using LLM-as-judge (GPT-o3) enables scalability but introduces evaluator bias
  - Single fusion pair ("periodic table + tarot") provides controlled comparison but limits generalizability
  - 5 trials per condition balances statistical power vs. API costs
- Failure signatures:
  - High variance across trials suggests model inconsistency
  - Evaluator model refusing to score indicates prompt framing issues
  - Session history contamination (mitigated by disposable sessions)
- First 3 experiments:
  1. Replicate HIPc/HIPn comparison on a new model not in the original study to validate framework portability
  2. Test a new fusion pair (e.g., "quantum mechanics + astrology") to assess whether findings generalize across semantic domains
  3. Replace GPT-o3 evaluator with a different model to quantify evaluator-dependent scoring variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does prompt-induced hallucination generalize across different domain fusion pairs?
- Basis in paper: [explicit] "While 'periodic table × tarot' served as a compelling test case, future work should explore other high-instability combinations—such as 'law and mythology' or 'physics and literature'—to examine whether PIH generalizes across domains."
- Why unresolved: Only one concept pair (periodic table + tarot) was tested systematically.
- What evidence would resolve it: Replicating the HIP/HQP protocol across diverse, systematically varied concept pairs with matched semantic distances.

### Open Question 2
- Question: Can internal model signals (logit distributions, attention patterns, semantic entropy) predict hallucination scores before generation completes?
- Basis in paper: [explicit] "By capturing logit distributions and attention patterns during generation, it would be possible to compute indicators such as semantic entropy, confidence variance, or token-level perplexity... These internal signals could then be statistically correlated with hallucination scores."
- Why unresolved: Current study relies solely on external behavioral HQP scoring.
- What evidence would resolve it: Correlating real-time internal metrics with HQP scores across many HIP trials.

### Open Question 3
- Question: Why do reasoning-oriented architectures show inconsistent hallucination profiles (DeepSeek-R1 higher than DeepSeek, but Gemini2.5Pro lower than Gemini2.0Flash)?
- Basis in paper: [inferred] The paper reports mixed results and states: "We caution against overinterpreting the result due to the modest statistical significance" (p ≈ 0.048 for DeepSeek-R1 vs DeepSeek).
- Why unresolved: Small sample sizes and unexplained architectural differences.
- What evidence would resolve it: Larger-scale controlled studies isolating reasoning optimization from other training variables.

### Open Question 4
- Question: How well do LLM-based HQP scores correlate with human expert judgments?
- Basis in paper: [explicit] "Validating HQP scores against human judgments will be essential to establish their external interpretability and trustworthiness."
- Why unresolved: All scoring used GPT-o3 as evaluator without human benchmarking.
- What evidence would resolve it: Side-by-side human expert evaluation of HIP responses with correlation analysis.

## Limitations
- Single fusion pair ("periodic table + tarot") limits generalizability to other semantically distant domains
- LLM-as-judge methodology introduces potential evaluator bias that could affect hallucination scoring consistency
- Controlled token-matching approach may not capture full complexity of real-world hallucination triggers

## Confidence

- **High Confidence**: HIPs consistently produce more hallucinated responses than null-fusion controls (p < 0.01); comparative ranking of models demonstrates reliable behavioral differences
- **Medium Confidence**: Conceptual fusion without logical grounding specifically triggers hallucination; Gemini2.5Pro's low scores as "epistemic conservatism" is plausible but not definitively proven
- **Low Confidence**: Reasoning-oriented models are more susceptible to hallucination than general-purpose models; generalizability beyond "periodic table + tarot" fusion remains unvalidated

## Next Checks
1. **Multi-Domain Replication**: Test the HIP/HQP framework with at least three additional semantically distant fusion pairs (e.g., "quantum mechanics + astrology", "constitutional law + culinary arts", "evolutionary biology + classical music theory") to assess whether the hallucination-inducing mechanism generalizes beyond the original fusion pair.

2. **Evaluator Model Comparison**: Repeat the hallucination scoring using three different evaluator models (e.g., GPT-4, Claude-3.5-Sonnet, and Llama-3) to quantify evaluator-dependent variance and establish the robustness of the HQP methodology.

3. **Real-World Prompt Analysis**: Apply the HIP/HQP framework to analyze hallucination susceptibility in actual user prompts from production LLM deployments, comparing controlled experimental results with naturally occurring prompt-induced hallucinations to validate ecological relevance.