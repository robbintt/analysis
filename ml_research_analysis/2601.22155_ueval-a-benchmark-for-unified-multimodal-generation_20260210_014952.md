---
ver: rpa2
title: 'UEval: A Benchmark for Unified Multimodal Generation'
arxiv_id: '2601.22155'
source_url: https://arxiv.org/abs/2601.22155
tags:
- image
- generation
- text
- tasks
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UEval is a benchmark for unified multimodal generation that requires
  models to produce both images and text in response to a single query. It consists
  of 1,000 expert-curated questions from 8 real-world tasks, with 10,417 rubric criteria
  for evaluation.
---

# UEval: A Benchmark for Unified Multimodal Generation

## Quick Facts
- arXiv ID: 2601.22155
- Source URL: https://arxiv.org/abs/2601.22155
- Reference count: 19
- Primary result: GPT-5-Thinking achieves highest score of 66.4/100 on unified multimodal generation benchmark

## Executive Summary
UEval is a comprehensive benchmark designed to evaluate unified multimodal generation capabilities, requiring models to simultaneously produce both images and text in response to single queries. The benchmark comprises 1,000 expert-curated questions spanning 8 real-world tasks, with 10,417 rubric criteria for fine-grained evaluation. Using a data-dependent rubric-based scoring system refined by human experts, UEval enables automatic evaluation while maintaining quality standards.

The benchmark reveals significant performance gaps between reasoning and non-reasoning models, with GPT-5-Thinking achieving the highest score of 66.4/100 and the best open-source model reaching only 49.1. Notably, experiments demonstrate that reasoning traces can substantially improve multimodal generation quality when transferred to non-reasoning models, suggesting new directions for model improvement.

## Method Summary
UEval introduces a novel benchmark for unified multimodal generation that requires models to produce both images and text from single queries. The benchmark consists of 1,000 expert-curated questions from 8 real-world tasks, evaluated using 10,417 rubric criteria. The evaluation employs a data-dependent rubric-based scoring system refined by human experts to enable fine-grained automatic evaluation. This approach differs from previous benchmarks by providing detailed scoring criteria that allow for nuanced assessment of both visual and textual outputs, making it particularly suitable for evaluating unified multimodal models.

## Key Results
- GPT-5-Thinking achieves highest score of 66.4/100 on the benchmark
- Best open-source model reaches only 49.1, highlighting significant performance gaps
- Reasoning models significantly outperform non-reasoning models
- Reasoning traces substantially improve multimodal generation quality when transferred to non-reasoning models

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive evaluation approach that combines expert-curated questions with detailed rubric-based scoring. The data-dependent nature of the rubrics allows for fine-grained assessment of both image and text outputs simultaneously. The inclusion of reasoning traces demonstrates that structured problem-solving approaches can significantly enhance multimodal generation quality, even when applied to models not originally designed for reasoning tasks.

## Foundational Learning

**Multimodal Generation**: Producing both images and text from single queries - needed to evaluate unified model capabilities across different output modalities
Quick check: Can models generate coherent image-text pairs for diverse real-world scenarios

**Rubric-based Evaluation**: Structured scoring criteria refined by experts - needed to enable automatic yet nuanced assessment of generation quality
Quick check: Do rubric scores correlate with human judgment across different cultural contexts

**Reasoning Traces**: Documented problem-solving steps - needed to improve generation quality through structured approaches
Quick check: Does incorporating reasoning traces improve non-reasoning model performance across task types

## Architecture Onboarding

**Component Map**: Expert curation -> Question generation -> Rubric development -> Model evaluation -> Performance analysis
**Critical Path**: Question creation → Rubric refinement → Model response generation → Automatic scoring → Performance analysis
**Design Tradeoffs**: Expert curation provides quality but limits scalability; automatic scoring enables efficiency but may miss nuanced quality aspects
**Failure Signatures**: Poor performance on cross-modal consistency tasks; difficulty maintaining coherence between generated images and text
**First Experiments**:
1. Test benchmark coverage by evaluating underrepresented task categories
2. Compare rubric-based scores with blind human evaluations
3. Analyze sensitivity by introducing controlled perturbations in question formulation

## Open Questions the Paper Calls Out

None specified in the source material.

## Limitations

- Limited coverage with only 1,000 questions spanning 8 tasks may not represent full diversity of real-world scenarios
- Rubric-based evaluation may not fully capture nuanced quality aspects that human judgment could identify
- Focus on GPT-5-Thinking as top performer raises questions about potential model-specific optimizations

## Confidence

- Benchmark construction methodology: High
- Evaluation design effectiveness: High
- Performance gap findings: Medium (limited by availability of unified models)
- Cross-cultural applicability: Low (potential subjectivity in expert criteria)

## Next Checks

1. Expand benchmark with additional questions from underrepresented task categories to test coverage completeness and identify potential blind spots
2. Conduct blind human evaluation studies comparing rubric-based scores with human judgments across diverse cultural contexts
3. Test benchmark sensitivity by introducing controlled perturbations in question formulation and rubric criteria to assess robustness of performance rankings