---
ver: rpa2
title: Understanding and Rectifying Safety Perception Distortion in VLMs
arxiv_id: '2502.13095'
source_url: https://arxiv.org/abs/2502.13095
tags:
- safety
- vlms
- shift
- activation
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes why vision-language models (VLMs) are more
  vulnerable to safety issues compared to their text-only LLM backbones. The authors
  identify that incorporating images into inputs causes an activation shift in VLMs
  that systematically pushes activations toward a "safer" direction, making the model
  perceive harmful requests as less risky than they actually are.
---

# Understanding and Rectifying Safety Perception Distortion in VLMs

## Quick Facts
- **arXiv ID**: 2502.13095
- **Source URL**: https://arxiv.org/abs/2502.13095
- **Reference count**: 40
- **Primary result**: Proposed ShiftDC method reduces VLM safety attack success rates by 5-10% compared to baseline defenses while maintaining visual reasoning capabilities

## Executive Summary
This paper addresses a critical safety vulnerability in vision-language models (VLMs) where the incorporation of visual inputs systematically distorts the model's safety perception. The authors discovered that image inputs cause activation shifts that push VLMs to perceive harmful requests as less risky than text-only models do. To address this, they developed ShiftDC (Activation Shift Disentanglement and Calibration), a training-free inference method that removes the safety-relevant component of activation shifts while preserving visual semantics. The method demonstrates significant improvements across multiple VLMs and safety benchmarks, offering a practical defense mechanism without requiring model retraining.

## Method Summary
The authors propose ShiftDC, a training-free inference method that disentangles and calibrates the activation shifts caused by image inputs in VLMs. The approach works by identifying and removing the safety-relevant component of activation shifts while preserving the visual semantics necessary for accurate reasoning. ShiftDC operates at inference time without requiring additional training or fine-tuning of the underlying VLMs. The method is tested across multiple VLM architectures including LLaVA-1.5-7B, LLaVA-1.6-34B, MiniGPT-4-7B, ShareGPT4V-7B, and Qwen-VL-7B, demonstrating its broad applicability.

## Key Results
- ShiftDC reduces attack success rates on safety benchmarks by 5-10% compared to baseline defense methods
- The method maintains visual reasoning capabilities on utility benchmarks (MME and MM-Vet)
- Performance validated across multiple VLM architectures: LLaVA-1.5-7B, LLaVA-1.6-34B, MiniGPT-4-7B, ShareGPT4V-7B, Qwen-VL-7B
- Tested on two major safety benchmarks: MM-SafetyBench and FigStep

## Why This Works (Mechanism)
VLMs incorporate visual inputs that cause systematic activation shifts toward "safer" perceptions, making harmful requests appear less risky than they actually are. This distortion occurs because the multimodal architecture processes visual information in ways that interfere with the safety alignment learned by the text-only LLM backbone. ShiftDC addresses this by disentangling the safety-relevant components of these activation shifts from the visual semantic components, effectively removing the distortion while preserving the model's ability to reason about visual content.

## Foundational Learning

**Activation Shift Analysis**: Understanding how image inputs affect model activations differently than text inputs alone is crucial for identifying the safety distortion mechanism. Quick check: Compare activation patterns between text-only and multimodal inputs on safety-relevant prompts.

**Disentanglement Techniques**: The ability to separate safety-relevant activation components from visual semantic components is fundamental to ShiftDC's approach. Quick check: Verify that removing safety-relevant components doesn't degrade performance on benign visual tasks.

**Safety Perception Calibration**: The concept that safety perceptions can be systematically calibrated through activation manipulation rather than retraining represents a novel approach to VLM safety. Quick check: Measure safety perception accuracy before and after calibration on known harmful prompts.

## Architecture Onboarding

**Component Map**: Input Image + Text Prompt -> VLM Backbone -> Activation Shift Detection -> Disentanglement Module -> Calibrated Output

**Critical Path**: Visual input processing → Activation shift generation → Safety-relevant component identification → Disentanglement and calibration → Final response generation

**Design Tradeoffs**: Training-free approach offers deployment flexibility but may not address fundamental safety alignment issues; preserving visual semantics while removing safety distortions requires precise component separation.

**Failure Signatures**: Residual safety distortion if disentanglement is incomplete; degraded visual reasoning if too much of the activation shift is removed; inconsistent performance across different VLM architectures.

**First Experiments**:
1. Baseline safety evaluation: Test VLMs on safety benchmarks without any defense mechanisms
2. Activation shift analysis: Compare activation patterns between harmful and benign inputs with and without visual components
3. Ablation study: Evaluate ShiftDC performance with different levels of disentanglement to find optimal balance

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions for future research, focusing instead on presenting the ShiftDC solution and its empirical validation.

## Limitations

- The analysis relies on averaged patterns across safety datasets that may not generalize to all harmful inputs
- Training-free nature means it may not address deeper safety alignment issues requiring fine-tuning
- Study focuses exclusively on English-language safety concerns, limiting multilingual generalizability

## Confidence

- **High confidence**: Empirical performance improvements on tested benchmarks
- **Medium confidence**: Claim that activation shifts systematically push models toward "safer" perceptions based on averaged patterns
- **Low confidence**: Generalizability to multilingual contexts and real-world safety incidents beyond curated benchmarks

## Next Checks

1. Test ShiftDC across a broader range of languages and cultural contexts to verify cross-lingual effectiveness
2. Conduct ablation studies to isolate which components of the activation shift are most critical for safety distortion versus legitimate visual processing
3. Evaluate the method's performance on real-world safety incidents beyond curated benchmarks to assess practical deployment readiness