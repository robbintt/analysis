---
ver: rpa2
title: EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation
arxiv_id: '2511.02953'
source_url: https://arxiv.org/abs/2511.02953
tags:
- depth
- event
- dataset
- event-based
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited large-scale annotated
  datasets for event-based depth estimation, which restricts model generalization.
  The authors introduce EvtSlowTV, a large-scale synthetic event dataset generated
  from real-world YouTube videos, containing over 13B events across diverse environments
  including hiking, flying, driving, and underwater exploration.
---

# EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation

## Quick Facts
- arXiv ID: 2511.02953
- Source URL: https://arxiv.org/abs/2511.02953
- Authors: Sadiq Layi Macaulay; Nimet Kaygusuz; Simon Hadfield
- Reference count: 40
- Primary result: Self-supervised depth estimation on MVSEC indoor flying achieves 0.1887 absolute mean depth error, outperforming state-of-the-art methods.

## Executive Summary
This paper addresses the critical bottleneck of limited large-scale annotated datasets for event-based depth estimation by introducing EvtSlowTV, a synthetic event dataset generated from 45 diverse YouTube videos. The dataset contains over 13 billion events spanning hiking, flying, driving, and underwater exploration scenarios. The authors propose a self-supervised learning framework using a teacher-student training strategy that eliminates the need for external sensor annotations while preserving the asynchronous properties of event data. The method achieves state-of-the-art performance on MVSEC indoor flying sequences, demonstrating that large-scale, diverse event datasets combined with self-supervised learning significantly enhance depth estimation performance.

## Method Summary
The approach involves two stages: first, generating synthetic events from real-world videos using ESIM simulator with adaptive frame sampling to preserve asynchronous event properties; second, training a depth estimation model using a teacher-student framework where a supervised teacher model fine-tuned on MVSEC guides self-supervised student learning on EvtSlowTV. Events are represented as 5-bin spatiotemporal volumes over 0.1665s windows, and depth and camera pose are jointly optimized using contrast maximization loss that maximizes sharpness of warped event accumulations. The teacher provides L1 distillation loss during student training, enabling stable self-supervised learning without ground truth depth annotations.

## Key Results
- Achieves 0.1887 absolute mean depth error on MVSEC indoor flying, outperforming ESVO (0.2339), EMVS (0.3937), and Ghosh et al. (0.2253)
- Teacher-student training improves depth prediction accuracy across different distance ranges and environmental conditions
- Large-scale, diverse event datasets combined with self-supervised learning significantly enhance depth estimation performance while maintaining computational efficiency
- Ablation studies show consistent improvement in 0-30m range for indoor flying (0.0396 vs 0.0646) and marginal gains in outdoor scenarios

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Frame Sampling Preserves Event Asynchronicity
Generating synthetic events from video frames using adaptive (not uniform) sampling better approximates the asynchronous triggering behavior of real event cameras. Frame sampling rate is dynamically adjusted based on maximum logarithmic irradiance gradient across spatial dimensions, producing sparse event distributions that maintain temporal precision where motion occurs. This preserves the asynchronous properties essential for accurate depth learning. Break condition: If contrast threshold C is set inappropriately or source video frame rate is too low, generated events may fail to capture fine-grained motion.

### Mechanism 2: Contrast Maximization Enables Self-Supervised Depth-Pose Joint Optimization
Depth and camera pose can be learned without external supervision by maximizing the sharpness of reprojected event accumulations. Events are back-projected into 3D using estimated depth and intrinsics, then reprojected to a reference frame using estimated pose. Accurate depth and pose produce high-contrast Image of Warped Events (IWE) where events from the same physical edge align spatially. Break condition: In highly dynamic scenes where events are triggered primarily by object motion rather than camera motion, contrast maximization may converge to incorrect depth/pose solutions.

### Mechanism 3: Teacher-Student Training Transfers Supervised Knowledge While Adapting to Event Properties
A supervised teacher model pre-trained on limited ground-truth data can stabilize self-supervised student learning on larger unlabeled event datasets. The teacher is frozen and provides L1 distillation loss during student training, giving a stability signal that prevents the student from drifting into poor local minima while allowing adaptation to broader data distribution. Break condition: If teacher training data is too narrow in distribution, teacher predictions may provide misleading supervision on out-of-domain sequences.

## Foundational Learning

- **Event Cameras (Neuromorphic Vision Sensors)**: Understanding that events are sparse, triggered only by log-intensity changes exceeding a threshold, is essential for interpreting dataset generation and contrast maximization logic. Quick check: Given a static scene with fixed lighting, will a moving event camera generate events? (Yes—brightness changes at pixel boundaries as camera moves relative to edges.)

- **Event Volume Representation**: Raw event streams are sparse and asynchronous, incompatible with standard CNNs. The paper aggregates events into structured spatiotemporal tensors (5 temporal bins over 0.1665s windows) using linear weighted accumulation. This representation is the actual network input. Quick check: If you have 10,000 events over 0.1665s window with 5 bins, how does network receive this? (As 5-channel volumetric tensor where each channel accumulates polarity-weighted events falling within normalized time bin.)

- **Scale-Invariant Depth Losses**: Monocular depth estimation has inherent scale ambiguity. The teacher model uses scale-invariant loss (comparing relative depth differences) and gradient matching (preserving edge structure). Without this, predictions could be arbitrarily scaled without affecting certain losses. Quick check: Why would model predicting all depths as exactly 0.5× true depth achieve low L1 loss but poor scale-invariant gradient loss? (L1 would penalize absolute differences proportionally, but gradient matching explicitly compares local depth discontinuities which would misalign at object boundaries.)

## Architecture Onboarding

- **Component map**: Event Volume Formation -> Encoder (skip-connection CNN) -> Decoder (depth prediction) -> Pose Head (6-DOF transformation) -> Back-projection Module -> Loss Computation (Contrast maximization + L1 distillation)
- **Critical path**: Teacher model training on MVSEC with supervised losses (L_si + λL_grad) → Teacher weights frozen → Student model initialized → Student trained on EvtSlowTV using contrast maximization + L1 distillation from teacher predictions → Evaluation on MVSEC indoor flying sequences
- **Design tradeoffs**: Dataset source (synthetic from video vs. real event camera) enables scale and diversity but introduces domain gap; event volume bin count (5 bins) balances temporal resolution vs. compute; teacher-student vs. fine-tuning shows marginal benefits suggesting conditional necessity
- **Failure signatures**: RMS error improves but log-scale error degrades (indicating absolute depth accuracy but scale consistency issues); performance drop on distant objects (>10m) due to sparse events; contrast maximization collapse if pose estimation fails
- **First 3 experiments**: 1) Generate small EvtSlowTV subset, train simple encoder-decoder with contrast maximization only, evaluate on MVSEC indoor flying to isolate dataset contribution; 2) Compare three training modes—supervised teacher only, fine-tuning, teacher-student—using exact metrics from Table 3; 3) Vary temporal bins (3, 5, 10) and window duration on held-out validation sequence to identify optimal representation hyperparameters

## Open Questions the Paper Calls Out

- Can the EvtSlowTV dataset, generated synthetically via ESIM simulation from YouTube videos, achieve comparable generalization to datasets captured directly from real event camera hardware? The paper identifies synthetic datasets often fail to generalize due to domain adaptation issues, yet EvtSlowTV itself is synthetic and its sim-to-real performance remains unquantified without comparison to real event camera datasets.

- How can proportional depth estimation accuracy (log-scale error) be improved within the self-supervised teacher-student framework while maintaining absolute depth accuracy? The authors acknowledge their method struggles with proportional depth estimation with rms log errors substantially higher than baseline methods, but do not propose mechanisms to address the scale-consistency limitations.

- To what extent does the static scene assumption in contrast maximization affect depth estimation accuracy in environments with independently moving objects? The methodology assumes static scenes for contrast maximization, yet EvtSlowTV includes diverse scenarios where dynamic objects are likely present, with no analysis of performance degradation attributable to violating this assumption.

## Limitations

- The dataset generation pipeline relies on synthetic events from ESIM, introducing domain gap from real event cameras without validation against real sequences beyond MVSEC indoor flying
- Teacher-student training shows marginal improvements over fine-tuning in most conditions, suggesting its necessity is conditional rather than universal
- Performance on distant objects (>10m) degrades significantly, limiting applicability to large-scale environments

## Confidence

- **High confidence**: The core claim that large-scale diverse datasets improve depth estimation generalization is well-supported by quantitative comparisons and aligns with established dataset scaling principles
- **Medium confidence**: The teacher-student training mechanism is novel but ablation studies show only marginal gains over fine-tuning in most scenarios, suggesting benefit may be dataset- and environment-dependent
- **Low confidence**: The dataset's fidelity to real event camera properties is untested beyond MVSEC indoor flying; claims about HDR potential and robustness to diverse environments are based on synthetic data without validation on real-world event streams

## Next Checks

1. Validate EvtSlowTV's domain fidelity by training the same model on real event camera sequences from a held-out MVSEC sequence and comparing performance degradation to synthetic-to-real transfer studies in other vision domains
2. Conduct systematic ablation of teacher-student vs. fine-tuning across all four environments in Table 3, quantifying where teacher guidance provides measurable benefit beyond supervised pretraining alone
3. Test event volume representation sensitivity by training models with 3, 5, and 10 temporal bins on EvtSlowTV and measuring both convergence speed and final depth accuracy on MVSEC indoor flying