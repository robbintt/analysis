---
ver: rpa2
title: 'InstructLR: A Scalable Approach to Create Instruction Dataset for Under-Resourced
  Languages'
arxiv_id: '2512.02213'
source_url: https://arxiv.org/abs/2512.02213
tags:
- instruction
- instructlr
- zarma
- language
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InstructLR, a framework for generating high-quality
  instruction datasets for low-resource languages. The method uses a dual-layer quality
  filtering approach that combines RAG-based automated checking with human validation
  to correct errors while keeping costs manageable.
---

# InstructLR: A Scalable Approach to Create Instruction Dataset for Under-Resourced Languages

## Quick Facts
- **arXiv ID:** 2512.02213
- **Source URL:** https://arxiv.org/abs/2512.02213
- **Reference count:** 36
- **Primary result:** Dual-layer filtering (RAG + human) cuts cost 88% vs full annotation while matching MT-Seed performance on Zarma, Bambara, Fulfulde datasets.

## Executive Summary
InstructLR introduces a scalable pipeline for building instruction-response datasets in low-resource languages (LRLs). The method generates drafts by translating seed instructions from a high-resource language and producing responses directly in the LRL, avoiding the artifacts of post-hoc machine translation. Automated quality filtering via a retrieval-augmented generation (RAG) checker flags drafts for human review, reducing annotation cost by 88% compared to full manual labeling. Applied to Zarma, Bambara, and Fulfulde, the resulting 50k-scale benchmarks improve fine-tuned model performance on instruction-following and downstream NER tasks, with BLEU scores up to 30.1 and human preference scores of 78-94%.

## Method Summary
The framework follows a five-stage pipeline: (1) Mistral-7B generates ~50k French seed instructions across 20 MMLU-inspired topics; (2) Gemini 2.5 Pro translates each seed into the target LRL and generates the paired response, applying loanword handling rules; (3) a RAG checker (Gemini 2.0 Flash + FAISS) retrieves context from a knowledge base (3k clean sentences, 20 grammar rules, bilingual glossaries) to classify drafts as accepted, low-priority, or top-priority; (4) human validators correct top-priority drafts; (5) all accepted/corrected pairs are aggregated into JSONL datasets and used to fine-tune models (e.g., Llama-3.1-8B) with unsloth + QLoRA, lr=2e-5, 3 epochs, 49k/1k train/test split.

## Key Results
- Fine-tuning on InstructLR datasets yields BLEU scores of 22.8 (Zarma), 30.1 (Bambara), and 28.9 (Fulfulde), outperforming zero-shot and MT-Seed baselines.
- Human evaluators prefer InstructLR outputs 78-94% of the time.
- Models trained on InstructLR data achieve 41-54% exact-match on NER tasks.
- The dual-layer filtering approach reduces annotation cost by 88% versus full human annotation.

## Why This Works (Mechanism)

### Mechanism 1
Generating instruction-response pairs directly in the target LRL via an LLM, guided by a high-resource language (HRL) seed, produces more fluent and contextually appropriate output than post-hoc machine translation of complete pairs. An LLM (e.g., Gemini 2.5 Pro) receives a seed instruction in a high-resource language (e.g., French), translates the instruction into the target LRL, and generates the response directly in the LRL, informed by the semantic context of both the HRL and LRL representations. This assumes the chosen LLM possesses at least a “mediocre, yet acceptable” baseline generative capability in the target LRL.

### Mechanism 2
A RAG-based automated quality checker, grounded in a curated linguistic knowledge base, can effectively filter and correct a large majority of raw drafts, making the human validation effort scalable. The RAG checker uses a FAISS-indexed knowledge base of clean sentences, grammar rules, and glossaries to retrieve relevant context, guiding an LLM (e.g., Gemini 2.0 Flash) in analyzing drafts for grammatical and orthographic errors and flagging them by priority for human review. This relies on the availability of a high-quality, relevant knowledge base for the target LRL and the checker LLM’s ability to interpret and apply these rules correctly.

### Mechanism 3
Fine-tuning an open-source model on a high-quality, multi-domain instruction dataset created by InstructLR significantly improves its performance on both instruction-following and downstream tasks in the target LRL. The curated dataset provides aligned instruction-response pairs across 20 diverse topics, adapting the model’s weights to the linguistic patterns and task formats of the LRL, overcoming its initial lack of support from pre-training. This assumes the generated dataset’s quality and diversity are sufficient for effective adaptation without reinforcing systematic errors.

## Foundational Learning

- **Instruction Tuning (IT):** Fine-tuning a pre-trained model on a dataset of instruction-response pairs to align it with human intent and improve its ability to follow commands. *Why needed:* This is the core task; understanding IT is essential to grasp how InstructLR improves model performance. *Quick check:* How does instruction tuning differ from standard fine-tuning for a text classification task?

- **Low-Resource Languages (LRLs):** Languages with limited digital text and linguistic resources, making standard approaches (like MT or synthetic data) fail by producing unnatural text, lacking cultural nuance, and suffering from hallucinations. *Why needed:* The entire problem is defined by the lack of data for LRLs. *Quick check:* Name two specific failure modes of using direct machine translation to create instruction datasets for a low-resource language.

- **Retrieval-Augmented Generation (RAG):** Combining a retriever (finding relevant info from a knowledge base) with a generator (an LLM) to produce a more grounded and accurate output. *Why needed:* RAG is used in the automated quality filtering layer. *Quick check:* In the context of InstructLR, what three types of information are retrieved by the RAG checker to assess a draft’s quality?

## Architecture Onboarding

- **Component map:** Seed Instruction Generator (Mistral-7B) → Instruction-Response Draft Generator (Gemini 2.5 Pro) → RAG-based Quality Checker (Gemini 2.0 Flash + FAISS) → Human Validation Interface → Final Dataset Aggregator.

- **Critical path:** Seed Instruction Generator → Instruction-Response Draft Generator → RAG-based Quality Checker → Human Validation → Final Dataset Aggregator. The human validation step is the ultimate quality gate.

- **Design tradeoffs:** Automation vs. Quality (RAG checker reduces cost but depends on knowledge base quality); Model Selection (choosing capable but expensive models for draft generation); Cost of Human Annotation (88% cost reduction vs. full annotation, but final quality depends on automated layer’s effectiveness).

- **Failure signatures:** Hallucinated words in drafts (LLM lacks LRL proficiency); high false-positive rate in RAG checker (checker flags correct drafts); low human preference scores (pipeline produces unnatural text); poor downstream task performance (dataset quality or diversity issues).

- **First 3 experiments:**
  1. **Pilot Generation with Draft Generator:** Run a small batch (e.g., 50 prompts) of seed instructions through your chosen LLM for the target LRL. Manually inspect the raw outputs for fluency, hallucination rate, and basic grammatical correctness.
  2. **RAG Checker Knowledge Base Construction:** Build a minimal knowledge base with 50-100 clean sentences, 5-10 key grammar rules, and a small glossary. Run the checker on a few generated drafts. Evaluate if its corrections and flags make sense.
  3. **End-to-End Single-Domain Pipeline:** Run the full InstructLR pipeline for a single, well-defined topic (e.g., “Mathematics”) to create a small dataset (e.g., 500-1000 pairs). Fine-tune a small open-source model (e.g., Gemma-2-2B) on this data and qualitatively evaluate its responses to new instructions in that domain.

## Open Questions the Paper Calls Out

- **Can the framework be adapted for languages where existing LLMs have absolutely no baseline proficiency?** The authors note that for LRLs with “no” LLM support, “alternative strategies for the automated layer would be needed,” and list applying the framework to languages with no existing coverage as future work. This is unresolved because the current pipeline relies on a model that possesses at least a “mediocre, yet acceptable” understanding of the target language to generate initial drafts.

- **Can automated quality filtering effectively detect factual hallucinations and cultural inconsistencies without human intervention?** The conclusion notes that current assessment focuses on grammar/fluency and states, “we will work to develop more sophisticated automated quality assessment techniques… [to] improve the detection of factual or cultural inconsistencies.” This is unresolved because the current RAG-based checker relies on grammar rules and clean sentences, but factual accuracy is not yet automatically validated.

- **Is it possible to replace the commercial LLM components with open-source alternatives while maintaining the current level of linguistic quality?** The authors explicitly aim in future work to “reduce the framework’s dependency on commercial LLMs to increase its accessibility.” This is unresolved because the framework currently selects commercial models (Gemini 2.5 Pro) specifically because they outperform open-source alternatives in generating coherent text for these specific low-resource languages.

## Limitations

- The knowledge base construction (3,000 clean sentences, 20 grammar rules) for each LRL represents a significant upfront cost that may limit applicability to languages without existing linguistic resources.
- The RAG checker’s effectiveness depends heavily on the quality of this knowledge base and the checker model’s LRL proficiency, both of which are difficult to validate without native speaker expertise.
- The framework assumes baseline LLM proficiency in the target LRL exists but doesn’t address what happens when this assumption fails—as evidenced by the Hindi/Russian word contamination in Bambara and Fulfulde outputs.

## Confidence

- **High Confidence:** The overall methodology architecture and the observed improvements in BLEU scores (22.8-30.1) and human preference (78-94% win rate) are well-supported by the experimental results presented.
- **Medium Confidence:** The 88% cost reduction claim and the downstream NER task improvements (41-54% exact match) are reasonable but depend on specific cost assumptions and may vary with different validation approaches.
- **Low Confidence:** The framework’s performance on LRLs with no existing linguistic resources or where LLMs have minimal LRL proficiency is uncertain, as the paper doesn’t test these edge cases.

## Next Checks

1. **Knowledge Base Scalability Test:** Implement the framework on a third LRL with minimal existing linguistic resources to empirically measure the knowledge base construction effort and assess whether the 88% cost reduction holds when starting from scratch.

2. **Hallucination Rate Analysis:** Conduct systematic sampling of raw draft outputs to quantify the frequency of non-LRL tokens and nonsensical content, then measure how effectively the RAG checker catches these errors versus allowing them through.

3. **Checker Model Proficiency Threshold:** Test the RAG checker’s performance using models with decreasing LRL proficiency levels to identify the minimum competency threshold required for acceptable automated quality filtering.