---
ver: rpa2
title: 'Artwork Interpretation with Vision Language Models: A Case Study on Emotions
  and Emotion Symbols'
arxiv_id: '2511.22929'
source_url: https://arxiv.org/abs/2511.22929
tags:
- vlms
- emotion
- images
- emotions
- artwork
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the ability of current vision language models
  (VLMs) to interpret emotions in artworks, focusing on concrete and symbolic expressions
  of emotion. The authors present a case study with 38 artworks from various periods
  and three VLMs (Llava-Llama-8B, Qwen-7B, Qwen-32B-AWQ).
---

# Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols

## Quick Facts
- arXiv ID: 2511.22929
- Source URL: https://arxiv.org/abs/2511.22929
- Reference count: 40
- VLMs show limited but promising ability to interpret emotions in artworks

## Executive Summary
This paper evaluates how well current vision language models (VLMs) can interpret emotions and emotion symbols in artworks. Through a qualitative case study of 38 artworks spanning various periods and styles, the authors assess three VLMs (Llava-Llama-8B, Qwen-7B, Qwen-32B-AWQ) on their ability to recognize basic content, express emotions, and identify symbols. The models perform well on concrete visual content and basic emotional expressions but struggle with highly abstract or symbolic images and symbol recognition. They also exhibit confirmation bias and inconsistency in answers. The study reveals that VLMs rely on conventionalized pattern matching rather than genuine compositional reasoning, limiting their effectiveness for complex art interpretation tasks.

## Method Summary
The study uses 38 artworks from various periods, presenting each to three VLMs (Llava-Llama-8B, Qwen-7B, Qwen-32B-AWQ) with eight sequential questions about content, emotions, and symbols. No training is performed; models run inference-only with a 150-token limit per answer. Four images are preprocessed to mask text or irrelevant regions. Two expert annotators qualitatively evaluate the outputs for reasonableness, comparing them to established art-historical knowledge. The evaluation focuses on identifying which aspects of emotion interpretation VLMs handle well versus poorly.

## Key Results
- VLMs recognize basic image content and express emotions reasonably well but struggle with abstract or symbolic images
- Models exhibit confirmation bias, answering "yes" to most yes/no questions about emotions and symbols
- Larger Qwen models show more detailed knowledge and better domain language than Llava-Llama-8B
- VLMs provide inconsistent answers to related questions due to lack of reflective architecture
- Symbol recognition is particularly challenging, with models overextending the concept to mean any expressive element

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Visual-Linguistic Feature Mapping
VLMs process visual information through a hierarchy where concrete visual features map more reliably than abstract interpretive properties. The CLIP image encoder extracts visual features projected into the LLM embedding space, with concrete features (objects, colors, compositions) having stronger cross-modal alignment than abstract concepts requiring inference chains.

### Mechanism 2: Conventionalized Pattern Matching
VLMs succeed by recognizing co-occurrence patterns between visual elements and their conventionalized meanings from training data. Models identify surface patterns (objects, stylistic devices) and retrieve associated meanings rather than performing genuine compositional reasoning.

### Mechanism 3: Generative Inconsistency from Lack of Reflective Architecture
VLMs produce inconsistent outputs because generation occurs without independent internal verification. Each question triggers separate generation without a persistent reasoning state that can cross-check semantic coherence across related queries.

## Foundational Learning

- **Vision-Language Model Architecture**
  - Why needed here: Understanding how CLIP encoders project into LLM embedding spaces explains why concrete features map better than abstract ones
  - Quick check question: Can you explain how an image encoder's output connects to a language model's token embeddings?

- **Contrastive Pre-training (CLIP)**
  - Why needed here: Explains why models recognize conventionalized patterns but fail on novel artistic combinations outside training distribution
  - Quick check question: What objective function trains CLIP, and what does it optimize for?

- **Confirmation Bias in Instruction-Tuned Models**
  - Why needed here: Explains why instruction-tuned models answer "yes" to most yes/no questions about emotions/symbols
  - Quick check question: Why do instruction-tuned models tend toward affirmative responses?

## Architecture Onboarding

- **Component map:**
  CLIP image encoder → visual feature extraction → projection layer → maps visual features to LLM embedding dimension → Transformer-based LLM → generates textual responses → constrained output (150 token max)

- **Critical path:**
  Image input → CLIP encoding → projection to LLM space → concatenation with text prompt → autoregressive generation → constrained output (150 token max)

- **Design tradeoffs:**
  Larger models (QWEN-32B-AWQ) have more domain knowledge but risk misattributing known artworks to similar famous pieces; smaller models (LLAVA-LLAMA-8B) have less art-historical vocabulary but better symbol recognition in some cases; quantization (AWQ) enables 32B model on single 48GB GPU but may affect nuance

- **Failure signatures:**
  Hallucinated objects not present in image (e.g., "halo" in Dürer's Melancholia); overextension of "symbol" to mean any expressive element; inconsistent polarity judgments across related questions; defaulting to "contemplation" as generic aesthetic descriptor

- **First 3 experiments:**
  1. Test on held-out artwork pairs to verify minimal-pair robustness finding (answers similar but distinguishable)
  2. Ablate artwork titles vs. no-titles to confirm visual-only inference (paper already did this; extend to measure performance gap)
  3. Compare symbol recognition accuracy across models using images with unambiguous conventional symbols (skull, heart, cross) vs. ambiguous artistic elements

## Open Questions the Paper Calls Out

### Open Question 1
What evaluation approaches can effectively combine fully automatic quantitative analysis with qualitative expert assessment for VLMs interpreting art? The authors explicitly call for identifying evaluation approaches that combine the advantages of Ozaki et al.'s fully automatic quantitative analysis with their qualitative analysis.

### Open Question 2
To what extent can fine-tuning VLMs on art-specific data improve emotion recognition accuracy and reduce output verbosity? Section 6 lists fine-tuning of VLMs for the analysis of emotions in artwork as an avenue to improve general ability and curb the verbosity that complicates tasks like audio description.

### Open Question 3
How does VLM performance on emotion recognition differ for non-Western artworks or works by female artists compared to the classical Western canon? The authors note in Section 6 that the case study was limited to the classical art history canon, including only one non-Western artwork and two by women.

## Limitations
- No quantitative evaluation framework, relying entirely on qualitative expert judgment
- Architecture uncertainty due to unspecified exact model checkpoints and quantization parameters
- Dataset scope limitation with only 38 artworks representing a small fraction of art-historical imagery

## Confidence
- **High Confidence**: VLMs recognize basic image content better than abstract properties; models exhibit confirmation bias; larger models show more domain knowledge but risk misattribution; VLMs provide inconsistent answers
- **Medium Confidence**: Models struggle specifically with abstract/symbolic images; pattern matching explains success better than compositional reasoning; models overextend "symbol" concept; domain language command differs between models
- **Low Confidence**: Specific quantitative thresholds for success/failure rates; generalizability to all artistic styles and periods; relative performance differences across interpretive difficulty spectrum

## Next Checks
1. Apply standardized emotion recognition benchmarks (e.g., ArtEmis dataset) to measure VLM performance quantitatively
2. Conduct inter-annotator agreement study with additional expert evaluators on the same image set
3. Test VLMs on a balanced dataset of global and gender-diverse artworks to assess cultural generalization