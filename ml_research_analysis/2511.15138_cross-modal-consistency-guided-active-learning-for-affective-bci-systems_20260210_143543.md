---
ver: rpa2
title: Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems
arxiv_id: '2511.15138'
source_url: https://arxiv.org/abs/2511.15138
tags:
- learning
- affective
- uncertainty
- active
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of EEG-based emotion recognition
  in brain-computer interface (BCI) systems, where noisy signals and subjective labels
  limit model performance. The authors propose an uncertainty-aware active learning
  framework that leverages cross-modal consistency between EEG and facial expression
  data to improve affective decoding.
---

# Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems

## Quick Facts
- **arXiv ID**: 2511.15138
- **Source URL**: https://arxiv.org/abs/2511.15138
- **Reference count**: 28
- **Primary result**: Achieved 0.853 accuracy at 50% labeling budget, outperforming random querying (0.584) and full supervision (0.597)

## Executive Summary
This paper addresses the challenge of EEG-based emotion recognition in brain-computer interface (BCI) systems, where noisy signals and subjective labels limit model performance. The authors propose an uncertainty-aware active learning framework that leverages cross-modal consistency between EEG and facial expression data to improve affective decoding. The method aligns multimodal representations in a shared latent space and uses cross-modal discrepancies as a signal for identifying noisy or ambiguous samples. During active learning, the model queries the most uncertain samples based on classifier predictions, refining the labeled dataset iteratively. Experiments on the ASCERTAIN dataset demonstrate that the proposed approach significantly improves label efficiency, achieving 0.853 accuracy at 50% labeling budgetâ€”outperforming both random querying (0.584) and full supervision (0.597). The method also shows steady uncertainty reduction over active learning iterations, confirming its effectiveness in selecting informative samples. Overall, the work highlights the potential of cross-modal consistency-guided active learning for robust, data-efficient affective BCI systems.

## Method Summary
The authors propose an uncertainty-aware active learning framework that leverages cross-modal consistency between EEG and facial expression data to improve affective decoding. The method aligns multimodal representations in a shared latent space and uses cross-modal discrepancies as a signal for identifying noisy or ambiguous samples. During active learning, the model queries the most uncertain samples based on classifier predictions, refining the labeled dataset iteratively. The approach is evaluated on the ASCERTAIN dataset, demonstrating significant improvements in label efficiency and accuracy compared to random querying and full supervision baselines.

## Key Results
- Achieved 0.853 accuracy at 50% labeling budget, significantly outperforming random querying (0.584) and full supervision (0.597)
- Demonstrated steady uncertainty reduction over active learning iterations
- Showed effectiveness in selecting informative samples through cross-modal consistency guidance

## Why This Works (Mechanism)
The method leverages cross-modal consistency between EEG and facial expression data as a signal for identifying noisy or ambiguous samples. By aligning multimodal representations in a shared latent space, the model can detect discrepancies that indicate uncertainty or label noise. This cross-modal signal is then used during active learning to query the most informative samples, iteratively refining the labeled dataset. The approach addresses the challenge of limited labeled data in affective BCI systems by focusing labeling efforts on the most uncertain and potentially noisy samples.

## Foundational Learning
- **Active Learning**: A machine learning paradigm where the model selects which samples to label, improving efficiency. Needed to reduce labeling costs in BCI systems. Quick check: Verify the query strategy effectively selects informative samples.
- **Cross-Modal Consistency**: Using agreement between different data modalities (EEG and facial expressions) to improve robustness. Needed to identify noisy or ambiguous samples. Quick check: Ensure cross-modal alignment in the shared latent space.
- **Uncertainty Quantification**: Measuring model confidence in predictions to guide sample selection. Needed to identify samples that will most improve model performance. Quick check: Validate uncertainty estimates correlate with sample informativeness.
- **Multimodal Representation Learning**: Learning shared representations across different data types. Needed to enable cross-modal consistency checking. Quick check: Verify meaningful alignment between EEG and facial expression features.

## Architecture Onboarding

**Component Map**: EEG Encoder -> Facial Expression Encoder -> Shared Latent Space -> Consistency Checker -> Uncertainty Estimator -> Query Strategy -> Labeled Dataset

**Critical Path**: The core workflow involves encoding EEG and facial expression data, aligning them in a shared latent space, checking for cross-modal consistency, estimating uncertainty, and using this information to select samples for labeling. This process iterates, with each cycle refining the model and labeled dataset.

**Design Tradeoffs**: The approach balances the benefits of cross-modal consistency guidance against the computational overhead of maintaining and aligning multiple modalities. While facial expression data can improve sample selection, it may not always be available in real-world BCI applications. The method must also handle potential noise in the facial expression modality.

**Failure Signatures**: The system may struggle when facial expression data is absent, noisy, or unreliable. Poor cross-modal alignment in the shared latent space could lead to ineffective uncertainty estimation. Over-reliance on cross-modal signals might miss important EEG-specific patterns.

**3 First Experiments**:
1. Validate cross-modal alignment quality in the shared latent space using correlation metrics
2. Test uncertainty estimation accuracy by comparing with human-labeled uncertainty
3. Evaluate sample selection quality by measuring improvement in model performance after each active learning iteration

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Reliance on facial expression data may limit applicability in scenarios where facial expressions are absent or unreliable
- Single-dataset evaluation (ASCERTAIN) limits generalizability across different EEG-affective computing scenarios
- Computational overhead of maintaining and aligning cross-modal representations during active learning iterations is not discussed

## Confidence

**High confidence**: The methodology's ability to improve label efficiency and accuracy when cross-modal data is available, and the observed superiority over random querying baselines.

**Medium confidence**: The general applicability of cross-modal consistency signals across different affective BCI contexts, given the single-dataset evaluation.

**Low confidence**: The robustness of the approach when facial expression data is noisy, absent, or when only EEG data is available.

## Next Checks

1. Test the framework on multiple EEG-affective computing datasets with varying signal quality and emotion annotation schemes to assess generalizability.
2. Evaluate performance when facial expression data is degraded or partially missing to determine the method's robustness in realistic scenarios.
3. Conduct ablation studies to quantify the contribution of cross-modal consistency versus pure uncertainty sampling, and measure computational overhead during active learning iterations.