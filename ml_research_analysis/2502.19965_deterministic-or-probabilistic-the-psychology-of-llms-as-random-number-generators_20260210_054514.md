---
ver: rpa2
title: Deterministic or probabilistic? The psychology of LLMs as random number generators
arxiv_id: '2502.19965'
source_url: https://arxiv.org/abs/2502.19965
tags:
- number
- random
- range
- llms
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how Large Language Models (LLMs) perform\
  \ as random number generators, revealing that despite their probabilistic design,\
  \ LLMs often produce deterministic outputs due to inherent training data biases.\
  \ The study tests six models (DeepSeek-R1, Gemini 2.0, GPT-4o-mini, Llama 3.1-8b,\
  \ Mistral-7b, and Phi-4-14b) across seven languages and three numerical ranges (1\u2013\
  5, 1\u201310, 1\u2013100), with 100 calls per configuration."
---

# Deterministic or probabilistic? The psychology of LLMs as random number generators

## Quick Facts
- **arXiv ID**: 2502.19965
- **Source URL**: https://arxiv.org/abs/2502.19965
- **Reference count**: 8
- **Primary result**: Despite probabilistic design, LLMs produce deterministic random numbers due to training data biases, with strong preferences for specific numbers across models and languages.

## Executive Summary
This paper investigates whether Large Language Models can function as true random number generators by testing six major models across seven languages and three numerical ranges. The study reveals that LLMs consistently produce non-uniform distributions with strong biases toward specific numbers (particularly 3, 7, 37, and 42), regardless of temperature settings or prompt language. The findings demonstrate that LLMs reproduce human cognitive biases in number selection rather than generating mathematically random outputs, challenging their suitability for tasks requiring genuine randomness.

## Method Summary
The study tested six LLMs (DeepSeek-R1, Gemini 2.0, GPT-4o-mini, Llama 3.1-8b, Mistral-7b, Phi-4-14b) across seven languages (Chinese, English, French, Hindi, Japanese, Russian, Spanish) and three ranges (1-5, 1-10, 1-100). For each configuration, 100 calls were made with six temperature settings (0.1 to 2.0), totaling 75,600 calls. The simple prompt "Give me a random number between 1 and X" was used without additional prompt engineering. Output distributions were analyzed using chi-squared tests, Cramér's V effect size, and a custom Randomness Index incorporating range coverage, coefficient of variation, and normalized entropy.

## Key Results
- All models showed strong deterministic biases, with chi-squared p-values below 2.19×10⁻¹⁵ indicating rejection of uniform distribution hypothesis
- Numbers 3 (range 1-5), 7 (range 1-10), and 42 (range 1-100) were consistently overrepresented across models and languages
- Temperature variations from 0.1 to 2.0 failed to normalize distributions or eliminate core biases
- Language influenced output distributions, with Japanese prompts shifting Gemini 2.0's preference from 3 to 1
- DeepSeek-R1's Chain-of-Thought reasoning revealed struggle with randomness, often defaulting to human cognitive biases

## Why This Works (Mechanism)

### Mechanism 1: Training Corpus Cognitive Bias Propagation
LLMs reproduce human cognitive biases in random number selection because training data contains patterned human responses. The self-attention mechanism learns statistical regularities in human number choice contexts (cultural references like "42", psychological preferences for prime numbers and central values), then reproduces these when prompted for randomness. Numbers like 3, 7, 37, 42, and 73 emerge from overrepresented sequences in training corpora. Break condition: Debiasing training corpora for number distribution would shift patterns toward uniform distribution.

### Mechanism 2: Tokenization Disconnect from Mathematical Semantics
LLMs cannot generate mathematically uniform random numbers because they process numbers as text tokens without numerical meaning. Numbers are tokenized based on character sequences rather than mathematical properties, so "42" has similar representational status to "horse" in latent space. Probability distributions over tokens reflect co-occurrence patterns, not mathematical uniformity. Break condition: Training with explicit mathematical uniformity objectives or number-aware tokenization would alter distribution patterns.

### Mechanism 3: Language-Specific Attention Pathway Divergence
Prompt language creates measurably different output distributions through language-specific self-attention computations. Different language prompts activate different training data subsets (e.g., Chinese prompts access Chinese corpus subset), producing different probability distributions over number tokens. DeepSeek-R1 reasons primarily in English/Chinese regardless of prompt language. Break condition: Identical number distribution patterns across language-specific corpus segments would eliminate prompt language effects.

## Foundational Learning

- **Temperature parameter in autoregressive sampling**: Rescales logits without changing rank ordering, explaining why T=2.0 doesn't eliminate bias. Quick check: If probability gap is 10x at T=1.0, what's the ratio at T=2.0?
- **χ² goodness-of-fit test for uniformity**: Quantifies deviation from uniform distribution; highest p-value was 2.19×10⁻¹⁵. Quick check: For 100 samples across 5 values, what χ² statistic indicates perfect uniformity?
- **Shannon entropy for distribution diversity**: Measures output variety relative to range size in randomness index. Quick check: What's maximum entropy for 10 equally likely outcomes?

## Architecture Onboarding

**Component map**: Prompt (language-specific) → Tokenizer → Embedding Layer → Transformer Layers (self-attention over training-corpus patterns) → Logits over number tokens → Temperature scaling → Sampling → Output number

**Critical path**: Logits computation in final transformer layer—this is where training-corpus biases are encoded as non-uniform probabilities over number tokens.

**Design tradeoffs**: Higher temperature increases exploration but doesn't eliminate bias (T=2.0 still favors same numbers). Chain-of-thought reasoning makes process visible but doesn't change biased outputs. Multilingual prompts show some variance but core biases persist.

**Failure signatures**: Extreme value avoidance (1 and X rarely selected), prime number fixation (3, 7, 37, 73 overrepresented), cultural artifacts (42 appears anomalously high), language-model meta-commentary (some models output disclaimers about randomness limitations).

**First 3 experiments**:
1. **Baseline bias mapping**: For your target model, run 100 calls each across ranges 1-5, 1-10, 1-100 with T=1.0. Compute χ² p-values and identify top-3 favored numbers. Expected: strong deviation from uniformity (p < 10⁻¹⁰).
2. **Temperature sensitivity test**: For range 1-100, test T = [0.1, 0.5, 1.0, 2.0] with 100 calls each. Measure whether favored numbers persist across temperatures. Expected: same "barcode" patterns as paper's Figure 7.
3. **Cross-lingual variance check**: Run same prompt in 3+ languages, compare output distributions. Expected: statistically significant differences (e.g., Japanese shifting preference toward 1) but core biases persist.

## Open Questions the Paper Calls Out

1. **Sampling parameter influence**: How do modifications to top-p and top-k parameters influence deterministic biases and output distributions? The study isolated temperature but maintained default settings for other decoding parameters, leaving their specific impact untested.

2. **Prompt engineering mitigation**: Can detailed prompt engineering interventions successfully mitigate deep-seated determinism and cognitive biases? The study used simple prompts to expose raw model bias without attempting intervention strategies, leaving their efficacy unknown.

3. **Numerical range configuration effects**: How does altering numerical range configuration (non-1 starting points or multi-token numbers) affect distribution and bias? The experiments used standard 1-N ranges where numbers typically map to single tokens, failing to account for potential biases from tokenization or boundary shifts.

## Limitations

- Sample size of 100 calls per configuration may be insufficient to fully characterize underlying probability distributions, especially for larger ranges
- Tokenization patterns and how numbers are processed as tokens aren't directly measured or verified
- Cross-lingual corpus access and actual corpus statistics for number distributions across languages remain unverified

## Confidence

**High Confidence**: LLMs exhibit strong deterministic bias in random number generation (p < 10⁻¹⁵ across all tested configurations)
**Medium Confidence**: Proposed mechanisms (training corpus bias propagation, tokenization disconnect, language-specific divergence) explain patterns but lack direct empirical validation
**Low Confidence**: Specific numerical preferences (3, 7, 37, 42, 73) have definitive causal explanations tied to human cognitive biases and cultural artifacts

## Next Checks

1. **Corpus Bias Verification**: Analyze actual training corpora of tested models to measure number distribution frequencies in contexts similar to random selection prompts. Compare these distributions to output patterns to establish direct causal links.

2. **Tokenization Pattern Analysis**: For each model, examine how numbers are tokenized and how token probabilities relate to numerical values. Test whether mathematical operations on token probabilities can produce more uniform distributions.

3. **Controlled Debiasing Experiment**: Fine-tune a base model on a corpus where number selection contexts have been artificially balanced to uniform distribution. Test whether output distributions shift toward uniformity compared to the original model.