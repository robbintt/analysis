---
ver: rpa2
title: How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn
  Social Agent Tasks
arxiv_id: '2505.04628'
source_url: https://arxiv.org/abs/2505.04628
tags:
- social
- llms
- arxiv
- target
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the need for systematic evaluation of large
  language models'' (LLMs) social communication capabilities in multi-user, multi-turn
  real-world social contexts. The authors introduce How Social Is It (HSII), a novel
  benchmark and evaluation framework that assesses LLMs across four stages: format
  parsing, target selection, target switching conversation, and stable conversation.'
---

# How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks

## Quick Facts
- arXiv ID: 2505.04628
- Source URL: https://arxiv.org/abs/2505.04628
- Reference count: 40
- Key outcome: Introduces HSII, a four-stage benchmark evaluating LLMs' social communication in multi-user, multi-turn scenarios, showing GPT-4 performs best while humans still lead.

## Executive Summary
This paper introduces How Social Is It (HSII), a novel benchmark designed to systematically evaluate large language models' (LLMs) capabilities in complex multi-user, multi-turn social scenarios. The benchmark addresses the gap in existing evaluations by focusing on realistic social interactions involving conflicts among multiple participants, moving beyond single-turn or dyadic communication tasks. The authors construct the HSII dataset from real-world news data, curate it to reflect authentic social complexity, and evaluate models across four distinct stages: format parsing, target selection, target switching conversation, and stable conversation. The paper also introduces COT-complexity to measure the efficiency of Chain-of-Thought reasoning in social tasks.

## Method Summary
The HSII benchmark evaluates LLMs through a four-stage pipeline: format parsing (instruction following), target selection (social awareness), target switching conversation (conversational adaptation), and stable conversation (sustained interaction). The dataset is constructed from news articles, extracting entities, relationships, and conflicts to create realistic social scenarios. Models are assessed using an overall HSII score that combines accuracy metrics across all stages, with weights α=β=γ=1.0. The COT-complexity metric measures the average number of reasoning cycles needed to reach a correct answer using a 6-step Chain-of-Thought process with self-reflection on "willing to help, professional, harmless, empathetic" dimensions.

## Key Results
- GPT-4 achieves the highest overall HSII score, demonstrating superior performance in multi-user social scenarios.
- Llama3-8b performs best among similarly sized models, showing strong social reasoning capabilities despite smaller parameter counts.
- Human responses outperform all tested LLMs, indicating significant room for improvement in complex social reasoning.
- Chain-of-Thought prompting improves performance, especially for weaker models, but does not close the gap with human-level social reasoning.
- COT-complexity reveals that reasoning efficiency varies significantly across models, with some requiring many more cycles to reach accuracy thresholds.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing multi-user, multi-turn social tasks into a four-stage evaluation pipeline provides a more systematic and diagnostic measure of an LLM's social capabilities than single-turn or holistic assessments.
- Mechanism: The HSII framework dissects social interaction into: 1) **Format Parsing** (instruction following), 2) **Target Selection** (social awareness and understanding of the scene), 3) **Target Switching Conversation** (conversational adaptation during transition), and 4) **Stable Conversation** (sustained multi-turn interaction management). These are combined into a single overall HSII score (Equation 1), weighted by hyperparameters (α, β, γ). This multi-stage approach isolates specific social skills, allowing for a granular analysis of where a model succeeds or fails.
- Core assumption: These four stages represent distinct, critical components of social competence, and their sequential performance is a valid proxy for overall social intelligence in multi-user contexts.
- Evidence anchors:
  - [abstract] "HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs..."
  - [section 5.2] "We introduce a novel metric, overall HSII score, to measure how social one tested LLM performs in four evaluation stages... ι = r1(1 + αr2(1 + β(r3 + γr4)))"
  - [corpus] A related paper also breaks down social intelligence, focusing on assessing multi-user dialogue state tracking capabilities, a sub-component of the broader interaction managed by HSII's later stages.
- Break condition: If subsequent research finds these stages are not predictive of real-world task success or are highly redundant, the metric's diagnostic power would be diminished.

### Mechanism 2
- Claim: A benchmark dataset derived from real-world news data, which contains inherent conflicts and complex relationships, creates a more authentic and challenging evaluation for social agents than synthetically generated or idealized scenarios.
- Mechanism: The HSII-Dataset is constructed by extracting entities, relationships, and conflict points from news articles (Section 5.1). This process leverages the complexity of real events (e.g., politics, business) to create scenarios where an agent must navigate competing interests among multiple users. The presence of "heightened conflict" is a core design principle, intended to stress-test an LLM's social reasoning beyond simple turn-taking.
- Core assumption: The algorithmic extraction and refinement process (using GPT-4 and human review) preserves enough of the original news scenario's authenticity and conflict structure to serve as a valid proxy for real-world social complexity.
- Evidence anchors:
  - [abstract] "The dataset is derived step by step from news dataset."
  - [section 5.1] "In our pipeline to augment the dataset’s authenticity, we curate news report excerpts from diverse sources... transforming them into a structured background setup... The presence of heightened conflict is instrumental in rigorously testing the models’ capabilities..."
  - [corpus] Corpus evidence is weak for this specific news-to-dataset mechanism; neighbors focus on multi-user features in apps (e.g., TikTok) or dialogue state tracking, not on the source of scenario data.
- Break condition: If the data transformation process flattens nuance, introduces systematic biases, or creates scenarios that are unrealistic for a conversational agent, the benchmark's connection to "real-world" social capability is broken.

### Mechanism 3
- Claim: Chain-of-Thought (CoT) prompting improves LLM performance on social tasks, and the novel "COT-complexity" metric quantifies the efficiency of this reasoning process.
- Mechanism: CoT prompting guides an LLM through a structured reasoning process (e.g., analyze psychological states, infer conflicts), which experiments show improves target selection accuracy. The "COT-complexity" (Equation 2) measures the average number of reasoning/reflection cycles (k) a model needs to reach a correct answer. A lower complexity score for a given accuracy indicates a more efficient reasoning process, providing a way to evaluate the trade-off between performance gains and computational cost.
- Core assumption: The number of reasoning cycles is a meaningful proxy for computational and cognitive efficiency, and the structured CoT framework (e.g., state analysis → conflict inference → action) genuinely aids social reasoning rather than simply adding tokens.
- Evidence anchors:
  - [abstract] "...we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency."
  - [section 6.4] "Table 2 reveal that incorporating a 6-step COT reasoning... leads to a plausible improvement... with an average lead of ∆ = 0.067."
- Break condition: If the relationship between the number of reasoning steps and actual computational cost is non-linear or inconsistent across models, or if CoT provides no benefit for certain task types, the metric loses its utility for comparison.

## Foundational Learning

- **Multi-Agent/Multi-User Interaction Protocols**
  - Why needed here: To understand the problem space the HSII benchmark addresses—moving beyond single-user, dyadic interactions to complex scenarios with multiple agents and users. This foundational concept explains the "why" behind the paper's focus on target selection and transition.
  - Quick check question: How does managing a conversation with two users (e.g., a parent and a teacher) differ fundamentally from managing a single-user dialogue?

- **Chain-of-Thought (CoT) Prompting & Self-Reflection**
  - Why needed here: To grasp the method used for performance enhancement and efficiency measurement. The paper builds on standard CoT by adding a self-reflection loop and formalizing a metric (COT-complexity) around it.
  - Quick check question: In a multi-turn social task, what would be the first "thought" in a CoT process designed to select the next conversational target?

- **Benchmark Evaluation Metrics (Accuracy vs. Win Rate)**
  - Why needed here: To understand how the paper constructs its overall score (HSII score). It combines an objective metric (accuracy of target selection, r2) with a subjective metric (win rate against a golden standard, r3, r4).
  - Quick check question: Why might a model have a high target selection accuracy but a low win rate in the subsequent conversation?

## Architecture Onboarding

- **Component Map:**
  - Core Component: The `HSII Evaluation Pipeline` takes a model's response to a multi-turn, multi-user prompt and passes it through four sequential stages: Format Parsing, Target Selection, Target Switching Conversation, and Stable Conversation. Each stage produces a sub-score.
  - Key Algorithm: The `Overall HSII Score Calculator` (Equation 1) takes the four sub-scores (r1, r2, r3, r4) and combines them using weighted hyperparameters (α, β, γ) into a single scalar value `ι`.
  - Data Source: The `HSII-Dataset` is a collection of multi-user, multi-turn dialogue scenarios derived from news articles, stored with structured fields like `domain`, `participants`, `conflicts`, and a `golden_response`.

- **Critical Path:**
  1. **Prompt Engineering:** Designing the prompt that presents the multi-user scenario and elicits a structured response from the LLM.
  2. **Response Parsing:** Reliably extracting the model's selected target and its generated utterance from its free-text output.
  3. **Adversarial Evaluation:** Using GPT-4 and human annotators to compare the model's utterance against the `golden_response` to determine a "win" or "loss." This is a key, non-trivial step for the subjective parts of the evaluation.

- **Design Tradeoffs:**
  - **Accuracy vs. Efficiency:** Using CoT improves accuracy but increases latency and cost. The `COT-complexity` metric is introduced to make this trade-off explicit. A new engineer must decide whether to optimize for the highest score or for the best score-per-computation-unit.
  - **Automated vs. Human Evaluation:** The framework relies on GPT-4 for adversarial evaluation of conversation quality to scale, but incorporates human evaluation for alignment. The tradeoff is between scalability (fully automated) and value alignment (human-in-the-loop).

- **Failure Signatures:**
  - **Parsing Failures:** The model may fail to format its output correctly (e.g., not specifying a target in the required format), causing a failure at the first stage (r1=0). This cascades, yielding a final HSII score of zero for that test case.
  - **Social Hallucination:** The model selects a target correctly but generates utterances that are socially inappropriate or ignore the established conflicts in the scene. This would result in high accuracy (r2) but low win rates (r3, r4).
  - **Infinite COT Loops:** During COT-complexity evaluation, a model may fail to reach the correct answer even after the maximum number of reflection cycles (N∞ = 128). This is recorded as infinite complexity and indicates the model's reasoning is fundamentally misaligned with the task.

- **First 3 Experiments:**
  1. **Baseline Profiling:** Run a suite of open-source models (e.g., Llama2-7b, Mistral-7b) through the HSII pipeline without CoT to establish a baseline for the four sub-scores and overall HSII score.
  2. **Ablation on Evaluation Components:** Run an ablation study to determine the impact of each stage. For example, compute the HSII score but skip the target selection check (set α=0) to see how much conversational quality alone contributes to the final score.
  3. **CoT Efficiency Analysis:** Implement the CoT self-reflection loop and measure the `COT-complexity` for different models. The goal is to plot "accuracy vs. average reasoning steps" to visualize the efficiency frontier.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does expanding the dataset scale and testing a more diverse range of LLMs alter the generalizability of the HSII score findings?
  - Basis in paper: [explicit] The conclusion states, "Looking forward, a compelling direction for future work is to expand the scale of our dataset and to test LLMs with a more diverse range."
  - Why unresolved: The current study uses a dataset of 8,305 samples and a limited set of representative models (mostly 7B-8B parameter size and GPT-4).
  - What evidence would resolve it: Benchmarking results from larger, more varied social scenario datasets applied to a wider array of state-of-the-art models.

- **Open Question 2:** How do LLMs perceive different character roles within society, and how might these roles evolve during complex multi-user interactions?
  - Basis in paper: [explicit] The conclusion identifies a need for "probing... how LLMs perceive different characters, the roles they should assume in society, and how these roles might evolve."
  - Why unresolved: The current metrics focus on task completion (HSII score) and reasoning efficiency (COT complexity) rather than the internal representation or dynamic evolution of social roles.
  - What evidence would resolve it: Qualitative analysis or new metrics tracking role adaptation and consistency over extended multi-turn dialogues.

- **Open Question 3:** Can specific training or prompting strategies mitigate the observed tendency of LLMs to "bypass explicit conflicts" rather than resolving them directly?
  - Basis in paper: [inferred] The paper notes in Section 6.3 that "Models try employing tricks to bypass explicit conflicts," such as shifting focus to positive imaginations to avoid addressing negative details.
  - Why unresolved: The paper identifies this avoidance behavior as a gap compared to human performance but does not propose a method to correct it.
  - What evidence would resolve it: A comparative study evaluating LLMs on conflict-heavy scenarios with and without specific conflict-resolution fine-tuning or directives.

## Limitations
- The sequential scoring mechanism may create compounding effects that obscure where models actually fail in social reasoning.
- The dataset construction process involves significant algorithmic transformation that may introduce artifacts not present in natural social interactions.
- Reliance on GPT-4 for both data generation and evaluation creates potential circularity issues, as the benchmark's difficulty may be tuned to GPT-4's specific capabilities.

## Confidence
- **High Confidence:** The experimental finding that GPT-4 outperforms other models on HSII metrics and that CoT prompting improves performance, particularly for smaller models. These results are directly supported by the presented data and align with established patterns in LLM evaluation literature.
- **Medium Confidence:** The claim that HSII provides a more systematic and diagnostic measure of social capabilities than existing benchmarks. While the four-stage approach is novel and methodologically sound, the paper provides limited comparative analysis against other social intelligence benchmarks to substantiate claims of superiority.
- **Low Confidence:** The assertion that news-derived scenarios with conflicts provide authentic representations of real-world social complexity. The paper describes the data construction process but does not validate whether the transformed scenarios maintain the nuance and authenticity of the original news events or whether they generalize to other social contexts.

## Next Checks
1. **Diagnostic Ablation Study:** Conduct an ablation analysis where models are evaluated on each HSII stage independently (bypassing the sequential dependency) to determine whether the four stages measure distinct capabilities or whether early-stage failures disproportionately impact later-stage scores. This would clarify whether the overall score meaningfully represents a combination of independent social skills.

2. **Benchmark Transferability Test:** Evaluate the same models on HSII using datasets constructed from different source domains (e.g., literature, social media, or workplace communications) to assess whether performance patterns remain consistent across different types of social scenarios. This would validate whether the news-derived dataset is representative of broader social interaction complexity.

3. **Human-Machine Gap Analysis:** Implement a controlled study where human participants perform the same HSII tasks and compare their performance distributions against LLM outputs. This would establish whether the reported "room for improvement" represents a meaningful gap in social reasoning capabilities or simply reflects differences in task interpretation between humans and machines.