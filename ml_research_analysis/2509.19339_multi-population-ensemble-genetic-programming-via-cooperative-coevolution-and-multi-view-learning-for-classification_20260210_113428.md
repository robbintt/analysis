---
ver: rpa2
title: Multi-population Ensemble Genetic Programming via Cooperative Coevolution and
  Multi-view Learning for Classification
arxiv_id: '2509.19339'
source_url: https://arxiv.org/abs/2509.19339
tags:
- large
- megp
- small
- negligible
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Multi-population Ensemble Genetic Programming
  (MEGP), a novel framework that integrates cooperative coevolution and multi-view
  learning to enhance classification in high-dimensional feature spaces. MEGP partitions
  data into conditionally independent feature subsets, assigning each to a separate
  population that evolves independently while contributing to an ensemble model.
---

# Multi-population Ensemble Genetic Programming via Cooperative Coevolution and Multi-view Learning for Classification

## Quick Facts
- arXiv ID: 2509.19339
- Source URL: https://arxiv.org/abs/2509.19339
- Reference count: 40
- Multi-population ensemble GP with cooperative coevolution and multi-view learning improves classification performance in high-dimensional feature spaces.

## Executive Summary
This study introduces MEGP, a framework that partitions high-dimensional datasets into conditionally independent feature subsets, assigning each to a separate population for parallel evolution. By combining cooperative coevolution with a multi-view learning approach, MEGP enhances generalization and convergence rates compared to baseline GP models. Experiments across eight benchmark datasets demonstrate MEGP's superior performance in Log-Loss, Precision, Recall, F1 score, and AUC.

## Method Summary
MEGP partitions input features into disjoint subsets using SPFP, assigning each to an independent GP population. Individuals are evolved using standard GP operators, but fitness is evaluated through both isolated (population-specific) and ensemble-level (collaborative) metrics. A hybrid selection mechanism balances local adaptation with global cooperation, while outputs are aggregated via a learnable softmax-based layer for adaptive decision fusion. The framework is evaluated on 8 UCI datasets with 54%/13%/33% train/validation/test splits.

## Key Results
- MEGP consistently outperforms baseline GP in Log-Loss, Precision, Recall, F1 score, and AUC across all eight benchmark datasets.
- MEGP demonstrates superior convergence rates and more efficient fitness gains during evolution compared to baseline models.
- Statistical analyses confirm significant improvements in both evolutionary progression and predictive accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partitioning features into conditionally independent subsets reduces search complexity and promotes specialized evolution within each population.
- Mechanism: SPFP algorithm assigns disjoint feature subsets to separate populations, enabling parallel exploration of distinct hypothesis space regions while preventing feature redundancy.
- Core assumption: Features can be meaningfully partitioned such that subsets provide complementary information.
- Evidence anchors: Abstract states MEGP "decomposes the input space into conditionally independent feature subsets"; Section III-B mentions SPFP ensures "well-balanced feature subset that maintains feature relevance while reducing redundancy."
- Break condition: If features are highly interdependent, enforced partitioning may discard critical cross-feature patterns.

### Mechanism 2
- Claim: Hybrid fitness mechanism combining isolated and ensemble-level fitness balances local adaptation with global cooperation.
- Mechanism: Dual elitism (EF_iso, EF_en) and probabilistic tournament selection (p_en) preserve both strong standalone classifiers and individuals that contribute to high-performing ensembles.
- Core assumption: Optimal ensemble is not simply collection of best isolated individuals; complementary non-locally-optimal individuals can provide superior global performance.
- Evidence anchors: Abstract mentions "hybrid selection mechanism incorporating both isolated and ensemble-level fitness promotes inter-population cooperation"; Section III-D defines dual-elitism parameters.
- Break condition: Excessive EF_en and p_en may over-prioritize short-term ensemble gains at cost of long-term diversity.

### Mechanism 3
- Claim: Learnable softmax-based aggregation layer provides adaptive decision fusion, improving generalization over fixed weighting.
- Mechanism: Outputs of multiple genes are linearly combined with optimized weights/biases, transformed via softmax, then aggregated at ensemble level with ensemble weights optimized via COBYLA.
- Core assumption: Populations evolve to capture different, complementary aspects of data; linear combination of probabilistic outputs can achieve better calibration and accuracy.
- Evidence anchors: Abstract mentions "outputs are aggregated via a differentiable softmax-based weighting layer, enhancing both model interpretability and adaptive decision fusion"; Section III-B describes gradient-based weight optimization.
- Break condition: If populations evolve highly correlated outputs or optimization gets stuck in poor local minima, adaptive layer may fail to leverage diversity.

## Foundational Learning

- Concept: **Genetic Programming (GP) Fundamentals**
  - Why needed here: MEGP is built on core GP cycle of selection, crossover, and mutation over tree-structured programs.
  - Quick check question: Can you explain how tournament selection and subtree crossover work in standard GP?

- Concept: **Ensemble Learning Principles**
  - Why needed here: MEGP's power derives from combining multiple learners; understanding diversity benefits is key.
  - Quick check question: What are main ideas behind bagging and boosting, and how does diversity among ensemble members contribute to performance?

- Concept: **Multi-View Learning Paradigm**
  - Why needed here: Paper explicitly frames feature partitioning as multi-view learning approach.
  - Quick check question: In multi-view learning, what is assumption about relationship between different feature views, and how can it benefit a model?

## Architecture Onboarding

- Component map: Input Processor (SPFP) -> Population Pool (P independent populations) -> Evolutionary Engine (per population) -> Individual/Population Fitness Module (FT_iso) -> Ensemble Synthesis Module (FT_en) -> Hybrid Selection Controller -> Adaptive Aggregation Layer

- Critical path:
  1. Initialize: Partition features and create P populations
  2. Evaluate: Compute FT_iso for all individuals in all populations
  3. Form Ensembles: Combine individuals with representatives from other populations to compute FT_en
  4. Select: Apply dual-elitism and tournament selection (using p_en) to choose parents
  5. Evolve: Apply crossover, mutation, reproduction to create next-generation populations
  6. Loop: Repeat from step 2 until stopping criterion
  7. Finalize: Optimize ensemble weights (w_en) and deploy final combined model

- Design tradeoffs:
  1. Number of Populations (P): More populations increase parallelism and diversity but raise communication overhead
  2. Ensemble Selection Probability (p_en): Higher p_en favors global cooperation but may reduce local robustness
  3. Elitism Split (EF_iso vs EF_en): Balances preserving strong local specialists vs good team players
  4. Computational Cost: Ensemble fitness evaluation and weight optimization add overhead compared to standard GP

- Failure signatures:
  1. Rapid convergence to homogeneous solutions indicates loss of diversity
  2. Stagnant ensemble performance suggests populations aren't evolving complementary