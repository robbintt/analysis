---
ver: rpa2
title: 'Burn After Reading: Do Multimodal Large Language Models Truly Capture Order
  of Events in Image Sequences?'
arxiv_id: '2506.10415'
source_url: https://arxiv.org/abs/2506.10415
tags:
- image
- images
- sentence
- sequence
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the TempVS benchmark to evaluate multimodal
  large language models' (MLLMs) ability to understand and reason about temporal sequences
  in image sequences. TempVS contains three main tests (event relation inference,
  sentence ordering, and image ordering) along with grounding tests, using 2,085 image
  sequences from cartoon animations, movies, and daily-life albums.
---

# Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?

## Quick Facts
- arXiv ID: 2506.10415
- Source URL: https://arxiv.org/abs/2506.10415
- Reference count: 17
- Models struggle with temporal reasoning in image sequences, with significant performance gaps compared to humans

## Executive Summary
This paper introduces TempVS, a benchmark designed to evaluate multimodal large language models' (MLLMs) ability to understand temporal sequences in image sequences. The benchmark contains three main tests (event relation inference, sentence ordering, and image ordering) along with grounding tests, using 2,085 image sequences from diverse sources. Evaluation of 38 state-of-the-art MLLMs reveals that current models struggle significantly with temporal reasoning, even when they can accurately ground individual events to images. The best-performing model achieved only 60.4% accuracy on two-event relation inference, compared to 82.5% for humans.

## Method Summary
The TempVS benchmark evaluates MLLMs across three core temporal reasoning tasks: event relation inference (determining chronological order between events), sentence ordering (arranging sentences describing events in temporal sequence), and image ordering (arranging images depicting events in temporal sequence). The benchmark uses 2,085 image sequences from cartoon animations, movies, and daily-life albums, requiring models to integrate both visual and linguistic modalities. Evaluation covers 38 MLLMs ranging from 0.5B to 78B parameters, including models like InternVL2.5-78B-MPO, GPT-4V, and various open-source alternatives. Human baseline performance is established for comparison, revealing substantial gaps in model capabilities.

## Key Results
- InternVL2.5-78B-MPO achieved only 60.4% accuracy on two-event relation inference, compared to 82.5% for humans
- Models struggle with temporal reasoning even when they can accurately ground individual events to images
- The benchmark reveals significant performance gaps across all evaluated MLLMs, suggesting fundamental limitations in current architectures or training approaches

## Why This Works (Mechanism)
The paper suggests that current MLLMs fail to capture temporal reasoning because they may lack explicit temporal modeling mechanisms. While models can process individual images and recognize events, they appear to struggle with integrating sequential information across multiple frames. The architecture may not be designed to capture temporal dependencies effectively, and current training objectives might not emphasize temporal coherence. The gap between event grounding ability and temporal reasoning suggests that the models' cross-modal fusion mechanisms are insufficient for capturing the temporal relationships between events.

## Foundational Learning
- Temporal reasoning in multimodal contexts: Required for understanding event sequences across visual and textual inputs
- Quick check: Can the model correctly order three simple events (e.g., "plant seed," "water plant," "harvest")?

- Event grounding vs temporal reasoning: Models may accurately identify events but fail to understand their chronological relationships
- Quick check: Does the model perform better on individual event recognition than on ordering those events?

- Multimodal integration challenges: Combining visual and linguistic information for temporal understanding presents unique difficulties
- Quick check: How does performance change when only visual or only textual information is provided?

## Architecture Onboarding

**Component Map:**
Input images -> Visual encoder -> Cross-modal fusion -> Temporal reasoning module -> Output predictions

**Critical Path:**
Visual encoding -> Multimodal fusion -> Temporal reasoning -> Prediction generation

**Design Tradeoffs:**
- Larger models show better performance but still fail significantly
- No clear architectural pattern distinguishes successful models
- Balance between visual detail capture and temporal reasoning capability

**Failure Signatures:**
- Accurate event grounding but incorrect temporal ordering
- Performance degradation with increased sequence length
- Inconsistent results across similar temporal reasoning tasks

**3 First Experiments:**
1. Evaluate model performance on single-modality versions of tasks (visual-only and text-only)
2. Test with progressively longer sequences to identify performance thresholds
3. Analyze attention patterns during temporal reasoning to identify potential bottlenecks

## Open Questions the Paper Calls Out
The paper raises several critical open questions regarding temporal reasoning in MLLMs. First, whether the fundamental architecture of current MLLMs is inherently limited for temporal reasoning, or if appropriate training objectives and data could overcome these limitations. Second, whether temporal reasoning is a skill that requires specialized architectural components, or if it can emerge from proper training of general-purpose models. Third, what specific aspects of temporal reasoning (causal relationships, event duration, sequential dependencies) are most challenging for current models. Finally, whether the performance gaps observed are due to limitations in the models themselves or in how we evaluate temporal reasoning capabilities.

## Limitations
- Performance gap between models and humans suggests fundamental architectural or training deficiencies, though exact causes remain unclear
- Benchmark may contain domain-specific biases from cartoon animations, movies, and daily-life album sources
- Evaluation focuses on accuracy metrics without detailed analysis of specific failure modes
- Limited exploration of whether performance issues stem from visual modality limitations, language modality limitations, or integration mechanisms
- Human baseline established with only 100 participants, which may not fully represent human performance diversity

## Confidence

**High Confidence:**
- Current MLLMs struggle with temporal reasoning in image sequences, well-supported by comprehensive evaluation of 38 models

**Medium Confidence:**
- Models fail at temporal reasoning even when they can accurately ground individual events, requires further validation
- Future research directions in architectural design, training objectives, and post-training methods are reasonable but speculative

## Next Checks
1. Conduct ablation studies varying the number of events in sequences to determine if performance degrades monotonically or at specific thresholds

2. Implement controlled experiments testing whether temporal reasoning failures stem from visual modality limitations, language modality limitations, or integration mechanisms

3. Develop and evaluate fine-tuning strategies specifically targeting temporal reasoning, such as curriculum learning approaches that gradually increase sequence complexity