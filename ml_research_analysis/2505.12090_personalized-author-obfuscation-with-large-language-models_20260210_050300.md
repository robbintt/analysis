---
ver: rpa2
title: Personalized Author Obfuscation with Large Language Models
arxiv_id: '2505.12090'
source_url: https://arxiv.org/abs/2505.12090
tags:
- obfuscation
- user
- author
- successful
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores using large language models for author obfuscation\
  \ through paraphrasing, focusing on user-level performance variations. The authors\
  \ train authorship verification models and find that LLMs can reduce detection accuracy,\
  \ but effectiveness varies significantly across authors\u2014a bimodal distribution."
---

# Personalized Author Obfuscation with Large Language Models

## Quick Facts
- **arXiv ID**: 2505.12090
- **Source URL**: https://arxiv.org/abs/2505.12090
- **Reference count**: 10
- **Primary result**: LLM-based author obfuscation reduces verification accuracy with significant performance variations across authors, improved through personalized SHAP-based prompting

## Executive Summary
This paper investigates using large language models (LLMs) for authorship obfuscation through paraphrasing, addressing the critical need for privacy protection in digital communications. The authors demonstrate that while LLMs can effectively reduce authorship detection accuracy, performance varies dramatically across different authors, exhibiting a bimodal distribution. They propose a personalized obfuscation approach using SHAP values to identify and target each author's distinctive stylistic features, which successfully reduces this bimodality and enhances overall obfuscation effectiveness. LLaMA-3.1 shows superior performance compared to GPT-4 in authorship obfuscation tasks.

## Method Summary
The authors develop a personalized author obfuscation system using large language models. First, they train an authorship verification model on training data to identify authorship attribution patterns. Using SHAP values, they analyze feature importance to determine each author's distinctive stylistic elements. This analysis informs personalized prompts that instruct LLMs to specifically target and modify these identified features during paraphrasing. The approach employs LLMs (specifically LLaMA-3.1 and GPT-4) to paraphrase original texts while focusing on obfuscating the most distinctive features for each individual author, rather than applying generic obfuscation strategies.

## Key Results
- LLM-based paraphrasing reduces authorship verification accuracy by 15-30% on average
- Obfuscation performance exhibits bimodal distribution across authors (some authors easily obfuscated, others resistant)
- Personalized SHAP-based prompting reduces bimodality and improves obfuscation effectiveness
- LLaMA-3.1 outperforms GPT-4 in authorship obfuscation tasks

## Why This Works (Mechanism)
The effectiveness stems from targeting author-specific stylistic features that are most distinctive for authorship attribution. By identifying these features through SHAP analysis of the verification model, the system can direct LLMs to focus paraphrasing efforts on the elements that contribute most to author identification. This personalized approach addresses the fundamental limitation of generic obfuscation methods that treat all authors uniformly, instead recognizing that different authors have varying degrees of distinctive stylistic patterns that require different obfuscation strategies.

## Foundational Learning

**Authorship Attribution** - Why needed: Core task being protected against; quick check: Verify verification accuracy on original texts exceeds random chance

**SHAP Values** - Why needed: Quantify feature importance for personalized targeting; quick check: Confirm SHAP values correlate with actual attribution importance

**Paraphrasing Quality Metrics** - Why needed: Ensure semantic preservation during obfuscation; quick check: Measure semantic similarity between original and obfuscated texts

**Bimodal Distribution Analysis** - Why needed: Understand why some authors resist obfuscation; quick check: Statistical test for distribution shape across authors

## Architecture Onboarding

**Component Map**: Training Data -> Authorship Verification Model -> SHAP Analysis -> Personalized Prompts -> LLM Paraphraser -> Obfuscated Output

**Critical Path**: The verification model and SHAP analysis form the critical path, as they determine which features the LLM must target during paraphrasing. Without accurate identification of distinctive features, the obfuscation effectiveness is limited.

**Design Tradeoffs**: The system balances between effective obfuscation (changing identifiable features) and text quality (maintaining semantic coherence). More aggressive feature modification improves privacy but risks degrading readability and meaning.

**Failure Signatures**: Complete failure occurs when the verification model cannot accurately attribute original texts, making SHAP analysis unreliable. Partial failure manifests as insufficient feature modification or over-modification that destroys text coherence.

**First Experiments**:
1. Measure baseline authorship verification accuracy on original texts to establish the attribution problem
2. Test generic (non-personalized) LLM paraphrasing to establish baseline obfuscation performance
3. Analyze SHAP value distributions across authors to understand feature importance patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Bimodal performance distribution suggests some authors' styles are inherently more resistant to obfuscation, with unclear underlying causes
- Comparison between LLaMA-3.1 and GPT-4 focuses on efficacy without fully explaining architectural or training differences driving performance gaps
- Evaluation framework assumes reducing verification accuracy is the primary goal, potentially overlooking semantic preservation and text quality considerations

## Confidence

**High Confidence**: The core finding that LLM-based obfuscation reduces authorship verification accuracy is well-supported by experimental results, with robust demonstration of bimodal performance distribution.

**Medium Confidence**: The effectiveness of SHAP-based personalized prompting is supported but generalizability to different datasets or verification models remains uncertain; LLaMA-3.1's comparative advantage is demonstrated but not fully explained.

**Low Confidence**: Causal mechanisms behind bimodal distribution and specific reasons for LLaMA-3.1's superior performance are not thoroughly investigated, making definitive claims premature.

## Next Checks
1. Conduct cross-dataset validation to assess whether observed bimodality and personalized prompting effectiveness generalize beyond the specific corpus used

2. Perform ablation studies to isolate which aspects of SHAP-based feature selection contribute most to obfuscation success and whether simpler methods achieve similar results

3. Evaluate semantic preservation and fluency of obfuscated texts using human judges or established metrics to ensure reduced detectability doesn't compromise text quality or coherence