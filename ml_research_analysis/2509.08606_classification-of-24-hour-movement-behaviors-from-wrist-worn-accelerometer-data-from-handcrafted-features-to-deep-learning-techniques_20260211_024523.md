---
ver: rpa2
title: 'Classification of 24-hour movement behaviors from wrist-worn accelerometer
  data: from handcrafted features to deep learning techniques'
arxiv_id: '2509.08606'
source_url: https://arxiv.org/abs/2509.08606
tags:
- algorithms
- features
- movement
- behavior
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared deep learning (DL) and classical machine learning
  (ML) algorithms for classifying 24-hour movement behaviors into sleep, sedentary,
  light intensity physical activity (LPA), and moderate-to-vigorous intensity physical
  activity (MVPA) using wrist-worn accelerometer data. Four DL algorithms (LSTM, BiLSTM,
  GRU, 1D-CNN) were trained on raw signals and handcrafted features, alongside six
  classical ML algorithms (XGBoost, RF, ANN, SVM, DT, LR) trained on handcrafted features.
---

# Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques

## Quick Facts
- arXiv ID: 2509.08606
- Source URL: https://arxiv.org/abs/2509.08606
- Reference count: 40
- Primary result: Deep learning models achieved ~85% overall accuracy vs ~79-80% for classical ML on 24-hour movement behavior classification

## Executive Summary
This study compared deep learning (DL) and classical machine learning (ML) algorithms for classifying 24-hour movement behaviors into sleep, sedentary, light physical activity (LPA), and moderate-to-vigorous physical activity (MVPA) using wrist-worn accelerometer data. The authors evaluated four DL algorithms (LSTM, BiLSTM, GRU, 1D-CNN) on raw signals and handcrafted features, alongside six classical ML algorithms (XGBoost, RF, ANN, SVM, DT, LR) on handcrafted features. While DL models trained on raw signals showed slightly higher overall accuracy (~85%) compared to those trained on handcrafted features (~79-80%), the improvement over classical ML methods was minimal. Random Forest demonstrated the best performance for distinguishing between LPA and MVPA classes, suggesting its utility for this challenging classification task.

## Method Summary
The study used the Capture-24 dataset containing 151 adults with 24-hour free-living data from Axivity AX3 accelerometers at 100 Hz. Data was segmented into 10-second non-overlapping windows, with 104 handcrafted features (37 time domain, 67 frequency domain) extracted per window. Four DL algorithms (LSTM, BiLSTM, GRU, 1D-CNN) were trained on both raw 4Ã—1000 input signals and the 104 features, while six classical ML algorithms (RF, XGBoost, ANN, SVM, DT, LR) were trained only on features. Models were evaluated using a participant-based 80/10/10 split (121/15/15 for train/validation/test), with performance metrics including overall accuracy, macro accuracy, Cohen's Kappa, and confusion matrices. MRMR feature selection was also evaluated to assess impact on performance.

## Key Results
- DL algorithms trained on raw signals achieved ~85% overall accuracy vs ~79-80% for those trained on handcrafted features
- Random Forest showed superior performance in distinguishing LPA (66%) and MVPA (76%) classes compared to other methods
- MRMR feature selection had minimal impact on performance, with accuracy variations of only 1-4% when reducing features from 104 to 60
- Highest confusion occurred between LPA and MVPA classes across all algorithms

## Why This Works (Mechanism)

### Mechanism 1: Temporal Dependency Preservation in Raw Signals
Deep learning models trained on raw signals may outperform those trained on handcrafted features by preserving sequential dependencies. Recurrent architectures (LSTM, BiLSTM, GRU) utilize gating mechanisms to retain long-term temporal context, while handcrafted feature extraction compresses signals into summary statistics that may discard sequential nuances required to distinguish between similar movement intensities. The 10-second raw signal window likely contains sequential patterns that predict behavior better than summary statistics.

### Mechanism 2: Class Confusion Reduction via Random Forest Ensembles
Random Forest appears more robust in distinguishing between LPA and MVPA compared to other approaches. RF utilizes an ensemble of decision trees with bootstrap sampling, allowing better definition of non-linear boundaries in feature space for classes with similar acceleration magnitudes. This ensemble approach handles feature correlations better than linear models (LR) or single trees (DT), which fail to generalize subtle distinctions between these intensity levels.

### Mechanism 3: Feature Redundancy Invariance
Reducing feature set size via MRMR does not significantly degrade performance because the extracted 104 features likely contain high mutual information. MRMR removes redundant features while retaining relevance, and classical models like RF and XGBoost handle redundancy well through feature sampling or splitting. The remaining features after selection still cover necessary time and frequency domain characteristics for classification.

## Foundational Learning

- **Concept: Windowing (Segmentation)**
  - Why needed here: Converts continuous 100Hz data into non-overlapping 10-second chunks, trading temporal resolution for pattern capture
  - Quick check: Does a 10-second window risk smoothing out "brisk and sporadic" activities, or is it sufficient for cyclical sleep/wake patterns?

- **Concept: Recurrent Neural Networks (RNN/LSTM/GRU)**
  - Why needed here: These architectures outperformed 1D-CNN on raw data by processing data sequentially, making them sensitive to the order of movement
  - Quick check: Why might a BiLSTM perform better than a standard LSTM for activity recognition, considering we usually analyze data after collection?

- **Concept: The Class Imbalance/Similarity Problem**
  - Why needed here: The study explicitly notes high confusion between LPA and MVPA, showing classification involves distinguishing subtle intensity gradients
  - Quick check: Why would "Sleep" be easier to classify (94%+) than "LPA" (63-76%) using the same accelerometer?

## Architecture Onboarding

- **Component map:** Axivity AX3 (100Hz) -> 10s Windows (1000 samples) -> Pipeline A (Raw): 4x1000 Matrix -> LSTM/GRU/BiLSTM -> Dense Layer -> Softmax; Pipeline B (Features): 10s Window -> Feature Extraction (104 stats) -> MRMR (Optional) -> RF/XGBoost -> Prediction

- **Critical path:** 1) Data Cleaning: Ensure no missing timestamps within 10s windows; 2) Feature Extraction (for ML): Accurate calculation of VM and frequency domain powers; 3) Sequence Shaping (for DL): Reshaping raw data into [Samples, TimeSteps, Channels] correctly

- **Design tradeoffs:** Raw DL (LSTM): Highest accuracy (~85%) but high computational cost and black box opacity; Feature RF: Competitive accuracy (~80%), much faster training, higher interpretability, better specific performance on difficult classes (LPA); 1D-CNN: Lower accuracy here (~80%) vs RNNs, suggesting spatial filtering alone is insufficient

- **Failure signatures:** High LPA/MVPA Confusion (model predicts "LPA" when user is doing "MVPA"); Overfitting to Validation (without early stopping, DL models memorize participant-specific patterns); Data Leakage (ensure participant-based split is respected)

- **First 3 experiments:** 1) Baseline Replication: Train Random Forest on 104 features using specific split (121/15/15) to verify ~80% benchmark; 2) Raw vs. Feature Ablation: Train LSTM first on 104 features, then on raw data to confirm ~5% gain; 3) Class Confusion Analysis: Generate confusion matrix specifically for LPA vs. MVPA to identify misclassification patterns

## Open Questions the Paper Calls Out

- How does varying the signal segmentation window size impact the predictive performance of deep learning algorithms compared to classical machine learning methods? The authors note that future studies may consider the influence of window size, particularly on DL performance, as this study only utilized a fixed 10-second window.

- Can incorporating individualized energy expenditure measures reduce the confusion between LPA and MVPA? The absence of measured energy expenditure forced the use of generic MET thresholds, which disregard individual variability in energy expenditure among participants.

- How well do models trained on datasets with wide age demographics generalize to specific, narrower age cohorts? The wide age range (18-91 years) of the dataset is identified as a potential limitation when applying models to participants with limited age ranges.

## Limitations

- Architectural details of DL models are referenced but not fully specified, making exact replication difficult
- Generalizability to other populations and accelerometer brands remains uncertain due to specific dataset characteristics
- The marginal improvement of DL over classical ML (~5% accuracy) may be partially attributed to implementation choices rather than fundamental superiority

## Confidence

- **High Confidence:** RF's superior performance in distinguishing LPA and MVPA classes; minimal impact of MRMR feature selection on overall accuracy
- **Medium Confidence:** Slight advantage of DL models trained on raw signals over feature-based approaches; overall classification accuracy benchmarks
- **Low Confidence:** Specific architectural choices driving DL performance; extent to which findings generalize beyond the Capture-24 dataset

## Next Checks

1. **Architecture Transparency:** Obtain or replicate the exact DL model specifications (layer dimensions, dropout rates, optimizer settings) to verify reported performance differences

2. **Cross-Dataset Validation:** Test the best-performing models (RF for LPA/MVPA, LSTM for overall accuracy) on an independent accelerometer dataset to assess generalizability

3. **Temporal Resolution Sensitivity:** Repeat experiments with varying window sizes (5s, 15s, 30s) to determine if the 10-second window is optimal for capturing movement patterns and reducing class confusion