---
ver: rpa2
title: A Multi-Agent System for Generating Actionable Business Advice
arxiv_id: '2601.12024'
source_url: https://arxiv.org/abs/2601.12024
tags:
- agent
- advice
- issue
- issues
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent LLM framework for transforming
  large-scale customer reviews into actionable business advice. The approach uses
  clustering to select representative reviews, iterative recommendation-evaluation
  loops for refining advice, and ranking based on feasibility and practicality.
---

# A Multi-Agent System for Generating Actionable Business Advice

## Quick Facts
- arXiv ID: 2601.12024
- Source URL: https://arxiv.org/abs/2601.12024
- Authors: Kartikey Singh Bhandari; Tanish Jain; Archit Agrawal; Dhruv Kumar; Praveen Kumar; Pratik Narang
- Reference count: 40
- Primary result: Multi-agent LLM framework outperforms single-model baselines on actionability, specificity, and non-redundancy across service domains.

## Executive Summary
This paper introduces a multi-agent LLM framework for transforming large-scale customer reviews into actionable business advice. The approach uses clustering to select representative reviews, iterative recommendation-evaluation loops for refining advice, and ranking based on feasibility and practicality. Experiments across three service domains show that the framework consistently outperforms single-model baselines on actionability, specificity, and non-redundancy, with medium-sized models approaching large model performance. Ablation studies confirm that both issue generation and iterative evaluation agents contribute significantly to advice quality, particularly in reducing redundancy and increasing impact. The system reliably produces concrete, operational recommendations, though novelty remains a weaker dimension, indicating a tendency toward conservative best practices.

## Method Summary
The framework integrates four components: clustering to select representative reviews, issue extraction, iterative recommendation-evaluation refinement, and ranking based on practicality and feasibility. Reviews are embedded and clustered, with one representative review selected per cluster. The Issue Agent extracts themes, the Recommendation Agent generates candidate advice, and the Evaluation Agent scores and provides feedback on Specificity, Relevance, Actionability, and Concision. This process iterates until a quality threshold is met or maximum iterations are reached. Finally, the Ranking Agent prioritizes advice based on practicality, cost, and expected efficacy.

## Key Results
- Multi-agent pipeline reduces variance across backbone models; single-model baselines show 6–9 point gaps vs. <2 point gaps in multi-agent.
- Medium-sized models approach large model performance under the scaffold, though novelty remains a weaker dimension.
- Iterative evaluation and issue generation agents significantly improve specificity, non-redundancy, and expected impact compared to ablations.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Corpus distillation via clustering reduces redundancy while preserving issue coverage.
- **Mechanism:** Reviews are embedded and clustered; a representative review (closest to centroid via cosine similarity) is selected per cluster. This maps thousands of reviews to m distinct issues, preventing agents from over-focusing on repeated complaints.
- **Core assumption:** Cluster centroids correspond to semantically distinct customer pain points.
- **Evidence anchors:**
  - [abstract] "framework integrates four components: clustering to select representative reviews... couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical"
  - [section 3.1] Representative review is selected as the one closest to the centroid of each cluster using cosine similarity; top m clusters correspond to top m issues.
  - [corpus] Neighbor papers on review-to-action pipelines (e.g., ReviewSense, actionable suggestion mining) similarly emphasize distillation/coverage but do not formalize centroid-based representative selection; limited direct corpus corroboration for this specific clustering strategy.
- **Break condition:** If clusters are dominated by noise or reviews contain multiple issues per document, single-representative selection may miss critical sub-issues or over-compress signal.

### Mechanism 2
- **Claim:** Iterative evaluation–refinement loops improve specificity and actionability without sacrificing concision.
- **Mechanism:** Recommendation Agent generates advice; Evaluation Agent scores on Specificity, Relevance, Actionability, Concision (1–5 scale) and returns free-text feedback. Loop continues until weighted score ≥ η (default 3.5) or max iterations reached. This steers outputs toward concrete, business-relevant language.
- **Core assumption:** LLM evaluators can reliably discriminate and improve advice quality along defined rubrics.
- **Evidence anchors:**
  - [abstract] "iterative evaluation" and "feedback driven advice refinement"
  - [section 3.4] Formalizes the loop: r(t) = A_Rec(f(t−1)), f(t) = A_Eval(r(t), θ); threshold η = 3.5 chosen to require better-than-acceptable quality.
  - [corpus] Related work on multi-agent LLM frameworks (e.g., self-refine, reflexion) shows iterative critique improves reliability, though domain-specific rubrics for business advice are less explored.
- **Break condition:** If evaluator rubrics align poorly with real business constraints (e.g., cost, regulatory limits), optimization may overfit to superficial specificity without practical feasibility.

### Mechanism 3
- **Claim:** Role-specialized agents reduce variance across backbone models and domains compared to single-model baselines.
- **Mechanism:** Division of labor—clustering, issue extraction, recommendation, evaluation, ranking—distributes reasoning load. Ablation shows removing Issue Agent hurts specificity/non-redundancy; removing Evaluation Agent reduces expected impact/novelty. The scaffold regularizes behavior even when individual models differ in raw capability.
- **Core assumption:** Each role can be adequately performed by general-purpose LLMs with appropriate prompting.
- **Evidence anchors:**
  - [abstract] "consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks"
  - [section 6.1] Multi-agent pipeline reduces variance across backbone models; vanilla baselines show 6–9 point gaps vs. <2 point gaps in multi-agent.
  - [corpus] Multi-agent LLM literature (e.g., AgentVerse, MacRec) reports similar stability gains through specialized agents; corpus supports general plausibility but not this exact domain configuration.
- **Break condition:** If any single agent’s failure mode is catastrophic (e.g., Issue Agent hallucinates themes), downstream agents propagate errors; robustness depends on minimum viable performance at each stage.

## Foundational Learning

- **Concept: Iterative Refinement with Verbal Feedback**
  - **Why needed here:** The Evaluation–Recommendation loop depends on structured critique (scores + free-text feedback) to incrementally improve advice. Understanding how to design rubrics, set thresholds (η), and bound iterations (T_max) is essential.
  - **Quick check question:** If your evaluator consistently scores 4/5 on specificity but feedback indicates vague proposals, how would you adjust rubric definitions or the feedback format?

- **Concept: Clustering for Corpus Distillation**
  - **Why needed here:** The Clustering Agent reduces large review corpora to representative samples. You need to grasp embedding choice, clustering algorithm (density-based vs. k-means), and centroid-based selection to maintain coverage without redundancy.
  - **Quick check question:** If clusters contain reviews with multiple distinct issues, would selecting one representative review per cluster risk information loss? How might you mitigate this?

- **Concept: Multi-Agent Role Specialization**
  - **Why needed here:** The pipeline’s robustness stems from role separation. Understanding when to add agents (e.g., dedicated Feasibility Agent) vs. merge tasks helps scale the architecture without unnecessary complexity.
  - **Quick check question:** If the Ranking Agent produces inconsistent priorities across runs, would you adjust the ranking prompt, add a calibration step, or introduce a dedicated cost/benefit analysis agent?

## Architecture Onboarding

- **Component map:** Clustering Agent → Issue Agent → Recommendation Agent → Evaluation Agent → Ranking Agent → Final Advice
- **Critical path:** Clustering → Issue Extraction → Recommendation Generation → Evaluation–Refinement Loop → Ranking → Final Advice. The evaluation–refinement loop is the highest-latency segment; set T_max to bound iterations.
- **Design tradeoffs:**
  - **Cluster granularity vs. coverage:** Fewer clusters improve focus but may miss edge-case issues; more clusters increase coverage but dilute signal.
  - **Evaluation threshold (η) vs. iteration cost:** Higher thresholds yield more specific advice but increase LLM calls and latency.
  - **Model size vs. cost:** Medium-sized models nearly match large models under the scaffold, but may lag in complex domains (e.g., hospitality with regulatory/emotional concerns).
- **Failure signatures:**
  - **High redundancy in final advice:** Likely clustering too fine or Issue Agent merging insufficient; review cluster count and theme consolidation.
  - **Low novelty scores across runs:** Evaluation/Ranking may over-constrain toward safe, consensus advice; consider adjusting rubrics or adding a divergence-promoting step.
  - **High variance across runs:** Check backbone model consistency, seed stability, and whether T_max is frequently reached (indicating poor convergence).
- **First 3 experiments:**
  1. **Cluster count sweep:** Vary m (e.g., 5, 10, 20) and measure impact on specificity, non-redundancy, and coverage of known issues.
  2. **Threshold calibration:** Test η values (3.0, 3.5, 4.0) and record iteration counts, latency, and score distributions per rubric.
  3. **Ablation replay:** Remove Issue Agent and/or Evaluation Agent in isolation; quantify per-dimension degradation (specificity, novelty, feasibility) to confirm paper-reported patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multi-agent framework be modified to generate more novel recommendations while preserving actionability and feasibility?
- Basis in paper: [explicit] Section 6.4 notes novelty is consistently the lowest-scoring dimension (low-mid 70s) and states "Exploring ways to encourage controlled experimentation and innovation is an important direction for future work."
- Why unresolved: The evaluation agent and ranking mechanisms may implicitly favor consensus best practices, damping creative but riskier proposals.
- What evidence would resolve it: Demonstrating modified prompts or scoring rubrics that raise novelty scores by ≥15 points without degrading actionability, specificity, or feasibility below current levels.

### Open Question 2
- Question: How well do LLM-as-judge quality scores correlate with human expert evaluations of business advice utility?
- Basis in paper: [explicit] The Limitations section states the evaluation "risks shared biases with the generators and may overestimate real world utility; human expert studies and online A/B tests would provide stronger validation."
- Why unresolved: All reported quality metrics use LLM judges; no human ground truth was collected.
- What evidence would resolve it: Correlation analysis comparing LLM judge dimension scores against blind ratings from business consultants or managers across at least 100 recommendations.

### Open Question 3
- Question: Does the framework generalize to industries beyond service sectors and to non-English review corpora?
- Basis in paper: [explicit] The Limitations section states "we only consider three English language service verticals, so generalisability to other industries and languages remains open."
- Why unresolved: Experiments covered only automotive, restaurant, and hospitality; all reviews were in English.
- What evidence would resolve it: Replicating the evaluation pipeline on at least two non-service domains (e.g., e-commerce products, software) and two non-English languages, showing composite quality scores within 5 points of English service-domain baselines.

### Open Question 4
- Question: How should the system support continuous deployment with streaming reviews, temporal drift, and post-intervention feedback?
- Basis in paper: [explicit] The Limitations section notes "clustering and issue identification are treated as static preprocessing; a more realistic deployment would support continuously updated clusters, temporal dynamics, and explicit cost benefit reasoning."
- Why unresolved: The pipeline processes fixed corpora; it does not adapt as new reviews arrive or track whether businesses implemented advice.
- What evidence would resolve it: A longitudinal deployment study showing incremental cluster updates, detection of emerging issues over time, and correlation between recommended interventions and subsequent review sentiment shifts.

## Limitations
- The single-representative-per-cluster approach may lose nuanced signals when reviews contain multiple distinct issues.
- Evaluation Agent rubrics are manually defined and may not fully capture real-world business constraints (e.g., regulatory compliance, budget limits).
- Novelty remains a weaker dimension in final outputs, suggesting the system tends toward conservative, consensus-driven advice.

## Confidence
- **High confidence:** The iterative refinement loop improves specificity and actionability, supported by clear ablation evidence and rubric-based scoring.
- **Medium confidence:** Medium-sized models perform nearly as well as large models under the scaffold, though this may not hold in highly complex domains requiring nuanced reasoning.
- **Medium confidence:** Role-specialized agents reduce variance compared to single-model baselines, though the exact contribution of each agent is interdependent and difficult to isolate.

## Next Checks
1. Test clustering granularity by varying m (number of clusters) and measuring impact on advice coverage, specificity, and redundancy across domains.
2. Calibrate the evaluation threshold η using domain expert feedback to ensure it balances advice quality with practical feasibility.
3. Introduce a feasibility/cost analysis agent to address the current system's tendency toward safe, non-novel advice and test its impact on novelty scores.