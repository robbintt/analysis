---
ver: rpa2
title: Simple Convergence Proof of Adam From a Sign-like Descent Perspective
arxiv_id: '2507.05966'
source_url: https://arxiv.org/abs/2507.05966
tags:
- adam
- convergence
- rate
- holds
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the theoretical convergence analysis of Adam,
  a widely-used optimizer for deep learning. Existing convergence proofs for Adam
  are path-dependent, treating it as a preconditioned SGD with momentum, leading to
  complex, lengthy proofs that are difficult to verify and extend.
---

# Simple Convergence Proof of Adam From a Sign-like Descent Perspective

## Quick Facts
- **arXiv ID:** 2507.05966
- **Source URL:** https://arxiv.org/abs/2507.05966
- **Reference count:** 40
- **Key outcome:** Proves Adam achieves optimal O(1/T^{1/4}) convergence rate, dimension-free and ε-independent, under specific variance and smoothness assumptions.

## Executive Summary
This paper provides a novel convergence proof for Adam by reinterpreting it as a sign-like optimizer rather than a preconditioned SGD with momentum. The key insight is reformulating Adam's update as $x_{t+1} = x_t - \gamma_t |m_t|/\sqrt{v_t} \circ \text{Sign}(m_t)$, where the magnitude and direction are decoupled. Under weak assumptions including generalized p-affine variance and (L_0, L_1, q)-smoothness, the authors prove Adam achieves the optimal convergence rate of O(1/T^{1/4}), which is dimension-free and independent of the numerical stability parameter ε. The analysis also reveals that momentum is theoretically necessary for Adam to converge to a stationary point rather than a noisy neighborhood.

## Method Summary
The paper proposes a novel interpretation of Adam by treating it as a sign-like optimizer. The update rule is reformulated as $x_{t+1} = x_t - \gamma_t u_t \circ \text{Sign}(m_t)$, where $u_t = |m_t|/(\sqrt{v_t}+\epsilon)$ is treated as a single random variable. This transformation simplifies the convergence analysis by avoiding the path-dependent coupling issues between $v_t$ and $g_t$ present in standard proofs. The proof introduces three key conditions: generalized p-affine variance, (L_0, L_1, q)-smoothness, and specific statistical assumptions about gradient coordinates. Under these conditions and with appropriate hyperparameter scheduling ($\beta_1 < \sqrt{\beta_2}$), the paper establishes the optimal O(1/T^{1/4}) convergence rate.

## Key Results
- Proves Adam achieves O(1/T^{1/4}) convergence rate, optimal for this problem class
- Convergence bound is dimension-free and independent of numerical stability parameter ε
- Theoretical analysis reveals momentum is necessary for convergence to stationary points (not just neighborhoods)
- Provides practical guidance: learning rate should scale as 1/√d for optimal convergence

## Why This Works (Mechanism)

### Mechanism 1
Treating Adam as a sign-based optimizer ($x_{t+1} = x_t - \gamma_t u_t \circ \text{Sign}(m_t)$) rather than a preconditioned gradient method simplifies convergence analysis by decoupling the step magnitude from the step direction. The authors reformulate the update rule to separate the magnitude term $u_t = |m_t|/(\sqrt{v_t}+\epsilon)$ from the direction $\text{Sign}(m_t)$. By bounding $u_t$ (Lemma B.2) and treating it as a unified random variable, the proof avoids the "path-dependent" coupling issues between $v_t$ and $g_t$ that complicate standard proofs. The core assumption is that the ratio $u_t = |m_t|/(\sqrt{v_t}+\epsilon)$ is bounded by a constant $R$ dependent on $\beta_1, \beta_2$ (specifically $\beta_1^2 < \beta_2$).

### Mechanism 2
Momentum ($\beta_1$) is theoretically necessary to transition the algorithm from converging to a noisy neighborhood (standard signSGD behavior) to converging to a stationary point. The proof demonstrates that without momentum ($\beta_1=0$), the update error term dominates, forcing convergence to a ball of radius $O(\sigma_0)$. With momentum satisfying $\beta_1 < \sqrt{\beta_2}$, the noise averaging allows the gradient norm to shrink at the optimal $O(1/T^{1/4})$ rate. The core assumption is that coefficients must satisfy $\beta_1 < \sqrt{\beta_2}$ and decay schedules such that $1-\beta_1 = O(1/T^{1/2})$ and $1-\beta_2 = O(1/T^{3/4})$.

### Mechanism 3
Optimal convergence rates can be dimension-free ($d$-independent) and $\epsilon$-independent if coordinate-wise statistical assumptions hold. By introducing Condition 2 (i.i.d. coordinates) and Condition 3 (stable $\ell_1/\ell_2$ norm ratio), the summation over dimensions simplifies in the descent inequality. This removes the $O(\sqrt{d})$ or $O(d)$ factors present in prior Adam bounds. The core assumption is that gradient coordinates are i.i.d. (Condition 2) and the ratio $\|\nabla F\|_1 / \|\nabla F\|_2$ is bounded by $C_1$ (Condition 3).

## Foundational Learning

- **Concept:** $(L_0, L_1, q)$-Smoothness
  - **Why needed here:** Standard $L$-smoothness (Lipschitz gradients) is too restrictive for deep networks (e.g., Transformers, LSTMs). This generalized smoothness allows the local Lipschitz constant to grow with the gradient norm, which is essential for proving convergence in realistic DL settings.
  - **Quick check question:** Does the loss landscape curvature increase as the gradient norm increases?

- **Concept:** Sign Descent vs. Preconditioning
  - **Why needed here:** This paper's core thesis relies on reinterpreting Adam not as adapting the learning rate per parameter (preconditioning), but as taking a step in the pure sign direction with a variable magnitude.
  - **Quick check question:** Is the optimizer determining *direction* (sign) or *step size* (magnitude) primarily?

- **Concept:** Generalized $p$-Affine Variance
  - **Why needed here:** Standard "bounded variance" assumptions are often unrealistic. This assumption allows noise to scale with the gradient (up to power $p$), modeling the noise regime observed in large model training more accurately.
  - **Quick check question:** Does the gradient noise remain constant or scale with the true gradient magnitude?

## Architecture Onboarding

- **Component map:** $g_t$ (gradient) -> $m_t$ (momentum) -> $v_t$ (second moment) -> $u_t = |m_t|/(\sqrt{v_t}+\epsilon)$ (magnitude) -> $\text{Sign}(m_t)$ (direction) -> $x_{t+1} = x_t - \gamma_t u_t \circ \text{Sign}(m_t)$ (update)

- **Critical path:** The derivation of the bound $R$ for the magnitude term $u_t$. The stability of the proof hinges on the inequality $\beta_1^2 < \beta_2$ to ensure $u_t$ remains bounded by $R = \frac{1-\beta_1}{\sqrt{1-\beta_2}\sqrt{1-\beta_1^2/\beta_2}}$.

- **Design tradeoffs:**
  - **Hyperparameter constraints:** The theoretical guarantees require $\beta_2$ close to 1 (specifically $1-\beta_2 = O(1/T^{3/4})$), which makes $v_t$ a long-term average, aligning Adam slightly closer to AdaGrad behavior than the standard short-term adaptive behavior.
  - **Learning Rate Scaling:** Case 2 in Corollary 3.3 implies $\gamma \propto 1/\sqrt{d}$. This suggests larger models require smaller learning rates to maintain the optimal bound, consistent with empirical practice in LLMs but contrasting with some scale-invariant theories.

- **Failure signatures:**
  - **Divergence:** If $\beta_1$ is set too high relative to $\beta_2$ (specifically $\beta_1 \ge \sqrt{\beta_2}$), the bound on $u_t$ loosens, potentially breaking the theoretical convergence guarantees.
  - **Stalling:** If momentum $\beta_1$ is insufficient for the noise level, the optimizer may converge only to a noise-dominated neighborhood.

- **First 3 experiments:**
  1. **Validation of Conditions:** Run KS-tests on your specific architecture (e.g., Transformer vs. CNN) to verify if Condition 2 (i.i.d. $|m|/\sqrt{v}$) holds during training.
  2. **Hyperparameter Sweep:** Validate the theoretical $\gamma \propto 1/\sqrt{d}$ scaling. Train models of varying width/depth and plot the optimal learning rate against parameter count to see if it follows the $1/\sqrt{d}$ trend or a different power law.
  3. **Ablation on $\beta$ Ratio:** Test the stability boundary by fixing $\beta_2=0.999$ and varying $\beta_1$ towards and past $\sqrt{\beta_2} \approx 0.9995$. Monitor for the predicted instability or performance degradation.

## Open Questions the Paper Calls Out

### Open Question 1
Can the optimal convergence rate be proven under generalized $p$-affine variance without relying on Conditions 1-3? The "Limitations" section in the Appendix states that extending the proof to weaker assumptions (Assumption C.3) necessitated the introduction of additional constraints (Conditions 1–3), acknowledging this as a limitation of the current work.

### Open Question 2
Is there a theoretical guarantee that the coordinates of the update magnitude $|m_t|/\sqrt{v_t}$ are independently and identically distributed? Page 5 states that Condition 2 "commonly holds in practice" and validates it using empirical Kolmogorov-Smirnov tests on ResNet-50 and GPT-2, but provides no formal theoretical derivation.

### Open Question 3
Can the sign-like descent perspective be extended to derive high-probability convergence bounds for Adam? The paper focuses on convergence in expectation ($\mathbb{E}[\|\nabla F(x_t)\|]$), whereas recent works cited (e.g., [2], [45]) have explored high-probability bounds for adaptive methods.

## Limitations
- Analysis critically depends on technical assumption $\beta_1^2 < \beta_2$, constraining hyperparameter space
- Conditions 2 and 3 (i.i.d. coordinates and bounded $\ell_1/\ell_2$ ratio) are unverified for most architectures
- Proof assumes gradient noise scales with gradient magnitude via generalized p-affine variance

## Confidence

- **High Confidence:** The convergence rate O(1/T^{1/4}) is correctly derived under the stated assumptions, and the reinterpretation of Adam as a sign-like optimizer is mathematically sound.
- **Medium Confidence:** The practical relevance of the theoretical bounds, particularly the $\gamma \propto 1/\sqrt{d}$ scaling rule, requires empirical validation across diverse architectures and data regimes.
- **Low Confidence:** The long-term stability of Adam under the exact hyperparameter decay schedules required by the proof (specifically $1-\beta_1 = O(1/T^{1/2})$ and $1-\beta_2 = O(1/T^{3/4})$) has not been demonstrated and may differ from standard practice.

## Next Checks

1. **Empirical Validation of Conditions:** Run KS-tests on Adam state variables across multiple architectures (CNNs, Transformers, LSTMs) to verify the i.i.d. assumption for $|m_t|/\sqrt{v_t}$ coordinates during training.

2. **Hyperparameter Scaling Experiment:** Train models of varying width/depth and systematically measure optimal learning rate to validate if the theoretical $\gamma \propto 1/\sqrt{d}$ scaling holds empirically.

3. **Boundary Stability Test:** Fix $\beta_2=0.999$ and vary $\beta_1$ from 0.9 to 0.9995 (approaching $\sqrt{\beta_2}$), monitoring both convergence behavior and the bound $R$ on $u_t$ to test the theoretical stability boundary.