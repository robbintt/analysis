---
ver: rpa2
title: Learning Half-Spaces from Perturbed Contrastive Examples
arxiv_id: '2602.02080'
source_url: https://arxiv.org/abs/2602.02080
tags:
- contrastive
- active
- learning
- example
- learner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies learning with perturbed contrastive examples,
  extending a prior model where an oracle provides a labeled example along with a
  "perfect" contrastive example of opposite label at minimum distance. The authors
  introduce a noise function f to control the amount of perturbation of this ideal
  contrastive example, with higher perturbation for points farther from the decision
  boundary.
---

# Learning Half-Spaces from Perturbed Contrastive Examples

## Quick Facts
- **arXiv ID**: 2602.02080
- **Source URL**: https://arxiv.org/abs/2602.02080
- **Reference count**: 40
- **Primary result**: Extends prior contrastive learning model by introducing a noise function f to control perturbation of contrastive examples, analyzing both deterministic and probabilistic settings for 1D thresholds and linear half-spaces.

## Executive Summary
This paper studies learning half-spaces from perturbed contrastive examples, extending a prior model where an oracle provides a labeled example along with a "perfect" contrastive example of opposite label at minimum distance. The authors introduce a noise function f to control the amount of perturbation of this ideal contrastive example, with higher perturbation for points farther from the decision boundary. They analyze two settings: deterministic and probabilistic perturbations.

For one-dimensional threshold functions and linear half-spaces under uniform distribution, the authors characterize both active and passive sample complexity in terms of f. Key results show that under certain conditions on f, contrastive examples can significantly speed up learning compared to standard membership queries. For example, learning homogeneous half-spaces becomes trivial (requiring only one query) under the deterministic model with appropriate f.

The paper provides a rich landscape of sample complexity bounds, showing improvements over non-contrastive learning while avoiding the trivialization present in the idealized model. Notably, the authors observe a qualitative difference between expected error after m samples and the number of samples needed to guarantee a certain error level, indicating that error distributions under probabilistic perturbations can be highly skewed.

## Method Summary
The paper introduces two new learning models: Deterministic AMDM (Approximate Minimum Distance Mechanism) and Probabilistic AMDM, both parameterized by a noise function f that controls perturbation magnitude based on distance from the decision boundary. The deterministic model returns a contrastive example within f(d) distance of the minimum-distance point, while the probabilistic model has expected distance bounded by f(d). For 1D thresholds, the paper analyzes active learning via binary search variants and passive learning via error analysis of constructed hypotheses. For k-dimensional half-spaces, similar analyses are provided for both homogeneous (origin-passing) and general cases, with active learners iteratively querying contrastive examples.

## Key Results
- Under deterministic AMDM with appropriate f, learning homogeneous half-spaces becomes trivial (1 query) while general half-spaces require O(1/f(1/2)) active queries
- For 1D thresholds, active learning sample complexity is characterized by τ_f(1→2ε), showing significant improvement over membership queries when f is sufficiently sublinear
- The probabilistic AMDM exhibits highly skewed error distributions, with a qualitative difference between expected error and the number of samples needed for guaranteed error bounds
- For general half-spaces under passive learning, the error bound scales as O(2^k) with dimension k, making it impractical for high-dimensional applications

## Why This Works (Mechanism)

### Mechanism 1: Distance-Governed Perturbation
- **Claim**: Contrastive examples that are closer to the decision boundary provide higher-quality learning signal and greater sample complexity reductions.
- **Mechanism**: A noise function $f$ controls perturbation based on distance $d$ from the primary point to the decision boundary. The oracle returns a contrastive example within $f(d)$ distance of the "ideal" minimum-distance point. Points closer to the boundary yield less perturbed (higher quality) contrastive examples.
- **Core assumption**: The noise function $f$ is non-decreasing; perturbation magnitude grows with distance from the boundary.
- **Break condition**: If $f(d)$ grows too aggressively (e.g., $f(d) \approx d$ for large $d$), contrastive examples can be arbitrarily far from the boundary, eliminating the learning benefit.

### Mechanism 2: Iterative Version Space Contraction
- **Claim**: Active learners can use contrastive feedback to shrink the version space (the set of hypotheses consistent with observed data) much faster than binary labels alone.
- **Mechanism**: For thresholds, querying the midpoint of the current version space and receiving a contrastive example allows bounding the target threshold within a smaller sub-interval. The paper defines $\tilde{f}$ to characterize this contraction per query. The number of queries needed is the number of times $\tilde{f}$ must be iterated to shrink the initial uncertainty to a target $\epsilon$.
- **Core assumption**: Realizability (the target concept is in the hypothesis class) and an active learning setting where the learner can choose queries adaptively.
- **Break condition**: In the probabilistic setting, a skewed error distribution can lead to rare but very large errors, meaning the "typical" number of samples for a guaranteed error level can be much lower than the worst-case bound.

### Mechanism 3: Geometric Angle-Based Hypothesis Construction
- **Claim**: A primary point and its contrastive example provide geometric constraints that allow constructing a low-error hypothesis without needing to know the exact decision boundary.
- **Mechanism**: Given a primary point $x$ and a contrastive point $x'$ of opposite label, a hypothesis can be formed by placing the decision boundary perpendicular to the line segment $x - x'$ near $x'$. Lemma 3.2 bounds the error of this constructed hypothesis based on the angle $\angle(x', x, x_{proj})$, where $x_{proj}$ is the true projection of $x$ onto the boundary. The closer $x'$ is to $x_{proj}$, the smaller the angle and the lower the error.
- **Core assumption**: The learner knows the rule by which contrastive examples are chosen (the $CE$ mechanism).
- **Break condition**: For non-homogeneous half-spaces, the error bound from this construction scales as $O(2^k)$ with dimension $k$ (Lemma 3.2, part ii), making it impractical for high-dimensional passive learning.

## Foundational Learning

- **Concept: Version Space**
  - Why needed here: The paper's analysis of active learning is fundamentally about how quickly the version space can be narrowed down. Understanding this is key to interpreting the sample complexity results ($M_{Pr}, M_{exp}$).
  - Quick check question: If a learner has seen three positive examples at $x=0.1, 0.3, 0.5$ and one negative example at $x=0.9$, what is the interval representing the possible locations for a target threshold $\theta$?

- **Concept: Sample Complexity**
  - Why needed here: This is the primary metric used to compare the efficiency of different learning models. The paper provides bounds for both high-probability ($M_{Pr}$) and expected error ($M_{exp}$) settings.
  - Quick check question: What is the difference between $M_{Pr, active}(\epsilon, \delta)$ and $M_{exp, active}(\epsilon)$?

- **Concept: Active vs. Passive Learning**
  - Why needed here: The paper analyzes both settings. In active learning, the learner chooses queries to maximize information gain, while in passive learning, examples are sampled i.i.d. The results show contrastive examples help in both, but the mechanisms differ.
  - Quick check question: Why does the paper state that active learning of homogeneous half-spaces becomes trivial (1 query) under the deterministic AMDM, while passive learning still requires multiple samples?

## Architecture Onboarding

- **Component map**: Oracle (primary example + contrastive example) -> Noise function f -> Learner (active or passive) -> Hypothesis
- **Critical path**:
  1. Define/choose a noise function $f$
  2. Select a learning setting: active or passive
  3. Select a contrastive mechanism: deterministic ($CE_{d,f}^{app}$) or probabilistic ($CE_{d,f}^{prop}$)
  4. Apply the appropriate algorithm (e.g., midpoint query for thresholds, iterative querying for half-spaces) based on the theoretical results in Sections 3 and 4
  5. The resulting sample complexity is bounded by the theorems associated with that setting, class, and $f$

- **Design tradeoffs**:
  - **Deterministic vs. Probabilistic AMDM**: Deterministic is simpler to analyze and provides stronger guarantees but may be less realistic. Probabilistic is more flexible but reveals a skew in error distribution, creating a gap between expected error and the expected number of samples for a guaranteed error.
  - **Choice of $f$**: A more aggressive $f$ (allowing larger perturbations) makes the model more realistic but increases sample complexity. The paper shows that for $f(r) \le r^{1+c}$, significant speedups over membership queries remain.
  - **Active vs. Passive**: Active learning yields much greater speedups, especially for homogeneous half-spaces, but requires the ability to query specific points.

- **Failure signatures**:
  - **Trivialization**: If $f(d)$ is too permissive, the contrastive signal becomes uninformative, and the sample complexity approaches that of standard learning.
  - **High-Dimensionality Curse**: For general (non-homogeneous) half-spaces, the passive learning error bounds scale exponentially with dimension ($O(2^k)$) when using the simple angle-based hypothesis construction. This approach is not practical for high-dimensional passive learning.
  - **Skewed Error Distribution (Probabilistic AMDM)**: A learner may observe low average error over many runs but still have a non-negligible probability of very high error in a given run. A system expecting consistent performance may fail unexpectedly.

- **First 3 experiments**:
  1. **Verify Threshold Learning Bounds**: Implement the deterministic AMDM for 1D thresholds with a chosen $f$ (e.g., $f(x) = x^2$). Measure the number of active queries needed to reach error $\epsilon$ and compare against the theoretical bound $\tau_{\tilde{f}}(1 \to 2\epsilon)$. This validates the core theoretical result.
  2. **Compare Deterministic vs. Probabilistic AMDM**: Implement both mechanisms for a simple half-space learning task. Run the active learning algorithm multiple times and plot the distribution of the final error. This should reveal the "highly skewed" error distribution discussed in the paper, contrasting the average error with the worst-case error.
  3. **Evaluate Impact of Noise Function $f$**: For a fixed concept class (e.g., homogeneous half-spaces) and learning setting (passive), test a family of noise functions (e.g., $f(r) = r^{1+c}$ for different $c$). Measure the sample complexity and observe how it increases as $c$ decreases (allowing larger relative perturbations), empirically demonstrating the tradeoff between perturbation tolerance and learning speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the passive learning bounds for general (non-homogeneous) half-spaces be improved from exponential to polynomial dependence on dimension $k$?
- Basis in paper: [explicit] The authors note the $2^k$ factor in Lemma 3.2(ii) is "undesirable, since standard passive learning bounds for half-spaces typically depend only linearly on k."
- Why unresolved: The geometric argument using Lemma 3.2(ii) inherently introduces the dimension-dependent factor when the decision boundary does not pass through the origin.
- What evidence would resolve it: Either an improved analysis yielding polynomial dependence on $k$, or a lower bound construction showing that exponential dimension dependence is unavoidable under AMDM.

### Open Question 2
- Question: How do sample complexity bounds change under distributions beyond uniform on a bounded domain, such as log-concave or Gaussian distributions?
- Basis in paper: [inferred] The paper restricts analysis to "half-spaces under the uniform distribution on a bounded domain," leaving other distributional assumptions unexplored despite their prominence in half-space learning literature.
- Why unresolved: The geometric arguments (e.g., Lemma 3.2) rely on volume calculations specific to the unit ball under uniform distribution.
- What evidence would resolve it: Sample complexity bounds for AMDM under isotropic log-concave or Gaussian distributions, showing whether the qualitative speedups persist.

### Open Question 3
- Question: Can the framework be extended to non-linear concept classes (e.g., polynomial separators or neural networks) while maintaining non-trivial sample complexity improvements?
- Basis in paper: [explicit] The paper "focuses our analysis exclusively on classes of (i) one-sided threshold functions in R, and (ii) linear half-spaces," explicitly limiting scope.
- Why unresolved: The proofs heavily exploit linear geometry and the simple structure of half-space decision boundaries.
- What evidence would resolve it: Sample complexity bounds for non-linear classes under AMDM, or negative results showing that contrastive examples yield no improvement beyond linear separators.

### Open Question 4
- Question: What is the precise characterization of noise functions $f$ for which the gap between expected sample requirement $N_{\text{active}}$ and expected error $M_{\text{exp,active}}$ becomes asymptotically large?
- Basis in paper: [explicit] The authors "observe a qualitative difference between expected error after m samples and the number of samples needed to guarantee a certain error level, indicating that error distributions under probabilistic perturbations can be highly skewed."
- Why unresolved: Tables 1(a) and 1(b) show gaps exist empirically for polynomial and exponential $f$, but no formal characterization of when/why this occurs is provided.
- What evidence would resolve it: A theorem characterizing families of $f$ where the gap is $\omega(1)$ versus $O(1)$, potentially connecting to tail properties of the contrastive example distribution.

## Limitations
- The paper's theoretical framework relies on idealized oracles that may not be directly implementable in practice
- The exponential scaling of error bounds for non-homogeneous half-spaces in passive learning (O(2^k)) represents a fundamental limitation for high-dimensional applications
- The highly skewed error distributions in the probabilistic setting, while theoretically interesting, may be difficult to characterize or predict in practical deployments

## Confidence
**High Confidence**: The core theoretical results for 1D thresholds under deterministic AMDM (Theorem 3.1) and the geometric error bounds for constructed hypotheses (Lemma 3.2) are well-supported with clear proofs and align with established active learning principles.

**Medium Confidence**: The sample complexity bounds for homogeneous half-spaces (Theorem 3.4) rely on iterative querying strategies that may be sensitive to the choice of noise function. The analysis assumes specific contraction properties that may not hold for all f.

**Low Confidence**: The practical implications of the highly skewed error distributions in probabilistic AMDM are not fully explored. While the paper notes this phenomenon, it doesn't provide guidance on how to mitigate or account for these rare but large errors in real applications.

## Next Checks
1. **Empirical Validation of Skewed Error Distributions**: Implement the probabilistic AMDM for a simple threshold learning task and run the active learning algorithm 1000 times. Plot the distribution of final errors to empirically verify the claim of high skewness and compare the average error to the worst-case error.

2. **Noise Function Robustness Testing**: For homogeneous half-spaces under passive learning, test a range of noise functions (e.g., f(r) = r^(1+c) for c ∈ {0.1, 0.5, 1.0}) and measure the actual sample complexity needed to achieve epsilon error. Compare this to the theoretical bounds to assess their tightness.

3. **High-Dimensional Passive Learning Evaluation**: Implement the angle-based hypothesis construction for non-homogeneous half-spaces in k=10 dimensions. Measure the actual error of the constructed hypothesis for various primary-contrastive point pairs and verify whether the O(2^k) scaling observed in Lemma 3.2 manifests empirically.