---
ver: rpa2
title: 'Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question
  Answering'
arxiv_id: '2510.11928'
source_url: https://arxiv.org/abs/2510.11928
tags:
- question
- passage
- topic
- passages
- discrepancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIND is a user-in-the-loop pipeline for detecting factual and cultural
  discrepancies in multilingual QA knowledge bases. It uses polylingual topic modeling
  to align documents thematically, generates questions in an anchor language, retrieves
  relevant passages in a comparison language, and uses LLMs to classify discrepancies.
---

# Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering

## Quick Facts
- arXiv ID: 2510.11928
- Source URL: https://arxiv.org/abs/2510.11928
- Authors: Lorena Calvo-Bartolomé; Valérie Aldana; Karla Cantarero; Alonso Madroñal de Mesa; Jerónimo Arenas-García; Jordan Boyd-Graber
- Reference count: 40
- Primary result: MIND pipeline detects factual contradictions and cultural discrepancies in multilingual QA with high F1 scores (0.91-0.92 weighted) on controlled data

## Executive Summary
MIND is a user-in-the-loop pipeline for detecting factual and cultural discrepancies in multilingual QA knowledge bases. It uses polylingual topic modeling to align documents thematically, generates questions in an anchor language, retrieves relevant passages in a comparison language, and uses LLMs to classify discrepancies. MIND was evaluated on bilingual QA data in maternal and infant health (Rosie) and on synthetic and Wikipedia datasets. It achieved high F1 scores on controlled data (0.91-0.92 weighted) and reliably surfaced real-world inconsistencies. Topic-based retrieval improved over baselines (MRR@3: 0.72 vs 0.64 for ANN), though human review remains necessary for fine-grained validation.

## Method Summary
MIND detects discrepancies by aligning multilingual documents through polylingual topic modeling (PLTM), generating questions from anchor passages, retrieving comparison passages using topic-weighted embeddings, and classifying answer pairs with LLMs. The pipeline uses MALLET for PLTM training with K=30 topics, BAAI/bge-m3 embeddings for retrieval, and llama3.3:70b LLM for question generation and discrepancy classification. Topic-based exact nearest neighbor retrieval with weighting (TB-ENN-W) outperforms unweighted and approximate methods. The system identifies four discrepancy types: contradiction, cultural discrepancy, no discrepancy, and not enough information.

## Key Results
- Topic-based retrieval (TB-ENN-W) achieves MRR@3 of 0.72 vs 0.64 for baseline ANN
- MIND achieves F1 scores of 0.91-0.92 weighted on synthetic FEVER-DPLACE-Q dataset
- System reliably detects real-world discrepancies in ROSIE maternal health dataset, though high NEI rates (35K+) indicate coverage gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topic-based document alignment across languages improves retrieval precision for discrepancy detection.
- Mechanism: Polylingual Topic Modeling (PLTM) learns shared topic distributions across anchor and comparison corpora, enabling semantically coherent cross-lingual passage retrieval within topic clusters rather than across the entire corpus.
- Core assumption: Documents in different languages sharing similar topic distributions are thematically aligned, even if not direct translations.
- Evidence anchors: [abstract] MIND highlights divergent answers to culturally sensitive questions through topic-aligned multilingual analysis; [section 3.1-3.2] PLTM provides per-language word-topic distributions and passage-level topic distributions; topic-based clustering filters passages by θ(c)_p,k > 0.
- Break condition: When documents in anchor/comparison corpora have fundamentally different topical coverage (e.g., domain mismatch), alignment quality degrades.

### Mechanism 2
- Claim: Question-anchored answer comparison isolates evidence-based inconsistencies from translation artifacts.
- Mechanism: Questions are generated in the anchor language only, and both anchor and comparison answers are generated from their respective passages using the same question. This ensures discrepancies arise from content differences, not translation drift.
- Core assumption: LLMs can generate faithful, passage-grounded Yes/No answers and reliably classify relationships (contradiction, cultural discrepancy, no discrepancy, not enough info).
- Evidence anchors: [abstract] MIND classifies answer discrepancies as contradictions or cultural differences; [section 3.6] Discrepancy detection prompts LLM to determine entailment/contradiction/cultural discrepancy/NEI given question and answer pair.
- Break condition: When generated questions are poorly decontextualized or passages lack sufficient information, false positives increase (Section 5 notes anecdotal content as error source).

### Mechanism 3
- Claim: Weighted topic-based exact nearest neighbor (TB-ENN-W) retrieval outperforms unweighted and approximate methods.
- Mechanism: Retrieval scores combine cosine similarity with topic weights (α = θ(a)_p,k), prioritizing passages under topics strongly associated with the query's source passage. ENN within clustered topic subsets maintains ranking quality.
- Core assumption: Topic relevance and embedding similarity are complementary signals for identifying semantically related comparison passages.
- Evidence anchors: [section 4.4.2, Table 2] TB-ENN-W achieves highest MRR@5 (0.623), NDCG@5 (0.418), and Recall@5 (0.306) for gpt-4o; statistical significance via Wilcoxon with Holm correction.
- Break condition: When topic distributions are diffuse or clustering parameters (λ, ℓ_min) are misconfigured, topic-based methods may not improve over baseline ENN.

## Foundational Learning

### Concept: Polylingual Topic Modeling (PLTM)
- Why needed here: Creates shared thematic space for cross-lingual document alignment without requiring parallel corpora.
- Quick check question: Can you explain why PLTM assumes "loose equivalence" rather than direct translation alignment?

### Concept: Natural Language Inference (NLI) for discrepancy classification
- Why needed here: Provides the theoretical framing for entailment, contradiction, and neutral relationships that MIND extends to cultural discrepancies.
- Quick check question: How does MIND's "cultural discrepancy" category differ from standard NLI contradiction?

### Concept: Retrieval evaluation metrics (MRR, NDCG, Recall@k)
- Why needed here: Required to interpret ablation study results and select optimal retrieval configuration.
- Quick check question: Why might MRR@3 be preferred over Recall@5 for this use case?

## Architecture Onboarding

### Component map
(1) PLTM training → (2) Topic-based clustering → (3) Question generation (LLM) → (4) Query decomposition (LLM) → (5) Topic-based retrieval (ANN/ENN + FAISS) → (6) Answer generation (LLM) → (7) Discrepancy classification (LLM) → (8) User-in-the-loop validation

### Critical path
PLTM quality determines clustering quality → clustering determines retrieval scope → retrieval quality determines comparison answer quality → answer quality determines discrepancy detection validity. User review gates final output.

### Design tradeoffs
- LLM choice: qwen:32b detects more discrepancy types but lower question/answer quality; gpt-4o highest quality but fails to detect cultural discrepancies in ROSIE; llama3.3:70b selected as balance
- Retrieval method: TB-ENN-W highest accuracy but slower (0.31s vs 0.02s for ANN); ANN fastest but lower MRR
- Topic count K: K=30 selected via NPMI coherence; higher K increases granularity but may fragment topics

### Failure signatures
- Decontextualization failures: Questions generated from anecdotal content (e.g., "Will you receive test results within 2 months?") yield no reliable counterparts
- False contradictions: Regulatory variations (e.g., California cardiac screening exception) classified as contradictions rather than cultural/contextual differences
- NEI overload: High NEI rates (35K+ in ROSIE) indicate anchor passages lacking comparison-language counterparts

### First 3 experiments
1. Validate PLTM alignment on a small held-out set: manually verify topic coherence and cross-lingual cluster quality before full pipeline run.
2. Compare LLMs on FEVER-DPLACE-Q subset: measure F1 per discrepancy category to determine best fit for your domain.
3. Ablate retrieval methods: run TB-ENN-W vs. baseline ENN on 50-100 anchor passages and compare MRR@5 and manual relevance judgments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can active learning be effectively integrated into MIND to reduce human annotation effort while maintaining discrepancy detection accuracy?
- Basis in paper: [explicit] "To further reduce human effort, we plan to incorporate active learning, prompting review only when model uncertainty is high."
- Why unresolved: The paper proposes active learning as future work but does not specify query strategies, uncertainty metrics, or evaluation protocols for this integration.
- What evidence would resolve it: A systematic comparison of active learning query strategies on ROSIE-MIND, measuring annotation reduction vs. detection performance (F1, agreement) under a fixed labeling budget.

### Open Question 2
- Question: Can fine-tuning MIND on domain-specific data like ROSIE-MIND improve its cross-domain generalization to other languages and knowledge bases?
- Basis in paper: [explicit] "To support this, we will fine-tune MIND using ROSIE-MIND."
- Why unresolved: The paper mentions fine-tuning as a next step but does not report results or analyze how domain adaptation affects performance on new datasets like WIKI-EN-DE.
- What evidence would resolve it: Experiments fine-tuning the LLM or retrieval components on ROSIE-MIND and evaluating on held-out domains (e.g., FEVER-DPLACE-Q, WIKI-EN-DE) to quantify gains in retrieval (MRR) and classification (F1) over the base model.

### Open Question 3
- Question: How should the taxonomy of discrepancy categories (e.g., contradiction, cultural discrepancy, contextual difference) be refined to improve inter-annotator and model agreement?
- Basis in paper: [inferred] Annotators frequently misclassify cultural discrepancies as contradictions or no discrepancies, and the authors note that "fine-grained category boundaries remain unstable across models and annotators," suggesting that clearer definitions or additional categories may help.
- Why unresolved: The paper uses a four-class scheme but observes systematic confusion, particularly between cultural discrepancy and contradiction, without proposing or testing refined schemes.
- What evidence would resolve it: An annotation study comparing the current taxonomy with expanded/redefined categories (e.g., adding "contextual/policy variation"), reporting inter-annotator agreement (Fleiss' κ) and model classification accuracy for each scheme.

### Open Question 4
- Question: Does MIND's performance degrade when applied to low-resource language pairs without high-quality parallel corpora or robust multilingual embeddings?
- Basis in paper: [inferred] MIND relies on machine translation for topic alignment when comparable corpora are unavailable and on multilingual embeddings for retrieval; the authors note that "translation quality is not critical," but evaluation is limited to English–Spanish and English–German, leaving low-resource settings unexplored.
- Why unresolved: The paper does not test on language pairs with limited translation quality or embedding coverage, nor does it analyze the impact of noisy alignment on PLTM and downstream retrieval.
- What evidence would resolve it: Experiments applying MIND to low-resource language pairs (e.g., English–Yoruba) with varying translation quality, measuring retrieval (Recall@5) and discrepancy detection performance under different alignment and embedding conditions.

## Limitations
- MIND relies heavily on LLM judgment quality, introducing variability not fully addressed by ablation studies
- Method assumes anchor and comparison corpora have overlapping topical coverage - domain mismatches will degrade alignment
- High "Not Enough Info" rates (35K+ in ROSIE) indicate significant gaps in coverage that the current method cannot resolve

## Confidence

- **High Confidence:** Topic-based retrieval improves MRR over baseline ANN (0.72 vs 0.64) - directly validated in Table 2 with statistical testing
- **Medium Confidence:** LLM-based discrepancy classification achieves high F1 on synthetic data (0.91-0.92 weighted) - but synthetic data may not capture real-world complexity
- **Low Confidence:** User-in-the-loop validation actually reduces manual workload - claimed but not empirically measured in the study

## Next Checks

1. **Error Analysis:** Manually review 50 false positive cultural discrepancies to determine if LLM is misclassifying regulatory variations as cultural differences
2. **Robustness Test:** Run MIND on a domain-mismatched corpus pair (e.g., medical + legal) to measure degradation in PLTM alignment quality
3. **LLM Dependency:** Repeat discrepancy classification using a smaller, open-source LLM (e.g., qwen:32b) on the same 100-passage test set to measure performance drop and prompt sensitivity