---
ver: rpa2
title: AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating
  to Devito
arxiv_id: '2601.18381'
source_url: https://arxiv.org/abs/2601.18381
tags:
- code
- devito
- retrieval
- knowledge
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops an AI agent system to automate the translation
  of legacy Fortran finite difference code into the modern Devito framework. The system
  integrates Retrieval-Augmented Generation (RAG) with open-source large language
  models within a multi-stage iterative workflow built on the LangGraph architecture.
---

# AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito

## Quick Facts
- arXiv ID: 2601.18381
- Source URL: https://arxiv.org/abs/2601.18381
- Reference count: 34
- Primary result: AI agent achieves 76.9% Grade-A success rate translating Fortran to Devito

## Executive Summary
This study introduces an AI agent system designed to automate the translation of legacy Fortran finite-difference code into the modern Devito framework. The system combines Retrieval-Augmented Generation with open-source large language models within a multi-stage iterative workflow built on LangGraph architecture. A Devito knowledge graph constructed from documentation and source code is leveraged by GraphRAG for efficient retrieval. The agent employs static analysis of Fortran code to generate targeted queries, retrieves relevant Devito examples, and synthesizes new code under Pydantic-structured output constraints. Quality validation combines conventional static analysis with LLM-based evaluation (G-Eval), covering execution correctness, structural integrity, API compliance, parameter consistency, and mathematical fidelity.

## Method Summary
The AI agent system integrates Retrieval-Augmented Generation with open-source large language models within a multi-stage iterative workflow built on LangGraph architecture. A Devito knowledge graph, constructed from documentation and source code, is leveraged by GraphRAG for efficient retrieval. The agent employs static analysis of Fortran code to generate targeted queries, retrieves relevant Devito examples, and synthesizes new code under Pydantic-structured output constraints. Quality validation combines conventional static analysis with LLM-based evaluation (G-Eval), covering execution correctness, structural integrity, API compliance, parameter consistency, and mathematical fidelity.

## Key Results
- Retrieval accuracy: Precision@5 = 0.964, Recall@5 = 0.930
- Translation performance: Grade-A success rate = 76.9%
- Quality validation: near-perfect scores in execution, structure, and API usage across 13 test cases

## Why This Works (Mechanism)
The system's effectiveness stems from combining multiple AI technologies in a structured workflow. The Devito knowledge graph provides a rich semantic foundation for retrieval, while static analysis of Fortran code enables precise query generation. The iterative refinement process allows the agent to progressively improve translations through multiple passes. Pydantic-structured output constraints ensure consistent code generation, and the multi-metric validation approach (combining static analysis and LLM-based evaluation) provides comprehensive quality assessment.

## Foundational Learning

**LangGraph Architecture** - Needed for building stateful, multi-actor AI workflows that can handle complex translation tasks. Quick check: Verify workflow can maintain state across multiple translation stages.

**GraphRAG Retrieval** - Required for efficient knowledge graph-based retrieval from Devito documentation and source code. Quick check: Test retrieval precision with queries of varying complexity.

**Pydantic Output Constraints** - Essential for ensuring generated code adheres to Devito's API specifications and structural requirements. Quick check: Validate that all generated code passes Pydantic validation.

**Static Analysis of Fortran** - Critical for understanding legacy code structure and generating accurate transformation queries. Quick check: Confirm static analysis correctly identifies all code constructs.

**LLM-based G-Eval** - Needed for comprehensive quality assessment that goes beyond traditional static analysis. Quick check: Compare G-Eval scores with human expert evaluation on sample translations.

## Architecture Onboarding

**Component Map**: Fortran Static Analysis -> Query Generation -> GraphRAG Retrieval -> Code Synthesis -> Pydantic Validation -> G-Eval Quality Assessment -> Output

**Critical Path**: Static Analysis → Query Generation → GraphRAG Retrieval → Code Synthesis → Validation → Output

**Design Tradeoffs**: The system prioritizes accuracy over speed, using multiple validation stages and iterative refinement. The choice of open-source LLMs balances cost and performance. Pydantic constraints ensure API compliance but may limit creative code generation approaches.

**Failure Signatures**: 
- Low retrieval precision indicates knowledge graph coverage gaps
- Pydantic validation failures suggest API specification mismatches
- G-Eval quality score drops indicate semantic or mathematical errors
- Static analysis errors point to complex Fortran constructs not handled properly

**First 3 Experiments**:
1. Test retrieval accuracy with progressively complex Fortran code snippets
2. Validate Pydantic constraints by attempting to generate intentionally malformed code
3. Compare G-Eval scores with human expert evaluation on a small sample set

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance relies heavily on quality and coverage of the Devito knowledge graph
- High precision/recall metrics are specific to 13 test cases and may not generalize
- LLM-based G-Eval introduces potential subjectivity and model-dependent variability
- Scalability challenges for extremely large or poorly documented legacy codebases not validated
- Static analysis may miss runtime-dependent behaviors or implicit dependencies

## Confidence

- Retrieval accuracy metrics (Precision@5, Recall@5): **High** - Based on direct measurements and clear evaluation methodology.
- Translation performance (Grade-A success rate): **Medium** - Limited to 13 test cases; generalizability uncertain.
- LLM-based quality validation (G-Eval): **Medium** - Depends on model consistency and evaluation design.

## Next Checks

1. Test the system on a broader set of legacy Fortran codes, including those with undocumented or ambiguous constructs, to assess robustness and generalization.
2. Evaluate the system's performance on codebases exceeding 1 million lines to identify scalability bottlenecks and resource constraints.
3. Compare the LLM-based G-Eval results with human expert validation on a subset of translations to quantify potential biases or inconsistencies in automated quality assessment.