---
ver: rpa2
title: 'MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models'
arxiv_id: '2508.17444'
source_url: https://arxiv.org/abs/2508.17444
tags:
- paraphrase
- dataset
- sentences
- marathi
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MahaParaphrase, a high-quality paraphrase
  detection corpus for Marathi, a low-resource Indic language. The dataset consists
  of 8,000 sentence pairs annotated as paraphrase (P) or non-paraphrase (NP), manually
  curated by native speakers.
---

# MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models

## Quick Facts
- **arXiv ID:** 2508.17444
- **Source URL:** https://arxiv.org/abs/2508.17444
- **Reference count:** 12
- **Primary result:** Introduces MahaParaphrase corpus (8K sentence pairs) and achieves 88.7% F1 with MahaBERT

## Executive Summary
This work introduces MahaParaphrase, a high-quality paraphrase detection corpus for Marathi, a low-resource Indic language. The dataset consists of 8,000 sentence pairs annotated as paraphrase (P) or non-paraphrase (NP), manually curated by native speakers. The corpus is divided into five buckets based on word overlap and semantic similarity, capturing varying degrees of paraphrastic relationships. Baseline BERT-based models, including MahaBERT, mBERT, IndicBERT, and Muril, are evaluated on this dataset. MahaBERT achieves the highest performance with an F1 score of 88.7%, demonstrating its effectiveness for paraphrase detection in Marathi. This dataset addresses the lack of quality paraphrase corpora for Marathi and provides a valuable resource for advancing research in Marathi natural language processing.

## Method Summary
The corpus construction combines cosine-similarity filtering of existing MahaCorpus pairs (0.8-0.99 threshold for paraphrases) with back-translation of Marathi sentences to English and back, filtering for similarity scores between 0.8-0.99. Both approaches yield balanced P/NP pairs. Sentence pairs are manually verified by four native Marathi speakers and bucketed by word overlap percentages (B1: 0-20%, B2: 20-40%, B3: 40-60%, B4: 60-80%, B5: 80-100%). BERT-based models (MahaBERT, mBERT, IndicBERT, Muril) are fine-tuned on this dataset for binary classification.

## Key Results
- MahaBERT achieves the highest F1 score of 88.7% on the MahaParaphrase dataset
- Model ranking: MahaBERT (88.7%) > IndicBERT (87.1%) > Muril (86.9%) > IndicBERT-MLM (85.9%) > mBERT (84.59%)
- Dataset contains 4,000 paraphrase pairs and 4,000 non-paraphrase pairs across five overlap buckets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bucketing sentence pairs by word overlap creates evaluation robustness across paraphrase difficulty levels.
- Mechanism: The corpus partitions both paraphrase (P) and non-paraphrase (NP) pairs into 5 buckets (B1–B5) based on lexical overlap percentages (0–20%, 20–40%, 40–60%, 60–80%, 80–100%). This forces models to distinguish semantic equivalence from surface similarity—e.g., B5 NP pairs have high word overlap but different meanings, testing sensitivity to word order and syntax.
- Core assumption: Word overlap correlates inversely with paraphrase detection difficulty; low-overlap paraphrases require synonym/semantic understanding, while high-overlap non-paraphrases require structural understanding.
- Evidence anchors:
  - [Section 3.4]: "This categorization into buckets makes the dataset robust and versatile for evaluation across different scenarios, such as high-overlap non-paraphrase sentence pairs or low-overlap paraphrase sentence pairs."
  - [Table 1]: Provides concrete P/NP examples per bucket with calculated overlap percentages.
  - [corpus]: Weak direct corpus evidence on bucketing efficacy—no neighbor papers replicate this granular stratification.
- Break condition: If models achieve near-identical accuracy across all buckets, the stratification may not be capturing meaningful difficulty variation.

### Mechanism 2
- Claim: Combining cosine-similarity filtering with back-translation yields a balanced real/synthetic dataset.
- Mechanism: Approach 1 retrieves sentence pairs from MahaCorpus with cosine similarity 0.8–0.99 (P) or <0.8 (NP) using MahaSBERT embeddings. Approach 2 generates synthetic paraphrases via Marathi→English→Marathi back-translation, filtering pairs with similarity 0.8–0.99 to exclude near-identical or meaning-shifted outputs. Equal sampling from both ensures diversity.
- Core assumption: BERTScore-based cosine similarity is a reliable proxy for semantic equivalence in Marathi; back-translation preserves meaning while introducing lexical variation.
- Evidence anchors:
  - [Section 3.2.1]: "If the cosine similarity (C.S) score was less than 0.8, the sentences were labeled as NP. If...between 0.8 and 0.99, labeled as P."
  - [Section 3.2.2]: Back-translation pipeline described with explicit filtering thresholds.
  - [corpus]: L3Cube-MahaEmotions paper similarly uses synthetic annotation via LLMs, suggesting L3Cube's broader reliance on synthetic augmentation for low-resource Marathi tasks.
- Break condition: If back-translation introduces systematic meaning drift not caught by the cosine filter, synthetic P labels may be noisy.

### Mechanism 3
- Claim: Language-specific pretraining (MahaBERT) outperforms multilingual models for Marathi paraphrase detection.
- Mechanism: MahaBERT, fine-tuned on L3Cube-MahaCorpus, leverages Marathi-specific tokenization and monolingual pretraining. This provides stronger contextual representations than mBERT (104 languages), MuRIL (17 Indian languages + transliteration pairs), or IndicBERT (12 Indian languages, ALBERT architecture).
- Core assumption: Monolingual pretraining on Marathi corpus captures morphological and syntactic patterns better than multilingual alternatives, despite smaller scale.
- Evidence anchors:
  - [Section 6]: "MahaBERT was the most accurate model, followed by IndicBERT (MLM + TLM), Muril, IndicBERT (MLM only) and MBERT" with F1 scores of 88.7, 87.1, 86.9, 85.9, 84.59 respectively.
  - [Section 5.4]: "MahaBERT was refined using the L3Cube-MahaCorpus and additional publicly accessible Marathi monolingual datasets."
  - [corpus]: Tokenization Matters paper highlights that subword segmentation quality significantly affects low-resource Indic language performance—supporting the hypothesis that language-specific vocabulary matters.
- Break condition: If evaluated on code-mixed or dialect-heavy Marathi, multilingual models may outperform MahaBERT due to broader vocabulary coverage.

## Foundational Learning

- Concept: **BERTScore / Cosine Similarity for Sentence Embeddings**
  - Why needed here: Core to both annotation (filtering P/NP pairs) and potential evaluation of paraphrase quality.
  - Quick check question: Given two Marathi sentences, can you explain why cosine similarity of 0.85 might still correspond to a non-paraphrase pair?

- Concept: **Back-Translation as Data Augmentation**
  - Why needed here: Used to generate synthetic paraphrases; understanding its failure modes (meaning drift, over-lexical-preservation) is critical for quality control.
  - Quick check question: What happens if the Marathi→English translation fails to capture a culture-specific term, and how would back-translation amplify this error?

- Concept: **Word Overlap vs. Semantic Similarity Decoupling**
  - Why needed here: Central to the bucketing logic—high overlap ≠ paraphrase, low overlap ≠ non-paraphrase.
  - Quick check question: Construct a theoretical NP pair with 90% word overlap in Marathi that has opposite meaning due to word order or negation placement.

## Architecture Onboarding

- Component map:
  MahaCorpus (1M sentences) -> [Sentence Pair Generation] -> [Approach 1 (Cosine Sim) + Approach 2 (Back-Translation)] -> [Human Verification - 4 native annotators] -> [Bucketing by Word Overlap (B1-B5)] -> [BERT Fine-tuning] -> [Evaluation - F1 Score]

- Critical path:
  1. MahaSBERT embedding quality -> cosine threshold validity
  2. Back-translation filter thresholds (0.8–0.99) -> synthetic data quality
  3. Human annotator consistency -> ground-truth reliability
  4. Bucket balance -> difficulty stratification effectiveness

- Design tradeoffs:
  - **Dataset size vs. quality**: 8K pairs is small vs. English corpora (MRPC ~5.8K, but larger English resources exist), but manually verified; paper explicitly notes this limitation.
  - **Threshold selection**: 0.8 cosine cutoff is heuristic; not empirically validated via ablation in this paper.
  - **Bucket distribution**: Uneven (Figure 3 shows varied counts), which may bias evaluation toward certain difficulty levels.

- Failure signatures:
  - Model accuracy near-random on B5 NP pairs -> indicates lexical shortcut reliance
  - High performance on synthetic pairs, low on Approach 1 pairs -> suggests overfitting to back-translation artifacts
  - Disagreement between annotators -> signals ambiguous paraphrase boundary cases

- First 3 experiments:
  1. **Ablate cosine thresholds**: Re-annotate a 500-pair subset with thresholds [0.75, 0.85, 0.90] and measure annotation consistency vs. human judgment; determine if 0.8 is optimal.
  2. **Cross-bucket transfer**: Train MahaBERT on only B1–B3 (low/medium overlap) and evaluate on B4–B5 to test generalization to high-overlap cases.
  3. **Synthetic vs. real data contribution**: Train two models—one on only Approach 1 pairs, one on only Approach 2 pairs—and compare F1 to isolate each annotation source's value.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size: 8,000 sentence pairs is relatively small compared to English paraphrase corpora, limiting generalizability despite manual verification
- Threshold selection: The 0.8 cosine similarity cutoff for filtering P/NP pairs is heuristic and not empirically validated through ablation studies
- Synthetic data quality: Back-translation pipeline may introduce subtle meaning shifts despite cosine filtering, potentially contaminating the paraphrase labels

## Confidence
- **High confidence:** MahaBERT achieving superior F1 (88.7%) over multilingual baselines on this specific Marathi corpus; the core methodology of combining cosine-similarity filtering with back-translation is clearly described and reproducible
- **Medium confidence:** The effectiveness of bucketing strategy for evaluation robustness; while conceptually sound, the paper lacks quantitative evidence showing meaningful performance variation across buckets
- **Medium confidence:** The claim that language-specific pretraining provides consistent advantages; results show MahaBERT winning but with relatively narrow margins over specialized multilingual models like IndicBERT and Muril

## Next Checks
1. **Ablate cosine thresholds:** Re-annotate a 500-pair subset using multiple cosine similarity thresholds (0.75, 0.85, 0.90) and measure inter-annotator agreement versus human judgment to empirically determine optimal filtering thresholds

2. **Cross-bucket generalization test:** Train MahaBERT exclusively on low/medium overlap buckets (B1-B3) and evaluate on high-overlap buckets (B4-B5) to verify the model learns semantic understanding rather than lexical pattern matching

3. **Synthetic vs. real data contribution:** Train separate models using only Approach 1 pairs (cosine similarity from MahaCorpus) versus only Approach 2 pairs (back-translated synthetic data) to isolate and quantify each annotation source's contribution to final performance