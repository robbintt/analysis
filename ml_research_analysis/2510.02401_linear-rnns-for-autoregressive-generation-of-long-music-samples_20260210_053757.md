---
ver: rpa2
title: Linear RNNs for autoregressive generation of long music samples
arxiv_id: '2510.02401'
source_url: https://arxiv.org/abs/2510.02401
tags:
- linear
- state
- training
- learning
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HarmonicRNN, a linear RNN architecture with pooling layers, achieves
  state-of-the-art performance on raw audio generation tasks, including the SC09,
  Beethoven, and YouTubeMix datasets. By using complex gated linear recurrent units
  (CG-LRU) and multi-host context parallelism, HarmonicRNN can train on sequences
  up to 1 million tokens (1 minute of audio at 16 kHz).
---

# Linear RNNs for autoregressive generation of long music samples

## Quick Facts
- **arXiv ID:** 2510.02401
- **Source URL:** https://arxiv.org/abs/2510.02401
- **Reference count:** 1
- **Primary result:** HarmonicRNN achieves state-of-the-art NLL and perceptual metrics on raw audio generation tasks using linear RNNs with pooling layers

## Executive Summary
HarmonicRNN is a linear RNN architecture that achieves state-of-the-art performance on raw audio generation tasks by combining complex gated linear recurrent units (CG-LRU) with U-net style pooling layers. The model can process sequences up to 1 million tokens (1 minute of audio at 16 kHz) and outperforms prior methods like SaShiMi and Mamba on both likelihood and perceptual quality metrics. Key innovations include sinusoidal embeddings for faster convergence and grouped convolutions for stable pooling operations.

## Method Summary
HarmonicRNN employs a U-net architecture with 4 down-pooling levels (2, 4, 4, 5), reducing temporal resolution by 160x total. It uses 36 temporal blocks (4 per level) with CG-LRU units, 128 feature width, and 256 RNN hidden dimension, totaling 7.3M parameters. Training uses AdamW with 1000-step warmup, batch size 32, and 500 epochs with EMA rate 0.999. Sinusoidal input embeddings and strided/dilated grouped convolutions (4 groups) for pooling are critical design choices that enable stable training and superior performance.

## Key Results
- Achieves NLL of 1.81 on SC09 (vs 1.95 for SaShiMi, 2.07 for Mamba)
- Achieves NLL of 1.47 on Beethoven (vs 1.63 for SaShiMi, 2.02 for Mamba)
- Achieves FID of 0.46 on YouTubeMix (vs 2.95 without pooling)

## Why This Works (Mechanism)
HarmonicRNN works by combining the computational efficiency of linear RNNs with U-net style pooling to capture long-range dependencies in audio sequences. The CG-LRU units maintain linear complexity while modeling complex temporal patterns, and the pooling layers enable multi-scale feature extraction. Sinusoidal embeddings provide stable positional information that accelerates convergence compared to learned embeddings.

## Foundational Learning

**CG-LRU (Complex Gated Linear Recurrent Unit)**
- *Why needed:* Enables linear-time recurrent processing while maintaining expressive power for audio modeling
- *Quick check:* Verify implementation matches De et al. (2024) and Botev et al. (2024) specifications

**Grouped Convolutions for Pooling**
- *Why needed:* Prevents instability during pooling operations and maintains feature diversity
- *Quick check:* Test with groups=1 vs groups=4 to observe stability differences

**Sinusoidal Embeddings**
- *Why needed:* Provides stable positional information without learned parameters that might overfit
- *Quick check:* Compare convergence speed with learned vs sinusoidal embeddings

**U-net Architecture with Pooling**
- *Why needed:* Enables multi-scale feature extraction necessary for coherent long audio generation
- *Quick check:* Train with and without pooling to observe NLL and FID differences

## Architecture Onboarding

**Component Map:** Input → Sinusoidal Embeddings → U-net Encoder (CG-LRU Blocks + Pooling) → U-net Decoder (CG-LRU Blocks + Upsampling) → Output

**Critical Path:** CG-LRU temporal blocks with residual connections → Pooling layers with grouped convolutions → Sinusoidal positional embeddings

**Design Tradeoffs:** Linear RNNs provide computational efficiency but require careful gating mechanisms (CG-LRU) to maintain expressiveness. Pooling layers improve perceptual quality but don't significantly affect NLL, creating a tradeoff between likelihood and sample quality.

**Failure Signatures:** Using dense convolutions (groups=1) for pooling causes training instability and worse NLL. Removing pooling maintains NLL but severely degrades perceptual quality (FID increases from 0.46 to 2.95).

**3 First Experiments:**
1. Train on SC09 with basic CG-LRU implementation to verify NLL reaches 1.81
2. Implement pooling ablation by removing all pooling layers to confirm NLL stability but FID degradation
3. Test alternative positional embeddings (learned vs sinusoidal) on SC09 to verify convergence differences

## Open Questions the Paper Calls Out
None

## Limitations
- CG-LRU implementation details are not fully specified, requiring reference to external papers for exact parameterization
- Sinusoidal embedding frequencies and formulas are not precisely defined beyond "inspired by Kingma et al. (2021) Appendix C"
- Claims about linear RNNs being "highly effective" generalize beyond this specific implementation

## Confidence
**High Confidence:** Architectural design choices and their effectiveness, state-of-the-art NLL claims
**Medium Confidence:** Perceptual quality improvements and pooling attribution
**Low Confidence:** General claims about linear RNNs' effectiveness for long-sequence audio modeling

## Next Checks
1. Implement CG-LRU using De et al. (2024) and Botev et al. (2024) specifications and verify SC09 NLL reaches 1.81
2. Train HarmonicRNN without pooling layers to confirm NLL remains ~1.81 while FID degrades from 0.46 to 2.95
3. Compare training convergence with sinusoidal vs learned positional embeddings on SC09 dataset