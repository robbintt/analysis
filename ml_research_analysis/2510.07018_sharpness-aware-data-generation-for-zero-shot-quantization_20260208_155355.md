---
ver: rpa2
title: Sharpness-Aware Data Generation for Zero-shot Quantization
arxiv_id: '2510.07018'
source_url: https://arxiv.org/abs/2510.07018
tags:
- data
- quantization
- loss
- gradient
- sharpness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of zero-shot quantization, which
  aims to quantize a pre-trained full-precision model without access to the original
  training data. The key challenge is generating synthetic data that effectively represents
  the real data distribution for quantization.
---

# Sharpness-Aware Data Generation for Zero-shot Quantization

## Quick Facts
- arXiv ID: 2510.07018
- Source URL: https://arxiv.org/abs/2510.07018
- Authors: Dung Hoang-Anh; Cuong Pham Trung Le; Jianfei Cai; Thanh-Toan Do
- Reference count: 9
- Primary result: Proposes SADAG for zero-shot quantization, achieving 65.94% and 69.11% top-1 accuracy on CIFAR-100 with 3/3 and 4/4 bit-widths respectively, outperforming Genie by 0.69% and 0.76%

## Executive Summary
This paper addresses zero-shot quantization by generating synthetic data that effectively represents real data distributions. The proposed SADAG method combines knowledge transfer from the full-precision model with sharpness-aware optimization. By maximizing gradient matching between synthetic and validation data reconstructions, SADAG reduces model sharpness during quantization. The approach demonstrates superior performance over state-of-the-art techniques across multiple datasets and model architectures.

## Method Summary
SADAG generates synthetic data by optimizing a multi-objective loss combining batch normalization matching, diversity preservation, and gradient matching. The method first warms up using only BN statistics, then performs sharpness-aware data generation through neighbor-based gradient matching approximation. Finally, the quantized model is calibrated using the generated dataset. The approach operates under the constraint of approximating the full Hessian with only the final fully-connected layer for computational tractability.

## Key Results
- CIFAR-100: Achieves 65.94% and 69.11% top-1 accuracy with 3/3 and 4/4 bit-widths, outperforming Genie by 0.69% and 0.76%
- ImageNet: Consistently outperforms previous approaches across different architectures and bit-width settings
- MobileNetV2 shows architectural sensitivity at extreme low-bit (2/2) precision with accuracy dropping to ~13%
- Computational overhead is ~1.5× slower than Genie due to gradient matching computations

## Why This Works (Mechanism)

### Mechanism 1: Sharpness Reduction via Gradient Matching
Maximizing gradient matching between synthetic and validation data reduces quantized model sharpness. Through first-order Taylor expansion, minimizing SAM loss correlates with maximizing cosine similarity between reconstruction loss gradients. The Hessian of the final fully-connected layer is approximated as scaled identity matrix to decouple the objective from full Hessian computation.

### Mechanism 2: Neighborhood Gradient Approximation Without Validation Data
Gradient matching with validation data is approximated by matching gradients between each generated sample and its worst-case neighbor. For each synthetic sample, a perturbation maximizes gradient dissimilarity within radius ν, then the method minimizes this dissimilarity. This works if synthetic samples are diverse enough to span the data space.

### Mechanism 3: BN Statistics as Distributional Anchor
Matching batch normalization statistics ensures generated data lies on the same distributional manifold as real training data. The BN layers store running statistics from original training, and minimizing L_BN forces synthetic samples to produce matching activation moments, grounding them in realistic feature space.

## Foundational Learning

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed: SADAG's core theoretical contribution links data generation to SAM objectives
  - Quick check: Can you explain why perturbing weights in the direction of maximum loss increase before gradient descent leads to flatter minima?

- **Concept: Layer-wise Reconstruction Loss for Quantization**
  - Why needed: The paper uses reconstruction loss between full-precision and quantized model activations rather than task loss
  - Quick check: Why would matching intermediate layer outputs be preferable to matching final predictions for low-bit quantization calibration?

- **Concept: Gradient Matching / Gradient Cosine Similarity**
  - Why needed: The method's proxy objective for sharpness is cosine similarity of gradients between data subsets
  - Quick check: If two datasets produce gradients with high cosine similarity but different magnitudes, how would this affect optimization trajectory?

## Architecture Onboarding

- **Component map:** Full-precision model (θ_FP, frozen) → provides BN statistics + reconstruction targets → Generator G + embeddings Z → produces synthetic batch X^(T) → Quantized model (θ_Q, initialized from θ_FP) → calibrated on X^(T) → Multi-objective loss: L_BN + λ₁·L_DIVERSE + λ₂·L_GRAD

- **Critical path:** Warm-up phase (N_w iterations): Train G and Z using only L_BN to get initial synthetic data distribution. SADAG phase (N_g iterations): For each sample, compute neighbor perturbation ε_i^(N) via Eq. 17, then update G and Z using full loss (Eq. 21). Quantization calibration (N_q iterations): Generate final dataset X^(T), calibrate θ_Q using reconstruction loss only.

- **Design tradeoffs:** Computational cost: SADAG is ~1.5× slower than Genie due to gradient matching computations. Layer selection for Hessian approximation: Paper uses only final FC layer (512K params for ResNet-18 vs 9.4K for first conv) for tractability. Neighbor radius ν: Set to 2; larger values may explore irrelevant regions. Number of generated images: 1024 images used for fair comparison; diminishing returns beyond 512.

- **Failure signatures:** Low diversity in generated samples → L_DIVERSE not converging, gradients cluster in narrow directions. Extreme low-bit (2/2) degradation on MobileNetV2 → accuracy drops to ~13%. λ₁ or λ₂ too large (>2) or too small (0) → performance degrades.

- **First 3 experiments:** 1) Reproduce Table 1 verification: Using real ImageNet subset, compare random selection vs gradient-matching selection for calibration. 2) Ablate loss components: Train with only L_BN (baseline Genie), then add L_DIVERSE, then add L_GRAD. 3) Hyperparameter sensitivity sweep: Test ν ∈ {0.5, 1, 2, 4} and (λ₁, λ₂) ∈ {(0.5,0.5), (1,1), (2,2)} on ResNet-18, 4/4 setting.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the gradient matching objective be effectively extended from the final fully-connected layer to multiple network blocks without incurring prohibitive computational costs? The current method only considers the final FC layer due to computational expense of full-model Jacobian/Hessian estimation.

- **Open Question 2:** Can integrating more rigorous Hessian approximation techniques replace the current diagonal relaxation assumption to improve sharpness estimation? The current diagonal Hessian approximation (cI) is a simplification that may reduce fidelity of sharpness minimization.

- **Open Question 3:** Is the assumption that the Hessian is a diagonal matrix with constant values valid for complex architectures like Vision Transformers (ViT)? The mathematical derivation relies on linearity of the final layer, which may not hold for non-CNN architectures.

## Limitations
- Computational tractability forces approximation of full Hessian with only final fully-connected layer
- Performance degradation on MobileNetV2 at extreme low-bit (2/2) precision suggests architectural sensitivity
- Gradient matching hyperparameters (ν=2, λ₁=λ₂=1) lack theoretical justification for their specific values

## Confidence
- **High Confidence:** Theoretical derivation connecting SAM loss to gradient matching - mathematical steps are explicit and follow established principles
- **Medium Confidence:** Neighborhood approximation strategy - effectiveness depends heavily on diversity of generated samples
- **Medium Confidence:** BN statistics matching mechanism - proven effective in prior work but may not generalize to architectures without BN layers

## Next Checks
1. **Validation on ResNet-18 CIFAR-100 subset:** Use first 10 classes to create validation set. Generate synthetic data with SADAG, compare against Genie by measuring both reconstruction loss and actual sharpness (via SAM loss on validation set).
2. **Neighborhood diversity analysis:** For fixed generated dataset, compute pairwise cosine similarity of gradients between all samples. If most samples have high similarity, neighborhood approximation breaks down.
3. **Architectural sensitivity test:** Apply SADAG to MobileNetV2 with bit-widths {2/2, 3/3, 4/4} on CIFAR-100. Compare accuracy degradation patterns to ResNet-20 to identify architectural interactions.