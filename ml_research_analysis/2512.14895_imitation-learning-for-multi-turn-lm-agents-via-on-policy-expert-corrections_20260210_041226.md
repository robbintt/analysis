---
ver: rpa2
title: Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections
arxiv_id: '2512.14895'
source_url: https://arxiv.org/abs/2512.14895
tags:
- expert
- testbed
- agent
- file
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: On-policy expert corrections (OECs) address covariate shift in
  multi-turn LM agent training by partially rolling out trajectories with a student
  model and then switching to an expert model mid-trajectory. Experiments on SWE-bench
  show OECs improve resolution rates by 14% (7B) and 13% (32B) over traditional imitation
  learning, while combining OEC and behavioral cloning achieves SOTA performance.
---

# Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections

## Quick Facts
- arXiv ID: 2512.14895
- Source URL: https://arxiv.org/abs/2512.14895
- Reference count: 40
- Key outcome: OECs improve SWE-bench resolution rates by 14% (7B) and 13% (32B) over traditional imitation learning

## Executive Summary
On-policy expert corrections (OECs) address covariate shift in multi-turn language model agent training by combining student model rollouts with expert interventions mid-trajectory. The method partially executes trajectories with the student model before switching to expert control, then uses the resulting on-policy data for training. This approach significantly outperforms traditional imitation learning baselines on the SWE-bench benchmark, with resolution rate improvements of 14% for 7B models and 13% for 32B models. The method also establishes new state-of-the-art performance when combined with behavioral cloning.

## Method Summary
OECs tackle covariate shift by generating expert demonstrations on-policy, where trajectories are partially executed by the student model before expert takeover. During training, the student model generates a trajectory until a masking function determines an expert correction is needed, at which point the expert model takes over to complete the trajectory. This on-policy data generation process ensures the training distribution matches the test-time deployment distribution. The approach includes on-policy masking to identify correction points and repetitive trajectory filtering to remove redundant data, both critical for performance.

## Key Results
- 14% resolution rate improvement over traditional imitation learning for 7B models on SWE-bench
- 13% resolution rate improvement over traditional imitation learning for 32B models on SWE-bench
- OEC + behavioral cloning achieves SOTA performance on SWE-bench
- Covariate shift between student and expert models increases throughout trajectories

## Why This Works (Mechanism)
The method addresses a fundamental mismatch between training and deployment conditions in multi-turn LM agents. Traditional imitation learning uses expert demonstrations collected offline, creating covariate shift when the student model encounters states not seen during training. By generating expert corrections on-policy, the training data distribution matches the actual deployment distribution where the student model is in control. The on-policy masking identifies optimal correction points where expert intervention is most valuable, while filtering removes redundant trajectories that don't contribute new learning signals.

## Foundational Learning
- **Covariate shift**: Distribution mismatch between training and test data that degrades model performance when agents encounter novel states
  - Why needed: Core problem OEC addresses; explains why offline expert data fails for multi-turn agents
  - Quick check: Compare state distributions between student-generated and expert-generated trajectories

- **Behavioral cloning**: Supervised learning from expert demonstrations to mimic expert behavior
  - Why needed: Standard baseline method that OEC improves upon
  - Quick check: Measure performance gap between behavioral cloning and OEC on same dataset

- **On-policy learning**: Data collection and learning happen under current policy rather than fixed expert policy
  - Why needed: Ensures training distribution matches deployment distribution
  - Quick check: Verify that OEC training data distribution matches student deployment distribution

## Architecture Onboarding

Component map: Student model -> Masking function -> Expert model -> Training data generation -> Student model update

Critical path: Student model execution -> Masking decision -> Expert takeover -> Trajectory completion -> Training update

Design tradeoffs: Expert intervention frequency vs. computational cost; on-policy data quality vs. offline data volume; filtering strictness vs. data diversity

Failure signatures: Performance plateaus if masking is too conservative; covariate shift increases if expert interventions are too sparse; training instability if filtering is too aggressive

First experiments:
1. Ablation study removing on-policy masking to quantify its contribution to performance
2. Covariate shift measurement throughout trajectories to validate increasing distribution mismatch
3. Filtering threshold sensitivity analysis to determine optimal redundancy removal

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost requires expert intervention at every training step, raising scalability concerns
- Experiments limited to software engineering domain (SWE-bench), limiting generalizability
- No investigation of long-term training stability or performance degradation over extended periods

## Confidence

Performance improvements: High confidence - Well-supported by experimental results on SWE-bench with multiple model sizes and ablation studies

Covariate shift phenomenon: Medium confidence - Supported by analysis but limited to single domain; measurement methodology could benefit from additional validation

Effectiveness of on-policy masking and filtering: High confidence - Clear ablation study results show significant performance drops when these components are removed

## Next Checks
1. Measure expert effort per training step and extrapolate to full dataset to quantify practical feasibility and identify scalability bottlenecks

2. Test OEC methodology on multi-turn agent tasks outside software engineering to evaluate generalizability of performance gains

3. Conduct extended training runs to assess whether performance improvements persist, plateau, or degrade over time