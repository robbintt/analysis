---
ver: rpa2
title: 'Normative Reasoning in Large Language Models: A Comparative Benchmark from
  Logical and Modal Perspectives'
arxiv_id: '2510.26606'
source_url: https://arxiv.org/abs/2510.26606
tags:
- reasoning
- normative
- epistemic
- valid
- deontic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the logical consistency of large language
  models in normative reasoning by comparing their performance on deontic (normative)
  and epistemic (knowledge-based) reasoning tasks. A new dataset was created to test
  reasoning patterns across both domains, incorporating cognitive factors that influence
  human reasoning.
---

# Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives

## Quick Facts
- arXiv ID: 2510.26606
- Source URL: https://arxiv.org/abs/2510.26606
- Reference count: 14
- Primary result: LLMs show human-like content effects and notable inconsistencies in specific normative inferences, particularly involving negation and the transition from obligation to permission.

## Executive Summary
This study evaluates the logical consistency of large language models in normative reasoning by comparing their performance on deontic (normative) and epistemic (knowledge-based) reasoning tasks. A new dataset was created to test reasoning patterns across both domains, incorporating cognitive factors that influence human reasoning. Results show that LLMs generally follow valid reasoning patterns but exhibit notable inconsistencies in specific normative inferences, particularly those involving negation and the transition from obligation to permission. Models also display human-like content effects, with performance varying based on whether conclusions align with common sense. While few-shot prompting improves accuracy, chain-of-thought reasoning does not consistently enhance robustness and sometimes introduces errors.

## Method Summary
The study created NeuBAROCO dataset with 640 deontic logic problems and 480 syllogistic reasoning problems, plus parallel epistemic versions. Five LLMs (GPT-4o, GPT-4o-mini, Llama-3.1-8B-In, Llama-3.3-70B-In, Phi-4) were evaluated using three prompting strategies: Zero-Shot, Few-Shot, and Zero-Shot Chain-of-Thought. Problems included three content types: congruent, incongruent, and nonsense. Models classified entailment relationships in normative and epistemic inference patterns.

## Key Results
- LLMs exhibit human-like content effects, with 20%+ accuracy gaps between congruent and incongruent conditions
- Mu-Mi pattern (obligation to permission) shows systematic failure despite logical validity
- MT/DA patterns with negation show 40-60% lower accuracy than MP/AC patterns
- Chain-of-thought prompting sometimes degrades performance by introducing reasoning errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting improves normative reasoning accuracy by enabling syntactic pattern matching across formal reasoning structures.
- Mechanism: In-context examples provide structural templates that models can align with query instances, bypassing the need for explicit logical rule internalization.
- Core assumption: Performance gains reflect structural alignment rather than verified logical inference chains.
- Evidence anchors:
  - [abstract] "While few-shot prompting improves accuracy, chain-of-thought reasoning does not consistently enhance robustness and sometimes introduces errors."
  - [section 6] "This can be attributed to the straightforward nature of leveraging similarities in syntactic structures within reasoning patterns. However, the observed improvements may result from superficial pattern matching rather than genuine reasoning."
  - [corpus] Weak direct support; related work (arXiv:2506.11068) shows modal expressions systematically bias normative judgments, suggesting surface-level linguistic features dominate.
- Break condition: Performance degrades when problem structure deviates from exemplar patterns, or when negation complexity increases (MT/DA patterns show 40-60% lower accuracy than MP/AC in Table 14).

### Mechanism 2
- Claim: Linguistic framing of modal operators interferes with logical inference, causing models to misclassify logically valid patterns.
- Mechanism: Models conflate colloquial interpretations (e.g., "can choose to" as optionality) with formal deontic meanings (permission), breaking inferences like OA⇒PA.
- Core assumption: Models encode modal semantics through distributional patterns rather than formal logical definitions.
- Evidence anchors:
  - [abstract] "notable inconsistencies in specific normative inferences, particularly those involving negation and the transition from obligation to permission."
  - [section 6] "GPT-4o did not infer 'You can choose to take care of your health' from 'You must take care of your health', interpreting 'can choose to' as an option rather than a statement of permission."
  - [corpus] arXiv:2506.11068 documents "Deontological Keyword Bias" where modal expressions systematically skew moral judgments.
- Break condition: Consistency requires explicit formal vocabulary (e.g., "obligatory"/"permissible" vs. "must"/"can choose to") or formal training on deontic logic structures.

### Mechanism 3
- Claim: Pre-trained priors over content cause logical validity judgments to correlate with common-sense plausibility.
- Mechanism: When conclusions contradict pre-trained world knowledge, models reject valid inferences; when conclusions align, they accept invalid ones.
- Core assumption: LLMs cannot fully suppress learned statistical associations during formal reasoning tasks.
- Evidence anchors:
  - [abstract] "Models also display human-like content effects, with performance varying based on whether conclusions align with common sense."
  - [section 3.2] Table 4 defines content types; Table 6 shows GPT-4o achieves 94% on congruent vs. 72.27% on incongruent content (Zero-Shot).
  - [corpus] arXiv:2502.09589 confirms "logical forms complement probability" and finds systematic deviations from formal validity correlated with prior probability.
- Break condition: Effect attenuates but doesn't disappear with nonsense content (intermediate performance), suggesting both content priors and task framing contribute.

## Foundational Learning

- Concept: **Deontic Square of Opposition**
  - Why needed here: The paper's core evaluation framework; understanding that OA↔¬P¬A and PA↔¬O¬A enables diagnosis of which inference patterns models fail.
  - Quick check question: Given "It is not permitted to steal," can you derive "It is obligatory not to steal" and explain why?

- Concept: **Free Choice and Ross Paradoxes**
  - Why needed here: Distinguishes whether models follow Standard Deontic Logic (SDL) or intuitive human reasoning patterns; critical for interpreting FC-Or-Elim (intuitively valid, SDL-invalid) and Ross-Or-Intro (SDL-valid, intuitively invalid).
  - Quick check question: Why does SDL validate "You must mail the letter" → "You must mail or burn the letter," and which pattern did LLMs follow?

- Concept: **Content Effects in Human Reasoning**
  - Why needed here: Establishes baseline for interpreting LLM behavior as "human-like bias" vs. novel failure mode; explains congruence/incongruence manipulation in experimental design.
  - Quick check question: If a model accepts "All birds fly; Tweety is a bird; therefore Tweety flies" but rejects "All birds fly; Penguins are birds; therefore Penguins fly," what bias is demonstrated?

## Architecture Onboarding

- Component map: Deontic Logic Task -> Syllogistic Task -> Epistemic Control -> Content Manipulation
- Critical path: Parse modal operator type (obligation/permission, necessity/possibility) → Identify formal inference pattern (e.g., Mu-Mi, Cat-MT, FC-Or-Elim) → Check validity against SDL or epistemic logic → Filter through content priors (if applicable) → Generate entailment judgment
- Design tradeoffs:
  - SDL formalism vs. intuitive validity: Paper uses SDL as ground truth, but acknowledges paradoxes create legitimate ambiguity
  - Single-run evaluation (temperature=0): Ensures reproducibility but may underestimate model variance
  - Template-based generation: Controlled formal patterns but limited naturalness; real-world normative language is more varied
- Failure signatures:
  - **Mu-Mi pattern**: Systematic rejection of valid OA⇒PA inference (Table 12: GPT-4o Few-Shot achieves 100% on most valid patterns but shows inconsistency here)
  - **Negation-heavy patterns**: MT/DA accuracy 40-60% lower than MP/AC (Figure 3)
  - **CoT degradation**: Reasoning chains introduce errors rather than correcting them (Section 6: "Chain-of-Thought does not necessarily enhance robustness and may instead introduce additional points of failure")
  - **Content-contingent validity**: 20%+ accuracy gaps between congruent and incongruent conditions (Table 6)
- First 3 experiments:
  1. **Modal vocabulary ablation**: Replace "can choose to" with "permitted to" in Mu-Mi items; measure whether phrasing or formal structure drives failure.
  2. **Negation depth scaling**: Systematically vary negation count (0, 1, 2) across logically equivalent forms; test hypothesis that negation complexity, not validity, predicts difficulty.
  3. **Content-constraint decoupling**: Use nonsense content with manipulated plausibility cues (e.g., "It is obligatory to flibbertigibbet" framed as social vs. nonsocial context); isolate content effects from formal structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to correctly infer permission from obligation (Mu-Mi pattern), given that even the best models fail this basic inference while accepting its logically equivalent contrapositive?
- Basis in paper: [explicit] The authors state: "The reasoning from obligation to permission (Mu-Mi) did not align with expected validity despite its simple form. This can lead to logical inconsistency, as LLMs tend to accept its logically equivalent contrapositive (NotMi-NotMu) as valid reasoning."
- Why unresolved: The paper identifies the problem—models misinterpret permission expressions like "can choose to" as indicating optionality rather than permission—but does not propose or test interventions to address this semantic misunderstanding.
- What evidence would resolve it: Experiments testing whether fine-tuning on permission-related paraphrases, or explicit instruction about the logical relationship between obligation and permission, improves Mu-Mi accuracy without degrading other patterns.

### Open Question 2
- Question: Under what conditions does chain-of-thought prompting improve versus harm normative reasoning, and what types of intermediate reasoning errors are introduced?
- Basis in paper: [explicit] The authors report: "Chain-of-Thought prompting generally produced minimal improvement or even negative effects... A common issue was the introduction of errors in intermediate reasoning steps, which led to incorrect final conclusions."
- Why unresolved: The paper documents the phenomenon but does not systematically characterize which reasoning patterns or content types are most susceptible to CoT-induced errors, nor whether alternative prompting strategies mitigate this.
- What evidence would resolve it: Fine-grained analysis of CoT outputs across all reasoning patterns, categorizing error types, combined with experiments testing modified CoT prompts designed to reduce intermediate reasoning failures.

### Open Question 3
- Question: Why does the relative difficulty of normative versus epistemic reasoning vary across tasks for LLMs, unlike the consistent human pattern where normative reasoning is easier?
- Basis in paper: [explicit] The authors state: "In the Syllogistic task, models perform comparably or better on normative problems... In contrast, in the Deontic Logic task, normative problems are more challenging than epistemic problems."
- Why unresolved: The paper observes the task-dependent pattern but does not explain what linguistic, structural, or training-related factors drive this reversal compared to human cognitive patterns.
- What evidence would resolve it: Ablation studies varying specific linguistic features (modal expressions, sentence structure) and content types across tasks, combined with probing experiments examining whether model representations differ systematically between normative and epistemic domains.

## Limitations
- Template-based problem generation may not capture real-world normative discourse complexity
- Single-run evaluation protocol may underestimate model variance and stability
- Binary classification masks potential gradations in model confidence and reasoning quality

## Confidence
- **High Confidence**: The general finding that LLMs exhibit human-like content effects in normative reasoning is well-supported by the data and consistent with prior work on cognitive biases in LLM outputs.
- **Medium Confidence**: The specific claim that few-shot prompting improves accuracy through structural alignment rather than genuine logical inference is plausible but requires further validation with controlled ablation studies.
- **Medium Confidence**: The identification of specific failure patterns (Mu-Mi, negation-heavy patterns) is methodologically sound, though the underlying causes require additional investigation.

## Next Checks
1. **Modal Vocabulary Ablation**: Replace "can choose to" with "permitted to" in Mu-Mi items to determine whether phrasing or formal structure drives the observed failure in OA⇒PA inferences.
2. **Negation Depth Scaling**: Systematically vary negation count across logically equivalent forms to test whether negation complexity, rather than validity, predicts difficulty in MT/DA patterns.
3. **Content-Constraint Decoupling**: Use nonsense content with manipulated plausibility cues to isolate content effects from formal structure, distinguishing between content priors and task framing influences.