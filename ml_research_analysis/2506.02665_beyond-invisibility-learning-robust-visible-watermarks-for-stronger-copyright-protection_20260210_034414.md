---
ver: rpa2
title: 'Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright
  Protection'
arxiv_id: '2506.02665'
source_url: https://arxiv.org/abs/2506.02665
tags:
- watermark
- image
- harvim
- watermarks
- visible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HARVIM, a new copyright protection method
  that learns visible watermarks that are hard to remove. Unlike previous attack-based
  approaches that rely on adversarial perturbations, HARVIM formulates watermark removal
  as an inverse problem and learns to place watermarks in regions that are intrinsically
  difficult to reconstruct.
---

# Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection

## Quick Facts
- arXiv ID: 2506.02665
- Source URL: https://arxiv.org/abs/2506.02665
- Reference count: 40
- Primary result: HARVIM learns visible watermarks that resist removal by flow-based and diffusion-based methods, with significantly lower reconstruction quality compared to random watermarks.

## Executive Summary
This paper introduces HARVIM, a new copyright protection method that learns visible watermarks that are hard to remove. Unlike previous attack-based approaches that rely on adversarial perturbations, HARVIM formulates watermark removal as an inverse problem and learns to place watermarks in regions that are intrinsically difficult to reconstruct. The method uses a bi-level optimization framework with a generative prior to guide the watermark learning process, and an approximation algorithm to make the optimization tractable. Experiments show that HARVIM successfully learns watermarks that resist both flow-based and diffusion-based removal methods, with significantly lower reconstruction quality compared to random watermarks.

## Method Summary
HARVIM formulates watermark learning as a bi-level optimization problem where the inner loop finds optimal reconstruction of watermarked regions and the outer loop adjusts watermark placement to maximize reconstruction error. The method uses a pretrained RealNVP flow model as generative prior and a conditional VAE watermark generator. A key innovation is the approximation algorithm that makes the bi-level optimization tractable through iterative λ warmstarting and unrolled gradient steps. The framework maximizes the discrepancy between optimal reconstruction and original content by placing watermarks in regions rich in visual detail that generative models struggle to inpaint.

## Key Results
- HARVIM reduces PSNR by 1.89-5.44 points compared to random watermarks when using Flow-R for removal on CelebA images
- The method successfully transfers to different image distributions (ImageNet, Cartoon) and removal techniques (RePaint diffusion-based method)
- HARVIM demonstrates strong resistance to both flow-based and diffusion-based removal methods while maintaining visual prominence for copyright protection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Placing watermarks on "hard-to-reconstruct" regions maximizes removal difficulty across diverse removal methods.
- Mechanism: HARVIM identifies image regions rich in visual detail (e.g., hair-background boundaries, textured areas) that generative models struggle to inpaint. By formulating watermark removal as an inverse inpainting problem, it learns watermark placement to maximize the gap between optimal reconstruction and original content.
- Core assumption: The hard-to-reconstruct region is an intrinsic property of the image, independent of any specific generative prior or removal architecture.
- Evidence anchors:
  - [abstract] "...our framework maximizes the discrepancy between the optimal reconstruction and the original content."
  - [Section 2.3] "HARVIM seeks an m that makes xT hard to reconstruct from observation y(m)."
  - [Section 3.3] "HARVIM placed watermarks along the boundaries between human hair and the background."
  - [corpus] Weak direct evidence; neighbor papers focus on watermark removal attacks, not proactive robust placement.
- Break condition: If a removal method uses a fundamentally different prior (not trained on real images), the intrinsic difficulty property may not transfer.

### Mechanism 2
- Claim: Visible watermarks provide universal protection against both direct misuse and AI-assisted misuse, unlike invisible adversarial perturbations.
- Mechanism: Visible watermarks render images commercially unusable directly and cause AI personalization (e.g., DreamBooth) to learn watermark patterns as part of the concept, degrading output quality. This does not require knowledge of specific attack vectors.
- Core assumption: AI personalization models will inevitably learn prominent visual patterns present in training data (backdoor mechanism effect).
- Evidence anchors:
  - [abstract] "...visible watermarks which are more robust to distortion attacks and provide protection against both direct misuse and AI-assisted misuse (e.g., through DreamBooth)."
  - [Section 2.2] "...DreamBooth learns watermark patterns from watermarked training images, leading to unusable outputs."
  - [Section 2.2] "Visible watermarks remain resilient to strong distortion attacks JPEG compression and Gaussian blur..."
  - [corpus] Neighbor papers confirm invisible watermarks are vulnerable to removal; none contradict visible watermark robustness claims.
- Break condition: If an attacker uses manual cropping/inpainting with human-in-the-loop, protection degrades to a labor cost barrier rather than computational hardness.

### Mechanism 3
- Claim: Meta-learning approximation with iterative λ warmstarting enables tractable bi-level optimization.
- Mechanism: The exact bi-level problem is NP-hard. HARVIM uses an iterative approach: increment λ, take K gradient steps from the previous solution, and update watermark parameters. This leverages local convexity properties of flow-based priors to keep K small.
- Core assumption: Flow-based generative priors preserve local convexity around solutions when λ and y change incrementally.
- Evidence anchors:
  - [abstract] "An approximate solution is developed to handle the intractable bi-level optimization."
  - [Section 2.4] "...log pG(x | y; λ) is locally convex at x∗(y; λ) when λ′ is close enough to λ."
  - [Section 2.4, Algorithm 1] Iterative update of λt and ˜xt from previous round.
  - [corpus] No direct evidence in neighbors; bi-level solvers for watermarking are novel per this work.
- Break condition: If K is too small or λ steps are too large, approximation error propagates and watermark quality degrades.

## Foundational Learning

- Concept: **Bi-level Optimization**
  - Why needed here: HARVIM's core formulation is bi-level: inner loop finds optimal reconstruction, outer loop adjusts watermark to maximize reconstruction error. Understanding gradient flow through inner-loop solutions is essential.
  - Quick check question: Can you explain why computing ∇_m x*(m) requires implicit differentiation or unrolling?

- Concept: **Normalizing Flows as Generative Priors**
  - Why needed here: HARVIM uses RealNVP (a flow model) pretrained on CelebA as the generative prior for both optimization and evaluation. Flow models enable exact likelihood computation, which is critical for the MAP formulation.
  - Quick check question: Why can flow models compute exact log-likelihood while GANs cannot?

- Concept: **Inverse Problems / Image Inpainting**
  - Why needed here: Watermark removal is framed as an inpainting inverse problem: reconstruct masked pixels from observed pixels using a generative prior. Understanding MAP estimation with priors is foundational.
  - Quick check question: In y = Ax + e, what does the diagonal binary matrix A represent for inpainting?

## Architecture Onboarding

- Component map:
  Watermark Generator -> Differentiable Mask A_m -> Bi-level Solver -> Generative Prior G -> Evaluation Suite
  (Watermark Generator outputs m, which is masked by A_m to create y(m); Bi-level Solver optimizes m using G as prior; Evaluation Suite tests removal methods)

- Critical path:
  1. Initialize: Random watermark m_0, λ_0 = 0, initial solution ˜x_0 via MLE.
  2. Loop until λ reaches target: (a) Increment λ; (b) Run K gradient steps on ˜x from previous solution; (c) Compute loss s(˜x, x_T) + R(m); (d) Backprop through unrolled K steps to update m.
  3. Output: Final learned watermark placement and pattern.

- Design tradeoffs:
  - **K (unroll steps)**: Small K (1-2) reduces compute but risks approximation error. Paper uses K=1.
  - **R(m) regularization strength**: Too strong → watermark disappears; too weak → watermark covers entire image. Paper uses 0.001 coefficient.
  - **Prior selection**: Flow prior (RealNVP) enables the iterative solver; diffusion priors would not have the same local convexity guarantees. Tradeoff is lower image quality vs. optimization tractability.

- Failure signatures:
  - **Watermark covers whole image**: R(m) coefficient too low.
  - **Watermark disappears**: R(m) coefficient too high.
  - **Poor transfer to out-of-distribution images**: Prior G not sufficiently general; consider larger/broader training data.
  - **SLBR/DeNet fail to detect watermark**: Expected behavior—blind removers fail on novel watermark styles, which is a success mode.

- First 3 experiments:
  1. **Sanity check on CelebA**: Train with random watermark initialization. Verify that PSNR degradation (vPSNR) is larger for HARVIM-learned vs. random watermarks against Flow-R.
  2. **Ablate K**: Run with K=1, 3, 5. Check if reconstruction quality improves at higher K and whether outer-loop gradients remain stable.
  3. **Transfer test**: Apply learned watermarks to ImageNet/Cartoon images. Measure vPSNR against RePaint (different architecture). Confirm degradation persists despite prior mismatch.

## Open Questions the Paper Calls Out

- Can HARVIM be adapted into a training-free framework that embeds open-ended text watermarks while maintaining real-time efficiency?
  - Basis in paper: [Explicit] The authors explicitly state in the Conclusion a desire to "explore a training-free version of Harvim that embeds open-ended text watermarks... to enhance real-time efficiency."
  - Why unresolved: The current implementation requires an iterative bi-level optimization process to learn the watermark, which is computationally intensive compared to single-pass methods.
  - Evidence would resolve it: A modified HARVIM algorithm capable of generating robust watermarks for arbitrary text inputs in a single forward pass without per-image training iterations.

- What is the theoretical minimum distortion (masking) required to guarantee that a watermark is provably unremovable?
  - Basis in paper: [Explicit] The Conclusion lists "determining the minimum distortion (masking) required to ensure a watermark is provably unremovable" as a primary direction for theoretical investigation.
  - Why unresolved: The paper currently relies on empirical optimization to maximize reconstruction error but lacks formal lower bounds on the masking intensity needed to mathematically ensure reconstruction failure.
  - Evidence would resolve it: A formal proof or bound defining the smallest watermark area or opacity level necessary to make the inverse problem ill-posed for any solver.

- What is the maximum tolerable noise or perturbation under which a personalized concept can still be provably learned by models like DreamBooth?
  - Basis in paper: [Explicit] The Conclusion proposes investigating the "maximum tolerable noise or perturbation under which a personalized concept can still be provably learned."
  - Why unresolved: While the paper empirically demonstrates that watermarks disrupt DreamBooth training, it does not characterize the theoretical breaking point of the fine-tuning process regarding noise levels.
  - Evidence would resolve it: A theoretical characterization of the noise threshold for personalization algorithms, defining the boundary between successful concept learning and failure.

## Limitations

- The bi-level optimization approximation assumes local convexity of the flow-based prior; performance on diffusion-based priors (RePaint) is weaker and not fully explained by the theoretical framework.
- Blind watermark removal methods (SLBR, DeNet) are intentionally outperformed by HARVIM, but this represents a deliberate vulnerability to novel watermark styles rather than a limitation.
- Transferability to out-of-distribution domains depends heavily on the generality of the RealNVP prior; performance may degrade significantly with domain shift.

## Confidence

- **High Confidence**: Visible watermark robustness against flow-based removal methods; effectiveness of bi-level optimization framework on CelebA distribution.
- **Medium Confidence**: Transferability to ImageNet and Cartoon datasets; performance against diffusion-based RePaint method.
- **Low Confidence**: Generalization to fundamentally different priors (e.g., GAN-based); behavior under manual human-in-the-loop inpainting attacks.

## Next Checks

1. Test HARVIM-learned watermarks against a GAN-based generative prior to verify claims about intrinsic difficulty vs. prior-specific optimization.
2. Evaluate HARVIM against manual human-in-the-loop inpainting on watermarked regions to quantify protection beyond computational barriers.
3. Vary the RealNVP training data (e.g., train on ImageNet instead of CelebA) to measure sensitivity of watermark placement quality to prior domain.