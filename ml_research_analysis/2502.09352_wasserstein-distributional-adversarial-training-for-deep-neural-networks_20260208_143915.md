---
ver: rpa2
title: Wasserstein distributional adversarial training for deep neural networks
arxiv_id: '2502.09352'
source_url: https://arxiv.org/abs/2502.09352
tags:
- adversarial
- training
- networks
- network
- distributional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes methods to train deep neural networks against
  distributional adversarial attacks, extending the TRADES approach used for pointwise
  attacks. The authors introduce a fine-tuning method that leverages sensitivity analysis
  for Wasserstein distributionally robust optimization problems.
---

# Wasserstein distributional adversarial training for deep neural networks

## Quick Facts
- **arXiv ID:** 2502.09352
- **Source URL:** https://arxiv.org/abs/2502.09352
- **Authors:** Xingjian Bai; Guangyi He; Yifan Jiang; Jan Obloj
- **Reference count:** 14
- **Primary result:** Proposed fine-tuning method improves Wasserstein distributional robustness while maintaining pointwise robustness for pre-trained models on RobustBench.

## Executive Summary
This paper extends TRADES adversarial training to Wasserstein distributional robustness optimization (W-DRO) for deep neural networks. The authors introduce a fine-tuning method that uses sensitivity analysis to approximate the W-DRO problem without solving infinite-dimensional measure optimization. The approach leverages a first-order approximation of the sensitivity parameter Υ to scale attack budgets per batch, making distributional training computationally tractable. Experiments show the method effectively improves W2 robustness across various pre-trained models while preserving W∞ robustness.

## Method Summary
The method fine-tunes pre-trained neural networks using W-PGD-Budget attacks with ReDLR loss for distributional attack generation. The core innovation is a sensitivity-based approach that estimates a global parameter Υ from 10% of training data, which is then used to allocate per-batch attack budgets. During training, the model parameters are updated using cross-entropy loss while adversarial examples are generated using the ReDLR loss. The fine-tuning process uses SGD with learning rate 0.01 (or 0.1 for last-layer-only) for up to 20 epochs, selecting the best model based on validation performance.

## Key Results
- Fine-tuning improves W2 adversarial accuracy by 2-8% across multiple pre-trained models while maintaining W∞ accuracy
- Whole-network perturbation with small noise initialization achieves best results (W2 accuracy ~50%, W∞ accuracy ~60%)
- Last-layer-only fine-tuning preserves pointwise robustness but yields smaller distributional gains
- Performance improvements are most pronounced for models trained on smaller datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: First-order sensitivity approximation enables tractable Wasserstein distributional robustness optimization without solving infinite-dimensional measure optimization.
- Mechanism: Theorem 4.1 shows that V(δ) ≈ V(0) + δΥ, where Υ = (E_P[‖∇x' J_θ(x,x,y)‖₁²])^1/2. This converts the sup over probability measures π∈Π₂(P,δ) into a tractable gradient-based perturbation direction T(x) = Υ⁻¹ sgn(∇x' J_θ)‖∇x' J_θ‖₁. The attack budget δ is distributed proportionally to local gradient magnitudes.
- Core assumption: The loss function (x,x',y) ↦ J_θ(x,x',y) is Lipschitz (weaker than L-smoothness required in prior work like Sinha et al. 2018).
- Evidence anchors:
  - [Theorem 4.1, page 4]: Explicit first-order approximation formula with error term o(δ)
  - [page 4]: "In Bai et al. (2023), a sensitivity approach is adapted to address the above issue. Instead of finding the exact value of V(δ), a first order approximation to V(δ) in the size of attack budget δ is derived."
  - [corpus]: Related work "Tight Robustness Certificates and Wasserstein Distributional Attacks" confirms sensitivity-based approaches are active research direction but notes global Lipschitz methods can yield loose bounds—this paper's first-order approach is distinct.
- Break condition: If loss landscape becomes highly non-Lipschitz (sharp curvature), the o(δ) error term dominates and approximation fails. Empirical signal: Υ estimates become highly volatile across samples.

### Mechanism 2
- Claim: ReDLR loss for attack generation focuses adversarial budget on correctly classified examples, improving distributional attack efficiency compared to standard DLR or KL divergence.
- Mechanism: ReDLR(z,y) = -(DLR)^-(z,y) returns 0 when image is misclassified, leaving those samples unattacked. This allows the distributional attacker to allocate more perturbation budget to vulnerable images far from decision boundaries rather than wasting budget on already-misclassified samples. Attack step uses W-PGD-20 with ReDLR; training step uses CE loss.
- Core assumption: Distributional attacks benefit from budget concentration on decision-boundary-proximate samples; misclassified samples provide no training signal for robustness improvement.
- Evidence anchors:
  - [page 5]: "ReDLR loss is flat when the image is misspecified. We keep L and L̃ as the cross-entropy for this step."
  - [page 3]: "In a key difference with the DLR, the ReDLR loss leaves misclassified images unaffected. This allows distributional attacker to 'save' their budget for images which actually need to be attacked."
  - [corpus]: No direct corroboration found—this is a paper-specific claim from Bai et al. (2023).
- Break condition: If most training samples are misclassified (very weak model), ReDLR provides near-zero attack signal. Switch to CE-based attacks if ReDLR attack success rate < 10%.

### Mechanism 3
- Claim: W-PGD-Budget with 10% sampling and moving-average Υ estimation makes global Wasserstein constraints computationally tractable during training.
- Mechanism: Υ is global (depends on full training set) and must be recomputed after each θ update. Solution: (1) Sample 10% of training set to estimate Ŷ, (2) Update Υ ← Υ + η(Ŷ - Υ) with η=0.1 or 1, (3) Compute local batch budget δ_B = δ·Υ_B/Υ where Υ_B is batch-local sensitivity. This decomposes global constraint into per-batch feasible attacks.
- Core assumption: 10% sampling provides sufficient Υ estimation accuracy; batch-level budget allocation approximates global Wasserstein constraint when θ changes slowly.
- Evidence anchors:
  - [Algorithm 1, page 6]: Full pseudocode showing sampling, Υ update, and δ_B computation
  - [Figure 7.1, page 7]: Empirical validation that 10% sampling yields Υ estimates close to reference values, especially for robust networks
  - [corpus]: "DRO-Augment Framework" paper synergizes WDRO with data augmentation but doesn't address computational tractability—this sampling approach appears novel.
- Break condition: If Υ variance across 10% samples is >20% of mean (Figure 7.1 shows this can happen for less robust networks), increase sampling ratio or use η<1 for stability.

## Foundational Learning

- Concept: **Wasserstein Distance W_p(P,Q)**
  - Why needed here: The threat model is defined by Wasserstein balls B_δ(P) around data distribution P. Unlike KL divergence, Wasserstein distance respects geometry of input space and allows perturbations between distributions with different supports.
  - Quick check question: Given two image distributions, would W_∞(P,Q) < δ guarantee all samples from Q are within δ l_∞-distance of some sample from P? (Yes—this is why W_∞ corresponds to pointwise threat model.)

- Concept: **TRADES (TRadeoff-inspired Adversarial DEfense)**
  - Why needed here: This paper extends TRADES from pointwise (l_∞) to distributional (W_2) threats. Understanding the original formulation—min_θ[E_P[L(f_θ(x),y)] + β·sup_{x'} L(f_θ(x), f_θ(x'))]—is prerequisite to understanding the W-DRO generalization in Eq. (5.1).
  - Quick check question: What does hyperparameter β control in TRADES? (Trade-off between clean accuracy and robustness—higher β prioritizes robustness.)

- Concept: **Min-Max Adversarial Training Formulation**
  - Why needed here: All methods (PGD-AT, TRADES, MART, this paper) share inner maximization (find worst-case attack) + outer minimization (update network parameters). The inner sup over π∈Π₂(P,δ) in Eq. (4.2) is the Wasserstein generalization.
  - Quick check question: Why is the inner maximization over probability measures π (couplings) rather than individual perturbations x'? (Distributional threats allow reallocating perturbation budget across samples—some images get larger perturbations, others smaller, subject to expected cost constraint.)

## Architecture Onboarding

- Component map: Pre-trained Model (θ) → Add noise/randomize last layer → Training Loop (≤20 epochs) → Sample 10% training data → Estimate Ŷ → Update global Υ (moving average) → For each batch: Compute local budget δ_B = δ·Υ_B/Υ → W-PGD-20 attack with ReDLR loss → Generate x'_adv → SGD update θ using CE loss on (x, x'_adv, y) → Validate on held-out set → Select best epoch

- Critical path: Accurate Υ estimation → Correct δ_B allocation → Effective W-PGD attacks → Meaningful gradient signal. If Υ is misestimated, batch budgets are wrong and training collapses.

- Design tradeoffs:
  - **Fine-tune last layer only vs. whole network**: Last-layer-only preserves pointwise robustness but limited distributional gains (Table 3 shows W₂ actually decreases). Whole-network with small noise achieves best results (Table 4) but risks catastrophic forgetting.
  - **η=1 (current estimate) vs. η=0.1 (smoothed)**: η=1 consistently outperforms in experiments despite higher variance—faster adaptation to changing θ dominates stability concerns.
  - **Learning rate τ=0.1 vs. 0.01**: Last-layer fine-tuning prefers τ=0.1; whole-network requires τ=0.01 to avoid destroying pretrained features.

- Failure signatures:
  - **W_∞ accuracy drops significantly** (e.g., Gowal et al. from 66.44→63.39): Model is trading pointwise for distributional robustness. Reduce β or use combined objective (5.6).
  - **Υ oscillates wildly across epochs** (Figure 7.4 shows 20→160 range): Learning rate too high or batch size too small. Reduce τ or increase batch size.
  - **Clean accuracy collapses** (Tables 9, 11 show drops to 57-66%): Whole-network perturbation too aggressive. Use smaller initial noise or switch to last-layer-only.

- First 3 experiments:
  1. **Baseline validation on Zhang et al. (2019)**: Reproduce Table 4 results (whole-network, τ=0.01, η=1) to verify pipeline correctness. Expected: ~60% W_∞, ~50% W₂.
  2. **Ablation on Υ sampling ratio**: Test 5%, 10%, 20% sampling to quantify variance-accuracy tradeoff. Monitor Υ estimate stability and final W₂ accuracy.
  3. **Combined objective feasibility test**: Implement Eq. (5.6) with small γ (e.g., 1.0) to test whether pointwise robustness can be preserved on Gowal et al. while still gaining distributional robustness. Compare W_∞ drop magnitude vs. β-only training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the combined objective (5.6) be effectively optimized to train neural networks from scratch, balancing both pointwise and distributional robustness without prohibitive computational cost?
- Basis in paper: [explicit] The authors state that minimizing the combined objective "would involve a very significant computational effort and we leave it for future research" (Page 5).
- Why unresolved: The current study focuses on fine-tuning pre-trained models using a simplified objective because the full min-max problem over both $W_2$ and $W_\infty$ threats is computationally intensive.
- What evidence would resolve it: A demonstration of a training regime that converges using the combined loss function (5.6) within a reasonable timeframe, achieving high accuracy on both pointwise and distributional attack benchmarks compared to fine-tuning methods.

### Open Question 2
- Question: How can the estimation of the global sensitivity parameter $\Upsilon$ be stabilized or reformulated to reduce volatility and computational burden on massive datasets?
- Basis in paper: [explicit] The authors note that "the training curve of $\Upsilon$ can be fairly volatile" and calculating it requires sampling 10% of the dataset, which would be infeasible for massive synthetic datasets (Page 10).
- Why unresolved: The current method relies on a moving average and subsampling, but the volatility suggests the sensitivity estimation might be noisy, and the subsampling strategy does not scale linearly to datasets of 100M images.
- What evidence would resolve it: Development of an estimator for $\Upsilon$ that converges faster or relies on local batch statistics without significant loss of attack efficacy, validated on datasets larger than CIFAR-10.

### Open Question 3
- Question: Can fine-tuning strategies be modified to prevent the observed degradation in pointwise robustness ($W_\infty$) when applied to models pre-trained on massive synthetic datasets?
- Basis in paper: [explicit] The authors observe that models pre-trained on huge datasets (Cui et al., Wang et al.) exhibit a "small decrease in $W_\infty$ (pointwise) robustness" after the proposed fine-tuning (Page 10).
- Why unresolved: The current fine-tuning method optimizes for distributional robustness, which appears to conflict with the features learned during massive-scale pre-training for pointwise robustness.
- What evidence would resolve it: A modified fine-tuning objective or regularization term that improves $W_2$ robustness on these specific models while maintaining or improving the baseline $W_\infty$ robustness.

### Open Question 4
- Question: Is the first-order approximation of the W-DRO problem accurate for larger perturbation budgets $\delta$, or does the $o(\delta)$ error term become significant?
- Basis in paper: [inferred] The method relies on Theorem 4.1, a first-order approximation derived for small $\delta$. The experiments strictly use $\delta=8/255$.
- Why unresolved: While effective for standard image perturbations, the theoretical guarantees and empirical efficacy are untested for larger distributional shifts where higher-order terms might dominate.
- What evidence would resolve it: A theoretical analysis of the error bounds for larger $\delta$ or empirical results showing the method's failure or success modes as the Wasserstein radius increases significantly.

## Limitations
- The sensitivity approximation assumes Lipschitz continuity of the loss function, which may not hold for highly non-smooth neural network landscapes
- ReDLR loss mechanism lacks external validation beyond the paper's own claims
- Computational tractability relies on sampling assumptions that may not generalize to larger datasets or more complex architectures
- Performance improvements are less marked for models pre-trained using massive synthetic datasets

## Confidence
- **High:** Experimental results showing improved W2 robustness on pre-trained models (Tables 3-4, 9-11)
- **Medium:** Sensitivity analysis approximation (Theorem 4.1 derivation appears sound, but error bounds need empirical validation)
- **Medium:** ReDLR loss effectiveness for distributional attacks (paper-specific claim with limited external verification)
- **Low:** Computational tractability claims (sampling ratio effects not fully explored across architectures)

## Next Checks
1. **Lipschitz verification:** Test sensitivity approximation breakdown by evaluating loss landscape curvature on models with known sharp features (e.g., ReLU networks with low width)
2. **ReDLR ablation:** Compare ReDLR-based attacks against standard DLR attacks on the same models to quantify the claimed budget efficiency improvement
3. **Sampling ratio sweep:** Systematically vary the 10% sampling ratio from 5% to 50% to measure impact on Υ estimate variance and final W2 accuracy across multiple architectures