---
ver: rpa2
title: Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization
  of Prolonged Exposure Therapy Elements
arxiv_id: '2506.09707'
source_url: https://arxiv.org/abs/2506.09707
tags:
- fidelity
- therapy
- lora
- exposure
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating therapist fidelity
  in Prolonged Exposure (PE) therapy for PTSD, which traditionally requires labor-intensive
  manual review of session recordings. The authors present a method for automatically
  localizing key PE therapy elements by fine-tuning a large pre-trained audio-language
  model (Qwen2-Audio) using Low-Rank Adaptation (LoRA) to process focused 30-second
  windows of audio-transcript input.
---

# Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements

## Quick Facts
- arXiv ID: 2506.09707
- Source URL: https://arxiv.org/abs/2506.09707
- Reference count: 0
- Primary result: 5.3s mean absolute error on temporal localization of PE therapy elements

## Executive Summary
This paper addresses the challenge of evaluating therapist fidelity in Prolonged Exposure (PE) therapy for PTSD, which traditionally requires labor-intensive manual review of session recordings. The authors present a method for automatically localizing key PE therapy elements by fine-tuning a large pre-trained audio-language model (Qwen2-Audio) using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. The model predicts normalized temporal offsets for three core protocol phases: therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3). Using a soft-supervision pipeline with LLM-based prompting and rater verification, the approach achieves a mean absolute error (MAE) of 5.3 seconds across tasks on a dataset of 308 real PE sessions, which falls within typical rater tolerance for timestamp review.

## Method Summary
The approach uses Qwen2-Audio-7B-Instruct with QLoRA (4-bit NF4 quantization) to fine-tune for temporal localization. Fixed-duration windows (30s, 60s, 120s) are sampled around annotated boundaries, with normalized offset targets in [0,1]. LoRA adapters (ranks 2, 4, 8) are applied to the LLM backbone while the encoder remains frozen. A regression head predicts the offset position. Training uses AdamW optimizer with cosine learning rate schedule, batch size 1, and early stopping. Labels come from LLM prompting verified by human raters.

## Key Results
- 30-second windows achieve 5.3s MAE, outperforming 60s (12.2s) and 120s (25.2s) windows
- LoRA fine-tuning with rank 8 significantly outperforms training only a regression head
- Audio-text representations capture prosodic transitions missed by text-only approaches
- Performance generalizes across different session structures and therapist styles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shorter input windows (30s) yield more precise temporal localization than longer windows (60s, 120s).
- **Mechanism:** The model must predict a normalized offset within each window. Larger windows dilute the temporal sharpness required for boundary detection—assigning a position in [0,1] across 120 seconds is inherently less precise than across 30 seconds, even with equivalent model capacity.
- **Core assumption:** Therapeutic phase transitions are local phenomena; extended context provides diminishing returns for boundary detection beyond a critical window.
- **Evidence anchors:**
  - [abstract] "Our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3s"
  - [Section 6] "30s window emerges as the optimal balance... P1 Avg MAE increased from 6.8±0.1s (30s) to 12.2±0.8s (60s) and 25.2±2.2s (120s)"
  - [corpus] No direct corpus evidence on window size effects; this appears novel to this work.
- **Break condition:** If therapy phase transitions require semantic context beyond 30 seconds (e.g., multi-turn conversational cues), the precision gain will be offset by context loss.

### Mechanism 2
- **Claim:** Joint audio-text representations enable detection of phase transitions that text-only approaches miss.
- **Mechanism:** Phase transitions in therapy often manifest in prosodic shifts, pauses, or speaker tone changes that occur between words. Text-only models, constrained by ASR timestamp granularity, cannot access these continuous signals.
- **Core assumption:** Therapeutic boundaries are marked by acoustic features (silence, prosody) in addition to lexical content.
- **Evidence anchors:**
  - [Section 5] "therapy phase transitions often occur in the silences or prosodic shifts between words, signals that text-only models inherently discard"
  - [Section 6] "Earlier attempts at feature-level fusion using independent audio and text encoders produced overlapping embeddings across therapeutic phases"
  - [corpus] Weak; no direct corpus papers compare audio-text fusion for temporal localization in therapy.
- **Break condition:** If phase transitions are purely lexical (explicit verbal markers), audio adds noise without signal.

### Mechanism 3
- **Claim:** Low-rank adaptation (LoRA) with appropriate rank improves over training only a regression head, but optimal rank depends on window size.
- **Mechanism:** LoRA enables efficient adaptation of the frozen LLM backbone, allowing the model to learn domain-specific representations. Higher ranks (r=8) help when input context is moderate (30-60s), but may overfit on broader windows (120s) where smaller ranks (r=2) generalize better.
- **Core assumption:** The pre-trained Qwen2-Audio model contains transferable representations that benefit from targeted adaptation rather than frozen feature extraction.
- **Evidence anchors:**
  - [Section 6] "LoRA fine-tuning generally outperformed the 'Head Only' baseline, especially at 60s and 120s"
  - [Section 6] "For 120s windows... higher LoRA ranks may overfit when the input spans become too broad"
  - [corpus] No corpus papers specifically address LoRA rank selection for audio-language temporal tasks.
- **Break condition:** If the target domain diverges too far from pre-training distribution, low-rank adaptation may be insufficient.

## Foundational Learning

- **Concept: Normalized temporal regression**
  - **Why needed here:** The model predicts boundary position as a continuous value in [0,1], not a discrete class. Understanding this formulation is essential for designing the loss function, data augmentation (random boundary placement within windows), and evaluation (denormalization to absolute timestamps).
  - **Quick check question:** Given a 30-second window starting at t=100s, if the model predicts offset 0.7, what is the absolute timestamp? (Answer: 100 + 0.7 × 30 = 121s)

- **Concept: Parameter-efficient fine-tuning (LoRA/QLoRA)**
  - **Why needed here:** Fine-tuning a 7B-parameter model with limited data and compute requires understanding how LoRA injects trainable low-rank matrices, how quantization (NF4, bfloat16) reduces memory, and how rank/alpha hyperparameters control adaptation capacity.
  - **Quick check question:** Why does LoRA use scaling factor α = 2r? What happens if α is too large relative to rank?

- **Concept: Soft supervision with LLM-based annotation**
  - **Why needed here:** Ground truth labels come from an LLM prompted on transcripts, then verified by human raters. Understanding this pipeline is critical for assessing label quality, designing verification protocols, and diagnosing annotation noise.
  - **Quick check question:** What types of errors might an LLM make when inferring timestamps from text alone, without audio?

## Architecture Onboarding

- **Component map:** Audio segment → Qwen2-Audio encoder → LoRA adapters → LLM backbone → Regression head → Normalized offset
- **Critical path:**
  1. Extract window centered on annotated boundary with random offset augmentation
  2. Prepare interleaved audio-transcript input with task prompt
  3. Forward pass through Qwen2-Audio with LoRA adapters
  4. Extract final hidden state of last non-padding token
  5. Regression head predicts normalized offset
  6. Loss = MAE between predicted and true normalized offset

- **Design tradeoffs:**
  - **Window size vs. precision:** 30s maximizes boundary precision but limits context; 120s provides semantic history at cost of localization accuracy
  - **LoRA rank vs. generalization:** Higher ranks (8) capture more domain detail but may overfit; lower ranks (2) generalize better on long windows
  - **Audio-text vs. text-only:** Audio captures prosody but adds computational cost and potential noise

- **Failure signatures:**
  - **High variance across seeds:** Suggests unstable training; try lower learning rate or higher dropout
  - **MAE scaling with window size:** Confirms context-granularity trade-off; reduce window or increase data augmentation
  - **P2 significantly worse than P1/P3:** Imaginal exposure is longest phase; may need phase-specific tuning

- **First 3 experiments:**
  1. **Reproduce baseline:** Train "Head Only" model (frozen backbone, only regression head) on 30s windows to isolate LoRA contribution
  2. **Window sweep:** Compare 30s/60s/120s windows with LoRA r=8 across all three phases to verify context-granularity tradeoff
  3. **Rank ablation:** For fixed 30s windows, compare r={2, 4, 8} to find optimal adaptation capacity for your data scale

## Open Questions the Paper Calls Out
- **Open Question 1:** Does the model generalize to PE therapy sessions from institutions and therapist populations not represented in the training data? (All 308 sessions from single institution)
- **Open Question 2:** What is the impact of the 10% verification rate on ground-truth label quality, and how would full human annotation affect model performance?
- **Open Question 3:** How robust is the model to sessions where therapy phases deviate from typical temporal structure (e.g., reordered phases or missing P1/P3)?
- **Open Question 4:** What is the marginal contribution of audio modality compared to transcript-only approaches for temporal localization precision?

## Limitations
- Soft-supervision pipeline introduces potential bias and error patterns from LLM prompting
- 5.3s MAE, while within rater tolerance, still represents meaningful temporal error
- Dataset of 308 sessions remains modest for fine-tuning a 7B-parameter model
- Window-based architecture assumes boundaries can be isolated within fixed-duration segments

## Confidence
**High Confidence** (supported by direct experimental evidence):
- Shorter windows (30s) yield better localization precision than longer windows (60s, 120s)
- LoRA fine-tuning outperforms training only a regression head
- Audio-text representations improve over text-only approaches for boundary detection

**Medium Confidence** (supported by ablation studies but with caveats):
- The 5.3s MAE represents clinically acceptable performance
- LoRA rank 8 provides optimal adaptation capacity for 30s windows
- Joint audio-text encoding captures prosodic cues missed by text-only models

**Low Confidence** (theoretical mechanisms with limited direct evidence):
- The diminishing returns of context beyond 30 seconds applies universally across therapeutic contexts
- LoRA's low-rank structure prevents overfitting in all cases
- The specific r=8 configuration would transfer to other audio-language temporal tasks

## Next Checks
1. **Cross-session evaluation**: Train and evaluate on non-overlapping sessions (rather than windows from the same session) to verify generalization beyond session-specific patterns.

2. **Error case analysis**: Manually review predictions with MAE > 10s to identify systematic failure modes (e.g., certain speaker combinations, background noise, or boundary types) and determine if these represent model limitations or annotation inconsistencies.

3. **Prompt ablation study**: Systematically vary the LLM prompting strategy (temperature, few-shot examples, prompt phrasing) to quantify the sensitivity of soft-supervision quality to prompt engineering choices.