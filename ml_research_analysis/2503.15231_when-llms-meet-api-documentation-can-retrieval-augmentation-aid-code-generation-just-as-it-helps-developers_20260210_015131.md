---
ver: rpa2
title: 'When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code Generation
  Just as It Helps Developers?'
arxiv_id: '2503.15231'
source_url: https://arxiv.org/abs/2503.15231
tags:
- code
- llms
- documents
- example
- libraries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether retrieval-augmented generation (RAG)
  can help large language models (LLMs) generate code using less common APIs by leveraging
  their documentation. It formulates API usage recommendation as a code completion
  task and evaluates RAG performance on four less common Python libraries (Polars,
  Ibis, GeoPandas, Ivy) with 1017 APIs.
---

# When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code Generation Just as It Helps Developers?

## Quick Facts
- arXiv ID: 2503.15231
- Source URL: https://arxiv.org/abs/2503.15231
- Authors: Jingyi Chen; Songqiang Chen; Jialun Cao; Jiasi Shen; Shing-Chi Cheung
- Reference count: 40
- Primary result: RAG improves LLM code generation by 83%-220% on less common APIs

## Executive Summary
This paper investigates whether retrieval-augmented generation can help large language models generate code using less common APIs by leveraging their documentation. The study formulates API usage recommendation as a code completion task and evaluates RAG performance on four less common Python libraries (Polars, Ibis, GeoPandas, Ivy) with 1017 APIs. Results show that RAG significantly improves LLM performance, with example code being the most critical documentation component. BM25 is identified as the most effective retriever, and LLMs can tolerate mild noise in documentation by referencing pre-trained knowledge or context.

## Method Summary
The researchers constructed a dataset of 1,017 APIs from four less common Python libraries (Polars, Ibis, GeoPandas, Ivy) by parsing official documentation into structured fields: Description, Parameters, and Example Code. They formulated API usage recommendation as a code completion task by masking invocation lines in example codes and providing the execution result as "Expected Output." The RAG pipeline uses BM25 retrieval to fetch the top-5 documents, which are appended to prompts given to LLMs (GPT-4o-mini, Qwen2.5-Coder-32B/7B, DeepSeek-Coder-V2). Performance is measured by pass rate (execution result string match) and retrieval effectiveness (Recall@k, MRR).

## Key Results
- RAG improves LLM performance by 83%-220% compared to no documentation
- Example code is the most critical documentation component for successful code generation
- BM25 is the most effective retriever for matching code completion queries to API documents
- LLMs can tolerate mild noise in documentation by referencing pre-trained knowledge or context

## Why This Works (Mechanism)

### Mechanism 1: Example-Driven In-Context Learning
The model treats code examples as few-shot prompts or structural templates, prioritizing syntactic patterns and variable mapping over semantic instructions. This aligns with how human developers prioritize copying working code. The core assumption is that the LLM has sufficient reasoning capability to map the input context to the provided example structure. Evidence shows that removing example code causes the largest drop in pass rates.

### Mechanism 2: Lexical Retrieval Alignment (BM25)
BM25 outperforms dense embedding retrieval for code completion tasks because code tokens have high specificity. The model exploits exact term frequency and inverse document frequency to match distinct technical identifiers, whereas dense embeddings may blur these specific technical identifiers into semantic vectors. This works when query and target document share significant lexical overlap.

### Mechanism 3: Contextual Noise Filtering
LLMs can generate correct code even when documentation contains mild noise by cross-referencing context or internal knowledge. The model validates provided information against code context and pre-trained priors. If a parameter is hallucinated in text but absent in example code, the model may deprioritize it. This works when noise is mild and contradicted by other valid evidence.

## Foundational Learning

- **Concept: Sparse vs. Dense Retrieval (BM25 vs. Embeddings)**
  - Why needed: To understand why BM25 beats embedding models for code where exact token matching matters more than semantic similarity
  - Quick check: If a user asks "how to sort a dataframe," would dense embedding or BM25 better find a document explicitly defining `df.sort()`?

- **Concept: Code Completion vs. Code Generation**
  - Why needed: The task is formulated as "code completion" (filling in a blank given context) rather than generating from scratch, providing grounding for the model
  - Quick check: Does the system require the user to write the function signature first, or does it generate the whole script based on a comment?

- **Concept: Context Window & "Lost in the Middle"**
  - Why needed: Understanding how LLMs attend to context helps explain why putting the right document in Top-5 is critical, but position matters
  - Quick check: If the correct API doc is the 4th of 5 retrieved documents, does the model attend to it as strongly as the 1st?

## Architecture Onboarding

- **Component map:** Knowledge Base (API Docs) -> Retriever (BM25) -> Generator (LLM) -> Evaluator (Execution-based validation)
- **Critical path:** Query Construction -> Retrieval -> Augmentation -> Generation
- **Design tradeoffs:** Example-Only vs. Full Doc (stripping descriptions saves tokens but might lose edge-case details); BM25 vs. Dense (BM25 wins on precision for known API names)
- **Failure signatures:** Blind Copying (copies example exactly even if parameters don't match); Hallucinated APIs (invents plausible but non-existent APIs if retrieval fails)
- **First 3 experiments:** Ablation on Content (run with only Description, Parameters, Example Code); Noise Injection (typo API name in text vs. example code); Retriever Shootout (benchmark BM25 against dense embeddings for "Code Context -> API Doc" pairs)

## Open Questions the Paper Calls Out

- Can enhanced reasoning capabilities enable LLMs to effectively distinguish and adapt mismatching code examples rather than simply copying incorrect patterns? (Basis: Authors suggest enhancing reasoning ability may help LLMs distinguish examples and learn API usage wisely)

- How can retrieval mechanisms be optimized to improve the ranking of correct API documents within the top-k context? (Basis: Authors note enhancing retrieval of proper documents remains interesting future work)

- To what extent do these findings generalize to less common libraries in non-Python ecosystems? (Basis: Authors acknowledge threat to validity regarding representativeness of subject libraries restricted to Python)

## Limitations

- The study is limited to four specific Python libraries, which may not generalize to broader or differently structured documentation
- Controlled noise experiments focus on minor typos and optional parameters, leaving uncertainty about performance with more severe documentation errors
- The claim that "examples dominate" is well-supported within this context, but the relative value of descriptions and parameters may shift for libraries with different documentation styles

## Confidence

- **High Confidence:** RAG improves LLM code generation performance (83%-220% gains measured), and example code is the most valuable documentation component
- **Medium Confidence:** BM25 is the optimal retriever for this task, and LLMs can tolerate mild documentation noise
- **Medium Confidence:** Findings about documentation structure prioritization apply beyond the tested libraries

## Next Checks

1. **Generalization Test:** Apply the same RAG pipeline to a different set of Python libraries (e.g., Plotly, SQLAlchemy) to verify that examples remain the dominant documentation component across documentation styles

2. **Noise Severity Boundary:** Systematically test the noise tolerance threshold by introducing progressively more severe documentation errors (wrong API names, incorrect parameter types) to identify where performance breaks down

3. **Retriever Robustness:** Benchmark BM25 against dense embeddings using a broader query distribution that includes more natural language descriptions rather than code-heavy queries to identify true boundary conditions