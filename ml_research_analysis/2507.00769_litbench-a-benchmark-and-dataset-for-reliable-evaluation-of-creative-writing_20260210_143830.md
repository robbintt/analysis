---
ver: rpa2
title: 'LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing'
arxiv_id: '2507.00769'
source_url: https://arxiv.org/abs/2507.00769
tags:
- writing
- creative
- human
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LitBench, a benchmark and dataset for reliable
  evaluation of creative writing generated by large language models (LLMs). The authors
  address the challenge of evaluating open-ended narratives due to lack of ground
  truths, and propose a solution using pairwise comparisons from Reddit's r/WritingPrompts.
---

# LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing

## Quick Facts
- **arXiv ID**: 2507.00769
- **Source URL**: https://arxiv.org/abs/2507.00769
- **Reference count**: 14
- **Primary result**: LitBench introduces a benchmark and dataset for reliable evaluation of creative writing generated by LLMs, achieving 78% accuracy with trained reward models versus 73% for the best zero-shot judge.

## Executive Summary
This paper addresses the challenge of evaluating open-ended creative writing generated by LLMs due to the lack of ground truth quality metrics. The authors propose LitBench, a benchmark built on pairwise comparisons from Reddit's r/WritingPrompts, consisting of 2,480 debiased human-labeled story comparisons paired with a 43,827-pair training corpus. The benchmark enables reliable evaluation of creative writing systems through preference fine-tuning, with Bradley-Terry and generative reward models achieving 78% accuracy on human preference prediction, outperforming all zero-shot LLM judges. The authors validate their reward models through an online human study and release LitBench as a vetted resource for automated evaluation and optimization of creative writing systems.

## Method Summary
The authors create LitBench by filtering story pairs from r/WritingPrompts based on minimum upvotes (≥10), significant upvote differences (≥25%), temporal constraints (later-posted story must be more popular), and length balancing. They train Bradley-Terry and generative reward models on 43,827 debiased pairwise preferences using standard fine-tuning procedures. The Bradley-Terry model learns a scalar reward function where probability of preference follows a sigmoid of reward difference, while generative models predict verdict tokens. For evaluation, zero-shot LLM judges use permutation-robust protocols averaging scores across both story orderings, while trained models are assessed against human-labeled test pairs and validated through online human studies with novel story pairs.

## Key Results
- Claude-3.7-Sonnet achieves 73% agreement with human preferences as the strongest off-the-shelf judge
- Bradley-Terry and generative reward models both attain 78% accuracy, outperforming all zero-shot judges
- Online human study confirms trained reward models consistently align with human preferences on novel LLM-generated stories
- Length debiasing reduces correlation between model predictions and story length from 0.58 to 0.02
- Chain-of-thought reasoning degrades generative reward model performance from 78% to 72% accuracy

## Why This Works (Mechanism)

### Mechanism 1: Preference Fine-Tuning for Creative Writing Evaluation
The paper demonstrates that training reward models on pairwise preference data improves evaluation accuracy for creative writing compared to zero-shot LLM judges. The Bradley-Terry model learns a scalar reward function where probability of preference follows a sigmoid of reward difference, directly optimizing for alignment with human choices. This approach assumes Reddit upvotes reliably proxy for human preferences regarding writing quality, though the authors acknowledge this is a significant assumption given other factors that can drive upvotes.

### Mechanism 2: Debiased Pairwise Data Curation
The authors implement a carefully filtered and debiased dataset of story pairs critical for training reliable reward models. They mitigate temporal bias by only pairing stories where the more popular one was posted later, and mitigate length bias by balancing the dataset so the chosen story isn't always the longer one. This curation assumes length and temporal exposure are primary confounding variables that would otherwise dominate the learned reward signal.

### Mechanism 3: Limitations of Chain-of-Thought for Generative Reward Models
The paper reveals that adding chain-of-thought reasoning to generative reward models for creative writing degrades performance compared to models that judge directly. The authors hypothesize that while CoT helps in logical domains, for subjective creative evaluation it introduces "textual noise" where reasoning may be post-hoc rationalization rather than causal explanation, confusing the model's training.

## Foundational Learning

- **Bradley-Terry Model**: A statistical model that converts pairwise preference data into scalar reward scores. Understanding this is essential for knowing what the reward model optimizes for. *Quick check*: Given pairwise comparisons, how does Bradley-Terry assign a single "skill" score to each item?

- **Reward Hacking**: When models learn to exploit proxy metrics rather than true objectives. This makes dataset quality crucial when training reward models. *Quick check*: What is reward hacking and why does it make bias mitigation essential in reward model training?

- **Position Bias in LLM-as-a-Judge**: The tendency of models to favor the first or second item in a comparison. The paper's evaluation protocol accounts for this by averaging scores over two permutations. *Quick check*: What is position bias and how does the paper's methodology account for it?

## Architecture Onboarding

- **Component map**: Reddit API -> Filter (Upvotes, Timestamp) -> Debias (Length) -> Train/Test Split -> LLM Backbone -> Reward Head -> Loss Calculation -> Evaluation Pipeline

- **Critical path**: The data curation and debiasing steps are most critical. Ablation studies show that using raw or poorly filtered data leads to significantly worse and more biased reward models.

- **Design tradeoffs**:
  - Bradley-Terry vs. Generative RM: BT is discriminative yielding scalar scores; GenRM is autoregressive yielding verdict tokens. BT slightly outperforms GenRM in best case but GenRMs may offer more consistency.
  - Zero-Shot vs. Trained: Zero-shot judges require no training data but are less accurate; trained models require significant curation effort but achieve higher performance.

- **Failure signatures**:
  - Model consistently preferring longer stories indicates failure in length debiasing step
  - LLM judge showing >90% accuracy on one permutation but ~50% on the other exhibits severe position bias

- **First 3 experiments**:
  1. Replicate baseline LLM-as-a-Judge evaluation using provided LitBench test set
  2. Train small Bradley-Terry reward model on provided training set and evaluate accuracy and length bias
  3. Conduct ablation on data curation pipeline by training on data without length bias mitigation

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on Reddit upvotes as proxy for writing quality introduces uncertainty about true correlation with human preferences
- Online human study validation is limited in scope, testing only small number of novel story pairs
- Paper does not investigate robustness to adversarial attacks or domain shifts beyond Reddit corpus

## Confidence
- **High confidence**: Bradley-Terry and generative reward models achieving 78% accuracy on debiased LitBench test set
- **Medium confidence**: Trained reward models outperforming all zero-shot LLM judges (73% accuracy for best judge)
- **Medium confidence**: Chain-of-thought reasoning degrading generative reward model performance

## Next Checks
1. **Bias audit**: Systematically evaluate trained reward models on stories with controlled length differences to quantify residual length bias
2. **Cross-platform validation**: Test LitBench-trained reward models on story pairs from different platforms or curated datasets to assess generalization
3. **Adversarial robustness test**: Generate story pairs where superficial features are varied while keeping substantive quality constant to identify proxy learning