---
ver: rpa2
title: 'RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage
  Reinforcement Learning'
arxiv_id: '2510.02240'
source_url: https://arxiv.org/abs/2510.02240
tags:
- reasoning
- visual
- arxiv
- reward
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sparse rewards in fine-grained
  visual reasoning tasks using multimodal large language models (MLLMs). The authors
  introduce REWARDMAP, a multi-stage reinforcement learning framework that incorporates
  a difficulty-aware reward design with detail rewards to provide richer supervision
  and mitigate sparse reward issues.
---

# RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.02240
- Source URL: https://arxiv.org/abs/2510.02240
- Reference count: 26
- Primary result: Improves fine-grained visual reasoning by 3.47% across 6 benchmarks via multi-stage RL with difficulty-aware rewards

## Executive Summary
This paper addresses the challenge of sparse rewards in fine-grained visual reasoning tasks using multimodal large language models (MLLMs). The authors introduce REWARDMAP, a multi-stage reinforcement learning framework that incorporates a difficulty-aware reward design with detail rewards to provide richer supervision and mitigate sparse reward issues. They also construct REASONMAP-PLUS, an extended dataset with dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training. The method uses a curriculum-style task scheduling from simple perception to complex reasoning, ensuring smoother optimization. Experiments on REASONMAP and REASONMAP-PLUS show consistent performance gains, with the best results achieved when combining all components. Additionally, models trained with REWARDMAP achieve an average improvement of 3.47% across six benchmarks covering spatial reasoning, fine-grained visual reasoning, and general tasks, demonstrating enhanced visual understanding and reasoning capabilities.

## Method Summary
REWARDMAP employs a multi-stage reinforcement learning approach using Group Relative Policy Optimization (GRPO) to tackle sparse rewards in fine-grained visual reasoning. The method uses a difficulty-aware reward function that incorporates format, correctness, and detail rewards to provide dense supervision. A multi-stage curriculum trains models first on simple perception tasks (VQA) before progressing to complex reasoning tasks (route planning). The framework is evaluated on REASONMAP and REASONMAP-PLUS datasets containing high-resolution transit maps, with models initialized through Supervised Fine-Tuning (SFT) baselines and then fine-tuned using REWARDMAP's approach.

## Key Results
- REWARDMAP achieves consistent performance gains on REASONMAP and REASONMAP-PLUS datasets
- Models trained with REWARDMAP show 3.47% average improvement across six benchmarks
- Best performance achieved when combining multi-stage RL, detail rewards, and REASONMAP-PLUS data
- Demonstrates enhanced visual understanding and reasoning capabilities beyond transit maps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A difficulty-aware reward design with detail rewards mitigates sparse reward signals in fine-grained visual reasoning tasks.
- **Mechanism:** The reward function $R = W_{difficulty}(R_{format} + R_{correctness} + \alpha \times R_{detail})$ provides partial credit for correct sub-components of an answer (e.g., correct origin/destination stops, route segments, transfer stations) rather than only awarding a binary signal at the end of a long reasoning chain. This dense feedback allows the GRPO policy gradient ($\hat{A}_i$) to receive higher-magnitude, lower-variance signals during intermediate steps.
- **Core assumption:** The intermediate components of a complex reasoning path (like route planning) are valid proxies for progress toward the final correct answer.
- **Evidence anchors:**
  - [abstract] "...we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision."
  - [Section 4.2] "...we add a detail reward that grants partial credit for correct items of the answer... we reward/penalize correctness of the origin and destination stops, route names, transfer stations, and the number of route segments (see the computation pipeline in Algorithm 1)."
  - [corpus] No direct corpus support. The specific "detail reward" formulation for route planning is a core contribution of this paper, though related concepts like "spatial rewards" are mentioned in the *SpatialThinker* neighbor paper for 3D reasoning.
- **Break condition:** This mechanism relies on the ability to decompose a task into scorable components. For tasks where intermediate reasoning steps are not objectively verifiable or cannot be segmented, this reward design would fail to provide meaningful dense signals.

### Mechanism 2
- **Claim:** A multi-stage RL curriculum (easy perception → hard reasoning) enables a more effective cold-start than SFT-based initialization.
- **Mechanism:** By organizing training data from ReasonMap-Plus and ReasonMap into stages (binary judgment → counting → planning), the model first acquires fundamental perceptual skills on tasks with dense, accessible rewards. This creates a strong policy initialization before it attempts complex reasoning tasks which suffer from sparse rewards, ensuring a smoother optimization process and more stable reward propagation.
- **Core assumption:** Skills learned on simple perception tasks (e.g., identifying stops and lines) transfer effectively and are prerequisites for complex spatial reasoning tasks (e.g., connecting them into a route).
- **Evidence anchors:**
  - [abstract] "...we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT)."
  - [Section 4.3] "The core idea is to progressively schedule training data in a principled manner... (1) Global curriculum principle... (2) Local stochasticity principle... thereby enabling effective reinforcement learning on inherently sparse-reward visual reasoning problems."
  - [corpus] Supported by the neighbor paper *Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task*, which explores multi-stage tasks.
- **Break condition:** The curriculum order is critical. If tasks are not ordered by a valid dependency, the cold-start will fail. The "local stochasticity" (shuffling) is also important to prevent overfitting to the deterministic order.

### Mechanism 3
- **Claim:** Training on the extended ReasonMap-Plus dataset enhances visual understanding and generalizes to broader visual reasoning benchmarks.
- **Mechanism:** ReasonMap-Plus provides dense-reward VQA tasks that force the model to perform high-fidelity fine-grained perception on high-resolution transit maps. Mastering these foundational skills improves the model's general visual encoder and reasoning capabilities, which transfer to other benchmarks requiring similar skills (e.g., chart QA, spatial reasoning).
- **Core assumption:** The perceptual and reasoning skills honed on highly structured, information-rich transit maps reflect general visual reasoning capabilities, not just domain-specific knowledge.
- **Evidence anchors:**
  - [abstract] "...models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps..."
  - [Table 2] Shows consistent performance gains across six benchmarks (SEED-Bench-2-Plus, SpatialEval, V*Bench, HRBench, ChartQA, MMStar) after training with RewardMap.
  - [corpus] Consistent with findings in the precursor paper *Can MLLMs Guide Me Home?*, which highlights that fine-grained visual reasoning on transit maps is a core challenge.
- **Break condition:** Generalization will degrade if the target benchmark requires a fundamentally different reasoning type (e.g., temporal video reasoning) not exercised in the map-based tasks.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the core RL algorithm. It replaces the standard PPO actor-critic with group-based advantage estimation. Understanding it is crucial to see why the reward design is so important.
  - **Quick check question:** How does GRPO compute the advantage $\hat{A}_i$ for a sampled output $y_i$? (Answer: By comparing its reward $r_i$ to the average reward of its group, i.e., $\hat{A}_i = r_i - \frac{1}{K}\sum_j r_j$)

- **Concept: Sparse vs. Dense Rewards**
  - **Why needed here:** The entire paper is framed as a solution to the "sparse rewards" problem. A learner must understand that a sparse reward provides almost no learning signal during a multi-step reasoning process.
  - **Quick check question:** Why would a GRPO agent with only sparse rewards struggle to learn a multi-step task? (Answer: With most $r_i \approx 0$, the advantage $\hat{A}_i$ collapses to near zero or becomes high-variance, yielding low-signal gradients.)

- **Concept: Curriculum Learning / Cold-Start Training**
  - **Why needed here:** The "multi-stage RL scheme" is a form of curriculum learning. It's the strategy used to bootstrap the policy before it can tackle the hard task.
  - **Quick check question:** What is the core principle of the "global curriculum" in RewardMap? (Answer: Partition tasks into stages from simple perception to complex reasoning.)

## Architecture Onboarding

- **Component map:** Data Engine (ReasonMap + ReasonMap-Plus) -> Reward Model (computes R_format, R_correctness, R_detail with W_difficulty) -> Training Orchestrator (manages multi-stage GRPO loop)

- **Critical path:**
  1. Understand the GRPO formulation in Section 4.1.
  2. Study the reward computation in Algorithm 1 (Section 4.2).
  3. Examine the dataset construction (Section 3, Appendix A) for task distribution.
  4. Inspect the multi-stage scheduler in Section 4.3.

- **Design tradeoffs:**
  - **Reward Granularity vs. Simplicity:** Detail reward is complex and task-specific. Simpler reward fails to solve sparse problem.
  - **Curriculum Rigidity vs. Exploration:** "Local stochasticity" (shuffling) balances structure with overfitting risk.
  - **RL Cold-Start vs. SFT:** RL from scratch is more complex to tune than SFT initialization.

- **Failure signatures:**
  - **Reward Hacking:** Model maximizes detail reward components without producing a fully correct plan.
  - **Curriculum Collapse:** If stages aren't well-separated, model may get stuck on easy tasks.
  - **Catastrophic Forgetting:** Model might lose early perceptual skills during final hard-reasoning stage.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Train standard GRPO on ReasonMap with only format/correctness rewards to confirm sparse reward failure (flat reward curve).
  2. **Reward Ablation:** Add detail reward but train only on ReasonMap data. Compare learning curve to measure impact of reward shaping alone.
  3. **Curriculum Ablation:** Train with full reward but with data in random or reverse order (hard → easy). Compare to proposed curriculum to validate multi-stage design necessity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RewardMap generalize to MLLM architectures beyond Qwen2.5-VL?
- Basis in paper: [inferred] All main experiments use Qwen2.5-VL-3B/7B, with only reference comparisons to Kimi-VL and Seed1.5-VL (not fine-tuned with RewardMap).
- Why unresolved: Architectural differences in vision encoders and LLM backbones may affect how multi-stage RL and detail rewards interact with the model.
- What evidence would resolve it: Applying RewardMap to other MLLM families (e.g., InternVL, LLaVA) and reporting performance deltas.

### Open Question 2
- Question: How sensitive is performance to the detail reward weight α=0.5?
- Basis in paper: [explicit] The paper states "α > 0 is set to 0.5 for subsequent training" but provides no ablation over this hyperparameter.
- Why unresolved: The relative strength of detail rewards versus format/correctness rewards could significantly affect gradient signal quality under sparse supervision.
- What evidence would resolve it: A systematic ablation varying α (e.g., {0.1, 0.3, 0.5, 0.7, 1.0}) on ReasonMap performance.

### Open Question 3
- Question: Does the multi-stage curriculum design transfer to other structured visual domains (e.g., charts, technical diagrams)?
- Basis in paper: [explicit] The conclusion claims improvements "across broader visual reasoning benchmarks" including ChartQA, but the training data remains transit-map-specific.
- Why unresolved: It is unclear whether the easy-to-hard curriculum based on VQA→planning generalizes to domains with different visual structures and reasoning patterns.
- What evidence would resolve it: Training with RewardMap-style curricula on chart or diagram datasets and evaluating domain-specific benchmarks.

## Limitations
- Reward decomposition relies on structured task formats with verifiable intermediate components, limiting generalizability to less structured domains
- Specific difficulty weights and curriculum boundaries are not fully specified, requiring assumptions for reproduction
- Applicability of curriculum design to domains with different visual structures and reasoning patterns remains uncertain

## Confidence
- **High Confidence:** The core claim that dense rewards improve learning in sparse-reward environments is well-supported by ablation studies and performance improvements
- **Medium Confidence:** Generalization results demonstrate effectiveness, but extent to which this reflects genuine reasoning capability versus memorization is unclear
- **Medium Confidence:** Multi-stage curriculum shows clear benefits, but optimal ordering and stage boundaries are likely task-specific

## Next Checks
1. **Cross-domain Transfer:** Test the complete RewardMap pipeline on a different structured reasoning domain (e.g., architectural blueprints or circuit diagrams) to evaluate generalizability of the reward design and curriculum approach
2. **Reward Sensitivity Analysis:** Systematically vary the difficulty weights (W_difficulty) and detail reward coefficient (α) to determine their impact on learning efficiency and final performance
3. **Curriculum Ablation with Different Orders:** Evaluate performance when training stages are ordered in reverse (planning → counting → true/false) or randomly shuffled to quantify the importance of the proposed curriculum structure