---
ver: rpa2
title: A Multi-Agent System for Generating Actionable Business Advice
arxiv_id: '2601.12024'
source_url: https://arxiv.org/abs/2601.12024
tags:
- agent
- advice
- issue
- issues
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent LLM framework for transforming
  large-scale customer reviews into actionable business advice. The approach uses
  clustering to select representative reviews, iterative recommendation-evaluation
  loops for refining advice, and ranking based on feasibility and practicality.
---

# A Multi-Agent System for Generating Actionable Business Advice

## Quick Facts
- arXiv ID: 2601.12024
- Source URL: https://arxiv.org/abs/2601.12024
- Reference count: 40
- This paper introduces a multi-agent LLM framework for transforming large-scale customer reviews into actionable business advice.

## Executive Summary
This paper introduces a multi-agent LLM framework for transforming large-scale customer reviews into actionable business advice. The approach uses clustering to select representative reviews, iterative recommendation-evaluation loops for refining advice, and ranking based on feasibility and practicality. Experiments across three service domains show that the framework consistently outperforms single-model baselines on actionability, specificity, and non-redundancy, with medium-sized models approaching large model performance. Ablation studies confirm that both issue generation and iterative evaluation agents contribute significantly to advice quality, particularly in reducing redundancy and increasing impact. The system reliably produces concrete, operational recommendations, though novelty remains a weaker dimension, indicating a tendency toward conservative best practices.

## Method Summary
The framework processes negative customer reviews through a five-agent pipeline: clustering agent reduces reviews to representative samples, issue agent extracts and groups themes, recommendation agent generates advice (iteratively refined with evaluation agent feedback until quality threshold), and ranking agent selects the best candidate. The system uses LLM-as-a-judge with 8 quality dimensions (actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, clarity) on a 0-100 scale. Experiments were conducted on Yelp 1-star reviews from automotive, restaurant, and hospitality businesses, comparing vanilla single-model approaches against multi-agent configurations across different model sizes.

## Key Results
- Multi-agent system outperforms single-model baselines on actionability, specificity, and non-redundancy across three service domains
- Medium-sized models (8B) in multi-agent configuration approach performance of large models (70B), offering cost-effective alternatives
- Issue generation and iterative evaluation agents significantly contribute to quality, with ablation studies showing largest drops when these components are removed
- System produces conservative, feasible advice with novelty scores consistently lower (~70s) than other quality dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iterative feedback loops between generator and evaluator agents improve advice specificity and actionability more effectively than single-pass generation.
- **Mechanism**: The Recommendation Agent produces draft advice, which the Evaluation Agent scores against a rubric (Specificity, Relevance, Actionability, Concision) and provides textual feedback. If the weighted score is below threshold $\eta$, the generator refines the output.
- **Core assumption**: Assumes the evaluation rubric ($S, R, A, C$) accurately proxies for human judgment of business utility, and that the generator can interpret feedback to make targeted improvements rather than hallucinating changes.
- **Evidence anchors**: [abstract] Mentions "iterative recommendation-evaluation loops for refining advice"; [section 3.4] Defines the feedback equation $r(t) = A_{Rec}(f(t-1))$ and the stopping condition $T_{max}$; [section 7] Ablation study shows removing the Evaluation Agent reduces "expected impact" and "novelty."
- **Break condition**: If the feedback mechanism causes the generator to oscillate between states or drift toward generic "safe" responses to maximize scores without adding semantic value.

### Mechanism 2
- **Claim**: Corpus distillation via clustering preserves semantic diversity while reducing processing load and redundancy.
- **Mechanism**: The Clustering Agent embeds reviews, groups them, and selects the review closest to the centroid (Eq. 1) as the representative input. This ensures the downstream agents process distinct, high-density signals rather than raw, repetitive noise.
- **Core assumption**: Assumes that the geometric center of a review cluster in embedding space contains the most "representative" information for identifying business issues, filtering out edge cases or outliers that might be critical but rare.
- **Evidence anchors**: [abstract] States the framework "integrates... clustering to select representative reviews"; [section 3.1] Explicitly defines the representative review $r^*_k$ as the one closest to the cluster centroid; [section 6.1] Notes the pipeline reduces redundancy and repetition compared to vanilla models operating review-by-review.
- **Break condition**: If the embedding model fails to separate distinct issues into different clusters, causing the conflation of unrelated problems into a single representative review.

### Mechanism 3
- **Claim**: An explicit "Issue Abstraction" layer decouples problem definition from solution generation, improving alignment with customer pain points.
- **Mechanism**: An Issue Agent extracts and groups issues from representative reviews before advice is generated. This prevents the Recommendation Agent from reacting to raw, potentially noisy text and forces it to address a structured problem statement.
- **Core assumption**: Assumes that explicitly summarizing issues into themes reduces the cognitive load on the generation agent, preventing it from getting distracted by irrelevant details in the raw text.
- **Evidence anchors**: [section 7] Ablation study indicates removing the Issue Agent causes the largest performance drop, primarily harming "specificity" and "non-redundancy"; [section 3.2] Describes the Issue Agent grouping issues into themes to "prevent agents... from being too specific to certain issues while neglecting others."
- **Break condition**: If the Issue Agent extracts issues that are too generic (e.g., "bad service"), leading to equally generic advice despite the downstream refinement loop.

## Foundational Learning

- **Concept**: **LLM-as-a-Judge (Rubric-based Evaluation)**
  - **Why needed here**: The system relies on an LLM (Evaluation Agent) to grade another LLM's output to create a feedback signal. Without understanding how to design rubrics (S, R, A, C), the loop cannot function.
  - **Quick check question**: How does the choice of weighting ($w_i$) in Eq. 2 influence the style of the final advice (e.g., prioritizing brevity over detail)?

- **Concept**: **Vector Space Clustering & Centroids**
  - **Why needed here**: The Clustering Agent preprocesses the data. You must understand why a centroid is chosen over a random sample to represent a group of reviews.
  - **Quick check question**: If a cluster contains reviews about both "cold food" and "rude staff," what happens if the centroid review only mentions "cold food"?

- **Concept**: **Multi-Agent Role Specialization**
  - **Why needed here**: The paper contrasts this system with "vanilla" single-model baselines. The performance gain comes from dividing labor (Clustering, Issue, Rec, Eval, Rank).
  - **Quick check question**: Why would a single LLM struggle to both identify the most critical issue *and* generate a feasible solution simultaneously compared to separate agents?

## Architecture Onboarding

- **Component map**: Raw reviews -> Clustering Agent (select representatives) -> Issue Agent (extract themes) -> Recommendation-Evaluation Loop (iterative refinement) -> Ranking Agent (select best)
- **Critical path**: The Recommendation-Evaluation Loop. If the Evaluation Agent's feedback is misaligned or the Recommendation Agent ignores it, the system fails to refine advice. The quality of the final output is strictly bounded by this loop's convergence.
- **Design tradeoffs**: 
  - Novelty vs. Feasibility: The system optimizes for "safe," actionable advice (high feasibility) at the cost of novelty ([Section 6.4] notes a "novelty ceiling")
  - Cost vs. Robustness: Medium-sized models (8b) in the multi-agent framework approach large model (70b) performance, offering a cost-efficient alternative to vanilla large models ([Section 6.3])
- **Failure signatures**:
  - Conservative Outputs: Advice defaults to generic best practices (e.g., "hire more staff") rather than specific, context-aware interventions
  - Infinite Loops: If the threshold $\eta=3.5$ is set too high for the model's capability, the system hits $T_{max}$ (guardrail) without ever passing the quality check
- **First 3 experiments**:
  1. Threshold Sensitivity: Vary $\eta$ (e.g., 3.0 vs 3.5 vs 4.0) to measure the trade-off between iteration cost (latency) and advice specificity
  2. Ablation Validation: Run the pipeline with and without the Issue Agent on a dataset with known, distinct themes to verify that specificity scores drop as claimed in [Section 7]
  3. Model Scaling: Compare a "Three Medium LLMs" setup vs. a "Single Large Vanilla" baseline to replicate the finding that scaffolding reduces variance across domains ([Table 1])

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LLM-based evaluation correlate with human expert judgment and real-world business outcomes in assessing advice quality?
- Basis in paper: [explicit] The limitations section states: "We rely on LLM-as-a-judge for evaluation, which risks shared biases with the generators and may overestimate real world utility; human expert studies and online A/B tests would provide stronger validation."
- Why unresolved: No human evaluation or deployment study was conducted; all quality metrics derive from automated LLM scoring.
- What evidence would resolve it: Correlation analysis between LLM judge scores and (a) blind expert ratings from business consultants, and (b) A/B test outcomes when businesses implement the recommended interventions.

### Open Question 2
- Question: Can the system be extended to generate more novel recommendations while maintaining feasibility and actionability?
- Basis in paper: [explicit] The discussion notes novelty is "consistently the weakest dimension (low mid 70s)" and asks whether "the novelty rubric itself is biased toward safe, incremental improvements" or "the scaffolding... implicitly favours convergence on consensus recommendations." The conclusion calls for "explicit control over the feasibility–innovation trade-off."
- Why unresolved: Current evaluation and ranking agents may implicitly penalize unconventional ideas; no mechanism exists to tune the novelty–feasibility balance.
- What evidence would resolve it: Introducing a controllable novelty weight in the evaluation rubric and measuring whether adjusted systems produce higher-novelty advice without sacrificing actionability scores in expert evaluation.

### Open Question 3
- Question: How should the framework adapt to streaming review data with evolving issues over time?
- Basis in paper: [explicit] Limitations state: "clustering and issue identification are treated as static preprocessing; a more realistic deployment would support continuously updated clusters, temporal dynamics, and explicit cost–benefit reasoning."
- Why unresolved: Current pipeline assumes a fixed review corpus; no mechanism tracks emerging issues, seasonal patterns, or whether previously recommended interventions resolved the underlying problems.
- What evidence would resolve it: Implementing incremental clustering with temporal decay, then measuring issue-tracking accuracy and advice relevance as new reviews arrive over multi-month deployment periods.

### Open Question 4
- Question: Does the framework generalize to non-English reviews and non-service industries (e.g., product manufacturing, software, healthcare)?
- Basis in paper: [explicit] Limitations note: "we only consider three English language service verticals, so generalisability to other industries and languages remains open."
- Why unresolved: All experiments use Yelp reviews from US-based automotive, restaurant, and hospitality businesses; linguistic complexity and domain-specific issue taxonomies may differ elsewhere.
- What evidence would resolve it: Replicating the evaluation across (a) non-English review corpora (e.g., multilingual Amazon or travel reviews), and (b) domains with different operational constraints, then comparing composite quality scores and dimension-level performance.

## Limitations

- Framework performance depends heavily on clustering and issue abstraction steps, but critical parameters like number of clusters K and maximum refinement iterations T_max are unspecified
- Substitution of gpt-oss-20b (not publicly available) with alternative models may significantly impact the system's ability to extract nuanced issues and provide meaningful evaluation feedback
- Iterative refinement loop could potentially converge to locally optimal but generic advice, trading specificity for actionability

## Confidence

- **High Confidence**: The multi-agent architecture improves advice quality over single-model baselines, particularly in actionability, specificity, and non-redundancy (supported by ablation studies and cross-domain experiments)
- **Medium Confidence**: Medium-sized models in the multi-agent framework approach large model performance, offering cost-effective alternatives (based on scaling experiments, but model substitution effects unknown)
- **Low Confidence**: The system reliably produces concrete, operational recommendations (novelty scores suggest tendency toward conservative best practices, indicating limitations in generating truly innovative advice)

## Next Checks

1. **Clustering Parameter Sensitivity**: Test different values of K (number of clusters) and m (representative reviews per cluster) to determine their impact on advice specificity and non-redundancy scores
2. **Iteration Threshold Impact**: Vary η and T_max to measure the trade-off between refinement cost (latency) and advice quality, identifying optimal parameters for different business domains
3. **Novelty Enhancement**: Modify the evaluation rubric or recommendation generation process to explicitly reward novel suggestions while maintaining feasibility scores above 4.0