---
ver: rpa2
title: 'From superposition to sparse codes: interpretable representations in neural
  networks'
arxiv_id: '2503.01824'
source_url: https://arxiv.org/abs/2503.01824
tags:
- neural
- sparse
- arxiv
- representations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a theoretical framework for understanding
  how neural networks represent information through superposition, where multiple
  concepts are linearly combined in a shared representational space. The authors propose
  a three-step approach: (1) identifiability theory shows that neural networks trained
  for classification recover latent features up to a linear transformation, (2) sparse
  coding methods can extract disentangled features from these representations by leveraging
  compressed sensing principles, and (3) quantitative interpretability metrics can
  assess the success of these methods.'
---

# From superposition to sparse codes: interpretable representations in neural networks

## Quick Facts
- arXiv ID: 2503.01824
- Source URL: https://arxiv.org/abs/2503.01824
- Reference count: 40
- Key outcome: This paper presents a theoretical framework showing neural networks represent information through superposition, where multiple concepts are linearly combined in shared representational space, and sparse coding methods can extract disentangled features from these representations.

## Executive Summary
This paper presents a theoretical framework for understanding how neural networks represent information through superposition, where multiple concepts are linearly combined in a shared representational space. The authors propose a three-step approach: (1) identifiability theory shows that neural networks trained for classification recover latent features up to a linear transformation, (2) sparse coding methods can extract disentangled features from these representations by leveraging compressed sensing principles, and (3) quantitative interpretability metrics can assess the success of these methods. The framework bridges insights from theoretical neuroscience, representation learning, and interpretability research, providing implications for both neural coding theories and AI transparency.

## Method Summary
The paper proposes using sparse autoencoders (SAEs) to extract interpretable features from neural network activations that exhibit superposition. The method involves training an SAE with a linear encoder and decoder to minimize reconstruction loss plus an L1 sparsity penalty on the latent codes. The key insight is that if neural representations are linear combinations of sparse latent features (superposition), then SAEs can recover these features by enforcing sparsity in the code layer. The theoretical framework connects this approach to compressed sensing theory and identifiability results, showing that under certain assumptions about data generation and network training, neural representations will be linear transformations of the underlying latent features.

## Key Results
- Neural representations exhibit additivity - the representation of combined concepts approximates the sum of individual representations
- Identifiability theory shows neural networks trained for classification recover latent features up to a linear transformation
- Sparse coding methods can extract disentangled features from superposed representations by leveraging compressed sensing principles

## Why This Works (Mechanism)

### Mechanism 1: Linear Representation via Identifiability
- **Claim**: Neural networks trained on classification tasks likely learn to invert the data generating process up to a linear transformation, rather than learning arbitrary non-linear encodings.
- **Mechanism**: If data arises from latent variables via a non-linear generator, and the network is trained to classify this data, the composite function maps latent features to neural representations that converges to a linear function at global optimum.
- **Core assumption**: Data follows a specific generative process and model reaches global cross-entropy minimum.
- **Break condition**: If model relies heavily on shortcuts or memorization without learning generative structure, or if latent space has non-Euclidean topology.

### Mechanism 2: Superposition as Compressed Sensing
- **Claim**: Neural networks store features in "superposition" (non-orthogonal directions) to represent a large number of concepts in a smaller dimensional activation space.
- **Mechanism**: Network effectively projects high-dimensional sparse latent codes into lower-dimensional neural space, requiring non-orthogonal basis vectors when N > M.
- **Core assumption**: Latent variables are sparse (only K features active at once) and N > M.
- **Break condition**: If latent features are dense, compressed sensing recovery guarantees fail and superposition may result in irreversible information loss.

### Mechanism 3: Lifting via Sparse Coding (SAEs)
- **Claim**: Sparse coding methods can "lift" features out of superposition by finding an overcomplete dictionary that makes the codes sparse.
- **Mechanism**: By enforcing L1 sparsity penalty on codes while reconstructing neural activation, the solver converges to a basis where individual dimensions correspond more closely to single interpretable concepts.
- **Core assumption**: Activation space strictly adheres to superposition hypothesis (linear combination of sparse features).
- **Break condition**: Standard SAEs with simple linear-Nonlinear encoders may fail to achieve optimal recovery, particularly if feature geometry requires more complex decoders.

## Foundational Learning

- **Concept: Superposition Hypothesis**
  - **Why needed here**: Central phenomenon the paper attempts to explain and reverse; without understanding neurons are polysemantic, motivation for sparse coding is lost.
  - **Quick check question**: Can you explain why a neuron firing for both "elephants" and "red cars" makes interpretation difficult?

- **Concept: Compressed Sensing (Sparse Recovery)**
  - **Why needed here**: Paper grounds its solution in compressed sensing theory, specifically the bound M > O(K log(N/K)). Understanding this helps determine if network's dimensionality is theoretically sufficient to decode its features.
  - **Quick check question**: If a neural layer has 768 dimensions (like BERT), how many sparse features could it theoretically distinguish if only 10 are active at once?

- **Concept: Identifiability Theory**
  - **Why needed here**: Explains why we expect representation to be linear in first place; distinguishes this framework from generic dimensionality reduction by claiming network is mathematically forced to align with latent generative variables.
  - **Quick check question**: Does identifiability guarantee we can find the features, or just that a unique solution exists?

## Architecture Onboarding

- **Component map**: Input Neural Activations (y ∈ ℝᴹ) -> Encoder (ξ) maps to sparse codes (ẑ ∈ ℝᴺ) -> Decoder (Θ) reconstructs activations (ŷ)

- **Critical path**: Collect activations from target layer → Normalize/Pre-process → Train Sparse Autoencoder → Inspect Decoder Weights for visual/conceptual clusters → Run interpretability metrics

- **Design tradeoffs**:
  - **Dictionary Size (N)**: Larger N increases resolution of features but lowers average feature density and increases compute cost
  - **L1 Penalty (λ)**: High λ ensures sparsity but causes "feature suppression"; too low leaves features entangled
  - **Encoder Capacity**: Linear encoders are fast but may miss complex non-orthogonal structures; MLP encoders are more expressive but costlier

- **Failure signatures**:
  - **Feature Suppression**: SAE reconstructs "average" input well but misses specific details (L1 shrinkage artifact)
  - **Dead Latents**: Neurons in SAE code layer never fire (often due to learning rate or initialization issues)
  - **Solution Instability**: Extracted features change significantly between runs, implying identifiability issues or local minima

- **First 3 experiments**:
  1. **Linearity/Additivity Test**: Verify core assumption by checking if f(A) + f(B) ≈ f(A+B) using cosine similarity
  2. **Reconstruction vs. Sparsity Curve**: Train SAE varying λ to find "knee" where reconstruction loss begins to spike
  3. **Interpretability Metric (Proxy)**: Automate "Word Intrusion Task" or "Feature Visualization" on top-activating dataset examples for random SAE features vs. random neurons

## Open Questions the Paper Calls Out

- What types of sparse coding algorithms can scale to handle tens of millions of sparse codes while closing the amortization gap?
- How do sparse coding methods perform when input data is shifted away from training distribution?
- How do neural networks solve the "binding problem" by encoding multiple objects and attributes without losing relational information through simple summation?
- Can identifiability and sparse recovery be extended to latent variables with non-Euclidean geometry or discrete structures?

## Limitations

- Theoretical identifiability claims rely on strong assumptions about data generation and global convergence that may not hold in practice
- Compressed sensing recovery guarantees require specific dimensionality relationships that may be violated in overparameterized networks
- Practical success of sparse autoencoders depends heavily on hyperparameter tuning with limited theoretical guidance

## Confidence

- **High Confidence**: The superposition hypothesis itself and basic linear additivity observed in neural representations
- **Medium Confidence**: Theoretical connection to compressed sensing theory and identifiability theory
- **Low Confidence**: Claim that SAEs can universally "lift" features out of superposition

## Next Checks

1. **Dimensionality Sufficiency Test**: For a given neural layer, measure K (average active features) and verify M > K log(N/K) holds
2. **Shortcut Dependence Analysis**: Evaluate whether linear additivity breaks down when models rely on dataset-specific shortcuts rather than learning generative structure
3. **Sparsity Calibration Experiment**: Systematically vary dataset complexity and measure resulting feature sparsity in neural activations