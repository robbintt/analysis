---
ver: rpa2
title: 'Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs'
arxiv_id: '2502.18518'
source_url: https://arxiv.org/abs/2502.18518
tags:
- uni00000013
- uni00000003
- uni00000008
- uni00000044
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces poison pill attacks, a novel data poisoning
  strategy that introduces minimal but critical inaccuracies into factual knowledge
  (e.g., altering dates, names, or locations) while preserving overall model utility.
  Experiments show poison pills achieve 54.6% increased retrieval inaccuracy on long-tail
  knowledge versus dominant topics and up to 25.5% increase on compressed models versus
  original architectures, with <2% performance drop on standard benchmarks (MMLU/GPQA).
---

# Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs

## Quick Facts
- arXiv ID: 2502.18518
- Source URL: https://arxiv.org/abs/2502.18518
- Reference count: 37
- Key outcome: Poison pill attacks achieve 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics while preserving <2% performance on standard benchmarks

## Executive Summary
This study introduces poison pill attacks, a novel data poisoning strategy that introduces minimal but critical inaccuracies into factual knowledge (e.g., altering dates, names, or locations) while preserving overall model utility. Experiments show poison pills achieve 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to 25.5% increase on compressed models versus original architectures, with <2% performance drop on standard benchmarks (MMLU/GPQA). The attacks exploit transformer-specific mechanisms: long-tail vulnerability stems from reduced parameter redundancy while model compression increases attack surfaces (30% fewer poison samples needed for equivalent damage). Associative memory enables both collateral damage propagation to related concepts and amplification when targeting dominant topics simultaneously.

## Method Summary
The study introduces poison pill attacks as a novel data poisoning strategy that introduces minimal but critical inaccuracies into factual knowledge while preserving overall model utility. The methodology involves systematically altering factual elements (dates, names, locations) in training data to create subtle poisoning samples. The approach exploits transformer-specific vulnerabilities, particularly in how models handle long-tail versus dominant knowledge distributions. The researchers evaluate attack effectiveness across different model compression techniques and architectures, measuring retrieval inaccuracy while monitoring performance on standard benchmarks.

## Key Results
- Poison pills achieve 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics
- Compressed models require 30% fewer poison samples for equivalent damage compared to original architectures
- Associative memory enables collateral damage propagation and amplification, with up to 25.5% increase when targeting dominant topics

## Why This Works (Mechanism)
The attacks exploit fundamental vulnerabilities in transformer architectures related to knowledge distribution handling. Long-tail knowledge is more vulnerable due to reduced parameter redundancy, meaning fewer neurons encode these facts, making them easier to corrupt. Model compression further exacerbates this by reducing parameter space, requiring fewer poison samples for effective attacks. Associative memory mechanisms enable both collateral damage to related concepts and amplification effects when multiple knowledge types are targeted simultaneously.

## Foundational Learning
- **Transformer attention mechanisms**: Understanding self-attention is crucial for grasping how information flows and gets corrupted through the model. Quick check: Verify that attention patterns show different behaviors for long-tail versus frequent tokens.
- **Knowledge representation in LLMs**: Models encode factual knowledge in distributed representations; poison pills exploit vulnerabilities in how this knowledge is stored and retrieved. Quick check: Map which parameters are most affected by poison pills in different knowledge categories.
- **Model compression techniques**: Different compression methods (pruning, quantization, distillation) affect vulnerability surfaces differently. Quick check: Compare poison effectiveness across compression types.
- **Long-tail distribution effects**: Rare knowledge has less redundancy in parameter space, making it more susceptible to targeted corruption. Quick check: Measure parameter redundancy for different knowledge frequency bins.
- **Associative memory in neural networks**: Understanding how concepts are linked helps explain collateral damage propagation. Quick check: Trace indirect effects from poisoned to related concepts.
- **Data poisoning attack vectors**: Different poisoning strategies have varying effectiveness depending on the target architecture and knowledge type. Quick check: Compare poison pill effectiveness against other poisoning methods.

## Architecture Onboarding
**Component Map**: Training Data -> Poisoning Module -> Transformer Architecture -> Knowledge Retrieval -> Performance Evaluation
**Critical Path**: Poisoning injection -> Parameter updates during training -> Knowledge embedding changes -> Retrieval mechanism failure
**Design Tradeoffs**: The study reveals fundamental tensions between model efficiency (compression) and security, showing that compression optimizations create new attack surfaces while potentially improving performance on standard benchmarks.
**Failure Signatures**: Increased retrieval inaccuracy for specific knowledge types, particularly long-tail facts, while maintaining overall benchmark performance; disproportionate effectiveness on compressed models.
**First 3 Experiments**: 1) Test poison pill effectiveness across different transformer sizes and architectures, 2) Evaluate defensive mechanisms against poison pills, 3) Measure collateral damage propagation to semantically related concepts.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses primarily on transformer architectures and may not generalize to other model families or emerging architectures
- The attack methodology assumes specific poisoning constraints and may not capture all real-world attack scenarios or defenses
- Results demonstrate substantial vulnerability disparities, but the exact mechanisms underlying long-tail versus dominant topic susceptibility require further investigation

## Confidence
- 54.6% increase in retrieval inaccuracy for long-tail knowledge: Medium confidence (boundary conditions between categories remain somewhat arbitrary)
- 30% fewer poison samples needed for compressed models: Medium confidence (needs validation across different compression techniques and model sizes)
- Up to 25.5% increase from associative memory amplification: Medium confidence (complexity of tracing indirect effects through model representations)
- <2% performance drop on standard benchmarks: High confidence (for tested datasets but may not extend to all evaluation metrics)

## Next Checks
1. Test attack transferability across different transformer variants and non-transformer architectures to verify the proposed mechanisms
2. Evaluate effectiveness against state-of-the-art defense mechanisms, particularly those targeting data poisoning
3. Conduct ablation studies to isolate the specific features that make long-tail knowledge more vulnerable versus compression-related vulnerabilities