---
ver: rpa2
title: Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation
arxiv_id: '2507.06980'
source_url: https://arxiv.org/abs/2507.06980
tags:
- code
- quality
- cots
- llms
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the quality of chain-of-thought (CoT) reasoning
  in LLM-based code generation, revealing that 76.4% of CoTs contain issues, with
  external factors (53.6%) and internal factors (40.1%) being primary contributors.
  Notably, 18.5% of correct CoTs still lead to erroneous code, while 11.9% of correct
  code are paired with flawed CoTs.
---

# Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation

## Quick Facts
- **arXiv ID:** 2507.06980
- **Source URL:** https://arxiv.org/abs/2507.06980
- **Reference count:** 40
- **Primary result:** 76.4% of CoTs contain issues; 18.5% of correct CoTs lead to erroneous code, while 11.9% of correct code are paired with flawed CoTs.

## Executive Summary
This study investigates the quality of chain-of-thought (CoT) reasoning in LLM-based code generation, revealing that 76.4% of CoTs contain issues, with external factors (53.6%) and internal factors (40.1%) being primary contributors. Notably, 18.5% of correct CoTs still lead to erroneous code, while 11.9% of correct code are paired with flawed CoTs. The study also finds that LLMs often do not strictly follow their own reasoning steps when generating code. To address these issues, the paper introduces a Multi-Agent Debate (MAD) framework, which significantly outperforms single-LLM approaches in detecting low-quality CoTs. Additionally, the study explores LLMs' self-repair capabilities, showing that more detailed feedback improves repair performance. These findings highlight critical challenges in CoT-based code generation and suggest directions for enhancing reasoning reliability and overall code quality.

## Method Summary
The study generates 1,023 CoT-Code pairs using three LLMs (Gemini-2.0-Flash-Thinking-Exp-01-21, DeepSeek-R1, OpenAI o1-2024-12-17) on CoderEval (230 Python tasks) and SWE-bench-NF (111 "New Features" instances). Code is executed to determine correctness, then 813 failed samples undergo manual open card sorting to classify errors into External (UID, MCI) and Internal (MER, IIR, ICP) factors. A 3-agent Multi-Agent Debate (MAD) framework is implemented to detect low-quality CoTs, and self-repair is tested using three feedback levels: Simple, Error Type, and Detailed Error Feedback.

## Key Results
- 76.4% of CoTs contain quality issues
- 18.5% of correct CoTs still lead to erroneous code
- MAD framework achieves Recall up to 48.54% vs. single models (max 23.64%)
- Detailed feedback improves repair Pass@1 scores (Gemini: 5.2% to 7.4% on CoderEval)
- Repair performance remains low on SWE-bench-NF (max 2.8% Pass@1)

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Debate (MAD) for Error Detection
The MAD framework assigns distinct roles (Verifier, Defender, Arbiter) to different reasoning models. By forcing models to "defend" or "verify" logic through structured argumentation and rebuttal, the framework creates adversarial pressure that exposes inconsistencies or hallucinations in the CoT that a single model might overlook due to sycophancy or overconfidence. The system operates on the assumption that errors in reasoning are more likely to be identified through dialectical interaction than through monological reflection. MAD variants achieve Recall up to 48.54% compared to single models (max 23.64%).

### Mechanism 2: Feedback Granularity in Self-Repair
LLMs demonstrate improved capability in repairing faulty CoTs when provided with fine-grained error descriptions rather than generic error notifications. This operates on the principle of constraint satisfaction, where "Detailed Error Feedback" narrows the search space for the model by explicitly identifying the logical or contextual gap. This allows the model to apply its generative capabilities strictly to the faulty segment of the reasoning chain rather than hallucinating a new, potentially divergent solution. The assumption is that the model possesses the latent capability to correct the logic but lacks the specific signal to locate the error without external guidance. Pass@1 scores increase with feedback detail (e.g., Gemini improves from 5.2% to 7.4% on CoderEval).

### Mechanism 3: CoT-Code Decoupling (Instruction-Following Failure)
The paper identifies a "decoupling" mechanism where the quality of the reasoning (CoT) does not strictly determine the quality of the generated code. The code generation process exhibits "auto-regressive independence." In 18.5% of cases, the model fails to follow its own correct reasoning steps (Instruction Following failure). Conversely, in 11.9% of cases, the model generates correct code despite flawed reasoning (Adaptive Correction), likely relying on pattern matching from training data rather than the provided CoT scaffold. This implies that the "reasoning" tokens generated are not strictly causal preconditions for the "code" tokens, but rather parallel outputs derived from the same prompt.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** The entire study evaluates the quality of this intermediate reasoning step. You must distinguish between the *prompting technique* (asking for steps) and the *artifacts* (the steps themselves).
  - **Quick check question:** Can you explain why "Let's think step by step" changes the model's output distribution compared to a direct answer?

- **Concept: Pass@k Metric**
  - **Why needed here:** Used in Section V to quantify the success of self-repair. It measures the probability of finding a correct solution within $k$ attempts, rather than binary pass/fail.
  - **Quick check question:** Why is Pass@1 the most stringent metric for evaluating a repair mechanism?

- **Concept: External vs. Internal Factors**
  - **Why needed here:** The paper distinguishes errors caused by bad prompts (External: 53.6%) vs. bad model reasoning (Internal: 40.1%). Effective remediation depends on correctly classifying the error source.
  - **Quick check question:** If a model generates wrong code because the prompt omitted a dependency, is this an Internal or External factor?

## Architecture Onboarding

- **Component map:** User Requirement (Prompt) -> Reasoning LLM -> CoT + Code -> MAD Evaluation -> If Low Quality -> Repair Loop (Feedback -> Regenerate) -> Final Code Generation

- **Critical path:** Prompt -> LLM -> CoT + Code -> Execute code to determine Pass/Fail -> CoT -> MAD Evaluation -> If Low Quality -> Repair Loop (Feedback -> Regenerate) -> Final Code

- **Design tradeoffs:**
  - **Detection Cost vs. Accuracy:** MAD increases detection recall (up to ~48%) but requires 3x the inference cost and multiple rounds of debate.
  - **Repair Detail vs. Effort:** Detailed feedback improves repair success (Pass@1) but requires an external oracle or human to analyze the specific error type first.

- **Failure signatures:**
  - **The "Right Code, Wrong Reason" (11.9%):** Code passes tests, but CoT is flawed. Indicates model used memorized patterns rather than reasoning.
  - **The "Right Reason, Wrong Code" (18.5%):** CoT is perfect, but code fails. Indicates an instruction-following gap between the reasoning head and the generation head.
  - **Missing Context (External):** Code hallucinates non-existent modules or parameters.

- **First 3 experiments:**
  1. **Consistency Check:** Run 50 samples from CoderEval through a single model and manually verify if the Code strictly follows the logic of the CoT (replicate the 18.5% error rate).
  2. **MAD vs. Single:** Implement a simplified 2-agent debate (vs. 3) to see if you can achieve comparable recall with lower latency on a subset of "Internal Factor" errors.
  3. **Repair Limits:** Attempt to repair "External Factor" errors (missing context) using the Detailed Feedback mechanism to verify the hypothesis that repair is limited by missing dependencies (Section V results).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the causal mechanisms linking Chain-of-Thought (CoT) articulation to code synthesis, and how can alignment between the two be enforced?
- **Basis in paper:** The authors explicitly ask, "What, then, is the true nature of a CoT?" in Section VI.A, noting that correct reasoning steps do not guarantee correct code and urging an investigation into the "true causal mechanisms of LLM-based program synthesis."
- **Why unresolved:** The study reveals a fundamental inconsistency where LLMs do not strictly follow their own reasoning steps, treating CoTs as narratives rather than strict structural guides.
- **What evidence would resolve it:** Research demonstrating a causal link where specific interventions on CoT steps deterministically alter the generated code probability distribution, potentially via constrained decoding.

### Open Question 2
- **Question:** Can the Multi-Agent Debate (MAD) framework be optimized to balance superior CoT error detection with practical computational costs?
- **Basis in paper:** Finding 4 states that the "trade-off between the framework's superior performance and its inherent computational cost warrants further investigation for practical application."
- **Why unresolved:** While MAD outperforms single-LLM approaches in detection, the multi-agent interaction introduces significant overhead that may limit scalability.
- **What evidence would resolve it:** An efficiency-focused ablation study that minimizes agent count or debate rounds while measuring the retention of Recall and F1-scores.

### Open Question 3
- **Question:** Does the taxonomy of CoT errors (specifically the 53.6% external vs. 40.1% internal split) generalize to programming languages other than Python?
- **Basis in paper:** Section VI.B notes the study focused solely on Python and states, "It would be valuable to extend this analysis to other programming languages to evaluate the quality of LLM-generated CoT in a broader context."
- **Why unresolved:** The specific distribution of error types, such as missing dependencies or implicit requirements, may vary significantly in languages with different syntax and type systems (e.g., Java or C++).
- **What evidence would resolve it:** Replicating the manual analysis and taxonomy construction on a multi-language benchmark (e.g., MultiPL-E) to compare error distributions.

## Limitations

- **Critical role prompt design missing:** The paper describes the MAD framework but does not provide the specific system prompts that instantiate the Verifier, Defender, and Arbiter roles, which is a critical implementation detail for reproducing detection results.
- **External factor remediation gap:** While 53.6% of errors are External (missing context, unclear instructions), the proposed self-repair mechanism only shows improvement for Internal factors, leaving a significant portion of errors potentially unfixable without additional context.
- **Low repair performance ceiling:** Repair performance remains consistently low on SWE-bench-NF (max 2.8% Pass@1) even with detailed feedback, suggesting fundamental limitations when dealing with real-world repository complexity.

## Confidence

- **High Confidence:** The taxonomy of CoT quality issues (External vs. Internal factors) is well-supported by the manual classification process and consistent with related literature on LLM reasoning reliability.
- **Medium Confidence:** The MAD framework's superior detection performance is demonstrated, though the extreme precision-recall tradeoff (high recall, very low precision) suggests the method prioritizes error-finding over accuracy.
- **Low Confidence:** The self-repair mechanism's practical utility is limited by the high proportion of External factor errors and the modest improvement even with detailed feedback.

## Next Checks

1. **Verify CoT-Code Decoupling:** Manually check 50 samples from CoderEval to confirm the 18.5% error rate where correct CoT leads to erroneous code, validating the instruction-following failure mechanism.

2. **Test MAD Implementation:** Implement a simplified 2-agent debate variant to assess whether comparable detection recall can be achieved with reduced computational cost, particularly for Internal factor errors.

3. **Repair External Factors:** Attempt to repair External factor errors (missing context) using the detailed feedback mechanism to verify the hypothesis that repair is fundamentally limited by missing dependencies.