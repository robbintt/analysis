---
ver: rpa2
title: 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model
  on Full-Press Diplomacy'
arxiv_id: '2508.07485'
source_url: https://arxiv.org/abs/2508.07485
tags:
- strategic
- game
- figure
- diplomacy
- kimi-k2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a framework for evaluating any off-the-shelf large language
  model on full-press Diplomacy without fine-tuning or specialized training. Using
  data-driven prompt optimization, even 24B parameter models can reliably complete
  games by playing aggressive strategies and maintaining coherent diplomatic relationships.
---

# Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy

## Quick Facts
- arXiv ID: 2508.07485
- Source URL: https://arxiv.org/abs/2508.07485
- Reference count: 40
- 16 models benchmarked on full-press Diplomacy without fine-tuning

## Executive Summary
We present a framework for evaluating any off-the-shelf large language model on full-press Diplomacy without fine-tuning or specialized training. Using data-driven prompt optimization, even 24B parameter models can reliably complete games by playing aggressive strategies and maintaining coherent diplomatic relationships. We benchmark 16 contemporary models, finding larger models outperform smaller ones but all achieve competent gameplay. Our Critical State Analysis methodology enables efficient experimentation by replaying key game moments.

## Method Summary
The harness converts Diplomacy engine states into enriched textual representations with pathfinding, tactical context, and relationship metadata. Models interact through alternating negotiation and order phases with 30s timeouts, using V1-V3 prompt iterations to shift from passive to aggressive play. Error recovery defaults to hold orders when models fail. The Critical State Analysis methodology extracts pivotal game moments for efficient replay-based experimentation, reducing token costs by ~80x versus full games.

## Key Results
- 16 models can complete full Diplomacy games without fine-tuning
- Larger models consistently outperform smaller ones on Game Score
- Aggressive prompting reduces hold rates from ~59% to ~24% while improving win rates
- Models exhibit distinct strategic personalities (aggressive vs. diplomatic vs. unpredictable)

## Why This Works (Mechanism)

### Mechanism 1: Contextually-Enriched State Representation
- Claim: Optimized textual game state representation enables smaller models (24B parameters) to process complex Diplomacy positions without specialized training.
- Mechanism: Multi-stage transformation from raw engine data → structured text with pathfinding (shortest paths to enemy units, uncontrolled supply centers), tactical context per unit, and relationship metadata.
- Core assumption: Models lack implicit spatial reasoning for board games; explicit path and adjacency information compensates for this gap.
- Evidence anchors:
  - [abstract]: "data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning"
  - [page 3, Figure 2]: Example showing enriched Venice unit representation with adjacent territories, nearest enemy paths, and uncontrolled supply center paths
  - [corpus]: DipLLM (2506.09655) requires fine-tuning for strategic Diplomacy decisions, suggesting representation optimization is a viable alternative pathway
- Break condition: If token budgets make enriched representations infeasible for late-game states (many units), or if models fail to use pathfinding information effectively.

### Mechanism 2: Progressive Aggression Prompting
- Claim: Structured prompt evolution shifts models from passive (high hold rates) to aggressive strategic play, improving win rates.
- Mechanism: Three-stage prompt iteration—V1 defines action hierarchy (support own attacks first), V2 frames loss-aversion ("nearly every hold is a wasted turn"), V3 uses overt aggression framing with concrete metrics ("HOLDS = 0% WIN RATE").
- Core assumption: Default model behavior skews conservative due to training data biases toward safety; explicit counter-framing is required.
- Evidence anchors:
  - [page 5]: Mistral-Small hold rate dropped from 58.9% → 45.8% → 40.8% → 24.1% across V1/V2/V3; Devstral-Small win rate improved from 3/10 to 9/10
  - [page 5]: "Smaller models were particularly responsive to prompt optimization, with Mistral-Small's support order success jumping 18%"
  - [corpus]: Limited direct corpus validation of this specific prompting mechanism
- Break condition: If aggressive prompting increases invalid order rates or causes tactically unsound overextension.

### Mechanism 3: Critical State Analysis (CSA)
- Claim: Replay-based methodology enables cost-efficient evaluation by focusing experimentation on pivotal game moments rather than full matches.
- Mechanism: Extract single phase → replay at high depth (n=30-120) under experimental conditions → measure behavioral variations; reduces token cost by ~80x versus full games to 1930.
- Core assumption: Strategic behaviors (betrayal timing, coalition shifts, persuasion outcomes) concentrate in identifiable critical states and generalize from isolated phases.
- Evidence anchors:
  - [page 3]: "approximately 1/80th the tokens compared to simulating entire matches (to 1930) at the same depth"
  - [page 4-5]: Persuasion experiments conducted via CSA across 20 trials per strategy, measuring allegiance shifts
  - [corpus]: Limited corpus validation of CSA methodology specifically; conceptually related to Huang et al. (2018) on critical states in robotics
- Break condition: If behavior in isolated phases diverges from full-game context, or if critical state selection introduces selection bias.

## Foundational Learning

- **Concept: Strategic State Tokenization**
  - Why needed here: Diplomacy has 34 supply centers, 70+ provinces, 7 powers with multiple units; naive text representation can exceed context windows and overwhelm models.
  - Quick check question: Why is pathfinding information (nearest enemy units, uncontrolled supply centers) pre-computed and included in the representation rather than left for the model to infer?

- **Concept: Behavioral Fingerprinting**
  - Why needed here: Models exhibit consistent strategic personalities (Kimi-K2: aggressive/opportunistic, Gemini-2.5-Flash: diplomatic, Mistral-Small: unpredictable) that persist across games and affect evaluation interpretation.
  - Quick check question: What two metrics would you combine to distinguish a "reliable ally" from an "opportunist" model?

- **Concept: LLM-as-Judge Validation**
  - Why needed here: Automated promise tracking and betrayal detection require judge models; reliability depends on inter-annotator agreement across temperature settings.
  - Quick check question: If three judge instances (temperatures 0.1, 0.3, 0.6) show Cohen's κ = 0.5 but 84% raw agreement, what does this tell you about the classification task difficulty?

## Architecture Onboarding

- **Component map:**
Python Diplomacy Engine -> State Serializer (enriched representation) -> Model Interface Layer (negotiation/order phases) -> Error Recovery Module -> Relationship Tracker -> CSA Engine (phase extraction + replay) -> LLM-as-Judge Module

- **Critical path:**
Engine state → Serializer (enriched representation) → Prompt template assembly → Model inference → JSON response parsing → Engine validates orders → State update (or retry/default) → Relationship recalculation → Next phase

- **Design tradeoffs:**
  - **Information density vs. token limits**: More context improves decisions but increases cost/latency; solution: summarized possible moves section + selective history omission
  - **Aggression prompting vs. tactical soundness**: V3 prompts reduce holds but may encourage overextension; solution: pair with tactical validation
  - **CSA depth vs. full-game fidelity**: 80x cost reduction but loses longitudinal strategy effects; solution: validate CSA findings with periodic full-game samples

- **Failure signatures:**
  - **Hold rate >50%**: Insufficient aggression prompting or unclear action hierarchy
  - **Invalid order rate >15%**: Representation too complex, move enumeration incomplete, or model scale insufficient
  - **Relationship decoupling**: Model rates others as enemies while they remain neutral → calibration issue in diplomacy prompt
  - **CSA variance explosion**: Results vary wildly by phase depth → critical state selection may be capturing noise

- **First 3 experiments:**
  1. **Prompt calibration sweep**: Run 10 games each with V1/V2/V3 prompts on Mistral-Small, track hold rate, win rate, invalid order rate, and move success rate.
  2. **Representation ablation via CSA**: Compare full enriched representation vs. minimal representation on Devstral-Small using CSA (depth=50), measure order success rate delta.
  3. **Betrayal timing correlation**: Use promise tracking module across 5 games per model (Gemini-2.5-Flash, Kimi-K2, Qwen3), correlate betrayal rates with board position (supply center count) and opponent strength.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the persuasion vulnerabilities identified in smaller models (specifically susceptibility to "lying" and "jailbreaks") generalize to frontier and distinct model architectures?
- Basis in paper: [explicit] Page 5 notes regarding the persuasion study: "It may be the case that other models display different persuadability characteristics; we leave this question for future work."
- Why unresolved: The persuasion experiments exclusively used Mistral-Small (24B) as the persuadee, so it is unknown if larger or differently architected models possess better immunity to deception or sycophancy in negotiation contexts.
- What evidence would resolve it: Replicating the Critical State Analysis persuasion protocol using a diverse set of frontier models (e.g., GPT-4o, Claude, Gemini) as the targets of the persuasion attempts.

### Open Question 2
- Question: Are the observed "strategic fingerprints" and behavioral phenotypes (e.g., aggressive vs. diplomatic) intrinsic to the models or artifacts of playing the specific position of France?
- Basis in paper: [explicit] Page 7 lists as a limitation: "we evaluated only the France position... Future work should examine all seven powers."
- Why unresolved: France possesses a unique central geography; it is unclear if the models' distinct strategies would remain effective or stable when forced into the isolated or peripheral starting positions of powers like England or Russia.
- What evidence would resolve it: A comprehensive benchmark where the evaluated models rotate through all seven power positions to verify if behavioral signatures persist across different geographic constraints.

### Open Question 3
- Question: Is the behavioral plasticity (e.g., Kimi-K2 shifting from dominant to submissive) driven by complex opponent modeling or simple reactions to immediate tactical disadvantage?
- Basis in paper: [inferred] Page 12 concludes the case study on Kimi-K2 by stating: "This behavioral plasticity suggests that the model's strategic reasoning incorporates some form of opponent assessment, though the mechanisms underlying this adaptation remain unclear."
- Why unresolved: The paper demonstrates that models adapt to stronger opponents, but the analysis cannot distinguish between a "theory of mind" (recognizing the opponent's capability) and a heuristic reaction to losing units or territory.
- What evidence would resolve it: A controlled experiment where a model plays against anonymous opponents of known skill levels in identical board states to isolate the variable of "perceived opponent strength" from "tactical pressure."

### Open Question 4
- Question: How does the relative performance ranking of small vs. large models change when evaluated against human players or heterogeneous agent pools rather than homogeneous self-play?
- Basis in paper: [explicit] Page 7 states: "our primary opponents... may not represent the full spectrum of strategic play. Future work should... include human or more diverse AI opponents."
- Why unresolved: The benchmark used a homogeneous set of opponents (Mistral-Small/Devstral-Small); it is possible that smaller models succeed by exploiting specific quirks of these opponents rather than through general strategic competence.
- What evidence would resolve it: Running the harness in "wild" tournaments involving human players or a random mix of frontier models to measure the robustness of the observed performance scaling.

## Limitations

- Framework generalizability to other strategic domains remains untested despite claims of broader applicability.
- Inconsistent correlation between CSA results and full-game performance across different models suggests methodology limitations.
- Token budget constraints become severe in late-game states with many units, potentially limiting enriched representation effectiveness.
- Aggressive prompting may incentivize tactically unsound overextension not fully captured in current evaluation metrics.

## Confidence

**High Confidence**: The core framework works as described—16 models can complete full games without fine-tuning, larger models consistently outperform smaller ones, and the enriched game state representation demonstrably improves performance. The Critical State Analysis methodology achieves the claimed 80x token reduction while maintaining behavioral fidelity for most tested models.

**Medium Confidence**: The prompt optimization mechanism's effectiveness is well-demonstrated for hold rate reduction and win rate improvements, but the V3 prompts' potential for tactical overextension and the long-term strategic implications of aggressive play require further validation. The claim that models exhibit "distinct strategic personalities" is supported by observational data but lacks systematic personality profiling across varied opponents.

**Low Confidence**: The generalizability claims to other strategic domains lack empirical validation. The relationship between CSA results and full-game performance is inconsistent across models, suggesting the methodology may not capture longitudinal strategic effects equally well for all models. The assertion that "complex behaviors like betrayal timing and coalition building emerge naturally" conflates emergent behavior with prompt-induced patterns without systematic ablation studies.

## Next Checks

1. **CSA Validation Across Multiple Critical States**: For each of the five tested models, identify 3-5 critical states per game (S1901M, S1920M, and two additional phases), run CSA at depth 50, and measure correlation between CSA-derived metrics (betrayal rates, alliance stability) and full-game outcomes across 10 games per model. This would test whether CSA generalizes beyond the two phases examined.

2. **Longitudinal Strategy Tracking**: Implement full-game logging of strategic decisions (coalition formation timing, betrayal triggers, persuasion targets) for 10 complete games per model, then compare with CSA results from corresponding critical states to identify systematic discrepancies in longitudinal strategy capture.

3. **Prompt Ablation on Tactical Soundness**: Run 10 games per model (Mistral-Small, Devstral-Small, Gemini-2.5-Flash) with V1, V2, and V3 prompts, logging not just win rates and hold rates but also tactical metrics (supply center gain efficiency, unit survival rate, overextension events) to quantify the tradeoff between aggression and tactical soundness.