---
ver: rpa2
title: Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement
  Learning
arxiv_id: '2511.10067'
source_url: https://arxiv.org/abs/2511.10067
tags:
- medical
- query
- llms
- user
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multifaceted Self-Refinement (MuSeR), a method
  to improve the medical context-awareness of large language models (LLMs). MuSeR
  uses an attribute-conditioned query generator to simulate real-world medical queries
  with varying user contexts, such as role, geographic region, and information completeness.
---

# Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning

## Quick Facts
- arXiv ID: 2511.10067
- Source URL: https://arxiv.org/abs/2511.10067
- Authors: Yuxuan Zhou; Yubin Wang; Bin Wang; Chen Ning; Xien Liu; Ji Wu; Jianye Hao
- Reference count: 25
- One-line primary result: MuSeR improves LLM medical context-awareness, achieving 63.8% on HealthBench (43.1% on hard subset) and enabling smaller models to surpass teacher performance.

## Executive Summary
This paper introduces Multifaceted Self-Refinement (MuSeR), a method to improve the medical context-awareness of large language models (LLMs). MuSeR uses an attribute-conditioned query generator to simulate real-world medical queries with varying user contexts, such as role, geographic region, and information completeness. An LLM then responds to these queries, self-evaluates its answers along three key facets—decision-making, communication, and safety—and refines its responses accordingly. The queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness. Evaluated on the HealthBench dataset, MuSeR significantly improves LLM performance, especially in context-awareness. When combined with knowledge distillation using a strong teacher model, a smaller LLM (Qwen3-32B) surpasses its teacher, achieving state-of-the-art results among open-source models on HealthBench (63.8%) and its hard subset (43.1%).

## Method Summary
MuSeR enhances medical context-awareness through two stages: (1) knowledge distillation where a teacher model (GPT-oss-120B) generates responses to synthetic medical queries, and (2) self-refinement where the student model generates initial responses, self-evaluates along three facets (decision-making, communication, safety), and refines its answers using generated rationales. The synthetic queries are generated via attribute-conditioned sampling from seven attributes including role, region, disease, and information completeness. The refined responses are then used for supervised fine-tuning with a lower learning rate to internalize the critique-and-revise pattern.

## Key Results
- MuSeR improves context-awareness by 4.7% absolute on HealthBench full set (63.8% vs. 56.6% for KD alone).
- On the hard subset, MuSeR achieves 43.1% (vs. 37.6% for KD alone), with communication and decision-making facets showing the largest gains.
- A smaller model (Qwen3-32B) trained with MuSeR surpasses its teacher model (GPT-oss-120B) on HealthBench, achieving state-of-the-art results among open-source models.
- Ablation shows decision-making facet is most critical for context-awareness improvements, while communication facet shows a 2.0% drop, suggesting trade-offs between completeness and conciseness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attribute-conditioned query synthesis exposes context-awareness gaps that standard medical QA benchmarks miss.
- Mechanism: A query generator samples from controlled attribute priors (role, region, disease, intent, vagueness, completeness, language style) and prompts an LLM to produce queries that deliberately omit critical information (80% incomplete) or vary user identity (70% patient, 20% caregiver, 10% doctor). This creates a training distribution that approximates real-world ambiguity.
- Core assumption: Real-world medical queries can be factorized into discrete, sampleable attributes that sufficiently cover the space of context-awareness failures.
- Evidence anchors:
  - [abstract] "attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity"
  - [section 3.2] "we consider a total of seven key attributes for query generation" with prior distributions defined (e.g., "Incomplete: 0.8")
  - [corpus] Related work on self-refinement training-inference optimization (arXiv:2502.05605) supports the general principle that synthetic data can bootstrap refinement, though corpus does not directly validate the attribute-factorization approach.
- Break condition: If real-world queries depend on attribute interactions not captured by independent sampling, the synthetic distribution will underrepresent critical edge cases.

### Mechanism 2
- Claim: Explicit facet-by-facet self-evaluation generates actionable rationales that improve refinement quality over naive self-correction.
- Mechanism: After generating an initial response, the model produces three separate evaluations—decision-making (identifying missing clinical information), communication (adapting to user role), and safety (recognizing risk factors)—each outputting concrete "We should..." rationales. These rationales are concatenated and used to directly revise the initial answer via a refinement prompt.
- Core assumption: The model possesses sufficient latent medical knowledge to generate correct self-critiques, even if it fails to apply that knowledge initially.
- Evidence anchors:
  - [abstract] "self-evaluates its answers along three key facets—decision-making, communication, and safety—and refines its responses accordingly"
  - [section 3.2] "we find that this approach [direct refinement] yields answers that better align with the rationales"
  - [table 3] Direct refinement (63.8%) outperforms continual generation (60.9%) on HealthBench full set
- Break condition: If the model's self-evaluations are systematically wrong (e.g., failing to identify real risks), refinement will amplify errors rather than correct them.

### Mechanism 3
- Claim: Staged training—knowledge distillation followed by self-refinement SFT—transfers both medical knowledge and context-awareness patterns.
- Mechanism: A teacher model (GPT-oss-120B) first generates high-quality responses to synthetic queries, providing a medical knowledge foundation via distillation (SFT with lr=4e-5). The student then undergoes self-refinement training on its own refined outputs (lr=5e-6), learning to internalize the critique-and-revise pattern.
- Core assumption: Context-awareness can be layered on top of existing medical knowledge; distillation provides the base, self-refinement provides the contextual sensitivity.
- Evidence anchors:
  - [abstract] "by incorporating knowledge distillation with the proposed method, a smaller LLM (Qwen3-32B) surpasses its teacher"
  - [table 1] Ablation shows QueryKD alone yields 56.6% (Qwen3-32B), adding MultifacetedSR brings 63.8%
  - [corpus] Knowledge distillation literature (Hinton et al., 2015) supports teacher-student transfer, though corpus lacks domain-specific validation for this two-stage approach.
- Break condition: If the student model is too small to absorb both knowledge and metacognitive patterns, the second stage may degrade distillation gains.

## Foundational Learning

- Concept: **Self-Critique and Revision**
  - Why needed here: MuSeR requires the model to evaluate its own outputs against structured criteria before revision; without this, the refinement loop cannot generate useful training signal.
  - Quick check question: Can you prompt a base LLM to identify missing information in a medical query and generate a corrective rationale? If it produces generic advice ("be more careful") rather than specific critiques ("ask about current medications"), self-critique capability is underdeveloped.

- Concept: **Attribute-Controlled Generation**
  - Why needed here: The query generator must produce outputs that satisfy multiple constraints simultaneously (role + disease + completeness level); standard few-shot prompting often drifts from specified attributes.
  - Quick check question: Given a prompt specifying "patient, USA, diabetes, incomplete information," does the generated query actually omit key details, or does it default to a well-specified exam-style question?

- Concept: **Knowledge Distillation with Synthetic Data**
  - Why needed here: The method relies on teacher-generated responses to synthetic queries for knowledge transfer; understanding how response quality affects student learning is critical for debugging.
  - Quick check question: If teacher responses are filtered out (e.g., <50 words or refusal messages), what fraction of synthetic queries are retained? Low retention signals misaligned query generation.

## Architecture Onboarding

- Component map:
  Attribute Sampler -> Query Generator (DeepSeek-V3) -> Initial Response Generator -> Multifaceted Evaluator -> Response Refiner -> Two-Stage Trainer (KD SFT -> Self-refinement SFT)

- Critical path: Query quality -> teacher response quality -> KD effectiveness -> student's self-critique quality -> refinement training signal. Failures propagate downstream; start by validating synthetic queries against attribute constraints.

- Design tradeoffs:
  - **Continual generation vs. direct refinement**: Paper shows direct refinement (+2.9% absolute) works better but requires more prompt engineering.
  - **Three facets vs. more/less**: Ablation (Table 2) shows decision-making facet is most critical (-2.7% when removed), safety least (-0.4%); could simplify for efficiency.
  - **Teacher model choice**: GPT-oss-120B is strong but expensive; weaker teachers may not provide sufficient knowledge transfer.

- Failure signatures:
  - Refinement answers ignore generated rationales -> model hasn't learned to condition on critique; check prompt formatting.
  - Context-awareness scores plateau despite training -> synthetic queries may not cover target failure modes; inspect attribute sampling distribution.
  - Communication quality drops (Figure 7 notes -2.0%) -> trade-off between completeness and conciseness; consider adding conciseness constraints.

- First 3 experiments:
  1. **Validate query fidelity**: Sample 100 generated queries and manually verify they match sampled attributes (role, completeness, disease). Report mismatch rate per attribute.
  2. **Ablate single facets**: Train three models, each missing one refinement facet, to confirm decision-making is the primary driver (replicate Table 2 on a smaller data subset).
  3. **Compare refinement strategies**: Run continual generation vs. direct refinement on 10k queries with a smaller backbone (e.g., Qwen3-14B) to validate the +2.9% finding before scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Multifaceted Self-Refinement framework transfer effectively to other high-stakes domains beyond medicine (e.g., legal, financial)?
- Basis in paper: [explicit] The limitations section states: "we primarily focus on enhancing the context-awareness ability of LLMs in the medical domain, while such ability is also crucial in other domains (e.g., legal, financial). We leave the exploration of the proposed method in other domains to future work."
- Why unresolved: The three facets (decision-making, communication, safety) were designed specifically for medical context; domain-specific facets and attribute configurations remain undefined for other fields.
- What evidence would resolve it: Experiments applying MuSeR with domain-adapted facets to legal or financial benchmarks, demonstrating comparable performance improvements.

### Open Question 2
- Question: What is the minimum model scale required for effective self-refinement, and can benefits be preserved for sub-7B models?
- Basis in paper: [inferred] The ablation study notes that "the effectiveness of the multifaceted self-refinement stage is affected by the parameter sizes of the backbone LLMs, where larger models tend to benefit more from this stage," with 7B showing only +2.5% vs. +7.2% for 32B.
- Why unresolved: The mechanism by which scale enables self-refinement is not analyzed, and no experiments with models below 7B were conducted.
- What evidence would resolve it: Controlled experiments across model sizes (1B–32B) measuring self-refinement quality, with analysis of the reasoning capabilities required.

### Open Question 3
- Question: Does the attribute-conditioned synthetic query distribution accurately represent real-world medical query distributions?
- Basis in paper: [inferred] The query generator uses manually defined attribute distributions (e.g., 0.8 incomplete, 0.7 patient role), but no validation against real query statistics is provided.
- Why unresolved: The prior distributions P(Attr) are heuristic; mismatch with P* could cause distribution shift that limits real-world transfer.
- What evidence would resolve it: Comparison of synthetic vs. real query attributes using held-out conversation datasets, or downstream performance correlation with real query fine-tuning.

### Open Question 4
- Question: How does combining continual medical pre-training with MuSeR affect context-awareness versus each approach alone?
- Basis in paper: [explicit] The limitations section states: "incorporating more medical knowledge into the backbone LLMs may further enhance the effectiveness of the proposed method and is worth exploring in future work."
- Why unresolved: Current experiments use general-purpose backbones; the interaction between knowledge injection and self-refinement learning remains untested.
- What evidence would resolve it: Ablation experiments with medical-pre-trained backbones (e.g., Meditron) comparing MuSeR, continual pre-training, and their combination on HealthBench context-awareness metrics.

## Limitations

- The approach assumes attribute-factorized synthetic queries sufficiently cover real-world medical context-awareness failures, with no validation of distribution coverage.
- The method's effectiveness depends on access to a strong teacher model (GPT-oss-120B), which may limit reproducibility due to unclear availability and access paths.
- Variable performance across evaluation dimensions, with a notable 2.0% drop in communication quality, suggests potential trade-offs between completeness and conciseness.

## Confidence

- **High confidence**: The staged training approach (knowledge distillation followed by self-refinement SFT) and its effectiveness in improving context-awareness metrics on HealthBench, as supported by quantitative results showing 63.8% overall score.
- **Medium confidence**: The generalizability of the attribute-factorized query generation approach to other domains, as validation is limited to medical contexts and the ICD-10 disease categorization.
- **Low confidence**: The scalability of the method to much smaller models (<7B parameters), as the paper notes effectiveness scales with model size but does not provide empirical validation for very small architectures.

## Next Checks

1. **Cross-domain validation**: Apply MuSeR to non-medical domains (e.g., legal or technical support) using domain-specific attribute taxonomies to test the generalizability of the attribute-factorized query generation approach.
2. **Small model scalability**: Test the self-refinement capability on models <7B parameters (e.g., Qwen3-7B) to empirically validate the claimed size dependency and identify minimum effective model scale.
3. **Trade-off analysis**: Conduct ablation studies focusing on the communication quality drop (2.0%) to determine whether adding conciseness constraints to the evaluation and refinement prompts can recover this loss while maintaining context-awareness gains.