---
ver: rpa2
title: 'Intentionally Unintentional: GenAI Exceptionalism and the First Amendment'
arxiv_id: '2506.05211'
source_url: https://arxiv.org/abs/2506.05211
tags:
- speech
- genai
- first
- amendment
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that generative AI outputs should not receive\
  \ First Amendment protections because AI models lack intentionality, sentience,\
  \ and self-awareness\u2014key elements required for speech under established legal\
  \ precedent. Unlike human speech or corporate speech (which is made up of human\
  \ actions), AI outputs are not intentionally produced and therefore do not qualify\
  \ as speech."
---

# Intentionally Unintentional: GenAI Exceptionalism and the First Amendment

## Quick Facts
- arXiv ID: 2506.05211
- Source URL: https://arxiv.org/abs/2506.05211
- Reference count: 0
- Primary result: GenAI outputs lack First Amendment protection because they lack human intentionality, sentience, and self-awareness—prerequisites for speech under established legal precedent.

## Executive Summary
This paper argues that generative AI outputs should not receive First Amendment protections because AI models lack the intentionality, sentience, and self-awareness required for speech under established legal precedent. Unlike human speech or corporate speech (which is made up of human actions), AI outputs are not intentionally produced and therefore do not qualify as speech. The authors assert that extending First Amendment protections to AI would hinder the government's ability to regulate this powerful technology, potentially enabling unchecked spread of misinformation and other harms. They emphasize that users of AI do not have a protected right to receive AI outputs, and any regulations on AI should not trigger strict constitutional scrutiny.

## Method Summary
The paper employs legal-theoretic analysis comparing generative AI to protected speech categories (code, corporate speech, expressive conduct) while distinguishing between model generation and user application of outputs. The authors examine foundation models (GPT-4, Claude, Gemini, Llama) and relevant legal precedents to establish that AI lacks the intentionality required for First Amendment protection. The method involves analyzing technical descriptions of how LLMs generate tokens and mapping these mechanisms against constitutional speech requirements.

## Key Results
- Foundation models lack intentionality, sentience, and self-awareness—prerequisites for protected speech
- Neither developers nor models qualify as constitutionally recognized speakers for foundation model outputs
- Granting First Amendment protection to GenAI outputs would trigger strict scrutiny, severely constraining regulatory capacity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: First Amendment protections require human intentionality as a necessary condition for "speech" to exist.
- Mechanism: The legal concept of protected speech requires both (1) a human origin and (2) communicative intent. GenAI outputs fail this test because models are statistical token predictors with no sentience, self-awareness, or intent to convey meaning. Without these prerequisites, outputs are non-expressive conduct, not speech.
- Core assumption: U.S. courts will maintain the historical requirement that protected speech originates from intentional human expression.
- Evidence anchors:
  - [abstract] "because these models lack intentionality, their outputs do not constitute speech as understood in the context of established legal precedent"
  - [section IV] "Intentionality, in turn, requires both sentience (the ability to feel, perceive, or experience subjectivity) and self-awareness"
  - [corpus] Weak direct support; neighboring papers focus on copyright rather than First Amendment doctrine
- Break condition: If courts extend "speech" to include probabilistic outputs that humans interpret meaningfully, regardless of source intent.

### Mechanism 2
- Claim: Neither developers nor models qualify as constitutionally recognized speakers for foundation model outputs.
- Mechanism: Developer speech requires intent to convey specific messages, but foundation models are intentionally trained for broad capability, not particular outputs. Developers cannot predict specific token associations. Models lack agency entirely. Without an identifiable speaker, there is no speech to protect.
- Core assumption: The legal distinction between creating a tool and being responsible for all tool outputs will hold for AI systems.
- Evidence anchors:
  - [section V.A] "developers do not intend for foundation models to convey any particular message"
  - [section V.B] "A model, like a parrot, generates coherent outputs, but researchers have shown that models do not understand what they generate"
  - [corpus] Not directly addressed in neighboring papers
- Break condition: If courts attribute developer intent broadly to all outputs, or if models achieve sufficient agency to qualify as speakers.

### Mechanism 3
- Claim: Granting First Amendment protection to GenAI outputs would trigger strict scrutiny, severely constraining regulatory capacity.
- Mechanism: If outputs are "speech," content-based regulations face strict scrutiny (compelling interest + narrow tailoring). Content-neutral regulations face intermediate scrutiny. This would block licensure systems, output restrictions, and many transparency requirements.
- Core assumption: Courts will not create a new intermediate scrutiny category specifically for AI-generated content.
- Evidence anchors:
  - [section III.A] "Such regulations may trigger the prior restraint doctrine by preventing the model from communicating"
  - [section III.B] "courts may construe such reports as compelled speech, and compelled speech invites strict scrutiny"
  - [corpus] Constitutional Law and AI Governance paper (FMR 0.70) examines First Amendment constraints on AI regulation
- Break condition: If courts apply rational basis or create a new scrutiny standard for non-human-generated content.

## Foundational Learning

- Concept: Strict vs. Intermediate Scrutiny
  - Why needed here: Determines how hard it is for government to defend a regulation. Strict scrutiny (compelling interest + narrow tailoring) is nearly impossible to satisfy; intermediate (important interest + substantial relation) is more manageable. The paper's argument hinges on which standard applies.
  - Quick check question: If a law restricts model outputs based on their content (e.g., no misinformation), which scrutiny level would apply if outputs are "speech"?

- Concept: Prior Restraint Doctrine
  - Why needed here: A prior restraint blocks speech before it occurs. The paper argues that model licensing requirements could be prior restraints if outputs are speech, creating an extremely high bar for regulation.
  - Quick check question: Why would a pre-deployment model approval system potentially qualify as a prior restraint?

- Concept: Token Prediction and Statistical Language Generation
  - Why needed here: Understanding that LLMs generate text via probability distributions over tokens—not reasoning or intent—is central to the paper's claim that outputs lack intentionality.
  - Quick check question: How does the token prediction mechanism differ fundamentally from how humans form communicative intent?

## Architecture Onboarding

- Component map: Intent Assessment Layer -> Speaker Identification Layer -> Scrutiny Assignment Layer -> Regulatory Viability

- Critical path: Intent Assessment → Speaker Identification → Scrutiny Assignment → Regulatory Viability
  - If "no intent" at step 1, skip to "non-speech" at step 3 (rational basis applies)

- Design tradeoffs:
  - Broader "speech" definition → stronger corporate speech rights → weaker regulation → higher misinformation risk
  - Narrower "speech" definition → stronger regulatory capacity → potential overreach risk (though other constitutional protections remain, per Section VI.C)

- Failure signatures:
  - Over-attribution: Treating model outputs as developer speech when developers disclaimed specific outputs
  - False equivalence: Treating stochastic generation as functionally equivalent to human communication
  - Scrutiny mismatch: Applying strict scrutiny to non-expressive conduct

- First 3 experiments:
  1. Map existing GenAI regulations (EU AI Act, state-level laws) against scrutiny levels to predict constitutional vulnerability if outputs are deemed speech.
  2. Test the developer intent argument: Document explicit disclaimers from major AI companies about outputs not representing their views.
  3. Analyze case law on non-human communication (animals, algorithms, automated systems) to identify precedent boundaries for "speaker" requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what point during model training or development do speech rights attach, if ever—when output becomes mostly coherent, fully grammatical, or some other threshold?
- Basis in paper: [explicit] The authors ask: "Is the gibberish output from the beginning of training protected speech? If not, when do we decide to attach protections? When the output has a few understandable words? Mostly understandable words? Perfect grammar?"
- Why unresolved: No legal precedent or technical framework exists for determining when a stochastic system's outputs cross from non-speech to potentially protected speech.
- What evidence would resolve it: Judicial rulings establishing clear criteria, or legislative standards defining thresholds based on model capabilities or coherence metrics.

### Open Question 2
- Question: What level of judicial scrutiny should courts apply when evaluating government regulations that restrict GenAI outputs?
- Basis in paper: [explicit] The authors argue rational basis is too low, strict scrutiny is too high, and propose "some form of heightened scrutiny akin to a weak version of intermediate scrutiny" without specifying the exact standard.
- Why unresolved: Courts have not yet ruled on how traditional First Amendment scrutiny levels map onto non-human, non-expressive conduct that may affect downstream speech.
- What evidence would resolve it: Appellate court decisions establishing the appropriate scrutiny level, or scholarly consensus on adapting existing frameworks to GenAI regulation.

### Open Question 3
- Question: How much human direction or curation is required before GenAI outputs could potentially qualify for First Amendment protection?
- Basis in paper: [explicit] The authors note: "Outputs may only be speech when the developer directs the model to produce a particular or distinct output. Merely applying an insignificant filter does not automatically turn non-speech into speech."
- Why unresolved: The boundary between tool-like generation and human-directed expression remains undefined for AI systems that operate probabilistically.
- What evidence would resolve it: Legal analysis comparing supervised fine-tuning, prompt engineering, and system prompts to established categories of protected human expression.

## Limitations

- The threshold for "significant human involvement" that would convert model outputs into protected speech remains undefined, creating ambiguity for fine-tuned or directed applications.
- The proposed "weak intermediate scrutiny" standard for AI regulation is theoretical—no empirical test exists for how courts would actually apply scrutiny levels to non-human-generated content.
- While the paper distinguishes between generation and use of outputs, the practical boundary between these categories may blur in real-world applications, potentially affecting how downstream speech is protected.

## Confidence

- **High confidence**: The core claim that foundation models lack intentionality, sentience, and self-awareness is well-supported by technical literature on LLM token prediction mechanisms and aligns with established legal requirements for protected speech.
- **Medium confidence**: The argument that neither developers nor models qualify as speakers for foundation model outputs, while logically sound, may face challenges if courts attribute developer intent more broadly or if models develop emergent capabilities suggesting agency.
- **Medium confidence**: The claim that granting First Amendment protection to AI outputs would severely constrain regulatory capacity is theoretically sound but untested, as no AI-specific scrutiny standard exists in current jurisprudence.

## Next Checks

1. **Scrutiny mapping experiment**: Analyze existing GenAI regulations (EU AI Act, state-level laws) against potential scrutiny levels to predict constitutional vulnerability if outputs are deemed speech versus non-speech.

2. **Developer intent documentation**: Test the developer intent argument by documenting explicit disclaimers from major AI companies about outputs not representing their views, establishing a record of disclaimed responsibility.

3. **Precedent boundary analysis**: Examine case law on non-human communication (animals, algorithms, automated systems) to identify clear precedent boundaries for "speaker" requirements and test whether these would extend to foundation models.