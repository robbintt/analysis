---
ver: rpa2
title: Block-Recurrent Dynamics in Vision Transformers
arxiv_id: '2512.19941'
source_url: https://arxiv.org/abs/2512.19941
tags:
- depth
- block
- raptor
- similarity
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Block-Recurrent Hypothesis (BRH), arguing
  that trained Vision Transformers (ViTs) can be accurately rewritten using only a
  small number of distinct blocks applied recurrently across depth. The authors provide
  empirical evidence across diverse ViTs showing block-structured representational
  similarity matrices, then operationalize this hypothesis by training block-recurrent
  surrogates called Raptors.
---

# Block-Recurrent Dynamics in Vision Transformers

## Quick Facts
- arXiv ID: 2512.19941
- Source URL: https://arxiv.org/abs/2512.19941
- Authors: Mozes Jacobs; Thomas Fel; Richard Hakim; Alessandra Brondetta; Demba Ba; T. Andy Keller
- Reference count: 40
- Primary result: Trained ViTs can be compressed into 2-4 recurrent blocks recovering 96-98.5% accuracy

## Executive Summary
This paper introduces the Block-Recurrent Hypothesis (BRH), arguing that trained Vision Transformers (ViTs) can be accurately rewritten using only a small number of distinct blocks applied recurrently across depth. The authors provide empirical evidence across diverse ViTs showing block-structured representational similarity matrices, then operationalize this hypothesis by training block-recurrent surrogates called Raptors. They demonstrate that Raptors can recover 96% of DINOv2 ImageNet-1k linear probe accuracy using only 2 recurrent blocks at equivalent computational cost. Through dynamical systems analysis, they find directional convergence into class-dependent angular basins, token-specific dynamics with cls executing sharp late reorientations while patch tokens show mean-field-like coherence, and late-layer updates collapsing to low-rank subspaces consistent with convergence to low-dimensional attractors.

## Method Summary
The method involves three key stages: First, extract layer-layer representational similarity matrices from a pretrained ViT (typically DINOv2) on validation data. Second, apply a contiguous max-cut dynamic programming algorithm to partition layers into k phases where layers within each phase show high similarity. Third, train Raptor surrogate models using a two-stage approach: Stage 1 uses teacher-forcing with autoregressive supervision to learn individual block transitions, while Stage 2 fine-tunes the entire recurrent chain end-to-end with pure autoregressive loss. The Raptors are architecturally identical to the original blocks but with shared weights and learned depth-scaling embeddings.

## Key Results
- Raptors with 2 recurrent blocks recover 96% of DINOv2 ImageNet-1k linear probe accuracy
- Layer-layer representational similarity strongly predicts Raptor reconstruction fidelity (R² ~0.94 with stochastic depth)
- ViT dynamics show directional convergence to class-dependent angular attractors on the unit sphere
- Patch tokens exhibit mean-field coherence while cls tokens perform sharp late reorientations

## Why This Works (Mechanism)

### Mechanism 1
Trained ViTs exhibit emergent block-recurrent structure that enables functional compression into few repeated blocks. Stochastic depth regularization during training increases layer-layer representational similarity, which correlates with successful recurrent approximation (R² improvement from ~0.88 to ~0.94 as SD increases from 0.0 to 0.6). Representational similarity within contiguous layer groups reflects functional equivalence, not merely geometric proximity. Evidence shows between-layer similarity matrices suggest few contiguous phases, and strong positive association between ViT's layer-layer representational similarity and Raptor reconstruction fidelity. Break condition: If intra-block layer swaps cause accuracy collapse, the hypothesis fails. Figure 15 shows intra-block swaps preserve accuracy while inter-block swaps cause collapse.

### Mechanism 2
Hybrid teacher-forcing + autoregressive training stabilizes recurrent distillation while ensuring closed-loop consistency. Stage 1 uses teacher forcing (λ > 0) to learn local transitions with ground-truth inputs; Stage 2 anneals λ → 0 and trains full autoregressive rollout, forcing blocks to handle their own prediction errors. Teacher forcing provides stable gradients for early learning, but autoregressive training is necessary for inference-time deployment. Evidence shows teacher forcing alone leads to complete collapse with poor accuracy (~3% on ImageNet-1K), and Table 2 shows +68.8% accuracy gain when adding autoregressive loss with TF annealing. Break condition: If Stage 2 (end-to-end autoregressive fine-tuning) doesn't improve over Stage 1 alone, the two-stage approach is unnecessary. Table 2 shows +5.7% gain from Second Stage.

### Mechanism 3
ViT depth implements directional convergence to class-dependent angular attractors with phase-local dynamics. Normalized token states ẋ_ℓ evolve on the unit sphere S^(d-1), with late layers showing: (i) low stable rank (~6), (ii) high patch coherence (mean-field alignment), (iii) cls-specific late reorientation. Norm growth precludes Euclidean attractors; directional analysis on the sphere captures the true computational geometry. Evidence shows γ_ℓ (cosine to final state) saturating near 1 with S-shaped curves, and cls tokens exhibit sharp late reorientations while registers show minimal angular speed variance. Break condition: If perturbations don't self-correct back toward unperturbed trajectories, attractor basin claim fails. Figure 10 shows perturbed paths bend back toward the baseline path.

## Foundational Learning

- Concept: **Teacher Forcing vs. Autoregressive Training**
  - Why needed here: Raptor uses hybrid training; understanding the train-test mismatch is essential for debugging convergence failures.
  - Quick check question: Given a 3-block Raptor with λ=0.5, what happens to gradients when block 2 receives its own prediction rather than ground truth?

- Concept: **Dynamical Systems on Manifolds (Sphere Convergence)**
  - Why needed here: The paper analyzes token trajectories on S^(d-1); Euclidean intuition about "convergence" will mislead.
  - Quick check question: Why does feature norm growth (Figure 9) preclude standard Euclidean attractor analysis?

- Concept: **Stochastic Depth as Regularization**
  - Why needed here: SD is identified as a causal factor in recurrent compressibility emergence.
  - Quick check question: If SD rate is 0.5, what fraction of layers are dropped per forward pass on average, and how might this encourage weight similarity?

## Architecture Onboarding

- Component map:
  - Max-cut partitioner -> Raptor blocks -> Depth scaling -> Two-stage trainer

- Critical path:
  1. Extract teacher activations → 2. Compute similarity matrix → 3. Max-cut partition → 4. Stage 1 per-block training (20 epochs, λ: 0.5→0) → 5. Stage 2 full AR training → 6. Linear probe fine-tuning

- Design tradeoffs:
  - **k=2 blocks**: Maximum compression (~96% accuracy recovery), but may miss phase-specific dynamics
  - **k=4 blocks**: Near-full recovery (~98.5%), minimal parameter savings
  - **Token loss weighting**: cls upweighting (λ_cls=0.45 in Stage 2) critical for classification tasks

- Failure signatures:
  - ~3% accuracy → Stage 1-only training (no AR)
  - High early-layer R² but low final-layer R² → Depth scaling missing or λ annealing too fast
  - Patch token reconstruction fails while cls succeeds → Check λ_patch weighting

- First 3 experiments:
  1. **Sanity check**: Train Raptor(k=2) on CIFAR-100 ViT with SD=0.0 vs SD=0.4; verify R² improves with SD
  2. **Ablation**: Run Stage 1 only (skip Stage 2) on DINOv2 subset; confirm accuracy collapse per Table 2
  3. **Dynamics probe**: Plot γ_ℓ for cls vs patch tokens; verify cls shows late saturation while patch converges earlier

## Open Questions the Paper Calls Out

- **Question**: What are the causal mechanisms that drive the emergence of block-recurrent structure during training, beyond the correlational role of stochastic depth?
- **Basis in paper**: The authors state: "While residual pathways and stochastic depth appear implicated in block recurrence, isolating causal mechanisms will require controlled training-dynamics at scale."
- **Why unresolved**: The paper demonstrates correlation between stochastic depth and recurrent compressibility, but does not establish causation or identify whether depth, residual architecture, training dynamics, or optimization properties are the primary drivers.
- **What evidence would resolve it**: Ablation studies systematically varying individual architectural components (residual connections, stochastic depth rates, layer normalization) while measuring recurrent compressibility; training dynamics analysis tracking when block structure emerges during optimization.

- **Question**: What causes the remaining 4% performance gap between Raptor surrogates and full DINOv2 models, and can this gap be closed without increasing the number of recurrent blocks?
- **Basis in paper**: The authors note: "although two tied blocks recover most of DINOv2, a small residual gap remains that may call for improved recurrent distillation or additional time-varying components."
- **Why unresolved**: The paper demonstrates the existence of the gap but does not determine whether it stems from fundamental limitations of the recurrent approximation, suboptimal training procedures, or irreducible complexity in the original model that cannot be captured by parameter-tied blocks.
- **What evidence would resolve it**: Systematic analysis of layer-wise reconstruction errors; comparing different distillation objectives; investigating whether the gap concentrates in specific token types or layers; testing whether learned depth-conditioning or input-dependent block selection closes the gap.

- **Question**: Do the observed block-recurrent dynamics and angular attractor geometry generalize across diverse vision transformer architectures, training objectives, and modalities?
- **Basis in paper**: The paper validates BRH primarily on DINOv2 with brief mention of SigLIP and ViT-L. The generalization to other architectures (e.g., Swin transformers,ConvNeXt hybrids), training paradigms (MAE, CLIP), and modalities (video, multimodal) remains unexplored.
- **Why unresolved**: The empirical validation focuses on a specific family of models, leaving open whether BRH reflects a universal property of attention-based vision systems or characteristics particular to DINOv2's training procedure.
- **What evidence would resolve it**: Applying Raptor methodology to a systematic benchmark of diverse architectures and training objectives; measuring whether phase structure and recurrent compressibility correlate with architectural features or training properties across model families.

## Limitations

- The work focuses exclusively on ViT architectures, leaving open whether the observed block-recurrent structure generalizes to other transformer variants like Swin, DeiT, or ConvNext.
- The dynamical systems analysis, while compelling, relies on angular metrics on the unit sphere that may not capture all relevant computational behaviors.
- The phase boundary detection via max-cut assumes contiguous block structures, potentially missing more complex recurrence patterns.

## Confidence

- **High**: The empirical demonstration that Raptors can recover 96% of DINOv2 accuracy with 2 recurrent blocks is robust and well-supported by multiple experiments.
- **Medium**: The link between stochastic depth and recurrent compressibility is supported but could benefit from more systematic ablation studies across different ViT sizes and training regimes.
- **Medium**: The dynamical systems analysis showing convergence to angular attractors is methodologically sound but based on correlation patterns that could have alternative explanations.

## Next Checks

1. Test Raptor compression across ViT-S and ViT-L variants to determine if the block-recurrent structure scales with model size or is specific to ViT-B.
2. Perform systematic ablation of stochastic depth rates (0.0 to 0.6) to quantify the exact relationship between SD and recurrent compressibility, including measuring representational similarity changes directly.
3. Validate the attractor basin hypothesis by measuring recovery dynamics from larger perturbations (e.g., 30-50% of token values) and checking if the return time correlates with basin depth estimates from the current γ_ℓ analysis.