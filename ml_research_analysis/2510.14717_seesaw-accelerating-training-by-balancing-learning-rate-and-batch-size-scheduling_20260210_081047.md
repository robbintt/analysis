---
ver: rpa2
title: 'Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling'
arxiv_id: '2510.14717'
source_url: https://arxiv.org/abs/2510.14717
tags:
- batch
- size
- learning
- rate
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Seesaw introduces a principled batch size scheduling algorithm
  for large language model pretraining, addressing the lack of theoretical grounding
  in existing batch ramp strategies. The method doubles the batch size whenever the
  learning rate would halve in standard schedulers, reducing serial training steps
  while preserving loss dynamics.
---

# Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling

## Quick Facts
- arXiv ID: 2510.14717
- Source URL: https://arxiv.org/abs/2510.14717
- Reference count: 40
- Key outcome: First finite-sample equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy linear regression; reduces wall-clock time by ~36% while matching cosine decay validation loss.

## Executive Summary
Seesaw introduces a principled batch size scheduling algorithm for large language model pretraining, addressing the lack of theoretical grounding in existing batch ramp strategies. The method doubles the batch size whenever the learning rate would halve in standard schedulers, reducing serial training steps while preserving loss dynamics. Theoretically, it establishes the first finite-sample equivalence between learning rate decay and batch size ramp-up for SGD on noisy linear regression, extending this to normalized SGD as a proxy for Adam under a variance-dominated regime. Empirically, Seesaw matches cosine decay performance at equal FLOPs while reducing wall-clock time by approximately 36% on 150M/300M/600M-parameter models trained at Chinchilla scale, approaching the theoretical limit implied by the analysis.

## Method Summary
Seesaw is a scheduler wrapper that converts standard learning rate decay schedules into equivalent batch size ramp-up schedules. At each decay point where a baseline scheduler would halve the learning rate, Seesaw instead multiplies the learning rate by 1/√2 and doubles the batch size. This preserves the effective learning rate dynamics while reducing the total number of serial training steps. The method requires minimal changes to existing training loops, needing only a scheduler interface that returns current (learning rate, batch size) pairs. Implementation involves determining decay points from the baseline schedule, applying the √2 scaling rule at each point, and ensuring batch sizes remain at or below the critical batch size.

## Key Results
- First finite-sample proof of equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy linear regression
- Reduces wall-clock training time by ~36% while matching cosine decay validation loss at equal FLOPs
- Theoretical maximum speedup is 2/π ≈ 63.7% serial steps; Seesaw achieves ~36% in practice
- Works with AdamW optimizer on 150M, 300M, and 600M parameter models at Chinchilla scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For SGD on noisy linear regression, learning-rate decay and batch-size ramp-up produce equivalent excess risk when αβ is held constant.
- Mechanism: The variance component of risk scales as η²σ²/B. Doubling B and halving η preserves η²/B, so noise dynamics are equivalent. The bias component converges comparably because both processes see the same total samples per phase.
- Core assumption: Bounded risk throughout training (Assumption 1: R(wt) ≤ cσ² after first schedule change).
- Evidence anchors: [abstract] "first finite-sample proof of equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy linear regression"; [Section 5, Theorem 1] Provides the formal equivalence with constant-factor bounds.

### Mechanism 2
- Claim: For normalized SGD (NSGD), equivalence requires α√β = constant, not αβ = constant.
- Mechanism: NSGD normalizes by E[||gt||²]. Under Assumption 2, E[||gt||²] ∝ σ²/B, so the effective learning rate becomes η̃ ∝ η√B. To preserve η̃ across phases, we need η₁/√β₁ = η₂/√β₂ when α₁√β₁ = α₂√β₂.
- Core assumption: Variance-dominated regime where E[||gt||²] ∝ σ²/B (Assumption 2).
- Evidence anchors: [Section 3.1] Derives NSGD update rule and shows reduction to SGD with rescaled learning rate; [Section 5, Corollary 1] Formal equivalence statement for normalized SGD.

### Mechanism 3
- Claim: The most aggressive schedule without divergence is α = √β, yielding maximum ~36.3% serial runtime reduction under cosine decay.
- Mechanism: If α < √β, the effective learning rate η̃ ∝ η(√β/α)^k grows unbounded across phases k, eventually exceeding η_max and causing divergence. At the boundary α = √β, step count reduces by factor 2/π ≈ 0.637.
- Core assumption: The schedule must satisfy α ≥ √β to prevent effective learning rate from growing.
- Evidence anchors: [Section 3.2, Lemma 1] Derives 2T/π step count for cosine decay equivalence; [Section 4.1, Figure 2] Shows α = 1, β = 4 (too aggressive) underperforms, while α = √2, β = 2 matches baseline.

## Foundational Learning

- **Bias-variance decomposition in SGD**: The equivalence proof relies on separately analyzing how bias (signal) and variance (noise) terms respond to LR and batch size changes.
  - Quick check: Can you explain why increasing batch size reduces variance but not bias in gradient estimates?

- **Critical batch size (CBS)**: Seesaw only works at or below CBS; beyond this, Assumption 2 fails and equivalence breaks down.
  - Quick check: What happens to sample efficiency if you double batch size beyond the critical batch size?

- **Normalized gradient descent vs. Adam**: The paper uses NSGD as a tractable proxy for Adam; understanding this simplification is crucial for knowing when results transfer.
  - Quick check: How does NSGD's single-scalar preconditioner differ from Adam's per-coordinate adaptation?

## Architecture Onboarding

- **Component map**: Baseline scheduler -> Seesaw scheduler wrapper -> Batch allocator -> Data loader -> Training loop

- **Critical path**:
  1. Identify decay points from baseline LR schedule (where LR would halve)
  2. At each decay point: multiply LR by 1/√α, multiply batch size by α (typically α = 2, so LR × 1/√2, B × 2)
  3. Ensure batch size never exceeds critical batch size estimate
  4. Warm up LR for first 10% of tokens before any decay/batch changes

- **Design tradeoffs**:
  - **α = √β (most aggressive)**: Maximum speedup but brittle near CBS boundary
  - **Larger α, smaller β**: More conservative, safer if CBS estimate is uncertain
  - **Hardware parallelism**: Speedup only materializes if you have unused parallel capacity to handle larger batches
  - **Memory vs. compute**: Larger batches need more memory; may need gradient accumulation, which eats into serial speedup

- **Failure signatures**:
  - **Divergence after multiple phases**: α < √β (check Lemma 4)
  - **Loss fails to match baseline**: Batch size exceeded CBS; try reducing initial B
  - **Instability with z-loss**: Paper notes potential issues; consider disabling or tuning separately
  - **No wall-clock speedup**: Parallelism bottleneck; increasing B doesn't reduce step time

- **First 3 experiments**:
  1. **Validation on small scale**: Train 150M model with cosine decay baseline. Implement Seesaw with α = √2, β = 2 at cosine's half-life points. Compare final validation loss and step count.
  2. **CBS boundary test**: At fixed model size, run Seesaw with initial batch sizes [128, 256, 512, 1024]. Plot final loss vs. initial B to find where Seesaw degrades relative to cosine.
  3. **Weight decay compatibility**: Sweep weight decay λ ∈ {1e-6, 1e-4, 1e-2} with Seesaw on 150M model. Confirm best (η, λ) transfers from cosine to Seesaw (paper finds (0.003, 0.0001) works for both).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Seesaw schedule be adapted to stabilize auxiliary losses, specifically z-loss, during the final stages of training?
- Basis in paper: [explicit] Appendix E notes instabilities with z-loss at the end of training for 600M models, stating the authors "leave this study for future work."
- Why unresolved: The standard Seesaw scaling rule appears to conflict with z-loss dynamics late in training, causing spikes that do not occur with cosine decay.
- What evidence would resolve it: A modified scaling rule for Seesaw that maintains z-loss stability without sacrificing the wall-clock speedup, validated on models ≥ 600M parameters.

### Open Question 2
- Question: Does a batch-size ramp equivalent to learning-rate decay exist for Normalized SGD/Adam when training far beyond the critical batch size (CBS)?
- Basis in paper: [explicit] Section 4.2 hypothesizes that past a certain batch size, "it is not possible to match the performance of learning rate decay by any equivalent batch size ramp up."
- Why unresolved: While Seesaw matches dynamics at or below CBS, the authors observe a widening performance gap at larger batch sizes (e.g., 8192), theorizing that mean-dominated gradients prevent equivalence.
- What evidence would resolve it: A formal proof showing impossibility for Normalized Gradient Descent (NGD) in the mean-dominated regime, or the discovery of a novel scaling law that bridges the gap.

### Open Question 3
- Question: Can the theoretical equivalence between learning-rate decay and batch-size ramp-up be rigorously extended from Normalized SGD (NSGD) to the full Adam optimizer?
- Basis in paper: [inferred] The theoretical analysis relies on NSGD as a "tractable proxy" for Adam (Section 3.1), but the empirical results utilize AdamW.
- Why unresolved: The extent to which Adam's coordinate-wise adaptivity and second-moment estimation preserve the η / √2 and B × 2 equivalence derived for NSGD remains unproven.
- What evidence would resolve it: A finite-sample analysis of Adam under Seesaw scheduling that accounts for the non-linear denominator v_t, confirming the proxy's accuracy.

## Limitations

- The equivalence proof relies critically on Assumptions 1 (bounded risk) and 2 (variance-dominated regime), which may not hold for highly non-convex objectives or when α and β are too aggressive.
- The critical batch size estimate and its interaction with Seesaw scheduling are not systematically characterized, creating uncertainty about when the method will fail.
- The extension from Normalized SGD to full Adam remains theoretical rather than rigorously proven, introducing approximation error.

## Confidence

*High Confidence*: The core theoretical result establishing finite-sample equivalence between LR decay and batch ramp-up for SGD on noisy linear regression. The proof structure is rigorous and the assumptions are clearly stated. The empirical observation that Seesaw reduces wall-clock time by ~36% at Chinchilla scale is well-supported by controlled experiments across three model sizes.

*Medium Confidence*: The extension to normalized SGD as a proxy for Adam. While the theoretical framework is sound and Malladi et al. (2022) provides supporting analysis through SDE lenses, the simplification from Adam's per-coordinate adaptation to NSGD's single-scalar preconditioner introduces approximation error that isn't fully quantified.

*Low Confidence*: The critical batch size estimate and its interaction with Seesaw scheduling. The paper states that Seesaw should operate at or below CBS, but doesn't provide a method for estimating CBS in practice, and the empirical results don't systematically explore the CBS boundary.

## Next Checks

1. **CBS Boundary Characterization**: Systematically sweep initial batch sizes below and above estimated CBS for a fixed model size (e.g., 150M), measuring final validation loss and wall-clock time to identify the precise CBS threshold where Seesaw performance degrades.

2. **Full Adam vs NSGD Transfer**: Implement the complete Seesaw algorithm using AdamW (not just NSGD) and compare performance to the NSGD-based implementation. Measure whether per-coordinate adaptation in Adam provides meaningful improvements or divergences from the NSGD equivalence.

3. **Long-Training Regime Validation**: Extend training runs beyond 3B tokens (Chinchilla scale) to 10B+ tokens, monitoring for potential accumulation of approximation errors or divergence that might emerge in extended training sessions.