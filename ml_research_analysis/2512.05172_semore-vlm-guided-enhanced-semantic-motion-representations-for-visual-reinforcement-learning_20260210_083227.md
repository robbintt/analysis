---
ver: rpa2
title: 'Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement
  Learning'
arxiv_id: '2512.05172'
source_url: https://arxiv.org/abs/2512.05172
tags:
- learning
- motion
- visual
- representations
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Semore introduces a novel VLM-guided framework for visual reinforcement
  learning that addresses the challenge of limited representation learning capability
  in visual RL. The method employs a dual-stream network to separately extract semantic
  and motion representations from RGB flows, while utilizing VLM with common-sense
  knowledge to retrieve key information from observations.
---

# Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.05172
- Source URL: https://arxiv.org/abs/2512.05172
- Reference count: 10
- Primary result: Achieves state-of-the-art performance in CARLA benchmarks with 201±54 episode reward in JayWalk scenarios

## Executive Summary
Semore introduces a novel VLM-guided framework for visual reinforcement learning that addresses the challenge of limited representation learning capability in visual RL. The method employs a dual-stream network to separately extract semantic and motion representations from RGB flows, while utilizing VLM with common-sense knowledge to retrieve key information from observations. A separately supervised approach is adopted to simultaneously guide the extraction of semantics and motion, allowing them to interact spontaneously. The method achieves state-of-the-art performance in CARLA benchmarks, outperforming other methods with an episode reward of 201±54 in JayWalk scenarios, a driving distance of 233±66 meters, and a crash intensity of 2043±98.

## Method Summary
Semore employs a dual-stream architecture where a semantic encoder extracts environmental information while a motion encoder captures temporal dynamics from frame differences. The system leverages a vision-language model (Qwen2-VL-7B-Instruct) with common-sense knowledge to retrieve key semantic information from observations through carefully crafted prompts. A separately supervised approach guides both encoders simultaneously, with a cross-attention mechanism allowing semantic features to enhance motion representation learning. The framework also implements a selective replay buffer mechanism that prioritizes high-quality transitions based on a decay factor, helping to mitigate catastrophic forgetting during training.

## Key Results
- Achieves 201±54 episode reward in CARLA JayWalk scenarios
- Reaches 233±66 meters driving distance with 2043±98 crash intensity
- Outperforms state-of-the-art methods across multiple CARLA benchmarks (JayWalk, HighBeam, HighWay)

## Why This Works (Mechanism)
The method's effectiveness stems from its dual-stream architecture that separately processes semantic and motion information, preventing interference between these complementary representations. By leveraging VLM with common-sense knowledge, the system can extract meaningful semantic information that guides representation learning beyond what's visible in the raw observations. The separately supervised approach with bidirectional cross-attention allows semantic and motion representations to enhance each other dynamically during training. The selective replay buffer mechanism ensures the agent focuses on high-quality transitions, preventing degradation of learned representations over time.

## Foundational Learning
- **Visual Reinforcement Learning**: Training agents to make decisions from visual inputs rather than low-dimensional state representations. Needed because real-world applications like autonomous driving require processing raw camera feeds.
- **Dual-stream Architecture**: Processing different types of information (semantic vs motion) through separate neural network pathways. Required to prevent interference between complementary representations and capture different aspects of the environment.
- **Vision-Language Models**: Multimodal models that can process both visual inputs and text, extracting semantic knowledge. Essential for providing common-sense understanding that guides representation learning beyond raw visual features.
- **Selective Replay Buffers**: Experience replay mechanisms that prioritize high-quality transitions based on certain criteria. Needed to prevent catastrophic forgetting and focus learning on the most informative experiences.
- **Cross-attention Mechanisms**: Neural network operations that allow information to flow bidirectionally between different representations. Required to enable semantic features to enhance motion representation learning dynamically.

## Architecture Onboarding
**Component Map:** Observation frames -> Semantic encoder (4-layer CNN + FC) -> Semantic features -> VLM (Qwen2-VL-7B-Instruct) -> Knowledge masks -> Motion encoder (4-layer CNN + FC) -> Frame differences -> Cross-attention -> Policy/value networks (SAC) -> Action selection

**Critical Path:** Visual observations flow through dual-stream encoders, with semantic information enhanced by VLM and motion information refined through cross-attention, ultimately feeding into SAC policy and value networks for action selection.

**Design Tradeoffs:** Separate encoders prevent interference but increase model complexity; VLM guidance provides semantic understanding but introduces computational overhead and dependency on prompt quality; selective replay improves sample efficiency but requires careful threshold tuning.

**Failure Signatures:** Poor semantic mask quality from CRIS leading to noisy supervision; motion representation degradation when cross-attention weights are imbalanced; training instability when selective replay buffer becomes too selective too quickly.

**First Experiments:** 1) Test dual-stream encoders independently on CARLA baseline to verify baseline performance. 2) Validate VLM integration by testing different prompt templates on semantic mask generation quality. 3) Evaluate selective replay buffer impact by comparing with standard replay buffer across training curves.

## Open Questions the Paper Calls Out
None

## Limitations
- Results depend heavily on specific VLM prompt engineering details not fully specified in the paper
- Selective replay buffer mechanism requires careful tuning of decay factor δ and threshold parameters
- Reward function formulation in CARLA scenarios is vaguely described, potentially affecting reproducibility

## Confidence
- **High Confidence**: The dual-stream architecture design and the general VLM-guided supervision approach are well-documented and theoretically sound.
- **Medium Confidence**: The reported benchmark results (JayWalk: 201±54 reward, 233±66m distance, 2043±98 crash intensity) are promising, but depend heavily on implementation details not fully specified in the paper.
- **Low Confidence**: The specific prompt engineering for VLM and the exact CRIS integration pipeline, which are critical for semantic mask generation and knowledge extraction.

## Next Checks
1. **VLM Prompt Validation**: Test different prompt templates for object extraction and action evaluation to verify sensitivity to prompt engineering and establish robust defaults.
2. **CRIS Integration Verification**: Implement and validate the CRIS segmentation pipeline with multiple checkpoints to ensure consistent mask generation quality.
3. **Reward Function Specification**: Implement and test the CARLA reward function with explicit formulations for crash avoidance and distance maximization to ensure alignment with the paper's objectives.