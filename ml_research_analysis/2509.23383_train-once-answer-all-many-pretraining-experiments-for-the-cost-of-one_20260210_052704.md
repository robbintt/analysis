---
ver: rpa2
title: 'Train Once, Answer All: Many Pretraining Experiments for the Cost of One'
arxiv_id: '2509.23383'
source_url: https://arxiv.org/abs/2509.23383
tags:
- training
- data
- experiments
- experiment
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to reduce the computational
  cost of pretraining experiments by conducting multiple experiments simultaneously
  within a single training run. The authors demonstrate this approach by training
  a 1.5B parameter model on 210B tokens while simultaneously conducting ten different
  experiments, including replications of previous work on memorization, contamination,
  poisoning, and forgetting, as well as novel experiments on knowledge acquisition,
  mathematical reasoning, and watermarking.
---

# Train Once, Answer All: Many Pretraining Experiments for the Cost of One

## Quick Facts
- arXiv ID: 2509.23383
- Source URL: https://arxiv.org/abs/2509.23383
- Authors: Sebastian Bordt; Martin Pawelczyk
- Reference count: 40
- One-line result: Train one model to conduct ten different pretraining experiments simultaneously, reducing computational cost by 10x

## Executive Summary
This paper proposes a novel approach to reduce the computational cost of pretraining experiments by conducting multiple experiments simultaneously within a single training run. The authors demonstrate this approach by training a 1.5B parameter model on 210B tokens while simultaneously conducting ten different experiments, including replications of previous work on memorization, contamination, poisoning, and forgetting, as well as novel experiments on knowledge acquisition, mathematical reasoning, and watermarking. They show that all experiments can be successfully replicated and introduce a method called Continual Pretraining Dependence Testing (CPDT) to test for dependencies between experiments before pretraining, finding negligible interactions in their setup.

## Method Summary
The authors train a single 1.5B parameter model (OLMo-2-1B-Exp) on 210B tokens of OLMo-mix-1124 data, simultaneously conducting ten different experimental interventions that modify 1.8% of the training data. Each experiment targets a distinct behavioral aspect of the model, including knowledge acquisition, benchmark contamination, memorization patterns, poisoning, forgetting, watermarking, mathematical reasoning, and factual knowledge dynamics. Before full training, they validate experiment independence using Continual Pretraining Dependence Testing (CPDT), which applies each experiment at high intensity to intermediate checkpoints for 100 steps and measures dependencies across all outcomes. The training uses the same architecture, initialization, and learning rate schedule as the baseline OLMo-2-1B model, allowing direct comparison of experimental effects.

## Key Results
- Successfully replicated findings from five prior works on memorization, contamination, poisoning, and forgetting using joint pretraining
- Demonstrated experiment independence with near-zero off-diagonal entries in CPDT dependence matrix (Figure 4b)
- Showed minimal impact on training dynamics with validation loss, output layer norm, and holdout benchmarks deviating by only ~0.3% from baseline
- Validated the feasibility of knowledge acquisition control and introduced novel experiments on mathematical reasoning and watermarking

## Why This Works (Mechanism)

### Mechanism 1: Independent Behavioral Slices
Multiple pretraining experiments modifying small data fractions can coexist without significant mutual interference because each experiment targets a distinct behavioral slice of the model. With limited data modification (1.8% total), the multitask nature of pretraining allows independent learning on different subproblems without cross-contamination of outcomes.

### Mechanism 2: CPDT for Dependency Detection
Continual Pretraining Dependence Testing (CPDT) can detect inter-experiment dependencies before committing to a full training run by applying each experiment's data intervention at high intensity (~1% of data) to an intermediate checkpoint for a small number of gradient steps (100 steps), measuring all outcome variables across all interventions, and constructing an nÃ—n dependence matrix.

### Mechanism 3: Training Stability to Localized Interventions
Small data modifications produce measurable behavioral changes without destabilizing overall training dynamics because the model's loss landscape is sufficiently high-dimensional that localized interventions create measurable effects on targeted outcomes while leaving aggregate loss and weight norm trajectories nearly unchanged.

## Foundational Learning

- **Causal Inference / No Interference (SUTVA)**: Why needed here - The entire approach rests on the assumption that experiment outcomes are independent. Quick check: Can you explain why Definition 5.1 (experiment independence) is necessary for valid joint experiments? If training on benchmark A improves performance on benchmark B, would joint contamination experiments be valid?

- **Continual Pretraining vs. Full Pretraining**: Why needed here - CPDT relies on the premise that short continual pretraining runs can reveal dependencies that would appear in full pretraining. Quick check: What are the limitations of using a 100-step continual pretraining run to detect dependencies that might require thousands of steps to manifest?

- **Training Dynamics and Stability in LLMs**: Why needed here - Understanding why 1.8% data modification doesn't destabilize training requires familiarity with how loss, gradient norms, and weight norms evolve during pretraining. Quick check: Why does the paper compare weight norm growth between OLMo-2-1B and OLMo-2-1B-Exp? What would divergent norms indicate?

## Architecture Onboarding

- **Component map**: Base model (OLMo-2-1B) -> Training data (OLMo-mix-1124, 210B tokens) -> Experimental layer (10 experiments, 3.7B tokens, 1.8%) -> Learning rate schedule (90k steps baseline, 10k steps decay) -> CPDT module (intermediate checkpoint + 100-step continual pretraining)

- **Critical path**: 1) Select experiments and validate independence via CPDT before full training, 2) Generate/prepare experimental data, 3) Integrate experimental tokens into training batches, 4) Train with monitoring of train loss, validation loss, and outcome-specific probes, 5) Evaluate experiment outcomes at checkpoints and final model

- **Design tradeoffs**: Replacement vs. augmentation (paper uses replacement to maintain token budget), Uniform vs. clustered insertion (uniform distribution chosen), CPDT intensity (1% data modification for 100 steps balances detection power vs. computational cost)

- **Failure signatures**: Divergent training dynamics (loss curves separating from baseline), CPDT-detected dependencies (large off-diagonal entries), Outcome contamination (experiment effects appearing in other evaluations)

- **First 3 experiments**: 1) Knowledge Acquisition (KA): Implement Algorithm 1 to dynamically control factual knowledge frequency, 2) Benchmark Contamination (BC): Insert ground-truth answers at varying repetition rates, 3) Memorization Patterns (MemP): Insert canaries at varying frequencies and lengths

## Open Questions the Paper Calls Out

- What is the maximum number of experiments (or total percentage of modified tokens) that can be conducted in a single training run before dependencies between experiments become statistically significant?

- Can the simultaneous experiment approach be applied to interventions that require modifying a significant fraction of the training data (e.g., >10%) without inducing negative interference?

- How can the CPDT method be adapted to validate "negative constraint" experiments, such as those requiring the strict exclusion of specific data throughout training?

## Limitations

- The study only validated ten specific experiments modifying 1.8% of the data, leaving untested whether larger-scale modifications or experiments targeting overlapping capabilities would maintain independence.

- The analysis of training stability focuses on aggregate metrics rather than fine-grained behavioral analysis, potentially missing subtle degradations in capabilities not explicitly measured.

- CPDT methodology is validated only on the specific experiments and model scale presented, with unknown effectiveness for detecting subtle long-term dependencies requiring full pretraining to manifest.

## Confidence

**High Confidence**: The core finding that multiple pretraining experiments can be conducted simultaneously with minimal interference in this specific experimental setup, supported by robust empirical evidence from training dynamics and outcome measurements.

**Medium Confidence**: The generalizability of the CPDT methodology to detect all types of dependencies before pretraining, as its effectiveness for detecting subtle long-term dependencies remains unproven.

**Low Confidence**: The claim that this approach can scale to substantially larger numbers of experiments or experiments targeting more overlapping behavioral dimensions, as the paper tests only 10 carefully designed independent experiments.

## Next Checks

1. **Stress Test CPDT Sensitivity**: Apply CPDT to detect dependencies between experiments with known interference (e.g., two reasoning interventions on similar problem types) to establish the method's detection threshold and false negative rate.

2. **Scaling Experiment Density**: Systematically increase the fraction of experimental data from 1.8% to higher levels (5%, 10%) while monitoring training stability and dependency emergence to identify the practical limits of the approach.

3. **Cross-Capability Impact Analysis**: Conduct detailed ablation studies where one experiment's success is measured while varying the intensity of a second experiment targeting overlapping capabilities, to quantify the trade-offs between concurrent experimental modifications.