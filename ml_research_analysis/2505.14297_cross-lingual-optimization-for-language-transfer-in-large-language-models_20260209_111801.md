---
ver: rpa2
title: Cross-Lingual Optimization for Language Transfer in Large Language Models
arxiv_id: '2505.14297'
source_url: https://arxiv.org/abs/2505.14297
tags:
- language
- target
- english
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-Lingual Optimization (CLO) is proposed to address the English-centric
  limitations of large language models (LLMs) when adapting to target languages, especially
  in low-resource settings. CLO leverages publicly available English SFT data and
  a translation model to perform cross-lingual transfer by prioritizing responses
  in the same language as the input.
---

# Cross-Lingual Optimization for Language Transfer in Large Language Models

## Quick Facts
- **arXiv ID**: 2505.14297
- **Source URL**: https://arxiv.org/abs/2505.14297
- **Reference count**: 13
- **Primary result**: CLO outperforms SFT in both target language proficiency and English preservation, especially in low-resource settings

## Executive Summary
Cross-Lingual Optimization (CLO) addresses English-centric limitations of large language models when adapting to target languages, particularly in low-resource settings. The method leverages publicly available English SFT data and a translation model to perform cross-lingual transfer by prioritizing responses in the same language as the input. CLO constructs preference pairs where English responses are preferred for English inputs and target language responses are preferred for target language inputs, explicitly teaching input-output language correspondence.

Experiments across six languages (Chinese, German, Korean, Indonesian, Swahili, Yoruba) using five models (Llama-2-7B, Llama-2-13B, Llama-3-8B, Mistral-7B-v0.1, Qwen-2.5-3B) demonstrate that CLO consistently outperforms standard SFT in both target language proficiency and English preservation. Notably, in low-resource languages like Swahili, CLO with only 3,200 samples surpasses SFT with 6,400 samples, showing superior data efficiency.

## Method Summary
CLO fine-tunes only attention layers of LLMs using a combined loss function: $L_{CLO} = 0.5 \cdot L_{SFT} + 0.5 \cdot L_{CL}$. The method constructs cross-lingual datasets by translating English SFT pairs (6,400 samples) into target languages via M2M100 1.2B. The SFT loss applies NLL only to target language outputs, while the cross-lingual loss contrasts preferred vs. rejected responses across languages within the same batch. Training uses a learning rate of 5e-5, $\beta=0.1$, 1 epoch, and batch size 8, with non-attention parameters frozen throughout.

## Key Results
- CLO consistently outperforms SFT across all tested languages and models in both target proficiency and English preservation
- CLO demonstrates superior data efficiency, with Swahili achieving 69.57% win rate using 3,200 samples vs. SFT's 74.93% with 6,400 samples
- Attention-only training matches full training for Chinese and Korean but degrades significantly for Swahili (29.69% vs. 69.57% win rate)

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual preference pairs
CLO constructs preference pairs that explicitly teach input-output language correspondence by contrasting accepted vs. rejected responses across languages within the same batch. This directly optimizes the policy to match output language to input language.

### Mechanism 2: Attention-only training
Training only attention layers is sufficient for language alignment while reducing parameter disruption, based on findings that language-related knowledge is concentrated in attention mechanisms rather than MLP layers.

### Mechanism 3: Target-only NLL loss
Using only target-language NLL loss prevents implicit English bias during optimization, as including English NLL would counteract the preference signal by reinforcing English generation.

## Foundational Learning

**Direct Preference Optimization (DPO)**: Understand how DPO maps reward functions to optimal policies without an explicit reward model. Quick check: Can you explain why DPO uses a reference model $\pi_{ref}$ and what the $\beta$ parameter controls?

**Negative Log-Likelihood (NLL) Loss**: Understand how NLL conditions the model to generate specific outputs given inputs. Quick check: For a sequence $y$ given input $x$, what does minimizing $-\log \pi_\theta(y|x)$ optimize?

**Cross-Lingual Transfer**: Understand why English-centric LLMs struggle with low-resource languages. Quick check: Why might an LLM understand a query in Swahili but still respond in English?

## Architecture Onboarding

**Component map**: English SFT Dataset -> Translation Model (M2M100 1.2B) -> Cross-Lingual Dataset -> CLO Training Loop -> Adapted Model

**Critical path**: 1) Prepare translation pipeline, 2) Translate all English SFT pairs to target language, 3) Construct preference pairs, 4) Implement modified DPO loss, 5) Freeze non-attention parameters during training

**Design tradeoffs**: Attention-only training is computationally cheaper but may fail for very low-resource languages; target-only NLL better enforces language matching but may affect English preservation; batch construction requires paired samples increasing memory overhead

**Failure signatures**: Model responds in English when given target input (preference signal too weak); target performance near zero (base model lacks target language representation); English performance drops significantly (attention-only insufficient); training instability (reduce learning rate or increase $\lambda$)

**First 3 experiments**: 1) Sanity check on high-resource language (Chinese/German) comparing CLO vs SFT on AlpacaEval, 2) Data efficiency test on low-resource language (Swahili) comparing CLO@3,200 vs SFT@6,400, 3) Ablation: attention-only vs full training for lowest-resource target

## Open Questions the Paper Calls Out

**Multiple language transfer**: How can CLO be adapted to facilitate simultaneous transfer to multiple target languages without degrading performance? The current methodology optimizes for a single target language per training run.

**Other preference optimization methods**: Is the cross-lingual loss function effective when applied to preference optimization algorithms other than DPO? The study restricted implementation to DPO, leaving compatibility with KTO or ORPO unverified.

**Cultural nuance capture**: Does training on machine-translated data fail to capture cultural nuances or linguistic characteristics specific to the target language? The evaluation relied on translated English benchmarks rather than native cultural datasets.

## Limitations
- Relies on OpenAssistant dataset filtering without specifying exact criteria beyond "highest ranking first turn samples"
- Translation quality via M2M100 1.2B is assumed adequate but not validated against other translation models
- Attention-only training shows significant degradation for very low-resource languages like Swahili

## Confidence
**High Confidence**: CLO consistently outperforms SFT across all tested languages and models; CLO demonstrates superior data efficiency; combined loss approach provides robust performance

**Medium Confidence**: Attention-only training is sufficient for medium/high-resource languages; target-only NLL loss is superior to target+English NLL; CLO maintains English capability without explicit English NLL

**Low Confidence**: Specific cross-lingual preference mechanism generalizes beyond tested language pairs; attention-only failure for Swahili is solely due to low-resource status; 55% additional memory overhead is acceptable for all deployment scenarios

## Next Checks
1. Repeat entire CLO pipeline using different translation model (GPT-4 or specialized bilingual models) for Swahili and Yoruba to quantify translation quality impact
2. Conduct systematic ablation study testing different attention layer subsets for Swahili to identify critical attention mechanisms for low-resource transfer
3. Select intermediate-resource language (Hindi or Vietnamese) to compare CLO performance against SFT and other cross-lingual transfer methods across full resource spectrum