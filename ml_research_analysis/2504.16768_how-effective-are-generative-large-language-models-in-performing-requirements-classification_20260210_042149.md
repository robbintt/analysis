---
ver: rpa2
title: How Effective are Generative Large Language Models in Performing Requirements
  Classification?
arxiv_id: '2504.16768'
source_url: https://arxiv.org/abs/2504.16768
tags:
- classification
- requirements
- tasks
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of generative large language\
  \ models (LLMs) for requirements classification, addressing the gap in exploring\
  \ generative models for this task. Three state-of-the-art generative LLMs\u2014\
  Bloom, Gemma, and Llama\u2014were tested across three widely used datasets (PROMISE\
  \ NFR, Functional-Quality, and SecReq) in over 400 experiments."
---

# How Effective are Generative Large Language Models in Performing Requirements Classification?

## Quick Facts
- arXiv ID: 2504.16768
- Source URL: https://arxiv.org/abs/2504.16768
- Reference count: 40
- Generative LLMs tested: Bloom, Gemma, Llama for zero-shot requirements classification across binary and multi-class tasks

## Executive Summary
This study evaluates generative large language models (LLMs) for requirements classification, addressing a gap in exploring generative models for this task. The authors test three state-of-the-art generative LLMs—Bloom, Gemma, and Llama—across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq) in over 400 experiments. Results show that model selection significantly impacts performance, with Bloom excelling in precision, Gemma in binary classification recall, and Llama offering balanced performance across both binary and multi-class tasks. Prompt formulation, particularly assertion-based prompts, plays a critical role in optimizing results. Generative models demonstrated robustness to dataset variations. However, when compared to non-generative models (e.g., All-Mini), generative LLMs generally underperformed in multi-class classification, while non-generative models excelled in binary tasks.

## Method Summary
The study evaluates zero-shot requirements classification using three generative LLMs (Bloom, Gemma, Llama) and two non-generative baselines (SBERT, All-Mini) across three datasets. Generative models use inference-based learning with assertion-based prompts (e.g., "This requirement is about usability: {text}"), while non-generative models rely on embedding-based learning with cosine similarity. Performance is measured using weighted precision, recall, and F1 scores. The experiments explore the impact of model selection, prompt formulation, and dataset variations on classification accuracy.

## Key Results
- Bloom excelled in precision, Gemma in binary classification recall, and Llama offered balanced performance across binary and multi-class tasks.
- Generative models demonstrated robustness to dataset variations.
- Non-generative models (e.g., All-Mini) generally outperformed generative LLMs in multi-class classification, while excelling in binary tasks.

## Why This Works (Mechanism)
The study leverages the generative capabilities of LLMs to classify requirements without fine-tuning, relying on prompt-based inference. Assertion-based prompts guide the models to predict class labels by framing the input as a statement about the requirement's category. This approach exploits the pre-trained knowledge of LLMs to generalize across diverse datasets, though performance varies by model and task complexity.

## Foundational Learning
- **Zero-shot learning**: Understanding how models perform tasks without task-specific training is crucial for evaluating the generalizability of generative LLMs. Quick check: Confirm the models are not fine-tuned on the datasets.
- **Assertion-based prompts**: Framing inputs as statements (e.g., "This requirement is about usability") is key to eliciting accurate predictions from generative models. Quick check: Verify the prompt structure is consistent across experiments.
- **Weighted metrics**: Using weighted averages accounts for class imbalance in datasets, ensuring fair performance evaluation. Quick check: Confirm the use of `average='weighted'` in metric calculations.

## Architecture Onboarding
- **Component map**: Dataset -> Prompt Formatter -> LLM -> Logits -> Softmax/Sigmoid -> Class Prediction -> Metrics
- **Critical path**: The prompt formulation and decoding strategy are critical for accurate predictions, as they directly influence the model's output.
- **Design tradeoffs**: Generative models offer flexibility but may underperform in complex tasks compared to embedding-based methods.
- **Failure signatures**: Out-of-memory errors with large models, incorrect metric calculations, and ambiguous decoding strategies can hinder reproducibility.
- **First experiments**:
  1. Test Bloom-560m on a small subset of the PROMISE NFR dataset using deterministic decoding.
  2. Compare the performance of assertion-based prompts versus alternative prompt structures.
  3. Validate the weighted metric calculations on a balanced dataset to ensure correctness.

## Open Questions the Paper Calls Out
None

## Limitations
- The decoding strategy for "is-about" prompts is ambiguous, affecting the interpretation of the evaluation methodology.
- "Expert-curated labels" for the non-generative baseline are not fully specified in the provided text, requiring external verification.
- No details are provided on data preprocessing, such as tokenization or dataset splitting strategies, which could affect reproducibility.

## Confidence
- **High confidence**: The overall task framing and the selection of generative models and datasets.
- **Medium confidence**: The performance trends (e.g., Bloom excelling in precision, Llama balancing tasks) given the reported metrics and experimental scale.
- **Low confidence**: The exact implementation details of prompt formatting and decoding, due to ambiguity in the text.

## Next Checks
1. Verify the decoding strategy for "is-about" prompts by consulting the original codebase or supplementary materials to confirm whether the model predicts the class label or performs entailment on logits.
2. Confirm the "expert-curated labels" for the non-generative baseline by reviewing the referenced work [7] to ensure alignment with the methodology.
3. Test the reproducibility of results using a smaller model (e.g., Bloom-560m) with deterministic decoding (temperature=0) to isolate the impact of model size and inference settings.