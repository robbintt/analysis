---
ver: rpa2
title: Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining
arxiv_id: '2502.06733'
source_url: https://arxiv.org/abs/2502.06733
tags:
- linupper
- reweighting
- training
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces novel loss-based reweighting strategies for
  pretraining large language models (LLMs). The key insight is that samples with different
  loss values should have different importance during training, and this importance
  should adapt dynamically as training progresses.
---

# Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining

## Quick Facts
- arXiv ID: 2502.06733
- Source URL: https://arxiv.org/abs/2502.06733
- Authors: Daouda Sow; Herbert Woisetschläger; Saikiran Bulusu; Shiqiang Wang; Hans-Arno Jacobsen; Yingbin Liang
- Reference count: 40
- Key outcome: Novel loss-based reweighting strategies (LinUpper, Quadratic, Extremes) that dynamically adjust sample importance during LLM pretraining, with LinUpper showing best performance by downweighting low-loss samples

## Executive Summary
This paper introduces dynamic loss-based sample reweighting strategies for pretraining large language models. The key insight is that samples with different loss values should have different importance during training, and this importance should adapt dynamically as training progresses. The authors propose several strategies (LinUpper, Quadratic, Extremes) that reweight samples based on their loss values, with LinUpper—which downweights low-loss samples—performing best. They provide theoretical analysis showing how these strategies affect convergence bounds, demonstrating that downweighting low-loss samples can lead to faster convergence. Experiments across various scales (7B, 1.4B, and smaller models) show improved performance and faster convergence compared to uniform sampling baselines.

## Method Summary
The method implements dynamic, instance-level loss-based sample reweighting for LLM pretraining. It computes per-sample losses during the forward pass, normalizes them to [-α, α], applies a strategy (LinUpper/Quadratic/Extremes) to generate scores, then uses temperature-scaled softmax to compute sample weights. These weights are applied during the backward pass to compute weighted gradients. The temperature parameter r is annealed from high values (e.g., 100) to lower values (e.g., 1) during training, transitioning from near-uniform sampling to focused learning on high-loss samples. The LinUpper strategy specifically upweights high-loss samples while capping maximum weight at 2/M to prevent overfitting to outliers.

## Key Results
- LinUpper strategy achieves best performance across multiple scales (7B, 1.4B, and smaller models)
- Downweighting low-loss samples accelerates convergence and improves final performance
- The approach adds negligible computational overhead compared to uniform sampling
- Weight bound (wᵢ ≤ 2/M) holds throughout training, validating theoretical guarantees
- Creates synergies when combined with existing domain-level reweighting methods

## Why This Works (Mechanism)

### Mechanism 1
Down-weighting low-loss samples accelerates convergence in gradient-based optimization under interpolation conditions. When samples have low loss gaps (f(xi; θt) − f(xi; θ*) ≈ 0), their gradients provide minimal optimization signal. The term δt = Σᵢ(1/M − wᵢ)(fᵢ(θt) − fᵢ(θ*)) in the convergence bound becomes negative when weights wᵢ are increased for high-loss-gap samples, tightening the bound. This concentrates computation on samples where the model has more to learn. This assumes interpolation regime holds—there exists θ* that simultaneously minimizes all individual sample losses. Evidence includes the abstract stating "deprioritizing redundant or uninformative data, which we find tend to work best" and Theorem 1 showing how down-weighting low-loss-gap samples improves the bound. If interpolation fails, δt can become positive regardless of reweighting, potentially harming convergence.

### Mechanism 2
Capping maximum sample weight (wᵢ ≤ 2/M) prevents overfitting to high-loss outliers while maintaining gradient diversity. Without the cap, optimization could trivially assign weight 2/M to the M/2 highest-loss samples and zero to others. This discards half the data and overfits to potentially noisy outliers. The KL-divergence regularized solution balances focusing on informative samples against maintaining data diversity, yielding LinUpper's capped exponential weighting. This assumes high-loss samples may include noisy/outlier data points that should not dominate training. Evidence includes Theorem 1 stating the max weight should not exceed twice the uniform weight, and Appendix B.3 comparing against DRO-KL which diverges with the same learning rate. If the dataset has minimal outliers and loss distribution is well-behaved, a looser bound might accelerate convergence further, but this is not tested.

### Mechanism 3
Annealing temperature parameter r provides curriculum learning without precomputed schedules or auxiliary models. Early training uses high r → softmax over scaled scores approaches uniform weights, ensuring diverse feature learning. As training progresses and r decreases, reweighting becomes sharper, focusing on higher-loss samples. This matches the intuition that early training needs broad exposure while later training benefits from targeting specific weaknesses. This assumes loss values become more informative about sample importance as training progresses; early losses are noisy indicators of long-term value. Evidence includes the abstract stating "allowing the model to dynamically focus on more informative or important samples at different training stages" and Algorithm 1 showing r is annealed during training. If r decreases too quickly, the model may prematurely focus on specific samples and under-learn diverse patterns.

## Foundational Learning

- **Concept**: Interpolation regime in overparameterized models
  - Why needed here: The theoretical convergence guarantees require that all individual sample losses can be simultaneously minimized. Without this assumption, the δt term could remain positive regardless of reweighting strategy.
  - Quick check question: Can you identify a parameter setting where the model achieves near-zero loss on all training samples? If not, the interpolation assumption may be violated.

- **Concept**: Biased vs. unbiased gradient estimators in SGD
  - Why needed here: Unlike uniform sampling where E[Σᵢ∇fᵢ/b] = ∇f, the reweighting scheme makes weights functions of the losses themselves, introducing bias: E[Σᵢwᵢ∇fᵢ] ≠ ∇f. This complicates convergence analysis and requires modified step-size constraints.
  - Quick check question: What happens to gradient bias if sample weights depend only on the sample itself (not on current loss)?

- **Concept**: KL-divergence regularization in optimization
  - Why needed here: Proposition 1 shows LinUpper emerges from minimizing δt + r·Σᵢwᵢlog(Mwᵢ). The KL penalty prevents extreme weight concentrations, balancing exploitation of high-loss samples against maintaining distributional diversity.
  - Quick check question: As r → 0, what happens to the optimal weight distribution? (Answer: It approaches the extreme solution of assigning 2/M to top M/2 samples.)

## Architecture Onboarding

- **Component map**: Forward pass → compute per-sample losses fᵢ → normalize losses to [-α, α] → apply strategy to get scores sᵢ → temperature scaling to compute wᵢ → weighted backward pass
- **Critical path**: 1) Implement `get_batch_loss_from_logits` to extract per-sample losses 2) Implement all-reduce to gather losses across GPUs before reweighting 3) Implement LinUpper with temperature annealing schedule 4) Modify backward pass to use weighted gradients 5) Ensure wᵢ ≤ 2/b is satisfied in practice
- **Design tradeoffs**: LinUpper (up-weight high-loss) is recommended by theory and empirically best. Quadratic (up-weight medium-loss) may help with noisy datasets. Temperature schedule: high initial r (e.g., 100) → anneal to r=1. Batch size sensitivity: larger batches require all-gathering more losses.
- **Failure signatures**: Training divergence if learning rate is too high for reweighted gradients; no improvement if temperature r remains too high throughout training; max weight exceeds 2/b indicates numerical instability; performance degradation may indicate interpolation assumption violated.
- **First 3 experiments**: 1) Reproduce GPT2-mini results on SlimPajama subset: train uniform baseline and LinUpper (r: 100→1) side-by-side 2) Ablate temperature schedule: train with fixed r∈{0.2, 0.4, 0.6, 0.8, 1.0, 100} to reproduce Figure 5 sensitivity analysis 3) Validate weight bound: log max(wᵢ) per step for 7B model training to confirm max(wᵢ) < 2/128 ≈ 0.0156 throughout training

## Open Questions the Paper Calls Out
- How does the dynamic loss-based reweighting framework perform in non-LLM contexts such as adversarial machine learning, domain adaptation, or imbalanced classification?
- What specific factors cause smaller models (e.g., 124M–300M parameters) to derive less benefit from instance-level reweighting compared to 7B parameter models?
- Can dynamic loss-based reweighting effectively replace heuristic-based data filtering (curation) when pretraining on raw, unfiltered web data?

## Limitations
- Theoretical analysis assumes convex loss functions, but LLM pretraining is inherently non-convex
- Temperature annealing schedule appears tuned to specific setup without ablation across different dataset sizes
- Does not address potential distributional shift where high-loss samples early in training may not remain informative later

## Confidence
- **High confidence**: Empirical observation that downweighting low-loss samples improves convergence and final performance across multiple scales; weight bound (wᵢ ≤ 2/M) holds in practice
- **Medium confidence**: Theoretical convergence analysis under interpolation assumptions; specific temperature schedule's optimality across different training scenarios
- **Low confidence**: Whether the approach generalizes to datasets with different characteristics (highly noisy data, very small datasets, or specialized domains)

## Next Checks
1. Test interpolation assumption violation: Intentionally corrupt 10-20% of training samples with random labels and retrain LinUpper to verify it doesn't catastrophically degrade performance or violate the weight bound
2. Validate temperature schedule sensitivity: Run ablation studies across different dataset sizes (10M, 100M, 1B samples) with temperature schedules r=100→1, r=50→0.5, and r=200→2 to identify if the schedule requires tuning per dataset scale
3. Examine bias toward simplicity: Analyze whether LinUpper disproportionately improves performance on simpler domains (e.g., Wikipedia) while underperforming on complex domains (e.g., ArXiv) compared to uniform sampling, using the per-domain validation metrics reported in Table 1