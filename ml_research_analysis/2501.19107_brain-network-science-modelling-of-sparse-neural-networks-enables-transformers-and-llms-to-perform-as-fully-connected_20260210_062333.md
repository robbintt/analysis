---
ver: rpa2
title: Brain network science modelling of sparse neural networks enables Transformers
  and LLMs to perform as fully connected
arxiv_id: '2501.19107'
source_url: https://arxiv.org/abs/2501.19107
tags:
- training
- network
- chts
- sparsity
- links
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Cannistraci-Hebb Training soft rule (CHTs),
  a brain-inspired dynamic sparse training method that addresses limitations of prior
  gradient-free approaches. CHTs improves upon the original Cannistraci-Hebb training
  (CHT) by introducing a soft sampling strategy for link removal and regrowth, balancing
  exploration and exploitation of network topology to avoid epitopological local minima.
---

# Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected

## Quick Facts
- arXiv ID: 2501.19107
- Source URL: https://arxiv.org/abs/2501.19107
- Reference count: 40
- Introduces CHTs, a dynamic sparse training method enabling Transformers and LLMs to match fully connected performance with 5-30% connectivity

## Executive Summary
This paper presents CHTs (Cannistraci-Hebb Training soft), a brain-inspired dynamic sparse training method that enables Transformers and large language models to achieve fully connected network performance with only 5-30% connectivity. The method builds upon previous Cannistraci-Hebb training by introducing a soft sampling strategy for link management and replacing computationally expensive path-based link prediction with a GPU-friendly node-based approximation. By combining this with a novel bipartite receptive field initialization, CHTs achieves ultra-sparse configurations (up to 99% sparsity) that outperform fully connected networks in MLP architectures for image classification tasks.

## Method Summary
CHTs improves upon the original Cannistraci-Hebb training by introducing a soft sampling strategy that balances exploration and exploitation of network topology to avoid epitopological local minima. The method replaces the computationally expensive path-based link prediction with a GPU-friendly node-based approximation, reducing complexity from O(N·d³) to O(N³). CHTs is initialized using a novel bipartite receptive field (BRF) network model that generates brain-like spatial connectivity patterns. When combined with a sigmoid gradual density decay strategy (CHTss), it further improves performance, enabling Transformers and LLaMA models to match or exceed fully connected counterparts with significantly reduced connectivity.

## Key Results
- Achieves ultra-sparse configurations (up to 99% sparsity) that outperform fully connected networks in MLP architectures for image classification
- Enables Transformers and LLaMA models to match or exceed fully connected performance with only 5-30% connectivity
- Outperforms state-of-the-art sparse training methods while maintaining competitive language modeling capabilities

## Why This Works (Mechanism)
The mechanism behind CHTs' success lies in its ability to dynamically adapt network topology during training while maintaining computational efficiency. The soft sampling strategy allows for better exploration of the solution space compared to hard thresholding approaches, while the node-based link prediction approximation enables scalable GPU implementation. The brain-inspired BRF initialization provides a structured starting point that captures spatial connectivity patterns similar to neural networks in the brain, potentially leading to more efficient learning trajectories.

## Foundational Learning
- **Dynamic sparse training**: A technique where network connectivity is adapted during training rather than being fixed a priori. Needed because static sparse networks often underperform due to suboptimal initial topology. Quick check: Can the method maintain or improve performance while reducing parameter count?
- **Epitopological local minima**: Suboptimal network configurations that trap sparse networks in poor performance. Needed because sparse networks are particularly susceptible to these minima. Quick check: Does the method demonstrate escape from poor local optima?
- **Bipartite receptive field modeling**: A network topology generation method inspired by brain connectivity patterns. Needed to provide structured initialization that captures spatial relationships. Quick check: Does BRF initialization improve convergence compared to random initialization?

## Architecture Onboarding
- **Component map**: Input -> BRF Initialization -> CHTs Training -> Soft Sampling -> Node-based Link Prediction -> Output
- **Critical path**: The training loop where topology adaptation (soft sampling) and link prediction updates occur in each iteration, directly affecting model performance
- **Design tradeoffs**: Computational efficiency (O(N³) vs O(N·d³)) vs. precision of link prediction, exploration vs. exploitation in topology adaptation, and brain-inspired initialization vs. task-specific requirements
- **Failure signatures**: Performance degradation when connectivity drops below critical thresholds, convergence issues when soft sampling parameters are misconfigured, and poor initialization when BRF parameters don't match task characteristics
- **First experiments**: 1) Compare CHTs performance across different sparsity levels (10%, 30%, 50%, 70%) on standard image classification benchmarks. 2) Ablation study isolating contributions of soft sampling vs. node-based link prediction. 3) Scalability test on larger Transformer models (ViT-Large, GPT-2 variants) with ImageNet-21k and C4 datasets.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond those addressed in the limitations section regarding scalability and benchmarking against other methods.

## Limitations
- Scalability to larger models and datasets beyond tested MLP and Transformer configurations remains unproven
- Generalizability of brain-inspired BRF initialization across different network architectures needs further validation
- Lack of ablation studies isolating contributions of individual innovations (soft sampling vs. node-based approximation)
- Absence of benchmarking against recent dynamic sparse training approaches like RigL or SNFS

## Confidence
- Core claim (CHTs enables 5-30% connectivity with full performance): Medium
- Brain-inspired initialization effectiveness: Medium
- Epitopological local minima avoidance: Low
- Computational efficiency gains: Low (not empirically validated)

## Next Checks
1. Benchmark CHTs against other leading dynamic sparse training methods (e.g., RigL, SNFS) on standard vision and language benchmarks
2. Conduct ablation studies to isolate contributions of soft sampling strategy and node-based link prediction approximation
3. Evaluate scalability on larger models (ViT-Large, GPT-2) and datasets (ImageNet-21k, C4) to assess practical applicability