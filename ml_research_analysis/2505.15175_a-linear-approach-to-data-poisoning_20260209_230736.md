---
ver: rpa2
title: A Linear Approach to Data Poisoning
arxiv_id: '2505.15175'
source_url: https://arxiv.org/abs/2505.15175
tags:
- poisoning
- then
- data
- arxiv
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a random matrix theory framework for quantifying
  the impact of structured backdoor poisoning in high-dimensional ridge regression.
  The core method derives closed-form asymptotic laws for the poisoned predictor,
  yielding explicit scaling relationships between poisoning efficacy, regularization
  strength, overparameterization ratio, and trigger characteristics.
---

# A Linear Approach to Data Poisoning

## Quick Facts
- arXiv ID: 2505.15175
- Source URL: https://arxiv.org/abs/2505.15175
- Reference count: 40
- Primary result: Derives closed-form asymptotic laws for poisoning efficacy in ridge regression, showing poisoned estimator aligns with poisoning direction

## Executive Summary
This paper presents a random matrix theory framework for quantifying the impact of structured backdoor poisoning in high-dimensional ridge regression. The core method derives closed-form asymptotic laws for the poisoned predictor, yielding explicit scaling relationships between poisoning efficacy, regularization strength, overparameterization ratio, and trigger characteristics. The primary result shows that the poisoned estimator asymptotically aligns with the poisoning direction, with attack success governed by a Gaussian-distributed score whose mean and variance admit explicit formulas in terms of model parameters. Synthetic experiments confirm the theoretical predictions across wide parameter sweeps, while MNIST backdoor tests demonstrate qualitatively consistent trends for the mean shift. The analysis provides a tractable, governance-ready lens for measuring poisoning vulnerability in linear models.

## Method Summary
The framework analyzes structured backdoor poisoning in high-dimensional ridge regression through random matrix theory. The method assumes isotropic Gaussian features with independent random labels, applies a rank-one poisoning spike to shift a fraction of training samples, and uses centered ridge regression with unpenalized intercept. The key innovation is deriving closed-form asymptotic laws for the poisoned predictor using Marčenko-Pastur theory and resolvent concentration, yielding explicit formulas for the mean shift and variance of the poisoned score. The approach enables tractable prediction of poisoning efficacy across the high-dimensional parameter space.

## Key Results
- The poisoned ridge estimator asymptotically aligns with the poisoning direction, with alignment strength governed by poisoning rate and inversely by regularization
- The poisoned score converges to a Gaussian distribution with explicit mean and variance formulas, providing a closed-form measure of attack efficacy
- Regularization has weak net effect on poisoning efficacy because mean and variance shifts partially cancel, though variance explodes near the interpolation threshold
- Synthetic experiments validate theoretical predictions across wide parameter sweeps, while MNIST backdoor tests show qualitative agreement for mean shift but variance divergence due to anisotropy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ridge estimator aligns with the poisoning direction proportional to poisoning rate and inversely with regularization.
- Mechanism: The rank-one spike v adds structure to the covariance via Woodbury decomposition. The resolvent concentrates onto v, yielding β̂^T a → Cv^T a where C scales with θ and decreases with λ.
- Core assumption: Isotropic Gaussian features with independent random labels (signal-free model).
- Evidence anchors:
  - [abstract] "show that the weights align with the poisoning direction"
  - [Proposition 4.4] "C = θ(1-θ)m(-λ) / ((1+cm(-λ))(2 + ||v||²θ(1-θ/2)(1-λm(-λ))))"
  - [corpus] Corpus confirms alignment is a recognized attack signature (Hayase et al., spectral signatures).
- Break condition: Anisotropic covariance violates isotropic Wishart resolvent equivalence; deterministic equivalent requires modification.

### Mechanism 2
- Claim: The poisoned score converges to a Gaussian with explicit mean and variance governing attack efficacy.
- Mechanism: Conditioned on training data, β̂^T x_0 ~ N(0, ||β̂||²) by Gaussianity of x_0. Asymptotically, ||β̂||² → σ² and β̂^T v → μ (constants from RMT), yielding the limit distribution via CLT-type concentration.
- Core assumption: High-dimensional proportional regime p/n → c ∈ (0,∞).
- Evidence anchors:
  - [Theorem 4.1] "β̂^T(x_0 + v) → N(μ, σ²)" with explicit formulas (5)-(6)
  - [Section 5, Step 5] Gaussian limit from independence and concentration
  - [corpus] Limited corpus evidence on explicit score distributions; related work focuses on detectability, not limit laws.
- Break condition: Finite-sample deviations when p, n are small; label-feature dependence introduces bias.

### Mechanism 3
- Claim: Regularization has weak net effect because mean and variance shift in opposite directions.
- Mechanism: Ridge shrinks μ (alignment) but also shrinks σ² (variance). The efficacy η = 1-Φ(-μ/σ) depends on the ratio, partially canceling.
- Core assumption: Ridge penalty on weights only; intercept unpenalized.
- Evidence anchors:
  - [Takeaway II, Section 2] "Regularisation does not help"
  - [Figure 3] Efficacy increases modestly with λ (mean effect dominates variance effect)
  - [corpus] No direct corpus validation; assumption is model-specific.
- Break condition: When λ → 0 and c → 1, variance blows up (interpolation threshold), breaking cancellation.

## Foundational Learning

- Concept: **Stieltjes Transform and Marčenko-Pastur Law**
  - Why needed here: The resolvent (1/n XX^T - zI)^{-1} concentrates to m(z)I where m(z) is the MP Stieltjes transform, enabling deterministic replacements.
  - Quick check question: For c = p/n = 2 and z = -1, what does m(z) equal approximately?

- Concept: **Woodbury Matrix Identity**
  - Why needed here: Decomposes low-rank perturbations (poisoning spike) into tractable inverses of small matrices (3×3 in this work).
  - Quick check question: If A^{-1} is known, how does (A + uv^T)^{-1} differ?

- Concept: **Leave-One-Out / Concentration Inequalities**
  - Why needed here: Decouples dependence between data points and resolvent for asymptotic limits (Burkholder martingale arguments).
  - Quick check question: Why does removing one sample change β̂ by O(1/n)?

## Architecture Onboarding

- Component map: Data (X, y) → Poison (shift θ-fraction by v, flip labels) → Center → Ridge Solve (λ) → Test on (x_0 + v)
- Critical path:
  1. Implement centering to compute Ẋ, ẽw (intercept unpenalized)
  2. Construct poisoning via X_pois = X + v u^T, y_pois = y + 2u
  3. Validate RMT predictions by sweeping (c, λ, ||v||, θ) and comparing empirical μ, σ² to formulas (5)-(6)
- Design tradeoffs:
  - Signal-free model (labels ⊥ features) vs. real data: cleaner theory but ignores genuine signal
  - Fixed trigger v vs. learned trigger: analytic simplicity vs. adaptive attack realism
  - Asymptotic vs. finite-n: closed forms vs. approximation errors
- Failure signatures:
  - c → 1 and λ → 0: variance explodes (interpolation threshold; Figure 5 shows σ blow-up)
  - Strongly anisotropic data: MP law fails, deterministic equivalent requires covariance structure
  - MNIST variance mismatch (Figure 5): isotropy assumption violated
- First 3 experiments:
  1. Replicate synthetic sweep (p=500, vary n for c∈{0.1,0.5,1.5,2.0}, λ∈{0.001,0.1,1.0}, θ∈{0.05,0.2}, ||v||∈{0.5,2.0}); plot η theoretical vs. empirical.
  2. Test alignment coefficient C = β̂^T v / ||v||² against Proposition 4.4 prediction; verify monotonicity in θ, λ.
  3. Probe interpolation threshold: set λ=1e-6, sweep c through 1.0; observe σ² divergence and compare to equation (10).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the closed-form poisoning efficacy change when features exhibit structured (anisotropic) covariance rather than isotropic Gaussian structure?
- Basis in paper: [explicit] "Extending the framework to structured covariance (e.g., anisotropic features)... is an important direction for future work. Extending to anisotropic features would be a feasible extension using an anisotropic deterministic equivalent lemma in place of the Wishart deterministic equivalent."
- Why unresolved: The current analysis assumes isotropic $N(0, I_p)$ features for analytic tractability; real datasets like MNIST have correlated features and class structure that cause divergence in variance predictions (Figure 5).
- What evidence would resolve it: Derivation of modified $\mu$ and $\sigma^2$ formulas incorporating an arbitrary covariance matrix $\Sigma$, validated on anisotropic synthetic data and real datasets with estimated $\Sigma$.

### Open Question 2
- Question: What are the asymptotic poisoning laws when the label $y$ depends on the features $x$ through an underlying signal, rather than being independent noise?
- Basis in paper: [explicit] "The theoretical work assumes that labels are independent of the data... this allows exact characterization of the poisoning effect but limits direct applicability to typical supervised learning problems where $x$ carries information about $y$."
- Why unresolved: Current proofs exploit label-feature independence for concentration arguments; introducing a signal component (e.g., $y = \text{sign}(\beta^* \top x)$) couples the poisoning direction with the true decision boundary.
- What evidence would resolve it: Theoretical analysis of poisoning when $y = f(x) + \epsilon$ for some signal $f$, showing how the mean shift $\mu$ decomposes into signal-aligned and poison-aligned components; experiments on data with known signal-to-noise ratios.

### Open Question 3
- Question: Can the RMT framework be extended to quantify poisoning in hierarchical models with group-specific random effects (e.g., recommendation systems with user/item embeddings)?
- Basis in paper: [explicit] "A natural extension of our framework is to models with hierarchical or crossed random effects... developing an RMT-style analysis of poisoning in these hierarchical models, and formalizing detection criteria based on random-effect diagnostics, is an interesting direction that we leave for future work."
- Why unresolved: The current linear model with unpenalized intercept does not capture variance components or shrinkage of group-level effects; the interplay between poisoning concentration on specific groups and random-effect variance estimates remains uncharacterized.
- What evidence would resolve it: Derivation of how group-confined poisoning affects random-effect estimators; empirical validation on synthetic hierarchical data showing whether monitoring variance component estimates can detect concentrated poisoning attacks.

### Open Question 4
- Question: Why does the variance term $\sigma^2$ diverge between theoretical predictions and MNIST experiments, while the mean shift $\mu$ shows qualitative agreement?
- Basis in paper: [inferred] "For the second order effect $\sigma$, we see a divergence of behaviour between the synthetic data model and the MNIST data... A plausible cause is violation of the isotropy assumption: correlated features and class structure increase output variance depending on the alignment between $v$ and high-variance directions."
- Why unresolved: The paper hypothesizes anisotropy as the cause but does not prove or quantify this effect; the exact mechanism by which feature correlations amplify variance relative to the isotropic theory remains unspecified.
- What evidence would resolve it: Controlled experiments varying the alignment between the trigger $v$ and the principal components of $\Sigma$ on anisotropic synthetic data; theoretical correction to $\sigma^2$ that accounts for trigger-eigenvector alignment.

## Limitations
- The framework assumes isotropic Gaussian features, but real datasets like MNIST exhibit strong anisotropy that causes variance predictions to diverge from empirical results
- The signal-free model (labels independent of features) simplifies analysis but limits applicability to real-world supervised learning where features contain signal about labels
- The analysis is asymptotic, with finite-sample effects potentially causing deviations, particularly near the interpolation threshold where variance explodes

## Confidence

- **High confidence**: Mechanism 1 (alignment with poisoning direction) and Theorem 4.1 (Gaussian limit for poisoned score) - these follow directly from RMT concentration and have clear synthetic validation
- **Medium confidence**: Mechanism 2 (regularization trade-off) - supported by synthetic trends but lacks direct empirical validation across the full parameter space
- **Low confidence**: MNIST variance predictions - the isotropy assumption is clearly violated, and empirical variance shows significant deviations from theory

## Next Checks

1. **Probe isotropy sensitivity**: Generate anisotropic Gaussian data with controlled covariance structure and measure how quickly theoretical predictions degrade as anisotropy increases
2. **Test signal impact**: Implement ridge regression on synthetic data with structured signal (labels dependent on features) and measure changes to alignment and score distributions
3. **Finite-sample validation**: For fixed n,p, systematically vary λ and θ to verify the predicted monotonic relationships between parameters and efficacy hold beyond asymptotic limits