---
ver: rpa2
title: A Hierarchical Variational Graph Fused Lasso for Recovering Relative Rates
  in Spatial Compositional Data
arxiv_id: '2509.20636'
source_url: https://arxiv.org/abs/2509.20636
tags:
- variational
- data
- spatial
- relative
- rates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of recovering relative molecular
  rates in spatial biological imaging data, where competitive sampling processes convolve
  signals from co-localized molecules. The authors develop a scalable Bayesian framework
  using a hierarchical variational graph fused lasso prior to model spatial compositional
  data.
---

# A Hierarchical Variational Graph Fused Lasso for Recovering Relative Rates in Spatial Compositional Data

## Quick Facts
- arXiv ID: 2509.20636
- Source URL: https://arxiv.org/abs/2509.20636
- Authors: Joaquim Valerio Teixeira; Ed Reznik; Sudpito Banerjee; Wesley Tansey
- Reference count: 40
- Primary result: Outperforms TIC normalization by 1-2 orders of magnitude in RMSE for spatial IMS data

## Executive Summary
This paper addresses the fundamental identifiability problem in spatial compositional data where competitive sampling processes convolve signals from co-localized molecules. The authors develop a scalable Bayesian framework using a hierarchical variational graph fused lasso prior to recover relative molecular rates from underdetermined compositional observations. By leveraging natural sparsity in spatial signal patterns and incorporating heavy-tailed shrinkage priors, the method achieves significant improvements over state-of-practice point estimate methodologies while providing well-calibrated uncertainty quantification.

## Method Summary
The method models spatial compositional data using a hierarchical variational graph-fused gamma lasso (HV-GFGL) framework. It employs a multinomial likelihood for observed counts and a negative multinomial CDF for censored observations below detection limits. The prior encourages piecewise-constant spatial patterns through a graph-fused lasso on log-rates, with heavy-tailed gamma-lasso shrinkage enabling recovery from underdetermined systems. Inference is performed via automatic differentiation variational inference (ADVI) using a structured variational family that captures spatial dependencies, outperforming mean-field alternatives in posterior coverage.

## Key Results
- RMSE improvements of 1-2 orders of magnitude over TIC normalization across all metabolites in simulation
- 90% credible interval coverage of 0.86 with HV-GFGL vs. 0.13-0.27 for mean-field benchmarks
- Real kidney tissue IMS data shows better anatomical structure recovery and artifact removal compared to standard approaches

## Why This Works (Mechanism)

### Mechanism 1
Sparsity constraints on spatial change-points recover latent relative rates from underdetermined compositional observations. The graph-fused gamma lasso prior penalizes differences between adjacent pixel log-rates, shrinking most edge differences toward zero while allowing heavy-tailed deviations at true change-points. This reduces effective degrees of freedom from M×D to approximately the number of spatial change-points, enabling identifiability when change-points are fewer than M−M/D.

### Mechanism 2
Sparse structured variational inference produces better-calibrated posteriors than mean-field VI for spatially-dependent latent variables. Rather than assuming independence, the hierarchical variational family couples vertex distributions through shared edge-specific shrinkage parameters ν_r,d. The variational variance at each vertex depends on the sum of inverse-shrinkage parameters from incident edges, imparting spatial dependence to the joint posterior while maintaining conditional independence for efficient sampling.

### Mechanism 3
Negative multinomial augmentation enables likelihood-based inference under left-censoring without discarding pixels. For pixels with molecules below detection limits, the joint likelihood factorizes as P(x^C|x^O, p)×P(x^O|p), where the censored component uses the negative multinomial CDF evaluated at detection limits. This preserves information from partially observed pixels rather than treating them as missing entirely.

## Foundational Learning

- Concept: Compositional data and the identifiability problem
  - Why needed here: Within-pixel proportions cannot uniquely determine within-molecule spatial rates without additional constraints.
  - Quick check question: Given two pixels with compositions (0.1, 0.4, 0.5) and (0.15, 0.6, 0.25), can you determine whether molecule C decreased or molecules A and B increased?

- Concept: Global-local shrinkage priors (gamma-lasso/Horseshoe family)
  - Why needed here: The model requires priors that shrink most edges to zero while allowing large deviations at true change-points—heavy tails prevent overshrinkage.
  - Quick check question: Why would a standard Laplace prior fail to recover a signal with both many zeros and a few large spikes?

- Concept: Variational inference vs. MCMC trade-offs
  - Why needed here: The method chooses approximate VI over exact MCMC for scalability; understanding the approximation quality matters for interpreting results.
  - Quick check question: What posterior property does mean-field VI systematically underestimate, and why does the structured variational family help?

## Architecture Onboarding

- Component map: Graph G={E,V} -> Observed counts x_{i,d} -> Multinomial likelihood + Negative multinomial CDF -> Graph-fused gamma lasso prior -> Variational family (Q(λ), Q(ν), Q(logθ|ν)) -> ELBO optimization

- Critical path:
  1. Initialize variational parameters using empirical Bayes heuristics (scale initialization to data)
  2. Sample ν_r,d from Q(ν), compute γ_i,d = Σ_{j∈ξ(i)} 1/ν_{ei,j,d}
  3. Sample logθ from Q(logθ|ν), transform to proportions via softmax
  4. Compute likelihood (including negative multinomial CDF for censored molecules)
  5. Evaluate ELBO and backpropagate; iterate to convergence (~25K iterations)

- Design tradeoffs:
  - Heavy-tailed vs. standard lasso: Gamma-lasso handles heterogeneous patterns better but adds hyperparameters; standard fused lasso failed on metabolite 5 (RMSE 1.07 vs 0.007)
  - Structured vs. mean-field VI: 6× better posterior coverage but ~25% slower per iteration (0.01s vs 0.008s)
  - Monte Carlo samples for censoring: More samples improve likelihood accuracy but increase memory; authors used 100 (simulation) or 10 (real data with high censoring)

- Failure signatures:
  - Overshrinkage: All posteriors collapse to uniform; check b*_{r,d} initialization relative to data scale
  - Undershrinkage: Noisy, non-piecewise estimates; check λ*_1d initialization or increase global shrinkage τ_d
  - Poor coverage with good RMSE: Mean-field behavior leaking in; verify structured variational family is correctly coupled
  - Numerical instability: High censoring (>85%) causing gradient issues; reduce Monte Carlo samples or add regularization

- First 3 experiments:
  1. Validation on simulated data with known ground truth: Replicate the 7-metabolite simulation study; verify RMSE improvement over TIC and posterior coverage matches Table 2. Check that identifiability holds across varying change-point densities.
  2. Ablation on variational family: Compare HV-GFGL vs. MF-GFGL vs. MF-GFL on the same simulated data; quantify coverage vs. computation trade-off. Confirm that the structured family's benefit scales with spatial dependence strength.
  3. Sensitivity to initialization: Systematically vary b*_{r,d} and λ*_1d initialization (e.g., 0.1×, 1×, 10× the recommended scale); measure convergence rate and final ELBO. Document failure modes for reproducibility.

## Open Questions the Paper Calls Out

### Open Question 1
What are the formal theoretical requirements for identifiability and finite-sample recovery rates in this hierarchical variational model? The authors note that their approach "would benefit from strong theoretical guarantees to precisely delineate the requirements for identifiability and, ideally, finite-sample rates." This remains unresolved as the paper relies on empirical validation rather than mathematical proofs.

### Open Question 2
Can the identifiability of relative rates be maintained if the sparse prior is adapted for smoother or longer length-scale spatial patterns? The authors observe that "adapting the prior to have a smoother penalty could prove fruitful... though whether the relative rates would still be identifiable is unclear." This is unresolved because smoother priors might fail to sufficiently constrain the underdetermined compositional system.

### Open Question 3
Can the sparse variational inference framework be effectively extended to handle spatio-temporal or spatial cohort data? The authors express interest in expanding the framework, noting that "scalability of this method to further dimensions... could be an exciting development." This is unresolved due to computational bottlenecks and the current model's design for 2D spatial graphs rather than higher-dimensional data structures.

## Limitations

- Simulation transparency: Critical details about true rate generation, spatial patterns, and censoring mechanism are not specified in the paper
- Negative multinomial implementation: Monte Carlo approximation scheme for CDF is referenced to Appendix without specification, creating reproducibility concerns
- Computational scalability: Scaling behavior to large tissue sections (>10,000 pixels) and many molecules (>100) is not characterized

## Confidence

- **High**: Outperformance of RMSE vs. TIC normalization (1-2 orders of magnitude); structured VI achieves better posterior coverage than mean-field; piecewise-constant spatial patterns enable identifiability
- **Medium**: Negative multinomial augmentation correctly handles left-censoring without discarding pixels; gamma-lasso prior provides appropriate heavy-tailed shrinkage; initialization heuristics scale effectively
- **Low**: Posterior calibration remains conservative (90% CI coverage 0.86 vs. nominal 0.90); identifiability guarantees under varying change-point densities; computational scaling to real-world IMS datasets

## Next Checks

1. **Identifiability Validation**: Systematically vary the number of spatial change-points relative to pixel count (M vs. M/D) in simulated data; quantify RMSE degradation when identifiability threshold is violated

2. **Negative Multinomial Stress Test**: Create simulations with >85% censoring across all molecules; evaluate whether ELBO optimization remains stable and whether posterior coverage degrades

3. **Scalability Benchmark**: Apply HV-GFGL to tissue sections with 10,000+ pixels and 100+ molecules; measure runtime, memory usage, and ELBO convergence; identify bottlenecks in Monte Carlo sampling for censored pixels