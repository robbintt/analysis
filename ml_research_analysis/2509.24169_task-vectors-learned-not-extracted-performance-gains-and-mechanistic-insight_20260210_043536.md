---
ver: rpa2
title: 'Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight'
arxiv_id: '2509.24169'
source_url: https://arxiv.org/abs/2509.24169
tags:
- layer
- heads
- layers
- figure
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Learned Task Vectors (LTVs) as a superior
  alternative to traditional task vector extraction methods in large language models.
  Instead of distilling task representations from model outputs, LTVs are directly
  optimized via gradient descent, achieving higher accuracy across classification
  and generation tasks while demonstrating flexibility to inject at arbitrary layers,
  positions, and even into ICL prompts.
---

# Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight

## Quick Facts
- **arXiv ID**: 2509.24169
- **Source URL**: https://arxiv.org/abs/2509.24169
- **Reference count**: 40
- **Primary result**: Learned Task Vectors (LTVs) achieve higher accuracy than traditional task vector extraction across classification and generation tasks by directly optimizing vectors via gradient descent.

## Executive Summary
This paper introduces Learned Task Vectors (LTVs) as a superior alternative to traditional task vector extraction methods in large language models. Instead of distilling task representations from model outputs, LTVs are directly optimized via gradient descent, achieving higher accuracy across classification and generation tasks while demonstrating flexibility to inject at arbitrary layers, positions, and even into ICL prompts. Mechanistically, LTVs primarily operate through attention-head OV circuits, with a small subset of "key heads" driving most of their effect. At a higher level, LTV propagation through the network is largely linear: early-layer TVs undergo rotation to align with task-relevant subspaces, while late-layer TVs experience primarily scaling. This rotation-stretch dynamic provides a unified explanation for how TVs at different depths shape final predictions, offering both a practical tool for effective task vector acquisition and principled insight into in-context learning mechanisms.

## Method Summary
The method involves learning a task-specific vector $\theta$ by directly optimizing it via gradient descent to minimize negative log-likelihood of task labels. The vector is injected into the residual stream at specified layers and positions, and subsequently processed by attention heads (primarily through OV circuits of a small subset of "key heads"). Training uses AdamW with early stopping, and the approach is evaluated on classification and synthetic tasks across multiple model architectures (Llama variants, Qwen2.5, Yi).

## Key Results
- LTVs consistently outperform extracted task vectors across multiple datasets (SST-2, TREC, SNLI, RTE, Capital, Capitalize, Antonym)
- Task vector effects are primarily mediated through attention-head OV circuits, with ablating the top 10% of "key heads" causing substantial performance drops
- LTV propagation through the network follows a largely linear pattern: early-layer TVs undergo rotation, late-layer TVs experience scaling
- LTVs demonstrate flexibility to be injected at arbitrary layers and positions, including into ICL prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learned Task Vectors (LTVs) outperform extracted vectors because gradient descent optimizes a vector unconstrained by the model's existing representational quality.
- **Mechanism:** Instead of extracting a vector $\theta$ from ICL hidden states (which may be suboptimal), LTVs initialize $\theta$ and update it via gradient descent to minimize $-\log p(y_q|x_q, \theta, L, P)$. This allows the vector to find an optimal trajectory in the residual stream that might not exist in natural ICL activations.
- **Core assumption:** The optimization landscape allows convergence to a vector that effectively steers the model without requiring extensive fine-tuning of model weights.
- **Evidence anchors:**
  - [abstract] "LTVs are directly optimized via gradient descent... superior alternative to traditional task vector extraction."
  - [Section 3] "This approach eliminates the need to manipulate ICL hidden states and uncovers the most effective TV, unconstrained by representation or demonstration quality."
  - [corpus] Evidence is weak/missing in provided neighbors regarding optimization superiority specifically, though related work discusses task vector construction generally.
- **Break condition:** If the task requires reasoning capabilities entirely absent in the base model's pre-training data, optimizing a residual stream vector may fail to elicit the behavior, as LTVs steer rather than teach.

### Mechanism 2
- **Claim:** Task Vectors influence predictions primarily by interacting with the OV (Output-Value) circuits of attention heads, specifically a small subset of "key heads."
- **Mechanism:** When a TV $\theta$ is injected at layer $l-1$, it is added to the hidden state $h_N^{l-1}$. Subsequent attention heads process this modified state. The aggregate influence is approximated by $\sum W_{O,k'}^l W_{V,k'}^l \theta$. The paper identifies that ablating the top 10% of "key heads" (identified via saliency scores) drastically drops performance, whereas random ablation does not.
- **Core assumption:** The "key heads" have attention patterns that focus on the final tokens (where TVs are injected) rather than "attention sink" tokens.
- **Evidence anchors:**
  - [abstract] "LTVs primarily operate through attention-head OV circuits, with a small subset of 'key heads' driving most of their effect."
  - [Section 4.2] "Reconstructing the TV effect via OV-transformed decompositions restores much of the performance gain... ablating key heads reduces performance far more than random ablations."
  - [corpus] "Label Words as Local Task Vectors" supports the role of attention heads in ICL processing.
- **Break condition:** If attention heads in the target model are heavily biased toward "attention sink" behavior (focusing only on the first token), they may fail to process the injected TV at the final position.

### Mechanism 3
- **Claim:** The propagation of a TV's effect through layers is largely linear, characterized by an early-phase rotation into task-relevant subspaces and a late-phase scaling of magnitude.
- **Mechanism:** The composite layer updates from injection layer $l$ to $L$ can be approximated by a linear transformation $W_{TV,(l)}$. This matrix decomposes into rotation ($Q$) and stretch ($\Sigma$). Early-layer TVs are initially misaligned with label unembeddings and must be rotated by intermediate layers. Late-layer TVs are already aligned and primarily undergo scaling.
- **Core assumption:** The Transformer's non-linearities (e.g., LayerNorm, Softmax) do not fully disrupt the linear transmission of the injected signal in the specific subspace occupied by the TV.
- **Evidence anchors:**
  - [abstract] "Early-layer TVs undergo rotation to align with task-relevant subspaces, while late-layer TVs experience primarily scaling."
  - [Section 4.3] "A reconstructed TV based on modeling $\theta_l$'s influence as linear achieves comparable accuracy... confirming the strong linearity of hidden-state updates."
- **Break condition:** If the injection layer is too deep (approaching the final layer), there may be insufficient depth remaining for the necessary "rotation" to align the vector with the output logits, potentially degrading accuracy compared to early injection.

## Foundational Learning

- **Concept**: **Residual Stream & Stream Modifications**
  - **Why needed here**: The paper injects vectors into the residual stream ($h_N^l$). Understanding that this stream accumulates information additively is crucial to grasping how a single vector can shift the output.
  - **Quick check question**: If I add vector $\theta$ to hidden state $h^l$, does it affect layer $l+1$'s computation? (Yes, via residual connections).

- **Concept**: **Attention Head Decomposition (OV Circuits)**
  - **Why needed here**: Mechanism 2 relies on the observation that TVs interact with $W_O W_V$ projections. You must distinguish the QK circuit (where to attend) from the OV circuit (what information to move) to understand *how* the TV is processed.
  - **Quick check question**: Does the OV circuit determine *where* the attention head looks or *what* value it writes? (What value it writes/moves).

- **Concept**: **Logit Lens / Unembedding**
  - **Why needed here**: The paper measures "Task Alignment" by decoding hidden states at intermediate layers using the unembedding matrix $W_U$. This technique projects hidden states into vocabulary space to verify if the model is "thinking" about the correct task labels early on.
  - **Quick check question**: Can I decode a hidden state from layer 10 directly to vocabulary logits? (Yes, via Logit Lens).

## Architecture Onboarding

- **Component map**: Input prompt -> Injection layer/position -> Residual stream addition -> Key head OV circuit processing -> Linear transformation (rotation early, stretch late) -> Unembedding to logits
- **Critical path**: The effectiveness of an LTV depends on the **Injection Layer -> Key Head Activation -> Subspace Alignment** path. If the injection layer is early, the TV *must* pass through layers that rotate it into the correct subspace. If late, it must align immediately.
- **Design tradeoffs**:
  - **Extraction vs. Training**: Extraction (Vanilla TV) is fast but brittle and constrained by ICL prompt quality. Training (LTV) is slower (requires optimization loop) but yields higher accuracy and flexibility (works at any layer/position).
  - **Early vs. Late Injection**: Early injection leverages the model's full depth to "process" the task vector (rotation), offering higher accuracy potential. Late injection is more direct (scaling) but riskier if the vector isn't pre-aligned.
- **Failure signatures**:
  - **Attention Sink Interference**: If key heads suffer from "attention sink" (focusing on token 0), they ignore the injected TV at the last token, causing the intervention to fail.
  - **Representation Saturation**: In very deep layers, if the hidden state is already highly specialized, adding a generic vector may result in minimal output change (erasure by non-linearities).
  - **Misalignment**: Late-layer injection of an "unrotated" vector may fail to boost correct logits because there are insufficient layers left to align it with the label subspace.
- **First 3 experiments**:
  1.  **Single-Layer LTV Training**: Train an LTV on a classification dataset (e.g., SST-2) by injecting at the middle layer (position -1). Verify that validation accuracy exceeds Zero-Shot and matches/exceeds ICL.
  2.  **OV Reconstruction Test**: Inject the trained LTV, compute the aggregate OV effect ($\sum W_{OV}\theta$), and inject *this* reconstruction. Confirm if performance is recovered (validating the OV mechanism).
  3.  **Key Head Ablation**: Identify the top 10% of attention heads by saliency score post-injection. Ablate them and observe the performance drop compared to ablating random heads.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does reconstructing TV effects through OV circuits succeed for some model architectures (Llama3-8B, Qwen2.5-32B, Yi-34B) but fail for others (Llama2-7B, Llama3.1-8B, Llama3-70B)?
- Basis in paper: [explicit] Appendix E.2 states: "reconstructing the TV effect through OV circuits proves effective for Llama3-8B, Llama3.2-3B, Qwen2.5-32B, and Yi-34B, but not for the other three models."
- Why unresolved: The paper does not investigate architectural or representational differences that could explain this discrepancy.
- What evidence would resolve it: A systematic comparison of OV circuit properties across models, or analysis of alternative low-level pathways in models where OV reconstruction fails.

### Open Question 2
- Question: What determines the layer depth threshold (~layers 18-20 in Llama3.1-8B) where TV operating modes transition from rotation-dominant to stretch-dominant?
- Basis in paper: [explicit] Appendix F.2 identifies this threshold but does not explain its origin: "the transition occurs between layers 18 and 20... This is consistent with previous findings, which report that ICL features a distinct transition pattern."
- Why unresolved: The paper observes the threshold but does not investigate whether it is task-dependent, model-scale-dependent, or linked to specific circuit structures.
- What evidence would resolve it: Experiments varying model scale, task complexity, and architectural variants to test which factors shift the transition layer.

### Open Question 3
- Question: Can LTVs trained on one task generalize or compose to handle related or novel tasks without retraining?
- Basis in paper: [inferred] The paper shows high intra-task clustering and inter-task separation of LTVs (Figure 4), but does not test whether LTVs can be combined or transferred across tasks.
- Why unresolved: The paper focuses on per-task training; the generalization properties of the learned vector space remain unexplored.
- What evidence would resolve it: Experiments testing arithmetic combinations of LTVs (e.g., LTV_task1 + LTV_task2) or zero-shot transfer of an LTV to a related task.

## Limitations
- The optimization-based approach assumes the existence of a steering vector within the residual stream space that can effectively guide the model, which may not hold for tasks requiring reasoning beyond the model's pre-training distribution.
- The identification of "key heads" as the primary mechanism may not generalize to models with different attention patterns or architectural variants.
- The extent to which the linear propagation model (rotation early, scaling late) holds across diverse architectures and tasks remains unclear.

## Confidence
- **High Confidence**: The empirical observation that LTVs outperform extracted vectors across multiple datasets and the validation of the OV-circuit mechanism through reconstruction and ablation experiments. The linear propagation model (rotation early, scaling late) is also well-supported by the layer-sweep and alignment analysis.
- **Medium Confidence**: The generalization of LTV effectiveness to arbitrary layers, positions, and ICL prompts. While demonstrated, the robustness across diverse model architectures (beyond the tested Llama variants) and the stability of performance under different initialization strategies require further validation.
- **Low Confidence**: The precise identification of "key heads" as the sole or primary mechanism for TV propagation. The saliency-based identification method may be sensitive to specific model implementations, and alternative mechanisms (e.g., MLP interactions) might play a non-trivial role in certain contexts.

## Next Checks
1. **Cross-Architecture Generalization**: Test LTV training and effectiveness on non-Transformer architectures (e.g., Mamba, RWKV) to validate whether the OV-circuit and linear propagation mechanisms are architecture-specific or universal principles of in-context learning.

2. **Initialization Sensitivity Analysis**: Systematically vary the initialization strategy for LTVs (e.g., zeros, random normal, Xavier, learned from a small probe set) and measure the impact on convergence speed, final accuracy, and stability of the learned vector's norm.

3. **Attention Mechanism Ablation Beyond Key Heads**: Extend the ablation study to include QK-circuit heads and MLP layers to determine if the OV-circuit mechanism is truly dominant, or if other architectural components contribute significantly to the TV's steering effect in specific scenarios.