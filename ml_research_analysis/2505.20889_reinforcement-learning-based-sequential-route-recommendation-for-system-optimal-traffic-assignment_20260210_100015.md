---
ver: rpa2
title: Reinforcement Learning-based Sequential Route Recommendation for System-Optimal
  Traffic Assignment
arxiv_id: '2505.20889'
source_url: https://arxiv.org/abs/2505.20889
tags:
- traffic
- assignment
- route
- network
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reformulates the static system-optimal (SO) traffic
  assignment problem as a sequential decision-making task solved using deep reinforcement
  learning (RL). The approach introduces an MSA-guided deep Q-learning algorithm that
  incrementally recommends routes to travelers as they appear, aiming to minimize
  total system travel time.
---

# Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment

## Quick Facts
- **arXiv ID**: 2505.20889
- **Source URL**: https://arxiv.org/abs/2505.20889
- **Reference count**: 26
- **Primary result**: RL-based sequential route assignment achieves 0.35% deviation from theoretical SO solution in the OW network

## Executive Summary
This paper reformulates the static system-optimal (SO) traffic assignment problem as a sequential decision-making task solved using deep reinforcement learning (RL). The approach introduces an MSA-guided deep Q-learning algorithm that incrementally recommends routes to travelers as they appear, aiming to minimize total system travel time. The RL agent learns to assign routes based on real-time network conditions and adapts its policy through interaction with the environment. Experiments on both the Braess paradox and Ortuzar-Willumsen (OW) networks demonstrate that the proposed method closely approximates the theoretical SO solution, achieving a deviation of only 0.35% in the OW network.

## Method Summary
The authors model SO traffic assignment as a sequential MDP where a single RL agent assigns routes to travelers one-by-one as OD demands arrive. The state includes edge-level information (travel time, volume, marginal travel time per link) and OD-level information (one-hot OD encoding, marginal travel time per candidate route). The reward is the negative marginal travel time. The method uses DQN with MSA-guided exploration, where after each episode, the assignment distribution is updated via MSA to guide action sampling. Route action sets are either k-shortest paths or SO-derived paths from MSA. Training uses ε-greedy exploration with parameters: 2 hidden layers (512, 256), γ=0.95, mini-batch=128, learning rate=2×10^-5.

## Key Results
- MSA-guided RL achieves 0.35% deviation from theoretical SO solution in the OW network
- Route action set design significantly impacts performance: RL-10-SP shows 3.49% deviation, while RL-SO-informed routes converge faster
- The method successfully handles the Braess paradox network, converging to the theoretical SO solution (total travel time = 498 minutes)
- Sequential RL assignment approximates static SO assignment closely while handling dynamic traveler arrivals

## Why This Works (Mechanism)
The approach works by reformulating SO traffic assignment as a sequential decision problem where an RL agent learns to assign routes incrementally. The MSA-guided exploration mechanism ensures that the agent explores routes that are likely to be in the SO solution, accelerating convergence. By using marginal travel time as the reward signal, the agent learns to minimize the system-wide impact of each assignment. The state representation captures both network congestion and route-specific information, enabling informed decisions.

## Foundational Learning
- **System-optimal traffic assignment**: Why needed - provides the theoretical benchmark for optimal route assignment minimizing total system travel time; Quick check - verify solution matches Wardrop equilibrium conditions
- **Marginal travel time**: Why needed - serves as the reward signal indicating the system-wide impact of adding a vehicle to a route; Quick check - verify marginal travel times are computed correctly for each link and route
- **Markov Decision Process formulation**: Why needed - enables sequential decision-making where each route assignment affects future states; Quick check - verify state transitions follow network dynamics
- **Deep Q-learning with experience replay**: Why needed - learns optimal policy from sequential experiences while handling high-dimensional state spaces; Quick check - monitor training curves for convergence
- **MSA (Method of Successive Averages)**: Why needed - provides theoretical SO solution for comparison and guides exploration during training; Quick check - verify MSA implementation matches theoretical SO solution
- **Route action set design**: Why needed - determines which routes are available for assignment, directly impacting solution quality; Quick check - ensure action set covers routes used in theoretical SO solution

## Architecture Onboarding

**Component map**: Travelers arrive -> RL agent observes state -> selects route from action set -> environment updates network conditions -> reward = -marginal travel time -> MSA updates distribution M -> repeat

**Critical path**: Route assignment loop: state observation → action selection → environment transition → reward calculation → experience storage → MSA update → policy learning

**Design tradeoffs**: Action set size vs. computational complexity (larger k-SP increases coverage but slows learning); exploration vs. exploitation (MSA-guided exploration accelerates convergence but may miss alternatives)

**Failure signatures**: Large optimality gap (>1%) indicates insufficient action set coverage or poor exploration; slow convergence suggests inadequate learning rate or missing MSA guidance; unstable training indicates inappropriate reward scaling

**3 first experiments**: 1) Train on Braess network with 3 predefined routes and verify convergence to 498 minutes total travel time; 2) Run ablation comparing MSA-guided RL vs. standard ε-greedy on OW network to quantify MSA benefit; 3) Test different action set sizes (k=10,15) on OW to measure impact on convergence speed and final optimality gap

## Open Questions the Paper Calls Out
None

## Limitations
- Missing OW network topology and link parameters prevents exact reproduction of benchmark solutions and training results
- Route action set design critically affects performance - omitting key SO routes leads to suboptimal solutions
- Limited reporting on training dynamics, wall-clock times, and hyperparameter sensitivity reduces reproducibility

## Confidence

**High confidence**: Theoretical framework linking SO traffic assignment to sequential RL is clearly defined and mathematically sound

**Medium confidence**: Experimental results are plausible but rely on undisclosed network parameters from external literature

**Low confidence**: Reproducibility of training dynamics and ablation studies due to unspecified hyperparameters and missing convergence data

## Next Checks

1. Reconstruct the OW network topology and link performance parameters from Ortúzar & Willumsen (1994) and verify the SO-MSA benchmark solution

2. Run ablations comparing MSA-guided RL with standard ε-greedy RL and with fixed-route assignment to isolate the contribution of MSA exploration

3. Perform sensitivity analysis on the action set size and composition (k-SP vs. SO-informed) to quantify their impact on convergence speed and final gap to SO