---
ver: rpa2
title: 'Learning with Expected Signatures: Theory and Applications'
arxiv_id: '2505.20465'
source_url: https://arxiv.org/abs/2505.20465
tags:
- signature
- expected
- process
- estimator
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes asymptotic theory for estimating the expected
  signature of continuous-time stochastic processes observed at discrete points. The
  authors prove consistency and asymptotic normality results for expected signature
  estimators under general conditions, bridging the gap between discrete-time empirical
  estimators and their continuous-time theoretical values.
---

# Learning with Expected Signatures: Theory and Applications

## Quick Facts
- **arXiv ID**: 2505.20465
- **Source URL**: https://arxiv.org/abs/2505.20465
- **Reference count**: 40
- **Primary result**: Establishes asymptotic theory for estimating expected signatures of continuous-time stochastic processes from discrete observations

## Executive Summary
This paper develops a comprehensive theoretical framework for estimating expected signatures of continuous-time stochastic processes from discrete observations. The authors prove consistency and asymptotic normality results for signature estimators under general conditions, addressing the fundamental challenge of bridging discrete-time empirical estimators with their continuous-time theoretical values. A key innovation is the introduction of a martingale correction technique that significantly reduces variance in signature estimation, particularly valuable for financial applications. The theoretical contributions are validated through extensive experiments across five machine learning applications including time series classification, derivative pricing, and systematic trading.

## Method Summary
The authors establish asymptotic theory for expected signature estimators of continuous-time stochastic processes observed at discrete points. They prove convergence results for processes with bounded variation and extend these to rough paths (including unbounded variation processes like Brownian motion) using stronger norms than probability. The core innovation is a variance reduction technique that replaces the outer Stratonovich integral with an It么 integral in the signature estimator, achieving unbiasedness with lower mean squared error. This martingale correction can be applied even to non-martingale processes as a data transformation hyperparameter, making it broadly applicable across different domains.

## Key Results
- Establishes consistency and asymptotic normality for expected signature estimators as observation granularity increases
- Introduces a martingale correction technique that reduces variance while maintaining unbiasedness
- Demonstrates 20-30 percentage point improvements in classification accuracy for synthetic data experiments
- Validates theoretical results across five ML applications: time series classification, derivative pricing, distributional regression, and systematic trading

## Why This Works (Mechanism)
The expected signature captures the essential characteristics of continuous-time stochastic processes by encoding their iterated integrals. For processes with bounded variation, the expected signature can be fully characterized by its expectation. For rough paths with unbounded variation, the authors show convergence in stronger norms by leveraging the algebraic and analytic properties of signatures. The martingale correction works by exploiting the orthogonality properties of It么 integrals, which have zero expectation and lower variance compared to Stratonovich integrals when applied to martingale-like observations.

## Foundational Learning
- **Expected Signatures**: Mathematical objects that encode the complete information of a stochastic process through its iterated integrals. Needed to understand the core mathematical objects being estimated. Quick check: Can be computed recursively using Chen's relation.
- **Rough Paths Theory**: Framework for analyzing stochastic processes with potentially unbounded variation. Needed to extend results beyond smooth processes. Quick check: Generalizes classical stochastic calculus to handle rougher paths.
- **Stratonovich vs It么 Integrals**: Two different integration theories for stochastic processes with different transformation properties. Needed to understand the variance reduction mechanism. Quick check: Stratonovich preserves classical chain rule while It么 introduces correction terms.
- **Martingale Properties**: Processes with zero drift where future increments are uncorrelated with past information. Needed to understand when the variance reduction applies. Quick check: Martingale difference sequences have orthogonal increments.
- **Asymptotic Normality**: Statistical property where estimators converge to normal distributions under certain conditions. Needed to establish the theoretical guarantees. Quick check: Often follows from central limit theorem or functional delta method.

## Architecture Onboarding
**Component Map**: Data Observations -> Expected Signature Estimator -> Martingale Correction -> ML Model
**Critical Path**: Discrete observations are transformed through signature computation, optionally corrected using martingale properties, then fed into downstream machine learning models for prediction or inference.
**Design Tradeoffs**: The martingale correction reduces variance but assumes approximate martingale behavior; the signature computation is computationally intensive but captures rich path information.
**Failure Signatures**: Poor performance may indicate violation of underlying process assumptions, insufficient observation frequency, or inappropriate application of martingale correction to non-martingale processes.
**First Experiments**: 1) Apply martingale correction to synthetic Brownian motion data with known properties, 2) Compare signature-based classification against standard time series methods on benchmark datasets, 3) Test variance reduction empirically on financial time series with known drift components.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results assume continuous-time processes observed at increasingly dense discrete points, which may not reflect real-world irregularly spaced observations
- The martingale correction relies on properties that may not hold exactly in practice, with limited theoretical guarantees for non-martingale applications
- Asymptotic guarantees may not hold when observation intervals are fixed or when processes exhibit regime changes violating stationarity assumptions

## Confidence
- Asymptotic theory for bounded variation processes: **High**
- Rough path convergence results: **Medium**
- Martingale correction performance: **Medium**

## Next Checks
1. Test the martingale correction on financial time series with known drift components to quantify bias-variance tradeoffs under realistic market conditions
2. Evaluate the method's performance when applied to irregularly spaced or partially observed time series to assess robustness to observation patterns
3. Compare the expected signature approach against alternative rough path methods on benchmark datasets where ground truth continuous-time processes are known through simulation