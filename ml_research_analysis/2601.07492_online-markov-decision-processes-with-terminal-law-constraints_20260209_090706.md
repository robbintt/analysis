---
ver: rpa2
title: Online Markov Decision Processes with Terminal Law Constraints
arxiv_id: '2601.07492'
source_url: https://arxiv.org/abs/2601.07492
tags:
- distribution
- episode
- initial
- policy
- periodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the periodic framework for online Markov
  Decision Processes (MDPs) without resets, where the goal is to find policies that
  minimize cumulative loss while returning agents to their initial state distribution
  after a fixed period. The authors formalize the problem of finding optimal periodic
  policies and identify sufficient conditions for well-posedness in tabular MDPs.
---

# Online Markov Decision Processes with Terminal Law Constraints

## Quick Facts
- **arXiv ID**: 2601.07492
- **Source URL**: https://arxiv.org/abs/2601.07492
- **Reference count**: 40
- **Primary result**: Introduces periodic MDP framework with terminal law constraints; achieves sublinear periodic regret of order $\tilde{O}(T^{3/4})$ for multi-agent reset-free learning

## Executive Summary
This paper introduces a periodic framework for online Markov Decision Processes (MDPs) without resets, where the goal is to find policies that minimize cumulative loss while returning agents to their initial state distribution after a fixed period. The authors formalize the problem of finding optimal periodic policies and identify sufficient conditions for well-posedness in tabular MDPs. They introduce the periodic regret as a performance metric and propose the first algorithms (MDPP-K and MDPP-U) for computing periodic policies in two multi-agent settings, both achieving sublinear periodic regret of order $\tilde{O}(T^{3/4})$.

## Method Summary
The method uses Online Mirror Descent (OMD) with exploration bonuses to solve the constrained optimization problem in tabular MDPs. The key innovation is relaxing the hard periodic constraint into an inequality that accounts for model uncertainty. The algorithm operates in two frameworks: MDPP-K assumes known initial distributions at episode start, while MDPP-U estimates these distributions using a single agent reset. Both algorithms use multi-agent independent trajectories to satisfy martingale difference sequence conditions required for concentration bounds, effectively simulating "fresh starts" without physical resets.

## Key Results
- First algorithms achieving sublinear periodic regret for reset-free learning with $M > 1$ homogeneous agents
- Regret bound of $\tilde{O}(T^{3/4})$ achieved for both MDPP-K and MDPP-U algorithms
- Theoretical identification of sufficient conditions for well-posedness in tabular MDPs
- Empirical validation on 11x11 four-room grid world with "Max-entropy" and "Obstacles" tasks

## Why This Works (Mechanism)

### Mechanism 1: Terminal Law Constraint Relaxation
The algorithm ensures feasibility by relaxing the hard periodic constraint ($\rho P^\pi = \rho$) into an inequality that accounts for model uncertainty. Instead of enforcing the terminal state distribution to exactly match the target, it solves for $\|\mu_N - \rho\|_1 \leq \langle \mu, b_t \rangle + \bar{\alpha}\|\rho_t - \rho\|_1$, where $\langle \mu, b_t \rangle$ acts as a buffer proportional to transition kernel uncertainty.

### Mechanism 2: Drift Control via Contraction
The contraction property prevents the agent's initial distribution from drifting infinitely far from the target across episodes. Assumption 2 requires that applying the transition operator reduces the distance between distributions by a factor $\alpha < 1$ (i.e., $\|\nu P^\pi - \nu' P^\pi\|_1 \leq \alpha\|\nu - \nu'\|_1$), ensuring errors decay geometrically rather than accumulate.

### Mechanism 3: Multi-Agent Independent Trajectories
Utilizing $M > 1$ homogeneous agents allows obtaining statistically independent trajectories for dynamics estimation. By observing trajectories of uniformly sampled agents, the algorithm satisfies martingale difference sequence conditions required for concentration bounds, effectively simulating episodic "fresh starts" without physical resets.

## Foundational Learning

**Occupancy Measures**: The paper convexifies the policy optimization problem by moving from non-convex policies $\pi$ to convex state-action distributions $\mu$ satisfying Bellman flow. *Quick check: Can you explain why optimizing over occupancy measures turns a non-convex RL problem into a convex one?*

**Online Mirror Descent**: The core optimization engine uses OMD to update policy distribution iteratively against adversarial losses, requiring knowledge of Bregman divergences. *Quick check: How does the learning rate $\eta$ in OMD balance the trade-off between following the gradient and staying close to the previous iterate?*

**Ergodicity and Mixing Times**: Theoretical guarantees rely heavily on the contraction parameter $\alpha$, related to the mixing time of the Markov chain. *Quick check: What is the relationship between the contraction factor $\alpha$ and the mixing time $\tau$ mentioned in the discussion?*

## Architecture Onboarding

**Component map**: Data Ingest -> Transition Estimator -> Bonus Calculator -> Constraint Solver -> Deployment

**Critical path**:
1. Sample one agent from $M$, observe trajectory $(x_{t,n}, a_{t,n})$
2. Increment counts and update transition model $\hat{p}$
3. Calculate exploration bonuses $b_t$ and $\bar{b}_t$
4. Solve constrained OMD problem using Lagrangian approach
5. Check feasibility, adjust $\bar{\alpha}$ if needed
6. Broadcast new policy $\pi_{t+1}$ to all agents

**Design tradeoffs**:
- **Known vs. Unknown $\rho_t$**: MDPP-K offers tighter regret bounds but requires known distributions; MDPP-U is more practical but incurs higher mixing time dependence
- **Complexity vs. Regret**: Grid search for $\bar{\alpha}$ ensures feasibility but increases computational cost

**Failure signatures**:
- Infeasibility Loop: Solver cannot find feasible $\mu$, indicating underestimated $\bar{\alpha}$
- Distribution Drift: Cumulative drift $\sum \|\rho_t - \rho\|_1$ grows linearly, causing regret to explode
- Dependency Violation: Correlated agents bias dynamics estimation, leading to divergence

**First 3 experiments**:
1. Replicate Max-Entropy Gridworld: Implement 11x11 four-room grid to verify MDPP-K returns agents to start corner while maximizing entropy
2. Sensitivity to $\alpha$: Stress test by increasing environment stochasticity to see if regret bounds degrade gracefully
3. Ablation on $M$: Reduce agent count to understand minimal population size required for stable dynamics estimation

## Open Questions the Paper Calls Out

**Open Question 1**: Can a lower bound for the adversarial infinite-horizon setting with unknown dynamics be established? The paper provides an upper bound of $\tilde{O}(T^{3/4})$ but the optimal rate is unknown, creating a gap compared to the $\tilde{O}(\sqrt{T})$ rate known for known dynamics.

**Open Question 2**: Is it possible to achieve sublinear periodic regret in the single-agent ($M=1$) reset-free setting? The current analysis relies on $M>1$ agents for independent trajectories, but the paper leaves the single-agent case for future work.

**Open Question 3**: Can non-asymptotic guarantees be achieved without resetting even a single agent? While Framework 2 allows resetting one agent to estimate initial distributions, handling this drift without any resets remains an open problem.

## Limitations

- **Terminal Law Relaxation**: Practical implementation requires careful tuning of buffer terms and drift parameters, with adaptive grid search introducing computational overhead
- **Contraction Assumption Dependency**: Regret bounds fundamentally depend on the contraction parameter $\alpha$, which may be difficult to estimate in practice
- **Multi-Agent Requirement**: Requires $M > 1$ homogeneous, independent agents, limiting applicability to scenarios where multiple independent agents are available

## Confidence

**High Confidence**: Theoretical framework is mathematically rigorous; $O(\tilde{T}^{3/4})$ regret bound is correctly derived; OMD-based algorithm is valid for constrained online learning

**Medium Confidence**: Specific regret bound constants and their dependence on problem parameters; multi-agent sampling scheme provides sufficient exploration

**Low Confidence**: Practical feasibility in complex, high-dimensional environments; sensitivity to hyperparameter choices; robustness when assumptions are violated

## Next Checks

1. **Numerical Stability Test**: Implement synthetic MDP with controllable contraction parameter $\alpha$ to empirically verify regret scaling as $(1-\alpha)^{-2}$ and feasibility across parameter range

2. **Multi-Agent Scaling Analysis**: Run experiments with varying agent counts $M$ to empirically measure how regret scales with $M$ and compare against theoretical predictions

3. **Assumption Violation Stress Test**: Design environments violating key assumptions (non-ergodic components, correlated agents, unbounded drift) to measure algorithm failure modes and regret degradation