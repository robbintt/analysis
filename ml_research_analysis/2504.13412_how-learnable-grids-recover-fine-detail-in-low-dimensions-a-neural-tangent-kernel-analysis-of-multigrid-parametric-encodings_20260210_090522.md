---
ver: rpa2
title: 'How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent
  Kernel Analysis of Multigrid Parametric Encodings'
arxiv_id: '2504.13412'
source_url: https://arxiv.org/abs/2504.13412
tags:
- encoding
- encodings
- grid
- kernel
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes multigrid parametric encodings (MPE) for improving
  neural networks' ability to learn high-frequency details in low-dimensional mappings.
  Using neural tangent kernel (NTK) theory, the authors prove that MPEs raise the
  eigenvalue spectrum of the NTK through their learnable grid structure, not their
  embedding space.
---

# How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings

## Quick Facts
- **arXiv ID:** 2504.13412
- **Source URL:** https://arxiv.org/abs/2504.13412
- **Reference count:** 40
- **Primary result:** MPE increases minimum NTK eigenvalue by 8 orders of magnitude over baseline, enabling 15 dB PSNR improvement in low-dimensional image regression

## Executive Summary
This paper introduces Multigrid Parametric Encodings (MPE) for coordinate-based MLPs, addressing their inability to learn high-frequency details in low-dimensional mappings. Unlike Fourier Feature Encodings (FFE) that rely solely on embedding space, MPE uses learnable grid parameters to raise the neural tangent kernel (NTK) eigenvalue spectrum. The authors prove that MPE's grid structure adds positive semidefinite matrices to the NTK, monotonically elevating eigenvalues and enabling faster convergence on high-frequency components. Experiments show MPE achieves 15 dB PSNR and 0.65 MS-SSIM improvements over baseline, with theoretical guarantees for spectral bias mitigation.

## Method Summary
MPE encodes input coordinates using multi-resolution learnable grids with bilinear/trilinear interpolation, concatenating these with original inputs before passing to an MLP. The NTK analysis shows MPE's grid parameters add PSD matrices to the kernel, raising eigenvalues by Weyl's inequality. FFE uses fixed sinusoidal embeddings at logarithmic frequencies. Both are compared against baseline MLPs on 2D image and 3D surface regression tasks. The architecture uses NTK parameterization (weights scaled by 1/√width) with ReLU activation, trained via SGD at high learning rates (LR=100) for 300 epochs.

## Key Results
- MPE increases minimum NTK eigenvalue by 8 orders of magnitude over baseline, 2 over FFE
- 15 dB PSNR and 0.65 MS-SSIM improvement over baseline, 12 dB PSNR and 0.33 MS-SSIM over FFE
- Grid structure, not embedding dimension, drives performance gains (validated by NTK ablation)
- Theoretical guarantee: adding PSD matrices monotonically raises eigenvalues via Weyl's inequality

## Why This Works (Mechanism)

### Mechanism 1: Grid Structure Raises NTK Eigenvalue Spectrum via PSD Matrix Addition
MPE raises NTK eigenvalues by adding learnable grid PSD matrices: K_MPE = K_MLP + Σ(K^MPE_grid). Each grid layer contributes a PSD Gram matrix, and Weyl's inequality guarantees λ_min(K_MLP) ≤ λ_min(K_MPE). This holds for all eigenvalues, ensuring spectral elevation.

### Mechanism 2: Spectral Bias Mitigation via Minimum Eigenvalue Elevation
Raising minimum eigenvalue accelerates high-frequency learning since training error decays as |Q(f_θ(X,t) - Y)| = |-e^{-Λt}QY|. Low eigenvalues (~10^-8 for baseline) cause slow convergence on high-frequency modes; MPE raises minimum to ~10^0, enabling practical convergence.

### Mechanism 3: Mechanism Isolation — Grid vs. Embedding
NTK analysis with grid term removed shows MPE spectrum collapses to baseline despite same embedding dimension. This proves performance gains stem from grid parameters, not embedding space, distinguishing MPE from FFE.

## Foundational Learning

- **Neural Tangent Kernel (NTK):** Describes training dynamics via K_NTK(xi, xj) = ⟨∇_θ f(xi), ∇_θ f(xj)⟩. Essential for understanding eigenvalue-based convergence analysis.
  - *Quick check:* Why does K_NTK relate to gradient descent convergence?

- **Spectral Bias in Coordinate-Based MLPs:** Standard MLPs learn low frequencies quickly but high frequencies slowly due to eigenvalue ordering. Understanding this failure mode is crucial for appreciating MPE's solution.
  - *Quick check:* Why does a low eigenvalue for a high-frequency eigenvector cause slow convergence?

- **Positive Semidefinite Matrices and Weyl's Inequality:** MPE's theoretical proof relies on PSD matrices and Weyl's inequality for eigenvalue bounds. Fundamental to understanding the mechanism.
  - *Quick check:* If A and B are PSD, what can you say about λ_min(A+B) vs. λ_min(A)?

## Architecture Onboarding

- **Component map:**
  Input (x ∈ R^d) → [MPE Encoding: Multi-resolution grids → Bilinear interpolation → Concatenate with x] → MLP (2-8 hidden layers, 256-512 width) → Output

- **Critical path:**
  1. Grid initialized with N(0, 0.01) learnable parameters
  2. Bilinear/trilinear interpolation must be differentiable for gradient flow
  3. Grid resolution parameter x determines finest learnable detail
  4. L layers and k parameters/node trade memory vs. spectrum boost

- **Design tradeoffs:**
  | Choice | MPE | FFE | Baseline |
  |--------|-----|-----|----------|
  | Memory | High (grid storage) | Low | Lowest |
  | Aliasing risk | None (grid adapts) | High at high L | N/A |
  | Min eigenvalue | ~10^0 | ~10^-6 | ~10^-8 |
  | PSNR gain | +15 dB | +3 dB | — |

- **Failure signatures:**
  - Blurry outputs → Baseline or low-L FFE (spectral bias not mitigated)
  - Aliasing artifacts → FFE with L too high for data frequency
  - Memory OOM → MPE grid too fine; switch to hash grid or sparse variant
  - Slow convergence → Grid initialization too large (breaks NTK regime) or learning rate mismatch

- **First 3 experiments:**
  1. Reproduce eigenvalue spectrum plot: Train baseline, FFE (L=8), and MPE (L=2, k=2, x=200) on 2D image. Compute NTK eigenvalues at epochs 150-300.
  2. Ablate grid contribution: Compute NTK with grid term zeroed analytically. Confirm spectrum collapses to baseline.
  3. Resolution sweep: Vary MPE grid resolution from x=50 to x=400 on ImageNet subset. Plot minimum eigenvalue vs. resolution.

## Open Questions the Paper Calls Out
1. How does the theoretical relationship between MPEs and NTK spectrum change with non-ReLU activations like SIRENs? (Conclusion mentions analysis doesn't account for different activation functions)
2. Can MLP capacity be significantly reduced to save memory without degrading fine detail capture? (Appendix C questions whether MLP needs to do less work)
3. How does altering the interpolation kernel affect the NTK eigenvalue lower bound? (Conclusion suggests future work on kernel optimization)

## Limitations
- Theoretical analysis assumes NTK approximation remains valid throughout training, but early training shows substantial kernel evolution
- Grid resolution limited by memory constraints; no analysis of scaling beyond current resolution limits
- Performance gains rely on specific NTK parameterization and high learning rates, limiting generalization to standard training regimes

## Confidence
- **High confidence:** MPE raises minimum eigenvalue by 8 orders magnitude over baseline and 2 over FFE (empirical measurements in Figure 4)
- **Medium confidence:** Grid structure is the exclusive mechanism for eigenvalue elevation (analytical proof with Weyl's inequality, but no experimental grid-ablated training comparison)
- **Low confidence:** NTK eigenvalue ordering precisely predicts convergence rates across all training regimes (assumes lazy training throughout, but paper shows early kernel evolution)

## Next Checks
1. **Training ablation experiment:** Train MPE with grid gradients zeroed mid-training to confirm spectrum collapse matches analytical prediction
2. **Early training kernel analysis:** Track NTK eigenvalues through first 50 epochs to quantify when spectrum stabilizes and whether lower bound is tight
3. **Beyond NTK regime:** Test MPE performance with smaller widths or larger learning rates to identify when NTK assumptions break and whether empirical gains persist