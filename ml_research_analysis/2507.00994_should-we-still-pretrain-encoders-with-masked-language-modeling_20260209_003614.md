---
ver: rpa2
title: Should We Still Pretrain Encoders with Masked Language Modeling?
arxiv_id: '2507.00994'
source_url: https://arxiv.org/abs/2507.00994
tags:
- training
- pretraining
- arxiv
- masking
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether masked language modeling (MLM)\
  \ remains the best pretraining objective for text encoders, compared to causal language\
  \ modeling (CLM). Through large-scale experiments on 38 models (210M\u20131B parameters)\
  \ and over 15,000 fine-tuning runs, the authors find that while MLM generally yields\
  \ better performance on text representation tasks, CLM-trained models are more data-efficient\
  \ and demonstrate improved fine-tuning stability."
---

# Should We Still Pretrain Encoders with Masked Language Modeling?

## Quick Facts
- arXiv ID: 2507.00994
- Source URL: https://arxiv.org/abs/2507.00994
- Authors: Hippolyte Gisserot-Boukhlef; Nicolas Boizard; Manuel Faysse; Duarte M. Alves; Emmanuel Malherbe; André F. T. Martins; Céline Hudelot; Pierre Colombo
- Reference count: 40
- Primary result: CLM+MLM biphasic pretraining outperforms MLM-only pretraining under fixed compute budgets

## Executive Summary
This paper investigates whether masked language modeling (MLM) remains the optimal pretraining objective for text encoders compared to causal language modeling (CLM). Through large-scale experiments on 38 models (210M-1B parameters) and over 15,000 fine-tuning runs, the authors find that while MLM generally yields better performance on text representation tasks, CLM-trained models offer superior data efficiency and fine-tuning stability. A biphasic training strategy—sequentially applying CLM followed by MLM—achieves optimal performance under fixed computational budgets, and adapting CLM-pretrained models with MLM is more effective than continuing MLM training from MLM-pretrained models.

## Method Summary
The authors conduct extensive experiments comparing CLM, MLM, and biphasic CLM→MLM pretraining strategies across multiple model sizes (210M, 610M, 1B parameters) on diverse downstream tasks. They systematically vary pretraining steps, masking ratios, and compute budgets while maintaining consistent architectures and optimization configurations. The study includes over 15,000 fine-tuning runs across 12 datasets covering text classification, question answering, and information retrieval tasks. The biphasic approach involves pretraining with CLM for a specified fraction of steps, then switching to MLM for the remainder, with careful attention to learning rate scheduling and attention mask modifications.

## Key Results
- MLM-trained encoders outperform CLM-trained encoders on text representation tasks (QA, SC) but CLM shows better early-stage data efficiency
- Biphasic CLM→MLM pretraining achieves optimal performance under fixed computational budgets
- Continuing pretraining from CLM-pretrained models with MLM is more effective than continuing MLM training from MLM-pretrained models
- The optimal CLM ratio in biphasic training is 25-50% depending on task type (more CLM for TC, more MLM for QA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential CLM→MLM training outperforms MLM-only pretraining under fixed compute budgets
- Mechanism: CLM first builds foundational token representations efficiently through next-token prediction, which requires modeling local dependencies. MLM then refines these representations by introducing bidirectional context, allowing the model to integrate global sequence information. The combination leverages CLM's faster early convergence while MLM provides the bidirectional attention essential for encoder tasks.
- Core assumption: The order matters—CLM→MLM works better than MLM→CLM because causal pretraining establishes stable weight initialization before bidirectional adaptation.
- Evidence anchors:
  - [abstract]: "a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget"
  - [section 4, Figure 6]: Shows CLM+MLM consistently improves performance vs. MLM alone across 12K, 22K, 42K step budgets, with 25%-75% to 50%-50% splits performing best
  - [corpus]: Weak direct evidence—corpus papers discuss MLM optimization but not biphasic CLM→MLM specifically
- Break condition: If training budget is extremely limited (<12K steps), CLM-only may suffice; if budget is unlimited, MLM-only may eventually catch up.

### Mechanism 2
- Claim: Adapting CLM-pretrained models with MLM is more effective than continuing MLM training from MLM-pretrained models
- Mechanism: CLM-pretrained models haven't overfit to the masked token distribution and maintain more plastic weights. When MLM is applied, the model learns bidirectional patterns without fighting against ingrained unidirectional biases. MLM-pretrained models, having already converged on masked reconstruction, show diminishing returns from continued MLM training.
- Core assumption: CLM checkpoints retain more optimization capacity than MLM checkpoints at equivalent training stages.
- Evidence anchors:
  - [abstract]: "continuing pretraining from a CLM-pretrained model with MLM is more effective than continuing MLM training from MLM-pretrained model"
  - [section 5, Figure 8-10]: CLM→MLM CPT consistently outperforms MLM→MLM CPT across SC, QA, IR tasks; shows steeper improvement curves
  - [corpus]: No direct corpus evidence on CPT transfer from CLM to MLM
- Break condition: If the CLM checkpoint has undergone extensive post-training (e.g., instruction tuning), MLM adaptation benefits may diminish.

### Mechanism 3
- Claim: CLM provides better fine-tuning stability and early data efficiency than MLM
- Mechanism: CLM's autoregressive objective produces more consistent gradient patterns across training, leading to smoother loss landscapes during fine-tuning. For early training, each CLM token prediction provides a learning signal, whereas MLM's masked tokens create sparse supervision—only ~40% of positions contribute loss per batch.
- Core assumption: Stability benefits transfer across downstream task types and learning rate regimes.
- Evidence anchors:
  - [section 3, Figure 4]: CLM outperforms MLM early (up to 10K-20K steps depending on task) before MLM catches up
  - [section 3, Figure 5]: CLM-pretrained models show lower variance across learning rates vs. MLM (smaller error bars across SC, TC, QA, IR)
  - [corpus]: "Mask and You Shall Receive" paper suggests MLM optimization matters, supporting that MLM is inherently noisier
- Break condition: For QA tasks requiring deep bidirectional reasoning, CLM's early advantage doesn't translate to final performance—MLM essential.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed here: Core baseline objective; understanding bidirectional attention advantage is essential for interpreting why QA/SC tasks prefer MLM
  - Quick check question: Can you explain why masking 40% of tokens requires the model to learn global context rather than local n-gram patterns?

- **Concept: Causal Language Modeling (CLM)**
  - Why needed here: The alternative pretraining objective; understanding autoregressive prediction explains data efficiency and stability claims
  - Quick check question: Why does next-token prediction create more consistent gradient flow than random mask reconstruction?

- **Concept: Attention Masking Patterns**
  - Why needed here: The fundamental architectural difference between encoder (bidirectional) and decoder (causal) attention; switching requires architecture changes
  - Quick check question: If you switch from CLM to MLM mid-training, what attention mask parameter must change, and does the architecture need modification?

## Architecture Onboarding

- **Component map:**
  Input tokens → Embedding layer → Transformer layers (shared) → Task heads → Attention mask switches: CLM: Causal (lower triangular), MLM: Bidirectional (full attention)

- **Critical path:**
  1. Initialize model with standard architecture (RoPE embeddings, RMSNorm, SwiGLU)
  2. Pretrain CLM with causal mask for N steps (no LR decay before switch)
  3. Switch to MLM objective + bidirectional mask
  4. Apply LR decay only in final 2K steps of MLM phase
  5. Fine-tune with bidirectional attention on downstream tasks

- **Design tradeoffs:**
  - CLM ratio vs. MLM ratio: Paper suggests 25-50% CLM optimal; more CLM helps TC, more MLM helps QA
  - Masking ratio (MLM phase): 40% is reasonable default; higher ratios (50%) favor IR, lower (20-30%) help smaller models on token tasks
  - CPT length: 12K steps often sufficient for MLM adaptation from CLM checkpoint

- **Failure signatures:**
  - QA performance collapses with CLM-only pretraining (bidirectional attention essential)
  - High variance across fine-tuning seeds with MLM-only (CLM stabilization needed)
  - Masking ratio mismatch: small models + high masking = underfitting; large models + low masking = inefficiency

- **First 3 experiments:**
  1. Baseline validation: Train 610M model MLM-only (40% masking, 42K steps) on your data; fine-tune on 2-3 target tasks to establish benchmark
  2. Biphasic ablation: Train same model with 50% CLM → 50% MLM split; compare final performance and convergence speed
  3. CPT from existing decoder: Take an open CLM checkpoint (e.g., from similar domain), apply 12K steps MLM CPT with 40% masking; compare against training from scratch

## Open Questions the Paper Calls Out
None

## Limitations
- The biphasic training benefits may not generalize to larger models (GPT-3 scale) where optimization dynamics differ significantly
- Computational budget comparisons assume equivalent hardware and optimization setups without accounting for practical checkpoint management overhead
- The optimal CLM/MLM ratio appears task-dependent, suggesting the 25-50% CLM recommendation requires task-specific tuning

## Confidence
- **High confidence**: MLM remains superior for encoder-specific downstream tasks (QA, SC) - aligns with established understanding of bidirectional context needs
- **Medium confidence**: Biphasic training strategy's general applicability - comprehensive ablation studies but optimal ratio is task-dependent
- **Low confidence**: Computational efficiency claims for production deployment - shows better performance under fixed compute but doesn't account for checkpoint management overhead

## Next Checks
1. **Scale validation**: Replicate the CLM→MLM training strategy on 3B+ parameter models to verify if biphasic benefits persist at scale, particularly examining whether MLM's bidirectional advantages diminish as model capacity increases
2. **Domain transfer testing**: Apply the CLM→MLM approach to non-English corpora and specialized domains (e.g., biomedical text) to assess whether observed efficiency gains hold across different token distributions and linguistic structures
3. **Production simulation**: Implement the biphasic training in a continuous pretraining pipeline where new data arrives incrementally, measuring whether CLM→MLM provides better adaptation efficiency compared to traditional MLM-only approaches in realistic deployment scenarios