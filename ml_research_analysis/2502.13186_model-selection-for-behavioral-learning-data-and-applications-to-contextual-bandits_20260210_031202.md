---
ver: rpa2
title: Model selection for behavioral learning data and applications to contextual
  bandits
arxiv_id: '2502.13186'
source_url: https://arxiv.org/abs/2502.13186
tags:
- data
- learning
- contextual
- each
- bandits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of model selection for behavioral
  learning data, specifically when the data is non-stationary and dependent. The authors
  propose two model selection methods: a general hold-out procedure and an AIC-type
  criterion, both adapted to non-stationary dependent data.'
---

# Model selection for behavioral learning data and applications to contextual bandits

## Quick Facts
- arXiv ID: 2502.13186
- Source URL: https://arxiv.org/abs/2502.13186
- Reference count: 40
- Primary result: Oracle inequalities with error bounds O((log n + log |M|)/n) for hold-out and O(log(n)/n) for AIC-type model selection adapted to non-stationary dependent behavioral learning data

## Executive Summary
This paper addresses the challenge of model selection for behavioral learning data where observations are non-stationary and dependent. The authors propose two novel model selection methods - a hold-out procedure and an AIC-type penalized likelihood criterion - both adapted to handle the dependencies inherent in sequential learning data. They demonstrate these methods on contextual bandit models, providing theoretical guarantees and applying them to both synthetic and real human categorization data. The key innovation is extending model selection theory beyond i.i.d. assumptions to capture the dynamics of behavioral learning processes.

## Method Summary
The authors present two model selection approaches for non-stationary dependent data. The hold-out method splits data into training (1:N-1) and validation (N:n) sets, fitting candidate models on training data and selecting based on validation log-likelihood. The AIC-type criterion uses penalized maximum likelihood with a penalty term scaling with model complexity. Both methods are applied to partition-based contextual bandits where the context space is divided into cells, each running an independent bandit algorithm (Gradient Bandit or Exp3-IX). Parameters are estimated via maximum likelihood, with the hold-out using early observations for training and the AIC-type using truncated horizons to avoid degeneracy when action probabilities vanish.

## Key Results
- Theoretical error bounds of O((log n + log |M|)/n) for the hold-out procedure and O(log(n)/n) for the AIC-type criterion
- In synthetic experiments, penalized likelihood with c=0.012 selected the true model in ~70% of cases versus ~50% for hold-out
- On real human categorization data, penalized likelihood consistently selected OnePerItem model (capturing individual object memorization) more often than hold-out
- Both methods successfully distinguish between different cognitive strategies (e.g., shape-based learning vs. exception-based learning)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hold-out estimator can achieve near-i.i.d. oracle inequality bounds for non-stationary dependent data when split points respect filtration predictability.
- Mechanism: Training data produces fitted models whose conditional densities become FN-1-measurable for the selection phase. The validation log-likelihood on t ≥ N is maximized across models, with error bounded via martingale concentration using Hellinger-KL inequalities adapted to dependent increments.
- Core assumption: Candidate conditional densities pm_t are predictable w.r.t. the filtration (Ft); no parametric form is required.
- Evidence anchors:
  - [abstract] "hold-out procedure... adapted to non-stationary dependent data"
  - [Theorem 1] Shows E[H^2_N,n(p_̂m)|F_N-1] bound with log(n-N+1)+log|M| penalty
  - [corpus] Weak direct support; neighboring papers focus on bandit algorithms, not hold-out theory for non-i.i.d.
- Break condition: If pm_t depends on future observations (violating predictability), or if training/validation periods have fundamentally different learning dynamics (e.g., strategy switching), bounds fail.

### Mechanism 2
- Claim: AIC-type penalized likelihood with proper Lipschitz parameterization achieves oracle inequality for partition-based contextual bandits using only early observations.
- Mechanism: MLE on truncated horizon T_ε (roughly O(√n)) avoids degeneracy when action probabilities vanish. Penalty scales with D_m (partition dimension), trading model complexity vs. fit. The Lipschitz condition (Assumption 2) controls how parameter errors compound through softmax updates across cells.
- Core assumption: Parameters lie in compact sets; CellBandit procedures satisfy smoothness (log-ratio bounded by L_ε||δ-θ||) and non-vanishing probabilities (π ≥ ε for t ≤ T_ε).
- Evidence anchors:
  - [Theorem 3] Oracle inequality with penalty cA^2_ε log(ε^-1)log(T_εA_ε)^2 D_m/T_ε
  - [Section 4.2] "Theoretical guarantees... hold when only the first √n observations are used"
  - [corpus] Weak; no neighbor papers explicitly prove similar AIC bounds for bandits
- Break condition: If true action probabilities vanish faster than ε, or if parameterization has discontinuities, Lipschitz bound fails and concentration breaks.

### Mechanism 3
- Claim: Partition-based contextual bandits with per-cell gradient updates model individual learning as context-dependent policy refinement.
- Mechanism: Context space X is partitioned into cells C; each cell runs an independent CellBandit (e.g., Gradient Bandit) updating action probabilities via policy gradient. The model pm_θm,t(a|x) = Σ_C π^θC_C,T^C_t(a)1_{x∈C} captures that learners treat similar contexts equivalently.
- Core assumption: Learners maintain stable partitions throughout learning; rewards/losses are bounded in [0,1].
- Evidence anchors:
  - [Algorithm 2] Partition-based contextual bandit formalization
  - [Section 5] Six models tested on categorization task with clear cognitive interpretations
  - [corpus] Moderate support; Schulz et al. (2018) cited for bandits in cognition
- Break condition: If learners dynamically change partitions (metalearning), single-partition models are misspecified; Appendix C discusses Exp4 for this case.

## Foundational Learning

- Concept: Filtration and conditional expectation
  - Why needed here: The theoretical framework treats learning data as adapted to a filtration (Ft), where action probabilities depend only on past observations and contexts. Understanding σ-algebras and conditional expectations is essential to interpret predictability requirements.
  - Quick check question: Given Ft = σ(A^{t-1}_1, X^t_1), what information is available before choosing At?

- Concept: Partial log-likelihood (Cox, 1975)
  - Why needed here: When modeling only action sequences—not contexts—the standard likelihood becomes partial log-likelihood. This distinguishes the approach from full joint modeling.
  - Quick check question: Why can't we use standard likelihood when context Xt is generated by an external process?

- Concept: Martingale concentration for dependent data
  - Why needed here: The proof of Theorem 1 uses exponential supermartingales and peeling arguments to derive bounds for dependent increments—standard i.i.d. tools don't apply.
  - Quick check question: What breaks if we treat sequential choices as independent when computing variance?

## Architecture Onboarding

- Component map: Data sequences (At, Xt, gt) -> Partition specification -> Parameter estimation via MLE -> Criterion computation (hold-out or penalized MLE) -> Model selection -> Parameter extraction

- Critical path: Data ingestion → Partition specification → Parameter estimation via MLE → Criterion computation → Model selection → Parameter extraction for winning model

- Design tradeoffs:
  - Hold-out: No parametric assumptions; wastes training data; sensitive to N choice; fails if learning strategy shifts
  - AIC-type: Uses all data (up to T_ε); requires Lipschitz verification; theoretically limited to early observations; needs penalty calibration
  - Partition complexity: More cells (OnePerItem) capture individual memorization but suffer high variance; fewer cells generalize but may miss exceptions

- Failure signatures:
  - Probabilities vanishing before T_ε: Indicated by near-zero π^θC_C,t for t < √n; check raw action frequencies
  - Overly aggressive penalty: Most individuals assigned to simplest model (OneForAll); inspect model distribution
  - Under-calibrated penalty: Excessive selection of high-D_m models even under simulation with simpler ground truth

- First 3 experiments:
  1. **Synthetic calibration**: Generate data from each of 6 models with known θ. Sweep N (hold-out) and c (penalized MLE) to find settings minimizing misclassification rate. Verify Figure 2b/c patterns reproduce.
  2. **Truncation sensitivity**: For Gradient Bandit, compare MLE using all n observations vs. truncated at √n. Check if parameter recovery degrades (Figure 2a pattern).
  3. **Real data pilot**: Apply both methods to 10-20 individuals from Mezzadri et al. (2022b) dataset. Check whether penalized MLE favors OnePerItem as predicted (≈70%); verify hold-out selects ByShapeExc more often (Figure 3 patterns).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a theoretical recommendation for the optimal split point $N$ in the hold-out procedure for non-stationary dependent data?
- Basis in paper: [Explicit] The authors state in Section 4.2 that for the learning scenario, "there is no theoretical recommendation for choosing N for the hold-out criterion," requiring empirical numerical adjustment instead.
- Why unresolved: Standard cross-validation theory relies on i.i.d. assumptions which fail for dependent learning trajectories, making the trade-off between training sample size and validation accuracy theoretically unclear.
- Evidence would resolve it: Derivation of an optimal $N$ that minimizes the risk bound based on the mixing or dependency structure of the learning process.

### Open Question 2
- Question: Does the presentation order of objects significantly influence the selection of the partition-based model (e.g., OnePerItem) in categorization tasks?
- Basis in paper: [Explicit] In Section 5, the authors note that the "OnePerItem" model is selected more frequently in real data than in simulations and suggest it would be "interesting for further study to see if this is linked to the presentation order of the objects."
- Why unresolved: The numerical illustrations used a fixed periodic sequence for synthetic data, and the real data analysis did not isolate the effect of the object sequence on the model selection outcome.
- Evidence would resolve it: A comparative study using synthetic data with varying presentation orders to observe shifts in model selection frequencies.

### Open Question 3
- Question: Can the theoretical error bounds for the AIC-type criterion be extended to the full data horizon $n$ rather than the truncated threshold $T_\varepsilon \approx \sqrt{n}$?
- Basis in paper: [Inferred] Theorems 3, 4, and 5 provide guarantees only for $t \leq T_\varepsilon$ (Section 4.1, Appendix A.1). This truncation is required because Assumption 1 fails once the learner stops making mistakes (probabilities vanish), making estimation theoretically impossible with current methods.
- Why unresolved: The mathematical tools currently rely on the likelihood being lower-bounded by $\varepsilon$, which is violated in later learning stages where the true policy becomes deterministic.
- Evidence would resolve it: A proof of convergence that accounts for vanishing gradients or a new estimator that remains consistent in the regime where exploration has ceased.

## Limitations

- The theoretical truncation point T_ε ≈ √n is too restrictive for practical use, though the paper successfully applies the AIC-type criterion to full datasets
- Methods' sensitivity to partition choice and hyperparameter tuning (N for hold-out, c for penalized MLE) remains empirically driven rather than theoretically grounded
- Both methods assume bounded rewards and compact parameter spaces, limiting applicability to unbounded or high-dimensional settings

## Confidence

- Hold-out procedure bounds (Theorem 1): High confidence - proof follows established martingale concentration techniques with clear assumptions
- AIC-type criterion bounds (Theorems 3-5): Medium confidence - requires careful verification of Lipschitz conditions and truncation point
- Synthetic experiment results: High confidence - methodology is standard and well-documented
- Real data application conclusions: Medium confidence - limited to specific categorization task and dependent on dataset accessibility

## Next Checks

1. **Synthetic identifiability test**: Generate data from all six models with known parameters, then measure selection accuracy across parameter ranges. Verify that OnePerItem requires distinct θ_C values to be identifiable.

2. **Truncation sensitivity analysis**: Compare parameter recovery and selection accuracy using truncated vs. full observation windows. Quantify performance degradation when stopping at T_ε versus using all n observations.

3. **Partition robustness study**: Vary the number and structure of partition cells, measuring how selection accuracy degrades when true partitions differ from model assumptions. Test whether the methods remain robust when learners exhibit partial cell-switching behavior.