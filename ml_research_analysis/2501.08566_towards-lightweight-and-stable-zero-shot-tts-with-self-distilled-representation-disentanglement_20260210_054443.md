---
ver: rpa2
title: Towards Lightweight and Stable Zero-shot TTS with Self-distilled Representation
  Disentanglement
arxiv_id: '2501.08566'
source_url: https://arxiv.org/abs/2501.08566
tags:
- speech
- speaker
- zero-shot
- content
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high computational cost and instability
  issues in existing zero-shot TTS systems by proposing a lightweight and stable architecture.
  The core method employs a two-stage self-distillation framework that disentangles
  linguistic content from speaker attributes at the data level, while using a multi-level
  speaker representation approach to model global timbre and temporal style features
  separately.
---

# Towards Lightweight and Stable Zero-shot TTS with Self-distilled Representation Disentanglement

## Quick Facts
- arXiv ID: 2501.08566
- Source URL: https://arxiv.org/abs/2501.08566
- Reference count: 0
- Achieves 22.5M parameters with 1.8% CER and 0.73 speaker similarity in zero-shot TTS

## Executive Summary
This paper addresses the high computational cost and instability issues in existing zero-shot TTS systems by proposing a lightweight and stable architecture. The core method employs a two-stage self-distillation framework that disentangles linguistic content from speaker attributes at the data level, while using a multi-level speaker representation approach to model global timbre and temporal style features separately. The system achieves superior computational efficiency with 22.5M parameters (compared to 200M+ in baselines), achieving RTFs of 0.13 and 0.012 on CPU and GPU respectively. It also demonstrates excellent content integrity with a CER of 1.8 and MOScon of 4.43, while maintaining competitive speaker similarity with SIM of 0.73 and MOSsim of 3.31.

## Method Summary
The method employs a two-stage self-distillation framework where a teacher model generates parallel speech pairs with identical linguistic content but different speaker identities. The student model is then trained on this synthetic data to learn content-speaker disentanglement. The architecture explicitly models multi-level speaker representations by separating global timbre (extracted via frozen pre-trained speaker recognition model ERes2NetV2) from temporal style features (pitch, duration, energy extracted via trainable mel encoder with cross-attention). The non-autoregressive design predicts duration upfront to avoid error accumulation, and uses VP-Flow in the content extraction module to capture high-quality content representations beyond text limitations.

## Key Results
- Achieves 22.5M parameters versus 200M+ in baseline models
- Reaches 0.13 and 0.012 RTF on CPU and GPU respectively
- Demonstrates 1.8% CER and 0.73 speaker similarity
- Maintains competitive quality with MOScon of 4.43 and MOSsim of 3.31

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-distillation creates training data that forces content-speaker disentanglement at the data level.
- Mechanism: A teacher model generates parallel speech pairs where the linguistic content is identical but the speaker identity differs. The student model receives synthesized speech (with speaker A's timbre removed) and must reconstruct it using only a prompt from speaker B. This eliminates the shortcut of extracting speaker info directly from the source speech mel-spectrogram.
- Core assumption: The teacher model has sufficient initial zero-shot capability to generate time-aligned, phonemically accurate speech in a different speaker's voice.
- Evidence anchors:
  - [abstract] "two-stage self-distillation framework that constructs parallel data pairs for effectively disentangling linguistic content and speakers from the perspective of training data"
  - [Section 3.2] "We input ˆx′ as the speech content for the student model... We randomly select another speech sample ˆx′′ as the prompt, which is uttered by the same speaker as x"
  - [corpus] "Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion" addresses similar disentanglement challenges, suggesting this is an active research direction, though no direct validation of this specific self-distillation approach exists in the corpus.
- Break condition: If teacher model generates misaligned or corrupted speech, student learns from noisy supervision, degrading content fidelity.

### Mechanism 2
- Claim: Explicitly separating global timbre from temporal style representations prevents feature coupling and enables fine-grained control.
- Mechanism: Timbre is extracted via a frozen pre-trained speaker recognition model (ERes2NetV2) producing a global embedding. Temporal style (pitch, duration, energy) is extracted via a trainable mel encoder with positional encoding, then injected into predictors via cross-attention. These operate on different timescales and pathways.
- Core assumption: Timbre and prosodic style are approximately independent factors that can be modeled separately without losing speaker identity fidelity.
- Evidence anchors:
  - [abstract] "explicitly models multi-level speaker representations from source and prompt speech"
  - [Section 2.2] "We adopt a trainable mel encoder to extract the temporally-related style representation, esty... For timbre extraction, we utilize a speaker recognition model as the timbre encoder"
  - [corpus] Weak support. Corpus papers address zero-shot TTS but do not validate this specific factorization approach.
- Break condition: If timbre and style are correlated in ways the model cannot recombine, speaker similarity degrades for out-of-distribution prompts.

### Mechanism 3
- Claim: Non-autoregressive architecture with explicit duration prediction avoids error accumulation and improves content stability.
- Mechanism: Unlike autoregressive models (VALL-E, CosyVoice), this system predicts duration upfront and uses it during inference, eliminating sequential prediction errors that compound. The variance adapter's predictor gradients are separated from the main network.
- Core assumption: Duration can be accurately predicted from style and content representations without autoregressive modeling.
- Evidence anchors:
  - [Section 1] "autoregressive speech modeling methods... also increase vulnerability to time series prediction errors such as omissions, incorrect readings, repetitions"
  - [Table 1] CER of 1.8% vs. Vall-E's 2.89% and GPT-SoVITS's 4.89%
  - [corpus] Spark-TTS and Flamed-TTS both pursue efficiency in zero-shot TTS, but via different architectural choices (LLM-based vs. flow-based), suggesting no consensus on optimal architecture.
- Break condition: If duration prediction is inaccurate, phoneme alignment errors cause intelligibility issues.

## Foundational Learning

- Concept: **Variational Autoencoders (VAEs) for speech latent spaces**
  - Why needed here: The content extraction module uses a Mel VAE with VP-Flow to map speech into a latent space that captures content beyond text limitations. Understanding KL divergence loss and the reparameterization trick is essential for debugging representation quality.
  - Quick check question: Can you explain why a VAE is used here instead of a deterministic encoder for mel-spectrogram compression?

- Concept: **Adaptive Instance Normalization (AdaIN)**
  - Why needed here: The mel decoder uses AdaIN layers to inject speaker timbre embeddings into the generation process. This is the mechanism by which speaker identity modulates the output without changing the content representation.
  - Quick check question: How does AdaIN differ from standard layer normalization, and why is it suited for style transfer tasks?

- Concept: **Contrastive learning for speaker embeddings**
  - Why needed here: The cyclic contrastive loss (L_cyc) enforces that reconstructed speech embeddings match the prompt speaker while diverging from others in the batch. Understanding InfoNCE-style objectives helps debug speaker similarity failures.
  - Quick check question: In the contrastive loss formula, what happens if all samples in a batch come from the same speaker?

## Architecture Onboarding

- Component map:
  Content Extraction: Linguistic encoder (phonemes) + Mel encoder (spectrograms) → VP-Flow → fused content representation e_con
  Speaker Adaptation: Style encoder (temporal) → Variation adapter (duration/pitch/energy predictors) + Timbre encoder (global, frozen ERes2NetV2) → linear projection
  Synthesis: Mel decoder with AdaIN → vocoder (NFS-HiFiGAN)

- Critical path:
  1. Prompt speech → timbre encoder → e_spk (global identity)
  2. Prompt speech → style encoder → e_sty (temporal prosody)
  3. Source text + source speech → content representation e_con
  4. e_con + predicted attributes + e_spk → mel decoder → output

- Design tradeoffs:
  - **Frozen vs. trainable timbre encoder**: Frozen reduces parameters but limits adaptation to domain-specific voices not in Speaker3D
  - **Self-distillation coefficient σ=0.8**: Higher values improve disentanglement but risk quality degradation if teacher outputs are flawed
  - **Non-autoregressive generation**: Faster and more stable, but may sacrifice some naturalness in prosodic variation

- Failure signatures:
  - High CER with good speaker similarity → content representation leaking speaker info; increase σ or check teacher alignment
  - Low speaker similarity with accurate content → timbre encoder not generalizing; consider domain-adaptive fine-tuning
  - Speaker embedding clusters separate from real speech in t-SNE → disentanglement incomplete; self-distillation not converging

- First 3 experiments:
  1. **Ablate self-distillation**: Train student without teacher-generated pairs (σ=0) and compare CER/SIM to confirm disentanglement contribution.
  2. **Cross-dataset speaker generalization**: Test on speakers from domains not in training data (e.g., accented speech, children) to evaluate timbre encoder robustness.
  3. **Latency profiling**: Measure RTF breakdown by component to identify bottlenecks; prioritize optimization of variation adapter if it dominates inference time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed lightweight architecture effectively scale to larger datasets (e.g., >10,000 hours) to bridge the speaker similarity gap with large-scale models like CosyVoice, or does the limited parameter count impose a ceiling on representational capacity?
- Basis: [inferred] The paper demonstrates superior efficiency using 531 hours of data but acknowledges trailing the state-of-the-art in speaker similarity (0.73 vs. 0.84), leaving the interaction between model capacity, data scale, and similarity performance unexplored.
- Why unresolved: The experiments compare a small, efficient model against large models trained on vastly more data, conflating the benefits of the architectural design with the constraints of data scale.
- What evidence would resolve it: An ablation study training the proposed 22.5M parameter model on the same 171K-hour dataset used by the baseline (CosyVoice) to isolate architectural limits from data availability.

### Open Question 2
- Question: How sensitive is the student model's convergence to error propagation or artifacts in the teacher model's synthetic data during the self-distillation phase?
- Basis: [inferred] The framework relies on the teacher model to generate parallel data pairs (synthetic speech with swapped speakers) to train the student, assuming the teacher provides a sufficiently high-quality ground truth for disentanglement.
- Why unresolved: The paper evaluates the final student performance but does not analyze the degradation of the student model if the teacher's zero-shot cloning quality is imperfect or contains "incorrect readings" mentioned in the introduction.
- What evidence would resolve it: An analysis of student model performance when trained on teacher-generated data with varying levels of synthetic noise or speaker leakage.

### Open Question 3
- Question: Does the optimal self-distillation coefficient ($\sigma$) generalize across different dataset sizes or linguistic typologies, or does it require specific tuning to maintain the balance between content integrity and speaker similarity?
- Basis: [inferred] Figure 4 identifies 0.8 as the optimal coefficient for mixing real and synthetic data for the specific 531-hour dataset, but the stability of this hyperparameter across diverse conditions is not discussed.
- Why unresolved: The trade-off curve between SIM and CER may shift significantly if the ratio of real-to-synthetic data is changed in a much larger or acoustically different training set.
- What evidence would resolve it: Experiments validating the chosen $\sigma$ value across varying training set sizes (e.g., 100h vs 1000h) or different languages.

## Limitations
- Architectural scalability to long-form content and streaming scenarios remains untested
- Generalization to diverse speakers (children, elderly, accented speech) not validated
- Evaluation scope limited to content integrity and speaker similarity without prosodic naturalness assessment

## Confidence
- **High Confidence**: Computational efficiency claims (RTF values, parameter count) are well-supported by reported ablation studies and baseline comparisons
- **Medium Confidence**: Content integrity (CER of 1.8%) and speaker similarity (SIM of 0.73) results are promising but depend on teacher model quality in self-distillation
- **Low Confidence**: MOS scores lack detailed methodology and the "competitive speaker similarity" claim without specific baseline MOS comparisons

## Next Checks
1. **Cross-dataset Speaker Generalization**: Evaluate the model on speakers from datasets not used in training (e.g., VCTK, LibriTTS, or accented speech corpora) to assess the robustness of the frozen speaker encoder and the generalization of the self-distillation approach.

2. **Ablation of Self-Distillation Coefficient**: Systematically vary the self-distillation coefficient σ from 0 to 1.0 and measure its impact on content integrity, speaker similarity, and model stability to determine the optimal trade-off between disentanglement and generation quality.

3. **Prosodic Control Evaluation**: Design a test set with controlled prosodic variations (pitch accents, boundary tones, speaking rate changes) and evaluate whether the model can accurately reproduce these variations when prompted, or if the disentanglement process has degraded prosodic controllability.