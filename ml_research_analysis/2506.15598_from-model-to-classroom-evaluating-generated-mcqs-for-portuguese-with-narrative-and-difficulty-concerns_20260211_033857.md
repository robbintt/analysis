---
ver: rpa2
title: 'From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative
  and Difficulty Concerns'
arxiv_id: '2506.15598'
source_url: https://arxiv.org/abs/2506.15598
tags:
- mcqs
- difficulty
- generation
- generated
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the quality of reading comprehension MCQs
  generated by large language models for Portuguese elementary students. The research
  compares generated MCQs with human-authored ones using expert review and psychometric
  analysis based on actual student responses.
---

# From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns

## Quick Facts
- arXiv ID: 2506.15598
- Source URL: https://arxiv.org/abs/2506.15598
- Reference count: 40
- Generated MCQs comparable to human-authored ones in difficulty, discrimination, and well-formedness

## Executive Summary
This study evaluates reading comprehension multiple-choice questions (MCQs) generated by large language models for Portuguese elementary students. The research compares one-step and two-step generation methods, controlling for narrative elements (character, setting, action, feeling, causal relationship) and difficulty levels. Expert review and psychometric analysis using actual student responses show that generated MCQs match human-authored ones in overall quality, though human-authored questions show slightly better distractor engagement and adherence to option selection principles.

## Method Summary
The study uses 12 narrative texts from Portugal's National Reading Plan and 58 human-authored MCQs from Escola Virtual as benchmarks. Two generation approaches are tested: one-step zero-shot prompting with GPT-4o or Gemma-2-27B generating complete MCQs including difficulty annotation, and two-step generation using fine-tuned Ptt5-v2-large for narrative-controlled question generation followed by Gemma-2 for options and difficulty. Generated MCQs undergo expert evaluation (well-formedness, narrative alignment, clarity, answerability, plausibility 1-5, difficulty 1-5) and psychometric analysis using Classical Test Theory metrics (difficulty 1-P, discrimination D, 3-rule test) with student responses.

## Key Results
- One-step generation produces MCQs with 86.7-93.3% acceptance rates versus 55.6% for two-step approach
- Generated MCQs are comparable to human-authored ones in difficulty (1-P) and discrimination (D) metrics
- Post-hoc difficulty estimation correlates more strongly with human perception than in-generation prediction
- Human-authored MCQs show better distractor engagement (3-rule test compliance) and option selection principles

## Why This Works (Mechanism)

### Mechanism 1: One-Step Generation Superiority
- Claim: Single-prompt MCQ generation produces higher quality questions than sequential component generation.
- Mechanism: Joint generation maintains coherence between question and answer options, preserving semantic alignment that sequential generation severs.
- Core assumption: Joint generation preserves task-relevant dependencies.
- Evidence anchors: [Section 4.6] Two-step method showed 55.6% answerability vs. 86.7-93.3% one-step; [Section 5.7] Ptt5-v2+Gemma-2 MCQs least difficult and discriminative.

### Mechanism 2: Post-Hoc Difficulty Estimation Alignment
- Claim: Assigning difficulty after full MCQ generation correlates more strongly with human perception.
- Mechanism: Complete MCQ structure provides evidence for difficulty estimation that partial prompts cannot.
- Core assumption: Difficulty is emergent from full item structure.
- Evidence anchors: [Section 6.1] Post-generation correlation improves; [Section 6.4] In-generation assignment may reduce accuracy.

### Mechanism 3: Language Variant Drift in Multilingual Models
- Claim: LLMs produce outputs in unintended language variants despite explicit instructions.
- Mechanism: Training data contains mixed variants; prompt adherence competes with learned token distributions.
- Core assumption: Prompt constraints compete with pretraining influences.
- Evidence anchors: [Section 4.5.1] Gemma-2 produced 8/45 Brazilian Portuguese questions; [Section 4.6] Models don't always follow prompt instructions.

## Foundational Learning

- **Classical Test Theory (CTT) Metrics**
  - Why needed: Essential for interpreting difficulty (1-P) and discrimination (D) results
  - Quick check: If MCQ has difficulty 0.8 and discrimination 0.1, what does this indicate about quality?

- **Distractor Plausibility and Option Design**
  - Why needed: Human-authored MCQs show better distractor engagement
  - Quick check: What does it mean if high-ability students select a distractor more often than low-ability students?

- **Prompt Engineering for Controlled Generation**
  - Why needed: Study uses zero-shot prompting to control narrative elements and difficulty
  - Quick check: Why might explicitly specifying "European Portuguese" fail to prevent Brazilian Portuguese outputs?

## Architecture Onboarding

- **Component map:** Input Text → Prompt Constructor → LLM (One-Step) → Raw MCQ Output → Post-Hoc Difficulty Estimator → Validation Layer → Human Review OR Psychometric Evaluation Pipeline

- **Critical path:** Prompt design → Model selection → Validation filters → Expert review → Student testing

- **Design tradeoffs:**
  - One-step vs. two-step: 73.3% vs. 55.6% acceptance, but two-step offers component-level flexibility
  - Open-weight (Gemma-2) vs. proprietary (GPT-4o): Comparable performance, tradeoff is cost vs. control
  - In-generation vs. post-hoc difficulty: Post-hoc improves correlation but adds inference call

- **Failure signatures:**
  - Brazilian Portuguese output → fine-tune on variant-specific data
  - Semantic flaws → add consistency check against source text
  - Multiple/no correct answers → answerability validation required
  - Low distractor engagement → increase semantic similarity between options

- **First 3 experiments:**
  1. Replicate one-step pipeline with different LLM on 5 texts, validate against 86.7-93.3% benchmark
  2. Test post-hoc difficulty estimation: generate MCQs without difficulty, then prompt model to assign difficulty, compare correlation
  3. Implement 3-rule test on generated MCQs with student responses to verify distractor quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does two-step approach inherently produce lower-quality questions, or is the gap due to the specific smaller model (Ptt5-v2) used in first step?
- Basis: [Section 4.6] Two-step method yielded lower performance; [Limitations] Authors acknowledge performance may reflect model capacity rather than approach
- Why unresolved: Study confounds method with model size/type
- Evidence needed: Controlled experiment using same LLM for both methods

### Open Question 2
- Question: Why do model-annotated difficulty values align more with expert perception than student performance?
- Basis: [Section 6.4] Model difficulty correlates more with experts than students
- Why unresolved: Paper identifies misalignment but doesn't investigate cognitive model similarities
- Evidence needed: Analysis of question types with expert-student divergence, probing LLM improvement with student-level features

### Open Question 3
- Question: What specific distractor characteristics drive higher student engagement in human-authored MCQs?
- Basis: [Section 5.6] Students engage more with human-authored distractors; better 3-rule compliance
- Why unresolved: Study demonstrates engagement gap but doesn't identify linguistic/pedagogical features
- Evidence needed: Fine-grained analysis of distractor attributes correlated with selection rates

## Limitations
- Two-step generation method performs significantly worse (55.6% vs. 86.7-93.3% answerability), suggesting component-level generation may be model-dependent
- Small corpus (12 texts, 58 human-authored MCQs) may limit generalizability
- Expert panel of 18 reviewers and student sample of 284 may have demographic limitations

## Confidence
- **High Confidence**: One-step generation superiority (well-formedness rates, answerability percentages)
- **Medium Confidence**: Post-hoc difficulty estimation improvement, language variant drift observation
- **Low Confidence**: Specific mechanisms explaining one-step vs. two-step quality differences, precise narrative element control relationship

## Next Checks
1. Conduct ablation study comparing one-step vs. two-step across multiple LLMs and fine-tuning configurations to isolate method vs. model capacity effects
2. Perform larger-scale study (200+ MCQs) comparing in-generation vs. post-hoc difficulty assignment with statistical testing of correlation differences
3. Replicate one-step generation pipeline with equivalent Portuguese texts in Spanish or French using same prompt templates to test language-specific phenomena