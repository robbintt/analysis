---
ver: rpa2
title: 'On The Landscape of Spoken Language Models: A Comprehensive Survey'
arxiv_id: '2504.08528'
source_url: https://arxiv.org/abs/2504.08528
tags:
- speech
- text
- tokens
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper provides a comprehensive survey of spoken language models
  (SLMs), categorizing them into three main types: pure speech LMs, speech+text LMs,
  and speech-aware text LMs. It outlines their architectures, training strategies,
  and evaluation methods, highlighting the shift from task-specific models to universal
  speech processing systems.'
---

# On The Landscape of Spoken Language Models: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2504.08528
- Source URL: https://arxiv.org/abs/2504.08528
- Reference count: 34
- Primary result: The paper provides a comprehensive survey of spoken language models (SLMs), categorizing them into three main types: pure speech LMs, speech+text LMs, and speech-aware text LMs, while highlighting key challenges in efficiency, data scarcity, and the need for standardized benchmarks.

## Executive Summary
This survey provides a comprehensive overview of Spoken Language Models (SLMs), which extend the capabilities of Large Language Models to process and generate speech. The authors categorize SLMs into three main types: pure speech LMs that operate solely on speech tokens, speech+text LMs that jointly model both modalities, and speech-aware text LMs that process speech as features for text-based reasoning. The survey outlines the architectural components, training strategies, and evaluation methods across these categories, highlighting the shift from task-specific models to universal speech processing systems. Key challenges include model efficiency, training data scarcity, and the need for standardized benchmarks to enable fair comparisons across different approaches.

## Method Summary
The survey synthesizes existing literature on SLMs by categorizing them based on their input representation and architecture. It describes the general pipeline for each category: speech encoders (SSL models like HuBERT or codecs like EnCodec) that extract features or generate discrete tokens, modality adapters that align speech representations with text embeddings, sequence models (LLMs) that perform the core language modeling, and speech decoders that convert output tokens back to waveforms. The training procedures vary from next-token prediction on discrete speech units to joint modeling of speech and text through cross-modal alignment. The survey evaluates these approaches across understanding tasks (ASR, translation, classification) and generative tasks (TTS, speech continuation) using various metrics including likelihood-based measures, generative perplexity, and task-specific benchmarks.

## Key Results
- SLMs can be categorized into three main types: pure speech LMs, speech+text LMs, and speech-aware text LMs
- The shift from task-specific models to universal speech processing systems is enabled by discrete speech tokenization and modality alignment
- Key challenges include model efficiency, training data scarcity, and the need for standardized benchmarks
- Open-source models and inclusive, safe designs are essential for broader accessibility and responsible deployment

## Why This Works (Mechanism)

### Mechanism 1: Discrete Speech Tokenization
Converting continuous audio waveforms into discrete token sequences enables standard LLM architectures to process speech natively. This is achieved through a speech encoder (e.g., HuBERT or neural audio codecs) that extracts features, followed by quantization (e.g., k-means clustering or Residual Vector Quantization) that maps audio segments to a finite vocabulary of "speech tokens," allowing the sequence model to treat speech exactly like text. The core assumption is that the discretization process preserves sufficient linguistic and acoustic information to reconstruct meaningful speech or derive semantic understanding.

### Mechanism 2: Modality Alignment via Adapters
Mapping speech features into the text embedding space of a pre-trained LLM allows the model to leverage existing linguistic reasoning capabilities for audio inputs. A "modality adapter" (e.g., a linear projector, Q-Former, or CNN) takes the output of the speech encoder and projects it into the dimension of the LLM's input embeddings, aligning the speech features so the LLM interprets them as if they were text representations. The core assumption is that the pre-trained text LLM possesses robust reasoning capabilities that can be triggered by non-text inputs if those inputs reside in the same vector space as the text tokens.

### Mechanism 3: Hierarchical Token Decomposition
Decoupling speech generation into hierarchical stages (coarse-to-fine) improves the synthesis of high-fidelity audio. The model generates "coarse" semantic tokens first (phonetic content), then conditions the generation of "fine" acoustic tokens (pitch, timbre) on the coarse output, separating the problem of "what to say" from "how to say it." The core assumption is that speech acoustics are high-frequency details that are better predicted conditionally based on a robust semantic plan rather than predicted all at once.

## Foundational Learning

- **Self-Supervised Learning (SSL) for Speech**
  - Why needed here: The survey identifies SSL models (e.g., wav2vec 2.0, HuBERT) as the primary source for speech representations, serving as the "speech encoder" in almost all architectures.
  - Quick check question: Can you explain the difference between learning phonetic content versus learning speaker identity in a speech encoder?

- **Residual Vector Quantization (RVQ)**
  - Why needed here: Understanding how "audio codec tokens" work requires understanding RVQ, which uses multiple layers of codebooks to compress audio efficiently while retaining quality.
  - Quick check question: How does RVQ differ from standard Vector Quantization, and why is it necessary for high-fidelity audio codecs like EnCodec?

- **Instruction Tuning**
  - Why needed here: The transition from "pre-training" to "post-training" relies on instruction tuning to turn a general distribution model into a chat or task-solving assistant.
  - Quick check question: What is the difference between training on $p(speech)$ (pre-training) versus training on $p(\cdot|speech, instruction)$ (post-training)?

## Architecture Onboarding

- **Component map:** Speech Encoder (HuBERT/EnCodec) -> Modality Adapter (Q-Former/Linear) -> Sequence Model (Transformer LLM) -> Speech Decoder (Vocoder/Codecs)
- **Critical path:** The **Modality Adapter** is the most critical "bridge." If the adapter fails to compress the long speech sequences or misaligns embeddings, the Sequence Model receives garbage input.
- **Design tradeoffs:**
  - **Discrete vs. Continuous Representations:** Discrete tokens (Pure SLMs) allow joint modeling with text but lose some acoustic detail. Continuous features (Speech-aware Text LMs) preserve detail but are harder to align with discrete text tokens.
  - **Frozen vs. Fine-tuned Backbone:** Freezing the LLM saves resources but may limit the model's ability to deeply integrate audio understanding.
- **Failure signatures:**
  - **Hallucination:** The model describes audio events that did not occur
  - **Catastrophic Forgetting:** The model learns to process speech but loses its ability to follow text instructions or reason logic
- **First 3 experiments:**
  1. **Sanity Check (Reconstruction):** Pass audio through the Encoder -> Tokenizer -> Decoder (bypassing the LLM) to verify that the "speech token" representation retains enough information for intelligible reconstruction.
  2. **Alignment Check (ASR):** Train *only* the Modality Adapter on a standard ASR task (Speech -> Text) with the LLM backbone frozen. If this fails, the adapter cannot map speech features to the LLM's semantic space.
  3. **Generation Check (TTS):** Prompt the model with text and verify if the output speech tokens (decoded via the Vocoder) produce intelligible speech, ensuring the causal loop works.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the optimal speech representation (discrete vs. continuous, phonetic vs. acoustic) for universal Spoken Language Models? This remains unresolved due to lack of thorough comparative ablations across different encoders and tokenization methods.

- **Open Question 2:** How do Spoken Language Models scale with respect to data size and model parameters compared to text LLMs? The field currently lacks specific scaling laws, leaving the relationship between parameter counts, data volume, and speech capabilities undefined.

- **Open Question 3:** How can the latency and size of SLMs be reduced to enable real-time, on-device duplex dialogue? Current SLMs are large and slow, creating an "inherent efficiency challenge" for practical real-time applications.

- **Open Question 4:** How can the community establish standardized benchmarks that cover the full range of spoken language tasks, particularly generation? Current models are evaluated on heterogeneous datasets, preventing direct comparison or reproducibility across different SLM approaches.

## Limitations

- The survey's comprehensiveness is limited by the rapidly evolving nature of the field, with many recent developments potentially not fully captured
- The categorization framework may oversimplify nuanced architectural variations and doesn't fully account for hybrid approaches
- Many proposed models remain closed-source, limiting reproducibility and independent validation of the claims
- Training data scarcity is identified as a fundamental bottleneck, yet specific strategies for data augmentation or efficient training are not deeply explored

## Confidence

- **High Confidence:** The categorization framework and architectural descriptions are well-supported by existing literature and provide a clear taxonomy for understanding SLM development
- **Medium Confidence:** The proposed mechanisms (discrete tokenization, modality alignment, hierarchical decomposition) are theoretically sound but lack comprehensive empirical validation across all model types
- **Low Confidence:** Predictions about future directions and the relative importance of different architectural choices are speculative given the field's rapid evolution

## Next Checks

1. **Benchmark Validation:** Implement the three SLM categories (pure speech, speech+text, speech-aware text) on a standardized evaluation suite using Dynamic-SUPERB or VoiceBench to empirically validate the survey's categorization framework and performance claims.

2. **Tokenization Impact Study:** Systematically compare different tokenization strategies (phonetic vs. codec tokens) on downstream tasks like ASR, TTS, and speech understanding to quantify the trade-offs between discrete and continuous representations.

3. **Efficiency Analysis:** Conduct controlled experiments varying model size, quantization levels, and adapter configurations to measure the efficiency claims and identify the actual bottlenecks in scaling spoken language models across different categories.