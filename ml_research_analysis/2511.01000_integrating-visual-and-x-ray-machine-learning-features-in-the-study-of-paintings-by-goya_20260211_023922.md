---
ver: rpa2
title: Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings
  by Goya
arxiv_id: '2511.01000'
source_url: https://arxiv.org/abs/2511.01000
tags:
- x-ray
- visual
- authentication
- feature
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of authenticating Francisco
  Goya's paintings, which are complicated by his stylistic evolution and historical
  forgeries. The authors propose a unified multimodal machine learning framework that
  applies identical feature extraction techniques to both visual and X-ray radiographic
  images of Goya paintings.
---

# Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya

## Quick Facts
- arXiv ID: 2511.01000
- Source URL: https://arxiv.org/abs/2511.01000
- Reference count: 31
- Authors: Hassan Ugail; Ismail Lujain Jaleel
- Primary result: 97.8% classification accuracy for Goya authentication using multimodal visual + X-ray feature fusion

## Executive Summary
This study presents a unified multimodal machine learning framework for authenticating Francisco Goya paintings by combining visual and X-ray radiographic imagery. The approach extracts identical handcrafted texture features (GLCM, LBP, entropy, energy, color distribution) from both modalities and fuses them through vector concatenation before processing with an optimized One-Class Support Vector Machine. Using 24 authenticated Goya paintings with corresponding X-ray images, the framework achieves 97.8% classification accuracy with a 0.022 false positive rate, demonstrating substantial improvement over single-modal approaches. A case study of "Un Gigante" shows practical efficacy with 92.3% authentication confidence.

## Method Summary
The framework extracts 14 handcrafted features from each visual and X-ray image, then concatenates them into a 28-dimensional feature vector. Features include GLCM descriptors (contrast, homogeneity, energy, correlation) across multiple distances and angles, rotation-invariant uniform LBP histograms, entropy, energy, mean, standard deviation, and HSV color statistics for visual images. The features undergo z-score normalization before being processed by an RBF-kernel One-Class SVM with hyperparameters (ν, γ) tuned via grid search optimizing F1 score. The dataset of 24 Goya paintings is split 80/20 for training/testing with 10-fold cross-validation, ensuring visual-X-ray pairs remain together in folds.

## Key Results
- 97.8% classification accuracy achieved using multimodal feature fusion
- False positive rate of 0.022 demonstrated through cross-validation
- Substantial performance improvement over visual-only (93.4%) and X-ray-only (91.7%) baselines
- Case study of "Un Gigante" achieved 92.3% authentication confidence
- One-Class SVM successfully learned from authenticated examples without requiring forged samples

## Why This Works (Mechanism)

### Mechanism 1
Unified multimodal feature concatenation improves classification robustness over single-modality approaches by capturing complementary information. Visual imagery reveals surface texture and brushwork, while X-ray imagery exposes subsurface density patterns and pentimenti. The concatenated feature vector creates a higher-dimensional decision boundary where authentic works form a tighter cluster than in unimodal spaces. The core assumption is that visual and X-ray features are non-redundant and the classifier can resolve the high-dimensional boundary without overfitting.

### Mechanism 2
One-Class SVM enables authentication using only positive examples by learning a decision function that envelops the density of authentic Goya works in feature space. Unlike binary classifiers, it rejects samples falling outside this learned boundary as anomalies. The method assumes the "authentic" class forms a tight, contiguous cluster in the transformed kernel space separable from outliers. This approach is particularly suitable when forged examples are scarce or unavailable.

### Mechanism 3
Handcrafted texture features (GLCM, LBP) transfer effectively from visual to radiographic domains by treating X-rays as specialized grayscale imagery. Identical mathematical operations extract comparable signatures of the artist's physical interaction with materials, regardless of whether the image shows reflected light or material density. The core assumption is that statistical properties of brushwork texture correlate with physical density of pigment application detected by X-rays.

## Foundational Learning

- **Concept: One-Class Support Vector Machine (OC-SVM)**
  - Why needed: Standard classifiers require labeled "fake" and "real" data, but verified fakes are scarce while authentic works are limited.
  - Quick check: How does the parameter $\nu$ affect the sensitivity of the model to outliers in the training set?

- **Concept: Grey-Level Co-occurrence Matrix (GLCM)**
  - Why needed: This core texture descriptor captures spatial relationships between pixel intensities, quantifying brushwork mathematically.
  - Quick check: If you rotate a painting by 45 degrees, should the GLCM features change, or should you use rotation-invariant features?

- **Concept: Cross-Validation with Paired Data**
  - Why needed: The dataset has 24 paintings, each with a Visual and X-ray pair that must remain together.
  - Quick check: If you put the Visual image of "Painting A" in the training fold and its X-ray image in the validation fold, are you testing generalization or memorization?

## Architecture Onboarding

- **Component map:** Input (Visual Image + X-ray Image) → Preprocessing (Resize 512x512, CLAHE) → Feature Extraction (GLCM, LBP, Entropy, Energy, HSV Stats) → Fusion (Vector Concatenation) → Normalization (Z-score scaling) → Model (One-Class SVM with RBF Kernel)

- **Critical path:** The hyperparameter tuning (ν and γ) is the most sensitive step. The paper uses Grid Search with 10-fold CV. If this step is skipped or performed on the test set, the 97.8% accuracy claim is invalid.

- **Design tradeoffs:** Handcrafted vs. Deep Learning trades raw classification power for interpretability and lower data requirements. Concatenation vs. Fusion uses simple concatenation for computational efficiency, assuming features are independent.

- **Failure signatures:** Stylistic Drift Overfitting if training paintings are mostly from one period, X-ray Calibration Drift from different machines, Data Leakage from improper splitting of visual/X-ray pairs.

- **First 3 experiments:**
  1. **Ablation Study:** Run the pipeline three times: (1) Visual-only features, (2) X-ray-only features, (3) Combined. Verify that Combined > max(Single Modalities).
  2. **Hyperparameter Sensitivity:** Vary ν (e.g., 0.05 to 0.5) and observe the False Positive Rate. Check if the claimed 0.022 is stable or a specific "sweet spot."
  3. **Out-of-Distribution Test:** Apply the trained model to a verified non-Goya painting from the same era (e.g., a Velázquez) to confirm it flags as an anomaly.

## Open Questions the Paper Calls Out

### Open Question 1
How effectively does the One-Class SVM framework identify confirmed forgeries, given the model is trained exclusively on positive samples? The reported 0.022 false positive rate is a statistical estimate derived from the training data distribution rather than an empirical measure based on testing against actual known forgeries. Empirical testing against a curated dataset of verified Goya forgeries would resolve this question.

### Open Question 2
Can the identical feature extraction pipeline generalize to artists with different radiographic signatures, such as those with less underdrawing? The model is optimized for Goya's specific stylistic evolution and documented compositional modifications that generate unique subsurface signatures. Validation on diverse artist datasets (e.g., Rembrandt or Raphael) would test cross-artist robustness without re-engineering the feature set.

### Open Question 3
Would deep learning fusion techniques outperform the current handcrafted feature approach if a larger dataset were available? The study acknowledges high accuracy in prior deep learning studies while defending handcrafted features based on limited training data and interpretability. A comparative ablation study on a larger multimodal art dataset would benchmark the current approach against end-to-end deep learning fusion models.

## Limitations

- Small proprietary dataset of only 24 authenticated paintings limits generalizability and prevents independent verification
- Unspecified preprocessing parameters (CLAHE clip limit, tile size, X-ray normalization method) could significantly impact results
- Lack of out-of-distribution testing on non-Goya period works represents a critical validation gap
- Case study validation lacks comparative context without baseline performance on known authentic/inauthentic examples

## Confidence

**High Confidence:** Technical implementation of feature extraction and One-Class SVM framework are sound and well-documented. Internal consistency of improvements over single-modal baselines is demonstrated.

**Medium Confidence:** 97.8% accuracy and 0.022 false positive rate are credible within constrained experimental setup, but generalizability to different X-ray acquisition protocols remains uncertain due to small sample size.

**Low Confidence:** Case study result (92.3% for "Un Gigante") lacks sufficient comparative context to establish real-world utility without testing on external known authentic/inauthentic examples.

## Next Checks

1. **Data Transparency:** Publish the complete list of 24 paintings with acquisition dates, original dimensions, and X-ray exposure parameters to enable reproducibility and dataset bias analysis.

2. **Robustness Testing:** Evaluate the trained model on X-ray images from different institutions (varying exposure times, resolutions) to assess sensitivity to acquisition conditions rather than artistic features.

3. **Temporal Generalization:** Train the model on paintings from Goya's early period only, then test on authenticated late-period works to determine if the model overfits to stylistic evolution within the training timeframe.