---
ver: rpa2
title: Improving French Synthetic Speech Quality via SSML Prosody Control
arxiv_id: '2508.17494'
source_url: https://arxiv.org/abs/2508.17494
tags:
- prosodic
- speech
- prosody
- ssml
- break
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the first end-to-end pipeline that automatically
  converts French speech into SSML-compliant markup for controlling pitch, speaking
  rate, volume, and pause duration in TTS systems. The cascaded architecture uses
  two QLoRA-fine-tuned Qwen 2.5-7B models: one predicts phrase-break positions and
  the other performs regression on prosodic targets.'
---

# Improving French Synthetic Speech Quality via SSML Prosody Control

## Quick Facts
- arXiv ID: 2508.17494
- Source URL: https://arxiv.org/abs/2508.17494
- Reference count: 17
- First end-to-end pipeline converting French speech to SSML markup for pitch, rate, volume, and pause control in TTS systems

## Executive Summary
This paper introduces the first end-to-end pipeline that automatically converts French speech into SSML-compliant markup for controlling pitch, speaking rate, volume, and pause duration in TTS systems. The cascaded architecture uses two QLoRA-fine-tuned Qwen 2.5-7B models: one predicts phrase-break positions and the other performs regression on prosodic targets. Evaluated on a 14-hour French podcast corpus, the system achieves 99.2% F1 for break placement and reduces mean absolute error on pitch, rate, and volume by 25-40% compared to LLM prompting and BiLSTM baselines. Perceptual evaluation with 18 participants shows a significant increase in naturalness, with MOS rising from 3.20 to 3.87 (p < 0.005) and 15 out of 18 listeners preferring the enhanced synthesis.

## Method Summary
The pipeline processes raw French audio through Demucs source separation, Whisper timestamped alignment, and syntagm segmentation to extract prosodic features. These features are normalized relative to a commercial Azure "Henri" voice baseline to compute delta values. Two QLoRA-fine-tuned Qwen 2.5-7B models operate in cascade: QwenA predicts break positions (up to 200 words, 1024-token limit), and QwenB performs regression on numeric prosody attributes using LoRA adapters injected into value and feed-forward projections. The system applies exponential smoothing (α=0.2) to pitch and rate, standardizes targets to unit variance, and clamps values within ±8% per syntagm.

## Key Results
- 99.2% F1 score for phrase-break prediction on test set
- 25-40% reduction in MAE for pitch, rate, and volume compared to LLM prompting and BiLSTM baselines
- MOS improvement from 3.20 to 3.87 (p < 0.005) in perceptual evaluation
- 15 out of 18 participants preferred enhanced synthesis over baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating structural tag placement from numerical prosody regression improves both accuracy and markup validity compared to single-model approaches.
- Mechanism: QwenA predicts only where `<break>` tags belong, producing a syntactically valid SSML skeleton. QwenB fills numeric attributes into pre-placed `<prosody>` elements. This division prevents error modes where LLMs under-generate tags or produce invalid SSML structures.
- Core assumption: Text contains sufficient linguistic cues to predict prosodic boundaries independently of acoustic features.
- Evidence anchors: Abstract states "one predicts phrase-break positions and the other performs regression on prosodic targets"; Section 4.4 notes "disentangling structural prediction from numerical regression yields optimal performance."

### Mechanism 2
- Claim: Computing prosodic deltas relative to a commercial TTS baseline yields transferable, interpretable control signals.
- Mechanism: Extract pitch, volume, rate from both natural podcast audio and synthetic Azure "Henri" voice. Compute normalized deltas (e.g., semitone offset for pitch, percentage change for rate). These relative values become training targets, grounding predictions in voice-agnostic adjustments rather than absolute acoustic values.
- Core assumption: The baseline voice has stable, neutral prosody against which meaningful deltas can be computed.
- Evidence anchors: Section 3 describes "relative delta values for SSML encoding... normalized relative to a baseline computed as the median over a sliding window"; Azure Henri selected for "clarity... and consistent yet neutral prosodic characteristics."

### Mechanism 3
- Claim: QLoRA fine-tuning with loss computed only on numeric tokens focuses model capacity on prosodic parameter regression.
- Mechanism: For QwenB, inject LoRA adapters into value and feed-forward projections. Zero out loss on categorical text tokens; train only on numeric attribute tokens. Standardization to unit variance stabilizes gradient updates during regression.
- Core assumption: The 7B base model already encodes sufficient linguistic understanding; only low-rank adaptation is needed for numerical prosody prediction.
- Evidence anchors: Section 4.4 states "Loss is computed on the numeric tokens, so categorical text incurs zero penalty" and "Targets are standardized to unit variance during optimization."

## Foundational Learning

- **SSML (Speech Synthesis Markup Language)**: Why needed: The entire pipeline produces SSML tags (`<prosody>`, `<break>`) that commercial TTS engines consume. Understanding SSML syntax is prerequisite to debugging output and extending the pipeline. Quick check: Can you explain why `<prosody pitch="+5%">` is a valid SSML tag but `<prosody pitch="loud">` would be rejected?

- **Prosodic features (pitch, rate, volume, breaks)**: Why needed: These four features constitute the prediction targets. Pitch uses semitone offsets; rate uses words-per-second deltas; volume uses LUFS; breaks use millisecond durations. Feature-specific normalization and clipping differ per parameter. Quick check: Why does the pipeline apply exponential smoothing (α=0.2) to pitch and rate but not to volume?

- **QLoRA fine-tuning**: Why needed: Both QwenA and QwenB use 4-bit quantization with rank-8 LoRA adapters. Understanding QLoRA is necessary to modify training configs, adjust rank/α, or debug adapter injection. Quick check: What is the computational trade-off between rank-8 and rank-32 adapters in terms of GPU memory vs. expressiveness?

## Architecture Onboarding

- Component map: Raw Audio → Demucs (source separation) → Whisper Timestamped (alignment) → Text + Timestamps → Syntagm Segmentation → Natural Features ←→ Azure Baseline Features → Delta Computation → SSML-annotated Text → [INFERENCE] Raw Text → QwenA (break prediction) → SSML Skeleton → QwenB (prosody regression) → Full SSML → Commercial TTS → Enhanced Audio

- Critical path: Data preparation (Whisper alignment accuracy 96.3% ARR determines quality of extracted features), QwenA training (F1=99.24% on break prediction), QwenB regression (MAE <1.1% on all prosodic coefficients).

- Design tradeoffs: Cascaded vs. single model (cascaded adds latency ~190ms for 150-word paragraph but ensures valid SSML structure), 7B vs. 32B models (Qwen3-32B achieves marginally higher structural similarity but requires 4.5× memory), sliding window (w=10) vs. global median (local normalization captures speaker dynamics).

- Failure signatures: Tag under-generation (output contains few/no tags, check QwenA perplexity), invalid SSML syntax (malformed tags, check post-processor), extreme prosody values (pitch/rate deltas exceed ±10%, check smoothing and clamping).

- First 3 experiments: Reproduce baseline metrics (fine-tune QwenA, verify F1 ≥99%), ablate cascade (replace QwenA output with gold break positions, measure QwenB MAE change), cross-voice transfer (apply pipeline to different Azure French voice, measure MOS degradation without recalibration).

## Open Questions the Paper Calls Out

- Would a unified single end-to-end model outperform the cascaded QwenA + QwenB architecture for joint prosodic prediction? (Future directions explicitly mention "unifying our cascaded approach into a single end-to-end model")

- Can the methodology transfer to languages with fundamentally different prosodic structures (e.g., tonal, pitch-accent, or stress-timed languages)? (Future directions mention "extending this methodology to additional languages")

- Would incorporating audio embeddings from the natural speech improve prosody prediction accuracy beyond text-only features? (Future directions propose "incorporating multimodal audio embeddings")

- How robust is break prediction on unpunctuated or informally structured text (e.g., social media transcripts)? (Limitations note the approach "assumes that punctuation and syntactic cues correlate well with natural prosodic boundaries")

## Limitations

- Data dependence on aligned natural-synthetic audio pairs limits generalization to other domains and genres beyond the 14-hour French podcast corpus
- Voice-specific normalization calibrated to Azure "Henri" requires recalibration for different baseline voices
- Unspecified hyperparameters (train/validation splits, fine-tuning epochs, batch size, learning rate, clipping bounds) limit reproducibility

## Confidence

- **High Confidence**: Break prediction accuracy (F1=99.24%) and perceptual naturalness gains (MOS 3.20→3.87, p<0.005) are well-supported by direct measurements and statistical tests
- **Medium Confidence**: Delta normalization mechanism and QLoRA fine-tuning efficiency are logically sound but lack direct ablation or comparison studies
- **Low Confidence**: Cross-voice transfer and out-of-domain robustness are acknowledged as limitations but not empirically tested

## Next Checks

1. Cross-Voice Transfer Test: Apply trained pipeline to a different French TTS voice (e.g., Amazon Polly or Google WaveNet), measure MOS degradation and quantify recalibration effort needed

2. Ablation of Delta Normalization: Train QwenB to predict absolute prosodic values instead of deltas, compare MAE/MOS to delta-based model

3. Whisper Hallucination Stress Test: Intentionally feed Whisper alignments with known silent segments, measure feature extraction accuracy and downstream SSML quality to quantify robustness to alignment errors