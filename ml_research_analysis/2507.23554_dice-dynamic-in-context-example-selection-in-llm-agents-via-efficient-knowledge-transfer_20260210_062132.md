---
ver: rpa2
title: 'DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge
  Transfer'
arxiv_id: '2507.23554'
source_url: https://arxiv.org/abs/2507.23554
tags:
- arxiv
- learning
- in-context
- selection
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of in-context learning (ICL)
  in LLM-based agents, where the effectiveness of demonstrations is highly sensitive
  to example selection. Poor demonstrations can introduce spurious dependencies that
  degrade agent performance across reasoning steps.
---

# DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer

## Quick Facts
- arXiv ID: 2507.23554
- Source URL: https://arxiv.org/abs/2507.23554
- Reference count: 40
- One-line primary result: Dynamic in-context example selection improves LLM agent performance by 7-9% on HotpotQA, 7-10% on AlfWorld, and 3-5% on Webshop by adaptively selecting relevant demonstrations at each reasoning step

## Executive Summary
This paper addresses the instability of in-context learning in LLM-based agents, where demonstration effectiveness is highly sensitive to example selection. Poor demonstrations can introduce spurious dependencies that degrade performance across reasoning steps. DICE proposes a dynamic selection framework that adaptively chooses the most relevant demonstrations at each timestep by maximizing transferable knowledge and minimizing task-specific noise. The approach is training-free, framework-agnostic, and operates entirely at inference time.

Extensive experiments across diverse domains demonstrate substantial improvements: on HotpotQA, DICE improves Exact Match scores by 7-9% across different agent frameworks; on AlfWorld, it boosts success rates by 7-10% across subcategories; on Webshop, it improves both average score and success rate. Theoretical analysis shows that DICE's selection criterion yields tighter generalization bounds. Stepwise selection consistently outperforms static approaches, and DICE remains effective even with low-quality demonstrations, highlighting its robustness and practical utility.

## Method Summary
DICE is a dynamic in-context example selection framework for multi-step LLM agents. It operates by extracting transferable knowledge (TK) from both current agent context and demonstration trajectories using a pre-trained encoder (gemma-2-2b-it), then selecting demonstrations that maximize the influence of causal knowledge transfer while minimizing spurious correlations. At each timestep, the agent's history is encoded into TK_t, and similarity scores are computed between TK_t and each demonstration's TK_d using cosine similarity. The top-M demonstrations are selected via a softmax over these scores, with the selection criterion J_i = I(d_i; TK_d_i) - β·I(TK_d_i; A_t) derived from information bottleneck theory. The framework integrates seamlessly with existing agent architectures like ReAct, Reflexion, and LATS without requiring retraining.

## Key Results
- HotpotQA: DICE improves Exact Match scores by 7-9% across different agent frameworks
- AlfWorld: DICE boosts success rates by 7-10% across subcategories
- Webshop: DICE improves both average score and success rate
- Stepwise selection consistently outperforms static task-level selection by 5-7 points
- DICE remains effective even with low-quality demonstrations, demonstrating robustness

## Why This Works (Mechanism)

### Mechanism 1: Causal Decomposition of Demonstration Knowledge
- Claim: Demonstrations contain both transferable knowledge (TK) that aids generalization and task-specific noise (ε_D) that introduces spurious correlations, and filtering the latter improves agent performance.
- Mechanism: The paper models ICL through a causal graph where TK → A_t (action) represents the beneficial path, while conditioning on demonstration D opens a collider structure ε_D → D ← TK → A_t, creating a backdoor path that allows irrelevant information to influence decisions. By selecting demonstrations that maximize TK relative to the current reasoning context, DICE blocks this spurious information flow.
- Core assumption: The causal graph accurately represents how demonstrations influence agent decisions, and TK can be meaningfully isolated from task-specific noise.
- Evidence anchors: [section 2.1]: "This graph intuitively explains a limitation of standard In-Context Learning (ICL). By providing the demonstration D as input, we inherently condition on D, which opens a collider structure... a backdoor path is created, allowing spurious task-specific information from ε_D to influence the generation of A_t."

### Mechanism 2: Information Bottleneck Selection Criterion
- Claim: Selecting demonstrations that compress task-specific details while preserving action-predictive information yields tighter generalization bounds and better performance.
- Mechanism: DICE optimizes J_i = I(d_i; TK_d_i) - β·I(TK_d_i; A_t), where the first term penalizes non-transferable information and the second rewards action-relevant knowledge. When β=1, this reduces to minimizing I(d_i; TK_i | A_t), which Theorem 2.3 connects to generalization gap bounds.
- Core assumption: Mutual information terms meaningfully capture transferable vs. spurious knowledge, and the theoretical bounds translate to empirical improvements.
- Evidence anchors: [section 2.2]: "This objective naturally arises from our causal analysis... by maximizing the influence of the causal path TK → A_t while minimizing the effect of the spurious dependency induced by the collider."

### Mechanism 3: Dynamic Stepwise Retrieval
- Claim: Selecting different demonstrations at each reasoning step outperforms static, task-level selection because information needs evolve as the agent progresses.
- Mechanism: At each timestep t, DICE extracts TK from current context H_t as a proxy for the next action, then retrieves demonstrations maximizing I(TK_d; TK_t) via cosine similarity. This adapts guidance to immediate decision needs rather than global task similarity.
- Core assumption: Current context provides sufficient signal to estimate what knowledge the next action requires, and retrieval similarity correlates with actual utility.
- Evidence anchors: [section 3.4, Table 4]: "Stepwise DICE consistently outperforms its taskwise variant, demonstrating the value of adapting demonstrations at each reasoning step." HotpotQA: 41.4 vs 36.3 EM; AlfWorld: 67.9 vs 61.2 SR.

## Foundational Learning

- Concept: **Collider Bias in Causal Graphs**
  - Why needed here: DICE's theoretical justification relies on understanding how conditioning on a variable (the demonstration) opens a collider path, allowing spurious information to influence outcomes. Without this, the decomposition into TK and ε_D lacks conceptual grounding.
  - Quick check question: Can you explain why conditioning on D in the graph ε_D → D ← TK creates a statistical dependency between ε_D and downstream variables?

- Concept: **Information Bottleneck Principle**
  - Why needed here: The selection criterion J_i derives from IB theory, trading off compression (minimizing I(d; TK)) against relevance (maximizing I(TK; A_t)). Understanding this tradeoff is essential for interpreting the objective and tuning β.
  - Quick check question: What does minimizing I(X; T) while maximizing I(T; Y) achieve in representation learning?

- Concept: **InfoNCE Lower Bound**
  - Why needed here: DICE approximates intractable mutual information using InfoNCE with cosine similarity. Understanding contrastive estimation helps diagnose when retrieval may fail and how to improve the encoder.
  - Quick check question: How does InfoNCE relate mutual information to contrastive learning objectives, and what assumptions does it make about the similarity function?

## Architecture Onboarding

- Component map: gemma-2-2b-it (Knowledge Retriever) -> Similarity Computation -> Top-M Selection -> Agent Context Update
- Critical path:
  1. Agent reaches timestep t with history H_t = (Demos, Task, a_1, o_1, ..., a_t, o_t).
  2. Knowledge Retriever extracts TK_t from H_t.
  3. For each demonstration d in pool, extract TK_d (can be pre-computed).
  4. Compute similarity(TK_d, TK_t) for all d; select top-M by score.
  5. Replace Demos in context with selected trajectories.
  6. Agent generates next action a_{t+1} using updated context.

- Design tradeoffs:
  - **Encoder choice**: Paper uses gemma-2-2b-it; larger models may extract better TK but increase latency. Assumption: open-source 2B model sufficient for TK extraction.
  - **Pool size vs. retrieval cost**: Larger pools provide more candidates but increase inference time. Paper doesn't specify pool sizes explicitly.
  - **M (number of demonstrations)**: Figure 3b shows 3 DICE examples ≈ 6 standard ICL examples; diminishing returns beyond this not explored.
  - **Stepwise vs. taskwise**: Stepwise provides +5.1 to +6.7 points improvement but requires retrieval at every step.

- Failure signatures:
  1. **No improvement over baseline**: Likely indicates poor TK extraction (encoder mismatch) or irrelevant demonstration pool.
  2. **Performance degradation in later steps**: May indicate accumulated context noise overwhelming TK signal; check history truncation.
  3. **High latency without quality gains**: Retrieval overhead dominates; consider caching TK_d embeddings or reducing pool size.
  4. **Worse performance with high-quality demonstrations**: Contradicts Figure 3a; check similarity function implementation and TK extraction prompts.

- First 3 experiments:
  1. **Replicate HotpotQA EM improvement**: Run ReAct + DICE on 100-question subset; target ≥8 point EM gain over ReAct baseline (32.1 → 40+). Validates end-to-end pipeline.
  2. **Ablate stepwise vs. taskwise**: Compare DICE stepwise vs. DICE taskwise on 50 HotpotQA questions; expect ~5 point gap. Confirms dynamic selection value.
  3. **Test low-quality pool robustness**: Restrict demonstration pool to scores <0.5; verify DICE still outperforms standard ICL by wider margin (per Figure 3c). Validates TK extraction works even with poor demonstrations.

## Open Questions the Paper Calls Out
- **Trainable Encoder for Transferable Knowledge**: The authors note that incorporating a trainable encoder to capture latent transferable knowledge is left for future exploration, as the current implementation uses only a fixed pre-trained LLM approach.
- **Cross-Model Architecture Scaling**: The effectiveness of DICE across different LLM architectures and model sizes beyond gpt-3.5-turbo is unexplored, with all experiments using the same model family.
- **Sensitivity to β and Similarity Function**: The paper does not investigate how sensitive DICE is to the regularization coefficient β or alternative similarity functions for estimating transferable knowledge.

## Limitations
- The causal decomposition into transferable knowledge and task-specific noise relies on assumptions about LLM internal reasoning that aren't directly validated.
- Performance depends heavily on the quality of the Knowledge Retriever, with no exploration of alternative encoders or learned representations.
- The framework's effectiveness across different model families and sizes remains untested, limiting generalizability claims.

## Confidence
- **Transferable Knowledge Decomposition**: Medium - Theoretically sound but assumptions about LLM behavior unverified
- **Information Bottleneck Effectiveness**: Medium - Elegant theory but practical impact depends on InfoNCE approximation accuracy
- **Dynamic vs. Static Selection**: High - Well-demonstrated empirical superiority across all three domains
- **Robustness to Low-Quality Demonstrations**: Medium - Figure 3c suggests effectiveness but may reflect TK extraction rather than selection criterion

## Next Checks
1. **Ablation on Knowledge Retriever Quality**: Replace gemma-2-2b-it with a smaller model or different encoder to quantify how much performance depends on TK extraction quality vs. the selection algorithm itself.

2. **Long-Horizon Task Evaluation**: Test DICE on tasks requiring 10+ reasoning steps to determine if performance gains persist or if accumulated context noise degrades effectiveness over extended interactions.

3. **Zero-Shot Transfer Across Domains**: Evaluate whether demonstrations effective in one domain (e.g., HotpotQA) remain useful when transferred to a different task type (e.g., AlfWorld) without domain-specific fine-tuning.