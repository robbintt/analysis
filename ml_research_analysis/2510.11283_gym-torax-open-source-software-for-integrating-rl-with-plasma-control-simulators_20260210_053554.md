---
ver: rpa2
title: 'Gym-TORAX: Open-source software for integrating RL with plasma control simulators'
arxiv_id: '2510.11283'
source_url: https://arxiv.org/abs/2510.11283
tags:
- plasma
- control
- torax
- gym-torax
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gym-TORAX is an open-source Python package that creates Gymnasium-compatible
  reinforcement learning environments for plasma control in tokamaks by wrapping the
  TORAX simulator. It enables RL researchers to define control actions, observations,
  and reward functions while handling the underlying plasma physics complexity.
---

# Gym-TORAX: Open-source software for integrating RL with plasma control simulators

## Quick Facts
- arXiv ID: 2510.11283
- Source URL: https://arxiv.org/abs/2510.11283
- Reference count: 29
- Primary result: PI controller achieves expected return 3.79 vs. 3.40 for open-loop in ITER hybrid ramp-up scenario

## Executive Summary
Gym-TORAX is an open-source Python package that bridges reinforcement learning research and plasma control simulation by wrapping the TORAX tokamak simulator into Gymnasium-compatible environments. The framework enables RL practitioners to define control actions, observations, and reward functions while abstracting away the complex physics underlying plasma behavior. The package includes a pre-built environment for the ITER hybrid ramp-up scenario and supports two discretization schemes (auto and fixed) for action and observation spaces. Testing with three baseline policies demonstrates the framework's functionality, with a PI controller achieving the highest performance.

## Method Summary
The framework wraps the TORAX plasma control simulator into a Gymnasium environment, providing a clean interface for RL algorithms. It implements two discretization schemes for both action and observation spaces: an auto scheme that automatically determines discretization parameters and a fixed scheme where users specify the number of bins. The package handles TORAX simulation execution, error detection, and environment termination conditions. For the baseline experiments, the ITER hybrid ramp-up scenario is implemented with three control actions (total current, NBI, ECRH), a full observation space, and a reward function that combines multiple plasma performance metrics. A grid search over PI controller gains is performed to find optimal parameters for the ramp-up phase.

## Key Results
- PI controller achieves expected return of 3.79 versus 3.40 for open-loop baseline
- Random policy yields negative return (-10.79), demonstrating the task's difficulty
- Framework successfully handles simulation failures with automatic episode termination
- Two discretization schemes provide flexibility for different RL approaches

## Why This Works (Mechanism)
The framework works by creating a standardized RL interface around the TORAX plasma physics simulator, allowing RL algorithms to interact with complex plasma dynamics through simple action-observation loops. The discretization schemes make continuous plasma control parameters compatible with discrete RL algorithms, while the reward function encodes plasma performance objectives that guide learning. The error handling ensures safe exploration by terminating episodes when simulations become unfeasible, preventing wasted computation on invalid states.

## Foundational Learning

**TORAX plasma simulator**: Physics-based tokamak simulation code used in fusion research; needed because it provides the realistic plasma dynamics that Gym-TORAX wraps for RL training.

**Gymnasium interface**: Standard RL environment API (reset(), step(), render()); needed because it allows Gym-TORAX to be compatible with existing RL libraries and algorithms.

**Discretization schemes**: Methods to convert continuous control spaces to discrete representations; needed because most RL algorithms require discrete action spaces, while plasma control involves continuous parameters.

**Plasma ramp-up scenarios**: Initial heating and current drive phases in tokamak operation; needed because they represent critical transitional phases where precise control is essential for successful fusion experiments.

**Reward shaping for plasma control**: Design of reward functions that balance multiple plasma parameters; needed because effective RL requires carefully constructed rewards that capture the complex objectives of fusion experiments.

## Architecture Onboarding

**Component map**: Gym-TORAX environment -> TORAX simulator -> Plasma physics model -> Observation/action spaces

**Critical path**: Action selection → TORAX simulation → State update → Reward calculation → Environment step

**Design tradeoffs**: Auto discretization (simpler but less control) vs. fixed discretization (more control but requires tuning); full observability vs. partial observability for computational efficiency.

**Failure signatures**: Simulation crashes → episode termination with r_t=-1000; action clipping → silent clipping with flag in info dict; infeasible plasma states → automatic episode reset.

**First experiments**:
1. Instantiate IterHybridEnv with bounded action space and verify observation dimensions
2. Run one step with random action and inspect info dict for clipping flags
3. Execute full episode with open-loop policy and verify expected return ≈ 3.40

## Open Questions the Paper Calls Out

**Can deep RL agents exceed PI controller performance?** The paper establishes baselines but does not train deep RL agents against these benchmarks, leaving open whether advanced algorithms can surpass the PI controller's return of 3.79.

**How does dynamic parameterization affect policy generalization?** Future parameterization tools are proposed but not implemented, raising questions about how agents would perform across varying tokamak geometries.

**What observation variables are necessary for L-H transition control?** Event-handling utilities for managing physics-related events like the L-H transition are noted as future work, indicating current limitations in handling these critical transitions.

## Limitations

**Reward function ambiguity**: Critical reward weights and functional forms are not specified, preventing exact reproduction of reported results.

**Action space configuration gaps**: Exact bounds and ramp-rate limits for control actions are not provided, affecting replication accuracy.

**Observation space implementation details**: Custom observation bounds applied to AllObservation class are unspecified, potentially impacting state representation.

## Confidence

**High Confidence**: Core contribution as Gymnasium wrapper for TORAX simulator with clear package structure and error handling.

**Medium Confidence**: PI controller superiority demonstrated but exact experimental setup uncertain due to missing reward function details.

**Low Confidence**: Broader claims about enabling RL research cannot be fully validated without complete experimental configuration.

## Next Checks

1. **Reward Function Reconstruction**: Contact authors to obtain exact reward function weights and g_i functional forms, or implement baseline configuration for plasma control objectives.

2. **Action Space Verification**: Run PI controller with stated optimal gains (k*_p=0.700, k*_i=34.257) to verify achievable expected return of 3.79.

3. **Baseline Policy Replication**: Reproduce open-loop and random policy results with γ=1 to verify expected returns of 3.40 and -10.79 using n=20, m=60 discretization.