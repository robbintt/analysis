---
ver: rpa2
title: Set-Theoretic Compositionality of Sentence Embeddings
arxiv_id: '2502.20975'
source_url: https://arxiv.org/abs/2502.20975
tags:
- sentence
- embeddings
- embedding
- sentences
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates set-theoretic compositionality in sentence
  embeddings by introducing six criteria based on three operations: TextOverlap, TextDifference,
  and TextUnion. A synthetic dataset of ~192K samples was created using GPT-3.5 to
  fuse consecutive sentences from CNN-DailyMail.'
---

# Set-Theoretic Compositionality of Sentence Embeddings

## Quick Facts
- **arXiv ID**: 2502.20975
- **Source URL**: https://arxiv.org/abs/2502.20975
- **Reference count**: 40
- **Primary result**: SBERT variants consistently satisfy set-theoretic compositionality criteria, outperforming larger LLMs despite their size

## Executive Summary
This paper evaluates set-theoretic compositionality in sentence embeddings by introducing six criteria based on three operations: TextOverlap, TextDifference, and TextUnion. A synthetic dataset of ~192K samples was created using GPT-3.5 to fuse consecutive sentences from CNN-DailyMail. Sixteen models were tested, including seven classical and nine LLM-based encoders. Results show SBERT variants consistently satisfy the proposed criteria, outperforming LLMs despite the latter's larger size. This suggests classical encoders better capture interpretable, set-like semantic transformations. The work offers a novel task-agnostic framework for evaluating compositional properties beyond downstream tasks.

## Method Summary
The study constructs a synthetic dataset of ~192K sentence pairs using GPT-3.5 to fuse consecutive sentences from CNN-DailyMail. Three operations are defined: TextOverlap (intersection), TextDifference (set difference), and TextUnion (union). Six criteria evaluate whether embeddings satisfy set-theoretic properties through similarity margins and geometric projections. Sixteen encoders are tested: seven classical (SBERT variants, USE, InferSent, LASER, SimCSE, RoBERTa) and nine LLMs (GPT-3, LLaMA variants, Mistral, OLMo, OpenELM, Qwen, Gemma). Embeddings are generated via mean pooling of final-layer token embeddings, and evaluation measures adherence to criteria using cosine similarity, dot product, L1, L2, and NED metrics.

## Key Results
- SBERT variants consistently outperform all LLMs on set-theoretic compositionality criteria
- Classical encoders achieve 74-76% adherence to algebraic difference approximation (C4) while LLMs range 49-62%
- Geometric projection analysis shows SBERT embeddings cluster as expected while LLM projections are more scattered
- Human validation of synthetic data achieved average score of 2.77/4, confirming dataset quality

## Why This Works (Mechanism)

### Mechanism 1: Geometric Positioning Reflects Semantic Set Operations
- Claim: If sentence embeddings capture set-like compositionality, then embeddings of operation results should occupy predictable geometric positions relative to input sentence embeddings.
- Mechanism: The paper proposes that TextOverlap embeddings should lie "between" input embeddings (C2), TextDifference embeddings should cluster near the source sentence embedding (C5), and TextUnion embeddings should lie centrally when input norms are comparable (C6). This is tested via projection onto the plane defined by input embeddings and measuring angular relationships.
- Core assumption: Semantic relationships in text map to linear geometric relationships in embedding space that can be captured by cosine similarity and angular measurements.
- Evidence anchors:
  - [abstract] "proposing six criteria based on three core 'set-like' compositions/operations: TextOverlap, TextDifference, and TextUnion"
  - [section 4] "the projection of the embedding of sentence O onto the plane defined by the embeddings of input sentences A and B will lie somewhere in the 'middle' of the input embeddings"
  - [corpus] Related work "Quantifying Compositionality of Classic and State-of-the-Art Embeddings" explores compositional meaning quantification, suggesting broader interest but no direct validation of this specific geometric mechanism.
- Break condition: If embeddings encode semantics non-linearly (as the paper acknowledges in Limitations), angular/projection-based tests may not capture true compositional structure.

### Mechanism 2: Training Objective Determines Embedding Compositional Quality
- Claim: Classical sentence encoders (SBERT variants) better satisfy set-theoretic criteria than LLMs because they are explicitly trained to produce meaningful embeddings rather than predict next tokens.
- Mechanism: SBERT models are fine-tuned on Natural Language Inference (NLI) data with contrastive objectives that directly shape embedding geometry. LLMs (decoder-only) produce embeddings as a byproduct of next-token prediction training, which does not impose set-like geometric constraints.
- Core assumption: The NLI/contrastive training paradigm produces embeddings whose geometric properties align with set-theoretic operations.
- Evidence anchors:
  - [abstract] "SBERT consistently demonstrates set-like compositional properties, surpassing even the latest LLMs"
  - [section 7] "classic encoders are exclusively trained to generate useful embeddings, as opposed to LLMs, which are decoder-only models and mainly trained for next-word prediction tasks"
  - [corpus] Corpus does not provide direct comparative studies of training objectives vs. set-theoretic compositional properties; this mechanism is proposed but not independently validated.
- Break condition: If LLMs were fine-tuned with embedding-specific objectives, they might match or exceed SBERT performance; this is not tested.

### Mechanism 3: Algebraic Vector Operations Approximate Semantic Difference
- Claim: The algebraic difference between input embeddings (∆EA,B = EA − EB) should be more similar to the TextDifference embedding than to the excluded sentence embedding.
- Mechanism: Drawing analogy from word embedding arithmetic (e.g., "king" − "man" + "woman" ≈ "queen"), the paper hypothesizes that embedding subtraction can approximate semantic difference (C4).
- Core assumption: Semantic difference can be linearly approximated in embedding space.
- Evidence anchors:
  - [section 4.2] "the algebraic difference between the input embeddings could serve as a potential approximation of the embedding of the 'Difference'"
  - [section 6.1, Table 3] SBERT variants show 74-76% adherence to C4 (cosine), while LLMs range 49-62%
  - [corpus] "How Do Language Models Compose Functions?" investigates compositional mechanisms in LLMs but does not address algebraic embedding operations; no direct corpus support.
- Break condition: If semantic difference requires non-linear transformation, linear subtraction will systematically fail; the paper's 74% success rate suggests partial but incomplete alignment.

## Foundational Learning

- Concept: **Set-theoretic operations on natural language**
  - Why needed here: The entire framework maps intersection/difference/union from set theory to TextOverlap/TextDifference/TextUnion operations on sentences. Without this grounding, the six criteria are not interpretable.
  - Quick check question: Given sentences A="The cat sat on the mat" and B="The dog sat on the mat," what is TextOverlap(A,B)? (Answer: "sat on the mat" or similar shared content)

- Concept: **Cosine similarity and embedding geometry**
  - Why needed here: All six criteria rely on cosine similarity comparisons and angular measurements between embeddings. C1, C3, C4 use similarity margins; C2, C5, C6 use projection angles.
  - Quick check question: If two embeddings have cosine similarity 0.9, what is their approximate angular relationship? (Answer: ~26° since cos(26°) ≈ 0.9)

- Concept: **Vector projection onto a plane**
  - Why needed here: Criteria C2, C5, C6 require projecting a target embedding onto the plane defined by two input embeddings to measure angular positioning. The appendix (A.2) provides the basis vector computation.
  - Quick check question: Given vectors x and y defining a plane, what are the two steps to compute the basis vectors for that plane? (Answer: Normalize x as b̂₁; compute b₂ = y − (y·b̂₁)b̂₁ and normalize as b̂₂)

## Architecture Onboarding

- Component map:
  - Synthetic Dataset Generator -> Sentence Encoders Under Test -> Evaluation Criteria Engine -> Results Aggregator

- Critical path:
  1. Load/construct synthetic dataset with input-target tuples for each operator
  2. Generate embeddings for all sentences using each encoder
  3. For each criterion, compute similarity/distance metrics and projection angles
  4. Aggregate percentage of samples satisfying conditions across margin grid
  5. Compare classical vs. LLM performance across criteria and metrics

- Design tradeoffs:
  - **Synthetic data generation**: GPT-3.5 fuses sentences; risk of propagation of LLM artifacts. Mitigated by human validation but not eliminated.
  - **Sentence-level only**: Document-level compositionality deferred; may not generalize to longer texts.
  - **Linear similarity metrics**: Paper acknowledges non-linear relationships may exist; L1/L2/cosine may miss complex semantic structures.
  - **Margin sampling**: Epsilon values sampled over 2D grid; computationally expensive and may miss edge cases.

- Failure signatures:
  - **C1/C3 violation**: Overlap/difference embeddings not closer to inputs than inputs are to each other → embeddings lack set-like structure
  - **C4 violation**: Algebraic difference not approximating semantic difference → linear assumption fails
  - **C2/C5/C6 violation**: Projections not in expected angular positions → geometric incoherence
  - **LLM underperformance**: Decoder-only models systematically fail criteria despite parameter scale → training objective mismatch

- First 3 experiments:
  1. **Reproduce C1 results on TextOverlap subset**: Compute Sim(A,O) − Sim(A,B) and Sim(B,O) − Sim(A,B) for 1000 random samples using SBERT-mini and LLaMA3; verify SBERT yields higher positive margin percentages (target: ~28-29% for both conditions met per Table 1)
  2. **Validate projection geometry for C2**: Select 100 TextOverlap samples; compute EOproj for GPT-3 and SBERT-L; verify normalized angles between EA and EB sum to ~1 (per Figure 4a and Appendix Figure 5)
  3. **Test C4 algebraic approximation hypothesis**: Compute ∆EA,B for 500 TextDifference samples across SBERT-mini and Gemma; compare Sim(∆EA,B, ED) vs. Sim(∆EA,B, EB); verify SBERT achieves >70% adherence (per Table 3) while LLMs achieve <65%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can imposing set-theoretic properties as constraints during training yield sentence embeddings that jointly optimize for both interpretability and downstream accuracy?
- Basis in paper: [explicit] The authors propose in the Discussion that "one follow-up work could be to impose these set-theoretic properties as constraints while training and fine-tuning LLMs" to bridge the gap between the high interpretability of classical encoders and the high benchmark performance of LLMs.
- Why unresolved: Current LLM embeddings achieve high accuracy but fail the proposed set-theoretic criteria, suggesting these are orthogonal optimization goals that have not yet been unified in a single training objective.
- What evidence would resolve it: A new model trained with set-theoretic regularization that scores highly on standard NLP benchmarks (accuracy) while simultaneously satisfying criteria C1–C6 better than standard LLMs.

### Open Question 2
- Question: Do the six proposed set-theoretic criteria scale effectively to document-level embeddings and long-context operations?
- Basis in paper: [explicit] The paper states in the Introduction: "We focus exclusively on sentence-level analysis, deferring document-level analysis as future work."
- Why unresolved: The compositional semantics of set operations may differ fundamentally in long documents where information is sparse and distributed, and the geometric properties observed in sentence embeddings may not hold at the document level.
- What evidence would resolve it: An extension of the synthetic dataset and evaluation framework to document pairs, showing whether the projection and similarity criteria (C1–C6) remain valid for encoders like Longformer or Doc2Vec.

### Open Question 3
- Question: Can non-linear distance metrics or learnable transformations reveal set-theoretic compositional properties in LLM embeddings that linear metrics (cosine/Euclidean) fail to capture?
- Basis in paper: [explicit] In the Limitations section, the authors note: "linear distance/similarity measures... may not be suitable for all kinds of embeddings as the relationships are embedded in a non-linear and complex manner."
- Why unresolved: The finding that LLMs fail the criteria relies entirely on linear geometry; the semantic relationships in high-dimensional LLM spaces might be structured non-linearly, hiding their true compositional capabilities.
- What evidence would resolve it: Re-evaluating the specific criteria (C1–C6) using supervised metric learning or manifold learning techniques to measure similarities, rather than raw cosine similarity.

## Limitations
- Synthetic data generation through GPT-3.5 may propagate LLM-specific biases and artifacts
- Linear similarity metrics may not capture non-linear semantic relationships in embedding spaces
- Results are limited to sentence-level compositionality without addressing document-level phenomena

## Confidence
- **High Confidence (8-10/10)**: Classical encoders (SBERT variants) consistently outperform LLMs on set-theoretic compositionality criteria; six proposed criteria successfully differentiate embedding behaviors; human validation confirms dataset quality
- **Medium Confidence (5-7/10)**: Geometric mechanisms (angular relationships, projections) accurately reflect semantic compositionality; training objective differences explain performance gap; algebraic vector operations approximate semantic differences
- **Low Confidence (1-4/10)**: Synthetic data captures all relevant compositional phenomena; results generalize to document-level compositionality; non-linear semantic relationships are not significant

## Next Checks
1. **Cross-dataset validation**: Evaluate the same criteria on naturally occurring sentence pairs from diverse sources (e.g., scientific abstracts, news articles) to verify synthetic data results generalize beyond GPT-3.5-generated samples.
2. **Fine-tuning experiment**: Fine-tune an LLM (e.g., Mistral) with NLI/contrastive objectives and re-evaluate set-theoretic criteria to directly test whether training objective differences explain the performance gap.
3. **Non-linear metric comparison**: Implement non-linear compositional metrics (e.g., MLP-based similarity, attention-weighted combinations) and compare results against the linear metrics used in the paper to assess whether geometric relationships capture the full compositional structure.