---
ver: rpa2
title: Using Large Language Models to Measure Symptom Severity in Patients At Risk
  for Schizophrenia
arxiv_id: '2508.10226'
source_url: https://arxiv.org/abs/2508.10226
tags:
- bprs
- transcripts
- scores
- data
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models (LLMs) can accurately
  predict Brief Psychiatric Rating Scale (BPRS) scores from clinical interview transcripts
  of patients at clinical high risk (CHR) for schizophrenia. Using a zero-shot approach
  without model fine-tuning, the LLM achieved a median concordance of 0.84 and ICC
  of 0.73 on PSYCHS transcripts, approaching human inter- and intra-rater reliability
  levels.
---

# Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia

## Quick Facts
- arXiv ID: 2508.10226
- Source URL: https://arxiv.org/abs/2508.10226
- Reference count: 0
- Zero-shot LLM achieved median concordance 0.84 and ICC 0.73 on structured psychiatric interviews

## Executive Summary
This study demonstrates that large language models can accurately predict Brief Psychiatric Rating Scale (BPRS) scores from clinical interview transcripts of patients at clinical high risk for schizophrenia. Using a zero-shot approach without model fine-tuning, the LLM achieved performance approaching human inter- and intra-rater reliability levels. The model showed particular accuracy with structured interviews and self-reported symptoms, and maintained comparable performance across multiple languages. Longitudinal data improved predictions through few-shot learning approaches, suggesting potential for standardized symptom assessment across treatment courses.

## Method Summary
The study used OpenAI's o3-mini reasoning model to predict 24-item BPRS scores from clinical interview transcripts of CHR patients in the AMP-SCZ cohort. The approach was zero-shot (no fine-tuning), with the complete BPRS manual and rating scale definitions provided in system prompts. The model processed 409 patients with both interview transcripts and BPRS assessments, including 348 English speakers and 61 foreign language speakers. Performance was evaluated using concordance (fraction of subscores within 1 point), ICC, Pearson correlation, and RMSE metrics, benchmarked against human reliability studies.

## Key Results
- LLM achieved median concordance of 0.84 and ICC of 0.73 on PSYCHS structured interviews
- Performance significantly better on semi-structured PSYCHS interviews versus unstructured open interviews
- Model maintained accuracy across languages with median concordance 0.88 and ICC 0.70
- Few-shot learning with longitudinal data improved predictions (RMSE 6.32 vs 7.19 for zero-shot)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLM inference can approximate human clinical raters on structured psychiatric assessments when given scale definitions.
- Mechanism: The transformer architecture captures contextual cues from interview transcripts and maps them to BPRS symptom domains via knowledge encoded during pre-training, without task-specific fine-tuning.
- Core assumption: The clinical constructs measured by BPRS have sufficient representation in the model's training corpus for accurate semantic mapping.
- Evidence anchors:
  - [abstract] "Using a zero-shot approach without model fine-tuning, the LLM achieved a median concordance of 0.84 and ICC of 0.73"
  - [Methods] "Instructions and background were placed in the system instructions section. Structured output was utilized"
  - [corpus] Related work (Galatzer-Levy et al.) shows similar LLM capability for depression measurement, but corpus lacks direct psychosis scale validation studies
- Break condition: Performance degrades significantly when scale definitions are omitted—paper notes LLM "could not reliably list the 24 symptoms" without the BPRS manual in the prompt.

### Mechanism 2
- Claim: Semi-structured interviews provide higher-fidelity signal for symptom extraction than unstructured conversations.
- Mechanism: PSYCHS interviews systematically elicit information overlapping with BPRS domains, reducing missing symptom data and improving the model's ability to infer severity from explicit patient statements.
- Core assumption: Self-reported symptoms are more reliably captured in text transcripts than clinician-observed behaviors requiring visual/auditory cues.
- Evidence anchors:
  - [Results] "Performance was better on semi-structured PSYCHS interviews compared to unstructured open interviews"
  - [Results] "Self-reported symptoms were typically more accurately predicted than observed symptoms, especially in PSYCHS interviews (p=0.0003)"
  - [corpus] Weak direct corpus support for this mechanism in psychosis; related work focuses on depression detection
- Break condition: Open interviews show systematic underestimation (mean predicted BPRS 28 vs. true 38) particularly for affective and positive symptoms.

### Mechanism 3
- Claim: In-context learning with longitudinal patient data improves prediction accuracy over zero-shot baselines.
- Mechanism: Providing paired previous transcript-score examples in the context window allows the model to calibrate to individual patient baselines and reporting styles.
- Core assumption: Symptom expression patterns and severity ranges are partially patient-specific, and grounding in prior assessments reduces variance.
- Evidence anchors:
  - [abstract] "Improved predictions when incorporating longitudinal data through one-shot or few-shot learning approaches"
  - [Results] "1-shot learning resulted in the most accurate predictions, with an RMSE of 6.32" vs. 7.19 for last-score control
  - [corpus] No corpus neighbors directly validate longitudinal in-context learning for psychiatric assessment
- Break condition: Adding previous transcripts alone (without scores) does not improve performance—scores are the critical calibration signal.

## Foundational Learning

- Concept: **Intraclass Correlation Coefficient (ICC)**
  - Why needed here: Primary metric for comparing LLM predictions to human inter-rater reliability; ICC(3,k) used for averaged repeated measures.
  - Quick check question: Why would ICC be preferred over simple Pearson correlation for this application?

- Concept: **Zero-shot vs. Few-shot Prompting**
  - Why needed here: The study explicitly compares zero-shot (no examples) against one-shot and two-shot learning with longitudinal data.
  - Quick check question: What component must be included in few-shot prompts for this task to improve accuracy?

- Concept: **Concordance in Clinical Ratings**
  - Why needed here: The study uses Hafkenscheid's concordance metric (fraction of subscores within 1 point) as a human benchmark.
  - Quick check question: Why is "within 1 point" a clinically meaningful tolerance for BPRS subscores?

## Architecture Onboarding

- Component map: Clinical transcript -> System prompt (BPRS manual + instructions) -> OpenAI o3-mini -> JSON output (24 subscores + ratings)

- Critical path:
  1. Include complete BPRS instruction manual in system prompt (required—model cannot list items without it)
  2. Specify structured JSON output format
  3. For longitudinal improvement, provide paired [previous_transcript, previous_scores] tuples

- Design tradeoffs:
  - Zero-shot: Lower setup cost, no patient history needed; accuracy ceiling limited
  - Few-shot: Requires longitudinal data; improves RMSE by ~12% (6.32 vs. 7.19)
  - Interview type: PSYCHS preferred (ICC 0.73) over open (ICC 0.42) but may not always be available

- Failure signatures:
  - Underestimation of affective symptoms (anxiety, depression) in open interviews
  - Overestimation of conceptual disorganization
  - Observed symptoms (blunted affect, tension) predicted less accurately than self-reported
  - Prompt sensitivity: "seemingly innocuous wording changes could strongly impact scoring"

- First 3 experiments:
  1. Validate zero-shot BPRS prediction on held-out English PSYCHS transcripts, measuring ICC and concordance against human benchmarks
  2. Test cross-language robustness by running identical English prompts on Spanish/Korean transcripts
  3. Compare 0-shot, 1-shot, and 2-shot predictions in patients with 3+ timepoints, isolating the contribution of previous scores vs. previous transcripts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs accurately assess CHR-specific instruments (SIPS, CAARMS, PSYCHS) that may be more sensitive for predicting psychosis conversion than the BPRS?
- Basis in paper: [explicit] "Consequently, we cannot yet determine whether LLMs can assess these disease-specific scales, which may be more sensitive for assessing the conversion to psychosis than the BPRS."
- Why unresolved: The PSYCHS instrument is too new for the LLM to list its component items, and AMP-SCZ data for SIPS/CAARMS was too incomplete at the time of analysis.
- What evidence would resolve it: Reapply the methodology to SIPS, CAARMS, or PSYCHS scores once more complete longitudinal data becomes available from AMP-SCZ.

### Open Question 2
- Question: Does LLM prediction accuracy continue to improve with additional longitudinal timepoints beyond three, or does it saturate?
- Basis in paper: [explicit] "Larger longitudinal sequences would be necessary to probe whether performance improvements saturate or continue to accrue with additional context."
- Why unresolved: Only 45 participants had three usable timepoints, and none had more than three in the current dataset.
- What evidence would resolve it: Evaluate n-shot learning performance in cohorts with denser longitudinal follow-up (4+ timepoints).

### Open Question 3
- Question: Can multimodal approaches combining video or audio data with transcripts improve detection of clinician-observed BPRS symptoms?
- Basis in paper: [explicit] "Future multimodal approaches that utilize video or direct audio data may have improved detection of these observed features."
- Why unresolved: The transcript medium preferentially captures verbal content but lacks information about patient movements, mannerisms, and affect observable on video.
- What evidence would resolve it: Compare LLM performance on transcripts alone versus transcripts plus synchronized audio/video recordings for observed symptom subscores.

### Open Question 4
- Question: Can LLM-derived symptom scores predict actual conversion to psychosis in CHR patients?
- Basis in paper: [inferred] The authors state "The eventual goal of these efforts would be to develop better risk stratification tools predicting the conversion of CHR patients," but the current study only validated against concurrent BPRS scores, not prospective outcomes.
- Why unresolved: This study focused on accuracy relative to human-rated BPRS, not on prognostic validity for psychosis conversion.
- What evidence would resolve it: Follow CHR patients longitudinally to assess whether LLM-predicted scores predict conversion status at 1-2 years.

## Limitations

- Strong dependence on exact prompt formulation, with "seemingly innocuous wording changes" potentially strongly impacting scoring
- Model's performance on observed symptoms (blunted affect, tension) significantly worse than on self-reported symptoms due to lack of visual/auditory cues in transcripts
- Cross-language performance demonstrated but without examination of cultural context adjustments or symptom expression variations

## Confidence

**High Confidence**:
- Zero-shot LLM inference can approximate human clinical raters on structured psychiatric assessments when given scale definitions
- Semi-structured interviews provide higher-fidelity signal for symptom extraction than unstructured conversations
- In-context learning with longitudinal patient data improves prediction accuracy over zero-shot baselines

**Medium Confidence**:
- LLM performance approaches human inter- and intra-rater reliability levels
- Self-reported symptoms are more reliably captured in text transcripts than clinician-observed behaviors
- Cross-language assessment is feasible with comparable accuracy

**Low Confidence**:
- The model can fully replace human clinical raters for BPRS assessment
- Performance will generalize to other structured psychiatric scales without modification
- The observed improvements from few-shot learning will persist with larger sample sizes

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary key components of the system prompt (BPRS manual wording, output format specifications, symptom definitions) across 10+ variants to quantify the impact on prediction accuracy and identify the most critical prompt elements.

2. **Clinical Ground Truth Validation**: Conduct a head-to-head comparison where the LLM and multiple human raters independently assess the same set of transcripts, then compare both absolute accuracy and inter-rater reliability metrics to establish true clinical equivalence.

3. **Longitudinal Performance Study**: Track model performance across extended treatment periods (6+ months) in a larger cohort to evaluate whether few-shot learning improvements persist, degrade, or improve over time, and whether the model can detect clinically meaningful symptom changes.