---
ver: rpa2
title: 'Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning'
arxiv_id: '2512.06250'
source_url: https://arxiv.org/abs/2512.06250
tags:
- learning
- maze
- exploration
- agent
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the challenge of determining when to switch
  between exploration and exploitation strategies in autonomous agents. The approach
  uses reinforcement learning to adaptively select switching thresholds between spiral
  exploration and A pathfinding policies for maze navigation.
---

# Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2512.06250
- **Source URL:** https://arxiv.org/abs/2512.06250
- **Reference count:** 27
- **Primary result:** 23-55% improvements in maze completion time using learned switching thresholds between exploration and exploitation strategies

## Executive Summary
This research introduces an adaptive policy selection framework that learns when to switch between spiral exploration and A* pathfinding for maze navigation. Using reinforcement learning with a compact 50-state representation, the agent dynamically selects coverage thresholds (20-60%) based on coverage percentage and distance-to-goal signals. Experiments across 240 test configurations demonstrate significant performance gains over single-strategy and fixed-threshold baselines, with improvements scaling with maze complexity. The approach achieves 83% reduction in runtime variance and 71% improvement in worst-case scenarios while generalizing to unseen wall configurations within each size class.

## Method Summary
The method employs tabular Q-learning with a discretized state space encoding coverage percentage (10 buckets) and distance-to-goal (5 buckets) into 50 states. The agent selects from five discrete threshold actions (20-60% coverage) using epsilon-greedy policy, switching from spiral exploration to A* pathfinding when the chosen threshold is reached. Q-values update every 50 steps based on a multi-component reward function combining step efficiency, coverage, and switch timing penalties. The agent learns adaptive thresholds intra-episode without requiring prior knowledge of maze structure, wall positions, or optimal threshold values.

## Key Results
- 23% improvement in completion time on 16×16 mazes compared to spiral-only baseline
- 55% improvement on 64×64 mazes with learned thresholds versus fixed 40% baseline
- 83% reduction in runtime variance across maze configurations

## Why This Works (Mechanism)

### Mechanism 1
Discretizing continuous progress signals into a compact state space enables efficient tabular Q-learning for threshold selection. Coverage percentage maps to 10 buckets and distance-to-goal to 5 normalized buckets, yielding 50 states. This coarse encoding preserves enough signal to differentiate maze structures while keeping the Q-table tractable at 250 floats. Core assumption: coverage rate and distance-to-goal trajectory jointly encode sufficient information about maze complexity to inform switching decisions.

### Mechanism 2
Reward shaping with multi-component objectives guides intra-episode learning when terminal rewards are sparse. The reward function combines step efficiency (penalizing excessive steps), coverage (rewarding thorough exploration), and switch timing (rewarding 30-50% thresholds, penalizing extremes). This provides intermediate feedback during each maze run. Core assumption: hand-crafted reward components correctly encode the desired trade-offs between exploration thoroughness and convergence speed.

### Mechanism 3
Intra-episode Q-learning enables threshold adaptation to the specific maze structure without pre-training or cross-episode transfer. Q-values update every 50 steps based on immediate feedback, allowing the agent to adjust its switching threshold dynamically as it discovers walls and corridors. Simple mazes trend toward lower thresholds (20-30%); complex mazes toward higher (50-60%). Core assumption: state trajectories during early exploration are predictive of overall maze complexity and optimal switching timing.

## Foundational Learning

- **Concept:** Q-learning (tabular)
  - Why needed here: The core algorithm for learning threshold selection; requires understanding state-action value updates, epsilon-greedy exploration, and the discount factor's role.
  - Quick check question: Given Q(s,a) = 100, α = 0.1, r = 5, γ = 0.9, max Q(s',a') = 80, compute the updated Q(s,a). (Answer: 100 + 0.1 × [5 + 0.9 × 80 − 100] = 97.7)

- **Concept:** Exploration-exploitation tradeoff
  - Why needed here: The switching problem itself is a meta-level exploration-exploitation decision; the agent must explore to discover maze structure before exploiting with A*.
  - Quick check question: Why might a fixed 40% threshold be suboptimal for both simple and complex mazes? (Answer: Simple mazes waste time exploring; complex mazes converge prematurely to suboptimal paths.)

- **Concept:** Reward shaping
  - Why needed here: The multi-component reward function is critical for guiding learning; understanding potential pitfalls (e.g., reward hacking) informs debugging.
  - Quick check question: If R_switching only rewarded 40% thresholds, what failure mode might emerge? (Answer: The agent might force switching at 40% regardless of maze structure, eliminating adaptive benefit.)

## Architecture Onboarding

- **Component map:** State encoder → Q-table → Policy selector → Navigation executor → Coverage/distance tracking → Q-table update (every 50 steps)
- **Critical path:** State encoder → Q-table lookup → Policy selector → Navigation executor → Coverage/distance tracking → Q-table update (every 50 steps). The switching decision is the control point; incorrect thresholds cascade into either excessive exploration or premature convergence.
- **Design tradeoffs:** Discrete vs. continuous thresholds: Discrete (5 values) simplifies learning but limits granularity; continuous would require function approximation (future work per Section VII.A.1). Intra-episode vs. cross-episode learning: Current design resets Q-tables each run; persistence could enable transfer but risks overfitting to specific wall configurations.
- **Failure signatures:** High variance in completion times across similar mazes → Q-learning not converging intra-episode; check learning rate and exploration schedule. Consistently early switching (≤20%) on complex mazes → Reward function may overweight step efficiency; review R_switching penalties. Incomplete runs at 128×128 → Time limits or resource constraints; note 86.7% completion rate suggests scaling limits. Identical performance to fixed-threshold baseline → Q-table not being updated or state representation is uninformative; verify update logic and bucket boundaries.
- **First 3 experiments:** Replicate 16×16 comparison (Spiral vs. Spiral Conv vs. Spiral RL) to validate 23% improvement claim; log Q-table evolution across 10 unique mazes to confirm adaptive threshold selection. Ablate reward components: Run Spiral RL with only R_steps, only R_coverage, only R_switching to isolate which component drives the 55% improvement on 64×64 mazes. Test generalization: Train on 5 of 10 mazes per size, evaluate on held-out 5 to verify the claim that "learned switching behavior generalizes within each size class to unseen wall configurations."

## Open Questions the Paper Calls Out

- **Can replacing the discrete threshold action space with a continuous one yield finer-grained adaptation and improved performance?**
  - Basis in paper: [explicit] The authors state in Limitations that "continuous actions may improve performance" and propose using policy gradient methods (PPO, SAC) in Future Directions.
  - Why unresolved: The current architecture restricts the agent to five discrete thresholds (20-60%), potentially missing optimal intermediate switching points.
  - What evidence would resolve it: A comparative study measuring completion times of agents using continuous action policies versus the discrete Q-learning agent on identical maze configurations.

- **Does persisting Q-tables across episodes enable effective transfer learning from smaller to larger maze environments?**
  - Basis in paper: [explicit] Future Directions proposes "implementing Q-table persistence" to test if knowledge transfers "from small mazes to larger environments."
  - Why unresolved: The current implementation resets the Q-table for every episode, isolating learning to a single run and preventing the accumulation of strategic knowledge.
  - What evidence would resolve it: Experiments initializing agents with Q-tables trained on 16×16 mazes and measuring convergence speed and final performance on 128×128 mazes.

- **Does the adaptive switching mechanism generalize to non-navigation domains with exploration-exploitation trade-offs?**
  - Basis in paper: [explicit] The paper lists "Domain generalization" as a future direction, specifically suggesting "robotic navigation" and "search problems" as potential testbeds.
  - Why unresolved: The current base policies (spiral exploration, A*) and state features are maze-specific; it is unknown if the switching logic functions effectively with different state representations.
  - What evidence would resolve it: Applying the adaptive threshold framework to a non-maze task (e.g., algorithm selection in search or resource allocation) using domain-specific metrics.

## Limitations

- The assumption that coverage percentage and distance-to-goal trajectories jointly encode sufficient information about maze complexity lacks theoretical guarantees.
- Maze generation specifics (wall density, connectivity) remain underspecified, limiting reproducibility of experimental results.
- The 86.7% completion rate at 128×128 suggests scalability limits that were not fully characterized.

## Confidence

- **High confidence** (4/5): The 23-55% improvement claims are well-supported by structured experimental comparisons across multiple maze sizes.
- **Medium confidence** (3/5): The 83% reduction in runtime variance claim is supported but relies on variance metrics that weren't fully specified in the methodology section.
- **Low confidence** (2/5): The 71% improvement in worst-case scenarios claim lacks the distribution analysis needed to verify it's not driven by outlier runs.

## Next Checks

1. Run ablation studies removing each reward component (R_steps, R_coverage, R_switching) to isolate which drives the 55% improvement on complex mazes.
2. Test generalization by training on 5 of 10 mazes per size class and evaluating on held-out configurations to verify learned thresholds transfer to unseen wall layouts.
3. Analyze Q-table evolution across 10 unique mazes per size to confirm adaptive threshold selection patterns match the claimed trend (simple → 20-30%, complex → 50-60%).