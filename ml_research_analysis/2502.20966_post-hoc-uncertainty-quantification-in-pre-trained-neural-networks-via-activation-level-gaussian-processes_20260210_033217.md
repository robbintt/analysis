---
ver: rpa2
title: Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level
  Gaussian Processes
arxiv_id: '2502.20966'
source_url: https://arxiv.org/abs/2502.20966
tags:
- uncertainty
- neural
- network
- pre-trained
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Gaussian Process Activation functions (GAPA)
  to quantify uncertainty in pre-trained neural networks by focusing on activation-level
  rather than weight-level uncertainty. GAPA attaches a one-dimensional Gaussian Process
  to each neuron's activation, preserving the original network predictions while providing
  principled uncertainty estimates.
---

# Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian Processes

## Quick Facts
- arXiv ID: 2502.20966
- Source URL: https://arxiv.org/abs/2502.20966
- Reference count: 10
- Primary result: GAPA-Variational achieves superior uncertainty quantification compared to Last-Layer Laplace on regression tasks while preserving original network predictions

## Executive Summary
This paper introduces Gaussian Process Activation functions (GAPA) as a post-hoc method for quantifying uncertainty in pre-trained neural networks. The key innovation is shifting from weight-space to activation-space uncertainty modeling by attaching one-dimensional Gaussian Processes to each neuron's activation. This preserves the original network's deterministic predictions while providing principled uncertainty estimates. Two variants are proposed: GAPA-Free uses empirical kernel methods for efficient training, while GAPA-Variational learns kernel hyperparameters via variational inference for greater flexibility.

## Method Summary
GAPA attaches a Gaussian Process to each neuron in the first hidden layer, setting the GP prior mean equal to the neuron's original activation function. This ensures the posterior mean matches the pre-trained network's output. Uncertainty is propagated through subsequent layers using delta approximation methods - linear transformations use standard variance propagation rules while non-linearities are linearized. GAPA-Free uses empirical kernel scaling with linear calibration, while GAPA-Variational learns kernel hyperparameters and variational covariance by optimizing Gaussian Negative Log-Likelihood with backpropagation. The method requires no retraining of the base network, making it practical for large-scale applications.

## Key Results
- GAPA-Variational outperforms Last-Layer Laplace Approximation across all metrics (NLL, CRPS, CQM) on regression tasks
- Superior performance on Taxi dataset with best results across all evaluation metrics
- GAPA-Free provides efficient alternative with good performance when computational resources are limited
- Method preserves original network predictions while adding uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Activation-level uncertainty allows post-hoc modification without altering deterministic predictions
- **Mechanism:** GP prior mean set to original activation ensures posterior mean matches pre-trained output
- **Core assumption:** Deterministic activation serves as sufficient prior mean; epistemic uncertainty captured by variance
- **Evidence anchors:** Abstract states "preserving the original mean predictions"; Section 2.2 shows μ_d(X_d) = m_d(X_d) = a₁(X_d)
- **Break condition:** Extreme non-stationarity where GP kernel cannot capture activation-input relationship

### Mechanism 2
- **Claim:** Gaussian uncertainty propagates through deep non-linear layers via deterministic approximations
- **Mechanism:** Linear layers use variance scaling (Σ_z = WΣ_aW^⊤); non-linearities use first-order Taylor expansion
- **Core assumption:** First-order linearization accurately describes probability distribution transformation
- **Evidence anchors:** Section 2.3 details delta approximation rules; linearization logic similar to linearized Laplace
- **Break condition:** Large input variance relative to non-linearity curvature causing linearization failure

### Mechanism 3
- **Claim:** Variational inference enables flexible uncertainty adaptation to data manifold
- **Mechanism:** Optimizes variational lower bound with inducing points; learns kernel hyperparameters via NLL minimization
- **Core assumption:** RBF kernel with optimized length-scales captures activation space uncertainty geometry
- **Evidence anchors:** Abstract notes "greater flexibility" of GAPA-Variational; Section 2.5 describes variational learning
- **Break condition:** Poor hyperparameter initialization or insufficient inducing points leading to uninformative estimates

## Foundational Learning

- **Concept: Gaussian Process (GP) Priors & Kernels**
  - **Why needed here:** Understanding RBF kernel "similarity" in activation space is crucial for interpreting OOD uncertainty
  - **Quick check question:** If two inputs produce similar pre-activations, should their GP uncertainty estimates be similar? (Answer: Yes, due to kernel covariance)

- **Concept: The Delta Method (Linearization)**
  - **Why needed here:** The paper relies on linearizing non-linearities for variance propagation
  - **Quick check question:** If Tanh activation is saturated (gradient ≈ 0), what happens to propagated variance? (Answer: It's squashed to near zero)

- **Concept: Variational Inference with Inducing Points**
  - **Why needed here:** GAPA-Variational uses inducing points to scale GP inference
  - **Quick check question:** Why use inducing points rather than full dataset for GP? (Answer: To reduce O(N³) computational complexity)

## Architecture Onboarding

- **Component map:** Pre-trained backbone -> GAPA Layer (GP + original activation) -> Variance propagation wrapper -> Final output
- **Critical path:** 1) Forward input to Layer 1; 2) Compute GP posterior variance; 3) Propagate (μ,Σ) through remaining layers; 4) Optimize GAPA-Variational hyperparameters via NLL minimization
- **Design tradeoffs:** GAPA-Free vs GAPA-Variational (speed vs. flexibility); First layer placement vs deeper layers (simpler vs potentially more uncertainty capture)
- **Failure signatures:** Constant uncertainty (poor length-scale), variance explosion/collapse (bad initialization), linearization breakdown (large OOD variance)
- **First 3 experiments:** 1) Sanity check mean preservation on toy dataset; 2) Replicate Taxi dataset NLL/CRPS comparison with LLA; 3) Ablate layer depth (last layer only vs first layer)

## Open Questions the Paper Calls Out
- Can GAPA be extended to classification tasks while preserving advantages over Laplace approximations?
- How can inference-time computational cost be reduced for large-scale applications?
- Does layer choice affect uncertainty quality, and is first layer optimal?
- How does delta approximation degrade with network depth and non-linearity strength?

## Limitations
- Empirical success primarily demonstrated on structured regression datasets with feed-forward networks
- Reliance on first-order delta approximation may break down for deep networks with highly non-linear activations
- Validation against Last-Layer Laplace only, lacking comparison to more recent deep uncertainty methods

## Confidence
- **High Confidence:** Mean preservation mechanism and basic variance propagation rules are mathematically sound
- **Medium Confidence:** Experimental improvements over Last-Layer Laplace on tested datasets
- **Low Confidence:** "Superior across all metrics" claim lacks broader method comparison

## Next Checks
1. **Ablation Study:** Apply GAPA to different layers (last vs first) to quantify value of propagating uncertainty through depth
2. **Architecture Stress Test:** Apply GAPA to simple CNN on CIFAR-10 regression to test robustness to non-feedforward architectures
3. **OOD Boundary Test:** Systematically evaluate uncertainty calibration on inputs with increasing distance from training manifold