---
ver: rpa2
title: 'TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel
  Planning'
arxiv_id: '2512.22673'
source_url: https://arxiv.org/abs/2512.22673
tags:
- tool
- user
- information
- tools
- whether
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TravelBench is a new benchmark for evaluating LLM agents on real-world
  travel planning tasks. It covers multi-turn interactions, tool use, and unsolvable
  cases by using real user queries, profiles, and a sandbox environment with cached
  tool outputs.
---

# TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning

## Quick Facts
- arXiv ID: 2512.22673
- Source URL: https://arxiv.org/abs/2512.22673
- Reference count: 40
- Primary result: Even the strongest models score around 75 points, indicating significant room for improvement in handling realistic travel scenarios.

## Executive Summary
TravelBench is a new benchmark designed to evaluate large language model (LLM) agents on real-world travel planning tasks. It addresses the gap in existing benchmarks by focusing on multi-turn interactions, tool use, and the ability to handle unsolvable cases. The benchmark uses real user queries and profiles, along with a sandbox environment that includes cached tool outputs, to create a realistic and reproducible evaluation setting. Three task subsets assess different capabilities: autonomous task completion, asking clarifying questions, and recognizing unsolvable cases. The evaluation uses an LLM-as-judge scoring system with tool-use penalties and meta-evaluation for stability.

## Method Summary
TravelBench is built around a sandboxed travel-planning environment that uses cached tool outputs to ensure reproducibility. It leverages real user queries and profiles to create realistic scenarios, and divides tasks into three subsets to evaluate different aspects of agent performance. The evaluation framework employs LLM-as-a-judge scoring, supplemented by meta-evaluation to assess judge consistency. Tool-use penalties discourage excessive API calls, and the benchmark is designed to be stable and reproducible across different models and settings.

## Key Results
- The benchmark includes real user queries and profiles, ensuring realistic evaluation.
- Three task subsets assess autonomous task completion, clarifying questions, and recognizing unsolvable cases.
- Even the strongest models achieve scores around 75 points, highlighting significant room for improvement.

## Why This Works (Mechanism)
TravelBench works by simulating realistic travel planning scenarios using cached tool outputs and real user data. The multi-turn interaction design reflects the complexity of actual travel planning, while the tool-augmented environment tests the agent's ability to leverage external APIs. The sandbox setup ensures reproducibility, and the three task subsets allow for targeted evaluation of different capabilities. The LLM-as-judge scoring system, combined with meta-evaluation, provides a structured way to assess performance while accounting for judge consistency.

## Foundational Learning
- **Multi-turn interactions**: Why needed - Travel planning often involves back-and-forth communication. Quick check - Can the agent maintain context across multiple turns?
- **Tool use**: Why needed - Real-world planning requires accessing external data (flights, hotels). Quick check - Does the agent effectively use APIs to gather information?
- **Recognizing unsolvable cases**: Why needed - Not all travel requests can be fulfilled. Quick check - Can the agent identify and communicate when a request cannot be completed?
- **LLM-as-judge scoring**: Why needed - Provides a scalable and consistent evaluation method. Quick check - Does the judge scoring align with human expectations?
- **Meta-evaluation for stability**: Why needed - Ensures reliability of judge assessments. Quick check - Are scores consistent across different judges or runs?

## Architecture Onboarding

**Component Map:**
User Queries -> Agent Planner -> Tool Sandbox (Cached Outputs) -> LLM-as-Judge -> Score

**Critical Path:**
User query → Agent planning → Tool API calls (via sandbox) → Response generation → Judge evaluation → Scoring

**Design Tradeoffs:**
- **Sandbox vs. live data**: Cached outputs ensure reproducibility but may miss real-time changes.
- **LLM-as-judge vs. human evaluation**: Scalable but may introduce bias; mitigated by meta-evaluation.
- **Tool-use penalties**: Discourages excessive API calls but may not reflect real-world utility.

**Failure Signatures:**
- Poor context retention across turns.
- Over-reliance on tools or failure to use them when needed.
- Inability to recognize unsolvable cases.
- Inconsistent scoring due to judge variability.

**First Experiments:**
1. Test agent performance on single-turn vs. multi-turn tasks to isolate context retention issues.
2. Vary tool output availability (e.g., missing flight data) to assess robustness.
3. Compare LLM-as-judge scores with human expert evaluations for edge cases.

## Open Questions the Paper Calls Out
None

## Limitations
- The sandboxed environment may not fully capture real-time changes in travel information.
- LLM-as-a-judge scoring introduces potential biases and variability.
- The benchmark's focus on travel planning may limit generalizability to other domains.
- A maximum score of 75 points suggests the benchmark may be overly challenging for tracking incremental improvements.

## Confidence

**High confidence:**
- Benchmark design methodology is well-documented and reproducible.
- Experimental setup with multiple models is clearly presented.

**Medium confidence:**
- LLM-as-a-judge scoring is validated through meta-evaluation but may still be subjective.
- Tool-use penalty system may not perfectly reflect real-world utility.

**Low confidence:**
- Benchmark's ability to predict real-world performance in live scenarios is uncertain.

## Next Checks
1. Conduct a human evaluation study where travel experts score the same agent responses to assess alignment between LLM-as-judge scores and human expert judgments, particularly for edge cases and subjective preferences.
2. Test the benchmark's stability by introducing controlled variations in tool outputs (e.g., different flight prices or hotel availability) to measure how sensitive agent performance is to real-time information changes.
3. Expand the benchmark to include cross-domain planning tasks (e.g., event planning or moving logistics) to evaluate whether the travel-specific design limits generalizability to other planning scenarios.