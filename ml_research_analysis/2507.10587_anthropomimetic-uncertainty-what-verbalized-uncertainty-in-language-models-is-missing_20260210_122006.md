---
ver: rpa2
title: 'Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models
  is Missing'
arxiv_id: '2507.10587'
source_url: https://arxiv.org/abs/2507.10587
tags:
- uncertainty
- language
- arxiv
- expressions
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how verbalized uncertainty in language models
  (LLMs) falls short of human standards, identifying key gaps in fluency, personalization,
  and consistency. Through empirical analyses, the authors show that LLMs' uncertainty
  expressions are heavily influenced by conversational context, subject domain, and
  data biases, leading to miscalibration and over-reliance on numerical formats.
---

# Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing

## Quick Facts
- **arXiv ID:** 2507.10587
- **Source URL:** https://arxiv.org/abs/2507.10587
- **Reference count:** 40
- **Primary result:** LLMs' verbalized uncertainty is miscalibrated, biased by context and domain, and lacks personalization and fluency needed for human-like communication.

## Executive Summary
This paper identifies critical shortcomings in how language models verbalize uncertainty, showing that current approaches fail to meet human standards for consistency, personalization, and fluency. Through empirical analysis across multiple models, domains, and conversational contexts, the authors demonstrate that LLM uncertainty expressions are heavily influenced by training data biases and contextual factors, leading to miscalibration and over-reliance on numerical formats. They propose "anthropomimetic uncertainty" as a framework for developing more natural, consistent, and personalized uncertainty communication that better aligns with human expectations and needs.

## Method Summary
The study evaluates verbalized uncertainty across multiple commercial LLMs (Claude 3.5 Haiku, Command R+, GPT-4o mini, DeepSeek Chat, Gemini 2.0 flash, Mistral Medium) using 250 TriviaQA questions and 100 questions each from 9 MMLU subjects. Models were prompted with specific persona-based system prompts across 8 conversational contexts. Uncertainty expressions were extracted using regex patterns from training corpora and responses, with calibration measured via Expected Calibration Error (ECE) and linguistic patterns analyzed using Yule's Y coefficient. Correctness was determined through a three-stage heuristic (exact match → ROUGE-L → LLM-as-judge).

## Key Results
- LLMs show systematic miscalibration in numerical confidence expressions across domains, with ECE varying significantly by subject matter.
- Conversational context strongly influences verbal uncertainty expressions, with models adopting different uncertainty registers based on persona and relationship dynamics.
- Training data analysis reveals strong biases toward numerical uncertainty expressions and certain epistemic markers, particularly in English-language corpora.

## Why This Works (Mechanism)
The paper demonstrates that LLMs learn uncertainty patterns from training data rather than developing intrinsic understanding, leading to context-dependent and biased expressions. The mechanism works because models replicate frequency distributions of uncertainty markers from their training corpora, which are themselves biased toward certain linguistic patterns and numerical formats. This creates a cascading effect where contextual prompts and domain-specific language further skew uncertainty expression, resulting in miscalibration and inconsistency.

## Foundational Learning
- **Expected Calibration Error (ECE):** Measures the discrepancy between predicted confidence and actual accuracy - needed to quantify numerical miscalibration; quick check: compute ECE across binned confidence levels.
- **Yule's Y coefficient:** Statistical measure of colligation between linguistic markers and outcomes - needed to identify systematic relationships between uncertainty expressions and correctness; quick check: calculate correlation between strengtheners/weakeners and answer accuracy.
- **Regex-based marker extraction:** Pattern matching for epistemic markers and numerical terms - needed to systematically identify uncertainty expressions in text; quick check: validate regex matches against manual annotation of sample responses.
- **Three-stage correctness heuristic:** Hierarchical approach to establishing answer accuracy - needed to handle varying response formats and uncertainty expressions; quick check: test on responses with mixed certainty levels.

## Architecture Onboarding
- **Component map:** Training corpora → Model weights → Prompt context → Response generation → Uncertainty parsing → Calibration analysis
- **Critical path:** Prompt construction → API query → Response parsing → Correctness determination → Metric computation
- **Design tradeoffs:** Trade numerical precision for linguistic fluency vs. maintaining calibration accuracy; balance computational efficiency of regex extraction against potential ambiguity.
- **Failure signatures:** Over-reliance on numerical formats indicates training data bias; inconsistent verbal expressions across contexts suggest poor generalization; high ECE reveals systematic miscalibration.
- **First experiments:**
  1. Replicate regex frequency analysis on sample training corpora to verify marker distributions.
  2. Test three-stage correctness heuristic on responses with mixed certainty levels.
  3. Compare ECE across different temperature settings to assess parameter sensitivity.

## Open Questions the Paper Calls Out
- **Personalization:** How can verbalized uncertainty be personalized to align with specific user traits, such as risk tolerance or numeracy skills? Current models lack mechanisms to infer individual user profiles regarding risk perception.
- **Multilinguality:** How does bias and calibration of verbalized uncertainty vary across different languages and cultural contexts? Most research focuses on English, but uncertainty expression varies greatly across languages.
- **Linguistic Calibration:** Can evaluation frameworks move beyond probabilistic calibration to assess the "linguistic calibration" of fluent uncertainty expressions? Current metrics destroy nuance by mapping words to numbers.

## Limitations
- Commercial API calls used without fixed generation parameters, introducing uncontrolled variability across model responses.
- Small sample size (100 questions per domain) may not capture full distribution of uncertainty patterns across subjects.
- Regex-based detection of uncertainty markers faces significant ambiguity issues with common auxiliary verbs.

## Confidence
- **High Confidence:** LLMs show systematic miscalibration in numerical confidence expressions across multiple models and domains.
- **Medium Confidence:** Conversational context significantly influences verbal uncertainty expressions, though larger samples would strengthen this claim.
- **Medium Confidence:** Data biases in uncertainty markers from training corpora are methodologically sound but may overstate impact without accounting for fine-tuning effects.

## Next Checks
1. **Parameter Sensitivity Analysis:** Re-run contextual evaluation with fixed generation parameters (temperature=0.7, top_p=1.0, max_tokens=2048) across all models to isolate hyperparameter effects.
2. **Statistical Significance Testing:** Apply bootstrapping with 1000 resamples to calculate 95% confidence intervals for ECE and Yule's Y across domains, conducting pairwise significance tests.
3. **Manual Annotation Validation:** Conduct human annotation of 100 random responses to independently verify the three-stage correctness heuristic and assess regex accuracy for ambiguous cases.