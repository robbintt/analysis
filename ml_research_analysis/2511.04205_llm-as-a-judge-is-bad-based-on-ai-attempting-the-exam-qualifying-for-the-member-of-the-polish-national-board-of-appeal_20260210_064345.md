---
ver: rpa2
title: LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member
  of the Polish National Board of Appeal
arxiv_id: '2511.04205'
source_url: https://arxiv.org/abs/2511.04205
tags:
- legal
- points
- exam
- were
- appeal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates whether large language models (LLMs) can pass\
  \ Poland\u2019s National Appeal Chamber exam and whether LLM-as-a-judge is reliable.\
  \ Models including GPT-4.1, Claude 4 Sonnet, and Bielik-11B-v2.6 were tested on\
  \ a knowledge test and a written judgment task using retrieval-augmented generation\
  \ pipelines."
---

# LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal

## Quick Facts
- arXiv ID: 2511.04205
- Source URL: https://arxiv.org/abs/2511.04205
- Reference count: 40
- Primary result: Models scored 70-74% on multiple-choice knowledge tests but failed written judgments (0-37/100), with LLM-as-a-judge evaluators overscoring by 40-50 points and missing critical legal errors.

## Executive Summary
This study evaluates whether large language models can pass Poland's National Appeal Chamber (Krajowa Izba Odwoławcza) qualifying exam and whether LLM-as-a-judge is reliable for legal evaluation. Three models—GPT-4.1, Claude 4 Sonnet, and Bielik-11B-v2.6—were tested on a knowledge test and written judgment task using retrieval-augmented generation pipelines. While models achieved 70-88% accuracy on multiple-choice questions, none passed the written judgment, with GPT-4.1 scoring 37/100, Claude 30, and Bielik 8. Critically, LLM-as-a-judge evaluators consistently overscored submissions by 40-50 points, missing fundamental legal errors like hallucinated citations. The study concludes that LLMs cannot replace human judges and that automated evaluation is unreliable without rigorous human oversight.

## Method Summary
The study tested three LLMs on Poland's National Appeal Chamber exam consisting of 50 multiple-choice questions and a written legal judgment task. Models were evaluated under closed-book, basic RAG, and advanced RAG conditions using hybrid retrieval (semantic + keyword) with Typesense vector store and sdadas/mmlw-retrieval-roberta-large-v2 embeddings. Formal requirements were extracted using structured JSON schemas via n8n workflows, while substantive legal reasoning was assessed by human expert committees. LLM-as-a-judge evaluation was performed using GPT-4o with an 8-criterion rubric. The study also analyzed citation accuracy, hallucination rates, and evaluator calibration between automated and human scoring.

## Key Results
- Knowledge test performance: 70-88% accuracy across models, with GPT-4.1 achieving 88.4% using advanced RAG
- Written judgment failure: All models scored below 40/100 (GPT-4.1: 37, Claude: 30, Bielik: 8), with zero points for legal provision use
- LLM-as-a-judge inflation: Automated evaluators scored submissions 85-90/100 versus human scores of 8-37/100
- Formal extraction excellence: >97% F1 accuracy on binary formal requirements, but zero substantive legal reasoning points
- Citation errors: Models cited non-existent articles and misapplied valid provisions throughout generated judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves knowledge test performance, but gains depend on pipeline sophistication and model compatibility.
- Mechanism: External legal documents are chunked, vectorized, and retrieved via hybrid search (semantic + keyword). Retrieved context grounds model answers, reducing reliance on parametric memory.
- Core assumption: The embedding model captures legal semantics, and retrieval surfaces jurisdictionally relevant provisions.
- Evidence anchors:
  - [abstract] "Three models... scored 70-74% on the knowledge test, with GPT-4.1 improving to 88.4% using advanced RAG."
  - [section 6.2, Table 1] GPT-4.1: closed-book 77.9% → advanced RAG 88.4% (+10.5pp); Claude: 69.5% → 85.1% (+15.6pp) on prior exams. On 2025 exam, Claude degraded with RAG (54% with answers in query vs. 64% closed-book).
  - [corpus] Limited direct corroboration; corpus papers focus on benchmarks and grading, not RAG-specific legal gains.
- Break condition: If retrieval returns semantically similar but legally irrelevant provisions (wrong statute, different jurisdiction), grounding fails. Paper notes this caused "hallucinations and improper citations."

### Mechanism 2
- Claim: LLMs excel at structured extraction of formal legal requirements but fail at substantive legal reasoning.
- Mechanism: Formal extraction is framed as schema-constrained JSON output (e.g., dates, binary flags, fee amounts). These are bounded, verifiable tasks. Substantive reasoning requires multi-step statutory interpretation, precedent synthesis, and logical argumentation—unbounded and context-dependent.
- Core assumption: Formal criteria are explicit in case text and map cleanly to schema fields.
- Evidence anchors:
  - [abstract] "Formal requirements extraction was highly accurate (>97% F1 for GPT-4.1), but substantive legal reasoning was weak, with all models receiving zero points for legal provision use."
  - [section 6.3, Table 2-3] GPT-4.1 achieved 98.6% accuracy on binary formal assessments; 100% on appealDate, publDate, unitName.
  - [section 6.4.2-4] All models scored 0/30 on "Legal Provisions" criterion; committee cited "non-existent articles," "misapplied provisions," "invented problems."
  - [corpus] Related work (Brazil OAB, US bar exams) confirms pattern: models pass multiple-choice but fail practical/essay sections requiring reasoning.
- Break condition: When formal criteria require implicit legal judgment (e.g., whether an allegation is "decidable" given missing facts), extraction degrades.

### Mechanism 3
- Claim: LLM-as-a-judge overvalues surface-level style and cannot reliably assess substantive legal quality.
- Mechanism: The evaluator LLM (GPT-4o) assesses generated judgments against a rubric. It assigns high scores for linguistic fluency, document structure, and apparent professionalism, but lacks the legal knowledge to detect citation errors, logical inconsistencies, or misapplied norms.
- Core assumption: The evaluator LLM can parse legal reasoning quality if given the rubric and reference answers.
- Evidence anchors:
  - [abstract] "An LLM-as-a-judge evaluator rated outputs more leniently than human experts, revealing over-reliance on style over substance."
  - [section 6.5, Table 5-6] GPT-4o gave GPT-4.1: 88/100 vs. committee 37/100; Claude: 90/100 vs. 30/100; Bielik: 85/100 vs. 8/100. LLM gave all models 28/30 on "Legal Provisions"; committee gave 0/30 universally.
  - [section 7] "The automated examiner consistently overvalued stylistic conformity and verbosity, occasionally ranking fatally flawed responses as 'near-passing.'"
  - [corpus] G-Eval [12] notes "biases of LLM judges toward LLM-generated text." Limited domain-specific corroboration for legal evaluation.
- Break condition: When evaluated texts contain plausible-sounding but legally incorrect reasoning, the evaluator cannot detect the error without domain expertise.

## Foundational Learning

- Concept: Hybrid retrieval (dense + sparse)
  - Why needed here: The paper's advanced RAG uses semantic vectors (sdadas/mmlw-retrieval-roberta-large) combined with keyword matching via Typesense. Legal queries need both conceptual matching and exact term precision (statute references).
  - Quick check question: Given a query about "appeal deadline calculation," would pure semantic search retrieve the correct statutory article if the query uses different phrasing than the law?

- Concept: Structured output extraction with JSON schemas
  - Why needed here: Formal assessment uses zero-shot LLM prompting with JSON schema constraints to extract dates, fees, binary flags. The n8n pipeline uses "Auto-fixing Output Parser" with gpt-4o to correct formatting errors.
  - Quick check question: If an LLM outputs "3 maja 2025" for a date field expecting "YYYY-MM-DD," what component handles normalization?

- Concept: Evaluation calibration between automated and human assessors
  - Why needed here: The core finding is that LLM-as-a-judge scores (85-90) diverge wildly from human expert scores (8-37). Understanding this gap is essential before deploying automated evaluation.
  - Quick check question: If an LLM evaluator gives 90/100 to a legally flawed document, is the problem the rubric, the evaluator, or the evaluation paradigm?

## Architecture Onboarding

- Component map:
  Data ingestion (sentences, summaries, PZP collections) -> Preprocessing (256-token chunks, deduplication, Polish date parsing) -> Hybrid search (Typesense with sdadas/mmlw-retrieval-roberta-large-v2 embeddings + keyword matching) -> Formal extraction pipeline (n8n with JSON schema constraints and GPT-4o error correction) -> Substantive assessment (allegation extraction, characterization, judgment generation) -> LLM-as-a-judge evaluation (GPT-4o with 8-criterion rubric)

- Critical path:
  1. Knowledge test: Query → hybrid search → context + question → model → answer extraction (regex `\b([abc])\b`)
  2. Written exam: Case facts → formal extraction (JSON schema) → substantive allegation analysis → judgment generation
  3. Evaluation: Generated judgment → GPT-4o rubric scoring → comparison with human committee scores

- Design tradeoffs:
  - **Chunk size (256 tokens)**: Fits embedding model constraints but may fragment legal provisions across chunks. Paper notes semantically similar but contextually unrelated provisions were sometimes retrieved.
  - **Including case law vs. statutes only**: Preliminary tests suggested case law helped, but committee noted exam questions were statute-based; case law may have introduced noise.
  - **Zero-shot extraction vs. fine-tuning**: Deliberately avoided fine-tuning due to cost; accepted higher error rates in structured output.
  - **Single-pass submission**: To match exam constraints, no iteration or correction allowed; tradeoff is lower scores but higher ecological validity.

- Failure signatures:
  - **Hallucinated provisions**: Models cited non-existent articles (e.g., "Article 514(4) PZP") or misapplied valid ones.
  - **Structural errors**: Incorrect adjudicating panel composition (3-member vs. 1-member), missing signatures/dates.
  - **LLM-as-a-judge inflation**: Consistent 40-50 point overestimation; inability to distinguish Bielik (8 pts) from GPT-4.1 (37 pts)—only 5-point spread in LLM scores vs. 29-point spread in human scores.
  - **Claude RAG degradation**: Claude performed worse with basic RAG than closed-book on 2025 exam (54% vs. 64%), suggesting retrieval-reasoning mismatch.

- First 3 experiments:
  1. **Reproduce knowledge test with ablated retrieval**: Run closed-book vs. basic RAG vs. advanced RAG on a held-out set of 50 questions. Measure accuracy delta per model. Hypothesis: GPT-4.1 gains most from advanced RAG; Claude may show degradation with noisy context.
  2. **Validate formal extraction on new cases**: Apply the JSON schema extraction pipeline to 20 KIO cases not in the validation set. Manually verify each field. Hypothesis: >95% F1 on date/binary fields; lower on complex fields like `expectedFee` (96%) or `allegationsDismissed` (89.8%).
  3. **Calibrate LLM-as-a-judge with adversarial examples**: Feed GPT-4o evaluator deliberately flawed judgments (correct style, incorrect law) vs. correct judgments. Measure score differentiation. Hypothesis: Evaluator will not reliably distinguish; scores will cluster near 85-90 regardless of legal correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning on Polish legal judgments enable LLMs to pass the NAC written examination threshold of 50 points?
- Basis in paper: [explicit] Section 8.1 states: "One of the most important areas of research is fine-tuning LLMs to write rulings or judgments (supervised fine-tuning). The training dataset for each training sample should contain the content of the case, fragments of regulations, rulings, and judgments."
- Why unresolved: No evaluated model was fine-tuned for Polish legal reasoning. All models scored below 40/100 on the written judgment task, with zero points for legal provision use.
- What evidence would resolve it: Fine-tune an LLM on historical NAC judgments and re-test using identical exam conditions; compare scores against the 50-point passing threshold.

### Open Question 2
- Question: Would structure- and logic-aware retrieval methods reduce hallucinated legal citations in judgment generation?
- Basis in paper: [explicit] Section 8.1 recommends: "Future work should move beyond purely linguistic similarity toward a structure- and logic-aware stack... hybrid retrieval methods (sparse + dense) with a law-specific cross-encoder re-ranker are worth exploring."
- Why unresolved: The current linguistic approach retrieved semantically similar but contextually unrelated provisions, causing models to cite non-existent statutes.
- What evidence would resolve it: Implement boundary-aligned chunking at legal units with law-specific re-ranking; measure citation accuracy and hallucination rates on a held-out test set.

### Open Question 3
- Question: Can LLM-as-a-judge systems be calibrated to match human expert scoring distributions for legal writing evaluation?
- Basis in paper: [explicit] LLM evaluators scored submissions 85-90/100 while human experts scored them 8-37/100, consistently over-valuing style over substance.
- Why unresolved: The paper demonstrates systematic bias but does not test calibration methods.
- What evidence would resolve it: Apply calibration techniques (expert annotation, multi-model consensus, or hybrid human-in-the-loop) and correlate LLM scores with human expert scores across multiple legal writing samples.

## Limitations

- **Jurisdiction-specific findings**: Results are based on Polish public procurement law exam, limiting generalizability to other legal domains or jurisdictions
- **Single LLM evaluator bias**: LLM-as-a-judge used only GPT-4o, which shares architectural lineage with GPT-4.1 being evaluated, raising potential bias concerns
- **Single-expert evaluation uncertainty**: Human expert evaluation, while described as coming from the exam committee, lacks detailed inter-rater reliability metrics

## Confidence

- **High confidence**: Knowledge test performance (70-88% accuracy) and formal requirement extraction accuracy (>97% F1) - these are objective, verifiable metrics
- **Medium confidence**: Written judgment failure (all models scored 0-37/100) - human evaluation was rigorous but single-expert evaluation introduces uncertainty
- **Medium confidence**: LLM-as-a-judge unreliability - demonstrated divergence from human scores but limited to single evaluator model
- **Low confidence**: Generalizability to other legal domains or exam types - findings are jurisdiction-specific

## Next Checks

1. **Cross-jurisdiction validation**: Test the same models on another country's legal qualifying exam (e.g., Brazilian OAB or US bar exam) to assess whether substantive reasoning failures generalize beyond Polish public procurement law
2. **Multi-judge calibration study**: Have 3-5 independent legal experts score the same LLM-generated judgments to establish inter-rater reliability and identify systematic scoring patterns in LLM-as-a-judge evaluations
3. **Retrieval-LLM compatibility analysis**: Systematically test different model-retrieval combinations (Claude with various embeddings, GPT-4.1 with different chunk sizes) to identify optimal configurations and explain Claude's RAG degradation on the 2025 exam