---
ver: rpa2
title: Abduction of Domain Relationships from Data for VQA
arxiv_id: '2502.09219'
source_url: https://arxiv.org/abs/2502.09219
tags:
- assign
- domain
- accuracy
- data
- color
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FAST-DAP, a practical heuristic algorithm
  for abducting domain relationships from visual question answering (VQA) data represented
  as ASP programs. The method processes ASP representations of images and queries
  to infer domain attributes (e.g., that "banana" belongs to the "fruit" domain),
  addressing the limitation that scene graphs typically lack such commonsense knowledge.
---

# Abduction of Domain Relationships from Data for VQA

## Quick Facts
- arXiv ID: 2502.09219
- Source URL: https://arxiv.org/abs/2502.09219
- Authors: Al Mehdi Saadat Chowdhury; Paulo Shakarian; Gerardo I. Simari
- Reference count: 8
- Primary result: FAST-DAP improves VQA accuracy from 59.98% to 81.01% on GQA

## Executive Summary
This paper introduces FAST-DAP, a heuristic algorithm for abductively inferring domain relationships (e.g., that "banana" belongs to the "fruit" domain) from visual question answering data represented as Answer Set Programming programs. The method addresses a critical limitation where scene graphs lack commonsense knowledge about object categories and their relationships. By leveraging query structure and ground truth answers, FAST-DAP processes ASP representations of images and queries in a single pass to identify and generalize domain relationships, using support-based filtering to reduce overfitting. Evaluated on the GQA dataset, FAST-DAP demonstrates significant improvements in VQA accuracy while requiring only 10% of training data to achieve 78.93% accuracy.

## Method Summary
FAST-DAP operates on ASP programs representing visual scenes (ΠI), queries (ΠQ), common rules (ΠR), and ground truth answers (ΠGT). The algorithm processes training examples in a single pass, extracting domain assignments directly from choice-type queries and abducing generalizations when queries fail to entail correct answers. For each example, if the query contains a `choose` predicate, domain assignments are extracted directly based on ground truth. When entailment fails, the algorithm identifies the minimal `select` predicate causing failure and generalizes the constant to a broader domain concept. Support counts track the frequency of each candidate domain assignment, and only those exceeding a threshold (determined via validation set) are retained in the final hypothesis ΠD. The approach uses the Clingo ASP solver for entailment checking and is implemented in Python 3.11.7.

## Key Results
- Baseline accuracy without domain information: 59.98%
- FAST-DAP accuracy with full training data: 81.01%
- Data efficiency: 78.93% accuracy using only 10% of training data
- Support threshold tuning: Optimal threshold of 59.5 (50th percentile) achieved 80.54% validation accuracy

## Why This Works (Mechanism)

### Mechanism 1: Query-Structure Extraction of Domain Knowledge
Certain VQA query formats explicitly encode domain-attribute pairs, enabling direct extraction without inference. For `choose(w,x,y)` predicates in ΠQ, the algorithm directly extracts `assign(x,w)` or `assign(y,w)` depending on ground truth. Single-choice questions add one assignment; binary-choice adds both as candidate domain relationships. This assumes query language designers encode domain categories explicitly in choice predicates.

### Mechanism 2: Ground-Truth-Guided Abductive Inference
When ASP programs fail to entail correct answers, analyzing the failure point reveals missing domain assignments. For each example where ΠI ∪ ΠQ ∪ ΠR ⊭ ΠGT, the algorithm identifies the minimal `select(i,c,j)` that produces an `empty` result. If `c` is not an object name in the scene, it's treated as a general concept; the algorithm then finds an object name `c'` such that adding `assign(c',c)` enables entailment. This assumes failure at a `select` predicate with a concept constant indicates missing domain knowledge.

### Mechanism 3: Support-Based Regularization
Frequency filtering of candidate domain relationships reduces overfitting to noisy or idiosyncratic training examples. Each candidate `assign(c',c)` increments a support counter. Only assignments exceeding a percentile-based threshold (tuned on validation set) are retained in the final ΠD. This assumes valid domain relationships appear consistently across multiple examples while spurious ones are rare.

## Foundational Learning

- Concept: **Answer Set Programming (ASP) semantics**
  - Why needed here: The entire framework represents scenes, queries, and domain knowledge as ASP programs; understanding entailment (Π1 |= Π2) and interpretation satisfaction is essential.
  - Quick check question: Given a program with facts `attr(55,yellow).` and rule `r(T,A) :- query(T,color,D), r(D,OID), attr(OID,A), assign(A,color).`, what happens if `assign(yellow,color)` is not in the program?

- Concept: **Scene graph representation**
  - Why needed here: Images are converted to scene graphs with objects, attributes, and relations; the ASP encoding (ΠI) directly mirrors this structure.
  - Quick check question: In Figure 1's scene graph, what types of nodes represent "banana" vs. "yellow" vs. "fruit"?

- Concept: **Abductive reasoning**
  - Why needed here: FAST-DAP finds hypotheses (ΠD) that, when added to existing knowledge, explain observations (ground truth answers).
  - Quick check question: If you observe that a query fails without `assign(banana,fruit)` but succeeds with it, what type of inference is this?

## Architecture Onboarding

- Component map:
  Input layer (ΠI, ΠQ, ΠR, ΠGT) -> Extraction module (choose predicates) -> Abduction module (failure analysis) -> Filtering module (support thresholding) -> ASP solver (Clingo) -> Output (ΠD)

- Critical path:
  1. Load training examples (ΠI, ΠQ, ΠGT triples)
  2. For each example, attempt entailment check
  3. If query contains `choose` -> direct extraction
  4. If entailment fails -> identify failure point -> abduce domain assignment
  5. Accumulate support counts across all examples
  6. Apply threshold filter; return final ΠD

- Design tradeoffs:
  - Single-pass efficiency vs. theoretical optimality: Algorithm makes no guarantee of finding optimal ΠD
  - Support threshold vs. coverage: Higher thresholds reduce false positives but may exclude valid rare relationships
  - Fallback rules vs. precision: Adding `assign(att,default)` fallbacks improves robustness (81.01% vs. 80.62%) but may mask missing domain knowledge

- Failure signatures:
  - Many `empty` results on test set -> ΠD incomplete; lower support threshold or increase training data
  - Low accuracy on "query" type questions (Table 1 baseline 36.07%) -> domain generalization required (lines 9-14)
  - High variance across validation thresholds -> noisy training data; consider data cleaning

- First 3 experiments:
  1. **Baseline validation**: Run ASP solver on test set with empty ΠD; confirm ~60% baseline accuracy
  2. **Threshold sweep**: Train on 20% data, validate across percentile thresholds (10th-70th); plot accuracy vs. threshold to find optimal point
  3. **Data efficiency curve**: Incrementally increase training subset size (10%, 20%, ... 100%); plot test accuracy to verify diminishing returns pattern shown in Figure 2a

## Open Questions the Paper Calls Out

- **Theoretical guarantees**: Despite strong practical performance, FAST-DAP has no theoretical guarantees for the solutions it obtains. The algorithm is heuristic and designed for tractability rather than optimality.

- **Meta-cognitive AI integration**: A promising direction is to refine FAST-DAP by incorporating meta-cognitive AI techniques to address the lack of theoretical guarantees.

- **Noise robustness**: The evaluation uses ground truth ASP representations, but when these representations are noisy (as would occur with neural scene graph parsers), there may be no explanation, limiting real-world applicability.

## Limitations

- **Single dataset evaluation**: Strong performance demonstrated only on GQA dataset, limiting generalization claims to other VQA datasets with different domain structures.

- **Query format dependency**: Algorithm relies on specific query patterns (choose predicates) that may not generalize to datasets with different query formats.

- **Conversion pipeline opacity**: Systematic conversion from GQA to ASP is not fully specified, creating reproducibility challenges.

## Confidence

- **High Confidence**: The core abductive mechanism and its effectiveness in extracting domain relationships from failure analysis is well-supported by empirical results showing 21% accuracy improvement.

- **Medium Confidence**: The query-structure extraction mechanism is well-specified but may have limited applicability depending on query format consistency across datasets.

- **Low Confidence**: The scalability claims and data efficiency demonstrations, while promising, are based on controlled experiments with GQA and require validation on larger, more diverse datasets.

## Next Checks

1. **Cross-Dataset Validation**: Evaluate FAST-DAP on at least two additional VQA datasets (e.g., VQA-v2, CLEVR) to assess generalization across different domain structures and query formats.

2. **Threshold Sensitivity Analysis**: Conduct ablation studies varying the support threshold across multiple validation sets to quantify the impact of hyperparameter selection on final performance.

3. **Ablation of Mechanisms**: Systematically disable each abductive mechanism (query extraction, failure analysis, filtering) to measure their individual contributions to the overall performance improvement.