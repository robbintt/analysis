---
ver: rpa2
title: Why this and not that? A Logic-based Framework for Contrastive Explanations
arxiv_id: '2507.08454'
source_url: https://arxiv.org/abs/2507.08454
tags:
- problem
- size
- contrastive
- formulas
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a logic-based framework for contrastive explanations
  addressing questions of the form "Why P but not Q?". The authors formalize several
  problems including contrastive explanation, global contrastive explanation, minimal
  separator, counterfactual contrastive explanation, and counterfactual difference.
---

# Why this and not that? A Logic-based Framework for Contrastive Explanations

## Quick Facts
- arXiv ID: 2507.08454
- Source URL: https://arxiv.org/abs/2507.08454
- Authors: Tobias Geibinger; Reijo Jaakkola; Antti Kuusisto; Xinghan Liu; Miikka Vilander
- Reference count: 26
- The paper introduces a logic-based framework for contrastive explanations addressing questions of the form "Why P but not Q?"

## Executive Summary
This paper presents a comprehensive logic-based framework for generating contrastive explanations that answer questions like "Why P but not Q?". The authors formalize several key problems including contrastive explanation, global contrastive explanation, minimal separator, counterfactual contrastive explanation, and counterfactual difference. The framework computes causes for both the observed outcome (P) and the alternative outcome (Q), explicitly comparing their differences to provide meaningful explanations. The work bridges logic-based explainable AI with existing notions of contrastive explanations, providing both theoretical foundations and practical implementations for generating explanations that compare why certain outcomes occur while others do not.

## Method Summary
The authors develop a logic-based framework for contrastive explanations that formalizes multiple problems using propositional logic. The framework computes causes for both the actual outcome (P) and the alternative outcome (Q), then explicitly compares these causes to explain the difference. For propositional logic, they prove that their framework captures cardinality-minimal versions of existing contrastive explanations. The authors provide extensive computational complexity analysis, proving most problems are Σ₂^P-complete. They implement their framework using Answer Set Programming (ASP) and demonstrate its application on real-world datasets including Iris, Wine, and Glass classification problems. The implementation shows practical feasibility while highlighting the computational challenges inherent in the theoretical framework.

## Key Results
- The framework successfully computes contrastive explanations for real-world classification problems (Iris, Wine, Glass datasets)
- Theoretical analysis proves that most contrastive explanation problems are Σ₂^P-complete, indicating significant computational complexity
- In propositional logic, the framework captures cardinality-minimal versions of existing contrastive explanation methods
- ASP implementation demonstrates practical feasibility while highlighting scalability limitations

## Why This Works (Mechanism)
The framework works by computing minimal sets of conditions (causes) that are necessary for both the actual outcome P and the alternative outcome Q. By explicitly comparing these minimal sets, the framework can identify what differs between the two scenarios. This contrastive approach aligns with how humans naturally reason about explanations - by comparing what happened with what could have happened differently. The use of propositional logic provides a formal foundation that allows for rigorous complexity analysis and guarantees about the minimality of explanations.

## Foundational Learning
- **Propositional Logic**: The foundation for formalizing explanation problems, needed to provide rigorous theoretical guarantees about explanation minimality
- **Computational Complexity (Σ₂^P)**: Understanding the complexity class helps explain why certain explanation problems are computationally hard and what to expect in practice
- **Answer Set Programming (ASP)**: The implementation framework that allows for declarative problem solving, checked by the practical implementation results
- **Minimal Separators**: A concept from graph theory applied to logic to find minimal differences between outcomes, needed to identify what causes the difference between P and Q
- **Counterfactual Reasoning**: The ability to reason about alternative scenarios, checked by the counterfactual explanation results on real datasets
- **Contrastive Explanation Theory**: The broader framework of explaining differences between outcomes, needed to position this work within existing explanation methods

## Architecture Onboarding

Component Map:
User Question -> Formalization -> ASP Encoding -> Solver -> Explanation Output

Critical Path:
1. User poses contrastive question (Why P but not Q?)
2. Problem is formalized in propositional logic
3. ASP encoding is generated based on formalization
4. ASP solver computes minimal causes for P and Q
5. Differences between causes are identified and presented as explanation

Design Tradeoffs:
- Theoretical completeness vs. practical computational efficiency
- Formal guarantees vs. human interpretability
- Generality of logic-based approach vs. domain-specific explanation methods
- Computational complexity vs. explanation quality

Failure Signatures:
- No explanation found: likely due to computational complexity exceeding practical limits
- Explanation too complex: indicates that minimal causes are still too large to be meaningful
- Inconsistent explanations: suggests issues with formalization or encoding
- Very slow response times: indicates the problem instance exceeds computational feasibility

First Experiments:
1. Test the framework on synthetic problems with known solutions to verify correctness
2. Compare explanation quality and runtime on small vs. large datasets
3. Evaluate explanation interpretability with domain experts on a subset of the Iris dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Most problems are Σ₂^P-complete, indicating significant practical computational challenges
- ASP implementation has not been validated on larger, more complex problems where theoretical complexity might severely impact performance
- The paper does not address how well the explanations align with human reasoning or interpretability requirements in real-world decision-making contexts

## Confidence
- Computational complexity analysis: High confidence (formal proofs provided)
- Practical implementation and real-world dataset results: Medium confidence (limited scale of evaluation)
- Theoretical relationship to existing explanation methods: High confidence (formal analysis)

## Next Checks
1. Test the ASP implementation on larger datasets (1000+ instances) to evaluate scalability and measure actual runtime performance against theoretical complexity predictions
2. Conduct user studies to assess whether the generated explanations are meaningful and interpretable to domain experts across different application areas
3. Compare the explanations generated by this framework against state-of-the-art black-box explanation methods (e.g., LIME, SHAP) on the same classification problems to evaluate practical utility