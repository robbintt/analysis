---
ver: rpa2
title: 'Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs'
arxiv_id: '2503.02846'
source_url: https://arxiv.org/abs/2503.02846
tags:
- factuality
- arxiv
- alignment
- mask-dpo
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mask-DPO, a fine-grained factuality alignment
  method that improves LLM factuality by incorporating sentence-level factuality masks
  into Direct Preference Optimization. Unlike vanilla DPO which applies uniform rewards
  to entire responses, Mask-DPO selectively applies rewards only to factually correct
  sentences in preferred samples and factually incorrect sentences in non-preferred
  samples, resolving ambiguity in preference learning.
---

# Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs

## Quick Facts
- arXiv ID: 2503.02846
- Source URL: https://arxiv.org/abs/2503.02846
- Reference count: 24
- Key outcome: Llama3.1-8B-Instruct with Mask-DPO achieved 77.53% factuality on ANAH test set, surpassing vanilla DPO (68.44%) and larger Llama3.1-70B-Instruct (53.44%)

## Executive Summary
Mask-DPO introduces a fine-grained factuality alignment method that addresses ambiguity in preference learning by applying rewards selectively to factually correct sentences. Unlike vanilla DPO which assigns uniform rewards to entire responses, Mask-DPO uses sentence-level factuality masks to target specific segments for reward or punishment. When trained on the ANAH dataset, the method significantly improved factuality performance, demonstrating both superior results on the target domain and strong generalization to out-of-domain Biography data.

## Method Summary
Mask-DPO extends Direct Preference Optimization by incorporating fine-grained factuality masks into the preference learning process. The method first generates factuality masks that identify which sentences in a response are factually correct or incorrect. During training, rewards are applied only to factually correct sentences in preferred samples and factually incorrect sentences in non-preferred samples, while other sentences receive neutral treatment. This selective reward mechanism resolves the ambiguity inherent in traditional preference learning where entire responses receive uniform treatment regardless of internal factual inconsistencies.

## Key Results
- Llama3.1-8B-Instruct with Mask-DPO achieved 77.53% factuality on ANAH test set
- Mask-DPO outperformed vanilla DPO (68.44%) and larger Llama3.1-70B-Instruct (53.44%) on ANAH dataset
- Demonstrated strong generalization to Biography data with 39.39% FactScore

## Why This Works (Mechanism)
Mask-DPO resolves the fundamental ambiguity in preference learning by breaking down responses into their constituent factual claims and applying rewards only to verifiable correct statements. This selective reinforcement allows the model to learn which specific knowledge assertions are reliable without being confused by factually incorrect information that may be present in otherwise preferred responses.

## Foundational Learning

**Direct Preference Optimization (DPO)**: A reward modeling approach that aligns models with human preferences through pairwise comparison. Needed to understand the baseline method being enhanced. Quick check: Verify DPO optimizes a KL-divergence-based objective between policy and reference distribution.

**Factuality Assessment**: The process of determining whether statements align with verifiable ground truth. Needed to understand how Mask-DPO identifies which sentences to reward. Quick check: Confirm factuality masks are binary (correct/incorrect) rather than probabilistic.

**Knowledge Graph Structure**: The implicit representation of factual relationships within a model's learned parameters. Needed to understand the scaling hypothesis about factuality alignment. Quick check: Verify the hypothesis connects factuality improvement to knowledge graph optimization.

## Architecture Onboarding

**Component Map**: Input Response -> Factuality Mask Generator -> Selective Reward Application -> DPO Update -> Aligned Model

**Critical Path**: The core training loop where factuality masks guide selective reward application to update the model parameters through the DPO objective.

**Design Tradeoffs**: 
- Granularity vs efficiency: Sentence-level masks provide fine control but increase computational overhead
- Binary vs continuous assessment: Binary masks simplify implementation but may lose nuance in partial correctness
- Mask generation cost: High-quality masks require significant annotation effort but improve learning signal

**Failure Signatures**: 
- Poor mask quality leading to incorrect reward signals
- Overfitting to specific factuality patterns in training data
- Degradation in general language capabilities due to overly restrictive factuality constraints

**First Experiments**:
1. Ablation study comparing full Mask-DPO against variants with different mask granularities (word-level, paragraph-level)
2. Cross-domain evaluation on additional factuality benchmarks beyond ANAH and Biography
3. Knowledge graph analysis comparing parameter updates between vanilla DPO and Mask-DPO

## Open Questions the Paper Calls Out
None

## Limitations

- The knowledge graph scaling hypothesis lacks comprehensive experimental validation and remains speculative
- The method's behavior on truly out-of-distribution data beyond tested domains is uncertain
- Reliance on binary factuality annotations may oversimplify nuanced factual errors

## Confidence

**High Confidence**: Experimental results on ANAH dataset showing Mask-DPO's superior performance over vanilla DPO and larger baseline models are robust and well-documented.

**Medium Confidence**: The sentence-level reward mechanism is technically sound, but the assumption about learning efficiency requires further theoretical justification.

**Low Confidence**: The implicit knowledge graph adjustment hypothesis is primarily theoretical with only preliminary proof-of-concept experiments.

## Next Checks

1. Conduct ablation studies isolating the impact of factuality masking versus other DPO modifications to quantify the specific contribution of the fine-grained approach.

2. Test Mask-DPO's performance on diverse factuality benchmarks (e.g., TruthfulQA, REALM) to assess generalizability beyond the ANAH and Biography domains.

3. Implement the proposed knowledge graph analysis methodology on multiple model families (Mistral, Gemma) and scales to empirically validate the implicit knowledge graph adjustment hypothesis.