---
ver: rpa2
title: 'Distributionally Robust Federated Learning: An ADMM Algorithm'
arxiv_id: '2503.18436'
source_url: https://arxiv.org/abs/2503.18436
tags:
- learning
- problem
- federated
- drfl
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distributionally Robust Federated Learning
  (DRFL), a novel framework that addresses data heterogeneity and distributional ambiguity
  in federated learning settings. DRFL constructs individual Wasserstein ambiguity
  sets for each client, allowing different data distributions while optimizing the
  worst-case expected loss across these sets.
---

# Distributionally Robust Federated Learning: An ADMM Algorithm

## Quick Facts
- arXiv ID: 2503.18436
- Source URL: https://arxiv.org/abs/2503.18436
- Reference count: 40
- Key outcome: DRFL outperforms standard FL approaches on heart, breast-cancer, and abalone datasets under various noise conditions by constructing individual Wasserstein ambiguity sets per client

## Executive Summary
This paper introduces Distributionally Robust Federated Learning (DRFL), a novel framework that addresses data heterogeneity and distributional ambiguity in federated learning settings. DRFL constructs individual Wasserstein ambiguity sets for each client, allowing different data distributions while optimizing the worst-case expected loss across these sets. The authors derive a tractable reformulation of DRFL and develop an ADMM-based algorithm to solve it efficiently, demonstrating improved accuracy and robustness compared to standard federated learning approaches under various noise conditions.

## Method Summary
The paper proposes DRFL, which constructs individual Wasserstein ambiguity sets per client to handle data heterogeneity in federated learning. The authors reformulate the robust optimization problem into a tractable constrained optimization and develop an ADMM algorithm to solve it efficiently. The algorithm splits primal variables into global and local groups, allowing clients to solve local subproblems while maintaining privacy. Experiments on UCI datasets (heart, breast-cancer, abalone) demonstrate that DRFL outperforms standard federated learning approaches, DRFA, AFL, and WAFL in terms of accuracy and mean squared error under various noise conditions.

## Key Results
- DRFL maintains accuracy >0.8 on heart dataset under Gaussian noise while standard SVM drops significantly
- Individual Wasserstein ambiguity sets provide better coverage of true heterogeneous distributions than aggregate-then-robustify approaches
- ADMM-based algorithm enables efficient solving of reformulated robust optimization problem in federated settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constructing individual Wasserstein ambiguity sets per client provides better coverage of true data distribution than aggregating distributions first.
- **Mechanism:** DRFL defines an ambiguity set $\mathcal{P}_s$ centered on local empirical distribution $\hat{P}_s$ for each client, contrasting with WAFL's single set around mixed global distribution. Local treatment allows union of sets to cover true heterogeneous distribution with higher probability.
- **Core assumption:** True distribution $P^{\star}_s$ for specific client resides within Wasserstein ball of radius $\rho_s$ around empirical distribution $\hat{P}_s$.
- **Break condition:** Extremely sparse client data makes empirical distribution $\hat{P}_s$ poor center, requiring large $\rho_s$ that makes robust solution trivial.

### Mechanism 2
- **Claim:** ADMM splitting strategy isolates data-dependent constraints to clients, enabling tractable solving that standard SGD cannot handle.
- **Mechanism:** Reformulates robust loss into constrained optimization where data samples appear in constraints. ADMM splits variables into global ($w$) and local groups, allowing clients to solve local subproblems involving their constraints and send only dual variables back to server.
- **Core assumption:** Loss function and model satisfy convexity or Lipschitz conditions allowing strong duality for tractable reformulation.
- **Break condition:** Non-convex local subproblem requiring approximation degrades convergence speed significantly.

### Mechanism 3
- **Claim:** Regularizing worst-case loss with penalty on dual norm of client weights forces model to generalize across clients without over-indexing on any single noisy client.
- **Mechanism:** Reformulation introduces penalty $\theta \cdot \|z + \gamma e + \eta\|_{p^*}$ arising from dualizing uncertainty in client weighting. This avoids scenario where specific client's noisy distribution dominates global model.
- **Core assumption:** Uncertainty in client weights lies within probability simplex constrained by $\ell_p$-norm ball.
- **Break condition:** Incorrect $\theta$ causes model to either ignore minority clients (too small) or treat all clients as equally unreliable (too large).

## Foundational Learning

- **Concept: Wasserstein Distance & Ambiguity Sets**
  - **Why needed here:** Measures how "far" true data distribution might be from observed empirical data, defining "ball" of distributions model must defend against.
  - **Quick check question:** If you double Wasserstein radius $\rho_s$, does model become more conservative or more aggressive?

- **Concept: ADMM (Alternating Direction Method of Multipliers)**
  - **Why needed here:** Core solver that breaks global problem into local pieces using Augmented Lagrangian to coordinate them via dual variables.
  - **Quick check question:** In ADMM, if primal residual does not converge, which parameter (stepsize $c$ or penalty $\rho$) typically needs adjustment?

- **Concept: Duality in Robust Optimization**
  - **Why needed here:** Converts intractable "infinite-dimensional" robust problem into "finite-dimensional" constrained problem using dual variables.
  - **Quick check question:** Why does strong duality allow us to swap "sup" (worst-case) distribution with "inf" (minimization) over dual variables?

## Architecture Onboarding

- **Component map:** Server maintains global model $w$ and dual variables, executes updates $P_w, P_z, P_t$; Client maintains local copies $\hat{w}_s$ and solves local subproblem $C_s$; Coordinator manages stopping criterion and stepsize $c$.

- **Critical path:** Reformulation of local client update $C_s$ (Page 13, Eq 14). This operator solves convex problem involving constraints $\Omega_s$ holding local data. Incorrect solution or coarse approximation prevents global model $w$ from converging to robust optimum.

- **Design tradeoffs:**
  - **Exact vs. Approximate Local Solve:** Paper suggests sampling subset of data ($\tilde{I}_s$) to approximate $C_s$, speeding iterations but introducing inexactness affecting final accuracy.
  - **Radius Selection ($\rho_s$):** Critical parameter set via cross-validation, but true production systems might require dynamic adjustment based on data volume.

- **Failure signatures:**
  - **Divergence:** Large stepsize $c$ or non-convex $C_s$ without proper initialization.
  - **Over-regularization:** Large $\theta$ dominates penalty term, pushing $w$ toward zero (trivial solution).
  - **Stagnation:** Duality gap between server and client variables doesn't close; check update rules for $\psi_s$.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Implement Gaussian noise injection on "Heart" dataset to verify DRFL maintains accuracy >0.8 while Standard SVM drops, validating noise-robustness mechanism.
  2. **Radius Sensitivity:** Sweep $\rho_s$ on single client while holding others fixed to observe break condition where local robustness becomes too conservative and degrades global accuracy.
  3. **Communication Cost:** Measure ADMM iterations to converge against FedAvg (SGD steps) to quantify trade-off between "robustness per step" vs "total communication rounds".

## Open Questions the Paper Calls Out
None

## Limitations
- ADMM-based algorithm may become computationally expensive for high-dimensional models or large client datasets due to constrained optimization subproblems
- Experiments validate robustness against Gaussian noise injection, but real-world data heterogeneity might involve more complex distributional shifts
- Paper uses cross-validation for radius selection but sensitivity to initial radius choices and convergence stability across datasets remains underexplored

## Confidence
- **High Confidence:** Theoretical foundation (Wasserstein ambiguity sets, ADMM reformulation) and core experimental results showing DRFL outperforms baselines under controlled noise conditions
- **Medium Confidence:** Practical implementation details (exact solver for C_s, stopping criteria, step-size selection) and generalizability of results to real-world federated learning scenarios with natural data heterogeneity

## Next Checks
1. **Convergence Robustness:** Run DRFL on heart dataset with varying step-sizes c (0.1, 1, 10) and different initializations to identify conditions where ADMM algorithm diverges or stagnates
2. **Noise Distribution Generalization:** Replace Gaussian noise with heavy-tailed distributions (e.g., Laplace, Cauchy) in breast-cancer experiment to test whether DRFL's robustness extends beyond symmetric, light-tailed noise
3. **Communication Efficiency:** Measure total number of ADMM iterations and corresponding communication rounds needed for DRFL to reach 95% of final accuracy, comparing against FedAvg's SGD iterations to quantify practical communication cost trade-off