---
ver: rpa2
title: 'AI-assisted German Employment Contract Review: A Benchmark Dataset'
arxiv_id: '2501.17194'
source_url: https://arxiv.org/abs/2501.17194
tags:
- clauses
- legal
- void
- dataset
- contracts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a benchmark dataset for AI-assisted review
  of German employment contracts, addressing the challenge of detecting void or unfair
  clauses in legal documents. The dataset contains 1,094 anonymized clauses with legality
  and category annotations, created by legal experts with 96.4% inter-annotator agreement.
---

# AI-assisted German Employment Contract Review: A Benchmark Dataset

## Quick Facts
- **arXiv ID**: 2501.17194
- **Source URL**: https://arxiv.org/abs/2501.17194
- **Reference count**: 0
- **Primary result**: Fine-tuned gpt-3.5-turbo-1106 achieved 61.5% F1-score for void/unfair clause detection

## Executive Summary
This paper introduces a benchmark dataset for AI-assisted review of German employment contracts, containing 1,094 anonymized clauses with legality annotations created by legal experts (96.4% inter-annotator agreement). The research evaluates both fine-tuning and prompt engineering approaches for detecting void or unfair clauses, finding that fine-tuning general-purpose LLMs yields superior performance (61.5% F1) compared to prompt engineering methods (26.8% F1). The study reveals that providing section titles as context sometimes reduces performance in fine-tuning while aiding prompt engineering, and identifies a systematic bias in proprietary models toward over-flagging clauses as problematic. The dataset serves as a foundation for developing AI tools to assist in employment contract review, with plans to expand to 10,000 clauses.

## Method Summary
The dataset consists of 1,094 anonymized German employment contract clauses with legality and category annotations created by legal experts. For reproduction, the 90/10 train/test split should use the first 90% of rows for training and last 10% for testing, with duplicates removed (retaining the highest severity label: Void > Unfair > Valid). Open-source models use `bert-base-german-cased` with batch size 8, learning rate 5e-5, and 3 epochs. Closed-source models fine-tune `gpt-3.5-turbo-1106` with system instructions but without section titles, as including titles reduces performance in fine-tuning experiments.

## Key Results
- Fine-tuning gpt-3.5-turbo-1106 achieved the highest F1-score of 61.5% for void/unfair detection
- Proprietary models exhibit systematic bias toward classifying clauses as problematic, leading to high recall but low precision
- Providing section titles as context reduced performance in fine-tuning experiments but improved prompt engineering results
- Open-source models like BERT achieved high precision (75% with title) but very low recall (23%), making them suitable for high-stakes filtering

## Why This Works (Mechanism)

### Mechanism 1: Parameter-Efficient Domain Adaptation via Fine-Tuning
Fine-tuning adjusts model weights to internalize the specific decision boundary between valid and void/unfair clauses based on 1,094 expert-labeled samples. This creates a specialized legal subspace within the model's representations, whereas prompt engineering relies on activating potentially diffuse or conflicting legal knowledge frozen in pre-trained weights. The 1,094 clauses must sufficiently represent German employment contract syntax and semantics to shift model weights without overfitting.

### Mechanism 2: Contextual Dilution via Metadata Injection
Providing section titles as context harms performance in fine-tuning but aids prompt engineering, suggesting a fundamental difference in how features are weighed during weight updates versus attention masking. In fine-tuning, the model may learn a "lazy" shortcut by over-weighting the section title rather than analyzing clause text. If the title is ambiguous relative to the specific clause text, precision collapses.

### Mechanism 3: Alignment-Induced Bias in Proprietary Models
Proprietary models exhibit a systemic bias towards classifying clauses as problematic, driven by safety alignment rather than legal accuracy. RLHF often aligns models to be cautious or protective, manifesting as a "Defender of the Employee" persona that causes models to hallucinate unfairness in valid clauses to avoid "missing" a risk.

## Foundational Learning

- **Legal Taxonomy & Label Hierarchy**: Understanding the 3-class problem (Valid, Unfair, Void) simplified into binary (Okay vs. Problematic) is critical, as "Void" implies "Unfair" but not vice versa. Quick check: If a model predicts "Unfair" for a clause labeled "Void" in ground truth, is this a False Negative or partial credit scenario?
- **Inter-Annotator Agreement (Cohen's Kappa)**: Legal text is subjective. The 96.4% agreement represents expert consensus, not objective truth, defining the upper bound of model performance. Quick check: Why does the 96.4% agreement matter when calculating theoretical maximum accuracy?
- **Class Imbalance Handling**: The dataset is ~80% "Valid", so a naive model predicting "Valid" for everything achieves 80% accuracy. Quick check: Why is Accuracy misleading for this dataset, and which metric gives the truest picture of utility?

## Architecture Onboarding

- **Component map**: Raw PDF/Text contracts -> Segmentation (Clause splitter) -> Anonymization (PII scrubbing) -> [Clause Text] + (Optional)[Title] -> Tokenizer -> Model (Fine-tuned Transformer) -> Classification Head (Binary: Problematic/Okay) -> Label + (If Prompt Engineering) Rationale
- **Critical path**: The Data Transformation logic (Section 5.2). The choice between "Clause Only" vs. "Clause & Title" has massive impact on performance. Default path must enforce "Clause Only" for fine-tuning.
- **Design tradeoffs**: Fine-tuning (GPT-3.5): Highest F1 (61.5%), higher cost, closed-source. Prompt Engineering (GPT-4): Lower F1 (~40%), easier iteration, prone to "Defender" bias. Open Source (BERT): High Precision (75% w/ Title), very low Recall (23%), full control, cheap inference. Decision: Use BERT for high-stakes filtering, GPT-3.5 for general review.
- **Failure signatures**: "Title Collapse" (Precision drops to ~8% when Title is injected during fine-tuning). "Defender Hallucination" (Model marks standard confidentiality clause as "Void" with moralizing explanation). "Annotation Mismatch" (Identical clauses having different labels causing training instability).
- **First 3 experiments**: 1) Establish Baseline: Replicate "Clause Only" fine-tuning of GPT-3.5 to verify F1 ~61.5% benchmark. 2) Title Ablation: Compare "Clause Only" vs. "Clause + Title" on BERT to confirm performance degradation. 3) Threshold Tuning: Adjust probability threshold for "Defender Biased" GPT-4o model to improve Precision.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does expanding the dataset to 10,000 clauses resolve the performance limitations observed in fine-tuned models? The authors explicitly state plans to increase dataset size to address the limitation that the current size might be insufficient for training and validation splits.
- **Open Question 2**: Why does providing section titles degrade performance in fine-tuning while improving results in prompt engineering? The authors note the "surprising" result that including section titles resulted in "slightly worse performance in all fine-tuning experiments" despite helping prompt engineering.
- **Open Question 3**: Can the inherent "employee-protector" bias in OpenAI models be mitigated to align with expert legal ground truth? The discussion highlights a "significant bias" where models position themselves as "defenders of the employee," leading to false positives that pose a "potential challenge for future applications."

## Limitations
- Dataset size (1,094 clauses) may be insufficient for capturing full distribution of German employment contract language
- 96.4% inter-annotator agreement still represents expert disagreement on approximately 4% of clauses
- Systematic bias in proprietary models toward over-flagging clauses conflicts with legal accuracy requirements
- Closed-source nature of best-performing models limits reproducibility and interpretability

## Confidence

- **High Confidence**: Dataset construction methodology, inter-annotator agreement metrics, and comparative performance of fine-tuning versus prompt engineering are well-documented and reproducible. Observed bias in proprietary models toward over-flagging clauses is consistently demonstrated.
- **Medium Confidence**: Claim that section titles reduce fine-tuning performance is supported by empirical results but lacks detailed analysis of underlying mechanism. 61.5% F1-score may not be absolute maximum achievable given dataset constraints.
- **Low Confidence**: Assertion that dataset size is primary limiting factor for performance improvements is plausible but not definitively proven. Alternative explanations could also explain observed results.

## Next Checks
1. Perform k-fold cross-validation on current dataset to establish more robust performance estimates and identify potential data leakage or sampling bias in fixed 90/10 split.
2. Conduct ablation studies isolating contribution of section titles versus clause text, including attention visualization for transformer models to verify whether models are learning to ignore titles during fine-tuning.
3. Evaluate whether increasing dataset size to planned 10,000 clauses significantly improves F1-scores for open-source models, testing hypothesis that data quantity rather than model sophistication is primary bottleneck.