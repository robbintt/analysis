---
ver: rpa2
title: 'Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations
  in Multimodal LLMs'
arxiv_id: '2601.16527'
source_url: https://arxiv.org/abs/2601.16527
tags:
- unlearning
- hallucination
- sare
- while
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper reveals that standard hallucination unlearning in multimodal
  LLMs suffers from structural fragility: models converge to sharp minima where hallucinations
  rapidly resurge after lightweight relearning attacks. To address this, the authors
  propose SARE, a sharpness-aware unlearning framework that treats erasure as a targeted
  min-max optimization problem.'
---

# Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs

## Quick Facts
- arXiv ID: 2601.16527
- Source URL: https://arxiv.org/abs/2601.16527
- Reference count: 40
- Primary result: SARE reduces ChairS from 69.6 to 37.3 on mPLUG-Owl-7B while maintaining generation quality and resisting relearning attacks

## Executive Summary
Standard hallucination unlearning in multimodal LLMs suffers from structural fragility, with models converging to sharp minima where hallucinations rapidly resurge after lightweight relearning attacks. To address this, the authors propose SARE, a sharpness-aware unlearning framework that treats erasure as a targeted min-max optimization problem. By simulating worst-case parameter perturbations via Targeted-SAM, SARE explicitly flattens the loss landscape around hallucinated concepts, ensuring stable erasure resistant to weight shifts. Extensive experiments show SARE significantly outperforms baselines in hallucination suppression while preserving generation quality and maintaining robust resistance to relearning, LoRA fine-tuning, and adversarial prompting attacks.

## Method Summary
SARE reformulates hallucination unlearning as a targeted min-max optimization problem. During training, it first computes the gradient of hallucination loss (L_neg), normalizes and scales it by ρ to create a perturbation ε*, then applies this perturbation to model weights. The model is trained to minimize hallucination loss under this worst-case perturbation while preserving linguistic fluency and visual grounding through decoupled gradient computation at unperturbed weights for capability preservation losses. Only multimodal mapping layers are tuned using AdamW with lr=1e-5, weight decay=0.05, for 1 epoch. Data curation uses CLIP-based alignment scoring to create three datasets: visual anchors (D_pos), hallucination targets (D_neg), and sentence-level samples (D_sent) with thresholds T0=32, T1=23, T2=27.5.

## Key Results
- SARE reduces ChairS hallucination scores from 69.6 to 37.3 on mPLUG-Owl-7B
- Maintains competitive generation quality (Bleu-4: 18.9, PPL: 0.101, Informativeness: 89.6)
- Shows robust resistance to relearning attacks (fine-tuning on 60-140 hallucination samples)
- Outperforms standard fine-tuning, DPO, and NPO baselines in both suppression and capability preservation

## Why This Works (Mechanism)

### Mechanism 1: Loss Landscape Flattening via Targeted Sharpness-Aware Minimization
Standard unlearning traps parameters in sharp minima where hallucination suppression is structurally fragile to small weight perturbations; flattening the loss landscape creates robust erasure. SARE reformulates unlearning as a targeted min-max problem where the inner loop identifies the perturbation direction (ϵ*) that maximizes hallucination likelihood, exposing the model's most vulnerable geometric region. The outer loop minimizes hallucination loss under this worst-case perturbation, explicitly penalizing sharp loss curvature via the gradient norm term ρ‖∇L_neg‖². Assumption: Hallucination resurgence after relearning is primarily caused by sharp loss landscape geometry rather than incomplete training.

### Mechanism 2: Simulated Worst-Case Attack via Gradient-Aligned Perturbation
The optimal perturbation ϵ* = ρ∇L_neg/‖∇L_neg‖ aligns with the gradient direction of hallucination loss, pointing toward the parameter region most vulnerable to relapse. During training, SARE computes the gradient of L_neg, normalizes it, scales by ρ, and applies this perturbation to model weights. Training then proceeds on this perturbed state, forcing the model to suppress hallucinations even when weights shift adversarially toward resurgence-prone regions. Assumption: The gradient direction of L_neg reliably identifies the most vulnerable parameter subspace; low-order Taylor approximation suffices for practical optimization.

### Mechanism 3: Multi-Objective Capability Preservation with Decoupled Gradient Computation
SARE preserves linguistic fluency and visual grounding by computing capability preservation losses (L_pos, L_sent) at unperturbed weights while computing hallucination suppression at perturbed weights. The final gradient g_final = λ₁∇L_neg(θ + ϵ*) + ∇L_pos(θ) + λ₂∇L_sent(θ) integrates two signals: robust hallucination suppression from the adversarially-perturbed state, and standard capability preservation from the current state. This decoupling prevents the sharpness penalty from degrading valid knowledge. Assumption: Capability preservation losses are sufficiently orthogonal to hallucination suppression that they need not be sharpened.

## Foundational Learning

- **Sharp vs. Flat Minima in Loss Landscapes**: Why needed: The entire paper hinges on the geometric intuition that sharp minima are fragile to parameter perturbation; you must understand curvature to diagnose why standard unlearning fails. Quick check: Can you sketch why a narrow basin in loss space causes performance to degrade catastrophically under small weight changes, whereas a wide basin tolerates perturbation?

- **Sharpness-Aware Minimization (SAM) and Its Min-Max Formulation**: Why needed: SARE adapts SAM to unlearning; understanding the original SAM formulation (min_θ max_ϵ L(θ+ϵ)) is prerequisite to grasping why Targeted-SAM differs. Quick check: Explain why SAM requires two forward-backward passes per iteration and how it approximates Hessian information without explicit computation.

- **Multimodal LLM Architecture (Vision Encoder + Projector + LLM Backbone)**: Why needed: The paper freezes backbone and tunes only multimodal mapping layers; knowing which components are trainable matters for implementation. Quick check: In an MLLM, which layer transforms visual features into the LLM's embedding space, and why might unlearning target this vs. the vision encoder?

## Architecture Onboarding

- **Component map**: CLIP alignment scorer → threshold filtering (T₀=32 for grounding, T₁=23 for hallucination, T₂=27.5 for sentence reliability) → outputs D_pos, D_neg, D_sent → Targeted-SAM optimization

- **Critical path**: 1. Perturbation direction computation (∇L_neg) — if incorrect, sharpness regularization is misdirected; 2. Dual gradient computation (perturbed vs. unperturbed) — if conflated, capability preservation degrades; 3. Early stopping (Epoch 1) — extended training causes "grounding collapse" (POPE ↓)

- **Design tradeoffs**: ρ (perturbation radius): small ρ (0.01) behaves like standard fine-tuning; large ρ (0.15) pushes parameters off valid manifold. Paper recommends ρ=0.05. λ₁ (unlearning weight): too low (0.1) → insufficient suppression; too high (0.4) → distribution collapse. Optimal at 0.3. λ₂ (sentence preservation): too low → linguistic degradation; too high (0.4) → gradient conflict prevents convergence. Optimal at 0.2–0.3. Compute overhead: 2× forward-backward per iteration vs. standard fine-tuning; still faster than DPO/NPO baselines.

- **Failure signatures**: Superficial unlearning: ChairS drops initially but rebounds to baseline under Relearn 60–140 → model is in sharp minimum (standard EFUF behavior). Grounding collapse: PPL improves but POPE drops sharply (Table 2, Sentence Loss Only) → model generating generic safe text decoupled from visual input. Distribution collapse: All metrics degrade at high λ₁ (0.4) → aggressive unlearning destabilizes probability manifold.

- **First 3 experiments**: 1. Reproduce Figure 1b vulnerability curve: Apply EFUF to mPLUG-Owl-7B, then fine-tune on 60/100/140 hallucination samples. Confirm ChairS resurgence rate matches paper (43.6 → 59.1). 2. Ablation on perturbation radius ρ: Train SARE with ρ ∈ {0.01, 0.05, 0.10, 0.15} on LLaVA-v1.5-7B. Plot ChairS vs. POPE. Expect optimal at 0.05. 3. Probe gradient alignment: After computing ϵ*, evaluate L_neg(θ+ϵ*) vs. L_neg(θ+random_ϵ). If perturbation-aligned direction does not produce significantly higher hallucination loss than random, first-order gradient approximation is not capturing true vulnerability.

## Open Questions the Paper Calls Out

- **How can the geometric regularization framework be adapted to address hallucinations involving incorrect object attributes, spatial positioning errors, or fallacious logical reasoning chains, rather than solely object existence?**: The current Targeted-SAM relies on maximizing the likelihood of hallucinated objects, which is conceptually distinct from correcting relational or logical errors where the objects themselves may exist. The Limitations section explicitly states the mechanism is currently optimized for object existence hallucinations with explicit visual evidence and has "yet to be extended" to nuanced types like attributes or abstract reasoning.

- **Can the static, alignment-based data curation pipeline be augmented to identify and mitigate latent hallucination triggers that are dormant in pretrained weights but activated by novel or out-of-distribution prompts?**: Static pipelines only suppress known hallucinations found in the current data distribution, leaving the model vulnerable to prompts that exploit unseen regions of the latent space. The authors note in the Limitations that the construction of the unlearning dataset relies on static filtering, which "cannot proactively capture latent hallucination triggers."

- **What approximations or modifications to the Targeted-SAM gradient computation are required to reduce the training overhead for real-time or ultra-large-scale unlearning scenarios?**: The rigorous min-max optimization guarantees robustness but creates a computational bottleneck that hinders scalability compared to standard single-step fine-tuning methods. The paper identifies "optimization efficiency" as a primary limitation, noting that the dual-step gradient computation "doubles the training time per iteration."

## Limitations

- The geometric hypothesis that sharp minima are the primary cause of hallucination resurgence may not capture the full complexity of multimodal knowledge representations
- One-epoch training constraint appears somewhat arbitrary, with claims about grounding collapse needing more systematic investigation
- CLIP-based data curation pipeline relies on alignment scores without detailed specification, making exact reproduction challenging

## Confidence

- **High confidence**: The geometric intuition that sharp minima are fragile to weight perturbations is well-established in optimization theory and the experimental evidence for SARE's effectiveness is robust across multiple model sizes and datasets
- **Medium confidence**: The specific mechanism of Targeted-SAM's superiority over standard fine-tuning, while demonstrated empirically, relies on assumptions about gradient alignment and first-order approximations that could break down in more complex scenarios
- **Low confidence**: The one-epoch training constraint and claim about grounding collapse appear somewhat arbitrary and need more systematic investigation

## Next Checks

1. **Landscape geometry verification**: After SARE training, perform targeted weight perturbations in random directions versus gradient-aligned directions to confirm that the flattened landscape actually increases the margin before hallucination resurgence occurs

2. **Cross-domain generalization**: Test whether SARE's hallucination suppression transfers to out-of-distribution visual concepts not present in the training set, or whether it creates blind spots in valid visual reasoning

3. **Architectural sensitivity**: Apply SARE to different multimodal architectures (e.g., models with different visual encoder sizes or attention mechanisms) to determine whether the effectiveness is architecture-specific or generalizes across MLLM designs