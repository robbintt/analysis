---
ver: rpa2
title: 'Disco-RAG: Discourse-Aware Retrieval-Augmented Generation'
arxiv_id: '2601.04377'
source_url: https://arxiv.org/abs/2601.04377
tags:
- chunk
- discourse
- disco-rag
- generation
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Disco-RAG introduces discourse-aware modeling into retrieval-augmented
  generation by parsing retrieved passages into intra-chunk RST trees and inter-chunk
  rhetorical graphs. These structures, combined with a discourse-driven planning module,
  enable the model to reason over evidence hierarchies and rhetorical connections,
  rather than concatenating flat text.
---

# Disco-RAG: Discourse-Aware Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2601.04377
- **Source URL:** https://arxiv.org/abs/2601.04377
- **Reference count:** 40
- **Primary result:** Discourse-aware RAG consistently outperforms standard RAG across Loong, ASQA, and SciNews benchmarks without fine-tuning.

## Executive Summary
Disco-RAG introduces discourse-aware modeling into retrieval-augmented generation by parsing retrieved passages into intra-chunk RST trees and inter-chunk rhetorical graphs. These structures, combined with a discourse-driven planning module, enable the model to reason over evidence hierarchies and rhetorical connections, rather than concatenating flat text. Evaluated on Loong, ASQA, and SciNews benchmarks, Disco-RAG consistently outperforms standard RAG and prior state-of-the-art methods without fine-tuning.

## Method Summary
Disco-RAG is a training-free 4-stage inference pipeline: (1) Retrieval using Qwen3-Embedding-8B to get top-10 chunks, (2) Parsing with LLM to construct intra-chunk RST trees and inter-chunk rhetorical graphs, (3) Planning to generate a discourse-aware blueprint, and (4) Generation conditioned on all structures. The method uses Llama-3.3-70B or Qwen2.5-72B with beam width 3. Discourse structures are cached offline for the corpus, with inter-chunk graphs generated online per query.

## Key Results
- **Loong benchmark:** Improves LLM Score by up to 10.0 points over standard RAG
- **ASQA benchmark:** Raises Exact Match by 3.1 points and DR Score by 9.2 points
- **SciNews benchmark:** Achieves new SOTA results across RL, BERTScore, SARI, and SummaC metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intra-chunk RST trees enable the model to distinguish core claims from qualifying conditions within each retrieved passage.
- **Mechanism:** An LLM-based parser segments each chunk into Elementary Discourse Units (EDUs), assigns nucleus/satellite roles, and labels rhetorical relations. This hierarchy exposes which statements are central versus supplementary.
- **Core assumption:** The backbone LLM can perform zero-shot RST parsing with sufficient accuracy to provide useful structural signals.
- **Evidence anchors:** Ablation shows removing RST trees drops LLM Score from 62.07 to 56.22; perturbation study shows shuffling relation labels drops score to 55.48.

### Mechanism 2
- **Claim:** Inter-chunk rhetorical graphs enable resolution of conflicting evidence by explicitly modeling cross-passage relations.
- **Mechanism:** Given all retrieved chunks, the parser jointly predicts directed rhetorical relations for each chunk pair, forming a global discourse scaffold.
- **Core assumption:** Listwise inference over all chunks captures global context better than pairwise comparisons.
- **Evidence anchors:** Figure 1 example shows correct synthesis of contradictory evidence; ablation confirms importance of graph structure.

### Mechanism 3
- **Claim:** Discourse-driven planning reduces hallucination by grounding generation in a rhetorical blueprint before synthesis.
- **Mechanism:** Given the query, chunks, RST trees, and rhetorical graph, the planner generates a natural-language paragraph outlining argumentative flow.
- **Core assumption:** Explicit planning imposes stronger structural constraints than implicit context conditioning alone.
- **Evidence anchors:** Ablation shows removing planner drops score from 62.07 to 59.75; generic planning baselines achieve only 50.64-51.38 vs. Disco-RAG's 62.07.

## Foundational Learning

- **Concept:** Rhetorical Structure Theory (RST) fundamentals
  - **Why needed here:** Understanding nucleus vs. satellite distinction and relation types is essential for interpreting parser output and debugging structural representations.
  - **Quick check question:** Given "Vitamin D reduced flu incidence by 12% in deficient adults during winter," which span is the nucleus and what relation would qualify the claim?

- **Concept:** Graph-based text representation
  - **Why needed here:** Inter-chunk rhetorical graphs require understanding directed edges, relation semantics, and how to traverse/serialize graph structures for prompt conditioning.
  - **Quick check question:** How would you represent a CONTRADICTS edge vs. an ELABORATES edge when constructing the prompt for generation?

- **Concept:** Planning-then-generation paradigms
  - **Why needed here:** Disco-RAG's planner operates as an intermediate reasoning step; understanding how plans condition generation clarifies the architecture.
  - **Quick check question:** What information must the plan contain to be useful vs. merely restating chunk content?

## Architecture Onboarding

- **Component map:** Retriever (Qwen3-Embedding-8B) → RST Parser (Llama-3.3-70B) → Planner (Llama-3.3-70B) → Generator (Llama-3.3-70B)
- **Critical path:** Offline: Corpus → Chunking → Intra-chunk RST Tree construction (cache these). Online: Query → Retrieval → Inter-chunk Graph inference → Planning → Generation.
- **Design tradeoffs:** Token cost is ~2.2× higher vs. standard RAG; latency increases ~2s per query. Chunk size maintains performance across 128-1024 token chunks. Zero-shot parser is ~10 F1 below supervised.
- **Failure signatures:** Parser outputs malformed trees → validate output format before conditioning. Planner reproduces chunk text verbatim → prompt engineering to enforce abstraction. Generation ignores plan → check attention distribution.
- **First 3 experiments:**
  1. Reproduce standard RAG vs. Disco-RAG on ASQA to validate the 3.1 EM point gain before attempting custom benchmarks.
  2. Ablate each component (RST tree, rhetorical graph, planner) on your domain data to identify which provides most signal.
  3. Test noise robustness by injecting 20-40% irrelevant chunks into retrieval results; compare degradation curves against standard RAG.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent does Disco-RAG generalize to low-resource languages or non-scientific genres where RST parsers may be less accurate?
- **Open Question 2:** Can the computational overhead be reduced through model distillation without sacrificing factual consistency gains?
- **Open Question 3:** How does RST compare to alternative discourse formalisms (e.g., PDTB) in scenarios involving ambiguous or conversational queries?
- **Open Question 4:** Is effectiveness dependent on the reasoning capacity of the backbone model, specifically does it function robustly with smaller models (<2B parameters)?

## Limitations
- The zero-shot RST parser's quality (nuclearity F1 of 63.1) may propagate errors through the entire pipeline.
- Computational overhead is significant: 2.2× tokens and 2s latency increase versus standard RAG.
- Reliance on a single discourse parser introduces a potential single point of failure.

## Confidence

- **High Confidence:** Core claim that discourse structures improve over flat text concatenation is well-supported by consistent performance gains across three benchmarks.
- **Medium Confidence:** Mechanism claims about how discourse structures enable reasoning are plausible but rely on indirect evidence.
- **Low Confidence:** Scalability claims to other domains and robustness to varying chunk sizes haven't been validated on truly long documents or domains with different discourse patterns.

## Next Checks

1. **Parser Quality Validation:** Measure discourse structure accuracy on domain-specific text using both automatic metrics and human evaluation. Compare performance with and without discourse structures when using a supervised parser versus the zero-shot LLM parser.

2. **Failure Mode Analysis:** Systematically test on queries where retrieved chunks contain contradictory evidence with varying levels of salience. Measure whether Disco-RAG consistently identifies and resolves contradictions versus when it fails.

3. **Scalability Testing:** Evaluate performance on documents exceeding 1024 tokens and across different discourse domains (legal, medical, technical) to test generalizability. Measure whether discourse structures remain useful with more complex argumentative structures.