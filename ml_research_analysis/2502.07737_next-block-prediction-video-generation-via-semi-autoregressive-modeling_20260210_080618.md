---
ver: rpa2
title: 'Next Block Prediction: Video Generation via Semi-Autoregressive Modeling'
arxiv_id: '2502.07737'
source_url: https://arxiv.org/abs/2502.07737
tags:
- video
- block
- generation
- arxiv
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Next-Block Prediction (NBP), a semi-autoregressive
  framework for video generation that improves upon traditional autoregressive modeling
  by predicting entire blocks of tokens simultaneously. The key idea is to shift the
  generation unit from individual tokens to equal-sized blocks (e.g., rows or frames),
  enabling bidirectional attention within each block for better spatial dependency
  modeling and reducing inference steps by a factor equal to block size.
---

# Next Block Prediction: Video Generation via Semi-Autoregressive Modeling

## Quick Facts
- **arXiv ID**: 2502.07737
- **Source URL**: https://arxiv.org/abs/2502.07737
- **Reference count**: 40
- **Primary result**: Introduces semi-autoregressive video generation that predicts blocks of tokens simultaneously, achieving 11x speedup and 4.4 FVD improvement over autoregressive models

## Executive Summary
This paper presents Next-Block Prediction (NBP), a semi-autoregressive framework for video generation that shifts from predicting individual tokens to predicting equal-sized blocks (rows or frames) simultaneously. By enabling bidirectional attention within each block while maintaining autoregressive ordering across blocks, NBP achieves both quality improvements and substantial inference speedup. The method generates 8.89 frames per second at 128x128 resolution while outperforming vanilla next-token prediction by 4.4 FVD points, with further improvements as model size scales from 700M to 3B parameters.

## Method Summary
NBP restructures video generation by treating blocks of tokens as the basic prediction unit rather than individual tokens. The framework processes equal-sized blocks (such as rows or entire frames) in parallel, with bidirectional attention allowed within each block to capture spatial dependencies more effectively. During inference, this approach reduces the number of generation steps by a factor equal to the block size, achieving significant speedup while maintaining or improving generation quality. The method maintains autoregressive characteristics across blocks to preserve temporal coherence while exploiting parallel computation within blocks for efficiency.

## Key Results
- Achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming vanilla next-token prediction by 4.4 points
- Generates 8.89 frames per second at 128x128 resolution, representing an 11x speedup over autoregressive baselines
- Demonstrates clear scaling benefits: increasing model size from 700M to 3B parameters reduces FVD to 55.3 on UCF101 and 19.5 on K600

## Why This Works (Mechanism)
The semi-autoregressive approach works by balancing the trade-off between generation efficiency and quality. By predicting blocks of tokens simultaneously, the method reduces the number of sequential steps required during inference, enabling parallel computation within each block. The bidirectional attention within blocks allows the model to capture richer spatial dependencies compared to the strictly causal attention of autoregressive models. Meanwhile, maintaining autoregressive ordering across blocks preserves the temporal coherence essential for video generation. This design effectively leverages modern parallel hardware while addressing the fundamental limitations of both fully autoregressive (slow) and fully non-autoregressive (quality degradation) approaches.

## Foundational Learning

**Bidirectional attention**: Allows tokens within a block to attend to each other in both directions, capturing full spatial context within that block. Needed to model complex spatial relationships that unidirectional attention cannot capture. Quick check: Verify that attention masks properly enable full connectivity within blocks while maintaining causal constraints across blocks.

**Block-wise parallel processing**: Processes multiple tokens simultaneously within predefined blocks rather than sequentially. Needed to achieve the stated 11x speedup by reducing the number of sequential generation steps. Quick check: Confirm that the speedup factor matches the inverse of the block size.

**Autoregressive across blocks**: Maintains sequential dependency from one block to the next while allowing parallelism within blocks. Needed to preserve temporal coherence and generation quality that fully non-autoregressive methods typically lose. Quick check: Ensure that the model cannot attend to future blocks during training and generation.

## Architecture Onboarding

**Component map**: Video tokens -> Block partitioner -> Block encoder (bidirectional attention) -> Block decoder (autoregressive across blocks) -> Output tokens

**Critical path**: The generation process flows from input tokens through block partitioning, where each block is processed with bidirectional attention to capture spatial context. The blocks are then generated sequentially (autoregressively) to maintain temporal coherence, with each block's generation conditioned on all previous blocks.

**Design tradeoffs**: The key tradeoff is between block size and quality. Larger blocks provide greater speedup and better spatial modeling but may reduce temporal precision and increase memory requirements. The method sacrifices fine-grained temporal control within blocks for computational efficiency and spatial context modeling.

**Failure signatures**: The approach may struggle with videos requiring precise temporal coordination between adjacent tokens (e.g., subtle motion transitions). Block boundaries could introduce artifacts if the model fails to properly condition across block boundaries. Very large block sizes might degrade quality by reducing the model's ability to capture fine temporal details.

**3 first experiments**:
1. Generate videos with varying block sizes (1x1, 2x2, 4x4) to observe the quality-speed tradeoff curve
2. Compare FVD scores on UCF101 using different block shapes (row-wise vs frame-wise) to identify optimal partitioning strategy
3. Test generation quality on longer sequences (32+ frames) to evaluate temporal consistency over extended durations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on FVD scores and inference speed without comparison to more recent video generation approaches like diffusion models
- Block-based approach may have limitations handling videos with complex temporal dynamics requiring fine-grained temporal precision
- Does not address how block size selection affects performance or generalization across different video domains

## Confidence
- **High confidence**: Demonstrated inference speedup and the basic claim that block-based prediction reduces generation steps by the block size factor
- **Medium confidence**: FVD score improvements, as these metrics can be sensitive to evaluation conditions and dataset characteristics
- **Medium confidence**: Scaling results, as experiments cover only three discrete model sizes and may not capture full scaling behavior

## Next Checks
1. Evaluate the model on longer video sequences (beyond 16 frames) to assess temporal consistency and scaling behavior for extended generation tasks

2. Compare against more recent non-autoregressive video generation approaches, including diffusion-based and latent diffusion models, to establish the relative position of this method in the current landscape

3. Conduct ablation studies varying block sizes systematically to identify optimal configurations for different video types and resolutions