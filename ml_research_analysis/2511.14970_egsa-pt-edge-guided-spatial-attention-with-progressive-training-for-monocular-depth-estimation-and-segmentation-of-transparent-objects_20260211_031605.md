---
ver: rpa2
title: EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular
  Depth Estimation and Segmentation of Transparent Objects
arxiv_id: '2511.14970'
source_url: https://arxiv.org/abs/2511.14970
tags:
- depth
- edges
- transparent
- segmentation
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of transparent object perception,
  specifically joint depth estimation and segmentation from monocular RGB images.
  Transparent objects violate the Lambertian reflection assumption, causing standard
  computer vision models to fail due to boundary ambiguities and cross-task interference
  between depth and segmentation predictions.
---

# EGSA-PT: Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects

## Quick Facts
- arXiv ID: 2511.14970
- Source URL: https://arxiv.org/abs/2511.14970
- Reference count: 28
- One-line primary result: EGSA improves depth accuracy over MODEST, especially in transparent regions, while maintaining segmentation performance.

## Executive Summary
This work tackles the challenge of transparent object perception, specifically joint depth estimation and segmentation from monocular RGB images. Transparent objects violate the Lambertian reflection assumption, causing standard computer vision models to fail due to boundary ambiguities and cross-task interference between depth and segmentation predictions. To address these issues, the authors introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism that replaces channel attention with boundary-aware spatial gating. EGSA uses edges—first derived from RGB images and later from predicted depth maps—to guide feature fusion and reduce destructive cross-task interactions. They also propose a progressive training strategy that transitions from RGB-based to depth-based edges in a self-training manner, eliminating the need for ground-truth depth during training. Evaluated on Syn-TODD and ClearPose benchmarks, EGSA consistently improves depth accuracy over the state-of-the-art method (MODEST), with the largest gains in transparent regions, while maintaining competitive segmentation performance. These results highlight edge-guided fusion as a robust approach for transparent object perception.

## Method Summary
The method employs a ViT-B backbone with a multi-scale decoder enhanced by Edge-Guided Spatial Attention (EGSA) blocks. EGSA computes spatial attention maps from depth and segmentation features, then modulates them with edge maps via a gating operation to reduce cross-task interference at boundaries. A progressive training strategy transitions from using Canny edges extracted from RGB images to edges derived from predicted depth maps, eliminating the need for ground-truth depth. The model is trained for 20 epochs with a batch size of 4, using Adam optimization with separate learning rates for encoder and decoder. The loss combines depth and segmentation terms, weighted to balance both tasks.

## Key Results
- EGSA improves depth accuracy over MODEST, with the largest gains in transparent regions.
- Progressive training from RGB to depth edges balances segmentation and depth performance.
- Removing channel attention while retaining spatial attention improves depth accuracy by 3.45 points.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Edge-guided spatial gating reduces destructive cross-task interference between depth and segmentation branches.
- **Mechanism:** The EGSA block computes spatial attention maps from depth (S_d) and segmentation (S_s) features, then modulates them with edge maps via `gated = S * (1 + β * E)`. This gates cross-branch feature mixing at boundaries, preventing segmentation from over-smoothing depth variation and depth from merging semantically distinct regions with similar depth values.
- **Core assumption:** Boundaries are loci of cross-task conflict; explicit edge signals can localize and suppress interference.
- **Evidence anchors:**
  - [abstract]: "EGSA... designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features."
  - [Section 1]: "segmentation may bias depth toward predicting nearly uniform values across an entire object... depth may bias segmentation to group spatially adjacent regions with similar depth values into a single semantic category."
  - [Table 3]: Removing channel attention while retaining spatial attention improves δ<1.05 from 65.28 to 68.73; adding edge guidance further improves to 68.38.
- **Break condition:** If edge maps are noisy or misaligned with true object boundaries (e.g., textured surfaces generating spurious edges), gating may suppress useful features or amplify noise.

### Mechanism 2
- **Claim:** Progressive transition from RGB-derived to depth-derived edges stabilizes training while enabling geometry-aware boundary guidance.
- **Mechanism:** During epochs t < T, edges E_k come from Canny on RGB. For t ≥ T, edges come from predicted depth. This bootstraps from stable texture edges, then shifts to task-relevant geometric edges as predictions improve.
- **Core assumption:** Early depth predictions are too unreliable for edge extraction; RGB edges provide a stable proxy that transfers.
- **Evidence anchors:**
  - [abstract]: "learning transitions from edges derived from RGB images to edges derived from predicted depth images... eliminates the need for ground-truth depth at training time."
  - [Section 3.3]: "Using predicted depth edges too early risks destabilizing training... relying solely on RGB-based edges risks overlooking task-specific geometric cues."
  - [Table 5]: RGB edges achieve δ<1.05=68.38; depth edges=66.32; progressive=68.49 (balanced).
- **Break condition:** If warm-up period T is too short, noisy depth edges destabilize fusion. If too long, the model overfits to RGB texture edges that don't generalize to geometry.

### Mechanism 3
- **Claim:** Replacing channel attention with spatial attention preserves more task-relevant features for dense prediction.
- **Mechanism:** Channel attention applies scalar weights per channel, potentially suppressing entire channels containing useful boundary or transparency cues. Spatial attention preserves all channels but modulates spatial locations, which EGSA further refines with edges.
- **Core assumption:** Task-relevant information for transparent objects is spatially localized (at boundaries), not channel-wise separable.
- **Evidence anchors:**
  - [Section 1]: "channel attention... may either down-weight channels containing useful information or reveal a redundancy across many channels, leading to inefficient use of model capacity."
  - [Table 3]: MODEST (CA+SA) δ<1.05=65.28; MODEST (SA only)=68.73—a 3.45 point gain from removing channel attention.
  - [corpus]: Limited direct corpus support; neighbor papers (M2H, DCIRNet) address multi-task learning but don't isolate channel vs. spatial attention effects.
- **Break condition:** If task-relevant features are strongly channel-correlated (not demonstrated here), removing channel attention could reduce selectivity.

## Foundational Learning

- **Concept: Spatial Attention Mechanisms**
  - **Why needed here:** EGSA builds on spatial attention (SAM), computing per-pixel importance via pooling + convolution. Without understanding how S_d and S_s are derived, the gating operation is opaque.
  - **Quick check question:** Can you sketch how a spatial attention map is computed from a C×H×W feature tensor?

- **Concept: Multi-Task Gradient Conflict**
  - **Why needed here:** The paper's core motivation is that joint depth/segmentation learning creates conflicting gradients at boundaries. Understanding PCGrad or GradNorm (cited) contextualizes why edge-guided fusion is an architectural alternative to gradient surgery.
  - **Quick check question:** Why might segmentation gradients push depth toward uniform predictions within object masks?

- **Concept: Edge Detection as Supervision Signal**
  - **Why needed here:** The progressive training strategy uses Canny edges as pseudo-labels, then self-generated depth edges. Understanding edge detection's sensitivity to thresholds and noise explains why warm-up is critical.
  - **Quick check question:** What happens to Canny edge maps on a highly textured transparent surface with reflections?

## Architecture Onboarding

- **Component map:** RGB -> Encoder (ViT-B or DINOv2) -> Reassemble module -> Multi-scale features (F^d_k, F^s_k) -> EGSA blocks -> Decoder (3 iterations) -> Depth and Segmentation heads

- **Critical path:**
  1. RGB → Encoder → multi-scale features (F^d_k, F^s_k).
  2. Edge extraction: Canny(RGB) or Canny(Predicted Depth) → E_k.
  3. EGSA: Compute S^d, S^s via SAM; apply gated modulation; cross-branch feature multiplication.
  4. Decoder: Fused features refined iteratively; predictions at each scale/iteration.
  5. Loss: L_depth (L2 + gradient L1 + normals L1) + L_seg (cross-entropy), weighted α=1, β=0.1.

- **Design tradeoffs:**
  - RGB edges vs. depth edges vs. progressive: RGB gives best depth accuracy but may include texture noise; depth edges improve segmentation but require stable predictions; progressive balances both.
  - Channel attention removal: Gains depth accuracy (Table 3), but may sacrifice some segmentation mIoU (92.84→91.94 baseline to SA-only).
  - Warm-up duration T: Not explicitly tuned in paper; Assumption: too short risks instability, too long limits geometric edge benefits.

- **Failure signatures:**
  - **Noisy edge maps** (e.g., textured glass): EGSA may gate incorrectly, producing fragmented depth or over-segmentation.
  - **Warm-up too short:** Depth-derived edges from early unstable predictions corrupt fusion.
  - **Excessive β values:** Edge signal overwhelms spatial attention, suppressing non-boundary regions.
  - **Segmentation degradation:** If cross-task fusion is too aggressive, segmentation mIoU drops (ClearPose: 90.98→84.61 with progressive).

- **First 3 experiments:**
  1. **Ablate edge source:** Train with RGB-only, depth-only, and progressive edges on Syn-TODD; compare δ<1.05 and mIoU to isolate edge contribution (replicates Table 5).
  2. **Vary warm-up T:** Test T ∈ {2, 5, 10, 15} epochs with 20-epoch training; monitor depth RMSE and edge map quality over time to find stability threshold.
  3. **Swap backbone:** Replace ViT-B with DINOv2; verify EGSA gains persist (Table 6 shows δ<1.05 improves to 71.0 with RGB edges). Check if channel attention removal remains beneficial with stronger features.

## Open Questions the Paper Calls Out
- **Question:** Can task-specific edge representations be learned end-to-end, rather than relying on hand-crafted detectors like Canny or simple gradient-based extraction from depth?
- **Basis in paper:** [explicit] The conclusion states: "...motivating future work on learning task-specific edge representations."
- **Why unresolved:** The current approach uses fixed edge extraction methods (Canny for RGB, gradients for depth), which may not capture the most informative boundaries for transparent object perception.
- **What evidence would resolve it:** A learned edge extractor trained jointly with the main task that outperforms Canny-based edges on both depth and segmentation metrics.

## Limitations
- Warm-up duration T for progressive training is not specified, risking instability if transition to depth edges occurs too early.
- Edge extraction from predicted depth maps lacks detailed parameterization (e.g., Canny thresholds), making faithful reproduction uncertain.
- Limited direct corpus evidence isolating channel vs. spatial attention effects; the benefit of removing channel attention is not robustly validated across domains.

## Confidence
- **High:** Core claim that edge-guided spatial attention improves depth accuracy over MODEST is well-supported by experiments on two benchmarks.
- **Medium:** Progressive training strategy's balance of RGB/depth edge benefits is demonstrated but warm-up duration T is unspecified.
- **Low:** Necessity of removing channel attention given sparse ablation in the literature and limited direct corpus support.

## Next Checks
1. **Ablate edge source:** Train with RGB-only, depth-only, and progressive edges on Syn-TODD; compare δ<1.05 and mIoU to isolate edge contribution.
2. **Vary warm-up T:** Test T ∈ {2, 5, 10, 15} epochs with 20-epoch training; monitor depth RMSE and edge map quality over time to find stability threshold.
3. **Swap backbone:** Replace ViT-B with DINOv2; verify EGSA gains persist and check if channel attention removal remains beneficial.