---
ver: rpa2
title: The BoBW Algorithms for Heavy-Tailed MDPs
arxiv_id: '2602.01295'
source_url: https://arxiv.org/abs/2602.01295
tags:
- regret
- skip
- heavy-tailed
- loss
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles the challenge of achieving best-of-both-worlds
  (BoBW) performance in episodic Markov Decision Processes (MDPs) with heavy-tailed
  feedback, where existing approaches struggle to adapt to both adversarial and stochastic
  environments. The authors develop two novel algorithms: HT-FTRL-OM for known transitions
  and HT-FTRL-UOB for unknown transitions.'
---

# The BoBW Algorithms for Heavy-Tailed MDPs

## Quick Facts
- arXiv ID: 2602.01295
- Source URL: https://arxiv.org/abs/2602.01295
- Reference count: 40
- The paper develops the first best-of-both-worlds algorithms for episodic MDPs with heavy-tailed feedback, achieving near-optimal regret in both adversarial and stochastic regimes.

## Executive Summary
This paper addresses the challenge of achieving best-of-both-worlds (BoBW) performance in episodic Markov Decision Processes (MDPs) with heavy-tailed feedback, where existing approaches struggle to adapt to both adversarial and stochastic environments. The authors develop two novel algorithms: HT-FTRL-OM for known transitions and HT-FTRL-UOB for unknown transitions. Both algorithms employ the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with a 1/Œ±-Tsallis entropy regularizer and introduce innovative skipping loss estimators to handle heavy-tailed losses. HT-FTRL-OM achieves nearly minimax-optimal instance-independent regret of √ï(T^{1/Œ±}) in adversarial regimes and logarithmic instance-dependent regret of O(log T) in stochastic regimes. Building on this, HT-FTRL-UOB extends the approach to unknown transitions by incorporating a pessimistic estimator with upper occupancy bounds, achieving a regret bound of √ï(T^{1/Œ±} + ‚àöT) in adversarial settings and O(log¬≤ T) in stochastic environments.

## Method Summary
The paper develops two algorithms for heavy-tailed MDPs: HT-FTRL-OM for known transitions and HT-FTRL-UOB for unknown transitions. Both use FTRL over occupancy measures with 1/Œ±-Tsallis entropy regularization. The key innovation is a skipping loss estimator that truncates extreme losses at adaptive thresholds while compensating for truncation bias through explicit bonus terms. For known transitions, HT-FTRL-OM achieves √ï(T^{1/Œ±}) adversarial regret and O(log T) stochastic regret. For unknown transitions, HT-FTRL-UOB adds Bernstein confidence sets and upper occupancy bounds to handle transition uncertainty, achieving √ï(T^{1/Œ±} + ‚àöT) adversarial regret and O(log¬≤ T) stochastic regret. The algorithms rely on a layer-structured MDP assumption and bounded Œ±-moment conditions for losses.

## Key Results
- HT-FTRL-OM achieves nearly minimax-optimal √ï(T^{1/Œ±}) instance-independent regret in adversarial regimes
- HT-FTRL-OM achieves logarithmic O(log T) instance-dependent regret in stochastic regimes
- HT-FTRL-UOB extends to unknown transitions with √ï(T^{1/Œ±} + ‚àöT) adversarial regret and O(log¬≤ T) stochastic regret
- First BoBW guarantees for heavy-tailed MDPs, bridging robust RL under heavy-tailed feedback and adaptive algorithm design

## Why This Works (Mechanism)

### Mechanism 1: Skipping Loss Estimator with Occupancy-Adaptive Thresholds
- Claim: Truncating extreme losses at adaptive thresholds enables stable FTRL updates while controlling bias through explicit bonus terms.
- Mechanism: For each (s,a), define threshold œÑ_t(s,a) = C¬∑œÉ¬∑t^(1/Œ±)¬∑x_t(s,a)^(1/Œ±). The skipping loss estimator ‚ÑìÃÇ^skip_t(s,a) = ‚Ñì_t(s,a)¬∑ùüô[|‚Ñì_t(s,a)| ‚â§ œÑ_t(s,a)]/x_t(s,a) combines truncation with importance sampling. A skipping bonus b^skip_t(s,a) = C^(1-Œ±)¬∑œÉ¬∑t^(1/Œ±-1)¬∑x_t(s,a)^(1/Œ±-1) compensates for truncation bias.
- Core assumption: Losses have bounded Œ±-moment: E[|‚Ñì|^Œ±] ‚â§ œÉ^Œ± for Œ± ‚àà (1,2]. Assumption: this moment bound holds uniformly across episodes.
- Evidence anchors:
  - [abstract] "novel skipping loss estimators tailored for heavy-tailed environments"
  - [section 4.1, Algorithm 1 lines 5-6] Explicit construction of œÑ_t and b^skip_t
  - [corpus] Heavy-tailed linear bandits (arXiv:2508.13679) uses similar truncation but lacks MDP occupancy structure
- Break condition: If x_t(s,a) becomes too small, the threshold œÑ_t may fail to truncate sufficient probability mass, and the importance weight 1/x_t(s,a) amplifies noise.

### Mechanism 2: Local Control of Heavy-Tailed Shifted Losses
- Claim: Shifting losses to advantage-function form enables second-moment bounds that vanish on optimal actions.
- Mechanism: Define shifted loss √™‚Ñì^shift_t(s,a) = Q^{P,œÄ_t}(s,a;√™‚Ñì^{skip}_t) - V^{P,œÄ_t}(s;√™‚Ñì^{skip}_t). Lemma 4.3 shows E[(√™‚Ñì^shift_t(s,a))^2] ‚â§ poly(H,S,A)¬∑(1-œÄ_t(a|s))¬∑œÉ¬≤¬∑t^(2/Œ±-1)¬∑x_t(s,a)^(2/Œ±-2). The (1-œÄ_t(a|s)) factor concentrates variance on suboptimal actions.
- Core assumption: Bounded Œ±-moment and finite horizon H. Assumption: layer structure (transitions only between adjacent layers) enables telescoping decomposition.
- Evidence anchors:
  - [abstract] "a local control mechanism for heavy-tailed shifted losses that overcomes the failure of standard bounded-loss analyses"
  - [section 4.2, Lemma 4.3] Explicit second-moment bound derivation
  - [corpus] No direct corpus match; standard bounded-loss FTRL-MDP results (Jin & Luo 2020) cannot handle heavy tails
- Break condition: If shifted losses accumulate across many layers without proper decomposition, pointwise bounds may degrade exponentially in H.

### Mechanism 3: Suboptimal Mass Propagation Principle
- Claim: Occupancy mass deviation between any policy and optimal policy is bounded by suboptimal action visitation, enabling logarithmic regret elimination of optimal-action terms.
- Mechanism: Lemma 4.4 states Œ£_s(œÅ^P,œÄ(s) - œÅ^P,œÄ‚Ä†(s))_+ ‚â§ H¬∑Œ£_{(s,a):a‚â†œÄ‚Ä†(s)} œÅ^P,œÄ(s,a). This propagates layer-wise, allowing penalty and skipping terms to exclude the optimal action ÀöœÄ(s) from summations‚Äîcritical for O(log T) stochastic regret.
- Core assumption: Deterministic optimal policy ÀöœÄ. Layer-structured MDP (transitions only to adjacent layers).
- Evidence anchors:
  - [abstract] "a local control mechanism for heavy-tailed shifted losses that overcomes the failure of standard bounded-loss analyses"
  - [section 4.2, Lemma 4.4 and proof in Appendix B.4] Full derivation with recursion
  - [corpus] Data-dependent MDP bounds (arXiv:2602.01903) uses similar exclusion principles for bounded losses
- Break condition: If optimal policy is not deterministic, or if transitions skip layers, the propagation inequality may not hold.

## Foundational Learning

- Concept: **Follow-the-Regularized-Leader (FTRL) over Occupancy Measures**
  - Why needed here: The algorithm optimizes over occupancy measure polytope Q(P) rather than policies directly, enabling convex optimization with regret decomposition.
  - Quick check question: Can you derive the occupancy measure œÅ^P,œÄ(s,a) from a given policy œÄ and transition P?

- Concept: **1/Œ±-Tsallis Entropy Regularization**
  - Why needed here: Standard log-barrier fails under heavy tails; Tsallis entropy with exponent 1/Œ± provides stability bounds matching T^(1/Œ±) rates.
  - Quick check question: What is the gradient of Œ®(x) = -Œ∑^(-1)¬∑Œ£ x(s,a)^(1/Œ±)?

- Concept: **Self-Bounding Regime (Definition 3.1)**
  - Why needed here: BoBW analysis requires a unified condition covering both stochastic and adversarially-corrupted environments; self-bounding condition Reg_T ‚â• E[Œ£ œÅ^P,œÄ_t¬∑Œî] - C_sb bridges both.
  - Quick check question: If losses are i.i.d. stochastic with gap Œî(s,a), verify that C_sb = 0 satisfies the constraint.

## Architecture Onboarding

- Component map:
```
Known Transitions:  HT-FTRL-OM
  ‚îî‚îÄ‚îÄ FTRL over Q(P) + 1/Œ±-Tsallis regularizer + Skipping loss estimator
Unknown Transitions: HT-FTRL-UOB  
  ‚îî‚îÄ‚îÄ Empirical model bP_i + Pessimistic estimator b‚Ñì_t + UOB u_t + Epoch doubling
```

- Critical path: (1) Compute occupancy x_t via convex optimization ‚Üí (2) Derive policy œÄ_t ‚Üí (3) Execute episode, collect trajectory ‚Üí (4) Update skipping estimator and (if unknown) transition model/epoch

- Design tradeoffs:
  - **Known vs Unknown transitions**: Unknown adds O(‚àöT) term and O(log¬≤T) stochastic regret due to transition uncertainty
  - **Threshold constant C**: Larger C truncates less (less bias, more variance); Eq. (16) sets C based on H,S,A,Œ±
  - **Learning rate Œ≤**: Controls stability vs adaptivity; Eq. (17) provides specific value

- Failure signatures:
  - **Exploding regret under heavy tails**: Check if threshold œÑ_t scales correctly with x_t^(1/Œ±); undersized thresholds cause instability
  - **O(T) regret in stochastic regime**: Check suboptimal mass propagation‚Äîoptimal action may not be excluded from penalty sums
  - **Unknown transitions failing**: Verify Bernstein confidence widths B_i and UOB bounds u_t ‚â• x_t

- First 3 experiments:
  1. **Validate adversarial rate**: Run HT-FTRL-OM on MDP with H=5, S=10, A=5, heavy-tailed losses (Pareto-distributed with Œ±=1.5), adversarial loss shifts; expect regret ~ T^(1/1.5) ‚âà T^0.67
  2. **Validate stochastic rate**: Same MDP with fixed loss distributions and gaps Œî(s,a) ‚àà [0.1, 1.0]; expect regret ~ log(T)
  3. **Unknown transition stress test**: Run HT-FTRL-UOB with sparse visitation (large S, small episodes per state); verify transition error terms don't dominate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the BoBW guarantees for HTMDPs be extended to the function approximation setting (e.g., linear MDPs)?
- Basis in paper: [explicit] The Conclusion explicitly states, "Future directions include... extending these guarantees to the function approximation setting."
- Why unresolved: The current algorithms (HT-FTRL-OM/UOB) rely on tabular occupancy measures and skipping estimators designed for discrete state-action pairs, which do not directly translate to continuous function spaces.
- What evidence would resolve it: An algorithm for linear MDPs with heavy-tailed feedback that maintains $O(T^{1/Œ±})$ adversarial regret and logarithmic stochastic regret.

### Open Question 2
- Question: Can the polynomial dependence on $H$, $S$, and $A$ in the regret bounds be tightened?
- Basis in paper: [explicit] The Conclusion identifies "tightening the bounds with respect to $S, A$ and $H$ factors" as a future direction.
- Why unresolved: The current analysis aggregates loose polynomial factors (e.g., poly($H, S, A$)) to manage the complex interaction between skipping errors, stability terms, and transition uncertainty.
- What evidence would resolve it: A refined analysis achieving optimal dependence on the horizon and state-space size, matching known lower bounds for heavy-tailed problems.

### Open Question 3
- Question: Can the instance-dependent regret in the unknown-transition stochastic setting be improved from $O(\log^2 T)$ to $O(\log T)$?
- Basis in paper: [inferred] Theorem 5.1 establishes a $O(\log^2 T)$ regret for unknown transitions in stochastic regimes, whereas the known transition case achieves the optimal $O(\log T)$.
- Why unresolved: The current regret decomposition and Upper Occupancy Bound (UOB) mechanism introduce an extra logarithmic factor when isolating transition uncertainty from heavy-tailed estimation errors.
- What evidence would resolve it: An analysis framework that controls transition errors without the additional logarithmic penalty typical of current UOB-based approaches.

## Limitations

- The analysis critically depends on the bounded Œ±-moment assumption for losses and the layer-structured MDP assumption.
- The skipping loss estimator introduces bias that requires careful calibration of the threshold parameter C.
- For unknown transitions, the regret bound includes both heavy-tailed and transition uncertainty terms, creating potential interference between the two sources of error.
- The proof structure relies heavily on the self-bounding regime condition, which may not capture all adversarial stochastic mixtures.

## Confidence

**High confidence** in the theoretical framework and main regret bounds given the assumptions. The FTRL-OM setup and Tsallis entropy regularization are well-established, and the skipping estimator construction follows from known heavy-tailed techniques. **Medium confidence** in the layer-structured MDP assumption's practical applicability - while enabling cleaner proofs, it may be restrictive. **Medium confidence** in the transition uncertainty analysis for HT-FTRL-UOB, as the interaction between heavy-tailed estimation and Bernstein-style confidence widths requires careful handling.

## Next Checks

1. **Threshold Calibration Test**: Run HT-FTRL-OM on a small MDP (H=3, S=5, A=5) with controlled Pareto losses (varying Œ± ‚àà [1.2, 1.8]) and systematically vary the threshold constant C. Plot empirical regret vs. C to verify the theoretical optimal scaling in Eq. 16.

2. **Optimal Policy Determinism Stress Test**: Modify the layer-structured assumption to allow non-deterministic optimal policies (œÄ‚Ä†(s) outputs distributions). Run both algorithms and measure degradation in stochastic regret to quantify dependence on deterministic optimality.

3. **Transition Uncertainty Isolation**: Create an MDP with known transitions but inject synthetic transition uncertainty into HT-FTRL-UOB (via perturbed empirical model). Compare regret growth to verify that the O(‚àöT) term from transitions is correctly bounded and doesn't interact adversely with heavy-tailed skipping.