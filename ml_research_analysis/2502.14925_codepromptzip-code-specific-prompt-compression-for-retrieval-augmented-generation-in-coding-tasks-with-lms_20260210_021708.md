---
ver: rpa2
title: 'CODEPROMPTZIP: Code-specific Prompt Compression for Retrieval-Augmented Generation
  in Coding Tasks with LMs'
arxiv_id: '2502.14925'
source_url: https://arxiv.org/abs/2502.14925
tags:
- code
- tokens
- prompt
- compression
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of prompt compression for Retrieval-Augmented
  Generation (RAG) in coding tasks, where lengthy prompts (often exceeding tens of
  thousands of tokens) create challenges due to limited context windows of language
  models and high computational costs. The core method idea involves a type-aware,
  priority-driven strategy that uses program analysis to identify token types in code
  and performs ablation analysis to rank their removal priorities based on their impact
  on task performance.
---

# CODEPROMPTZIP: Code-specific Prompt Compression for Retrieval-Augmented Generation in Coding Tasks with LMs

## Quick Facts
- arXiv ID: 2502.14925
- Source URL: https://arxiv.org/abs/2502.14925
- Reference count: 18
- Improves RAG coding tasks by 8.7-28.7% over baselines with compression ratio control

## Executive Summary
This paper introduces CodePromptZip, a method for compressing retrieved code examples in Retrieval-Augmented Generation (RAG) workflows for coding tasks. The approach uses type-aware priority ranking based on program analysis and ablation studies to identify which code tokens can be removed while maintaining task performance. A copy-enhanced sequence-to-sequence model (CodeT5) is trained to compress code while adhering to specified compression ratios, enabling better context window utilization and reduced computational costs in RAG pipelines.

## Method Summary
CodePromptZip compresses retrieved code examples through a type-aware, priority-driven strategy that leverages program analysis to categorize tokens by syntactic type (Symbol, Signature, Invocation, Identifier, Structure). Ablation analysis determines removal priorities based on performance impact, and a small language model with copy mechanism learns to produce compressed code while maintaining target compression ratios. The method handles both parsable and unparsable code, making it suitable for real-world RAG applications where retrieved examples may be incomplete.

## Key Results
- Outperforms entropy-based and distillation-based baselines by 23.4%, 28.7%, and 8.7% for Assertion Generation, Bugs2Fix, and Code Suggestion tasks respectively
- Maintains performance close to oracle levels while generalizing to unparsable code (42.0-42.1% Exact Match vs. Oracle N/A)
- Demonstrates strong generalization across different base language models (GPT-3.5, Gemini, CodeLlama)

## Why This Works (Mechanism)

### Mechanism 1: Type-Aware Priority Ranking for Structured Pruning
Code tokens are prioritized for removal based on their syntactic type, with task-specific hierarchies emerging from ablation analysis. Program analysis (AST parsing) categorizes tokens, and ablation studies measure performance degradation when each type is removed. Priority is computed as `Priority(T) = τ_code/T / dT`, with higher compression and lower degradation yielding higher removal priority.

### Mechanism 2: Copy-Enhanced Seq2Seq Compression with Ratio Conditioning
An encoder-decoder LM augmented with a copy mechanism produces extractive compressed code while adhering to specified compression ratios. CodeT5 encoder produces hidden states; decoder cross-attention yields attention distribution over source tokens. A copy module computes `p_gen` (generation vs. copy probability), with final distribution `P(y) = p_gen * P_vocab(y) + (1-p_gen) * P_copy(y)`.

### Mechanism 3: Learning-Based Compressor for Parsable and Unparsable Code
A learned compressor generalizes beyond AST-dependent methods, handling incomplete/unparsable code that fails parser-based approaches. The LM models probabilistic token relationships without requiring valid syntax, learning patterns from parsable code that transfer to unparsable examples with similar token distributions.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG) for Code
  - Why needed here: CodePromptZip compresses retrieved code examples in RAG workflows
  - Quick check question: Given a query "fix null pointer exception in method X," what type of examples would a RAG system retrieve, and where would they appear in the prompt?

- **Concept**: Sequence-to-Sequence Models with Attention and Copy Mechanisms
  - Why needed here: The compressor is built on CodeT5 (encoder-decoder) with cross-attention and a copy module
  - Quick check question: In a seq2seq model with copy attention, how does the decoder decide whether to generate a token from vocabulary vs. copy from input?

- **Concept**: Abstract Syntax Trees (AST) and Token Type Taxonomy
  - Why needed here: The priority ranking depends on parsing code into ASTs and classifying tokens by type
  - Quick check question: For the code `if (x > 0) { return x; }`, which tokens are "Structure" vs. "Identifier," and why does this distinction matter for compression priority?

## Architecture Onboarding

- **Component map**: Input code + ratio + task token -> Priority-Driven Generator (training only) -> CodeT5 + Copy LM -> Compressed code -> Base LM

- **Critical path**:
  1. For new task: define token taxonomy and run ablation to derive priority ranking
  2. Generate training pairs via Algorithm 1 across ratios 0.1-0.9
  3. Fine-tune CodeT5 with copy mechanism on constructed dataset
  4. At inference: feed code + ratio + task token -> compressor -> compressed example -> base LM

- **Design tradeoffs**:
  - Oracle vs. Learned Compressor: Oracle achieves best performance but requires parsable code and parser dependency; learned compressor trades ~4-5% performance for generalization
  - Fewer shots vs. higher compression: Fewer examples with lower τ_code outperform more examples with higher compression
  - Copy mechanism overhead: Adds complexity but critical for ratio control; without it, model struggles to hit target ratios

- **Failure signatures**:
  - Ratio drift: Actual compression ratio deviates significantly from specified τ_code
  - Performance collapse on unparsable code: Exact Match/CodeBleu drops sharply for incomplete code
  - Cross-task transfer failure: Compressor trained on one task performs poorly on another (~10-20% drops)

- **First 3 experiments**:
  1. Ablation by token type: Remove each token type from retrieved examples and measure task performance degradation to establish priority ranking
  2. Ratio control test: Train compressor with copy mechanism; at inference, set τ_code ∈ {0.1, 0.3, 0.5, 0.7} and compare actual vs. target ratios
  3. Unparsable code robustness: Truncate 1-3% of tokens from end of test examples to simulate incomplete code

## Open Questions the Paper Calls Out
None

## Limitations
- Cross-task generalization limits: Type-based priority rankings may not transfer to structurally different tasks like repository-level code search
- Language-specific dependencies: Implementation relies on JavaParser; priority rankings may not apply to languages with different syntactic structures
- Computational overhead considerations: No analysis of inference-time latency differences between compressed and uncompressed RAG workflows

## Confidence

**High confidence** in: The core mechanism of type-aware priority ranking using program analysis and ablation studies to determine token removal order, and experimental results showing 8.7-28.7% improvements over baselines.

**Medium confidence** in: The claim that CodePromptZip generalizes to unparsable code while maintaining performance close to oracle levels, though evaluation only tests truncation at example ends.

**Low confidence** in: The assertion that fewer examples with lower compression ratios are always preferable, as experiments only test up to 5-shot settings with limited ratio combinations.

## Next Checks

1. **Cross-task transfer validation**: Apply trained CodePromptZip models to a structurally different coding task (e.g., code summarization) and measure performance degradation to test type-based priority ranking generalization limits.

2. **Multi-language robustness test**: Implement token type classification and priority ranking for Python code using AST parsing, then train and evaluate CodePromptZip on the same three tasks using Python datasets to validate language portability claims.

3. **Latency overhead measurement**: Instrument the full RAG pipeline with and without CodePromptZip compression, measuring end-to-end inference latency for varying context lengths and compression ratios to quantify practical computational trade-offs.