---
ver: rpa2
title: 'CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning
  in SLMs'
arxiv_id: '2509.18536'
source_url: https://arxiv.org/abs/2509.18536
tags:
- reasoning
- ccqa
- response
- question
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited effectiveness of inference-time
  reasoning strategies for small language models (SLMs), where conventional approaches
  like self-correction and self-consistency often degrade performance due to SLMs'
  struggles with complex inputs and inconsistent outputs. The proposed method, Cycle-Consistency
  in Question Answering (CCQA), generates multiple solutions using chain-of-thought
  prompting, then applies backward question generation with a fine-tuned Flan-T5 model
  to create questions from each solution.
---

# CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs

## Quick Facts
- arXiv ID: 2509.18536
- Source URL: https://arxiv.org/abs/2509.18536
- Authors: Jin Young Kim; Ji Won Yoon
- Reference count: 10
- Primary result: CCQA improves SLM reasoning accuracy across arithmetic and commonsense benchmarks, achieving 69.60% on GSM8K and 38.74% on CommonSenseQA.

## Executive Summary
This paper addresses the limited effectiveness of inference-time reasoning strategies for small language models (SLMs), where conventional approaches like self-correction and self-consistency often degrade performance due to SLMs' struggles with complex inputs and inconsistent outputs. The proposed method, Cycle-Consistency in Question Answering (CCQA), generates multiple solutions using chain-of-thought prompting, then applies backward question generation with a fine-tuned Flan-T5 model to create questions from each solution. By measuring lexical (BLEU) and semantic (cosine similarity) similarity between generated and original questions, CCQA selects the solution whose regenerated question most closely matches the original. Extensive experiments across eight models (ranging from 135M to 3B parameters) and six benchmarks show CCQA consistently outperforms state-of-the-art reasoning methods, with notable improvements such as 69.60% accuracy on GSM8K (vs 53.83% for USC) and 38.74% on CommonSenseQA (vs 33.99% for USC). The approach establishes a new practical baseline for efficient reasoning in SLMs by requiring minimal computational overhead while substantially enhancing reasoning capabilities.

## Method Summary
CCQA operates by first generating N candidate solutions using chain-of-thought prompting on the target SLM. It then detects whether majority voting would be reliable (LCV condition: max answer frequency ≥ ceil(N/2)). If reliable, it returns the majority answer; if not, it invokes a fine-tuned Flan-T5-base model to generate questions from each solution's reasoning path and answer. The generated questions are compared to the original question using a weighted combination of BLEU and cosine similarity scores, and the solution with the highest similarity score is selected. The method requires minimal additional computation (only invoked under LCV conditions) while providing a more reliable quality signal than majority voting alone.

## Key Results
- CCQA achieves 69.60% accuracy on GSM8K compared to 53.83% for USC and 52.60% for SC
- On CommonSenseQA, CCQA reaches 38.74% accuracy versus 33.99% for USC and 29.33% for SC
- Under LCV conditions (36.46% of GSM8K problems), 80.85% of SC-selected answers are incorrect, demonstrating the need for verification
- CCQA shows consistent improvements across all tested model sizes (135M-3B parameters) and reasoning types

## Why This Works (Mechanism)

### Mechanism 1
Correct reasoning paths produce regenerated questions that are more similar to the original question than incorrect paths. Cycle-consistency verification operates on the principle that if a solution (reasoning path + answer) is correct, the reverse transformation (generating a question from that solution) should reconstruct something semantically close to the original input. This provides a quality signal that majority voting lacks. The core assumption is that the backward question generator produces discriminative questions—meaning questions from correct solutions are systematically more similar to originals than those from incorrect solutions. Break condition: If the question generator produces low-quality or non-discriminative questions (similar scores for correct and incorrect solutions), the mechanism degrades to random selection among candidates.

### Mechanism 2
Detecting low-confidence voting (LCV) conditions enables selective application of expensive verification only when majority voting would fail. LCV is triggered when no answer achieves majority (max frequency < ceil(N/2)). Under LCV, SLM outputs are highly varied, and the paper reports 80.85% of SC-selected answers in LCV cases are incorrect. CCQA only invokes backward question generation in these cases, reducing computational overhead. The core assumption is that LCV reliably identifies when voting is unreliable; non-LCV cases can trust majority vote. Break condition: If LCV threshold is poorly calibrated (too sensitive → unnecessary computation; too strict → misses unreliable voting), efficiency-accuracy tradeoff degrades.

### Mechanism 3
A specialized, fine-tuned backward question generator outperforms using the target SLM itself for question generation. SLMs struggle to generate accurate questions from reasoning paths and answers. The authors fine-tune Flan-T5-base (258M parameters) on reversed question-answer pairs from training sets. This auxiliary model is lightweight and produces higher-quality questions than unadapted SLMs. The core assumption is that the question generator's quality transfer across domains—trained on GSM8K/CSQA/StrategyQA, it generalizes to other benchmarks. Break condition: If domain shift is severe (e.g., specialized scientific reasoning), the fine-tuned generator may produce non-discriminative questions.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: CCQA builds on CoT to generate multiple candidate reasoning paths. Understanding how CoT elicits step-by-step reasoning is prerequisite.
  - Quick check question: Can you explain why CoT improves multi-step arithmetic but may not help with tasks requiring factual retrieval?

- **Concept: Self-Consistency (SC) and Majority Voting**
  - Why needed here: CCQA uses SC as its first pass; understanding when SC works (consistent outputs) and fails (high variance) is essential.
  - Quick check question: What happens to majority voting accuracy when five generated answers are [18, 24, 27, 35, 42]?

- **Concept: Cycle Consistency in Generative Models**
  - Why needed here: The paper adapts cycle consistency (A→B→A should approximate identity) from vision/translation to reasoning verification.
  - Quick check question: In machine translation, back-translation improves quality. What assumption does this share with CCQA's backward question generation?

## Architecture Onboarding

- **Component map:** Solution Generator (Target SLM) -> LCV Detector -> [if LCV] Backward Question Generator (Fine-tuned Flan-T5-base) -> Similarity Scorer -> Selector
- **Critical path:** Solution generation (N forward passes) → LCV check → [if LCV] backward question generation (N passes through Flan-T5) → similarity computation → selection. Non-LCV cases short-circuit to majority vote.
- **Design tradeoffs:**
  - N (number of solutions): Higher N increases coverage but computational cost; paper tests N=2–10, shows diminishing returns beyond N=5–6
  - α/β weights: 0.4 BLEU + 0.6 cosine optimized via grid search; may need retuning for different domains
  - Question generator size: Flan-T5-base (258M) chosen for efficiency-quality balance; larger models may improve quality but violate SLM deployment constraints
- **Failure signatures:**
  - Non-discriminative scores: All generated questions have similar similarity scores → selection becomes random
  - Generator hallucination: Flan-T5 generates questions unrelated to solution → low scores across all candidates
  - LCV miscalibration: Too many/few samples routed to verification → inefficiency or missed corrections
  - Domain mismatch: Question generator trained on math/commonsense fails on specialized domains
- **First 3 experiments:**
  1. Reproduce LCV statistics: On your target dataset, measure what fraction of samples trigger LCV and SC accuracy under LCV vs. non-LCV. This validates the problem exists in your domain.
  2. Ablate similarity metrics: Test BLEU-only, cosine-only, and combined scoring. The paper claims 0.4/0.6 split is optimal but does not show full ablation—verify on your data.
  3. Question generator quality audit: Sample 50 solutions (25 correct, 25 incorrect), generate backward questions, and manually assess whether correct-solution questions are genuinely more similar to originals. This validates the core assumption before full integration.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does CCQA generalize effectively to reasoning domains beyond arithmetic and commonsense, such as symbolic logic, causal reasoning, or multi-hop QA?
  - Basis in paper: [explicit] The Limitations section states: "our evaluation is limited to arithmetic and commonsense reasoning in English, leaving broader domains for future work."
  - Why unresolved: The current experiments cover only six benchmarks across two reasoning categories. It is unclear whether cycle-consistency via question regeneration is equally discriminative for tasks requiring different reasoning patterns.
  - What evidence would resolve it: Systematic evaluation on benchmarks such as ProofWriter (symbolic), WIQA (causal), or HotpotQA (multi-hop), comparing CCQA against SC and USC under identical experimental conditions.

- **Open Question 2:** Can CCQA benefit larger language models (e.g., 7B+ parameters), or does its advantage diminish as model scale increases?
  - Basis in paper: [inferred] The paper focuses exclusively on SLMs (135M–3B parameters) and attributes CCQA's gains to SLMs' high output variance and weak complex-input processing—limitations that may not apply to larger models.
  - Why unresolved: No experiments are reported for models beyond 3B, so it remains unknown whether cycle-consistency verification is redundant or even detrimental when the model already produces more consistent outputs.
  - What evidence would resolve it: Apply CCQA to Llama3.1-8B, Qwen2.5-7B, and Mistral-7B on the same six benchmarks, measuring gains over SC and analyzing whether LCV frequency decreases with scale.

- **Open Question 3:** How robust is CCQA to failures or systematic biases in the backward question generator?
  - Basis in paper: [explicit] The Limitations section notes: "the effectiveness of the proposed framework depends on the quality of the backward question generator; if the component produces low-quality questions, then CCQA's overall performance degrades."
  - Why unresolved: While the paper shows Flan-T5 outperforms Llama-1B and Qwen-0.5B for question generation, it does not analyze failure modes (e.g., numeric distortion, missing constraints) nor quantify how often generator errors lead to incorrect answer selection.
  - What evidence would resolve it: Error analysis of generated questions on a held-out test set, measuring (a) question quality independent of downstream accuracy, and (b) correlation between question-generation error types and final answer accuracy.

## Limitations
- The effectiveness depends critically on the quality of the backward question generator; if it produces low-quality questions, CCQA's performance degrades significantly
- The approach is limited to arithmetic and commonsense reasoning in English, with no evaluation on other reasoning domains or languages
- The method requires an additional fine-tuned model (Flan-T5-base), adding complexity to deployment despite being more efficient than self-correction approaches

## Confidence
- **High confidence:** CCQA improves SLM reasoning accuracy across multiple benchmarks when compared to baseline methods (USC, CoT). The experimental methodology and results are reproducible.
- **Medium confidence:** The backward question generator fine-tuned on Flan-T5-base is the optimal choice. While results show it outperforms target SLMs for question generation, ablation studies with other architectures are absent.
- **Low confidence:** The LCV detection mechanism consistently identifies unreliable voting conditions across diverse domains. The 80.85% incorrect SC selections under LCV is reported for GSM8K only, with no validation on other benchmarks.

## Next Checks
1. **Cross-domain LCV validation:** Measure LCV frequency and SC accuracy under LCV on CommonsenseQA and StrategyQA to verify the 80.85% failure rate generalizes beyond arithmetic problems.
2. **Ablation of question generator architecture:** Replace Flan-T5-base with a smaller model (e.g., Flan-T5-small) and larger models (e.g., Flan-T5-large) to test the claimed efficiency-quality tradeoff and determine if gains are architecture-dependent.
3. **Semantic similarity robustness:** Manually annotate 100 CCQA-selected solutions across benchmarks to verify that high similarity scores indeed correlate with correct reasoning, particularly for commonsense tasks where semantic similarity is less straightforward than arithmetic verification.