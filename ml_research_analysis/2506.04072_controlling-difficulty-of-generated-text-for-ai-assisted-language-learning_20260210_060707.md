---
ver: rpa2
title: Controlling Difficulty of Generated Text for AI-Assisted Language Learning
arxiv_id: '2506.04072'
source_url: https://arxiv.org/abs/2506.04072
tags:
- level
- language
- jlpt
- user
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of adapting large language models
  (LLMs) for beginner-level language learners, whose conversational abilities are
  often overwhelmed by the near-native complexity of typical LLM outputs. The authors
  explore modular, controllable generation techniques that do not require model fine-tuning,
  specifically using future discriminators (FUDGE) to bias model outputs toward simpler
  language.
---

# Controlling Difficulty of Generated Text for AI-Assisted Language Learning

## Quick Facts
- arXiv ID: 2506.04072
- Source URL: https://arxiv.org/abs/2506.04072
- Reference count: 40
- Primary result: FUDGE-controlled LLM outputs improve comprehensibility for beginner Japanese learners from 40.4% to 84.3%

## Executive Summary
This paper addresses the challenge of adapting large language models for beginner-level language learners, whose conversational abilities are often overwhelmed by the near-native complexity of typical LLM outputs. The authors explore modular, controllable generation techniques that do not require model fine-tuning, specifically using future discriminators (FUDGE) to bias model outputs toward simpler language. In a user study with beginner Japanese learners, FUDGE significantly improved output comprehensibility—from 40.4% to 84.3%—and maintained fluency. They also introduce a new automatic metric, Token Miss Rate (TMR), which correlates strongly with human judgments of understandability. The results demonstrate that modular control techniques can make LLMs effective conversation partners for absolute beginners without expensive retraining.

## Method Summary
The method uses FUDGE (Yang & Klein, 2021) to control LLM output difficulty during decoding without model fine-tuning. A lightweight ModernBERT predictor estimates P(difficulty_level | current_prefix) for each candidate next token, which is combined with base LLM logits via weighted sum: ŷ = λa + (1-λ)x. The predictor is trained on prefix-level judgments from the jpWaC-L corpus with JLPT vocabulary annotations. During inference, top-k=50 sampling is used with λ∈[0.25,0.9] to balance difficulty control and fluency. Output difficulty is automatically assessed using Token Miss Rate (TMR), calculated from word-level tokenization with Sudachi and vocabulary lookup against JLPT level bins.

## Key Results
- FUDGE significantly improves output comprehensibility for beginner Japanese learners (40.4% to 84.3%)
- Prompting alone fails to control output difficulty effectively, showing only modest TMR improvements (~1.5%)
- Token Miss Rate correlates strongly with human judgments of understandability (ρ = 0.78)
- Over a 6-turn conversation, FUDGE maintains lower Token Miss Rate (4.4%) compared to uncontrolled models (8.3%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Future discriminators enable real-time difficulty control during decoding without modifying the base LLM.
- Mechanism: A lightweight predictor model estimates P(difficulty_level | current_prefix) for each candidate next token. This probability is combined with the base LLM's logits via weighted sum: ŷ = λa + (1-λ)x, where λ controls control strength. The predictor is trained on prefix-level judgments rather than complete sequences, enabling it to estimate "future" difficulty from partial context.
- Core assumption: Difficulty can be reliably predicted from sentence prefixes before the full utterance is generated.
- Evidence anchors:
  - [abstract]: "the use of future discriminators (Yang and Klein, 2021) significantly improves output comprehensibility (from 40.4% to 84.3%)"
  - [Section 3.2]: "FUDGE operates by approximating P(xi|x1:i−1, a) ∝ P(a|x1:i) · P(xi|x1:i−1)"
  - [corpus]: Related work "Controlling Language Difficulty in Dialogues with Linguistic Features" addresses similar difficulty adaptation but uses different control methods
- Break condition: If the predictor cannot reliably estimate difficulty from prefixes (low accuracy on held-out prefix data), the approach degrades to random token selection bias.

### Mechanism 2
- Claim: Prompting alone fails because LLMs exhibit "alignment drift" during multi-turn conversations.
- Mechanism: Even with explicit difficulty instructions, LLMs gradually produce more complex language over successive turns. Without external constraint, the model's prior training on fluent, sophisticated text dominates over the temporary instruction signal.
- Core assumption: Instruction-following for stylistic constraints is weaker than the model's learned prior distribution.
- Evidence anchors:
  - [abstract]: "while prompting alone fails to control output difficulty"
  - [Section 5.2]: "prompting results in only modest improvements in Token Miss Rate (roughly −1.5%)"
  - [Section 6.4]: "uncontrolled models suffer from 'alignment drift'—gradually straying more and more from the target difficulty"
  - [corpus]: Weak direct corpus support for alignment drift; this appears to be the paper's contribution
- Break condition: If future LLMs have stronger instruction-following for difficulty, the gap between prompting and FUDGE may narrow.

### Mechanism 3
- Claim: Token Miss Rate (TMR) serves as a reliable automatic proxy for human comprehensibility judgments.
- Mechanism: TMR = count_above / total_tokens, where tokens are mapped to JLPT levels via vocabulary lookup after word-level tokenization (Sudachi). This provides granular difficulty assessment at the utterance level.
- Core assumption: Vocabulary level is the primary driver of comprehensibility for beginner learners.
- Evidence anchors:
  - [abstract]: "Token Miss Rate (TMR), that quantifies the proportion of incomprehensible tokens per utterance and correlates strongly with human judgments"
  - [Section 4.1]: "This intuitively measures the percentage of the output that is comprehensible"
  - [Section 6.4]: "high TMR is strongly correlated with a round being not comprehensible to human subjects (ρ = 0.78)"
  - [corpus]: No corpus papers validate TMR specifically; this is a novel contribution
- Break condition: For languages without standardized vocabulary level mappings (like JLPT), this metric requires constructing alternative level annotations.

## Foundational Learning

- Concept: **Controllable Text Generation via Plug-in Modules**
  - Why needed here: Understanding that control can be applied externally during decoding rather than requiring model fine-tuning is the conceptual foundation for FUDGE.
  - Quick check question: Can you explain why P(x_i | x_{1:i-1}, a) differs from P(x_i | x_{1:i-1}) and how a discriminator bridges this gap?

- Concept: **CEFR/JLPT Proficiency Levels**
  - Why needed here: The entire evaluation framework depends on mapping learner proficiency (A1-C1) to vocabulary difficulty bins. Without this, TMR cannot be computed.
  - Quick check question: What JLPT level corresponds to CEFR A1, and why does this mapping matter for training the predictor?

- Concept: **Top-k Sampling with Logit Manipulation**
  - Why needed here: FUDGE operates on the truncated next-token distribution (top-k=50) rather than the full vocabulary, making inference tractable.
  - Quick check question: Why does FUDGE only modify logits for the top-k candidates rather than all tokens in the vocabulary?

## Architecture Onboarding

- Component map:
  - **Base LLM** (Qwen2.5-72B or GPT-4) -> **Predictor Model** (ModernBERT fine-tuned) -> **Logit Combiner** (ŷ = λa + (1-λ)x) -> **Tokenizer** (Sudachi mode C) -> **Vocabulary Bins** (JLPT N5-N1)

- Critical path:
  1. Load base LLM and predictor model (requires 5+ GPUs for 72B model)
  2. For each decoding step: generate top-k logits from LLM → run predictor on each candidate prefix → combine logits → sample next token
  3. Compute TMR post-hoc using Sudachi tokenization + vocabulary lookup

- Design tradeoffs:
  - Higher λ (e.g., 0.9) improves difficulty control but may reduce fluency; paper found degradation above λ=0.9
  - Overgenerate method (k=5 candidates, rerank by TMR) is more expensive and fails when no good candidates exist
  - Predictor trained on jpWaC may not generalize to domains outside web text

- Failure signatures:
  - High perplexity with low TMR: over-constrained output (λ too high)
  - High TMR despite FUDGE: predictor accuracy is low, or vocabulary bins don't cover domain tokens
  - "Undetected tokens" counted as understood (per Section 4.1) may inflate comprehension estimates

- First 3 experiments:
  1. **Baseline validation**: Run the "self-chat" pipeline (Section 5.1) comparing Baseline Prompt vs. FUDGE (λ=0.8) on 15 dialogues per level; expect ~1.5% TMR reduction for prompting vs. ~4% for FUDGE
  2. **Predictor accuracy test**: Evaluate the ModernBERT predictor on held-out jpWaC sentences; if prefix-level accuracy <70%, retrain with more data or adjust architecture
  3. **λ sweep**: Test λ ∈ {0.25, 0.5, 0.8, 0.9} measuring TMR, perplexity, and trigram diversity; select λ that balances TMR <15% with perplexity within 10% of baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: How can difficulty control be maintained consistently over longer, multi-turn conversations beyond the 6-turn sessions tested?
  - Basis in paper: [explicit] The authors state: "Future work should seek to further understand how these metrics change over time and how to ensure consistency in difficulty control over long conversations."
  - Why unresolved: The paper only evaluated 6-turn conversations and observed "alignment drift" in uncontrolled models, but did not test whether FUDGE maintains control over extended dialogues (e.g., 20+ turns).
  - What evidence would resolve it: A longitudinal user study measuring TMR and comprehension rates across conversations of varying lengths, comparing FUDGE's stability against baseline methods.

- **Open Question 2**: Does the FUDGE-based difficulty control approach generalize effectively to languages other than Japanese?
  - Basis in paper: [inferred] The entire study is conducted on Japanese, using Japanese-specific resources (JLPT levels, Sudachi tokenizer, JReadability). No experiments or claims are made about other languages.
  - Why unresolved: Japanese has unique linguistic features (agglutination, lack of word boundaries) that affect tokenization and difficulty assessment. The approach may not transfer directly to languages with different morphological or syntactic properties.
  - What evidence would resolve it: Replication studies applying FUDGE with language-appropriate difficulty classifiers and tokenizers to languages like English, Spanish, or Mandarin, with comparable user studies.

- **Open Question 3**: How should Token Miss Rate handle undetected or unbinned tokens to avoid skewing comprehensibility estimates?
  - Basis in paper: [explicit] The authors acknowledge: "This calculation implicitly classifies tokens that were unable to be binned to a difficulty level as being understood by the learner. This can potentially skew the score for utterances with a high proportion of undetected tokens."
  - Why unresolved: The current TMR formula treats any token not found in JLPT vocabulary bins as comprehensible, which may overestimate understandability when unknown proper nouns, colloquialisms, or new vocabulary appear.
  - What evidence would resolve it: Comparative analysis of TMR variants that penalize or flag undetected tokens, correlated with human judgments on utterances containing high proportions of out-of-vocabulary terms.

- **Open Question 4**: Can modular difficulty control techniques effectively support intermediate-to-advanced learners (CEFR B1–C2), not just absolute beginners?
  - Basis in paper: [inferred] The study explicitly targeted A1–A2 learners, noting that previous work (Tyen et al., 2024) underrepresented beginners. The authors do not evaluate whether the same approach scales to higher proficiency levels where nuance and style matter more.
  - Why unresolved: Advanced learners may require control over subtler dimensions (register, idiomaticity, domain-specific vocabulary) that binary "above/below level" classification does not capture.
  - What evidence would resolve it: User studies with B1–C2 learners evaluating whether FUDGE-controlled outputs appropriately challenge rather than oversimplify, using metrics tailored to advanced proficiency.

## Limitations

- The approach relies heavily on Japanese-specific resources (JLPT vocabulary levels, Sudachi tokenizer) that may not transfer directly to other languages
- The ModernBERT predictor is trained on web text from jpWaC-L corpus and may not generalize well to specialized domains
- The study only evaluated 6-turn conversations, leaving uncertainty about how difficulty control performs over longer, more complex interactions

## Confidence

**High Confidence**: The FUDGE implementation effectively reduces Token Miss Rate and improves human comprehensibility ratings. The ablation showing prompt-only methods fail while FUDGE succeeds is robust, supported by both automatic metrics (TMR reduction from ~13% to ~4%) and human evaluations (comprehensibility improvement from 40.4% to 84.3%).

**Medium Confidence**: The Token Miss Rate metric as a proxy for comprehensibility. While correlation with human judgments is strong (ρ = 0.78), the metric assumes vocabulary level is the primary comprehensibility driver. This may not hold for languages where grammatical complexity or idiomatic expressions are equally important.

**Medium Confidence**: The claim that modular control techniques eliminate the need for fine-tuning. While FUDGE achieves strong results without model retraining, the paper doesn't compare against lighter fine-tuning approaches that might offer better fluency-difficulty tradeoffs.

## Next Checks

1. **Cross-linguistic validation**: Apply the FUDGE framework to another language with CEFR-aligned vocabulary resources (e.g., English or Spanish) to verify the approach generalizes beyond Japanese and JLPT-specific mappings.

2. **Long-term conversation stability**: Extend the multi-turn conversation evaluation to 30+ turns per dialogue to better characterize alignment drift and test whether FUDGE maintains difficulty control across extended interactions.

3. **Domain adaptation stress test**: Evaluate FUDGE on non-web text domains (e.g., academic papers, technical documentation, or domain-specific dialogues) to measure predictor robustness when vocabulary distributions shift significantly from training data.