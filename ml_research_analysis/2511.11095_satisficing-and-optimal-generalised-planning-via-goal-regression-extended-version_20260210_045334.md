---
ver: rpa2
title: Satisficing and Optimal Generalised Planning via Goal Regression (Extended
  Version)
arxiv_id: '2511.11095'
source_url: https://arxiv.org/abs/2511.11095
tags:
- planning
- problem
- plan
- goal
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MOOSE, a novel generalised planning approach\
  \ that leverages goal regression and problem relaxation to synthesize and execute\
  \ first-order Condition\u2192Actions rules. The method solves training problems\
  \ optimally for individual goals, performs goal regression, and lifts the resulting\
  \ macro-actions into reusable rules."
---

# Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)

## Quick Facts
- arXiv ID: 2511.11095
- Source URL: https://arxiv.org/abs/2511.11095
- Reference count: 40
- Primary result: Novel generalized planning approach that synthesizes and executes Condition→Actions rules via goal regression, achieving superior coverage and solution quality on classical, numeric, and optimal planning domains

## Executive Summary
This paper introduces MOOSE, a novel generalized planning approach that leverages goal regression and problem relaxation to synthesize and execute first-order Condition→Actions rules. The method solves training problems optimally for individual goals, performs goal regression, and lifts the resulting macro-actions into reusable rules. Theoretical results prove MOOSE's soundness and completeness under True Goal Independence (TGI) and its optimality under both TGI and Optimal Goal Independence (OGI) assumptions. Experiments on classical, numeric, and optimal planning domains show MOOSE outperforms state-of-the-art planners in synthesis cost, planning coverage, and solution quality.

## Method Summary
MOOSE is a generalized planning approach that synthesizes first-order Condition→Actions rules from optimal solutions to training problems. The method decomposes conjunctive goals into singleton goals, solves them optimally using A* with LM-cut heuristic, performs goal regression to identify relevant state preconditions, and lifts the resulting macro-actions into reusable rules. During execution, the system instantiates applicable rules by querying a SQLite database with the current state and goals, enabling fast retrieval without replanning. The approach is sound and complete under True Goal Independence (TGI) and optimal under Optimal Goal Independence (OGI) assumptions.

## Key Results
- Solves 719.6/720 classical test problems with significantly less memory usage and time compared to baselines
- Achieves superior coverage and solution quality across classical, numeric, and optimal planning domains
- Provides formal guarantees of soundness, completeness, and optimality under specific independence assumptions
- Demonstrates practical instantiation speed through database-based rule querying

## Why This Works (Mechanism)

### Mechanism 1: Relevant State Abstraction via Goal Regression
MOOSE isolates minimal necessary preconditions by computing the preimage of a goal through an optimal plan. For each step, it computes regr(g, a) = (g \setminus add(a)) ∪ pre(a), filtering out irrelevant state atoms and identifying exactly which facts must be true before an action to achieve the subsequent goal.

### Mechanism 2: Complexity Reduction via Goal Independence (TGI)
The approach maintains tractability by decomposing conjunctive goals into singleton goals and solving them greedily. Under True Goal Independence, this relaxation avoids exponential cost while preserving completeness, allowing the system to solve each goal optimally in sequence.

### Mechanism 3: Instantiation via Database Querying
MOOSE achieves fast instantiation by treating planning states as databases and lifted rules as SQL-like queries. The system queries for a grounding f:var(r)→P[O] such that stateCond(r)|_f ⊆ s, enabling constant-time retrieval of applicable macro-actions without search.

## Foundational Learning

- **STRIPS Planning & Grounding**: Understanding how ground actions derive from lifted schemas is necessary to comprehend the "lifting" process in reverse. Quick check: Given a ground action `move(robot1, locA, locB)`, can you identify the lifted schema and the variable bindings?

- **Goal Regression (Preimage Computation)**: This is the core synthesis engine. One must understand how to work backward from a goal state to identify necessary predecessor states. Quick check: If goal g = {p} and action a has pre(a)={q} and add(a)={p}, what is regr(g, a)?

- **First-Order Logic (Variable Binding)**: The generalization capability depends on replacing specific objects with variables to create general rules. Quick check: How does substituting a constant `cake` with a variable `?obj` change the applicability of a rule `holding(?obj)`?

## Architecture Onboarding

- **Component map**: Synthesizer (Python with Fast Downward) -> SQLite database (rule storage) -> Executor (logic engine for instantiation) -> Optimizer (SYMK/Scorpion interface)

- **Critical path**: Data Prep → Decomposition → Search (A* with LM-cut) → Regression → Lifting → Instantiation

- **Design tradeoffs**: Coverage vs. Optimality (TGI assumption), Memory vs. Synthesis Cost (SQLite storage), Macro-actions vs. search depth (reduces search but increases grounding time)

- **Failure signatures**: "Deadend" in Synthesis (planner fails for singleton goal), Instantiation Cycle (Algorithm 3 detects conflict), Suboptimal Plans (learned goal ordering not optimal)

- **First 3 experiments**: 
  1. Run "Greedy Algorithm" on Logistics domain to check TGI validity
  2. Synthesize rules on 2-package problem and manually inspect Figure 2 lifted rules
  3. Compare instantiation time against LAMA on training distribution to verify amortization benefit

## Open Questions the Paper Calls Out
None

## Limitations
- TGI assumption is critical but not systematically validated across all domains
- Optimal planner's runtime during synthesis is not reported, making practical cost unclear
- No ablation studies provided to quantify individual contributions of each mechanism

## Confidence

- **High**: Soundness and completeness under TGI (Proposition 11, Theorem 12) - formally proven
- **Medium**: Optimality under OGI (Theorem 13) - proof relies on additional assumption with limited validation
- **Medium**: Practical performance gains - extensive empirical results but potential selection bias
- **Low**: Memory efficiency claims - no synthesis memory profiling provided

## Next Checks

1. Conduct systematic TGI analysis: For each domain tested, explicitly identify whether goals are truly independent and document which specific goal pairs violate TGI when the approach fails.

2. Perform ablation study: Compare MOOSE against variants that (a) skip goal regression, (b) skip lifting, and (c) use naive action selection instead of database querying to isolate each mechanism's contribution.

3. Profile synthesis cost: Measure and report the time and memory usage of the optimal planner during the synthesis phase across all domains to provide complete computational complexity analysis.