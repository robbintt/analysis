---
ver: rpa2
title: 'On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language
  Learning'''''
arxiv_id: '2509.06535'
source_url: https://arxiv.org/abs/2509.06535
tags:
- fairclip
- clip
- fairness
- clip-ft
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated the reproducibility of FairCLIP, a method
  proposed to improve group fairness in CLIP by minimizing image-text similarity score
  disparities across sensitive groups using the Sinkhorn distance. The experiments
  were conducted on two datasets: Harvard-FairVLMed for zero-shot glaucoma classification
  and FairFace for gender prediction.'
---

# On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''

## Quick Facts
- **arXiv ID**: 2509.06535
- **Source URL**: https://arxiv.org/abs/2509.06535
- **Reference count**: 26
- **Primary result**: FairCLIP reduces Sinkhorn distances between group and population distributions but does not improve fairness metrics or performance compared to standard fine-tuning.

## Executive Summary
This reproducibility study investigated FairCLIP, a method proposed to improve group fairness in CLIP by minimizing image-text similarity score disparities across sensitive groups using the Sinkhorn distance. The authors attempted to reproduce the original results on two datasets: Harvard-FairVLMed for zero-shot glaucoma classification and FairFace for gender prediction. Despite successfully reproducing the reduction in Sinkhorn distances between group and population distributions, they found that FairCLIP did not significantly improve fairness metrics (DPD, DEOdds) or performance (AUC) compared to standard fine-tuning. The study revealed implementation discrepancies between the original paper and code, including model selection on the test set and incorrect similarity score normalization, which may have contributed to the original paper's positive findings.

## Method Summary
The study reproduced FairCLIP by fine-tuning CLIP on Harvard-FairVLMed and FairFace datasets using the original paper's methodology. The approach adds a fairness regularizer to the standard CLIP contrastive loss, where the regularizer is the Sinkhorn distance between image-text similarity score distributions for each sensitive group and the overall population. The authors implemented both the original code's version (which normalizes similarity scores) and an "aligned" version that follows the paper's mathematical description. They evaluated the method across four sensitive attributes (race, gender, ethnicity, language) using AUC, DPD, DEOdds, and Sinkhorn distances, running three trials per configuration. The study also tested FairCLIP+ for multi-attribute fairness and compared against CLIP-FT baseline and linear probe baselines.

## Key Results
- FairCLIP effectively reduces Sinkhorn distances between group and population distributions (from ~3000 to ~30 for some groups), but this does not translate to improved fairness metrics or performance.
- Both the original implementation (with test-set model selection and score normalization) and the corrected implementation showed no significant improvement over CLIP-FT in AUC, DPD, or DEOdds.
- High variance across runs (often exceeding mean differences) suggests instability in the method's effectiveness.
- FairCLIP+ (multi-attribute extension) showed slight improvements over single-attribute FairCLIP but remained worse than CLIP-FT baseline.

## Why This Works (Mechanism)

### Mechanism 1: Sinkhorn Distance Regularization for Distribution Matching
- **Claim**: Minimizing the Sinkhorn distance between population and subgroup distributions of image-text similarity scores reduces fairness disparities.
- **Mechanism**: The method computes similarity scores ⟨x_I, x_T⟩ for image-text pairs, estimates distributions per sensitive group, and adds a regularization term L_Fair = Σ d(D_batch, D_group) to the contrastive loss, where d is the Sinkhorn distance (an optimal transport metric).
- **Core assumption**: Smaller distributional distance between groups directly correlates with improved fairness metrics (DPD, DEOdds) and maintained performance.
- **Evidence anchors**: Sinkhorn distances decreased significantly (Table 5: FairCLIP reduces distances from ~3000 to ~30 for some groups) but DPD and DEOdds showed no consistent improvement.
- **Break condition**: The paper demonstrates this mechanism fails—Sinkhorn distances decreased significantly but fairness metrics showed no improvement.

### Mechanism 2: Contrastive Fine-tuning with Fairness-Penalty Trade-off
- **Claim**: The hyperparameter λ controls a tunable trade-off between fairness (regularizer) and task performance (CLIP loss).
- **Mechanism**: L_FairCLIP = L_CLIP + λ·L_Fair, where λ balances the standard contrastive objective against the fairness regularizer.
- **Core assumption**: There exists a λ regime where both fairness and performance improve simultaneously, or at least where the fairness gain outweighs performance loss.
- **Evidence anchors**: AUC consistently lower than CLIP-FT baseline across tested λ values; high variance made stable trade-offs difficult to identify.
- **Break condition**: No λ found where trade-off is favorable; performance degraded while fairness metrics remained unchanged.

### Mechanism 3: Multi-Attribute Generalization via Weighted Aggregation
- **Claim**: Extending fairness to multiple sensitive attributes simultaneously improves cross-attribute fairness.
- **Mechanism**: L_Fair+ = Σ_A w_A · L_Fair^A, where weights sum to 1, allowing joint optimization across attributes like race and gender.
- **Core assumption**: Attributes are sufficiently independent that joint optimization doesn't cause interference, and imbalanced groups can be handled via weighting.
- **Evidence anchors**: FairCLIP+ showed slight improvements over single-attribute FairCLIP but remained worse than CLIP-FT; high standard deviations made differences statistically weak.
- **Break condition**: Multi-attribute optimization did not provide clear benefits over single-attribute approach.

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - **Why needed here**: FairCLIP builds on pre-trained CLIP; understanding image-text similarity scoring and zero-shot classification is essential to interpret the regularizer's effect on the embedding space.
  - **Quick check question**: Can you explain why the diagonal elements of I·T^⊤ represent matched image-text similarity scores?

- **Concept: Group Fairness Metrics (DPD, DEOdds, ES-AUC)**
  - **Why needed here**: The entire evaluation hinges on whether these metrics improve; DPD measures prediction rate parity, DEOdds measures equal true/false positive rates across groups.
  - **Quick check question**: If a model achieves AUC=70% for all groups but has DPD=15%, is it "fair" according to these definitions?

- **Concept: Optimal Transport and Sinkhorn Distance**
  - **Why needed here**: The FairCLIP regularizer uses Sinkhorn distance; understanding it as an entropy-regularized Wasserstein distance explains why it measures distributional alignment differently than simpler metrics like MMD.
  - **Quick check question**: Why might minimizing Sinkhorn distance not correlate with DPD reduction, even though both measure group-level differences?

## Architecture Onboarding

- **Component map**: Image Encoder (ViT-B/16 or ViT-L/14) → Image Features x_I → Similarity Computation: diag(I·T^⊤) → Group-wise Distribution Estimation → Sinkhorn Distance Computation → L_Fair → Combined Loss: L_CLIP + λ·L_Fair → Backpropagation

- **Critical path**:
  1. Load pre-trained CLIP checkpoint (official OpenAI weights)
  2. Construct prompts from clinical notes (GPT-4 summaries) or custom templates
  3. Compute per-batch similarity scores and group labels
  4. Calculate Sinkhorn distance using Geomloss library
  5. Backpropagate combined loss; select model on validation set (not test set—this was an original implementation bug)

- **Design tradeoffs**:
  - λ=10^-7 (original paper) vs λ=10^-5 (FairCLIP+): Larger λ enforces stronger fairness constraint but degrades AUC more severely
  - Sinkhorn vs MMD distance: Paper tested both; MMD showed no distance reduction effect even with large λ
  - ViT-B/16 vs ViT-L/14: Smaller model showed slightly better fairness gains relative to baseline; larger model retained more performance

- **Failure signatures**:
  - **Implementation mismatch**: Official code divides similarity scores by their sum before computing Sinkhorn distance—this is NOT in the paper's math. The "aligned" implementation (A-FairCLIP) corrects this but still shows no fairness gains.
  - **Test-set model selection**: Original implementation selected best checkpoint on test data; corrected version uses validation set and shows worse results.
  - **High variance**: Standard deviations often exceed mean differences, suggesting seed-dependent instability.

- **First 3 experiments**:
  1. **Reproduce CLIP-FT baseline**: Fine-tune CLIP on Harvard-FairVLMed with standard contrastive loss only; verify AUC, DPD, DEOdds match baseline (AUC~70%, high variance expected).
  2. **Verify Sinkhorn distance reduction**: Train FairCLIP with λ=10^-7 on single attribute (e.g., gender); confirm pattern—distances should drop from ~100s to <10.
  3. **Correlate distance with fairness**: Plot Sinkhorn distance vs DPD/DEOdds across training epochs; test the core assumption that these should correlate negatively.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why does minimizing the Sinkhorn distance between group and population distributions fail to translate into improved fairness metrics (DPD, DEOdds) or performance?
- **Basis in paper**: The Discussion section states that results suggest "minimizing the (Sinkhorn) distance of the subgroups and the population is not necessarily positively related to the performance nor fairness."
- **Why unresolved**: The authors empirically demonstrate the lack of correlation but do not provide a theoretical explanation for why distribution matching in this specific feature space fails to impact downstream classification outcomes.
- **What evidence would resolve it**: A theoretical analysis connecting similarity score distributions to classification boundaries, or the identification of a specific metric that correlates with improved fairness metrics.

### Open Question 2
- **Question**: Did the implementation discrepancies and model selection on the test set in the original study contribute to the false positive findings regarding FairCLIP's efficacy?
- **Basis in paper**: Section 3.3 notes the original code selected models on the test set and deviated from the mathematical description. The authors suggest this "may help explain why the parameters reported by Luo et al. (2024) outperformed" the corrected implementations.
- **Why unresolved**: While suggested as a probable cause, the authors do not isolate this variable to prove it was the sole or primary driver of the discrepancy between the original results and the reproduction.
- **What evidence would resolve it**: A controlled ablation study re-running the original setup with test-set selection versus correct validation-set selection to quantify the impact of the data leakage.

### Open Question 3
- **Question**: Is the failure to improve fairness specific to the Sinkhorn distance, or is the fundamental approach of regularizing similarity score distributions ineffective?
- **Basis in paper**: Appendix E states that replacing Sinkhorn with Gaussian and Laplacian MMD also showed "little to no effect" on distances or performance.
- **Why unresolved**: The failure of two distinct distribution metrics implies the issue might be the general approach rather than the specific distance function, but this has not been confirmed across a wider range of potential regularizers.
- **What evidence would resolve it**: Experiments utilizing alternative distribution metrics or regularizers within the same framework to determine if any variant can successfully achieve the desired fairness-performance trade-off.

## Limitations

- **High variance across runs**: Standard deviations often exceed mean differences, making it difficult to draw definitive conclusions about method effectiveness.
- **Implementation discrepancies**: Differences between the original code and paper's mathematical description, including test-set model selection and similarity score normalization, may have inflated original results.
- **Limited hyperparameter search**: The study used only three runs per configuration and did not extensively explore the λ parameter space or alternative distance metrics.

## Confidence

- **High Confidence**: The observation that FairCLIP reduces Sinkhorn distances between group and population distributions is well-supported by empirical results across multiple runs and datasets.
- **Medium Confidence**: The claim that minimizing Sinkhorn distances does not correlate with improved fairness metrics is supported, but the high variance in results makes definitive conclusions difficult.
- **Low Confidence**: The conclusion that FairCLIP fundamentally fails as a fairness method is premature given the limited hyperparameter search and potential for undiscovered λ regimes that might yield better trade-offs.

## Next Checks

1. **Extended hyperparameter search**: Conduct a comprehensive search over λ values (spanning 10^-8 to 10^-2) with more seeds (n=10) to determine if any regime exists where Sinkhorn distance reduction correlates with fairness improvements.

2. **Alternative distribution metrics**: Test whether other distributional distance measures (e.g., MMD, Wasserstein) show different patterns of correlation with fairness metrics compared to Sinkhorn distance.

3. **Robustness analysis**: Evaluate the method's sensitivity to data imbalance and sample size by subsampling Harvard-FairVLMed to simulate smaller, more imbalanced datasets and measuring variance in fairness outcomes.