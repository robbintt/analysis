---
ver: rpa2
title: 'The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment'
arxiv_id: '2511.21331'
source_url: https://arxiv.org/abs/2511.21331
tags:
- multimodal
- modalities
- modality
- audio
- confu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConFu, a contrastive learning framework that
  unifies pairwise and higher-order multimodal alignment within a single objective.
  It extends standard contrastive objectives by incorporating fused representations
  of modality subsets, enabling the model to capture both pairwise dependencies and
  synergistic higher-order interactions, such as XOR-like relationships that pairwise
  methods miss.
---

# The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment

## Quick Facts
- arXiv ID: 2511.21331
- Source URL: https://arxiv.org/abs/2511.21331
- Reference count: 40
- Primary result: ConFu unifies pairwise and higher-order contrastive learning, outperforming baselines on synthetic and real-world multimodal benchmarks while maintaining robustness to modality noise and competition.

## Executive Summary
ConFu introduces a contrastive learning framework that unifies pairwise and higher-order multimodal alignment within a single objective. By combining standard pairwise InfoNCE losses with fused representations that capture synergistic interactions among modalities, ConFu achieves superior performance on both one-to-one and two-to-one retrieval tasks. The framework demonstrates robust performance across synthetic XOR tasks, sentiment analysis, and bird species classification benchmarks, particularly excelling when modality noise or competition is present.

## Method Summary
ConFu extends contrastive learning by jointly optimizing pairwise alignment losses between individual modalities and fused alignment losses between combinations of modalities and the remaining modality. The framework uses modality-specific encoders and projectors to create shared embeddings, then applies lightweight MLPs to fuse pairs of modalities. The total loss combines three pairwise InfoNCE terms with three fused InfoNCE terms, weighted by λ. Training uses AdamW optimizer with cosine annealing, batch sizes 64-256, and temperature-scaled dot products for similarity estimation.

## Key Results
- Achieves highest zero-shot accuracy on AV-MNIST (71.2%) and strong performance on MUSTARD classification (45.7% accuracy)
- Maintains competitive R@10 retrieval scores across MOSI, UR-FUNNY, and MUStARD for both 1→1 and 2→1 settings
- Demonstrates robustness to Gaussian noise, maintaining 45.4% accuracy vs. 35.4% for Tri-CLIP when vision is corrupted
- Shows stable performance on SSW60 (71.44% accuracy) where both modalities are informative, and VB100 where audio is uninformative

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ConFu captures higher-order, synergistic dependencies (e.g., XOR-like relationships) among three or more modalities that pairwise-only methods cannot.
- Mechanism: The core mechanism is the joint optimization of pairwise contrastive objectives (aligning each modality pair) with fused contrastive objectives (aligning a fused representation of two modalities with the third). This combined loss maximizes a contrastive lower bound on the total correlation among all modalities, which decomposes into both pairwise and higher-order information terms.
- Core assumption: The information-theoretic decomposition of Total Correlation (TC) holds, and maximizing its contrastive lower bound effectively forces the model to learn representations that satisfy both pairwise and synergistic dependencies.
- Evidence anchors: [abstract] "...enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone..." [section 3.2] "This interpretation of TC separates pairwise and higher-order dependencies." [section 3.4, Figure 3] Shows ConFu and SYMILE solving the XOR task, while pairwise methods fail. [corpus] Related work like TRIANGLE and GRAM also targets higher-order alignment, but ConFu's specific decomposition is claimed as a differentiator.
- Break condition: If the fusion network `g` is insufficiently expressive to model the interaction between modalities, the higher-order term `I(X_k; X_i, X_j)` may not be maximized, and synergistic information will not be captured.

### Mechanism 2
- Claim: ConFu maintains strong performance on standard one-to-one (1→1) retrieval tasks while enabling superior two-to-one (2→1) retrieval.
- Mechanism: By explicitly including all pairwise InfoNCE losses (`L_pair`) alongside the fused losses (`L_fused`), the model is penalized if individual modality embeddings drift apart. This preserves the geometry needed for direct 1→1 alignment. The fused representations are then pulled into alignment with the remaining modality, creating a consistent space for compositional 2→1 queries.
- Core assumption: The loss weighting factor `λ` can be tuned to find a Pareto-optimal balance where neither pairwise nor fused alignment dominates.
- Evidence anchors: [abstract] "...while still maintaining strong pairwise correspondence." [section 5.2, Table 2] ConFu shows competitive or superior R@10 for both 1→1 and 2→1 retrieval settings across MOSI, UR-FUNNY, and MUStARD. [corpus] "Principled Multimodal Representation Learning" (arXiv:2507.17343) notes traditional methods rely on pairwise contrastive learning, restricting alignment across all modalities, which ConFu addresses.
- Break condition: If `λ` is set too high, `L_fused` may dominate, degrading pairwise alignment and harming 1→1 retrieval. Conversely, too low `λ` may result in weak fused representations and poor 2→1 performance.

### Mechanism 3
- Claim: ConFu exhibits greater robustness to noise-induced distribution shifts and modality competition compared to baselines.
- Mechanism: The paper hypothesizes that the explicit modeling of fused representations makes the system less reliant on any single, potentially noisy, modality. The fusion process averages or integrates signals, diluting the impact of noise in one channel. Furthermore, the joint optimization may mitigate "modality competition" where a dominant modality suppresses weaker ones.
- Assumption: The fusion network learns to weigh modalities according to their signal quality, even without explicit attention mechanisms.
- Evidence anchors: [section 6.2, Table 6] Under Gaussian noise applied to vision, ConFu (A+V) achieves 45.4% accuracy vs. Tri-CLIP (V) at 35.4%. [section 5.3] On VB100, where audio is uninformative, ConFu (A+V) maintains performance close to the visual-only baseline, showing it doesn't fail when one modality is weak. [section 6.1, Figure 5] Analysis of prediction overlaps shows a subset of samples are uniquely solved by the audiovisual model. [corpus] Evidence on robustness to modality competition is a direct contribution of this paper; corpus papers do not explicitly test this for ConFu.
- Break condition: If a modality is not just noisy but adversarially corrupted, the fusion network might be misled. The lightweight MLP fusion may lack the capacity for sophisticated robustness seen in larger, adaptive fusion modules.

## Foundational Learning

- Concept: **InfoNCE Loss & Contrastive Learning**
  - Why needed here: The entire ConFu objective is built upon InfoNCE as a tractable lower bound for mutual information. Understanding how contrastive learning pulls positive pairs together and pushes negative pairs apart in the embedding space is essential to grasp how `L_pair` and `L_fused` work.
  - Quick check question: Can you explain how the InfoNCE loss in Eq. (2) provides a lower bound on `I(X_i; X_j)`?

- Concept: **Total Correlation (TC) / Multi-information**
  - Why needed here: The paper uses TC as the theoretical justification for its combined objective. TC quantifies all dependencies (pairwise and higher-order) among a set of variables. ConFu aims to maximize a bound on TC.
  - Quick check question: How does TC decompose for three variables (X1, X2, X3), and which parts correspond to pairwise and higher-order dependencies?

- Concept: **Modality Competition**
  - Why needed here: This is a key problem in multimodal learning that ConFu claims to mitigate. It occurs when one modality (e.g., vision) is much stronger and causes the model to ignore weaker but complementary signals (e.g., audio) during joint training.
  - Quick check question: What evidence from the paper suggests ConFu helps with modality competition, and how might the fused objective contribute to this?

## Architecture Onboarding

- Component map:
  1. Modality Encoders (`f_θ`): One per modality (e.g., ResNet for image/audio, BERT for text). Processes raw input into feature vectors `h_i`.
  2. Projectors (`p_ϕ`): Small networks (often MLPs) that map encoder features `h_i` into the shared embedding space `z_i`.
  3. Fusion Networks (`g_ψ`): Lightweight MLPs that take features from two modalities (`h_i`, `h_j`) and produce a fused embedding `z_ij`. There are three such networks for the three possible pairs in a trimodal setting.
  4. Density Ratio Estimators (`s_ω`, `t_η`): Implemented as temperature-scaled dot products. They compute similarity scores for contrastive losses.
  5. Loss Aggregator: Combines `L_pair` (sum of 3 pairwise InfoNCEs) and `L_fused` (sum of 3 fused InfoNCEs) using weighting `λ`.

- Critical path:
  1. Data Prep: Ensure you have a dataset with **co-occurring triplets** (e.g., image, audio, text for the same sample). The paper uses Bird-MML, A V-MNIST, and MultiBench datasets.
  2. Forward Pass:
     - Encode all three modalities to get `h1`, `h2`, `h3`.
     - Project them to get embeddings `z1`, `z2`, `z3`.
     - Use fusion networks to get fused embeddings: `z12`, `z13`, `z23`.
  3. Loss Computation:
     - Compute `L_pair`: InfoNCE for pairs (z1,z2), (z1,z3), (z2,z3).
     - Compute `L_fused`: InfoNCE for pairs (z3,z12), (z2,z13), (z1,z23).
     - Combine: `L = (1-λ)*L_pair + λ*L_fused`. Start with `λ=0.5`.
  4. Training: Optimize all parameters (encoders, projectors, fusion networks) jointly using the combined loss.

- Design tradeoffs:
  1. **Fusion Network Complexity**: The paper uses shallow MLPs for efficiency. More complex fusion (e.g., cross-attention) might capture richer interactions but increases compute cost and risk of overfitting.
  2. **Choice of `λ`**: Controls the balance between preserving 1→1 alignment (lower `λ`) and learning higher-order synergy (higher `λ`). Must be tuned per dataset (see Appendix B.1).
  3. **Encoder Architecture**: ConFu is agnostic. Using pretrained vs. from-scratch encoders is a major tradeoff between performance and training stability/data efficiency.

- Failure signatures:
  1. **1→1 Retrieval Degrades**: `L_fused` may be overpowering `L_pair`. Increase `λ` towards 0 or check for bugs in pairwise loss computation.
  2. **No Benefit from Fusion (2→1 ≈ 1→1)**: The fusion network might be collapsing or failing to integrate information. Check gradients into `g_ψ`. The dataset may lack genuine complementarity.
  3. **Performance Collapses with Noisy/Weak Modality**: If adding a modality hurts results (like in VB100 for some baselines), the model is not robust. Check if ConFu still maintains performance (as in Table 4). If not, the fusion network might be overfitting to noise.

- First 3 experiments:
  1. **Validate on Synthetic XOR**: Replicate the experiment from Section 3.4 & Figure 3. Train ConFu vs. a pairwise CLIP model on the controlled XOR data with varying `p̂`. This proves the core capability to capture synergy.
  2. **Ablation on `λ`**: On a benchmark like MOSI or UR-FUNNY, sweep `λ` from 0 to 1 and plot mean R@10 for both 1→1 and 2→1 retrieval. Find the Pareto front as suggested in Appendix B.1.
  3. **Robustness Test**: Following Section 6.2, take a trained model and evaluate its zero-shot classification accuracy on a test set where Gaussian noise is added to one modality (e.g., vision) at increasing severities. Compare degradation rates against baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Contrastive Fusion framework be adapted to effectively train on data where modalities are partially missing or not fully co-occurring?
- Basis in paper: [explicit] The authors state in the Limitations section that "ConFu currently relies on fully aligned modalities during training, and exploring ways to relax or bypass this requirement remains crucial for many applications."
- Why unresolved: The current formulation assumes the presence of complete triplets (e.g., image, audio, text) for every training sample, which restricts its use in real-world scenarios where data is often unaligned or incomplete.
- What evidence would resolve it: Demonstrating successful training on datasets with synthetically removed modalities or real-world unpaired data (e.g., using pseudo-pair construction techniques) without significant performance degradation.

### Open Question 2
- Question: What are the optimal strategies for selectively pruning alignment terms to manage the combinatorial computational cost as the number of modalities increases?
- Basis in paper: [explicit] The paper notes that "as the number of modalities increases, computational demands may also rise" and suggests that "selectively pruning certain alignment terms... opens a promising avenue for future adaptation and research."
- Why unresolved: While the paper provides a theoretical extension to $M$ modalities, it primarily validates performance on $M=3$, leaving the efficiency-accuracy trade-off of pruning specific terms in higher-order settings unexplored.
- What evidence would resolve it: A systematic study evaluating the performance retention versus computational savings when removing specific higher-order or pairwise loss terms in a system with $M > 3$ modalities.

### Open Question 3
- Question: How would incorporating factorized representations into the framework impact the model's ability to disentangle unique versus shared cross-modal information?
- Basis in paper: [explicit] The authors list "extending ConFu to incorporate factorized representations inspired by recent factorization-based and redundancy-reduction approaches" as a prospective direction.
- Why unresolved: The current fusion approach creates a joint embedding, which may conflate information unique to specific modalities with information common to the group, potentially limiting interpretability or fine-grained control.
- What evidence would resolve it: Experiments showing that a factorized variant of ConFu can successfully isolate unique modality features (e.g., visual-only traits) while maintaining the synergistic retrieval performance of the original model.

### Open Question 4
- Question: Can adaptive mechanisms be effectively integrated into ConFu to dynamically regulate modality competition and prevent dominant modalities from suppressing weaker ones?
- Basis in paper: [explicit] The paper identifies "modality competition" as a known challenge in the introduction and explicitly lists "developing adaptive mechanisms to regulate modality competition" as a future direction in the conclusion.
- Why unresolved: Although ConFu demonstrates robustness to noise, it does not currently employ dynamic gradient modulation or gating to explicitly balance the influence of strong versus weak modalities during the fusion process.
- What evidence would resolve it: Improved performance on benchmarks characterized by severe modality imbalance (like VB100) through the integration of a dynamic balancing component, compared to the static fusion MLP.

## Limitations
- The framework requires fully aligned multimodal data for training, limiting applicability to real-world scenarios with missing or unaligned modalities
- Fusion MLP capacity may be insufficient for modeling complex interactions in diverse real-world scenarios
- Robustness claims rely on comparisons to potentially outdated baselines rather than state-of-the-art noisy multimodal learning methods

## Confidence

- **Mechanism 1 (Higher-order dependencies)**: High - Well-validated on synthetic XOR task with clear failure modes for pairwise methods
- **Mechanism 2 (Dual 1→1 and 2→1 retrieval)**: High - Strong empirical support across multiple benchmarks with consistent performance improvements
- **Mechanism 3 (Robustness to noise/modality competition)**: Medium - Evidence is compelling but based on comparisons with potentially outdated baselines; no ablation on fusion network capacity

## Next Checks

1. **Ablation on Fusion Network Capacity**: Systematically vary fusion MLP depth and width on AV-MNIST XOR task to determine if shallow networks are sufficient for capturing higher-order interactions or if capacity limitations explain mixed real-world results

2. **Cross-dataset Transfer Learning**: Train ConFu on one dataset (e.g., MOSI) and evaluate zero-shot retrieval on held-out datasets with different modality combinations to test whether higher-order alignment truly generalizes beyond specific training distributions

3. **Robustness under Adversarial Noise**: Replace Gaussian noise with structured adversarial perturbations targeting individual modalities to test whether ConFu's fusion mechanism provides genuine robustness or merely averages out random noise