---
ver: rpa2
title: Longitudinal Monitoring of LLM Content Moderation of Social Issues
arxiv_id: '2510.01255'
source_url: https://arxiv.org/abs/2510.01255
tags:
- content
- moderation
- refusal
- social
- openai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces AI Watchman, a longitudinal auditing system\
  \ that tracks LLM content moderation of social issues by monitoring refusals across\
  \ models and time. Using a dataset of over 3,100 Wikipedia pages on 421 social issues,\
  \ the system queries OpenAI\u2019s GPT-4.1, GPT-5, the moderation endpoint, and\
  \ DeepSeek in English and Chinese."
---

# Longitudinal Monitoring of LLM Content Moderation of Social Issues

## Quick Facts
- arXiv ID: 2510.01255
- Source URL: https://arxiv.org/abs/2510.01255
- Reference count: 40
- Key outcome: AI Watchman system tracks LLM content moderation by monitoring refusals across models and time, revealing model-specific and temporal differences in moderation of social issues.

## Executive Summary
This work introduces AI Watchman, a longitudinal auditing system that tracks LLM content moderation of social issues by monitoring refusals across models and time. Using a dataset of over 3,100 Wikipedia pages on 421 social issues, the system queries OpenAI's GPT-4.1, GPT-5, the moderation endpoint, and DeepSeek in English and Chinese. Refusal rates range from 1.2%-3.9% overall, with GPT-5 showing the lowest. The system identifies model-specific and temporal differences in moderation, including increases in refusals for Israel- and abortion-related content. It also highlights subtle non-explicit refusal strategies. Findings emphasize the need for transparent, ongoing auditing of AI content moderation given its impact on public discourse. The dataset, code, and interactive website are publicly available.

## Method Summary
The study creates a longitudinal monitoring system that tracks LLM content moderation by detecting refusals when models are asked to repeat Wikipedia text about social issues. The system queries multiple models (GPT-4.1, GPT-5, DeepSeek, OpenAI Moderation Endpoint) in English and Chinese using a "repeat after me" prompt structure. It runs biweekly, detecting refusals through API error codes, phrase matching (model-specific patterns), and length-related refusals handled by truncating to 19,000 characters and retrying. The Social Issues Dataset comprises 3,121 Wikipedia pages across 421 topics, sourced from Pew Research Center and supplemented with 20 Chinese-sensitive topics. The system calculates per-category/topic flagging rates over time to identify patterns in content moderation.

## Key Results
- Overall refusal rates range from 1.2%-3.9% across models, with GPT-5 showing the lowest refusal rate
- GPT-5 exhibits the lowest refusal rate compared to other models
- Temporal analysis reveals increases in refusals for Israel- and abortion-related content over time
- The system identifies subtle non-explicit refusal strategies beyond direct refusals

## Why This Works (Mechanism)
The system works by systematically querying LLMs with socially sensitive content and detecting when models refuse to process or repeat the text. By using a simple "repeat after me" prompt structure and monitoring for specific refusal patterns (error codes, refusal phrases, length-related issues), the system can quantify content moderation across different models and time periods. The longitudinal design enables detection of temporal patterns and model-specific differences in how LLMs handle socially sensitive topics.

## Foundational Learning
- **Longitudinal auditing methodology**: Systematically tracking model behavior over time to detect changes in content moderation patterns. Needed to understand how moderation evolves and identify temporal trends. Quick check: Compare refusal rates across multiple time periods to verify temporal patterns.
- **Refusal detection via multiple signals**: Using API error codes, phrase matching, and length-based detection to comprehensively identify refusals. Needed because models employ various strategies to avoid processing sensitive content. Quick check: Manually review flagged responses to validate detection accuracy.
- **Multilingual monitoring**: Querying models in both English and Chinese to capture cultural and regional differences in content moderation. Needed to understand how moderation varies across languages and contexts. Quick check: Compare refusal rates between English and Chinese versions of the same topics.

## Architecture Onboarding
- **Component map**: Social Issues Dataset -> Query Pipeline -> Refusal Detection -> Analysis Dashboard
- **Critical path**: Wikipedia content retrieval → API querying → Refusal detection → Rate calculation → Temporal analysis
- **Design tradeoffs**: Phrase-based detection vs. ML classification (simplicity vs. accuracy), biweekly vs. weekly monitoring (cost vs. granularity), English-only vs. bilingual monitoring (coverage vs. complexity)
- **Failure signatures**: High false positives from non-explicit refusals, API cost barriers, model behavior changes breaking detection patterns
- **First experiments**:
  1. Query a small subset of topics with all models to validate refusal detection
  2. Compare English vs. Chinese refusal rates for identical topics
  3. Test detection accuracy by manually reviewing flagged vs. unflagged responses

## Open Questions the Paper Calls Out
None

## Limitations
- Refusal detection relies on phrase-based matching which may miss subtle or evolving moderation strategies
- API costs limit monitoring frequency and scope, with estimated annual costs of $11,280-$14,300 for weekly runs
- Temporal patterns may be influenced by external factors not controlled for in the study

## Confidence
- Confidence in refusal rates: Medium - based on specific detection methodology that may not capture all forms of moderation
- Confidence in temporal differences: Medium - derived from same methodology and may be influenced by external factors
- Confidence in overall conclusions about need for transparent auditing: High - study provides clear evidence of model variability and impact on public discourse

## Next Checks
1. Manually review a random sample of flagged and unflagged responses to assess detection accuracy
2. Replicate the study with a different set of social issues to test generalizability
3. Analyze the impact of content length on refusal rates to validate the truncation approach