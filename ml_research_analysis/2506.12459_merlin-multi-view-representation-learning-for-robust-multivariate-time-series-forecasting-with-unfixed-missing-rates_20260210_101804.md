---
ver: rpa2
title: 'Merlin: Multi-View Representation Learning for Robust Multivariate Time Series
  Forecasting with Unfixed Missing Rates'
arxiv_id: '2506.12459'
source_url: https://arxiv.org/abs/2506.12459
tags:
- missing
- forecasting
- time
- learning
- merlin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses robustness of multivariate time series forecasting
  models when data is incomplete, with varying missing rates over time. It proposes
  a method called Merlin that combines offline knowledge distillation and multi-view
  contrastive learning to align semantics between incomplete and complete observations,
  and across different missing rates.
---

# Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates

## Quick Facts
- arXiv ID: 2506.12459
- Source URL: https://arxiv.org/abs/2506.12459
- Reference count: 40
- Key outcome: Merlin improves forecasting accuracy with varying missing rates, achieving MAE of 19.84 on PEMS04 dataset with 90% missing rate compared to 27.43 for best baseline.

## Executive Summary
Merlin addresses the challenge of multivariate time series forecasting with unfixed missing rates by combining offline knowledge distillation and multi-view contrastive learning. The method uses a teacher model trained on complete data to guide a student model on incomplete data, while contrastive learning aligns representations across different missing rates. Experiments on four real-world datasets demonstrate significant improvements in forecasting accuracy, enabling models to handle varying missing rates without retraining.

## Method Summary
Merlin employs a two-stage training approach with a Spatial-Temporal Identity MLP backbone. First, a teacher model is trained on complete observations using standard L1 loss. Then, a student model is trained on incomplete observations with multiple missing rates simultaneously, using a weighted sum of losses: L1 forecasting loss, hidden representation distillation (MSE), result distillation (MSE), and multi-view contrastive loss. The contrastive learning module aligns representations across different missing rates by treating observations with the same underlying semantics but different missing rates as positive pairs.

## Key Results
- Merlin achieves significant improvements over baselines across all datasets and missing rates
- On PEMS04 with 90% missing rate, Merlin achieves MAE of 19.84 versus 27.43 for best baseline
- The method enables handling unfixed missing rates without retraining
- STID+Merlin outperforms all baselines, demonstrating the effectiveness of spatial-temporal embeddings combined with Merlin's approach

## Why This Works (Mechanism)

### Mechanism 1: Offline Knowledge Distillation for Semantic Transfer
- **Claim:** Teacher model trained on complete observations can guide student model to extract similar semantics from incomplete observations.
- **Mechanism:** Teacher trained on complete data, then frozen; teacher's hidden representations and outputs used as soft targets for student via MSE loss.
- **Evidence anchors:** Abstract states teacher guides student to mine semantics from incomplete observations similar to those from complete observations; section 3.4 describes teacher guiding student to generate similar representations.
- **Break condition:** If complete training data unavailable, teacher quality degrades.

### Mechanism 2: Multi-View Contrastive Learning for Rate-Invariant Representations
- **Claim:** Contrastive learning can align semantics between incomplete observations with different missing rates, enabling single model to handle unfixed rates.
- **Mechanism:** Positive pairs constructed from same time point with different missing rates; contrastive loss encourages similarity within positive pairs and dissimilarity with negative pairs.
- **Evidence anchors:** Abstract mentions ensuring semantic alignment across different missing rates; section 3.5 describes using observations with different missing rates at same time point as positive pairs.
- **Break condition:** If missing rate distribution shifts dramatically beyond training range, learned invariance may not generalize.

### Mechanism 3: Spatial-Temporal Identity Embeddings as Missing-Resilient Priors
- **Claim:** Explicit spatial and temporal identity embeddings provide global semantic anchors that help model recover from local information loss.
- **Mechanism:** Spatial identity, daily temporal, and weekly temporal embeddings concatenated with input-hidden representation, carrying periodicity and spatial correlation information.
- **Evidence anchors:** Section 3.3 introduces spatial-temporal identity embeddings to provide additional global information; section 4.2 shows STID+Merlin works better by introducing these embeddings.
- **Break condition:** If spatial-temporal structure is weak or non-stationary, embeddings may provide misleading priors.

## Foundational Learning

- **Concept: Knowledge Distillation (Offline)**
  - **Why needed here:** Understanding how teacher-student frameworks transfer learned representations without requiring teacher updates during student training.
  - **Quick check question:** Why might MSE loss between representations outperform KL divergence for a regression task like time series forecasting?

- **Concept: Contrastive Learning (SimCLR-style)**
  - **Why needed here:** Understanding how positive/negative pair construction and temperature-scaled cosine similarity shape representation spaces.
  - **Quick check question:** How does the temperature parameter τ affect the relative weight of hard vs. easy negatives in the contrastive loss?

- **Concept: Time Series Semantics (Global vs. Local)**
  - **Why needed here:** Distinguishing periodicity/trends (global) from detailed variations (local) clarifies why missing values are particularly damaging.
  - **Quick check question:** When missing values are zero-filled, what artifacts (sudden jumps, flat lines) might a model mistakenly learn as signal?

## Architecture Onboarding

- **Component map:**
  - **Teacher STID:** Embedding Layer → L FC Layers (with residual) → Regression Layer (trained on complete data, frozen during student training)
  - **Student STID:** Same architecture, trained on incomplete data at multiple missing rates with KD + CL constraints
  - **Contrastive Head:** FC projection (H^L_E → Z_E, dim=16) → pairwise contrastive loss across views
  - **Loss Aggregator:** L_Final = β1·L_Pre + β2·(L_HD + L_RD) + β3·L_CL (β1=2, β2=2, β3=1)

- **Critical path:**
  1. Train teacher on complete observations with standard L1 loss
  2. Generate masked training data at rates 25%, 50%, 75%, 90%
  3. Train student on all masked versions simultaneously with combined losses
  4. Deploy only student model for inference (no teacher or imputation needed)

- **Design tradeoffs:**
  - Combined vs. staged losses: Combined outperforms multi-stage due to "information forgetting" in staged training.
  - MSE vs. KL divergence for distillation: MSE minimizes numerical differences; KL aligns distributions. Paper finds MSE superior for regression.
  - Batch size: Larger batches yield more negative pairs for CL, improving semantics capture, but excessive size causes underfitting.

- **Failure signatures:**
  - Sharp accuracy drop at >75% missing rate → CL may not align sufficiently; consider higher β3 or lower τ.
  - Student underperforms teacher even at low missing rates → KD weight (β2) may be too low or teacher quality is compromised.
  - Training instability or convergence to poor local optima → τ may be too small; try increasing from 1.0 to 1.5.

- **First 3 experiments:**
  1. **Ablation by component:** Train w/o HD, w/o RD, w/o KD, w/o CL to quantify each mechanism's contribution (Figure 3 shows HD and CL are critical).
  2. **Temperature sweep:** Test τ ∈ {0.5, 0.75, 1.0, 1.5} to balance convergence speed vs. local optimality risk.
  3. **Unfixed-rate evaluation:** Train single model on mixed missing rates; test on segmented test set with time-varying rates to validate robustness claim (Table 5 protocol).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does offline knowledge distillation perform when the teacher and student models utilize different neural network structures?
- Basis in paper: [explicit] The Conclusion states: "In future work, we plan to investigate the effects of offline knowledge distillation when the teacher and student models utilize different neural network structures."
- Why unresolved: All experiments in the paper utilize the same backbone architecture for both the teacher and student models.
- What evidence would resolve it: Experiments comparing performance when transferring knowledge from a complex teacher (e.g., Transformer) to a lightweight student (e.g., MLP) on the same datasets.

### Open Question 2
- Question: To what extent does Merlin's performance degrade if the teacher model is trained on significantly corrupted data rather than the assumed complete observations?
- Basis in paper: [inferred] While Appendix B briefly tests a 5% missing rate in the training set, the core methodology assumes the teacher is trained on complete observations.
- Why unresolved: Real-world historical data may lack truly "complete" periods, and the robustness of the distillation process relies heavily on the quality of the teacher's representations.
- What evidence would resolve it: A sensitivity analysis measuring student performance as the missing rate in the teacher's training set increases beyond 5% (e.g., 10-30%).

### Open Question 3
- Question: Can Merlin maintain its robustness when applied to complex missing patterns, such as temporal blackouts or sensor failures, rather than random point missing?
- Basis in paper: [inferred] Section 2.1 notes that mainstream works are typically conducted under the "random point missing" scenario, and this paper primarily follows this convention.
- Why unresolved: Random masking distributes missing values uniformly, whereas structural missingness (e.g., block missing) disrupts semantics differently and may not be aligned effectively by current contrastive learning strategies.
- What evidence would resolve it: Evaluation of Merlin on datasets modified to include contiguous blocks of missing values or specific variate dropout.

## Limitations
- Relies heavily on availability of complete training data for teacher model
- Effectiveness depends on appropriate temperature scaling and batch size selection for contrastive learning
- May not generalize well to missing rates exceeding training range

## Confidence

- **High Confidence:** Teacher-student distillation mechanism's effectiveness in transferring complete-data semantics to incomplete data is well-supported by experimental results showing consistent improvements across all datasets and missing rates.
- **Medium Confidence:** Multi-view contrastive learning's ability to handle unfixed missing rates without retraining is demonstrated but relies on strong assumptions about similarity of representations across different missing rates.
- **Medium Confidence:** Spatial-temporal identity embeddings' contribution to robustness is supported by ablation studies but could benefit from more detailed analysis of when these embeddings are most effective.

## Next Checks

1. **Distribution Shift Test:** Evaluate Merlin's performance when test missing rates exceed the maximum training rate (e.g., 95%+ missing) to assess limits of rate-invariant learning.
2. **Teacher Quality Dependency:** Systematically vary teacher model quality (by training on different proportions of complete data) to quantify impact on student performance, particularly for high missing rates.
3. **Hyperparameter Sensitivity Analysis:** Conduct comprehensive grid search over temperature (τ) and contrastive loss weight (β3) parameters to identify optimal settings for different missing rate ranges and dataset characteristics.