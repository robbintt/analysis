---
ver: rpa2
title: 'VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual
  Perceptual Grounding'
arxiv_id: '2509.24776'
source_url: https://arxiv.org/abs/2509.24776
tags:
- reasoning
- visual
- perception
- textual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VTPerception-R1, a two-stage training framework
  that explicitly decouples visual and textual perception from reasoning in multimodal
  large language models. The first stage employs perception-augmented supervised fine-tuning
  to train models to generate concise, task-relevant perceptual descriptions before
  reasoning.
---

# VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding

## Quick Facts
- **arXiv ID**: 2509.24776
- **Source URL**: https://arxiv.org/abs/2509.24776
- **Reference count**: 38
- **Primary result**: Explicit visual and textual perceptual grounding improves multimodal reasoning accuracy, achieving SOTA on 4/6 benchmarks

## Executive Summary
VTPerception-R1 introduces a two-stage training framework that decouples perception from reasoning in multimodal large language models. The approach first trains models to generate concise, task-relevant perceptual descriptions before reasoning, then applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Through systematic experiments on four benchmarks using Qwen2.5-VL models, the study finds that explicit perception consistently yields the largest performance gains, particularly for smaller models. The framework significantly improves reasoning accuracy and robustness, demonstrating the critical importance of balanced visual and textual grounding for multimodal reasoning.

## Method Summary
VTPerception-R1 employs a two-stage training approach. Stage I uses perception-augmented supervised fine-tuning to train models to generate structured outputs with explicit `<description>` fields containing task-relevant perceptual evidence. Stage II applies perception-aware reinforcement learning with DAPO optimizer, using composite rewards for accuracy, format compliance, visual key-info coverage, textual key-info coverage, repetition penalty, and consistency between reasoning and perceived evidence. The framework is evaluated on MMMU, MathVista, EMMA, and OlympiaBench benchmarks using Qwen2.5-VL-32B and Qwen2.5-VL-7B models.

## Key Results
- Explicit perception consistently yields the largest performance gains across all benchmarks
- VTPerception-R1 achieves SOTA results on 4 out of 6 evaluated benchmarks
- Smaller 7B models benefit more from explicit perception compared to larger 32B models
- Removing consistency reward causes largest drops in reasoning accuracy (-3.26 on C-MMBench)

## Why This Works (Mechanism)

### Mechanism 1: Structured Perception-Decoupling
The model is trained to first verbalize task-relevant perceptual evidence, creating an intermediate representation that subsequent reasoning must reference. This explicit "scratchpad" structure prevents reasoning from hallucinating premises not grounded in inputs. The core assumption is that reasoning quality depends on quality of perception input, and models can learn to generate useful perceptual summaries when explicitly structured.

### Mechanism 2: Multi-Modal Perception Rewards
Jointly rewarding visual (R_vkey) and textual (R_tkey) evidence coverage yields balanced grounding and reduces modality bias. R_vkey rewards recall of visual key facts in `<description>`, while R_tkey rewards recall of textual constraints in ``. Together they prevent over-reliance on one modality, based on the assumption that key information can be automatically or manually extracted from training samples.

### Mechanism 3: Consistency Reward Enforcement
R_cons aligns reasoning outputs with perceived evidence, reducing hallucinations. It extracts entities/attributes from `` + `<answer>` and from `<description>` + question, rewarding alignment between reasoning and perceived evidence. The core assumption is that entity extraction is reliable and inconsistent reasoning is more likely hallucinated.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed: Stage II uses DAPO, a variant of RLVR, to optimize perception and reasoning jointly. Understanding policy gradients, clipped objectives, and advantage normalization is essential.
  - Quick check: Can you explain how GRPO/DAPO computes group-relative advantages and why asymmetric clipping (ε_low ≠ ε_high) matters for stability?

- **Concept: Multimodal Perception in Vision-Language Models**
  - Why needed: The framework hinges on diagnosing and fixing perception bottlenecks. Distinguishing visual grounding from textual grounding and understanding when models hallucinate is critical.
  - Quick check: For a geometry problem with diagram and text, what types of perceptual errors would you expect from a 7B vs. 32B MLLM?

- **Concept: Reward Engineering for Structured Outputs**
  - Why needed: The total reward combines 6 components with weights. Understanding tradeoffs between accuracy, format, coverage, and consistency rewards is essential.
  - Quick check: If R_vkey weight is too high relative to R_acc, what failure mode might emerge?

## Architecture Onboarding

- **Component map**: Raw CoT -> Image Formal Description -> CoT Restructuring -> Quality Filtering -> Key Info Extraction -> SFT/RL Training -> Qwen2.5-VL-7B-Instruct -> Structured outputs (`<description>`, ``, `<answer>`)

- **Critical path**: 
  1. Prepare SFT data with `<description>` fields (12K samples, 3 epochs)
  2. Run SFT to establish perception-to-reasoning interface
  3. Construct RL data with visual/textual key info extraction (22K samples)
  4. Run DAPO RL with composite reward for 2 epochs
  5. Evaluate on MMMU, MathVista, EMMA, AI2D, C-MMBench

- **Design tradeoffs**: 
  - Explicit vs. Implicit Perception: Explicit yields higher gains but requires annotated/extracted key info; implicit is cheaper but less controllable
  - SFT-only vs. SFT+RL: Table 2 shows RL adds +4.6 MathVista, +6.5 C-MMBench; RL is essential for full gains but requires more compute
  - Model Scale: 7B models benefit more from explicit perception but are more prone to self-induced hallucinations under structured prompting

- **Failure signatures**:
  - Generic `<description>` unrelated to question → check key-info relevance
  - Format collapse (missing segments) → increase R_fmt weight
  - High R_vkey but low R_acc → visual coverage doesn't translate to reasoning; check task alignment
  - Repetitive outputs → increase R_rep penalty

- **First 3 experiments**:
  1. Baseline diagnostic: Run Qwen2.5-VL-7B-Instruct on MathVista/MMMU subsets with and without explicit visual notes to reproduce perception gaps
  2. Ablation on rewards: Train Stage II with each reward removed (following Table 3 protocol) on 12K SFT + 1 epoch RL to confirm contribution of R_cons, R_vkey, R_tkey
  3. Key-info quality test: Manually inspect 50 samples' extracted visual/textual key info; correlate coverage scores with human judgment of relevance

## Open Questions the Paper Calls Out

### Open Question 1
Can VTPerception-R1 be effectively extended to incorporate external retrieval mechanisms or tool use within the perception-aware reinforcement learning stage? The current framework only uses internal model perception and doesn't address how dynamic, external information sources could be integrated into the two-stage training or how rewards would account for the relevance and fidelity of retrieved/tool-generated evidence.

### Open Question 2
Is the performance gain from explicit perception, and the balance between visual and textual perception rewards, consistent across different MLLM architectures beyond Qwen2.5-VL? The study systematically evaluates only two model sizes of the same architecture, and without testing on other model families, it remains unclear if the findings on the importance of balanced visual-textual grounding are general principles or artifacts of the Qwen2.5-VL architecture.

### Open Question 3
Why does the removal of the visual key-info reward lead to a performance improvement on the MathVista benchmark? This counterintuitive result challenges the assumption that stronger visual grounding is always beneficial and suggests a one-size-fits-all reward weighting may be suboptimal. The underlying reason is not investigated in the paper.

### Open Question 4
What are the mechanistic reasons that textual perception rewards yield more stable improvements for smaller models (7B) compared to larger ones (32B)? The systematic study finds that for 7B models, textual perception provides "more stable" gains across tasks, whereas for 32B models improvements are "marginal" and "inconsistent." The paper documents the empirical phenomenon but doesn't explain the underlying mechanism.

## Limitations
- The approach relies on manually or semi-automatically extracting key visual and textual information for reward computation, which becomes prohibitively expensive at large scale
- The composite reward function with six components and fixed weights represents a narrow operational point where small perturbations could significantly alter model behavior
- While showing strong performance on educational benchmarks, effectiveness on real-world multimodal tasks (medical imaging, robotics, social media analysis) remains untested

## Confidence
- **High confidence**: The core claim that explicit perception structuring improves multimodal reasoning performance is well-supported by ablation studies and achieves state-of-the-art results on multiple benchmarks
- **Medium confidence**: The relative importance of different reward components is established through controlled ablation, but optimal weighting and thresholding remain somewhat arbitrary
- **Low confidence**: The claim that smaller models benefit more from explicit perception lacks theoretical grounding and may not hold across different architectures or training regimes

## Next Checks
1. **Cross-domain robustness test**: Apply VTPerception-R1 to three non-educational multimodal tasks (medical image analysis, product visual QA, and document layout understanding) to assess generalization beyond tested benchmarks
2. **Hyperparameter sensitivity analysis**: Systematically vary each reward weight and coverage thresholds across five values each, measuring performance impact on a held-out validation set to identify whether current configuration represents a robust optimum
3. **Zero-shot generalization probe**: Evaluate models trained with different perception-augmentation strategies on completely unseen multimodal reasoning tasks without fine-tuning to test whether perceptual grounding creates transferable reasoning capabilities