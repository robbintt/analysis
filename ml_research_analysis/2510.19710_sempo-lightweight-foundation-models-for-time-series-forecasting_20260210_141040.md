---
ver: rpa2
title: 'SEMPO: Lightweight Foundation Models for Time Series Forecasting'
arxiv_id: '2510.19710'
source_url: https://arxiv.org/abs/2510.19710
tags:
- time
- series
- forecasting
- sempo
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SEMPO, a lightweight foundation model for
  time series forecasting that significantly reduces model size and pre-training data
  scale while maintaining strong generalization. The method employs two key innovations:
  (1) an energy-aware spectral decomposition module that models both high-energy and
  low-energy frequency signals to improve data utilization, and (2) a mixture-of-prompts
  enabled Transformer that learns dataset-specific prompts for parameter-efficient
  adaptation.'
---

# SEMPO: Lightweight Foundation Models for Time Series Forecasting

## Quick Facts
- arXiv ID: 2510.19710
- Source URL: https://arxiv.org/abs/2510.19710
- Authors: Hui He; Kun Yi; Yuanchi Ma; Qi Zhang; Zhendong Niu; Guansong Pang
- Reference count: 40
- Primary result: 19.9%-36.0% MSE improvements over state-of-the-art models with 100x more parameters

## Executive Summary
This paper introduces SEMPO, a lightweight foundation model for time series forecasting that achieves state-of-the-art performance with dramatically reduced model size and pre-training data requirements. The key innovations are an energy-aware spectral decomposition module that captures both high-energy and low-energy frequency signals, and a mixture-of-prompts enabled Transformer that learns dataset-specific prompts for efficient adaptation. With only 6.5M parameters trained on 83M time points, SEMPO outperforms existing models with hundreds of millions of parameters trained on billions of time points across 16 datasets.

## Method Summary
SEMPO employs a two-stage training approach: first, an energy-aware spectral decomposition (EASD) module transforms time series to frequency domain, partitions by learnable energy threshold, and applies dual-branch spectral masking to capture both dominant and subtle temporal patterns. Second, a mixture-of-prompts (MoP) enabled Transformer learns heterogeneous temporal patterns through small dataset-specific prompts with adaptive routing. The model is pre-trained for 10 epochs with reconstruction loss, then fine-tuned for 20 epochs with joint forecasting and reconstruction loss while freezing the backbone.

## Key Results
- Achieves 19.9%-36.0% MSE improvements over state-of-the-art models
- Uses only 6.5M parameters vs. hundreds of millions in competing models
- Trained on 83M time points vs. billions required by existing methods
- Demonstrates strong performance in both zero-shot and few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
**Energy-aware spectral decomposition improves pre-training data utilization by capturing low-energy frequency signals that current models ignore.**
The EASD module transforms time series to frequency domain via FFT, partitions by learnable energy threshold, and applies dual-branch spectral masking. This prevents high-energy components from overwhelming subtle but informative low-energy signals that encode persistent temporal dynamics. Core assumption: Low-energy frequency signals contain transferable patterns essential for forecasting generalization. Break condition: When time series contains primarily high-energy dominant patterns with negligible low-energy information.

### Mechanism 2
**Mixture-of-prompts enables parameter-efficient adaptation to heterogeneous temporal patterns without requiring large network architectures.**
A pool of prompt-based experts is learned, with a token-dependent adaptive router computing gating scores via linear-softmax transformation. Each token receives a weighted combination of experts, which are reparameterized into structured key-value pairs for self-attention injection. Core assumption: Heterogeneous cross-domain patterns can be captured through soft combinations of small prompt experts rather than large monolithic networks. Break condition: When domains share highly similar patterns, prompt routing adds complexity without significant benefit.

### Mechanism 3
**Two-stage training separates general knowledge acquisition from domain-specific adaptation.**
Stage 1 (energy-aware pre-training) freezes Transformer backbone and learns reconstruction via MSE on masked spectral data. Stage 2 (MoP tuning) keeps backbone frozen, trains only MoP module and prediction heads using multi-resolution forecasting loss plus reconstruction loss. Core assumption: Pre-trained spectral representations provide sufficient inductive bias for downstream tasks when combined with lightweight adaptation. Break condition: When few-shot data is extremely limited (<5%), reconstruction loss may dominate and prevent effective adaptation.

## Foundational Learning

**Spectral Energy Bias in Transformers**
- Why needed: Self-attention mechanisms inherently weight high-energy frequency components more heavily, causing information loss in low-energy but informative signals
- Quick check: Can you explain why FFT magnitude squared represents spectral energy, and how self-attention's weighting characteristic amplifies dominant frequencies?

**Prompt Tuning vs. Fine-tuning**
- Why needed: Understanding that prompts are learnable continuous vectors injected into attention's key-value space, enabling efficient adaptation with orders of magnitude fewer parameters
- Quick check: How does prepending learnable prompts to key/value matrices differ from traditional fine-tuning of attention weights?

**Mixture of Experts Routing**
- Why needed: MoPFormer applies sparse expert routing to prompts rather than weights, requiring understanding of gating functions and soft expert selection
- Quick check: What is the computational difference between routing tokens to prompt experts versus routing to weight experts in standard MoE?

## Architecture Onboarding

**Component map**: Input → Instance Normalization → EASD (FFT → Energy Partition → Dual Spectral Masking → iFFT) → Patchify & Project → MoPFormer Encoder → Prediction/Reconstruction Heads

**Critical path**:
1. Spectral masking quality determines what temporal patterns reach the model
2. Router gating distribution controls prompt expert utilization
3. Prompt reparameterization (MLP + reshape) affects attention key-value alignment
4. Two-stage training schedule balances generalization vs. adaptation

**Design tradeoffs**:
- Mask number N_m: More masks capture diverse frequency patterns but risk redundancy (optimal: 4)
- Prompt number I: More prompts increase domain capacity but require more routing decisions (optimal: 128)
- Latent dimension D_p: Higher capacity improves large datasets, degrades small datasets (optimal: 256)
- Energy threshold τ: Too high misses low-energy signals; too low includes noise

**Failure signatures**:
- Training loss plateaus early → Check spectral masking not too aggressive (verify τ distribution)
- Router collapse (single expert dominates) → Increase temperature or add load balancing loss
- Zero-shot worse than few-shot significantly → Pre-training may be underfitting; increase epochs/data
- Inference OOM with batch_size > 32 → Prompt concatenation doubles K/V sequence length; reduce batch or use gradient checkpointing

**First 3 experiments**:
1. Reproduce ablation (Table 3): Replace EASD with random patch masking on single dataset to verify 19% MSE degradation claim; use identical hyperparameters
2. Scaling validation: Train with 10M, 50M, 100M data points to verify scaling curve plateaus after 83M; plot MSE vs. data size
3. Cross-domain routing analysis: Visualize gating scores on held-out domain (not in pre-training) to verify prompt specialization; compare with in-domain routing patterns using t-SNE visualization

## Open Questions the Paper Calls Out

**Open Question 1**: How can the SEMPO architecture be extended to explicitly model cross-variable dependencies and multivariate interactions?
- Basis: The Conclusion states that this work "paves the way for future advancements in considering multivariate interactions" as a necessary step to empower the model with more capabilities
- Why unresolved: The current methodology utilizes channel independence, decomposing multivariate inputs into univariate series to support any-variate forecasting, thereby ignoring inter-series correlations
- Resolution: An architectural extension incorporating a correlation modeling module (e.g., graph attention or channel-mixing layers) and a comparative evaluation on datasets where variables are highly correlated

**Open Question 2**: Can the framework be adapted to support flexible distribution forecasting rather than point estimates?
- Basis: The authors identify "flexible distribution forecasting" in the Conclusion as a specific area for future work to enhance the model's time series capabilities
- Why unresolved: The current training paradigm minimizes Mean Squared Error (MSE) via the loss functions, which is optimized for deterministic point prediction rather than probabilistic uncertainty quantification
- Resolution: Modifying the output head to predict distribution parameters (e.g., using Gaussian negative log-likelihood or quantile loss) and evaluating performance using probabilistic metrics like CRPS

**Open Question 3**: Does the Energy-Aware Spectral Decomposition (EASD) module retain its effectiveness when applied to large-scale foundation models?
- Basis: While the paper demonstrates that a lightweight SEMPO with EASD outperforms large models without EASD, it does not test if adding EASD to a large model (e.g., Chronos-L) would further improve performance
- Why unresolved: The experiments are limited to a 6.5M parameter model; the "energy bias" identified in large models is solved in the small model context, but the interaction between EASD and massive model capacity is unknown
- Resolution: A comparative study applying the EASD module as a pre-processing layer to existing large-scale backbones (like Time-MoE or Chronos) and measuring the change in forecasting error

## Limitations
- Data scale claim discrepancy: Paper cites 83M time points but listed datasets sum to only 63M points
- Unspecified parameterization: Missing specific values for spectral masking probability parameter ρ and limit α
- Limited scalability validation: No testing of EASD effectiveness when applied to large-scale foundation models

## Confidence

**High Confidence (8/10)**:
- Overall model architecture and training pipeline
- Core performance claims (19.9%-36.0% MSE improvements)
- Ablation study methodology and results
- Dataset selection and evaluation protocols

**Medium Confidence (6/10)**:
- Specific spectral masking parameterization
- Exact data selection criteria for 83M points
- Optimal hyperparameter settings (τ initialization, reconstruction loss weighting)
- Router behavior in heterogeneous domains

**Low Confidence (4/10)**:
- Generalization beyond tested domains
- Performance in extremely few-shot scenarios (<5 samples)
- Computational efficiency claims under different hardware configurations
- Robustness to noise and distribution shifts

## Next Checks

1. **Spectral Decomposition Sensitivity Analysis**: Implement a controlled experiment varying the energy threshold τ initialization and masking parameters ρ/α to determine their impact on forecasting performance across different dataset characteristics (stationary vs. non-stationary, periodic vs. aperiodic).

2. **Data Scale Validation**: Systematically train models on increasing data scales (10M, 25M, 50M, 83M, 100M time points) to verify the claimed plateau effect and quantify the marginal benefit of additional pre-training data.

3. **Cross-Domain Router Analysis**: For a held-out dataset not in pre-training, visualize and quantify prompt expert specialization through gating score distributions and attention weight patterns, comparing with in-domain performance to assess the model's ability to adapt to truly novel temporal patterns.