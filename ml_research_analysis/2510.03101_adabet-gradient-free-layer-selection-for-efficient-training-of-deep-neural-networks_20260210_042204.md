---
ver: rpa2
title: 'AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural
  Networks'
arxiv_id: '2510.03101'
source_url: https://arxiv.org/abs/2510.03101
tags:
- training
- selection
- layers
- layer
- betti
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AdaBet, a gradient-free layer selection approach
  for efficient on-device training of deep neural networks. The method addresses the
  challenge of adapting pre-trained models to user-specific data distributions under
  limited compute and memory constraints on edge and mobile devices.
---

# AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks

## Quick Facts
- **arXiv ID**: 2510.03101
- **Source URL**: https://arxiv.org/abs/2510.03101
- **Reference count**: 40
- **Primary result**: Gradient-free layer selection achieves 5% higher accuracy than gradient-based baselines while reducing peak memory by 40% on average

## Executive Summary
AdaBet addresses the challenge of efficient on-device training of deep neural networks by selecting which layers to fine-tune without requiring backpropagation or labels. The method uses topological features—specifically the first Betti number of layer activations—to rank layers by their learning capacity, then normalizes by activation size to balance importance against computational cost. Evaluated across four DNN architectures and three datasets, AdaBet demonstrates that gradient-free selection can achieve superior accuracy-memory trade-offs compared to traditional fine-tuning approaches, making it practical for edge and mobile deployment where memory constraints are critical.

## Method Summary
AdaBet performs layer selection through a two-phase process: first, it accumulates activations from multiple forward passes through all layers, computes the first Betti number for each layer using persistent homology (via Ripser), and normalizes by activation size to create a cost-aware ranking. The top ρ fraction of layers (default 10%) are selected for fine-tuning while others remain frozen. During the training phase, only selected layers undergo backpropagation using standard optimization. The method eliminates the memory overhead of gradient computation during selection, requiring only forward passes, and achieves effective layer selection without any labels by relying purely on activation topology.

## Key Results
- Achieves 5% higher classification accuracy than gradient-based baselines on average
- Reduces peak memory consumption by 40% during training compared to full fine-tuning
- Works across diverse architectures (ResNet, VGG16, MobileNet, ViT) and datasets (Dogs, Pets, CUB, Flowers)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The first Betti number (b₁) of layer activations serves as a proxy for learning capacity, identifying layers that benefit most from fine-tuning.
- Mechanism: The first Betti number counts independent 1-dimensional holes (loops) in the activation space's topological structure. Layers with higher b₁ values exhibit greater representational complexity, which the paper links to Rademacher Average capacity. By ranking layers by normalized b₁, AdaBet prioritizes layers where parameter updates can meaningfully alter the learned representation.
- Core assumption: Topological complexity of activations correlates with a layer's potential to improve through fine-tuning on new data distributions.
- Evidence anchors:
  - [section] §4.2.1 states "the first Betti Number, b₁, measures the number of one-dimensional holes... and is directly linked to the Rademacher Average capacity of a layer" and that "higher capacity is generally associated with an increased risk of overfitting, such layers may be more suitable candidates for fine-tuning."
  - [section] Figure 3 shows that Betti number patterns correlate with DBSCAN cluster counts, suggesting b₁ captures representational structure.
  - [corpus] No direct corpus validation of the Betti–capacity link was found; neighboring papers do not address topological layer selection.

### Mechanism 2
- Claim: Normalizing Betti numbers by activation size (|aᵢ|) balances representational importance against computational cost.
- Mechanism: Raw b₁ values tend to scale with activation volume, biasing selection toward large layers regardless of efficiency. Dividing by |aᵢ| yields a cost-aware score: ˆbᵢ₁ = bᵢ₁ / |aᵢ|. This penalizes layers where storing activations for backpropagation would consume disproportionate memory, aligning selection with on-device constraints.
- Core assumption: Memory footprint during training correlates with activation size; layers with high normalized b₁ provide better accuracy-per-memory return.
- Evidence anchors:
  - [section] §4.2.4 notes "the first Betti Number, b₁... tends to be slightly higher in layers with a larger number of activations" and "in convolutional layers, the memory needed to store activation values is often significantly greater than that required for storing the layer's parameters."
  - [section] Figure 8 (ablation) shows activation normalization outperforms no normalization and parameter normalization in accuracy–memory trade-off at ρ=0.1.
  - [corpus] Corpus does not provide external validation of this normalization strategy.

### Mechanism 3
- Claim: Gradient-free selection eliminates the memory overhead of backpropagation during the selection phase, reducing peak memory by ~40% on average.
- Mechanism: Standard backpropagation requires storing all intermediate activations and gradients (≈3× forward-pass memory). AdaBet computes b₁ using only forward-pass activations, which can be discarded after selection. Only selected layers undergo backpropagation during retraining, and non-selected layers remain frozen, avoiding gradient storage for the full network.
- Core assumption: On-device memory is the primary bottleneck; forward-pass computation for selection is acceptable overhead.
- Evidence anchors:
  - [abstract] "AdaBet achieves an average 5% higher classification accuracy than gradient-based baselines while reducing peak memory consumption by 40% on average."
  - [section] Table 4 shows peak training memory reductions: ResNet50 ↓66.31%, ViT ↓75.77%, MobileNetV2 ↓14.74%, VGG16 ↓5.72% vs. Full Training.
  - [section] §1 notes "the backward pass typically consumes 3× more memory than the forward pass due to gradient and activation storage."
  - [corpus] Corpus neighbors confirm gradient-free methods address memory/energy constraints but do not replicate AdaBet's specific gains.

### Mechanism 4 (Secondary)
- Claim: Activations can be pooled across multiple small-batch forward passes without backpropagation, enabling reliable b₁ estimation under tight memory limits.
- Mechanism: Small batches yield sparse activation samples, making b₁ unstable. AdaBet accumulates activations from multiple forward passes (e.g., 5 batches of size 8 → effective batch 40) before computing b₁. Since only forward passes are needed, this avoids gradient memory overhead.
- Core assumption: Layer topology stabilizes with sufficient activation samples; batch order does not introduce systematic bias.
- Evidence anchors:
  - [section] §4.2.2 describes activation pooling: "we can run several forward passes... and collect all their activations before computing Betti Numbers."
  - [section] Figure 12 shows accuracy improves with more accumulated batches, plateauing around 5 batches.
  - [corpus] No corpus papers address this specific pooling strategy.

## Foundational Learning

- **Concept: Persistent Homology and Betti Numbers**
  - Why needed here: Core to understanding how AdaBet quantifies layer "learning capacity" without gradients.
  - Quick check question: Can you explain what b₀, b₁, and b₂ represent for a torus?

- **Concept: Backpropagation Memory Dynamics**
  - Why needed here: Motivates why gradient-free selection matters; you must understand activation/gradient storage costs to interpret the 40% memory reduction claim.
  - Quick check question: Why does the backward pass typically require ~3× the memory of the forward pass?

- **Concept: Transfer Learning and Layer Freezing**
  - Why needed here: AdaBet is a method for selecting which layers to fine-tune; understanding standard transfer learning (freeze all but final layer) provides the baseline context.
  - Quick check question: In vanilla transfer learning, which layers are typically updated, and what are the limitations of this approach?

## Architecture Onboarding

- **Component map:**
  Input batch -> Forward pass through all layers -> Capture activations aᵢ -> Compute b₁ for each layer via Ripser -> Normalize: ˆbᵢ₁ = bᵢ₁ / |aᵢ| -> Rank layers -> Select top ρ fraction -> Freeze non-selected layers -> Retrain selected layers

- **Critical path:**
  1. Accumulate activations from multiple forward passes (default: 5 batches × size 8 = 40 samples)
  2. Compute first Betti number per layer using persistent homology library (Ripser)
  3. Normalize by activation size, rank, select top ρ (default: 0.1 = 10% of layers)
  4. Execute standard fine-tuning on selected layers only

- **Design tradeoffs:**
  - ρ (layer proportion): Lower ρ → less memory, faster training, but potential underfitting. Default 0.1 works across tested setups; may need increase for fine-grained tasks (e.g., CUB-200)
  - Batch size vs. accumulation: Small batch sizes fit memory but require more accumulation passes; large batches reduce passes but spike memory
  - Normalization strategy: Activation-size normalization balances accuracy/memory best (validated in ablation); parameter normalization degrades performance

- **Failure signatures:**
  - Extremely low b₁ variance across layers (flat ranking) → selection becomes near-random; check data diversity or model saturation
  - High accuracy variance across runs → likely insufficient activation accumulation; increase pooled batches
  - Memory still high despite low ρ → verify that non-selected layers are fully frozen (no gradient computation)

- **First 3 experiments:**
  1. Replicate single-dataset result (e.g., ResNet50 on Oxford-IIIT Pets): Measure accuracy vs. Full Training and Transfer Learning; profile peak memory to confirm ~60%+ reduction
  2. Ablation on ρ: Sweep ρ ∈ {0.05, 0.1, 0.2, 0.5, 1.0} on one model/dataset; plot accuracy, memory, and training time to identify knee point
  3. Batch accumulation sensitivity: Fix ρ=0.1, vary accumulated batches {1, 3, 5, 7, 10}; measure selection stability (rank correlation) and final accuracy to validate 5-batch default

## Open Questions the Paper Calls Out

- **Hardware-aware integration**: How can AdaBet integrate hardware-specific constraints, such as FLOPs, latency, and energy costs, into its layer ranking mechanism?
- **Distribution shift stability**: How do gradual or abrupt runtime distribution shifts impact the stability of Betti Number rankings and the resulting accuracy?
- **Finer granularity selection**: Can topological features be effectively computed at a finer, sub-layer granularity to support tensor-level parameter selection?

## Limitations
- Theoretical foundation linking Betti numbers to learning capacity is asserted but not rigorously proven or empirically validated
- Computational overhead of multiple forward passes for activation accumulation may be prohibitive for extremely resource-constrained devices
- Generalization beyond vision models and natural image datasets remains untested

## Confidence

- **High Confidence**: Memory reduction claims (40% average), computational mechanism (forward-only selection), and basic topological computation methodology
- **Medium Confidence**: Accuracy improvements (5% average over baselines), dependent on implementation details of topological computation
- **Low Confidence**: Theoretical link between Betti numbers and learning capacity, and general applicability to non-vision domains

## Next Checks

1. **Reproduce topological selection on a single dataset**: Implement AdaBet's layer selection on ResNet50 with Oxford-IIIT Pets, comparing selected layers against the paper's Figure 9. Verify the same layers are consistently selected across runs.

2. **Validate memory claims**: Profile peak memory usage during AdaBet's selection phase versus standard fine-tuning, confirming the ~3× memory reduction during selection as claimed.

3. **Test ablation on ρ parameter**: Systematically vary ρ across {0.05, 0.1, 0.2, 0.5, 1.0} on one model-dataset pair, measuring accuracy, memory, and training time to identify the knee point in the accuracy-memory tradeoff curve.