---
ver: rpa2
title: 'Emergent Lexical Semantics in Neural Language Models: Testing Martin''s Law
  on LLM-Generated Text'
arxiv_id: '2511.21334'
source_url: https://arxiv.org/abs/2511.21334
tags:
- text
- training
- semantic
- martin
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically tests Martin's Law - the relationship
  between word frequency and polysemy - in neural language models across training.
  Using DBSCAN clustering of contextualized embeddings, the study analyzes four Pythia
  models (70M-1B parameters) at 30 training checkpoints.
---

# Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text

## Quick Facts
- arXiv ID: 2511.21334
- Source URL: https://arxiv.org/abs/2511.21334
- Reference count: 14
- Primary result: Martin's Law compliance follows non-monotonic trajectory with peak correlation (r > 0.6) at intermediate training checkpoint

## Executive Summary
This paper systematically tests Martin's Law - the relationship between word frequency and polysemy - in neural language models across training. Using DBSCAN clustering of contextualized embeddings, the study analyzes four Pythia models (70M-1B parameters) at 30 training checkpoints. Results reveal a non-monotonic developmental trajectory: Martin's Law emerges around checkpoint 100, peaks at checkpoint 104 (r > 0.6), then degrades by checkpoint 105. Smaller models experience catastrophic semantic collapse at late checkpoints, while larger models show graceful degradation. The frequency-specificity tradeoff remains stable across all models.

## Method Summary
The study analyzes four Pythia models (70M, 160M, 410M, 1B parameters) at 30 checkpoints logarithmically spaced from initialization to ~10^5 steps. For each checkpoint, 100 text samples of 512 tokens each are generated (temperature=1.0). Final-layer hidden states are extracted for alphabetic tokens ≥3 characters, and DBSCAN clustering (ε=0.3, min_samples=2, cosine distance) groups context-sensitive usages per word. Polysemy is operationalized as cluster count per word (excluding noise points), and Spearman correlations are computed between frequency and polysemy, and between frequency and specificity, for the top 500 most frequent words.

## Key Results
- Martin's Law follows non-monotonic trajectory: emerges at checkpoint 100, peaks at checkpoint 104 (r > 0.6), then degrades by checkpoint 105
- Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation
- Frequency-specificity tradeoff remains stable (r ≈ -0.3) across all models throughout training

## Why This Works (Mechanism)

### Mechanism 1
Martin's Law compliance follows a non-monotonic trajectory with an optimal intermediate checkpoint. Early training develops distributional semantic structure that yields frequency-polysemy correlations (r > 0.6 at checkpoint ~10^4). Late training introduces competing pressures—potentially memorization or collapse into degenerate solutions—that override this structure. The degradation reflects optimization dynamics rather than measurement artifact (the clustering pipeline remains constant across checkpoints).

### Mechanism 2
Model capacity determines whether polysemous representations can be maintained under continued training. Smaller models (70M, 160M) lack parameter space to compress semantic structure while optimizing next-token prediction, leading to catastrophic collapse where polysemous word count drops to zero. Larger models (410M, 1B) have sufficient capacity for semantic reorganization rather than collapse.

### Mechanism 3
DBSCAN clustering of contextualized embeddings provides a valid operationalization of word senses. Final-layer hidden states capture context-sensitive meaning; DBSCAN groups usages into sense clusters without pre-specifying cluster count; cluster count per word serves as polysemy proxy P(w) = |clusters(w)|. This approach has been validated for sense induction tasks and does not require pre-specifying the number of clusters.

## Foundational Learning

- Concept: Martin's Law (frequency-polysemy correlation)
  - Why needed here: This is the core linguistic regularity being tested; understanding that high-frequency words tend to have more meanings is essential to interpreting the correlation coefficients.
  - Quick check question: In natural language, would you expect "run" (high frequency) or "photosynthesis" (low frequency) to have more distinct senses?

- Concept: Contextualized embeddings
  - Why needed here: The methodology depends on extracting context-sensitive vector representations; without this, the clustering approach cannot capture sense variation.
  - Quick check question: Why would the same word type (e.g., "bank") need different embeddings in "river bank" vs. "bank account"?

- Concept: DBSCAN clustering parameters (ε, min_samples)
  - Why needed here: The choice of ε = 0.3 and min_samples = 2 directly affects polysemy estimates; understanding density-based clustering is critical for interpreting results.
  - Quick check question: What happens to cluster count if ε is set too small? Too large?

## Architecture Onboarding

- Component map: Pythia model suite (70M, 160M, 410M, 1B) -> Text generation (100 samples × 512 tokens) -> Embedding extraction (final-layer hidden states) -> DBSCAN clustering (ε=0.3, min_samples=2) -> Per-word polysemy counting -> Spearman correlation computation

- Critical path: Checkpoint loading → Text generation → Embedding extraction → Per-word clustering → Correlation computation
  - Bottleneck: DBSCAN must run separately for each word appearing ≥5 times (top 500 most frequent)

- Design tradeoffs:
  - DBSCAN vs. alternatives: No pre-specified K (advantage) but sensitive to ε choice (limitation acknowledged in Section 6.1)
  - Final-layer vs. earlier layers: Final layer chosen but not justified; may miss intermediate semantic representations
  - Top 500 words only: Ensures reliable clustering but excludes low-frequency behavior

- Failure signatures:
  - Semantic collapse: Polysemous word count → 0 (small models at late checkpoints)
  - Graceful degradation: Correlation declines but polysemy persists (large models)
  - Flat semantic space: Frequency-specificity correlation stuck at r ≈ -0.3 (weaker than natural language's r ≈ -0.5 to -0.7)

- First 3 experiments:
  1. Replicate pipeline on a single checkpoint to validate clustering output—verify that "run" produces multiple clusters and "photosynthesis" produces one.
  2. Sweep ε values (0.2–0.5) at checkpoint 10^4 to assess sensitivity of Martin's Law correlation to granularity.
  3. Compare checkpoint 10^4 vs. 10^5 for 70M model: manually inspect generated samples to confirm semantic impoverishment vs. clustering artifact.

## Open Questions the Paper Calls Out

### Open Question 1
Does the peak Martin's Law correlation (ρ > 0.6) reflect genuine convergence to human-like semantic structure or a coincidental statistical pattern? Without a human baseline from the training corpus, the authors cannot determine whether the peak correlation represents successful learning or merely a coincidental pattern. Evidence would require applying the identical DBSCAN methodology to the Pile dataset to establish a ground-truth correlation baseline.

### Open Question 2
Do specific training dynamics, such as learning rate schedules or optimization objectives, causally determine the location of the semantic peak and subsequent degradation? The current analysis is descriptive and observational; it identifies the non-monotonic trajectory but does not isolate the training variables causing the degradation. Evidence would require intervention studies varying learning rate schedules and loss functions to observe shifts in the degradation phase.

### Open Question 3
Are the observed semantic collapse and non-monotonic trajectory artifacts of the DBSCAN clustering algorithm and fixed hyperparameters? Density-based clustering may struggle with varying embedding distributions during training, potentially misclassifying late-stage embeddings as noise (collapse) rather than distinct senses. Evidence would require replicating the analysis across a range of ε values and comparing results with Gaussian Mixture Models or Agglomerative Clustering to verify robustness.

## Limitations
- The study relies on model-generated text rather than naturally occurring language, raising questions about whether observed patterns reflect genuine linguistic regularities
- DBSCAN clustering with fixed ε = 0.3 may not reliably capture semantic shifts as embedding space evolves during training
- The frequency-specificity tradeoff (r ≈ -0.3) is notably weaker than natural language patterns (r ≈ -0.5 to -0.7), suggesting model behavior may not reflect genuine linguistic universals

## Confidence
- High confidence: The non-monotonic trajectory finding (emergence → peak → degradation) is well-supported by consistent patterns across all four models
- Medium confidence: The catastrophic collapse interpretation for small models depends on the assumption that zero polysemy reflects genuine semantic impoverishment rather than clustering artifacts
- Low confidence: The frequency-specificity tradeoff stability claim is problematic since this value is notably weaker than natural language patterns

## Next Checks
1. **ε sensitivity analysis**: Systematically vary DBSCAN's ε parameter (0.2, 0.3, 0.4, 0.5) at multiple checkpoints to determine whether the non-monotonic pattern persists across different clustering granularities.
2. **Cross-validation with alternative sense induction**: Apply an independent method (e.g., BERT-based contextual similarity or WordNet-based sense counting on generated text) to verify that small models genuinely produce semantically impoverished text rather than just failing DBSCAN clustering.
3. **Human validation study**: Have linguists annotate a subset of generated text samples for sense variation, comparing small vs. large models at late checkpoints to determine whether the clustering results align with human perception of polysemy.