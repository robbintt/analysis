---
ver: rpa2
title: Generalising E-prop to Deep Networks
arxiv_id: '2512.24506'
source_url: https://arxiv.org/abs/2512.24506
tags:
- time
- recurrent
- e-prop
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the E-prop algorithm, a biologically plausible
  approximation of Real-Time Recurrent Learning (RTRL), to deep recurrent neural networks
  with multiple layers. The key innovation is a recursive algorithm that propagates
  eligibility traces both across time and depth using local learning rules at synapses.
---

# Generalising E-prop to Deep Networks

## Quick Facts
- arXiv ID: 2512.24506
- Source URL: https://arxiv.org/abs/2512.24506
- Reference count: 7
- Primary result: Theoretical extension of E-prop algorithm to deep recurrent networks using nested recursions

## Executive Summary
This paper extends the E-prop algorithm, a biologically plausible approximation of Real-Time Recurrent Learning (RTRL), to deep recurrent neural networks with multiple layers. The key innovation is a recursive algorithm that propagates eligibility traces both across time and depth using local learning rules at synapses. The method maintains E-prop's computational efficiency (linear in sequence length and depth) while enabling accurate credit assignment across arbitrarily deep networks. The core approach uses two nested recursions: one for temporal credit assignment within layers and another for hierarchical credit assignment between layers. This allows online, forward-mode gradient computation without backpropagation through time, making it more plausible for brain-like learning systems.

## Method Summary
The paper presents a mathematical framework for deep E-prop using two nested recursions that propagate eligibility traces across both time and depth. The temporal recursion within each layer follows: ε_h,t = ∂h_t/∂θ + ∂h_t/∂h_{t-1}·ε_h,t-1, while the hierarchical recursion between layers uses: ε_z,t = ∂z_t/∂h_t·ε_h,t + ∂z_t/∂z_{t-1}·ε_z,t-1. These combine to handle arbitrary depth. The algorithm maintains local synaptic computation requirements by storing scalar traces that are updated using only locally accessible derivative information and incoming trace values from previous timesteps and lower layers. This approach approximates the RTRL sensitivity matrix while reducing computational complexity from quartic to quadratic in hidden dimension.

## Key Results
- Mathematical framework extends E-prop to arbitrary depth using nested recursions
- Maintains computational efficiency (linear in sequence length and depth)
- Provides biologically plausible alternative to backpropagation through time
- Identifies open questions about parameter management and online weight updates

## Why This Works (Mechanism)

### Mechanism 1: Partial Derivative Approximation
Replacing the total derivative with the partial derivative in the RTRL sensitivity matrix approximation reduces computational complexity from quartic to quadratic in hidden dimension while preserving useful gradient information. E-prop approximates dh/dθ ≈ ∂h/∂θ, ignoring indirect parameter influences mediated through other hidden states across time. This means each parameter column is assumed to only directly affect its corresponding hidden unit, eliminating the need to track cross-parameter interactions. The core assumption is that indirect influences of parameters on distant hidden states are negligible for effective learning.

### Mechanism 2: Nested Dual Recursion
Credit assignment across both time and depth can be decomposed into two independent recursive traces that combine at each network node. Two eligibility traces operate simultaneously—ε_h propagates gradients temporally within each layer via ε_h,t = ∂h_t/∂θ + ∂h_t/∂h_{t-1}·ε_h,t-1, while ε_z propagates gradients hierarchically via ε_z,t = ∂z_t/∂h_t·ε_h,t + ∂z_t/∂z_{t-1}·ε_z,t-1. The combined recurrence handles arbitrary depth. The core assumption is that the gradient path structure forms a two-dimensional lattice where backward traversal can be decomposed into horizontal (time) and vertical (depth) movements that sum independently.

### Mechanism 3: Local Synaptic Trace Storage
Each synapse can independently maintain and update eligibility traces using only locally accessible derivative information and incoming trace values. Synapses store scalar traces updated via ε_l,t = ∂h^l_t/∂h^l_{t-1}·ε^l_{t-1} + K where K depends on whether the synapse is terminal (∂h/∂θ) or receives hierarchical input (∂h/∂h^{l-1}·ε^{l-1}). No global coordination required. The core assumption is that synapses have access to (1) derivative of their activation function, (2) derivative of hierarchical projection from layer below, and (3) incoming eligibility trace from previous timestep and lower layer.

## Foundational Learning

- **Concept: Backpropagation Through Time (BPTT)**
  - Why needed: Understanding what E-prop approximates and why it's considered biologically implausible; BPTT is the baseline for comparison.
  - Quick check question: Explain why BPTT requires memory proportional to sequence length and cannot update weights online.

- **Concept: Forward-Mode vs Reverse-Mode Automatic Differentiation**
  - Why needed: RTRL and E-prop use forward-mode accumulation, fundamentally different from backprop's reverse-mode; this determines complexity tradeoffs.
  - Quick check question: Why does forward-mode differentiation through an RNN produce an H³ sensitivity tensor instead of scalar gradients?

- **Concept: Eligibility Traces in Reinforcement Learning**
  - Why needed: The biological motivation draws from TD-learning eligibility traces; understanding the analogy clarifies what information these traces encode.
  - Quick check question: What does an eligibility trace decay rate control, and how does this relate to temporal credit assignment horizon?

## Architecture Onboarding

- **Component map:**
  - EligibilityTrace[l][t]: Per-layer temporal trace, shape matches layer parameters
  - HierarchicalTrace[l][t]: Per-layer depth trace, aggregates gradients from lower layers
  - DerivativeCache: Stores ∂h/∂h_{t-1}, ∂z/∂h computed during forward pass
  - GradientAccumulator: Collects trace × loss-gradient products for weight updates

- **Critical path:**
  1. Forward pass: Compute h^l_t for all layers, cache activation derivatives
  2. Trace update: For each layer bottom-to-top, update ε^l_t = temporal_contribution + hierarchical_contribution
  3. Loss computation: At episode end, compute ∂L/∂output
  4. Gradient synthesis: ∂L/∂θ = ∂L/∂output × ε^{output_layer}_T

- **Design tradeoffs:**
  - Separate recurrent/input/output weights → 3× trace memory per layer (limitation noted in paper)
  - Online weight updates → eligibility traces become "off-policy" as weights drift
  - Deeper networks → linear trace memory growth but potential approximation degradation

- **Failure signatures:**
  - Trace explosion: Unbounded growth when spectral radius of recurrent dynamics > 1
  - Approximation collapse: Gradient magnitude → 0 faster than BPTT on long-dependency tasks
  - Memory blowup: Per-parameter-group traces exceed available memory for deep networks with separate weight types

- **First 3 experiments:**
  1. Single-layer parity check: Validate implementation matches original E-prop on copy/memory tasks from Bellec et al. (2020)
  2. Two-layer temporal XOR: Compare deep E-prop vs BPTT convergence on task requiring both depth and temporal credit assignment
  3. Depth scaling sweep: Measure gradient approximation error (||∇_E-prop - ∇_BPTT||) as function of network depth at fixed width

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the E-prop approximation perform when performing credit assignment across both depth and time for large networks on challenging tasks?
- Basis in paper: [explicit] The authors state: "it remains unclear how effective the E-prop approximation remains when performing credit assignment across both depth and time for large networks in challenging tasks."
- Why unresolved: The paper provides only a mathematical derivation with no experimental validation.
- What evidence would resolve it: Benchmarks comparing deep E-prop to BPTT on standard deep RL and sequence modeling tasks, measuring convergence speed and final performance.

### Open Question 2
- Question: Can deep E-prop scale to realistic tasks requiring reasoning and long-context recall, such as language modeling?
- Basis in paper: [explicit] The authors note: "Scaling and testing the performance of approximate algorithms like this on realistic and challenging tasks, especially based on language and requiring both reasoning and long-context recall is an open question in the field."
- Why unresolved: E-prop has only been tested on single-layer networks with relatively straightforward tasks.
- What evidence would resolve it: Experiments on language benchmarks (e.g., WikiText, Penn Treebank) comparing perplexity and training efficiency against BPTT baselines.

### Open Question 3
- Question: How can gradient information be transmitted back to originating layers for actual weight updates in a biologically plausible manner?
- Basis in paper: [explicit] The authors identify that "All the gradient information gets forwarded 'up and to the right'... To actually perform the weight update, this information would need to be transmitted back to its originating layers somehow."
- Why unresolved: The mathematical framework computes gradients at the output but provides no mechanism for backward transmission to earlier layers.
- What evidence would resolve it: A proposed mechanism for feedback communication and experiments showing it maintains learning performance.

### Open Question 4
- Question: Can truly online weight updates be achieved without eligibility traces becoming off-policy?
- Basis in paper: [explicit] The authors state: "It is possible to update weights online but this results in the eligibility traces becoming subtly off-policy since they would then be accumulating gradient information from stale weights."
- Why unresolved: The brain appears to perform synaptic plasticity online, but the mathematical consequences of online updates in E-prop remain unanalyzed.
- What evidence would resolve it: Theoretical analysis of off-policy drift magnitude and experiments comparing online vs. episode-end updates on performance.

## Limitations
- No experimental validation provided despite being theoretical
- Unclear mechanism for transmitting gradients back to earlier layers for weight updates
- Separate parameter groups require 3× trace memory per layer

## Confidence
- Mathematical framework correctness: Medium
- Practical performance claims: Low
- Implementation feasibility: Low
- Scalability to deep networks: Low

## Next Checks
1. Implement the algorithm on a 2-layer RNN and validate gradient accuracy against BPTT on a simple copy task
2. Test gradient approximation quality across varying depths (2-5 layers) on sequential MNIST
3. Evaluate convergence speed and final performance on a language modeling benchmark compared to BPTT baseline