---
ver: rpa2
title: 'Evaluating Recabilities of Foundation Models: A Multi-Domain, Multi-Dataset
  Benchmark'
arxiv_id: '2508.21354'
source_url: https://arxiv.org/abs/2508.21354
tags:
- pens
- recommendation
- setting
- last
- books
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RECBENCH-MD, a comprehensive benchmark for
  evaluating the recommendation capabilities of foundation models across diverse datasets
  and domains. The study benchmarks 19 foundation models across 15 datasets spanning
  10 domains, covering eight fine-tuning settings ranging from zero-shot to multi-domain
  training.
---

# Evaluating Recabilities of Foundation Models: A Multi-Domain, Multi-Dataset Benchmark

## Quick Facts
- arXiv ID: 2508.21354
- Source URL: https://arxiv.org/abs/2508.21354
- Reference count: 22
- Primary result: RECBENCH-MD benchmark evaluates 19 foundation models across 15 datasets spanning 10 domains, finding in-domain fine-tuning optimal while cross-dataset and multi-domain training enhance adaptability

## Executive Summary
This paper introduces RECBENCH-MD, the first comprehensive benchmark for evaluating the recommendation capabilities of foundation models across diverse domains and datasets. The study systematically benchmarks 19 foundation models across 15 datasets spanning 10 domains, covering eight fine-tuning settings ranging from zero-shot to multi-domain training. The benchmark reveals that in-domain fine-tuning consistently achieves optimal performance, while cross-dataset transfer learning and multi-domain training significantly enhance model adaptability. Larger models particularly benefit from joint training, and the effectiveness of cross-domain fine-tuning depends heavily on the source dataset's characteristics. The work provides open-source code and datasets to facilitate future research in recommendation foundation models.

## Method Summary
The benchmark evaluates foundation models on pairwise user-item click prediction using 15 datasets across 10 domains, with standardized test sets (~20K samples) and fine-tune sets (~100K samples). Models are fine-tuned using LoRA with rank=32, alpha=128, learning rate=1e-4, and Adam optimizer. Two approaches are evaluated: prompt-based (Yes/No token logits) and embedding-based (cosine similarity). Eight fine-tuning settings are tested from zero-shot to multi-domain multi-dataset training. Primary metrics are AUC, NDCG, and MRR, with RRA@K measuring cross-dataset contribution. The benchmark runs on single Nvidia A100 GPU with early stopping on validation AUC.

## Key Results
- In-domain fine-tuning consistently achieves optimal recommendation performance compared to zero-shot and cross-domain settings
- Cross-dataset transfer learning provides effective practical support for new recommendation scenarios without requiring entity overlap
- Larger foundation models benefit disproportionately from joint training on multiple datasets or domains
- Effectiveness of cross-domain fine-tuning depends heavily on the source dataset's characteristics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-domain fine-tuning consistently yields superior recommendation performance compared to zero-shot and cross-domain settings, as models acquire domain-specific collaborative signals.
- **Mechanism:** Fine-tuning on data from the target domain aligns the foundation model's pre-trained semantic representations with domain-specific user-item interaction patterns, reducing the distributional gap between pre-training and deployment.
- **Core assumption:** The foundation model's pre-trained knowledge provides a sufficiently flexible representation that can be efficiently specialized to domain-specific patterns via gradient updates on limited in-domain data.
- **Evidence anchors:**
  - [abstract] "Our findings suggest that in-domain fine-tuning achieves optimal performance..."
  - [section 5.2] "Compared to zero-shot baselines (Setting B), domain-specific fine-tuning strategies (Setting C and Setting E) consistently achieve superior performance... This is primarily because large models have acquired domain-specific collaborative knowledge through fine-tuning."
  - [corpus] Weak direct evidence; corpus neighbors focus on multi-domain transfer rather than in-domain specialization.
- **Break condition:** If the target domain's interaction patterns are fundamentally inconsistent with the semantic priors encoded in the foundation model (e.g., highly specialized industrial systems with non-textual features), in-domain fine-tuning may yield diminishing returns or overfit to sparse signals.

### Mechanism 2
- **Claim:** Cross-dataset and cross-domain fine-tuning provide effective warm-up for new recommendation scenarios by transferring text-based collaborative knowledge without requiring explicit entity overlap.
- **Mechanism:** Text-based knowledge transfer leverages rich item descriptions and user profiles to learn transferable user interest patterns. When fine-tuned on a source dataset, the model encodes generalizable behavioral patterns that can partially transfer to target datasets, even across domains, via shared semantic structure.
- **Core assumption:** User interests and item semantics exhibit enough consistency across datasets/domains that patterns learned from textual features in one context can inform predictions in another, even without shared users or items.
- **Evidence anchors:**
  - [abstract] "...cross-dataset transfer learning provides effective practical support for new recommendation scenarios."
  - [section 5.3] "Cross-dataset fine-tuning generally improves recommendation performance... the effectiveness of cross-domain fine-tuning depends heavily on the source dataset's characteristics."
  - [corpus] Related work (e.g., Gaussian Mixture Flow Matching for Multi-Domain Sequential Recommendation) supports the feasibility of cross-domain transfer but focuses on sequential patterns rather than text-based transfer.
- **Break condition:** Negative transfer occurs when source and target domains have conflicting interaction dynamics or when source dataset quality is poor, leading to performance degradation below zero-shot baselines.

### Mechanism 3
- **Claim:** Larger foundation models benefit disproportionately from joint training on multiple datasets or domains, exhibiting stronger cross-domain generalization.
- **Mechanism:** Larger models have higher capacity to encode diverse patterns from multiple datasets simultaneously without catastrophic forgetting. Their richer pre-trained representations enable better abstraction of transferable features, allowing them to leverage auxiliary domain knowledge more effectively.
- **Core assumption:** The scaling laws that apply to general language tasks extend to recommendation tasks, where larger parameter counts enable more robust multi-task learning across heterogeneous datasets.
- **Evidence anchors:**
  - [abstract] "Larger models benefit more from joint training, and the effectiveness of cross-domain fine-tuning depends heavily on the source dataset's characteristics."
  - [section 5.4] "The performance gap between Setting H and Setting G narrows with larger models—for instance, the improvement on HM drops from 30.0% (BERT base) to 16.8% (Llama-38B)..."
  - [corpus] Limited direct evidence; corpus neighbors discuss multi-domain learning but not model scaling effects.
- **Break condition:** If computational constraints prohibit fine-tuning large models, or if training data is insufficient to regularize the larger capacity, joint training may lead to overfitting or optimization instability.

## Foundational Learning

- **Concept: Foundation Models (e.g., LLMs)**
  - **Why needed here:** The entire benchmark evaluates how well general-purpose foundation models can perform recommendation tasks, requiring understanding of their pre-training objectives and capabilities.
  - **Quick check question:** Can you explain the difference between a foundation model pre-trained on general text and a specialized recommendation model trained only on interaction data?

- **Concept: Recommendation Paradigms (Prompt-based vs. Embedding-based)**
  - **Why needed here:** RECBENCH-MD evaluates two distinct approaches—prompt-based ranking and embedding-based matching—which leverage foundation models differently.
  - **Quick check question:** How does a prompt-based approach differ from an embedding-based approach in terms of how the model processes user-item pairs?

- **Concept: Transfer Learning in Recommendation**
  - **Why needed here:** The benchmark's core contribution is evaluating cross-dataset and cross-domain transfer, a key challenge in deploying recommendation foundation models.
  - **Quick check question:** Why might a model fine-tuned on book recommendations help with movie recommendations, and what are the risks?

## Architecture Onboarding

- **Component map:** Datasets -> Preprocessing -> Fine-tuning -> Evaluation -> Analysis
- **Critical path:**
  1. **Data preprocessing:** Standardize datasets, truncate user sequences to 20 items, extract textual features
  2. **Fine-tuning:** Apply LoRA to foundation model with specified setting (e.g., single-domain, multi-domain)
  3. **Evaluation:** Compute metrics on test set; aggregate results across datasets for multi-domain settings
  4. **Analysis:** Compare performance across settings, models, and domains; identify transfer patterns

- **Design tradeoffs:**
  - **Dataset diversity vs. cost:** Including more domains improves generalization testing but increases computational overhead
  - **Prompt-based vs. embedding-based:** Prompt-based aligns better with LLM pre-training but may be less efficient for large-scale retrieval; embedding-based is scalable but less semantically rich
  - **Joint training vs. sequential fine-tuning:** Joint training (Setting H) maximizes performance but risks domain interference; sequential fine-tuning (appendix) offers flexibility with potential catastrophic forgetting

- **Failure signatures:**
  - **Zero-shot collapse:** Performance near random (AUC ≈ 0.5) on all datasets, indicating model lacks recommendation capability
  - **Negative transfer:** Cross-dataset fine-tuning degrades performance below zero-shot, suggesting source-target mismatch
  - **Optimization instability:** Large fluctuations in validation AUC across runs, especially with small datasets or aggressive learning rates

- **First 3 experiments:**
  1. **Zero-shot baseline (Setting B):** Evaluate all 19 models on all 10 test datasets without fine-tuning to establish intrinsic RECABILITIES
  2. **Single-domain fine-tuning (Setting C):** Fine-tune a subset of models (e.g., BERT, OPT, Llama) on each dataset individually and evaluate on matching test set to measure in-domain performance
  3. **Cross-dataset transfer (Setting F):** Fine-tune models on source datasets (e.g., H&M, MIND) and evaluate on all target datasets to quantify transferability and identify best source datasets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do foundation models perform on sequential recommendation tasks compared to the current pair-wise click prediction benchmark?
- **Basis in paper:** [explicit] Appendix A states the current evaluation "does not encompass sequential recommendation, which represents a crucial area for future development and enhancement."
- **Why unresolved:** The benchmark currently limits evaluation to matching and ranking paradigms based on user-item pairs, ignoring the temporal dynamics of user behavior.
- **What evidence would resolve it:** Extending RecBench-MD to include sequential datasets and metrics (e.g., Hit Ratio over time) to evaluate temporal user modeling capabilities.

### Open Question 2
- **Question:** Can advanced optimization strategies mitigate the performance degradation caused by optimization conflicts in multi-dataset training?
- **Basis in paper:** [inferred] Section 5.2 notes that Setting E (in-domain multi-dataset) underperforms Setting C on datasets like Goodreads and H&M due to "optimization conflicts between datasets."
- **Why unresolved:** The paper identifies the negative transfer but does not test potential interventions such as gradient surgery or adaptive loss weighting.
- **What evidence would resolve it:** Experiments applying conflict-mitigation techniques to the identified problematic dataset pairs to see if multi-dataset training can surpass single-dataset performance.

### Open Question 3
- **Question:** How can the optimal sequential fine-tuning order be determined dynamically without prior knowledge of single-dataset performance ranks?
- **Basis in paper:** [inferred] Appendix C.1 analyzes the "dominant influence" of the second fine-tuning step and suggests using lower-ranked data first, but relies on pre-existing rank knowledge.
- **Why unresolved:** The proposed heuristic is static and cannot be applied to novel domains where relative dataset utility is unknown.
- **What evidence would resolve it:** Developing a curriculum learning algorithm capable of dynamically assessing dataset utility during training to automate the ordering process.

## Limitations
- The benchmark focuses on datasets with textual features, potentially limiting generalizability to domains with non-textual or sparse content
- Fixed dataset sizes (20K test, 100K fine-tune) may not reflect production-scale requirements or real-world data distributions
- Evaluation metrics emphasize ranking quality but don't capture business-relevant outcomes like diversity, fairness, or user satisfaction

## Confidence

**High Confidence:** In-domain fine-tuning achieves optimal performance (supported by consistent experimental results across multiple datasets and settings, with clear performance gaps between zero-shot and fine-tuned models).

**Medium Confidence:** Cross-dataset transfer learning provides effective practical support for new recommendation scenarios (supported by empirical evidence, but effectiveness varies significantly based on source-target compatibility, with some cases showing negative transfer).

**Medium Confidence:** Larger models benefit more from joint training (supported by observed scaling effects, but the relationship between model size and multi-domain performance requires further validation across broader model families and training regimes).

## Next Checks

1. **Cross-Domain Transfer Robustness:** Systematically evaluate negative transfer cases by identifying dataset pairs where fine-tuning degrades performance below zero-shot baselines, then analyze feature space alignment to predict when transfer will fail.

2. **Scaling Law Validation:** Extend the model size analysis beyond the three tested sizes (BERT base, Llama-7B, Llama-38B) to include smaller and larger models, verifying whether the observed scaling benefits hold across the full spectrum of foundation model sizes.

3. **Real-World Deployment Testing:** Validate benchmark findings using production-scale datasets with millions of interactions, testing whether the relative performance rankings of fine-tuning settings remain consistent under realistic data volumes and computational constraints.