---
ver: rpa2
title: 'SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning'
arxiv_id: '2602.01990'
source_url: https://arxiv.org/abs/2602.01990
tags:
- task
- expert
- tasks
- instruction
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAME addresses catastrophic forgetting in multimodal continual
  instruction tuning by stabilizing both expert routing and expert functionality.
  It uses spectral-aware routing to decompose routing dynamics into task-relevant
  and history-preserving subspaces, updating only critical directions for new tasks
  while protecting those important for previous tasks.
---

# SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning

## Quick Facts
- **arXiv ID:** 2602.01990
- **Source URL:** https://arxiv.org/abs/2602.01990
- **Reference count:** 40
- **Primary result:** Achieves 66.82% average accuracy on CoIN benchmark, outperforming best baseline by 2.8%

## Executive Summary
SAME addresses catastrophic forgetting in multimodal continual instruction tuning by stabilizing both expert routing and expert functionality. The method uses spectral-aware routing to decompose routing dynamics into task-relevant and history-preserving subspaces, updating only critical directions for new tasks while protecting those important for previous tasks. It applies curvature-aware Riemannian scaling to regulate expert updates using historical input covariance, preventing overwriting of previously acquired behaviors. Additionally, SAME introduces adaptive expert activation to freeze low-utility yet historically important experts during training, reducing computation and cross-task interference.

## Method Summary
SAME combines three stabilization mechanisms for continual multimodal instruction tuning: spectral-aware routing, curvature-aware Riemannian scaling, and adaptive expert activation. Spectral-aware routing uses spectral decomposition to separate routing updates into subspaces for new tasks and historical preservation. Curvature-aware Riemannian scaling applies historical input covariance to scale expert updates, maintaining previously learned behavior patterns. Adaptive expert activation selectively freezes experts based on their historical importance and current utility, reducing computational overhead and preventing cross-task interference. The method is evaluated on the CoIN benchmark with tasks including TextVQA, ChartQA, and ImageNet classification.

## Key Results
- Achieves 66.82% average accuracy on CoIN benchmark, outperforming best baseline by 2.8%
- Demonstrates superior long-horizon stability, particularly on challenging tasks like TextVQA (60.69%) and ImageNet (90.21%)
- Reduces per-task training time by 32.1 minutes and GPU memory usage by 2.3K MiB/GPU
- Avoids formatting-induced forgetting patterns observed in baseline methods

## Why This Works (Mechanism)
The paper addresses catastrophic forgetting through three complementary stabilization mechanisms. Spectral-aware routing decomposes the routing dynamics into two subspaces: one capturing task-relevant changes and another preserving historical routing patterns. This ensures that updates for new tasks don't overwrite critical routing decisions learned from previous tasks. Curvature-aware Riemannian scaling uses historical input covariance to scale expert updates, effectively creating a Riemannian manifold where the geometry reflects the historical input distribution. This prevents updates from deviating too far from previously learned behavior manifolds. Adaptive expert activation introduces a selective freezing mechanism based on historical importance scores and current utility, reducing both computational overhead and interference between tasks by keeping low-utility but historically important experts inactive during training.

## Foundational Learning

**Spectral decomposition** - Why needed: To separate routing updates into task-specific and history-preserving components. Quick check: Verify that the decomposition matrix correctly isolates critical routing directions.

**Riemannian geometry in parameter space** - Why needed: To create a geometrically-aware update space that respects historical input distributions. Quick check: Confirm that curvature scaling matrices properly reflect input covariance structure.

**Mixture-of-Experts routing** - Why needed: To enable dynamic expert selection while maintaining stability across tasks. Quick check: Validate that gating mechanisms preserve task-specific routing patterns.

**Catastrophic forgetting prevention** - Why needed: To maintain performance on previously learned tasks while acquiring new capabilities. Quick check: Test retention of early task performance after training on later tasks.

**Multimodal instruction tuning** - Why needed: To handle diverse input types (text, images) with unified instruction-following capabilities. Quick check: Verify consistent performance across different modality combinations.

## Architecture Onboarding

**Component map:** Input → Expert Pool → Spectral Router → Curvature Scaling → Adaptive Activation → Output

**Critical path:** Input → Expert Pool → Output (with stabilization mechanisms modifying intermediate flows)

**Design tradeoffs:** The method trades computational overhead for stability, using historical information to guide updates. The spectral decomposition adds complexity but provides targeted preservation of routing patterns.

**Failure signatures:** Catastrophic forgetting in routing patterns, overwriting of expert functionality, computational inefficiency from unnecessary expert updates.

**3 first experiments:**
1. Baseline MoE performance without stabilization mechanisms
2. Spectral-aware routing only (no curvature scaling or adaptive activation)
3. Full SAME system with all three stabilization components

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation primarily on CoIN benchmark may not represent all real-world deployment scenarios
- Performance on non-image modality tasks and highly similar task semantics remains unclear
- Computational overhead of maintaining historical covariance matrices could become prohibitive at larger scales

## Confidence

**High confidence:** Core stabilization mechanisms (spectral-aware routing, curvature-aware scaling) based on established mathematical principles and consistent empirical improvements.

**Medium confidence:** Adaptive expert activation mechanism due to limited ablation studies on its individual contribution.

**Medium confidence:** Long-term stability claims, as evaluation covers multiple tasks but limited temporal horizons.

## Next Checks

1. **Cross-modal generalization test:** Evaluate SAME on multimodal tasks involving text, audio, and video to verify stabilization mechanisms generalize beyond image-centric benchmarks.

2. **Resource scaling analysis:** Measure training time and memory overhead when scaling to 100+ experts and longer task sequences to identify practical deployment limits.

3. **Semantic similarity stress test:** Design experiments where consecutive tasks have high semantic overlap (e.g., different fine-grained object categories) to test whether routing stabilization prevents appropriate adaptation to similar but distinct tasks.