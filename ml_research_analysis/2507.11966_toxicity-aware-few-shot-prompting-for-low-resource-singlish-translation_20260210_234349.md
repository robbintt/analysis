---
ver: rpa2
title: Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation
arxiv_id: '2507.11966'
source_url: https://arxiv.org/abs/2507.11966
tags:
- translation
- language
- singlish
- prompt
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a two-stage, human-in-the-loop framework for
  toxicity-preserving translation in low-resource settings. It focuses on Singlish-to-Chinese/Malay/Tamil
  translation, curating a few-shot prompt pool through iterative human annotation
  to capture slang, tone, and harmful expressions.
---

# Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation

## Quick Facts
- **arXiv ID**: 2507.11966
- **Source URL**: https://arxiv.org/abs/2507.11966
- **Reference count**: 29
- **Primary result**: Proposed framework shows GPT-4o mini translations approaching gold reference quality for Chinese (3.83 vs 4.07) and Malay (4.09 vs 4.08), but lagging in Tamil (2.49 vs 3.30).

## Executive Summary
This paper introduces a two-stage, human-in-the-loop framework for toxicity-preserving translation from Singlish to Chinese, Malay, and Tamil. The approach addresses the challenge that standard translation models sanitize toxic content, losing cultural nuance and intended tone. By curating a few-shot prompt pool through iterative human annotation, the framework captures slang, tone, and harmful expressions specific to Singlish. The method uses semantic similarity (via embeddings) and back-translation for model and prompt selection, showing that GPT-4o mini approaches gold reference quality for Chinese and Malay translations while revealing significant challenges for Tamil.

## Method Summary
The approach consists of two stages: first, iterative human annotation (3 rounds) to curate 20 few-shot examples from initial translations by multiple LLMs; second, dynamic k-shot selection where for each input sentence, the top-k most semantically similar examples are retrieved using text-embedding-3-large and combined with the input for translation. Models are evaluated using semantic similarity metrics (direct translation and back-translation) and human ratings on 1-5 scales for meaning and tone. The framework was tested on 200 sentences with human evaluation comparing to gold references.

## Key Results
- GPT-4o mini achieved human evaluation scores of 3.83 vs 4.07 (Chinese) and 4.09 vs 4.08 (Malay) compared to gold references
- Tamil translations significantly underperformed at 2.49 vs 3.30 compared to gold references
- Malay required more custom translations (8.8 per sentence) due to orthographic variability
- Back-translation similarity served as a cost-effective proxy but imperfectly captured toxicity preservation

## Why This Works (Mechanism)

### Mechanism 1
LLMs generally default to refusing or sanitizing toxic inputs due to safety alignment. By explicitly including verified, toxic-in-context examples in the prompt, the model conditions its generation on these demonstrations, shifting the output distribution toward preserving tone rather than neutralizing it. The 3-round annotation process filters out "safe" but inaccurate translations.

### Mechanism 2
Dynamic few-shot retrieval based on semantic similarity improves translation accuracy for code-mixed inputs compared to static prompting. Retrieving the top-k examples that are semantically closest to the input sentence provides the model with specific "local" analogies, helping map specific slang terms or sentence structures to the target language more effectively.

### Mechanism 3
Back-translation semantic similarity serves as a cost-effective proxy for human evaluation in low-resource settings. If Singlish → Target → Singlish' yields high similarity between original and reconstruction, it implies the Target translation preserved core semantic information, though this metric is "meaning-preserving" not explicitly "toxicity-preserving."

## Foundational Learning

- **Concept**: In-Context Learning (Few-Shot Prompting)
  - **Why needed here**: This is the primary lever used to bypass model safety training by providing input-output pairs in the prompt window
  - **Quick check question**: If you provide 5 examples of polite translations in the prompt but ask for a rude translation, how will the model likely respond?

- **Concept**: Embeddings and Cosine Similarity
  - **Why needed here**: Used for both the retrieval mechanism (finding similar examples) and the evaluation metric (comparing back-translations)
  - **Quick check question**: Why might cosine similarity on embeddings be a flawed metric for detecting "sarcasm" or "implicit toxicity"?

- **Concept**: Code-Switching / Code-Mixing
  - **Why needed here**: Singlish is a blend of syntax and vocab from multiple languages where standard NLP pipelines often fail
  - **Quick check question**: How does the "vocabulary mismatch" problem affect the retrieval of few-shot examples for a Creole language?

## Architecture Onboarding

- **Component map**: Source Corpus (Singlish safety dataset) → Curated Pool (20 human-verified pairs) → Retriever (embedding similarity) → Translator (GPT-4o mini) → Evaluator (semantic similarity)

- **Critical path**: The Human Annotation Loop (Stage 1). The entire system relies on the quality of those 20 curated examples. If humans provided "sanitized" translations to avoid discomfort, the "toxicity-preserving" capability is compromised before it even runs.

- **Design tradeoffs**:
  - Annotator Demographics: Public-sector volunteers vs. students (volunteers stricter/more moralizing)
  - Metric vs. Reality: Using back-translation similarity is cheap but imperfect (it misses tone); human eval is accurate but expensive

- **Failure signatures**:
  - "Politeness Trap": Model translates meaning correctly but strips profanity
  - Tamil Lag: Distinct drop in quality (2.49 vs 3.30 gold) suggesting few-shot signal too weak for morphologically rich languages
  - Orthographic Drift: Malay needs "custom translations" (8.8 per sentence) indicating model struggles with romanized slang spelling

- **First 3 experiments**:
  1. Pool Ablation: Run with "sanitized" vs "raw" pool to quantify toxicity loss from example selection
  2. k-Sensitivity Analysis: Vary k specifically for Tamil subset to determine if low score is from insufficient context or model incapacity
  3. Annotator Bias Audit: Compare pools from "strict" vs "colloquial" annotators to measure human loop variance

## Open Questions the Paper Calls Out

1. How can evaluation frameworks effectively detect implicit, context-dependent toxicity that is lost during translation but missed by embedding-based similarity metrics? The authors note some forms of harm may be implicit and lose force when translated, but these subtleties are not explicitly measured.

2. To what extent does annotator background (e.g., public-sector vs. student) skew the definition of "gold standard" toxicity in low-resource settings? The paper identifies conflict between strict public-sector volunteers and colloquial student annotators but does not quantify the impact on prompt pool quality.

3. Why do LLMs specifically struggle to retain expressive tone and profanity in Tamil compared to Chinese or Malay? While the paper attributes Tamil underperformance to "structural challenges" and "overly sanitized" outputs, the specific failure mode remains unidentified.

## Limitations
- The framework shows significant language-dependent variability, with Tamil underperforming substantially compared to Chinese and Malay
- Semantic similarity metrics fail to explicitly measure tone preservation, potentially conflating meaning accuracy with toxicity preservation
- Heavy reliance on human annotation for the few-shot pool introduces both cost and potential demographic bias

## Confidence

**High Confidence**: Framework design and implementation details are well-specified; methodology for iterative human annotation is clearly described and reproducible.

**Medium Confidence**: GPT-4o mini approaching gold reference quality for Chinese and Malay is supported by human evaluation, though margins suggest room for improvement.

**Low Confidence**: Back-translation similarity as reliable proxy for toxicity preservation is weak; metric measures semantic consistency not tone or cultural nuance.

## Next Checks

1. **Pool Ablation Study**: Generate parallel translations using a "sanitized" few-shot pool versus the original "raw" pool to directly measure how much toxicity preservation depends on specific examples chosen.

2. **Language-Specific k-Shot Sensitivity**: Systematically vary the number of retrieved examples (k=5, 10, 15, 20, 25) for each target language and measure both semantic similarity and human-rated tone preservation.

3. **Annotator Demographics Impact Analysis**: Create two parallel few-shot pools - one curated by "strict" annotators and one by "colloquial" annotators - then generate translations and conduct blind human evaluation to quantify human annotation variance.