---
ver: rpa2
title: Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher
  Sampling
arxiv_id: '2507.00518'
source_url: https://arxiv.org/abs/2507.00518
tags:
- vmf-exp
- action
- sampling
- large
- orono
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces von Mises-Fisher exploration (vMF-exp), a
  scalable method for exploring large action sets in reinforcement learning problems
  where hyperspherical embedding vectors represent these actions. vMF-exp involves
  initially sampling a state embedding representation using a von Mises-Fisher distribution,
  then exploring this representation's nearest neighbors, which scales to virtually
  unlimited numbers of candidate actions.
---

# Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling

## Quick Facts
- arXiv ID: 2507.00518
- Source URL: https://arxiv.org/abs/2507.00518
- Reference count: 40
- Method scales reinforcement learning exploration to virtually unlimited actions using hyperspherical embeddings

## Executive Summary
This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable method for exploring large action sets in reinforcement learning where actions are represented as hyperspherical embedding vectors. The approach addresses the fundamental scalability limitations of Boltzmann Exploration by sampling a state embedding representation using a von Mises-Fisher distribution and exploring its nearest neighbors, requiring only sampling a fixed-size vector regardless of the number of actions. Theoretical analysis shows that vMF-exp asymptotically maintains the same probability of exploring each action as Boltzmann Exploration while overcoming its computational bottlenecks. Extensive experiments on simulated data, public datasets, and large-scale deployment in a global music streaming service demonstrate that vMF-exp can effectively explore large action sets and recommend more diverse playlists without compromising performance compared to truncated alternatives.

## Method Summary
The von Mises-Fisher exploration method represents actions as points on a hypersphere and uses a von Mises-Fisher distribution to sample state embedding representations. Instead of computing softmax values for all actions as in traditional Boltzmann Exploration, vMF-exp samples a fixed-size vector d and explores the nearest neighbors in the embedding space. This approach scales to virtually unlimited numbers of candidate actions while maintaining theoretical guarantees about exploration probabilities. The method is particularly well-suited for scenarios where actions have natural embedding representations, such as items in recommendation systems or moves in board games.

## Key Results
- Achieves scalability to virtually unlimited action sets by requiring only sampling of fixed-size vectors, independent of action count
- Asymptotically maintains the same exploration probability as Boltzmann Exploration under theoretical assumptions
- Successfully deployed at scale on a global music streaming service, demonstrating practical relevance and effectiveness
- Enables more diverse playlist recommendations without performance degradation compared to truncated Boltzmann Exploration

## Why This Works (Mechanism)
The method leverages the geometric properties of hyperspherical embeddings where similar actions cluster together in embedding space. By sampling from a von Mises-Fisher distribution centered at the current state representation, the method naturally focuses exploration on actions that are semantically similar to the current context. The nearest-neighbor search in this continuous embedding space provides an efficient approximation of the full action set exploration that would be computationally prohibitive with traditional softmax-based methods. This geometric approach transforms the discrete action selection problem into a continuous sampling problem that scales independently of the action set size.

## Foundational Learning

1. **Hyperspherical Embeddings**
   - Why needed: Provides geometric structure for representing high-dimensional actions where similarity is meaningful
   - Quick check: Verify actions can be meaningfully embedded on a unit sphere with cosine similarity reflecting action similarity

2. **von Mises-Fisher Distribution**
   - Why needed: Enables directional sampling on the hypersphere that concentrates probability mass around mean direction
   - Quick check: Confirm concentration parameter κ appropriately controls exploration-exploitation tradeoff

3. **Nearest Neighbor Search in Embedding Space**
   - Why needed: Efficiently identifies relevant actions from sampled state representation without exhaustive search
   - Quick check: Validate that k-nearest neighbors algorithm performs well in the embedding space with appropriate distance metrics

## Architecture Onboarding

**Component Map**: State Embedding -> vMF Distribution Sampling -> Nearest Neighbor Search -> Action Selection

**Critical Path**: The method requires: (1) learned or pre-computed action embeddings, (2) current state representation, (3) vMF sampling with appropriate concentration parameter, and (4) efficient nearest neighbor search infrastructure. The most critical components are the embedding quality and the nearest neighbor search implementation, as poor embeddings or slow search will degrade performance.

**Design Tradeoffs**: The main tradeoff is between embedding dimension d and exploration quality - higher dimensions provide better action discrimination but increase computational cost of nearest neighbor search. The concentration parameter κ controls exploration-exploitation balance, with higher values focusing more tightly on similar actions. The method assumes embeddings are pre-computed or can be generated efficiently, which may not hold for all action types.

**Failure Signatures**: Common failure modes include: (1) poor action embeddings leading to ineffective exploration, (2) inappropriate concentration parameter causing either insufficient exploration or random wandering, (3) nearest neighbor search returning irrelevant actions due to embedding space structure, and (4) scalability issues when d becomes large despite theoretical guarantees.

**3 First Experiments**:
1. Verify vMF sampling produces expected concentration around mean direction with varying κ parameters
2. Test nearest neighbor search recall and precision on known embedding relationships
3. Compare exploration coverage between vMF-exp and Boltzmann Exploration on small action sets where both are computationally feasible

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on specific assumptions that may not hold in all practical scenarios
- Empirical validation is primarily focused on music streaming applications, limiting generalizability
- Computational complexity analysis assumes idealized conditions that may not account for real-world implementation challenges
- Performance in highly non-stationary environments with rapidly changing action sets has not been thoroughly explored

## Confidence

**High**: Core theoretical framework and basic scalability claims based on well-established mathematical properties of von Mises-Fisher distributions and hyperspherical geometry.

**Medium**: Experimental results within the music streaming domain demonstrate clear practical benefits, though may be domain-specific.

**Low**: Claims about general applicability across diverse reinforcement learning scenarios lack comprehensive validation across different domains and action space characteristics.

## Next Checks

1. Test vMF-exp on diverse reinforcement learning benchmarks beyond music recommendation, including domains with different action space characteristics and reward structures to validate generalizability.

2. Conduct extensive ablation studies to quantify the impact of different embedding dimensions and distribution parameters on exploration efficiency and identify optimal configurations for different problem types.

3. Evaluate performance in non-stationary environments with dynamic action sets to assess robustness to environmental changes and identify failure modes in evolving contexts.