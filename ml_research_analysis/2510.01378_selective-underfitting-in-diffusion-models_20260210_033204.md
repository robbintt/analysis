---
ver: rpa2
title: Selective Underfitting in Diffusion Models
arxiv_id: '2510.01378'
source_url: https://arxiv.org/abs/2510.01378
tags:
- score
- training
- diffusion
- region
- supervision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Diffusion models are trained to approximate a score function but\
  \ this leads to a paradox: perfect learning would just reproduce training data,\
  \ not create new samples. The authors argue that models solve this by selectively\
  \ underfitting\u2014accurately learning the score only in a restricted \"supervision\
  \ region\" (near training data) and extrapolating differently elsewhere."
---

# Selective Underfitting in Diffusion Models

## Quick Facts
- arXiv ID: 2510.01378
- Source URL: https://arxiv.org/abs/2510.01378
- Reference count: 40
- Primary result: Models generalize by selectively underfitting—accurately learning the score only in a restricted "supervision region" near training data and extrapolating differently elsewhere.

## Executive Summary
Diffusion models face a paradox: perfect score learning would reproduce training data, not generate novel samples. This paper resolves this by showing models solve it through selective underfitting—accurately fitting the score only within a restricted "supervision region" near training data and extrapolating differently elsewhere. The authors formalize this region and demonstrate via experiments that larger models improve fit inside this region but underfit more outside, enabling generalization. They also show that the size of this region affects memorization vs. generalization and propose a framework for analyzing generative performance by separating supervision loss from extrapolation behavior.

## Method Summary
The paper analyzes diffusion models trained with denoising score matching (DSM) on ImageNet 256×256 latents. They define a "supervision region" where training data density is high (thin shells around data points in high dimensions) and show that inference trajectories quickly leave this region. Models are evaluated by computing score error separately inside and outside this region across different architectures (SiT, U-Net) and scales. They introduce "Perception-Aligned Training" (PAT) as a framework where generation quality depends on both supervision efficiency (fitting data) and extrapolation efficiency (inductive bias quality). Key experiments include region size ablation studies and architectural comparisons to demonstrate selective underfitting and the PAT principle.

## Key Results
- Larger models improve supervision efficiency (lower loss on training data) but increase extrapolation underfitting, enabling better generalization
- Enlarging the supervision region forces memorization, while restricting it restores generalization
- REPA and convolutional architectures improve extrapolation efficiency without significantly harming supervision efficiency
- The PAT framework explains recent advances in diffusion model training and suggests general principles for efficient training

## Why This Works (Mechanism)

### Mechanism 1: High-Dimensional Concentration Creates Supervision Region
Training data distributions concentrate on thin "shells" around data points in high dimensions, restricting effective supervision to this narrow region. In high dimensions, Gaussian noise vectors have concentrated norms, creating a mixture of Gaussians where mass concentrates on thin spherical shells rather than filling space uniformly. The supervision region $T_t(\delta)$ is formally derived and shown to have high probability concentration bounds.

### Mechanism 2: Freedom of Extrapolation Enables Generalization
Generalization arises from "Freedom of Extrapolation"—the model underfits the empirical score outside the supervision region, allowing it to generate novel samples rather than memorizing. The empirical score inside the supervision region collapses to a vector pointing to the nearest training point (trivial solution). Because inference trajectories quickly leave this region, the model's inductive bias takes over, interpolating smoothly rather than pointing to specific training points.

### Mechanism 3: Perception-Aligned Training (PAT)
"Perception-Aligned Training" improves generation by ensuring the model's extrapolation behavior aligns with human perception rather than Euclidean distance. Standard diffusion spaces often have poor perceptual geometry. PAT techniques regularize the model so that extrapolations in "perceptually close" directions are favored, steering the direction of extrapolation by architectural bias or representation regularization without harming supervision fit.

## Foundational Learning

**Concept: Denoising Score Matching (DSM)**
- Why needed here: The paper analyzes "what" and "where" the model learns, specifically referencing the gap between the learned score $s_\theta$ and the empirical score $s^*$ (Equation 2)
- Quick check question: Can you derive why the empirical score $s^*(z_t, t)$ points towards the nearest neighbor in high-dimensional space?

**Concept: Concentration of Measure**
- Why needed here: Essential for understanding why the "supervision region" is a thin shell. In high dimensions, random vectors are almost always orthogonal and have predictable norms
- Quick check question: Explain why a high-dimensional Gaussian distribution looks like a "shell" rather than a "ball" centered at the mean

**Concept: Inductive Bias**
- Why needed here: The paper reframes generalization not as fitting data, but as how the model behaves when not supervised (extrapolation), which is dictated by architecture
- Quick check question: Why does a convolutional layer imply a different extrapolation behavior than a dense attention layer when moving away from training data?

## Architecture Onboarding

**Component map:**
- Supervision Region: Thin shells centered at $\alpha_t x_i$ with radius $\sigma_t \sqrt{d}$
- Extrapolation Region: The vast volume of space between/away from these shells
- Score Network: The function approximator, split into supervision efficiency (capacity to fit shells) and extrapolation efficiency (inductive bias quality)
- Metric $r^*$: Relative distance $\frac{\|z_t - \alpha_t x\|}{\sigma_t \sqrt{d}}$; $r^* \approx 1$ implies "in supervision shell"; $r^* \gg 1$ implies "extrapolating"

**Critical path:**
1. Training: Model sees $z_t$ mostly where $r^* \approx 1$ (shells)
2. Inference: Sampling trajectories start at noise and quickly diverge such that $r^* > 1$ (leaving shells)
3. Performance: Dependent on how well the model's intrinsic "extrapolation" (set by architecture/PAT) handles this departure

**Design tradeoffs:**
- Transformer vs. Convolution: Transformers have high supervision efficiency (fit data well, scalable) but lower extrapolation efficiency (weaker perceptual bias). Convolutions have high extrapolation efficiency (good perceptual priors) but lower supervision efficiency
- REPA: Adds a regularizer to improve extrapolation efficiency without harming supervision efficiency significantly

**Failure signatures:**
- Memorization: Occurs if the supervision region is too broad or if the model globally overfits the empirical score
- Low Quality (Blur): Occurs if extrapolation efficiency is poor or supervision is insufficient

**First 3 experiments:**
1. **Inference Trajectory Analysis**: Plot the distribution of relative distance $r^*$ during training vs. inference for an existing model. Verify that inference trajectories leave the supervision region ($r^* > 1$) early
2. **Region Size Ablation (Experiment 4.3)**: Train models with fixed $D_{score}$ but increasing $D_{region}$. Confirm that widening the supervision region forces memorization (identical generated images to $D_{score}$)
3. **Extrapolation vs. Supervision Efficiency Plot**: Train different architectures (Conv vs. Attn) and plot FID against Supervision Loss. Observe if the curve shifts down for Conv but requires more compute to reach the same Supervision Loss

## Open Questions the Paper Calls Out
None

## Limitations
- The "supervision region" definition relies heavily on high-dimensional concentration of measure, which may not hold for all data modalities or noise schedules
- Most experiments focus on class-conditional ImageNet, with limited validation across diverse domains (text, audio, multimodal)
- The PAT framework is proposed as a unified principle but lacks comprehensive ablation studies across different perception alignment methods

## Confidence

**High Confidence:**
- The selective underfitting phenomenon itself (Models fit the empirical score well in the supervision region but underfit outside it)
- The relationship between supervision region size and memorization behavior
- The general framework of separating supervision efficiency from extrapolation efficiency

**Medium Confidence:**
- The exact mathematical characterization of the supervision region
- The claim that PAT explains recent advances like REPA and transformer architectures
- The proposed framework as a universal principle for designing training recipes

**Low Confidence:**
- The specific numerical values for supervision region boundaries across different datasets
- The universal applicability of PAT across all generative modeling domains
- The claim that perception alignment is the primary driver of recent diffusion model improvements

## Next Checks

1. **Cross-Domain Validation**: Test selective underfitting on non-image domains (text, audio, video) to verify if the high-dimensional concentration argument holds universally or if domain-specific modifications are needed.

2. **Perception Alignment Ablation**: Conduct a comprehensive study comparing different perception alignment techniques (REPA, perceptual loss, feature matching) to isolate which aspects of PAT contribute most to improved generation quality.

3. **Dynamic Supervision Region Analysis**: Implement real-time monitoring of supervision region boundaries during training across different model scales to verify if larger models consistently increase the region size as predicted, and correlate this with memorization metrics.