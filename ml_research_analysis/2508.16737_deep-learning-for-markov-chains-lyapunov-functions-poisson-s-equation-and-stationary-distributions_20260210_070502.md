---
ver: rpa2
title: 'Deep Learning for Markov Chains: Lyapunov Functions, Poisson''s Equation,
  and Stationary Distributions'
arxiv_id: '2508.16737'
source_url: https://arxiv.org/abs/2508.16737
tags:
- learning
- where
- neural
- deep
- markov
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep learning framework for solving fundamental
  integral equations arising in Markov chain analysis, including Lyapunov function
  construction, Poisson's equation, and stationary distribution estimation. The core
  method uses neural networks trained via stochastic gradient descent to satisfy integral
  equations derived from first-transition analysis, employing a double-sampling trick
  to obtain unbiased gradient estimators.
---

# Deep Learning for Markov Chains: Lyapunov Functions, Poisson's Equation, and Stationary Distributions

## Quick Facts
- **arXiv ID:** 2508.16737
- **Source URL:** https://arxiv.org/abs/2508.16737
- **Reference count:** 9
- **Primary result:** Neural network framework for solving Lyapunov functions, Poisson's equation, and stationary distributions using stochastic gradient descent on integral equations

## Executive Summary
This paper introduces a deep learning framework for solving fundamental integral equations arising in Markov chain analysis, including Lyapunov function construction, Poisson's equation, and stationary distribution estimation. The core method uses neural networks trained via stochastic gradient descent to satisfy integral equations derived from first-transition analysis, employing a double-sampling trick to obtain unbiased gradient estimators. The approach is shown to be effective even for Markov chains on non-compact state spaces. Theoretical analysis establishes finite-sample convergence rates of O(n^{-2s/(2s+d)}) up to logarithmic factors for Hölder continuous target functions of order s in d dimensions. Numerical experiments on queueing models demonstrate accurate computation of Lyapunov functions, Poisson equation solutions, and stationary distributions, validating the method's effectiveness for complex Markovian systems where analytical solutions are difficult to obtain.

## Method Summary
The method frames the solution of integral equations like u = g + Hu as minimizing an integrated squared residual. To obtain unbiased gradient estimators without explicit value function targets, the "double-sampling trick" uses two independent transitions given state X₀ to construct a product loss. This decouples the non-linearity of the square function from the expectation, ensuring the gradient of the product loss equals the gradient of the true expected error. For non-compact state spaces, the algorithm simulates trajectories until they hit a compact subset K, treating the entire excursion as a single meta-transition. Networks are trained using Adam optimizer with clipped outputs to ensure stability, and the method applies to Lyapunov functions, Poisson equation solutions, and stationary distributions through different formulations of the integral equations.

## Key Results
- **Finite-sample convergence:** O(n^{-2s/(2s+d)}) up to logarithmic factors for Hölder continuous target functions
- **Unbiased gradients:** Double-sampling trick eliminates bias in gradient estimation for integral equations
- **Non-compact extension:** Successfully handles unbounded state spaces via return-time truncation to compact sets
- **Practical validation:** Accurate computation of Lyapunov functions, Poisson solutions, and stationary distributions on queueing models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unbiased gradient estimation for integral equations is achievable without explicit value function targets.
- **Mechanism:** The method frames the solution of equations like u = g + Hu as minimizing an integrated squared residual. Because the loss involves a square of an expectation, standard gradients are biased. The "double-sampling trick" uses two independent transitions (X₁, X₋₁) given state X₀ to construct a product loss. This decouples the non-linearity of the square function from the expectation, ensuring the gradient of the product loss equals the gradient of the true expected error.
- **Core assumption:** The transition kernel P(x, ·) allows for efficient i.i.d. sampling of next states.
- **Evidence anchors:**
  - [Section 2] Eq. (2.4) and (2.5) explicitly define the product loss and the resulting unbiased gradient estimator.
  - [Section 2] States: "Differentiating the expression above yields a simple unbiased gradient estimator."
  - [Corpus] Related work (Baird 1995) introduced the residual gradient algorithm, which this paper adapts.
- **Break condition:** If the sampling process is correlated or if only a single sample is used, the gradient estimator becomes biased, potentially converging to a different fixed point or failing to converge.

### Mechanism 2
- **Claim:** Finite-sample convergence rates depend critically on a problem-specific conditioning constant.
- **Mechanism:** The algorithm minimizes the residual ||(I - H̃)u||. Theorem 3 relates this residual error to the actual function error ||u - u*|| via a coercivity constant κ. This constant reflects the spectral properties of the "killed" kernel H̃. If the kernel is strongly contractive (small Λ, large κ), the mapping from residual error to solution error is well-conditioned, allowing standard statistical learning rates (O(n^{-2s/(2s+d)})) to apply.
- **Core assumption:** Assumption 4(iv) holds: the kernel dominates the sampling measure (ν H̃ ≤ Λ ν) with Λ small enough to ensure κ ∈ (0,1).
- **Evidence anchors:**
  - [Section 3] Theorem 3 explicitly includes the c₁/κ² factor in the error bound.
  - [Section 9] Proposition 6 derives the bound ||H̃||_{L²(ν)} ≤ √Λ · sup H̃(x, K)^{1/2}, defining the conditioning.
  - [Corpus] "Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations" corroborates the difficulty of stability in iterative methods without strict conditions.
- **Break condition:** If the Markov chain is "lazy" or near-periodic, κ → 0, causing the error bound to explode and learning to stall.

### Mechanism 3
- **Claim:** Neural networks can approximate functions on non-compact (infinite) state spaces by simulating return times.
- **Mechanism:** Neural networks require compact input domains for uniform approximation guarantees. To handle unbounded state spaces, the algorithm simulates trajectories not for one step, but until they hit a compact subset K. By treating the entire "excursion" as a single meta-transition, the input domain is effectively compressed into K, allowing the network to learn the value function only on the compact set while accounting for dynamics outside it.
- **Core assumption:** The Markov chain is recurrent enough that the return time τ(x) = inf{n ≥ 1 : X_n ∈ K} is finite and tractable to simulate.
- **Evidence anchors:**
  - [Section 6] Describes using stopping times τ(x) to handle non-compact S.
  - [Section 6] Algorithm 4 details the "Non-compact FTA-RGA" logic.
  - [Corpus] Corpus evidence is weak regarding this specific return-time truncation mechanism; the paper introduces this adaptation.
- **Break condition:** If the chain is transient or wanders far from K for long periods, the simulation cost becomes prohibitive, and the variance of the gradient estimator may explode.

## Foundational Learning

- **Concept:** **Lyapunov Drift Conditions**
  - **Why needed here:** The paper automates the construction of Lyapunov functions v satisfying Pv ≤ v - 1. Understanding this inequality is necessary to interpret the output of the neural network as a certificate of stability.
  - **Quick check question:** Given a neural network output u_θ, how would you verify if it satisfies the Lyapunov condition P u_θ ≤ u_θ - 1 on a hold-out set?

- **Concept:** **Bias-Variance Tradeoff in Gradient Estimation**
  - **Why needed here:** The "double-sampling" method eliminates bias but increases variance and computational cost (two simulations per step). Understanding this tradeoff is crucial when comparing FTA-RGA to semi-gradient methods (like TD learning).
  - **Quick check question:** Why does differentiating a squared conditional expectation (single sample) result in a biased gradient, whereas the product form (double sample) does not?

- **Concept:** **Contraction Mappings in L²**
  - **Why needed here:** The theoretical guarantees rely on the operator H̃ being a contraction (Assumption 4). Without this, the integral equation might lack a unique solution, making the learning target ill-defined.
  - **Quick check question:** If the sampling measure ν is chosen poorly (e.g., missing high-probability regions of the chain), how might the operator norm ||H̃||_{L²(ν)} change?

## Architecture Onboarding

- **Component map:** State vector x ∈ ℝᵈ -> Simulator (transition kernel P(x,·)) -> Network u_θ(x) -> Loss (product of residuals) -> Unbiased gradient estimator -> Adam optimizer

- **Critical path:** The implementation hinges on the **Double Sampler**. You must draw *two independent* next states (or trajectories) for every state X₀ drawn from the sampling measure ν.

- **Design tradeoffs:**
  - **One-step (Alg 1) vs. Return-time (Alg 4):** Use Alg 1 for compact spaces (faster, parallelizable). Use Alg 4 for non-compact spaces (slower, handles stability analysis), but beware the variance of long trajectories.
  - **Network Size vs. Smoothness:** The error bound decreases as W^{-2s/d}. In high dimensions (d large) or with non-smooth targets (s small), you need massive width W.

- **Failure signatures:**
  - **Divergence:** If the network output grows unbounded, check the clipping parameter B or the Lyapunov assumption (Assumption 1).
  - **Flat Gradients:** If the chain is slow-mixing, the "weight" W(x) in the loss might be near 1, causing poor conditioning; the gradients may vanish or provide poor signal.
  - **Stationary Distribution Collapse:** In Algorithm 3, if you don't enforce a normalization constraint (e.g., π_θ(0)=1), the network may collapse to the zero function.

- **First 3 experiments:**
  1. **Sanity Check (AR Process):** Replicate Section 7.3 on a 1D autoregressive process where the analytical solution for Poisson's equation is known (u*(x) = 2x). Verify the "Poisson RGA" converges to the linear function.
  2. **Stability Test (Queueing):** Implement Algorithm 4 for a simple G/G/1 queue on a compact domain. Compare the learned u_θ against a brute-force Monte Carlo simulation of hitting times to verify it learns the "minimal" Lyapunov function.
  3. **Gradient Variance Study:** Implement both the single-sample gradient (biased) and the double-sample gradient (unbiased). Plot convergence speed vs. step size to observe the bias-variance tradeoff empirically.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the implicit contraction parameter Λ and the choice of sampling density ν impact computational efficiency and convergence rates in high-dimensional state spaces?
- **Basis in paper:** [explicit] The conclusion explicitly identifies "the role of the parameter Λ which is connected with the choice of the sampling density" as a consideration left for future research, noting these issues will manifest prominently in high-dimensional settings.
- **Why unresolved:** The paper establishes general convergence rates but does not quantify the sensitivity of these rates to the specific choice of ν or the magnitude of Λ as the state dimension d increases.
- **What evidence would resolve it:** A theoretical analysis deriving explicit bounds for Λ in high dimensions, or empirical simulations demonstrating performance scaling with different choices of ν.

### Open Question 2
- **Question:** What is the optimal strategy for selecting the compact subset K in Algorithm 4 to minimize truncation error and computational cost for non-compact state spaces?
- **Basis in paper:** [inferred] The paper extends the method to non-compact spaces by simulating trajectories until they return to a compact set K, but the selection of K appears arbitrary in the numerical illustrations without theoretical guidance.
- **Why unresolved:** The method relies on the stopping time τ defined by K; a suboptimal choice of K could significantly increase the length of simulated trajectories or introduce boundary artifacts not accounted for in the error bounds.
- **What evidence would resolve it:** A sensitivity analysis showing the impact of the size and location of K on solution accuracy, or a theoretical heuristic for determining K based on the Markov chain's drift parameters.

### Open Question 3
- **Question:** Can the "killed-kernel domination" condition (Assumption 4(iv)) be verified or relaxed without requiring explicit knowledge of the transition density's spectral properties?
- **Basis in paper:** [inferred] The sample complexity guarantee relies on Assumption 4(iv), which the authors note is typically ensured by choosing ν as a Perron eigenfunction—a quantity that is often difficult to compute analytically.
- **Why unresolved:** If the assumption does not hold or cannot be checked, the finite-sample guarantees provided are not applicable, creating a gap between the theory and practical implementation for general models.
- **What evidence would resolve it:** A modified theoretical proof that relaxes this domination condition, or a diagnostic test computable from simulation data to verify the assumption holds for a chosen sampling density.

## Limitations
- The theoretical analysis relies on Assumption 4(iv) requiring the kernel to dominate the sampling measure with a small contraction constant Λ, which is difficult to verify in practice and represents a critical limitation.
- The return-time approach for non-compact spaces introduces high-variance gradient estimates when return times are large, though the paper doesn't quantify this variance explicitly.
- The paper does not provide practical guidance for selecting the sampling measure ν beyond uniform distributions, which can significantly impact conditioning and convergence.

## Confidence
- **High confidence:** The double-sampling trick for unbiased gradient estimation is mathematically sound and well-established in the reinforcement learning literature (Baird 1995).
- **Medium confidence:** The finite-sample convergence rates (O(n^{-2s/(2s+d)})) follow standard statistical learning theory for nonparametric regression, though they depend critically on the problem-specific conditioning constant κ.
- **Low confidence:** The extension to non-compact state spaces via return-time truncation is innovative but relies on implicit assumptions about return time distributions and variance control that are not fully characterized.

## Next Checks
1. **Verify the conditioning constant:** For a simple Markov chain (e.g., a discretized diffusion process), compute both the theoretical contraction constant Λ and the empirical operator norm of the kernel on various sampling measures to validate whether Assumption 4 holds in practice.
2. **Benchmark against semi-gradient methods:** Implement both the unbiased FTA-RGA and a biased single-sample variant on a test problem, comparing convergence speed, variance, and final accuracy to quantify the practical cost of bias elimination.
3. **Stress test the non-compact extension:** Design a Markov chain where return times to K have heavy-tailed distribution. Measure how the variance of the gradient estimator scales with trajectory length and whether standard learning rates still apply.