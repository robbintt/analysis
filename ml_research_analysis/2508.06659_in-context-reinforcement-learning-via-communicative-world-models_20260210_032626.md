---
ver: rpa2
title: In-Context Reinforcement Learning via Communicative World Models
arxiv_id: '2508.06659'
source_url: https://arxiv.org/abs/2508.06659
tags:
- learning
- agent
- coral
- message
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORAL enables a Control Agent to rapidly learn in new, unseen environments
  by leveraging a pre-trained Information Agent that communicates a transferable representation
  of the environment dynamics. The Information Agent learns to predict future states,
  rewards, and consistency of messages, while the Causal Influence Loss ensures messages
  are effective for the Control Agent.
---

# In-Context Reinforcement Learning via Communicative World Models

## Quick Facts
- arXiv ID: 2508.06659
- Source URL: https://arxiv.org/abs/2508.06659
- Reference count: 40
- Key outcome: CORAL achieves 2-3x faster sample efficiency than PPO and 5x faster than World Model baseline in sparse-reward grid-world tasks

## Executive Summary
CORAL introduces a novel in-context reinforcement learning framework where a pre-trained Information Agent (IA) communicates a transferable representation of environment dynamics to a Control Agent (CA) that rapidly learns in new, unseen environments. The IA learns to predict future states, rewards, and consistency of messages through self-supervised world modeling, while the Causal Influence Loss ensures messages are effective for the CA. This approach decouples representation learning from task-specific policy optimization, producing more generalizable communicative priors than end-to-end RL. CORAL demonstrates significant sample efficiency gains (2-3x faster than PPO) and enables strong zero-shot performance in larger, unseen environments.

## Method Summary
CORAL frames single-agent ICRL as a two-agent communication game, where an Information Agent (IA) learns a world model and communicates via latent messages to a Control Agent (CA) that selects actions. The IA is pre-trained with a composite loss combining self-supervised dynamics prediction (next observation, reward, termination), temporal coherence (predicting next message from current message and action), and causal influence shaping (maximizing the utility-weighted effect of messages on the CA's policy). During deployment, the IA is frozen and provides contextual messages to a randomly initialized CA that learns via PPO. The framework is implemented using Navix (JAX-native MiniGrid) environments with a Transformer-based IA and MLP-based CA, trained with separate Adam optimizers.

## Key Results
- CORAL achieves 2-3x faster sample efficiency than PPO and 5x faster than a World Model baseline in sparse-reward grid-world tasks
- Strong zero-shot performance in larger, unseen environments when deploying a frozen IA with a new CA
- Ablation study shows temporal coherence loss is critical for sample efficiency gains across unseen environments
- Causal influence shaping produces communication that is actionable rather than merely informative

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Representation Learning via Self-Supervised World Modeling
Separating representation learning from task-specific policy optimization produces more generalizable communicative priors than end-to-end RL. The IA is trained exclusively with self-supervised dynamics prediction losses rather than task rewards, forcing it to learn abstract dynamics shared across environments instead of task-specific shortcuts. This works when environments in training and deployment distributions share underlying dynamics structure. If deployment environments have fundamentally different dynamics, the IA's predictions will be systematically wrong, misleading the CA.

### Mechanism 2: Causal Influence Loss for Utility-Weighted Communication Shaping
Shaping messages to maximize their causal effect on the CA's policy, weighted by outcome utility, produces communication that is actionable rather than merely informative. The ICE (Instantaneous Causal Effect) measures the KL divergence between π(·|o, m) and π(·|o, 0), multiplied by utility U_t, then maximized. This encourages the IA to generate communications that produce decisive policy shifts correlated with high-value outcomes. This mechanism works when policy shifts induced by messages correlate with improved action selection, not random exploration. If the CA's policy is already near-optimal or the message dimension is too small to convey relevant distinctions, causal influence will plateau without further gains.

### Mechanism 3: Temporal Coherence for Stable Communication Protocols
Enforcing predictability between consecutive messages creates a stable, learnable communication protocol that accelerates CA adaptation. The temporal coherence loss trains the IA to predict m_{t+1} from (m_t, a_t), encouraging messages to form a compact state representation from which future states can be inferred, reducing message noise. This works when temporal smoothness in the communication channel corresponds to meaningful state abstraction rather than information suppression. In environments with highly discontinuous dynamics, coherence constraints may cause the IA to suppress critical state information.

## Foundational Learning

- Concept: **Emergent Communication in Multi-Agent RL**
  - Why needed here: CORAL frames single-agent ICRL as a two-agent communication game. Understanding cheap talk settings, speaker-listener coordination, and information-theoretic communication objectives is essential.
  - Quick check question: Why does the "cheap talk" assumption (messages don't affect environment transitions) matter for the gradient flow through L_Causal?

- Concept: **World Models and Latent Dynamics Prediction**
  - Why needed here: The IA is fundamentally a world model predicting (ô_{t+1}, r̂_{t+1}, d̂_{t+1}). Understanding how latent dynamics models compactly represent transition structure is crucial.
  - Quick check question: How does learning dynamics for communication differ from learning dynamics for planning (e.g., Dreamer, MuZero)?

- Concept: **PPO and Generalized Advantage Estimation (GAE)**
  - Why needed here: The CA uses PPO; the utility signal U_t combines GAE and value change. Understanding advantage estimation and policy gradient basics is necessary to interpret the causal influence objective.
  - Quick check question: What does GAE's λ parameter trade off, and why is the advantage estimate detached from the computation graph in L_Causal?

## Architecture Onboarding

- Component map:
  - Information Agent (IA): Transformer encoder (context length 4, 4 heads, 128 hidden) → message head (32-dim, tanh-bounded) + prediction heads for (next_obs, reward, done, next_message)
  - Control Agent (CA): Two-stream encoder (obs embedding + message embedding) → shared MLP (128 hidden) → actor head (categorical, 7 actions) + critic head (scalar value)
  - Training loop: JAX vmap for 128 parallel envs → rollout buffer (128 steps) → separate Adam optimizers for IA (self-supervised losses) and CA (PPO with clipping)

- Critical path:
  1. Pre-training: Jointly optimize IA (L_Dyn + L_Coh + L_Causal) and CA (PPO) on diverse task distribution T (Empty, DoorKey, LavaGap, DynamicObstacles, etc.)
  2. Deployment: Freeze IA, reinitialize CA randomly, train CA with PPO while IA provides fixed contextual messages

- Design tradeoffs:
  - Message dimension (16/32/64): Ablation shows 32 is optimal—smaller loses expressivity, larger introduces noise
  - Transformer vs. GRU for IA: Transformer outperforms GRU on tasks requiring longer temporal reasoning
  - Loss weights: λ_Dyn=0.5, λ_Causal=0.1, λ_Coh=0.05—dynamics grounding is primary, communication shaping is secondary

- Failure signatures:
  - High ICE, low performance: Messages are distracting (like random message baseline), not informative—check L_Causal gradient flow
  - Low ICE, low performance: IA not learning to communicate—verify L_Dyn convergence first
  - Good pre-training, poor deployment transfer: IA overfit to training tasks—increase task diversity in T

- First 3 experiments:
  1. ICE dynamics sanity check: Pre-train on a single simple task (Empty-8x8), plot ICE vs. episodic return. Verify ICE rises during learning, falls at convergence (per Figure 3 pattern)
  2. Temporal coherence ablation: Train IA without L_Coh, deploy on DoorKey-8x8. Expect ~15-25% slower sample efficiency per ablation results
  3. Zero-shot transfer validation: Pre-train on full T, freeze IA, deploy on held-out larger variant (e.g., DynObs-16x16). Compare success rate against PPO baseline—expect CORAL to show statistically significant gains (Table 2)

## Open Questions the Paper Calls Out

- Can the CORAL framework bridge the "transfer gap" to generalize to high-dimensional domains like Atari or continuous control tasks in MuJoCo?
- Would a structured protocol using discrete tokens improve expressivity and compositional reasoning compared to the current fixed-dimensional dense vectors?
- How does introducing a transmission cost for messages impact the efficiency and sparsity of the emergent communication protocol?
- Can a single pre-trained Information Agent (IA) effectively coordinate multiple specialized Control Agents (CAs) in complex multi-agent topologies?

## Limitations
- Empirical validation confined to structured, low-dimensional grid-world environments
- Assumed shared dynamics structure across training and deployment environments not stress-tested
- Causal influence shaping mechanism lacks direct evidence from related work
- Temporal coherence contribution minimally contextualized in broader emergent communication literature

## Confidence
- Sample efficiency gains (2-3× faster than PPO): Medium
- Causal influence shaping mechanism: Low
- Temporal coherence contribution: Medium
- Cross-domain transferability: Low

## Next Checks
1. Test CORAL on continuous control benchmarks (e.g., DM Control Suite) to assess cross-domain transferability
2. Conduct an ablation on the message dimension (16 vs 64) across all tasks to verify the claimed optimal value of 32
3. Measure and report the KL divergence between π(·|o, 0) and π(·|o, m) over training to confirm that L_Causal induces the intended policy shift magnitude