---
ver: rpa2
title: 'Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation
  of Structured Medical Data'
arxiv_id: '2507.02628'
source_url: https://arxiv.org/abs/2507.02628
tags:
- data
- column
- value
- please
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Medical Data Pecking approach automates quality assessment
  of structured medical data by generating unit tests tailored to the research context,
  including condition, geography, and population. Using large language models and
  vector database grounding, it creates test suites validated against literature-based
  statistics and executes them on datasets.
---

# Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data

## Quick Facts
- **arXiv ID**: 2507.02628
- **Source URL**: https://arxiv.org/abs/2507.02628
- **Reference count**: 40
- **Primary result**: Automated quality assessment of structured medical data identifies 20-43 data quality issues per cohort using context-aware unit tests

## Executive Summary
Medical Data Pecking (MDPT) introduces an automated approach for evaluating structured medical data quality by generating context-specific unit tests. The system leverages large language models with Bing Search API grounding to retrieve epidemiological statistics and medical codes, creating test suites tailored to specific conditions, geographies, and populations. When applied to four medical cohorts (Type 2 Diabetes, Chronic Kidney Disease, Hypertension, Congestive Heart Failure) across three datasets, MDPT identified 20-43 data quality issues per cohort, demonstrating its potential to improve data reliability for downstream analyses.

## Method Summary
MDPT uses GPT-4o to generate unit tests based on study specifications including condition, geography, and coding systems. The process involves retrieving epidemiological statistics via Bing Search API, validating and correcting values through a two-pass LLM validation, and retrieving relevant medical codes from OHDSI vocabularies using vector database similarity search. The generated tests are executed using a DFtest framework that compares observed data against expected values using standardized mean difference thresholds (≥0.2) and proportion ratio bounds (±15%). The approach produces pass/fail results with coverage metrics and identifies specific data quality issues.

## Key Results
- MDPT generated 60-90 tests per cohort across four conditions (T2D, CKD, HTN, CHF)
- 20-43 data quality issues identified per cohort through test execution
- Reference validity ranged from 35-93% across datasets, with grounding accuracy at 43-61%
- Tests frequently failed for drug prevalence due to lack of available literature statistics
- All of Us dataset showed 6% missing diagnosis data, causing 37% of tests to fail

## Why This Works (Mechanism)

### Mechanism 1: LLM-Grounded Test Generation with External Knowledge Retrieval
A large language model receives study parameters and queries Bing Search API for epidemiological statistics, then retrieves medical codes via vector similarity search over OHDSI vocabularies. This creates context-specific data quality tests at scale. The approach assumes literature-derived epidemiological statistics serve as valid proxies for expected data characteristics. However, up to 13% of tests lacked relevant grounding when literature was unavailable for specific condition-geography pairs.

### Mechanism 2: Two-Pass Validation for Hallucination Reduction
After initial grounding retrieves statistics, a secondary LLM call evaluates each entry—classifying as "correct," "needs correction," or "remove." Corrected values replace originals; flagged entries are discarded before test generation. This validation reduces but doesn't eliminate hallucinated statistics, with 12-26% of tests still having incorrect references despite validation.

### Mechanism 3: Statistical Comparison with Context-Aware Tolerance Thresholds
The method uses magnitude-based thresholds (SMD ≥ 0.2) and bounded ratio checks (0.85-1.15) rather than pure statistical significance. This approach accounts for large sample sizes where small differences may be statistically significant but not clinically meaningful. However, these thresholds may flag legitimate design choices as "failures" when datasets have genuinely different population characteristics than literature benchmarks.

## Foundational Learning

- **Unit Testing & Coverage Concepts**
  - Why needed here: The approach adapts software engineering unit testing (individual tests for specific assertions, coverage metrics) to data quality assessment.
  - Quick check question: Can you explain the difference between a metadata-level test (column presence), distribution-level test (population-wide statistics), and subpopulation-level test (intra-group patterns)?

- **Vector Database Semantic Search**
  - Why needed here: MDPT retrieves medical codes from OHDSI vocabularies using Chroma vector database with embeddings.
  - Quick check question: Why would a vector similarity search retrieve "fatty liver disease" codes when querying for "NAFLD" (non-alcoholic fatty liver disease)?

- **Epidemiological Statistics & SMD**
  - Why needed here: The method relies on prevalence, incidence, comorbidity rates, and standardized mean difference to compare observed vs. expected data.
  - Quick check question: In a dataset of 400,000 patients, why might a small absolute percentage difference be statistically significant (p < 0.05) but fail the SMD ≥ 0.2 threshold?

## Architecture Onboarding

- **Component map**: User Input Handler -> Statistics Retrieval Module -> Vector Code Retrieval -> Two-Pass Validator -> Test Matrix Generator -> Test Code Generator -> DFtest Execution Framework -> Coverage Reporter

- **Critical path**: User Input → Statistics Retrieval (with Bing grounding) → Code Retrieval (vector DB) → Two-Pass Validation → Test Matrix → Python Test Generation → DFtest Execution → Report Generation

- **Design tradeoffs**:
  - LLM hallucinations vs. automation scale: Two-pass validation reduces but doesn't eliminate incorrect references (12-26%)
  - General population benchmarks vs. dataset specificity: Using literature statistics flags intentional sampling decisions as "failures"
  - Statistical thresholds: SMD ≥ 0.2 and ±15% ratio bounds may be too strict for exploratory datasets or too loose for regulatory contexts
  - Coverage definition: Metadata-level coverage inflates "tested" columns without substantive validation

- **Failure signatures**:
  - High "irrelevant" test rate (up to 13%) indicates missing epidemiological literature for condition-geography pairs
  - Drug prevalence tests frequently lack references—drug prescription statistics are often unavailable in public literature
  - ICD-9 code formatting mismatches (e.g., missing dots in MIMIC-III) cause false failures
  - Tests passing despite non-correct references suggests spurious alignment from incorrect expected values

- **First 3 experiments**:
  1. Run MDPT on T2D cohort in All of Us with default settings. Manually review the 60 generated tests, categorizing each as accurate/inaccurate/incorrect/irrelevant. Compare assessment against Table 1 to calibrate expectations.
  2. Rerun T2D tests with modified ratio bounds (±10% and ±25%) and SMD thresholds (0.1 and 0.3). Document how pass/fail counts change to understand threshold impact on failure rates.
  3. Generate HTN test suite for SyntheticMass, then apply the same tests to a different dataset with HTN data (e.g., MIMIC-III HTN patients). Observe which failures are dataset-specific vs. test-quality issues.

## Open Questions the Paper Calls Out

- Does the use of automated data quality reports lead to an "implied truth effect," causing researchers to overestimate data validity?
- How can the Medical Data Pecking approach be adapted to handle unstructured data modalities such as clinical notes or imaging?
- How can the grounding accuracy of LLM-generated tests be improved to reduce the prevalence of incorrect reference values?

## Limitations

- Grounding accuracy remains limited at 43-61%, with significant proportion of tests having incorrect or irrelevant references
- Performance degrades when epidemiological literature lacks condition-specific statistics for target geography or population
- Tests may flag legitimate design choices (e.g., intentional population oversampling) as quality issues when datasets differ from literature benchmarks
- Drug prevalence testing frequently fails due to unavailability of prescription statistics in public literature

## Confidence

- **High Confidence**: The test generation mechanism and execution framework work as described. The approach successfully identifies data quality issues through automated test generation and execution, with 20-43 issues detected per cohort.
- **Medium Confidence**: The validation mechanism reduces but doesn't eliminate hallucinations (12-26% of tests still had incorrect references). The statistical thresholds (SMD ≥ 0.2, ±15% ratio) are reasonable but may need adjustment for different clinical contexts.
- **Low Confidence**: The method's performance on datasets with significantly different population characteristics from literature benchmarks, and its ability to handle conditions without well-documented epidemiological statistics.

## Next Checks

1. Apply MDPT to a condition with limited epidemiological literature (e.g., rare diseases) and compare test quality metrics against the main study results.
2. Run MDPT on a dataset with intentionally different demographics (e.g., pediatric-only vs adult population) to assess how threshold sensitivity handles expected population differences.
3. Disable the Bing Search API grounding and run MDPT to quantify the increase in hallucinated statistics and incorrect test generation, establishing the grounding step's contribution to quality.