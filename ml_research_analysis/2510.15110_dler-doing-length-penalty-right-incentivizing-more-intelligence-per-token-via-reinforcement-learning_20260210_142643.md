---
ver: rpa2
title: 'DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token
  via Reinforcement Learning'
arxiv_id: '2510.15110'
source_url: https://arxiv.org/abs/2510.15110
tags:
- length
- accuracy
- reasoning
- dler
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of improving reasoning efficiency
  in large language models by reducing the length of their chain-of-thought responses
  without sacrificing accuracy. The authors argue that previous reinforcement learning
  (RL) approaches using length penalties suffer from poor optimization rather than
  the penalty design itself.
---

# DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.15110
- Source URL: https://arxiv.org/abs/2510.15110
- Authors: Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov
- Reference count: 40
- Key outcome: Achieves state-of-the-art accuracy-to-length trade-offs in reasoning models, cutting response length by over 70% while maintaining or improving accuracy

## Executive Summary
DLER addresses the challenge of improving reasoning efficiency in large language models by reducing the length of their chain-of-thought responses without sacrificing accuracy. The paper argues that previous reinforcement learning approaches using length penalties suffer from poor optimization rather than the penalty design itself. DLER introduces three key innovations: batch-wise reward normalization, higher clipping thresholds, and dynamic sampling to overcome the identified challenges of biased advantage estimation, entropy collapse, and sparse reward signals.

## Method Summary
DLER is a GRPO-based reinforcement learning recipe that optimizes length-penalized reasoning through three modifications: (1) batch-wise reward normalization instead of group-wise to stabilize advantage estimates under high variance, (2) asymmetric clipping thresholds (0.2 low, 0.28 high) to preserve gradients on high-entropy exploratory tokens, and (3) dynamic sampling that filters uninformative prompts to create an implicit curriculum. The method trains DeepSeek-R1 models on mathematical reasoning tasks using a simple truncation penalty (reward=0 if response exceeds 4000 tokens) combined with these optimization improvements to achieve significant length reduction while maintaining accuracy.

## Key Results
- Achieves 70% reduction in response length while maintaining or improving accuracy on mathematical benchmarks
- Batch-wise normalization recovers ~3% accuracy over GRPO at similar token counts on AIME-24
- Higher clipping thresholds alleviate entropy collapse, allowing exploration to continue after initial drops
- Dynamic sampling prevents premature overfitting to easy prompts, enabling better length budget utilization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Batch-wise reward normalization mitigates biased advantage estimation caused by high reward variance under truncation penalties.
- **Mechanism:** Truncation increases per-prompt reward variance (observed: 0.4 vs 0.29 for aggressive vs lenient truncation). GRPO's group-wise normalization amplifies this bias when many responses are cut off and assigned zero reward. Switching to batch-level normalization stabilizes advantage estimates by aggregating statistics across 512 prompts rather than 16 rollouts per prompt.
- **Core assumption:** The bias in advantage estimation is the primary driver of accuracy degradation under length penalties, not the penalty design itself.
- **Evidence anchors:** [Section 3.1]: "more aggressive truncation lengths lead to higher per-prompt variance on average, with corresponding values of {0.4, 0.32, 0.3, 0.29}" and [Section 3.1, Figure 2]: Batch-wise normalization recovers ~3% accuracy over GRPO at similar token counts on AIME-24.

### Mechanism 2
- **Claim:** Higher clipping thresholds retain gradients on high-entropy exploratory tokens, preventing entropy collapse.
- **Mechanism:** Clipped tokens (especially by the upper threshold 1+ε) are disproportionately low-probability, high-entropy tokens like "Wait," "Alternatively," "Hmm"—transitional cues that drive reasoning exploration. Standard clipping zeros their gradients, causing premature convergence. Decoupling thresholds (ε_low=0.2, ε_high=0.28) allows these tokens to contribute to updates.
- **Core assumption:** High-entropy tokens are causally important for reasoning path exploration, not merely correlated with longer outputs.
- **Evidence anchors:** [Section 3.2, Figure 3]: Clipped tokens show lower probability (~0.2) and higher entropy (~0.7) than unclipped tokens and [Section 3.2]: "enabling higher clipping threshold clearly alleviates entropy collapse: the entropy... not only avoids vanishing, but even increases after an initial drop."

### Mechanism 3
- **Claim:** Dynamic sampling filters uninformative prompts, creating an implicit curriculum that improves length budget utilization.
- **Mechanism:** Early in training, ~50% of prompts have all 16 rollouts truncated (zero reward). Later, ~40% have all correct (uniform reward). Both extremes provide weak gradients. Discarding and resampling until batch is balanced filters these, progressively introducing harder examples that require longer reasoning.
- **Core assumption:** The training signal quality, not just quantity, determines whether the model learns to use its token budget efficiently.
- **Evidence anchors:** [Section 3.3, Figure 4]: Shows ratio of all-zero and all-one reward prompts over training steps and [Section 3.3]: "Without dynamic sampling, the model tends to plateau at a shorter length... indicating that it has prematurely overfitted into a suboptimal local minimum."

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** DLER modifies GRPO's core advantage estimation; understanding the baseline (Equation 1) is prerequisite.
  - **Quick check question:** Can you explain why GRPO normalizes rewards within each prompt's rollouts rather than globally?

- **Concept: PPO Clipping Mechanism**
  - **Why needed here:** DLER's higher clipping threshold modification only makes sense if you understand how clipping constrains policy updates.
  - **Quick check question:** What happens to gradient flow when the probability ratio π_θ/π_old exceeds 1+ε?

- **Concept: Entropy as Exploration Signal**
  - **Why needed here:** The paper diagnoses entropy collapse as a failure mode; you need to connect token-level entropy to policy diversity.
  - **Quick check question:** Why does low output entropy indicate insufficient exploration in RL training?

## Architecture Onboarding

- **Component map:** Prompt Dataset (40K math problems) → Rollout Generator (16 samples/prompt, temp=1.0) → Reward Computation (correctness + truncation penalty) → Advantage Normalization (batch-wise, not group-wise) → Policy Update (decoupled clipping: 0.2 low, 0.28 high) → Dynamic Filter (discard all-0 or all-1 reward groups, resample)

- **Critical path:** The interaction between truncation penalty → reward variance → advantage bias. If truncation is too aggressive (<2000 tokens for hard problems), variance spikes and normalization cannot compensate.

- **Design tradeoffs:**
  - Truncation length: Shorter = faster rollouts but higher variance. Paper uses 4000 as default.
  - Clipping asymmetry: Higher upper threshold preserves exploration but may increase variance. ε_high=0.28 worked; higher was not tested.
  - Filter aggressiveness: Discarding all uninformative prompts slows training but improves final accuracy.

- **Failure signatures:**
  - Accuracy drops while length decreases → likely advantage bias (check batch-wise normalization)
  - Length plateaus far below target → likely overfitting to easy prompts (check dynamic sampling)
  - Entropy → 0 early → clipping too aggressive (increase ε_high)

- **First 3 experiments:**
  1. **Ablation on normalization:** Train with group-wise vs batch-wise normalization only, holding clipping and sampling constant. Expect ~2-3% accuracy gap on AIME-24.
  2. **Clipping threshold sweep:** Test ε_high ∈ {0.2, 0.28, 0.36} with batch-wise normalization. Monitor entropy trajectory; expect collapse at 0.2, recovery at 0.28+.
  3. **Dynamic sampling validation:** Train with and without filtering on a held-out easy subset. Without filtering, expect length to plateau ~2000 tokens below target.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the DLER recipe generalize to non-mathematical reasoning domains such as coding or general instruction following?
- Basis in paper: [inferred] The experiments are restricted to mathematical benchmarks (AIME, MATH, Olympiad) and the DeepScaleR-Preview-Dataset (Section 4.1), leaving performance on other reasoning modalities untested.
- Why unresolved: The relationship between conciseness and correctness may differ in coding (where syntax/compilation constraints exist) compared to the free-form textual chains of thought used in math.
- What evidence would resolve it: Application of DLER to code generation benchmarks (e.g., HumanEval, MBPP) or logical deduction tasks to verify if batch-wise normalization and higher clipping preserve accuracy in non-math domains.

### Open Question 2
- Question: Is the "correctness ratio" a sufficiently robust proxy for question difficulty in the difficulty-aware (DA-DLER) variant?
- Basis in paper: [inferred] Section 3.5 states difficulty is "estimated from the correctness ratio of model responses," which implicitly assumes the model's current capability accurately reflects intrinsic problem difficulty.
- Why unresolved: A model may fail a complex problem due to a simple error (low difficulty estimate) or solve a simple problem via luck (high difficulty estimate), potentially causing DA-DLER to assign incorrect truncation limits.
- What evidence would resolve it: A comparison of DA-DLER performance when using the correctness ratio versus an external difficulty classifier or "oracle" difficulty labels.

### Open Question 3
- Question: Are the specific hyperparameters for update-selective merging (top 25% updates, 0.7 scale) optimal across different model architectures?
- Basis in paper: [inferred] Section 4.6 introduces the strategy with specific values ("retain only the top 25%... scale by a factor of 0.7") to address data scarcity, but does not ablate these specific choices or test them on the 1.5B model.
- Why unresolved: These values appear tuned for the Nemotron-8B experiment; their transferability to the DeepSeek-R1-1.5B/7B models or different domain shifts is not established.
- What evidence would resolve it: An ablation study varying the merge percentage (e.g., top 10% vs 50%) and scaling factor across the different model sizes presented in the main results.

### Open Question 4
- Question: Is there a lower bound on truncation length where DLER fails to maintain accuracy, regardless of optimization improvements?
- Basis in paper: [inferred] The paper argues accuracy degradation is due to optimization, not penalty design (Key Insight 1), and achieves reductions of roughly 70% (Section 1).
- Why unresolved: It is unclear if simple truncation can compress reasoning indefinitely or if there is a "critical reasoning mass" (floor) below which the chain of thought cannot be shortened without logical loss.
- What evidence would resolve it: Training runs with progressively aggressive truncation targets (e.g., 512 or 256 tokens) on complex tasks like AIME-24 to identify the accuracy failure point.

## Limitations
- The claimed superiority of optimization over penalty design is demonstrated through a narrow ablation space, leaving open whether other length penalty formulations could achieve similar results
- Dynamic sampling may introduce confirmation bias by systematically avoiding easy problems during training
- The difficulty-aware variant's perplexity-based difficulty proxy is not validated against ground-truth solution lengths or human-judged complexity

## Confidence
- **High Confidence:** The empirical improvements in accuracy-to-length trade-offs (70% reduction with maintained accuracy) are well-supported by the AIME-24, MATH, and AMC benchmark results
- **Medium Confidence:** The mechanism explanations (advantage bias, entropy collapse, sparse rewards) are internally consistent with the data presented, but rely on correlational rather than causal evidence
- **Low Confidence:** The generalization of results to domains beyond competition mathematics (e.g., coding, scientific reasoning) is asserted but not demonstrated

## Next Checks
1. **Ablation on truncation thresholds:** Systematically vary the maximum length cutoff (2000, 4000, 6000, 8000 tokens) while keeping DLER's optimization intact to reveal whether the 4000-token threshold is optimal
2. **Cross-domain generalization test:** Apply DLER-trained models to coding benchmarks (HumanEval, MBPP) and scientific reasoning tasks (GPQA, ScienceQA) to validate whether optimization improvements transfer beyond mathematical reasoning
3. **Ground-truth difficulty validation for DA-DLER:** Replace the distilled perplexity metric with either human-annotated difficulty scores or average solution lengths from reference solutions to confirm whether the proxy metric captures meaningful difficulty variation