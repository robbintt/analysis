---
ver: rpa2
title: Bayesian Concept Bottleneck Models with LLM Priors
arxiv_id: '2410.15555'
source_url: https://arxiv.org/abs/2410.15555
tags:
- concepts
- bc-llm
- concept
- does
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BC-LLM, a novel method for learning Concept
  Bottleneck Models (CBMs) that addresses the accuracy-interpretability tradeoff in
  interpretable machine learning. The key innovation is using Large Language Models
  (LLMs) within a Bayesian framework to iteratively search over a potentially infinite
  set of interpretable concepts rather than relying on a predefined finite set.
---

# Bayesian Concept Bottleneck Models with LLM Priors

## Quick Facts
- arXiv ID: 2410.15555
- Source URL: https://arxiv.org/abs/2410.15555
- Reference count: 40
- Key outcome: BC-LLM achieves state-of-the-art performance in learning Concept Bottleneck Models by iteratively discovering concepts using LLM-driven proposals within a Bayesian framework

## Executive Summary
This paper introduces BC-LLM, a novel method for learning Concept Bottleneck Models (CBMs) that addresses the accuracy-interpretability tradeoff in interpretable machine learning. The key innovation is using Large Language Models (LLMs) within a Bayesian framework to iteratively search over a potentially infinite set of interpretable concepts rather than relying on a predefined finite set. The method employs a split-sample Metropolis update where the LLM proposes concepts based on a subset of data while the remaining data is used to accept or reject these proposals. This approach allows BC-LLM to explore concepts data-adaptively while maintaining statistical rigor. Across image, text, and tabular datasets, BC-LLM outperforms interpretable baselines and even black-box models in certain settings, converges more rapidly towards relevant concepts, provides robust uncertainty quantification for out-of-distribution samples, and demonstrates strong performance in clinical applications where interpretability is crucial.

## Method Summary
BC-LLM is a method for learning Concept Bottleneck Models that iteratively discovers interpretable concepts through LLM-guided Bayesian inference. The core algorithm uses a split-sample Metropolis-within-Gibbs sampling approach where the LLM proposes candidate concepts based on a data subset, and these proposals are accepted or rejected using held-out data via a partial Bayes factor. The method starts with a greedy warm-start initialization, then iteratively drops one concept, summarizes the remaining data with a keyphrase model, proposes new candidate concepts, annotates them with the LLM, and accepts or rejects based on statistical criteria. This allows exploration of a potentially infinite concept space while maintaining statistical rigor and providing calibrated uncertainty quantification.

## Key Results
- BC-LLM outperforms interpretable baselines and achieves competitive performance with black-box models across image, text, and tabular datasets
- The method converges more rapidly to relevant concepts compared to greedy or random initialization approaches
- BC-LLM provides robust uncertainty quantification, showing higher entropy for out-of-distribution samples compared to other CBM methods
- The approach is cost-effective, requiring only O(KT) LLM queries compared to hundreds or thousands needed by existing methods
- Strong performance in clinical applications where interpretability is crucial, with the ability to handle limited prior knowledge about relevant concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BC-LLM iteratively discovers relevant concepts from a potentially infinite set by combining LLM-driven proposals with statistically rigorous acceptance criteria
- Mechanism: The split-sample Metropolis update uses an LLM to propose candidate concepts based on a data subset (S), then accepts/rejects proposals using held-out data (Sc) via a partial Bayes factor
- Core assumption: The LLM's partial posterior (conditional on S) is approximately consistent with some prior, but the method is proven to converge even if this assumption is violated
- Evidence anchors: Theorem 3.1 proves convergence to true concepts asymptotically even with poor LLM priors

### Mechanism 2
- Claim: The Bayesian formulation provides calibrated uncertainty quantification for selected concepts and out-of-distribution (OOD) robustness
- Mechanism: The Metropolis-within-Gibbs sampling yields a posterior distribution over concept sets. Ensemble predictions from posterior samples naturally express uncertainty
- Core assumption: Data is IID; regularity conditions hold for Laplace approximation of split-sample posteriors
- Evidence anchors: Table 1 shows higher OOD entropy for BC-LLM vs. other CBMs; Figure 2 illustrates uncertainty for OOD samples

### Mechanism 3
- Claim: BC-LLM is cost-effective, requiring O(KT) LLM queries versus O(nW) for pre-defined concept approaches
- Mechanism: The multiple-try variant batches concept proposals and annotations, amortizing LLM calls
- Core assumption: The LLM can reliably extract/annotate M candidate concepts in a single batched prompt
- Evidence anchors: Section 3.3 details computational cost; experiments use GPT-4o-mini with minutes per iteration

## Foundational Learning

- **Bayesian Variable Selection / Metropolis-within-Gibbs**:
  - Why needed here: BC-LLM extends Bayesian sparse modeling to an infinite concept space. The Metropolis-within-Gibbs loop is the core inference engine
  - Quick check question: Can you explain how a Metropolis-Hastings acceptance ratio ensures the target posterior is a stationary distribution of the Markov chain?

- **Concept Bottleneck Models (CBMs)**:
  - Why needed here: BC-LLM is a method for learning CBMs. Understanding the interpretability-accuracy tradeoff in standard CBMs motivates the approach
  - Quick check question: Why do "soft" CBMs risk information leakage, and how does BC-LLM maintain "hard" binary concepts?

- **LLM Prompting for Structured Output**:
  - Why needed here: The method relies on LLMs to output structured keyphrases, concept proposals, and annotations in JSON format
  - Quick check question: What prompting strategies could reduce hallucinations when an LLM extracts concept values from clinical notes?

## Architecture Onboarding

- **Component map**: Initialization (LLM keyphrase extraction -> BoW model -> initial concepts) -> Iterative Loop (drop concept -> keyphrase model -> LLM proposals -> LLM annotation -> acceptance step)

- **Critical path**: The keyphrase model (Step 1) is the bridge from raw data to LLM prompts. Its quality dictates proposal relevance. The acceptance step (Step 4) is the statistical guardrail.

- **Design tradeoffs**:
  - Split fraction ω: Balance between proposal quality (higher ω) and statistical power for rejection (lower ω)
  - Number of candidates M: Higher M improves exploration but increases LLM call cost and potential noise
  - Iterations T: Higher T improves uncertainty quantification but is costly; T≈5 is often enough for prediction accuracy

- **Failure signatures**:
  - Concept drift/looping: LLM keeps proposing semantically identical concepts
  - Annotation inconsistency: LLM gives different values for the same concept across iterations
  - Poor keyphrase summarization: The keyphrase model fails to surface discriminative signals

- **First 3 experiments**:
  1. Ablation on split fraction ω: Vary ω ∈ {0.3, 0.5, 0.7} on a validation split. Measure prediction AUC and concept diversity
  2. Robustness to LLM miscalibration: Manually inject incorrect prior concepts into the LLM prompt. Verify that the accept/reject step filters them out over iterations
  3. Cost-performance curve: Plot final model AUC vs. total LLM tokens used for varying T (2, 5, 10) and M (5, 10, 15)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the BC-LLM framework be effectively extended to high-dimensional settings with significantly more concepts (e.g., K > 20) or massive datasets using mini-batching techniques?
  - Basis in paper: The discussion section explicitly states BC-LLM is currently designed for K ≤ 20 and suggests "future directions can consider further speeding up posterior inference, such as through mini-batching of observations or concepts"
  - Why unresolved: The current computational cost scales with O(nTK), which may become prohibitive as the concept space expands
  - What evidence would resolve it: A demonstration of BC-LLM running on a dataset requiring hundreds of concepts or millions of observations without losing statistical rigor

- **Open Question 2**: How sensitive is the method's convergence to the initialization strategy, specifically the "greedy warm-start" procedure?
  - Basis in paper: Section 3.3 notes that a greedy warm-start is used because "Gibbs sampling can be slow to converge," but it does not quantify the performance drop if random initialization were used
  - Why unresolved: It is unclear if the statistical guarantees hold as robustly without this specific heuristic or if the warm-start biases the posterior exploration
  - What evidence would resolve it: Ablation studies comparing posterior samples and convergence times between greedy warm-starts and standard random initializations

- **Open Question 3**: Does BC-LLM maintain its performance advantages when using smaller, open-source LLMs that may have weaker prior knowledge or reasoning capabilities?
  - Basis in paper: Appendix D notes that performance dropped when switching from GPT-4o-mini to Cohere's Command-R
  - Why unresolved: The trade-off between LLM capability (cost/availability) and the Bayesian correction mechanism's ability to fix "bad" priors is not fully characterized
  - What evidence would resolve it: Benchmarks of BC-LLM using various open-source LLMs (e.g., Llama, Mistral) to determine the minimum model size required

## Limitations
- The method requires careful hyperparameter tuning (ω, M, T) that may not generalize across domains
- The Bayesian framework assumes IID data, which may not hold for sequential or time-series clinical data
- Concept interpretability depends heavily on LLM quality and may not transfer to domain-specific terminology

## Confidence

- **High**: The Bayesian formulation and convergence theory are sound
- **Medium**: Empirical results show competitive performance but lack extensive ablation studies
- **Medium**: Computational cost claims are plausible but not independently verified

## Next Checks
1. Test concept proposal quality with systematically degraded LLM responses (temperature=1.0 vs 0.1) to verify the acceptance step's filtering capability
2. Run OOD detection with synthetic concept corruption to quantify uncertainty calibration beyond Table 1's comparison
3. Implement a head-to-head cost comparison measuring total LLM tokens for BC-LLM versus pre-defined concept approaches across varying K values