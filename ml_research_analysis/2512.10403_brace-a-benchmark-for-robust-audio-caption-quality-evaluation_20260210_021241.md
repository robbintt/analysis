---
ver: rpa2
title: 'BRACE: A Benchmark for Robust Audio Caption Quality Evaluation'
arxiv_id: '2512.10403'
source_url: https://arxiv.org/abs/2512.10403
tags:
- caption
- audio
- captions
- benchmark
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BRACE, a benchmark designed to evaluate
  reference-free audio caption quality metrics and large audio language models. BRACE
  consists of two sub-benchmarks: BRACE-Main for comparing caption quality, and BRACE-Hallucination
  for detecting hallucinated content.'
---

# BRACE: A Benchmark for Robust Audio Caption Quality Evaluation

## Quick Facts
- arXiv ID: 2512.10403
- Source URL: https://arxiv.org/abs/2512.10403
- Reference count: 40
- Primary result: CLAP-based metrics achieve only 70.01 F1-score on caption quality evaluation, while LALMs reach 63.19 F1-score

## Executive Summary
BRACE is a benchmark designed to evaluate reference-free audio caption quality metrics and large audio language models. It consists of two sub-benchmarks: BRACE-Main for comparing caption quality and BRACE-Hallucination for detecting hallucinated content. The benchmark is constructed through high-quality filtering, LLM-based caption corruption, and human annotation, providing a rigorous framework for advancing audio captioning evaluation.

Experiments reveal significant limitations in current audio-text alignment models, with even the best-performing CLAP-based metric achieving only 70.01 F1-score on BRACE-Main and top LALMs reaching around 76-96 F1-score on BRACE-Hallucination. These results highlight the challenges in detecting fine-grained differences and hallucinations in audio captions, emphasizing the need for improved evaluation methods and model development.

## Method Summary
BRACE is constructed through a three-step process: high-quality filtering of audio-caption pairs, LLM-based caption corruption to create challenging evaluation scenarios, and human annotation to ensure benchmark quality. The benchmark includes two sub-benchmarks - BRACE-Main for comparing caption quality and BRACE-Hallucination for detecting hallucinated content. The construction process involves careful curation and validation to create a comprehensive evaluation framework for audio captioning models.

## Key Results
- CLAP-based metrics achieve only 70.01 F1-score on BRACE-Main for caption quality comparison
- Top LALMs reach 63.19 F1-score on BRACE-Main, with performance around 76-96 F1-score on BRACE-Hallucination
- Current audio-text alignment models show significant limitations in detecting fine-grained differences and hallucinations

## Why This Works (Mechanism)
The benchmark works by creating a controlled evaluation environment that tests models' ability to detect subtle differences and hallucinations in audio captions. Through LLM-based corruption and human annotation, BRACE generates challenging scenarios that reveal the limitations of current models in understanding fine-grained audio-text relationships.

## Foundational Learning
- **Audio-text alignment**: Understanding how audio features map to textual descriptions - needed for evaluating caption quality, quick check: can models correctly match audio clips to captions
- **Hallucination detection**: Identifying when captions contain information not present in the audio - needed for ensuring caption accuracy, quick check: can models detect invented content
- **Quality metric evaluation**: Measuring how well evaluation metrics can distinguish between caption quality - needed for improving assessment methods, quick check: do metrics correlate with human judgment

## Architecture Onboarding
**Component map**: Audio samples → Caption generation/corruption → Quality evaluation → Hallucination detection
**Critical path**: Audio input → Caption processing → Feature extraction → Comparison/evaluation
**Design tradeoffs**: Balancing benchmark difficulty with practical applicability, ensuring diverse test cases while maintaining focus on key challenges
**Failure signatures**: Models struggling with fine-grained distinctions, difficulty detecting subtle hallucinations, performance degradation on complex audio scenarios
**First experiments**: 1) Test model performance on simple vs. complex audio captions, 2) Evaluate hallucination detection accuracy on controlled corrupted captions, 3) Compare CLAP-based metrics vs. LALMs on identical benchmark subsets

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Potential biases introduced by LLM-based caption corruption process
- Reliance on human annotations introduces variability in annotation quality
- Benchmark focuses on English captions only, limiting multilingual applicability
- Evaluation metrics may not capture all aspects of caption quality

## Confidence
- High confidence in the claim that current models show significant limitations (based on F1-scores well below 100%)
- Medium confidence in BRACE providing a rigorous evaluation framework (detailed methodology but limited independent validation)
- Medium confidence in performance comparisons between CLAP-based metrics and LALMs (controlled experiments but may not generalize to all variants)

## Next Checks
1. Conduct ablation studies to isolate the impact of LLM-based caption corruption on benchmark difficulty and model performance
2. Expand the benchmark to include multilingual captions and evaluate model performance across different languages
3. Implement additional evaluation metrics beyond F1-score to assess caption quality aspects like semantic coherence and temporal accuracy