---
ver: rpa2
title: 'Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label
  Learning'
arxiv_id: '2509.17971'
source_url: https://arxiv.org/abs/2509.17971
tags:
- mixup
- learning
- complementary
- labels
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of complementary-label learning
  (CLL), where models are trained with labels indicating classes an instance does
  not belong to, rather than standard ordinary labels. The authors identify that the
  widely-used Mixup data augmentation technique is ineffective for CLL due to noise
  introduced when mixing complementary labels that may contain the true class.
---

# Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning

## Quick Facts
- arXiv ID: 2509.17971
- Source URL: https://arxiv.org/abs/2509.17971
- Reference count: 40
- Intra-Cluster Mixup improves CLL accuracy by 30% on MNIST and 10% on CIFAR datasets

## Executive Summary
This paper addresses the challenge of complementary-label learning (CLL), where models are trained with labels indicating classes an instance does not belong to, rather than standard ordinary labels. The authors identify that the widely-used Mixup data augmentation technique is ineffective for CLL due to noise introduced when mixing complementary labels that may contain the true class. To address this, they propose Intra-Cluster Mixup (ICM), which clusters examples first and then applies Mixup only within each cluster to reduce noise while preserving the benefits of label sharing. ICM consistently improves CLL performance across various state-of-the-art algorithms and settings, achieving significant accuracy increases of 30% on MNIST and 10% on CIFAR datasets compared to baselines.

## Method Summary
The authors propose Intra-Cluster Mixup (ICM) as a data augmentation technique specifically designed for complementary-label learning. Unlike standard Mixup which can introduce harmful noise when mixing complementary labels, ICM first clusters the training data into meaningful groups based on feature similarity. The augmentation is then applied only within each cluster, ensuring that mixed examples remain semantically coherent. This approach maintains the label-sharing benefits of Mixup while reducing the noise that occurs when mixing examples from different classes. The method is evaluated across multiple CLL algorithms and demonstrates consistent improvements on both balanced and imbalanced CLL scenarios using synthetic and real-world labeled datasets.

## Key Results
- ICM achieves 30% accuracy improvement on MNIST compared to baseline CLL methods
- ICM provides 10% accuracy improvement on CIFAR datasets
- The method consistently improves performance across various state-of-the-art CLL algorithms and settings

## Why This Works (Mechanism)
ICM works by reducing label noise in the augmentation process. Standard Mixup in CLL scenarios can be problematic because when you mix two examples with complementary labels, there's a chance one of those labels actually contains the true class. By clustering first and then mixing only within clusters, ICM ensures that mixed examples share more semantic similarity, reducing the likelihood that the mixed label will incorrectly contain the true class. This preserves the generalization benefits of Mixup while mitigating its main weakness in CLL contexts.

## Foundational Learning
- **Complementary-label learning**: A learning paradigm where training labels indicate classes an instance does not belong to rather than the correct class. Needed because obtaining ordinary labels can be expensive while complementary labels are easier to acquire.
- **Mixup augmentation**: A data augmentation technique that creates synthetic training examples by linearly interpolating between pairs of examples and their labels. Needed for improving model generalization and robustness.
- **Clustering-based augmentation**: Using unsupervised clustering to group similar examples before applying augmentation techniques. Needed to ensure semantic coherence in synthetic examples.

## Architecture Onboarding

**Component Map**: Data → Clustering → Mixup (within clusters) → Augmented Data → CLL Model

**Critical Path**: The clustering step is critical because it determines which examples can be safely mixed. Poor clustering leads to noise in the augmented data, negating ICM's benefits.

**Design Tradeoffs**: The method trades computational overhead from clustering against improved CLL performance. The choice of clustering algorithm affects both quality and speed.

**Failure Signatures**: Performance degradation when clusters are poorly formed, leading to mixing examples from different semantic groups. This manifests as accuracy worse than standard Mixup.

**First Experiments**: 
1. Test ICM with varying numbers of clusters on MNIST to find optimal clustering granularity
2. Compare different clustering algorithms (K-means, hierarchical, DBSCAN) for ICM performance
3. Evaluate ICM's sensitivity to cluster size and mixing ratio parameters

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the Intra-Cluster Mixup (ICM) framework be effectively extended to multi-complementary-label learning settings?
- Basis in paper: The authors state in the "Limitation and Future Works" section that they "have not yet evaluated our approach in scenarios where instances carry multiple complementary labels" and identify this as an important direction.
- Why unresolved: The current study restricts experiments to instances associated with a single complementary label, leaving the interaction between intra-cluster mixing and multiple label constraints unexplored.
- What evidence would resolve it: Experimental results applying ICM to datasets where instances possess more than one complementary label, comparing performance against multi-complementary-label baselines.

### Open Question 2
- Question: How can ICM be adapted to prevent performance degradation in low-capacity models such as MLPs or linear classifiers?
- Basis in paper: The "Limitation and Future Works" section notes that ICM yields reduced performance on simpler models (MLPs, Linear) on FMNIST because they struggle to accommodate the increased complexity and feature overlap introduced by the augmentation.
- Why unresolved: The noise reduction benefits of ICM currently appear to be overshadowed by the feature complexity it introduces, which exceeds the capacity of simpler architectures.
- What evidence would resolve it: A modified ICM technique or a specific regularization adjustment that results in performance gains for MLPs and linear models comparable to those observed in ResNet18.

### Open Question 3
- Question: Does ICM enable CLL algorithms to scale effectively to datasets with significantly higher class counts (e.g., >100 classes)?
- Basis in paper: The authors note in the "Datasets" section that current state-of-the-art CLL algorithms struggle to produce meaningful classifiers on datasets with 100 classes (e.g., TinyImageNet), which is why they limit benchmarking to 10-20 classes.
- Why unresolved: It is unclear if the noise reduction provided by ICM is sufficient to overcome the optimization challenges inherent in high-dimensional output spaces where standard CLL currently fails.
- What evidence would resolve it: Successful training and evaluation of an ICM-enhanced model on a high-cardinality dataset like TinyImageNet or CIFAR100 where standard CLL methods typically fail.

## Limitations
- Performance claims rely heavily on comparisons with existing CLL methods, but ICM's effectiveness with newer CLL algorithms remains untested
- The clustering step could introduce its own biases depending on algorithm and parameters used
- Computational overhead from clustering is not thoroughly explored, especially for large datasets
- Method's effectiveness on extremely large-scale datasets or in online learning scenarios remains untested

## Confidence
- High confidence in ICM's effectiveness for standard CLL benchmarks (MNIST, CIFAR)
- Medium confidence in ICM's general applicability across diverse datasets and conditions
- Medium confidence in the claim that Mixup is inherently unsuitable for CLL without modifications
- Low confidence in ICM's scalability to very large datasets or real-time applications

## Next Checks
1. Test ICM with recently developed CLL algorithms not included in the original comparisons to assess broader applicability
2. Evaluate ICM's performance on larger-scale datasets (ImageNet-scale) and measure computational overhead
3. Conduct ablation studies to determine the impact of different clustering algorithms and parameters on ICM's performance