---
ver: rpa2
title: 'EnronQA: Towards Personalized RAG over Private Documents'
arxiv_id: '2505.00263'
source_url: https://arxiv.org/abs/2505.00263
tags:
- question
- email
- emails
- https
- enronqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EnronQA is a large-scale benchmark for retrieval augmented generation
  over private documents, containing 103,638 emails with 528,304 question-answer pairs
  across 150 user inboxes. The dataset is designed to enable personalized and private
  RAG benchmarking, addressing the gap where most existing benchmarks use public data
  like Wikipedia.
---

# EnronQA: Towards Personalized RAG over Private Documents

## Quick Facts
- arXiv ID: 2505.00263
- Source URL: https://arxiv.org/abs/2505.00263
- Authors: Michael J. Ryan; Danmei Xu; Chris Nivera; Daniel Campos
- Reference count: 40
- Primary result: Large-scale benchmark with 103,638 emails and 528,304 QA pairs for private RAG evaluation

## Executive Summary
EnronQA addresses the critical gap in RAG benchmarking by providing a large-scale dataset of private emails with synthetically generated question-answer pairs. The benchmark contains 103,638 emails from 150 user inboxes, totaling 528,304 QA pairs, specifically designed to evaluate personalized retrieval over private documents. Unlike existing benchmarks that use public data like Wikipedia, EnronQA enables more realistic assessment of RAG systems handling sensitive, user-specific information. The dataset reveals significant headroom for improvement in personalized RAG pipelines, with BM25 achieving 87.5% accuracy without query rewriting.

## Method Summary
EnronQA was constructed through a multi-stage LLM pipeline that generates questions meeting specific quality criteria including specificity, objectivity, groundedness, and overall quality. The synthetic generation approach ensures questions are well-formed while avoiding the pitfalls of public benchmarks where memorized LLM knowledge can inflate retrieval scores. The dataset focuses on the Enron email corpus, which provides realistic private business communication data. Questions were generated to reflect actual information needs that users might have when searching through their private email archives, creating a more authentic evaluation environment for personalized RAG systems.

## Key Results
- EnronQA provides better calibration for retrieval quality compared to public benchmarks like NaturalQuestions and TriviaQA
- BM25 achieves 87.5% accuracy on EnronQA without query rewriting, revealing headroom for improvement
- LoRA-based factual memorization shows competitive performance to long-context approaches, though retrieval still outperforms memorization
- The benchmark successfully avoids LLM memorization inflation that plagues public dataset evaluations

## Why This Works (Mechanism)
EnronQA works by providing a private document benchmark that eliminates the memorization bias inherent in public dataset evaluations. When RAG systems are tested on public benchmarks, they can often answer questions correctly using memorized knowledge rather than actual document retrieval. By using private email data with synthetically generated questions, EnronQA forces systems to actually retrieve and use the document content. The multi-stage generation pipeline ensures questions are specific and grounded in the actual document content while maintaining objectivity and quality standards. This creates a more realistic evaluation environment that better reflects real-world RAG use cases where users query their own private document collections.

## Foundational Learning
- **Synthetic Question Generation**: Why needed - To create high-quality, controlled questions without human annotation costs; Quick check - Verify questions meet specificity and objectivity criteria
- **Retrieval Quality Calibration**: Why needed - To distinguish between memorization and actual retrieval performance; Quick check - Compare scores on public vs private benchmarks
- **Private Document Retrieval**: Why needed - Real-world RAG systems often handle sensitive user data; Quick check - Ensure benchmark questions cannot be answered without document access
- **Personalization in RAG**: Why needed - Different users have different information needs and document collections; Quick check - Evaluate across diverse user inboxes
- **Factual Memorization vs Retrieval**: Why needed - To understand when systems rely on memory vs actual document lookup; Quick check - Test with modified document content
- **Benchmark Construction Methodology**: Why needed - To ensure reproducibility and validity of evaluation results; Quick check - Validate generation pipeline consistency

## Architecture Onboarding

**Component Map**: LLM Generator -> Quality Filter -> QA Pair Storage -> RAG Pipeline Evaluation

**Critical Path**: Synthetic question generation → Quality filtering → Benchmark storage → System evaluation → Performance analysis

**Design Tradeoffs**: Synthetic generation enables large-scale creation without human annotation costs but may miss real user query diversity; private data ensures realistic evaluation but limits public sharing; multi-stage pipeline ensures quality but adds complexity to benchmark creation

**Failure Signatures**: Over-reliance on memorized knowledge (inflated scores on public benchmarks), poor generalization to diverse user inboxes, inability to handle nuanced private document queries, computational inefficiency in large-scale retrieval

**3 First Experiments**:
1. Compare retrieval accuracy on EnronQA vs public benchmarks to quantify memorization bias
2. Evaluate different retrieval methods (BM25, dense retrieval) on the same user inbox to assess method sensitivity
3. Test LoRA memorization vs long-context approaches on identical question sets to measure relative performance

## Open Questions the Paper Calls Out
The paper acknowledges that the synthetic nature of question generation may not fully capture the diversity and complexity of real user queries to private document collections. While the multi-stage LLM pipeline ensures quality metrics like specificity and objectivity, the lack of human-annotated questions means the benchmark may miss edge cases or nuanced information needs that arise in actual private document retrieval scenarios.

## Limitations
- Synthetic question generation may not capture the full diversity of real user queries
- Limited analysis of privacy-preserving aspects beyond the dataset's private nature
- Focus primarily on retrieval accuracy with less attention to computational costs or scalability
- May not generalize well to document types beyond email
- Benchmark construction complexity may limit easy replication for other private datasets

## Confidence
High - Benchmark provides better retrieval quality calibration compared to public datasets
Medium - Relative performance of different RAG approaches is demonstrated but may miss practical considerations
Medium - Claims about headroom for improvement are supported but should be interpreted cautiously given synthetic questions

## Next Checks
1. Conduct human evaluation studies comparing EnronQA-generated questions with real user queries to assess ecological validity and identify potential gaps in question diversity
2. Extend the benchmark to include explicit privacy metrics and adversarial testing to evaluate how well different RAG approaches protect sensitive information in private documents
3. Test the benchmark across different document types and domains beyond email to evaluate generalizability of the results to other private document collections