---
ver: rpa2
title: Cross-Modal Knowledge Distillation for Speech Large Language Models
arxiv_id: '2509.14930'
source_url: https://arxiv.org/abs/2509.14930
tags:
- speech
- text
- knowledge
- distillation
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies catastrophic forgetting and modality inequivalence\
  \ as key challenges in speech large language models (LLMs), where adding speech\
  \ capabilities degrades text-based reasoning and spoken query performance. The authors\
  \ propose a cross-modal knowledge distillation framework that transfers knowledge\
  \ from a text-based teacher LLM to a speech LLM via two complementary channels:\
  \ text-to-text (T\u2192T) and speech-to-text (S\u2192T) distillation."
---

# Cross-Modal Knowledge Distillation for Speech Large Language Models

## Quick Facts
- arXiv ID: 2509.14930
- Source URL: https://arxiv.org/abs/2509.14930
- Reference count: 0
- The paper demonstrates that cross-modal knowledge distillation significantly improves speech LLM performance across dialogue and audio understanding tasks

## Executive Summary
This paper addresses the challenge of catastrophic forgetting and modality inequivalence in speech large language models (LLMs), where adding speech capabilities often degrades text-based reasoning and spoken query performance. The authors propose a cross-modal knowledge distillation framework that transfers knowledge from a text-based teacher LLM to a speech LLM through two complementary channels: text-to-text (T→T) and speech-to-text (S→T) distillation. Using only ~60k samples, the method significantly improves Qwen2.5-Omni's performance across dialogue and audio understanding tasks, achieving 77.19% overall accuracy on the VoiceBench benchmark compared to 75.08% baseline.

## Method Summary
The proposed framework leverages a text-based teacher LLM to guide the training of a speech LLM through two complementary distillation channels. The text-to-text (T→T) channel ensures preservation of text-based reasoning capabilities by having both teacher and student process text inputs and match their outputs. The speech-to-text (S→T) channel focuses on speech understanding by having the teacher process transcribed speech while the student processes raw speech, with their outputs aligned. This dual-channel approach addresses the modality inequivalence problem by ensuring the speech LLM maintains strong performance on both textual and spoken inputs while preserving the reasoning capabilities of the text-based teacher.

## Key Results
- The enhanced model achieves 77.19% overall accuracy on VoiceBench benchmark vs. 75.08% baseline
- Significant improvements in open-ended QA, knowledge QA, and instruction following tasks
- Audio analysis reasoning improves, demonstrating better cross-modal alignment while preserving textual knowledge

## Why This Works (Mechanism)
The effectiveness stems from addressing two fundamental challenges in speech LLMs: catastrophic forgetting (loss of text-based reasoning when speech capabilities are added) and modality inequivalence (difficulty in processing speech vs text inputs equally well). By using a strong text-based teacher model as a reference, the framework ensures that the speech LLM maintains high-quality text reasoning while learning to process speech inputs. The dual-channel approach allows for targeted knowledge transfer, where T→T distillation preserves textual capabilities and S→T distillation enhances speech understanding, creating a balanced model that performs well across both modalities.

## Foundational Learning

**Speech LLM Architecture** - Understanding the integration of speech processing modules with transformer-based LLM architectures is crucial for implementing cross-modal approaches. Quick check: Verify compatibility between speech encoder outputs and LLM token embeddings.

**Knowledge Distillation Fundamentals** - The framework relies on transferring knowledge from teacher to student models through output matching. Quick check: Ensure temperature scaling and loss weighting are properly configured for stable training.

**Cross-Modal Alignment** - The method requires understanding how to align representations across different input modalities (speech vs text). Quick check: Validate that speech-to-text alignment produces consistent semantic representations.

## Architecture Onboarding

**Component Map:** Speech Encoder -> LLM Backbone -> Text Decoder -> Teacher LLM (Text-only)
**Critical Path:** Speech input → Encoder → Backbone → Decoder → Output, with Teacher parallel path for distillation
**Design Tradeoffs:** T→T distillation preserves text reasoning but may limit speech-specific optimization; S→T distillation enhances speech understanding but requires accurate transcriptions
**Failure Signatures:** Performance degradation on text tasks indicates insufficient T→T distillation; poor speech understanding suggests inadequate S→T distillation
**3 First Experiments:** 1) Test T→T distillation alone to verify text reasoning preservation, 2) Test S→T distillation alone to assess speech understanding improvements, 3) Combine both channels to evaluate complementary effects

## Open Questions the Paper Calls Out

None

## Limitations

- The dataset size of ~60k samples may limit scalability to larger models or more diverse domains
- Evaluation focuses primarily on VoiceBench benchmark and specific dialogue/audio tasks, leaving uncertainty about generalization to other speech domains or languages
- The methodology assumes access to a strong text-based teacher model, which may not always be available or may introduce its own biases

## Confidence

High confidence: The effectiveness of the cross-modal knowledge distillation framework in improving both speech understanding and preserving text-based reasoning capabilities is well-supported by quantitative results across multiple benchmarks.

Medium confidence: Claims about the generality of improvements across diverse speech domains and the robustness of the approach to different teacher model architectures require additional validation.

Low confidence: The scalability of the approach to significantly larger models and the long-term stability of the distilled knowledge representations have not been thoroughly examined.

## Next Checks

1. Evaluate the distilled model on multilingual speech datasets to assess cross-language generalization and identify potential language-specific limitations.

2. Conduct ablation studies comparing the effectiveness of different distillation channel combinations (T→T vs S→T) and their relative contributions to overall performance gains.

3. Test the approach with teacher models of varying sizes and architectures to determine the minimum requirements for effective knowledge transfer and identify potential bottlenecks.