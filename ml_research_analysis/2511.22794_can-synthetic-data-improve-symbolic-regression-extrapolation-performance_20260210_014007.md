---
ver: rpa2
title: Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?
arxiv_id: '2511.22794'
source_url: https://arxiv.org/abs/2511.22794
tags:
- data
- extrapolation
- synthetic
- teacher
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether synthetic data can improve symbolic
  regression (SR) extrapolation performance. Using Kernel Density Estimation (KDE)
  to identify sparse input regions, synthetic data is generated via knowledge distillation
  from teacher models (NN, RF, GP) to train student GP models.
---

# Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?

## Quick Facts
- arXiv ID: 2511.22794
- Source URL: https://arxiv.org/abs/2511.22794
- Authors: Fitria Wulandari Ramlan; Colm O'Riordan; Gabriel Kronberger; James McDermott
- Reference count: 35
- Primary result: Synthetic data augmentation via knowledge distillation improves symbolic regression extrapolation performance, with GPp models showing most consistent gains when trained on GPe-generated synthetic data

## Executive Summary
This study investigates whether synthetic data can improve symbolic regression (SR) extrapolation performance. Using Kernel Density Estimation (KDE) to identify sparse input regions, synthetic data is generated via knowledge distillation from teacher models (NN, RF, GP) to train student GP models. Across six benchmark datasets, GP models often improve when trained on synthetic data, especially in extrapolation areas. The most consistent improvements occur when synthetic data from GPe is used to train GPp. Changes in interpolation areas show only slight changes. Statistical significance analysis reveals heterogeneous errors, where model performance varies across input space regions. The results demonstrate that synthetic data augmentation offers a practical solution for better extrapolation in symbolic regression.

## Method Summary
The researchers employed Kernel Density Estimation to identify sparse regions in the input space, then generated synthetic data through knowledge distillation from teacher models (Neural Networks, Random Forests, and Gaussian Processes) to train student GP models. The approach involved creating synthetic data in extrapolation regions identified by KDE, then using this augmented dataset to train GP models. Six benchmark datasets were used to evaluate performance, comparing standard GP training against GP training with synthetic data augmentation. Statistical significance analysis was performed to assess heterogeneous error patterns across different input space regions.

## Key Results
- GP models showed improved extrapolation performance when trained on synthetic data, particularly in sparse input regions
- Most consistent improvements occurred when GPp was trained using synthetic data generated by GPe
- Statistical analysis revealed heterogeneous error patterns, with model performance varying across different input space regions

## Why This Works (Mechanism)
Synthetic data augmentation addresses the fundamental challenge of extrapolation in symbolic regression by providing additional training examples in regions where the original data is sparse or absent. Knowledge distillation from teacher models allows the generation of synthetic data that captures complex patterns and relationships that may be difficult to learn from limited real data alone. By focusing on extrapolation regions identified through KDE, the approach specifically targets the areas where traditional symbolic regression models struggle most. The teacher models serve as expert guides, providing high-quality synthetic examples that help the student GP model develop better generalization capabilities beyond the original training distribution.

## Foundational Learning

**Symbolic Regression**: Why needed - forms the core problem domain; Quick check - can the student model discover mathematical expressions from data

**Knowledge Distillation**: Why needed - enables transfer of learned patterns from teacher to student models; Quick check - does the distilled knowledge improve student performance

**Kernel Density Estimation**: Why needed - identifies sparse regions requiring synthetic data generation; Quick check - are the identified regions actually problematic for extrapolation

**Gaussian Processes**: Why needed - serves as both teacher and student model in the experimental framework; Quick check - does GP as teacher provide meaningful synthetic data

**Neural Networks**: Why needed - provides alternative teacher model perspective; Quick check - does NN-generated synthetic data differ meaningfully from GP-generated data

**Random Forests**: Why needed - adds ensemble learning perspective as teacher model; Quick check - does RF-generated synthetic data complement other teacher models

## Architecture Onboarding

Component Map: KDE -> Teacher Models (NN, RF, GP) -> Synthetic Data Generation -> Student GP Training

Critical Path: Data preprocessing with KDE → Teacher model training → Knowledge distillation → Synthetic data creation → Student GP training with augmented data → Performance evaluation

Design Tradeoffs: Using multiple teacher models provides diverse synthetic data but increases computational cost; focusing on extrapolation regions targets key weaknesses but may neglect interpolation improvements; knowledge distillation quality depends heavily on teacher model performance

Failure Signatures: Poor synthetic data quality from weak teacher models; overfitting to synthetic data; heterogeneous error patterns suggesting incomplete coverage of extrapolation regions

First Experiments: 1) Compare single teacher model vs. multiple teacher models for synthetic data generation; 2) Test different KDE bandwidths for identifying extrapolation regions; 3) Evaluate synthetic data impact on varying dataset sizes and noise levels

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data quality heavily depends on teacher model performance, limiting generalizability
- Heterogeneous error patterns suggest the approach may be less effective in certain extrapolation regions
- Computational cost increases with multiple teacher models and synthetic data generation

## Confidence
- Claim: Synthetic data augmentation improves extrapolation performance - Medium
- Claim: GPp benefits most consistently from GPe synthetic data - High

## Next Checks
1. Test the synthetic data augmentation approach on additional benchmark datasets with varying levels of noise and complexity to assess generalizability
2. Evaluate the impact of different teacher model architectures and hyperparameters on synthetic data quality and subsequent GP performance
3. Investigate whether combining synthetic data from multiple teacher models (ensemble approach) yields better extrapolation performance than using a single teacher model's synthetic data