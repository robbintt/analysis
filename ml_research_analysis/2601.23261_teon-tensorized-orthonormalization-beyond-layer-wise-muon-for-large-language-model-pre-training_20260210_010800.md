---
ver: rpa2
title: 'TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language
  Model Pre-Training'
arxiv_id: '2601.23261'
source_url: https://arxiv.org/abs/2601.23261
tags:
- muon
- teon
- arxiv
- training
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TEON, a tensor-level generalization of the
  Muon optimizer that extends gradient orthogonalization from individual matrices
  to structured tensors. By modeling stacking multiple gradient matrices as higher-order
  tensors, TEON improves the training performance of Transformer models.
---

# TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training

## Quick Facts
- **arXiv ID**: 2601.23261
- **Source URL**: https://arxiv.org/abs/2601.23261
- **Reference count**: 30
- **Primary result**: TEON achieves up to √K× better convergence bounds than Muon and consistently improves validation perplexity on GPT and LLaMA models (60M-1B parameters).

## Executive Summary
TEON extends the Muon optimizer by generalizing gradient orthogonalization from individual matrices to structured tensors, enabling capture of cross-layer dependencies in Transformer models. By stacking gradients from multiple layers into a tensor and applying mode-specific matricization followed by orthogonalization, TEON achieves stronger convergence guarantees and empirical performance improvements over Muon and AdamW. Extensive experiments demonstrate consistent perplexity reductions across model scales, with theoretical analysis showing up to √K× better convergence bounds when gradients exhibit strong singular-vector alignment.

## Method Summary
TEON operates by collecting momentum matrices from K consecutive layers (default K=2), stacking them into a tensor T ∈ ℝ^(m×n×K), and performing mode-i matricization followed by orthogonalization via Newton-Schulz or PolarExpress (5 iterations). The orthogonalized slices are then folded back and distributed to each layer with scaling √(m/n). The method specifically targets QKV gradient matrices where cross-layer singular-vector alignment is strongest, while using AdamW for embeddings, positional encodings, and normalization layers. Training uses mixed precision on H100/A100 GPUs with FineWeb dataset, sequence length 8,192, batch size 512, weight decay 0.1, and cosine learning rate schedule.

## Key Results
- TEON achieves up to √K× better convergence bounds than Muon when gradients exhibit strong singular-vector alignment
- Consistent validation perplexity improvements across GPT and LLaMA models (60M-1B parameters)
- Mode-1 matricization outperforms mode-2 when right singular vectors of QKV gradients are aligned
- Stacking only QKV matrices yields better results than including MLP gradients
- PolarExpress provides closest approximation to exact SVD among tested methods

## Why This Works (Mechanism)

### Mechanism 1: Cross-Layer Gradient Correlation Exploitation
Stacking gradients from multiple layers into a tensor enables capture of cross-layer dependencies that layer-wise Muon ignores. By forming tensor T ∈ ℝ^(m×n×K) and applying mode-i matricization followed by orthogonalization, TEON treats correlated gradient directions jointly rather than independently. Performance degrades when singular vectors across candidate layers show low alignment (⟨v₁⁽ᵏ⁾, v₁⁽ᵏ'⁾⟩ → 0), as confirmed by ablation studies.

### Mechanism 2: Smoothness Constant Reduction Under TEON Norm
TEON achieves up to √K× better convergence bound than Muon by operating under a norm with smaller effective smoothness constant. Under NTR analysis, convergence depends on the product L·||∇f||*. When gradients exhibit strong singular-vector alignment, LMUON ≈ K·LTEON, giving TEON a √K advantage in convergence rate. Under misaligned gradients, LMUON ≈ LTEON, and convergence bounds match.

### Mechanism 3: Mode-Specific Matricization Preserves Alignment Structure
Choice of matricization mode (1 vs. 2) determines whether right or left singular-vector alignment is exploited. Mode-1 matricization M₁(G) ∈ ℝ^(m×nK) concatenates gradient matrices horizontally. If right singular vectors v₁ are aligned across layers, M₁(G) has low effective rank, and mode-1 orthogonalization captures this structure efficiently. Mode-2 analogously exploits left singular-vector alignment.

## Foundational Learning

- **Muon Optimizer (Newton-Schulz Orthogonalization)**: TEON extends Muon's matrix-level orthogonalization; understanding Muon is prerequisite. Quick check: Can you explain why replacing Σ with I in SVD (UΣVᵀ → UVᵀ) prevents gradient rank collapse?

- **Tensor Matricization (Mode-i Unfolding)**: TEON's core operation reduces tensors to matrices via mode-i unfolding. Quick check: Given a tensor G ∈ ℝ^(m×n×K), what are the dimensions of M₁(G) and M₃(G)?

- **Operator and Nuclear Norms as Dual Pairs**: TEON's convergence analysis relies on dual norm relationships. Quick check: Why does the dual of operator norm equal nuclear norm?

## Architecture Onboarding

- **Component map**: Gradient collector → Tensor former → Mode-1 matricizer → Orthogonalizer → Updater

- **Critical path**: QKV gradient stacking → Mode-1 unfolding → Approximate SVD (5 iterations, PolarExpress preferred) → Per-layer parameter update

- **Design tradeoffs**: K larger → potential √K gain but requires K-way singular-vector alignment; degrades quickly. Stacking MLP gradients → increases aspect ratio imbalance, hurts SVD approximation accuracy. Exact vs. approximate SVD → exact gives best results; PolarExpress closest among approximations.

- **Failure signatures**: Validation perplexity worse than Muon → check if non-QKV layers are stacked. Numerical instability → likely from highly rectangular M₁(G); verify K≤2 and exclude MLP. Slow convergence early → may indicate wrong matricization mode; confirm using mode-1 for right-singular alignment.

- **First 3 experiments**:
  1. Run Muon vs. TEON (K=2, QKV only, mode-1, PolarExpress 5 iterations) on GPT-Small/1B tokens; expect ~0.3-0.5 PPL improvement.
  2. Compare mode-1 vs. mode-2 on same setup; expect mode-1 to win by ~0.2-0.6 PPL.
  3. Test K∈{2,4,6} with exact SVD; expect monotonic degradation as K increases, confirming alignment dilution.

## Open Questions the Paper Calls Out
None

## Limitations
- Cross-layer gradient correlation strength remains an empirical question not fully resolved, with performance sensitive to singular-vector alignment assumptions
- Approximate SVD quality affects both convergence guarantees and final performance, with limited rigorous bounds on approximation error propagation
- Theoretical smoothness assumptions rely on NTR analysis that may not capture practical training dynamics uniformly across all training phases

## Confidence
- **High confidence** in: TEON's implementation feasibility and basic performance improvement over Muon
- **Medium confidence** in: The √K theoretical convergence advantage, dependent on smoothness constant ratios in practice
- **Low confidence** in: Universal applicability of mode-1 matricization, as alignment patterns may vary across architectures

## Next Checks
1. Systematically measure ⟨v₁⁽ᵏ⁾, v₁⁽ᵏ'⁾⟩ and ⟨u₁⁽ᵏ⁾, u₁⁽ᵏ'⁾⟩ across all layer types and training epochs for both GPT and LLaMA architectures
2. Track LMUON and LTEON ratios empirically during training by measuring gradient norms under both norms at regular intervals
3. Test TEON on architectures with different layer organizations (e.g., Vision Transformers, MLPs) to determine whether QKV-specific alignment patterns generalize