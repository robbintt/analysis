---
ver: rpa2
title: Backpropagation-Free Metropolis-Adjusted Langevin Algorithm
arxiv_id: '2505.18081'
source_url: https://arxiv.org/abs/2505.18081
tags:
- forward-mode
- mala
- step
- proposal
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first backpropagation-free gradient-based
  Markov chain Monte Carlo (MCMC) algorithm by incorporating forward-mode automatic
  differentiation into the Metropolis-adjusted Langevin algorithm (MALA). The key
  insight is to use sampled tangent vectors as part of the proposal mechanism, eliminating
  the need for reverse-mode backpropagation.
---

# Backpropagation-Free Metropolis-Adjusted Langevin Algorithm

## Quick Facts
- **arXiv ID:** 2505.18081
- **Source URL:** https://arxiv.org/abs/2505.18081
- **Reference count:** 40
- **One-line primary result:** Introduces first backpropagation-free gradient-based MCMC algorithm using forward-mode AD with sampled tangent vectors.

## Executive Summary
This paper presents the first backpropagation-free gradient-based Markov chain Monte Carlo algorithm by incorporating forward-mode automatic differentiation into the Metropolis-adjusted Langevin algorithm (MALA). The key innovation is using sampled tangent vectors as part of the proposal mechanism, eliminating the need for reverse-mode backpropagation. Four new algorithms are proposed: Forward MALA, Line Forward MALA, Pre-conditioned Forward MALA, and Pre-conditioned Line Forward MALA. These methods demonstrate competitive performance with traditional MALA while offering significantly reduced computational cost and memory footprint, successfully scaling to high-dimensional models where traditional MALA encounters memory errors.

## Method Summary
The method replaces reverse-mode backpropagation in MALA with forward-mode automatic differentiation. For each proposal, a unit tangent vector is sampled from the sphere, and the first-order forward-mode operator returns both the function value and directional derivative. This JVP replaces the full gradient in the Langevin drift term. The proposal mechanism incorporates the tangent distribution, and since the reverse proposal uses the same tangent vector, these terms cancel in the Metropolis-Hastings acceptance ratio. Four variants are introduced: standard forward MALA (using new tangent for reverse proposal), line-based MALA (constraining proposals to sampled lines for variance reduction), preconditioned forward MALA (using second-order forward-mode for Hessian-vector products), and preconditioned line-based MALA (combining both approaches).

## Key Results
- Forward-mode samplers achieve comparable or better performance metrics (KL divergence, ESS, accuracy) with 25-34% reduction in wall-clock time
- Successfully scales to high-dimensional models where traditional MALA encounters memory errors
- Pre-conditioned Line Forward MALA performs particularly well across multiple model architectures
- Demonstrates viability for Bayesian neural networks and CNNs where reverse-mode MALA fails due to memory constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Forward-mode AD with sampled tangent vectors yields valid gradient-based MCMC proposals without backpropagation.
- **Mechanism:** Sample unit tangent vector v̂ ~ Uniform(S^D-1). First-order forward-mode operator F₁(θ, v̂) returns both f(θ) and directional derivative ∇f(θ)·v̂. This JVP replaces full gradient in Langevin drift: θ* = θ_t + (η²/2)(∇f(θ_t)·v̂_t)v̂_t + ηz_t. Tangent distribution incorporated into proposal, and since q(v̂*) = q(v̂_t), terms cancel in MH acceptance ratio.
- **Core assumption:** Directional derivative along uniformly sampled direction provides sufficient gradient information to guide proposals toward high-density regions, even with 1/D bias.
- **Evidence anchors:** [abstract] sampling tangent vector can be incorporated into proposal mechanism for MALA; [Section 4.1, Eq. 5] FMALA proposal explicitly uses (∇f(θ_t)·v̂_t)v̂_t; [Section 3.1] E[(∇f(θ)·v)v] = ∇f(θ) for v ~ N(0,I); unit sphere variant analyzed in App. B.
- **Break condition:** In high dimensions with strongly anisotropic target geometry, 1/D bias and variance inflation may cause FMALA to degrade relative to reverse-mode MALA.

### Mechanism 2
- **Claim:** Constraining proposals to a sampled line reduces variance and improves step-size robustness.
- **Mechanism:** Two-stage Line-FMALA: sample direction v̂, then perform 1D MALA along that line. Proposal becomes scalar: α* = α_t + (η²/2)∇f(θ_t)·v̂ + ηz_t where α_t = θ_t·v̂. Forward and reverse proposals share same v̂, ensuring consistent geometry evaluation.
- **Core assumption:** Restricting movement to single direction per step trades exploration for more stable acceptance, which helps when curvature varies across directions.
- **Evidence anchors:** [Section 4.2, Eq. 6-7] Line-FMALA proposal mechanism defined; [Figure 5] Line-based samplers show broader step-size stability than MALA in 10D funnel; [Section 5.1] "line-based samplers exhibit greater robustness to the step size η".
- **Break condition:** If target has significant probability mass far from any line through current position, line-based sampling may require many iterations to explore.

### Mechanism 3
- **Claim:** Second-order forward-mode provides Hessian-vector products for position-specific preconditioning without full Hessian computation.
- **Mechanism:** F₂(θ, v̂) returns f(θ), ∇f(θ)·v̂, and v̂^T∇²f(θ)v̂. Quadratic term scales both drift and noise: θ* = θ_t + (η²/2|v̂^T∇²f(θ)v̂|)(∇f·v̂)v̂ + ηz_t/√|v̂^T∇²f(θ)v̂|. For PC-Line-FMALA, same v̂ used for both forward and reverse, aligning curvature information.
- **Core assumption:** Curvature along sampled direction approximates local geometry well enough to improve conditioning, even though v̂^T∇²f(θ)v̂ is 1D slice of full Hessian.
- **Evidence anchors:** [Section 4.3, Eq. 8-10] Pre-conditioned proposals defined using |v̂^T∇²f(θ)v̂|; [Table 1] PC-L-FMALA achieves lowest KL in 10D funnel (0.039 vs 0.160 for MALA); [Table 3] PC-L-FMALA achieves best NLL on BNN regression (-0.691 vs -0.458 for MALA).
- **Break condition:** PC-FMALA (non-line) underperforms because forward and reverse proposals evaluate Hessian terms along different v̂ directions, causing covariance mismatch.

## Foundational Learning

- **Concept:** Metropolis-Hastings acceptance criterion
  - **Why needed here:** The MH step ensures detailed balance despite gradient approximations. Understanding why q(v̂*) and q(v̂_t) cancel is essential for seeing why forward-mode works.
  - **Quick check question:** If proposal distributions q(θ*|θ_t, v̂_t) and q(θ_t|θ*, v̂*) have different means but share same tangent distribution, does MH still yield correct stationary distribution?

- **Concept:** Forward-mode automatic differentiation and Jacobian-vector products
  - **Why needed here:** The entire method hinges on F₁(θ, v) returning both f(θ) and ∇f·v in O(2×) forward pass time without storing intermediates.
  - **Quick check question:** For scalar output function, why does JVP equal directional derivative, and why does this have lower memory cost than VJP (reverse-mode)?

- **Concept:** Langevin dynamics as gradient flow with noise
  - **Why needed here:** MALA's proposal θ* = θ + (ε²/2)∇log p(θ) + εz derives from discretizing Langevin diffusion; replacing ∇log p with directional approximation preserves structure if MH corrects.
  - **Quick check question:** What happens to stationary distribution if you use biased gradient estimator in drift term without MH correction?

## Architecture Onboarding

- **Component map:** fojax/ (core JAX implementation) -> Forward-mode operators (F1, F2) -> Samplers (FMALA, LineFMALA, PCFMALA, PCLineFMALA) -> MH acceptance
- **Critical path:** 1. Initialize θ_0, set step size η, select sampler variant 2. Per iteration: sample v̂, compute F₁ or F₂, construct proposal, evaluate reverse proposal, compute γ, accept/reject 3. Burn-in, thin, compute ESS/diagnostics
- **Design tradeoffs:** FMALA vs Line-FMALA: FMALA uses 2 forward passes (new v̂* for reverse), Line uses 1 forward pass but restricted exploration; Preconditioning: PC-L-FMALA adds curvature adaptation but requires F₂ (second-order dual numbers); Step size η: Line variants more robust (Fig. 5); FMALA benefits from η̃ = η√D bias correction (App. B.1)
- **Failure signatures:** High rejection rate with FMALA in high D: check if bias correction (Section 4.4, App. B.1) is applied; PC-FMALA underperforms PC-L-FMALA: expected due to Hessian direction mismatch (Section 5.2); Out-of-memory with MALA but not forward-mode: reverse-mode requires storing computation graph; forward-mode does not (Table 6 shows MALA OOM at N=30,000)
- **First 3 experiments:** 1. 2D Rosenbrock visualization: Run FMALA and Line-FMALA for 1000 steps, plot proposal contours as in Fig. 1-4 2. Funnel distribution (10D, 50D): Replicate Table 1 subset, focus on KL divergence and ESS 3. BNN regression (small network, D~1000): Compare Line-FMALA vs MALA on synthetic data, measure NLL, wall-clock time, memory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can forward-mode samplers maintain competitive performance against reverse-mode methods in distributions with highly varying curvature at extreme dimensions?
- Basis in paper: [explicit] Section 6 states that "forward-mode samplers underperform at higher dimensions" for the ill-conditioned funnel distribution.
- Why unresolved: The experiments only test up to 100D for the funnel, where degradation is already observed, suggesting potential scalability limits with specific geometries.
- What evidence would resolve it: Demonstrating low KL divergence and high ESS on the funnel distribution at dimensions significantly greater than 100D.

### Open Question 2
- Question: Does the directional mismatch of Hessian information in PC-FMALA fundamentally limit its efficiency compared to line-based variants?
- Basis in paper: [explicit] Section 5.2 notes that PC-FMALA underperforms because "Hessian information for the forward and reverse proposals are not evaluated along the same direction."
- Why unresolved: The paper identifies the misalignment issue but offers no solution other than defaulting to the line-based sampler, leaving the full potential of standard PC-FMALA uncertain.
- What evidence would resolve it: A modified PC-FMALA algorithm that aligns Hessian evaluations or a theoretical proof showing that line restrictions are strictly necessary for stability.

### Open Question 3
- Question: Can forward-mode automatic differentiation be effectively integrated into multi-step sampling trajectories like Hamiltonian Monte Carlo (HMC)?
- Basis in paper: [inferred] The paper focuses exclusively on single-step MALA algorithm, while related work mentions HMC.
- Why unresolved: It is unknown if noise and bias inherent in forward-mode gradient estimates would accumulate excessively over multiple leapfrog steps required by HMC.
- What evidence would resolve it: A functional Forward-Mode HMC implementation that achieves comparable Effective Sample Size (ESS) to standard HMC without diverging.

## Limitations

- Performance degradation in high-dimensional anisotropic distributions due to 1/D bias in directional gradient estimation
- Directional Hessian approximation provides only limited curvature information, potentially insufficient for highly ill-conditioned problems
- Experimental validation primarily limited to synthetic distributions and standard benchmark datasets, leaving questions about complex real-world posterior distributions

## Confidence

- **High Confidence:** Core claim that forward-mode AD can replace reverse-mode in MALA proposals is well-supported by theoretical analysis and experimental validation. Memory and computational efficiency gains (25-34% wall-clock reduction) are directly measurable and consistently observed.
- **Medium Confidence:** Performance of preconditioned variants on real-world tasks is promising but based on limited testing. Superiority of line-based methods over standard FMALA is demonstrated but theoretical justification for why direction alignment matters is less rigorous.
- **Low Confidence:** Scalability claims to "high-dimensional" problems are somewhat overstated - while method avoids memory errors, 100D funnel results show significant performance degradation relative to preconditioned variants, suggesting practical limits to dimensionality.

## Next Checks

1. **High-D Correlation Structure Test:** Evaluate proposed samplers on high-dimensional Gaussian with controlled correlation structure (e.g., AR(1) or Toeplitz) to systematically assess how directional bias affects convergence across different correlation strengths and lengths.

2. **Alternative Curvature Estimators:** Implement and compare against alternative low-rank Hessian approximation methods (e.g., randomized Hutchinson trace estimators) to isolate whether directional curvature approximation or forward-mode framework is primarily responsible for PC-L-FMALA's performance.

3. **Real-World Bayesian Inference:** Apply methods to real Bayesian inference problem with known posterior structure (e.g., logistic regression on moderately sized classification dataset with D=10,000 features) to validate scalability claims beyond synthetic benchmarks.