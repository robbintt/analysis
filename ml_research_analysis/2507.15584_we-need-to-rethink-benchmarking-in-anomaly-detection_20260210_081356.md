---
ver: rpa2
title: We Need to Rethink Benchmarking in Anomaly Detection
arxiv_id: '2507.15584'
source_url: https://arxiv.org/abs/2507.15584
tags:
- detection
- anomaly
- data
- algorithms
- anomalies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This position paper argues that current anomaly detection benchmarking
  practices are insufficient, as they fail to capture the diversity of anomalies across
  different applications and often allow simple methods to perform competitively with
  complex state-of-the-art algorithms. The authors identify three key areas for improvement:
  (1) developing a taxonomy-based scenario-specific approach to benchmarking, (2)
  analyzing anomaly detection pipelines by their individual components (preprocessing,
  model selection, and ensembles), and (3) evaluating algorithms based on scenario-specific
  objectives that go beyond traditional rank-based metrics.'
---

# We Need to Rethink Benchmarking in Anomaly Detection

## Quick Facts
- arXiv ID: 2507.15584
- Source URL: https://arxiv.org/abs/2507.15584
- Authors: Philipp Röchner; Simon Klüttermann; Franz Rothlauf; Daniel Schlör
- Reference count: 40
- Primary result: A simple Quantiles-based approach performs similarly to established baselines and state-of-the-art methods on ADBench, revealing limitations in current benchmarking practices.

## Executive Summary
This position paper argues that current anomaly detection benchmarking practices are insufficient for driving meaningful research progress. The authors demonstrate that simple methods can perform competitively with complex state-of-the-art algorithms when evaluated using traditional aggregate metrics across heterogeneous datasets. They identify three key areas for improvement: developing scenario-specific benchmarking approaches using taxonomies, analyzing anomaly detection pipelines by their individual components (preprocessing, model selection, and ensembles), and evaluating algorithms based on scenario-specific objectives beyond traditional rank-based metrics. The paper calls for more meaningful evaluations that align with practical needs and better encourage research progress.

## Method Summary
The paper's core demonstration involves implementing a simple Quantiles-based anomaly detection method and comparing it against established baselines and state-of-the-art methods on the ADBench benchmark. The Quantiles approach calculates the 5th and 95th percentiles for each feature in the clean training data, then scores test samples based on the fraction of features that fall outside these boundaries. Performance is measured using ROC-AUC per dataset, with average ranks calculated across all datasets. The authors also propose three directions for improvement: (1) scenario-specific benchmarking using taxonomies to categorize datasets by characteristics like anomaly type and data structure, (2) component-wise analysis of anomaly detection pipelines including preprocessing and normalization effects, and (3) scenario-specific evaluation metrics that capture objectives like anomaly diversity and severity.

## Key Results
- A simple Quantiles-based approach achieves statistically similar average ranks to established baselines (DeepSVDD, OCSVM) and state-of-the-art methods (GoAD, DTE) on the ADBench benchmark.
- The Quantiles method cannot detect certain complex anomaly types, such as points in the middle of an annulus, despite competitive aggregate performance.
- Aggregating performance across heterogeneous datasets allows simple heuristics to mask structural failures in detecting complex anomaly types.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating performance across heterogeneous datasets allows simple heuristics to mask structural failures in detecting complex anomaly types.
- **Mechanism:** Statistical averaging via rank-based metrics allows strong performance on "easy" anomalies to compensate for total failure on "hard" anomalies. The Quantiles approach achieves competitive average ranks by solving only the easiest subset of the benchmark, hiding its inability to handle complex geometries.
- **Core assumption:** Benchmark datasets contain a mix of anomaly difficulties, and current aggregate metrics fail to weight "hard" examples sufficiently.
- **Evidence anchors:**
  - [abstract] "demonstrate that a simple Quantiles-based approach performs similarly to established baselines... highlighting the limitations of current evaluation practices."
  - [section 1] "The performance of the Quantiles approach was not statistically different... [but] it cannot detect certain types of anomalies [e.g.,] an anomalous sample in the middle of an annulus."
  - [corpus] Paper 102055 similarly critiques the disconnect between aggregate metrics and actual detection utility.

### Mechanism 2
- **Claim:** Constraining evaluation to specific scenarios reveals that specialized algorithms outperform generalist methods only when data characteristics match the algorithm's inductive bias.
- **Mechanism:** By grouping datasets via a taxonomy (e.g., "local" vs. "global" anomalies), one avoids Simpson's paradox. This forces algorithms to prove utility on specific data structures rather than relying on average performance across unrelated domains.
- **Core assumption:** Anomalies cannot be defined universally; they are context-dependent, making a "one-size-fits-all" optimizer theoretically impossible.
- **Evidence anchors:**
  - [section 2.1] "Treating results from highly dissimilar benchmark datasets equally can produce paradoxical findings... known as Simpson's paradox."
  - [section 2.2] "LOF outperforms other algorithms on local anomalies" despite lower overall ranks.
  - [corpus] Paper 25097 validates domain-specific benchmarking for Synthetic Aperture Radar.

### Mechanism 3
- **Claim:** A significant portion of performance variance in anomaly detection pipelines is attributable to preprocessing choices rather than core model architecture.
- **Mechanism:** Anomaly detection relies heavily on distance or density metrics. Standard preprocessing methods are sensitive to outliers, distorting the feature space and causing sophisticated models to fail regardless of their architectural complexity.
- **Core assumption:** Current benchmarks often standardize preprocessing "boilerplate," obscuring the fact that preprocessing is doing the heavy lifting.
- **Evidence anchors:**
  - [section 3.1] "normalization method used can have a significant impact... [standard methods] are sensitive to large values, which can negatively affect subsequent anomaly detection."
  - [abstract] "analyzing anomaly detection pipelines by their individual components."

## Foundational Learning

- **Concept:** Simpson's Paradox in Benchmarking
  - **Why needed here:** The paper argues that aggregated results hide specific failures. Understanding this paradox is necessary to grasp why a "good" average rank is misleading.
  - **Quick check question:** Can an algorithm rank highest on average across 10 datasets but fail completely on the only 2 datasets relevant to your specific application?

- **Concept:** Taxonomy of Anomalies (Local vs. Global vs. Contextual)
  - **Why needed here:** The core proposal relies on splitting benchmarks by scenario. You must distinguish between a point being globally distant vs. locally dense but contextually wrong.
  - **Quick check question:** In a time-series of server temperatures, is a sudden 10°C spike (Global) treated the same as a 2°C spike during maintenance hours (Contextual)?

- **Concept:** Top-Heavy Evaluation Metrics (e.g., nDCG)
  - **Why needed here:** The paper advocates for objectives beyond ROC-AUC.
  - **Quick check question:** Does your evaluation metric care more about correctly ranking the top 1% of risks (high severity) or the overall ordering of all data points?

## Architecture Onboarding

- **Component map:** Scenario Definition -> Preprocessing -> Model Core -> Evaluation
- **Critical path:** The path from Scenario Definition to Preprocessing. If the scenario is not defined before preprocessing, you will likely default to standard scalers that crush the outlier signals you intend to detect.
- **Design tradeoffs:**
  - Generality vs. Specificity: Using broad benchmarks allows easy comparison but risks the "Quantiles" failure mode. Specific scenarios reduce comparability but increase relevance.
  - End-to-End vs. Modular: End-to-end optimization is easier to code but harder to debug. Modular analysis is higher effort but scientifically rigorous.
- **Failure signatures:**
  - The "Annulus" Failure: High ROC-AUC on benchmarks but failure to detect a point surrounded by a ring of normal data.
  - The "Simpson" Reversal: Algorithm A beats B in aggregate, but B beats A in every specific sub-category.
- **First 3 experiments:**
  1. Implement the 2-line "Quantiles" algorithm and run it against your current SOTA model on your validation set. If SOTA doesn't significantly outperform Quantiles, your benchmark is too easy or aggregated incorrectly.
  2. Retrain the pipeline keeping the model fixed but swapping Min-Max scaling for Robust Scaling. Report performance delta to isolate the gain from preprocessing vs. architecture.
  3. Split your test set into "Local" and "Global" anomaly types and report ranks separately. Verify if your "best" model is actually just winning on one specific type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a unified taxonomy be constructed to categorize anomaly detection scenarios by characteristics like anomaly type, data type, and processing constraints?
- Basis in paper: [explicit] "First, we need to identify anomaly detection scenarios based on a common taxonomy."
- Why unresolved: Existing taxonomies often focus on single aspects rather than the multi-dimensional characteristics necessary for meaningful scenario-specific benchmarking.
- What evidence would resolve it: A consensus framework where algorithms show consistent, distinct performance within specific scenario subsets but vary significantly across them.

### Open Question 2
- Question: To what extent are performance gains in modern anomaly detection attributable to model innovation versus scenario-specific preprocessing techniques?
- Basis in paper: [explicit] The authors note that current approaches "complicate understanding improvements, making it unclear whether enhancements originate from new algorithms or different data preprocessing."
- Why unresolved: The field lacks systematic ablation studies isolating the variance contributed by components like normalization versus the core model architecture.
- What evidence would resolve it: Modular benchmarking studies that independently vary preprocessing and modeling components to quantify their individual contributions.

### Open Question 3
- Question: How can evaluation metrics be redesigned to capture scenario-specific objectives such as anomaly diversity, severity, or robustness beyond standard rank-based scores?
- Basis in paper: [explicit] "We propose studying performance objectives that focus on properties relevant to anomaly detection tasks... We encourage the anomaly detection community to suggest additional evaluation objectives."
- Why unresolved: Standard metrics like ROC-AUC treat all errors equally and fail to reflect domain-specific constraints, such as the need for diverse detections in quality control or severity assessments in autonomous driving.
- What evidence would resolve it: Validation of new metrics in applied domains demonstrating higher correlation with operational utility than traditional rank-based averages.

## Limitations
- The paper is primarily a position paper rather than presenting novel experimental results, though it does include the Quantiles baseline demonstration.
- The proposed taxonomy-based approach requires significant community consensus and standardization to be widely adopted.
- The specific evaluation objectives beyond rank-based metrics are suggested but not fully developed or validated with real-world case studies.

## Confidence
- **High:** The fundamental argument about Simpson's paradox in benchmarking is well-established and mathematically sound.
- **Medium:** The Quantiles baseline demonstration is convincing but limited in scope to one benchmark.
- **Medium:** The component-wise analysis proposal is logical but lacks comprehensive empirical validation across multiple pipeline components.
- **Medium:** The scenario-specific evaluation metrics are conceptually sound but require further development and validation.

## Next Checks
1. Implement the Quantiles baseline on your specific dataset and verify whether it performs competitively with your current approach.
2. Conduct a preprocessing ablation study by comparing standard scaling versus robust scaling while keeping the model fixed.
3. Stratify your test set by anomaly type (local vs. global) and report performance separately to identify if your model has blind spots.