---
ver: rpa2
title: 'JoyAgent-JDGenie: Technical Report on the GAIA'
arxiv_id: '2510.00510'
source_url: https://arxiv.org/abs/2510.00510
tags:
- arxiv
- agents
- agent
- wang
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for building effective
  generalist agents through the integration of heterogeneous agent paradigms, hierarchical
  memory, and a validated tool suite. The system combines Plan-Execute and ReAct agents
  coordinated through posterior voting to balance reliability and adaptability.
---

# JoyAgent-JDGenie: Technical Report on the GAIA

## Quick Facts
- arXiv ID: 2510.00510
- Source URL: https://arxiv.org/abs/2510.00510
- Reference count: 40
- Primary result: 75.2 Pass@1 and 82.4 Pass@3 on GAIA validation; 67.1 Pass@1 on test

## Executive Summary
This paper proposes a unified framework for building effective generalist agents through the integration of heterogeneous agent paradigms, hierarchical memory, and a validated tool suite. The system combines Plan-Execute and ReAct agents coordinated through posterior voting to balance reliability and adaptability. A hierarchical memory system integrates working, semantic, and procedural layers to enable long-horizon continuity and adaptive control. The tool suite focuses on search, code execution, and multimodal parsing with schema-consistent interfaces. Evaluated on the GAIA benchmark, the framework achieves 75.2 Pass@1 and 82.4 Pass@3 on validation, and 67.1 Pass@1 on the test set, surpassing open-source baselines and approaching proprietary system performance.

## Method Summary
The JoyAgent-JDGenie framework employs a "Fusion" architecture combining heterogeneous agent paradigms (Plan-Execute and ReAct) coordinated by a Critic model via posterior voting. The system uses Claude-4-sonnet as the primary model and o4-mini for the Critic. It implements a hierarchical memory system spanning working, semantic, and procedural layers, and a tool suite focused on search (Google/Bing/DuckDuckGo), code execution, and multimodal parsing. The architecture runs agents in parallel, analyzes their execution trajectories, and uses the Critic to select the final answer through voting.

## Key Results
- Achieves 75.2 Pass@1 and 82.4 Pass@3 on GAIA validation set
- Scores 67.1 Pass@1 on GAIA test set, approaching proprietary system performance
- Outperforms open-source baselines with 30-60% gains on Plan-Execute baselines
- Demonstrates the importance of system-level integration for advancing generalist agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining heterogeneous agent paradigms via critic-based voting balances the stability of structured planning with the flexibility of dynamic reasoning.
- Mechanism: The system runs a "Fusion" architecture. A Plan–Execute agent (low variance, reliable for deterministic decomposition) runs alongside ReAct agents (high variance, better for exploratory adaptation). A Critic model reviews the execution trajectories from both and votes on the final answer. This ensemble method counters the individual failure modes of single-paradigm agents (rigidity vs. instability).
- Core assumption: The Critic model possesses sufficient capability to distinguish between high-quality trajectories and error-prone ones, and the error modes of Plan-Execute and ReAct agents are sufficiently uncorrelated.
- Evidence anchors:
  - [section] Section 3.1 describes the "heterogeneous ensemble" and "posterior voting."
  - [section] Table 2 shows the "Fusion" method achieving 75.2 average score, outperforming "Single" (71.5) and "Multiple (3)" (70.3).
  - [corpus] "Aime" validates the critique of rigid "plan-and-execute" frameworks, supporting the need for fusion.
- Break condition: If task complexity is uniformly low (Level 1), the overhead of the multi-agent ensemble may degrade performance relative to a simple, fast ReAct loop.

### Mechanism 2
- Claim: Hierarchical memory maintains long-horizon continuity by separating immediate context (Working) from compressed historical knowledge (Semantic) and control guidelines (Procedural).
- Mechanism: The system prevents context window saturation and "forgetting" by offloading data. Working memory handles the live loop. Semantic memory stores compressed embeddings of past trajectories for retrieval. Procedural memory dynamically adjusts system prompts to guide agent behavior based on accumulated experience.
- Core assumption: Retrieval based on semantic similarity successfully surfaces relevant precedent cases that help resolve the current task state.
- Evidence anchors:
  - [abstract] Mentions "hierarchical memory system spanning working, semantic, and procedural layers."
  - [section] Section 3.2 details the retrieval mechanism mediated by semantic similarity search.
  - [corpus] "Yunque DeepResearch" identifies "escalating contextual noise" as a critical bottleneck in agents, validating the need for structured memory.
- Break condition: If the semantic retrieval mechanism is noisy (returning irrelevant precedents) or procedural prompts are outdated, the injected context will confuse the base model rather than aid it.

### Mechanism 3
- Claim: A constrained, schema-consistent tool suite focused on "essential" capabilities reduces error propagation compared to unconstrained tool expansion.
- Mechanism: Rather than maximizing tool count, the system refines search, code execution, and multimodal parsing. It enforces schema-consistent outputs (structured data) rather than free-form text. This allows downstream reasoning to operate on reliable data structures, reducing parsing failures.
- Core assumption: Structured tool outputs are significantly easier for the reasoning model to consume and verify than unstructured text.
- Evidence anchors:
  - [section] Section 3.3 states the tool suite results in "30–60% gains on Plan–Execute baselines."
  - [section] Section 4.2.1 highlights that specific search engines (Google via SerpAPI) with fine-grained filtering provide essential capabilities others lack.
  - [corpus] "Profile-Aware Maneuvering" (AWorld) emphasizes that "extended contexts and noisy tool outputs can undermine system reliability," supporting the focus on refined tools.
- Break condition: If a task requires a capability outside the "refined" set (e.g., specialized database queries not covered by general search), the agent lacks the necessary interface to solve it.

## Foundational Learning

- Concept: **ReAct vs. Plan–Execute Paradigms**
  - Why needed here: The architecture explicitly relies on the distinct statistical properties of these two patterns to form a robust ensemble. You must understand why one excels at exploration and the other at reliability to debug the voting mechanism.
  - Quick check question: Can you identify a task type where a ReAct agent would likely loop or drift, but a Plan-Execute agent would succeed?

- Concept: **Schema-Consistent Interfaces**
  - Why needed here: The system claims stability through structured tool outputs. Understanding how to define and parse JSON schemas or structured data objects is critical for extending the tool suite without introducing "brittle" text parsing.
  - Quick check question: Why does returning a standardized JSON object from a tool reduce the "error propagation" mentioned in the paper compared to returning a paragraph of text?

- Concept: **Pass@N Metric**
  - Why needed here: The results report Pass@1 and Pass@3. Understanding the probabilistic nature of LLM generation is necessary to interpret why ensemble voting (effectively a form of sampling and selection) improves scores.
  - Quick check question: How does the "Fusion" architecture approximate a Pass@3 evaluation strategy within a single inference flow?

## Architecture Onboarding

- Component map:
  Supervisor Agent -> Single/Worker Agents -> Critic Model -> Communication Hub -> Memory Layers -> Tool Pool

- Critical path:
  Task Input -> **Task Augment** -> Parallel Execution (ReAct Loop + Plan-Exec Supervisor) -> Tool Invocation (Search/Code) -> **Critic Voting** -> Final Answer

- Design tradeoffs:
  - **Latency vs. Accuracy**: The Fusion method runs multiple agents (Single + Multi), significantly increasing inference cost and time for higher Pass@1 scores.
  - **Tool Specificity**: Relying heavily on Google/SerpAPI for fine-grained search filtering creates dependency on a specific third-party API quality (Table 4 shows significant variance between engines).

- Failure signatures:
  - **Cascading Errors**: Multi-agent systems often degrade on simple tasks (Level 1) due to communication overhead or unnecessary complexity ("performance degrades on simple Level problems").
  - **Search Brittleness**: Failure to find specific facts if the search API lacks date/location filtering (Table 4 shows Bing scoring 58.8 vs Google's 75.2).
  - **Context Drift**: In Single Agent mode, lack of a supervisor can lead to loops or deviation on complex tasks.

- First 3 experiments:
  1.  **Ablate the Critic**: Run the validation set with just the ReAct agent vs. just the Plan-Execute agent vs. the Fusion mode to confirm the voting mechanism's contribution to the 75.2 score.
  2.  **Search Engine Swap**: Run identical queries using Google vs. DuckDuckGo vs. Bing to empirically verify the impact of "fine-grained filtering" on task success rates.
  3.  **Memory Injection**: Test a Level 3 task by pre-loading the Semantic memory with a relevant prior trajectory vs. an irrelevant one to observe the impact on procedural adjustments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning or test-time scaling methods outperform static posterior voting for coordinating heterogeneous agent ensembles?
- Basis in paper: [explicit] Conclusion states: "dynamic self-improvement through reinforcement learning and test-time scaling may enable ensembles to evolve coordination strategies beyond static voting mechanisms."
- Why unresolved: The current framework uses fixed posterior voting (3 or 5 models); no adaptive coordination mechanisms were explored.
- What evidence would resolve it: Comparative experiments where ensemble coordination weights or strategies are learned dynamically through RL training or adjusted at inference time.

### Open Question 2
- Question: Why does introducing a browser-use agent cause significant performance deterioration in multi-agent systems?
- Basis in paper: [explicit] Section 4.2.1 states: "After introducing the browser agent, the system performance exhibits significant deterioration" without providing a resolution.
- Why unresolved: The paper documents the degradation but does not diagnose whether it stems from tool conflicts, communication overhead, or agent role interference.
- What evidence would resolve it: Ablation studies isolating browser agent interactions, communication patterns, and error propagation to identify the failure mode.

### Open Question 3
- Question: How can agents autonomously generate and refine their own tools to reduce manual engineering overhead?
- Basis in paper: [explicit] Conclusion identifies "autonomous tool evolution could allow agents to generate and refine their own tools, reducing manual engineering overhead" as a promising direction.
- Why unresolved: The current framework relies on a manually curated tool suite of 17 interpreters and search APIs.
- What evidence would resolve it: Demonstrations of agents successfully synthesizing new tools (e.g., custom parsers or API wrappers) during task execution that improve Pass@1 on held-out tasks.

### Open Question 4
- Question: To what extent does fine-grained search filtering (date, location, category) versus raw result quality explain Google's 16.4-point advantage over Multi-Source search?
- Basis in paper: [inferred] Table 4 shows Google (75.2) substantially outperforms Multi-Source (68.5); the paper hypothesizes fine-grained filtering conditions may be essential but does not validate this.
- Why unresolved: The mechanism for Google's superiority is speculated but not experimentally isolated from result relevance or coverage differences.
- What evidence would resolve it: Controlled ablations where Google's filtering capabilities are disabled or replicated across other search engines to measure the contribution of filtering versus underlying index quality.

## Limitations
- The Critic model voting mechanism lacks algorithmic detail, making it difficult to reproduce the exact performance gains
- Heavy reliance on Google Search with SerpAPI creates third-party dependency that may not generalize across regions
- The system's performance degrades on simple Level 1 tasks due to multi-agent overhead and communication complexity

## Confidence
- **High confidence**: The hierarchical memory architecture and its separation into working, semantic, and procedural layers is well-described and theoretically sound. The tool suite design with schema-consistent interfaces is clearly articulated and validated by performance gains.
- **Medium confidence**: The ensemble voting mechanism likely contributes to performance gains, as evidenced by the Fusion method outperforming single-agent baselines in Table 2. However, the exact contribution and robustness of the Critic model remains unclear without implementation details.
- **Low confidence**: The reproducibility of the exact 75.2 Pass@1 score depends on unknown factors including the critic algorithm, specific memory models, and agent prompts that are not disclosed in the paper.

## Next Checks
1. **Ablate the Critic**: Run the validation set with just the ReAct agent vs. just the Plan-Execute agent vs. the Fusion mode to confirm the voting mechanism's contribution to the 75.2 score.
2. **Search Engine Swap**: Run identical queries using Google vs. DuckDuckGo vs. Bing to empirically verify the impact of "fine-grained filtering" on task success rates.
3. **Memory Injection**: Test a Level 3 task by pre-loading the Semantic memory with a relevant prior trajectory vs. an irrelevant one to observe the impact on procedural adjustments.