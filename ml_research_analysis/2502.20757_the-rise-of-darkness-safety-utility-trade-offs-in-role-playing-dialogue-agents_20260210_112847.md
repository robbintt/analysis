---
ver: rpa2
title: 'The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents'
arxiv_id: '2502.20757'
source_url: https://arxiv.org/abs/2502.20757
tags:
- safety
- utility
- admp
- character
- villain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the safety-utility trade-off in role-playing
  dialogue agents, where character simulation often generates unsafe content. The
  authors propose an Adaptive Dynamic Multi-Preference (ADMP) method that dynamically
  adjusts safety and utility preferences based on risk coupling between villain characters
  and user queries.
---

# The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents

## Quick Facts
- **arXiv ID:** 2502.20757
- **Source URL:** https://arxiv.org/abs/2502.20757
- **Reference count:** 38
- **Key outcome:** ADMP improves safety metrics while maintaining utility, with CMS further enhancing safety with minimal utility degradation

## Executive Summary
This paper addresses the safety-utility trade-off in role-playing dialogue agents, where character simulation often generates unsafe content. The authors propose an Adaptive Dynamic Multi-Preference (ADMP) method that dynamically adjusts safety and utility preferences based on risk coupling between villain characters and user queries. They also introduce Coupling Margin Sampling (CMS) to enhance handling of high-risk scenarios. Experimental results show that ADMP improves safety metrics while maintaining utility, with CMS further enhancing safety with minimal utility degradation.

## Method Summary
The authors propose ADMP, which modifies the generation target to include explicit safety and utility preference tokens before the response. The model learns to predict these preference scores based on character setting and user query, creating a "thinking step" that forces the model to assess risk before generating content. CMS enhances this by sampling high-risk edge cases through comparison against a Typical Interaction Library (TIL) of villain character interactions. The method uses dynamic weights that adjust based on the coupling degree between character and query, allowing post-hoc control over the safety-utility trade-off without retraining.

## Key Results
- ADMP significantly improves safety metrics while maintaining utility in role-playing scenarios
- CMS further enhances safety with minimal utility degradation by targeting high-risk edge cases
- The method demonstrates effective dynamic adjustment of safety-utility preferences based on risk coupling detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly generating preference tokens (Safety vs. Utility) prior to the response text may force the model to internalize the "risk coupling" of a character-query pair before committing to an output.
- **Mechanism:** ADMP modifies the generation target to include preference tokens, creating a conditional probability framework where the model learns to assess risk level and utility requirements before generating dialogue.
- **Core assumption:** The model can learn a reliable mapping from input context to numerical preference scores that accurately reflects the safety/utility trade-off.
- **Evidence anchors:** ADMP loss formulation in section 4.2, abstract description of dynamic preference adjustment
- **Break condition:** If preference scores become decorrelated from actual safety (reward hacking) or the model ignores preference tokens during response generation (attention collapse)

### Mechanism 2
- **Claim:** Targeted sampling of high-risk edge cases (Coupling Margin Sampling) enhances the model's discriminative ability at the safety boundary.
- **Mechanism:** CMS constructs a Typical Interaction Library for villain characters and samples queries with high semantic similarity to this library, forcing the model to train on sparse, high-risk examples.
- **Core assumption:** "Risk coupling" is a detectable semantic property derived from the interaction between character profile and specific query.
- **Evidence anchors:** Abstract description of CMS, section 4.3 explanation of high-risk scenario handling
- **Break condition:** If the TIL fails to capture diverse ways users trigger unsafe responses or if similarity metrics are too crude to distinguish risky from safe interactions

### Mechanism 3
- **Claim:** Separating safety weight optimization from generation allows post-hoc control over safety-utility trade-off without retraining.
- **Mechanism:** ADMP allows adjustment of hyperparameters like rejection sampling threshold and coupling degree weight during inference, shifting model bias toward safety in high-coupling scenarios.
- **Core assumption:** The latent space supports interpolation between safe and useful behaviors based on input signals without catastrophic forgetting.
- **Evidence anchors:** Section 5.7 on parameter k's influence, section 4.3.3 on safety weight selection
- **Break condition:** If increasing safety weights causes refusal loops or if safety/utility features share neural circuits causing interference

## Foundational Learning

- **Concept: Safety-Utility Trade-off (Pareto Frontier)**
  - **Why needed here:** This paper is fundamentally about navigating the trade-off where increasing character fidelity (utility) often degrades safety.
  - **Quick check question:** Can you explain why asking a villain character for "bomb-making instructions" might yield high utility (character consistency) but low safety?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** The paper contrasts ADMP against standard alignment methods like DPO, which typically optimize for a single reward.
  - **Quick check question:** How does standard DPO struggle when trying to optimize for two conflicting rewards (e.g., "be harmful" vs "be helpful") simultaneously?

- **Concept: Semantic Similarity & Embeddings**
  - **Why needed here:** The CMS mechanism relies on computing semantic similarity between user queries and a "Typical Interaction Library" to detect risk.
  - **Quick check question:** Why might simple keyword matching fail to detect the "risk coupling" that semantic similarity captures in this context?

## Architecture Onboarding

- **Component map:** Character Setting ($r$) + User Query ($x$) -> Risk Assessment (CMS/TIL) -> Weight Mapper -> ADMP Model -> Preference Tokens + Response
- **Critical path:**
  1. Data Prep: Construct TIL for villain characters
  2. Sampling: Use CMS to select high-risk queries and assign dynamic preferences
  3. Training: Fine-tune LLM using ADMP loss (predicting preferences + response)
  4. Inference: Model generates preference tokens first, then generates response conditioned on those tokens
- **Design tradeoffs:**
  - Detection Granularity vs. Compute: Comprehensive TIL is compute-intensive but improves risk detection
  - Refusal Rate vs. Utility: Aggressive rejection sampling maximizes safety but may degrade role-playing experience
  - Static vs. Dynamic Alignment: ADMP is more complex than static DPO but offers context-aware safety
- **Failure signatures:**
  - Over-Refusal: Model generates high safety scores for benign queries, ruining role-play
  - Reward Hacking: Model generates "safe" preference tokens but follows with subtle unsafe content
  - TIL Drift: If TIL is not representative of actual user attacks, risk coupling detection fails
- **First 3 experiments:**
  1. Coupling Degree Validation: Visualize distribution of $G(r,x)$ scores for known safe vs. unsafe queries
  2. Ablation on Preference Tokens: Compare ADMP with fixed vs. dynamic preferences
  3. Hyperparameter Sweep ($k$): Plot Safety vs. Utility scores while varying coupling weight $k$

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can risk coupling detection be refined to accurately identify unsafe generation triggers in complex or subtle role-playing contexts?
- **Basis in paper:** Authors state detection of risk couplings is "not always perfect, especially in complex or subtle cases"
- **Why unresolved:** Current TIL-based semantic similarity may fail to capture indirect provocations or nuanced narrative shifts
- **What evidence would resolve it:** Ablation study showing improved detection rates on subtle, multi-turn adversarial dialogues

### Open Question 2
- **Question:** To what extent does the current training data cover the full distribution of human preferences regarding safety in narrative-driven scenarios?
- **Basis in paper:** "Dataset used for training could be more diverse, as it may not fully capture the range of human preferences in narrative-driven scenarios"
- **Why unresolved:** RoleBench may lack breadth of annotated safety-utility trade-offs for model generalization
- **What evidence would resolve it:** Human evaluation comparing model performance on newly curated diverse narrative dataset versus RoleBench

### Open Question 3
- **Question:** Are there specific categories of high-risk scenarios where Coupling Margin Sampling (CMS) fails to enhance safety?
- **Basis in paper:** Authors acknowledge "there are still some high-risk scenarios that might not be fully addressed"
- **Why unresolved:** CMS relies on sampling based on existing coupling degrees, potentially missing novel risk types
- **What evidence would resolve it:** Detailed failure analysis of CMS module on hold-out set of extreme-risk adversarial samples

### Open Question 4
- **Question:** Does ADMP's balance between safety and utility degrade or improve when applied to significantly larger foundation models?
- **Basis in paper:** Experiments limited to LLaMA-3-8B and Mistral-12B; scaling effects untested
- **Why unresolved:** Larger models possess different emergent capabilities that may interact unpredictably with dynamic preference adjustment
- **What evidence would resolve it:** Comparative metrics on safety (SafetyBench) and utility (SocialBench) for ADMP applied to LLaMA-3-70B model

## Limitations
- Character Coverage Gap: Method focuses exclusively on villain characters, limiting generalizability to other character archetypes
- Evaluation Dataset Representativeness: SafeChat2k and DDAC may not capture full spectrum of real-world role-playing scenarios
- Model Generalization: Unclear whether ADMP maintains safety with characters outside training distribution or facing zero-day attack patterns

## Confidence
- **High Confidence:** Core mechanism of using preference tokens to create a "thinking step" is technically sound
- **Medium Confidence:** Effectiveness of CMS depends on quality and comprehensiveness of TIL construction
- **Low Confidence:** Claim that ADMP maintains utility while improving safety needs more scrutiny in real-world scenarios

## Next Checks
1. **Cross-Character Validation:** Test ADMP on non-villain characters to verify risk coupling mechanism generalization and measure over-refusal in challenging but safe topics
2. **Adversarial Stress Testing:** Conduct red-teaming exercises across different coupling weight settings to reveal blind spots or preference token gaming
3. **Long-term Stability Analysis:** Evaluate whether ADMP maintains consistent safety-utility trade-offs over extended conversations with context accumulation