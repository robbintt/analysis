---
ver: rpa2
title: 'Evaluating Binary Decision Biases in Large Language Models: Implications for
  Fair Agent-Based Financial Simulations'
arxiv_id: '2501.16356'
source_url: https://arxiv.org/abs/2501.16356
tags:
- responses
- random
- llms
- reject
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether Large Language Models (LLMs) can
  reliably produce fair binary choices for use in agent-based financial market simulations,
  where true randomness is essential. Using three state-of-the-art GPT models, the
  researchers tested for uniform output distribution and Markovian independence under
  one-shot and few-shot sampling approaches, as well as varying the temperature parameter.
---

# Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations

## Quick Facts
- arXiv ID: 2501.16356
- Source URL: https://arxiv.org/abs/2501.16356
- Reference count: 5
- Three GPT models tested show strong binary decision biases that must be managed for fair agent-based financial simulations

## Executive Summary
This study investigates whether Large Language Models (LLMs) can reliably produce fair binary choices for agent-based financial market simulations, where true randomness is essential. Using three state-of-the-art GPT models, the researchers tested for uniform output distribution and Markovian independence under one-shot and few-shot sampling approaches, as well as varying the temperature parameter. They found that no model simultaneously achieved uniform distribution and Markovian properties, with GPT-4o-Mini-2024-07-18 showing the best performance in one-shot tests while GPT-4-0125-preview and GPT-3.5-turbo-0125 exhibited extreme biases. These findings highlight inherent biases in LLMs that must be carefully managed when integrating them into financial market models.

## Method Summary
The researchers tested three GPT model versions (GPT-4-0125-preview, GPT-4o-Mini-2024-07-18, and GPT-3.5-turbo-0125) using 100 independent API calls per experiment. They evaluated two hypotheses: HP1 (uniform distribution of yes/no responses) and HP2 (Markovian independence of consecutive decisions). Testing was conducted under both one-shot and few-shot sampling approaches, with temperature parameter varied from 0.5 to 2.0. The analysis included χ² goodness-of-fit tests for uniformity and transition matrix analysis for Markovian properties. Common Crawl web data was analyzed to understand potential training data biases.

## Key Results
- No model achieved both uniform distribution and Markovian independence simultaneously
- GPT-4o-Mini-2024-07-18 showed best one-shot uniformity (32-43% yes responses, not statistically different from uniform)
- GPT-4-0125-preview and GPT-3.5-turbo-0125 exhibited extreme biases (98-99% and 87-98% yes responses respectively)
- Few-shot sampling improved distributional outcomes but failed Markovian tests in most cases
- Model version had larger impact than temperature adjustments on binary output distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherit non-uniform token priors from training data, causing systematic deviations from expected 50/50 binary distributions
- Mechanism: The paper analyzed Common Crawl web data and found Yes/No responses are substantially non-uniform (P(Yes) ≈ 10% in natural text, not 50%). Models trained on such corpora internalize these skewed frequencies, producing biased binary outputs even when explicitly prompted for random choices.
- Core assumption: Token-level probability distributions in training data directly influence model output priors for simple binary decisions.
- Evidence anchors:
  - [abstract] "LLMs are founded on human text, and inherit patterns from their training data"
  - [section] Table 6 shows Common Crawl analysis with Yes comprising only 9.7% of Yes/No instances (truncated) and 10% (full sites), χ² tests confirm significant deviation from uniformity
  - [corpus] Limited corpus support for this specific mechanism in financial simulation contexts
- Break condition: If fine-tuning or RLHF explicitly counteracts natural language frequency biases for decision tokens

### Mechanism 2
- Claim: Sampling method (one-shot vs few-shot API queries) fundamentally alters output distribution properties
- Mechanism: One-shot sampling queries model state independently per decision, preserving internal priors. Few-shot sampling requests multiple outputs in a single call, allowing the model to observe its sequential generation and over-compensate for "randomness" through excessive alternation. This improves distributional uniformity through averaging but introduces non-Markovian dependencies.
- Core assumption: LLMs adjust their output distribution when generating sequences vs single responses, treating batch requests as pattern-completion tasks.
- Evidence anchors:
  - [abstract] "repeated independent API calls produce different distributions compared to batch sampling within a single call"
  - [section] Tables 1-4 show one-shot vs few-shot χ² results differ; few-shot yields better uniformity but P(Yes|Yes) drops to 6-36% (excessive alternation)
  - [corpus] Corpus evidence on ABM-specific sampling methods is limited
- Break condition: If API implementation decouples sequential generation within batch requests

### Mechanism 3
- Claim: Temperature parameter adjustments do not reliably correct binary distribution biases in most GPT model versions
- Mechanism: Temperature controls softmax sharpness (T = 1/β in σ(z)i = e^βzi / Σe^βzj), theoretically allowing calibration of output randomness. However, across settings 0.5-2.0, GPT-4-0125-preview and GPT-3.5-turbo maintained 73-100% yes responses. Only GPT-4o-Mini showed non-linear response to temperature, but still failed to achieve both uniformity and Markovian independence simultaneously.
- Core assumption: Temperature affects token sampling but cannot overcome strong learned priors for specific tokens.
- Evidence anchors:
  - [abstract] "We explore the Temperature parameter... Even with optimised temperature settings, no model achieves both uniform distribution and Markovian outputs"
  - [section] Figure 1 and Table 5 show GPT-4o-Mini's non-linear temperature response; other models maintained biases regardless of setting
  - [corpus] Weak corpus support for temperature effects on binary decision bias specifically
- Break condition: If models are explicitly trained/calibrated to produce uniform distributions under specific temperature settings

## Foundational Learning

- Concept: **Markov Property (Memorylessness)**
  - Why needed here: Financial ABMs assume agent decisions are independent of previous decisions. The paper tests Hypothesis 2 (HP2) for P(Yes_n | Yes_{n-1}) = P(Yes_n). Non-Markovian outputs would simulate artificial agent collusion.
  - Quick check question: In a binary sequence, if P(Yes) = 0.5 but P(Yes|previous_Yes) = 0.2, is this Markovian?

- Concept: **Chi-Square (χ²) Goodness of Fit Test**
  - Why needed here: The paper uses χ² tests (df=1, α=0.05) to evaluate uniformity (HP1) and Markovian independence (HP2). Critical value is 3.841. Tables 1-4 report χ² and p-values for interpretation.
  - Quick check question: With 100 trials yielding 43 yes and 57 no, χ² = (43-50)²/50 + (57-50)²/50 = 1.96. Can you reject H₀ at α=0.05?

- Concept: **Negative Recency Bias**
  - Why needed here: The paper compares LLM randomness to human psychological biases. Negative recency is the tendency to switch after runs (humans over-alternate). The paper tests whether LLMs "beat" humans on this metric using sliding window analysis.
  - Quick check question: After observing "HHH" in coin flips, if humans predict "T" with probability >0.5, what bias does this demonstrate?

## Architecture Onboarding

- Component map:
  ```
  Agent-Based Financial Simulation
  ├── LLM Decision Module (per agent)
  │   ├── API Query Interface
  │   │   ├── One-Shot: 1 call → 1 decision (independent sampling)
  │   │   └── Few-Shot: 1 call → n decisions (batch sampling)
  │   ├── Temperature Parameter (0.5-2.0)
  │   └── Prompt Template ("yes or no" | "Answer randomly, yes or no")
  ├── Bias Validation Layer
  │   ├── Uniformity Test: χ² goodness-of-fit, H₀: p_yes = 0.5
  │   └── Markovian Test: χ² on transitions, H₀: P(Yes|Yes) = P(Yes)
  └── Agent Decision Log (for post-hoc analysis)
  ```

- Critical path:
  1. **Model selection**: GPT-4o-Mini-2024-07-18 shows best one-shot uniformity (32-43% yes); GPT-4-0125-preview and GPT-3.5-turbo-0125 are extremely biased (98-99% and 87-98% yes)
  2. **Sampling method**: One-shot for independent agent decisions (accepts distributional bias); Few-shot for better uniformity (accepts sequential dependencies)
  3. **Validation**: Run both HP1 (uniformity) and HP2 (Markovian) tests before production deployment

- Design tradeoffs:
  - **One-shot vs Few-shot**: One-shot preserves decision independence but yields biased distributions; Few-shot approaches uniformity through averaging but introduces non-Markovian dependencies (P(Yes|Yes) = 6-36% in few-shot vs closer to marginal in one-shot)
  - **Model version over temperature**: Model sub-version has larger impact than temperature adjustments. Switching from GPT-4-0125-preview to GPT-4o-Mini improves uniformity; temperature tuning has minimal effect on most models.
  - **Prompt framing**: Adding "randomly" only improved GPT-4o-Mini (χ² dropped from 12.96 to 1.96); other models maintained strong biases regardless.

- Failure signatures:
  - **Extreme yes-bias (>90%)**: Using wrong model version (GPT-4-0125-preview or GPT-3.5-turbo) → switch to GPT-4o-Mini-2024-07-18
  - **Non-Markovian outputs in few-shot**: Expected behavior; models over-alternate (P(Yes|Yes) << P(Yes)) → use one-shot if independence is critical
  - **Temperature adjustments ineffective**: Expected for GPT-4 and GPT-3.5; only GPT-4o-Mini shows non-linear response → prioritize model selection over parameter tuning
  - **Prompt reframing fails**: Expected for most models → only GPT-4o-Mini responds to "randomly" framing

- First 3 experiments:
  1. **Model baseline comparison**: Run one-shot queries (100 independent API calls) with Q2 ("Answer randomly, yes or no") on GPT-4o-Mini-2024-07-18. Compute χ² for uniformity. Expected: ~43% yes, χ² ≈ 2.0, fail to reject H₀ (not statistically different from uniform).
  2. **Markovian independence test**: On the same data, compute transition matrix and test P(Yes|Yes) vs P(Yes). Expected: χ² ≈ 36.2, reject H₀ (non-Markovian).
  3. **Few-shot batch analysis**: Request 100 responses per API call across 10 batches. Compute per-batch uniformity and Markovian tests. Expected: Most batches pass uniformity (~58% Markovian per batch), but full sequence fails Markovian due to within-batch sequential dependencies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training differences cause the extreme performance variations between model sub-versions (e.g., GPT-4-0125-preview's 98-99% yes bias vs. GPT-4o-Mini's 32-43%)?
- Basis in paper: [explicit] The conclusion states the need for "investigating the relationship between model architecture, underlying causes of bias and decision-making capabilities."
- Why unresolved: The study documents substantial variations but does not investigate mechanistic causes—whether stemming from architecture, RLHF, fine-tuning, or other training procedures.
- What evidence would resolve it: Ablation studies across model variants controlling for architecture size, training data composition, and alignment procedures to isolate causal factors.

### Open Question 2
- Question: Do non-OpenAI LLMs exhibit similar binary decision biases, and can any achieve both uniform distribution and Markovian independence simultaneously?
- Basis in paper: [explicit] The conclusion calls for "evaluating performance across non-OpenAI models."
- Why unresolved: Only three OpenAI models were tested; generalisability to other model families (e.g., Claude, LLaMA, Gemini) remains unknown.
- What evidence would resolve it: Replication of the one-shot/few-shot testing protocol across multiple non-OpenAI LLM families using identical prompts and statistical tests.

### Open Question 3
- Question: What methods can effectively mitigate binary decision biases in LLMs for ABM applications without sacrificing Markovian independence?
- Basis in paper: [explicit] The conclusion calls for "exploring methods to mitigate identified decision biases."
- Why unresolved: Temperature adjustment failed to produce both uniformity and Markovian properties; no mitigation strategy was tested beyond prompt framing ("randomly").
- What evidence would resolve it: Systematic evaluation of bias mitigation techniques (e.g., calibrated decoding, post-hoc correction, fine-tuning) measuring both distributional uniformity and sequential independence.

## Limitations
- The study focuses on three GPT model versions, leaving uncertainty about whether other LLM architectures exhibit similar biases
- Common Crawl corpus analysis is limited to a subset of sites and may not fully represent training data distributions
- Temperature parameter tests cover only the range 0.5-2.0, leaving uncertainty about behavior at extreme values

## Confidence
- **High Confidence**: GPT-4-0125-preview and GPT-3.5-turbo-0125 exhibit extreme yes-bias (98-99% and 87-98% yes responses respectively) - supported by consistent results across multiple test conditions and significant χ² values
- **Medium Confidence**: GPT-4o-Mini-2024-07-18 achieves near-uniform distribution in one-shot tests (32-43% yes) - results show statistical non-significance but the exact mechanism for this improvement is not fully explained
- **Medium Confidence**: Few-shot sampling improves distributional uniformity but introduces non-Markovian dependencies - consistent pattern across models but mechanism (over-compensation for randomness) is inferred rather than directly measured
- **Low Confidence**: LLMs have mixed ability to outperform humans in avoiding negative recency bias - this claim relies on comparison to human behavior studies not directly conducted in this paper

## Next Checks
1. **Cross-Architecture Validation**: Test the same binary decision protocols across diverse LLM architectures (BERT, Claude, LLaMA) to determine whether the observed biases are specific to GPT models or represent a broader challenge for transformer-based language models in financial simulations.

2. **Extended Temperature Analysis**: Conduct experiments across the full temperature range (0.0 to 5.0) with fine-grained increments to map the precise relationship between temperature settings and binary output distributions, particularly for GPT-4o-Mini which showed non-linear responses.

3. **Multi-lingual and Cultural Context Testing**: Repeat the binary decision experiments using prompts in multiple languages and cultural contexts to assess whether training data language and cultural biases influence binary decision outcomes, and whether the model selection recommendations hold across linguistic boundaries.