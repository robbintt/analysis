---
ver: rpa2
title: 'OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization
  Problems'
arxiv_id: '2506.10764'
source_url: https://arxiv.org/abs/2506.10764
tags:
- optimization
- solution
- tasks
- arxiv
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "OPT-BENCH introduces a benchmark of 30 optimization problems\u2014\
  20 Kaggle ML tasks and 10 NP-complete challenges\u2014to evaluate LLM agents on\
  \ iterative reasoning and solution refinement. It includes OPT-Agent, a framework\
  \ that simulates human-like problem solving by generating, validating, and iteratively\
  \ improving solutions using historical feedback."
---

# OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems

## Quick Facts
- arXiv ID: 2506.10764
- Source URL: https://arxiv.org/abs/2506.10764
- Reference count: 40
- Primary result: OPT-BENCH introduces 30 optimization problems to evaluate LLM agents on iterative reasoning and solution refinement, showing that historical context significantly enhances performance across both ML and NP tasks.

## Executive Summary
OPT-BENCH presents a comprehensive benchmark of 30 optimization problems—20 Kaggle ML tasks and 10 NP-complete challenges—to evaluate LLM agents on iterative reasoning and solution refinement. The benchmark introduces OPT-Agent, a framework that simulates human-like problem solving by generating, validating, and iteratively improving solutions using historical feedback. Experiments on 9 state-of-the-art LLMs demonstrate that incorporating historical context consistently enhances optimization performance across both ML and NP tasks. The benchmark highlights the importance of iteration count and temperature tuning, with moderate temperatures generally yielding optimal results. Open-source models showed higher error rates and lagged behind proprietary ones on NP tasks, indicating room for improvement. OPT-BENCH provides a robust platform for advancing LLM-driven optimization in real-world challenges.

## Method Summary
OPT-BENCH evaluates LLM agents through iterative optimization on 30 tasks (20 Kaggle ML competitions and 10 NP-complete problems). The OPT-Agent framework operates through three actions: Draft (generate initial solution), Improve (refine using historical context), and Debug (fix errors via feedback). Each task runs for 5, 10, or 20 optimization steps with temperature settings of 0, 0.2, 0.5, and 0.8. Performance is measured using Win Count (comparing with/without history), Buggy Rate (invalid solution proportion), Rank (relative performance), and Improvement Rate (IR = metric_ratio across tasks). Proprietary LLMs are accessed via API while open-source models use LMDeploy. Historical context accumulates trajectory nodes including solutions, metrics, and error information to guide subsequent iterations.

## Key Results
- Incorporating historical context significantly improves optimization performance across both ML and NP tasks, with Win Count ratios consistently favoring history-enabled runs.
- Moderate sampling temperatures (0.2-0.5) generally yield optimal results, balancing exploration and exploitation, though open-source models show different temperature sensitivity.
- Historical feedback is less effective for NP combinatorial problems than ML optimization tasks due to discontinuous reasoning requirements and constraint satisfaction challenges.
- Open-source models demonstrated higher error rates and lagged behind proprietary models on NP tasks, indicating architectural limitations in discrete reasoning.

## Why This Works (Mechanism)

### Mechanism 1
Incorporating historical context (previous solutions, metrics, error analyses) into prompts improves iterative optimization performance compared to context-free baselines. The model receives accumulated trajectory information—valid solutions with their performance metrics, and buggy solutions with diagnostic error messages—which reduces the search space by steering subsequent iterations toward promising regions and away from previously failed approaches. This mechanism assumes LLMs can effectively parse and leverage structured historical feedback to make incremental adjustments rather than generating de novo solutions each iteration.

### Mechanism 2
Moderate sampling temperatures (0.2-0.5) yield better optimization outcomes than deterministic (T=0) or highly stochastic (T=0.8) decoding for most proprietary models. Temperature controls the exploration-exploitation tradeoff. Very low temperatures cause premature convergence to suboptimal solutions; high temperatures introduce excessive randomness that disrupts coherent reasoning from historical feedback. Moderate temperatures allow sufficient exploration while maintaining stability. This assumes the optimal temperature is task-type dependent, with NP tasks benefiting from slightly higher temperatures than ML tasks due to different solution space structures.

### Mechanism 3
Historical feedback is less effective for NP combinatorial problems than ML optimization tasks due to discontinuous reasoning requirements. ML optimization has a continuous loss landscape where incremental parameter adjustments reliably improve metrics. NP problems have discrete solution spaces where valid solutions require satisfying hard constraints; when feedback indicates a constraint violation, models tend to generate entirely new solutions rather than surgical fixes. This assumes LLMs trained primarily on continuous reasoning tasks lack specialized mechanisms for discrete constraint satisfaction reasoning.

## Foundational Learning

- **Iterative Refinement vs. Single-Pass Generation**: Why needed here: OPT-BENCH explicitly evaluates multi-step optimization, contrasting with most LLM benchmarks that measure one-shot accuracy. Understanding this distinction is essential for interpreting Win Count and Improvement Rate metrics. Quick check question: Can you explain why a model might achieve high single-pass accuracy but poor iterative improvement rates?

- **NP-Complete Problems and Combinatorial Search Spaces**: Why needed here: 10 of 30 benchmark tasks are classical NP problems (TSP, graph coloring, knapsack). These have exponentially large discrete solution spaces where gradient-based optimization doesn't apply, requiring fundamentally different reasoning than ML hyperparameter tuning. Quick check question: Why does adding one city to a TSP instance dramatically increase search complexity, but adding one data point to an ML dataset typically doesn't?

- **Temperature as Exploration Control**: Why needed here: The ablation study shows temperature significantly impacts both solution validity (buggy rate) and optimization effectiveness. Without understanding softmax temperature's role in sampling, results across Tables 4-5 may appear contradictory. Quick check question: If a model at T=0.2 produces valid but suboptimal solutions and T=0.8 produces diverse but often invalid solutions, what adjustment strategy would you try?

## Architecture Onboarding

- **Component map**: Task loading -> Initial draft generation -> Validation execution -> Valid: Compute metric, append to history, trigger Improve action / Invalid: Extract error message, append to history, trigger Debug action -> Iterate until step limit -> Return best valid solution and trajectory

- **Critical path**: 1. Task loading (problem description + dataset/instance) 2. Initial draft generation via LLM 3. Validation execution → branches to: Valid: Compute metric, append to history, trigger Improve action / Invalid: Extract error message, append to history, trigger Debug action 4. Iterate until step limit 5. Return best valid solution and trajectory

- **Design tradeoffs**: Context window growth: Each iteration adds ~500-2000 tokens (code + analysis); 20-step runs approach token limits for some models. Assumption: Truncation or summarization strategies needed for longer horizons. Draft vs. Refine initialization: Draft setting (generate from scratch) achieves higher IR but higher buggy rates; Refine setting (modify initial solution) is more stable but may trap in local optima. Temperature per-task vs. per-model: Paper uses fixed temperature across tasks; per-task tuning may improve results but increases evaluation complexity.

- **Failure signatures**: High buggy rate with history: Model ignores historical error messages → check prompt formatting, reduce history length. Metric plateau after N steps: Model cycling through similar solutions → increase temperature, add explicit novelty constraints to prompt. Win Count near 50/50: Historical context not being utilized → verify history injection in prompt, check for context window overflow

- **First 3 experiments**: 1. Baseline replication: Run OPT-Agent on 3 ML + 3 NP tasks with history disabled, then enabled; verify Win Count patterns match Table 2/3 to confirm correct implementation. 2. Temperature sweep: For a single model (e.g., gemini-2.0-flash), test T ∈ {0, 0.2, 0.5, 0.8} on 5 NP tasks; plot buggy rate vs. rank to identify optimal operating point. 3. Context length ablation: Truncate history to most recent 3/5/10 iterations; measure impact on IR to determine effective context window for your target model.

## Open Questions the Paper Calls Out

- How can the context window size be effectively reduced to allow for extended optimization horizons without losing critical historical information? The authors state in the Limitations section that "accumulated code and historical data expand the context window, raising evaluation costs and nearing token limits," and they explicitly seek "methods to reduce the context window size."

- Why is historical information less effective for NP tasks compared to ML tasks, and how can LLMs be improved to perform better incremental refinement on combinatorial problems? Section 3.5.2 asks, "Why Historical Information is Less Effective for NP Tasks Compared to ML?" noting that LLMs often generate "a completely new solution instead of building upon prior feedback" for NP problems.

- Why do certain models (e.g., DeepSeek-R1) show degraded performance with more optimization steps, and how can long-context utilization be stabilized? Section 3.3 notes that for some models, "IR(w,w.o) decreases as the number of steps increases from 10 to 20, suggesting that some LLMs fall short on effectively utilizing long context information."

## Limitations

- Context window constraints limit the number of optimization steps due to linear growth of historical feedback, potentially exceeding model context windows in extended runs.
- NP task generality is limited as the tested problems (TSP, graph coloring, knapsack) represent only a subset of combinatorial optimization, and observed limitations may not generalize to other NP classes.
- Temperature sensitivity variance shows that open-source models exhibit divergent temperature responses compared to proprietary models, requiring per-model calibration not captured in fixed ablation studies.

## Confidence

- **High confidence**: The effectiveness of historical context for ML optimization tasks is well-supported by consistent Win Count improvements and IR > 1.0 across multiple models and step counts (Table 2).
- **Medium confidence**: Temperature effects show clear patterns for proprietary models but exhibit unexplained variance for open-source alternatives, requiring per-model calibration.
- **Low confidence**: The assertion that historical feedback is less effective for NP tasks due to discontinuous reasoning needs stronger empirical support, as the paper provides qualitative explanations but limited quantitative comparison.

## Next Checks

1. **Context truncation study**: Systematically evaluate IR at 20 steps with history truncated to last 3, 5, and 10 iterations to identify optimal context window size and verify if performance degradation is linear or threshold-based.

2. **Incremental vs. complete solution analysis**: For NP tasks, instrument OPT-Agent to log whether each iteration builds on previous solutions or generates new ones. Compare IR between these strategies to quantify the "start from scratch" problem.

3. **Extended horizon testing**: Run OPT-Agent on 5 ML and 5 NP tasks for 50 iterations with history enabled to assess whether performance plateaus, degrades, or maintains improvements beyond the tested 20-step limit.