---
ver: rpa2
title: 'JavelinGuard: Low-Cost Transformer Architectures for LLM Security'
arxiv_id: '2506.07330'
source_url: https://arxiv.org/abs/2506.07330
tags:
- arxiv
- prompt
- jailbreak
- injection
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces JavelinGuard, a suite of low-cost transformer
  architectures optimized for detecting malicious intent in LLM interactions, specifically
  targeting prompt injection and jailbreak attacks. The authors propose five progressively
  sophisticated architectures: Sharanga (baseline transformer), Mahendra (enhanced
  attention-weighted pooling), Vaishnava and Ashwina (hybrid neural-forest and neural-XGBoost
  architectures), and Raudra (advanced multi-task framework).'
---

# JavelinGuard: Low-Cost Transformer Architectures for LLM Security

## Quick Facts
- **arXiv ID**: 2506.07330
- **Source URL**: https://arxiv.org/abs/2506.07330
- **Reference count**: 20
- **One-line primary result**: Proposes five progressively sophisticated transformer architectures for malicious LLM prompt detection, achieving 25-40× faster inference than GPT-4o while maintaining ~92.8% accuracy.

## Executive Summary
This paper introduces JavelinGuard, a suite of low-cost transformer architectures optimized for detecting malicious intent in LLM interactions, specifically targeting prompt injection and jailbreak attacks. The authors propose five progressively sophisticated architectures: Sharanga (baseline transformer), Mahendra (enhanced attention-weighted pooling), Vaishnava and Ashwina (hybrid neural-forest and neural-XGBoost architectures), and Raudra (advanced multi-task framework). These models, built on modern compact BERT variants like ModernBERT, achieve rapid inference speeds even on CPU hardware while maintaining high accuracy. Across nine diverse adversarial benchmarks, including the newly introduced JavelinBench, the models demonstrate superior cost-performance trade-offs compared to existing open-source guardrail models and large LLMs.

## Method Summary
JavelinGuard presents five transformer-based architectures built on ModernBERT/NeoBERT/EuroBERT backbones for binary/multi-label classification of malicious LLM prompts. The architectures range from a simple mean-pooled encoder (Sharanga) to a sophisticated multi-task model with attention-weighted pooling and focal loss (Raudra). Training uses a corpus of 120,021 samples from various open-source datasets plus synthetic data, with focal loss to handle class imbalance. The models are evaluated across nine benchmarks including NotInject, BIPIA, Garak, and the newly introduced JavelinBench.

## Key Results
- Raudra achieves the highest average accuracy (~92.8%) with near-perfect F1 scores on fully malicious datasets
- JavelinGuard models are 25-40× faster than GPT-4o for inference, achieving sub-40ms latency on CPU hardware
- Attention-weighted pooling in Mahendra and Raudra improves signal-to-noise ratio for adversarial detection compared to global mean pooling
- The multi-task design in Raudra consistently yields the most robust performance with low false-positive rates on benign inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Task-specific attention pooling improves the signal-to-noise ratio for adversarial detection compared to global mean pooling.
- **Mechanism**: Mahendra and Raudra use the [CLS] token as a query against the entire sequence output. This calculates attention scores for every token, creating a weighted sum that theoretically prioritizes tokens indicative of adversarial intent over irrelevant context, rather than averaging all tokens equally.
- **Core assumption**: Adversarial prompts contain specific token patterns or semantic structures that are distinguishable from benign text when weighted appropriately by attention.
- **Evidence anchors**: [Section 3.2] claims attention-weighted pooling "spotlights tokens relevant to adversarial manipulations," and the abstract mentions it as a key differentiator for Mahendra.

### Mechanism 2
- **Claim**: Decoupling feature extraction from classification via Hybrid Neural-Tree architectures preserves interpretability while maintaining low inference latency.
- **Mechanism**: Vaishnava and Ashwina freeze the transformer weights after an initial fine-tuning phase and feed the resulting [CLS] embeddings into a Random Forest or XGBoost classifier. This treats the transformer strictly as a feature extractor, allowing the tree-based model to learn hard decision boundaries on top of semantic embeddings.
- **Core assumption**: The semantic embeddings from the frozen transformer provide a sufficiently linearly separable feature space for the tree-based classifier to distinguish malicious intent without further weight updates.
- **Evidence anchors**: [Section 3.3] notes Vaishnava blends "neural encoder's semantic depth with the interpretability of a Random Forest," while [Section 3.4] explains Ashwina leverages XGBoost to "handle more complex feature interactions."

### Mechanism 3
- **Claim**: Multi-task specialized loss functions reduce the conflation of "toxic content" with "jailbreak attempts."
- **Mechanism**: Raudra implements separate attention-weighting schemes and classification heads for different labels (e.g., jailbreak vs. prompt_injection). By applying focal loss with per-task weighting, the model penalizes confusion between these distinct classes, forcing the encoder to learn distinct representations for "harmful content" vs. "structural manipulation."
- **Core assumption**: Jailbreaks and prompt injections possess distinct latent features that a single binary classifier might otherwise map to a generic "malicious" cluster, causing false positives on benign but aggressive text.
- **Evidence anchors**: [Section 3.5] states Raudra assigns "each label its own specialized token-weighting scheme... reducing interference across different adversarial behaviors."

## Foundational Learning

- **Concept: Encoder-only vs. Decoder-only Architectures**
  - **Why needed here**: The paper explicitly contrasts JavelinGuard (ModernBERT/Encoder) against GPT-4o (Decoder). You must understand that encoders see the full context at once (bidirectional), making them faster for classification but incapable of generation.
  - **Quick check question**: Why does the paper claim encoder models are 25–40× faster than GPT-4o for this task? (Answer: No sequential token generation required; single forward pass).

- **Concept: Focal Loss & Class Imbalance**
  - **Why needed here**: Datasets like ToxicChat and NotInject are heavily imbalanced (mostly safe). Standard Cross-Entropy loss would bias toward "safe"; Focal Loss is used to focus learning on the rare "malicious" hard negatives.
  - **Quick check question**: In Section 3.5, what parameter $\gamma$ was selected for Raudra, and what does it do? (Answer: $\gamma=3.0$, it reduces the loss contribution from easy examples, focusing the model on hard misclassified cases).

- **Concept: The "Lost in the Middle" Phenomenon**
  - **Why needed here**: The authors identify this as a key limitation for long context inputs (8192 tokens). Understanding this helps explain why the authors suggest "segmentation" as a mitigation strategy in the results.
  - **Quick check question**: Why does the "Lost in the Middle" problem pose a risk for a classifier with an 8k context window? (Answer: Attention mechanisms may dilute signals located in the middle of a long sequence, potentially missing embedded attacks).

## Architecture Onboarding

- **Component map**: Raw prompt text -> ModernBERT/NeoBERT/EuroBERT backbone -> Embeddings -> (Mean/Attention-Weighted/CLS) Pooling -> Classification Head (Linear/Random Forest/XGBoost/Multi-Task Deep Residual) -> Binary/Multi-label output

- **Critical path**: Start with Sharanga (ModernBERT-large) to establish a baseline F1 and latency. If false positives on benign data are too high, implement Mahendra to test if attention-weighted pooling helps. If you need to explicitly distinguish "jailbreak" from "prompt injection" or handle domain shifts, deploy Raudra.

- **Design tradeoffs**: Raudra offers the highest accuracy (~92.8%) and robustness but has the most complex training (grid search on loss/gamma) and lowest interpretability. Vaishnava/Ashwina offer partial interpretability (feature importance) and potentially lower false positives, but inference can be slower (up to 100ms in Table 5) due to the tree inference step. GPT-4o offers the best zero-shot reasoning but is prohibitively slow (800ms+) for real-time guardrails.

- **Failure signatures**: High FPR on "Academic" Prompts (flags benign prompts containing phrases like "ignore previous requests"), domain shift performance drops on specialized domains not seen in training, latency spikes on CPU if tree depth/tree count is too high.

- **First 3 experiments**: 1) Benchmark Sharanga vs. Raudra on the NotInject series to measure trade-off between speed of mean pooling vs. accuracy of multi-task attention. 2) Evaluate Mahendra vs. GPT-4o on JavelinBench specifically looking for failure cases where the smaller model misses semantic nuances that the LLM catches. 3) Strip the attention pooling from Mahendra to revert to mean pooling and measure the drop in Macro F1 on the BIPIA benchmark to quantify the value of attention mechanisms.

## Open Questions the Paper Calls Out

- Can state-space models (e.g., Mamba) or long-context transformer variants (e.g., LongFormer, Performer) outperform ModernBERT-based architectures on adversarial prompt detection while maintaining low inference latency?
- How can interpretability methods (e.g., layer-wise relevance propagation) be effectively integrated into deep multi-task architectures like Raudra without sacrificing detection performance?
- Can automated segmentation and context-windowing strategies effectively mitigate the "lost in the middle" problem for very long prompts without introducing segmentation boundary errors?
- What distillation and pruning techniques can reduce Raudra's latency and memory footprint for real-time edge deployment while preserving its robust multi-task detection capabilities?

## Limitations

- **Data Transparency**: The exact composition and distribution of training data across the 120,021 samples remains unspecified, making it impossible to verify claims about adversarial pattern detection.
- **Architectural Opacity**: The attention-weighted pooling mechanism lacks detailed architectural diagrams or pseudocode, obscuring whether it genuinely captures adversarial intent or overfits to keyword patterns.
- **Dataset Validity**: JavelinBench, the primary adversarial benchmark, is not publicly released, preventing independent verification of claimed superiority over GPT-4o.

## Confidence

**High Confidence**: The core claim that encoder-only transformers achieve 25-40× faster inference than decoder-only LLMs for classification tasks is well-established in the literature and directly observable from latency measurements.

**Medium Confidence**: The claim that Raudra's multi-task architecture reduces conflation between jailbreak and prompt injection detection is supported by reported Macro F1 scores (92.8%) but requires independent verification on unseen adversarial samples.

**Low Confidence**: The assertion that JavelinGuard achieves "state-of-the-art" performance is difficult to validate without access to JavelinBench and the ability to run GPT-4o on the same data under identical conditions.

## Next Checks

1. **Benchmark Replication**: Request access to JavelinBench or a standardized subset to independently verify the claimed superiority over GPT-4o, specifically testing whether the performance gap persists when both models are evaluated on identical subsets with consistent preprocessing.

2. **Ablation Study**: Implement a Mahendra variant without attention-weighted pooling to quantify the exact contribution of the attention mechanism, testing across all nine benchmarks to determine whether the complexity is justified.

3. **Domain Transfer Test**: Evaluate Raudra on a domain-shifted dataset (e.g., medical or financial prompts not present in training corpus) to assess the claimed "robustness" to distribution shift, validating whether the multi-task architecture genuinely learns adversarial intent or merely memorizes training patterns.