---
ver: rpa2
title: 'KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?'
arxiv_id: '2507.11408'
source_url: https://arxiv.org/abs/2507.11408
tags:
- reasoning
- answer
- which
- question
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Causal CoT Graphs (CCGraphs) to model fine-grained\
  \ causal dependencies in LLM reasoning traces. By automatically extracting these\
  \ graphs from mathematical reasoning traces across 1671 problems from MATH500, GSM8K,\
  \ and AIME datasets, the authors find that reasoning nodes in the CCGraphs are effective\
  \ mediators between questions and answers\u2014a necessary condition for reasoning."
---

# KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?

## Quick Facts
- arXiv ID: 2507.11408
- Source URL: https://arxiv.org/abs/2507.11408
- Reference count: 40
- Primary result: Reasoning nodes in extracted Causal CoT Graphs are effective mediators between questions and answers in mathematical reasoning.

## Executive Summary
This paper introduces Causal CoT Graphs (CCGraphs) to model fine-grained causal dependencies in LLM reasoning traces. By automatically extracting these graphs from mathematical reasoning traces across 1671 problems from MATH500, GSM8K, and AIME datasets, the authors find that reasoning nodes in the CCGraphs are effective mediators between questions and answers—a necessary condition for reasoning. Empirical analysis with 15 open-weight LLMs (1B-70B parameters) shows that these models emphasize reasoning paths captured by the CCGraphs, indicating internal realization of similar structures. Attention suppression experiments demonstrate that removing reasoning nodes significantly increases answer uncertainty, while reasoning paths consistently receive higher probability mass compared to random paths.

## Method Summary
The authors extract Causal CoT Graphs (CCGraphs) from LLM reasoning traces by parsing mathematical expressions using SymPy and constructing directed acyclic graphs through backward expansion from the answer. They generate reasoning traces using OpenAI o3 with a 5-shot prompt on 1,671 math problems across three datasets. For each trace, they identify reasoning nodes (mathematical expressions) and construct causal graphs by matching answer expressions to context through exact string matches or shared parse tree nodes. They then apply attention suppression interventions to isolate the causal contribution of reasoning nodes, measuring the increase in answer entropy and comparing path probabilities of reasoning paths against random paths of equal length.

## Key Results
- Reasoning nodes act as effective mediators: Attention suppression of all reasoning nodes significantly increases answer uncertainty (p < 10^-12) across all 15 tested models
- LLMs emphasize reasoning paths: Path probability rankings consistently show reasoning paths at the 100th percentile compared to random paths
- Information flows through reasoning tokens: Suppressing attention to reasoning nodes cuts information flow and increases answer entropy, validating the mediation analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning nodes in CCGraphs act as causal mediators between questions and final answers, which constitutes a necessary condition for genuine reasoning.
- **Mechanism:** Mathematical expressions in CoT traces form a directed acyclic graph where intermediate reasoning nodes (ˆr ∈ R) transmit causal influence from question nodes (ˆq ∈ Q) to the answer node (ˆa). When attention to reasoning tokens is suppressed, information flow is cut off, increasing answer entropy.
- **Core assumption:** Attention suppression models the counterfactual where reasoning tokens are absent without inducing out-of-distribution behavior (per Bogdan et al., 2025, cited in Section 4).
- **Evidence anchors:**
  - [abstract] "reasoning nodes in the CCGraphs are effective mediators between questions and answers—a necessary condition for reasoning"
  - [section 5.1, Table 2] Attention suppression over all reasoning nodes significantly increases answer uncertainty (p < 10^-12) across all 15 models tested
  - [corpus] Paper 18937 ("Unveiling and Causalizing CoT") provides complementary causal perspective; paper 65813 supports causal chain discovery in reasoning contexts
- **Break condition:** If reasoning nodes had no indirect effect (IE ≈ 0), CoT traces would be "mere decoration" rather than genuine reasoning—this was rejected by experimental evidence.

### Mechanism 2
- **Claim:** LLMs implicitly realize structures similar to CCGraphs, assigning higher probability mass to reasoning paths (R paths) than to random paths through the same trace.
- **Mechanism:** The probability of transitioning through CCGraph-aligned reasoning paths P(R) is computed as the product of token-level conditional probabilities. When ranked against random paths of equal length, reasoning paths consistently rank at the 100th percentile, indicating the model's internal structure aligns with extracted causal graphs.
- **Core assumption:** Comparing path probabilities controls for confounds like path length and token frequency; relative rank is informative even without marginalizing over intermediate tokens.
- **Evidence anchors:**
  - [abstract] "LLMs emphasize the reasoning paths captured by the CCGraphs, indicating that the models internally realize structures similar to our graphs"
  - [section 5.3, Figure 4] "Pronounced spike at the 100th percentile" for all tested LLMs across all three dataset splits
  - [corpus] Limited direct corpus support; related work (paper 72258) addresses implicit CoT but uses different methodology
- **Break condition:** If P(R) were not consistently higher than random paths, the CCGraph would not reflect the model's implicit reasoning structure.

### Mechanism 3
- **Claim:** Information flows through reasoning tokens via attention mechanisms, and suppressing attention to specific nodes isolates their causal contribution.
- **Mechanism:** Attention suppression zeros out attention weights from suppressed tokens across all layers and heads (Equations 2-3), ensuring no information flows from those tokens. This enables controlled, graph-aligned interventions rather than stochastic perturbations.
- **Core assumption:** The intervention does not problematically induce out-of-distribution behavior (asserted by Bogdan et al., 2025).
- **Evidence anchors:**
  - [section 4] Formal definition of modified attention: A^(ϕ)_i excludes contributions from tokens in X_suppressed
  - [section 5.2] Path suppression experiments show significant effect on answer entropy with high Kolmogorov distance (D_KS) values
  - [corpus] Paper 42413 uses causal prompting interventions for implicit analysis; paper 40141 provides framework for CoT-based reasoning enhancement
- **Break condition:** If attention suppression caused out-of-distribution behavior or didn't effectively cut information flow, causal attribution would be invalid.

## Foundational Learning

- **Causal Mediation Analysis**
  - Why needed here: The paper frames reasoning as a mediation problem—determining whether reasoning nodes transmit causal influence from question to answer
  - Quick check question: If I intervene on a mediator variable and the outcome changes, what does that tell me about the mediator's role?

- **Directed Acyclic Graphs (DAGs) in Causal Inference**
  - Why needed here: CCGraphs are DAGs where nodes represent mathematical expressions and edges represent causal dependencies; understanding DAG properties is essential for interpreting the extraction algorithm
  - Quick check question: Why must a causal graph be acyclic, and how does the algorithm enforce this property?

- **Attention Mechanisms and Information Flow**
  - Why needed here: The intervention methodology relies on suppressing attention to specific tokens; understanding attention is necessary to interpret the causal intervention results
  - Quick check question: When attention to a token is suppressed, can information from that token still influence later predictions through any pathway?

## Architecture Onboarding

- **Component map:** Question Q → o3 CoT generation → expression parsing → CCGraph construction → R-path selection → attention suppression → entropy/probability analysis

- **Critical path:** Question → o3 CoT generation → expression parsing → CCGraph construction → R-path selection → attention suppression → entropy/probability analysis

- **Design tradeoffs:**
  - Scalability vs. coverage: Automated extraction handles 1671 problems but fails on geometry/abstract math (requires semantic similarity, not just algebraic parsing)
  - Manual intervention: ~10% of graphs required manual fixes for LaTeX errors or natural language interruptions (88/40K vertices, 71/300K edges)
  - Path selection: Top-k longest paths (k=5 for GSM8K, k=10 for MATH500/AIME) may not capture all relevant reasoning routes

- **Failure signatures:**
  - **Singleton graph:** Only answer node remains after pruning—indicates missing causal links between question and answer
  - **Parsing errors:** Unmatched LaTeX delimiters, natural language interruptions in equations
  - **Low D_KS values:** Intervention shows weak effect (e.g., Llama 3.3 70B on AIME: D_KS = 0.30)—suggests weaker mediation

- **First 3 experiments:**
  1. **Mediation verification:** Run attention suppression on all reasoning nodes for 50 samples; confirm H(P_A) increases significantly (replicate Table 2 for your model)
  2. **Path probability ranking:** For 20 problems, compute rank_M(R) with M=10 random paths; check for 100th percentile spike pattern
  3. **Ablation study:** Compare M(G) vs. M(G^C) interventions (suppressing math tokens vs. non-math tokens) to test relative importance on your dataset domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CCGraph extraction be generalized to geometry or abstract mathematics by incorporating semantic similarity rather than relying solely on algebraic parsing?
- Basis in paper: [Explicit] The authors state in the Limitations: "Our approach to extracting CCGraphs is not directly applicable to some reasoning problems, such as geometry... future work geared toward developing techniques to capture semantic similarity... would be compelling."
- Why unresolved: The current algorithm (Algorithm 1) depends on symbolic parsing (SymPy) to match expressions, which fails when entities are defined by natural language descriptions (e.g., "Let H be a normal subgroup...") or spatial relations.
- What evidence would resolve it: A modified extraction pipeline utilizing LLM-embedded semantic similarity to construct edges for non-algebraic problems, showing that these "semantic" CCGraphs also function as effective mediators in attention suppression experiments.

### Open Question 2
- Question: Does the natural language context (discourse connectives) provide distinct, irreducible causal support for reasoning in complex mathematical problems?
- Basis in paper: [Inferred] In Section 6.1, the authors investigate if "math is all you need" by suppressing non-math tokens ($M(G^C)$). They find that while math suppression hurts simple problems, the distinction is less clear for AIME/MATH500, suggesting the "glue" text may be necessary for complex deductive structures.
- Why unresolved: The intervention results for complex datasets did not show a statistically significant difference between suppressing math vs. suppressing text, leaving the specific role of discourse structure ambiguous.
- What evidence would resolve it: Targeted suppression of specific discourse connectives (e.g., "therefore," "implies") rather than all non-math tokens, which would isolate the causal contribution of linguistic structure to the final answer.

### Open Question 3
- Question: Does Reinforcement Learning with Verifiable Rewards (RLVR) systematically reduce the "intrinsic uncertainty" required for exploring diverse reasoning paths compared to distillation?
- Basis in paper: [Inferred] Section 6.2 contrasts the "exponential" rank distribution of the RLVR-trained Qwen3 32B (over-confident) with the "bell-shaped" distribution of the distilled DeepSeek R1 32B (better exploration), suggesting RLVR may "worsen exploration."
- Why unresolved: The study compares only two specific models; it is unclear if this behavior is a direct result of the RLVR training paradigm or specific to the Qwen3 architecture/data.
- What evidence would resolve it: A controlled ablation study training identical base models with RLVR versus distillation, followed by an analysis of their R-path rank distributions and pass@k scaling performance.

## Limitations
- Graph extraction reliability: SymPy-based parsing fails on ~10% of problems and cannot handle geometry or abstract mathematics requiring semantic similarity
- Dataset bias: All reasoning traces come from OpenAI o3, potentially limiting generalizability to other CoT generation methods
- Intervention validity: Claims about attention suppression not inducing out-of-distribution behavior require further validation

## Confidence
- **High Confidence**: The mediation analysis showing reasoning nodes increase answer entropy when suppressed (p < 10^-12 across 15 models) demonstrates robust causal influence
- **Medium Confidence**: The claim that LLMs internally realize CCGraph-like structures through path probability rankings shows consistent 100th percentile performance but relies on relative comparisons
- **Low Confidence**: The generalizability of CCGraph extraction to non-algebraic mathematics and the precise implementation details of the parser remain uncertain

## Next Checks
1. **Parser Robustness Test**: Run CCGraph extraction on 100 held-out problems with varying mathematical complexity (algebra, geometry, calculus). Measure singleton graph rate and manual intervention frequency. Compare against baseline 10% failure rate to identify domain-specific limitations.

2. **Attention Suppression Distribution Analysis**: For 50 samples across 3 model sizes, compute KL divergence between original and suppressed token distributions at intermediate layers. Verify that suppression doesn't induce catastrophic distributional shifts that would invalidate causal attribution.

3. **Cross-Prompt Generalization**: Generate reasoning traces using 3 different prompting strategies (different shot counts, different CoT styles) on the same 100 problems. Extract CCGraphs and repeat mediation analysis. Measure correlation between o3-generated and alternative prompt graphs to assess prompt bias.