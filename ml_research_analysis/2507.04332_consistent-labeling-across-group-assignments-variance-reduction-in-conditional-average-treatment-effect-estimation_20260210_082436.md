---
ver: rpa2
title: 'Consistent Labeling Across Group Assignments: Variance Reduction in Conditional
  Average Treatment Effect Estimation'
arxiv_id: '2507.04332'
source_url: https://arxiv.org/abs/2507.04332
tags:
- claga
- treatment
- cate
- group
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of inconsistent learning behavior
  in CATE estimation algorithms, where the same instance receives different predictions
  depending on its group assignment. To quantify this, the authors introduce the discrepancy
  ratio metric.
---

# Consistent Labeling Across Group Assignments: Variance Reduction in Conditional Average Treatment Effect Estimation

## Quick Facts
- arXiv ID: 2507.04332
- Source URL: https://arxiv.org/abs/2507.04332
- Authors: Yi-Fu Fu; Keng-Te Liao; Shou-De Lin
- Reference count: 13
- Primary result: CLAGA reduces PEHE by up to 46.3% on synthetic data and improves AUUC on real-world datasets

## Executive Summary
This paper addresses inconsistent CATE predictions across group assignments in causal inference, where the same instance receives different predictions depending on whether it's assigned to treatment or control. The authors introduce the discrepancy ratio metric to quantify this inconsistency and propose CLAGA (Consistent Labeling Across Group Assignments) to eliminate it. CLAGA uses K-fold cross-validation to generate consistent out-of-sample predictions as learning targets, which can be applied to any existing CATE algorithm. Experiments show significant performance improvements, particularly for complex models and limited data scenarios.

## Method Summary
CLAGA addresses inconsistent CATE estimation by using K-fold cross-validation to generate consistent out-of-sample predictions as learning targets. The method trains K primary estimators on K-1 folds each, generates out-of-sample predictions for held-out instances, and then trains a secondary regression model on these consistent labels. This eliminates the assignment-dependent variance that plagues standard CATE algorithms. The approach is algorithm-agnostic and can be applied to any existing CATE estimation method.

## Key Results
- CLAGA reduces PEHE by up to 46.3% on synthetic data compared to baseline CATE algorithms
- Improves AUUC (Area Under the Uplift Curve) on real-world datasets
- Particularly effective when applied to complex models prone to overfitting
- Shows consistent improvements across multiple CATE algorithms including R-learner, X-learner, and DR-learner
- Performance benefits emerge when primary estimators are complex enough to capture patterns but not so weak that out-of-sample predictions are unreliable

## Why This Works (Mechanism)

### Mechanism 1: Cross-Validation Label Consistency
Standard CATE algorithms construct learning targets that depend on observed treatment assignment W. When an instance appears in treatment vs. control, it receives different surrogate labels, leading to inconsistent predictions. CLAGA trains K primary estimators on K-1 folds, generating out-of-sample predictions for each held-out instance. These predictions become consistent learning targets for a secondary estimator because they no longer depend on the instance's own W.

### Mechanism 2: Error Decomposition and Variance Isolation
The paper decomposes PEHE into five terms. Three terms—including "squared difference of groupwise means" (SDMG) and "weighted variance across group assignments" (WVG)—depend only on surrogate label construction, not model training. CLAGA targets these specifically by making labels assignment-invariant, setting SDMG to zero and reducing WVG to simple variance.

### Mechanism 3: Secondary Estimator Smoothing
After K-fold predictions generate consistent labels, a secondary regression model learns from the relabeled dataset. This model sees a single label per instance rather than conflicting assignment-dependent targets, reducing its tendency to memorize group-specific patterns. The secondary model can be any regression algorithm—CLAGA is algorithm-agnostic.

## Foundational Learning

- **Potential Outcomes Framework (Rubin, 1974)**: CATE is defined as τ(x) = μ₁(x) − μ₀(x), where μ₁ and μ₀ are potential outcomes under treatment and control. The fundamental problem is that only Y(W) is observed for each unit.
  - Quick check: Can you explain why the treatment effect τ(x) is a deterministic function of x, not a random variable?

- **Precision in Estimation of Heterogeneous Effect (PEHE)**: This is the primary evaluation metric; the error decomposition and CLAGA's theoretical justification are built around minimizing PEHE.
  - Quick check: What does PEHE measure, and why is it impossible to compute directly on real-world data?

- **K-Fold Cross-Validation for Label Generation**: CLAGA's core procedure relies on generating out-of-sample predictions via K-fold CV. Understanding why in-sample predictions would fail here is critical.
  - Quick check: Why must the predictions used as labels come from estimators that did NOT see the target instance during training?

## Architecture Onboarding

- **Component map**: Dataset D = {(Xi, Wi, Yi)} -> K primary estimators -> Out-of-sample predictions -> Relabeled dataset D' -> Secondary estimator -> Final CATE estimator

- **Critical path**: 
  1. Partition D into K folds
  2. For each fold i: train gi on D - Di, predict on Di
  3. Collect all out-of-sample predictions into D'
  4. Train secondary regression model on D'
  5. Deploy secondary model as final CATE estimator

- **Design tradeoffs**:
  - K selection: Higher K = more stable predictions but O(K) compute cost. Paper uses K=10 for small datasets, K=2 for large.
  - Primary estimator complexity: CLAGA helps more with complex models prone to overfitting, but very weak models may produce noisy labels.
  - Secondary model choice: Any regression model works; regularization should balance fitting noisy primary predictions.

- **Failure signatures**:
  - No improvement or degradation: Primary estimators too weak (underfitting), producing unstable labels.
  - High computational cost: K too large for dataset size.
  - Discrepancy ratio remains high: Verify out-of-sample predictions are truly out-of-sample.

- **First 3 experiments**:
  1. Run existing CATE algorithm (e.g., R-learner) on ACIC-2016 without CLAGA. Record PEHE. Apply CLAGA with K=5 and same base algorithm. Compare PEHE reduction.
  2. On synthetic dataset with known ground truth, run CLAGA with K∈{2,5,10,20}. Plot PEHE vs. K to identify optimal fold count.
  3. Fix dataset size, vary primary estimator complexity (e.g., num_leaves in LightGBM). Plot PEHE with and without CLAGA to confirm CLAGA benefits emerge at higher complexity.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on out-of-sample prediction quality: Effectiveness depends on primary estimators producing reasonably unbiased predictions on held-out data.
- Error decomposition specificity: Theoretical justification assumes PEHE as the relevant metric and that variance components dominate performance issues.
- Computational overhead: Requires training K copies of base estimator plus secondary model, creating O(K) multiplicative cost.

## Confidence
- **High confidence**: Mechanism 3 (secondary estimator smoothing) and practical implementation details, supported by empirical results across multiple datasets.
- **Medium confidence**: Mechanism 1 (cross-validation label consistency) and Mechanism 2 (error decomposition), theoretically sound but dependent on assumptions about estimator quality.
- **Low confidence**: Performance under severe data limitations or with extremely weak base estimators, where noisy out-of-sample predictions could dominate.

## Next Checks
1. **Cross-validation stability test**: Run CLAGA with K∈{2,5,10,20} on synthetic dataset with known ground truth. Plot PEHE vs. K to identify optimal fold count and verify performance degrades when K is too small.
2. **Base estimator strength sensitivity**: Systematically vary complexity of primary CATE estimator (e.g., LightGBM num_leaves from 10 to 200) on fixed dataset. Plot PEHE with and without CLAGA to confirm benefits emerge for complex, overfit-prone models.
3. **Calibration analysis**: Beyond PEHE, evaluate calibration of CATE estimates with and without CLAGA on real-world datasets. Compare whether variance reduction translates to better-calibrated treatment effect predictions for decision-making.