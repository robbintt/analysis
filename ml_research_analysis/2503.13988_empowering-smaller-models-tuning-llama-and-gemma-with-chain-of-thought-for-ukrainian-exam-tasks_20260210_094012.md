---
ver: rpa2
title: 'Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for
  Ukrainian Exam Tasks'
arxiv_id: '2503.13988'
source_url: https://arxiv.org/abs/2503.13988
tags:
- tasks
- language
- answer
- solution
- letter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap of compact language models
  on reasoning-intensive tasks in underrepresented languages like Ukrainian. The authors
  fine-tune LLaMA and Gemma models using parameter-efficient techniques combined with
  chain-of-thought prompting and task-specific knowledge generation.
---

# Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks

## Quick Facts
- **arXiv ID**: 2503.13988
- **Source URL**: https://arxiv.org/abs/2503.13988
- **Reference count**: 40
- **Primary result**: Fine-tuning LLaMA and Gemma with chain-of-thought improves Ukrainian exam task performance by up to 17.4% on complex matching tasks and 1.6% overall

## Executive Summary
This paper addresses the performance gap of compact language models on reasoning-intensive tasks in underrepresented languages like Ukrainian. The authors fine-tune LLaMA and Gemma models using parameter-efficient techniques combined with chain-of-thought prompting and task-specific knowledge generation. Their approach improves test scores by up to 17.4% on complex matching tasks and 1.6% overall compared to answer-only tuning, while also enhancing interpretability. The proposed method outperforms larger proprietary models like GPT-4o mini and Mistral Large in Ukrainian exam tasks, demonstrating that efficient fine-tuning of compact models can deliver competitive performance in low-resource settings. The study also evaluates how merging quantized adapters affects generation quality.

## Method Summary
The authors employ parameter-efficient fine-tuning (PEFT) techniques, specifically LoRA, to adapt LLaMA and Gemma models for Ukrainian exam tasks. They implement a joint training approach that combines chain-of-thought reasoning with task-specific knowledge generation. The models are fine-tuned on a curated dataset of Ukrainian exam questions from 2021-2023, with a focus on both answer generation and intermediate reasoning steps. The training incorporates LoRA adapters with rank 16, learning rate of 3e-04, and 4 epochs, while also exploring the effects of 4-bit quantization on model efficiency and performance.

## Key Results
- Chain-of-thought fine-tuning achieved up to 17.4% improvement on complex matching tasks compared to answer-only tuning
- Overall performance increased by 1.6% across all task types compared to answer-only approaches
- The proposed method outperformed GPT-4o mini and Mistral Large on Ukrainian exam tasks despite using smaller models

## Why This Works (Mechanism)
The approach works by decomposing complex reasoning tasks into intermediate steps, allowing smaller models to leverage structured reasoning processes typically associated with larger models. By training on both the final answers and the reasoning pathways, the models develop better problem-solving capabilities while maintaining computational efficiency through parameter-efficient fine-tuning. The combination of chain-of-thought prompting with task-specific knowledge generation enables the models to handle the linguistic and cultural nuances of Ukrainian exam questions while preserving their reasoning capabilities.

## Foundational Learning

### Parameter-Efficient Fine-Tuning (PEFT)
**Why needed**: Full fine-tuning of large language models is computationally expensive and memory-intensive, making it impractical for many applications and resource-constrained environments.
**Quick check**: Verify that LoRA adapters can be merged with base models without significant performance degradation and that they reduce the number of trainable parameters by 90-99%.

### Chain-of-Thought Reasoning
**Why needed**: Complex reasoning tasks require decomposition into intermediate steps, and training models to generate these reasoning traces improves their problem-solving capabilities.
**Quick check**: Ensure that chain-of-thought outputs are logically consistent and lead to correct final answers across diverse problem types.

### Quantization (4-bit)
**Why needed**: Reduces memory footprint and enables deployment on consumer hardware while maintaining acceptable performance levels.
**Quick check**: Confirm that 4-bit quantized models maintain at least 95% of the performance of their full-precision counterparts on representative tasks.

## Architecture Onboarding

### Component Map
Data Pipeline -> LoRA Adapter Training -> Model Merging -> Evaluation Pipeline

### Critical Path
The critical path involves data preprocessing, LoRA adapter training with chain-of-thought and knowledge generation objectives, merging adapters with base models, and evaluation on exam tasks. Each stage must maintain data quality and model stability to ensure final performance gains.

### Design Tradeoffs
The approach balances model size and efficiency against reasoning performance. While 4-bit quantization enables deployment on limited hardware, it may introduce subtle quality degradation. The choice of LoRA rank 16 represents a compromise between parameter efficiency and fine-tuning capacity, though optimal values may vary by task and language.

### Failure Signatures
- Degraded performance on tasks requiring multi-step reasoning
- Loss of linguistic nuance in Ukrainian language processing
- Inconsistent reasoning chains that fail to reach correct conclusions
- Memory issues during adapter merging with quantized weights

### First Experiments
1. Compare performance of answer-only vs chain-of-thought fine-tuning on a small subset of tasks
2. Test different LoRA ranks (8, 16, 32) to identify optimal parameter efficiency-performance balance
3. Evaluate the impact of 4-bit vs 8-bit quantization on both performance and memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does 4-bit quantization introduce subtle degradation in model performance on reasoning-intensive tasks, and if so, how can it be mitigated while preserving efficiency?
- **Basis in paper**: [explicit] The authors state: "the use of 4-bit quantization, while beneficial for efficiency, might also introduce subtle degradation in model performance, which requires further exploration."
- **Why unresolved**: The paper observes that merging 4-bit LoRA adapters with base models causes quality degradation, but the extent and nature of quantization effects on reasoning quality remain unclear.
- **What evidence would resolve it**: Systematic comparison of model performance across different quantization levels (e.g., 2-bit, 4-bit, 8-bit) on the same tasks, with analysis of specific error types introduced by quantization.

### Open Question 2
- **Question**: Can the joint topic and chain-of-thought tuning approach generalize effectively to multimodal reasoning tasks involving visual and textual elements?
- **Basis in paper**: [explicit] The authors identify as a prospect: "exploring multimodal reasoning capabilities" and note that "ZNO-Vision benchmark further extends evaluation to multimodal contexts by incorporating visual elements."
- **Why unresolved**: The current work focuses exclusively on text-based tasks, while many real exam questions include images, diagrams, or other visual components that require integrated reasoning.
- **What evidence would resolve it**: Applying the tuning methodology to multimodal datasets (e.g., ZNO-Vision) and comparing performance against text-only baselines and proprietary multimodal models.

### Open Question 3
- **Question**: What is the optimal hyperparameter configuration (learning rate, LoRA rank, batch size) for chain-of-thought tuning on underrepresented languages, and how does it differ from high-resource language settings?
- **Basis in paper**: [explicit] The authors acknowledge: "limited hyperparameter exploration in the experiments could influence the generalization of the obtained results."
- **Why unresolved**: The paper uses fixed hyperparameters (rank 16, learning rate 3e-04, 4 epochs) without systematic ablation, leaving uncertainty about whether these settings are optimal for Ukrainian or other low-resource languages.
- **What evidence would resolve it**: Comprehensive hyperparameter sweep experiments across multiple low-resource languages, identifying configurations that maximize both efficiency and performance gains.

## Limitations
- Evaluation is constrained to a relatively small set of exam questions (2021-2023 Ukrainian external independent testing data)
- Comparison with proprietary models is limited to GPT-4o mini and Mistral Large without testing against other contemporary models
- The paper does not address potential overfitting to the specific exam format
- Analysis of quantized adapter merging effects on generation quality lacks quantitative metrics and detailed error analysis

## Confidence

**Major Claim Confidence Labels:**
- Chain-of-thought fine-tuning improves reasoning performance in Ukrainian tasks: **High**
- Parameter-efficient methods maintain competitive performance compared to full fine-tuning: **Medium**
- Proposed approach outperforms larger proprietary models in Ukrainian exam tasks: **Medium**
- Quantized adapter merging preserves generation quality: **Low** (insufficient quantitative evidence)

## Next Checks
1. Test model generalization on diverse Ukrainian language tasks beyond standardized exam questions, including different domains and reasoning types
2. Conduct ablation studies to isolate the contributions of chain-of-thought prompting versus task-specific knowledge generation in the fine-tuning process
3. Evaluate performance against a broader range of proprietary and open-source models using standardized benchmarks for Ukrainian language processing