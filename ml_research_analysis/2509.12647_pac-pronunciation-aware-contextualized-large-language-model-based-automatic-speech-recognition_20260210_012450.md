---
ver: rpa2
title: 'PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic
  Speech Recognition'
arxiv_id: '2509.12647'
source_url: https://arxiv.org/abs/2509.12647
tags:
- speech
- recognition
- context
- inproc
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Pronunciation-Aware Contextualized (PAC)
  framework for LLM-based ASR that addresses two key challenges: effective pronunciation
  modeling and robust homophone discrimination. The core idea is a two-stage learning
  paradigm that first employs Pronunciation-Guided Context Learning (PGCL) to interleave
  grapheme and phoneme annotations with distractor words, and then uses Pronunciation-Discriminative
  Reinforcement Learning (PDRL) with perturbed label sampling to enhance homophone
  discrimination.'
---

# PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2509.12647
- Source URL: https://arxiv.org/abs/2509.12647
- Reference count: 0
- Key outcome: 30.2% relative WER reduction on Librispeech compared to pre-trained LLM-based ASR models

## Executive Summary
This paper addresses key challenges in LLM-based ASR: effective pronunciation modeling and robust homophone discrimination. The proposed Pronunciation-Aware Contextualized (PAC) framework employs a two-stage learning paradigm. First, Pronunciation-Guided Context Learning (PGCL) interleaves grapheme and phoneme annotations with distractor words to encourage phonemic attention. Second, Pronunciation-Discriminative Reinforcement Learning (PDRL) with perturbed label sampling enhances homophone discrimination. Experiments show PAC achieves significant WER improvements over strong baselines on both Librispeech and AISHELL-1 datasets.

## Method Summary
PAC proposes a two-stage learning framework for LLM-based ASR. Stage 1 uses Pronunciation-Guided Context Learning (PGCL) with interleaved grapheme-phoneme contexts and homophone distractors to encourage phonemic attention. Stage 2 employs Pronunciation-Discriminative Reinforcement Learning (PDRL) with perturbed label sampling to strengthen homophone discrimination. The model uses a 7B parameter FireRed-LLM with LoRA fine-tuning, and incorporates CTC modules to reduce hallucination. Context is constructed using grapheme-only, grapheme-phoneme, and grapheme-phoneme-distractor variants.

## Key Results
- 30.2% relative WER reduction on Librispeech compared to pre-trained LLM-based ASR models
- 53.8% relative WER reduction on AISHELL-1 compared to pre-trained LLM-based ASR models
- 31.8% relative reduction in biased WER for long-tail words on Librispeech compared to strong baselines

## Why This Works (Mechanism)

### Mechanism 1
Interleaved grapheme-phoneme context with distractor words encourages the model to rely on phonemic cues rather than defaulting to graphemic priors. By presenting phoneme annotations alongside grapheme-only homophone distractors, the model must resolve conflicts using pronunciation information. The distractor creates controlled ambiguity that forces phonemic attention. Core assumption: The model can learn to attend to phonemic tokens when graphemic information alone is insufficient for disambiguation.

### Mechanism 2
Perturbed label sampling in reinforcement learning creates adversarial training scenarios that strengthen homophone discrimination. During PDRL, target keywords are replaced with homophone distractors and context is correspondingly perturbed. The model receives reward signals based on B-WER differences between hypotheses, learning to penalize homophone confusions. Core assumption: Exposure to deliberately perturbed context-label pairs transfers to better discrimination on unperturbed test cases.

### Mechanism 3
Explicit phonemic context shifts attention weights toward pronunciation-relevant tokens during inference. Visualized attention scores suggest that when phoneme annotations are present, the model's final LLM layer assigns higher attention to phonemic cues associated with target keywords, compared to grapheme-only baselines that misidentify homophones. Core assumption: Attention patterns reflect genuine mechanistic use of phonemic information rather than memorization of training examples.

## Foundational Learning

- **Grapheme-to-phoneme (G2P) conversion**: PAC requires converting context words into phoneme sequences for interleaved representation using tools like g2p-en and pypinyin. Quick check: Can you explain how G2P errors would propagate through PGCL training?

- **Minimum Word Error Rate (MWER) training**: PDRL uses MWER-style reinforcement learning with B-WER as the reward signal. Understanding how reward is computed from N-best hypotheses is essential. Quick check: How does the reward assignment differ between hypotheses with B-WER above vs. below the average?

- **Contextual biasing in ASR**: PAC builds on prior contextual ASR methods but adds phonemic modeling. Understanding baseline approaches clarifies what PAC improves upon. Quick check: Why do conventional contextual methods struggle with homophones even when the keyword is in the context?

## Architecture Onboarding

- **Component map**: Audio Encoder -> Adapter -> LLM Backbone (7B FireRed-LLM with LoRA) -> Context Construction Module -> CTC Module -> Transcription

- **Critical path**: Speech X → Audio Encoder → Adapter → LLM input tokens → Context C → Context Construction → Interleaved grapheme-phoneme tokens → LLM processes [speech tokens + instruction + context tokens] → transcription Y. During PDRL: Generate N-best hypotheses → compute B-WER rewards → update model

- **Design tradeoffs**: LoRA fine-tuning vs. full fine-tuning for efficiency; distractor sampling probability (P1=P2=1/3) balances context diversity; N-best size (N=8) affects reward signal granularity vs. computation

- **Failure signatures**: High B-WER on long-tail words indicates poor G2P quality or insufficient distractor diversity; hallucinated keywords suggest CTC filtering threshold needs adjustment; minimal PGCL improvement requires verifying phoneme annotation alignment

- **First 3 experiments**: 1) PGCL ablation on Librispeech test-clean (N=2000) expecting progressive B-WER reduction; 2) Homophone discrimination test with synthetic utterances comparing recognition accuracy with/without C_gp context; 3) Attention visualization replication on held-out examples verifying phonemic context increases attention on target phoneme tokens vs. grapheme-only baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Limited cross-linguistic validation with only English and Mandarin tested
- Mechanism attribution challenge: cannot definitively separate contributions of PGCL vs PDRL vs combined effects
- Attention visualization interpretability issues - higher attention scores don't prove causal use of phonemic information

## Confidence
- **High confidence** in WER reduction measurements and relative comparisons between PAC and baseline models
- **Medium confidence** in the core mechanism of PGCL (interleaved grapheme-phoneme context with distractors)
- **Low confidence** in the claim that PDRL's perturbed label sampling specifically improves homophone discrimination

## Next Checks
1. **Cross-linguistic generalization test**: Evaluate PAC on a third language with significantly different phonemic inventory (e.g., Finnish or Arabic) to measure whether the same PGCL+PDRL approach maintains performance

2. **Mechanism isolation experiment**: Train three variants on Librispeech: (a) PGCL without distractors, (b) PGCL with distractors but no PDRL, (c) PDRL without PGCL context. Compare WER and B-WER on homophone-rich test sets

3. **Adversarial robustness evaluation**: Construct test sets where G2P tools fail (proper nouns, loanwords), homophone contexts are ambiguous, or speech is corrupted with real-world noise. Measure whether PAC's gains persist under these conditions