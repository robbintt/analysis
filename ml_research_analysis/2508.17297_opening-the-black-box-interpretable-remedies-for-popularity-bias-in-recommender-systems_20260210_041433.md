---
ver: rpa2
title: 'Opening the Black Box: Interpretable Remedies for Popularity Bias in Recommender
  Systems'
arxiv_id: '2508.17297'
source_url: https://arxiv.org/abs/2508.17297
tags:
- bias
- popularity
- items
- neurons
- popsteer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses popularity bias in recommender systems, where
  popular items are disproportionately recommended while less popular items are underrepresented.
  The authors propose PopSteer, a post-hoc method using Sparse Autoencoders (SAEs)
  to interpret and mitigate this bias at the neuron level.
---

# Opening the Black Box: Interpretable Remedies for Popularity Bias in Recommender Systems

## Quick Facts
- **arXiv ID:** 2508.17297
- **Source URL:** https://arxiv.org/abs/2508.17297
- **Reference count:** 36
- **Primary result:** PopSteer improves fairness metrics (long-tail coverage, Gini Index) with minimal accuracy loss compared to baselines on MovieLens 1M and Last.fm datasets.

## Executive Summary
This paper addresses popularity bias in sequential recommender systems, where popular items dominate recommendations while less popular items are underrepresented. The authors propose PopSteer, a post-hoc method using Sparse Autoencoders (SAEs) to interpret and mitigate this bias at the neuron level. By analyzing activation patterns in synthetic user profiles with clear preferences for popular or unpopular items, PopSteer identifies neurons encoding popularity signals and adjusts them to promote fairer item exposure while maintaining recommendation accuracy. Experiments on MovieLens 1M and Last.fm datasets using the SASRec model demonstrate that PopSteer significantly improves fairness metrics with minimal impact on accuracy and offers interpretability by revealing which neurons contribute to popularity bias.

## Method Summary
PopSteer is a post-hoc method that mitigates popularity bias by identifying and steering neurons encoding popularity signals. The approach uses SAEs to analyze activation patterns in synthetic user profiles: one group with strong preferences for popular items (R_pop) and another for unpopular items (R_unpop). For each neuron, Cohen's d measures the effect size of these activation differences, with larger values indicating stronger encoding of popularity bias. Neurons with the largest |d| values are selected and adjusted through steering to reduce their contribution to popularity bias while preserving recommendation accuracy. The method is evaluated on MovieLens 1M and Last.fm datasets using the SASRec model, comparing against baselines like IPR, FA*IR, PCT, and P-MMF.

## Key Results
- PopSteer achieves significantly better fairness-accuracy trade-offs than baselines on both datasets
- Improves long-tail coverage and Gini Index while maintaining nDCG performance
- Provides interpretability by revealing specific neurons contributing to popularity bias
- Allows fine-grained control over the fairness-accuracy balance through parameters Î± and N

## Why This Works (Mechanism)
PopSteer works by leveraging SAEs to interpret the internal representations of sequential recommenders. SAEs decompose user embeddings into interpretable feature directions, allowing researchers to identify which neurons encode popularity signals by comparing activation patterns in synthetic profiles with clear popularity preferences. The method uses Cohen's d to quantify how much each neuron's activation differs between popular and unpopular item interactions, identifying the most significant bias-encoding neurons. By adjusting these neurons' contributions through steering, PopSteer reduces popularity bias while preserving the model's ability to make accurate recommendations based on other user preferences.

## Foundational Learning
**Sparse Autoencoders (SAEs):** Neural networks that learn to compress and reconstruct input data with sparsity constraints, enabling interpretable feature decomposition. *Why needed:* SAEs reveal which neurons encode specific signals (like popularity) by analyzing activation patterns. *Quick check:* Verify that SAE reconstruction loss converges while maintaining active neuron fraction.

**Cohen's d Effect Size:** Statistical measure quantifying the difference between two groups' distributions. *Why needed:* Measures how strongly each neuron's activation differs between popular and unpopular item interactions. *Quick check:* Confirm |d| values are computed correctly per neuron across synthetic profile groups.

**Neuron Steering:** Technique to adjust neuron contributions by modifying their activation patterns. *Why needed:* Reduces the influence of bias-encoding neurons while preserving other recommendation signals. *Quick check:* Verify that steering equations correctly scale neuron activations based on their standard deviation.

## Architecture Onboarding

**Component Map:** MovieLens/Last.fm datasets -> 5-core filtering -> Chronological split -> SASRec pre-training -> SAE training -> Synthetic profile generation -> Neuron identification (Cohen's d) -> Neuron steering -> Evaluation

**Critical Path:** The SAE analysis of activation patterns in synthetic profiles is the critical path, as it enables identification of bias-encoding neurons. Without accurate neuron identification, the steering component cannot effectively mitigate popularity bias.

**Design Tradeoffs:** The method trades some recommendation accuracy for improved fairness, but PopSteer minimizes this tradeoff compared to baselines. The choice of synthetic profile generation (top/bottom 20% of items) affects which neurons are identified as bias-encoding.

**Failure Signatures:** Dead neurons in SAE (low active neuron fraction), accuracy collapse after steering (sharp nDCG drop), synthetic profiles that don't isolate popularity signal (overlapping activation distributions).

**First Experiments:**
1. Preprocess datasets with 5-core filtering and chronological split; train baseline SASRec to reported nDCG@10
2. Train SAE on user embeddings with Top-K sparsity; verify stable reconstruction and active neuron fraction
3. Generate R_pop/R_unpop synthetic profiles and compute Cohen's d per neuron; visualize activation distributions

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Does PopSteer generalize to recommendation architectures beyond attention-based sequential models like SASRec?
**Basis in paper:** Experiments exclusively use SASRec as the backbone model, leaving untested whether the neuron-level bias identification and steering approach works with collaborative filtering, graph neural networks, or other architectures.
**Why unresolved:** Different model architectures have distinct internal representations; neurons in attention layers may encode popularity differently than embeddings in matrix factorization or GNN message-passing layers.
**What evidence would resolve it:** Experiments applying PopSteer to diverse architectures (e.g., NCF, LightGCN, GRU4Rec) comparing fairness-accuracy trade-offs and neuron interpretability.

### Open Question 2
**Question:** Can the neuron-level intervention framework be extended to mitigate other bias types (e.g., demographic, position, or selection bias)?
**Basis in paper:** The paper draws inspiration from SAE-based bias mitigation in language models and notes SAEs can uncover "meaningful features," but only validates the approach for popularity bias.
**Why unresolved:** Different biases may manifest through distinct activation patterns or neuron clusters that may not be as cleanly separable using Cohen's d on synthetic profiles.
**What evidence would resolve it:** Studies generating synthetic profiles targeting other bias dimensions and measuring whether identified neurons correspond to those biases.

### Open Question 3
**Question:** How robust is the synthetic user profile generation method to variations in how "popular" and "unpopular" item sets are defined?
**Basis in paper:** The paper defines popular/unpopular items as roughly 20% of interactions, but this threshold is arbitrary and may affect which neurons are identified as bias-encoding.
**Why unresolved:** Different thresholds or definitions (e.g., head/tail percentages, interaction frequency vs. rating counts) could yield different neuron rankings and steering outcomes.
**What evidence would resolve it:** Sensitivity analysis varying the popularity threshold and measuring consistency in identified neurons and resulting fairness metrics.

### Open Question 4
**Question:** What are the long-term dynamics when PopSteer-modified recommendations are fed back into the system?
**Basis in paper:** The evaluation is single-shot; the paper does not address feedback loops where altered recommendations change future user interactions and retraining data.
**Why unresolved:** Mitigating popularity bias in one cycle may create unintended consequences (e.g., new biases, user disengagement) when deployed iteratively.
**What evidence would resolve it:** Simulation studies or longitudinal experiments measuring fairness, accuracy, and user engagement over multiple retraining cycles.

## Limitations
- Missing critical hyperparameters for SASRec (hidden dimension, layers, attention heads) and SAE auxiliary loss formulation
- Neuron selection process for mitigation underspecified (thresholds beyond top-N ranking unclear)
- No statistical significance testing reported for performance comparisons
- Limited to sequential recommendation models, generalizability to other architectures untested

## Confidence
- **High confidence** in the core methodology and general fairness-accuracy trade-off direction
- **Medium confidence** in absolute performance numbers due to unknown baseline details
- **Low confidence** in interpretability claims without complete neuron activation analysis pipeline

## Next Checks
1. **Baseline SASRec reproduction:** Implement exact architecture and train to reported nDCG@10 performance before proceeding to PopSteer
2. **SAE training verification:** Implement auxiliary loss as per Gao et al. and verify reconstruction loss convergence with specified active neuron fraction
3. **Neuron selection validation:** Implement Cohen's d computation and neuron ranking; visually inspect activation distributions for R_pop vs R_unpop to confirm method isolates popularity signals