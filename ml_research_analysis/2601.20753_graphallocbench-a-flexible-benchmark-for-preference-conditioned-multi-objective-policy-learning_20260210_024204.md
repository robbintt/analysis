---
ver: rpa2
title: 'GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective
  Policy Learning'
arxiv_id: '2601.20753'
source_url: https://arxiv.org/abs/2601.20753
tags:
- objective
- problem
- pareto
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphAllocBench introduces a novel, flexible benchmark for Preference-Conditioned
  Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), addressing
  the lack of realistic, scalable testbeds. It features CityPlannerEnv, a graph-based
  resource allocation environment inspired by city management, where agents balance
  competing objectives like congestion, growth, and sustainability by allocating resources
  across demands represented as bipartite graphs.
---

# GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning

## Quick Facts
- **arXiv ID:** 2601.20753
- **Source URL:** https://arxiv.org/abs/2601.20753
- **Reference count:** 21
- **Primary result:** HGNN-based approaches significantly outperform MLP baselines on large-scale graph-structured allocation problems.

## Executive Summary
GraphAllocBench introduces a novel benchmark for Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL). It features CityPlannerEnv, a graph-based resource allocation environment inspired by city management, where agents balance competing objectives like congestion, growth, and sustainability. The benchmark includes 100+ diverse problems with varying objective functions, dependency structures, and preference conditions, along with two novel evaluation metrics: Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS), complementing the standard hypervolume metric. Experiments show that a PCPL approach using Heterogeneous Graph Neural Networks (HGNNs) significantly outperforms Multi-Layer Perceptron (MLP) baselines, particularly on large-scale problems with 100 demands and resources.

## Method Summary
The benchmark uses CityPlannerEnv, a Gymnasium-based environment where agents allocate resources across demands represented as bipartite graphs. The observation space consists of normalized allocation matrices concatenated with preference vectors. Two feature extractors are evaluated: MLP (flattened input, 2×128 units, SiLU activation) and HGNN (heterogeneous graph attention layers with residual connections + global pooling). PPO with preference conditioning is used for training, employing Smooth Tchebycheff Scalarization to convert multi-objective rewards to scalars. Preferences are sampled from flat Dirichlet distribution per rollout. Evaluation uses HV ratio, PNDS (Das-Dennis sampling), and OS (swept preference sampling).

## Key Results
- HGNN+AttentionPool achieves HV 30.2±2.4 vs MLP 5.5±0.9 on Problem 6c (100 nodes each)
- Smooth Tchebycheff Scalarization performs most robustly on highly non-convex Pareto fronts
- MLP achieves higher OS (0.84-0.88) than HGNN+AttentionPool (0.65-0.68) despite lower HV

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference-conditioned HGNNs improve allocation quality on large-scale graph-structured problems compared to MLPs.
- Mechanism: The HGNN feature extractor preserves bipartite resource-demand dependency structure through heterogeneous graph attention layers, followed by preference-conditioned multi-head attention pooling that aggregates node embeddings while attending to the preference vector.
- Core assumption: Graph topology encodes actionable allocation constraints that are lost when flattening the allocation matrix for MLPs.
- Evidence anchors:
  - [abstract] "HGNN-based approaches significantly outperform standard MLP baselines, particularly on larger graphs with 100 demands and resources"
  - [Section 6.2, Table 2] HGNN+AttentionPool achieves HV 30.2±2.4 vs MLP 5.5±0.9 on Problem 6c (100 nodes each)
  - [corpus] Weak external validation; neighbor papers discuss PCPL generally but not HGNN-specific gains
- Break condition: When dependency graphs are fully-connected with uniform edge weights, the structural advantage of HGNNs diminishes and MLP convergence stability may dominate.

### Mechanism 2
- Claim: Smooth Tchebycheff Scalarization enables approximation of non-convex Pareto fronts where weighted-sum scalarization fails.
- Mechanism: The scalarization uses a moving ideal point estimate and smoothness hyperparameter to handle discontinuities, computing distance from the ideal point along each objective dimension rather than linearly combining them.
- Core assumption: A reasonably accurate moving ideal point can be maintained during training despite exploration variance.
- Evidence anchors:
  - [Section C.3] "Smooth Tchebycheff Scalarization performed the most robustly in terms of hypervolume achieved when the agent encountered highly non-convex Pareto fronts"
  - [Section 5.1, Problems 1c, 3b] Non-convex and discontinuous fronts show degraded but non-zero HV, indicating partial approximation
  - [corpus] arXiv:2511.16476 examines scalarization limitations in MORL, suggesting scalarization struggles persist in complex environments
- Break condition: When the Pareto front has sharp discontinuities with large jumps between adjacent discrete points, preference interpolation fails regardless of scalarization choice.

### Mechanism 3
- Claim: Ordering Score captures preference-alignment that hypervolume alone misses.
- Mechanism: OS computes Spearman rank correlation between preference weights and resulting objective values across swept preference samples, measuring whether higher preference on objective i yields higher J_i regardless of magnitude.
- Core assumption: Monotonically increasing preference-response is sufficient to indicate preference-conditioning success, even if absolute magnitudes are suboptimal.
- Evidence anchors:
  - [Section 4.3] "ordering score becomes an important way to evaluate how well an agent follows the preference it is given"
  - [Section 6.2, Table 2] MLP achieves higher OS (0.84-0.88) than HGNN+AttentionPool (0.65-0.68) despite lower HV, suggesting MLP finds stable local solutions
  - [corpus] No direct external validation of OS metric; this appears novel to this paper
- Break condition: When multiple objectives share identical production dependencies (coupled rewards), OS may show spurious alignment without true preference-conditioning.

## Foundational Learning

- Concept: Pareto optimality and domination
  - Why needed here: PCPL aims to approximate the Pareto front; understanding that solution x_1 dominates x_2 if all objectives are ≥ and at least one is > is essential for interpreting PNDS and hypervolume metrics.
  - Quick check question: If solution A has objectives [5, 3] and solution B has [4, 4], does A dominate B?

- Concept: Scalarization for multi-objective RL
  - Why needed here: The paper uses Smooth Tchebycheff to convert vector rewards to scalars for PPO; understanding why weighted-sum fails for non-convex fronts is critical.
  - Quick check question: Why can't a weighted sum of objectives reach a concave region of a Pareto front?

- Concept: Bipartite graph message passing
  - Why needed here: The CityPlannerEnv represents resource-demand dependencies as a bipartite graph; HGNN layers propagate information between resource and demand node types.
  - Quick check question: In a bipartite graph with resources R and demands D, which node types exchange messages in a single HGNN layer?

## Architecture Onboarding

- Component map:
  CityPlannerEnv (Gymnasium) -> Feature extractor (MLP/HGNN) -> Preference conditioning (concatenation + attention pooling) -> PPO policy/value heads -> Multi-objective rewards

- Critical path:
  1. Define environment config (|R|, |D|, dependencies, objective functions, horizon T)
  2. Choose feature extractor based on graph complexity (MLP for |nodes| < 20, HGNN for larger)
  3. Set scalarization (Smooth Tchebycheff with tuned smoothness parameter)
  4. Train with preferences sampled from flat Dirichlet per rollout
  5. Evaluate with evenly-spaced Das-Dennis preferences for HV/PNDS, swept preferences for OS

- Design tradeoffs:
  - MLP vs HGNN: MLP converges faster with better OS on small graphs; HGNN achieves 3-5× higher HV on 100-node graphs but with lower OS (may find global solutions that don't follow preference order locally)
  - Attention vs MeanMax pooling: Attention pooling improves HV but adds parameters; MeanMax is simpler but dilutes node-specific information
  - Moving ideal point normalization: Handles unknown ideal points but introduces training instability when new optima are discovered

- Failure signatures:
  - High HV + Low PNDS: Agent found some good solutions but many are dominated; check for mode collapse
  - High OS + Low HV: Agent follows preference order but solutions are far from Pareto front; likely stuck in local optima
  - High variance across seeds on Problems 1a-c: Sharp objective changes cause unstable gradient estimates
  - Low PNDS with sparse rewards (Problem 1c): Agent creates insufficient production, wasting resources without crossing reward thresholds

- First 3 experiments:
  1. Reproduce baseline (Problem 0) with MLP and compare HV ratio to reported ~1.0; verify your training pipeline matches the paper's setup
  2. Test MLP vs HGNN on Problem 6a (100 nodes) with identical hyperparameters; confirm HGNN achieves ≥2× HV improvement
  3. Ablate preference conditioning by removing concatenation from HGNN pooling; expect OS to drop significantly while HV may remain similar

## Open Questions the Paper Calls Out
None

## Limitations
- HGNN advantage is only demonstrated on synthetic problems; real-world applicability remains untested
- Smooth Tchebycheff performance on discontinuous Pareto fronts shows "degraded but non-zero HV" without quantifying approximation error bounds
- OS metric lacks external validation despite being presented as a key evaluation tool
- PPO hyperparameters and exact HGNN architecture details are underspecified, limiting precise reproduction

## Confidence

- High: HGNN improves HV on large-scale problems (100-node graphs) vs MLP; this is empirically demonstrated with multiple seeds
- Medium: Preference conditioning through attention pooling works; though OS suggests MLP may follow preferences better, HV shows HGNN finds better solutions
- Low: Novel evaluation metrics (PNDS, OS) provide meaningful differentiation; these are novel metrics without external validation

## Next Checks

1. Implement ablation study comparing Smooth Tchebycheff vs weighted-sum scalarization on Problems 3b and 5b with sharp Pareto discontinuities
2. Test HGNN robustness by randomly rewiring 10-30% of bipartite edges and measuring HV degradation
3. Verify OS metric by creating a synthetic preference-conditioned policy that deliberately ignores preferences but achieves high HV, confirming OS detects misalignment