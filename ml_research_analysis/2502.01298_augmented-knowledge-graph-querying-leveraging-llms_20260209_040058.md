---
ver: rpa2
title: Augmented Knowledge Graph Querying leveraging LLMs
arxiv_id: '2502.01298'
source_url: https://arxiv.org/abs/2502.01298
tags:
- query
- data
- system
- sparql
- templates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SparqLLM, a framework that enhances Knowledge
  Graph (KG) querying by combining Retrieval-Augmented Generation (RAG) with Large
  Language Models (LLMs). SparqLLM addresses the challenge of complex SPARQL query
  generation for non-expert users by integrating template-based methods with LLMs
  to automatically generate queries from natural language questions.
---

# Augmented Knowledge Graph Querying leveraging LLMs

## Quick Facts
- arXiv ID: 2502.01298
- Source URL: https://arxiv.org/abs/2502.01298
- Reference count: 20
- This paper presents SparqLLM, a framework that enhances Knowledge Graph querying by combining RAG with LLMs to generate SPARQL queries from natural language questions.

## Executive Summary
SparqLLM is a framework that enhances Knowledge Graph (KG) querying by combining Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). The system addresses the challenge of complex SPARQL query generation for non-expert users by integrating template-based methods with LLMs to automatically generate queries from natural language questions. SparqLLM uses an ETL pipeline to construct KGs from raw data, employing the SemIoE ontology for Industry 5.0 applications. The framework demonstrates high performance across components, with template retrieval achieving 81% accuracy, query generation reaching up to 82.7% Harmonic Result Accuracy, and dashboard visualization achieving 100% success in generating appropriate visualizations.

## Method Summary
SparqLLM integrates template-based retrieval with LLMs to generate SPARQL queries from natural language questions. The framework uses an ETL pipeline to construct KGs from raw data, employing the SemIoE ontology for Industry 5.0 applications. Template retrieval matches user questions to 360 pre-defined SPARQL templates using dense embeddings (jina-embeddings-v3). Retrieved templates serve as structural scaffolds for the LLM (LLAMA 3.1 70B) to generate valid SPARQL queries. Generated queries are executed against GraphDB, with iterative refinement loops for error correction. The system also includes automatic dashboard generation using Plotly, where the LLM selects appropriate visualizations based on query results and generates executable Python code.

## Key Results
- Template retrieval achieves 81% accuracy using jina-embeddings-v3 model
- Query generation reaches up to 82.7% Harmonic Result Accuracy with LLAMA 3.1 70B
- Dashboard visualization achieves 100% success rate in generating appropriate visualizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-based retrieval grounds LLM query generation, reducing hallucination by constraining output to valid SPARQL structures.
- Mechanism: User questions are embedded using sentence transformers and matched against a vector database of 360 pre-defined SPARQL templates. Retrieved templates serve as structural scaffolds for the LLM.
- Core assumption: Natural language queries map semantically to a finite set of query patterns; template coverage is sufficient for user intents.
- Evidence anchors:
  - "By integrating template-based methods as retrieved-context for the LLM, SparqLLM enhances query reliability and reduces semantic errors."
  - "The jina-embeddings-v3 model outperformed others, achieving an accuracy of 0.81 and an MCC of 0.8... using two templates achieved optimal performance."
  - Weak direct evidence; neighbor papers discuss KG-LLM synergies broadly but not template-based RAG specifically.
- Break condition: User queries require query structures outside the 360-template coverage (e.g., complex multi-hop joins, subqueries); retrieval accuracy degrades with ambiguous phrasing.

### Mechanism 2
- Claim: Iterative query refinement through execution feedback compensates for LLM reasoning gaps.
- Mechanism: Generated SPARQL is executed against GraphDB. If execution fails, the error message feeds back to the LLM for query correction. This loop continues until success or a threshold is reached.
- Core assumption: Execution errors are recoverable through LLM self-correction; error messages provide sufficient diagnostic signal.
- Evidence anchors:
  - "If the query fails, the system enters a feedback loop where the model analyzes the error, refines the query, and retries."
  - "With templates, ESR reached 100% for Qwen 2 72B; without templates, ESR decreased to 79.2%."
  - "From Symbolic to Neural and Back" discusses KG-enhanced LLMs but does not specifically validate iterative refinement loops.
- Break condition: Errors stem from fundamental semantic mismatches (wrong entity/relationship) rather than syntax; cascading corrections fail to converge.

### Mechanism 3
- Claim: LLM-guided visualization selection with code generation produces reliable dashboards through staged validation.
- Mechanism: Query results are analyzed by the LLM to classify as "plot" or "table." For plots, the LLM generates Plotly Python code with contextual inputs (query structure, data summaries). Execution errors trigger code refinement.
- Core assumption: Data summaries and query structure provide sufficient context for correct visualization type selection; generated code is executable in the target environment.
- Evidence anchors:
  - "The LLM selects a chart type from the predefined list and generates Python code... if errors arise, the system provides feedback to the LLM."
  - "The system generated all selected visualizations without errors, achieving a 100% success rate" (though visualization type selection accuracy was 70%).
  - No direct corpus evidence for LLM-generated visualization code reliability.
- Break condition: Complex data structures exceed predefined chart types; visualization type misclassification leads to inappropriate displays.

## Foundational Learning

- Concept: **SPARQL query structure and execution model**
  - Why needed here: Understanding SELECT, FILTER, GROUP BY patterns is essential to interpret template design and diagnose query failures.
  - Quick check question: Can you trace how a SPARQL SELECT query with a FILTER clause retrieves entities from a triplestore?

- Concept: **Dense retrieval and embedding similarity**
  - Why needed here: Template retrieval relies on cosine/IP similarity between query embeddings and template embeddings in vector space.
  - Quick check question: Given two sentences, how would a sentence embedding model determine semantic similarity?

- Concept: **RAG prompt engineering with structured context**
  - Why needed here: The LLM must synthesize ontology schema, retrieved template, and user question into valid SPARQL—prompt design determines success.
  - Quick check question: How does providing a SPARQL template as context differ from asking an LLM to generate SPARQL from scratch?

## Architecture Onboarding

- Component map:
  - ETL Pipeline -> RML mapping -> RDF triples -> GraphDB (triplestore with SemIoE ontology)
  - SPARQL templates -> jina-embeddings-v3 -> Vector DB (IVF_FLAT index) -> Top-k retrieval
  - LLM (LLAMA 3.1 70B) + template + ontology -> SPARQL -> GraphDB execution -> feedback loop
  - Result classification (plot/table) -> Plotly code generation -> execution -> render

- Critical path:
  1. Template retrieval accuracy directly bounds query generation success (81% retrieval → up to 82.7% HRA)
  2. Query execution feedback loop is the reliability safety net
  3. Visualization depends on successful query execution (67 valid samples from 96 for dashboard evaluation)

- Design tradeoffs:
  - Embedding model size vs. portability: NV-Embed-v2 has higher retrieval scores (62.65) but jina-embeddings-v3 (572M) was selected for balance
  - Template count vs. precision: 2 templates optimal (80% accuracy); more templates dilute relevance
  - LLM size vs. cost: LLAMA 3.1 70B achieved best HRA (82.7%) but requires significant compute vs. 3B/8B variants

- Failure signatures:
  - Template retrieval mismatch: FILTER queries underperform vs. SELECT/GROUP BY (confusion matrix in Fig. 3)
  - Entity complexity: Sensor entity ESR (87.5%) lower than Observation (100%) due to complex SOSA/SSN relationships
  - Visualization type misclassification: 30% of "Table" cases incorrectly predicted as "Plot"

- First 3 experiments:
  1. **Embedding model benchmark**: Compare jina-embeddings-v3, NV-Embed-v2, and smaller models on template retrieval accuracy and MCC using the 100-sample dataset. Verify 2-template retrieval is optimal.
  2. **Query generation ablation**: Run LLAMA 3.1 70B with and without template context on the 96-query dataset. Measure ESR, RCA, RMR, HRA. Confirm template contribution (ESR gap: ~21%).
  3. **Visualization pipeline validation**: Execute the dashboard component on successful query results. Measure type selection accuracy (target: 70%) and code generation success rate (target: 100%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of automatic visualization type selection be improved, particularly for distinguishing tabular representations?
- Basis in paper: Section VI states the decision-making process for visualization selection "could benefit from integrating advanced techniques" and highlights the 30% error rate for "Table" predictions (Section V.C).
- Why unresolved: The current LLM-based selection struggles to differentiate when a "Table" is more appropriate than a "Plot" (70% vs. 83% accuracy).
- What evidence would resolve it: Comparative evaluation showing improved F1-scores for "Table" classification using fine-tuned models or advanced heuristics.

### Open Question 2
- Question: Can the framework effectively extend its dynamic visualization capabilities to complex types like heatmaps, geospatial maps, and network graphs?
- Basis in paper: Section VI explicitly lists "heatmaps, geospatial maps, or network graphs" as future work to accommodate diverse data structures.
- Why unresolved: The current implementation is limited to standard charts (bar, line, scatter) and tables, restricting utility for spatial or relational data.
- What evidence would resolve it: Successful generation and rendering of these complex visualization types from relevant SPARQL query results without errors.

### Open Question 3
- Question: To what extent does the system's performance depend on the pre-defined set of templates, and how does it handle queries requiring novel graph traversals?
- Basis in paper: The system relies on a fixed repository of 360 manually curated templates (Section IV.A), and evaluation was restricted to specific entities (Observation, Sensor, etc.).
- Why unresolved: It is unclear if the RAG mechanism can generalize to queries that fall outside the semantic scope of the existing template bank.
- What evidence would resolve it: Evaluation results on out-of-domain queries where no high-similarity template exists, measuring the "zero-shot" generation capability.

## Limitations

- Framework effectiveness is constrained by completeness and semantic coverage of its 360 SPARQL templates, which may not capture all user query patterns.
- Evaluation relies on a proprietary dataset and ontology (SemIoE), limiting reproducibility and generalizability across domains.
- Visualization component achieves perfect code generation success but only 70% accuracy in chart type selection.

## Confidence

- Template-based retrieval grounding LLM query generation: **High** (strong quantitative evidence: 81% accuracy, 0.8 MCC, supported by iterative refinement success rates)
- Iterative query refinement through execution feedback: **Medium** (empirical success rates demonstrated, but no analysis of convergence behavior or failure cases)
- LLM-guided visualization selection with code generation: **Medium** (100% generation success but only 70% classification accuracy; visualization quality not evaluated)
- Framework usability improvement for non-expert users: **Low** (claimed but not empirically validated through user studies or expert evaluations)

## Next Checks

1. Conduct template coverage analysis: Systematically test the 360 templates against a diverse benchmark of natural language queries to identify gaps in query pattern coverage and measure how many real-world queries fall outside the template coverage.

2. Evaluate visualization quality and user satisfaction: Deploy the visualization dashboard to domain experts and collect qualitative feedback on chart appropriateness and usability. Compare LLM-selected visualizations against expert-curated selections across the evaluation dataset.

3. Stress-test the feedback loop: Create SPARQL queries with progressively complex semantic errors (wrong entities, incorrect relationships, impossible constraints) and measure the LLM's ability to correct them through iterative refinement. Identify error types that cannot be resolved through execution feedback.