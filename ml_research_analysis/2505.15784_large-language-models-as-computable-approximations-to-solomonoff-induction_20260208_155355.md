---
ver: rpa2
title: Large Language Models as Computable Approximations to Solomonoff Induction
arxiv_id: '2505.15784'
source_url: https://arxiv.org/abs/2505.15784
tags:
- arxiv
- solomonoff
- zhang
- language
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical connection between large language
  models (LLMs) and Algorithmic Information Theory (AIT), specifically showing that
  LLM training approximates the Solomonoff prior and inference approximates Solomonoff
  induction. The authors prove that minimizing prediction loss during training corresponds
  to searching for shortest programs that generate training data, linking learning
  efficiency to data compressibility.
---

# Large Language Models as Computable Approximations to Solomonoff Induction

## Quick Facts
- arXiv ID: 2505.15784
- Source URL: https://arxiv.org/abs/2505.15784
- Authors: Jun Wan; Lingrui Mei
- Reference count: 31
- Primary result: LLM training approximates Solomonoff prior and inference approximates Solomonoff induction; low-confidence few-shot selection improves accuracy, especially for smaller models

## Executive Summary
This paper establishes a theoretical connection between large language models and Algorithmic Information Theory by showing that LLM training approximates the Solomonoff prior and inference approximates Solomonoff induction. The authors prove that minimizing prediction loss during training corresponds to searching for shortest programs that generate training data, linking learning efficiency to data compressibility. Based on these insights, they introduce a novel few-shot example selection strategy that prioritizes samples where the model exhibits lower predictive confidence, which consistently improves accuracy on text classification benchmarks compared to high-confidence selection.

## Method Summary
The paper proposes a framework linking LLM training to Solomonoff induction, where training loss minimization approximates the Solomonoff prior through program length optimization. The authors derive that next-token prediction implements approximate Solomonoff induction with a scaling factor dependent on context length. Based on this theoretical foundation, they introduce a few-shot example selection strategy that selects samples where the model assigns lower probability to the correct label, implemented by iteratively selecting samples with minimum confidence scores for the ground-truth label token. Experiments use Qwen and Llama models on SMS spam detection, emotion recognition, and AG NEWS categorization, comparing low-confidence versus high-confidence selection strategies.

## Key Results
- LLM training loss minimization corresponds to minimizing program length, approximating the Solomonoff prior
- Next-token prediction implements approximate Solomonoff induction with scaling factor t²/(t+1)²
- Low-confidence few-shot example selection consistently outperforms high-confidence selection on tested benchmarks
- Performance gains are particularly pronounced for smaller model architectures (3B parameters)
- Qwen 2.5 3B: SMS accuracy improves from 76.62% to 90.07% using low-confidence selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM training loss minimization approximates the Solomonoff prior by searching for short programs that generate training data.
- Mechanism: The training process constructs a program f(x,s) = (m(2), n(x)(2), s(2), e(x)(2)) where e(x)(2) is a compressed encoding of the data. Minimizing loss corresponds to minimizing |e(x)(2)|, which increases the approximate prior M(x) = Σ 2^(-ℓ(f(x,s))) toward the true Solomonoff prior M(x).
- Core assumption: Lossless compression via arithmetic coding with the LLM reflects algorithmic compressibility of the data.
- Evidence anchors:
  - [abstract]: "training process computationally approximates Solomonoff prior through loss minimization interpreted as program length optimization"
  - [section 4.1]: "minimizing the binary encoding length |e(x)(2)| is crucial for making M(x) as close as possible to M(x)"
  - [corpus]: Related work "Universal pre-training by iterated random computation" similarly connects sequence models to Solomonoff induction, suggesting convergent theoretical interest but not independent validation.
- Break condition: If loss minimization decouples from compression length (e.g., regularization artifacts, optimization pathologies), the approximation degrades.

### Mechanism 2
- Claim: Next-token prediction approximates Solomonoff induction with a scaling factor dependent on context length.
- Mechanism: The conditional probability M(xt+1|x1:t) ≈ (t²/4(t+1)²) · Pθ(xt+1|x1:t). During normalization, the context-dependent factor cancels, leaving Pθ proportional to the inductive prediction. Longer contexts yield tighter approximation.
- Core assumption: The encoding length ratio |e(x1:t)(2)|² / |e(x1:t+1)(2)|² ≈ 1 for large t, and the encoding cost approximates cumulative negative log-likelihood.
- Evidence anchors:
  - [abstract]: "next-token prediction implements approximate Solomonoff induction"
  - [section 4.2, Theorem 3]: Derives M(xt+1|x1:t) ≈ t²/4(t+1)² · P(xt+1|x1:t)
  - [corpus]: Weak direct evidence. "Solomonoff-Inspired Hypothesis Ranking with LLMs" applies similar principles but does not test the specific scaling relationship.
- Break condition: If token encodings do not satisfy the asymptotic ratio assumption (e.g., non-stationary distributions, vocabulary artifacts), the approximation bound loosens.

### Mechanism 3
- Claim: Selecting few-shot examples where the model assigns lower probability to the correct label accelerates adaptation more than high-confidence examples.
- Mechanism: From the Solomonoff convergence theorem, prediction error is weighted by μ(x1:t). Samples with larger M(0|x1:t) − μ(0|x1:t) contribute more to error reduction. Low-confidence examples indicate larger divergence between model and target, thus higher information value for convergence.
- Core assumption: Multiple computable distributions with similar Kolmogorov complexity can solve the task, and subsets of data originate from different such distributions.
- Evidence anchors:
  - [abstract]: "strategy yields significant performance improvements, particularly for smaller model architectures"
  - [section 5.3, Table 1]: Low-confidence selection outperforms high-confidence across Qwen 3B/7B and Llama 3.2 3B/3.1 8B on SMS, EMOTION, AG NEWS (e.g., Qwen 2.5 3B: 76.62% → 90.07% on SMS).
  - [corpus]: No corpus validation found for this specific low-confidence selection method.
- Break condition: If low-confidence reflects noise or label errors rather than distributional divergence, selection may amplify misleading signal.

## Foundational Learning

- Concept: Solomonoff Prior and Induction
  - Why needed here: Central theoretical object; paper claims LLMs approximate this uncomputable but semi-computable universal predictor.
  - Quick check question: Can you explain why M(x) = Σ_{p:U(p)=x*} 2^(-ℓ(p)) embodies Occam's razor?

- Concept: Prefix Kolmogorov Complexity
  - Why needed here: Grounds the program-length interpretation of compression and defines the invariance property.
  - Quick check question: Why must programs form a prefix code for the Solomonoff prior to be a semi-measure?

- Concept: Elias Gamma Coding
  - Why needed here: Used to encode program components as prefix codes; code length ≈ 2 log₂ n affects the t²/(t+1)² scaling factor.
  - Quick check question: What is the code length for integer n under Elias gamma coding?

## Architecture Onboarding

- Component map:
  - Program construction f(x,s) = (m(2), n(x)(2), s(2), e(x)(2)): model weights, decoding iterations, random seed, compressed data
  - Approximate prior M(x): sum over seeds of 2^(-program length)
  - Inference mapping: Pθ(xt+1|x1:t) → M(xt+1|x1:t) via scaling factor t²/4(t+1)²
  - Few-shot selector: iterates samples, computes confidence = model probability of correct label, selects K lowest per class

- Critical path:
  1. Understand how loss minimization → shorter e(x)(2) → higher M(x) → better approximation
  2. Trace inference derivation from prior ratio to scaled probability
  3. Implement confidence-based selection with temperature=0 for determinism

- Design tradeoffs:
  - Smaller models benefit more from low-confidence selection (paper shows larger gains for 3B vs 7B/8B), but may be more sensitive to noise in low-confidence samples
  - Temperature=0 ensures determinism aligned with Turing machine view but eliminates diversity; may not suit generative tasks beyond classification

- Failure signatures:
  - High-confidence selection outperforms low-confidence: check for label noise, task misalignment with model's pre-training distribution, or insufficient selection pool
  - Scaling factor t²/(t+1)² does not cancel in practice: check vocabulary size effects, non-stationary context

- First 3 experiments:
  1. Reproduce SMS classification with Qwen 2.5 3B, comparing low-confidence vs high-confidence selection; verify temperature=0 determinism
  2. Ablate context length: test whether longer prompts reduce the gap between selection strategies (per convergence theorem prediction)
  3. Inject controlled label noise into selection pool; measure at what noise level low-confidence selection degrades below high-confidence baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the low-confidence few-shot selection strategy generalize to complex generative tasks (e.g., reasoning or code synthesis) where "correctness" is harder to define than in text classification?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion and Limitations that "Further research is needed to ascertain the generalizability of these findings across a broader range of tasks... beyond specific text classification tasks."
- Why unresolved: The experimental validation was restricted to classification benchmarks (SMS, Emotion, AG News) where confidence is simply the softmax probability of the ground-truth label token.
- What evidence would resolve it: Empirical results showing performance improvements when applying the selection strategy to generative benchmarks (e.g., GSM8K or HumanEval) using metrics like pass@k or model-based evaluation.

### Open Question 2
- Question: Why does the performance gain of the low-confidence selection strategy diminish with increasing model scale, and does this imply a theoretical limit to the approximation?
- Basis in paper: [inferred] The authors observe in Section 5.3 that "the magnitude of the performance gain appears to moderate with increasing model scale," suggesting larger models may already possess robust priors that reduce the marginal utility of informative samples.
- Why unresolved: The paper does not provide a theoretical explanation for why the "weakness exposure" benefit decreases as the Solomonoff approximation becomes more precise (i.e., as model size grows).
- What evidence would resolve it: A theoretical analysis or empirical scaling law predicting the intersection point where high-confidence selection becomes equivalent to low-confidence selection.

### Open Question 3
- Question: Can computable proxies for the Kolmogorov complexity of target distributions ($K(\mu)$) be developed to predict the convergence rate of LLMs on specific tasks?
- Basis in paper: [explicit] In Appendix A (Limitations), the authors note that "the precise quantification of factors like the Kolmogorov complexity of target distributions ($K(\mu)$) in real-world LLM scenarios remains a complex endeavor, making direct measurement challenging."
- Why unresolved: The error bounds in the paper rely on $K(\mu)$, which is fundamentally incomputable, leaving a gap between the theoretical convergence guarantees and practical predictability of model performance.
- What evidence would resolve it: The derivation of a practical metric correlated with $K(\mu)$ (e.g., compression ratios or efficient sample complexity) that accurately forecasts the difficulty of a new dataset for a given LLM.

### Open Question 4
- Question: How does the finite context window of current LLMs restrict the theoretical convergence guarantees provided by Solomonoff induction?
- Basis in paper: [inferred] Theorem 3 relies on asymptotic approximations for "large context length $t$," but real LLMs operate with fixed, finite context windows ($t_{max}$), potentially violating the conditions required for the error bound convergence.
- Why unresolved: The paper proves that $M(x_{t+1}|x_{1:t})$ converges to $\mu(x_{t+1}|x_{1:t})$ as $t \to \infty$, but it does not analyze the error behavior or approximation quality when $t$ is truncated by architectural constraints.
- What evidence would resolve it: An analysis of prediction accuracy degradation as sequence length approaches the context limit, specifically comparing the theoretical "ideal" induction against the observed "truncated" induction.

## Limitations
- Theoretical framework relies on strong assumptions about equivalence between loss minimization and compression length
- Empirical validation limited to classification tasks with limited model diversity
- Low-confidence selection assumes prediction uncertainty correlates with information value, which may not hold for noisy data
- Finite context windows in real LLMs violate asymptotic assumptions required for theoretical guarantees
- Incomputability of Kolmogorov complexity limits practical predictability of model performance

## Confidence
- High confidence: The theoretical connection between loss minimization and program length (Mechanism 1) is well-grounded in information theory and the empirical results for low-confidence selection on the tested benchmarks are robust and reproducible
- Medium confidence: The Solomonoff induction approximation during inference (Mechanism 2) follows from the framework but depends on asymptotic assumptions that may not hold in practice for finite contexts and non-stationary distributions
- Low confidence: The claim that multiple computable distributions with similar Kolmogorov complexity can solve the task (Mechanism 3) is assumed rather than proven, and the effectiveness of low-confidence selection in more challenging settings remains untested

## Next Checks
1. Test the low-confidence selection strategy on a benchmark with controlled label noise to determine its robustness and the noise threshold at which it degrades below high-confidence selection
2. Evaluate the scaling relationship between context length and the approximation gap by measuring how the performance difference between low-confidence and high-confidence selection changes as prompt length increases
3. Apply the theoretical framework to a generative task (e.g., code completion or summarization) to test whether the Solomonoff induction approximation holds beyond classification and whether low-confidence selection remains effective