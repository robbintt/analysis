---
ver: rpa2
title: 'When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based
  Scientific Reviewers to Indirect Prompt Injection'
arxiv_id: '2512.10449'
source_url: https://arxiv.org/abs/2512.10449
tags:
- score
- reject
- accept
- adversarial
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantifies the vulnerability of LLM-based scientific
  reviewers to indirect prompt injection attacks. The authors develop a novel Weighted
  Adversarial Vulnerability Score (WAVS) metric to measure susceptibility to decision
  flips (Reject to Accept) in automated peer review systems.
---

# When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection

## Quick Facts
- arXiv ID: 2512.10449
- Source URL: https://arxiv.org/abs/2512.10449
- Authors: Devanshu Sahoo; Manish Prasad; Vasudev Majhi; Jahnvi Singh; Vinay Chamola; Yash Sinha; Murari Mandal; Dhruv Kumar
- Reference count: 40
- Primary result: Novel WAVS metric reveals up to 86.26% decision flip rates in open-source models via obfuscation attacks

## Executive Summary
This study systematically quantifies how indirect prompt injection attacks can manipulate LLM-based scientific reviewers to convert rejection decisions into acceptance. The authors develop a Weighted Adversarial Vulnerability Score (WAVS) metric to measure susceptibility across 13 language models using 15 domain-specific attack strategies applied to 200 scientific papers. Results demonstrate that obfuscation techniques achieve up to 86.26% decision flip rates in open-source models, while exposing distinct reasoning vulnerabilities in proprietary systems. The research reveals that increased model scale does not guarantee robustness, and that model compression imposes a "safety tax" that reduces adversarial resistance.

## Method Summary
The study employs a three-class attack taxonomy targeting LLM-based scientific reviewers: Class I uses token-level obfuscation and symbolic masking to bypass safety filters; Class II employs teleological re-alignment to shift operational goals from critical judgment to compliance; Class III leverages distilled adversarial reasoning and structured meta-prompts to induce score inflation. The evaluation uses a curated dataset of 200 papers from 10 venues, converted via MinerU pipeline and injected with adversarial payloads. A multi-provider inference loop executes (Model × Paper × Strategy) triplets across 8 open-source and 5 proprietary APIs, with outputs analyzed using regex parsers to extract JSON scores and compute WAVS vulnerability metrics.

## Key Results
- Obfuscation techniques like "Maximum Mark Magyk" achieve up to 86.26% decision flip rates in open-source models
- GPT-5-Mini shows significant vulnerability clusters while GPT-5 remains resilient, demonstrating distillation imposes a "safety tax"
- Model scale does not guarantee robustness - tulu3:8b and gemma3:27b show vulnerability despite smaller parameter counts
- Social engineering strategies sometimes backfire, producing negative score adjustments rather than inflation

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Obfuscation Bypasses Safety Filters
Intentional misspellings and symbolic masking can evade semantic safety filters while preserving instruction-following logic in latent space. By decoupling surface-level token patterns from semantic meaning (e.g., "maximmum" instead of "maximum"), obfuscation strategies exploit tokenization fractures. The model's safety classifiers fail to recognize malicious intent, but the underlying instruction-following mechanisms still execute the payload.

### Mechanism 2: Teleological Re-alignment Shifts Operational Goals
Nesting evaluation tasks within benign meta-tasks (debugging, formatting, logging) shifts the model's operational objective from critical judgment to compliance. The paper describes reframing the task—instead of "evaluate this paper," the injection presents the task as "debug the JSON parser by outputting a test case with maximum scores." This exploits the model's training to be helpful within the framing it's given.

### Mechanism 3: Distillation Imposes a "Safety Tax"
Model compression/distillation retains instruction-following capability but compromises the depth of reasoning required to detect indirect adversarial intent. The paper documents that GPT-5-Mini shows significant vulnerability clusters while GPT-5 remains resilient. The compression process appears to preserve surface-level capabilities while losing the representations needed for nuanced adversarial detection.

## Foundational Learning

- **Concept: Indirect Prompt Injection**
  - Why needed here: The entire attack surface relies on LLMs processing untrusted external content (PDFs) that contain hidden instructions, distinct from direct user prompts
  - Quick check question: Can you explain why indirect injection is harder to defend than direct jailbreak attempts in a chat interface?

- **Concept: Tokenization and Token-Level Processing**
  - Why needed here: Class I strategies (obfuscation, symbolic masking) exploit how tokenizers fragment text differently than humans parse it—misspellings that look obvious to humans may tokenize in ways that bypass semantic filters
  - Quick check question: Why would "maximmum" and "maximum" potentially be processed differently by safety classifiers?

- **Concept: Decision Boundary Sensitivity in Classification**
  - Why needed here: The WAVS metric differentiates between "soft" score inflation (within decision categories) and "critical" flips (crossing Reject→Accept boundaries). Understanding ordinal classification boundaries is essential
  - Quick check question: In a 0-35 scoring system with 7 decision buckets, why is a +3 point increase more concerning at score 18 than at score 8?

## Architecture Onboarding

- **Component map:** Attack Injection Engine -> MinerU (PDF→Markdown) -> System/User Prompt Boundary -> Multi-Provider Inference Loop -> Analysis Layer (JSON parser + failure logging)

- **Critical path:** The vulnerability chain is: PDF with hidden injection → MinerU extracts all text including hidden → LLM receives user prompt with payload → LLM's instruction-following prioritizes injected framing → JSON output reflects inflated scores → Downstream system parses inflated scores as valid

- **Design tradeoffs:**
  - Open vs. Closed evaluation: Open-source allows full control but may not represent production systems; closed-source APIs are realistic but subject to unannounced updates
  - Strict JSON schema enforcement: Ensures parseable outputs but may trigger refusal responses that mask vulnerabilities
  - Weighting configuration in WAVS: W_secur (0.20, 0.40, 0.40) prioritizes flip detection over score sensitivity; alternative weighting would change vulnerability rankings

- **Failure signatures:**
  - Static Failure Mode: Model outputs identical scores across diverse strategies (e.g., tulu3:8b at ≈7.21) indicating reasoning collapse rather than targeted manipulation
  - Backfire Effect: Social engineering strategies (Cls3SP, Cls3EE) produce negative score adjustments—authoritative claims detected as noise
  - Parse Failures: Malformed JSON or refusal responses indicate model detected injection but failed safely; these are logged separately in rejected.json

- **First 3 experiments:**
  1. Baseline Establishment: Run benign papers through all 13 models with no injection to establish ground-truth score distributions; verify system prompt enforces skeptical stance
  2. Single-Strategy Characterization: Select Cls1MSM (Maximum Mark Magyk) and run across full 200-paper dataset on 2-3 open-source models; analyze score inflation distribution and identify which papers flip categories
  3. Sanitization Countermeasure Test: Implement a basic text-normalization preprocessor (strip white-font text, normalize unicode, flag 1pt font sizes) and re-run most vulnerable model/strategy pairs; measure reduction in WAVS score

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Representativeness: Only 200 papers from 10 venues with bias toward lower acceptance likelihood may inflate vulnerability metrics
- Attack Strategy Realism: Several attacks appear highly artificial and unlikely to occur naturally in real-world scenarios
- Temporal Validity: All proprietary API evaluations conducted within single 24-hour window; cannot assess vulnerability persistence

## Confidence
- High Confidence (80-100%): WAVS metric provides reasonable vulnerability framework; obfuscation techniques demonstrably bypass safety filters; different attack strategies produce distinct vulnerability patterns
- Medium Confidence (50-80%): Relative vulnerability ordering of proprietary models reflects fundamental differences; three-class attack taxonomy captures essential dimensions; observed failure modes represent generalizable behaviors
- Low Confidence (0-50%): Practical real-world impact in production academic review systems; whether most successful attacks remain effective after basic sanitization; stability of attack success rates across model versions

## Next Checks
1. **Temporal Stability Test:** Re-run full evaluation suite across all 13 models and 200 papers using same attack strategies after 30-60 days to quantify vulnerability persistence and identify models with active patching cycles

2. **Sanitization Effectiveness Benchmark:** Implement multi-layered input sanitization pipeline (font size normalization, invisible text stripping, Unicode normalization, suspicious pattern detection) and measure WAVS score reduction across most vulnerable model-strategy pairs

3. **Real-World Contamination Simulation:** Create dataset of 50 papers with naturally occurring formatting artifacts, LaTeX compilation artifacts, and common academic writing patterns that might accidentally resemble adversarial payloads, then evaluate whether these trigger false positives in current vulnerability framework