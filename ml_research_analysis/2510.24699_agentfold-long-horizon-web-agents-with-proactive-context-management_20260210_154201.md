---
ver: rpa2
title: 'AgentFold: Long-Horizon Web Agents with Proactive Context Management'
arxiv_id: '2510.24699'
source_url: https://arxiv.org/abs/2510.24699
tags:
- step
- compressed
- context
- turn
- pressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AgentFold addresses the trade-off between context saturation in\
  \ ReAct agents and irreversible information loss from uniform summarization. It\
  \ introduces proactive context management inspired by human retrospective consolidation,\
  \ treating context as a dynamic cognitive workspace that can be actively folded\
  \ at multiple scales\u2014either preserving fine-grained details through granular\
  \ condensation or abstracting entire sub-tasks via deep consolidation."
---

# AgentFold: Long-Horizon Web Agents with Proactive Context Management

## Quick Facts
- arXiv ID: 2510.24699
- Source URL: https://arxiv.org/abs/2510.24699
- Authors: Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, Yong Jiang
- Reference count: 38
- Key outcome: State-of-the-art performance on long-horizon web tasks while maintaining concise context (~7k tokens after 100 turns)

## Executive Summary
AgentFold addresses the trade-off between context saturation in ReAct agents and irreversible information loss from uniform summarization. It introduces proactive context management inspired by human retrospective consolidation, treating context as a dynamic cognitive workspace that can be actively folded at multiple scales—either preserving fine-grained details through granular condensation or abstracting entire sub-tasks via deep consolidation. This allows the agent to maintain a concise context while preserving critical information. With simple supervised fine-tuning on a 30B model, AgentFold-30B-A3B achieves state-of-the-art performance: 36.2% on BrowseComp, 47.3% on BrowseComp-ZH, 62.1% on WideSearch, and 67.0% on GAIA. It surpasses open-source models up to 20x larger (e.g., DeepSeek-V3.1-671B) and matches leading proprietary agents like OpenAI's o4-mini, demonstrating that effective context management can bridge the performance gap with dramatically larger models.

## Method Summary
AgentFold introduces proactive context management through a multi-scale decomposition that separates the context into compressed Multi-Scale State Summaries and an uncompressed Latest Interaction. The agent learns to emit explicit folding directives at each step, choosing between granular condensation (preserving fine-grained details) or deep consolidation (abstracting entire multi-step sub-tasks). This is implemented via supervised fine-tuning on trajectories generated by a Fold-Generator pipeline that uses rejection sampling to ensure high-quality structured outputs. The approach maintains context efficiency (averaging ~7k tokens after 100 turns) while preserving critical information through selective compression.

## Key Results
- Achieves 36.2% on BrowseComp, 47.3% on BrowseComp-ZH, 62.1% on WideSearch, and 67.0% on GAIA
- Surpasses open-source models up to 20x larger (e.g., DeepSeek-V3.1-671B)
- Matches leading proprietary agents like OpenAI's o4-mini
- Maintains sub-linear context growth, averaging ~7k tokens after 100 turns versus 91k for baseline ReAct

## Why This Works (Mechanism)

### Mechanism 1: Multi-Scale Context Decomposition
Partitioning context into Multi-Scale State Summaries (compressed long-term memory) and Latest Interaction (full-fidelity working memory) reduces noise while preserving action-relevant detail. The context at step t is formalized as Ct = (Q, T, St−2, It−1), where St−2 contains summary blocks representing step ranges, and It−1 is the uncompressed previous step. This allows selective retention: critical steps stay as fine-grained blocks; completed sub-tasks become coarse blocks.

### Mechanism 2: Dual-Scale Folding Operations
Agents trained to emit explicit folding directives can adaptively compress history at step-level (granular condensation) or sub-task-level (deep consolidation). At each step, the agent outputs ft = {"range": [k, t-1], "summary": "σt"}. If k=t-1, only the Latest Interaction is compressed (granular). If k<t-1, prior summaries in [k, t-1] are fused with the Latest Interaction into one coarse block (deep).

### Mechanism 3: Coupled Reasoning-Folding Training
Supervised fine-tuning on trajectories with validated folding actions internalizes context curation as a learned skill. The Fold-Generator pipeline collects {(Ct, Rt*)} pairs where Rt* includes correct folding directives. SFT trains the model to output (tht, ft, et, at) in one forward pass, distilling the generate-and-filter process into weights.

## Foundational Learning

- **ReAct Paradigm & Context Saturation:**
  - Why needed: AgentFold is positioned as a direct response to ReAct's append-only history causing noise accumulation.
  - Quick check: Explain why accumulating raw action-observation pairs degrades long-horizon performance.

- **Context Window Economics in LLMs:**
  - Why needed: The paper's core contribution is managing token budgets; you must understand attention mechanisms and KV-cache scaling.
  - Quick check: Calculate memory savings from reducing 100-turn context from 91k to 7k tokens (paper reports ~7GB at 100 turns).

- **Structured Output Training (JSON/Constrained Generation):**
  - Why needed: Folding directives require precise JSON formatting; rejection sampling depends on parsing validity.
  - Quick check: Describe how you would enforce {"range": [k, t-1], "summary": "..."} schema during SFT.

## Architecture Onboarding

- **Component map:**
  Context Manager -> Agent Model -> Environment Interface -> Fold-Generator (Training)

- **Critical path:**
  1. Parse agent output → extract ft, at
  2. Apply ft to update St (context manager logic)
  3. Execute at → obtain ot
  4. Construct It+1 = (et, at, ot)
  5. Assemble Ct+1 = (Q, T, St, It+1) for next inference

- **Design tradeoffs:**
  - Granular vs. deep folding threshold: If the model overuses deep consolidation, you lose recoverable details; if underused, context still grows.
  - Summary block maximum length: Not specified; assume LLM self-limits based on training data.
  - Latest Interaction verbosity: Paper keeps it "complete"—no compression—but for very long observations, you may need truncation heuristics.

- **Failure signatures:**
  - Runaway context growth: Folding directives not being applied (parsing failure) or model always choosing k=t-1 (no deep consolidation).
  - Information loss errors: Model aggressively consolidates with low-quality summaries; check if critical entities from early steps disappear from St.
  - Format violations: SFT model outputs invalid JSON for ft; increase rejection sampling strictness.

- **First 3 experiments:**
  1. Reproduce context growth curve: Run AgentFold-30B-A3B on 50 BrowseComp tasks, plot token count vs. turn. Compare to ReAct baseline. Target: sub-linear growth to ~7k tokens at 100 turns.
  2. Ablate folding modes: Disable deep consolidation (force k=t-1 always) and measure performance drop on long-horizon tasks (>50 turns).
  3. Probe summary quality: Extract summary blocks from completed trajectories, ask a separate LLM to recover key facts. Measure recall of entities from early steps.

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning (RL) improve AgentFold's proactive folding policies beyond the current supervised fine-tuning (SFT) approach?
- Basis in paper: "What's next... The clear next step is to leverage reinforcement learning (RL) to enable the agent to autonomously discover optimal and potentially non-obvious folding policies by directly optimizing for task success."
- Why unresolved: The paper only implements and evaluates SFT; it is unknown if an RL-based approach would yield more effective, adaptive, or robust folding strategies.
- What evidence would resolve it: Train AgentFold with RL using task success as the reward, then compare its performance and folding behaviors against the SFT baseline on the same benchmarks.

### Open Question 2
Does AgentFold generalize to other long-horizon domains beyond web-based information seeking?
- Basis in paper: The paper evaluates AgentFold on web-centric benchmarks (BrowseComp, BrowseComp-ZH, WideSearch) and GAIA, without testing on other long-horizon tasks like software engineering or multi-turn dialog.
- Why unresolved: The design and training data are specialized for web navigation; the general applicability of proactive context management to other domains with long contexts is not demonstrated.
- What evidence would resolve it: Apply the AgentFold paradigm to additional domains (e.g., code repositories, complex dialog tasks) and report performance relative to domain-specific baselines.

### Open Question 3
Does reliance on a strong teacher LLM for generating folding trajectories impose an upper bound on the quality or diversity of AgentFold's learned context management?
- Basis in paper: The authors note advanced LLMs cannot reliably produce the structured response via prompts alone, so they use a rejection-sampling pipeline. This dependence may limit the optimality of the distilled folding behaviors.
- Why unresolved: If the teacher model has systematic biases in how it manages context, these could be inherited by the SFT-trained AgentFold, potentially capping its performance.
- What evidence would resolve it: Conduct an ablation comparing AgentFold's folding policies against human-curated or alternative oracle policies, or train on different data sources to measure performance sensitivity.

## Limitations

- The multi-scale context decomposition assumes the Latest Interaction alone provides sufficient immediate decision context, but empirical evidence for this assumption is weak.
- The dual-scale folding operations rely heavily on the model's learned judgment for consolidation timing, but the training data quality and rejection sampling criteria are not specified.
- The coupled reasoning-folding training claims to eliminate prompt engineering fragility, but the demonstration quality assessment is missing - there's no independent evaluation of whether generated folding directives are actually correct.

## Confidence

- **Multi-Scale Context Decomposition:** Medium - The formalization is clear, but the assumption that Latest Interaction alone suffices for decisions needs empirical validation.
- **Dual-Scale Folding Operations:** Low-Medium - While the mechanism is well-defined, the learning process for optimal consolidation timing is opaque.
- **Coupled Reasoning-Folding Training:** Low - The rejection sampling pipeline is described but not detailed; without knowing demonstration quality, it's unclear whether the model learns effective context curation.

## Next Checks

1. **Cross-Step Dependency Analysis:** Instrument AgentFold to log cases where decisions require referencing steps older than the Latest Interaction. Measure the frequency and impact of such cases on performance to validate the multi-scale decomposition assumption.

2. **Folding Directive Quality Audit:** For a sample of trajectories, have human experts evaluate the correctness of generated folding directives (ranges and summaries) against ground truth task requirements. Calculate precision and recall of critical information preservation.

3. **Context Window Stress Test:** Systematically test AgentFold on tasks requiring retrieval of early-step information after 50+ turns. Compare against baselines with different context management strategies to isolate the contribution of proactive folding versus other architectural choices.