---
ver: rpa2
title: Learning Safe Autonomous Driving Policies Using Predictive Safety Representations
arxiv_id: '2512.17586'
source_url: https://arxiv.org/abs/2512.17586
tags:
- learning
- driving
- srpl
- safety
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the SRPL framework for improving SafeRL in
  autonomous driving. The method augments policy observations with predictive safety
  representations from a Steps-to-Cost model that estimates future constraint violations.
---

# Learning Safe Autonomous Driving Policies Using Predictive Safety Representations

## Quick Facts
- arXiv ID: 2512.17586
- Source URL: https://arxiv.org/abs/2512.17586
- Reference count: 34
- Key outcome: SRPL framework improves SafeRL in autonomous driving by augmenting policy observations with predictive safety representations from Steps-to-Cost model, achieving statistically significant improvements in success rate (r = 0.65-0.86) and cost reduction (r = 0.70-0.83, p < 0.05).

## Executive Summary
This paper introduces the SRPL framework for learning safe autonomous driving policies through predictive safety representations. The method augments policy observations with a Steps-to-Cost (S2C) model that predicts timesteps until constraint violations, enabling anticipatory decision-making. Experiments on WOMD and NuPlan datasets demonstrate that SRPL achieves statistically significant improvements in success rates and cost reduction across multiple SafeRL algorithms, with particular effectiveness in enhancing robustness to sensor noise and cross-dataset generalization.

## Method Summary
SRPL augments SafeRL policy observations with predictive safety representations from a Steps-to-Cost model that estimates future constraint violations. The S2C model is trained via negative log-likelihood on a dedicated safety buffer containing retrospectively labeled states with observed steps-to-violation. During policy training, the S2C predictions are concatenated with raw observations before being fed to the policy network. The framework is evaluated across five SafeRL algorithms (CPO, PPOLag, TRPOPID, OnCRPO, P3O) on WOMD and NuPlan autonomous driving datasets, demonstrating improved success rates, cost reduction, and robustness to observation noise.

## Key Results
- SRPL achieves statistically significant improvements in success rate with effect sizes r = 0.65-0.86 across multiple algorithms
- Cost reduction improvements show effect sizes r = 0.70-0.83 (p < 0.05) compared to baseline methods
- SRPL enhances robustness to sensor noise, reducing action output variance by 3.5pp at σ=0.01 and 79.7pp at σ=0.1 compared to baseline CPO
- Cross-dataset generalization is stronger when training on the more diverse WOMD dataset

## Why This Works (Mechanism)

### Mechanism 1: Predictive Safety State Augmentation
Augmenting observations with predicted time-to-violation enables anticipatory decision-making. The Steps-to-Cost model Sν:S→ΔHs predicts probability distribution over discretized bins representing timesteps until constraint violation, concatenated with raw state observations before policy input. Core assumption: future constraint violations have detectable precursors in current state representations.

### Mechanism 2: Interleaved Supervised-RL Training with Safety Buffer
Dedicated safety experience buffer improves sample efficiency for learning safety representations. States are labeled with ground-truth steps-to-cost values during rollouts, stored in buffer Bs, and the S2C model trains via negative log-likelihood minimization on mini-batches from Bs, interleaved with policy updates. Core assumption: historical safety-relevant experiences generalize to future state visitation distributions.

### Mechanism 3: Policy Output Stabilization via Auxiliary Safety Signal
Safety representations reduce action variance under observation noise by providing stable auxiliary input channel. When raw observations are corrupted, the learned S2C representation acts as smoothing signal, reducing policy sensitivity to input perturbations. Core assumption: S2C model learns noise-robust features less sensitive to sensor perturbations than raw observations.

## Foundational Learning

- **Constrained Markov Decision Processes (CMDPs)**: SafeRL formulates driving as CMDPs with cost constraints, not pure reward maximization. Understanding J_R(πθ) vs J_C(πθ) separation is prerequisite. Quick check: Given a policy with expected cost J_C(πθ)=1.8 and threshold κ=1.5, is this policy feasible under CMDP formulation?

- **Lagrangian-based SafeRL Methods**: Baseline algorithms (CPO, PPOLag, TRPOPID, OnCRPO, P3O) employ Lagrangian multipliers or trust-region approaches for constraint satisfaction. Algorithm-specific behavior affects SRPL compatibility. Quick check: Why might CPO's trust-region constraints combined with SRPL cause excessive conservatism?

- **Classification over Discretized Horizon**: S2C discretizes continuous steps-to-cost into categorical bins (bin size b=2, horizon Hs=60). Treating time-to-violation prediction as classification enables probabilistic output distribution. Quick check: If Hs=60 and b=2, how many output classes does S2C model predict? What happens to prediction granularity if b=10 instead?

## Architecture Onboarding

- **Component map:** Rollouts → retroactive labeling → safety buffer Bs → S2C model updates → target network sync → augmented state [s⊕Sν′(s)] → policy network → SafeRL algorithm updates

- **Critical path:** 1) Rollout collection → retroactively label each state with observed steps-to-violation 2) Discretize labels and populate Bs 3) Sample mini-batch from Bs, update Sν via cross-entropy loss 4) Periodically copy Sν→Sν′ (target network sync) 5) Construct augmented state s_aug=[s⊕Sν′(s)], feed to policy 6) Execute SafeRL algorithm update on πθ using s_aug

- **Design tradeoffs:** Longer Hs improves foresight but increases prediction difficulty and output dimensionality; smaller b increases granularity but requires more S2C capacity and data; algorithm selection: PPOLag for cost reduction, P3O for success rate, avoid CPO for complex environments; WOMD training yields better cross-domain transfer than NuPlan

- **Failure signatures:** SRPL-CPO conservatism: reduced reward and success rate despite cost improvement; cross-dataset degradation: performance drops transferring from less diverse (NuPlan) to more diverse (WOMD); high-noise collapse: at σ>0.1, cost rises sharply; stagnant S2C loss: if buffer lacks diverse violation examples, S2C overfits to dominant class

- **First 3 experiments:** 1) Train baseline PPOLag vs SRPL-PPOLag on 500 WOMD scenarios; verify S2C loss decreases and success rate improves (expect r>0.65 effect size) 2) Ablation on Hs∈{30,60,90} and b∈{1,2,5} on validation; monitor S2C accuracy and downstream success rate 3) Noise robustness test: evaluate trained SRPL-P3O at σ∈{0.01,0.05,0.1} on 500 scenarios; verify action variance reduction and cost stability compared to baseline

## Open Questions the Paper Calls Out

- **Open Question 1:** Does SRPL augmentation maintain robustness advantages against realistic, physically-based sensor corruptions (e.g., rain, fog, lens flare) as effectively as it does against synthetic Gaussian noise? The current study only validates robustness using mathematical noise injection, which fails to capture structured artifacts of real-world environmental sensor interference.

- **Open Question 2:** Can "universal" safety representations be developed to provide consistent benefits across diverse SafeRL algorithms, specifically mitigating the conservatism observed in SRPL-CPO? The results show high variance in efficacy; SRPL-P3O excelled in success rates, while SRPL-CPO became overly conservative.

- **Open Question 3:** Does the SRPL framework maintain computational efficiency and performance stability when scaled to the full volume of datasets like WOMD and NuPlan? The study relied on sampled subsets (5,000 training scenarios); it remains unclear if interleaved updates impose computational bottleneck or if sample efficiency degrades with significantly more data.

## Limitations
- Unexplained performance drop of SRPL-CPO on complex environments where excessive conservatism reduces both reward and success rate despite lower costs
- Critical hyperparameters (horizon Hs=60, bin size b=2) lack systematic sensitivity analysis
- Several algorithmic details remain unspecified including optimizer configurations, learning rates, and buffer update frequencies
- Generalization benefits observed from WOMD training may not extend to other diverse datasets beyond NuPlan

## Confidence

**High Confidence:** Statistically significant improvements in success rate (r = 0.65-0.86) and cost reduction (r = 0.70-0.83) across multiple algorithms and datasets are well-supported by experimental results with appropriate significance testing.

**Medium Confidence:** Claims about robustness to sensor noise and cross-dataset generalization are supported by experiments but could benefit from broader noise ranges and more diverse test domains.

**Low Confidence:** Specific performance degradation patterns of SRPL-CPO in complex environments and exact conditions under which safety buffer mechanisms fail are mentioned but not thoroughly analyzed.

## Next Checks
1. **Algorithm-Agnostic Validation:** Test SRPL augmentation across additional SafeRL algorithms (e.g., SAC-Lagrangian, Lagrangian PPO) to determine if benefits extend beyond the five evaluated methods.

2. **Extended Robustness Testing:** Evaluate SRPL under progressively higher noise levels (σ > 0.1) and different noise distributions to establish the precise failure threshold where S2C predictions become unreliable.

3. **Safety Buffer Analysis:** Conduct ablation studies varying buffer sizes, update frequencies, and sampling strategies to quantify their impact on S2C model learning efficiency and downstream policy performance.