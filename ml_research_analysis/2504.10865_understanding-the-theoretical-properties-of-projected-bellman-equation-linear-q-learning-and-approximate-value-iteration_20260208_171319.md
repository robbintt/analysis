---
ver: rpa2
title: Understanding the theoretical properties of projected Bellman equation, linear
  Q-learning, and approximate value iteration
arxiv_id: '2504.10865'
source_url: https://arxiv.org/abs/2504.10865
tags:
- q-learning
- policy
- solution
- condition
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides new theoretical insights into the projected
  Bellman equation (PBE), linear Q-learning, and approximate value iteration (AVI)
  in reinforcement learning. The authors analyze sufficient conditions for the existence
  and uniqueness of solutions to PBE, including the strictly negatively row dominating
  diagonal (SNRDD) assumption and conditions derived from AVI.
---

# Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration

## Quick Facts
- **arXiv ID**: 2504.10865
- **Source URL**: https://arxiv.org/abs/2504.10865
- **Reference count**: 40
- **Primary result**: New theoretical insights into PBE, linear Q-learning, and AVI, including sufficient conditions for existence/uniqueness, SNRDD convergence guarantees, and ε-greedy policy pathologies

## Executive Summary
This paper provides new theoretical insights into the projected Bellman equation (PBE), linear Q-learning, and approximate value iteration (AVI) in reinforcement learning. The authors analyze sufficient conditions for the existence and uniqueness of solutions to PBE, including the strictly negatively row dominating diagonal (SNRDD) assumption and conditions derived from AVI. They prove that SNRDD ensures convergence of linear Q-learning and explore its relationship with AVI convergence. Additionally, the paper presents examples showing scenarios where Q-learning converges while AVI does not, and vice versa. The authors also investigate pathological behaviors arising from using epsilon-greedy policies, demonstrating cases where the number of solutions changes with epsilon and where increasing epsilon introduces a solution yielding an optimal policy that Q-learning cannot converge to.

## Method Summary
The paper analyzes theoretical properties of PBE, linear Q-learning, and AVI through mathematical proofs and constructed examples. The authors introduce the SNRDD condition as a sufficient condition for Q-learning convergence, prove its relationship to AVI convergence conditions, and examine pathological behaviors with epsilon-greedy policies. They provide deterministic Q-learning (Algorithm 2) and AVI update rules with specific MDP matrices from examples, using γ=0.99 and requiring feature normalization ∥ϕ(s,a)∥∞ ≤ 1/√p for SNRDD verification.

## Key Results
- SNRDD condition guarantees unique solution to PBE and ensures linear Q-learning convergence
- AVI and Q-learning have distinct sufficient convergence conditions, allowing scenarios where one converges while the other diverges
- Epsilon-greedy policies can cause non-existence or multiplicity of solutions, with the number of solutions changing non-smoothly with epsilon
- Regularization parameter η > 3 can enforce SNRDD condition for stability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the system matrix satisfies the Strictly Negatively Row Dominating Diagonal (SNRDD) condition, the Projected Bellman Equation (PBE) is guaranteed to have a unique solution, and linear Q-learning converges to it.
- **Mechanism:** The SNRDD condition ensures the system dynamics are a contraction in the infinity norm. Specifically, it guarantees the matrix T(θ, ...) has a one-sided Lipschitz constant less than zero. This maps the stochastic update to an ODE with a globally asymptotically stable equilibrium point.
- **Core assumption:** The matrix T(θ, π_θ, ν_θ) (dependent on features, policy, and sampling distribution) must satisfy S_i(A) < 0 for all rows i.
- **Evidence anchors:**
  - [abstract]: "They prove that SNRDD ensures convergence of linear Q-learning..."
  - [Section 3.1]: Defines SNRDD and Theorem 3.2 proving existence/uniqueness.
  - [corpus]: "Periodic Regularized Q-Learning" cites regularization as a method to stabilize convergence, which aligns with this paper's use of regularization to satisfy SNRDD.
- **Break condition:** If the feature matrix Φ is poorly scaled or the regularization η is too low, the diagonal dominance condition fails, potentially leading to divergence or non-existence of a fixed point.

### Mechanism 2
- **Claim:** Convergence of Approximate Value Iteration (AVI) relies on a distinct sufficient condition (infinity norm < 1) compared to Q-learning (SNRDD), meaning one can converge while the other diverges.
- **Mechanism:** AVI convergence depends on the contraction property of the projected operator Φ(Φ⊤DΦ)^(-1)Φ⊤DPΠΦ (Condition 6/7). Q-learning relies on the SNRDD property of T. Proposition 3.13 shows SNRDD implies AVI convergence under non-negative diagonal elements, but the reverse is not generally true.
- **Core assumption:** The stability of AVI is separable from the stability of the Q-learning ODE trajectory.
- **Evidence anchors:**
  - [Section 5]: "We provide examples where AVI converges while linear Q-learning does not, and vice versa."
  - [Section 3.2]: Theorem 3.10 details the AVI sufficient conditions distinct from SNRDD.
  - [corpus]: "Is Bellman Equation Enough for Learning Control?" discusses insufficiencies in Bellman equations for control, relevant to the divergence examples.
- **Break condition:** If the matrix T(θ) at the solution point is not Hurwitz (stable), Q-learning fails even if AVI converges (Example 13.2).

### Mechanism 3
- **Claim:** Using ε-greedy policies introduces discontinuities that can invalidate existence guarantees or cause the number of solutions to change non-smoothly with ε.
- **Mechanism:** The existence proofs rely on Brouwer's fixed-point theorem, which requires continuity. The ε-greedy policy is discontinuous in θ. This can bifurcate the solution set, causing solutions to appear/disappear as ε varies.
- **Core assumption:** The theoretical guarantees derived for Lipschitz/continuous policies do not generalize to hard threshold policies like ε-greedy.
- **Evidence anchors:**
  - [Section 6]: "The first example shows depending on the value of ε, there is a chance of non-existence or multiplicity of the solution..."
  - [abstract]: "...investigates pathological behaviors arising from using epsilon-greedy policies..."
  - [corpus]: Corpus signals are weak for this specific pathological mechanism regarding ε-greedy discontinuities; insights are derived primarily from the paper text.
- **Break condition:** Increasing exploration ε can introduce an unstable solution that yields an optimal policy but to which Q-learning cannot converge (Example 14.2).

## Foundational Learning

- **Concept: Projected Bellman Equation (PBE)**
  - **Why needed here:** This is the central equation linear Q-learning and AVI attempt to solve. Understanding that the true Q-function might lie outside the linear feature space, requiring a projection, is the root cause of the convergence difficulties analyzed.
  - **Quick check question:** Can you explain why simply solving the Bellman equation in a linear subspace (PBE) differs from the tabular case?

- **Concept: Strictly Negatively Row Dominating Diagonal (SNRDD)**
  - **Why needed here:** This is the paper's core mathematical tool. It is a structural property of matrices that guarantees stability (contraction) in non-Euclidean norms, serving as a sufficient condition for all major positive results in the text.
  - **Quick check question:** Given a matrix A, how would you check if it satisfies SNRDD?

- **Concept: ODE Method for Stochastic Approximation**
  - **Why needed here:** The paper links the discrete stochastic updates of Q-learning to continuous-time ODEs. Convergence is proven by showing the ODE has a globally asymptotically stable equilibrium (via contraction theory).
  - **Quick check question:** In the context of Q-learning, what does it imply if the corresponding ODE trajectory diverges?

## Architecture Onboarding

- **Component map:** Φ (feature matrix) -> T (operator matrix) -> θ (parameters) -> π_θ (policy) -> μ_βθ (sampling distribution)
- **Critical path:**
  1. Verify feature scaling (∥φ∥_∞ ≤ 1/√p)
  2. Select regularization η (suggested η > 3 for standard scaling) to enforce SNRDD
  3. Avoid ε-greedy if theoretical guarantees of existence/uniqueness are required; prefer Lipschitz/softmax policies
- **Design tradeoffs:**
  - **Regularization (η) vs. Accuracy:** Higher η guarantees SNRDD (stability) but may bias the solution further from the true Q-function
  - **AVI vs. Q-learning:** AVI might converge in cases where Q-learning diverges (and vice versa); the paper provides conditions to check which applies to your feature set
- **Failure signatures:**
  - **Sub-optimal convergence:** Q-learning converges to a unique point, but that point induces a sub-optimal policy (Example 13.3)
  - **Pathological Epsilon:** Algorithm oscillates or fails to converge only at specific critical values of ε
- **First 3 experiments:**
  1. **Verify SNRDD:** Construct a small gridworld, generate random features, and compute T to see if SNRDD holds. Adjust η until it does
  2. **Divergence Reproduction:** Replicate Example 13.1/13.2 settings to observe AVI converging while Q-learning diverges (and vice versa)
  3. **Epsilon Sensitivity:** Run Q-learning with ε-greedy across a range of ε values to identify the "critical values" where the number of solutions changes (Example 14.1)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical properties of the Projected Bellman Equation (PBE), linear Q-learning, and approximate value iteration (AVI) be extended to non-linear function approximation settings?
- **Basis in paper:** [explicit] The conclusion states, "Future studies would include considering non-linear function approximation."
- **Why unresolved:** The current analysis relies on linear algebra properties (specifically SNRDD) that may not directly transfer to non-linear feature maps.
- **What evidence would resolve it:** Proofs of convergence or existence/uniqueness for Q-learning and AVI utilizing non-linear approximators (e.g., neural networks).

### Open Question 2
- **Question:** Is it possible to construct examples where condition (7) holds, but there is a divergence in convergence behavior between AVI and Q-learning?
- **Basis in paper:** [explicit] Section 3.3 notes, "it is not clear whether we can construct such example with condition (7), which requires further research."
- **Why unresolved:** While examples exist for condition (6), the specific structural properties of condition (7) make constructing analogous counter-examples non-trivial.
- **What evidence would resolve it:** A concrete MDP example where condition (7) is satisfied, but only one algorithm converges while the other fails.

### Open Question 3
- **Question:** Under what conditions does the unique solution guaranteed by the SNRDD assumption ensure an optimal policy rather than a sub-optimal one?
- **Basis in paper:** [inferred] Remark 4.6 and Example 13.3 demonstrate that SNRDD guarantees convergence to a unique solution, but this solution can still yield a sub-optimal policy.
- **Why unresolved:** The paper establishes stability (convergence) but does not define conditions that guarantee the quality (optimality) of the converged fixed point.
- **What evidence would resolve it:** Theoretical conditions linking the SNRDD parameters or feature matrix structure to the global optimality of the resulting policy.

## Limitations
- SNRDD condition may be overly restrictive in practice, potentially limiting applicability to real-world problems
- ε-greedy policy pathologies demonstrated only on small MDPs, raising scalability concerns
- Analysis focuses on linear function approximation, leaving non-linear extensions as open questions

## Confidence
- **High Confidence**: The existence and uniqueness results under SNRDD (Theorem 3.2) are mathematically rigorous and well-proven
- **Medium Confidence**: The separation of convergence conditions between Q-learning and AVI (Proposition 3.13) is theoretically sound but may be conservative in practical settings
- **Medium Confidence**: The pathological ε-greedy examples (Section 14) demonstrate the mechanism clearly but may represent edge cases rather than common failure modes

## Next Checks
1. Test SNRDD verification on larger, randomly generated MDPs to assess practical applicability of the condition
2. Evaluate the impact of different feature normalization schemes on the SNRDD condition and convergence behavior
3. Implement and test the proposed Lipschitz/softmax policies as alternatives to ε-greedy to verify if they eliminate the identified pathologies while maintaining exploration effectiveness