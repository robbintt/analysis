---
ver: rpa2
title: 'FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data
  Synthesis Pipeline'
arxiv_id: '2508.16514'
source_url: https://arxiv.org/abs/2508.16514
tags:
- data
- math
- problem
- flames
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLAMES, a systematic framework for assessing
  and improving large language model (LLM) math reasoning through synthetic data generation.
  The authors perform a controlled study of 12 data synthesis agents, 6 quality control
  strategies, and both problem and solution generation models, identifying that complexity-enhancing
  agents yield the best improvements on most math benchmarks.
---

# FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline

## Quick Facts
- arXiv ID: 2508.16514
- Source URL: https://arxiv.org/abs/2508.16514
- Reference count: 30
- Primary result: Novel agents and dataset improve math reasoning across 5 benchmarks

## Executive Summary
This paper introduces FLAMES, a systematic framework for assessing and improving large language model (LLM) math reasoning through synthetic data generation. The authors perform a controlled study of 12 data synthesis agents, 6 quality control strategies, and both problem and solution generation models, identifying that complexity-enhancing agents yield the best improvements on most math benchmarks. They find that maintaining broader problem coverage is more beneficial than filtering for perfectly reliable solutions, and that solution model quality has a greater impact than problem generation model quality. Building on these insights, the authors propose two novel agents—Taxonomy-Based Key Concepts and Distraction Insertion—to enhance out-of-domain generalization and robustness. By blending these with two strong existing agents, they create the FLAMES dataset, which outperforms public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on FLAMES achieves 81.4% on MATH, surpassing much larger models like Llama3 405B, GPT-4o, and Claude 3.5 Sonnet.

## Method Summary
The FLAMES framework evaluates math data synthesis through controlled experiments across 12 agents, 6 quality control strategies, and 5 evaluation benchmarks. Synthetic problems are generated using GSM8K/MATH seed problems, Qwen2.5-32B-Instruct problem model, and Qwen2.5-Math-7B-Instruct solution model. Quality control involves deduplication, decontamination, and solution filtering strategies. The best-performing mixture combines Suggester-Editor (50%), IQC (20%), Taxonomy Key Concepts (20%), and Distraction Insertion (10%) agents, yielding 150K+ problems fine-tuned on DeepSeek-Math-7B or Qwen2.5-Math-7B models.

## Key Results
- Complexity-enhancing agents (Suggester-Editor, IQC) outperform practice and robustness agents on all benchmarks
- "First" solution filtering maintains 150K+ problems vs. 45K with strict self-consistency filtering
- Taxonomy Key Concepts agent improves out-of-domain performance by 3.4% on CollegeMath
- Distraction Insertion agent improves robustness on GSMPlus by 3.7%
- FLAMES Small dataset improves OlympiadBench by 15.7% over baseline

## Why This Works (Mechanism)

### Mechanism 1
Complexity-enhancing data agents produce superior math reasoning improvements compared to practice-focused or robustness-focused agents. Agents like Suggester-Editor and IQC iteratively add reasoning steps to seed problems, forcing the student model to learn multi-step decomposition and intermediate reasoning rather than pattern matching on simpler variants.

### Mechanism 2
Maintaining higher problem coverage with imperfect solutions outperforms aggressive filtering for solution correctness. Filtering strategies that reject problems without self-consistent solutions inadvertently remove harder, legitimate problems, reducing training diversity and limiting generalization.

### Mechanism 3
Synthetic data from simpler seed problems enables easy-to-hard generalization to competition-level benchmarks. Complexity-enhancing agents generate problems with similar underlying concepts but increased reasoning depth, teaching transferable problem-solving strategies rather than memorizing specific difficulty distributions.

## Foundational Learning

- Concept: Self-consistency for solution verification
  - Why needed here: Multiple solutions generated for the same problem can be compared to identify correct answers without ground-truth labels.
  - Quick check question: Given three sampled solutions with answers [42, 42, 41], what would a majority-vote filter output?

- Concept: Curriculum via complexity augmentation
  - Why needed here: Agents like Suggester-Editor iteratively add reasoning steps, implicitly creating a difficulty curriculum during data synthesis.
  - Quick check question: How does converting "solve 2x+3=7" to "solve 2x+3=7 where x is the sum of two consecutive integers" change reasoning requirements?

- Concept: Out-of-distribution (OOD) evaluation vs. in-domain benchmarking
  - Why needed here: The paper explicitly separates in-domain (GSM8K, MATH), OOD (CollegeMath), robustness (GSMPlus), and competition (OlympiadBench) to measure different generalization axes.
  - Quick check question: Why might a model score 90% on GSM8K but only 20% on CollegeMath even if both test high-school-level math?

## Architecture Onboarding

- Component map:
  - Seed Problems (GSM8K/MATH) -> Problem Synthesis Model (Qwen2.5-32B-Instruct) -> Data Agents (12 strategies) -> Solution Synthesis Model (Qwen2.5-Math-7B-Instruct) -> Quality Control (deduplication/decontamination/filtering) -> Student Model (DeepSeek-Math-7B) -> Evaluation Suite (5 benchmarks)

- Critical path:
  1. Select data agent(s) based on target improvement axis (complexity for in-domain/competition, Taxonomy for OOD, Distraction for robustness)
  2. Generate 150K+ synthetic problems using Qwen2.5-32B-Instruct with agent-specific prompts
  3. Synthesize solutions using Qwen2.5-Math-7B-Instruct with temperature 0.7
  4. Apply "First" or "Majority + First" quality control (avoid solvability filtering)
  5. Fine-tune student model for 5 epochs, select checkpoint with best GSM8K+MATH average

- Design tradeoffs:
  - Coverage vs. Precision: "First" keeps all problems with first solution (faster, higher coverage); "Majority + First" adds verification at compute cost
  - Agent mixture vs. single agent: Blending 4 agents (50% Suggester-Editor, 20% IQC, 20% Taxonomy, 10% Distraction) balances all evaluation axes but requires managing multiple synthesis pipelines
  - Solution model quality vs. cost: Stronger solution models (Qwen2.5-Math-7B vs. DeepSeek-7B-RL) matter more than problem model quality (Table 5: -7.1 MATH drop with weaker solution model)

- Failure signatures:
  - Low competition/OOD scores with high in-domain → too much in-domain practice data, not enough complexity or Taxonomy agents
  - Large GSM8K→GSMPlus-Distraction gap → insufficient robustness training; add Distraction Insertion agent
  - Solvability filter removes >30% of problems → filter is rejecting legitimate hard problems; switch to "First" strategy
  - Scale doesn't improve results → check solution model quality; solution errors may increase with scale

- First 3 experiments:
  1. **Baseline comparison**: Generate 150K problems using Suggester-Editor agent, fine-tune DeepSeek-Math-7B, evaluate on all 5 benchmarks to establish complexity-agent baseline
  2. **Coverage ablation**: Compare "First" vs. "Strict Self-Consistency" on same 150K problem pool to validate coverage-precision tradeoff on your target student model
  3. **Agent mixture optimization**: Test the paper's FLAMES Small mixture (50/20/20/10 split) against single best agent to measure mixture benefit before scaling to 1M+

## Open Questions the Paper Calls Out

### Open Question 1
Can the FLAMES framework be extended to detect and filter inherent biases in synthetic data generation? The authors identify inherent bias in generated data as a limitation and suggest introducing an additional bias detection step in the framework.

### Open Question 2
How does the FLAMES methodology perform in low-resource languages where strong teacher models for solution generation are unavailable? The authors note that reliance on a strong teacher model may be violated when working with less common languages.

### Open Question 3
Do the findings regarding complexity-enhancing agents transfer to non-mathematical reasoning domains such as coding or logic? The paper strictly focuses on mathematical reasoning, leaving unknown whether agents like Suggester-Editor or Taxonomy-Based Key Concepts are optimal for other domains.

## Limitations
- Limited to 7B-parameter student models, leaving uncertainty about performance at larger scales
- Focus on Qwen2.5 models may not generalize to other model families or training paradigms
- Does not fully explore the boundary where solution noise overwhelms training signal

## Confidence

- **High Confidence**: Mechanism 1 (complexity agents improve performance), Mechanism 2 (coverage > filtering), and overall FLAMES dataset creation process are well-supported by controlled experiments and ablation studies.
- **Medium Confidence**: Mechanism 3 (easy-to-hard generalization) is supported but could benefit from more extensive cross-domain transfer studies.
- **Medium Confidence**: Optimal agent mixture ratios are empirically derived but may not generalize across different student model architectures or target domains.

## Next Checks
1. **Scale sensitivity analysis**: Evaluate FLAMES-trained models at 8B and 70B scales to determine if coverage-precision tradeoffs shift with model capacity.
2. **Cross-model synthesis validation**: Replicate core findings using alternative synthesis models (e.g., Llama-3, GPT-4) to test model-family independence of the framework.
3. **Long-tail problem analysis**: Analyze the distribution of problem types retained vs. filtered across different quality control strategies to quantify coverage loss of rare but valid problem types.