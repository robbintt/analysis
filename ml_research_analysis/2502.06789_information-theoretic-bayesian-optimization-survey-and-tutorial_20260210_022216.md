---
ver: rpa2
title: 'Information-theoretic Bayesian Optimization: Survey and Tutorial'
arxiv_id: '2502.06789'
source_url: https://arxiv.org/abs/2502.06789
tags:
- optimization
- information
- bayesian
- entropy
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of information-theoretic
  Bayesian optimization methods, which use concepts like entropy and mutual information
  to guide optimization processes. The survey covers the evolution of these methods
  from early approaches like IAGO to recent developments such as Alpha Entropy Search
  and Joint Entropy Search.
---

# Information-theoretic Bayesian Optimization: Survey and Tutorial

## Quick Facts
- arXiv ID: 2502.06789
- Source URL: https://arxiv.org/abs/2502.06789
- Reference count: 40
- Primary result: Information-theoretic acquisition functions outperform traditional methods by considering global uncertainty and information gain about the optimum

## Executive Summary
This paper provides a comprehensive survey of information-theoretic Bayesian optimization methods that use concepts like entropy and mutual information to guide the optimization process. The survey traces the evolution from early approaches like IAGO to recent developments including Alpha Entropy Search and Joint Entropy Search. These methods are shown to achieve better exploration-exploitation balance than traditional acquisition functions by considering global information from the entire predictive distribution rather than just local information. The paper demonstrates how these methods can be adapted to complex scenarios including multi-objective, constrained, parallel, and high-dimensional optimization problems, while also identifying key advantages and future research directions.

## Method Summary
The survey covers Bayesian optimization using Gaussian processes as surrogate models, where information-theoretic acquisition functions maximize expected information gain about the optimizer location or optimal value. Key methods include Predictive Entropy Search (PES), Max-value Entropy Search (MES), and Joint Entropy Search (JES), which use mutual information to select evaluation points. The core approach involves iteratively fitting a GP to observed data, computing acquisition functions based on entropy reduction, and selecting the next point to evaluate. MES is highlighted for its tractable closed-form approximation using sampled optima. The methods are evaluated on synthetic test functions and real-world applications like hyperparameter tuning in machine learning.

## Key Results
- Information-theoretic acquisition functions typically outperform traditional methods like Expected Improvement across benchmark problems
- MES provides a computationally efficient alternative to PES by targeting information about optimal values rather than optimizer locations
- JES combines information about both optimizer location and optimal value but requires more complex computations
- Information-theoretic methods show promise in constrained and multi-objective optimization scenarios
- Performance degrades significantly in high-dimensional spaces (>8D) without specialized adaptations

## Why This Works (Mechanism)

### Mechanism 1: Global Uncertainty Quantification
Information-theoretic acquisition functions achieve better exploration-exploitation balance by considering the full predictive distribution rather than local information at individual points. This allows them to identify high-value regions that local methods like Expected Improvement might miss, especially in multi-modal landscapes where multiple promising regions exist.

### Mechanism 2: Information Gain About the Optimum
These methods maximize mutual information between candidate evaluations and the optimum location/value, systematically reducing uncertainty about the solution. By selecting points where observing them is expected to most reduce entropy about either the optimizer location or optimal value, the methods converge more efficiently to the true optimum.

### Mechanism 3: Symmetry-Based Computational Simplification
Exploiting mutual information symmetry (I(X,Y) = I(Y,X)) transforms intractable entropy computations into tractable forms. PES reformulates the problem from reducing entropy about the optimizer distribution to reducing entropy about the optimal value given candidate points, where the first term is analytically available and the second involves conditioning on discrete optimum samples.

## Foundational Learning

- **Gaussian Process Posterior Predictive Distribution**: Why needed - All information-theoretic acquisition functions require computing entropies from the GP posterior; understanding how μ(x*) and σ²(x*) derive from the kernel and observed data is foundational. Quick check - Given a GP with RBF kernel, how does the posterior variance change at a point far from all observations?

- **Differential Entropy and Mutual Information**: Why needed - Section 3 derives entropy H(X) and mutual information I(X,Y) as the theoretical basis for all surveyed methods; without this, Equations 9-11 and subsequent acquisition functions are opaque. Quick check - If two random variables X and Y are independent, what is I(X,Y)?

- **Monte Carlo Estimation for Intractable Expectations**: Why needed - Methods like IAGO, MES, and JES all approximate intractable integrals via sampling; understanding sampling requirements and error bounds is crucial. Quick check - How many samples are typically needed to estimate an expectation with error < ε?

## Architecture Onboarding

- **Component map**: Surrogate Model -> Acquisition Function -> Optimum Sampler -> Acquisition Optimizer -> Loop Controller
- **Critical path**: Initialize with n₀ observations → fit GP → sample L candidate optima → compute α(x) for candidates → select x_next = argmax → evaluate f(x_next) → update data → repeat
- **Design tradeoffs**: PES vs MES (information about location vs value with complexity differences); JES vs separate methods (combined information vs implementation complexity); discretization resolution (accuracy vs scalability)
- **Failure signatures**: EI-like exploitation bias (concentration on local improvements); high-dimensional collapse (>8D degradation); constraint infeasibility (over-exploration of infeasible regions)
- **First 3 experiments**: 1) Benchmark MES vs EI on synthetic 2D Branin function; 2) Ablate number of optimum samples K in MES on multi-modal 1D function; 3) Test PES on constrained optimization with known feasible region

## Open Questions the Paper Calls Out

- **Open Question 1**: Can generalized notions of entropy, such as Sharma-Mittal entropy, be utilized to develop more general acquisition functions? The authors explicitly list exploring generalized notions of entropy as a line for innovation, but generalization requires new theoretical derivations for acquisition function approximations.

- **Open Question 2**: What are the theoretical convergence guarantees, such as upper bounds on cumulative regret, for the expanding array of information-theoretic acquisition functions? The paper identifies studying theoretical guarantees of convergence to determine regret bounds as useful research direction, but the field has focused on empirical performance rather than rigorous theoretical bounds.

- **Open Question 3**: How can information compression techniques enhance the scalability of these methods to compete with metaheuristics in problems with cheaper evaluations? The conclusion suggests enhancing scalability through information compression techniques is worth exploring to compete with metaheuristics, but adapting them for "cheaper evaluation" scenarios remains an open challenge.

## Limitations

- Comparative performance claims lack systematic benchmarking across diverse function classes and implementation differences
- High-dimensional scalability remains severely limited (>8D) with minimal practical guidance on when to switch approaches
- Limited discussion of computational complexity trade-offs, particularly the O(N⁴) scaling of PES methods
- Claims about superior performance in constrained and multi-objective scenarios have limited empirical validation

## Confidence

- **High confidence**: Mathematical foundations of entropy and mutual information in Bayesian optimization (Section 3) are rigorously derived and internally consistent
- **Medium confidence**: Information-theoretic acquisition functions outperform traditional methods based on aggregated evidence from multiple papers though lacking unified benchmarking
- **Low confidence**: Claims about superior performance in constrained and multi-objective scenarios due to limited empirical validation

## Next Checks

1. Implement and benchmark MES vs EI on synthetic test functions: Create unified experimental framework comparing MES and EI on Branin, Hartmann6, and Hartmann6 with constraints across 50 iterations to verify exploration behavior and regret curves

2. Systematically evaluate dimensionality scaling: Test information-theoretic methods on synthetic functions from 2D to 12D to quantify performance degradation and identify precise dimensionality threshold where methods become ineffective

3. Conduct ablation study on computational components: Measure runtime contributions of Monte Carlo sampling, GP hyperparameter optimization, and acquisition optimization in PES/MES to determine which components drive O(N⁴) complexity and test approximation strategies