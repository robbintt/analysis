---
ver: rpa2
title: 'Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent
  Collaboration'
arxiv_id: '2511.19417'
source_url: https://arxiv.org/abs/2511.19417
tags:
- reasoning
- perceiver
- multimodal
- agent
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of extending large language models
  (LLMs) to multimodal reasoning without costly retraining. The authors propose BEMYEYES,
  a modular multi-agent framework that orchestrates collaboration between efficient
  vision-language models (VLMs) as perceivers and powerful LLMs as reasoners through
  multi-turn conversations.
---

# Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration

## Quick Facts
- **arXiv ID**: 2511.19417
- **Source URL**: https://arxiv.org/abs/2511.19417
- **Reference count**: 40
- **Primary result**: A modular multi-agent framework enabling text-only LLMs to perform multimodal reasoning by orchestrating collaboration between VLMs (perceivers) and LLMs (reasoners), achieving 7.1% and 12.1% accuracy improvements on MMMU-Pro and MathVision benchmarks respectively.

## Executive Summary
This paper introduces BEMYEYES, a modular multi-agent framework that extends text-only large language models (LLMs) to multimodal reasoning without costly retraining. The approach decouples perception and reasoning into specialized agents: efficient vision-language models (VLMs) act as perceivers that process visual inputs and generate textual descriptions, while frozen LLMs serve as reasoners that apply their knowledge to solve tasks based on these descriptions. A data synthesis pipeline generates synthetic training data through role-play simulations, fine-tuning perceiver agents for effective collaboration. Experiments demonstrate significant performance improvements across multiple benchmarks, with the framework showing robustness across different model families and generalization to specialized domains like medical reasoning.

## Method Summary
BEMYEYES orchestrates collaboration between a small VLM (perceiver) and a frozen LLM (reasoner) through multi-turn conversations. The framework generates synthetic training data by having GPT-4o role-play both agents, creating multimodal questions paired with images, answers, and simulated conversations. The perceiver is fine-tuned on successful conversations to improve context-aware visual description and role-consistent communication. At inference, the orchestration layer manages turn-taking, system prompts, termination conditions, and answer extraction. The approach enables text-only LLMs to perform multimodal reasoning by converting visual information into textual descriptions that the LLM can process.

## Key Results
- BEMYEYES achieves 49.9% accuracy on MMMU-Pro, outperforming GPT-4o's 42.8% and Gemini-2.0-Flash's 38.9%
- On MathVision, the framework reaches 49.6% accuracy compared to GPT-4o's 37.5% and Gemini-2.0-Flash's 28.7%
- DeepSeek-R1 paired with Qwen2.5-VL-7B perceiver outperforms large-scale proprietary VLMs like GPT-4o
- The framework generalizes to specialized domains, achieving 45.7% on medical image question answering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling perception and reasoning into specialized agents enables text-only LLMs to perform multimodal reasoning without parameter modification.
- Mechanism: A small VLM (perceiver) processes visual inputs and generates textual descriptions; a frozen LLM (reasoner) applies its knowledge and reasoning capabilities to solve tasks based solely on these descriptions. Role-specific system prompts establish each agent's expertise and constraints.
- Core assumption: Visual information can be sufficiently conveyed through natural language descriptions for complex reasoning tasks.
- Evidence anchors:
  - [abstract] "orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations"
  - [section 2.2-2.3] "The perceiver agent focuses on interpreting and conveying visual information, while the reasoner agent, a large, frozen LLM, leverages its advanced knowledge and reasoning capabilities"
  - [corpus] Related work on role-specialized medical collaboration (MAM) supports generalization of this pattern
- Break condition: Visual information is too complex, abstract, or spatial for effective textual description (filtered out in pipeline).

### Mechanism 2
- Claim: Multi-turn iterative dialogue enables clarification, error correction, and progressive refinement of visual grounding.
- Mechanism: The reasoner actively queries the perceiver for specific details, requests clarifications when descriptions are ambiguous, and refines reasoning based on updated information. This continues until a maximum turn limit or task completion.
- Core assumption: Reasoning agents can identify what visual information they lack and formulate appropriate queries.
- Evidence anchors:
  - [section 2.3] "the reasoner agent is instructed to request clarifications or additional descriptions from the perceiver agent when necessary"
  - [Table 4] Single-turn ablation shows consistent performance drops across all benchmarks (e.g., 49.9 → 48.3 on MMMU-Pro)
  - [corpus] Related multi-agent debate work (Du et al., 2023) demonstrates similar benefits from iterative exchange
- Break condition: Conversation exceeds turn budget without convergence, or perceiver cannot answer follow-up queries.

### Mechanism 3
- Claim: Supervised fine-tuning on synthetic collaborative conversations improves the perceiver's ability to coordinate with reasoners, beyond standalone perception gains.
- Mechanism: GPT-4o role-plays both agents to generate synthetic multi-turn conversations; successful conversations (correct final answers) are used to fine-tune the perceiver on context-aware visual description and role-consistent communication.
- Core assumption: Strong VLMs can simulate effective perceiver-reasoner interactions that transfer to smaller models.
- Evidence anchors:
  - [section 2.5-2.6] "12,145 multimodal questions, each paired with corresponding images, answers, and simulated conversations"
  - [Table 4] SFT improves BEMYEYES performance (e.g., 49.1 → 49.9 on MMMU-Pro) while standalone VLM shows minimal improvement (39.8 → 40.6)
  - [corpus] Distillation approaches in related work show similar patterns of collaboration-specific learning
- Break condition: Synthetic data distribution diverges significantly from target domain; role-play simulations exhibit systematic biases.

## Foundational Learning

- Concept: **Multi-agent orchestration with role specialization**
  - Why needed here: The framework relies on clear role boundaries and communication protocols between agents.
  - Quick check question: Can you explain how to design system prompts that prevent role confusion between agents?

- Concept: **Vision-language model architectures and tokenization**
  - Why needed here: Selecting and configuring appropriate VLMs as perceiver agents requires understanding their capabilities and limitations.
  - Quick check question: What is the difference between a VLM with native vision encoding versus a text-only LLM?

- Concept: **Supervised fine-tuning with conversational data**
  - Why needed here: Training perceivers requires formatting multi-turn dialogue as training sequences with proper context conditioning.
  - Quick check question: How do you structure training examples when the target includes responses conditioned on prior conversation history?

## Architecture Onboarding

- Component map:
  - **Perceiver Agent**: Small VLM (Qwen2.5-VL-7B, InternVL3-8B) — processes image + text, generates descriptions, answers reasoner queries
  - **Reasoner Agent**: Frozen LLM (DeepSeek-R1, GPT-4) — receives only text, performs reasoning, queries perceiver
  - **Orchestration Layer**: Manages turn-taking, system prompts, termination conditions, answer extraction
  - **Data Synthesis Pipeline**: GPT-4o role-play → conversation generation → filtering → SFT dataset

- Critical path:
  1. Define role-specific system prompts (perceiver aware of reasoner's blindness; reasoner aware of perceiver's limitations)
  2. Run data synthesis: question generation → conversation simulation (max 8 samples) → filtering (answerable, correct)
  3. Fine-tune perceiver on all its turns conditioned on dialogue history (lr=5e-6, 3 epochs)
  4. At inference: orchestrate multi-turn dialogue (max 5 turns), extract final answer from perceiver

- Design tradeoffs:
  - **Perceiver size vs. efficiency**: Smaller VLMs are cheaper to fine-tune but may have weaker perception
  - **Turn budget vs. accuracy**: More turns enable refinement but increase latency and cost
  - **Synthetic data source**: GPT-4o produces higher-quality data but is expensive; weaker models may introduce noise

- Failure signatures:
  - **Role confusion**: Perceiver attempts reasoning; reasoner hallucinates visual details
  - **Redundant exchanges**: Agents repeat information without progress (mitigated by SFT)
  - **Perception cascade**: Perceiver misdescribes image → reasoner builds on incorrect premises
  - **Non-convergence**: Max turns reached without confident answer

- First 3 experiments:
  1. **Baseline validation**: Run text-only LLM and standalone VLM on target benchmark to establish gaps; verify BEMYEYES recovers/improves performance.
  2. **Ablation: turn budget**: Compare 1-turn vs. 3-turn vs. 5-turn settings to quantify iterative dialogue contribution on your task domain.
  3. **Perceiver swap test**: Replace default perceiver with alternative VLM (e.g., InternVL3-8B) using same SFT data to assess modularity and model-family robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating a reinforcement learning (RL)-based training pipeline further enhance the collaborative effectiveness and performance of the perceiver and reasoner agents beyond the current supervised fine-tuning (SFT) approach?
- **Basis in paper:** [explicit] The authors state in the Limitations and Future Work section: "given the recent success of reinforcement learning (RL) in improving multimodal reasoning... incorporating an RL-based training pipeline may further improve the effectiveness of BeMyEyes, an exciting direction we leave for future work."
- **Why unresolved:** The current framework relies exclusively on supervised fine-tuning using synthetic data generation; the potential for RL to optimize the interaction dynamics or reward structures between agents remains unexplored.
- **What evidence would resolve it:** Implementing an RL loop (e.g., RLHF or DPO) to train the perceiver agent based on the reasoner's success signals and comparing the resulting accuracy and convergence speed against the SFT baseline on benchmarks like MMMU-Pro.

### Open Question 2
- **Question:** How effectively does the framework generalize to non-visual modalities such as audio and video, where perception requires processing temporal sequences rather than static images?
- **Basis in paper:** [explicit] The authors note: "our experiments focus exclusively on vision as the target modality, although the framework is general and can naturally be extended to other modalities such as audio and video."
- **Why unresolved:** The current study validates the framework only on image-based tasks (charts, diagrams, medical imagery). It is unclear if the multi-turn conversational format is efficient for describing temporal data or if the synthetic data pipeline works for audio/video generation.
- **What evidence would resolve it:** Adapting the perceiver agent to an audio/video encoder (e.g., Video-LLaMA) and evaluating performance on video reasoning benchmarks (e.g., Video-MME) using the same orchestration mechanism.

### Open Question 3
- **Question:** What is the performance upper bound of this modular approach compared to a hypothetical, natively multimodal reasoning model of equivalent scale?
- **Basis in paper:** [explicit] The paper states: "a comparison against an upper bound established by a hypothetical, fully-trained multimodal DeepSeek-R1 would provide deeper insights into the effectiveness of our approach; however, developing such large-scale models remains beyond our current scope."
- **Why unresolved:** It is currently unknown if the "textual bottleneck" (converting vision to text description) incurs a significant loss of information or reasoning capability compared to a model that processes visual embeddings directly within its hidden states.
- **What evidence would resolve it:** Training a monolithic, native multimodal version of DeepSeek-R1 (or a comparable frontier model) on the same data and comparing its performance on MMMU/MathVision against the BeMyEyes configuration.

### Open Question 4
- **Question:** How can the framework be made robust against error propagation where the reasoner agent is misled by hallucinations or omission errors from the perceiver agent?
- **Basis in paper:** [inferred] In Section 4.1 (Error Breakdown), the authors identify a failure group where "the fully multimodal reasoner would answer correctly in isolation, but is misled when reasoning jointly with the perceiver, often due to perception or communication errors."
- **Why unresolved:** The current system relies on the reasoner to query the perceiver, but lacks a mechanism for the reasoner to verify the veracity of the perceiver's descriptions or challenge potential hallucinations.
- **What evidence would resolve it:** Introducing a verification mechanism (e.g., asking the perceiver for raw bounding box coordinates or confidence scores) and measuring the reduction in the "misled" error group identified in the error breakdown analysis.

## Limitations
- The framework's performance depends heavily on the quality of synthetic data and role-specific system prompts, which are not fully specified in the paper
- The approach assumes visual information can be effectively conveyed through text descriptions, which may not hold for highly abstract or spatial reasoning tasks
- The paper does not address potential biases introduced by synthetic data or the scalability of data synthesis for larger domains

## Confidence
- **High Confidence**: The core architectural insight that decoupling perception and reasoning into specialized agents enables text-only LLMs to perform multimodal reasoning without parameter modification is well-supported by experimental results across multiple benchmarks and model families.
- **Medium Confidence**: The claim that supervised fine-tuning on synthetic collaborative conversations improves perceiver coordination beyond standalone perception gains is supported by ablation studies, but the long-term stability and generalization of these improvements across diverse domains remains uncertain.
- **Low Confidence**: The assertion that the framework generalizes well to specialized domains like medical reasoning is based on a single case study (MAM) and lacks systematic evaluation across multiple specialized domains.

## Next Checks
1. **Prompt Template Validation**: Replicate the synthetic data generation process using the disclosed methodology to verify that the quality of generated conversations matches the paper's claims. Compare performance when using different prompt templates or weaker VLM models for role-play.
2. **Cross-Domain Robustness Test**: Apply the framework to a new specialized domain (e.g., legal reasoning or scientific visualization) and evaluate whether the same data synthesis and fine-tuning approach transfers successfully. Document any domain-specific modifications required.
3. **Bias and Distribution Shift Analysis**: Analyze the synthetic training data for systematic biases or distribution shifts compared to real-world multimodal reasoning tasks. Measure performance degradation when applying the fine-tuned perceiver to out-of-distribution examples.