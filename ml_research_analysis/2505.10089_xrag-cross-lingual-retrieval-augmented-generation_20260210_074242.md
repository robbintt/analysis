---
ver: rpa2
title: 'XRAG: Cross-lingual Retrieval-Augmented Generation'
arxiv_id: '2505.10089'
source_url: https://arxiv.org/abs/2505.10089
tags:
- question
- answer
- english
- articles
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XRAG, a benchmark for evaluating cross-lingual
  retrieval-augmented generation (RAG) in scenarios where user queries and retrieved
  documents are in different languages. The benchmark covers both monolingual retrieval
  (English documents for non-English queries) and multilingual retrieval (documents
  in both English and the query language), using challenging questions generated from
  recent news articles requiring cross-document reasoning.
---

# XRAG: Cross-lingual Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.10089
- Source URL: https://arxiv.org/abs/2505.10089
- Authors: Wei Liu, Sony Trenous, Leonardo F. R. Ribeiro, Bill Byrne, Felix Hieber
- Reference count: 40
- Key outcome: Introduces benchmark revealing cross-lingual RAG challenges: response language correctness (1-12% error) in monolingual retrieval and reasoning across languages in multilingual settings

## Executive Summary
This paper introduces XRAG, a benchmark for evaluating cross-lingual retrieval-augmented generation (RAG) where queries and retrieved documents may be in different languages. The benchmark covers two settings: monolingual retrieval (English documents for non-English queries) and multilingual retrieval (documents in both English and the query language), using questions generated from recent news articles requiring cross-document reasoning. Human evaluation confirms the questions are difficult, with human performance at 85% compared to LLMs at 55-63%. The benchmark reveals two key challenges: models struggle with generating responses in the correct language for monolingual retrieval, and face difficulties in reasoning across languages for multilingual retrieval.

## Method Summary
The XRAG benchmark uses a three-step pipeline with GPT-4o-2024-08-06 to generate questions from News Crawl articles (June-November 2024) and Wiki 2024 events. First, summaries are generated from pairs of related articles. Second, simple Q&A pairs are created from these summaries. Third, cross-document questions requiring multi-hop reasoning are synthesized. The dataset covers 5 languages (English, German, Spanish, Chinese, Arabic) with 1000 monolingual and 300 multilingual questions per language. Human QA verification, professional translation, and distractor retrieval via BGE-M3 retriever ensure quality. Evaluation uses an LLM-as-a-Judge panel (GPT-4o, Claude 3.5 Sonnet, Mistral-large) with majority voting and lingua library for language detection.

## Key Results
- Response Language Correctness (RLC) is a major failure mode, with models incorrectly responding in English 1-12% of the time in monolingual retrieval
- Human performance on XRAG is 85% accuracy, while evaluated LLMs achieve only 55-63% accuracy
- In multilingual retrieval, the primary challenge is reasoning across languages rather than generating non-English text
- Parametric knowledge leakage is effectively controlled, with accuracy below 16% when no documents are provided

## Why This Works (Mechanism)
None

## Foundational Learning
- Cross-lingual RAG: Retrieval-augmented generation where query and document languages differ. Why needed: To evaluate real-world scenarios where users ask questions in their native language but relevant information exists in multiple languages.
- Response Language Correctness (RLC): Models generating responses in wrong language (e.g., English instead of German). Why needed: Previously unreported failure mode that significantly impacts user experience in cross-lingual applications.
- Multi-hop reasoning: Questions requiring synthesis across multiple documents. Why needed: Ensures benchmark measures true retrieval and reasoning capabilities rather than simple lookup.
- LLM-as-a-Judge: Using LLMs to evaluate answer correctness and language adherence. Why needed: Scalable evaluation method for multilingual benchmarks where human annotation is expensive.
- Professional translation with entity-awareness: Translating questions while preserving named entities. Why needed: Maintains factual consistency across languages while respecting cultural naming conventions.

## Architecture Onboarding

**Component Map**: News Crawl articles → Summary generation → Simple QA creation → Cross-document question synthesis → Human verification → Professional translation → Distractor retrieval (BGE-M3) → LLM judge evaluation (GPT-4o, Claude 3.5, Mistral-large) → lingua language detection

**Critical Path**: Article collection → Question generation pipeline → Human verification → Translation → Evaluation

**Design Tradeoffs**: Fixed 1000+300 question split per language balances comprehensive evaluation with manageable annotation costs; LLM-as-a-Judge provides scalability but may introduce consistency concerns.

**Failure Signatures**: Response Language Correctness errors (1-12% of cases); questions answerable without retrieval (<16% accuracy baseline); language detection failures in evaluation.

**First Experiments**:
1. Test all questions against LLMs with no documents to verify parametric knowledge leakage stays below 20%
2. Run lingua language detection on all model outputs to quantify RLC error rates
3. Verify distractor articles are topically relevant but contain contradictory information

## Open Questions the Paper Calls Out
- How does LLM performance change when cross-lingual RAG requires integrating information from more than two languages simultaneously? (explicit)
- What specific prompting or architectural adjustments can effectively mitigate Response Language Correctness (RLC) errors? (explicit)
- How does the ratio of distracting documents to supporting documents affect cross-lingual reasoning accuracy? (explicit)

## Limitations
- BGE-M3 retriever configuration lacks detailed parameters (embedding dimensions, chunk sizes, similarity thresholds)
- Professional translation workflow specifics beyond "entity-aware" handling are not documented
- LLM-as-a-Judge consistency and potential bias across different models is uncertain

## Confidence
**High Confidence**:
- RLC identification as key failure mode with 1-12% error rates
- Human vs. LLM performance gap (85% vs 55-63%)
- Parametric knowledge leakage control (<16% accuracy without retrieval)

**Medium Confidence**:
- Characterization of monolingual vs. multilingual retrieval difficulties
- 1000+300 question split provides sufficient coverage
- Three-step pipeline produces consistently challenging questions

## Next Checks
1. Reproduce distractor selection using BGE-M3 with documented parameter settings and verify topical relevance vs. contradictory information
2. Implement comprehensive language detection using lingua library on all model outputs across all five languages
3. Systematically test all benchmark questions against evaluated models without documents to verify accuracy remains below 20%