---
ver: rpa2
title: 'Towards On-Device Personalization: Cloud-device Collaborative Data Augmentation
  for Efficient On-device Language Model'
arxiv_id: '2508.21313'
source_url: https://arxiv.org/abs/2508.21313
tags:
- data
- user
- on-device
- personalized
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying personalized language
  models on resource-constrained edge devices, where storage and computational limitations
  hinder both personalization and performance. The proposed method, CDCDA-PLM, introduces
  a cloud-device collaborative framework that leverages a powerful cloud-based LLM
  to augment users' limited local data with synthetic, personalized samples.
---

# Towards On-Device Personalization: Cloud-device Collaborative Data Augmentation for Efficient On-device Language Model

## Quick Facts
- arXiv ID: 2508.21313
- Source URL: https://arxiv.org/abs/2508.21313
- Reference count: 40
- Primary result: Cloud-device collaborative framework achieves on-device personalization with 80% storage reduction and 3× speed improvements

## Executive Summary
This paper addresses the challenge of deploying personalized language models on resource-constrained edge devices by introducing a cloud-device collaborative framework called CDCDA-PLM. The method leverages a powerful cloud-based LLM to augment users' limited local data with synthetic, personalized samples, which are then filtered and used to fine-tune a small on-device model via parameter-efficient fine-tuning (LoRA). Experiments across six personalization tasks demonstrate that CDCDA-PLM significantly outperforms existing baselines, achieving performance close to cloud-based models while enabling fast, offline inference on-device.

## Method Summary
The proposed method, CDCDA-PLM, introduces a cloud-device collaborative framework that addresses the challenge of on-device personalization by leveraging cloud resources for data augmentation. The approach begins by collecting a user's limited local data, which is then sent to a cloud-based LLM for augmentation. The cloud model generates synthetic, personalized samples based on the user's data, which are filtered using semantic consistency, diversity, and length criteria. These filtered samples are then used to fine-tune a small on-device model via parameter-efficient fine-tuning (LoRA), enabling efficient personalization while minimizing storage and computational requirements. The framework achieves significant performance improvements over existing baselines while reducing storage by over 80% and decoding speed by up to 3× compared to cloud-based models.

## Key Results
- Outperforms existing baselines across six personalization tasks
- Achieves performance close to cloud-based models while enabling offline inference
- Reduces storage by over 80% and decoding speed by up to 3× compared to cloud-based models

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of personalization on resource-constrained devices through a collaborative approach. By leveraging a powerful cloud-based LLM for data augmentation, the framework overcomes the limitation of users' limited local data. The synthetic samples generated by the cloud model are specifically tailored to the user's context, ensuring personalization. The filtering mechanism ensures that only high-quality, diverse samples are used for fine-tuning, preventing overfitting and maintaining model performance. The use of parameter-efficient fine-tuning (LoRA) further reduces the computational burden on the device, enabling efficient on-device personalization without compromising performance.

## Foundational Learning

1. **Parameter-efficient fine-tuning (LoRA)**
   - Why needed: Reduces computational and storage requirements for fine-tuning large models on resource-constrained devices
   - Quick check: Verify that LoRA adapters can be efficiently loaded and unloaded without affecting base model parameters

2. **Semantic consistency filtering**
   - Why needed: Ensures that augmented data maintains the user's original intent and context while preventing overfitting
   - Quick check: Evaluate filtering performance across different domains and data distributions

3. **Data augmentation for personalization**
   - Why needed: Addresses the challenge of limited user data for effective personalization on edge devices
   - Quick check: Measure the impact of augmented data size on personalization performance

4. **Cloud-device collaborative framework**
   - Why needed: Leverages cloud resources for computationally intensive tasks while maintaining on-device inference capabilities
   - Quick check: Assess the trade-off between cloud communication overhead and performance gains

## Architecture Onboarding

**Component Map:**
User Data -> Cloud LLM -> Semantic Filter -> LoRA Fine-tuning -> On-device Model

**Critical Path:**
1. Local data collection and preprocessing
2. Cloud-based data augmentation and filtering
3. On-device LoRA fine-tuning
4. Inference on edge device

**Design Tradeoffs:**
- Cloud vs. on-device computation balance
- Data privacy vs. personalization quality
- Storage efficiency vs. model performance
- Communication overhead vs. augmentation quality

**Failure Signatures:**
- Poor personalization performance indicates inadequate local data or ineffective augmentation
- High storage usage suggests inefficient LoRA implementation
- Slow inference indicates computational bottlenecks on device

**First 3 Experiments:**
1. Evaluate storage reduction and inference speed on actual edge hardware
2. Test semantic consistency filtering across diverse domains and languages
3. Assess performance with varying amounts of local data and user distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation focuses on public datasets rather than real-world deployment scenarios
- Assumes availability of a powerful cloud-based LLM for augmentation
- Does not address power consumption and battery life impact on actual edge devices
- Semantic consistency filtering may not generalize across all domains

## Confidence
- Performance improvements over baselines: High
- Storage reduction claims: Medium (based on reported parameter counts but lacking real-device validation)
- Speed improvements: Medium (simulation-based rather than measured on actual devices)
- Effectiveness of semantic filtering: Medium (shown effective for tested tasks but may not generalize)

## Next Checks
1. Conduct real-device deployment tests measuring actual memory usage, inference latency, and power consumption across different edge hardware platforms
2. Evaluate the framework's performance with varying amounts of local data and different user data distributions to assess robustness
3. Test the semantic consistency filtering approach on additional domains and languages to validate generalization capabilities