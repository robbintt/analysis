---
ver: rpa2
title: 'DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil
  Engineering'
arxiv_id: '2507.11527'
source_url: https://arxiv.org/abs/2507.11527
tags:
- tasks
- task
- llms
- instructions
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DrafterBench, an open-source benchmark for
  evaluating large language model (LLM) agents on technical drawing revision tasks
  in civil engineering. The benchmark addresses the gap in industrial-specific evaluation
  by providing 1,920 drawing revision tasks with 46 customized tools, derived from
  real-world engineering documents.
---

# DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering

## Quick Facts
- **arXiv ID:** 2507.11527
- **Source URL:** https://arxiv.org/abs/2507.11527
- **Reference count:** 24
- **Primary result:** OpenAI o1 achieved the highest average score of 79.90 on DrafterBench, with Claude 3.5 Sonnet and Deepseek-v3-685B following closely.

## Executive Summary
This paper introduces DrafterBench, an open-source benchmark for evaluating large language model (LLM) agents on technical drawing revision tasks in civil engineering. The benchmark addresses the gap in industrial-specific evaluation by providing 1,920 drawing revision tasks with 46 customized tools, derived from real-world engineering documents. The benchmark tests four core capabilities: structured data comprehension, function execution, instruction following, and critical reasoning. Experimental results on six state-of-the-art LLMs show OpenAI o1 achieving the highest average score of 79.90, with significant performance variations across task difficulty parameters.

## Method Summary
DrafterBench provides 1,920 technical drawing revision tasks derived from real-world civil engineering documents. Tasks are stratified by six parameters (language style, value specificity, instruction completeness, objects count, operations count, task category) to control complexity. The benchmark uses a single-turn code generation approach where agents receive instructions and file states, then generate Python code calling 46 customized PDF manipulation tools. A dual-tool system records operation paths without modifying files, and evaluation compares recorded paths against ground truth rather than visual output. Performance is scored across six subtasks with false-positive penalties and weak-point consideration.

## Key Results
- OpenAI o1 achieved the highest average score of 79.90, with Claude 3.5 Sonnet and Deepseek-v3-685B following closely.
- Models showed consistent ~20% gap between plan execution accuracy (~40-50%) and other subtasks (~60-75%) across all tested models.
- Vague details caused 5-12% performance degradation, while incomplete instructions caused ~18% degradation for most models (OpenAI o1 showed resilience).
- Instruction completeness was the primary factor affecting recording function usage, with models struggling to follow new system prompt policies.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Tool Operation Recording
- **Claim:** Recording operation paths rather than comparing output drawings provides more accurate evaluation of LLM agent performance in technical drawing tasks.
- **Mechanism:** Dual tools mirror original function signatures but capture arguments and operation sequences without modifying files. The recorded paths are compared against ground truth, eliminating noise from coding style variations and detecting "unclean" solutions that produce correct outputs through incorrect means.
- **Core assumption:** The operation path (sequence of function calls with parameters) is a more reliable indicator of agent capability than final visual output, since multiple incorrect paths can converge to visually identical results.
- **Evidence anchors:**
  - [Section 2, Page 2]: "It is difficult to assess the quality of the LLMs' performance directly from the revised drawings. This is because not all steps in a solution make any visible changes, resulting in 'what can be seen is not what has been done'."
  - [Section 3.2, Page 6]: "They do not modify the drawing but record the argument details and the ground operations path, following the rules, such as repeated operations being recorded as one."
  - [corpus]: ITBench employs systematic benchmarking methodology for IT automation tasks, suggesting operation-path evaluation has cross-domain validity.

### Mechanism 2: Six-Parameter Difficulty Stratification
- **Claim:** Controlling task complexity through discrete parameters enables systematic capability profiling across distinct failure modes in industrial automation.
- **Mechanism:** Tasks are stratified by: (1) language style (structured/unstructured), (2) value specificity (precise/vague), (3) instruction completeness (complete/incomplete), (4) objects count (single/multiple), (5) operations count (single/multiple), (6) task category (12 types). This creates 192 unique configurations with 5 samples each, derived from 100+ real-world revision documents.
- **Core assumption:** The six selected parameters capture the key difficulty dimensions distinguishing real-world industrial task performance from simplified benchmark performance.
- **Evidence anchors:**
  - [Section 3.1, Pages 4-5]: "The difficulty of each task varies greatly and is generally controlled by six parameters in four dimensions"
  - [Section 5.1, Pages 7-8]: Figure 5 shows performance degradation patternsâ€”vague details cause 5-12% degradation; incomplete instructions cause ~18% degradation for most models (OpenAI o1 shows resilience).
  - [corpus]: CAMB (civil aviation maintenance benchmark) similarly employs multi-dimensional task categorization, supporting parameterized difficulty as effective methodology.

### Mechanism 3: Subtask-Level Error Attribution with Penalty Weighting
- **Claim:** Decomposing evaluation into six subtasks with false-positive penalties enables precise identification of capability gaps and discourages unsafe industrial behavior.
- **Mechanism:** Performance scored across: argument defining, variable transferring, function calling, single tool selection, multi-tool selection, and plan execution. Each uses TP-FP penalties (Equation 1) to discourage unexpected operations. Plan execution uses IoU scoring (Equation 2). Comprehensive score (Equation 3) penalizes uneven performance across task types.
- **Core assumption:** The consistent ~20% gap between plan execution accuracy (~40-50%) and other subtasks (~60-75%) across all models reflects a fundamental limitation in sequential attention, not a measurement artifact.
- **Evidence anchors:**
  - [Section 5.2, Page 8]: "There is an obvious gap between the accuracy of plan execution and the other five subtasks. It is interesting that the difference is constantly around 20% for all models."
  - [Section 4, Pages 6-7]: Equations 1-3 detail scoring with explicit FP penalties and weak-point consideration for industrial reliability.
  - [corpus]: Weak corpus evidence for this specific FP-penalty approach; most agent benchmarks focus on task completion rather than path correctness.

## Foundational Learning

- **Concept: Function-Calling Agents vs. ReAct-Style Agents**
  - **Why needed here:** DrafterBench explicitly avoids multi-turn ReAct interaction, preferring single-turn function calling via code generation. This design choice prioritizes industrial stability over interactive flexibility.
  - **Quick check question:** Given a multi-step drawing revision task, would you implement: (a) a chain of observe-think-act cycles with environment feedback, or (b) a single code block executing all operations in one turn?

- **Concept: Implicit Policy Integration**
  - **Why needed here:** Industrial tasks require agents to follow unstated conventions (e.g., saving files with specific naming patterns, logging incomplete instructions rather than executing anyway). The benchmark tests this via system prompt policies.
  - **Quick check question:** An instruction says "delete the dimension lines" without specifying save behavior. Should the agent save changes automatically?

- **Concept: Ground Truth Path vs. Ground Truth Output**
  - **Why needed here:** The benchmark evaluates operation sequences, not final drawings. This fundamental departure from output-based evaluation catches hidden errors that produce visually correct results.
  - **Quick check question:** If an agent produces a correct final drawing but includes an unnecessary "delete then undo" operation, should it receive full marks?

## Architecture Onboarding

- **Component map:** Task Generator -> Dual Tool Layer -> Tool Library (46 functions) -> Evaluation Engine -> Scoring System
- **Critical path:** 1. Receive preprocessed instruction + marker file 2. Parse instruction against system prompt's implicit policies 3. Generate single-turn code calling provided tools 4. Dual tools execute, recording operation path without modifying files 5. Compare recorded path against ground truth path for subtask-level scoring
- **Design tradeoffs:**
  - **Single-turn vs. multi-turn:** Single-turn is more efficient and stable for batch processing but less flexible for error recovery
  - **Path comparison vs. output comparison:** Path comparison catches hidden errors but requires ground truth paths for all tasks
  - **FP penalty vs. pure recall:** Penalizing unexpected operations aligns with industrial reliability but discourages creative solutions
- **Failure signatures:**
  - **Vague detail errors:** Agent fills placeholder text (e.g., `fontcolor="general color"`) instead of inferring reasonable value
  - **Incomplete instruction errors:** Agent asks user for clarification instead of logging via recording function as specified in system prompt
  - **Policy non-compliance:** Agent ignores new system prompt policies in favor of default/intrinsic behaviors
  - **Plan execution gap:** Consistent ~20% accuracy drop in plan execution vs. other subtasks across all tested models
- **First 3 experiments:**
  1. **Baseline capability profile:** Run full 1920-task benchmark on target model. Analyze breakdown by 6 parameters and 6 subtasks to identify specific weakness patterns (e.g., if vague details cause >10% degradation, focus on reasoning enhancement).
  2. **Prompt engineering iteration:** Modify default prompts (Appendix D templates) to strengthen implicit policy adherence for incomplete instructions. Measure impact on recording function usage and comprehensive score (Equation 3).
  3. **Tool complexity analysis:** Compare performance on simple-pipeline tasks (adding text) vs. graph-structure tasks (adding vectors) to determine if function-calling pipeline complexity correlates with plan execution gaps.

## Open Questions the Paper Calls Out

- **Question:** How can LLMs be enhanced to better infer reasonable values from vaguely defined instructions rather than executing literal text or using placeholders?
  - **Basis in paper:** [explicit] The authors note that when faced with vague details, current models often fill arguments with description words (e.g., setting `fontcolor="general color"`) or placeholders, rather than speculating on the user's exact intention.
  - **Why unresolved:** The paper highlights this as a necessary improvement for industrial application, but does not propose a specific mechanism to solve this "literal execution" tendency.
  - **What evidence would resolve it:** A study showing models successfully interpolating specific values (e.g., hex codes for "general color") on the DrafterBench vague-instruction subset without human intervention.

- **Question:** To what extent can LLMs be adapted to prioritize new, explicit domain policies over their intrinsic safety or behavioral policies?
  - **Basis in paper:** [explicit] The paper states that LLMs "perform quite stubbornly" when instructions conflict with intrinsic policies, often ignoring system information regarding error handling in favor of ingrained habits.
  - **Why unresolved:** While the paper identifies this rigidity as a barrier to integration in diverse industrial scenarios, it does not offer a solution for dynamically overriding intrinsic policies for low-risk tasks.
  - **What evidence would resolve it:** Experimentation showing a model adhering to a custom "delayed interaction" policy (logging errors) despite an intrinsic bias toward asking the user for clarification.

- **Question:** How does the automation capability of LLMs in civil engineering tasks generalize to non-English environments?
  - **Basis in paper:** [explicit] In the limitations section, the authors explicitly state, "Multilingual input will be included in future work to consider the ability of LLMs to automate industrial tasks in non-English environments."
  - **Why unresolved:** The current benchmark and experiments are strictly limited to English, leaving the cross-lingual robustness of these technical agents unknown.
  - **What evidence would resolve it:** Evaluation results from a multilingual version of DrafterBench showing performance metrics across different languages.

## Limitations
- The benchmark focuses exclusively on civil engineering drawing tasks, limiting generalizability to other domains.
- The single-turn evaluation approach may not capture the full capability of multi-turn ReAct-style agents that could perform better in complex, iterative tasks.
- The six difficulty parameters, while comprehensive for drawing tasks, may not generalize to other industrial automation domains.

## Confidence
- **High Confidence:** Task stratification methodology, dual-tool evaluation mechanism, and overall benchmark architecture are well-specified and reproducible. The observed ~20% plan execution gap across all models is a robust finding.
- **Medium Confidence:** Performance rankings among models are reliable within the drawing domain, but cross-domain generalization remains uncertain. The FP penalty approach's superiority over alternatives needs more validation.
- **Low Confidence:** The benchmark's ability to predict real-world industrial performance and the optimal difficulty parameter selection for domains beyond civil engineering.

## Next Checks
1. **Cross-Domain Applicability Test:** Apply the six-parameter difficulty framework to a non-drawing domain (e.g., mechanical engineering drawings or construction schedules) to verify parameter relevance and identify needed modifications.
2. **Multi-Turn vs. Single-Turn Comparison:** Implement a ReAct-style multi-turn evaluation on a subset of tasks to quantify the performance difference and determine if single-turn evaluation systematically underestimates agent capability.
3. **Ground Truth Generation Validation:** Implement and validate the automated ground truth path generation process on a held-out dataset to ensure it captures all valid solution paths and doesn't introduce systematic bias.