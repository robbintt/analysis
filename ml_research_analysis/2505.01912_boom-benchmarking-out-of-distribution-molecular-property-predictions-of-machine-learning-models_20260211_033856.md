---
ver: rpa2
title: 'BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine
  Learning Models'
arxiv_id: '2505.01912'
source_url: https://arxiv.org/abs/2505.01912
tags:
- property
- performance
- tasks
- molecular
- molecules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BOOM is the first systematic benchmark for out-of-distribution
  (OOD) molecular property prediction, enabling fair evaluation of generalization
  beyond training data. It defines OOD via concept/label shift in property space and
  provides 10 diverse quantum chemical and experimental datasets (QM9, 10k, Lipophilicity)
  with train/ID/OOD splits.
---

# BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models

## Quick Facts
- **arXiv ID**: 2505.01912
- **Source URL**: https://arxiv.org/abs/2505.01912
- **Reference count**: 40
- **Primary result**: BOOM is the first systematic benchmark for out-of-distribution (OOD) molecular property prediction, revealing no model achieves strong OOD generalization (top models show ~3× higher OOD RMSE than ID).

## Executive Summary
BOOM introduces the first systematic benchmark for out-of-distribution molecular property prediction, defining OOD via concept/label shift in property space using Kernel Density Estimation. The benchmark evaluates 15 models across 10 diverse datasets (QM9, 10k, Lipophilicity) with 150+ model-task combinations. Across all experiments, no model achieves strong OOD generalization—top performers show ~3× higher OOD RMSE than in-distribution performance. The study finds that high-inductive-bias models (3D GNNs like GotenNet, GeoFormer) outperform transformer-based foundation models on OOD tasks, supervised pretraining only helps when pretraining and finetuning tasks are correlated, and data augmentation with small OOD fractions improves generalization. These findings highlight OOD generalization as a critical frontier for chemical machine learning.

## Method Summary
BOOM defines OOD molecular property prediction via concept/label shift using KDE on property values, where lowest 10% probability samples form OOD test sets. The benchmark evaluates 15 models (Random Forest, MLP, ChemBERTa, MoLFormer, Regression Transformer, ModernBERT, Chemprop, EGNN, IGNN, TGNN, MACE, GotenNet, Graphormer, ET, GeoFormer) across 10 datasets from QM9, 10k Dataset, and Lipophilicity. Models are trained on standard splits with default hyperparameters, then evaluated using RMSE and binned R² for OOD correlation. The study includes pretraining ablation studies for foundation models, hyperparameter optimization sweeps, and data augmentation experiments with OOD samples.

## Key Results
- No model achieves strong OOD generalization: top models show ~3× higher OOD RMSE than ID performance
- High-inductive-bias models (3D GNNs like GotenNet, GeoFormer) perform best on OOD tasks, achieving top performance on 7/10 tasks
- Current chemical foundation models (MoLFormer, ChemBERTa, Regression Transformer) fail to extrapolate beyond training distributions
- Pretraining strategies improve ID performance (12-35%) but show negligible OOD improvements; supervised pretraining helps only when pretraining and finetuning tasks are correlated
- Hyperparameter tuning offers modest OOD gains (up to 60% RMSE reduction) without hurting ID performance
- Data augmentation with small OOD fractions (~4%) consistently improves generalization across tasks

## Why This Works (Mechanism)

### Mechanism 1
Models with high inductive bias (3D GNNs) achieve better OOD performance than transformer-based foundation models because 3D models with E(3) equivariant/invariant symmetries encode physical constraints (rotations, translations, reflections) that restrict the hypothesis space to physically plausible solutions, reducing overfitting to spurious training correlations. This works when physical symmetries encode relevant constraints for molecular property prediction. Evidence includes GotenNet achieving top performance on 7/10 tasks and 3D models showing lower OOD RMSE across nearly all tasks. However, electronic structure properties (HOMO, LUMO, Gap, µ) show poor OOD even for 3D models—authors hypothesize missing explicit electronic structure in representations.

### Mechanism 2
Supervised pretraining improves OOD generalization only when pretraining and finetuning tasks share correlated property distributions because transfer effectiveness depends on shared causal structure between tasks; uncorrelated pretraining may encode task-specific shortcuts that harm extrapolation. This works when Pearson correlation between property distributions proxies for shared underlying causal factors. Evidence shows significant degradation in OOD performance when pretraining task dataset is uncorrelated (Pearson correlation coefficient less than 0.35) and normalized OOD RMSE increases as pretraining-finetuning correlation decreases below ~0.5. Break conditions occur when correlation doesn't reflect causal relatedness or pretraining data contains systematic biases.

### Mechanism 3
Data augmentation with small fractions of OOD samples improves OOD generalization because including tail-distribution samples converts low-density regions into in-distribution regions, reducing concept shift magnitude. This works when learned structure-property relationships from augmented samples generalize to unobserved OOD samples. Evidence shows data augmentation consistently yields sizable generalization improvements even with small fractions (~4%) of augmented data and OOD RMSE decreases monotonically with more augmented examples for 7/8 QM9 tasks. Break conditions occur when augmented samples are unrepresentative of full OOD distribution or when property-function relationships are highly non-local.

## Foundational Learning

- **Concept: Out-of-Distribution (OOD) Generalization**
  - Why needed here: BOOM is the first systematic benchmark for molecular OOD prediction; understanding distribution shift is prerequisite.
  - Quick check question: Why do random train/test splits fail to measure OOD performance?

- **Concept: Concept/Label Shift vs Covariate Shift**
  - Why needed here: BOOM defines OOD via property-space (label) shift using KDE, not structure-space shift.
  - Quick check question: If you train on molecules with HOMO ∈ [-0.25, -0.15] Eh, what type of shift occurs when testing on HOMO ∈ [-0.35, -0.30] Eh?

- **Concept: E(3) Equivariance/Invariance**
  - Why needed here: Top-performing OOD models (GotenNet, MACE, EGNN) explicitly encode 3D rotational/translational symmetries.
  - Quick check question: Why should a molecular property predictor give the same output for a molecule rotated 90°?

- **Concept: Inductive Bias in Model Architecture**
  - Why needed here: Paper's central finding—high-inductive-bias models outperform large pretrained transformers on OOD tasks.
  - Quick check question: What inductive bias does a GNN have that a SMILES-based transformer lacks?

## Architecture Onboarding

- **Component map**: Property dataset -> KDE-based splitting -> Train/ID/OOD splits -> Model training -> RMSE and binned R² evaluation
- **Critical path**: Load property dataset → fit KDE → extract train/ID/OOD splits → train model on train split (default hyperparameters first) → evaluate RMSE and binned R² on both ID and OOD test splits → compute ID-OOD gap (ratio or difference) as primary OOD metric
- **Design tradeoffs**: 3D vs 2D representations (3D models achieve better OOD but require DFT-computed coordinates; 2D models scale to billions of molecules), Pretrained vs scratch (Pretraining improves ID by 12-35% but shows negligible OOD improvement), Property-space vs structure-space OOD (Property-space aligns with discovery; structure-space may miss property extremes)
- **Failure signatures**: S-shaped parity plots (models cluster OOD samples but cannot extrapolate beyond training range), High ID R², low OOD binned R² (Shortcut learning on ID features), Autoregressive token errors (Regression Transformer predicts "00913" for "0.913")
- **First 3 experiments**: Baseline OOD gap (Train GotenNet, ChemBERTa, and Random Forest on QM9 HOMO; report ID/OOD RMSE ratio. Expect: GotenNet < RF < ChemBERTa in OOD gap), Pretraining ablation (Train ChemBERTa with and without MLM pretraining on QM9 Gap; measure OOD binned R² change. Expect: pretraining improves ID but not OOD), Data augmentation sensitivity (Add N∈{0, 100, 1000} OOD samples to QM9 ZPVE training; plot OOD RMSE vs N. Expect: diminishing returns after ~1000 samples)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can explicitly incorporating electronic structure into molecular representations resolve the observed failure of models to generalize OOD on electronic properties (HOMO, LUMO, Gap)? This remains unresolved because no current model architecture achieved strong OOD performance on these specific electronic tasks, suggesting input representations lack necessary physical information. Evidence would be a model utilizing orbital features achieving high binned OOD R² on HOMO/LUMO tasks where current models fail.

- **Open Question 2**: What specific pretraining objectives or task correlations are required to transfer chemical knowledge that improves OOD generalization, given that standard masked language modeling fails? This remains unresolved because current foundation models rely on MLM/PLM objectives which appear to capture features that do not extrapolate to tail distributions. Evidence would be identification and validation of a pretraining protocol yielding statistically significant OOD error reductions across benchmark tasks.

- **Open Question 3**: Which specific architectural design choices in ModernBERT drive its superior OOD performance compared to other transformer-based chemical models? This remains unresolved because while improvement is empirically observed, the causal mechanism—whether due to rotary positional embeddings, pre-normalization, or other factors—is not isolated. Evidence would be an ablation study on ModernBERT's architectural components quantifying their individual impact on OOD RMSE.

- **Open Question 4**: Can the strong OOD performance of high-inductive-bias 3D models (like GotenNet) be maintained while scaling to the parameter and data sizes of large language models? This remains unresolved because there is a performance-scalability trade-off where best OOD models are smaller, physics-informed architectures, while scalable large transformers fail to extrapolate. Evidence would be a scaling law analysis demonstrating 3D equivariant models retain OOD accuracy advantages as parameter counts approach those of large language models.

## Limitations
- Benchmark focuses on property-space shifts which may not capture all real-world discovery scenarios where structure-property relationships change simultaneously
- KDE-based OOD split assumes property distributions are stationary, potentially missing more complex shift patterns
- Evaluation relies heavily on RMSE and binned R² metrics which may not fully capture model behavior in extreme extrapolation regions

## Confidence
- **High confidence**: Models with E(3) equivariant architectures achieve superior OOD performance; data augmentation consistently improves generalization; supervised pretraining benefits depend on task correlation
- **Medium confidence**: Superiority of high-inductive-bias models holds across diverse datasets, though performance varies by property type
- **Low confidence**: Generalization improvements from hyperparameter tuning and specific correlation threshold (0.35) for pretraining effectiveness require further validation

## Next Checks
1. **Property-specific validation**: Test whether BOOM findings hold for properties with stronger electronic structure dependence (e.g., polarizability, dipole moment) using models that explicitly encode electronic features
2. **Shift pattern expansion**: Evaluate model performance under covariate shift (structure-space OOD) to determine if BOOM's conclusions extend beyond property-space distribution shifts
3. **Multi-task correlation analysis**: Investigate whether supervised pretraining effectiveness scales with multi-task property correlation matrices rather than single-task Pearson coefficients