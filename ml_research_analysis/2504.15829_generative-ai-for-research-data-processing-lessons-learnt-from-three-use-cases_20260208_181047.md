---
ver: rpa2
title: 'Generative AI for Research Data Processing: Lessons Learnt From Three Use
  Cases'
arxiv_id: '2504.15829'
source_url: https://arxiv.org/abs/2504.15829
tags:
- generative
- data
- research
- https
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the use of generative AI, specifically Claude
  3 Opus, for research data processing across three use cases: extracting plant species
  names from historical seedlists, extracting data points from EU health technology
  assessment documents, and assigning industry codes to Kickstarter projects. The
  study evaluates accuracy and consistency by comparing AI outputs against manually
  constructed ground truths.'
---

# Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases

## Quick Facts
- arXiv ID: 2504.15829
- Source URL: https://arxiv.org/abs/2504.15829
- Authors: Modhurita Mitra; Martine G. de Vos; Nicola Cortinovis; Dawa Ometto
- Reference count: 40
- Primary result: Generative AI (Claude 3 Opus) with temperature=0 achieves high accuracy and consistency for research data extraction tasks when appropriate prompts are used.

## Executive Summary
This paper evaluates generative AI as a tool for processing unstructured research data across three distinct use cases: extracting plant species names from historical seedlists, extracting data points from EU health technology assessment documents, and assigning industry codes to Kickstarter projects. The study demonstrates that large language models can perform these complex, previously difficult-to-automate tasks with high accuracy and consistency when temperature is set to 0 and appropriate prompts are used. The findings suggest generative AI is suitable for large-scale data processing tasks that lack simple rule-based solutions, provided careful prompt engineering and evaluation are applied.

## Method Summary
The study uses Claude 3 Opus via API with temperature set to 0 for all experiments. For each use case, the authors developed specific prompts requesting JSON-formatted outputs to ensure machine-readability. PDF documents were processed using PyPDF for text extraction, with data chunking implemented to handle context length constraints. The pipeline included retry logic for network errors and post-processing to extract structured data from model responses. Ground truth datasets were manually constructed for validation, and consistency was evaluated by running the same prompts multiple times on identical inputs.

## Key Results
- Generative AI achieved high accuracy and consistency across all three use cases when temperature was set to 0
- Setting temperature to 0 was critical for ensuring objective, reproducible outputs in data extraction tasks
- JSON output format facilitated downstream processing and reduced post-processing requirements
- Prompt engineering was identified as the most time-consuming aspect of implementation
- The approach successfully handled unstructured text that was previously difficult to process with rule-based methods

## Why This Works (Mechanism)

### Mechanism 1: Temperature=0 for Consistency
Setting temperature to 0 forces the model to select the most probable next token, minimizing stochastic variability. This "exploitation" behavior ensures objective outputs for well-defined extraction tasks. The assumption is that the most statistically probable token sequence corresponds to the factually correct extraction. This approach would fail for creative generation tasks requiring diverse outputs.

### Mechanism 2: LLMs as Semantic Interpreters
Large language models use pre-trained linguistic patterns to interpret unstructured document structure and content, replacing complex rule-based logic. The prompt acts as a "soft program" mapping input text to desired output schemas via attention mechanisms. This assumes the model's pre-training data sufficiently covers domain language. For strictly uniform document structures requiring high-speed processing, simple regex scripts would be more efficient.

### Mechanism 3: Structured JSON Output
Constraining outputs to JSON format acts as a structural guardrail, forcing the model to organize internal representations into discrete fields. This reduces post-processing friction and potentially increases reliability by limiting the "generative" aspect to content rather than format. The assumption is that the model has sufficient instruction-following capability to produce valid JSON. Complex nested information may lead to malformed JSON requiring error handling.

## Foundational Learning

- **Temperature vs. Consistency**: Why needed: Critical configuration setting for reproducible results. Quick check: "If I run the same prompt twice and get different answers, which parameter should I check first?"
- **Ground Truthing**: Why needed: GenAI can hallucinate; evaluation against human-verified datasets is essential. Quick check: "Can I trust AI output for 30,000 documents without sampling 50 for manual review?"
- **Context Window & Chunking**: Why needed: Research documents can be long; understanding token limits is necessary for pipeline design. Quick check: "What happens if the answer is on page 50 but my prompt+context limit only allows 20 pages?"

## Architecture Onboarding

- **Component map**: Pre-processor (PDF/Image to text) -> Chunking Logic (split for context limits) -> Prompt Assembler (combine instructions with text) -> LLM Engine (Claude 3 Opus, Temperature=0) -> Post-processor (JSON parser) -> Error Handler (retry logic)
- **Critical path**: Prompt Engineering. The architecture fails if prompts do not unambiguously define extraction targets.
- **Design tradeoffs**: Proprietary APIs (lower barrier, vendor lock-in, reproducibility issues) vs. Open Weights (reproducibility, high technical skill, hardware requirements). Accuracy vs. Speed: multiple runs for consistency checks trade off against processing time for large datasets.
- **Failure signatures**: Lexical but not semantic consistency (wording changes while meaning same), Hallucination (invented plausible data), Truncation (output stops mid-JSON due to token limits).
- **First 3 experiments**:
  1. Consistency Stress Test: Run same prompts 3 times on 5 diverse documents; verify byte-for-byte identical JSON output.
  2. Ground Truth Benchmark: Manually label 20 documents per use case; calculate precision/recall for "hard" cases.
  3. Prompt Sensitivity Check: Modify ambiguous prompts and verify accuracy improvements without introducing new errors.

## Open Questions the Paper Calls Out

1. How do generative AI models perform on research data processing tasks when evaluated using standard quantitative metrics rather than illustrative examples? The current study is qualitative and plans future quantitative evaluation with precision, recall, and accuracy metrics.

2. Can the high accuracy and consistency observed in pilot phases be maintained when scaling to statistically significant numbers of data samples? Current findings are based on limited samples (e.g., "very limited sample of seedlist pages") and plan to apply methods to full datasets.

3. To what extent does the deprecation of proprietary models threaten the long-term computational reproducibility of research using generative AI? The paper highlights the conflict between high-performing proprietary APIs and scientific reproducibility requirements without offering a definitive solution.

## Limitations

- Study relies on proprietary Claude 3 Opus model, making exact reproduction difficult without API access
- Absence of quantitative metrics (precision, recall, F1 scores) limits rigor of accuracy claims
- Ground truth construction not detailed, leaving unclear how manual labeling bias might affect results
- Does not address cost implications of large-scale API usage or long-term sustainability
- Temperature=0 setting may not generalize to use cases requiring creative or interpretive output

## Confidence

- **High Confidence**: Temperature=0 to consistency mechanism is well-supported by experimental design and aligns with LLM behavior theory
- **Medium Confidence**: LLMs replacing rule-based logic claim is supported by case studies but lacks systematic error analysis or traditional method comparison
- **Low Confidence**: JSON formatting inherently increasing reliability is weakly supported; paper does not test alternative formats or measure structural parsing errors

## Next Checks

1. **Consistency Stress Test**: Run the extraction pipeline with temperature=0 three times on 5 diverse documents from each use case. Verify if JSON output is byte-for-byte identical across all runs.

2. **Ground Truth Benchmark**: Manually label 20 documents per use case. Run the pipeline and calculate precision/recall, focusing on "hard" cases (e.g., OCR errors in seedlists or ambiguous HTA committee names).

3. **Prompt Sensitivity Check**: Take one ambiguous document (e.g., the HAS HTA case). Modify the prompt to resolve the ambiguity (e.g., "Return the parent organization, not the committee"). Verify the fix improves accuracy without introducing new errors.