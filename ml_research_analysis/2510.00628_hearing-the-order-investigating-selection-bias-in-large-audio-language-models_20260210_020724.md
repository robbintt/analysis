---
ver: rpa2
title: 'Hearing the Order: Investigating Selection Bias in Large Audio-Language Models'
arxiv_id: '2510.00628'
source_url: https://arxiv.org/abs/2510.00628
tags:
- bias
- selection
- lalms
- arxiv
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates selection bias in Large Audio-Language
  Models (LALMs), where models show systematic performance variations based on the
  position of the correct answer in multiple-choice questions. The authors conduct
  extensive experiments across six LALMs on three benchmarks (MMAU, MMAR, MMLU) and
  their spoken versions.
---

# Hearing the Order: Investigating Selection Bias in Large Audio-Language Models

## Quick Facts
- arXiv ID: 2510.00628
- Source URL: https://arxiv.org/abs/2510.00628
- Reference count: 0
- Models show systematic accuracy variations of up to 24% based on answer position in multiple-choice questions

## Executive Summary
This paper systematically investigates selection bias in Large Audio-Language Models (LALMs), where models exhibit position-dependent performance variations in multiple-choice questions. Through extensive experiments across six LALMs on three benchmarks and their spoken versions, the authors demonstrate that shuffling answer positions can cause performance fluctuations up to 24% and even change model rankings. The study shows that selection bias is a universal problem affecting all tested models regardless of architecture or scale, and proposes permutation-based evaluation strategies (cyclic and full permutations) as effective mitigation approaches, with full permutation providing the most reliable assessment of model capability.

## Method Summary
The study evaluates six LALMs (Gemini-2.0-Flash, Phi-4-Multimodal, Qwen2.5-Omni-3B/7B, Voxtral-Mini-3B, Voxtral-Small-24B) on MMAU, MMAR, and MMLU benchmarks plus their spoken versions. For each question, the correct answer is systematically assigned to each position (A, B, C, D) while shuffling other options. Performance is measured using accuracy, Δaccuracy (vs. original), Relative Standard Deviation (RSD), and Choice KL Divergence (CKLD). Mitigation strategies include cyclic permutation (4 rotations) and full permutation (24 orderings) with majority voting. All evaluations use temperature=0 and follow OpenAI simple-evals protocol.

## Key Results
- LALMs show accuracy fluctuations up to 24% depending on correct answer position (Phi-4-Multimodal)
- Selection bias affects model rankings, with rank reversals observed when answer positions shift
- Full permutation evaluation (24 orderings) effectively mitigates bias, providing most reliable capability estimates
- Bias patterns differ between speech and text modalities for some models (e.g., Voxtral-Mini-3B shows opposite preferences)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LALMs exhibit systematic position-based selection bias causing accuracy fluctuations up to 24%
- Mechanism: Models develop implicit positional preferences during training that favor or avoid certain option slots regardless of content
- Core assumption: Bias reflects learned positional correlations rather than true reasoning degradation
- Evidence anchors: Shuffling causes 24% fluctuations; Phi-4-Multimodal favors position A and avoids D; no corpus evidence on positional bias in LALMs
- Break condition: Bias magnitude below measurement noise threshold or uniform position preference

### Mechanism 2
- Claim: Full permutation evaluation (24 orderings) reliably mitigates selection bias
- Mechanism: Averaging predictions across all orderings cancels positional preferences; majority vote reflects semantic understanding
- Core assumption: Model's underlying reasoning is consistent across permutations; only output selection is position-biased
- Evidence anchors: Full permutation provides most reliable capability estimates; mitigation borrowed from LLM literature
- Break condition: Computational cost prohibitive or bias persists even with permutation

### Mechanism 3
- Claim: Selection bias may be partially inherited from base text-only LLMs but altered through audio-language instruction tuning
- Mechanism: Fine-tuning from text LLMs may retain positional biases, but audio modality and new training data introduce different patterns
- Core assumption: Instruction tuning and audio integration affect positional encoding or attention patterns
- Evidence anchors: Voxtral shows similar bias to Mistral; Qwen series shows divergence after fine-tuning; no corpus evidence on inheritance
- Break condition: Bias patterns entirely model-specific with no correlation to base LLM

## Foundational Learning

- Concept: **Selection Bias vs. Position Bias**
  - Why needed here: Distinguishes general position effects from specific MCQ option ordering bias
  - Quick check question: Can you explain why a model performing well when correct answer is in position A but poorly in position D exhibits selection bias rather than general reasoning failure?

- Concept: **Permutation-Based Debiasing**
  - Why needed here: Mitigation strategy relies on cyclic (4 rotations) and full (24 permutations) option reordering with majority voting
  - Quick check question: Why does full permutation (24 orderings) provide better bias reduction than cyclic permutation (4 orderings), and what is the computational tradeoff?

- Concept: **Cross-Modal Bias Transfer**
  - Why needed here: Understanding whether biases originate from text pretraining, audio integration, or emerge from multimodal training
  - Quick check question: If Voxtral-Small-24B shows similar bias patterns to Mistral-Small-24B-Instruct but Qwen2.5-Omni-7B differs from Qwen2.5-7B-Instruct, what does this suggest about the role of audio-language fine-tuning?

## Architecture Onboarding

- Component map:
  ```
  Audio Input → Audio Encoder → Multimodal Fusion → LLM Backbone → Option Selection → Output
  Text Options (A/B/C/D) → ↑
  Positional Encoding (implicit) → Bias Pattern (model-specific)
  ```

- Critical path:
  1. Identify model-specific bias patterns by testing accuracy with correct answer fixed at each position (A, B, C, D)
  2. Measure bias severity using RSD and CKLD metrics
  3. Apply full permutation (24 orderings) with majority voting for unbiased evaluation
  4. Compare against base text LLM (if available) to assess bias inheritance

- Design tradeoffs:
  - **Cyclic vs. Full Permutation**: Cyclic (4× compute) provides moderate debiasing; full (24× compute) provides maximum debiasing but may be computationally prohibitive
  - **Identifier Removal**: Removing A/B/C/D labels reduces accuracy but effect on bias is inconsistent across models
  - **Speech vs. Text Modality**: Speech inputs show different bias patterns than text, requiring modality-specific evaluation

- Failure signatures:
  - Accuracy variance >10% across positions indicates severe selection bias
  - RSD >0.15 or CKLD >0.01 suggests non-uniform position preference
  - Model rankings change when correct answer position shifts (Figure 3 shows rank reversals)
  - Different bias patterns in speech vs. text versions of same benchmark

- First 3 experiments:
  1. **Baseline bias characterization**: Test LALM on MMAU/MMAR with correct answer systematically placed at each position (A, B, C, D). Calculate Δaccuracy, RSD, and CKLD. Expect 5-24% fluctuations depending on model.
  2. **Permutation ablation**: Compare original evaluation vs. cyclic permutation vs. full permutation on same benchmark. Verify RSD/CKLD decrease with full permutation while accuracy increases or stabilizes.
  3. **Modality comparison**: Run same evaluation on text benchmark (MMLU) vs. spoken version (SPEECH-MMLU) to check if audio modality alters bias patterns. Some models (e.g., Voxtral-Mini-3B) show opposite preferences in speech vs. text.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can selection bias in LALMs be mitigated through training-level interventions rather than inference-time permutation methods?
- Basis in paper: Conclusion states work "establishes a foundation for future research to move beyond permutation, toward more advanced solutions"
- Why unresolved: Paper only evaluates post-hoc mitigation (cyclic/full permutation) and notes computational overhead
- What evidence would resolve it: Experiments comparing selection bias in models trained with bias-aware objectives vs. standard training

### Open Question 2
- Question: Why do some LALMs inherit selection bias from text counterparts while others develop distinct bias patterns after audio-language instruction tuning?
- Basis in paper: Section 4.2 shows Voxtral maintains similar bias to Mistral while Qwen series shows clear divergence, but no explanation provided
- Why unresolved: Paper identifies phenomenon but doesn't investigate architectural or training factors determining bias inheritance
- What evidence would resolve it: Systematic ablation studies varying instruction-tuning data composition, audio encoder architectures, and fusion mechanisms

### Open Question 3
- Question: How does choice of TTS system affect selection bias measurements in spoken benchmark evaluations?
- Basis in paper: Methodology uses only GPT-4o mini TTS without investigating whether different TTS systems influence model attention patterns
- Why unresolved: Paper treats spoken benchmarks as equivalent regardless of synthesis method, but audio characteristics could interact with positional attention mechanisms
- What evidence would resolve it: Experiments measuring selection bias on same text benchmarks converted using multiple TTS systems with varying acoustic properties

## Limitations
- The study tests only six LALMs and three benchmarks, limiting generalizability claims about bias being "universal"
- Computational cost of full permutation evaluation (24× inference) may be prohibitive for large-scale deployment
- Focus on multiple-choice questions with exactly four options limits applicability to other task formats

## Confidence
- **High Confidence**: Empirical demonstration of selection bias across six LALMs with quantitative metrics showing consistent position-based performance variations up to 24%
- **Medium Confidence**: Claim that selection bias is "universal" across LALMs, as the study tests limited number of models and benchmarks
- **Low Confidence**: Specific mechanisms driving different bias patterns in speech vs. text modalities and whether bias represents fundamental architectural limitations or training data artifacts

## Next Checks
1. **Architecture Dissection**: Test same models on text-only versions of benchmarks to quantify how much bias persists without audio input
2. **Bias Transfer Analysis**: Fine-tune base text LLM with audio-language instruction data while controlling for training data order effects
3. **Computational Feasibility Study**: Evaluate practical impact of full permutation evaluation on real-world applications by measuring inference time increases and cost implications