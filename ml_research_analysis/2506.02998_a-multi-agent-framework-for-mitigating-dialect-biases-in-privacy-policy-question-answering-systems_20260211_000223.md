---
ver: rpa2
title: A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering
  Systems
arxiv_id: '2506.02998'
source_url: https://arxiv.org/abs/2506.02998
tags:
- dialect
- privacy
- agent
- policy
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent framework to reduce dialectal
  bias in privacy policy question-answering systems. The method employs a Dialect
  Agent to translate user queries from various English dialects into Standard American
  English, preserving intent, and a Privacy Policy Agent to refine answers using domain
  expertise.
---

# A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems

## Quick Facts
- arXiv ID: 2506.02998
- Source URL: https://arxiv.org/abs/2506.02998
- Reference count: 20
- Primary result: Improves GPT-4o-mini zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA

## Executive Summary
This paper addresses dialectal bias in privacy policy question-answering systems by introducing a multi-agent framework that translates user queries from 50 English dialects into Standard American English while preserving intent, then generates and refines answers using specialized LLM agents. The approach significantly improves both accuracy and fairness, reducing performance disparities between SAE and other dialects by up to 82% as measured by maximum difference in F1 scores. The framework achieves these gains without requiring additional training data, matching or surpassing few-shot baselines through effective prompt-based role specialization.

## Method Summary
The framework employs two specialized LLM agents: a Dialect Agent that translates dialectal queries into Standard American English using structured linguistic metadata, and a Privacy Policy Agent that generates answers based on policy segments and translated questions. The agents collaborate iteratively, with the Dialect Agent validating whether the answer preserves the original intent and triggering revisions if needed (maximum 2 iterations). The system uses prompt-based role specialization to create distinct reasoning paths without fine-tuning, and was evaluated on PrivacyQA and PolicyQA datasets using both zero-shot and few-shot settings across 50 English dialects generated via the rule-based Multi-VALUE framework.

## Key Results
- Zero-shot accuracy improves from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA
- Reduces performance disparities between SAE and other dialects by up to 82% (Max Diff metric)
- Matches or exceeds few-shot baselines without requiring additional training data
- Achieves high translation fidelity with BLEU score 46.5 and ROUGE-L 80.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit dialect metadata in prompts enables more accurate translation of dialectal queries to Standard American English.
- Mechanism: The Dialect Agent receives structured linguistic information (phonetic, grammatical, lexical, cultural) about each dialect before processing queries. This contextual knowledge allows the LLM to map dialect-specific expressions to semantically equivalent SAE phrasing.
- Core assumption: The quality of dialect information directly affects translation fidelity; incomplete or inaccurate metadata degrades performance.
- Evidence anchors:
  - [abstract] "Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent"
  - [section 4, Table 5] Removing dialect info drops Initial F1 from 0.5772 to 0.5210; Final stage still reaches 0.5894 but remains below fully informed system (0.5966)
  - [corpus] Weak direct evidence—neighbor papers focus on multi-agent coordination patterns, not dialect handling specifically
- Break condition: Dialect metadata is missing, incorrect, or the dialect has no documented linguistic profile available.

### Mechanism 2
- Claim: Iterative feedback between specialized agents corrects misinterpretations that single-pass translation misses.
- Mechanism: After the Privacy Policy Agent generates an initial answer, the Dialect Agent evaluates whether the response captures the original dialectal intent. If discrepancies exist, the Privacy Policy Agent revises its answer. This loop runs up to 2 iterations maximum.
- Core assumption: The Dialect Agent can reliably detect when nuances are lost and provide actionable feedback.
- Evidence anchors:
  - [abstract] "Privacy Policy Agent...refines predictions using domain expertise"
  - [section 4, Table 4] PrivacyQA improves from 0.53 to 0.59 (zero-shot) and 0.58 to 0.61 (few-shot) after iterative refinement
  - [section 4, Table 6] Dialect Agent overrides in 22.99% (zero-shot) and 31.75% (few-shot) of cases; 63.4-72.1% of overrides are beneficial
  - [corpus] "Talk Structurally, Act Hierarchically" paper corroborates iterative refinement benefits in LLM multi-agent systems
- Break condition: Maximum iteration count reached without agreement, or agents converge on incorrect interpretation.

### Mechanism 3
- Claim: Domain-specific role prompting creates specialized reasoning paths without requiring fine-tuning.
- Mechanism: Each agent receives role-specific system prompts defining expertise boundaries. The Dialect Agent is cast as a "linguist specializing in [dialect]" while the Privacy Policy Agent is framed as a "privacy policy expert." This role separation constrains each agent's reasoning to relevant knowledge domains.
- Core assumption: LLMs can maintain role consistency and domain boundaries across multi-turn interactions.
- Evidence anchors:
  - [section 3] Full prompt templates shown in Appendix B define explicit role boundaries and task constraints
  - [section 4] Zero-shot multi-agent matches or exceeds few-shot baselines, suggesting role prompting substitutes for example-based learning
  - [corpus] CAMEL and ChatDev frameworks (cited in Related Work) demonstrate role-based agent collaboration patterns
- Break condition: Role prompts are underspecified, or task requires knowledge outside either agent's defined expertise.

## Foundational Learning

- Concept: **Dialectal variation in NLP**
  - Why needed here: Understanding that non-standard English dialects (AAVE, Chicano English, Aboriginal English) systematically differ from SAE in syntax, morphology, and lexicon—these differences cause performance gaps in models trained predominantly on SAE.
  - Quick check question: Can you explain why a model might correctly answer "Do you sell my data?" but fail on "Does y'all sell my datums?" despite identical intent?

- Concept: **Prompt-based role specialization**
  - Why needed here: The framework relies on prompting to create distinct agent behaviors without architectural changes or fine-tuning.
  - Quick check question: What information must be included in a system prompt to make an LLM behave as a domain expert?

- Concept: **Performance disparity metrics**
  - Why needed here: The paper measures fairness using Avg Diff (average performance gap between SAE and other dialects) and Max Diff (worst-case gap). Understanding these metrics is essential for evaluating bias mitigation.
  - Quick check question: If a model achieves 0.70 F1 on SAE and 0.50 on a minority dialect, what is the performance disparity?

## Architecture Onboarding

- Component map: User Query (dialect) → Dialect Agent [translation + validation] → SAE Query + Privacy Policy Segment → Privacy Policy Agent [answer generation] → Dialect Agent [agreement check] → (if disagrees) Privacy Policy Agent [revision] → Final Answer

- Critical path: Dialect Agent translation quality → Privacy Policy Agent answer accuracy → Dialect Agent validation precision. Errors compound if translation distorts intent before domain reasoning begins.

- Design tradeoffs:
  - Translation fidelity vs. over-normalization: Converting to SAE risks erasing cultural markers; the paper prioritizes functional accuracy over dialect preservation
  - Iteration depth vs. latency/cost: Max 2 iterations balances improvement gains against inference overhead
  - Explicit dialect labels vs. automatic detection: Current design requires dialect metadata; unsupervised detection remains future work

- Failure signatures:
  - High override rates (>40%) suggest systematic translation failures
  - Detrimental overrides (18.7-24.1% in Table 6) indicate Dialect Agent introducing errors
  - Large remaining Max Diff (>0.03) after multi-agent processing signals unresolved dialect-specific challenges

- First 3 experiments:
  1. **Baseline dialect gap measurement**: Run zero-shot GPT-4o-mini on PrivacyQA/PolicyQA across 5-10 dialects using Multi-VALUE transformations; record F1 and Max Diff to establish initial disparity.
  2. **Ablate dialect metadata**: Compare Dialect Agent performance with full linguistic info vs. generic "you are a linguistics expert" prompt; quantify translation quality drop using BLEU/ROUGE against human references.
  3. **Override analysis**: Track which dialects trigger highest disagreement rates between agents; manually inspect 20-30 cases where Dialect Agent overrides were detrimental to identify failure patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-agent framework perform on naturally occurring dialectal variations compared to the synthetic perturbations used in the current evaluation?
- Basis in paper: [explicit] The Limitations section states that the approach relies on synthetic dialectal data generated using rule-based transformations which "may not fully capture the nuances of naturally occurring dialect variations," and explicitly calls for evaluation on "real-world dialectal data and user-generated queries."
- Why unresolved: The study relied entirely on the Multi-VALUE framework to generate dialectal queries from standard datasets (PrivacyQA and PolicyQA) rather than testing on corpora containing organic dialect usage.
- What evidence would resolve it: Benchmarking the framework's F1 scores and fairness metrics on a dataset of privacy-related questions authored by native speakers of the target dialects (e.g., AAVE, Aboriginal English).

### Open Question 2
- Question: Can automated dialect detection be effectively integrated into the framework to maintain performance without requiring explicit dialect metadata from the user?
- Basis in paper: [explicit] The Limitations section notes that the method "depends on accurate dialect metadata," and the Implications section suggests future research should explore "privacy-preserving, unsupervised methods to infer dialectal features directly from user queries."
- Why unresolved: The current Dialect Agent requires a prompt containing specific "dialect_info" to function, meaning the system must know the user's dialect a priori.
- What evidence would resolve it: An end-to-end evaluation where a dialect detection module precedes the Dialect Agent, showing that the system maintains high accuracy and low disparity without manual dialect labels.

### Open Question 3
- Question: Does the framework generalize effectively to high-stakes domains other than privacy policies, such as healthcare or legal AI?
- Basis in paper: [explicit] The Conclusion states: "Future work should explore extending this approach to high-stakes domains such as healthcare, legal AI, and financial services, where language accessibility is critical."
- Why unresolved: The paper only validates the method on privacy policy QA tasks; it is unknown if the translation of technical jargon combined with dialect translation introduces latency or errors in more critical fields.
- What evidence would resolve it: Application of the multi-agent framework to medical or legal QA datasets with dialectal variations, measuring both accuracy and expert evaluation of answer safety.

### Open Question 4
- Question: How well does the multi-agent collaboration strategy generalize to structurally distinct languages with high dialectal diversity?
- Basis in paper: [explicit] The Limitations section notes: "our study focuses on English dialects, and it remains an open question how well this framework generalizes to other languages with diverse linguistic variations."
- Why unresolved: The morphological and syntactic perturbations handled by the Dialect Agent were specific to English varieties (e.g., AAVE, Indian English) and may not map cleanly to languages with different dialectal mechanics.
- What evidence would resolve it: Replicating the study on a non-English corpus (e.g., Arabic or Chinese dialects) to determine if the "translate to standard" agent strategy yields comparable fairness improvements.

## Limitations
- Framework depends critically on availability and accuracy of dialect metadata for all 50 dialects
- Translation quality may degrade for less-documented dialects, potentially introducing new biases
- Rule-based Multi-VALUE dialect generation may not capture all semantic nuances of real dialectal speech
- Detrimental overrides occur in 18.7-24.1% of cases, indicating Dialect Agent can introduce errors

## Confidence
- **High confidence** in the accuracy improvement claims (0.394→0.601 on PrivacyQA, 0.352→0.464 on PolicyQA) given the specific metric definitions and reported standard errors
- **Medium confidence** in the fairness improvement claims due to the sensitivity of Avg Diff and Max Diff metrics to dialect sampling and the assumption that Multi-VALUE faithfully represents real dialectal variation
- **Low confidence** in the generalizability across LLM models, as the evaluation only tested four specific models (GPT-4o-mini, Llama 3.1 8B, DeepSeek-R1-Distill-Qwen-14B) without exploring broader model families

## Next Checks
1. **Translate-and-back-test**: Run 50 dialect queries through the full pipeline, then translate answers back to the original dialect using Multi-VALUE's inverse rules; measure semantic drift to quantify translation fidelity.
2. **Error case clustering**: Analyze the 18.7-24.1% detrimental override cases to identify systematic failure patterns (e.g., specific dialect families, query types, or policy domains) that could inform targeted improvements.
3. **Ablation on few-shot exemplars**: Systematically vary the content and diversity of the 8 few-shot examples per agent to determine the minimum exemplar quality required to match zero-shot performance, informing resource-constrained deployment scenarios.