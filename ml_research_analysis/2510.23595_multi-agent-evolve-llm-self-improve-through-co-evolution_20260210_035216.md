---
ver: rpa2
title: 'Multi-Agent Evolve: LLM Self-Improve through Co-evolution'
arxiv_id: '2510.23595'
source_url: https://arxiv.org/abs/2510.23595
tags:
- question
- answer
- judge
- questions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Multi-Agent Evolve (MAE) is a self-improving framework that uses\
  \ three interacting LLM roles\u2014Proposer, Solver, and Judge\u2014to generate\
  \ and solve questions without human-curated data. The Proposer generates questions,\
  \ the Solver answers them, and the Judge evaluates both, providing domain-agnostic\
  \ rewards."
---

# Multi-Agent Evolve: LLM Self-Improve through Co-evolution

## Quick Facts
- arXiv ID: 2510.23595
- Source URL: https://arxiv.org/abs/2510.23595
- Reference count: 40
- Primary result: MAE achieves 4.54% average improvement across math, coding, reasoning, and general knowledge benchmarks

## Executive Summary
Multi-Agent Evolve (MAE) is a self-improving framework that uses three interacting LLM roles—Proposer, Solver, and Judge—to generate and solve questions without human-curated data. The system employs difficulty-aware rewards, format rewards, and quality filtering to ensure stable training. On Qwen2.5-3B-Instruct, MAE achieves a 4.54% average improvement across multiple benchmark categories, outperforming both base and supervised fine-tuning baselines.

## Method Summary
MAE operates through a three-role system where a Proposer generates questions, a Solver answers them, and a Judge evaluates both outputs to provide domain-agnostic rewards. The framework uses difficulty-aware rewards, format rewards, and quality filtering to maintain training stability. The self-improvement loop iterates through question generation, solving, evaluation, and model updates without requiring external human-labeled datasets.

## Key Results
- MAE achieves 4.54% average improvement across math, coding, reasoning, and general knowledge benchmarks
- Outperforms both base model and supervised fine-tuning baselines on Qwen2.5-3B-Instruct
- Ablation studies confirm necessity of each role and effectiveness of quality filtering and format reward mechanisms

## Why This Works (Mechanism)
The system creates a self-contained feedback loop where each role complements the others: the Proposer generates diverse questions, the Solver attempts answers, and the Judge provides calibrated rewards. This closed-loop system eliminates dependency on external human data while maintaining diversity through format rewards and filtering out low-quality generations. The three-role architecture prevents collapse into repetitive patterns and ensures continuous learning through novel question generation.

## Foundational Learning

**Co-evolutionary training** - Simultaneous improvement of multiple agents through iterative feedback
- Why needed: Enables self-improvement without external supervision
- Quick check: Verify all three roles show improvement over training iterations

**Domain-agnostic reward modeling** - Judge evaluates across diverse tasks without task-specific fine-tuning
- Why needed: Eliminates need for multiple specialized reward models
- Quick check: Test Judge consistency across completely different domain types

**Difficulty-aware reward shaping** - Rewards scaled based on question complexity
- Why needed: Prevents model from exploiting easy questions and encourages challenging task generation
- Quick check: Monitor reward distribution across difficulty levels

## Architecture Onboarding

**Component Map**
Proposer -> Solver -> Judge -> Reward feedback -> Model update -> (repeat)

**Critical Path**
1. Proposer generates question (with format diversity)
2. Solver attempts answer
3. Judge evaluates both question quality and answer correctness
4. Rewards used to update Proposer and Solver models

**Design Tradeoffs**
- Three separate roles increase computational cost but provide diverse feedback
- Quality filtering reduces training data but prevents reward hacking
- Domain-agnostic Judge simplifies deployment but may lack task-specific sensitivity

**Failure Signatures**
- Collapsing reward distributions indicating Judge bias
- Decreasing question diversity suggesting Proposer saturation
- Inconsistent reward patterns revealing Judge instability

**3 First Experiments**
1. Verify Judge consistency by evaluating same question-answer pairs multiple times
2. Test diversity of generated questions across different temperature settings
3. Validate reward scaling by comparing performance on easy vs. hard questions

## Open Questions the Paper Calls Out
None

## Limitations
- Single Judge model may introduce systematic bias despite domain-agnostic claims
- Requires careful calibration of Judge parameters and filtering thresholds
- Computational overhead of three-role system not quantified for larger models

## Confidence

**High Confidence**: Three-role architectural design is well-specified and technically sound
**Medium Confidence**: Performance improvements are plausible but magnitude requires careful interpretation
**Low Confidence**: Domain-agnostic evaluation claims lack thorough cross-domain validation

## Next Checks
1. Test Judge model's reward consistency and quality across significantly different domains not represented in original training data
2. Run MAE for extended training periods (50+ generations) to assess long-term stability and convergence
3. Quantify total computational overhead of three-role system compared to supervised fine-tuning, scaling analysis to larger models