---
ver: rpa2
title: 'GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic
  Text-Attributed Graphs'
arxiv_id: '2511.16778'
source_url: https://arxiv.org/abs/2511.16778
tags:
- graph
- learning
- gcl-ot
- node
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GCL-OT addresses the challenge of learning on heterophilic text-attributed
  graphs by identifying three granular heterophily patterns: partial, complete, and
  latent homophily. It proposes a graph contrastive learning framework with optimal
  transport, introducing three tailored mechanisms: a RealSoftMax-based similarity
  estimator for partial heterophily, a prompt-based filter for complete heterophily,
  and OT-guided soft supervision for latent homophily.'
---

# GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs

## Quick Facts
- arXiv ID: 2511.16778
- Source URL: https://arxiv.org/abs/2511.16778
- Authors: Yating Ren; Yikun Ban; Huobin Tan
- Reference count: 40
- Primary result: Achieves up to 82.72% accuracy on heterophilic text-attributed graphs, outperforming state-of-the-art methods

## Executive Summary
GCL-OT introduces a graph contrastive learning framework specifically designed for heterophilic text-attributed graphs. The method identifies three granular heterophily patterns—partial, complete, and latent homophily—and proposes three tailored mechanisms to address each: a RealSoftMax-based similarity estimator for partial heterophily, a prompt-based filter for complete heterophily, and OT-guided soft supervision for latent homophily. Theoretical analysis demonstrates tighter mutual information bounds and reduced Bayes error compared to standard approaches. Extensive experiments on nine benchmarks show consistent improvements over state-of-the-art methods, with particular robustness to structural and textual perturbations.

## Method Summary
GCL-OT processes text-attributed graphs by first augmenting text via GPT-3.5, then encoding through a dual architecture (DistilBERT for text, GNN for structure). The method computes RealSoftMax similarity between token and neighbor embeddings, applies a learnable prompt-based filter to exclude irrelevant connections, and solves optimal transport using Low-Rank Sinkhorn to obtain soft alignment assignments. These assignments guide contrastive learning through two losses: alignment loss (L_MHA) and latent homophily mining loss (L_LHM), combined with classification loss (L_NC) for node classification.

## Key Results
- Achieves up to 82.72% accuracy on heterophilic benchmarks (Wisconsin, Cornell, Texas)
- Outperforms state-of-the-art methods by 2-5% accuracy on average across nine datasets
- Demonstrates superior robustness to edge deletion (50%) and text word removal (20%) perturbations
- Shows consistent performance improvements across all three heterophily patterns

## Why This Works (Mechanism)

### Mechanism 1: RealSoftMax for Partial Heterophily
RealSoftMax improves alignment when only a subset of a node's text is semantically relevant to its neighbors, preventing noisy or irrelevant tokens from diluting the learning signal. This operator calculates similarity via weighted log-sum-exp of interactions, emphasizing the most salient keyword-neighbor pairs while maintaining gradient flow. It smoothly interpolates between mean (β → ∞) and maximum (β → 0) behaviors.

### Mechanism 2: Prompt-Based Filtering for Complete Heterophily
A learnable "filter prompt" allows the model to adaptively exclude completely irrelevant node pairs from alignment. The similarity matrix is expanded with a sink vector (the prompt), and embeddings below the prompt's similarity threshold are aligned with this dummy vector instead of neighbors, removing them from contrastive loss calculation.

### Mechanism 3: OT-Guided Soft Supervision for Latent Homophily
The OT assignment matrix serves as soft labels to discover semantically similar nodes lacking direct edges. Instead of treating unconnected nodes as negatives, high values in the OT matrix indicate potential semantic alignment, encouraging the model to pull these latent neighbors closer in embedding space.

## Foundational Learning

- **Concept: Optimal Transport (Entropic Regularization)**
  - Why needed: Standard InfoNCE loss fails on heterophilic graphs where relationships are mixed; OT provides soft fractional alignment that finds the most efficient way to correlate text and structure distributions.
  - Quick check: How does increasing entropy regularization parameter ε affect OT assignment matrix "softness"? (Answer: Makes distribution more uniform/diffuse)

- **Concept: Mutual Information (MI) Bounds**
  - Why needed: The paper theoretically justifies its loss function by proving it tightens MI lower bounds between structure and text views compared to standard InfoNCE.
  - Quick check: Does minimizing L_LHM increase or decrease MI lower bound? (Answer: Increases/maximizes it)

- **Concept: Heterophily vs. Homophily**
  - Why needed: Core problem definition—homophily assumes neighbors are similar, heterophily assumes opposites attract or random connections.
  - Quick check: Why does standard GCN fail on heterophilic graphs? (Answer: Aggregates neighbor features, acting as low-pass filter that smooths distinct classes into indistinguishable mixtures)

## Architecture Onboarding

- **Component map**: LLM (frozen) → Text Augmentation → PLM (trainable) + GNN (trainable) → RealSoftMax Similarity + Prompt Filter → OT Solver (LRSinkhorn) → Alignment & Soft Supervision Losses → Classification

- **Critical path**: 1) Augment text via LLM → 2) Encode via PLM & GNN → 3) Compute RealSoftMax similarity matrix S → 4) Apply Filter Prompt to expand to S̄ → 5) Run LRSinkhorn to get Q* → 6) Compute L_MHA and L_LHM using Q*

- **Design tradeoffs**: RealSoftMax β balances between hard max (low β) and mean (high β); LRSinkhorn rank r reduces complexity O(Nr) but may lose transport details; filter prompt initialization impacts noise filtering effectiveness

- **Failure signatures**: Model collapse if prompt too aggressive (sparse OT matrix, near-zero alignment loss); over-smoothing if β too high on heterophilic data (indistinguishable embeddings in t-SNE)

- **First 3 experiments**: 1) Ablate RealSoftMax vs Max-Pooling/Mean-Pooling on high-heterophily dataset; 2) Visualize OT assignment matrix Q* to confirm latent homophily discovery; 3) Sensitivity analysis of prompt threshold on Wisconsin dataset

## Open Questions the Paper Calls Out
1. Can OT-guided soft supervision effectively scale to discover latent homophily in graphs where implicit links are extremely sparse or semantically ambiguous?
2. How robust is the prompt-based filter to semantic hallucinations or systematic noise generated by the frozen LLM during text augmentation?
3. Does Low-Rank Sinkhorn approximation compromise theoretical MI guarantees on very dense or large-scale graphs?

## Limitations
- RealSoftMax effectiveness contingent on presence of partial heterophily with relevant text subsets
- Prompt-based filter may over-regularize in strictly homophilic graphs or fail on mixed patterns
- OT-guided soft supervision vulnerable to poor text embeddings producing arbitrary assignments
- Model complexity may face scalability challenges on large graphs despite LRSinkhorn optimization

## Confidence
- **High Confidence**: Consistent outperformance across all nine benchmarks; successful handling of all three heterophily patterns; tighter MI bounds and reduced Bayes error
- **Medium Confidence**: RealSoftMax advantage for partial heterophily; prompt filter effectiveness; OT-guided soft supervision reliability
- **Low Confidence**: Performance guarantees on significantly larger graphs; mechanism robustness across diverse text distributions; computational efficiency at scale

## Next Checks
1. **RealSoftMax ablation across text distributions**: Test with RealSoftMax disabled vs enabled across graphs with varying text attribute lengths and distributions to isolate mechanism contribution
2. **OT assignment matrix validation**: Extract and visualize Q* for Wisconsin/Cornell nodes, compare high-value non-diagonal entries against independent semantic similarity scores to quantify precision and false positive rate
3. **Prompt filter sensitivity analysis**: Grid search over prompt initialization strategies and filtering thresholds on Wisconsin, measuring accuracy, OT matrix sparsity, and gradient norms to identify optimal operating zone