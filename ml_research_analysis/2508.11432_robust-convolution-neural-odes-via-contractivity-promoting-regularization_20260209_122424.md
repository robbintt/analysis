---
ver: rpa2
title: Robust Convolution Neural ODEs via Contractivity-promoting regularization
arxiv_id: '2508.11432'
source_url: https://arxiv.org/abs/2508.11432
tags:
- nodes
- neural
- node
- accuracy
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving the robustness of
  Convolutional Neural ODEs (NODEs) against input noise and adversarial attacks. The
  authors propose using contraction theory to regularize NODEs during training, ensuring
  that trajectories converge exponentially fast to each other.
---

# Robust Convolution Neural ODEs via Contractivity-promoting regularization

## Quick Facts
- arXiv ID: 2508.11432
- Source URL: https://arxiv.org/abs/2508.11432
- Authors: Muhammad Zakwan; Liang Xu; Giancarlo Ferrari-Trecate
- Reference count: 40
- Primary result: Contractive regularization improves NODE robustness by up to 34% under noise and 30% against adversarial attacks

## Executive Summary
This paper proposes a contractivity-promoting regularization framework to enhance the robustness of Convolutional Neural ODEs (NODEs) against input perturbations and adversarial attacks. The authors leverage contraction theory to ensure that trajectories in the NODE dynamics converge exponentially, thereby limiting the impact of input noise. They develop two types of regularizers: one based on the Jacobian matrix and another using weight regularization for slope-restricted activation functions, with the latter being computationally efficient for convolutional architectures. Experiments on MNIST and FashionMNIST demonstrate significant improvements in robustness while maintaining competitive clean accuracy.

## Method Summary
The method regularizes NODE dynamics during training to ensure contractivity, which guarantees that any two trajectories converge exponentially. The authors develop two regularizers: (1) a Jacobian-based penalty that enforces negative definiteness of the contraction metric, and (2) a computationally efficient weight regularization for slope-restricted activations using Gershgorin circle theorem bounds. For convolutional NODEs, contractivity is promoted by penalizing convolution filter weights based on their center elements and L1 norms. The model uses Smooth Leaky ReLU activation, Forward Euler discretization, and optimizes time-varying weights at each integration step. The regularizer is added to the cross-entropy loss with a weight parameter γ.

## Key Results
- CNODEs achieve up to 34% improvement in test accuracy under Gaussian noise compared to vanilla NODEs
- CNODEs show up to 30% improvement in adversarial robustness against FGSM and PGD attacks
- Transferability attack results suggest robustness is not due to gradient obfuscation
- Performance remains stable across different contraction rate values (ρ)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing contraction in the NODE dynamics appears to bound the sensitivity of the output to input perturbations.
- **Mechanism:** By definition, a contractive system ensures that any two trajectories (e.g., a clean input and a perturbed input) converge exponentially. The authors enforce this by penalizing the Jacobian of the dynamics if it violates negative definiteness conditions, effectively ensuring the Lipschitz constant of the flow map is less than one.
- **Core assumption:** The discretized training trajectory sufficiently approximates the continuous dynamics such that local contractivity at sampled points implies global robustness.
- **Evidence anchors:**
  - [abstract] "For a contractive dynamical system two trajectories starting from different initial conditions converge to each other exponentially fast."
  - [section II-B] Definition 1 and Eq. (5) formally link contraction to the Jacobian condition $\Gamma(x) \succ 0$.
  - [corpus] Corpus neighbors focus on GNNs and general DL theory; direct validation of this specific contraction mechanism is missing from the provided neighbors.
- **Break condition:** If the regularization weight $\gamma$ is too low, the violation penalty may be insufficient to enforce strict contractivity, allowing perturbations to persist or amplify.

### Mechanism 2
- **Claim:** Contractivity can be approximated efficiently by regularizing convolution filter weights rather than computing full Jacobians.
- **Mechanism:** For slope-restricted activations (e.g., Smooth Leaky ReLU), the paper uses the Gershgorin circle theorem to derive sufficient conditions for contraction based solely on the values of the weight matrices (diagonal vs. off-diagonal sums). This transforms a computationally expensive eigenvalue optimization into a cheap L1-style penalty on convolution filters.
- **Core assumption:** The activation functions are slope-restricted (bounded derivatives) and the Gershgorin bounds are tight enough to not over-constrain the network capacity.
- **Evidence anchors:**
  - [section III-A] Theorem 1 provides the algebraic condition (Eq. 8) linking weights to contraction rate $\rho$.
  - [section III-B] Lemma 1 and Eq. (14) map these weight conditions directly to convolutional filter elements.
  - [corpus] Evidence for this specific algebraic simplification is not present in the provided corpus neighbors.
- **Break condition:** If the activation function derivatives are unbounded or mis-specified (e.g., standard ReLU without smoothing), the weight bounds may no longer guarantee contraction.

### Mechanism 3
- **Claim:** The improved robustness is likely due to genuine dynamical stability rather than gradient masking (obfuscation).
- **Mechanism:** The authors test robustness against "transferability attacks" (adversarial examples generated from a vanilla, non-robust NODE). If the defense relied on obfuscated gradients, transferred attacks would likely fail. The success of transferred attacks implies the model learns genuinely smoother decision boundaries rather than hiding gradient information.
- **Core assumption:** Adversarial examples generated by standard attacks (FGSM/PGD) on vanilla NODEs are sufficient proxies for testing gradient obfuscation.
- **Evidence anchors:**
  - [section IV] "Our observations indicate that CNODEs achieve higher accuracy when subjected to transfer attacks... suggesting that the robustness... is not due to gradient obfuscation."
  - [abstract] "...suggesting that gradient obfuscation is not the source of improved robustness."
  - [corpus] No external corpus evidence was found to corroborate or refute the transferability claim.
- **Break condition:** If an adaptive attack specifically targets the contractivity constraint (e.g., optimizing over the ODE solver steps), it might find vulnerabilities that standard FGSM/PGD miss.

## Foundational Learning

- **Concept:** **Neural Ordinary Differential Equations (NODEs)**
  - **Why needed here:** The paper treats the neural network not as discrete layers, but as the discretization of a continuous-time dynamical system $\dot{x} = f(x, t)$. Understanding this is required to interpret "trajectories" and "Jacobian" in this context.
  - **Quick check question:** How does the "depth" of a NODE relate to the integration time $T$?

- **Concept:** **Contraction Theory**
  - **Why needed here:** This is the core theoretical tool. Unlike stability (convergence to a point), contraction refers to the convergence of *any* two trajectories to each other. It is the mathematical justification for why perturbations (distance between clean and noisy trajectories) shrink.
  - **Quick check question:** If a system is contractive, what happens to the distance between a clean state $x(t)$ and a perturbed state $\hat{x}(t)$ over time?

- **Concept:** **Gershgorin Circle Theorem**
  - **Why needed here:** The paper uses this theorem to bound the eigenvalues of the system Jacobian using only the weights of the network. This allows them to replace expensive Jacobian computations with simple weight penalties.
  - **Quick check question:** Can a matrix be positive definite if the absolute value of a diagonal element is smaller than the sum of absolute values of the off-diagonal elements in that row?

## Architecture Onboarding

- **Component map:** Pre-processing Conv (3x3) -> NODE Block (Conv 3x3 + Smooth Leaky ReLU) -> Fully Connected (dim 10)
- **Critical path:** The calculation of the regularization term (Eq. 14) involves scanning the convolution filters. You must ensure the center elements $\{C_d^d\}_c$ and absolute sums are extracted correctly during the backward pass to apply the penalty.
- **Design tradeoffs:**
  - **Strictness vs. Expressivity:** Enforcing strict contraction (high $\rho$) guarantees robustness but may limit the network's ability to learn complex features (Remark 2 notes "loss of expressivity might be unavoidable").
  - **Compute vs. Accuracy:** The paper uses weight regularization (cheap) instead of full Jacobian regularization (expensive), trading theoretical exactness for computational feasibility.
- **Failure signatures:**
  - **Clean Accuracy Drop:** If regularization weight $\gamma$ is too high, the model may fail to fit the training data.
  - **NaNs during training:** If the ODE step size $h$ is too large relative to the weights, Forward Euler discretization can become unstable (even if the continuous ODE is stable).
- **First 3 experiments:**
  1. **Sanity Check (Clean Data):** Train CNODE on MNIST/FashionMNIST with $\gamma=0$ (Vanilla) vs. $\gamma > 0$. Verify that clean accuracy remains comparable (approx. 98% for MNIST).
  2. **Noise Robustness:** Inject Gaussian noise ($\sigma=0.1$ to $0.3$) into the test set. Plot accuracy degradation curves. The paper claims a significant gap (e.g., 65% vs 94% at $\sigma=0.1$) should appear.
  3. **Hyperparameter Sweep:** Vary the contraction rate $\rho$ (e.g., 0.1 to 15) as shown in Appendix V-B to confirm that performance is insensitive to the specific choice of $\rho$ on your specific dataset.

## Open Questions the Paper Calls Out
- **Question:** Can computationally efficient regularizers be developed for NODE architectures where the dynamics $f$ do not rely on slope-restricted activation functions?
- **Question:** What is the theoretical bound on the loss of representation power in contractive NODEs, and how does it compare to other robust architectures?
- **Question:** Does the Gershgorin-based sufficient condition for contractivity impose excessive conservatism that degrades performance on complex, high-dimensional datasets?

## Limitations
- The contractivity framework assumes Lipschitz-continuous dynamics and slope-restricted activations; deviations from these assumptions could break the theoretical guarantees
- The empirical evaluation focuses on relatively simple datasets (MNIST, FashionMNIST) with relatively small model architectures
- The paper does not evaluate against adaptive white-box attacks specifically designed to circumvent the contractivity constraints
- Time-varying weights (θₖ) add computational complexity compared to weight-shared implementations

## Confidence
- **High confidence** in the theoretical framework linking contraction theory to input robustness
- **Medium confidence** in the experimental claims of 34% noise robustness and 30% adversarial robustness improvements, as they are dataset-specific and may not generalize to complex image domains
- **Medium confidence** in the transferability attack analysis as a proxy for ruling out gradient obfuscation, since adaptive attacks could still exploit the regularizer

## Next Checks
1. **Architecture Scaling Test:** Replicate the method on CIFAR-10/100 with deeper NODE architectures to assess if robustness gains scale with problem complexity
2. **Adaptive Attack Vulnerability:** Implement a white-box attack that explicitly optimizes over the regularization term (e.g., using projected gradient descent on both the input and the latent trajectory) to probe for hidden vulnerabilities
3. **Activation Generalization:** Replace Smooth Leaky ReLU with alternative slope-restricted activations (e.g., tanh-based) to verify if the method's effectiveness depends on the specific activation choice or the slope restriction property itself