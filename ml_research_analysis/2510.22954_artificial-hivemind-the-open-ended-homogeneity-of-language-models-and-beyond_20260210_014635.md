---
ver: rpa2
title: 'Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)'
arxiv_id: '2510.22954'
source_url: https://arxiv.org/abs/2510.22954
tags:
- qwen
- b-instruct
- human
- responses
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces INFINITY-CHAT, a large-scale dataset of 26K
  real-world open-ended user queries to language models. It develops the first comprehensive
  taxonomy of such queries, comprising 6 top-level categories and 17 subcategories.
---

# Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)

## Quick Facts
- **arXiv ID:** 2510.22954
- **Source URL:** https://arxiv.org/abs/2510.22954
- **Reference count:** 40
- **Primary result:** Introduces INFINITY-CHAT dataset and reveals "Artificial Hivemind" effect where diverse LMs produce semantically similar outputs for open-ended prompts, with implications for alignment and creativity.

## Executive Summary
This paper introduces INFINITY-CHAT, a large-scale dataset of 26K open-ended user queries to language models, and systematically analyzes mode collapse across 70+ LMs. The research reveals two key phenomena: intra-model repetition where individual models generate semantically similar responses across multiple runs (avg similarity > 0.8 in 79% of cases), and inter-model homogeneity where different model families produce strikingly similar outputs (similarity ~0.82). The study also finds that reward models and LM judges fail to calibrate with human preferences when valid answers are subjective, highlighting a fundamental limitation in current alignment approaches that assume monolithic values rather than pluralistic preferences.

## Method Summary
The study constructs INFINITY-CHAT by mining 26K open-ended queries from WildChat and filtering for open-endedness using GPT-4o classification. For evaluation, INFINITY-CHAT100 (100 verified queries) is used with 25 annotators per item providing absolute ratings and pairwise preferences (31,250 total labels). The core methodology generates 50 responses per query per model using high-entropy decoding (temperature=1.0, top-p=0.9), then computes pairwise cosine similarity using OpenAI's text-embedding-3-small. The paper evaluates 70+ LMs, analyzing both intra-model repetition and inter-model homogeneity patterns, while also testing calibration of LMs, reward models, and LM judges against human preference distributions.

## Key Results
- Intra-model repetition: 79% of response pairs exceed 0.8 similarity even with high stochasticity settings
- Inter-model homogeneity: DeepSeek-V3 and gpt-4o share similarity of 0.82, suggesting convergent "solution spaces"
- Reward model miscalibration: Correlations with human ratings drop significantly on similar-quality subsets where annotators disagree

## Why This Works (Mechanism)

### Mechanism 1: Semantic Collapse under Open-Endedness
Language models collapse semantically into narrow "high probability" manifolds in open-ended tasks. Even with temperature=1.0 and top-p=0.9, models repeatedly sample from sharp probability peaks, manifesting as intra-model repetition where the same model generates semantically identical responses (avg similarity > 0.8) across runs. This suggests learned prior probabilities are overly concentrated.

### Mechanism 2: Cross-Model Training Priors (Inter-Model Homogeneity)
Diverse model families converge on similar outputs because they share overlapping training corpora, synthetic data contamination, or alignment objectives that penalize deviation. This creates an "Artificial Hivemind" where distinct models (e.g., GPT-4o vs. DeepSeek-V3) produce verbatim overlapping phrases, suggesting the "solution space" for open-ended prompts is constrained by shared pre-training priors rather than model-specific reasoning.

### Mechanism 3: Calibration Failure in Pluralistic Preferences
Reward Models and LM Judges fail to align with human judgments when valid answers are subjective because they're optimized for a single "consensus" ground truth rather than a distribution of valid preferences. Current alignment (RLHF) optimizes for average preference, but in open-ended scenarios humans naturally disagree. RMs assign divergent scalar scores to responses that humans rate as comparably good but different.

## Foundational Learning

- **Open-Endedness vs. Ground Truth**: The "Artificial Hivemind" is a failure mode specific to open-ended tasks without ground truth. Understanding this distinction is necessary to differentiate standard hallucination from semantic homogeneity. *Quick check:* Does the effect imply models are wrong, or just uniform? (Answer: Uniform).

- **Diversity Metrics (Pairwise Similarity)**: The paper quantifies "Hivemind" using average pairwise sentence embedding similarity. Cosine similarity > 0.8 indicates near-duplicate semantic content, not just similar topics. *Quick check:* If two models write different stories about "time as a river" using exact same adjectives, will their sentence embedding similarity be high? (Likely yes).

- **Pluralistic Alignment**: The paper critiques current RLHF for assuming "monolithic" values. You need to grasp why optimizing for the "average human" destroys the model's ability to satisfy "individual, idiosyncratic" human preferences. *Quick check:* Why do Reward Models correlate poorly with human ratings on items where humans disagree with each other?

## Architecture Onboarding

- **Component map:** INFINITY-CHAT dataset (26K queries) -> Response Pool (50 responses per query per model) -> Diversity Engine (OpenAI embeddings + PCA) -> Human Layer (25 annotators per item)

- **Critical path:** 1) Data Mining: Extract open-ended queries from WildChat using GPT-4o classification. 2) Generation: Sample responses (N=50) with high entropy settings. 3) Quantification: Compute pairwise cosine similarity. If > 0.8, flag as "Artificial Hivemind". 4) Calibration Check: Compare RM scalar scores against variance of human ratings.

- **Design tradeoffs:** Sentence Embeddings use text-embedding-3-small which captures semantic similarity effectively but may miss nuance in creative structure. Human Data uses 25 annotators which is expensive but necessary to distinguish "bad response" from "polarizing response".

- **Failure signatures:** Semantic Clustering where "Write a metaphor about time" results in only 2 clusters (River/Weaver) instead of 25 distinct clusters. Calibration Drop where correlation between RM and Human Rating drops below 0.3 when human annotator entropy increases.

- **First 3 experiments:**
  1. Reproduce the Cluster Map: Pick "Write a metaphor about time" prompt. Generate 50 responses from 3 different models. Plot embeddings. If they cluster into "River" and "Weaver" only, you've reproduced the effect.
  2. Intra-Model Stability Test: Generate 10 responses for "Name a snack for a space mission" using fixed model. Calculate pairwise similarity. If > 70% are > 0.7 similarity, the model is trapped in mode collapse.
  3. Calibration Audit: Take 20 prompts with high human disagreement. Compare RM scores against standard deviation of human scores. Poor inverse correlation indicates RM fails to recognize valid pluralistic responses.

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the primary causal mechanisms (shared training data, alignment procedures, memorization) driving high inter-model homogeneity across diverse LLMs? The authors state causes remain unclear due to proprietary training details and highlight need for future work to rigorously investigate sources of cross-model repetition.

- **Open Question 2:** To what extent is homogenization attributable to pretraining data versus post-training alignment (RLHF, SFT)? Appendix A.2 outlines extending the testbed to foundation models to disentangle roles of pretraining and post-training in shaping convergent behaviors.

- **Open Question 3:** Can reward models and LM judges be modified to reliably recognize and reward multiple distinct responses of equal quality? While the paper notes current models fail to calibrate to human ratings on similar-quality responses, it infers current pipelines assume single consensus notion of quality and overlook pluralistic preferences.

## Limitations
- Analysis depends heavily on sentence embedding similarity as proxy for semantic homogeneity, which may not capture all dimensions of creative or semantic diversity
- Exact clustering methodology for sampling diverse responses from model pools remains unspecified, potentially affecting reproducibility
- Analysis assumes high similarity indicates problematic mode collapse rather than legitimate convergence on optimal solutions for open-ended tasks

## Confidence
- **High Confidence:** Empirical observation of intra-model repetition (similarity > 0.8 in 79% of cases) and inter-model homogeneity (similarity ~0.82 across distinct models)
- **Medium Confidence:** Interpretation that this represents problematic "mode collapse" rather than legitimate convergence is reasonable but could benefit from additional qualitative analysis
- **Low Confidence:** Proposed mechanism linking phenomenon to shared training data or alignment objectives remains speculative without access to proprietary training details

## Next Checks
1. **Qualitative Validation:** Select 20 high-similarity response pairs and have human annotators independently assess whether they represent genuine semantic duplication or superficial similarity masking substantive differences.

2. **Alternative Metric Comparison:** Repeat analysis using alternative diversity metrics (distinct-n, self-BLEU) to verify whether embedding similarity alone captures full scope of homogeneity effect.

3. **Controlled Experiment:** Generate responses using models trained on completely disjoint datasets (when available) to test whether inter-model similarity persists independent of shared training corpora.