---
ver: rpa2
title: 'Beyond Understanding: Evaluating the Pragmatic Gap in LLMs'' Cultural Processing
  of Figurative Language'
arxiv_id: '2510.23828'
source_url: https://arxiv.org/abs/2510.23828
tags:
- arabic
- proverbs
- understanding
- idioms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models show a consistent performance decline across
  tasks involving culturally grounded figurative language, with Egyptian Arabic idioms
  proving most challenging, followed by Arabic proverbs, then English proverbs. In
  a new pragmatic use task, accuracy dropped 14 percentage points compared to understanding,
  though providing contextual sentences improved performance by 11 percentage points.
---

# Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language

## Quick Facts
- **arXiv ID**: 2510.23828
- **Source URL**: https://arxiv.org/abs/2510.23828
- **Reference count**: 40
- **Primary result**: Large language models show consistent performance decline across culturally grounded figurative language tasks, with Egyptian Arabic idioms most challenging

## Executive Summary
This study reveals significant gaps in how large language models process culturally grounded figurative language, demonstrating that while models can often interpret figurative meaning, they struggle considerably with appropriate pragmatic use. The research introduces a new evaluation framework that goes beyond simple comprehension to assess whether models can use idioms and proverbs appropriately in context. Egyptian Arabic idioms proved most challenging for models, followed by Arabic proverbs and then English proverbs, with a notable 14 percentage point drop in accuracy when moving from understanding to pragmatic use tasks.

The findings highlight a fundamental limitation in current LLM capabilities: cultural and linguistic nuance remains difficult for models to navigate even when literal meaning is understood. Providing contextual sentences improved performance by 11 percentage points, suggesting that richer input can partially bridge the gap. The release of Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation, provides researchers with essential tools for further investigating these challenges and developing more culturally competent language models.

## Method Summary
The researchers evaluated multiple large language models across three task types: idiomatic understanding, proverb comprehension, and pragmatic use of figurative expressions. They used a three-language approach comparing Egyptian Arabic, Modern Standard Arabic, and English. For pragmatic use evaluation, models were tested on their ability to appropriately incorporate idioms into sentences rather than just identifying meanings. A new dataset called Kinayat was created specifically for Egyptian Arabic idioms, designed to support both figurative understanding and pragmatic use assessment. Human annotators provided ground truth judgments for comparison, with inter-annotator agreement measured across all evaluations.

## Key Results
- Performance declined consistently across task types: Egyptian Arabic idioms > Arabic proverbs > English proverbs
- Pragmatic use task accuracy dropped 14 percentage points compared to understanding tasks
- Contextual sentences improved performance by 11 percentage points
- Models achieved at most 86% agreement with human annotators on connotative meaning
- Kinayat dataset released as first resource for Egyptian Arabic idiom evaluation

## Why This Works (Mechanism)
The performance patterns observed likely stem from the models' training data composition and their limited exposure to culturally specific figurative language. Models trained predominantly on formal text may lack the nuanced cultural grounding required for dialect-specific idioms. The 11 percentage point improvement with contextual sentences suggests that models can leverage surrounding information to compensate for cultural knowledge gaps, but this compensation is incomplete. The significant drop in pragmatic use accuracy (14 percentage points) indicates that comprehension alone does not translate to appropriate application, pointing to a disconnect between semantic understanding and pragmatic competence in current LLMs.

## Foundational Learning
- **Cultural grounding in language**: Understanding that figurative expressions carry cultural context beyond literal meaning - needed to interpret why certain expressions are challenging for models; quick check: identify cultural references in common idioms
- **Pragmatic language use**: Ability to use language appropriately in social contexts rather than just understanding meaning - needed to distinguish between comprehension and application; quick check: test if model can generate contextually appropriate responses
- **Cross-linguistic figurative expression patterns**: Recognition that idioms and proverbs vary significantly across languages and cultures - needed to explain performance differences between Arabic and English; quick check: compare idiom structures across languages
- **Human-LLM agreement metrics**: Methods for evaluating model performance against human judgment - needed to establish baseline performance standards; quick check: calculate Cohen's kappa for inter-annotator agreement
- **Contextual embedding utilization**: How models leverage surrounding text to inform interpretation - needed to explain the 11 percentage point improvement; quick check: test model with and without contextual sentences
- **Dialectal variation impact**: Understanding how regional language variations affect model performance - needed to explain Egyptian Arabic challenges; quick check: compare performance across different Arabic dialects

## Architecture Onboarding
- **Component map**: Input text -> Language identification -> Figurative expression detection -> Cultural context analysis -> Pragmatic appropriateness scoring -> Output generation
- **Critical path**: Text processing → Cultural grounding assessment → Contextual integration → Pragmatic validation → Response generation
- **Design tradeoffs**: Broad language coverage vs. deep cultural specificity, general figurative language models vs. dialect-specific models, understanding vs. pragmatic use evaluation
- **Failure signatures**: Consistent underperformance on dialect-specific idioms, inability to generate contextually appropriate figurative expressions, high variance in performance across cultural contexts
- **First experiments**: 1) Test model performance on idioms with and without cultural context explanation, 2) Evaluate pragmatic use across different formality levels, 3) Compare dialect-specific vs. general Arabic model performance

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research, including: How can we better integrate cultural knowledge into LLM training to improve performance on dialect-specific figurative language? What architectural modifications might enable models to better distinguish between literal and figurative meanings in culturally ambiguous contexts? Can we develop evaluation frameworks that more accurately capture the pragmatic competence required for real-world language use? How does the performance gap between understanding and pragmatic use manifest across different types of figurative expressions beyond idioms and proverbs?

## Limitations
- Relatively small sample size (10 prompts per model and language) may limit generalizability
- The 86% maximum agreement with human annotators suggests persistent evaluation challenges
- Kinayat dataset representativeness of full Egyptian Arabic figurative expression spectrum remains to be established
- Findings may not generalize to other dialects or non-Arabic figurative language forms

## Confidence
- **High confidence**: General pattern of performance decline across task types and positive impact of contextual information
- **Medium confidence**: Specific magnitude of performance differences (14 percentage points, 11 percentage points) and 86% maximum agreement with human annotators
- **Low confidence**: Broader implications for LLM deployment in culturally diverse contexts and generalizability to other dialects or figurative language forms

## Next Checks
1. Replicate the pragmatic use task with a larger sample size (50-100 prompts per language) to establish more robust performance baselines
2. Conduct cross-validation using multiple human annotator groups to verify the consistency of the 86% agreement threshold
3. Expand evaluation to include additional Arabic dialects and non-Arabic figurative language forms to test generalizability of findings