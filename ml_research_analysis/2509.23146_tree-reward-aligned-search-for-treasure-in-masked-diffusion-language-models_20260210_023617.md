---
ver: rpa2
title: Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models
arxiv_id: '2509.23146'
source_url: https://arxiv.org/abs/2509.23146
tags:
- reward
- arxiv
- diffusion
- preprint
- masked
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning masked diffusion
  language models (MDLMs) with task-specific rewards during inference, a problem that
  has been underexplored compared to autoregressive models. The key difficulties arise
  from the parallel unmasking in MDLMs, which leads to highly correlated search branches
  and inefficient exploration, and the high variance in reward estimation due to the
  categorical nature of MDLM predictions.
---

# Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models

## Quick Facts
- arXiv ID: 2509.23146
- Source URL: https://arxiv.org/abs/2509.23146
- Reference count: 40
- Primary result: TREASURE achieves state-of-the-art reward alignment for MDLMs across four controllable generation tasks while using matched compute budgets.

## Executive Summary
This paper addresses the challenge of aligning masked diffusion language models (MDLMs) with task-specific rewards during inference. The key innovation is TREASURE, a tree-search method that overcomes the limitations of parallel unmasking in MDLMs—namely, highly correlated search branches and high-variance reward estimation. By introducing UNMASKBRANCH (branching at first-hitting unmasking events) and RESUBSTITUTESCORE (deterministic low-variance scoring), TREASURE achieves superior reward alignment compared to existing methods while maintaining diversity, particularly in low-NFE regimes.

## Method Summary
TREASURE is a tree-search algorithm designed specifically for MDLMs that combines two core innovations. UNMASKBRANCH uses first-hitting sampling to jump directly to the next unmasking event, selecting one masked position uniformly and expanding the top-b(n) token candidates from the model's prediction at that single timestep. RESUBSTITUTESCORE provides deterministic reward estimates by filling masked positions with argmax predictions rather than sampling, reducing variance while maintaining approximation quality. The method operates within a fixed NFE budget, naturally scaling with sequence length, and includes theoretical guarantees for both branching efficiency and tree width monotonicity.

## Key Results
- TREASURE achieves state-of-the-art reward scores across four tasks (perplexity, linguistic acceptability, sentiment, toxicity) under matched NFE budgets
- Outperforms Best-of-N and FK-steering baselines, especially in low-NFE regimes (NFE ∈ {2,4,6,8})
- Maintains competitive diversity (Distinct-n) while improving reward scores
- Demonstrates improved branching efficiency in terms of NFEs and provides theoretical error bounds for reward approximation

## Why This Works (Mechanism)

### Mechanism 1: Commitment-Event Branching for Diverse Exploration
Branching only at unmasking events produces diverse search trajectories while requiring only one model evaluation per parent node. First-hitting sampling jumps directly to the next unmasking time using a closed-form equation, avoiding wasted evaluations on steps where no token commits. At each commitment event, UNMASKBRANCH selects one masked position uniformly and expands the top-b(n) token candidates from the model's prediction at that single timestep.

### Mechanism 2: Resubstitution Scoring for Low-Variance Value Estimation
Deterministically filling masked positions with argmax tokens produces reward estimates that approximate the expected reward with error bounded by the model's predictive uncertainty. RESUBSTITUTESCORE reuses the probability distribution from the branching step to construct a single proxy completion, avoiding sampling variance from categorical draws.

### Mechanism 3: Tree Width Monotonicity for Guaranteed Improvement
Increasing the tree width m(n) is guaranteed to produce final rewards at least as good as narrower trees. With deterministic tie-breaking and coupled randomness, a larger tree width means the final candidate pool is a superset of the smaller pool, so the maximum reward over a superset cannot decrease.

## Foundational Learning

- **First-Hitting Sampling (FHS) for MDLMs**: Why needed: TREASURE relies on FHS to skip intermediate diffusion steps and jump directly to unmasking events. Quick check: Given n masked tokens remaining and current time τ_n, can you derive the next unmasking time τ_{n-1} using the inverse noise schedule?

- **KL-Regularized Test-Time Alignment**: Why needed: The paper frames alignment as optimizing a tradeoff between reward maximization and staying close to the pretrained distribution. Quick check: If λ → 0 in the KL-regularized objective, what happens to the target distribution p_tar?

- **Hamming-Lipschitz Continuity for Rewards**: Why needed: The theoretical guarantee for RESUBSTITUTESCORE depends on the reward function having bounded sensitivity to token changes. Quick check: For a sentiment classifier that outputs 1 for positive and 0 for negative text, is this Hamming-Lipschitz? What about perplexity computed from a language model?

## Architecture Onboarding

- Component map: MDLM (pretrained) -> UNMASKBRANCH (expands candidates) -> RESUBSTITUTESCORE (prunes via reward) -> TopK selection -> loop until fully unmasked
- Critical path: 1) Initialize with all-mask sequence at τ_L = 1; 2) For each node, call UNMASKBRANCH to get (Z, τ_{n-1}, μ_n); 3) For each child in Z, call RESUBSTITUTESCORE to get deterministic reward estimate; 4) Prune to top m(n) candidates; 5) Repeat until n=0, return argmax by reward
- Design tradeoffs: Beam width b(n) vs. tree width m(n) controls local vs. global exploration; NFE budget vs. sequence length scales naturally with sequence length; resubstitution vs. sampled scoring balances variance vs. bias
- Failure signatures: Low diversity in early steps suggests branching inefficiency; unstable pruning indicates high-variance estimates; reward overfitting suggests reward function limitations
- First 3 experiments: 1) Sanity check with b(n)=2, m(n)=1 on 16-token prompt; 2) Ablate RESUBSTITUTESCORE vs. sampled scoring under fixed NFE; 3) Scale tree width, observe monotonicity from m(n) ∈ {1, 2, 4, 8, 16}

## Open Questions the Paper Calls Out

- How can TREASURE be adapted to scale effectively for long-context and multimodal Masked Diffusion Language Models (MDLMs)?
- Can theoretically-grounded adaptive schedules be developed to dynamically balance exploration (beam width) and efficiency?
- What are the theoretical implications of unmasking multiple token groups simultaneously (parallel unmasking) on the sampling distribution?

## Limitations

- Theoretical guarantees for branching efficiency and approximation quality may not hold under practical conditions with stochastic rewards
- Hamming-Lipschitz assumption for reward functions may not apply to discrete classification tasks with sharp decision boundaries
- Tree width monotonicity guarantee may not manifest in practice due to reward noise or finite-sample effects

## Confidence

- **High Confidence (Experimental Claims):** TREASURE achieves SOTA reward scores across all four tasks under matched NFE budgets; maintains diversity while improving reward; shows consistent improvements over baselines
- **Medium Confidence (Theoretical Claims):** UNMASKBRANCH reduces NFE compared to naive branching; RESUBSTITUTESCORE provides low-variance reward estimates; tree width monotonicity guarantees
- **Low Confidence (Unvalidated Assumptions):** Reward approximation error remains bounded across all tasks and prompts; model confidence distributions at commitment events are sufficiently diverse; theoretical guarantees extend to practical finite-NFE settings

## Next Checks

**Validation Check 1: Branching Diversity Analysis**
For a fixed prompt and TREASURE configuration (b(n)=4, m(n)=8), record the distribution of unique tokens selected at each commitment event across 20 runs. Compute the effective branch count (unique branches / b(n) × 100%) at each step. Compare against a baseline that branches at every diffusion step. If effective branch count for TREASURE is <30% while naive sampling achieves >70%, this suggests the commitment-event assumption may not hold.

**Validation Check 2: Resubstitution Error Quantification**
For a subset of 5 prompts, run TREASURE with resubstitution scoring and simultaneously estimate the true expected reward using 100 sampled completions per candidate. Compute the mean absolute error between resubstitution estimates and Monte Carlo estimates. If average error >0.1 reward units, this indicates the approximation may be too loose for reliable pruning.

**Validation Check 3: Tree Width Monotonicity Stress Test**
For a fixed prompt and beam width (b(n)=6), run TREASURE with tree widths m(n) ∈ {1, 2, 4, 8, 16, 32} under NFE=8 constraint. Plot final reward vs. tree width. If the curve plateaus after m(n)=8 or shows non-monotonic behavior, this suggests the theoretical guarantee may not hold in practice.