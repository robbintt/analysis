---
ver: rpa2
title: 'APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs'
arxiv_id: '2511.17818'
source_url: https://arxiv.org/abs/2511.17818
tags:
- annotations
- counterfactual
- dataset
- behavior
- potassium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using LLMs to generate counterfactual annotations
  for off-policy evaluation (OPE) in healthcare, addressing the scalability challenge
  of expert-labeled annotations. The method guides LLMs to predict clinical features
  under alternate treatments using domain knowledge, then transforms these predictions
  into counterfactual annotations via known reward functions.
---

# APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs

## Quick Facts
- arXiv ID: 2511.17818
- Source URL: https://arxiv.org/abs/2511.17818
- Reference count: 30
- LLMs generate counterfactual annotations for OPE in healthcare, reducing RMSE by up to 83% in high-distribution-shift settings.

## Executive Summary
This paper proposes using large language models (LLMs) to generate counterfactual annotations for off-policy evaluation (OPE) in healthcare settings, addressing the scalability challenge of expert-labeled annotations. The method guides LLMs to predict clinical features under alternate treatments using domain knowledge, then transforms these predictions into counterfactual annotations via known reward functions. Evaluated on MIMIC-IV data for IV potassium and sodium repletion, multiple LLMs achieved comparable performance in predicting downstream lab values. Incorporating LLM-generated counterfactual annotations significantly improved OPE estimates, reducing RMSE by up to 83% in high-distribution-shift settings.

## Method Summary
The APRIL framework uses LLMs to generate counterfactual annotations for OPE by predicting how clinical features evolve under alternate treatments. The approach combines structured patient context, domain knowledge from UpToDate, and JSON-formatted queries to prompt LLMs for lab value predictions. These predictions are then transformed into scalar rewards using known clinical reward functions, and incorporated into a Direct Method (DM) OPE estimator with linear regression. The method addresses the scalability challenge of expert-labeled annotations while maintaining clinical validity.

## Key Results
- Multiple LLMs (o1, o3-mini, gpt-4o-mini, Gemini 1.5, Claude 3.7) achieved comparable performance predicting downstream lab values from patient context
- LLM-generated counterfactual annotations reduced RMSE by up to 83% in high-distribution-shift settings (e.g., dosage cohorts)
- Diminishing returns observed with additional annotations, quantifiable via marginal entropy over the action space
- Averaging annotations from multiple LLMs did not improve performance over the best single source

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can predict downstream clinical lab values from structured patient context with clinically meaningful accuracy.
- Mechanism: Prompts combine structured patient state (15 features from 4hr pre-treatment window), domain knowledge from UpToDate, and JSON-formatted queries for lab values under specified treatments. LLMs output both prediction and justification.
- Core assumption: LLMs encode sufficient medical knowledge to reason about electrolyte response trajectories.
- Evidence anchors: LLMs predicted serum potassium and sodium with clinically meaningful accuracy; differences across LLMs were modest.
- Break condition: Performance degrades for multi-feature outcomes, rare conditions, or underrepresented populations.

### Mechanism 2
- Claim: Predicted lab values can be deterministically transformed into scalar counterfactual rewards via known clinical reward functions.
- Mechanism: Reward function R(x) maps lab value x to [0,1] using reference range [a,b] with Gaussian decay outside the range.
- Core assumption: Reward functions are known, single-outcome, and clinically meaningful.
- Evidence anchors: Full reward function specification with reference ranges and Gaussian decay formulation.
- Break condition: Rewards depend on multiple clinical features or outcomes not captured by single-lab functions.

### Mechanism 3
- Claim: Augmenting DM OPE with LLM-generated counterfactual annotations reduces RMSE, especially under high distribution shift.
- Mechanism: DM+ learns reward model from behavior data and counterfactual annotations, then estimates target policy value.
- Core assumption: IPS ratios are unknown or unreliable, making DM safer than IS-based estimators.
- Evidence anchors: RMSE reduced by 83% in potassium dosage cohort and 49% in sodium dosage cohort under high distribution shift.
- Break condition: High policy overlap, annotation noise, or when true IPS ratios are known and DR/IS methods become preferable.

## Foundational Learning

- Concept: Off-Policy Evaluation (OPE)
  - Why needed here: Core problem—estimating target policy's expected reward using only offline behavior data when deployment is unsafe.
  - Quick check question: Why does standard OPE fail when target policy frequently takes actions rare or absent in behavior dataset?

- Concept: Counterfactual annotations
  - Why needed here: Augmentation strategy—synthetic predictions of what reward would have been observed under alternative action.
  - Quick check question: How do counterfactual annotations differ from simply collecting more behavior data?

- Concept: Direct Method vs Importance Sampling estimators
  - Why needed here: Paper uses DM because IPS ratios are inferred from finite data and may be unreliable.
  - Quick check question: Under what conditions would IS-based or doubly-robust estimator outperform DM?

## Architecture Onboarding

- Component map: Patient context -> prompt construction -> LLM counterfactual lab prediction -> reward computation -> R̂+ training -> policy value estimate

- Critical path: Patient context → prompt construction → LLM counterfactual lab prediction → reward computation → R̂+ training → policy value estimate

- Design tradeoffs:
  - DM vs DR/IS: Chose DM due to unknown IPS ratios; DR could add robustness but requires reliable IPS
  - Single vs multiple LLM sources: Averaging multiple LLMs did not improve over best single source
  - Annotation volume: Diminishing returns set in as marginal action entropy approaches maximum (~500-700 annotations)

- Failure signatures:
  - High policy overlap: Little room for improvement
  - Low LLM prediction accuracy: Noise propagates into annotations
  - Complex reward structure: Method limited to single-lab reward functions

- First 3 experiments:
  1. Replicate lab prediction accuracy: Query multiple LLMs on MIMIC-IV contexts, compare predicted vs actual downstream labs using weighted F1 across clinically relevant categories.
  2. Evaluate OPE improvement under controlled shift: Construct behavior/target splits by gender, comorbidity, and dosage; compare DM vs DM+ RMSE with 500 annotations per condition.
  3. Validate entropy-based stopping criterion: Track marginal action entropy H(A) as annotations are added; confirm RMSE plateaus when entropy approaches maximum.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-generated counterfactual annotations improve OPE accuracy for clinical tasks involving complex, multi-feature reward functions?
- Basis in paper: [explicit] Section 5 states the study is limited to single-feature reward functions and future work should evaluate more complex outcomes.
- Why unresolved: Current study only validates method on single-lab reward functions.
- What evidence would resolve it: Successful application on tasks where reward is a composite score or depends on multiple clinical variables.

### Open Question 2
- Question: Is inclusion of external domain knowledge (e.g., UpToDate text) strictly necessary for reliable lab value predictions, or can LLMs rely solely on structured EHR data?
- Basis in paper: [inferred] Section 3.2 notes prompt includes UpToDate to guide LLM due to extraneous EHR data, but does not ablate this component.
- Why unresolved: Unclear if performance boost comes from LLM's pre-trained knowledge or injected context.
- What evidence would resolve it: Ablation study comparing prediction accuracy between prompts with and without domain knowledge text.

### Open Question 3
- Question: Do LLM-generated annotations provide similar benefits when incorporated into Doubly Robust (DR) or Importance Sampling (IS) estimators compared to Direct Method (DM) used?
- Basis in paper: [inferred] Section 3.3 explains authors chose DM to mitigate bias from inferred IPS ratios, leaving interaction with other estimators unexplored.
- Why unresolved: Different estimators handle noise and bias differently; annotations might interact unpredictably with IS or DR estimators.
- What evidence would resolve it: Empirical comparison of RMSE for DM+, IS+, and DR+ estimators using same LLM-generated annotations.

### Open Question 4
- Question: Does the proposed entropy-based metric generalize to determine annotation saturation points in tasks with larger action spaces?
- Basis in paper: [inferred] Authors observed diminishing returns at ~700 annotations for small action spaces (4-6 actions), but unclear if metric scales for complex decision problems.
- Why unresolved: Relationship between action space size and "saturation point" of marginal entropy is not theoretically established.
- What evidence would resolve it: Evaluation on task with significantly larger discrete action space to see if entropy metric accurately flags diminishing returns.

## Limitations
- Method restricted to single-outcome reward functions that can be deterministically transformed from predicted clinical features
- Annotation quality bounded by LLM prediction accuracy, which degrades for multi-feature outcomes or rare conditions
- Assumes known, clinically meaningful reward functions with reference ranges
- Only evaluated on MIMIC-IV data for two specific electrolyte repletion scenarios
- Marginal improvement diminishes as behavior and target policies overlap more

## Confidence
- **High**: LLMs can predict downstream clinical lab values from structured patient context with clinically meaningful accuracy; predicted lab values can be deterministically transformed into scalar counterfactual rewards via known clinical reward functions.
- **Medium**: Augmenting direct method OPE with LLM-generated counterfactual annotations reduces RMSE, especially under high distribution shift.
- **Low**: The method generalizes well to other clinical domains, treatment types, or reward structures beyond single-lab transformations.

## Next Checks
1. Test LLM prediction accuracy on subpopulations with limited training data (rare conditions, specific demographics) to assess robustness.
2. Evaluate performance on clinical domains with multi-attribute rewards or complex dependencies between clinical features.
3. Conduct ablation studies varying annotation volume beyond 500 per condition to precisely quantify the marginal entropy stopping criterion.