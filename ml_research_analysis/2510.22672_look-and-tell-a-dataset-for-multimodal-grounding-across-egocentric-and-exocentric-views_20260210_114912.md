---
ver: rpa2
title: 'Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric
  Views'
arxiv_id: '2510.22672'
source_url: https://arxiv.org/abs/2510.22672
tags:
- gaze
- speech
- multimodal
- dataset
- mention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multimodal dataset for studying referential
  communication across egocentric and exocentric perspectives, featuring synchronized
  gaze, speech, and video recordings from 25 participants in a kitchen setting. The
  dataset includes 2,707 annotated referential expressions, 3D scene reconstructions,
  and rich metadata, enabling systematic evaluation of how spatial representations
  (2D vs.
---

# Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views

## Quick Facts
- arXiv ID: 2510.22672
- Source URL: https://arxiv.org/abs/2510.22672
- Reference count: 10
- Introduces a multimodal dataset for studying referential communication across egocentric and exocentric perspectives with synchronized gaze, speech, and video recordings

## Executive Summary
This work introduces a multimodal dataset for studying referential communication across egocentric and exocentric perspectives, featuring synchronized gaze, speech, and video recordings from 25 participants in a kitchen setting. The dataset includes 2,707 annotated referential expressions, 3D scene reconstructions, and rich metadata, enabling systematic evaluation of how spatial representations (2D vs. 3D; ego vs. exo) affect multimodal grounding. A key finding is that gaze precedes speech in 41.1% of referential expressions, with an average lag of −189 ms, indicating that speakers often initiate utterances while still fixating on the referent.

## Method Summary
The dataset was collected using Meta Project Aria glasses with eye tracking, audio, and egocentric video (1408×1408 at 30 fps), complemented by GoPro exocentric cameras. Participants (25 total, 19 female, 6 male) performed 5 sessions each in a kitchen setting, collecting 3.67 hours of data (396,208 RGB frames). The annotation pipeline involved WhisperX for transcription with word-level timestamps, GPT-based prompting for mention extraction, Molmo for object detection (2D points), and SAM2 for mask propagation. Manual annotation was required for 641 difficult cases (tiny/ambiguous items). Gaze-speech synchrony was measured as Δ = fixation offset minus mention onset, yielding a mean lag of −189 ms with 41.1% gaze preceding speech.

## Key Results
- Gaze precedes speech in 41.1% of referential expressions
- Mean gaze-speech lag is −189 ms (SD = 541 ms)
- Dataset contains 2,707 annotated referential expressions with >90% mention coverage
- Systematic annotation pipeline achieves comprehensive multimodal grounding data

## Why This Works (Mechanism)
The study demonstrates that referential grounding in multimodal communication involves complex temporal coordination between gaze and speech. The dataset captures naturalistic interactions where speakers reference objects while performing kitchen tasks, revealing that gaze often initiates before verbal mention, suggesting a cognitive planning phase where visual attention precedes linguistic formulation. The synchronized egocentric and exocentric views enable analysis of how spatial perspectives influence grounding strategies.

## Foundational Learning
- Egocentric vs. exocentric perspectives: Why needed - to understand how different spatial representations affect grounding; Quick check - verify both view types are properly synchronized
- Gaze-speech synchrony: Why needed - measures temporal coordination in referential communication; Quick check - validate Δ metric calculation across multiple mention types
- Multimodal annotation pipeline: Why needed - enables comprehensive analysis of complex interactions; Quick check - ensure all modalities are properly aligned in time
- Coreference resolution in situated dialogue: Why needed - handles pronoun resolution in context-dependent references; Quick check - verify 25-token window effectiveness
- 3D scene reconstruction from egocentric video: Why needed - provides spatial context for grounding analysis; Quick check - validate point cloud quality against ground truth

## Architecture Onboarding

**Component Map:**
Meta Aria glasses (eye tracking, audio, egocentric video) -> WhisperX transcription -> GPT mention extraction -> Molmo detection -> SAM2 mask propagation -> Manual annotation -> Analysis

**Critical Path:**
Data collection (Aria + GoPro) -> Transcription (WhisperX) -> Mention extraction (GPT) -> Object localization (Molmo + SAM2) -> Gaze-speech synchrony analysis

**Design Tradeoffs:**
- Automated vs. manual annotation: 747 mentions required manual handling due to small/ambiguous objects
- Sampling rate for Molmo: Unspecified nmin threshold affects localization consistency
- Coreference resolution window: 25 tokens may miss longer-distance references

**Failure Signatures:**
- Molmo localization fails on small objects (spice containers) and look-alikes (sugar vs. wheat-flour)
- Coreference resolution errors when pronouns lack clear antecedents within 25-token window
- Gaze-speech lag calculation errors if fixation detection algorithm parameters are incorrect

**3 First Experiments:**
1. Reconstruct the complete GPT mention extraction pipeline using Appendix A.1 templates and test on a held-out subset
2. Evaluate Molmo/SAM2 performance with varying sampling rates on objects known to cause failures
3. Cross-validate gaze-speech synchrony metrics by independently processing raw .vrs files

## Open Questions the Paper Calls Out
None

## Limitations
- GPT mention extraction templates not fully specified, creating potential reproducibility gaps
- Molmo sampling parameters (nmin threshold) and SAM2 hyperparameters unspecified, affecting localization consistency
- 747 mentions required manual handling, indicating systematic failures on small/ambiguous objects
- Coreference resolution rules (25-token window, cooking-action heuristics) may miss contextual cues

## Confidence
- High: Gaze-speech synchrony findings, overall dataset construction methodology
- Medium: Mention extraction pipeline, object localization accuracy
- Low: Impact of manual intervention on quantitative metrics

## Next Checks
1. Reconstruct the complete GPT mention extraction pipeline using the referenced Appendix A.1 templates and test on a held-out subset
2. Evaluate Molmo/SAM2 performance with varying sampling rates on objects known to cause failures (spice containers, look-alikes)
3. Cross-validate gaze-speech synchrony metrics by independently processing raw .vrs files with the described fixation detection algorithm