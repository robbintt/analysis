---
ver: rpa2
title: 'Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling
  Framework for Deep Brain Stimulation'
arxiv_id: '2510.25023'
source_url: https://arxiv.org/abs/2510.25023
tags:
- shared
- private
- latents
- spire
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SPIRE, a deep multi-encoder autoencoder designed
  to disentangle shared and private neural dynamics in multi-region recordings. SPIRE
  uses novel alignment and disentanglement losses to factorize neural activity into
  cross-regional shared latents and region-specific private latents, trained only
  on baseline data.
---

# Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation

## Quick Facts
- arXiv ID: 2510.25023
- Source URL: https://arxiv.org/abs/2510.25023
- Reference count: 40
- Key outcome: SPIRE disentangles shared and private neural dynamics using multi-encoder autoencoders, outperforming classical models on synthetic benchmarks and revealing stimulation-specific signatures in human DBS recordings.

## Executive Summary
SPIRE is a deep learning framework designed to disentangle shared (cross-regional) and private (region-specific) neural dynamics from multi-region recordings. Using a dual GRU encoder-decoder architecture with alignment and disentanglement losses, SPIRE factors neural activity into separate subspaces without requiring stimulation data during training. The model demonstrates superior performance on synthetic benchmarks under nonlinear distortions and temporal misalignments, and reveals that shared latents encode stimulation-specific reorganization in human DBS recordings.

## Method Summary
SPIRE employs dual GRU encoders per region to project hidden states into shared (z_sh) and private (z_pr) latent sequences, aligned via ConvAlign modules and regularized through orthogonality constraints. Trained solely on baseline data, the model recovers cross-regional structure and generalizes to stimulation conditions. The architecture includes variance guards to prevent collapse, alignment regularizers to maintain interpretability, and phased loss schedules that prioritize reconstruction early and disentanglement later.

## Key Results
- On synthetic benchmarks, SPIRE outperforms DLAG in recovering shared/private structure under nonlinear distortions and time-varying delays.
- Applied to human DBS recordings, shared latents reliably encode stimulation-specific signatures that generalize across brain regions and frequencies.
- Shared latents decode stimulation frequency significantly better than private latents (p < 0.001), indicating systematic reorganization in the shared subspace.

## Why This Works (Mechanism)

### Mechanism 1
Factorizing neural activity into shared and private subspaces isolates cross-regional coordination from region-specific dynamics. Each region has a GRU encoder projecting hidden states into separate shared (z_sh) and private (z_pr) latent sequences. Orthogonality loss (L_orth) penalizes cross-covariance between standardized latents, reducing redundancy. Variance guards prevent collapse by nudging shared latents toward unit variance and enforcing a floor on private variance. This works if shared and private components are statistically separable through linear projections.

### Mechanism 2
Lightweight temporal alignment modules (ConvAlign + linear mapper) enable robust shared-subspace recovery under inter-regional lags and nonlinear distortions. ConvAlign applies depthwise 1D convolution (per-dimension kernels initialized as impulses) before a learnable linear mapper (initialized to identity). This allows small temporal shifts and subspace rotations between regions while regularization terms keep transformations near-identity for interpretability. Effective when temporal misalignments are modest and can be approximated by short convolutional kernels.

### Mechanism 3
Training on baseline (off-stimulation) data establishes a reference frame; applying this model to stimulation data reveals perturbation-induced reorganization in latent space. The model learns intrinsic coordination without stimulation artifacts. When stimulation-on data is passed through the frozen encoder, shifts in latent distributions reflect reorganization rather than training contamination. This assumes stimulation effects are shifts within the learned space, not out-of-distribution transformations.

## Foundational Learning

- **Concept: Variance-Invariance-Covariance Regularization (VICReg)**
  - Why needed: VICReg enforces similar variance, invariance to region identity, and low covariance across dimensions in shared latents.
  - Quick check: Can you explain why VICReg might be preferred over simple cosine similarity or MSE for aligning latent subspaces?

- **Concept: Orthogonality Constraints in Latent Space**
  - Why needed: L_orth penalizes cross-covariance between shared and private latents to enforce disentanglement and prevent redundant encoding.
  - Quick check: If shared and private latents had high cross-covariance, what would reconstruction analysis reveal about their relative contributions?

- **Concept: Sequence-to-Sequence Autoencoders with GRUs**
  - Why needed: SPIRE uses GRU encoders/decoders for temporal modeling of LFP signals, with hidden states carrying temporal context essential for interpreting latent trajectories.
  - Quick check: How does a GRU decoder differ from a simple linear decoder in terms of the dynamics it can reconstruct?

## Architecture Onboarding

- **Component map:** Input → GRU encoder → hidden states → linear projections → shared/private latents → ConvAlign + mapper → aligned shared latents → Concatenate [shared, private] → GRU decoder → reconstruction

- **Critical path:** 1) Input → GRU encoder → hidden states 2) Hidden states → linear projections → shared/private latents 3) Shared latents → ConvAlign + mapper → aligned shared latents 4) Concatenate [shared, private] → GRU decoder → reconstruction 5) Compute all loss terms; backpropagate

- **Design tradeoffs:** Latent dimensionality (d_sh, d_pr) balances variance capture vs overfitting; ConvAlign kernel size (default 9) handles lags but increases parameters; loss weight scheduling emphasizes reconstruction early, alignment and disentanglement later.

- **Failure signatures:** Private variance collapse (all variance absorbed by shared latents); private latents near-zero. Shared-private leakage (high CCA between shared and private within a region). DLAG-style numerical instability (rank-deficient covariance matrices).

- **First 3 experiments:**
  1. Reproduce synthetic benchmark (D0–D2): Train SPIRE on provided synthetic datasets; verify CCA correlation with ground-truth latents matches reported values (~0.9 for shared in D1/D2).
  2. Ablation of L_orth: Remove orthogonality loss and measure increase in shared-private CCA; confirm disentanglement degradation.
  3. Latent decoding on held-out stimulation data: Train Random Forest to decode stimulation frequency from shared vs. private latents; verify shared latents achieve higher accuracy (as in Figure 7).

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SPIRE scale to more than two brain regions while maintaining effective shared-private disentanglement, and how does factorization quality degrade as the number of regions increases? The current architecture was designed and validated only for two-region configurations; alignment and orthogonality losses may become unstable with multiple shared subspaces.

- **Open Question 2:** What precise biophysical or circuit-level mechanisms do individual shared latent dimensions encode, and can they be mapped to interpretable neural processes? The model is trained purely on statistical reconstruction objectives without mechanism-level or anatomical constraints that would tie latent dimensions to specific neural circuits.

- **Open Question 3:** How would incorporating probabilistic objectives and uncertainty quantification affect SPIRE's latent decomposition and its ability to generalize under perturbation? The current deterministic autoencoder provides no measure of confidence in latent estimates, which may be critical for distinguishing genuine stimulation-induced reorganization from noise.

## Limitations
- Real DBS data is IRB-protected and unavailable, preventing direct validation of perturbation-based claims in Figures 4-7.
- Synthetic data generation is only described qualitatively in Appendix A.2.1; exact parameter values and noise models must be inferred.
- The orthogonal projection assumption may fail if shared/private dynamics are nonlinearly entangled beyond linear projection capacity.

## Confidence

- **High:** SPIRE architecture design, synthetic benchmark performance, variance guard functionality, orthogonality loss effects.
- **Medium:** Shared/private disentanglement quality in real DBS data, generalization of baseline-trained latents to stimulation conditions, decoding accuracy of stimulation frequency from shared latents.
- **Low:** Exact synthetic data generation parameters, ConvAlign robustness to large temporal shifts, encoder stability under stimulation-induced out-of-distribution inputs.

## Next Checks

1. **Synthetic benchmark replication:** Train SPIRE on D0-D2 datasets with inferred generator parameters; verify CCA recovery scores match reported ~0.9 for shared latents in D1/D2.
2. **Ablation of orthogonality loss:** Remove L_orth and measure increase in shared-private CCA within regions; confirm disentanglement degrades as expected.
3. **Real data preprocessing fidelity:** Replicate bipolar derivation, downsampling, and lag augmentation pipeline on available multi-region LFP data; verify alignment and variance guard stability.