---
ver: rpa2
title: 'AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak
  Defenders'
arxiv_id: '2505.24519'
source_url: https://arxiv.org/abs/2505.24519
tags:
- safety
- amia
- image
- intention
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMIA is an inference-time defense method that enhances the safety
  of large vision-language models (LVLMs) against jailbreak attacks without retraining.
  It combines automatic masking of image patches least relevant to the input text
  with joint intention analysis to uncover hidden harmful intents.
---

# AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders

## Quick Facts
- arXiv ID: 2505.24519
- Source URL: https://arxiv.org/abs/2505.24519
- Reference count: 23
- Primary result: AMIA improves defense success rate against multimodal jailbreak attacks from 52.4% to 81.7% without retraining.

## Executive Summary
AMIA is an inference-time defense method that protects large vision-language models (LVLMs) against jailbreak attacks by combining automatic image patch masking with joint intention analysis. The method masks image patches least relevant to the input text to disrupt adversarial perturbations, then requires the model to explicitly analyze the intent of the request before generating a final response. Experiments across four jailbreak datasets show AMIA significantly improves safety metrics while preserving general utility with only a 2% average accuracy drop.

## Method Summary
AMIA operates without retraining by modifying inputs and prompts at inference time. The method uses an external vision encoder (VisRAG-Ret) to calculate cosine similarity between image patches and the text query, masking the K patches with lowest similarity scores. The masked image and original text are then fed into a structured prompt requiring the model to first generate an intention analysis (tagged [INTENTION ANALYSIS]) before producing the final response ([FINAL RESPONSE]). This approach leverages the LLM's text-based safety alignment while disrupting visual attack vectors through selective masking.

## Key Results
- Defense success rate improves from 52.4% to 81.7% across diverse LVLMs on jailbreak datasets
- General utility preserved with only 2% average accuracy drop on MMVP benchmark
- Modest inference overhead of approximately 14% due to additional processing steps
- Both masking and intention analysis components are essential, as shown by ablation studies

## Why This Works (Mechanism)

### Mechanism 1
Selectively masking image patches least relevant to the input text disrupts adversarial perturbations while preserving task-critical visual information. An external encoder calculates cosine similarity between image patches and the text query, masking the K patches with lowest similarity scores to filter out noise that might carry adversarial payloads.

### Mechanism 2
Explicitly forcing a model to generate an intention analysis reactivates the inherent text-based safety alignment of the LLM backbone. By requiring a structured output that first analyzes the intent before responding, the model leverages its stronger text-only safety training to reject harmful prompts.

### Mechanism 3
Combining visual masking with semantic analysis is necessary to handle multimodal attacks where single-modality defenses fail. Masking disrupts the visual attack surface while intention analysis addresses semantic jailbreaks, creating a synergistic defense that addresses both perturbation-based and semantic attacks.

## Foundational Learning

- **Concept: Vision-Language Alignment (CLIP/VisRAG)**
  - Why needed here: To understand how cosine similarity works between image patches and text for implementing the masking logic.
  - Quick check question: How would the masking strategy change if the vision encoder failed to align the text "cut the red wire" with the image patch containing the red wire?

- **Concept: Adversarial Perturbations (PGD)**
  - Why needed here: To understand what AMIA defends againstâ€”small pixel changes that can manipulate model outputs.
  - Quick check question: Why does masking a patch potentially disrupt a PGD attack more than it disrupts the semantic meaning of a natural image?

- **Concept: Inference-Time Defense**
  - Why needed here: To distinguish AMIA's approach of modifying inputs/prompts from fine-tuning or RLHF.
  - Quick check question: What is the trade-off of using inference-time defense (AMIA) versus safety fine-tuning regarding cost and model capability retention?

## Architecture Onboarding

- **Component map:** Input (Image + Text) -> Relevance Encoder (VisRAG-Ret) -> Masking Module -> Prompt Constructor -> Target LVLM
- **Critical path:** The relevance encoding must happen fast enough not to bottleneck the main LVLM generation. The masking ratio (K) is the single most sensitive hyperparameter.
- **Design tradeoffs:** K (Patches Masked) - increasing K improves safety but rapidly degrades general utility. The paper settles on K=3 as a balance. Encoder Choice - using a weaker encoder might mask relevant features by mistake.
- **Failure signatures:** Over-Refusal (K too high or aggressive prompt), Latency Spike (two forward passes plus longer generation), Semantic Drift (mask covers crucial diagram parts).
- **First 3 experiments:**
  1. Ablation Reproduction: Run LLaVA-v1.5-7B on VisualAdv-Harmbench with only masking and only intention analysis to verify synergistic lift.
  2. Sensitivity Sweep: Vary K from 0 to 5 on clean utility dataset (MMVP) to visualize the "accuracy cliff."
  3. Encoder Swap: Replace VisRAG-Ret with standard CLIP ViT-L/14 in masking step to measure sensitivity to encoder choice.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptive or input-aware masking strategies outperform the fixed N=16, K=3 configuration across diverse attack types and image resolutions? The paper uses fixed hyperparameters for simplicity but notes exploring adaptive strategies as an exciting direction.

- **Open Question 2:** How does AMIA's effectiveness scale to higher-resolution images and alternative visual encoders beyond VisRAG-Ret? The current evaluation uses VisRAG-Ret with fixed patch configurations; applicability to other encoders or high-resolution settings is untested.

- **Open Question 3:** Does AMIA maintain robustness against attack types not covered in current evaluation, such as black-box transfer attacks or multi-turn adversarial conversations? The paper evaluates only prompt-manipulation and optimization-based attacks, leaving emerging threat vectors untested.

## Limitations

- **Encoder dependency:** The masking strategy critically relies on VisRAG-Ret's ability to correctly identify semantically irrelevant patches. No ablation on alternative encoders is provided.
- **Adversarial adaptiveness:** The paper does not test adaptive attacks that specifically target the masking mechanism or intention-analysis prompt.
- **Judge reliability:** Safety metrics depend on ChatGPT judge ratings, which are known to be inconsistent and potentially vulnerable to manipulation.

## Confidence

- **High confidence:** The ablation results showing synergy of masking + intention analysis are internally consistent and well-documented.
- **Medium confidence:** The claimed 81.7% DSR improvement and 2% utility drop are supported by paper results but need replication on independent datasets.
- **Low confidence:** Claims about robustness against adaptive multimodal jailbreak attacks are not empirically validated.

## Next Checks

1. **Encoder robustness:** Replace VisRAG-Ret with standard CLIP encoder and measure change in DSR and utility to quantify defense's dependence on specific encoder choice.

2. **Adaptive attack evaluation:** Design a PGD-based attack targeting masked regions specifically or prompts bypassing intention-analysis step, and measure impact on AMIA's defense success rate.

3. **Judge consistency:** Evaluate inter-rater reliability of ChatGPT judge across multiple runs and compare ratings with human judgments on subset of jailbreak prompts to quantify measurement noise.