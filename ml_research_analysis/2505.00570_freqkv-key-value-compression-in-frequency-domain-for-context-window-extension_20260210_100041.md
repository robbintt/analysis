---
ver: rpa2
title: 'FreqKV: Key-Value Compression in Frequency Domain for Context Window Extension'
arxiv_id: '2505.00570'
source_url: https://arxiv.org/abs/2505.00570
tags:
- freqkv
- context
- compression
- cache
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FreqKV, a parameter-free method for extending
  context windows in large language models (LLMs) by compressing key-value (KV) cache
  in the frequency domain. The key insight is that KV states exhibit energy concentration
  in low-frequency components, allowing high-frequency components to be discarded
  with minimal information loss.
---

# FreqKV: Key-Value Compression in Frequency Domain for Context Window Extension

## Quick Facts
- arXiv ID: 2505.00570
- Source URL: https://arxiv.org/abs/2505.00570
- Authors: Jushi Kai; Yixuan Wang; Boyi Zeng; Haoli Bai; Bo Jiang; Ziwei He; Zhouhan Lin
- Reference count: 40
- Key outcome: Parameter-free frequency-domain KV compression extends context windows up to 256K tokens while maintaining stable perplexity (~7.73 on PG-19) without architectural modifications.

## Executive Summary
FreqKV extends context windows in large language models by compressing key-value cache in the frequency domain using Discrete Cosine Transform. The method exploits the observation that KV states concentrate energy in low-frequency components, allowing high-frequency components to be discarded with minimal information loss. By iteratively compressing earlier tokens while preserving recent ones uncompressed, FreqKV enables context extension beyond training lengths without requiring position extrapolation or architectural changes.

The approach achieves competitive perplexity and outperforms existing KV compression methods on long-context benchmarks while maintaining negligible compression overhead and significantly reduced training time compared to full fine-tuning approaches. The method is validated on LLaMA-2 and LLaMA-3 models across multiple long-context evaluation tasks.

## Method Summary
FreqKV applies Discrete Cosine Transform (DCT) to KV states along the sequence dimension, retaining low-frequency components based on a retaining ratio (default 0.5). Key states are compressed before applying Rotary Position Embeddings (RoPE), using cache indices rather than original sequence positions to avoid position extrapolation. A fixed number of initial "sink" tokens (default 4) remain uncompressed to stabilize attention. When the cache reaches maximum size, FreqKV compresses older tokens, appends new ones, and repeats. The method uses LoRA fine-tuning with minimal training steps compared to full fine-tuning.

## Key Results
- Extends context windows from 4K to 256K tokens while maintaining stable perplexity around 7.73 on PG-19
- Outperforms existing KV compression methods on LongBench, RULER, and Needle-in-a-Haystack benchmarks
- Matches or surpasses full fine-tuning performance on long-context language modeling with minimal training at 8K length
- Achieves negligible compression overhead (<0.3%) that does not grow with context length

## Why This Works (Mechanism)

### Mechanism 1
The core insight is that low-frequency components of KV states contain most essential information while high-frequency components are redundant. FreqKV applies DCT to transform KV states to frequency domain, retains low-frequency components, and applies inverse DCT with rescaling. This lossy compression preserves global semantic information while discarding local details. The assumption is that energy/information concentrates in low frequencies, though this may not hold for all domains (e.g., mathematical texts).

### Mechanism 2
Iterative compression enables context extension beyond training length by compressing KV cache when full. Earlier tokens undergo more compression iterations, aligning with autoregressive nature. Key states are compressed before RoPE, and RoPE is applied using cache position indices rather than original sequence positions. This avoids position extrapolation while maintaining positional understanding through compressed representations.

### Mechanism 3
Retaining initial "sink" tokens uncompressed stabilizes attention mechanisms. The model assigns disproportionate importance to initial tokens to stabilize softmax distribution. Keeping these tokens uncompressed prevents degradation. The assumption is that these attention sinks are critical for stability, though the exact number needed may vary by task.

## Foundational Learning

**Discrete Cosine Transform (DCT)**: Transforms sequence data to frequency domain using cosine functions of varying frequencies. Needed to understand how FreqKV compresses and reconstructs KV states. Quick check: For sequence length N, retaining L frequency components and applying IDCT yields a reconstructed sequence of length N.

**Rotary Position Embeddings (RoPE)**: Encodes relative position information into key and query vectors using rotational transformations. Needed to understand FreqKV's approach of compressing keys before RoPE application. Quick check: In standard RoPE, rotary embeddings are applied to key and query vectors before attention computation.

**KV Cache**: Stores past keys and values in autoregressive decoding to avoid recomputation. Needed to understand why compression is necessary (linear growth with sequence length) and how FreqKV modifies cache management. Quick check: During token N generation, the query attends to cached KV states from tokens 1 through N-1.

## Architecture Onboarding

**Component map**: DCT Module -> Frequency Filter -> IDCT with Rescale -> Cache Manager -> Attention with RoPE using cache indices

**Critical path**: Cache Full → Extract non-sink KV → DCT → Filter → IDCT → Rescale → Replace in Cache → Append new tokens. During attention: Load Sink/Compressed/New KV → Apply RoPE (keys) → Concatenate → Compute Attention.

**Design tradeoffs**: 
1. Retaining Ratio (γ): Higher γ retains more information but increases cache size and latency
2. Sink Size (S): More sink tokens provide stability but reduce compression ratio
3. Compression Frequency: More frequent compression increases overhead

**Failure signatures**: 
1. Perplexity Explosion: Omitting rescaling factor causes signal amplitude growth with each compression
2. Performance Drop on Long Contexts: Compressing keys after RoPE causes OOB position embeddings
3. Degradation on Detail-Heavy Tasks: Significant performance gap on symbolic/math datasets

**First 3 experiments**:
1. Perturbation Analysis: Perturb input tokens, compute KV states, transform to frequency domain, measure cosine similarity between perturbed and original low/high-frequency components
2. Retaining Ratio Ablation: Train models with different γ (0.25, 0.5, 0.75) and evaluate perplexity/latency across context lengths
3. Extrapolation Test: Train at 8K context, evaluate perplexity and benchmark performance at 16K, 32K, and 64K tokens

## Open Questions the Paper Calls Out

**Open Question 1**: Can frequency-domain compression be improved by exploiting head-wise differences in power spectrum distributions? The paper shows different attention heads exhibit varying power spectrum distributions but applies uniform compression. Head-specific compression ratios remain unexplored.

**Open Question 2**: Can FreqKV be combined with position encoding methods like LongRoPE or PI for further gains? The paper compares against position methods but doesn't explore combinations. Synergistic effects with position interpolation remain untested.

**Open Question 3**: What architectural modifications could address performance gaps on dense symbolic/mathematical content? The paper acknowledges degradation on Proof-pile due to localized variations being harder to preserve under compression, but hybrid approaches remain unexplored.

**Open Question 4**: How well does FreqKV generalize to non-LLaMA architectures with different attention mechanisms? The paper claims architecture-agnostic but only validates on LLaMA models. Energy concentration patterns may differ in architectures with linear attention or MoE.

## Limitations

- Performance degrades on detail-heavy tasks like mathematical reasoning and code generation where local details matter more than global semantics
- Iterative compression introduces approximation error that accumulates over time, though long-term behavior beyond 256K tokens remains uncertain
- The assumption that low-frequency components contain sufficient information is empirically supported but not theoretically proven for all domains

## Confidence

**High Confidence**: Perplexity maintenance (~7.73 on PG-19), compression overhead (<0.3%), and training efficiency claims are well-supported by experiments and reproducible.

**Medium Confidence**: Benchmark performance claims are supported but primarily measured within original context windows, with more variability beyond training length.

**Low Confidence**: Universal applicability claim across all LLM tasks is not fully substantiated, with acknowledged limitations on symbolic/mathematical content.

## Next Checks

1. **Detail-Preserving Task Validation**: Test FreqKV on code generation (HumanEval, MBPP) and mathematical reasoning (GSM8K, MATH) datasets to quantify performance degradation when high-frequency information is discarded.

2. **Long-Term Error Accumulation Analysis**: Implement simulation tracking approximation error over thousands of compression iterations (e.g., 1M tokens) to measure how reconstruction error grows and whether it causes catastrophic performance collapse.

3. **Theoretical Frequency Analysis**: Conduct controlled experiment varying input token types and measuring energy distribution across frequency bands to test whether low-frequency concentration assumption holds universally or is task-dependent.