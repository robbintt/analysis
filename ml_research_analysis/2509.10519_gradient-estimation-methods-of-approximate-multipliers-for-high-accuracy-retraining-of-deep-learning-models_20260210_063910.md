---
ver: rpa2
title: Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining
  of Deep Learning Models
arxiv_id: '2509.10519'
source_url: https://arxiv.org/abs/2509.10519
tags:
- retraining
- accuracy
- appmults
- gradient
- appmult
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of gradient estimation in approximate
  multipliers (AppMults) used in deep learning accelerators, where inaccuracies in
  AppMults degrade model accuracy and necessitate retraining. The authors propose
  two methods to compute more precise gradients of AppMults compared to the conventional
  straight-through estimator (STE) approach, which approximates AppMult gradients
  using those of accurate multipliers.
---

# Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models

## Quick Facts
- **arXiv ID:** 2509.10519
- **Source URL:** https://arxiv.org/abs/2509.10519
- **Reference count:** 40
- **Primary result:** LUT-based gradient estimation improves retraining accuracy by up to 23.69% on ImageNet with Vision Transformers

## Executive Summary
This paper tackles the problem of retraining deep learning models that have been accelerated with approximate multipliers (AppMults), which introduce inaccuracies that degrade model performance. The authors propose two novel gradient estimation methods—LUT-2D and LUT-1D—that replace the standard straight-through estimator (STE) to compute more accurate gradients for AppMults during retraining. These methods use lookup tables to characterize fine-grained or compact gradients, enabling better recovery of accuracy after hardware approximation. Experiments show significant improvements across CIFAR-10 and ImageNet benchmarks, with LUT-2D achieving the highest accuracy and LUT-1D offering a good balance of accuracy and efficiency.

## Method Summary
The paper introduces two gradient estimation methods for approximate multipliers used in deep learning accelerators. The first, LUT-2D, uses 2D lookup tables to store smoothed output values and compute fine-grained gradients via central differences, achieving the highest retraining accuracy. The second, LUT-1D, is more efficient, using 1D lookup tables that store compact gradients representing average changing rates. Both methods apply smoothing with a half window size to reduce stair-like artifacts in multiplier outputs. During retraining, these LUTs replace STE in the backward pass, allowing more precise gradient computation for approximate GEMM operations. The methods are validated on CNNs and Vision Transformers using CIFAR-10/100 and ImageNet datasets.

## Key Results
- LUT-2D improves retraining accuracy by 3.83% on average over STE on CIFAR-10 with CNNs.
- LUT-1D achieves comparable accuracy (3.72% improvement) with shorter runtime per epoch.
- On ImageNet with Vision Transformers, LUT-1D improves retraining accuracy by 23.69% on average versus state-of-the-art retraining frameworks.
- LUT-2D incurs 1.9x runtime overhead per epoch; LUT-1D only 1.2x, making it more practical for large-scale models.

## Why This Works (Mechanism)
The paper's methods work by replacing the naive straight-through estimator (STE), which ignores AppMult inaccuracies in the backward pass, with gradient estimates derived from actual AppMult behavior. LUT-2D uses fine-grained, smoothed gradients to accurately reflect how small changes in inputs affect outputs, while LUT-1D provides a compact, efficient approximation. This enables the retraining process to account for and correct for the specific error patterns introduced by each approximate multiplier, leading to better recovery of model accuracy.

## Foundational Learning
- **Approximate Multipliers (AppMults):** Multipliers designed for hardware efficiency at the cost of accuracy; needed to reduce energy and latency in accelerators.
- **Straight-Through Estimator (STE):** A gradient approximation method that treats non-differentiable operations as identity during backprop; quick check: does the method use actual AppMult gradients instead?
- **Look-Up Tables (LUTs):** Precomputed mappings used to store AppMult outputs and gradients; needed for fast, accurate gradient estimation during retraining.
- **Half Window Size (HWS):** Parameter for smoothing AppMult output; quick check: HWS=32 for 8-bit, 16 for 7-bit as per paper.
- **Central Difference Method:** Numerical technique for estimating gradients; used in LUT-2D for interior points.
- **Average Changing Rate:** Compact gradient representation used in LUT-1D; trades off accuracy for efficiency.

## Architecture Onboarding
- **Component Map:** Forward LUT -> Approximate GEMM -> Backward LUT -> Gradient Accumulation
- **Critical Path:** During retraining, the forward pass uses the AppMult LUT to simulate quantization and multiplication, while the backward pass uses the gradient LUT to propagate accurate gradients.
- **Design Tradeoffs:** LUT-2D offers highest accuracy but 1.9x runtime overhead; LUT-1D sacrifices some accuracy for 1.2x overhead and scalability to large models.
- **Failure Signatures:** Gradient vanishing (if HWS too small), accuracy degradation (if LUT mismatch), or runtime blowup (if LUTs not optimized).
- **First Experiments:** 1) Generate and visualize Forward/Backward LUTs for a sample AppMult. 2) Verify gradient smoothness after HWS smoothing. 3) Compare retraining accuracy with STE baseline on CIFAR-10.

## Open Questions the Paper Calls Out
- How can the optimal Half Window Size (HWS) for the smoothing function be determined automatically rather than empirically?
- Can the high accuracy of the LUT-2D method be preserved while significantly reducing its runtime overhead for large-scale models?
- Is the proposed LUT-based gradient estimation approach effective for approximate multipliers with bit-widths greater than 8?

## Limitations
- CUDA kernel implementation for efficient LUT-based GEMM is not fully disclosed, requiring assumptions for reproduction.
- The paper does not provide a systematic method for selecting the Half Window Size (HWS) for new multipliers or bit-widths.
- LUT-2D's high computational cost makes it impractical for large Vision Transformers; only LUT-1D is evaluated on these models.

## Confidence
- **Methodology Clarity:** Medium-High
- **Reproducibility of LUT-2D accuracy:** High
- **Reproducibility of LUT-1D efficiency:** Medium
- **Scalability to >8-bit multipliers:** Low (not studied)
- **Practicality for large models:** Medium (depends on implementation)

## Next Checks
1. Implement and profile the CUDA kernel for LUT-based forward/backward passes; compare runtime against STE baseline.
2. Systematically vary the HWS parameter (e.g., 16, 32, 64) and measure impact on retraining accuracy and gradient smoothness.
3. Verify gradient consistency: ensure the backward LUT gradient values are mathematically consistent with the forward LUT's error profile for boundary cases.