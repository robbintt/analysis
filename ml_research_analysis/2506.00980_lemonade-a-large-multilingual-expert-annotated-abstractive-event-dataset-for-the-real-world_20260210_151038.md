---
ver: rpa2
title: 'LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset
  for the Real World'
arxiv_id: '2506.00980'
source_url: https://arxiv.org/abs/2506.00980
tags:
- event
- entity
- field
- type
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LEMONADE, a large multilingual event extraction
  dataset derived from ACLED, containing 39,786 expert-annotated events across 20
  languages and 171 countries. The authors address limitations in existing event extraction
  resources by introducing abstractive event extraction (AEE), which normalizes event
  arguments to categorical, numerical, or entity values rather than text spans.
---

# LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World

## Quick Facts
- arXiv ID: 2506.00980
- Source URL: https://arxiv.org/abs/2506.00980
- Reference count: 40
- Key outcome: 39,786 expert-annotated events across 20 languages and 171 countries in LEMONADE dataset

## Executive Summary
This paper introduces LEMONADE, a large multilingual event extraction dataset derived from ACLED, containing 39,786 expert-annotated events across 20 languages and 171 countries. The authors address limitations in existing event extraction resources by introducing abstractive event extraction (AEE), which normalizes event arguments to categorical, numerical, or entity values rather than text spans. They evaluate various LLMs and zero-shot systems on LEMONADE, introducing ZEST, a novel zero-shot entity linking system. Their best zero-shot system achieves 58.3% end-to-end F1, while ZEST reaches 45.7% AEL F1, significantly outperforming OneNet (23.7%). However, zero-shot results lag behind supervised models by 20.1% and 37.0% in end-to-end and AEL tasks respectively, highlighting the need for further research in this domain.

## Method Summary
The paper introduces LEMONADE, a dataset of 39,786 expert-annotated events in 20 languages, and defines the Abstractive Event Extraction (AEE) task with three subtasks: Event Detection (ED), Abstractive Event Argument Extraction (AEAE), and Abstractive Entity Linking (AEL). Supervised models fine-tune LLaMA-3.2 (1B/3B), LLaMA-3.1 8B, or Aya Expanse 8B to generate structured JSON output. The zero-shot ZEST system implements a three-stage pipeline: entity retrieval using mGTE embeddings, candidate filtering with GPT-4o, and entity assignment with GPT-4o. The dataset includes 10,707 entities with descriptions generated by GPT-4o.

## Key Results
- Best zero-shot system achieves 58.3% end-to-end F1 on LEMONADE
- ZEST achieves 45.7% AEL F1, outperforming OneNet (23.7%) by 22 percentage points
- Zero-shot systems lag supervised models by 20.1% (E2E) and 37.0% (AEL) F1
- Supervised models show significant performance gaps on unseen entities (12.6% vs 84.5% for seen entities)

## Why This Works (Mechanism)

### Mechanism 1
A multi-stage retrieval-filtering-assignment approach improves zero-shot abstractive entity linking over span-based pipelines. ZEST generates queries from documents to retrieve candidate entities, filters them by evidence, then assigns them to event roles. This avoids reliance on explicit span detection, which struggles with abstractive entities. The core assumption is that relevant entity descriptions can be approximated from document context alone. Evidence shows ZEST achieves 45.7% AEL F1 vs. OneNet's 23.7%. This could fail if entity descriptions are unavailable or documents lack sufficient context for query generation.

### Mechanism 2
Large language models with strong multilingual pre-training outperform specialized event extraction models in zero-shot settings. GPT-4o leverages broad pre-training to handle diverse languages and event types without task-specific fine-tuning, while models like GoLLIE rely on span-based training that doesn't transfer well to abstractive tasks. The core assumption is that multilingual pre-training coverage aligns with target language/entity distributions. Evidence shows GPT-4o achieves 79.6% ED F1 vs. GoLLIE's 23.6%. This could fail for very low-resource languages with minimal pre-training data.

### Mechanism 3
Supervised fine-tuning on domain-specific data substantially narrows performance gaps in both end-to-end and entity linking tasks. Fine-tuning adapts models to the specific event schema, argument types, and entity database of LEMONADE, capturing patterns that zero-shot methods miss. The core assumption is that training data distribution sufficiently covers test-time event types and entities. Evidence shows zero-shot systems lag supervised models by 20.1% (E2E) and 37.0% (AEL). This could fail if training data is sparse or biased toward certain languages/event types.

## Foundational Learning

### Concept: Abstractive vs. Extractive Event Extraction
Why needed here: LEMONADE introduces AEE, which normalizes arguments to categorical/numerical/entity values rather than text spans. This is fundamental to understanding the task formulation.
Quick check question: Can you explain why extracting "fatalities=0" is an abstractive argument compared to extracting the span "no one was killed"?

### Concept: Entity Linking with Long-Tail Entities
Why needed here: LEMONADE includes many domain-specific entities (e.g., regional militias) absent from Wikipedia. Traditional EL systems struggle with these.
Quick check question: How does ZEST's approach to entity linking differ from span-based methods like OneNet?

### Concept: Zero-Shot vs. Supervised Transfer in Multilingual Settings
Why needed here: The paper benchmarks both zero-shot and supervised approaches across 20 languages, revealing performance gaps and language-specific challenges.
Quick check question: Why does GPT-4o outperform supervised models on some low-resource languages (e.g., Burmese) in event detection?

## Architecture Onboarding

### Component map
Dataset (LEMONADE) -> Event Detection (ED) -> Abstractive Event Argument Extraction (AEAE) -> Abstractive Entity Linking (AEL) -> ZEST (Query generation -> Entity retrieval -> Entity filtering -> Entity assignment)

### Critical path
1. Construct entity database with descriptions (GPT-4o-generated)
2. Implement ZEST's retrieval stage using mGTE embeddings
3. Integrate LLM (GPT-4o) for filtering and assignment prompts
4. Chain ED (e.g., GPT-4o), AEAE (e.g., AC4S), and AEL (ZEST) for end-to-end evaluation

### Design tradeoffs
- Zero-shot vs. supervised: Zero-shot offers flexibility but lags in performance; supervised requires annotated data
- Span-based vs. abstractive: Span-based methods (GoLLIE, OneNet) fail on abstractive entities; ZEST avoids span detection
- Language coverage vs. performance: Low-resource languages show higher variance; multilingual models (Aya Expanse) may help

### Failure signatures
- AEL performance collapses for unseen entities (supervised models <13% F1 on unseen entities)
- GoLLIE fails on Burmese/Somali due to limited multilingual pre-training
- End-to-end performance sensitive to ED errors (incorrect event type cascades to all arguments)

### First 3 experiments
1. Replicate ZEST's zero-shot AEL results using GPT-4o and mGTE; measure F1 on test set
2. Ablate ZEST stages: skip filtering or assignment to quantify contribution
3. Compare zero-shot vs. supervised AEL on seen vs. unseen entities to diagnose generalization gaps

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance of abstractive entity linking (AEL) be significantly improved for entities not present in the training data? The authors explicitly state in the Discussion that future work should focus on "entity linking for unseen entities," noting that "all significantly struggle with new entities, with the best model achieving only 30.4%" (in the zero-shot "Unseen" category). This is unresolved because Table 4 shows a drastic performance drop for supervised models on "Unseen" entities compared to "Seen" ones (e.g., Aya Expanse drops from 84.5% to 12.6%), indicating current models rely heavily on memorization rather than generalization to the long-tail, specialized entities in LEMONADE. Evidence would be a system achieving an F1 score competitive with "Seen" entity performance (>75%) on the "Unseen" subset.

### Open Question 2
What techniques can effectively close the performance gap between zero-shot and supervised models in the Abstractive Event Extraction (AEE) task? The Discussion highlights "closing the performance gap between supervised and zero-shot models on all subtasks" as a primary focus for future research. This is unresolved because there is a significant deficit in zero-shot capabilities, with the best zero-shot system lagging behind supervised models by 20.1% in end-to-end F1 and 37.0% in AEL F1, making zero-shot models currently impractical for the high-stakes humanitarian applications targeted by this work. Evidence would be a novel zero-shot approach that reduces the end-to-end performance gap to under 5% compared to the supervised state-of-the-art on LEMONADE.

### Open Question 3
How can existing event coreference resolution methods be adapted to the abstractive event extraction (AEE) setting to support aggregate analysis? The Limitations section states that the paper "does not address event coreference resolution across multiple documents... We leave this promising direction for future research." This is unresolved because the current AEE framework and LEMONADE dataset are structured for single-document extraction, but the authors note that "aggregating multilingual sources for global event analysis" is a key challenge that requires resolving coreference across documents to avoid redundancy. Evidence would be an extension of the AEE framework that includes a cross-document coreference module, evaluated by its ability to cluster multiple source documents referring to the same ACLED event ID without conflicting abstractive annotations.

## Limitations
- Dataset covers primarily conflict-related events from ACLED, limiting applicability to other domains
- Zero-shot systems require significant LLM API usage (GPT-4o) that may not be feasible for all applications
- Supervised models show substantial performance gaps on low-resource languages and unseen entities

## Confidence

**High Confidence**: Dataset construction methodology, evaluation metrics, and supervised fine-tuning results. The controlled experiments and clear documentation support reliable reproduction.

**Medium Confidence**: ZEST's zero-shot entity linking performance. While results are promising, the dependence on LLM APIs and query generation quality introduces variability.

**Low Confidence**: Cross-lingual generalization claims. Performance disparities across languages are significant, and the reasons for these gaps are not fully explored.

## Next Checks
1. Ablate ZEST's filtering and assignment stages to quantify their individual contributions to the 45.7% AEL F1 score
2. Measure ZEST's performance with smaller context windows or fewer candidate entities to assess practical deployment costs
3. Evaluate both supervised and zero-shot models on a non-conflict dataset (e.g., ACE or MAVEN) to assess the true versatility of LEMONADE-trained systems