---
ver: rpa2
title: Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric
  Knowledge of LLMs for Conflict Forecasting
arxiv_id: '2505.09852'
source_url: https://arxiv.org/abs/2505.09852
tags:
- conflict
- llms
- macro
- forecasting
- parametric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models can forecast
  violent conflict using only their pretrained knowledge (parametric) or when augmented
  with external context via retrieval-augmented generation (RAG). Experiments compare
  GPT-4 and LLaMA-2 across conflict-prone regions from 2020-2024, evaluating categorical
  trend prediction (escalate, de-escalate, stable, peace) and fatality estimates.
---

# Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting

## Quick Facts
- arXiv ID: 2505.09852
- Source URL: https://arxiv.org/abs/2505.09852
- Reference count: 10
- LLMs (GPT-4, LLaMA-2) can forecast conflict trends and fatalities, with RAG augmentation improving accuracy, especially for classification.

## Executive Summary
This study evaluates whether large language models can forecast violent conflict using only their pretrained knowledge (parametric) or when augmented with external context via retrieval-augmented generation (RAG). Experiments compare GPT-4 and LLaMA-2 across conflict-prone regions from 2020-2024, evaluating categorical trend prediction (escalate, de-escalate, stable, peace) and fatality estimates. Results show GPT-4 outperforms LLaMA-2 in all settings, with RAG augmentation improving accuracy for both models, especially in classification tasks. The findings demonstrate that LLMs can capture conflict patterns when provided with structured external data, with open-source models showing promise for resource-constrained applications.

## Method Summary
The study uses GDELT news metadata and ACLED conflict event data to forecast conflict trends and fatalities across Ethiopia, Sudan, Somalia, Israel, and Iran (2020-2024). Two experimental settings are evaluated: parametric (zero-shot, internal knowledge only) and non-parametric (RAG with retrieved context). RAG retrieves 3-month news summaries, fatality counts, and tone scores via FAISS, summarized by GPT-3.5, and prompts GPT-4 or LLaMA-2-13B-chat. Forecasts are evaluated using classification metrics (accuracy, F1) and regression (MAE), with three evaluation types: categorical labels, binned regression, and labels derived from predicted fatalities.

## Key Results
- GPT-4 achieved macro F1 scores up to 0.67 for categorical prediction and MAE around 93-177 for fatality estimates.
- RAG augmentation improved GPT-4's performance across most tasks and countries, with notable gains in class-from-fatalities (e.g., Israel: F1-macro 0.16 → 0.33).
- LLaMA-2 showed minimal or inconsistent gains from RAG, with some cases (e.g., Ethiopia) showing performance degradation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG augmentation improves conflict forecasting by providing temporal context absent from static pretrained weights.
- Mechanism: External context (news summaries, fatality counts, tone scores) grounds model predictions in recent events, compensating for the knowledge cutoff inherent to parametric knowledge. The model synthesizes retrieved evidence with learned priors to generate more accurate trend labels and fatality estimates.
- Core assumption: The model can effectively integrate retrieved context without succumbing to knowledge conflict or hallucination.
- Evidence anchors:
  - [abstract]: "Incorporating external information could enhance model performance by providing up-to-date context otherwise missing from pretrained weights."
  - [section 4 Results]: "Adding retrieved context via RAG improved GPT-4's performance across most tasks and countries. Notable gains were observed in class-from-fatalities (e.g., F1-macro in Israel: 0.16 → 0.33)."
  - [corpus]: "Conflict-Aware Soft Prompting for RAG" addresses failures when retrieved context contradicts parametric knowledge—relevant but not directly tested in this paper.

### Mechanism 2
- Claim: Larger, more capable models (GPT-4) leverage retrieved context more effectively than smaller open-source models (LLaMA-2).
- Mechanism: Model capacity influences the ability to parse structured inputs (fatalities, tone scores) and unstructured summaries, then reconcile them with internal priors. GPT-4's stronger instruction-following enables consistent gains; LLaMA-2 shows inconsistent or negative effects.
- Core assumption: Performance gains from RAG depend on sufficient model reasoning capacity, not just retrieval quality.
- Evidence anchors:
  - [section 5 Discussion]: "GPT-4 appears to leverage [context] more reliably, suggesting better instruction-following, contextual grounding, and inference capabilities. LLaMA, while modestly improved with RAG, still struggled with precision and recall."
  - [section 4 Results]: "LLaMA, however, showed minimal or inconsistent gains from RAG. In some cases (e.g., Ethiopia), performance worsened or outputs failed entirely."
  - [corpus]: Weak direct evidence; neighboring papers focus on knowledge conflict resolution rather than capacity-mediated integration.

### Mechanism 3
- Claim: Regional and contextual variability determines RAG effectiveness—context quality, not just quantity, drives gains.
- Mechanism: GDELT article quality, event density, and reporting consistency vary by region. High-quality, temporally relevant context improves classification and regression; sparse or inconsistent context yields marginal or negative returns.
- Core assumption: Retrieved summaries accurately reflect ground-truth conflict dynamics.
- Evidence anchors:
  - [section 5 Discussion]: "Somalia and Israel were more volatile, possibly due to less consistent article quality, varying event dynamics, or weaker patterns in historical data."
  - [section 6 Limitations]: "RAG summaries can introduce ambiguous or noisy context."
  - [corpus]: No direct corpus evidence on region-specific retrieval quality.

## Foundational Learning

- **Concept: Parametric vs. Non-Parametric Knowledge in LLMs**
  - Why needed here: Understanding the distinction is essential to interpret why zero-shot predictions fail on recent events and why RAG helps.
  - Quick check question: Can a model pretrained only on data up to 2023 accurately forecast conflict in April 2024 without external input?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper's Experiment 2 relies on RAG to inject structured indicators and news summaries; understanding retrieval, embedding, and context windowing is prerequisite.
  - Quick check question: What happens to forecast accuracy if the retrieval corpus has no articles from the past 3 months?

- **Concept: Knowledge Conflict in LLMs**
  - Why needed here: The paper references parametric vs. retrieved knowledge tension; corpus neighbors (e.g., "Conflict-Aware Soft Prompting") show this can degrade performance.
  - Quick check question: If retrieved context says "de-escalation" but the model's parametric prior strongly expects "escalation," which will dominate output?

## Architecture Onboarding

- **Component map:**
  - GDELT (article metadata, tone, Goldstein scores) + ACLED (fatality counts, event records) -> FAISS-based semantic search over 3-month article window -> GPT-3.5 summarization of retrieved articles -> Structured prompt (summary, average tone, Goldstein score, 12-week fatality series) -> GPT-4 or LLaMA-2-13B-chat generates trend label + fatality estimate -> Classification metrics (accuracy, F1), regression (MAE), binned regression accuracy

- **Critical path:**
  1. Query GDELT for region + date range -> extract URLs + metadata
  2. Scrape articles -> embed and index in FAISS
  3. For each forecast month: retrieve relevant articles -> summarize -> combine with ACLED fatalities
  4. Prompt LLM with structured context -> parse output (label + fatality estimate)
  5. Compare against ACLED ground truth -> compute metrics

- **Design tradeoffs:**
  - GPT-4 vs. LLaMA-2: Performance vs. cost/access; LLaMA may be viable in resource-constrained settings but requires validation
  - 3-month context window: Captures recent trends but may miss longer-term patterns
  - Temperature 0.2: Reduces stochasticity but may limit exploration of plausible scenarios
  - Threshold-based labeling: Simplifies evaluation but may misclassify nuanced dynamics

- **Failure signatures:**
  - Near-zero accuracy in categorical prediction (e.g., LLaMA-2 Ethiopia Exp 2: 0.0000) -> model cannot parse task or context
  - MAE does not improve with RAG -> context not translating to numeric precision
  - Macro F1 much lower than accuracy -> model bias toward majority class (Stable/Peace)

- **First 3 experiments:**
  1. Replicate Experiment 1 (parametric) for a single country; verify baseline metrics match reported values (e.g., GPT-4 Somalia class-from-fatalities accuracy ~0.83).
  2. Add RAG with 3-month context for same country; compare F1-macro and MAE deltas.
  3. Swap LLaMA-2 for GPT-4 on identical prompts; quantify performance gap and identify failure modes (e.g., output parsing errors, context ignored).

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Does domain-specific fine-tuning improve LLM conflict forecasting reliability compared to zero-shot or RAG-only approaches?
  - Basis in paper: [explicit] The authors state they evaluated only "general-purpose LLMs in zero-shot settings" and suggest "fine-tuning with domain-specific signals" for future work.
  - Why unresolved: Current experiments did not involve updating model weights, leaving the potential benefits of domain adaptation untested.
  - What evidence would resolve it: Benchmarks comparing the authors' zero-shot baselines against models fine-tuned on historical conflict data.

- **Open Question 2**
  - Question: Can multilingual retrieval pipelines reduce forecasting errors in non-English speaking conflict zones?
  - Basis in paper: [explicit] The conclusion lists "multilingual retrieval" as a necessary area for future research to increase applicability.
  - Why unresolved: The study was limited to "English-only evaluation," potentially missing local context available only in regional languages.
  - What evidence would resolve it: Performance comparisons between English-only RAG and systems incorporating local language news sources (e.g., Arabic for the Middle East).

- **Open Question 3**
  - Question: To what extent does incorporating human-in-the-loop feedback improve the reliability of LLM-based early warning systems?
  - Basis in paper: [explicit] The conclusion highlights the need to explore "human-in-the-loop pipelines to increase reliability."
  - Why unresolved: The current system is fully automated, and while RAG helps, models still struggle with fine-grained precision.
  - What evidence would resolve it: User studies measuring error correction rates and forecasting accuracy when human analysts filter or verify model inputs and outputs.

## Limitations
- Prompt Template Dependency: Performance heavily depends on prompt phrasing and structure, yet only a placeholder GitHub link is provided for the exact templates.
- Context Quality Variance: The effectiveness of RAG is tightly coupled to GDELT article quality, which may vary by region.
- Knowledge Conflict Handling: The paper does not directly test how the models resolve contradictions between parametric and retrieved knowledge.

## Confidence
- **High Confidence**: GPT-4 outperforms LLaMA-2 in all experimental settings; RAG improves GPT-4 performance when context is high quality; LLaMA-2 is more sensitive to context quality and prompt structure.
- **Medium Confidence**: Larger models (GPT-4) can integrate retrieved context more effectively due to better instruction-following and contextual grounding; RAG is most beneficial for classification over regression tasks.
- **Low Confidence**: Open-source models are broadly viable for conflict forecasting with appropriate prompting and context—current results suggest strong performance gaps persist, especially for rare event prediction.

## Next Checks
1. **Replicate with Exact Prompts**: Obtain and run the experiment using the actual prompt templates from the authors to isolate the effect of phrasing on RAG performance, especially for LLaMA-2.
2. **Regional Context Quality Audit**: Manually inspect retrieved GDELT articles for top- and bottom-performing regions to confirm whether context quality (density, tone consistency, recency) correlates with forecast accuracy.
3. **Cross-Model Ablation on Context**: Remove RAG context for a subset of high-performing forecasts and measure degradation; repeat with LLaMA-2 to quantify context dependency and identify failure thresholds.