---
ver: rpa2
title: Can Hallucination Correction Improve Video-Language Alignment?
arxiv_id: '2502.15079'
source_url: https://arxiv.org/abs/2502.15079
tags:
- video
- haca
- caption
- language
- correction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HACA, a self-training framework that improves
  video-language alignment by treating hallucination correction as a training objective.
  Unlike prior work that focuses on hallucination mitigation, HACA fine-tunes Video-LLMs
  to detect and correct misaligned video descriptions, enhancing their ability to
  reason about spatio-temporal relationships.
---

# Can Hallucination Correction Improve Video-Language Alignment?

## Quick Facts
- **arXiv ID:** 2502.15079
- **Source URL:** https://arxiv.org/abs/2502.15079
- **Reference count:** 20
- **One-line primary result:** HACA fine-tuning improves video-language alignment accuracy by up to 17.9% and mAP by 5.7 points

## Executive Summary
This paper introduces HACA, a self-training framework that improves video-language alignment by treating hallucination correction as a training objective. Unlike prior work that focuses on hallucination mitigation, HACA fine-tunes Video-LLMs to detect and correct misaligned video descriptions, enhancing their ability to reason about spatio-temporal relationships. By generating corrected captions for inaccurate descriptions, the model learns finer-grained alignment signals beyond binary entailment labels. Experiments on VELOCITI and SSv2 datasets show HACA outperforms baseline models by up to 17.9% accuracy and 5.7 mAP points, demonstrating its effectiveness in improving video-text alignment. Additionally, HACA maintains strong performance on question-answering tasks, indicating it preserves multi-task capabilities while enhancing alignment.

## Method Summary
HACA is a self-training framework that fine-tunes Video-LLMs (Video-LLaVA, VideoChat2) to improve video-language alignment through hallucination correction. The method creates a dataset of (video, description, correction) triplets from VideoCon, where misaligned captions are paired with their ground-truth corrections. During fine-tuning, the model is trained to output "Yes" for aligned captions or generate corrected captions for misaligned ones using MLE loss. A masking correction task augments the data by randomly masking 45% of content words in captions, forcing the model to reconstruct accurate descriptions. The framework maintains performance on question-answering tasks while significantly improving alignment on entailment and retrieval benchmarks.

## Key Results
- HACA achieves 17.9% higher accuracy on VELOCITI binding task compared to baseline entailment fine-tuning
- On SSv2 retrieval, HACA improves mAP by 5.7 points over standard fine-tuning approaches
- HACA maintains strong performance on MSRVTT-QA (GPT-Score) while improving alignment, unlike pure entailment fine-tuning which degrades QA performance

## Why This Works (Mechanism)
HACA works by providing richer supervision signals during fine-tuning. Instead of only learning binary entailment labels (Yes/No), the model learns to generate full corrected captions for misaligned descriptions. This forces the Video-LLM to understand fine-grained differences between correct and incorrect descriptions, improving its ability to reason about spatio-temporal relationships and factual consistency. The masking correction task further enhances this by requiring the model to reconstruct accurate descriptions from partially obscured input, strengthening its alignment capabilities beyond simple classification.

## Foundational Learning

- **Concept: Video-Language Alignment**
  - **Why needed here:** This is the core problem HACA is designed to solve. It's the process of creating a shared representational space where video features and text features can be compared for similarity, enabling tasks like retrieval and entailment.
  - **Quick check question:** Explain why a Video-LLM might struggle to distinguish between two video captions that are semantically similar but differ in a key temporal detail.

- **Concept: Large Vision-Language Models (Video-LLMs)**
  - **Why needed here:** HACA is a fine-tuning framework specifically for Video-LLMs. Understanding their components (visual encoder, LLM, adapter) is crucial for applying the method.
  - **Quick check question:** Identify which components of a Video-LLM are frozen and which are fine-tuned in the HACA framework.

- **Concept: Hallucination in Vision-Language Models**
  - **Why needed here:** The paper frames hallucination correction as a training objective. One must understand that hallucination is the generation of text that is not grounded in the visual input.
  - **Quick check question:** Define "hallucination" in the context of a Video-LLM. How does HACA differ from prior work that focuses on *mitigation*?

## Architecture Onboarding

- **Component map:** Video-LLM (Visual Encoder, Visual-Text Adapter, LLM) -> HACA Fine-tuning Pipeline -> (video, description, correction) triplets -> MLE Loss + Masking Augmentation -> Improved Alignment

- **Critical path:**
  1. **Data Preparation:** Create a dataset of (video, description, correction) triplets. This involves taking ground-truth captions and generating synthetic "contrastive" or "hallucinated" descriptions using an external model or heuristics.
  2. **Fine-tuning:** The Video-LLM is fine-tuned on this dataset. For aligned pairs, the target output is a confirmation. For misaligned pairs, the target output is the corrected caption. This is done using a Maximum Likelihood Estimation (MLE) loss. A masking correction task can be added as data augmentation.
  3. **Inference/Evaluation:** The fine-tuned model is used in a zero-shot manner on downstream tasks. For entailment, its probability of generating "Yes" vs "No" is used. For retrieval, this probability is used to rank videos against a text query.

- **Design tradeoffs:**
  - **Signal Fidelity vs. Cost:** The HACA objective provides a richer learning signal (full correction) than a binary entailment label, but it requires the generation of a synthetic correction dataset and more complex supervision.
  - **Data Augmentation:** Adding the masking correction task improves performance but adds complexity to the training data generation pipeline.
  - **Training Scope:** The model is fine-tuned on a specific dataset (VideoCon). While results show generalization, performance on out-of-domain video types or tasks not covered by the fine-tuning data (e.g., long videos) may not improve or could degrade.

- **Failure signatures:**
  - The model may fail to correct a hallucinated caption and instead incorrectly confirm it (Figure 5a), showing it hasn't learned the fine-grained distinction.
  - Performance on general question-answering tasks could drop if the fine-tuning is too narrow, although HACA is designed to mitigate this (Table 2).
  - The correction mechanism is dependent on the quality and types of hallucinations present in the training data.

- **First 3 experiments:**
  1. **Baseline Entailment Fine-tuning:** Fine-tune a pre-trained Video-LLM (e.g., Video-LLaVA) using only binary entailment labels (Yes/No) on the VideoCon dataset to establish a performance baseline.
  2. **HACA Objective Fine-tuning:** Fine-tune the same Video-LLM using the HACA objective, where the model must output a corrected caption for misaligned pairs. Compare performance on VELOCITI and SSv2 retrieval against the baseline.
  3. **Ablation on Masking Augmentation:** Fine-tune a model using the HACA objective combined with the masking correction task. Compare performance to HACA alone to quantify the contribution of the augmentation. Vary the masking ratio (e.g., 15%, 30%, 45%, 60%) to find an optimal setting.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the HACA framework effectively scale to long-form video understanding without excessive computational overhead?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that the approach "has not been evaluated on long videos, due to the limitation of computational resources," and identify this as a direction for future work.
- **Why unresolved:** The current experiments are restricted to shorter video clips (e.g., 8 frames), leaving the efficacy of hallucination correction on longer temporal contexts untested.
- **What evidence would resolve it:** Evaluation results on long-video benchmarks (e.g., Video-MME or EgoSchema) demonstrating that HACA maintains high alignment accuracy and correction capabilities without prohibitive memory costs.

### Open Question 2
- **Question:** Does fine-tuning on hallucination correction cause the model to lose the ability to detect error types absent from the training data?
- **Basis in paper:** [inferred] Table 1 shows that on the "Agent Coref" test (a misalignment type absent in the VideoCon training data), the pretrained Video-LLaVA outperforms the HACA-fine-tuned version.
- **Why unresolved:** The paper suggests that the specific composition of the fine-tuning data (VideoCon) may lead to a regression in capabilities not explicitly targeted by the correction objective, such as coreference resolution.
- **What evidence would resolve it:** An ablation study where HACA is trained on a dataset containing agent coreference errors, or an analysis of catastrophic forgetting on specific linguistic phenomena not present in the correction training set.

### Open Question 3
- **Question:** Can the reliance on ground-truth video descriptions be removed while maintaining the alignment improvements provided by HACA?
- **Basis in paper:** [explicit] The authors list the assumption of "availability of ground-truth video caption annotations for fine-tuning" as a primary limitation of the method.
- **Why unresolved:** The current self-training mechanism depends on ground truth to formulate the "corrected" response targets, creating a dependency on labeled data.
- **What evidence would resolve it:** A demonstration of a fully self-supervised variant that uses model-generated high-confidence captions as pseudo-ground-truth, achieving comparable results to the current supervised implementation.

## Limitations

- The method has not been evaluated on long-form videos due to computational constraints, limiting its applicability to extended video content.
- Performance on error types absent from the training data (e.g., agent coreference) may degrade, suggesting potential catastrophic forgetting of specific linguistic capabilities.
- The approach relies on the availability of ground-truth video caption annotations for fine-tuning, creating a dependency on labeled data that limits its self-supervised potential.

## Confidence

- **High Confidence:** HACA improves video-language alignment metrics (accuracy on VELOCITI, mAP on SSv2) compared to baseline entailment fine-tuning.
- **Medium Confidence:** HACA preserves strong performance on question-answering tasks while improving alignment, though this is relative to a baseline.
- **Medium Confidence:** Qualitative analysis showing HACA's ability to correct hallucinated captions is persuasive but based on selective examples.

## Next Checks

1. **Data Generation Validation:** Re-implement the data generation pipeline to create the (video, description, correction) triplets from VideoCon. Ensure that the process for creating contrastive captions and their corrections is faithful to the paper's description. Compare the distribution and types of hallucinations in the generated training data to those reported in the paper.

2. **Inference Prompt Fidelity:** Implement the exact prompt formats used for evaluation on VELOCITI and SSv2 to extract `P_yes` scores. Run inference on a small subset of the test sets to verify that the model outputs probability distributions for "Yes/No" responses as expected, and that these align with the scoring mechanism described in the paper.

3. **Temporal Ordering Ablation:** Conduct an ablation study on the masking correction task's impact on the temporal ordering (Chrono) subtask of VELOCITI. Vary the masking ratio (e.g., 15%, 30%, 45%, 60%) and report performance on the Chrono task to identify the optimal setting and understand the sensitivity of this specific reasoning capability to the augmentation.