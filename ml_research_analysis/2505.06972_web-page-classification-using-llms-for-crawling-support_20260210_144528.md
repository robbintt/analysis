---
ver: rpa2
title: Web Page Classification using LLMs for Crawling Support
arxiv_id: '2505.06972'
source_url: https://arxiv.org/abs/2505.06972
tags:
- pages
- page
- index
- content
- title
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of efficient web crawling for
  collecting new pages by leveraging large language models (LLMs) to classify web
  pages into "Index Pages" and "Content Pages." The proposed method uses GPT-4o-mini
  and GPT-4o to classify pages based on their title and body text, with the goal of
  identifying index pages as starting points for accessing new content. A dataset
  was constructed using an automated annotation method that identifies "Content Listing
  Pages" to label page types, with manual review ensuring quality.
---

# Web Page Classification using LLMs for Crawling Support

## Quick Facts
- arXiv ID: 2505.06972
- Source URL: https://arxiv.org/abs/2505.06972
- Reference count: 15
- Primary result: LLM-based classification of web pages into index and content types significantly improves crawling efficiency for discovering new pages, with GPT-4o-mini + All Pages hybrid achieving highest coverage on challenging sites.

## Executive Summary
This study introduces a method to improve web crawling efficiency by using large language models (LLMs) to classify web pages as either "Index Pages" (containing hyperlinks to other pages) or "Content Pages" (news articles, columns). The approach leverages GPT-4o-mini and GPT-4o to classify pages based on their title and body text, with the goal of identifying index pages as starting points for accessing new content. A dataset was constructed using an automated annotation method that identifies "Content Listing Pages" to label page types, with manual review ensuring quality. The method was evaluated on its page classification performance and coverage of new pages. Results showed that LLM-based methods significantly outperformed baseline approaches, achieving high precision and F1 scores in classification, and demonstrating superior coverage of new pages, particularly when using 100 index pages as starting points. Hybrid methods combining LLM predictions with shallow hierarchy pages achieved the best coverage in challenging datasets.

## Method Summary
The method uses LLMs (GPT-4o-mini and GPT-4o) to classify web pages into "Index Pages" and "Content Pages" based on their title and optionally body text. The classification is performed via OpenAI API with structured prompts. A dataset was automatically annotated using "Content Listing Pages" - comprehensive listing pages that link to all content pages on a site. Pages listed on these are annotated as content pages; unlisted pages are treated as index pages. Manual review filters low-quality sites. Body text is extracted using a modified version of ExtractContent3. The method is evaluated for both classification accuracy (precision, recall, F1) and crawling coverage (proportion of new pages reachable from predicted index pages).

## Key Results
- GPT-4o with Title + Body achieves highest F1 score of 0.894, outperforming baseline rule-based methods.
- LLM-based methods significantly outperform baseline approaches in coverage of new pages, especially when using 100 index pages as starting points.
- Hybrid methods (LLM predictions + shallow hierarchy pages) achieve best coverage on challenging "Noisy-Test" datasets.
- GPT-4o-mini shows performance degradation when body text is included due to noise sensitivity.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can classify web pages into "Index Pages" and "Content Pages" with higher precision than rule-based methods by leveraging semantic understanding of page titles and body text.
- Mechanism: The LLM receives a prompt containing the web page's title and optionally body text, then performs binary classification. GPT-4o with Title + Body achieved the highest F1 score (0.894) because it can interpret both succinct title signals and richer body content to distinguish pages designed for navigation from content-delivery pages.
- Core assumption: The linguistic patterns distinguishing index pages (e.g., "Latest News," category labels) from content pages (e.g., full articles) are learnable from title/body text alone, without HTML structure analysis.
- Evidence anchors:
  - [abstract] "The proposed method uses GPT-4o-mini and GPT-4o to classify pages based on their title and body text"
  - [section 3.3] "GPT-4o with Title + Body achieves the highest F1 score, outperforming the baseline methods"
  - [corpus] Related work "Neural Prioritisation for Web Crawling" confirms neural methods are applied to crawling prioritization, though not specifically for page type classification.
- Break condition: LLMs will fail when body text extraction is noisy (e.g., includes unrelated link previews), particularly for smaller models like GPT-4o-mini which showed performance degradation with Title + Body input.

### Mechanism 2
- Claim: Automatically annotated datasets using "Content Listing Pages" provide a scalable proxy for human labels, enabling evaluation without costly manual annotation.
- Mechanism: Some websites maintain comprehensive listing pages (e.g., "All Content Archive"). Pages linked from these are annotated as content pages; unlisted pages are treated as index pages. Manual review filters low-quality sites.
- Core assumption: Content listing pages exist and accurately represent the site's page taxonomy; pages not listed are likely navigational/index pages.
- Evidence anchors:
  - [section 2.1] "By classifying the web pages listed on content listing pages as content pages and the pages not listed on content listing pages as index pages, annotation can be performed at a lower cost"
  - [section 2.1] "we perform a simple manual review of the annotation results and retain only those websites deemed to be of sufficient quality"
  - [corpus] No direct corpus evidence for this specific annotation method; this appears novel to this paper.
- Break condition: The annotation method fails for websites without comprehensive listing pages (hence the "Noisy-Test" set cannot be annotated and serves only for coverage evaluation).

### Mechanism 3
- Claim: Hybrid methods combining LLM-predicted index pages with shallow-hierarchy pages achieve more robust new-page coverage than either approach alone, especially on challenging sites.
- Mechanism: Pure LLM selection may miss new pages on sites where index pages are insufficient. The hybrid approach allocates half of starting points to LLM-identified index pages and half to shallow-hierarchy pages regardless of type, hedging against incomplete index page detection.
- Core assumption: Shallow-hierarchy pages (closer to homepage) are more likely to link to new content regardless of their classification.
- Evidence anchors:
  - [section 3.3] "hybrid methods consistently achieve high coverage under the evaluated conditions. Notably, GPT-4o-mini + All Pages with Title achieves the best results in terms of the average coverage on the noisy-test set"
  - [section 3.3] "when using 100 index pages as starting points... LLMs achieve higher coverage than baseline methods, particularly when using 100 index pages"
  - [corpus] "Document Quality Scoring for Web Crawling" addresses quality filtering but not the hybrid index/shallow strategy described here.
- Break condition: When allowed to use 300+ starting pages, the simple "All Pages" baseline outperforms on noisy-test sets because comprehensive coverage from shallow pages becomes more effective than selective index-page crawling.

## Foundational Learning

- Concept: **Web crawling fundamentals (freshness vs. coverage tradeoff, cold-start problem)**
  - Why needed here: The paper's entire motivation rests on the tension between comprehensive crawling (resource-intensive) and selective crawling (misses new pages). Understanding sitemaps, RSS, and update-frequency heuristics clarifies why LLM-based page typing offers an alternative path.
  - Quick check question: Can you explain why relying solely on past update frequency fails for newly created pages?

- Concept: **Binary classification metrics (precision, recall, F1)**
  - Why needed here: The evaluation reports these metrics with index pages as the positive class. High precision with lower recall indicates LLMs are conservative—rarely misclassifying content pages as index pages but missing some true index pages.
  - Quick check question: If a method has precision 0.99 and recall 0.57, what does this tell you about its error profile?

- Concept: **LLM input design (title vs. body text tradeoffs)**
  - Why needed here: The experiments show GPT-4o improves with body text but GPT-4o-mini degrades. This reflects noise sensitivity in smaller models when body extraction includes irrelevant elements (related article summaries, navigation text).
  - Quick check question: Why might adding more information (body text) hurt performance for some models?

## Architecture Onboarding

- Component map:
  Data Collection Layer -> Annotation Layer -> Classification Layer -> Crawling Strategy Layer

- Critical path:
  1. Identify target websites and verify presence of content listing pages for annotated evaluation.
  2. Implement body text extraction (the paper modifies ExtractContent3 library).
  3. Construct classification prompt and test title-only vs. title+body inputs.
  4. Evaluate classification F1, then run coverage experiments with 10/30/100/300/1000 starting pages.

- Design tradeoffs:
  - **GPT-4o-mini vs. GPT-4o**: Mini is cheaper but struggles with noisy body text; GPT-4o handles body text robustly but at higher cost.
  - **Title-only vs. Title+Body**: Title-only is simpler and works well across both models; Title+Body helps GPT-4o but hurts GPT-4o-mini.
  - **Pure LLM vs. Hybrid**: Pure LLM is sufficient for sites with clear index pages; Hybrid is safer for diverse/noisy sites but dilutes selectivity.
  - Assumption: The paper does not quantify API cost per classification—this is listed as future work.

- Failure signatures:
  - **Low recall with high precision**: LLM is over-conservative, missing index pages—likely due to prompt phrasing or training bias toward content-like text.
  - **Coverage drops at 300+ starting pages on noisy sites**: Indicates LLM-only selection becomes counterproductive; hybrid or shallow-only is better.
  - **Body text degradation in GPT-4o-mini**: Check extraction pipeline for noise (related links, ads) that smaller models cannot filter.

- First 3 experiments:
  1. **Baseline reproduction**: Implement the "Rule-Based" method (pages with ≤9 words in title = index) and compare F1 against GPT-4o-mini title-only on a small labeled subset.
  2. **Input ablation**: Test GPT-4o with title-only, body-only, and title+body to isolate which signal drives improvement.
  3. **Coverage scaling test**: Starting from 10 to 1000 pages, plot coverage curves for LLM vs. All Pages vs. Hybrid on a held-out site to identify the crossover point where hybrid becomes unnecessary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can crawling efficiency be further improved by subdividing "Index Pages" into sub-categories, such as distinguishing between pages that link to new content versus those that link to archived content?
- Basis in paper: [explicit] The conclusion suggests that "Index pages could be divided into pages that contain links to new pages and pages that contain links to old pages," allowing crawlers to prioritize the former.
- Why unresolved: The current study treats all index pages as a single class, meaning crawlers may waste resources visiting index pages that only point to stale content.
- What evidence would resolve it: Experimental results comparing the "new page" discovery rate of a multi-class classifier against the current binary approach.

### Open Question 2
- Question: Does the improvement in crawling efficiency and coverage justify the computational cost and latency of using large language models compared to simpler heuristics?
- Basis in paper: [explicit] The authors list "Validation of Computational Cost" as a future challenge, noting that "it is necessary to verify whether the improvement in collection efficiency justifies the increase in computational costs."
- Why unresolved: While the paper demonstrates superior coverage for LLMs, it does not quantify the trade-off against the financial/operational expense of API calls or inference time required for classification.
- What evidence would resolve it: A cost-benefit analysis measuring the "coverage gain per dollar" or "coverage gain per second" for LLM-based methods versus the rule-based baseline.

### Open Question 3
- Question: Can lightweight or smaller language models achieve the necessary classification performance to maintain high new page coverage, or is the reasoning capability of larger models strictly required?
- Basis in paper: [explicit] The paper identifies "Lightweight LLMs" as a key challenge, hypothesizing that "achieving sufficient classification performance with lightweight LLMs will be a key challenge" since higher accuracy does not always correlate with better coverage.
- Why unresolved: The study primarily relied on GPT-4o and GPT-4o-mini; it is unknown if smaller, faster models would fail to generalize or if they hit a "good enough" threshold for crawling purposes.
- What evidence would resolve it: Benchmarks of parameter-efficient models (e.g., 7B or smaller) on the same task, specifically correlating their F1 scores with the resulting coverage metrics.

### Open Question 4
- Question: How robust is the LLM-based classification method when applied to a diverse range of non-news websites and in live environments where website structures change dynamically?
- Basis in paper: [explicit] The authors state that "Practical evaluation" is a future challenge because the current "evaluation in this study is limited to a small number of news websites" and static snapshots.
- Why unresolved: The current dataset relies on static snapshots of mostly news sites; the method may perform differently on e-commerce sites, forums, or single-page applications (SPAs) where "body text" extraction is difficult or structure is volatile.
- What evidence would resolve it: Evaluation results from a live crawling deployment across a heterogeneous set of web domains (e.g., Common Crawl data) rather than curated news datasets.

## Limitations
- Annotation method scalability is limited to sites with comprehensive listing pages, creating sampling bias in evaluation datasets.
- Computational cost per classification is not quantified, making real-world feasibility assessment difficult.
- Body text extraction quality issues can degrade GPT-4o-mini performance, but specific modifications and their impact are not detailed.

## Confidence
- **High Confidence**: LLMs can distinguish index pages from content pages using title and body text, improving coverage over rule-based baselines when using 100 index pages as starting points.
- **Medium Confidence**: Hybrid methods are superior for noisy datasets, though the specific conditions requiring hybrid selection need further validation.
- **Low Confidence**: Automatic annotation via content listing pages provides a scalable alternative to manual labeling, demonstrated only on sites with existing listing pages.

## Next Checks
1. **Cost-benefit analysis**: Measure per-page API cost for GPT-4o-mini and GPT-4o classifications and compare to coverage gains against the baseline "All Pages" method across different starting-page counts.
2. **Robustness test**: Evaluate LLM classification and coverage on a dataset of sites without content listing pages (using manual annotations) to validate generalization beyond the annotated evaluation set.
3. **Body extraction stress test**: Systematically inject synthetic noise (e.g., related-article summaries, ads) into body text and measure degradation in GPT-4o-mini vs. GPT-4o performance to quantify noise sensitivity.