---
ver: rpa2
title: 'When Benchmarks Leak: Inference-Time Decontamination for LLMs'
arxiv_id: '2601.19334'
source_url: https://arxiv.org/abs/2601.19334
tags:
- contamination
- clean
- benchmark
- decontamination
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeconIEP addresses test-set contamination in large language model
  evaluation, where leaked benchmark items inflate reported performance. The method
  applies small, bounded perturbations to input embeddings during inference, guided
  by a less-contaminated reference model, to suppress memorization-driven shortcuts
  while preserving reasoning behavior.
---

# When Benchmarks Leak: Inference-Time Decontamination for LLMs

## Quick Facts
- arXiv ID: 2601.19334
- Source URL: https://arxiv.org/abs/2601.19334
- Authors: Jianzhe Chai; Yu Zhe; Jun Sakuma
- Reference count: 27
- Primary result: Inference-time embedding perturbations suppress test-set contamination in LLM evaluation with minimal clean performance loss

## Executive Summary
DeconIEP addresses test-set contamination in large language model evaluation, where leaked benchmark items inflate reported performance. The method applies small, bounded perturbations to input embeddings during inference, guided by a less-contaminated reference model, to suppress memorization-driven shortcuts while preserving reasoning behavior. Across multiple open-weight models and benchmarks, DeconIEP reduces residual contamination with minimal degradation in clean performance compared to existing inference-time baselines.

## Method Summary
DeconIEP trains a small generator network to produce bounded embedding perturbations that mitigate contamination effects at inference time. The generator learns to map input embeddings to perturbations that minimize the KL divergence between the contaminated model's output and a less-contaminated reference model's output, while maintaining task accuracy. Perturbations are constrained by an ℓ∞ norm bound to preserve semantic meaning. The method requires only a small auxiliary dataset of contaminated samples disjoint from the test set and operates without modifying the evaluated model's weights.

## Key Results
- Achieves 60-90% reduction in residual contamination across multiple benchmarks and contamination levels
- Maintains clean performance with only 0.5-2% benign utility drop compared to 5-15% for baselines
- Robust to reference model contamination up to 30% while preserving decontamination effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bounded embedding perturbations selectively suppress contamination-driven inference pathways while preserving reasoning.
- **Mechanism:** Contaminated samples trigger shortcut neurons and late-layer retrieval pathways that bypass genuine reasoning. Small ℓ∞-bounded perturbations δ(x) disrupt these memorization-specific circuits without altering the input's semantic neighborhood, forcing the model to rely on generalizable features instead.
- **Core assumption:** Memorization shortcuts are more sensitive to local embedding perturbations than distributed reasoning features.
- **Evidence anchors:** [abstract] "applies small, bounded perturbations to input embeddings during inference, guided by a less-contaminated reference model, to suppress memorization-driven shortcuts while preserving reasoning behavior" [section 1] "by applying small, targeted perturbations in the embedding space, we weaken contamination-driven dependencies and steer the model toward a cleaner inference regime" [corpus] Related work (Shortcut Neuron Analysis, Short-circuiting) confirms internal activation manipulation reduces memorization but causes larger clean degradation—DeconIEP achieves lower BUD, suggesting embedding-space intervention is more targeted.
- **Break condition:** If contamination pathways become as robust to embedding noise as reasoning features, or if ζ becomes too large and corrupts semantics (cosine similarity drops below paraphrase baseline).

### Mechanism 2
- **Claim:** A less-contaminated reference model provides a sufficient behavioral anchor for training perturbations, even if not perfectly clean.
- **Mechanism:** The generator Gθ learns to minimize KL divergence between the perturbed contaminated model fcon(e(x) + δ(x)) and reference model fref(e(x)). Theorem 1 shows this KL objective upper-bounds the performance gap to the ideal clean model. The reference need only exhibit relatively less contamination to guide the generator away from shortcuts.
- **Core assumption:** The reference model's output distribution is closer to clean behavior on contaminated inputs than the contaminated model's distribution.
- **Evidence anchors:** [section 4.2] "Theorem 1 shows that the average KL divergence between the mitigated contaminated model and the clean model upper-bounds the performance gap" [section 5.5, Figure 5] "performance is largely stable as the reference contamination increases from 0% to 30% across benchmarks" [corpus] No direct corpus evidence on reference model contamination tolerance; this is a paper-specific finding.
- **Break condition:** If the reference model is heavily contaminated (>30%), distributionally mismatched, or differs substantially in alignment behavior, the perturbation guidance becomes unreliable.

### Mechanism 3
- **Claim:** Instance-adaptive perturbations learned from a small auxiliary dataset generalize to unseen contaminated inputs.
- **Mechanism:** Rather than optimizing per-sample perturbations (infeasible at inference), the generator amortizes optimization by learning to map embedding sequences to perturbations. The KL+CE training objective produces structured, directional perturbations that target contamination patterns across the auxiliary set.
- **Core assumption:** Contamination manifests in learnable patterns that transfer from auxiliary samples to test samples.
- **Evidence anchors:** [section 4.3] "We therefore amortize the optimization by learning a generator Gθ" [section E.5, Figure 10] Random noise with matched ℓ∞ norm is substantially less reliable and becomes unstable as ζ grows, while learned perturbations improve smoothly. [corpus] ECO Prompts (Liu et al., 2024) uses random embedding corruption but achieves limited decontamination—supports the need for structured perturbations.
- **Break condition:** If the auxiliary dataset is too small (<100 samples per Figure 11) or unrepresentative, the generator fails to learn generalizable perturbation patterns.

## Foundational Learning

- **Concept:** KL divergence as a distribution-level distance metric
  - **Why needed here:** The entire training objective is framed as minimizing KL between perturbed contaminated output and reference output. Understanding that KL(P∥Q) measures how much information is lost when Q approximates P is essential for interpreting why this aligns behavior.
  - **Quick check question:** If KL(pcon∥pref) = 0, what does that imply about the two output distributions?

- **Concept:** Embedding-space perturbations vs. token-space modifications
  - **Why needed here:** The method operates in continuous embedding space rather than discrete text. This preserves semantic content (high cosine similarity) while altering internal representations—different from prompt rewriting methods that change the actual tokens.
  - **Quick check question:** Why might embedding perturbations preserve task difficulty better than paraphrasing the input text?

- **Concept:** Membership inference attacks (MIA) and their false-negative problem
  - **Why needed here:** Appendix B proves that detect-then-filter approaches fail because any non-zero FNR leaves residual contamination. This motivates the inference-time approach. Understanding the FNR/FPR tradeoff explains why filtering is fundamentally limited.
  - **Quick check question:** If contamination rate α = 20% and FNR = 10%, what fraction of the filtered test set is still contaminated? (Use Equation 9).

## Architecture Onboarding

- **Component map:** Input question x → embedding sequence e(x) → Generator Gθ → perturbation δ(x) → perturbed embeddings e(x)+δ(x) → contaminated model → output pcon; Reference model → output pref (for training only)

- **Critical path:**
  1. Sample auxiliary batch from Daux (disjoint from test set)
  2. Forward through contaminated model with perturbed embeddings
  3. Forward through reference model with clean embeddings
  4. Compute L = λKL · KL(pcon∥pref) + λCE · CE(pcon, yref)
  5. Backprop to Gθ only (contaminated and reference models frozen)

- **Design tradeoffs:**
  - **ζ (perturbation budget):** Larger ζ → stronger decontamination (lower RC) but higher BUD. Table 4 shows monotonic tradeoff. Default ζ = 10^-3 balances both.
  - **|Daux| (auxiliary set size):** More samples → better generalization but diminishing returns. Figure 11 suggests 400 samples is sufficient; 100 provides most gains.
  - **Reference model choice:** Base pretrained checkpoint is standard; earlier checkpoints expected to have less exposure. Imperfect references (≤30% contamination) are tolerated.

- **Failure signatures:**
  - **BUD spikes on clean benchmarks:** ζ too large; reduce budget. Check cosine similarity between original and perturbed embeddings.
  - **RC remains high:** Generator not learning; check that auxiliary data is from same benchmark distribution and reference is less contaminated.
  - **Instability across contamination levels:** Random noise baseline shows this—verify generator is actually learning (compare to random ablation).

- **First 3 experiments:**
  1. **Reproduce ablation (Figure 10):** Train with learned perturbations vs. random noise at ζ ∈ {10^-5, 10^-4, 10^-3}. Verify learned perturbations show smooth RC reduction while random becomes unstable.
  2. **Reference contamination robustness (Figure 5):** Construct references with 0%, 10%, 20%, 30% benchmark leakage and measure RC degradation. Confirm findings hold for your model/benchmark pair.
  3. **Auxiliary set size sweep (Figure 11):** Train with |Daux| ∈ {0, 50, 100, 200, 400, 800} and plot RC curve. Identify minimum viable size for your setting.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can DeconIEP be effectively adapted for black-box API access where internal gradients and embeddings are unavailable?
- **Basis in paper:** [explicit] The authors state in the Limitations section that DeconIEP is a white-box method requiring access to input embeddings and gradients, which "limits applicability to closed-model APIs without additional approximations."
- **Why unresolved:** The current methodology relies on backpropagation through the contaminated model to train the perturbation generator, which is impossible with standard API access.
- **What evidence would resolve it:** A modification of the framework using zeroth-order optimization or query-based methods that achieves comparable decontamination on closed models.

### Open Question 2
- **Question:** How does DeconIEP perform when the reference model is heavily contaminated or significantly distributionally mismatched?
- **Basis in paper:** [explicit] The authors note that while robust to moderate contamination (tested up to 30%), "performance may degrade when the reference is heavily contaminated, distributionally mismatched, or differs substantially in alignment behavior."
- **Why unresolved:** The experiments primarily used the base pretrained model as the reference, which inherently has low contamination; the failure modes under a highly corrupted reference are not mapped.
- **What evidence would resolve it:** Empirical results showing the degradation curve of Residual Contamination (RC) as the reference model's contamination rate approaches or exceeds that of the evaluated model.

### Open Question 3
- **Question:** Can formal guarantees be established to ensure that bounded embedding perturbations preserve semantic meaning and task difficulty for all inputs?
- **Basis in paper:** [explicit] The Limitations section states, "we do not provide formal guarantees that semantics/difficulty are preserved for all inputs," despite empirical evidence of high cosine similarity.
- **Why unresolved:** The paper relies on empirical metrics like cosine similarity to argue for semantic invariance, but lacks theoretical proofs bounding the semantic shift.
- **What evidence would resolve it:** A theoretical framework proving that for a given budget $\zeta$, the semantic divergence remains below a strict threshold across the data distribution.

### Open Question 4
- **Question:** How can the inference-time overhead introduced by the perturbation generator be minimized for very large-scale evaluation?
- **Basis in paper:** [explicit] The authors identify in the Limitations section that "reducing overhead for very large-scale evaluation remains an important direction," despite the method being lightweight relative to baselines.
- **Why unresolved:** The current architecture requires a forward pass through a 4-layer Transformer generator for every single input token, adding latency.
- **What evidence would resolve it:** A study comparing the latency-accuracy trade-off of the current generator against distilled or heuristic-based perturbation methods.

## Limitations

- The method requires white-box access to the contaminated model for training the perturbation generator, limiting applicability to closed APIs
- No formal guarantees that bounded perturbations preserve semantic meaning and task difficulty for all inputs
- Performance depends on having a reference model that is sufficiently less contaminated than the evaluated model

## Confidence

- **High confidence:** The BUD vs RC tradeoff is empirically validated across multiple contamination levels and benchmarks. The monotonic relationship with perturbation budget ζ is reproducible and theoretically grounded.
- **Medium confidence:** The mechanism by which embedding perturbations selectively disrupt contamination pathways while preserving reasoning. While supported by results, the neural basis is not directly measured.
- **Medium confidence:** The robustness of the method to reference model contamination up to 30%. This is demonstrated but may not generalize to all model pairs or contamination types.
- **Low confidence:** Generalization to highly semantic contamination (paraphrases) and to benchmarks with different contamination sources. These are not extensively tested.

## Next Checks

1. **Neural circuit isolation test:** Use activation patching or integrated gradients to compare feature attribution maps between original and perturbed inputs on contaminated vs clean samples. Verify that perturbations reduce attribution to late-layer memorization neurons while preserving reasoning-related features.

2. **Reference model mismatch stress test:** Systematically vary reference model contamination from 0% to 50% in 10% increments, measuring both RC and BUD. Identify the exact contamination threshold where the KL objective fails to guide perturbations toward clean behavior.

3. **Semantic contamination transfer test:** Create contaminated auxiliary sets using different paraphrasing strategies than the test set contamination. Train generators on each and measure RC on the held-out paraphrase set to validate cross-contamination generalization.