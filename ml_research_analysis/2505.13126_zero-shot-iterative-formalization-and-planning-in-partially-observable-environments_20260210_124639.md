---
ver: rpa2
title: Zero-Shot Iterative Formalization and Planning in Partially Observable Environments
arxiv_id: '2505.13126'
source_url: https://arxiv.org/abs/2505.13126
tags:
- object
- receptacle
- action
- closed
- location
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PDDLego+ is a framework that uses LLMs to iteratively formalize,
  plan, grow, and refine PDDL representations in partially observable environments
  without needing existing trajectories. Unlike prior work that focused on fully observable
  environments or assumed complete domain knowledge, PDDLego+ generates a complete
  PDDL domain and problem files from partial observations, refines them based on solver
  or simulation errors, and updates them with new observations.
---

# Zero-Shot Iterative Formalization and Planning in Partially Observable Environments

## Quick Facts
- arXiv ID: 2505.13126
- Source URL: https://arxiv.org/abs/2505.13126
- Reference count: 40
- Key outcome: PDDLego+ achieves 86% success on CoinCollector and 38% on ALFWorld, outperforming LLM-as-planner baselines (52% and 5%) through iterative PDDL refinement.

## Executive Summary
PDDLego+ is a framework that uses large language models to iteratively formalize, plan, grow, and refine PDDL representations in partially observable environments without needing existing trajectories. Unlike prior work that focused on fully observable environments or assumed complete domain knowledge, PDDLego+ generates complete PDDL domain and problem files from partial observations, refines them based on solver or simulation errors, and updates them with new observations. The method demonstrates that LLMs can build executable, interpretable plans in challenging, partially observable settings while maintaining robustness to environment complexity.

## Method Summary
PDDLego+ operates by having an LLM generate initial PDDL domain (DF) and problem (PF) files from the first observation, then iteratively refines them through a dual-loop error correction system. When the Fast Downward planner fails to parse or solve the PDDL, the LLM receives the error message and refines the representation in an inner loop. If the solver produces a valid plan but execution fails in simulation, the outer loop provides the simulation error and current state to the LLM for refinement. The system grows the PDDL representation with each successful plan execution by incorporating newly discovered objects, rooms, or relations from new observations. The framework uses sub-goal decomposition with templated goal structures to enable progress in partially observable settings where the full goal is initially unreachable.

## Key Results
- Achieved 86% success rate on CoinCollector versus 52% for LLM-as-planner baseline
- Achieved 38% success rate on ALFWorld versus 5% for LLM-as-planner baseline
- Demonstrated that learned domain knowledge can be reused for future tasks, though with model-dependent benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative PDDL formalization enables planning under partial observability by growing world models incrementally.
- Mechanism: The LLM generates initial domain (DF) and problem (PF) files from the first observation `obs_0`. After each successful plan execution, new observations are appended (`obs_{i+1} = [obs^1_i, ..., obs^{J+1}_i]`), and the LLM updates the PDDL to incorporate newly discovered rooms, objects, or relations. This transforms a POMDP into a progressively fuller representation.
- Core assumption: The LLM can reliably translate textual observations into syntactically valid PDDL predicates and maintain consistency across updates.
- Evidence anchors:
  - [abstract]: "iteratively formalize, plan, grow, and refine PDDL representations in a zero-shot manner, without needing access to any existing trajectories"
  - [section 3]: "df^{j+1}_i, pf^{j+1}_i = LLM(obs_{i+1}, df^j_i, pf^j_i)" — shows the update rule
  - [corpus]: PSALM-V (arXiv 2506.20097) similarly induces symbolic action semantics through interaction, supporting the iterative formalization paradigm.

### Mechanism 2
- Claim: Dual-loop error refinement (solver errors inner loop, simulation errors outer loop) recovers from both syntactic and semantic PDDL mistakes.
- Mechanism: When the Fast Downward planner fails to parse or solve PDDL, a solver error `err_sol` is returned to the LLM with the error message for refinement (inner loop ①). When the solver produces a valid plan but execution fails in simulation, a simulation error `err_sim` triggers the outer loop ② with state reset. Each loop allows up to 5 retries.
- Core assumption: Error messages are sufficiently informative for the LLM to diagnose and fix the underlying PDDL flaw.
- Evidence anchors:
  - [section 3]: "In case of errors in either of the two stages, the LLM refines the representation based on the current representation, an informative error message, and the trajectory so far"
  - [section 5.3, Fig 9]: o3-mini achieves 89.9% solver error fix rate and 93.6% simulation error fix rate on ALFWorld
  - [corpus]: No direct corpus comparison for dual-loop refinement; this appears novel to PDDLego+.

### Mechanism 3
- Claim: Sub-goal decomposition with templated goal structures enables progress in partially observable settings where the full goal is initially unreachable.
- Mechanism: Rather than specifying the final goal immediately, the LLM generates achievable sub-goals (e.g., `:goal (at ?location)` for unvisited locations in CoinCollector, or `:goal (opened ?receptacle)` for ALFWorld search). The "detailed prompt" provides both PDDL goal templates and procedural hints (e.g., "pick up the sharp object before slicing"). Upon sub-goal completion, the next sub-goal is generated.
- Core assumption: The LLM can correctly identify which sub-goal is appropriate given the current world state and will not generate impossible or redundant goals.
- Evidence anchors:
  - [section 4, Goal Specification]: "Unlike Zhang et al. (2024) who manually provided the sub-goal in PDDL, we consider two styles of goal specification in the LLM sub-goal generation prompt"
  - [section 5.4, Fig 12]: Ablation shows detailed prompt (hints + goal templates) achieving 38% success vs. 7% for simple prompt on ALFWorld with o3-mini
  - [corpus]: Related work (e.g., SwiftSage, arXiv 2305.17390) uses goal decomposition but in LLM-as-planner mode; LLM-as-formalizer decomposition is less explored.

## Foundational Learning

- Concept: **PDDL (Planning Domain Definition Language)**
  - Why needed here: PDDL separates domain knowledge (types, predicates, action semantics) from problem instances (objects, initial states, goals). PDDLego+ generates both files; without understanding PDDL syntax (:requirements, :types, :precondition, :effect), the framework is opaque.
  - Quick check question: Given action `(:action open-door :parameters (?loc1 - location ?loc2 - location) :precondition (door-closed ?loc1 ?loc2) :effect (and (not (door-closed ?loc1 ?loc2)) (door-open ?loc1 ?loc2)))`, what happens if the precondition is omitted?

- Concept: **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper explicitly models environments as POMDPs where the agent receives `obs_i` (partial textual description) rather than full state `S_i`. Understanding that the agent must balance exploration (information gathering) with exploitation (goal achievement) is essential.
  - Quick check question: In a POMDP, why can't an agent generate a complete plan at time step 0?

- Concept: **LLM-as-Formalizer vs. LLM-as-Planner**
  - Why needed here: The paper positions itself against "LLM-as-planner" baselines (direct action generation). LLM-as-formalizer delegates plan search to a symbolic planner, gaining formal guarantees (soundness, completeness) at the cost of requiring valid PDDL generation. This tradeoff explains the architecture choice.
  - Quick check question: If an LLM-as-formalizer generates syntactically correct but semantically wrong PDDL (e.g., missing preconditions), will the planner catch this?

## Architecture Onboarding

- Component map: Observation (obs_i) → LLM → PDDL (df_i, pf_i) → Fast Downward Solver → Plan p_i → Simulation

- Critical path:
  1. Initial observation `obs_0` triggers PDDL generation (`df_0`, `pf_0`)
  2. Solver attempts planning; on failure, inner loop refines PDDL
  3. Valid plan executes in simulation; on failure, outer loop refines PDDL with error context
  4. On success, new observations `obs_{i+1}` trigger PDDL growth (new objects, predicates)
  5. Repeat until goal reached or retry limits exhausted

- Design tradeoffs:
  - **Prompt detail vs. flexibility**: Detailed prompts (goal templates + hints) improve success (38% vs 7% on ALFWorld) but reduce generality; simple prompts may transfer better to unseen domains
  - **Model selection**: Only models >100B parameters work reliably; o3-mini outperforms GPT-4.1 on error recovery (89.9% vs 57.5% solver fix rate). Smaller models (Llama-70B, Qwen-32B) perform "significantly worse"
  - **Retry limits**: 5 retries per error type balances robustness against infinite loops; increasing retries may help harder tasks but increases latency and cost

- Failure signatures:
  - **Hallucinated facts in PF** (8 cases in error analysis): LLM invents objects like `(in winebottle cabinet1)` never observed; plan executes but fails at simulation
  - **Inconsistent world state** (4 cases): LLM overwrites rather than appends to PF, "forgetting" previously observed rooms/doors
  - **Bad goals** (6 cases): Solver crashes on `(exists (?to - receptacle) (not (at toilet1)))` or empty goals
  - **Action semantics errors in DF** (7 cases): Missing preconditions like `(holding ?o)` before `PutObject` cause silent failures only exposed during execution

- First 3 experiments:
  1. **Reproduce CoinCollector baseline**: Run PlanGen (LLM-as-planner) vs. PDDLego+ with o3-mini on 20 trials of 5-room instances. Verify PDDLego+ achieves ~85% vs. PlanGen's ~55%. Log all solver/simulation errors and categorize them using the taxonomy in Section 6.
  2. **Ablate error refinement loops**: Disable the inner loop (no solver error refinement) and measure success rate drop. Expected: significant degradation toward PDDLego baseline (49% on CoinCollector per Fig 6). This validates the dual-loop contribution.
  3. **Test domain reuse (RAG_DF)**: After a successful PDDLego+ run, freeze the learned domain file (DF) and reuse it for new trials where only PF is predicted. Compare success rates: paper shows this helps GPT-4.1 but surprisingly hurts o3-mini (Fig 10). Investigate whether incomplete DFs (missing preconditions) cause the regression.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PDDLego+ effectively operate in environments that lack readily available, informative error messages by automatically synthesizing feedback?
- Basis in paper: [explicit] The authors explicitly identify the need to apply the method to environments where error feedback is not readily available and must be "automatically synthesized."
- Why unresolved: The current framework relies on manually implemented, fine-grained error messages (e.g., "You can't slice without a knife") to drive the refinement loops, rather than the vague default feedback of simulators.
- What evidence would resolve it: Successful deployment of PDDLego+ in a text or vision-based environment using a module that infers failure causes from generic feedback (e.g., "Nothing happens").

### Open Question 2
- Question: Can a unified, structured memory architecture replace the current reliance on environment-specific prompt engineering?
- Basis in paper: [explicit] The authors list working towards "unified, structured memory that is agnostic to domains" as a key direction to limit the dependence on environment-specific prompt wording.
- Why unresolved: The current "detailed prompt" requires specific PDDL goal templates and hint blocks (e.g., "pick up the sharp object before slicing") to achieve high success, suggesting the model lacks a generic reasoning structure.
- What evidence would resolve it: A version of PDDLego+ achieving comparable success rates on ALFWorld using a single, domain-agnostic prompt without task-specific heuristic hints.

### Open Question 3
- Question: Why does the provision of a "ground-truth" Domain File (DF) during retrieval-augmented generation fail to improve success rates for the o3-mini model?
- Basis in paper: [inferred] Section 5.5 notes a "performance drop" for o3-mini when provided with retrieved DFs and states this "warrants future investigation," despite improving results for other models.
- Why unresolved: The authors expected the "ground-truth" domain knowledge to aid planning, but found that incomplete DFs sometimes succeeded where complete ones failed, suggesting complex interactions between the model's reasoning and the provided constraints.
- What evidence would resolve it: An ablation study analyzing how o3-mini utilizes retrieved constraints versus self-generated constraints, specifically measuring the rate of semantic hallucinations in both conditions.

## Limitations

- The method depends heavily on the LLM's ability to generate valid PDDL and interpret fine-grained error messages, with significant hallucination and inconsistency rates that could undermine reliability in more complex domains.
- Manual creation of "fine-grained" error messages for simulation failures is a critical engineering step not fully detailed in the appendix, raising concerns about generalizability.
- The claim that learned domain knowledge can be reused for future tasks is supported but the failure case (RAG_DF degrading o3-mini performance) suggests this benefit is model-dependent and not universally reliable.

## Confidence

- **High**: The iterative PDDL formalization mechanism is clearly described and the dual-loop error refinement is a novel and well-defined contribution.
- **Medium**: Success rates are reported but the sensitivity to different error message granularities and the reproducibility of the "fine-grained" errors are unclear.
- **Low**: The claim that learned domain knowledge can be reused for future tasks is supported but the failure case (RAG_DF degrading o3-mini performance) suggests this benefit is model-dependent and not universally reliable.

## Next Checks

1. **Error Message Granularity**: Test PDDLego+ with default vague simulation errors versus the paper's "fine-grained" errors to quantify the impact on solver fix rates and overall success.
2. **Model Scaling Sensitivity**: Systematically evaluate success rates across the model spectrum (Llama-70B, Qwen-32B, GPT-4.1, o3-mini) on a fixed task to identify the minimum viable model size and pinpoint where performance collapses.
3. **Domain File Stability**: For a successful PDDLego+ run, measure the stability of the learned DF across multiple trials on the same domain. Quantify how often the LLM regenerates the DF from scratch versus reusing and refining the existing one.