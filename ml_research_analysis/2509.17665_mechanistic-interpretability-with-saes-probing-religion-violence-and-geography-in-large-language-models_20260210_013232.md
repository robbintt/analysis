---
ver: rpa2
title: 'Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography
  in Large Language Models'
arxiv_id: '2509.17665'
source_url: https://arxiv.org/abs/2509.17665
tags:
- features
- religion
- language
- across
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how religious identities are internally
  represented in large language models (LLMs) using mechanistic interpretability.
  The authors analyze latent feature activations across five models (GPT-2, Gemma-2,
  and Llama3.1) using Sparse Autoencoders (SAEs) via the Neuronpedia API, examining
  overlap between religion- and violence-related concepts and probing semantic patterns.
---

# Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models

## Quick Facts
- **arXiv ID**: 2509.17665
- **Source URL**: https://arxiv.org/abs/2509.17665
- **Reference count**: 33
- **Primary result**: Islam shows consistently higher association with violence-related features across all analyzed models, with Violence Association Index scores above 100 compared to other religions

## Executive Summary
This paper investigates how religious identities are internally represented in large language models (LLMs) using mechanistic interpretability. The authors analyze latent feature activations across five models (GPT-2, Gemma-2, and Llama3.1) using Sparse Autoencoders (SAEs) via the Neuronpedia API, examining overlap between religion- and violence-related concepts and probing semantic patterns. Islam shows consistently higher association with violence-related features across all models, with Violence Association Index (VAI) scores above 100 compared to other religions (94-117 range). While all five religions show comparable internal cohesion, geographic associations largely reflect real-world distributions but with Western bias (Europe/North America most frequent). Crime-related activation texts show Islam with highest proportion in most models (2.44-3.46%), though some models show Hinduism higher. The findings reveal how models embed both factual distributions and cultural stereotypes, highlighting the value of structural analysis for auditing internal representations beyond just outputs.

## Method Summary
The study uses Sparse Autoencoders to decompose LLM activations into interpretable latent features. The authors query the Neuronpedia API with minimal prompts ("This is a [keyword]") for religious terms (Table 4) and violence terms (Table 5) across five models. They retrieve top 20 activating feature IDs per prompt, calculate overlap between religion and violence feature sets using a normalized Violence Association Index (VAI), and analyze activation texts for crime and geographic keywords. The approach focuses on structural analysis of internal representations rather than output behavior, providing insight into latent biases embedded in model reasoning.

## Key Results
- Islam consistently shows higher violence association across all models (VAI consistently >100)
- All five religions demonstrate comparable internal cohesion in their feature representations
- Geographic associations reflect real-world distributions but show Western bias (Europe/North America most frequent)
- Crime-related activation texts show Islam with highest proportion in most models (2.44-3.46%)

## Why This Works (Mechanism)

### Mechanism 1: Sparse Decomposition of Polysemantic Neurons
If Sparse Autoencoders (SAEs) successfully enforce sparsity, they decompose dense, polysemantic LLM activations into monosemantic, human-interpretable latent features. An SAE encoder maps internal LLM activation vectors to a higher-dimensional sparse vector where only a small subset of features activate for a given input. This forces the model to separate entangled concepts (e.g., "mosque" and "violence") into distinct feature dimensions. The mechanism fails if the SAE reconstruction error is high, or if features remain polysemantic (activating for unrelated concepts), rendering the "religion" vs. "violence" separation meaningless.

### Mechanism 2: Feature Overlap as Association Proxy
The statistical overlap of activated feature IDs between prompt groups (e.g., religious terms and violence terms) serves as a proxy for the strength of internal conceptual associations within the LLM. By querying the SAE with controlled prompts, the system retrieves top activating feature IDs. The "Violence Association Index" (VAI) quantifies the intersection of these sets, revealing how tightly the model links the concepts in latent space. This fails if the prompt set is too small, causing random feature activation noise to dominate the signal.

### Mechanism 3: Semantic Fingerprinting via Activation Context
Analyzing the text contexts that maximally activate a specific feature (activation texts) reveals the "semantic fingerprint" or stereotypical associations encoded in that feature. The Neuronpedia API provides example texts that strongly activate a given feature. By scanning these texts for secondary keywords (e.g., geographic terms, crime terms), researchers infer the feature's role in the model's reasoning (e.g., associating "Islam" features with "Middle East" or "terrorism"). The mechanism fails if the keyword list is incomplete or if the activation texts are outliers, providing a distorted view of the feature's typical usage.

## Foundational Learning

- **Concept: Neuron Polysemanticity**
  - **Why needed here**: The entire premise of using SAEs relies on understanding that standard LLM neurons are "polysemantic" (responding to multiple unrelated concepts). Without this concept, one would not understand why SAEs are necessary to isolate "religion" from "violence" signals.
  - **Quick check question**: Why can't we simply probe raw neurons in GPT-2 to find the "Islam" concept?

- **Concept: Intrinsic vs. Extrinsic Bias**
  - **Why needed here**: The paper explicitly distinguishes between bias in *outputs* (extrinsic) and bias in *internal representations* (intrinsic). Understanding this distinction is required to see why the paper's methodology (probing internals) is novel compared to traditional output auditing.
  - **Quick check question**: Why does a model producing neutral outputs still potentially require an internal bias audit?

- **Concept: Residual Stream vs. Attention Layers**
  - **Why needed here**: The study uses SAEs trained on different layers (indicated by `-res` and `-att` in Table 1). Knowing the difference helps in interpreting why certain associations might appear in the residual stream (accumulated context) versus attention heads (token relationships).
  - **Quick check question**: In Table 1, what is the functional difference between the `res-jb` and `att-kk` SAE configurations?

## Architecture Onboarding

- **Component map**: Input Layer -> Interpretability Layer (SAEs via Neuronpedia API) -> Analysis Engine (Overlap Calculator + Semantic Scanner)

- **Critical path**:
  1. **Prompt Construction**: Define minimal sentences for Religion (Table 4) and Violence (Table 5).
  2. **Feature Retrieval**: Query Neuronpedia API for top 20 feature IDs per prompt.
  3. **Intersection Analysis**: Compute raw overlap and normalized VAI to identify bias.
  4. **Semantic Validation**: Retrieve activation texts for top features and scan for geographic/crime keywords to verify the *nature* of the association.

- **Design tradeoffs**:
  - **Prompt minimalism**: The authors use "This is a..." templates to reduce noise, but this may strip necessary context, potentially missing nuance that appears in longer sentences.
  - **Metric selection**: The authors abandoned cosine similarity for discrete feature overlap because "results were too noisy due to extreme sparsity." This improves robustness but loses vector magnitude information.
  - **Pre-trained SAEs**: Relying on Neuronpedia saves compute but limits the audit to specific model layers and architectures already available on the platform.

- **Failure signatures**:
  - **Sparsity Noise**: If prompts activate <20 features out of 100k+, overlap metrics become unstable.
  - **Western-Centric Features**: The system may fail to detect non-Western geographic associations if the SAEs themselves lack features for those regions.
  - **Template Artifacts**: If the phrase "This is a..." accidentally activates syntactic features unrelated to the keyword noun.

- **First 3 experiments**:
  1. **Baseline Sanity Check**: Query the SAE with neutral, non-religious concepts (e.g., "This is a car") and verify that the "Violence Association Index" is near zero to validate the metric.
  2. **Ablation on Prompt Context**: Repeat the "Islam" and "Christianity" queries using diverse sentence structures (beyond "This is a...") to ensure the observed bias is not an artifact of the specific prompt template.
  3. **Cross-Architecture Comparison**: Compare the VAI of `gemmascope-res-16k` (2B parameter model) vs. `gemmascope-res-16k` (9B parameter model) to test the hypothesis that larger models exhibit more compact/abstract (and potentially different) bias structures.

## Open Questions the Paper Calls Out
- Do multilingual LLMs exhibit different religion-violence associations across languages, or are these biases consistent regardless of linguistic context? (The paper analyzed only English prompts on English-centric models; no cross-linguistic comparison was conducted.)
- Can targeted adjustments to latent feature weights identified by SAEs effectively reduce biased outputs without degrading overall model performance? (The paper demonstrates that SAEs can uncover biases but does not test whether intervening on those features changes downstream behavior.)
- Do internal religion-violence associations detected via SAEs predict measurable biases in downstream tasks such as text classification, summarization, or retrieval? (The study establishes internal representation biases but does not link them quantitatively to output-level harms.)

## Limitations
- Feature interpretation validity is uncertain as activation texts may not fully represent a feature's typical behavior, potentially introducing sampling bias
- Cross-cultural generalizability is limited by Western-centric geographic associations reflecting training data distribution rather than model reasoning capability
- Sparsity threshold effects are not well-characterized, with methodology relying on top-20 feature activations without reporting percentage of total features

## Confidence
- **High confidence**: Islam shows consistently higher violence association across all five models (VAI consistently >100)
- **Medium confidence**: Comparable internal cohesion across all five religions, though semantic interpretation remains subjective
- **Low confidence**: Geographic association patterns, particularly Western bias, due to potential keyword list incompleteness

## Next Checks
1. **Cross-linguistic validation**: Repeat the analysis using non-English prompts (e.g., Arabic, Hindi) to determine if violence association patterns persist across languages
2. **Temporal stability analysis**: Track how violence association indices change across different SAE checkpoints or model versions to assess stability
3. **Counterfactual prompting experiment**: Test whether adding context that contradicts stereotypes reduces the violence association score, distinguishing between learned associations and inherent bias