---
ver: rpa2
title: Preserving AUC Fairness in Learning with Noisy Protected Groups
arxiv_id: '2505.18532'
source_url: https://arxiv.org/abs/2505.18532
tags:
- fairness
- noisy
- protected
- groups
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preserving AUC fairness when
  protected group labels are noisy, a common issue in real-world datasets like surveys
  or AI-generated media. Existing methods fail under such conditions, leading to fairness
  violations.
---

# Preserving AUC Fairness in Learning with Noisy Protected Groups

## Quick Facts
- arXiv ID: 2505.18532
- Source URL: https://arxiv.org/abs/2505.18532
- Reference count: 40
- Addresses AUC fairness under noisy protected group labels

## Executive Summary
This paper tackles the challenge of maintaining AUC fairness when protected group labels are corrupted by noise, a common issue in real-world datasets. The authors propose the first robust AUC fairness approach using distributionally robust optimization (DRO) with total variation bounds between clean and noisy group distributions. Their method theoretically guarantees fairness preservation while maintaining strong AUC performance, validated through extensive experiments on both tabular and image datasets.

## Method Summary
The method employs distributionally robust optimization to bound the total variation distance between clean and noisy protected group distributions. It uses a stochastic gradient descent-ascent algorithm with Sharpness-Aware Minimization to optimize the objective. The approach estimates noise levels using pre-trained multi-modal foundation models like CLIP for images, and empirically tunes parameters for tabular data. The method guarantees fairness preservation while maintaining AUC performance through carefully designed optimization constraints.

## Key Results
- Achieves significantly lower fairness violations (0.0316 vs 0.0766 for AUCMax baseline) across varying noise levels
- Maintains higher Min/Max fairness scores while preserving overall AUC performance
- Outperforms state-of-the-art approaches on both tabular (Adult, Bank, Default) and image datasets (FF++, DFDC, DFD, Celeb-DF)

## Why This Works (Mechanism)
The method works by explicitly modeling the distribution shift between clean and noisy protected group labels through total variation bounds. By optimizing a distributionally robust objective that accounts for this uncertainty, the model learns representations that are invariant to label noise while maintaining fairness constraints. The SGDA optimization ensures both the fairness constraints and the classification objective are simultaneously optimized, with SAM providing additional robustness to local minima.

## Foundational Learning
- **Distributionally Robust Optimization**: Needed to handle uncertainty in protected group labels; check by verifying the TV bound implementation matches the theoretical formulation.
- **Total Variation Distance**: Measures distribution shift between clean and noisy labels; verify by computing TV distances on validation sets.
- **Stochastic Gradient Descent-Ascent**: Enables constrained optimization for fairness; check convergence by monitoring constraint satisfaction over epochs.
- **Sharpness-Aware Minimization**: Improves generalization by avoiding sharp minima; verify by comparing training/test performance with/without SAM.
- **Multi-modal Foundation Models**: Provide semantic understanding for noise estimation; check by validating CLIP-based similarity scores against ground truth.

## Architecture Onboarding
**Component Map**: Data preprocessing -> TV noise estimation -> DRO optimization (SGDA+SAM) -> Model training
**Critical Path**: The DRO optimization loop with SGDA updates is critical - any instability here directly impacts fairness preservation.
**Design Tradeoffs**: Balances fairness preservation against classification performance through the TV bound parameter γ; tighter bounds provide better fairness but may reduce AUC.
**Failure Signatures**: 
- Constraint violations (>0.05) indicate need for higher dual learning rates
- Overall AUC ≈ 0.5 suggests over-regularization
- Training instability suggests batch sampling issues
**First Experiments**:
1. Implement stratified batch sampler ensuring all (label, group) combinations per batch
2. Test TV distance estimation using CLIP on sample image pairs
3. Verify SGDA convergence with simple synthetic data and known constraints

## Open Questions the Paper Calls Out
**Open Question 1**: How can the noise ratio estimation for the Total Variation distance bound be concretely implemented for tabular data without relying on multi-modal foundation models like CLIP? The authors acknowledge the lack of a concrete noise estimation strategy for tabular modalities as a limitation.

**Open Question 2**: Can this distributionally robust optimization framework be extended to preserve fairness in other pairwise ranking metrics, such as partial AUC or average precision? The authors list this as future work, noting the current formulation is tailored to standard AUC definitions.

**Open Question 3**: Is it possible to theoretically guarantee both high utility and strict fairness simultaneously when training with noisy protected group labels? The authors note that ensuring both utility and fairness simultaneously when training with noisy groups remains an open challenge.

## Limitations
- Lack of concrete noise estimation strategy for tabular modalities
- Theoretical framework specific to standard AUC definitions
- Trade-off between utility and fairness not fully resolved

## Confidence
- **Method validity**: High - The DRO formulation and SGDA optimization framework are mathematically sound
- **Theoretical guarantees**: High - The fairness preservation bounds and convergence analysis appear rigorous
- **Empirical performance claims**: Medium - Results depend on precise hyperparameter tuning not fully specified
- **Computational efficiency**: High - The algorithmic complexity and convergence rates are clearly stated

## Next Checks
1. Verify constraint satisfaction: Monitor the fairness violation metric throughout training on validation set; if consistently >0.05, adjust dual variable learning rate η_λ upward by factor of 10.
2. Baseline replication: Implement and run the three compared methods (baseline, ROCMax, AUCMax) using identical hyperparameters and datasets; compare their fairness violations and Min/Max ratios against reported results.
3. Hyperparameter sensitivity analysis: Systematically vary initial learning rates η_θ across full specified ranges for both tabular and image datasets; record effects on both fairness violation and AUC performance.