---
ver: rpa2
title: Dance Dance ConvLSTM
arxiv_id: '2507.01644'
source_url: https://arxiv.org/abs/2507.01644
tags:
- step
- convlstm
- dance
- ddcl
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Dance Dance ConvLSTM (DDCL), a novel method
  for automatically generating Dance Dance Revolution (DDR) charts using a ConvLSTM-based
  model. DDCL improves upon the previous Dance Dance Convolution (DDC) methodology
  by integrating beat-position relevant information through a branched bidirectional
  ConvLSTM encoder, allowing for better tracking of audio contextual information relative
  to beat positions.
---

# Dance Dance ConvLSTM

## Quick Facts
- **arXiv ID**: 2507.01644
- **Source URL**: https://arxiv.org/abs/2507.01644
- **Reference count**: 40
- **Primary result**: Novel ConvLSTM-based DDR chart generation method that generalizes across BPMs and achieves higher accuracy than DDC

## Executive Summary
Dance Dance ConvLSTM (DDCL) introduces a novel approach to automatic Dance Dance Revolution (DDR) chart generation using a branched bidirectional ConvLSTM encoder. The method improves upon the previous Dance Dance Convolution (DDC) by integrating beat-position relevant information through the ConvLSTM architecture, allowing the model to better track audio contextual information relative to beat positions. This adaptation enables DDCL to generate charts for various BPMs, whereas DDC was limited to a constant 120 BPM. The model achieves substantially higher accuracy in both step placement and step selection tasks compared to DDC, with an F1 score of 0.7245 for step placement versus DDC's 0.5797.

## Method Summary
DDCL employs a two-stage pipeline for DDR chart generation. The Step Placement model uses a branched bidirectional ConvLSTM encoder to process mel-spectrogram features sampled 32 times per beat, followed by an LSTM decoder that outputs binary predictions for step placement at each sub-beat. The Step Selection model then generates specific arrow presses using an autoregressive LSTM that conditions on both previous steps and local audio features processed through a ConvLSTM branch. The entire system is trained on the Fraxtil dataset of 95 charts using Adam optimization with early stopping based on validation PR-AUC.

## Key Results
- DDCL achieves an F1 score of 0.7245 for step placement, compared to DDC's 0.5797
- The model improves step selection accuracy across all evaluated charts
- DDCL successfully generalizes to various BPMs, unlike DDC which was limited to 120 BPM
- The ConvLSTM encoder enables better tracking of local musical motifs and rhythmic cues

## Why This Works (Mechanism)

### Mechanism 1: Beat-Aligned Sampling
By sampling audio features 32 times per beat rather than at fixed time intervals, DDCL normalizes temporal distance between observations across variable tempos. This ensures rhythmic patterns appear consistently spaced in the input tensor, preventing fast songs from appearing "stretched" or slow songs "compressed" compared to training data.

### Mechanism 2: ConvLSTM Spatial Preservation
ConvLSTM layers maintain the 2D structure (time x frequency) of mel-spectrograms in the hidden state, allowing the model to track movement of specific frequency patterns over time. This spatial preservation helps identify musical motifs that dictate step placement more efficiently than flattened LSTM inputs.

### Mechanism 3: Two-Stage Pipeline Separation
Separating generation into "placement" (when to step) and "selection" (which arrow) with audio context injected into both improves rhythmic coherence. The first stage solves rhythmic alignment using full audio context, while the second solves kinesthetic patterning using step history and local audio features.

## Foundational Learning

- **ConvLSTM (Convolutional LSTM)**: Core architectural change from DDC that replaces matrix multiplication with convolution operations inside LSTM gates, allowing spatial feature preservation in audio processing. *Quick check: How does the dimensionality of the hidden state in a ConvLSTM differ from a standard LSTM when processing a spectrogram?*

- **Mel-Spectrograms**: 2D image-like representation (Frequency x Time) of audio with log scaling to match human perception, justifying convolutional layer usage. *Quick check: Why use 3 channels with different STFT window sizes (23ms, 46ms, 92ms) for the input?*

- **Autoregressive Sequence Generation**: The Step Selection model generates steps one by one, conditioning on its own previous outputs, creating sequential dependencies that limit parallelization. *Quick check: Why is the Step Selection model harder to parallelize than the Step Placement model?*

## Architecture Onboarding

- **Component map**: Pre-processor (BPM detection + mel-spectrogram extraction) -> Step Placement Network (branched ConvLSTM -> Max Pool -> LSTM Decoder -> Dense) -> Step Selection Network (LSTM history || ConvLSTM audio -> Concatenate -> Dense)

- **Critical path**: BPM detection is the single point of failure. Incorrect BPM estimation causes misaligned 32-sample window slicing, which propagates errors through the entire pipeline.

- **Design tradeoffs**: Accuracy vs. Speed (higher complexity increases accuracy but reduces generation speed), Parallelism (step placement is parallelizable while step selection is sequential), and computational expense of ConvLSTM layers versus standard LSTMs.

- **Failure signatures**: Off-beat steps indicate BPM detection failure or incorrect beat alignment, impossible patterns may emerge from out-of-distribution difficulties, and overfitting on specific charting styles when training on mixed datasets.

- **First 3 experiments**: 
  1. Sanity Check: Verify 32 audio samples align perfectly with beat grid for a song with known constant BPM
  2. Ablation: Compare Step Placement performance using standard LSTM versus ConvLSTM to quantify spatial feature benefits
  3. Inference Test: Generate charts for songs with significant tempo changes to verify dynamic BPM handling

## Open Questions the Paper Calls Out

- Can the ConvLSTM encoder utilized in DDCL generalize to improve performance in broader Music Information Retrieval (MIR) tasks beyond rhythm game charting?

- Would a fully end-to-end architecture combining step placement and selection yield higher accuracy than the current two-stage pipeline?

- Does training separate models for specific charting disciplines (e.g., "technical" vs. "stream") improve performance compared to training on a mixed dataset?

## Limitations

- The BPM detection algorithm (van de Wetering/ArrowVortex) is critical for proper temporal alignment but lacks implementation details in the paper
- The model may struggle with songs containing significant tempo changes if the beat tracker cannot handle dynamic BPM
- The exact kernel sizes for ConvLSTM layers are unspecified, potentially affecting reproducibility

## Confidence

- **High Confidence**: The architectural improvements (ConvLSTM vs LSTM, beat-aligned sampling) are clearly described and demonstrate measurable performance gains on the Fraxtil dataset
- **Medium Confidence**: The claim that DDCL generalizes to various BPMs is supported by the methodology but lacks extensive cross-BPM validation across diverse datasets
- **Low Confidence**: The paper doesn't address potential failures with songs containing irregular rhythms, tempo changes, or heavily syncopated patterns that might confuse the beat tracker

## Next Checks

1. Cross-Dataset Validation: Test DDCL on an independent DDR chart dataset to verify generalization beyond the Fraxtil dataset
2. Beat Tracker Robustness: Systematically test BPM detection accuracy across songs with varying BPM ranges (e.g., 80-180 BPM) and tempo changes to identify failure thresholds
3. Human Evaluation Study: Conduct a blind comparison study where experienced DDR players rate the musicality and playability of DDCL-generated charts versus DDC charts across different BPM ranges and difficulty levels