---
ver: rpa2
title: Learning Distributions over Permutations and Rankings with Factorized Representations
arxiv_id: '2505.24664'
source_url: https://arxiv.org/abs/2505.24664
tags:
- permutations
- perm
- learning
- permutation
- lehmer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to learning probability
  distributions over permutations by leveraging alternative representations such as
  Lehmer codes, Fisher-Yates draws, and Insertion-Vectors. These representations form
  a bijection with the symmetric group, enabling unconstrained learning using conventional
  deep learning techniques like Masked Language Modeling (MLM) and autoregressive
  models.
---

# Learning Distributions over Permutations and Rankings with Factorized Representations

## Quick Facts
- arXiv ID: 2505.24664
- Source URL: https://arxiv.org/abs/2505.24664
- Authors: Daniel Severo; Brian Karrer; Niklas Nolte
- Reference count: 40
- Primary result: Novel approach for learning probability distributions over permutations using factorized representations (Lehmer codes, Fisher-Yates draws, Insertion-Vectors) that enable unconstrained deep learning and achieve state-of-the-art results on jigsaw puzzle benchmark

## Executive Summary
This paper introduces a novel approach to learning probability distributions over permutations by leveraging alternative representations such as Lehmer codes, Fisher-Yates draws, and Insertion-Vectors. These representations form a bijection with the symmetric group, enabling unconstrained learning using conventional deep learning techniques like Masked Language Modeling (MLM) and autoregressive models. The method allows for a trade-off between model expressivity and computational efficiency, with the least expressive mode subsuming well-established models like Mallow's and the Repeated Insertion Model (RIM). Experiments demonstrate significant improvements on the jigsaw puzzle benchmark, outperforming previous diffusion and convex-relaxation methods.

## Method Summary
The approach transforms permutation learning into sequence modeling by using alternative representations (Lehmer codes, Fisher-Yates draws, Insertion-Vectors) that form bijections with permutations. This enables unconstrained learning with standard deep learning techniques like MLM and autoregressive models. The method provides a trade-off between expressivity and computational efficiency through control of neural function evaluations (NFEs), where lower NFEs use factorized sampling for speed while maintaining valid permutations. The paper establishes a novel relationship between insertion-vectors and Lehmer codes that enables efficient batched decoding.

## Key Results
- Achieves state-of-the-art performance on jigsaw puzzle benchmark, outperforming diffusion and convex-relaxation methods
- Introduces two new benchmarks: learning cyclic permutations and re-ranking movies based on user preferences
- Demonstrates significant expressivity gains at low NFE for factorized representations versus inline notation
- Shows Fisher-Yates representation can model cyclic permutations perfectly at any NFE, while inline fails at low NFE

## Why This Works (Mechanism)

### Mechanism 1: Factorized Representations via Bijections
Learning alternative representations of permutations (Lehmer codes, Fisher-Yates draws, Insertion-Vectors) that form bijections with the symmetric group enables unconstrained learning using standard language modeling objectives. The method transforms the constrained permutation learning problem into an unconstrained sequence modeling problem by using representations where each element's domain depends only on position, not on previous values. The bijection property ensures any valid sequence of values in these domains maps to exactly one valid permutation.

### Mechanism 2: Expressivity-Compute Trade-off via NFE Control
The number of neural function evaluations (NFEs) controls a trade-off between model expressivity and computational efficiency. By partitioning the sequence into sets and sampling elements within each set independently given previous sets, the model can operate at different NFE levels. Low NFE (1-5) prioritizes speed and works for simple distributions; high NFE ($n$) captures complex distributions but is slower. Factorized representations maintain validity even at low NFE, unlike inline notation which can only model delta functions at this extreme.

### Mechanism 3: Connection Between Insertion-Vectors and Lehmer Codes
A novel relationship $V(X)_k = k - L(X^{-1})_k$ enables efficient batched decoding of insertion-vectors by leveraging existing Lehmer algorithms. The insertion vector element counts smaller elements to the left of the position where value $k$ appears in the permutation, which equals $k$ minus the Lehmer code of the inverse permutation. Since Lehmer encoding/decoding has known $O(n \log n)$ algorithms, this enables efficient batched operations for insertion-vectors.

## Foundational Learning

- Concept: Permutations and the Symmetric Group
  - Why needed here: The entire approach is built on representing and learning distributions over $S_n$ (the group of all $n!$ permutations). Understanding inline notation, inverses, transpositions, and bijections is prerequisite to grasping how alternative representations work.
  - Quick check question: Given a permutation $\pi = [3, 1, 4, 2]$, what is $\pi^{-1}$? How many inversions does it contain?

- Concept: Factorized vs. Autoregressive Language Models
  - Why needed here: The method uses MLM (masked language modeling) and AR (autoregressive) objectives. Understanding conditional independence assumptions in factorized distributions is critical for the NFE-expressivity trade-off mechanism.
  - Quick check question: In a fully factorized model with $P(X) = \prod_i P(X_i)$, can the model capture that $X_2$ must differ from $X_1$? What happens when generating permutations in inline notation under this factorization?

- Concept: Cross-Entropy Loss with Domain Constraints
  - Why needed here: Training minimizes cross-entropy subject to the constraint that invalid permutations have zero probability. Understanding how softmax output domains enforce this constraint in factorized representations is key to implementation.
  - Quick check question: For a Lehmer code of length $n=4$, what are the valid domains for positions $1, 2, 3, 4$? How would you mask a softmax output to respect these varying domains?

## Architecture Onboarding

- Component map: Data Preparation -> Representation Encoder -> Transformer Backbone -> Domain-Constrained Output Heads -> Representation Decoder -> Sampling/Decoding

- Critical path:
  1. Data Preparation: Encode target permutations into chosen representation using `lehmer_encode`, `fisher_yates_encode`, or `insertion_vector_encode`
  2. Training: Feed representation sequences to transformer, compute cross-entropy loss with position-aware domain masking
  3. Sampling: For MLM, partition indices into $k$ sets; sample each set conditioned on previous ones. For AR, sample sequentially
  4. Decoding: Convert sampled representation to inline permutation via `lehmer_decode`, `fisher_yates_decode`, or `insertion_vector_decode`

- Design tradeoffs:
  - Representation Choice: Lehmer is intuitive (SWOR interpretation) and relates to Kendall's tau; Fisher-Yates is fastest to decode and naturally handles cyclic permutations via Sattolo's algorithm; Insertion-Vector uniquely supports conditioning on sub-permutations (critical for MovieLens re-ranking)
  - NFE Selection: Low NFE (1-5) prioritizes speed and works for simple distributions; high NFE ($n$) captures complex distributions but is slower
  - MLM vs. AR: MLM allows parallel sampling of multiple positions (faster); AR guarantees full expressivity but requires sequential sampling

- Failure signatures:
  - Invalid Permutations (Inline at Low NFE): Model produces sequences with repeated elements or missing values. See Figure 6 (middle plot) for inline at NFE < 5
  - Mode Collapse: Low diversity in sampled permutations (low % unique), suggesting distribution not captured
  - Decoding Mismatch: Round-trip test `decode(encode(perm)) != perm` indicates implementation bug
  - Poor Generalization: High train accuracy but failure on held-out cyclic permutations indicates overfitting

- First 3 experiments:
  1. Representation Validation: For 1000 random permutations of length $n=10$, verify `decode(encode(perm)) == perm` for all three representations
  2. Overfitting Test (Jigsaw Mini): Create a tiny jigsaw dataset (100 2x2 puzzles). Train a small transformer with Lehmer representation using MLM (1 NFE). Verify 100% train accuracy within 100 steps
  3. NFE-Expressivity Curve: Implement the cyclic permutation experiment (Section 5.2) for $n=10$ with Fisher-Yates and inline at NFEs $\in \{1, 2, 5, 10\}$. Plot % valid and % cyclic to confirm Fisher-Yates maintains validity across NFEs while inline degrades

## Open Questions the Paper Calls Out
None

## Limitations
- Critical dependence on bijection properties between permutations and alternative representations being maintained during encoding/decoding
- Computational overhead from position-dependent masking not fully analyzed in practice
- Limited empirical validation of claims that the method "subsumes" previous models like Mallows and RIM

## Confidence
- **High Confidence**: Bijection properties and theoretical framework for NFE-expressivity trade-off are mathematically sound
- **Medium Confidence**: Jigsaw puzzle benchmark results are convincing, but synthetic experiments and MovieLens benchmark limit generalizability
- **Low Confidence**: Claims about subsuming previous models require further empirical validation

## Next Checks
1. **Round-trip Validation Study**: Implement systematic testing of `decode(encode(perm)) == perm` for 10,000 random permutations across all three representations and sizes n ∈ {10, 50, 100}. Report failure rates and identify edge cases where bijections may break in practice.

2. **Distribution Recovery Benchmark**: Create a controlled experiment where synthetic distributions over permutations are generated (e.g., Mallows distributions with varying θ). Train the proposed model at different NFE levels and measure KL divergence to ground truth, comparing against baseline RIM and Mallows implementations.

3. **Scalability Analysis**: Measure training/inference time and memory usage for increasing n (permutation length) and compare against inline autoregressive baselines. Include measurements for the position-dependent masking overhead and analyze how the claimed computational advantages manifest in practice.