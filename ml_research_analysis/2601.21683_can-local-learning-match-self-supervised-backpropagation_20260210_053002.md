---
ver: rpa2
title: Can Local Learning Match Self-Supervised Backpropagation?
arxiv_id: '2601.21683'
source_url: https://arxiv.org/abs/2601.21683
tags:
- local-ssl
- learning
- layer
- clapp
- bp-ssl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges local and global self-supervised learning (SSL)
  algorithms for deep neural networks. It proves that under conditions of orthonormal
  weight matrices and unconstrained trainable lateral connections, local SSL algorithms
  (e.g., CLAPP) can implement exactly the same weight updates as global backpropagation-based
  SSL.
---

# Can Local Learning Match Self-Supervised Backpropagation?
## Quick Facts
- arXiv ID: 2601.21683
- Source URL: https://arxiv.org/abs/2601.21683
- Reference count: 40
- Key outcome: CLAPP++ achieves state-of-the-art performance among local SSL methods on CIFAR-10 (80.51%), STL-10 (78.66%), and Tiny ImageNet (36.63%)

## Executive Summary
This paper bridges local and global self-supervised learning (SSL) algorithms for deep neural networks. It proves that under conditions of orthonormal weight matrices and unconstrained trainable lateral connections, local SSL algorithms (e.g., CLAPP) can implement exactly the same weight updates as global backpropagation-based SSL. When orthonormality is violated (e.g., shrinking layer widths), adding direct feedback from the last layer improves local SSL's approximation of global SSL gradients. The theory guides the development of CLAPP++ variants for convolutional networks, incorporating spatial dependence in feedback projections. Empirically, CLAPP++ achieves state-of-the-art performance among local SSL methods on standard benchmarks while maintaining biological plausibility through Hebbian-like updates.

## Method Summary
The paper establishes theoretical conditions under which local SSL algorithms can exactly match global backpropagation updates. When weight matrices are orthonormal and lateral connections are unconstrained, local algorithms like CLAPP implement identical weight updates to global SSL. For non-orthonormal cases, direct feedback connections from the output layer improve approximation quality. These theoretical insights are translated into practical CLAPP++ variants for convolutional networks, incorporating spatial dependence in feedback projections. The method maintains Hebbian-like updates modulated by neuromodulatory signals, preserving biological plausibility while achieving competitive performance.

## Key Results
- CLAPP++ achieves 80.51% accuracy on CIFAR-10, 78.66% on STL-10, and 36.63% on Tiny ImageNet
- Under orthonormal conditions, local SSL algorithms exactly implement global SSL weight updates
- Direct feedback connections improve local SSL performance when orthonormality is violated
- CLAPP++ outperforms other local SSL methods while maintaining biological plausibility

## Why This Works (Mechanism)
The mechanism relies on the mathematical equivalence between local and global updates under specific conditions. When weight matrices are orthonormal, the gradient information required for weight updates can be locally computed using lateral connections between consecutive layers. The key insight is that orthonormality ensures the forward and backward paths through the network preserve the necessary information for gradient computation. When this condition is violated, direct feedback connections from the output layer provide the missing gradient information, allowing local algorithms to approximate global updates more accurately.

## Foundational Learning
- Orthonormality of weight matrices: Why needed - Ensures forward and backward information preservation for local gradient computation. Quick check - Verify weight matrix satisfies W^T W = I
- Lateral connections between consecutive layers: Why needed - Provides local mechanism for computing weight updates without global backpropagation. Quick check - Confirm trainable lateral connections exist between all consecutive layers
- Direct feedback connections: Why needed - Compensates for orthonormality violations by providing output-layer gradient information. Quick check - Verify feedback paths from output to all intermediate layers
- Hebbian learning rules: Why needed - Maintains biological plausibility through local, correlation-based weight updates. Quick check - Confirm weight updates follow Hebbian principles (pre-post correlation)
- Neuromodulatory signals: Why needed - Modulates Hebbian updates to implement global SSL objectives locally. Quick check - Verify presence of global error signals modulating local updates

## Architecture Onboarding
Component map: Input -> Convolutional Layers -> Lateral Connections -> Output Layer -> Direct Feedback Connections -> Convolutional Layers

Critical path: Forward pass through convolutional layers → Lateral connection computation → Weight update using neuromodulatory signals

Design tradeoffs: Orthonormality vs. expressivity (wider layers sacrifice orthonormality), local vs. global computation (lateral connections vs. backpropagation), biological plausibility vs. performance (Hebbian updates vs. exact gradients)

Failure signatures: Performance degradation when layer widths are reduced (orthonormality violation), instability in deep networks without proper initialization, sensitivity to hyperparameters controlling neuromodulatory signal strength

First experiments:
1. Verify orthonormality of initial weight matrices using W^T W ≈ I
2. Test performance with and without direct feedback connections across different layer widths
3. Compare CLAPP++ performance against standard global SSL methods on CIFAR-10

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Theoretical analysis assumes idealized conditions including orthonormal weight matrices and unconstrained trainable lateral connections
- Performance degradation observed when orthonormality is violated through reduced layer widths
- Practical implementation in convolutional networks requires additional assumptions about spatial dependence in feedback projections

## Confidence
High confidence: Mathematical derivation of equivalence under orthonormal conditions; empirical results on standard benchmarks
Medium confidence: Practical implications for biological plausibility; generalization to convolutional architectures
Low confidence: Performance stability across diverse network architectures and datasets; scalability to larger-scale problems

## Next Checks
1. Evaluate CLAPP++ variants on larger-scale datasets (ImageNet-1k) and deeper architectures to assess scalability
2. Conduct systematic ablation studies varying layer widths and orthonormality conditions
3. Implement alternative methods for enforcing or approximating orthonormality in practical networks