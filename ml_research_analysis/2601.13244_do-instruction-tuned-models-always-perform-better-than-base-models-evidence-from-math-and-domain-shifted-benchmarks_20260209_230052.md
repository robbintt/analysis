---
ver: rpa2
title: Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence
  from Math and Domain-Shifted Benchmarks
arxiv_id: '2601.13244'
source_url: https://arxiv.org/abs/2601.13244
tags:
- base
- instruction-tuned
- arxiv
- performance
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the widely held assumption that instruction-tuned
  models always outperform base models. The authors conduct a systematic comparison
  using Pass@20 evaluation across standard math benchmarks (GSM8K, Math-500), perturbed
  variants designed to disrupt solution patterns, and a domain-specific medical benchmark
  (MedCalc).
---

# Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks

## Quick Facts
- **arXiv ID**: 2601.13244
- **Source URL**: https://arxiv.org/abs/2601.13244
- **Authors**: Prateek Munjal; Clement Christophe; Ronnie Rajan; Praveenkumar Kanithi
- **Reference count**: 28
- **Primary result**: Base models consistently outperform instruction-tuned models on GSM8K under zero-shot CoT evaluation, with drops up to 32.67% for Llama3-70B.

## Executive Summary
This paper challenges the widely held assumption that instruction-tuned models always outperform base models. The authors conduct a systematic comparison using Pass@20 evaluation across standard math benchmarks (GSM8K, Math-500), perturbed variants designed to disrupt solution patterns, and a domain-specific medical benchmark (MedCalc). Their results show that under zero-shot chain-of-thought evaluation, base models consistently outperform instruction-tuned variants on GSM8K, with drops as high as 32.67% for Llama3-70B. Base models also surpass instruction-tuned models on MedCalc in zero-shot settings. The performance advantage of instruction tuning is found to be highly evaluator-dependent, particularly for LaTeX-heavy outputs, and primarily benefits small language models rather than large ones. These findings suggest that instruction tuning does not universally improve reasoning capabilities and highlight the need for more robust evaluation benchmarks.

## Method Summary
The study compares base and instruction-tuned models across four benchmarks using Pass@20 evaluation. Base models employ CoT decoding with K=20 samples, selecting top-K candidates at the first token then greedy decoding. Instruction-tuned models use repeated stochastic sampling (temperature=0.05, max_tokens=8192). An auxiliary LLM (Qwen3-0.6B) extracts answers from base model outputs using a provided JSON prompt. Both standard grader (regex-based) and MathVerify evaluators assess outputs. The evaluation uses five model families (Qwen, LLaMA, SmolLM, DeepSeek, Kimi) across three scales, deployed via vLLM on 8x NVIDIA H200 GPUs.

## Key Results
- Base models consistently outperform instruction-tuned models on GSM8K under zero-shot CoT, with Qwen2.5-72B showing 30.92% advantage and Llama3-70B showing 32.67% advantage
- Base models also surpass instruction-tuned models on MedCalc in zero-shot settings
- Instruction-tuned model performance is highly evaluator-dependent, with up to 10.8% scoring differences for LaTeX-heavy outputs
- Instruction tuning primarily benefits smaller models rather than large ones

## Why This Works (Mechanism)
The performance differences arise from the mismatch between instruction tuning objectives and reasoning task requirements. Instruction tuning optimizes for following explicit instructions and producing structured responses, but this may interfere with the model's ability to perform free-form reasoning. Base models, trained primarily on next-token prediction without instruction-specific fine-tuning, maintain more flexible reasoning capabilities. The CoT decoding approach allows base models to explore multiple reasoning paths systematically, while instruction-tuned models may be constrained by their training to produce more formulaic responses. Additionally, the evaluator sensitivity suggests that instruction-tuned models may produce mathematically equivalent but syntactically different LaTeX expressions that standard graders fail to recognize.

## Foundational Learning
- **Chain-of-Thought prompting**: Why needed - enables complex reasoning by generating intermediate steps. Quick check - verify model produces coherent multi-step reasoning traces.
- **Pass@k evaluation**: Why needed - measures retrieval quality from multiple generated candidates. Quick check - ensure K samples are diverse and independently generated.
- **Instruction tuning**: Why needed - adapts models to follow explicit instructions and produce structured outputs. Quick check - verify instruction-tuned models respond appropriately to zero-shot prompts.
- **Zero-shot vs few-shot learning**: Why needed - distinguishes between task-specific priming and general capability. Quick check - compare performance with and without exemplars.
- **LaTeX expression evaluation**: Why needed - math problems often require precise mathematical notation. Quick check - validate grader handles all LaTeX variants (\dfrac, \frac, \text{}, etc.).
- **Domain shift evaluation**: Why needed - tests generalization beyond training distribution. Quick check - verify domain-specific benchmarks have appropriate difficulty and coverage.

## Architecture Onboarding

**Component map**: Input -> Base/Instruction-tuned Model -> Sampling Strategy -> Answer Extraction -> Evaluator -> Pass@20 Score

**Critical path**: Question input → Sampling (CoT for base, stochastic for instruction-tuned) → Answer generation → Extraction (auxiliary LLM for base, json_repair for instruction-tuned) → Evaluation (grader/MathVerify) → Pass@20 computation

**Design tradeoffs**: The study uses different sampling strategies for base versus instruction-tuned models - CoT decoding for base models versus repeated stochastic sampling for instruction-tuned models. This design choice aims to maximize each model type's strengths but makes it difficult to isolate the effect of instruction tuning from sampling methodology. The use of auxiliary LLM for answer extraction from base models versus direct JSON output from instruction-tuned models represents another tradeoff between extraction complexity and output structure.

**Failure signatures**: 
- Evaluator discrepancies up to 10.8% due to LaTeX variations
- Base model answer extraction failures when outputs lack clear structure
- Performance differences between zero-shot and few-shot settings
- Model size-dependent instruction tuning benefits

**First experiments**:
1. Run both standard grader and MathVerify on identical outputs to quantify scoring variance
2. Manually audit 50 evaluator-mismatched cases to assess true correctness
3. Test answer extraction pipeline on held-out examples before full evaluation

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The evaluation is confined to arithmetic word problems and medical calculations, lacking assessment of logical reasoning, commonsense inference, or other reasoning domains
- The specific CoT-only evaluation protocol prevents comparison with standard prompting approaches
- The use of different sampling strategies for base versus instruction-tuned models confounds the effect of instruction tuning
- The absence of ablation studies isolating instruction tuning from other post-training factors

## Confidence
- **High confidence**: GSM8K benchmark results under zero-shot CoT conditions
- **Medium confidence**: Extension to "reasoning" more broadly due to narrow task scope
- **Medium confidence**: Evaluator-dependent and size-dependent findings due to limited benchmark diversity

## Next Checks
1. **Evaluator robustness test**: Run both standard grader and MathVerify on all outputs to quantify scoring variance; manually audit 50 mismatched cases to assess true correctness
2. **Task diversity expansion**: Evaluate on non-mathematical reasoning benchmarks (e.g., strategyQA, CSQA) using identical zero-shot CoT protocol to test generalizability of base model advantage
3. **Sampling strategy ablation**: Compare base model performance using repeated stochastic sampling (temp=0.05) versus CoT decoding to isolate whether performance differences stem from instruction tuning or sampling methodology