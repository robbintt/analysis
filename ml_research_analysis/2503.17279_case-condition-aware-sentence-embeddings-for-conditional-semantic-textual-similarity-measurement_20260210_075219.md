---
ver: rpa2
title: CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual
  Similarity Measurement
arxiv_id: '2503.17279'
source_url: https://arxiv.org/abs/2503.17279
tags:
- sentence
- embeddings
- condition
- similarity
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for computing semantic textual similarity
  between sentences under a given condition (C-STS). The core idea is to use LLM-based
  encoders with a prompt that encodes the condition given the sentence, followed by
  a supervised projection step using a lightweight FFN to align the embeddings with
  the C-STS task.
---

# CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement

## Quick Facts
- arXiv ID: 2503.17279
- Source URL: https://arxiv.org/abs/2503.17279
- Authors: Gaifan Zhang; Yi Zhou; Danushka Bollegala
- Reference count: 19
- Primary result: LLM-based embeddings with condition subtraction achieve 74.94% Spearman correlation on C-STS dataset

## Executive Summary
The paper introduces CASE, a method for measuring semantic textual similarity under conditions (C-STS). Unlike traditional STS that assigns fixed similarity scores, C-STS requires context-dependent similarity measurements where the meaning of "similar" changes based on a given condition. CASE uses LLM-based encoders with a specific prompt structure to encode conditions conditioned on sentences, followed by supervised dimensionality reduction. The method demonstrates that LLM embeddings significantly outperform MLM embeddings for C-STS tasks, and that subtracting unconditional condition embeddings improves both isotropy and performance.

## Method Summary
CASE computes semantic textual similarity between sentences under conditions by encoding the condition given each sentence using a frozen LLM with a specific prompt format. The method calculates two embeddings per sentence: f(c;I(s)) for the condition encoded with the sentence, and f(c;I(∅)) for the condition encoded alone. The final embedding is the difference f(c;I(s)) − f(c;I(∅)), which is then projected to a lower-dimensional space using a lightweight supervised feed-forward network trained on human similarity ratings. The cosine similarity between projected embeddings of two sentences under the same condition serves as the C-STS score.

## Key Results
- LLM embeddings (NV-Embed-v2) consistently outperform MLM embeddings (E5-v3) on C-STS tasks
- Subtracting unconditional condition embeddings improves isotropy (Iiso increases from ~0.947 to ~0.961) and C-STS performance
- Non-linear supervised FFN with 8× dimensionality reduction (4096→512) achieves 74.94% Spearman correlation, outperforming linear projections
- The proposed method achieves state-of-the-art results on the C-STS dataset while using fewer parameters than previous approaches

## Why This Works (Mechanism)

### Mechanism 1
Encoding the condition while the sentence influences attention produces embeddings that better capture conditional semantic similarity. The prompt "retrieve [condition], given [sentence]" causes the sentence tokens to modify attention scores for condition tokens during pooling, making the condition embedding reflect how the sentence contextualizes the condition's semantics.

### Mechanism 2
Subtracting the unconditional condition embedding f(c;I(∅)) improves isotropy and removes condition-intrinsic semantics irrelevant to the comparison. This subtraction leaves only the sentence-conditioned variation, increasing isotropy (Iiso values rise toward 1) and making subtle differences more distinguishable.

### Mechanism 3
A lightweight supervised FFN can project high-dimensional LLM embeddings to a lower-dimensional space while preserving or improving C-STS alignment. The non-linear FFN with LeakyReLU activation and dropout outperforms linear projection, achieving better performance with 8× compression.

## Foundational Learning

- **Concept**: Semantic Textual Similarity (STS) vs. Conditional STS (C-STS)
  - Why needed: Traditional STS assigns single similarity scores; C-STS requires different scores under different conditions, changing what "similar" means
  - Quick check: Given "The dress is orange" and "The dress is gray," what is their similarity under condition "color"? Under condition "clothing type"?

- **Concept**: Isotropy in embedding spaces
  - Why needed: The paper claims subtraction improves isotropy, which correlates with better similarity discrimination
  - Quick check: If all embeddings cluster near one direction (anisotropy), what happens to cosine similarity differences between slightly different sentence pairs?

- **Concept**: Attention-based pooling in LLM encoders
  - Why needed: CASE relies on sentence tokens influencing attention during condition encoding
  - Quick check: How does attention-weighted pooling differ from mean pooling, and what might happen if irrelevant tokens dominate attention?

## Architecture Onboarding

- **Component map**: Prompt construction → LLM encoding (twice per sentence) → Subtraction → FFN projection → Cosine similarity
- **Critical path**: I(s) + c → LLM → f(c;I(s)), f(c;I(∅)) → Subtraction → FFN → CASE(s,c) → Cosine similarity
- **Design tradeoffs**:
  - Prompt ordering: "retrieve [condition], given [sentence]" outperforms reverse despite counter-intuitiveness
  - Linear vs. non-linear FFN: Non-linear converges faster and achieves higher Spearman but adds hyperparameters
  - Dimensionality: 512-dim preserves ~90% of fine-tuned Qwen3-8B performance; lower dimensions degrade gracefully
- **Failure signatures**:
  - Spearman near random (<10%): Check prompt formatting, ensure condition is encoded (not sentence), verify subtraction applied
  - Training loss doesn't decrease: Learning rate too high/low, or embeddings lack conditional signal
  - Large gap between linear and non-linear FFN: Non-linear may be overfitting; reduce dropout or switch to linear
- **First 3 experiments**:
  1. Baseline replication: Use NV-Embed-v2 with "cond − c" setting, no FFN; compute Spearman on test set. Target: ~37.6%
  2. FFN ablation: Train linear and non-linear FFN at 512-dim; compare convergence speed and final Spearman. Target: non-linear >74% within 20 epochs
  3. Isotropy verification: Compute Iiso before and after subtraction for chosen encoder; confirm increase toward 0.95+

## Open Questions the Paper Calls Out

- **Multilingual evaluation**: How effective is CASE in multilingual settings where cross-lingual conditional semantics must be measured? The authors identify this as an important future research direction due to lack of non-English C-STS datasets.

- **Bias analysis**: To what extent does the supervised projection and condition subtraction mechanism mitigate or amplify social biases present in base LLM embeddings? The authors note they have not evaluated this impact.

- **Isotropy causality**: Is the performance improvement from condition subtraction primarily caused by improved isotropy in the embedding space? The paper establishes correlation but not causation between these metrics.

## Limitations

- The prompt direction "retrieve [condition], given [sentence]" is counter-intuitive and the mechanism for why it outperforms the natural ordering is not fully explained
- The isotropy improvement is statistically small (0.947 to 0.961) and the causal link to performance gains is not rigorously established
- The method has not been evaluated for multilingual settings or bias amplification despite using potentially biased pre-trained encoders

## Confidence

- **High**: LLM embeddings consistently outperform MLM embeddings; supervised FFN projection improves performance even at 8× compression
- **Medium**: Condition subtraction improves isotropy and C-STS performance; non-linear FFN outperforms linear projection
- **Low**: The specific prompt ordering is optimal; isotropy improvements directly cause performance gains

## Next Checks

1. **Attention weight analysis**: Extract and visualize attention distributions from the prompt "retrieve [condition], given [sentence]" to verify that sentence tokens meaningfully modulate condition token attention during pooling

2. **Isotropy ablation study**: Systematically vary isotropy (via different subtraction strategies or dimensionality) and measure corresponding Spearman correlation changes to establish causality

3. **Cross-dataset generalization**: Evaluate CASE on STS datasets without conditions (e.g., STS-B) to verify that the subtraction mechanism doesn't degrade general STS performance