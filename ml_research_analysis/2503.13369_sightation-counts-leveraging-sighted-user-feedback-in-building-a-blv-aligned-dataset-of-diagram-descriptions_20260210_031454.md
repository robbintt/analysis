---
ver: rpa2
title: 'Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned
  Dataset of Diagram Descriptions'
arxiv_id: '2503.13369'
source_url: https://arxiv.org/abs/2503.13369
tags:
- diagram
- text
- sighted
- dataset
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SIGHTATION, a large-scale dataset for generating
  diagram descriptions that are aligned with the needs of blind and low-vision (BLV)
  users. The authors address the challenge of creating accessible diagram descriptions
  by using a two-pass guided generation approach with vision-language models (VLMs),
  where the first pass generates question-answer pairs that guide the second pass
  to produce more useful descriptions.
---

# Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions

## Quick Facts
- arXiv ID: 2503.13369
- Source URL: https://arxiv.org/abs/2503.13369
- Authors: Wan Ju Kang; Eunki Kim; Na Min An; Sangryul Kim; Haemin Choi; Ki Hoon Kwak; James Thorne
- Reference count: 40
- The paper introduces SIGHTATION, a large-scale dataset for generating diagram descriptions that are aligned with the needs of blind and low-vision (BLV) users.

## Executive Summary
SIGHTATION addresses the challenge of creating accessible diagram descriptions by using a two-pass guided generation approach with vision-language models (VLMs). The method employs sighted users to assess rather than generate descriptions, distributing tasks across multiple aspects like factuality, informativeness, succinctness, and diversity. BLV educators validate the descriptions for their usefulness in educational contexts. The resulting dataset includes 5,000 diagrams and 137,000 samples, covering completion, preference alignment, retrieval, question answering, and reasoning tasks.

## Method Summary
The paper introduces a two-pass guided generation approach where a VLM first generates question-answer pairs that identify salient diagram elements, then conditions the second-pass description generation on these QA pairs. Sighted users assess the generated descriptions across 9 aspects (factuality, informativeness, succinctness, diversity, etc.) through preference choice, quality ratings, and best-sentence selection. These assessments are distributed across 30 annotators to reduce bias. BLV educators then validate the descriptions for educational usefulness. The dataset is used for fine-tuning and preference optimization experiments.

## Key Results
- A 2B model tuned on SIGHTATION outperforms a 3B model fine-tuned on chart comprehension in 8 out of 11 automatic metrics
- Retrieval tuning on SIGHTATION improves precision by 65% compared to a COCO-tuned baseline
- DPO-tuned models show 0.76σ improvement in usefulness for open-ended questions and 1.0σ+ in interpretiveness and diversity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Two-pass guided generation with QA-based latent supervision improves BLV-aligned description quality.
- **Mechanism:** First-pass VLM generates question-answer pairs that identify salient diagram elements. These QA pairs condition the second-pass description generation, directing attention toward information BLV users need rather than indiscriminate visual narration.
- **Core assumption:** QA pairs effectively encode which diagram elements are pedagogically or informationally critical.
- **Evidence anchors:**
  - [Section 3.2]: "We hypothesized that introducing auxiliary data such as plausible question-answer pairs, would have a good effect as they assist the description generator with understanding which parts are critical."
  - [Section A.1]: Generated QA pairs rated 92.66% "Excellent" vs. 73.86% for AI2D-original pairs.
  - [Corpus]: Related work (VideoA11y) confirms training data quality limitations propagate to description quality.
- **Break condition:** If QA pairs are low-quality, redundant, or miss critical diagram elements, guided generation provides no benefit or degrades output.

### Mechanism 2
- **Claim:** Sighted users are more effective as assessors than as generators of BLV-aligned descriptions.
- **Mechanism:** Assessment tasks (preference choice, quality rating) require visual grounding and comprehension but not BLV-specific articulation skills. Distributing assessment across 30 annotators on finer-grained aspects reduces individual annotator bias while keeping cognitive load manageable.
- **Core assumption:** Sighted users can reliably evaluate factuality and informativeness based on visual access, even if they cannot produce BLV-optimal descriptions.
- **Evidence anchors:**
  - [Abstract]: "Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards."
  - [Section 3.3]: Assessment tasks partitioned across 30 annotators; Cronbach's α ≥ 0.70 indicates acceptable reliability.
  - [Corpus]: Related work (Lundgard & Satyanarayan 2022, cited in paper) confirms sighted-BLV preference misalignment.
- **Break condition:** If assessment rubrics don't correlate with BLV preferences, sighted assessments become noise.

### Mechanism 3
- **Claim:** Preference alignment via DPO on BLV-validated pairs transfers BLV preferences into smaller models.
- **Mechanism:** SIGHTATION-PREFERENCE provides 16k chosen-rejected pairs. DPO optimization directly shapes model behavior without requiring a separate reward model, which the authors note may not accurately represent BLV preferences.
- **Core assumption:** Preference pairs validated by sighted assessors and BLV educators are sufficiently representative of broader BLV needs.
- **Evidence anchors:**
  - [Section 5.1]: 2B DPO-tuned model shows 0.76σ improvement in usefulness for open-ended questions; 1.0σ+ in interpretiveness and diversity.
  - [Table 13]: Fine-tuned 7B model preferred over 72B baseline by BLV educators across usefulness dimensions.
  - [Corpus]: Weak direct evidence in corpus; related accessibility papers focus on description generation rather than preference tuning.
- **Break condition:** If preference pairs capture annotator idiosyncrasies rather than generalizable BLV preferences, tuning overfits to the specific annotator pool.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** SIGHTATION uses DPO for preference alignment because reward models trained on generic data may misrepresent BLV preferences. DPO bypasses reward modeling entirely.
  - **Quick check question:** Can you explain why DPO is preferred over RLHF when the reward model's training distribution mismatches the target user group?

- **Concept: Annotator Bias in Crowdsourced Datasets**
  - **Why needed here:** The paper explicitly addresses annotator bias from small annotator pools. Distributing assessment across 30 sighted users mitigates this compared to few-expert generation.
  - **Quick check question:** Why does the paper argue assessment tasks are less bias-prone than generation tasks for sighted annotators?

- **Concept: VLM-as-a-Judge Limitations**
  - **Why needed here:** The paper shows VLM judges (QVQ-72B-Preview) align better with sighted educators than BLV educators, indicating VLM evaluation cannot substitute for BLV validation.
  - **Quick check question:** What evidence in the paper suggests VLM judges may not capture BLV-specific preferences?

## Architecture Onboarding

- **Component map:** QA Generator (VLM) → Description Generator (VLM, two-pass) → Sighted Assessment (preference, ratings, best-sentence) → BLV Educator Validation → Dataset Construction (Completions, Preference, Retrieval, VQA, Reasoning) → Fine-tuning (SFT, DPO, Contrastive)

- **Critical path:**
  1. QA pair quality determines guided generation effectiveness (verify with VLM-as-judge rating ≥4.0/5)
  2. Sighted assessment consistency (Cronbach's α ≥0.7) determines dataset reliability
  3. BLV educator validation on usefulness dimensions (Useful-Sum, Useful-MCQ, Useful-OEQ) determines final alignment quality

- **Design tradeoffs:**
  - **Assessment granularity vs. annotator fatigue:** 9 aspects across 3 groups increases signal but requires careful task distribution
  - **Model size vs. tuning cost:** 2B model fully fine-tuned outperforms 3B baseline; 7B requires PEFT. Smaller models may benefit more from dataset alignment
  - **Sighted vs. BLV validation:** BLV educators provide ground truth but are scarce; sighted assessors scale but require proxy validation

- **Failure signatures:**
  - VLM judge agrees with sighted assessors but disagrees with BLV educators (observed in Table 13)
  - DPO-tuned model shows no improvement or degradation on succinctness (observed in 2B model, Table 3)
  - Retrieval model trained on SIGHTATION fails to generalize to natural images (check cross-validation on COCO)

- **First 3 experiments:**
  1. **QA quality ablation:** Replace generated QA pairs with AI2D original questions; compare description ratings by BLV educators on usefulness dimensions
  2. **Assessor pool size sweep:** Vary sighted annotator pool from 5 to 30; measure rating consistency (Cronbach's α) and correlation with BLV validation
  3. **Preference pair composition:** Compare in-model, cross-model, and synthetic contender pairs for DPO; evaluate which subset drives BLV usefulness improvements

## Open Questions the Paper Calls Out
None

## Limitations
- The paper relies on sighted annotators for assessment but acknowledges their potential bias relative to BLV preferences, with only BLV educator validation providing final quality assurance
- QA pair quality is critical to guided generation effectiveness, but the paper doesn't establish how representative the 92.66% "Excellent" rating is for the full dataset
- The preference alignment experiments show strong BLV educator preferences for fine-tuned models, but the sample size (3 BLV educators) is small and may not capture population-level preferences

## Confidence
- High confidence in the two-pass guided generation mechanism, supported by strong QA quality metrics and ablation showing 92.66% excellence rating
- Medium confidence in sighted assessment effectiveness, as Cronbach's α ≥0.7 suggests reliability but doesn't prove alignment with BLV preferences
- Medium confidence in preference tuning outcomes, given BLV educator validation but limited sample size
- Low confidence in VLM judge reliability for BLV preferences, as Table 13 shows VLM alignment with sighted rather than BLV educators

## Next Checks
1. **QA quality ablation study:** Replace generated QA pairs with AI2D original questions and measure impact on BLV educator ratings of description usefulness across all dimensions
2. **Assessor pool sensitivity:** Conduct experiments varying sighted annotator pool sizes (5, 10, 15, 20, 30) and measure how rating consistency (Cronbach's α) and BLV educator correlation change
3. **Preference pair diversity analysis:** Compare BLV usefulness improvements across in-model, cross-model, and synthetic contender pairs in the DPO training to determine which subset drives gains