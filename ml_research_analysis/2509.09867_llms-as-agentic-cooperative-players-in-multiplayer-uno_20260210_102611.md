---
ver: rpa2
title: LLMs as Agentic Cooperative Players in Multiplayer UNO
arxiv_id: '2509.09867'
source_url: https://arxiv.org/abs/2509.09867
tags:
- player
- prompting
- llms
- agent
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLMs as Agentic Cooperative Players in Multiplayer UNO

## Quick Facts
- **arXiv ID:** 2509.09867
- **Source URL:** https://arxiv.org/abs/2509.09867
- **Reference count:** 20
- **Primary result:** Zero-shot LLMs achieve modest win rates in UNO; cooperative assistance is suggestive but not statistically significant.

## Executive Summary
This paper evaluates whether decoder-only LLMs can function as autonomous and cooperative agents in the card game UNO. Using RLCard as the environment, the authors test multiple model sizes and prompting strategies in two modes: autonomous 1v1 play against a random agent, and three-player cooperative play where the LLM assists a rule-based teammate. The main contribution is a zero-shot prompting pipeline that converts game state into natural language and uses autoregressive generation for action selection. While the autonomous mode achieves a statistically significant win rate for the largest model, cooperative performance remains inconclusive, suggesting that the implicit modeling required for effective assistance may exceed the capabilities of the tested models.

## Method Summary
The method uses RLCard's UNO environment, converting game state dictionaries (hands, legal actions, history) into natural-language prompts. LLMs receive these prompts and output action tokens via autoregressive decoding. Two prompting strategies are compared: "Cloze," which rotates action labels to mitigate Base Rate Probability bias, and "Counterfactual," which queries for good/bad action pairs. The pipeline runs zero-shot inference, maps output tokens to legal actions, and logs outcomes. Evaluation is via win-rate percentages compared to baselines, with statistical significance assessed by one-sided one-proportion z-tests.

## Key Results
- LLaMA3.3-70B with cloze prompting achieved statistically significant autonomous win rates (p < 0.05) against a random agent.
- Cooperative play showed no statistically significant improvement in win rates.
- Cloze prompting outperformed counterfactual prompting in both autonomous and cooperative modes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting game state to natural language enables pretrained LLMs to perform structured decision-making without task-specific fine-tuning.
- Mechanism: RLCard observation dictionaries (hand contents, legal actions, history) are transformed into concise natural-language prompts; the LLM assigns probabilities to action tokens via autoregressive decoding; greedy selection picks the highest-probability legal move.
- Core assumption: The model's pretrained knowledge includes sufficient understanding of card-game logic and sequential reasoning to map text descriptions to sensible actions.
- Evidence anchors:
  - [abstract] "These models receive full game-state information and respond using simple text prompts under two distinct prompting strategies."
  - [section IV.A] "The game state information was extracted from RLCard and reformatted for readability... we expanded these into full descriptions to improve the model's comprehension."
  - [corpus] Weak direct evidence; neighbor papers address LLMs in poker (arXiv:2602.00528) and Diplomacy (arXiv:2506.09655), suggesting emergent strategic reasoning, but no direct comparison to UNO-style cooperation.
- Break condition: If game state encoding exceeds context window or becomes ambiguous, token probabilities will not reflect strategic quality.

### Mechanism 2
- Claim: Cloze prompting with rotation mitigates Base Rate Probability (BRP) bias, improving action selection over naive single-pass prompting.
- Mechanism: Legal actions are rotated through label positions (A, B, C...) across multiple prompt permutations; cumulative probabilities are aggregated per action; the highest-scoring action is selected, reducing positional token bias.
- Core assumption: BRP bias is consistent across rotations and can be averaged out; the model's semantic preference survives aggregation.
- Evidence anchors:
  - [section IV.A] "Tokens like 'A' or 'yes' tend to have higher BRP than others... we implemented a rotation mechanism: the prompt was repeated as many times as there were legal actions... Summing the probabilities across all rotations allows for a more balanced comparison."
  - [Figure 3] Shows per-shift probability distributions and cumulative scoring for action selection.
  - [corpus] No direct corpus evidence on BRP mitigation in game settings; mechanism is paper-internal.
- Break condition: If the model assigns semantically incoherent probabilities across rotations, aggregation may amplify noise rather than reduce bias.

### Mechanism 3
- Claim: Cooperative assistance requires implicit modeling of teammate state and future game dynamics, which zero-shot LLMs do not reliably perform.
- Mechanism: The LLM is prompted to "help another player win" but receives no explicit representation of teammate strategy or long-horizon payoff; assistance relies on the model inferring helpful moves from game state alone.
- Core assumption: Helpful moves can be identified via local heuristics (e.g., playing action cards against opponents) without explicit teammate modeling.
- Evidence anchors:
  - [section VI] "Three configurations approached or achieved statistical significance... only LLaMA3.3-70B with cloze prompting was the only configuration with p < 0.05."
  - [section VIII] "UNO's cooperative mechanics are inherently constrained: support arises mainly through action cards... strategic impact of each card depends heavily on seating order and hand composition."
  - [corpus] Neighbor paper on poker LLMs (arXiv:2602.00528) notes strategic reasoning challenges under uncertainty; no direct cooperative-play comparison.
- Break condition: If the teammate's objective or the game's turn-order dynamics change, the LLM's implicit assistance heuristics may become counterproductive.

## Foundational Learning

- Concept: **Decoder-only autoregressive generation**
  - Why needed here: The entire action-selection pipeline depends on next-token prediction from context; understanding softmax over logits and greedy decoding is essential for debugging probability outputs.
  - Quick check question: Given logits [2.0, 1.0, 0.5] for actions A, B, C, which action does greedy decoding select?

- Concept: **Base Rate Probability (BRP) bias**
  - Why needed here: Explains why naive cloze prompting overestimates early-listed actions; rotation mechanism is the mitigation strategy.
  - Quick check question: If action A always appears first in the prompt, why might it receive inflated probability even if semantically suboptimal?

- Concept: **First-player advantage in sequential games**
  - Why needed here: Baseline win rates are order-dependent; fair evaluation requires position-controlled comparisons.
  - Quick check question: In a two-player game where Player 1 wins 51% of matches, how do you adjust for seat order when testing a new agent?

## Architecture Onboarding

- Component map:
  RLCard Environment -> Agent Class -> Prompt Builder -> Prompting Strategy Module -> LLM Inference Backend -> Action Mapping -> RLCard Environment

- Critical path:
  1. RLCard emits observation dict -> 2. Agent converts to prompt -> 3. Prompting strategy generates one or more prompt variants -> 4. LLM returns logits -> 5. Softmax + aggregation -> 6. Action index returned to RLCard -> 7. Game advances, repeat.

- Design tradeoffs:
  - Cloze vs. counterfactual: Cloze is more token-efficient and yielded higher win rates; counterfactual provides richer semantic evaluation but scales inference cost linearly with action count.
  - Rotation count: More rotations reduce BRP bias but multiply inference calls; the paper uses N rotations for N legal actions.
  - Prompt verbosity: Extended prompts (few-shot, free-form) underperformed concise rule-style prompts, trading context for clarity.

- Failure signatures:
  - Model always selects first-listed action: BRP bias not mitigated; check rotation implementation.
  - Win rate equals baseline in cooperative mode: LLM may not be modeling teammate objective; inspect prompt framing and teammate position.
  - Inconsistent action mapping: Token-to-action index mapping may be misaligned; verify legal action ordering.

- First 3 experiments:
  1. **Baseline replication**: Run 1,000 games of LLM vs. random agent in second-player seat; confirm win rate exceeds 48.96% baseline with p < 0.05.
  2. **Rotation ablation**: Compare single-prompt cloze vs. full rotation cloze on a small model (e.g., LLaMA3.2-1B); quantify BRP bias reduction.
  3. **Cooperative seat swap**: Place LLM in seat 1 (assisted position) vs. seat 2 (assistant); verify assistance effect is position-dependent, not artifact of seat ordering.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs deliver significant strategic support in more complex cooperative environments than UNO?
- **Basis in paper:** [explicit] The authors list "Alternative cooperative environments" as a primary goal for future work to test collaboration under stricter rules.
- **Why unresolved:** The current study is limited to UNO, where cooperative mechanics are constrained to specific action cards (Skip, Reverse) and seating order.
- **What evidence would resolve it:** Replicating the experimental setup in games with deeper strategic interactions (e.g., Hanabi or Bridge) showing statistically significant teammate assistance.

### Open Question 2
- **Question:** How does LLM cooperative performance vary when paired with adaptive agents or humans instead of rule-based agents?
- **Basis in paper:** [explicit] Section VIII (Limitations) and Section IX note that the study relied exclusively on rule-based teammates, which may interact differently with LLMs than learning-based or human partners.
- **Why unresolved:** Rule-based agents are deterministic; it is unknown if LLMs can adapt their assistance strategies to dynamic, non-deterministic partners.
- **What evidence would resolve it:** Comparative experiments pairing the LLM agent with DQN agents or human players and measuring win-rate improvements.

### Open Question 3
- **Question:** Can proprietary frontier models (e.g., GPT-4, Claude) achieve consistent cooperative gains where open-source models struggled?
- **Basis in paper:** [explicit] Section IX explicitly calls for an "Expanded model list," noting that proprietary models were excluded due to resource constraints.
- **Why unresolved:** Only models up to 70B parameters were tested; the authors cannot determine if the observed difficulty in cooperation is a fundamental limitation of the method or a limitation of model scale/capability.
- **What evidence would resolve it:** Benchmarking the same cooperative UNO setup using high-parameter proprietary APIs to see if they yield significant assistance (p < 0.05) more consistently.

## Limitations

- Cooperative assistance did not reach statistical significance; the LLM's implicit modeling of teammate state may be insufficient.
- BRP mitigation is heuristic; the rotation mechanism's effectiveness is inferred, not directly ablated.
- The study is limited to UNO; cooperative mechanics in more complex games remain untested.

## Confidence

- **Core finding (autonomous win rates):** Medium — the experimental design is clear and reproducible, but effect sizes are small and only one configuration reaches significance.
- **BRP mitigation mechanism:** Low — the rotation mechanism is reasonable but lacks direct ablation evidence.
- **Cooperative assistance claim:** Low — the LLM is asked to help but receives no explicit teammate representation; any assistance is inferred from action card targeting.

## Next Checks

1. **BRP Ablation:** Run a direct comparison of single-prompt vs. rotated cloze prompting on a small model to quantify the magnitude of bias reduction and verify the rotation mechanism's necessity.

2. **Cooperative Mode Ablation:** Replace the LLM assistant with a rule-based helper that explicitly models the teammate's hand; compare win rates to the LLM condition to isolate whether any benefit is due to implicit reasoning or mere action card targeting.

3. **Baseline Replication:** Replicate the 1v1 autonomous baseline with the LLM in seat 2 (second player) and confirm that the win rate exceeds the 48.96% threshold with statistical significance, as this is the only claim that meets the paper's own p < 0.05 criterion.