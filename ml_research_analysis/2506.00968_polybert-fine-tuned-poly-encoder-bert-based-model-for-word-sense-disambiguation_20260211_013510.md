---
ver: rpa2
title: 'PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation'
arxiv_id: '2506.00968'
source_url: https://arxiv.org/abs/2506.00968
tags:
- polybert
- word
- sense
- target
- semantics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations in existing Word Sense Disambiguation
  (WSD) methods that fail to balance token-level (local) and sequence-level (global)
  semantics, leading to insufficient semantic representation, and incur unnecessary
  computational costs due to redundant training inputs. To overcome these limitations,
  the authors propose PolyBERT, a poly-encoder BERT-based model with batch contrastive
  learning for WSD.
---

# PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation

## Quick Facts
- arXiv ID: 2506.00968
- Source URL: https://arxiv.org/abs/2506.00968
- Reference count: 19
- F1-score improvement of 2% over baseline WSD methods

## Executive Summary
This paper addresses limitations in existing Word Sense Disambiguation (WSD) methods that fail to balance token-level and sequence-level semantics while incurring unnecessary computational costs. PolyBERT introduces a poly-encoder BERT-based model with batch contrastive learning that fuses local and global semantic information through multi-head attention, while using correct senses of other target words in the same batch as negative samples to reduce computational overhead. Experimental results show PolyBERT outperforms baseline WSD methods by 2% in F1-score and reduces GPU hours by 37.6% compared to PolyBERT without batch contrastive learning.

## Method Summary
PolyBERT employs a dual-encoder architecture with BERT-Large for both context and gloss encoding. The context-encoder uses a poly-encoder with multi-head attention (8 heads) to fuse the target token embedding with the full context sequence, creating a rich semantic representation. The gloss-encoder extracts the [CLS] token as the gloss representation. During training, batch contrastive learning computes a similarity matrix across all target words and glosses in the batch, using diagonal elements (correct pairs) as positives and off-diagonal elements (other target words' correct glosses) as negatives. This approach reduces computational cost by eliminating the need to explicitly feed all possible incorrect glosses.

## Key Results
- Outperforms baseline WSD methods (Huang's GlossBERT and Blevins's BEM) by 2% in F1-score
- Reduces GPU hours by 37.6% compared to PolyBERT without batch contrastive learning
- Trained on SemCor 3.0 corpus and evaluated on standard WSD benchmarks (SE02, SE03, SE13, SE15)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing local (token-level) and global (sequence-level) semantics enriches the representation of ambiguous words better than using either in isolation.
- Mechanism: The model uses a poly-encoder architecture where the target token embedding is replicated to form a Query matrix that attends to the full context embedding sequence via multi-head attention, integrating specific word identity with broader contextual cues.
- Core assumption: The target word's static position in the BERT output carries sufficient local signal, which can be refined by "looking around" the sequence via attention without diluting the specific token identity.
- Evidence anchors: [abstract] "...poly-encoder with a multi-head attention mechanism is utilized to fuse token-level (local) and sequence-level (global) semantics..."
- Break condition: If the context window is too short or the target token embedding is noisy (e.g., out-of-vocabulary or subword fragmentation issues), the attention mechanism may attend to irrelevant noise rather than disambiguating cues.

### Mechanism 2
- Claim: Using correct senses of other target words in the same batch as negative samples reduces computational cost without degrading performance.
- Mechanism: During training, the model computes a similarity matrix between all target words and all glosses in the batch, where diagonal elements represent positive pairs and off-diagonal elements serve as negative pairs, eliminating the need to explicitly feed all possible incorrect glosses.
- Core assumption: Glosses within a random batch are sufficiently diverse to act as effective "hard" or "semi-hard" negative examples for disambiguation learning.
- Evidence anchors: [abstract] "...BCL utilizes the correct senses of other target words in the same batch as negative samples... reducing training inputs."
- Break condition: If the batch size is too small or contains semantically clustered words, the negative samples may be too confusing or insufficiently diverse, hurting convergence.

### Mechanism 3
- Claim: Asymmetric representation strategies are effective; global summarization suffices for glosses while attended fusion is necessary for context.
- Mechanism: The model treats inputs differently - the context-encoder uses complex poly-attention because context is variable and complex, while the gloss-encoder simply extracts the [CLS] token, assuming definitions are static and self-contained.
- Core assumption: Definitions (glosses) are concise and unambiguous enough that a global [CLS] embedding captures their meaning without needing specific token-level focus.
- Evidence anchors: [section III.A] Eq. 5 states $r_g = B_G(g)[0]$ (extracting [CLS]), contrasting with Eq. 3-4 for the context.
- Break condition: If a gloss is long, complex, or contains the target word itself in a misleading way, relying solely on the [CLS] token may lose critical details necessary for distinguishing fine-grained senses.

## Foundational Learning

- Concept: **Poly-Encoder Architecture**
  - Why needed here: This is the core structural innovation. You must understand how a "long" profile (context) is compressed into codes via attention to a "short" profile (target) to grasp why this differs from standard Bi-Encoders.
  - Quick check question: Can you explain why the Query matrix $Q$ is derived from the target token rather than the [CLS] token in this specific architecture?

- Concept: **Batch Contrastive Learning (BCL)**
  - Why needed here: The efficiency gains (37.6% GPU reduction) come entirely from this training strategy. Understanding the loss function (maximizing diagonal vs. off-diagonal similarity) is required to debug training loops.
  - Quick check question: In a batch of size $B$, how many negative samples does a single target word effectively compare against in one forward pass?

- Concept: **Word Sense Disambiguation (WSD) Basics**
  - Why needed here: The task requires mapping a context to a specific definition (gloss). Understanding that "sense" is discrete and usually tied to external knowledge (WordNet) explains the structure of the inputs.
  - Quick check question: Why is this modeled as a similarity matching problem rather than a standard multi-class classification problem with a fixed softmax layer?

## Architecture Onboarding

- Component map:
  - Context-Encoder (BERT-Large) -> Poly-Attention Module -> Fused Context Vector
  - Gloss-Encoder (BERT-Large) -> [CLS] Token -> Gloss Vector
  - Scoring Head: Dot product between Fused Context Vector and Gloss Vector

- Critical path:
  1. Identify target word index $t$ in context
  2. Extract $r_{wt}$ (local) and $E_C$ (global) from $B_C$
  3. Compute Poly-Attention: $r_{wt}$ attends to $E_C$ → $r^F_{wt}$
  4. Extract $r_g$ from $B_G$
  5. Compute dot product scores

- Design tradeoffs:
  - **Accuracy vs. Efficiency**: The Poly-Encoder adds computational overhead (attention layers) compared to a simple Bi-encoder, but BCL recovers efficiency by reducing data loading/redundancy
  - **Assumption**: The paper claims BCL reduces GPU hours by 37.6%. Verify if this accounts for the *additional* computation of the poly-attention heads or if it purely measures the reduction in forward/backward passes due to fewer input pairs

- Failure signatures:
  - **Index Misalignment**: If the tokenizer splits the target word into subwords (e.g., "playing" → "play" + "##ing"), simply taking the $t$-th token embedding may grab the wrong subword or only part of the word semantic
  - **Batch Homogeneity**: If using BCL, ensure the dataloader shuffles effectively. A batch containing the same word repeated renders BCL useless (negatives become positives)

- First 3 experiments:
  1. **Sanity Check (Overfit)**: Train on a tiny subset (10 sentences) with BCL disabled. Verify the model can perfectly fit these instances
  2. **Ablation (Architecture)**: Run PolyBERT vs. a baseline Bi-encoder (using only [CLS] or only target token) on the dev set to verify the contribution of the fusion mechanism
  3. **Efficiency Validation**: Measure peak GPU memory and time per epoch with BCL enabled vs. disabled (full negative sampling) to reproduce the 37.6% claim

## Open Questions the Paper Calls Out

- Question: How can few-shot learning strategies be effectively integrated with PolyBERT to reduce reliance on large, manually labeled datasets?
  - Basis in paper: [explicit] The Conclusion states that PolyBERT "still requires a large amount of manually labeled data... This underscores a primary direction for future WSD research: enhancing few-shot approaches."
  - Why unresolved: The current model architecture and training pipeline are demonstrated using the large SemCor 3.0 corpus, without testing on low-resource or data-scarce scenarios
  - What evidence would resolve it: Experiments evaluating PolyBERT's performance on domain-specific datasets (e.g., medical or legal texts) using only a handful of labeled examples per sense

- Question: How does the batch size influence the quality of negative sampling in the Batch Contrastive Learning (BCL) mechanism?
  - Basis in paper: [inferred] The paper notes BCL utilizes "correct senses of other target words in the same batch as negative samples," but does not analyze how batch dimensions affect the diversity or difficulty of these negatives
  - Why unresolved: If the batch size is too small or contains semantically similar words, the contrastive signal may be weak; conversely, larger batches increase computational memory loads
  - What evidence would resolve it: An ablation study varying the batch size and reporting the resulting impact on the model's differentiation capabilities and F1-scores

- Question: To what extent does the number of poly codes (`polym`) impact the balance between local and global semantics?
  - Basis in paper: [inferred] The methodology introduces `polym` as a hyperparameter for the poly-encoder, but the paper provides no sensitivity analysis regarding how different values for `polym` affect the fusion of token-level and sequence-level features
  - Why unresolved: It is unclear if the chosen number of poly codes is optimal for all parts of speech or if a higher dimension offers diminishing returns
  - What evidence would resolve it: A parameter sweep testing various values of `polym` (e.g., 1 to 64) to identify the inflection point where semantic representation stabilizes or degrades

## Limitations

- Critical training hyperparameters (batch size, learning rate, optimizer settings) are omitted, making faithful reproduction difficult
- The paper relies on BERT's subword tokenization which may fragment target words, potentially capturing only partial word meaning without addressing this edge case
- Performance claims don't include recent LLM-based approaches (ChatGPT, GPT-4) that achieve higher F1 scores, limiting comparison scope

## Confidence

- **High confidence** in the core architectural contribution: The poly-encoder design with multi-head attention for fusing local and global semantics is clearly specified through equations 1-4, and the BCL training strategy (equations 7-10) is well-defined
- **Medium confidence** in performance claims: The 2% F1 improvement over baselines is reported but the exact implementation details of comparison models aren't provided
- **Low confidence** in reproducibility: Critical training hyperparameters are omitted and the paper mentions code availability but provides no URL

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the `polym` parameter (query replication factor) and batch size to quantify their impact on both F1 performance and GPU memory consumption, validating whether the claimed 37.6% efficiency gain is robust across different computational constraints

2. **Tokenization Robustness Test**: Create a controlled test set where target words are known to be fragmented by BERT's tokenizer (e.g., words ending in "-ing", "-ed", "-s"). Compare PolyBERT's performance on these vs. intact words to empirically assess the impact of subword tokenization on sense disambiguation accuracy

3. **Negative Sample Diversity Evaluation**: Implement a batch analysis tool to measure the semantic diversity of glosses within each training batch. Calculate the average pairwise cosine similarity between glosses in a batch to verify whether BCL's negative samples are sufficiently diverse to provide meaningful contrastive signals, or if batch homogeneity could explain potential performance plateaus