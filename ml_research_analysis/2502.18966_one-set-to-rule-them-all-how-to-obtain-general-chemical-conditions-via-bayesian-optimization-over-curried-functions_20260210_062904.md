---
ver: rpa2
title: 'One Set to Rule Them All: How to Obtain General Chemical Conditions via Bayesian
  Optimization over Curried Functions'
arxiv_id: '2502.18966'
source_url: https://arxiv.org/abs/2502.18966
tags:
- optimization
- number
- figure
- reaction
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of identifying general chemical
  reaction conditions that perform well across multiple substrates, a critical need
  in pharmaceutical and materials research. The authors formulate this as an optimization
  problem over curried functions, where the goal is to find parameters that maximize
  an aggregated objective function across a set of tasks (substrates).
---

# One Set to Rule Them All: How to Obtain General Chemical Conditions via Bayesian Optimization over Curried Functions

## Quick Facts
- arXiv ID: 2502.18966
- Source URL: https://arxiv.org/abs/2502.18966
- Reference count: 40
- Demonstrates that optimization over multiple substrates yields more transferable optima compared to single-substrate optimization

## Executive Summary
This paper addresses the challenge of identifying general chemical reaction conditions that perform well across multiple substrates, a critical need in pharmaceutical and materials research. The authors formulate this as an optimization problem over curried functions, where the goal is to find parameters that maximize an aggregated objective function across a set of tasks (substrates). The key method involves Bayesian optimization (BO) with partial monitoring, where each experiment evaluates conditions on only one substrate at a time. Through systematic benchmarking on four real-world chemical reaction datasets, the authors demonstrate that multi-substrate optimization yields more transferable optima and that simple sequential acquisition strategies perform comparably to more complex joint optimization approaches.

## Method Summary
The authors propose a Bayesian optimization framework for finding reaction conditions that generalize across multiple chemical substrates. They formulate the problem as optimizing a curried function where parameters are first applied to select reaction conditions, then evaluated on individual substrates. The method uses partial monitoring where each experiment can only evaluate one substrate at a time. Two acquisition strategies are explored: sequential approaches that decouple parameter and substrate selection, and joint optimization that selects both simultaneously. The framework is implemented in the open-source CurryBO library, enabling integration with existing BO frameworks.

## Key Results
- Multi-substrate optimization yields more transferable optima compared to single-substrate optimization
- Simple sequential acquisition strategies perform comparably to joint optimization approaches
- More explorative acquisition functions (higher UCB β values) show better performance across different experimental settings
- The open-source CurryBO library enables rapid integration with existing BO frameworks

## Why This Works (Mechanism)
The framework works by framing the search for general conditions as an optimization problem over curried functions, where the first argument selects reaction parameters and the second argument selects which substrate to evaluate. This allows the BO algorithm to learn which parameter regions tend to perform well across multiple substrates rather than overfitting to any single substrate. The partial monitoring setup reflects real laboratory constraints where experiments can only evaluate one substrate at a time. By aggregating results across substrates, the optimization process naturally favors conditions with broader applicability.

## Foundational Learning

**Bayesian Optimization** - Sequential optimization strategy for expensive black-box functions
*Why needed*: Handles the computational cost of evaluating chemical reactions
*Quick check*: Can optimize functions with expensive evaluations in 10-100 iterations

**Curried Functions** - Functions that take arguments one at a time, returning new functions
*Why needed*: Allows separating parameter selection from substrate selection
*Quick check*: Can decompose f(x,y) into g(x)(y) for optimization purposes

**Partial Monitoring** - Bandit setting where actions don't reveal full information
*Why needed*: Reflects real lab constraints where only one substrate can be evaluated per experiment
*Quick check*: Can make decisions with incomplete feedback about the objective function

**Acquisition Functions** - Functions that balance exploration and exploitation in BO
*Why needed*: Guide the search for optimal reaction conditions
*Quick check*: Can improve convergence speed compared to random sampling

## Architecture Onboarding

**Component Map**: BO optimizer -> Curried objective function -> Substrate selection -> Parameter selection -> Experimental evaluation

**Critical Path**: Acquisition function evaluation -> Parameter/substrate selection -> Experimental execution -> Result incorporation -> Model update

**Design Tradeoffs**: Sequential acquisition (simpler, faster computation) vs joint optimization (potentially better exploration, higher computational cost)

**Failure Signatures**: 
- Poor generalization indicates overfitting to specific substrates
- Slow convergence suggests inadequate exploration
- Conflicting optima across substrates may require constraint handling

**3 First Experiments**:
1. Implement single-substrate optimization as baseline for comparison
2. Test sequential acquisition strategy on a simple two-substrate system
3. Compare performance of different acquisition functions (UCB with varying β values)

## Open Questions the Paper Calls Out
None

## Limitations
- Does not address potential conflicts where optimal conditions for different substrates may be mutually exclusive
- Assumes all substrates can be meaningfully evaluated under the same parameter ranges
- Practical implementation details for handling edge cases (substrate-specific failures) are not fully explored

## Confidence

**High**: The demonstration that multi-substrate optimization yields more transferable optima than single-substrate approaches

**Medium**: The claim that simple sequential acquisition strategies perform comparably to joint optimization methods

**Medium**: The recommendation for explorative acquisition functions (higher UCB β values) across different experimental settings

## Next Checks

1. Test the framework on reaction datasets where substrates have fundamentally incompatible optimal conditions to assess how the method handles conflicting objectives

2. Implement the open-source CurryBO library in an independent research group's experimental workflow to validate reproducibility and ease of integration

3. Compare performance across acquisition functions using statistical significance testing rather than qualitative assessment to strengthen the empirical claims