---
ver: rpa2
title: Structured Information for Improving Spatial Relationships in Text-to-Image
  Generation
arxiv_id: '2509.15962'
source_url: https://arxiv.org/abs/2509.15962
tags:
- structured
- information
- generation
- language
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a lightweight, plug-and-play approach for
  improving spatial relationships in text-to-image generation. The method uses a fine-tuned
  T5-small language model to automatically convert natural language prompts into tuple-based
  structured information, encoding objects and their spatial relations.
---

# Structured Information for Improving Spatial Relationships in Text-to-Image Generation

## Quick Facts
- **arXiv ID:** 2509.15962
- **Source URL:** https://arxiv.org/abs/2509.15962
- **Authors:** Sander Schildermans; Chang Tian; Ying Jiao; Marie-Francine Moens
- **Reference count:** 0
- **Primary result:** Lightweight method improves spatial accuracy in text-to-image generation using structured tuple-based prompts without architectural modifications

## Executive Summary
This paper introduces a plug-and-play approach for improving spatial relationships in text-to-image generation by automatically converting natural language prompts into structured tuple-based information. The method uses a fine-tuned T5-small language model to extract objects and their spatial relations from prompts, which are then appended to the original text and fed into Stable Diffusion XL. Experiments show that this approach improves spatial accuracy while maintaining image quality, outperforming both plain prompts and two leading baselines. The automatically generated tuples match the quality of human-crafted ones, demonstrating the feasibility of lightweight tuple extraction.

## Method Summary
The approach involves a two-stage pipeline where a fine-tuned T5-small language model first extracts structured tuple information from natural language prompts, encoding objects and their spatial relationships. These tuples are formatted as compact strings and appended to the original prompt before being fed into Stable Diffusion XL. The method requires no modifications to the diffusion model architecture, making it a lightweight, portable solution. The structured information serves as a redundant explicit signal that reinforces spatial relationships during image generation, helping the model ground abstract descriptions to concrete spatial arrangements.

## Key Results
- Automatically generated tuples achieve quality comparable to human-crafted ones, validating the approach
- The method improves spatial accuracy while maintaining overall image quality as measured by Inception Score
- Outperforms both plain prompts and two leading baselines in spatial relationship accuracy
- Fine-tuned T5-small model shows superior performance compared to larger LLMs when training data is limited

## Why This Works (Mechanism)

### Mechanism 1: Semantic Disambiguation via Structured Redundancy
- **Claim:** Appending tuple-based structured information to prompts improves spatial accuracy by providing a redundant, explicit signal that reinforces object relations
- **Mechanism:** Natural language prompts often contain ambiguous spatial prepositions. By extracting entities and relations into a normalized format `(subject, relation, object)` and concatenating it with the original prompt, the method likely reduces the search space for the diffusion model's cross-attention layers
- **Core assumption:** The pre-training corpus of Stable Diffusion XL contains sufficient semi-structured or code-like data that the model can effectively condition on this appended tuple syntax without requiring weight updates
- **Evidence anchors:** [Section 4.3] states the improvement arises because the "structured representation emphasizes spatial elements... while maintaining strong alignment with the natural language patterns on which diffusion models are trained"

### Mechanism 2: Scale-Constrained Semantic Parsing
- **Claim:** Fine-tuning a small encoder-decoder model (T5-small) is more data-efficient for structured extraction than using larger decoder-only models when training data is scarce
- **Mechanism:** With only 500 training samples, large parameter models are prone to overfitting or catastrophic forgetting of formatting constraints, producing malformed sequences. A smaller model has limited capacity, which acts as a regularizer
- **Core assumption:** The mapping from natural language spatial descriptions to the tuple format is deterministic and rule-based enough to be captured by a smaller model without extensive world knowledge
- **Evidence anchors:** [Section 4.2] Table 1 shows Llama-3-8B achieving near-zero BLEU scores, while T5-small achieves 0.98

### Mechanism 3: Training-Free Inversion Guidance
- **Claim:** Improving spatial faithfulness does not require modifying the weights of the diffusion model (U-Net), but can be achieved solely through input-space manipulation
- **Mechanism:** The method operates as a pre-processing step. By reformulating the prompt, it likely shifts the initial latent distribution and guides the cross-attention maps during denoising
- **Core assumption:** The errors in spatial generation are primarily due to ambiguous text encoding rather than fundamental limitations in the diffusion process itself
- **Evidence anchors:** [Abstract] explicitly mentions "without any architectural modifications"

## Foundational Learning

- **Concept:** **Semantic Parsing / Information Extraction**
  - **Why needed here:** The core contribution relies on converting unstructured text into structured tuples `(object, relation, object)`
  - **Quick check question:** Given the prompt "A red sphere sits on a blue cube," what are the two object tuples and one spatial relation tuple expected by this system?

- **Concept:** **Encoder-Decoder vs. Decoder-Only Architectures**
  - **Why needed here:** The paper highlights a performance gap between T5 (Encoder-Decoder) and Llama (Decoder-Only) on a small dataset
  - **Quick check question:** Why might an encoder-decoder model be more robust to "noise" in prompt formatting when performing strict structured extraction compared to a generative decoder-only model?

- **Concept:** **Cross-Attention in Latent Diffusion**
  - **Why needed here:** To understand why appending text works, you need to grasp how the U-Net conditions on text embeddings
  - **Quick check question:** In Stable Diffusion, which layer is primarily responsible for "reading" the text prompt and influencing the image structure: the self-attention layers or the cross-attention layers?

## Architecture Onboarding

- **Component map:** StructuredPrompter (T5-small) -> Augmenter (concatenation) -> Generator (SDXL) -> Evaluator (Qwen2.5-VL-3B-Instruct)
- **Critical path:** The T5-small fine-tuning loop. If this component generates malformed tuples, the downstream SDXL generation will be nonsensical
- **Design tradeoffs:** The paper trades off the raw reasoning power of an LLM for the data efficiency and stability of T5-small
- **Failure signatures:** Malformed Tuples, Token Truncation, Overfitting
- **First 3 experiments:**
  1. Tokenizer Validity Check: Pass 100 random prompts through the fine-tuned T5-small and verify the output strictly matches the regex `\([^)]+\)`
  2. Ablation on Concatenation: Generate images with three inputs: (a) Plain prompt, (b) Structured tuples only, and (c) Plain + Structured (proposed)
  3. Data Scaling Limit: Attempt to fine-tune T5-small with only 50, 100, and 200 samples to identify the breaking point where BLEU scores drop below 0.90

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the tuple-based structured information approach generalize to complex, photorealistic images beyond synthetic shape datasets?
- **Basis in paper:** The method is evaluated exclusively on the TV-Logic dataset, which consists of "synthetic images of simple colored shapes," leaving its efficacy on natural images unconfirmed
- **Why unresolved:** The paper restricts experiments to a controlled, low-complexity domain to reduce ambiguity, but real-world T2I tasks involve textures, lighting, and complex semantics not present in colored shape datasets
- **What evidence would resolve it:** Quantitative results on standard T2I benchmarks featuring natural images, such as COCO or GenEval

### Open Question 2
- **Question:** Can the method maintain spatial accuracy in scenes containing more than two objects?
- **Basis in paper:** Section 3.1 states: "we restrict the dataset to two-object scenes, excluding the counting and manipulation subsets"
- **Why unresolved:** The binary tuple structure and the restriction to two-object scenes suggest potential difficulties in managing the combinatorial complexity of multi-object spatial graphs
- **What evidence would resolve it:** Evaluation of the StructuredPrompter on prompts containing three or more objects

### Open Question 3
- **Question:** Would a larger language model outperform T5-small if trained on a larger dataset?
- **Basis in paper:** Section 4.2 notes that larger models (Llama-3-8B) produced "malformed sequences" and suggests they "overfit" on the limited 500-sample training set
- **Why unresolved:** The study concludes T5-small is sufficient based on a small data regime, but it is unknown if the capacity of larger LLMs could capture more complex spatial dependencies if provided with sufficient training data
- **What evidence would resolve it:** Fine-tuning larger LLMs on an expanded training dataset and comparing tuple extraction accuracy and downstream image quality

## Limitations

- The method's reliance on a fine-tuned T5-small model for tuple extraction introduces a potential failure point if the structured information contains errors or hallucinations
- The evaluation relies heavily on automated metrics and a single visual reasoning model for spatial accuracy, lacking human evaluation
- The dataset used for fine-tuning (500 samples) is relatively small, and the method's performance on out-of-distribution prompts or long, complex scenes is not thoroughly explored

## Confidence

- **High Confidence:** The claim that T5-small outperforms larger models (Llama-3-8B) on structured extraction with limited data, as evidenced by BLEU scores and ablation studies
- **Medium Confidence:** The assertion that appending structured tuples improves spatial accuracy, based on quantitative metrics (IS, spatial accuracy) and comparison with baselines
- **Low Confidence:** The generalizability of the method to diverse, complex prompts and its robustness to linguistic variation, due to limited qualitative validation and ablation on prompt complexity

## Next Checks

1. **Human Evaluation on Spatial Accuracy:** Recruit 10-20 annotators to rate spatial relationships in generated images for 50 prompts (both plain and augmented). Compare inter-annotator agreement and check if the improvement is consistent across linguistic diversity.

2. **Robustness to Linguistic Variation:** Construct a test set of 100 prompts with varying syntactic complexity (e.g., nested relations, ambiguous prepositions, long-distance dependencies). Measure the T5 extraction accuracy and downstream spatial faithfulness. Identify the breaking point where the method fails.

3. **Scaling Analysis of T5 Fine-Tuning:** Fine-tune T5-small with 50, 100, 200, and 500 samples. Plot BLEU scores and downstream spatial accuracy as a function of training data. Determine the minimum effective sample size and whether performance plateaus or degrades with more data.