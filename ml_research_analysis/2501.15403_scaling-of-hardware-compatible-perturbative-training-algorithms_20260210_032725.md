---
ver: rpa2
title: Scaling of hardware-compatible perturbative training algorithms
arxiv_id: '2501.15403'
source_url: https://arxiv.org/abs/2501.15403
tags:
- gradient
- perturbation
- network
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multiplexed gradient descent (MGD) is a perturbative zeroth-order
  training method for hardware-based neural networks that estimates gradients by correlating
  parameter perturbations with cost changes. The authors extend MGD to include both
  weight perturbation (perturbing individual weights) and node perturbation (perturbing
  neuron inputs), showing node perturbation can be more efficient for dense networks.
---

# Scaling of hardware-compatible perturbative training algorithms

## Quick Facts
- arXiv ID: 2501.15403
- Source URL: https://arxiv.org/abs/2501.15403
- Reference count: 40
- Primary result: MGD trains networks with >1M parameters to backpropagation-level accuracy without hardware-incompatible operations

## Executive Summary
Multiplexed Gradient Descent (MGD) is a perturbative zeroth-order training method that estimates gradients by correlating parameter perturbations with cost changes. The authors extend MGD to include both weight perturbation and node perturbation, demonstrating that node perturbation can be more efficient for dense networks. MGD can train large networks (over 1 million parameters) to accuracy matching backpropagation while being compatible with hardware constraints like limited write cycles. The method challenges previous assumptions about perturbative methods' scalability and offers a practical solution for training future neuromorphic hardware.

## Method Summary
MGD is a perturbative zeroth-order training method that estimates gradients without computing derivatives. It works by perturbing network parameters and measuring the resulting changes in the cost function. The authors extend traditional weight perturbation to include node perturbation, where instead of perturbing individual weights, they perturb the inputs to neurons. This approach correlates parameter perturbations with cost changes to estimate gradients, allowing training without hardware-incompatible operations like matrix multiplications or floating-point arithmetic. The framework can implement standard optimizers like Adam and demonstrates 37% fewer weight updates compared to vanilla gradient descent.

## Key Results
- MGD achieves backpropagation-level accuracy on benchmark datasets with networks containing over 1 million parameters
- Training time does not scale linearly with network size despite gradient estimation time doing so
- Node perturbation proves more efficient than weight perturbation for dense networks
- MGD can implement standard optimizers like Adam, achieving 37% fewer weight updates than vanilla gradient descent

## Why This Works (Mechanism)
MGD works by estimating gradients through perturbation correlation rather than analytical differentiation. By systematically perturbing parameters (weights or node inputs) and measuring the resulting changes in the cost function, the method can approximate the gradient direction without requiring backpropagation. This zeroth-order approach makes it compatible with hardware constraints that limit certain operations, while still achieving competitive training performance.

## Foundational Learning

**Perturbative training methods**: Why needed - to enable training in hardware with limited computational capabilities; Quick check - can estimate gradients without explicit derivative calculations

**Zeroth-order optimization**: Why needed - allows optimization without gradient information; Quick check - relies on function evaluations rather than analytical gradients

**Hardware-incompatible operations**: Why needed - identifies bottlenecks in traditional training; Quick check - includes matrix multiplications and floating-point arithmetic

**Multiplexed gradient estimation**: Why needed - improves gradient estimation efficiency; Quick check - combines multiple perturbation strategies

## Architecture Onboarding

**Component map**: Input data -> Perturbation module -> Network with modified parameters -> Cost function evaluation -> Gradient estimation -> Parameter update

**Critical path**: Perturbation → Cost evaluation → Correlation calculation → Parameter update

**Design tradeoffs**: 
- Weight perturbation offers fine-grained control but may be inefficient for dense networks
- Node perturbation is more efficient for dense networks but may be less precise
- Multiplexing combines benefits but increases complexity

**Failure signatures**: 
- Poor correlation between perturbations and cost changes indicates suboptimal perturbation magnitude
- High variance in gradient estimates suggests insufficient perturbation multiplexing
- Slow convergence may indicate inappropriate optimizer choice

**First experiments**:
1. Train a small dense network on MNIST using both weight and node perturbation to compare efficiency
2. Implement Adam optimizer within MGD framework on CIFAR-10 to verify optimizer compatibility
3. Test MGD on a network with 100k+ parameters to validate scaling claims

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on sparse, recurrent, or attention-based architectures remains untested
- Behavior with alternative neuron models common in neuromorphic hardware is not explored
- Specific implementations for hardware constraints like limited write cycles lack detailed overhead analysis

## Confidence
- High confidence: MGD achieves backpropagation-level accuracy on tested benchmarks with over 1 million parameters
- Medium confidence: Training time does not scale linearly with network size despite gradient estimation time doing so
- Medium confidence: MGD can accommodate hardware constraints like limited write cycles

## Next Checks
1. Test MGD performance on sparse and recurrent neural network architectures to verify scaling claims hold beyond dense feedforward networks
2. Implement MGD with different neuron models (spiking, stochastic) common in neuromorphic hardware to assess hardware compatibility beyond theoretical framework
3. Conduct a comprehensive comparison of MGD's computational efficiency relative to backpropagation across varying hardware platforms, including energy consumption metrics and write-cycle limitations