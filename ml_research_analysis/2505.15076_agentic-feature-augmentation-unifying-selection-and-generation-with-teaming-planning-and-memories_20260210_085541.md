---
ver: rpa2
title: 'Agentic Feature Augmentation: Unifying Selection and Generation with Teaming,
  Planning, and Memories'
arxiv_id: '2505.15076'
source_url: https://arxiv.org/abs/2505.15076
tags:
- feature
- selection
- generation
- agent
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MAGS, a multi-agent system that unifies feature
  selection and generation through agentic teaming and planning. The framework includes
  a router agent for deciding between generation or selection, a generator agent for
  creating new features, and a selector agent for removing redundant ones.
---

# Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories

## Quick Facts
- arXiv ID: 2505.15076
- Source URL: https://arxiv.org/abs/2505.15076
- Authors: Nanxu Gong; Sixun Dong; Haoyue Bai; Xinyuan Wang; Wangyang Ying; Yanjie Fu
- Reference count: 37
- One-line primary result: MAGS achieves up to 95.6% accuracy and 0.956 R², outperforming state-of-the-art feature engineering methods across six datasets.

## Executive Summary
This paper proposes MAGS, a multi-agent system that unifies feature selection and generation through agentic teaming and planning. The framework includes a router agent for deciding between generation or selection, a generator agent for creating new features, and a selector agent for removing redundant ones. It employs long and short-term memory mechanisms to guide agents and uses offline PPO reinforcement learning to train the router. Experiments on six datasets show MAGS consistently outperforms state-of-the-art feature engineering methods across classification and regression tasks, achieving up to 95.6% accuracy and 0.956 R². Ablation studies confirm the importance of both memory mechanisms and RL fine-tuning. The method also demonstrates robustness across different downstream models and offers interpretable, traceable feature sets.

## Method Summary
MAGS operates as a multi-agent system where a router agent coordinates between generator and selector agents to iteratively refine feature sets. The router uses offline PPO reinforcement learning to make decisions about whether to generate new features or select existing ones based on dataset context. Both agents utilize in-context learning with short-term memory for immediate feedback and long-term memory for guidance from past successes. The system alternates between action selection and feature set evaluation, storing high-performing examples in memory pools. This creates an adaptive feature engineering process that balances exploration (generation) with exploitation (selection) to optimize downstream task performance.

## Key Results
- MAGS consistently outperforms state-of-the-art feature engineering methods across six datasets
- Achieves up to 95.6% accuracy and 0.956 R² on benchmark tasks
- Ablation studies confirm both memory mechanisms and RL fine-tuning are essential for performance gains
- Demonstrates robustness across different downstream models and feature types

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Teaming for Unified Feature Selection and Generation
- Claim: Unifying feature selection and generation within a collaborative multi-agent system improves downstream task performance compared to applying them separately.
- Mechanism: A router agent strategically coordinates a generator agent (creating new informative features) and a selector agent (eliminating redundant ones) in an iterative process. This allows the system to balance expanding the feature space with expressive combinations and pruning irrelevant features, creating a task-adaptive representation.
- Core assumption: The iterative, coordinated interplay of adding and removing features leads to better representations than a sequential or isolated approach.
- Evidence anchors:
  - [abstract]: "...unification of feature generation and selection is modeled as agentic teaming and planning...comprising a selector agent...a generator agent...and a router agent that strategically coordinates their actions...this unified agentic framework consistently achieves superior task performance..."
  - [section 4.2, Table 1]: MAGS (type B for both) outperforms individual generation (G) and selection (S) baselines across multiple datasets.
  - [corpus]: Evidence is weak; related papers discuss agentic systems generally but not this specific unified feature engineering mechanism.
- Break condition: If the feature space is too large or complex for the agents to effectively navigate within a reasonable number of iterations, or if the coordination overhead outweighs the benefits of unification, performance gains may diminish compared to simpler, faster methods.

### Mechanism 2: Dual Memory-Augmented In-Context Learning
- Claim: Equipping the generator and selector agents with both short-term and long-term memory enhances their decision-making quality and optimization stability.
- Mechanism: Short-term memory provides agents with immediate feedback from the current exploration iteration (recent actions, feature sequences, performance), enabling fine-grained adaptation. Long-term memory provides access to high-quality demonstrations from past successful iterations, guiding agents toward globally promising directions. These memories are injected into the LLM agents' prompts.
- Core assumption: Analogical learning from past successes and recent feedback effectively guides LLM-based agents in a discrete feature space.
- Evidence anchors:
  - [abstract]: "...leverage in-context learning with short-term memory for immediate feedback refinement and long-term memory for globally optimal guidance."
  - [section 4.3, Figure 3]: Ablation studies show a performance drop when either "w/o Long" or "w/o Short" memory is removed, indicating their individual and synergistic contributions.
  - [corpus]: Related work (arXiv:2504.21304) uses generator-critic LLM agents, but specific dual memory evidence for feature engineering is thin in the provided corpus.
- Break condition: If the memory context exceeds the LLM's token limit (e.g., too many features, long history) or if the stored demonstrations are not diverse/relevant enough, the in-context learning may fail or provide noisy guidance, degrading performance.

### Mechanism 3: Offline RL Fine-Tuning of the Router Agent
- Claim: Fine-tuning the router agent's policy using offline Proximal Policy Optimization (PPO) improves its ability to make task-aware, context-sensitive decisions between selection and generation.
- Mechanism: The router's initial policy is refined using a dataset of (prompt, action, score) triplets collected from initial explorations. PPO optimizes the policy to maximize the expected downstream task performance (reward) associated with its routing decisions, while preventing overly large, destabilizing updates. This allows the router to learn dataset-specific strategies (e.g., favor generation in low-dimensional spaces, selection in high-dimensional noisy ones).
- Core assumption: A non-fine-tuned LLM's general reasoning is insufficient for optimal planning in the specific, vast discrete feature space, and offline PPO can effectively align the router's policy with the global optimization objective.
- Evidence anchors:
  - [abstract]: "...employ offline Proximal Policy Optimization (PPO) reinforcement fine-tuning to train the router agent for effective decision-making..."
  - [section 4.3, Table 2]: RL fine-tuning shifts the router's decision distribution (e.g., more generation on low-dimensional datasets, more selection on high-dimensional ones), correlating with improved performance. Ablation "w/o RL" shows performance drop.
  - [corpus]: Evidence for offline PPO fine-tuning a router specifically for feature engineering is absent in the provided corpus.
- Break condition: If the offline training data is insufficient, biased, or of poor quality, the PPO fine-tuning could lead to a suboptimal or brittle policy that fails to generalize or adapt within the iterative process.

## Foundational Learning

- Concept: **Reinforcement Learning (RL) and Proximal Policy Optimization (PPO)**
  - Why needed here: The paper uses offline PPO to train the router agent. Understanding the basics of policy gradients, the concept of a reward signal (downstream task performance), and why PPO is chosen for this fine-tuning task is critical.
  - Quick check question: Can you explain how PPO uses a "reward" to update a policy, and why the term "proximal" is important for stability?

- Concept: **In-Context Learning with Large Language Models (LLMs)**
  - Why needed here: The generator and selector agents are LLMs that use "memory" (textual demonstrations) inserted into their prompts to guide their behavior without weight updates. Understanding how to structure prompts and leverage examples for this few-shot learning is key.
  - Quick check question: How does providing examples of successful feature transformations in an LLM's prompt differ from training the LLM's weights?

- Concept: **Feature Engineering: Selection vs. Generation**
  - Why needed here: The core problem is unifying these two opposing operations. One must understand the goals of each (reducing redundancy vs. adding expressive features) and the trade-offs involved (e.g., risk of adding noise vs. risk of losing informative interactions).
  - Quick check question: Give an example where applying only feature selection might hurt model performance, and another where only feature generation might be detrimental.

## Architecture Onboarding

- Component map:
  - Router Agent (LLaMA-3.2-3B) -> Memory Pool -> Generator Agent (GPT-3.5-Turbo)
  - Router Agent (LLaMA-3.2-3B) -> Memory Pool -> Selector Agent (GPT-3.5-Turbo)
  - Router Agent (LLaMA-3.2-3B) -> Offline RL Module -> Router Agent (LLaMA-3.2-3B)
  - Generator/Selector Agent -> Environment/Downstream Task -> Memory Pool

- Critical path: The system operates in a nested loop. 1) **Initialization**: Start with raw features. 2) **Iteration Loop**: Restart with raw features. 3) **Action Loop**: a) **Router** decides Select vs Generate. b) **Memory** retrieves relevant short/long-term context. c) Corresponding **Agent** (Selector/Generator) acts, modifying the feature set. d) **Evaluation**: The new feature set is scored on the downstream task. e) **Memory Update**: Results stored in short-term memory. 4) **End of Iteration**: Best results archived to long-term memory. 5) **Router Fine-Tuning**: Periodically, offline PPO uses accumulated data to update the Router's policy.

- Design tradeoffs:
  - **LLM Choice:** Router uses a smaller, open-source model (LLaMA-3.2-3B) which is fine-tuned. Generator/Selector use a powerful commercial API (GPT-3.5-Turbo) for in-context learning. This trades off cost/controllability vs. generative capability.
  - **Memory Token Limits:** Storing too much history can exceed LLM context windows. The paper uses a sampling strategy for long-term memory to mitigate this.
  - **Iterative Complexity:** The multi-step, multi-iteration process is computationally expensive. The paper explicitly notes this as a limitation.

- Failure signatures:
  - **Runaway Feature Generation:** Router gets stuck in a "Generate" loop, continuously adding features without selection, leading to high dimensionality, noise, and token limit issues.
  - **Premature Convergence:** Router becomes too risk-averse (e.g., over-selecting) and the system fails to explore beneficial feature interactions.
  - **Token Overflow:** Prompt with feature set and memory examples exceeds the context length of the LLM agents, causing API errors or truncated context.
  - **No Performance Improvement:** The RL fine-tuning fails to converge or the agents fail to learn from in-context examples, resulting in feature sets that do not improve upon the original.

- First 3 experiments:
  1. **Reproduce Ablation on a Single Dataset:** Pick one dataset (e.g., `svmguide3`). Run MAGS and the "w/o Router" ablation. Confirm that intelligent routing significantly outperforms random selection/generation.
  2. **Analyze Router's Learned Policy:** After training, inspect the router's decisions. Does it correctly learn to favor generation on a simple, low-dimensional dataset and selection on a noisy, high-dimensional one? (Table 2).
  3. **Validate Memory Impact:** Run the "w/o Long" and "w/o Short" ablations. Plot the performance trajectory over iterations to see how each memory type affects convergence speed and final performance.

## Open Questions the Paper Calls Out
The paper acknowledges several open questions including how to scale the memory mechanisms for high-dimensional feature spaces without exceeding LLM context limits, whether the approach generalizes to noisy real-world datasets with missing values, and how to reduce the computational expense of the iterative exploration process. The authors also note that the theoretical foundation for why specific memory combinations and offline PPO fine-tuning are optimal remains underexplored.

## Limitations
- Computational expense of iterative multi-agent exploration and RL fine-tuning limits scalability
- Reliance on LLM APIs introduces potential variability and cost concerns
- Memory mechanisms face practical constraints from context window limits in high-dimensional spaces
- Offline RL training assumes availability of quality exploration data which may not always be feasible

## Confidence

**High Confidence**: The unified agentic framework architecture is well-defined and the ablation studies convincingly demonstrate the individual contributions of memory mechanisms and RL fine-tuning to performance improvements.

**Medium Confidence**: While experimental results show consistent outperformance across six datasets, the computational expense and potential overfitting to specific dataset characteristics warrant caution. The generalizability to real-world, noisy datasets with many missing values remains to be thoroughly tested.

**Low Confidence**: The theoretical foundation for why the specific combination of short-term and long-term memory with offline PPO fine-tuning is optimal for feature engineering in LLM agents is not rigorously established. The break conditions identified (token overflow, insufficient memory diversity) represent significant practical challenges.

## Next Checks

1. **Computational Efficiency Benchmarking**: Measure and compare the wall-clock time and computational resources required by MAGS against baseline feature engineering methods across varying dataset sizes and dimensionalities.

2. **Robustness to Noisy Real-World Data**: Evaluate MAGS on datasets with significant missing values, outliers, and class imbalance to assess its practical utility beyond the curated UCI datasets used in the current study.

3. **Memory Mechanism Scaling Analysis**: Systematically test how performance degrades as feature dimensionality increases, specifically measuring when memory context exceeds LLM token limits and how different sampling strategies for long-term memory affect results.