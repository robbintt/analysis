---
ver: rpa2
title: Emergent Analogical Reasoning in Transformers
arxiv_id: '2602.01992'
source_url: https://arxiv.org/abs/2602.01992
tags:
- reasoning
- analogical
- training
- learning
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes analogical reasoning in Transformers using
  category-theoretic functors and introduces a synthetic task to study it under controlled
  settings. The task distinguishes analogical reasoning (transferring relational structure
  across domains) from compositional reasoning (combining facts within a domain).
---

# Emergent Analogical Reasoning in Transformers

## Quick Facts
- **arXiv ID:** 2602.01992
- **Source URL:** https://arxiv.org/abs/2602.01992
- **Reference count:** 40
- **Primary result:** Analogical reasoning emerges in Transformers in a delayed, non-monotonic manner distinct from compositional reasoning, relying on geometric embedding alignment and vector addition mechanisms.

## Executive Summary
This paper investigates when and how analogical reasoning—the ability to transfer relational structures across domains—emerges in Transformer models. Using category theory, the authors formalize analogy as structure-preserving functors between domains and create a synthetic task to study this capability under controlled conditions. They find that analogical reasoning emerges in a distinct, delayed manner compared to compositional reasoning, being sensitive to model scale, optimization choices, and data characteristics. Mechanistically, it relies on geometric alignment of entity embeddings across categories followed by functor application via vector addition. The findings reveal that analogy depends on specific representational structures rather than simple scaling effects.

## Method Summary
The authors formalize analogical reasoning using category theory, where a functor maps entities between isomorphic domains while preserving relational structure. They construct a synthetic task with two isomorphic categories (E₁ and E₂) connected by a functor f, where the model must perform both compositional reasoning (2-hop paths within categories) and analogical reasoning (mapping entities between categories via the functor). The task uses directed entity-relation graphs with 20 entities and 10,000 relations, training a 1-layer Transformer (d=128) with Adam optimizer (LR=10⁻⁴, batch size 32, weight decay=0). Evaluation tracks accuracy on compositional vs analogical facts, while mechanistic analysis examines Dirichlet Energy of embeddings and attention patterns.

## Key Results
- Analogical reasoning emerges in a delayed, non-monotonic manner compared to compositional reasoning
- Geometric alignment of entity embeddings (measured by decreasing Dirichlet Energy) precedes analogical reasoning emergence
- Functor application occurs via vector addition: eₜ ≈ eₛ + f, with increasing attention from functor to source entity and parallelism between (eₜ-eₛ) and f
- Model width shows a U-shaped effect (best at 128-256 dimensions), and weight decay accelerates emergence
- Pretrained LLMs exhibit similar layer-wise structural alignment during in-context learning

## Why This Works (Mechanism)

### Mechanism 1: Structural Alignment via Embedding Geometry
- Claim: Analogical reasoning requires geometric alignment of entity embeddings across categories before reasoning can occur.
- Mechanism: During training, entity embeddings from different categories progressively align so that entities sharing relational roles are embedded nearby. This is quantified by Dirichlet Energy E(E) = Σᵢⱼ Aᵢⱼ‖hᵢ - hⱼ‖², where lower energy indicates relationally connected entities are closer together.
- Core assumption: Relational structure can be captured as geometric relationships in continuous embedding space.
- Evidence anchors:
  - [abstract]: "geometric alignment of relational structure in the embedding space...quantified by decreasing Dirichlet Energy"
  - [Section 4.1]: "Analogical performance improves after the Dirichlet Energy has substantially decreased, suggesting that embedding-level structural alignment precedes the emergence of analogical reasoning"
  - [corpus]: Related work "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models" (arxiv:2502.20332) supports structured reasoning mechanisms in LLMs
- Break condition: Graph sparsity above ~50% prevents structural alignment entirely; too few relations (|R|=100) fails to provide sufficient relational diversity.

### Mechanism 2: Functor Application via Vector Addition
- Claim: Once embeddings are aligned, the Transformer implements analogical mapping as additive vector arithmetic.
- Mechanism: The functor token f attends to source entity eₛ via attention mechanism, retrieving entity information. Residual connections then integrate: eₜ ≈ eₛ + f. This is verified by measuring (1) attention scores from f to eₛ increasing, and (2) parallelism cos(eₜ - eₛ, f) increasing.
- Core assumption: Attention-plus-residual can implement linear transformations in representation space.
- Evidence anchors:
  - [abstract]: "functor application via vector addition in the Transformer"
  - [Section 4.2]: "parallelism measure increases concurrently with analogical reasoning performance...attention score increases at the same time analogical reasoning performance improves"
  - [corpus]: Limited direct corpus support; "Self-supervised Analogical Learning using Language Models" (arxiv:2502.00996) addresses related analogical reasoning issues
- Break condition: If attention scores from functor to source entity remain low, no information transfer occurs; parallelism must increase for correct target prediction.

### Mechanism 3: Layer-wise Structural Refinement in Pretrained LLMs
- Claim: In pretrained LLMs, analogous structural alignment emerges progressively along the layer axis during inference rather than training steps.
- Mechanism: Through in-context learning, LLMs refine representations forward through layers. Dirichlet Energy computed on hidden states decreases in later layers, accompanied by sharp increases in target probability via logit lens analysis.
- Core assumption: In-context learning induces gradient descent-like optimization during the forward pass.
- Evidence anchors:
  - [abstract]: "structural alignment appearing progressively along the layer axis"
  - [Section 5]: "energy begins to decrease in the later layers, and this decrease is closely accompanied by a corresponding increase in the probability of the correct answer"
  - [corpus]: Corpus evidence is limited; paper cites Von Oswald et al. (2023) on in-context learning as gradient descent, but not present in corpus neighbors
- Break condition: Insufficient model depth relative to problem complexity; experiments show more entities require deeper layers for alignment to complete.

## Foundational Learning

- **Concept: Dirichlet Energy on Graphs**
  - Why needed here: Primary quantitative metric for measuring structural alignment in embedding space.
  - Quick check question: Given adjacency matrix A and embeddings H, would you expect Dirichlet Energy to increase or decrease when structurally similar entities cluster together?

- **Concept: Attention Mechanism with Residual Connections**
  - Why needed here: Core architectural components enabling the functor application via information retrieval and additive integration.
  - Quick check question: How does the combination of attention (information routing) and residual connections (additive composition) differ from using either alone?

- **Concept: Category-Theoretic Functors**
  - Why needed here: Formal framework defining analogy as structure-preserving mappings between domains (categories).
  - Quick check question: How does a functor (mapping between relational structures) differ from simply mapping individual entities based on surface similarity?

## Architecture Onboarding

- **Component map:**
  Input tokens (eₛ, f) → Embedding layer [structural alignment stored here] → Attention: f attends to eₛ [information retrieval] → Residual: x + Attn(x) [additive integration: eₛ + f ≈ eₜ] → Unembed → predict eₜ

- **Critical path:**
  1. Training on atomic facts builds relational structure in embeddings
  2. Dirichlet Energy drops → geometric alignment achieved
  3. Attention learns to route source entity info to functor position
  4. Residual addition produces target entity approximation

- **Design tradeoffs:**
  | Choice | Effect |
  |--------|--------|
  | Model width 64 | Fails almost entirely |
  | Model width 128-256 | Sweet spot for analogy |
  | Model width 512 | Degrades performance |
  | More layers | Inverse scaling (worse) |
  | Weight decay 0.01-0.1 | Accelerates emergence |
  | Weight decay 1.0 | Prevents emergence |

- **Failure signatures:**
  - High Dirichlet Energy + low analogical accuracy → embeddings never aligned
  - Low attention (f→eₛ) scores → functor not retrieving source info
  - Low parallelism cos(eₜ-eₛ, f) → vector addition mechanism not functioning
  - Transient behavior (gain then lose) → overfitting disrupts geometry (Appendix B)

- **First 3 experiments:**
  1. **Baseline replication:** Train 1-layer Transformer (d=128) on synthetic task with |E|=20, |R|=10000; plot Dirichlet Energy, attention scores, and parallelism vs training steps alongside accuracy.
  2. **Ablation sweep:** Vary weight decay ∈ {0, 0.01, 0.1, 1.0}; measure time-to-analogy-emergence and final accuracy.
  3. **LLM probing:** Apply logit lens to Gemma-2-2B on analogical prompts (Figure 7a format); plot layer-wise Dirichlet Energy and target probability to verify layer-axis alignment.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Transformers infer latent relational correspondences between domains without being provided with an explicit functor token during training?
  - Basis: Appendix L states the current setup explicitly introduces a functor token $<f>$, whereas humans can often reason about relationships between categories without such explicit supervision.
  - Why unresolved: The current synthetic task forces the model to learn the application of a provided functor rather than the discovery of the mapping itself.
  - What evidence would resolve it: Experiments using a modified task setup where the model must infer the cross-category mapping purely from implicit structural regularities in the data, without a dedicated token, showing if geometric alignment still emerges.

- **Open Question 2:** Does the identified geometric alignment mechanism persist when relational structures across domains are non-isomorphic or sparse?
  - Basis: Appendix L notes that real-world domains rarely share isomorphic structures, whereas the toy tasks assume identical structures by construction; additionally, Section C shows analogical reasoning fails with high graph sparsity.
  - Why unresolved: The vector addition mechanism implies a global structural alignment that may break down if the relational graphs differ or if edges are missing.
  - What evidence would resolve it: Evaluating the Dirichlet Energy and alignment metrics in tasks where the source and target categories share only partial structural overlap or utilize small-world network topologies.

- **Open Question 3:** Does the emergence of functor-like geometric structures causally improve sample efficiency in downstream learning tasks?
  - Basis: Section 6 states that while analogy is often attributed to human sample efficiency, "our study does not yet establish whether such structure is effectively leveraged during learning to improve sample efficiency."
  - Why unresolved: The paper demonstrates that the model uses this structure to solve the analogical task but does not verify if acquiring this structure enables faster learning of new knowledge.
  - What evidence would resolve it: Transfer learning experiments comparing the data requirements for learning a new task in models that have acquired the geometric alignment capability versus those that have not.

## Limitations
- Mechanistic claims rely on correlational observations rather than causal interventions
- Geometric alignment hypothesis depends on specific metrics (Dirichlet Energy) that may not capture all aspects of relational structure
- LLM experiments use in-context learning rather than fine-tuning, making it difficult to separate architecture-specific effects from training-induced representations

## Confidence
- **High confidence**: The synthetic task design successfully isolates analogical vs compositional reasoning, and the three-phase emergence pattern is consistently observed across experiments. The inverse scaling with depth and the critical role of weight decay are robustly demonstrated.
- **Medium confidence**: The geometric alignment mechanism via Dirichlet Energy captures meaningful structure, but alternative explanations cannot be fully ruled out. The vector addition interpretation of functor application is plausible but requires interventional validation.
- **Low confidence**: Claims about layer-wise structural refinement in pretrained LLMs depend on specific experimental conditions that may not generalize. The mechanistic explanations for in-context learning remain speculative.

## Next Checks
1. **Interventional validation**: Test whether forcing geometric alignment (via auxiliary loss on Dirichlet Energy) accelerates analogical reasoning emergence, and whether perturbing embedding geometry disrupts it. This would provide causal evidence for the structural alignment hypothesis.

2. **Mechanistic ablation**: Disable attention between functor and source entity (zero attention weights) and verify that analogical accuracy drops while compositional accuracy remains intact. Conversely, force attention to flow regardless of geometry and check if this bypasses the need for embedding alignment.

3. **Cross-task generalization**: Apply the same mechanistic analysis (Dirichlet Energy, attention patterns, vector parallelism) to established analogical reasoning benchmarks (e.g., SAT analogy questions) to determine if the proposed mechanisms operate in naturalistic settings beyond the synthetic task.