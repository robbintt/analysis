---
ver: rpa2
title: Emergent LLM behaviors are observationally equivalent to data leakage
arxiv_id: '2505.23796'
source_url: https://arxiv.org/abs/2505.23796
tags:
- game
- they
- data
- player
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that emergent conventions observed in LLM-based\
  \ coordination games are not spontaneous but rather reflect data leakage\u2014models\
  \ reproducing knowledge from their training data. The authors tested multiple LLMs\
  \ with a coordination game prompt and found that most models correctly identified\
  \ the game structure, optimal post-success strategies, and likely global convergence\
  \ outcomes."
---

# Emergent LLM behaviors are observationally equivalent to data leakage

## Quick Facts
- arXiv ID: 2505.23796
- Source URL: https://arxiv.org/abs/2505.23796
- Authors: Christopher Barrie; Petter Törnberg
- Reference count: 9
- Primary result: Emergent conventions in LLM-based coordination games are data leakage from pretraining, not genuine emergence

## Executive Summary
This paper argues that emergent behaviors observed in LLM-based agent-based modeling (ABM) simulations are actually manifestations of data leakage rather than spontaneous emergence. The authors tested multiple LLMs with a coordination game prompt and found that models consistently identified game structures, optimal strategies, and convergence patterns that exist in their training data. They demonstrate that what appears to be emergent convention formation is observationally equivalent to models recalling knowledge from pretraining corpora. The study concludes that data leakage poses a fundamental challenge for generative ABM research, as off-the-shelf LLMs inevitably reflect their training data, and calls for interpretability probing and perplexity measurement as potential solutions.

## Method Summary
The study used a two-player naming game with symmetric payoff structure (+100 for matching, -50 for mismatching) across 100 rounds. Fourteen different LLM models were queried 10 times each with prompts asking them to identify the game type, optimal post-success strategies, and global convergence predictions. A GPT-4.1 annotation model with JSON schema output was used to classify responses into three dimensions: coordination game identification, optimal move knowledge, and convergence prediction. Manual verification was performed on annotation justifications using text snippets.

## Key Results
- Most tested LLMs correctly identified the coordination game structure from the prompt
- Models demonstrated knowledge of optimal post-success moves and global convergence patterns
- GPT-4.1 response explicitly recognized the payoff structure and named the "naming game" scenario
- The inventory pruning rule mechanically drives convergence independent of LLM reasoning
- Observational equivalence exists between genuine emergence and training data recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs map payoff structures to known game-theoretic concepts during prompt interpretation
- Mechanism: When presented with symmetric reward/penalty rules, models retrieve associated patterns from pretraining—including optimal strategies and convergence predictions—rather than computing novel solutions
- Core assumption: The scientific literature on coordination games appears in model training corpora
- Evidence anchors: Most tested LLMs correctly identified coordination game structure, optimal post-success moves, and global convergence patterns
- Break condition: If models failed to recognize game structure or predicted incorrect convergence dynamics

### Mechanism 2
- Claim: The inventory pruning rule creates mechanical convergence independent of LLM reasoning
- Mechanism: The hard-coded lexicon reduction after successful matches statistically advantages the matched word in subsequent rounds—LLMs need only "lock in" to an already-favored outcome
- Core assumption: Convergence claims require demonstrating LLM agency beyond following mechanically-induced probabilities
- Evidence anchors: "The inventory pruning rule is hard-coded into their simulation code... As such, it is by mechanics statistically more likely to get a success on this word/convention in subsequent pairings"
- Break condition: If convergence occurred without inventory pruning, or if LLMs chose non-advantaged options

### Mechanism 3
- Claim: Emergent behavior and memorization are observationally equivalent in LLM-based simulations
- Mechanism: Both genuine reasoning and training-data recall produce identical outputs when task structures match pretraining examples—no behavioral signature distinguishes them
- Core assumption: Contamination cannot be fully eliminated when using models trained on unknown internet-scale corpora
- Evidence anchors: GPT-4.1 response shows explicit identification of coordination game structure and optimal strategies
- Break condition: If interpretability methods could reliably distinguish recall from reasoning

## Foundational Learning

- Concept: **Data contamination in evaluation**
  - Why needed here: The paper's central argument depends on understanding how training-test overlap invalidates claims of novel behavior
  - Quick check question: Can you explain why a model correctly answering questions about a paper it was trained on doesn't demonstrate reasoning ability?

- Concept: **Coordination games and convention formation**
  - Why needed here: The naming game is a specific instantiation; understanding general coordination dynamics is prerequisite to evaluating contamination claims
  - Quick check question: In a pure coordination game with symmetric payoffs, why is "stick with what worked" the rational strategy?

- Concept: **Observational equivalence in complex systems**
  - Why needed here: The paper's philosophical contribution hinges on distinguishing internal mechanisms from external observations
  - Quick check question: If two systems produce identical outputs, what additional evidence would you need to infer different internal processes?

## Architecture Onboarding

- Component map: Prompt template -> LLM agent -> action selection -> payoff evaluation -> lexicon pruning (hard-coded) -> pairing for next round
- Critical path: 1. Design prompt that masks or obfuscates game structure, 2. Run multi-round simulation with convergence tracking, 3. Probe for contamination post-hoc using recognition questions, 4. Compare recognized vs. unrecognized game instantiations
- Design tradeoffs: Off-the-shelf LLMs offer fast deployment but unknown training data and high contamination risk; custom-trained models provide controllable training data but are expensive and may lack broad knowledge; obfuscation strategies may fail if models recognize underlying structure
- Failure signatures: Models correctly predict global convergence dynamics before completing simulation rounds; models identify game type when asked directly; convergence patterns match published research results exactly; varying payoff magnitudes doesn't change strategy
- First 3 experiments: 1. Replicate recognition probe across additional models with temperature variation, 2. Design a genuinely novel coordination mechanism not in literature and test if convergence still occurs, 3. Apply perplexity-based contamination detection to prompt templates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can technical strategies like interpretability probing or next-token perplexity measurement effectively distinguish data leakage from genuine emergence in generative ABMs?
- Basis in paper: The authors explicitly ask whether these techniques would help overcome the problems described
- Why unresolved: While these techniques exist for standard evaluation, their efficacy in verifying the validity of complex social simulations has not been established
- What evidence would resolve it: Experiments demonstrating that these metrics successfully identify memorization in LLM agents playing contaminated games versus novel scenarios

### Open Question 2
- Question: Is it possible to successfully "invent" a novel game structure that prevents LLMs from relying on structural analogies found in their training data?
- Basis in paper: The authors suggest inventing a completely novel game but note that data leakage remains intractable without strict control over training data
- Why unresolved: The vast scale of LLM pre-training corpora makes it difficult to ensure a game's structural logic is entirely absent from the model's prior knowledge
- What evidence would resolve it: The successful demonstration of emergent behaviors in a game specifically designed to violate standard game-theoretic logics common in literature

### Open Question 3
- Question: What methodological standards are required to validate claims of emergence in off-the-shelf LLMs given the impossibility of inspecting their training data?
- Basis in paper: The paper concludes that data contamination is a "fundamental challenge" and calls for this question to be at the heart of future research
- Why unresolved: Current mitigations are inadequate, leaving a vacuum for valid experimental design
- What evidence would resolve it: The adoption of a standardized framework where proposed emergent behaviors are rigorously tested against "contamination null hypotheses"

## Limitations
- The prompt variations using "nonsensical strings" are described but not detailed, making it impossible to assess obfuscation adequacy
- The binary classification threshold for "correctly predicts global convergence" is not rigorously defined
- The analysis doesn't quantify how much convergence occurs mechanically versus through LLM reasoning
- Proposed interpretability and perplexity-based solutions remain theoretical without empirical validation

## Confidence

- **High confidence**: LLMs recognize and retrieve known coordination game patterns from pretraining when presented with familiar payoff structures
- **Medium confidence**: All observed emergent behaviors in LLM-based ABMs are necessarily data leakage
- **Low confidence**: Interpretability and perplexity-based solutions as definitive contamination detection methods

## Next Checks

1. Design and test a coordination game with asymmetric information or multi-step commitments that doesn't match known literature patterns, then assess whether LLMs still exhibit the claimed emergent behaviors

2. Run simulations with and without inventory pruning across multiple models to measure the relative contribution of mechanical versus reasoning-driven convergence

3. Systematically vary prompt obfuscation techniques (unicode substitutions, novel terminology) and measure the degradation in game recognition rates to establish how much masking is needed to prevent data leakage