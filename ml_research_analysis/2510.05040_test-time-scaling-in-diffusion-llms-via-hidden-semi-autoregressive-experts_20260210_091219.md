---
ver: rpa2
title: Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts
arxiv_id: '2510.05040'
source_url: https://arxiv.org/abs/2510.05040
tags:
- block
- arxiv
- tokens
- reasoning
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies test-time scaling in diffusion-based large language
  models (dLLMs) by examining how different token unmasking orders affect performance.
  It identifies that dLLMs implicitly learn a mixture of semi-autoregressive experts
  during training, with different masking schedules revealing different specialized
  behaviors.
---

# Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts

## Quick Facts
- arXiv ID: 2510.05040
- Source URL: https://arxiv.org/abs/2510.05040
- Reference count: 40
- Primary result: HEX inference boosts GSM8K accuracy from 24.72% to 88.10% using test-time scaling

## Executive Summary
This work investigates test-time scaling in diffusion-based large language models (dLLMs) by examining how different token unmasking orders affect model performance. The authors discover that dLLMs implicitly learn a mixture of semi-autoregressive experts during training, with different masking schedules revealing different specialized behaviors. They introduce HEX (Hidden Semi-Autoregressive EXperts), a training-free inference method that ensembles across heterogeneous block schedules through majority voting, achieving state-of-the-art results on multiple reasoning benchmarks without model fine-tuning.

## Method Summary
The authors analyze how different block schedules during inference affect dLLM performance, discovering that models exhibit schedule-dependent behaviors that can be interpreted as learning implicit mixtures of semi-autoregressive experts. HEX inference works by generating predictions using multiple heterogeneous block schedules in parallel, then combining these predictions through majority voting to produce the final output. This approach leverages the model's implicit schedule-specific expertise without requiring any architectural modifications or additional training.

## Key Results
- GSM8K reasoning accuracy improved from 24.72% to 88.10% using HEX inference
- MATH benchmark performance increased from 16.40% to 40.00%
- ARC-C accuracy improved from 54.18% to 87.80%
- TruthfulQA performance increased from 28.36% to 57.46%
- HEX outperforms both top-K margin inference and GRPO fine-tuned models

## Why This Works (Mechanism)
The mechanism relies on the observation that dLLMs implicitly learn different specialized behaviors for different masking schedules during training. When using a single fixed schedule at inference time, the model is forced to commit to one specialization, potentially missing opportunities where other schedules would perform better. HEX addresses this by ensembling predictions across multiple schedules, effectively leveraging the model's latent schedule-specific expertise. The majority voting mechanism selects the most confident predictions across the schedule ensemble, compensating for the weaknesses of individual schedules.

## Foundational Learning
- **Diffusion-based language modeling**: Unlike standard autoregressive models, dLLMs use iterative denoising processes with masking schedules to generate text, requiring understanding of how masking order affects learning dynamics
- **Semi-autoregressive inference**: The concept that models can generate multiple tokens in parallel while maintaining some sequential dependencies, with different schedules creating different trade-offs between speed and accuracy
- **Test-time scaling**: The idea that inference-time techniques can improve model performance without additional training, particularly relevant for optimizing existing model capabilities
- **Majority voting ensembling**: A technique where multiple predictions are combined by selecting the most frequently occurring output, assuming that consensus indicates higher confidence and accuracy
- **Block schedule heterogeneity**: The use of different token unmasking patterns during inference, where each pattern creates different optimization pressures and specialized behaviors in the model

## Architecture Onboarding

**Component map**: Input text -> Multiple parallel dLLM forward passes with different block schedules -> Prediction aggregation via majority voting -> Final output

**Critical path**: The core inference pipeline remains the standard dLLM forward pass, but executed multiple times in parallel with different masking schedules. The majority voting step is computationally lightweight compared to the model inference itself.

**Design tradeoffs**: HEX trades increased computational cost (multiple forward passes) for improved accuracy without requiring model fine-tuning. The approach maintains model architecture integrity but may be impractical for real-time applications due to the linear scaling of inference time with the number of schedules.

**Failure signatures**: Performance degradation occurs when the majority voting mechanism selects incorrect predictions across schedules, or when all schedules perform poorly on a particular task. The method may also fail when schedule-specific expertise doesn't generalize across different reasoning types.

**3 first experiments**:
1. Run HEX with varying numbers of block schedules to determine the optimal ensemble size for accuracy vs. computational cost tradeoff
2. Compare HEX performance against single-schedule inference across different reasoning task categories to identify which schedule types excel at specific problem types
3. Analyze intermediate reasoning chains generated by different schedules to understand when and why majority voting succeeds or fails

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- HEX is training-free and doesn't modify the underlying model architecture, making it unclear if training with heterogeneous schedules from the start would provide superior or similar benefits
- The computational overhead scales linearly with the number of schedules, potentially making it impractical for very large models or real-time applications
- The majority voting mechanism assumes equal competence across all block schedules, which may not hold for all tasks or model scales

## Confidence
- Claim that dLLMs implicitly learn semi-autoregressive experts: **Medium** - supported by empirical evidence but alternative explanations exist
- Claim that single fixed schedules collapse performance: **High** - well-demonstrated across multiple tasks and models
- Claim that HEX establishes a new paradigm for test-time scaling: **Medium** - represents a novel approach but training-free methods may have fundamental limitations
- Claim that HEX outperforms GRPO fine-tuned models: **High** - clear quantitative improvements on benchmark tasks

## Next Checks
1. Train a dLLM from scratch using heterogeneous block schedules during training, then compare its performance to HEX on the same tasks to determine if training-time schedule diversity provides similar or superior benefits.

2. Analyze the reasoning chains generated by HEX across different block schedules to identify whether the majority voting mechanism consistently selects higher-quality intermediate steps, or if it sometimes produces coherent-looking but incorrect reasoning.

3. Measure the wall-clock time and memory requirements of HEX inference across different model scales (7B, 13B, 70B parameters) and determine the point at which the computational overhead negates the performance benefits, particularly for real-time or resource-constrained applications.