---
ver: rpa2
title: 'Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled
  Reward Machines'
arxiv_id: '2510.27329'
source_url: https://arxiv.org/abs/2510.27329
tags:
- agent
- states
- reward
- numeric
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces coupled reward machines (RMs) to address
  the scalability challenges of long-horizon tasks with unordered subtasks. The authors
  propose three RM generalizations: numeric RMs for compact task descriptions, agenda
  RMs that associate states with remaining subtasks, and coupled RMs that allow parallel
  learning of subtask-specific policies.'
---

# Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines

## Quick Facts
- arXiv ID: 2510.27329
- Source URL: https://arxiv.org/abs/2510.27329
- Reference count: 5
- Primary result: Introduces coupled reward machines to scale RL for long-horizon unordered tasks from exponential to linear complexity

## Executive Summary
This paper addresses the scalability challenges of reinforcement learning for long-horizon tasks with unordered subtasks. Traditional reward machines grow exponentially with the number of subtasks, making them impractical for complex tasks. The authors propose coupled reward machines (CoRM) that split states by individual subtasks and maintain them in parallel, enabling concurrent policy updates. CoRM learns low-level policies for each subtask plus a high-level ordering policy based on expected path lengths, demonstrating superior convergence speed and scalability compared to state-of-the-art reward machine algorithms.

## Method Summary
The method introduces coupled reward machines that generalize Boolean reward machines by splitting states with the same label into parallel states associated with each subtask. CoRM learns multiple Q-value functions in parallel (one per subtask) and uses an episode-length-dependent final reward to align low-level policies with global optimality. The high-level policy selects completion order based on minimum steps to goal states, with exploration for discovering better orderings. The approach handles numeric reward machines by translating them to Boolean form, then labeling states with agendas to form coupled states.

## Key Results
- CoRM scales learning complexity from exponential to linear in the number of unordered subtasks
- Parallel Q-learning across coupled RM states enables concurrent policy updates without waiting for sequential task completion
- Episode-length-dependent final rewards align low-level policies with global optimality
- Experimental results on Delivery, Office, and Water domains show CoRM outperforms state-of-the-art RM algorithms in convergence speed and scalability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coupled RMs reduce learning complexity from exponential to linear in the number of unordered subtasks.
- Mechanism: Instead of tracking all permutations of task completion (which creates |U| states that grows factorially), coupled RMs split states by individual subtasks and maintain them in parallel. The agent learns a low-level policy for each subtask τ ∈ T (the agenda) plus Boolean objectives B, yielding |T ∪ B| policies rather than one monolithic policy over S × U.
- Core assumption: Subtasks are independent enough that learning their policies separately preserves global optimality when combined with appropriate high-level ordering and episode-length-aware rewards.
- Evidence anchors:
  - [abstract] "the amount of information to learn increases exponentially with the number of unordered subtasks... CoRM scales better than state-of-the-art RM algorithms"
  - [Page 2] "states with the same label correspond to one Q-value function and are thus symmetric and can be reduced to one state"
  - [Page 5] "the amount of low-level information to learn increases only linearly with |U′|. In contrast... exponential in the number of unordered subtasks"
  - [corpus] Related work on sparse-reward RL (DISCOVER, arXiv:2505.19850) confirms scalability challenges in long-horizon tasks, but does not directly address RM-based decomposition.
- Break condition: If subtasks have strong interdependencies (e.g., completing τ₁ changes the dynamics of τ₂), linear decomposition may fail to capture global structure.

### Mechanism 2
- Claim: Parallel Q-learning across coupled RM states enables concurrent policy updates without waiting for sequential task completion.
- Mechanism: When the agent is in environment state s with agenda T = {τ₁, τ₂, ..., τ_N}, it exists simultaneously in all coupled RM states ⟨d, T, τ_k⟩. Equation 3 performs Q-updates for each τ_k in parallel. Upon completing τ_ℓ, Equation 4 applies the episode-length-dependent final reward, and the experience buffer stores the RM transition for later learning.
- Core assumption: The agent can track multiple RM states concurrently without interference, and the labeling function provides ground-truth subtask completion signals.
- Evidence anchors:
  - [abstract] "Coupled RMs have coupled states associated with each subtask in the agenda"
  - [Page 5] "As long as the agent does not transition to the next RM state, the information about how to complete each subtask τ_k is updated concurrently"
  - [Page 5] "the agent learns low-level policies to locate box 1, box 2, and the station"
  - [corpus] No direct corpus evidence on parallel RM-state learning; most related work (e.g., Pushdown RMs, arXiv:2508.06894) focuses on expressiveness rather than parallelism.
- Break condition: If labeling is noisy or delayed, parallel updates may reinforce incorrect associations.

### Mechanism 3
- Claim: Episode-length-dependent final rewards align low-level policies with global optimality.
- Mechanism: Instead of constant reward on subtask completion, R(K) = γ^(ΔK+1) + R(K^x) (Eq. 11) penalizes longer episodes relative to K_min. This ensures that locally slower policies are preferred if they enable globally shorter solutions. The window ΔK_w prevents reward collapse for large ΔK.
- Core assumption: The environment is deterministic enough that K_min meaningfully reflects optimal behavior, and intermediate rewards are bounded (r_min, r_max).
- Evidence anchors:
  - [Page 5–6] Derivation shows that without R(K) dependence, locally optimal π₁ with K₁ > K_min would be preferred over locally sub-optimal but globally optimal π₂ with K₂ = K_min
  - [Page 6] "Without this global context expressed in the episode-length signal, the low-level policies would be learnt in isolation"
  - [Page 7] Water domain ablation: CoRM-0 (without joint optimization) "converges to sub-optimal policies"
  - [corpus] LSTS (arXiv:2509.14380) uses sampling-based guidance but does not prove optimality; CoRM claims optimality in deterministic settings.
- Break condition: In stochastic environments, K_min may not reflect expected optimal cost; future work suggests using expected steps instead.

## Foundational Learning

- Concept: **Tabular Q-learning and the exploration-exploitation tradeoff**
  - Why needed here: CoRM builds directly on Q-learning updates (Eqs. 2–4). Understanding how Q-values converge, why infinite visitation is required, and how ε-greedy exploration works is essential before adding RM structure.
  - Quick check question: Can you explain why Q-learning convergence requires visiting all (s, a) pairs infinitely often, and what happens if the reward function changes mid-training?

- Concept: **Reward Machines as task-structure encodings (Boolean RMs from Icarte et al. 2022)**
  - Why needed here: This paper generalizes Boolean RMs. You need to understand how δ_u (state-transition function) and δ_r (reward function) operate on propositions, and how an MDPRM extends the state space to S × U.
  - Quick check question: Given an RM with states u₀ → u₁ → u₂ (terminal), what happens to Q-learning if two different paths through the RM reach u₂ with different cumulative rewards?

- Concept: **Hierarchical decomposition and options framework**
  - Why needed here: CoRM learns low-level policies (one per subtask) and a high-level ordering policy. This mirrors the options framework (Sutton & Barto), though CoRM's ordering is based on η_u (minimum steps to goal) rather than option-value functions.
  - Quick check question: How does learning separate policies for "reach box 1" and "reach box 2" differ from learning a single policy for "collect all boxes in any order" in terms of sample complexity?

## Architecture Onboarding

- Component map:
  - Numeric RM (user input) → translated to Boolean RM → labeled with agendas to form Agenda RM → states with x=T split into Coupled RM
  - CoRM agent maintains: Q-table (or DDQN network) over S × U′, η-table for minimum steps per RM state, temporary buffer for RM-transition experiences
  - High-level controller reads η values from coupled states, selects subtask with minimum η (or explores with probability ξ)
  - Low-level learners perform parallel Q-updates (Eq. 3) until RM transition, then apply final reward R(K) (Eq. 4)

- Critical path:
  1. Define task as Numeric RM with discrete bounded variables (Assumption 1)
  2. Translate to Boolean RM via unrolling (Supplementary Material algorithm)
  3. Label each state with ⟨d, T, x⟩; merge symmetric states with same label
  4. Split states with x=T into coupled states
  5. Initialize Q(s, τ) for all τ ∈ T ∪ B and η_u = ∞ for all RM states
  6. At each step: observe environment, update all coupled-state Q-values, check RM transitions
  7. On RM transition: compute R(K) at episode end, update buffer experiences
  8. High-level decision: with prob (1-ξ), select τ with minimum η; else explore unused transitions

- Design tradeoffs:
  - **ΔK_w selection**: Larger windows preserve optimality but slow convergence; smaller values risk sub-optimality. Paper uses ΔK_w = -1 for independent tabular tasks (reward=1 always) and ΔK_w = 80 for Water domain
  - **Exploration ξ**: Paper uses ξ=0.1. Too high wastes samples on bad orderings; too low may miss optimal orderings
  - **Buffer scaling**: CoRM scales buffer by |U′| (number of subtasks), CRM scales by |U| (exponential). Memory advantage is linear vs. exponential
  - **Assumption 4**: Assumes unordered subtasks. If tasks must be ordered, numeric-variable encapsulation doesn't apply directly (noted as future work)

- Failure signatures:
  - **Exponential memory blowup**: If you're using Boolean RMs instead of coupled RMs, state count explodes (e.g., Delivery with >5 boxes runs out of memory per Page 7)
  - **Convergence to sub-optimal policies**: If ΔK_w = -1 but subtasks interact (e.g., Water domain), agent learns locally optimal behaviors. Use ablation to detect
  - **No improvement over CRM**: Check if labeling function correctly identifies subtask completion; incorrect labels break the agenda tracking
  - **η values not updating**: Exploration bias towards unused transitions should fill η-table; if stuck, increase ξ or check RM connectivity

- First 3 experiments:
  1. **Delivery domain, 2 boxes**: Implement tabular CoRM with ΔK_w = -1. Verify that Q-tables for τ∈{box1, box2, station} converge and η values reflect shortest paths. Compare steps-to-convergence against CRM (should see ~2-3x speedup per Fig 3a)
  2. **Ablation on Water domain (3 balls)**: Run CoRM with ΔK_w = -1 vs. ΔK_w = 80 using DDQN. Confirm that CoRM-0 (no joint optimization) converges to lower average reward per step (Fig 3e). This validates the episode-length mechanism
  3. **Scalability stress test**: Run Delivery with 4, 6, 8 boxes. Measure runtime and memory. Confirm linear scaling for CoRM vs. exponential for B-CRM (Fig 3g). If CoRM runtime scales exponentially, check that state merging is correctly implemented

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CoRM framework be extended to handle stochastic environments where transition outcomes are non-deterministic?
- Basis in paper: [explicit] The authors state in the conclusion: "To extend the CoRM algorithm to stochastic environments, we aim to base its high-level policy on the expected number of steps to reach a goal state, rather than the shortest observed path."
- Why unresolved: The current high-level decision-making relies on $\eta_u$, the minimum observed steps to a goal. This deterministically minimizes path length but does not account for transition probabilities or variance in stochastic settings.
- What evidence would resolve it: A modified CoRM algorithm that utilizes expected steps or transition probabilities in its high-level policy, demonstrating convergence in stochastic MDPs.

### Open Question 2
- Question: Is it possible to encapsulate ordered or sequential subtasks within the numeric variables of a Coupled Reward Machine without losing the benefits of compact representation?
- Basis in paper: [explicit] In Section 3.2, the text notes: "Encapsulation of ordered tasks into a numeric variable is a useful extension to be realised in future work."
- Why unresolved: The current methodology (Assumption 4) restricts numeric variables to collections of subtasks that can be completed in any order to facilitate the "coupled" state structure.
- What evidence would resolve it: A formal definition and empirical demonstration of Numeric RMs that can process partially ordered constraints while maintaining the linear scaling properties of CoRM.

### Open Question 3
- Question: Do the subtask-specific policies learned by CoRM generalize effectively to new domains or distinct task instances?
- Basis in paper: [explicit] The conclusion lists investigating "the generalisation capabilities of coupled RMs across different domains" as a primary direction for future work.
- Why unresolved: The experimental results focus on sample efficiency and convergence speed within fixed training environments (Delivery, Office, Water), leaving transfer learning performance unexplored.
- What evidence would resolve it: Experiments showing that low-level policies ($Q$-values) learned in one environment can successfully initialize or accelerate learning in a different environment with shared subtasks.

## Limitations

- The linear vs. exponential scaling claim assumes subtasks are truly independent; interdependencies could break the coupled RM decomposition
- Episode-length-dependent rewards assume deterministic environments where K_min is well-defined; stochastic dynamics may invalidate this optimality guarantee
- The translation from numeric to Boolean RMs requires "discrete bounded variables" (Assumption 1); handling continuous or unbounded variables remains future work

## Confidence

- Scalability improvements (linear vs exponential): **High** - supported by experimental data and clear theoretical reduction in state space
- Optimality guarantees under coupled RMs: **Medium** - proven for deterministic cases but untested in stochastic environments
- Parallel Q-learning convergence: **Medium** - the mechanism is sound but lacks empirical validation of convergence rates compared to sequential updates

## Next Checks

1. Test CoRM on stochastic versions of the Water domain where K_min varies across episodes; measure degradation in performance
2. Implement a variant with coupled subtasks (e.g., need to collect box1 before box2) and verify that coupled RMs fail gracefully or require modification
3. Profile memory usage and convergence time for CoRM vs CRM on tasks with 10+ unordered subtasks to confirm asymptotic scaling claims