---
ver: rpa2
title: Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning
arxiv_id: '2507.15788'
source_url: https://arxiv.org/abs/2507.15788
tags:
- training
- tasks
- reasoning
- performance
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether small-scale LLMs can develop a generalizable
  Theory of Mind (ToM) through reinforcement learning with verifiable rewards (RLVR).
  The authors systematically train models on combinations of three prominent ToM datasets
  (HiToM, ExploreToM, FANToM) and evaluate their zero-shot performance on held-out
  benchmarks (OpenToM, FANToM List, and higher-order HiToM tasks).
---

# Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.15788
- Source URL: https://arxiv.org/abs/2507.15788
- Authors: Sneheel Sarangi; Hanan Salam
- Reference count: 5
- Primary result: RLVR improves in-distribution ToM performance up to 65% but fails to generalize to held-out tasks

## Executive Summary
This paper systematically evaluates whether small-scale LLMs can develop generalizable Theory of Mind (ToM) capabilities through reinforcement learning with verifiable rewards (RLVR). Training models on combinations of three prominent ToM datasets shows dramatic in-distribution gains (up to 65% accuracy) but complete failure to transfer to held-out benchmarks. The authors demonstrate that prolonged RL training leads to "hacking" of statistical patterns in training data, producing inverted difficulty curves where models perform better on harder, unseen tasks than trained ones. This indicates the learned behavior is narrow overfitting rather than acquisition of true, abstract ToM capability, challenging the effectiveness of RLVR for instilling generalizable social reasoning in smaller models.

## Method Summary
The authors train Qwen2.5-7B-Instruct models using REINFORCE++ (a variant of REINFORCE with batch-normalized rewards as baseline, no critic) on three ToM datasets: HiToM (order-1 to order-3 belief reasoning), FANToM (binary false-belief classification), and ExploreToM (narrative-based ToM). Models receive structured format rewards (0.1) for proper output tags and correctness rewards (1.0) for matching ground truth. Seven training configurations are evaluated, with zero-shot transfer to held-out benchmarks (OpenToM, FANToM List, higher-order HiToM). Performance is measured across 12 tasks, focusing on the generalization gap between in-distribution and out-of-distribution accuracy.

## Key Results
- RLVR achieves up to 65% in-distribution accuracy gains (FANToM) but <5% improvement on held-out tasks
- Prolonged RL training produces "inverted difficulty curves" where models perform better on unseen, harder tasks than trained ones
- Single-order training causes negative transfer, with Order-4-only training collapsing Order-1 performance by 31 points
- Models fail to learn flexible ToM representations that transfer across reasoning orders or dataset formats

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RLVR produces dramatic in-distribution gains by optimizing verifiable rewards through policy gradient updates on a 7B parameter model.
- **Mechanism:** The REINFORCE++ algorithm normalizes rewards across training batches and uses this as a baseline for policy gradient estimation. Models receive format rewards (0.1) for structured output with thinking tags and correctness rewards (1.0) for matching ground truth answers. This creates strong optimization pressure toward task-specific patterns.
- **Core assumption:** Reward signals accurately capture the target capability rather than surface-level patterns (Assumption: this is what the paper investigates and finds to be false).
- **Evidence anchors:**
  - [abstract] "performance on in-distribution tasks improves, this capability fails to transfer to unseen ToM tasks with different characteristics"
  - [Section 4.2.1] "Models trained on FANToM showed the largest improvements, outperforming the baselines by 65%"
  - [corpus] Related work (Logic-RL, DeepSeek-R1) showed RLVR success in logical reasoning domains, motivating this ToM application
- **Break condition:** If training data contains spurious correlations between surface features and correct answers, RL will exploit these rather than learning transferable reasoning.

### Mechanism 2
- **Claim:** Prolonged RL training causes models to "hack" statistical artifacts in training data, producing inverted difficulty curves that paradoxically show better performance on harder, unseen tasks.
- **Mechanism:** The HiToM dataset uses templated generation where structural artifacts become more pronounced in higher-order examples. When trained on Orders 1-3, the model achieves 94.2% on unseen Order 4 tasks—higher than trained orders—indicating exploitation of predictive patterns rather than cognitive mastery.
- **Core assumption:** Genuine ToM capability should show monotonically decreasing performance with increasing reasoning order (Assumption: based on human cognitive load patterns).
- **Evidence anchors:**
  - [abstract] "prolonged RL training leads to models 'hacking' the statistical patterns of the training datasets"
  - [Section 5] "the RL-trained model inverted this difficulty curve, performing best on the unseen and most complex fourth-order task"
  - [corpus] Prior work (Shapira et al., 2023; Ullman, 2023) warned that benchmark performance may reflect shortcuts rather than genuine capability
- **Break condition:** If training data is procedurally generated with consistent templates, statistical exploitation becomes tractable for small models.

### Mechanism 3
- **Claim:** Single-order training creates specialized, conflicting strategies that produce negative transfer across reasoning orders.
- **Mechanism:** Training exclusively on Order 4 tasks drops Order 1 performance to 35.0% (31-point collapse from baseline), while Order 2-only training yields 76.7% on Order 3 but only 41.7% on Order 1. Each specialization learns order-specific heuristics incompatible with other complexity levels.
- **Core assumption:** Transferable ToM should improve or maintain baseline performance across related tasks (Assumption: standard transfer learning expectation).
- **Evidence anchors:**
  - [Section 4.2.3] "training only on O4, which drops O1 performance to 35.0%, a nearly 31-point collapse from the baseline"
  - [Section 5] "the model did not learn a flexible internal representation of the characters' beliefs that could be queried in different ways"
  - [corpus] Corpus lacks direct evidence on multi-order training dynamics; this paper provides novel negative transfer evidence
- **Break condition:** Joint training across multiple orders (1, 2, 3) maintains performance while enabling some generalization, suggesting curriculum composition matters critically.

## Foundational Learning

- **Concept: Theory of Mind (ToM) orders**
  - **Why needed here:** The paper evaluates 1st through 4th-order belief reasoning (e.g., "A believes that B believes that C thinks..."). Understanding this hierarchy is essential for interpreting the inverted difficulty curve results.
  - **Quick check question:** Can you explain why 4th-order ToM is cognitively harder than 1st-order for humans?

- **Concept: Distribution shift and OOD generalization**
  - **Why needed here:** The core finding is that in-distribution gains (up to 65%) don't transfer to held-out datasets (OpenToM, FANToM List). Distinguishing ID vs OOD evaluation is critical.
  - **Quick check question:** If a model achieves 90% on training distribution but 45% on held-out tasks from the same domain, what does this suggest about learned representations?

- **Concept: Policy gradient methods (REINFORCE variants)**
  - **Why needed here:** REINFORCE++ eliminates the critic model from PPO, using batch-normalized rewards as baseline. Understanding this simplification helps explain the training dynamics.
  - **Quick check question:** Why might removing a learned value baseline affect optimization stability or convergence behavior?

## Architecture Onboarding

**Component map:**
Training Datasets (HiToM, FANToM, ExploreToM) -> Data Curation (900 train, 300 val, 300 test per dataset) -> Qwen2.5-7B-Instruct (base model) -> REINFORCE++ (no critic, batch-normalized rewards) -> Reward Function: S_format (0.1) + S_correct (1.0) -> Evaluation: 12 tasks across 4 benchmarks (3 held-out)

**Critical path:**
1. **Data preparation:** Ensure 70% ToM-requiring / 30% simpler tasks ratio (ExploreToM), include factual questions to reduce spurious policy learning
2. **Training loop:** 10 epochs, batch size 8, 8 rollouts, learning rate 5e-7, temperature 0.6
3. **Checkpoint selection:** Pick best-performing on validation set (not final epoch)
4. **Evaluation protocol:** Test on ALL 12 tasks including training distributions to measure generalization gap

**Design tradeoffs:**
- **REINFORCE++ vs PPO:** Simpler, less compute, but no learned baseline may increase variance
- **Binary classification training (FANToM) vs list-format evaluation:** Enables verifiable rewards during training but creates format mismatch at evaluation
- **Joint vs single-dataset training:** Joint training (Hi-Fan-Exp) achieves strong in-distribution but still fails OOD; specialization gives 65% gains on single dataset

**Failure signatures:**
- In-distribution accuracy rising while OOD stays flat (Figure 2 divergence pattern)
- Inverted difficulty curves (higher accuracy on unseen Order 4 than trained Order 1)
- Performance collapse on format variations (binary → list tasks within same dataset)
- Negative transfer: training on dataset A reduces performance on dataset B below baseline

**First 3 experiments:**
1. **Replicate single-dataset training:** Train on FANToM only, evaluate on FANToM test + FANToM List + OpenToM. Confirm 65% in-distribution gain with <5% OOD improvement.
2. **Ablate training order composition:** Train separate models on {O1}, {O4}, {O1,O2}, {O1,O2,O3} using HiToM, then evaluate all on O1-O4. Verify negative transfer from O4-only and inverted curve from O1-O3.
3. **Probe early stopping:** Check if OOD generalization exists at epoch 1-3 before diverging from ID performance. Plot ID vs OOD accuracy per epoch to identify if there's a generalization window that closes with overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model scale enable the emergence of generalizable Theory of Mind via RLVR, or is the observed overfitting a fundamental limitation of the method regardless of size?
- Basis in paper: [explicit] The authors conclude that "for small LLMs," RLVR fails to induce abstract ToM, contrasting this with the success of larger models like DeepSeek-R1 in the logical reasoning domain mentioned in the introduction.
- Why unresolved: The study is restricted to a 7B parameter model, leaving the performance ceiling for larger architectures (e.g., 70B+) untested.
- What evidence would resolve it: Replicating the exact RLVR training regimen on significantly larger models to observe if the generalization gap between in-distribution and out-of-distribution tasks narrows.

### Open Question 2
- Question: Can reward mechanisms be designed to assess reasoning fidelity rather than just outcome correctness to prevent models from "hacking" statistical patterns?
- Basis in paper: [explicit] The conclusion suggests that developing socially intelligent AI will require "novel reward mechanisms that can assess the fidelity of the reasoning process itself."
- Why unresolved: The current study utilized a binary reward based on final answer correctness, which inadvertently incentivized the model to exploit dataset artifacts (reward hacking) rather than learn the underlying cognitive principles.
- What evidence would resolve it: Implementing a process-based reward model (PRM) that penalizes logically invalid intermediate steps even if the final answer is correct, and checking for improved OOD generalization.

### Open Question 3
- Question: Is the "inverted difficulty curve" phenomenon specific to the templated structure of synthetic datasets like HiToM, or does it occur in naturalistic social reasoning data?
- Basis in paper: [inferred] The authors posit that the paradoxical performance gain on higher-order tasks was due to the model exploiting "structural artifacts in the templated HiToM data."
- Why unresolved: It is unclear if the RL agent's ability to hack the benchmark relies on the repetitive statistical structure of synthetic templates or if similar shortcuts exist in naturalistic dialogue.
- What evidence would resolve it: conducting the same variable-order training experiments on purely naturalistic, non-templated ToM narratives to see if the difficulty curve normalizes or remains inverted.

## Limitations
- Results are limited to 7B parameter models, leaving open whether larger models can develop generalizable ToM capabilities
- Datasets may share underlying structural similarities that limit true distribution shift assessment
- REINFORCE++'s simplicity (no critic, batch-normalized rewards) may be suboptimal compared to more sophisticated RL methods
- The inverted difficulty curve phenomenon may be specific to templated synthetic datasets rather than naturalistic data

## Confidence
- **High confidence:** The core finding that RLVR improves in-distribution performance while failing to generalize to held-out tasks is well-supported by systematic evaluation across 12 tasks and 7 training configurations.
- **Medium confidence:** The mechanism of "statistical hacking" through inverted difficulty curves is plausible but could be confounded by dataset-specific artifacts rather than general model behavior.
- **Medium confidence:** The negative transfer results from single-order training are robust within the study's controlled setting but may not extend to more naturalistic, curriculum-based training approaches.

## Next Checks
1. **Dataset independence validation:** Conduct ablation studies with synthetic ToM tasks generated using different templates and narrative structures to ensure the negative transfer results aren't artifacts of procedural generation patterns.
2. **Scale sensitivity analysis:** Repeat the experiments with 13B and 33B parameter models to determine whether model scale affects the ability to develop generalizable ToM capabilities versus statistical pattern exploitation.
3. **Alternative RL algorithm comparison:** Implement PPO or proximal policy optimization with learned value baselines to test whether the simplicity of REINFORCE++ contributes to the observed "hacking" behavior or if similar results emerge with more sophisticated algorithms.