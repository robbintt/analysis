---
ver: rpa2
title: When Agents Break Down in Multiagent Path Finding
arxiv_id: '2508.03777'
source_url: https://arxiv.org/abs/2508.03777
tags:
- agents
- agent
- turn
- schedule
- vertex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multiagent Path Finding with Malfunctioning
  Agents (MAPFMA) problem, which extends the classical MAPF problem by allowing agents
  to experience delays due to malfunctions. The authors show that centralized adaptation
  of schedules is computationally intractable, as deciding whether a given schedule
  can be adapted without increasing makespan is NP-hard even for planar graphs.
---

# When Agents Break Down in Multiagent Path Finding

## Quick Facts
- arXiv ID: 2508.03777
- Source URL: https://arxiv.org/abs/2508.03777
- Reference count: 26
- Key outcome: NP-hard to adapt centralized schedules to preserve makespan; distributed protocols achieve makespan increase ≤ k for k malfunctions

## Executive Summary
This paper studies Multiagent Path Finding with Malfunctioning Agents (MAPFMA), where agents can experience unexpected delays during execution. The authors prove that centralized adaptation of schedules to preserve the original makespan is NP-hard, even for planar graphs. To address this, they propose two distributed protocols—CBM and CCBM—that adapt schedules locally without global replanning, guaranteeing a makespan increase of at most k turns after k malfunctions.

## Method Summary
The paper introduces two distributed protocols for adapting MAPF schedules when agents malfunction. The Check Before Moving (CBM) protocol uses local agent-state checking where delayed agents have priority, while the Check Counter Before Moving (CCBM) protocol shifts coordination to network nodes via vertex counters. Both protocols ensure feasibility and bound makespan increase to ℓ+k after k malfunctions, avoiding the NP-hardness of centralized makespan-preserving adaptation.

## Key Results
- Centralized schedule adaptation preserving original makespan is NP-hard (Theorem 1)
- CBM protocol achieves makespan increase ≤ k turns for k malfunctions via local priority-based resolution
- CCBM protocol achieves same bound by encoding schedule order into vertex counters, eliminating agent-side computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prioritizing delayed agents in local conflict resolution bounds makespan increase to at most k turns after k malfunctions, without global replanning.
- Mechanism: CBM's "modification phase" checks if target vertex is "healthy" (unoccupied by delayed agent). On-time agents encountering "unhealthy" vertex perform delay-1 and switch to delayed state permanently. Delayed agents always have priority, preventing cascading delays.
- Core assumption: Agents can reliably observe neighbor status in real-time.
- Evidence anchors:
  - [abstract] "We prove that following our primary communication protocol, the increase in makespan after k malfunctions is bounded by k additional turns."
  - [section 4.1, Theorem 2] Proves CBM produces feasible schedule of length ℓ+1 after one malfunction, and no agent delays more than once.
- Break condition: Protocol fails if agents cannot determine neighbor status or if delays exceed designed-for duration.

### Mechanism 2
- Claim: Shifting coordination to network nodes via vertex counters achieves same makespan bound without requiring agent-side computation.
- Mechanism: CCBM stores counting function at each vertex tracking agent passage. Agents carry expected counter values. Before moving, agent checks if destination counter matches expected value; if not, it delays.
- Core assumption: Infrastructure nodes maintain persistent, atomically-readable state.
- Evidence anchors:
  - [abstract] "shifts the necessary computations onto the network's nodes, ensuring robustness without requiring enhanced agent processing power"
  - [section 5, Theorem 3] Proves CCBM produces feasible schedule of length ℓ+k after k malfunction-1 operations.
- Break condition: Protocol fails if counters are corrupted, non-atomic, or agents' expected-counter lists desynchronize.

### Mechanism 3
- Claim: Centralized schedule adaptation preserving makespan is computationally intractable, motivating distributed approaches.
- Mechanism: Theorem 1 reduces 3-SAT to schedule-adaptation decision problem. Single malfunction cascades through variable-selection, assignment-verification, and clause-verification agents. Schedule of length ℓ exists iff 3-SAT formula is satisfiable.
- Core assumption: Hardness applies to makespan preservation decision problem.
- Evidence anchors:
  - [section 3, Theorem 1] "It is NP-hard to decide if there exists a set of delay-1 operations... so that the resulting schedule is of length ℓ."
  - [section 3, proof sketch] Details 3-SAT reduction and agent roles.
- Break condition: Hardness claim specific to makespan preservation; does not preclude efficient heuristics tolerating makespan increase.

## Foundational Learning

- Concept: Makespan vs. Sum-of-Costs in MAPF
  - Why needed here: Paper's guarantees are in terms of makespan (time until all agents arrive), not sum-of-costs (total agent travel time). Confusing these leads to misapplying bounds.
  - Quick check question: If 3 agents need 5, 7, and 10 turns to reach goals, what is the makespan? What is the sum-of-costs?

- Concept: Delay-1 Operation (Formal Definition)
  - Why needed here: Paper defines this precisely (Section 2): agents follow original schedule but spend one extra turn at vertex. This is not merely "waiting" but formal schedule transformation. Essential for correct protocol implementation.
  - Quick check question: If agent a's original schedule visits vertices [v1, v2, v3, v4] and performs delay-1 at turn 2, what is its new schedule for turns 2-5?

- Concept: Distributed vs. Centralized Coordination Tradeoffs
  - Why needed here: Paper's core thesis is that centralized adaptation is intractable, so distributed protocols with local information are necessary. Understanding what each agent must know clarifies protocol design constraints.
  - Quick check question: In CBM, what is maximum number of hops away an agent must query to handle k malfunctions? How does CCBM eliminate this requirement?

## Architecture Onboarding

- Component map:
  - Agent Module: State (delayed/on-time or counter-list), local sensor for adjacent vertices, decision logic for delay-1 vs. move, motion executor
  - Infrastructure Module (CCBM only): Vertex counter storage, atomic read/write interface, optional counter reset mechanism
  - Communication Substrate: Reliable local broadcast (CBM) or reliable read (CCBM). No global broadcast required
  - Initial Schedule Provider: Pre-computed feasible schedule σ (makespan ℓ) distributed to all agents before execution

- Critical path:
  1. Pre-deployment: Compute and distribute initial feasible schedule σ (makespan ℓ) using any MAPF solver
  2. Protocol selection: Choose CBM (agent-centric) or CCBM (infrastructure-centric) based on agent compute capabilities and vertex infrastructure availability
  3. Runtime loop (each turn): Decision phase (all agents decide delay-1 or proceed), Modification phase (CBM: check neighbor states, update state if needed), Action phase (move according to updated schedule)
  4. Malfunction handling: When agent malfunctions (delay-1 forced), protocol automatically propagates delays through system

- Design tradeoffs:
  - CBM: No infrastructure changes required. Agents must scan radius-k neighborhood for k-turn delays. Communication overhead grows with k. Suitable for systems with capable agents and limited infrastructure
  - CCBM: Requires instrumented vertices with persistent counters. Agents only query adjacent vertex. More robust to larger k, but infrastructure-dependent. Suitable for warehouse/controlled environments
  - Upper bound design: If maximum possible delay d_max exceeds designed-for k, protocol guarantees break. Must over-provision k or implement fallback (global pause/replan)

- Failure signatures:
  - Collision: Two agents occupy same vertex → suspect counter corruption (CCBM) or neighbor-status misread (CBM). Check communication/sensor reliability
  - Deadlock: Agent delays indefinitely → suspect cycle of delayed agents with identical priority (CBM) or counter mismatch beyond k delays (CCBM). Check if actual delays exceed designed k
  - Makespan >> ℓ+k: Suspect multiple independent malfunctions or protocol implementation error causing unnecessary cascading delays

- First 3 experiments:
  1. Validation on simple graphs (e.g., Figure 1 from paper): Implement both protocols, induce single delay-1, verify makespan = ℓ+1 and no collisions. Baseline confirmation
  2. Scaling with k: On grid graph, systematically increase malfunction-1 operations from 1 to 10. Verify makespan bound ℓ+k holds. Profile communication overhead (CBM) vs. counter-read cost (CCBM)
  3. Adversarial delay timing: Test worst-case delay timing (adversarial model from paper) vs. random delay timing. Compare makespan increase to validate theoretical bound holds in both cases but is tighter under random delays

## Open Questions the Paper Calls Out

- Can heuristic algorithms be developed that effectively circumvent the NP-hardness of centralized schedule adaptation proved in Theorem 1?
- How do the proposed protocols perform if malfunctions follow a probabilistic distribution rather than an adversarial model?
- Can the protocols be modified to guarantee safety and liveness when a single agent malfunctions for a duration exceeding the pre-defined limit k?
- What is the impact of the CCBM protocol on the total energy (flowtime) compared to the optimal solution, rather than just the makespan?

## Limitations

- Theoretical guarantees depend on reliable agent-to-agent communication (CBM) or atomic counter operations (CCBM), which are not validated experimentally
- Protocols assume pre-computed schedules and do not address schedule regeneration when malfunction count exceeds parameter k
- No experimental validation of practical performance or comparison to centralized replanning under realistic failure scenarios

## Confidence

- **High Confidence**: NP-hardness proof for centralized adaptation is rigorous and follows standard reduction techniques. Makespan bound of k additional turns is formally proven
- **Medium Confidence**: Distributed protocols are correctly specified and their theoretical bounds are proven, but practical implementation details and robustness are not addressed
- **Low Confidence**: Claims about scalability and practical performance compared to centralized replanning lack experimental support

## Next Checks

1. Implement both CBM and CCBM protocols and validate on simple test cases with single and multiple malfunctions, confirming makespan bounds and collision-free operation

2. Analyze infrastructure requirements for CCBM by simulating counter corruption or race conditions and measuring impact on protocol guarantees

3. Profile communication overhead for CBM as a function of malfunction count and graph topology, comparing to periodic centralized replanning costs for practical scalability assessment