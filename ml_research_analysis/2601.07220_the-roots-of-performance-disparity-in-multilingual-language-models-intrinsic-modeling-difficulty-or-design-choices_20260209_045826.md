---
ver: rpa2
title: 'The Roots of Performance Disparity in Multilingual Language Models: Intrinsic
  Modeling Difficulty or Design Choices?'
arxiv_id: '2601.07220'
source_url: https://arxiv.org/abs/2601.07220
tags:
- language
- languages
- linguistics
- computational
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The survey shows that cross-linguistic performance gaps in multilingual\
  \ language models arise primarily from modeling artifacts\u2014such as tokenization\
  \ fragmentation, byte-encoding inefficiencies, and imbalanced data sampling\u2014\
  rather than intrinsic linguistic difficulty. Normalization of segmentation, encoding,\
  \ and exposure can substantially reduce these gaps."
---

# The Roots of Performance Disparity in Multilingual Language Models: Intrinsic Modeling Difficulty or Design Choices?

## Quick Facts
- arXiv ID: 2601.07220
- Source URL: https://arxiv.org/abs/2601.07220
- Authors: Chen Shani; Yuval Reif; Nathan Roll; Dan Jurafsky; Ekaterina Shutova
- Reference count: 40
- Primary result: Cross-linguistic performance gaps in multilingual language models arise primarily from modeling artifacts—such as tokenization fragmentation, byte-encoding inefficiencies, and imbalanced data sampling—rather than intrinsic linguistic difficulty.

## Executive Summary
This survey paper systematically examines why multilingual language models exhibit performance disparities across languages. Through comprehensive analysis of 40+ papers, the authors identify that gaps often shrink when segmentation, encoding, and data exposure are normalized. The work distinguishes between intrinsic linguistic difficulty and modeling artifacts, concluding that most disparities stem from design choices in tokenization, sampling, and capacity allocation rather than fundamental differences in language complexity.

The paper provides actionable recommendations for more equitable multilingual modeling, including morphology-aware tokenization, byte-normalized sampling, and typology-aware capacity allocation. It also highlights the need for linguistically informed evaluation metrics and corpus design that reflect semantic exposure and diversity rather than raw token counts.

## Method Summary
The paper conducts a comprehensive survey of existing literature on multilingual language modeling, synthesizing findings from 40+ papers to identify root causes of cross-linguistic performance disparities. The methodology involves systematic analysis of tokenization effects, encoding asymmetries, sampling strategies, and capacity allocation mechanisms. Rather than presenting new experimental results, the work aggregates evidence from controlled studies, intervention analyses, and observational research to build a causal framework distinguishing intrinsic linguistic difficulty from modeling artifacts.

## Key Results
- Cross-linguistic performance gaps in multilingual language models primarily stem from modeling artifacts (tokenization fragmentation, byte-encoding inefficiencies, imbalanced data sampling) rather than intrinsic linguistic difficulty
- Normalization of segmentation, encoding, and exposure can substantially reduce these gaps
- Morphology- and script-aware tokenization, byte- or morpheme-normalized sampling, and typology-aware capacity allocation are effective interventions
- The findings highlight the need for linguistically informed evaluation metrics and corpus design that reflect semantic exposure and diversity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frequency-based subword tokenizers fragment morphologically complex languages and non-Latin scripts, inflating sequence length without adding semantic content.
- **Mechanism:** BPE/WordPiece prioritize frequent character sequences. High-resource Latin scripts accumulate larger subwords; morphologically rich or byte-heavy scripts fragment into shorter pieces, reducing information-per-token and effective training signal.
- **Core assumption:** Tokenization quality causally affects downstream performance, not just perplexity.
- **Evidence anchors:**
  - [abstract] "gaps often shrink when segmentation, encoding, and exposure are normalized"
  - [section 2.1] "frequency-based subword tokenization further amplifies disparities: high-resource Latin scripts benefit from larger, more informative subwords, while non-Latin scripts are split into shorter fragments, lowering information-per-token"
  - [section 2.2] "morphology-aware segmentation substantially reduces surprisal gaps induced by standard BPE"
  - [corpus] No direct corpus validation; related papers focus on benchmarking rather than tokenization causal chains.
- **Break condition:** If morphology-aware tokenizers show no downstream improvement over BPE for agglutinative languages, mechanism is confounded.

### Mechanism 2
- **Claim:** UTF-8 byte-length asymmetry creates systematic underexposure for non-Latin scripts under equal token budgets.
- **Mechanism:** Latin characters require 1 byte; Devanagari, Arabic, Chinese require up to 3 bytes. Token-based sampling allocates equal token counts but unequal semantic content, shrinking effective context windows for byte-heavy scripts.
- **Core assumption:** Equal token budgets do not imply equal semantic coverage.
- **Evidence anchors:**
  - [abstract] "byte-encoding inefficiencies" and "imbalanced data sampling" as primary disparity sources
  - [section 2.1] "UTF-8 byte-length asymmetry creates a byte premium... In fixed-size corpora, this reduces effective character exposure for non-Latin scripts"
  - [section 3.2] "token-based sampling penalizes byte-heavy scripts, whereas byte-normalized sampling narrows gaps"
  - [corpus] No direct corpus evidence; related benchmarks do not isolate byte-premium effects.
- **Break condition:** If byte-normalized sampling shows no improvement over token-based sampling for CJK/Arabic scripts, mechanism is secondary to other factors.

### Mechanism 3
- **Claim:** Shared-parameter training induces negative transfer when typological diversity exceeds effective model capacity.
- **Mechanism:** Gradient conflicts arise when distant languages compete for shared weights. Related languages benefit from transfer; typologically distant languages suffer interference and representation collapse.
- **Core assumption:** Capacity constraints bind before data coverage saturates.
- **Evidence anchors:**
  - [abstract] "normalization of segmentation, encoding, and exposure can substantially reduce these gaps"
  - [section 2.6] "the curse of multilinguality refers to declining per-language performance as more languages share parameters... typological distance amplifies interference"
  - [section 2.6] "controlled studies show that adding related languages improves low-resource performance, but distant languages harm both settings"
  - [corpus] Weak support; survey paper (arXiv:2502.09457) mentions multilingual challenges but does not test gradient interference directly.
- **Break condition:** If modular/language-specific adapters show no improvement over dense shared models for typologically diverse language sets, interference is not the primary driver.

## Foundational Learning

- **Concept: Subword tokenization (BPE/WordPiece)**
  - **Why needed here:** The paper's central claim is that tokenization fragmentation drives performance gaps. You must understand how frequency-based merges create vocabulary bias toward high-resource scripts.
  - **Quick check question:** Given a multilingual corpus with 90% English and 10% Finnish, will BPE produce longer subwords for English or Finnish, and why?

- **Concept: UTF-8 variable-width encoding**
  - **Why needed here:** The byte-premium mechanism assumes you understand why different scripts consume different byte counts per character under UTF-8.
  - **Quick check question:** How many UTF-8 bytes does the character 'A' require versus '语'? What does this imply for equal-token-budget training?

- **Concept: Typological distance vs. genealogical relatedness**
  - **Why needed here:** The paper distinguishes structural similarity (syntax, morphology) from shared ancestry when predicting transfer and interference.
  - **Quick check question:** Can two genetically unrelated languages have low typological distance? Give an example from the paper's framing.

## Architecture Onboarding

- **Component map:** Tokenizer layer (BPE/WordPiece vs. byte-level vs. morphology-aware) -> Sampling strategy (token-based vs. byte-normalized vs. information-normalized) -> Architecture core (dense shared parameters vs. modular adapters vs. typology-routed mixture-of-experts) -> Evaluation layer (subword perplexity vs. character/byte-level metrics vs. typology-aware probes)

- **Critical path:**
  1. Audit current tokenizer: measure tokens-per-word and tokens-per-byte across target languages (use Subword Evenness metric from §2.3)
  2. Identify disparity drivers: script inefficiency (byte premium), morphological fragmentation, or both
  3. Select intervention: byte-normalized sampling, script-balanced vocabulary, or morphology-aware segmentation
  4. Re-evaluate with character/morpheme-level metrics, not just subword perplexity

- **Design tradeoffs:**
  - Byte-level models: eliminate script bias but increase sequence length 2-3x for some scripts; higher compute cost
  - Morphology-aware tokenizers: require language-specific resources (Morfessor training data); may not scale to 100+ languages
  - Modular capacity: reduces interference but adds deployment complexity and parameter count

- **Failure signatures:**
  - Tokenizer produces single-character tokens for target language while producing multi-character tokens for English → fragmentation confirmed
  - Perplexity gap persists after byte-normalized sampling → check vocabulary overlap, not just exposure
  - Low-resource language improves with related-language pretraining but degrades with distant-language data → interference pattern

- **First 3 experiments:**
  1. **Tokenizer audit:** Compute tokens-per-sentence and bytes-per-token for 10 typologically diverse languages using your current tokenizer. Compare against character counts to quantify fragmentation.
  2. **Byte-normalized sampling pilot:** Retrain a small multilingual model with byte-proportional sampling instead of token-proportional. Measure perplexity change per language (expect improvement for CJK, Arabic, Devanagari).
  3. **Adapter isolation test:** Add language-specific adapters for 2 typologically distant low-resource languages. Compare downstream task performance against dense baseline to isolate interference effects.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can we develop principled methods to predict optimal data and capacity requirements per language based on linguistic typology, rather than relying on trial-and-error or uniform allocation?
- **Basis in paper:** [explicit] The conclusion states: "Future work should explore language-adaptive strategies: predicting data and capacity needs per language."
- **Why unresolved:** Current approaches use heuristics or equal allocation; the paper shows typological diversity affects capacity needs but offers no predictive framework for right-sizing per-language resources.
- **What evidence would resolve it:** A validated model that accurately predicts per-language data and parameter requirements from typological features (morphological complexity, script type, vocabulary overlap), demonstrated through controlled pretraining experiments.

### Open Question 2
- **Question:** How should multilingual pretraining curricula be designed to maximize positive transfer from related languages while minimizing gradient conflicts and negative interference?
- **Basis in paper:** [explicit] The conclusion calls for "designing curricula that prioritize transfer from related languages" and Section 2.6 documents that "gradient conflicts are common when distant languages are trained jointly."
- **Why unresolved:** Existing work shows relatedness helps and distance hurts, but curriculum design for optimal ordering, mixing ratios, or staged training across typologically diverse language groups remains unexplored.
- **What evidence would resolve it:** Comparative studies of curriculum strategies (e.g., typology-clustered vs. random ordering; adaptive mixing based on gradient conflict detection) showing consistent gains across diverse language pairs.

### Open Question 3
- **Question:** What evaluation metrics can fairly compare multilingual model performance while disentangling true linguistic competence from tokenization and encoding artifacts?
- **Basis in paper:** [explicit] The conclusion states: "Evaluation must also evolve: metrics should reflect cross-linguistic differences in task difficulty while capturing fairness." Section 3.3 notes: "Perplexity is sensitive to tokenizer choice."
- **Why unresolved:** Subword perplexity conflates segmentation decisions with predictability; character/byte-level metrics reduce tokenizer bias but remain sensitive to UTF-8 encoding asymmetries and script granularity.
- **What evidence would resolve it:** Development and validation of metrics (e.g., morpheme-normalized surprisal, semantic-equivalence probes) that show stable cross-linguistic rankings independent of tokenization choices.

### Open Question 4
- **Question:** How can multilingual models be adapted for truly low-resource or endangered languages with minimal or non-standardized training corpora?
- **Basis in paper:** [explicit] The conclusion states: "Truly low-resource and endangered languages require innovative approaches under scarcity." The limitations section acknowledges "data sparsity, orthographic variation, and non-standardized corpora in such languages could yield patterns not observed in higher-resource languages."
- **Why unresolved:** Most empirical findings derive from languages with at least moderate resources; extreme scarcity introduces challenges (dialect variation, script instability, lack of evaluation benchmarks) not addressed by current normalization strategies.
- **What evidence would resolve it:** Successful adaptation methods (e.g., cross-lingual transfer from typologically related languages, few-shot learning, or synthetic data augmentation) demonstrated on languages with <1M tokens of training data.

## Limitations
- The survey relies on correlational evidence and intervention studies rather than controlled experiments presented within the paper itself
- No single controlled experiment demonstrates that normalizing segmentation, encoding, and exposure eliminates all performance gaps when other factors are held constant
- The causal chain from tokenization artifacts to performance gaps lacks direct corpus-level validation
- Recommendations for morphology-aware tokenization and byte-normalized sampling face scalability constraints for low-resource languages

## Confidence

**High Confidence:**
- Tokenization fragmentation systematically disadvantages morphologically complex and non-Latin scripts through frequency-based subword merging that favors high-resource languages
- UTF-8 byte-length asymmetry creates exposure imbalances under equal token budgets, disadvantaging byte-heavy scripts
- These modeling artifacts are identifiable through tokenizer diagnostics (tokens-per-word, Subword Evenness, vocabulary overlap)

**Medium Confidence:**
- Normalizing segmentation, encoding, and exposure through interventions like morphology-aware tokenization, byte-normalized sampling, and modular capacity allocation can substantially reduce cross-linguistic performance gaps
- Gradient interference from shared-parameter training affects typologically distant language pairs more severely than related pairs

**Low Confidence:**
- The relative contribution of intrinsic linguistic difficulty versus modeling artifacts to overall performance disparities across all languages
- Whether interventions that work for specific language pairs will generalize to typologically diverse multilingual settings
- The existence and severity of interference effects without controlled experiments specifically designed to isolate them

## Next Checks

1. **Controlled tokenization ablation study**: Train identical multilingual models using three different tokenization strategies (frequency-based BPE, byte-level, and morphology-aware) on the same corpus with equalized semantic exposure. Measure downstream task performance across typologically diverse languages to isolate tokenization effects from other factors.

2. **Byte-premium validation corpus**: Construct a multilingual corpus where each language's representation is proportional to semantic content (measured by characters or morphemes) rather than tokens. Train models with both token-based and byte-normalized sampling on this corpus, then evaluate with character-level metrics to directly test whether byte-premium effects persist when semantic exposure is equalized.

3. **Interference isolation experiment**: Using a controlled multilingual setup with 2-3 related language pairs and 2-3 typologically distant pairs, systematically vary model capacity and shared parameters while holding data quantity constant. Measure gradient conflict through representation similarity and downstream performance to quantify interference effects independently of data quantity or capacity constraints.