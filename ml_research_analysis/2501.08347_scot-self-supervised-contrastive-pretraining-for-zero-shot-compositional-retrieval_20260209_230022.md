---
ver: rpa2
title: 'SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional
  Retrieval'
arxiv_id: '2501.08347'
source_url: https://arxiv.org/abs/2501.08347
tags:
- image
- text
- scot
- retrieval
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCOT addresses zero-shot compositional image retrieval by pretraining
  a composition model using large-scale image-text pairs and a frozen vision-language
  encoder. It generates modification texts and modified captions via an LLM, using
  the resulting text embeddings as supervision targets instead of requiring image
  triplets.
---

# SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval

## Quick Facts
- **arXiv ID**: 2501.08347
- **Source URL**: https://arxiv.org/abs/2501.08347
- **Reference count**: 40
- **Primary result**: Zero-shot compositional retrieval method achieving competitive performance on FashionIQ and CIRR benchmarks

## Executive Summary
SCOT introduces a novel approach to zero-shot compositional image retrieval that eliminates the need for image triplet supervision. By leveraging large-scale image-text pairs and a frozen vision-language encoder, SCOT pretrains a composition function using text embeddings as supervision targets. The method generates modification texts and modified captions via an LLM, training the composition function to align composed image-modification embeddings with target text embeddings using contrastive loss. This self-supervised approach achieves strong results on both FashionIQ and CIRR benchmarks while being task-agnostic and not requiring task-specific fine-tuning.

## Method Summary
SCOT addresses zero-shot compositional image retrieval by pretraining a composition model using large-scale image-text pairs and a frozen vision-language encoder. The key innovation is using text embeddings from the frozen encoder as supervision targets instead of requiring image triplets. An LLM generates modification texts and modified captions, which are then processed by the frozen vision-language model to create text embeddings. The composition function is trained with a contrastive loss to align the composed image-modification embedding with the target text embedding. This approach enables effective compositional retrieval without any task-specific fine-tuning, achieving competitive performance on benchmark datasets.

## Key Results
- On FashionIQ, SCOT achieves R@10 of 38.45% and R@50 of 60.03% using a BLIP-2 backbone
- On CIRR, SCOT reaches R@1 of 36.82% and R@10 of 74.48%, surpassing other zero-shot approaches
- SCOT's text-based supervision proves more effective than retrieved image supervision
- Performance improves with better vision-language backbones and larger training datasets

## Why This Works (Mechanism)
SCOT's effectiveness stems from leveraging the rich semantic representations learned by frozen vision-language models during pretraining. By using text embeddings as supervision targets, the method avoids the data scarcity issues associated with image triplet supervision. The contrastive loss effectively aligns the composed image-modification embeddings with target text embeddings, enabling the model to learn compositional transformations without explicit task-specific fine-tuning. The LLM-generated modifications provide diverse and contextually relevant compositional examples that the model can learn from.

## Foundational Learning
- **Vision-Language Encoders**: Pre-trained models that map images and text to a shared embedding space
  - Why needed: Provide rich semantic representations for both images and text
  - Quick check: Frozen BLIP-2 backbone achieves strong performance
- **Contrastive Learning**: Training objective that pulls similar embeddings together and pushes dissimilar ones apart
  - Why needed: Enables effective alignment between composed embeddings and target embeddings
  - Quick check: Contrastive loss formulation drives compositional learning
- **LLM-Generated Modifications**: Using large language models to create diverse compositional examples
  - Why needed: Provides scalable way to generate compositional modifications without manual annotation
  - Quick check: Generated modifications are contextually relevant and diverse

## Architecture Onboarding

Component Map: Image + LLM Modification -> Composition Function -> Contrastive Loss -> Frozen VL Encoder -> Text Embedding Target

Critical Path: Image and modification text are composed by the composition function, passed through the frozen vision-language encoder, and compared to the text embedding target via contrastive loss.

Design Tradeoffs:
- Frozen VL encoder vs fine-tuning: Freezing reduces computational cost but may limit adaptability
- Text supervision vs image supervision: Text is more scalable but may lose fine-grained visual details
- LLM-generated modifications vs manual annotations: LLM is more scalable but potentially less controlled

Failure Signatures:
- Poor performance on out-of-domain compositional tasks
- Sensitivity to specific LLM prompt engineering
- Degradation when using smaller vision-language backbones

First Experiments:
1. Ablation study comparing different vision-language backbones (BLIP-2 vs smaller models)
2. Evaluation of different LLM prompt engineering strategies for modification generation
3. Testing on out-of-domain compositional retrieval tasks beyond fashion imagery

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy dependence on the quality of the underlying vision-language backbone
- Limited investigation of failure cases and challenging compositional modifications
- Potential biases introduced by specific LLM prompts and models used

## Confidence
- Primary performance claims: High
- Generality of approach: Medium
- Text-supervision methodology robustness: High for tested scenarios, Medium for different LLM configurations

## Next Checks
1. Test SCOT's performance when substituting BLIP-2 with smaller or specialized vision-language backbones to assess scalability and domain transferability
2. Conduct ablation studies varying LLM prompt engineering and modification generation strategies to quantify sensitivity to these design choices
3. Evaluate SCOT on out-of-domain compositional retrieval tasks (e.g., medical imaging or remote sensing) to test generalization beyond the reported datasets