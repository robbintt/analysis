---
ver: rpa2
title: 'Explaining k-Nearest Neighbors: Abductive and Counterfactual Explanations'
arxiv_id: '2501.06078'
source_url: https://arxiv.org/abs/2501.06078
tags:
- sufficient
- reason
- every
- problem
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the computational complexity of generating\
  \ explanations for k-Nearest Neighbor (k-NN) classifiers, focusing on abductive\
  \ explanations (minimum sufficient reasons) and counterfactual explanations. The\
  \ authors analyze both continuous settings (using \u2113p-norms) and discrete settings\
  \ (using Hamming distance), establishing a detailed landscape of positive and negative\
  \ complexity results."
---

# Explaining k-Nearest Neighbors: Abductive and Counterfactual Explanations

## Quick Facts
- arXiv ID: 2501.06078
- Source URL: https://arxiv.org/abs/2501.06078
- Authors: Pablo Barceló; Alexander Kozachinskiy; Miguel Romero Orth; Bernardo Subercaseaux; José Verschae
- Reference count: 12
- Key outcome: NP-hardness for computing minimum sufficient reasons in all settings, tractability for counterfactual explanations under the ℓ₂-norm, and intractability under the ℓ₁-norm and in the discrete setting.

## Executive Summary
This paper establishes the computational complexity of generating two types of explanations for k-Nearest Neighbor (k-NN) classifiers: abductive explanations (minimum sufficient reasons) and counterfactual explanations. The authors analyze both continuous settings using ℓₚ-norms and discrete settings using Hamming distance, proving NP-hardness for finding minimum sufficient reasons across all settings while showing tractability for counterfactual explanations specifically under the ℓ₂-norm. They also demonstrate that despite theoretical intractability, practical implementations using SAT and Integer Quadratic Programming solvers can compute explanations efficiently for datasets with hundreds of features and thousands of points.

## Method Summary
The paper analyzes the computational complexity of generating explanations for k-NN classifiers by establishing formal reductions and algorithmic approaches. For counterfactual explanations under ℓ₂-norm, the authors reduce the problem to convex quadratic programming by representing classification regions as unions of polyhedra. For minimum sufficient reasons, they prove NP-hardness by reducing from the Vertex Cover problem. Practical implementations use SAT solving with cardinality constraints for discrete settings and Integer Quadratic Programming for continuous settings. The approach handles three distance metrics (ℓ₁, ℓ₂, Hamming) and considers both binary and multi-class classification scenarios.

## Key Results
- Finding minimum sufficient reasons is NP-hard in all settings (discrete and continuous)
- Counterfactual explanations under ℓ₂-norm are computationally tractable via convex quadratic programming
- Counterfactual explanations under ℓ₁-norm and Hamming distance are NP-complete
- Practical implementations using SAT and IQP solvers can compute explanations for datasets with hundreds of features and thousands of points

## Why This Works (Mechanism)

### Mechanism 1: Geometric Linearization for ℓ₂ Tractability
The ℓ₂-norm's geometric property that equidistant boundaries form hyperplanes allows classification regions to be represented as unions of polyhedra. This enables reduction to minimizing a convex quadratic form subject to linear constraints, making the problem polynomial-time solvable.

### Mechanism 2: Hardness via Combinatorial Reduction
Finding minimum sufficient reasons maps to the Vertex Cover problem. The reduction encodes graph edges as training data points such that a set of features constitutes a sufficient reason if and only if it corresponds to a vertex cover.

### Mechanism 3: Solver-Based Feasibility for Intractable Cases
Despite theoretical NP-hardness, practical explanations can be computed using SAT or Integer Quadratic Programming solvers. The constraints (e.g., distance to counterfactual must be less than distance to nearest neighbor) translate into boolean satisfiability clauses or integer constraints that modern solvers can efficiently prune.

## Foundational Learning

- **Concept: Convex Quadratic Programming (CQP)**
  - Why needed here: To understand why the ℓ₂ counterfactual problem is "easy" (polynomial time). The paper proves tractability by reducing the problem to minimizing a distance (quadratic objective) inside a polyhedron (linear constraints).
  - Quick check question: Can you explain why minimizing a distance squared subject to linear inequalities is solvable in polynomial time, while minimizing the same subject to "strict" inequalities requires a slight modification?

- **Concept: Complexity Reductions (NP-hardness)**
  - Why needed here: The paper proves problems are hard by reducing Vertex Cover and Partition problems to explanation tasks, showing the slowness is intrinsic to the problem logic.
  - Quick check question: If you had an oracle that could instantly solve the "Minimum Sufficient Reason" problem for k-NN, how could you use the construction in Theorem 1 to solve a Vertex Cover problem?

- **Concept: Tie-Breaking in k-NN**
  - Why needed here: The paper defines an "optimistic" view where ties favor the positive classification, which is critical for the correctness of formal proofs and solver implementations.
  - Quick check question: In the "optimistic" view defined in Section 2, if a point is equidistant to (k+1)/2 positive and (k+1)/2 negative points, how is it classified?

## Architecture Onboarding

- **Component map:** Metric Space Interface -> Explanation Engine Router -> Solver Backend -> Explanation Output
- **Critical path:** The routing decision between Convex Solver (ℓ₂/continuous) and Combinatorial Solver (NP-hard paths). Choosing the wrong path results in unnecessary computational cost or incorrect results.
- **Design tradeoffs:** Completeness vs. Speed (SAT guarantees finding solutions but scales poorly compared to IQP or polynomial ℓ₂ methods); Optimistic vs. Pessimistic Classification (implementation must strictly adhere to "optimistic" tie-breaking rule).
- **Failure signatures:** Solver Timeout in NP-hard paths as dataset size increases; Infeasible Solution (IQP) if optimistic view is not correctly encoded in distance constraints.
- **First 3 experiments:**
  1. Implement the polynomial-time convex method for 1-NN counterfactuals on a simple 2D dataset and verify the counterfactual lies on the bisector between nearest positive and negative points.
  2. Using the SAT encoding from Section 9.2, measure runtime to find a counterfactual for a binary dataset as features scale from 50 to 300, comparing against theoretical exponential curve.
  3. Implement the "Check Sufficient Reason" algorithm (Proposition 6) for 1-NN discrete data and verify that removing any single feature from a "minimal" sufficient reason changes the classification.

## Open Questions the Paper Calls Out

- Is ℓ₂ the only metric for which k-Counterfactual Explanation is tractable, or does tractability extend to ℓₚ-norms where p > 2?
- Can the positive complexity results for k-Counterfactual Explanation be extended to multi-label classification settings when k ≥ 3?
- Can the NP-hard k-Minimum Sufficient Reason problem be approximated by polynomial-time algorithms?

## Limitations

- Computational intractability of computing minimum sufficient reasons in all but most restricted settings (k=1 in discrete space)
- Theoretical worst-case complexity remains exponential despite practical solver feasibility
- Assumes "optimistic" tie-breaking rule for k-NN which may not align with all implementations
- Theoretical results depend on P≠NP separation

## Confidence

- **High Confidence:** Complexity results for counterfactual explanations under ℓ₂-norm (Theorem 2) and NP-hardness of minimum sufficient reasons across all settings (Theorems 1, 3, 4, 6)
- **Medium Confidence:** Practical solver implementations and experimental results showing explanations can be computed for datasets with hundreds of features
- **Low Confidence:** Exact behavior of "optimistic" tie-breaking rule in edge cases and interaction with strict inequality modifications in ℓ₂ implementation

## Next Checks

1. Implement the polynomial-time ℓ₂ counterfactual method and verify it correctly finds solutions on simple 2D datasets where the optimal counterfactual lies on the hyperplane between nearest neighbors.
2. Benchmark the SAT encoding for discrete counterfactuals on increasing feature dimensions (n=50 to 300) to empirically verify theoretical exponential scaling.
3. Test the "Check Sufficient Reason" algorithm for k=1 discrete data to confirm that removing any single feature from a minimal sufficient reason changes the classification.