---
ver: rpa2
title: 'QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer
  Pairs'
arxiv_id: '2511.12504'
source_url: https://arxiv.org/abs/2511.12504
tags:
- qa-noun
- noun
- semantic
- question
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QA-Noun introduces a QA-based framework for capturing noun-centered
  semantic relations, addressing the gap left by prior QA-based approaches focused
  on verbal semantics. It defines nine question templates to represent both explicit
  syntactic and implicit contextual roles for nouns, enabling fine-grained decomposition
  of sentence meaning into interpretable QA pairs.
---

# QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs

## Quick Facts
- arXiv ID: 2511.12504
- Source URL: https://arxiv.org/abs/2511.12504
- Reference count: 25
- Primary result: Introduces QA-based framework capturing noun-centered semantic relations with 9 question templates, achieving near-complete AMR coverage and 130% higher granularity than fact-based decomposition methods

## Executive Summary
QA-Noun introduces a question-answering framework for representing nominal semantics, addressing the gap left by prior QA-based approaches focused on verbal semantics. The framework defines nine question templates that capture both explicit syntactic and implicit contextual roles for nouns, enabling fine-grained decomposition of sentence meaning into interpretable QA pairs. Combined with QA-SRL, it yields over 130% higher granularity than recent fact-based decomposition methods like FactScore and DecompScore, providing a comprehensive solution for fine-grained semantic decomposition essential for cross-text alignment tasks.

## Method Summary
The framework uses nine fixed question templates (Possession, Location, Time, Partitive, Copular, Property, Quantity, Sub-Specification) to systematically capture noun-centered semantic relations. A two-annotator consolidation protocol improves argument coverage through independent annotation followed by reconciliation. The approach achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations. A fine-tuned LLaMA-3-8B parser automatically generates QA pairs, which are integrated with QA-SRL and filtered through CORE to remove redundancy, yielding structured predicate-argument decompositions rather than unconstrained atomic facts.

## Key Results
- Achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations
- Combines with QA-SRL to yield over 130% higher granularity than fact-based decomposition methods
- Strong argument detection performance (F1 56.8-56.9) with practical deployment feasibility using LLaMA-3-8B

## Why This Works (Mechanism)

### Mechanism 1
Template-based decomposition systematically captures noun-centered semantic relations with interpretable role labels. Nine fixed question templates map to common noun-argument relations, with the Property template adding open-vocabulary slots for context-specific attributes. This enables both consistency and expressivity while assuming a limited template set can cover the majority of semantically relevant noun-argument relations.

### Mechanism 2
Two-annotator consolidation protocol improves argument coverage while maintaining annotation quality. Independent annotation by two trained annotators followed by reconciliation addresses coverage gaps observed in single-annotator crowdsourcing, assuming individual annotators systematically miss valid arguments with partially non-overlapping errors.

### Mechanism 3
Combining QA-Noun with QA-SRL yields substantially finer-grained semantic decomposition than fact-based methods. The structured predicate-argument decomposition provides more granular and linguistically grounded meaning units than LLM-generated declarative facts, assuming structured decomposition offers advantages for downstream alignment tasks.

## Foundational Learning

- **Semantic Role Labeling (SRL)**: Understanding predicate-argument structure is prerequisite for QA-Noun's extension of traditional verbal SRL to nouns. Quick check: Can you explain the difference between an agent role and a theme role in "The company increased prices"?

- **QA-SRL Paradigm**: QA-Noun is designed to complement QA-SRL; the question-answer format replaces formal role labels with natural language questions. Quick check: How does QA-SRL represent "John baked a cake" using question-answer pairs?

- **Abstract Meaning Representation (AMR)**: AMR serves as the primary coverage benchmark; QA-Noun achieves near-complete AMR coverage while surfacing additional relations. Quick check: What does an AMR graph look like for "Bowie recorded a song for his 1971 album"?

## Architecture Onboarding

- Component map: Annotation Interface -> Two-annotator Protocol -> Template Set -> LoRA Fine-tuned Parser -> QA-Noun + QA-SRL Integration -> CORE Redundancy Filter
- Critical path: Template design (NomBank-derived iteration) → Pilot annotation → Consolidated dataset (2,029 noun mentions, 4,869 arguments) → LoRA fine-tuning → Integration with QA-SRL parser → CORE filtering → Downstream alignment tasks
- Design tradeoffs: Template flexibility vs. consistency (Property template), coverage vs. annotation cost (two-annotator protocol), model scale vs. deployment feasibility (LLaMA-405B vs. LLaMA-8B), granularity vs. redundancy (CORE filtering)
- Failure signatures: Low role assignment accuracy (58.5% SRA), template overlap issues, domain generalization challenges, single-annotator training data noise
- First 3 experiments: 1) Template coverage validation on target domain samples, 2) Parser quality baseline on held-out test sentences, 3) Integration redundancy analysis using CORE filtering

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on synthetic QA pairs rather than human judgments on parser output
- Single-annotator training data may contain systematic biases
- Property template's open-ended nature introduces evaluation ambiguity

## Confidence

**High Confidence**: Template coverage relative to AMR/NomBank, annotation protocol effectiveness (72.8% IAA), and granularity gains over fact-based methods (130% improvement). Parser's strong argument detection performance (F1 56.8-56.9) and framework's practical deployment feasibility support these claims.

**Medium Confidence**: Template coverage completeness across domains, single-annotator training data quality, and cross-domain parser generalization. Lack of human evaluation on parser output and single-annotator protocol introduce uncertainty.

**Low Confidence**: Real-world downstream task performance beyond semantic decomposition, Property template evaluation consistency, and computational cost scaling for large-scale applications.

## Next Checks

1. **Human evaluation on parser output**: Have annotators judge quality and completeness of QA pairs generated by fine-tuned parser on 100 held-out sentences, measuring precision, recall, and annotator agreement.

2. **Cross-domain template coverage analysis**: Apply QA-Noun to narrative, conversational, and technical domains; identify noun-argument relations not captured by existing templates and assess need for template expansion.

3. **Integration redundancy and coherence analysis**: Systematically analyze CORE-filtered QA pairs from 200 sentences to quantify redundancy reduction rates and identify patterns of semantic overlap that persist despite filtering.