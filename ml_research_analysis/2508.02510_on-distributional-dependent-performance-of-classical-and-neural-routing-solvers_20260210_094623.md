---
ver: rpa2
title: On Distributional Dependent Performance of Classical and Neural Routing Solvers
arxiv_id: '2508.02510'
source_url: https://arxiv.org/abs/2508.02510
tags:
- distribution
- neural
- base
- node
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel subsampling approach for Neural Combinatorial
  Optimization (NCO) in routing problems. Instead of sampling problem instances independently,
  the method generates a large "Base Node Distribution" and samples training/test
  instances from it, creating recurring structural patterns across instances.
---

# On Distributional Dependent Performance of Classical and Neural Routing Solvers

## Quick Facts
- **arXiv ID:** 2508.02510
- **Source URL:** https://arxiv.org/abs/2508.02510
- **Reference count:** 40
- **Primary result:** Neural solvers trained on subsampled distributions can match or outperform classical meta-heuristics under realistic runtime constraints

## Executive Summary
This paper introduces a novel subsampling approach for Neural Combinatorial Optimization (NCO) in routing problems that generates recurring structural patterns across training instances. Instead of sampling problem instances independently, the method creates a large "Base Node Distribution" and samples training/test instances from it, encouraging neural models to learn spatial patterns and relations within the distribution. The authors evaluate representative NCO methods (POMO, BQ, NeuOpt, and SGBS-EAS) on TSP and CVRP tasks against classical meta-heuristics (LKH3 and HGS-CVRP), finding that models trained on subsampled distributions often outperform or match their counterparts trained on full distributions, particularly on structured problem distributions like Rotation and Explosion.

## Method Summary
The method generates a large "Base Node Distribution" ($G_{base}$) containing $N_{base}$ nodes, then samples training and test instances by selecting subsets of $N$ nodes from this base. This creates recurring structural patterns across instances, as nodes from the base distribution reappear in different combinations. The approach is evaluated on TSP and CVRP using both constructive (POMO, BQ) and improvement (SGBS-EAS, NeuOpt) NCO methods, comparing against classical solvers (LKH3 for TSP, HGS-CVRP for CVRP) under realistic runtime constraints.

## Key Results
- Models trained on subsampled distributions often outperform or match counterparts trained on full distributions, especially on structured distributions like Rotation and Explosion
- For CVRP on Uchoa benchmark distributions, some neural methods achieve negative relative gaps, meaning they outperform classical solvers under realistic runtime constraints (0.7s and 5s)
- Performance gains are most pronounced when training and testing distributions align, demonstrating the trade-off between generalization and in-distribution performance

## Why This Works (Mechanism)

### Mechanism 1: Structural Recurrence via Subsampling
Training on instances subsampled from a fixed "Base Node Distribution" encourages neural solvers to memorize and exploit spatial relationships that reoccur across instances. Unlike standard generation where every instance consists of entirely new random nodes, this method forces the model to see specific node configurations repeatedly, allowing it to learn the underlying topology of the base distribution rather than solving each instance from first principles.

### Mechanism 2: In-Distribution Specialization (Overfitting as a Feature)
The subsampling approach deliberately trades out-of-distribution generalization for superior in-distribution performance. By tightly coupling training and test sets to the same fixed seed nodes, the model overfits to the specific attributes of that distribution. While this harms robustness across random instances, it maximizes performance when the deployment environment is known and static (e.g., a logistics company serving a fixed set of clients).

### Mechanism 3: Inference-Time Efficiency Advantage
Neural solvers can outperform classical heuristics strictly when runtime budgets are tight, leveraging the fast forward-pass of the network against the iterative search of meta-heuristics. The subsampling training provides a strong "warm start" in terms of policy knowledge. At inference, the neural solver produces high-quality solutions in milliseconds, while classical solvers require iterations to converge.

## Foundational Learning

- **Neural Combinatorial Optimization (NCO):** Uses neural networks to learn construction policies or improvement heuristics for combinatorial problems, distinct from hard-coded algorithms. *Why needed:* The paper modifies the input pipeline for NCO methods. *Quick check:* Does the model learn a step-by-step construction rule (constructive) or how to improve an existing solution (improvement)?

- **TSP vs. CVRP:** Traveling Salesman Problem (TSP) involves finding the shortest route visiting all nodes once, while Capacitated Vehicle Routing Problem (CVRP) adds vehicle capacity constraints and multiple vehicles. *Why needed:* The paper evaluates on both, but strongest results are seen in CVRP. *Quick check:* Why might a "clustered" distribution be harder or easier for a solver than a uniform one?

- **Base Node Distribution ($G_{base}$):** A large, fixed "pool" of nodes from which smaller instances are sampled. *Why needed:* This is the core contribution. *Quick check:* If I sample two training instances from $G_{base}$, are they completely independent? (No, they likely share some nodes).

## Architecture Onboarding

- **Component map:** Data Generator -> Subsampler -> NCO Model -> Evaluator
- **Critical path:** 1) Defining $G_{base}$ correctly (e.g., setting cluster centers for the "X" distribution) 2) Ensuring subsampling guarantees specific probability of node reoccurrence between train and test sets 3) Aligning runtime limit ($T_{max}$) during evaluation to reflect "realistic constraints"
- **Design tradeoffs:** Generalization vs. Performance (optimizes for high performance on known domain at cost of poor generalization), Storage vs. On-the-fly (can pre-generate base distribution or generate procedurally)
- **Failure signatures:** Uniformity collapse (implementing on purely Uniform distributions shows no improvement), Time-budget mismatch (if $T_{max}$ is too high, classical solvers dominate)
- **First 3 experiments:** 1) Baseline Verification: Train POMO on standard random instances vs. Subsampled instances; evaluate on subsampled test set 2) Distributional Ablation: Test trained model on test set subsampled from different $G_{base}$ 3) Runtime Sensitivity: Plot Relative Gap (%) against $T_{max}$ (0.1s to 60s) to find crossover point

## Open Questions the Paper Calls Out

### Open Question 1
Do hybrid training strategies that blend full-distribution exposure with subsampled instances improve the trade-off between distribution-specific performance and generalization robustness? The authors explicitly suggest future research could explore "hybrid strategies that blend full and subsampled training."

### Open Question 2
Can the subsampling approach be extended to industrial-sized routing problems or instances with significantly larger node counts (e.g., >1000 nodes)? The authors state that "scaling these insights to larger or industrial datasets also remains an important direction."

### Open Question 3
Why does the subsampling approach enable neural solvers to outperform OR baselines on CVRP but fails to close the performance gap on TSP? Results show neural solvers achieve negative relative gaps on X10k CVRP dataset while TSP results remain positive, a discrepancy the authors don't fully mechanistically explain.

### Open Question 4
Can adaptive or curriculum-based subsampling strategies further accelerate convergence or improve solution quality? The authors briefly mention "adaptive subsampling methods" as a potential avenue for future research.

## Limitations
- Generalization boundaries are unclear - the approach explicitly trades generalization for in-distribution performance
- Classical solver configuration may not be optimized for the specific runtime constraints used in evaluation
- Distribution selection bias - results are particularly strong on the Uchoa distribution, which is a curated benchmark designed to test classical solvers

## Confidence

**High Confidence:**
- Neural solvers trained on subsampled distributions show superior in-distribution performance compared to standard training
- Distributional structure significantly impacts solver performance, with structured distributions yielding larger gains
- Under realistic runtime constraints (0.7s, 5s), neural methods can match or exceed classical solvers

**Medium Confidence:**
- The subsampling approach provides a general framework for improving NCO methods across different architectures
- Performance gains are primarily due to structural recurrence rather than increased training data

**Low Confidence:**
- The approach would generalize to real-world routing scenarios without modification
- Observed gains would persist across different problem sizes beyond N=100

## Next Checks

1. **Generalization Stress Test:** Train a neural solver using subsampling on one Base Node Distribution (e.g., Uchoa), then evaluate on test instances from a completely different distribution (e.g., Uniform). Measure the performance drop compared to standard training to quantify the generalization cost.

2. **Runtime Budget Calibration:** For each runtime constraint (0.7s, 5s, 50s), optimize classical solver parameters specifically for that budget. Re-run the comparison to determine if the neural advantage persists when classical solvers are configured for the same operational constraints.

3. **Real-World Distribution Validation:** Generate a Base Node Distribution using real-world geographic data (e.g., actual customer locations from logistics companies) rather than synthetic patterns. Train and evaluate neural solvers on this distribution to assess whether the approach provides benefits in realistic deployment scenarios.