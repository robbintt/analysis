---
ver: rpa2
title: 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and
  Test-Time Compute Scaling'
arxiv_id: '2508.16745'
source_url: https://arxiv.org/abs/2508.16745
tags:
- reasoning
- arxiv
- state
- rule
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reasoning capabilities of large language
  models by training them on one-dimensional cellular automata (1dCA) tasks. The authors
  created a benchmark where models must infer underlying Boolean rules from state
  sequences and then apply these rules to predict future states.
---

# Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling

## Quick Facts
- arXiv ID: 2508.16745
- Source URL: https://arxiv.org/abs/2508.16745
- Reference count: 40
- One-line primary result: Most neural architectures struggle with multi-step reasoning beyond two steps in cellular automata tasks, with Chain-of-Thought training showing the strongest performance gains.

## Executive Summary
This paper investigates the reasoning capabilities of large language models by training them on one-dimensional cellular automata (1dCA) tasks. The authors created a benchmark where models must infer underlying Boolean rules from state sequences and then apply these rules to predict future states. By using disjoint train/test rule sets, they ensure the models cannot simply memorize but must generalize. The study compares multiple architectures including transformers, LSTMs, state-space models (Mamba), and the Associative Recurrent Memory Transformer (ARMT).

The key finding is that most neural architectures struggle with multi-step reasoning beyond two steps, even with increased depth or recurrence. The paper demonstrates that explicit intermediate supervision through Chain-of-Thought approaches remains the most effective method for enabling deep reasoning, while Adaptive Computation Time (ACT) and reinforcement learning with GRPO provide valuable alternatives when such supervision is unavailable. The authors find that increasing model depth is more effective than increasing width for improving reasoning capabilities.

## Method Summary
The authors created a benchmark using one-dimensional cellular automata with binary states, where models must infer underlying Boolean rules from state sequences and predict future states. They trained multiple architectures including transformers, LSTMs, Mamba, and ARMT on disjoint train/test rule sets to prevent memorization. The study employed various techniques to enhance reasoning depth including Adaptive Computation Time (ACT), reinforcement learning with GRPO, and Chain-of-Thought training. Performance was evaluated on single-step and multi-step prediction tasks up to four steps ahead, with a focus on how different architectural choices and training methods affect reasoning depth.

## Key Results
- Most neural architectures show sharp performance drops for multi-step reasoning beyond two steps, despite achieving high accuracy on single-step predictions
- Increasing model depth proves more effective than increasing width for improving reasoning capabilities across different architectures
- Chain-of-Thought training achieves near-perfect accuracy for up to four-step predictions when step-by-step supervision is available
- Adaptive Computation Time provides modest improvements by allowing variable computation steps, while GRPO enables learning reasoning traces without intermediate supervision

## Why This Works (Mechanism)
The paper demonstrates that explicit intermediate supervision through Chain-of-Thought approaches enables models to learn step-by-step reasoning processes rather than just pattern matching. When provided with step-by-step supervision, models can develop internal representations that capture the abstract reasoning principles underlying cellular automata rules. The recurrence and memory mechanisms in architectures like ARMT and LSTMs help maintain state information across steps, but their effectiveness is limited without proper supervision. ACT allows models to allocate more computation to difficult reasoning steps, while GRPO enables reinforcement learning to discover effective reasoning traces through reward signals.

## Foundational Learning
- **Cellular Automata Rules**: Why needed - These provide a controlled environment to study abstract reasoning; Quick check - Can the model predict the next state given a rule and initial configuration?
- **Boolean Logic Operations**: Why needed - The underlying rules are based on Boolean functions; Quick check - Does the model correctly implement AND, OR, XOR operations in state transitions?
- **Sequence-to-Sequence Modeling**: Why needed - The task requires mapping input sequences to output sequences; Quick check - Can the model handle variable-length input and output sequences?
- **Multi-step Reasoning**: Why needed - The core challenge is extending reasoning beyond immediate next-step predictions; Quick check - Does accuracy degrade predictably as prediction horizon increases?
- **Chain-of-Thought Training**: Why needed - Provides intermediate supervision for complex reasoning tasks; Quick check - Does providing intermediate steps improve final prediction accuracy?
- **Adaptive Computation Time**: Why needed - Allows variable computational resources per reasoning step; Quick check - Does the model learn to spend more computation on difficult steps?

## Architecture Onboarding

Component map: Input State Sequence -> Encoder -> Reasoning Module -> Decoder -> Output State Sequence

Critical path: Input encoding -> Rule inference -> State transition application -> Output generation

Design tradeoffs: Depth vs width scaling (depth more effective), recurrence vs attention mechanisms, fixed vs adaptive computation steps

Failure signatures: Sharp accuracy drop beyond 2 steps, overfitting to specific rule patterns, inability to generalize to unseen rules

First experiments:
1. Evaluate single-step prediction accuracy across all architectures to establish baseline performance
2. Test multi-step prediction (3-4 steps) to measure reasoning depth limitations
3. Compare depth scaling versus width scaling within each architecture family

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to one-dimensional cellular automata with binary states, limiting generalizability to more complex reasoning tasks
- Potential unintended memorization of state patterns rather than true rule learning, despite efforts to mitigate this
- Focus on exact state prediction accuracy may not fully capture whether models have learned abstract reasoning principles versus pattern matching

## Confidence

High: Most architectures struggle with multi-step reasoning beyond two steps, supported by systematic experiments across multiple architectures with clear performance degradation patterns.

Medium: Explicit intermediate supervision through Chain-of-Thought is "most effective" for deep reasoning, supported within cellular automata domain but may not generalize to all reasoning tasks.

Medium: ACT and GRPO provide valuable alternatives when intermediate supervision is unavailable, reasonable but based on modest performance improvements with uncertain utility for more complex tasks.

## Next Checks

1. Cross-domain generalization: Evaluate the same architectures and techniques on multi-step reasoning tasks from other domains (e.g., mathematical problem solving, logical inference, or multi-hop question answering) to assess whether cellular automata findings transfer to more naturalistic reasoning scenarios.

2. Rule abstraction analysis: Conduct ablation studies where models are tested on novel rules that share structural similarities with training rules versus completely different rule families to determine whether models are learning abstract reasoning principles or memorizing specific rule patterns.

3. Interpretability verification: Use attention visualization or probing techniques to examine whether models that achieve high accuracy on multi-step predictions are actually applying the correct rules at each step, or whether they are using alternative strategies that happen to produce correct outputs.