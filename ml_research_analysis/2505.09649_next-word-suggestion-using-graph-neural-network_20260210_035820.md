---
ver: rpa2
title: Next Word Suggestion using Graph Neural Network
arxiv_id: '2505.09649'
source_url: https://arxiv.org/abs/2505.09649
tags:
- context
- word
- graph
- language
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel next-word prediction model that combines
  Graph Neural Networks (GNNs) and LSTMs to capture both global and local context
  from text. The approach constructs a word co-occurrence graph from Wikipedia articles
  and uses Graph Convolutional Networks (GCNs) to learn global context embeddings
  for each word.
---

# Next Word Suggestion using Graph Neural Network

## Quick Facts
- arXiv ID: 2505.09649
- Source URL: https://arxiv.org/abs/2505.09649
- Authors: Abisha Thapa Magar; Anup Shakya
- Reference count: 3
- Primary result: Proposed GCN-LSTM hybrid achieves up to 67.3% training accuracy and 36.21% test accuracy on Wikipedia-based next-word prediction

## Executive Summary
This work introduces a novel next-word prediction model that combines Graph Neural Networks (GNNs) and LSTMs to capture both global and local context from text. The approach constructs a word co-occurrence graph from Wikipedia articles and uses Graph Convolutional Networks (GCNs) to learn global context embeddings for each word. These embeddings are then fed into an LSTM trained on n-gram sequences to predict the next word. Experiments on custom Wikipedia corpora show that the proposed Context Embedding (CE) method outperforms a random embedding baseline, achieving training accuracies up to 67.3% and test accuracies up to 36.21% across domains like sports, celebrity, and music.

## Method Summary
The proposed method constructs a word co-occurrence graph from Wikipedia articles where nodes represent unique words and edges represent co-occurrence within sentences. A two-layer GCN is trained on link prediction to learn 64-dimensional context embeddings for each word node. These embeddings capture global semantic context by aggregating information from neighboring nodes through message-passing. The learned embeddings are then used as input features for an LSTM trained on n-gram sequences (1-10 gram) to predict the next word. The architecture uses a many-to-one LSTM with 200 hidden units, post-padding sequences with zeros for batching. The models are trained independently - GCN for link prediction, then LSTM for next-word prediction.

## Key Results
- Context Embedding (CE) method outperforms random embedding baseline on next-word prediction
- Training accuracy reaches 67.3% while test accuracy achieves 36.21% across Wikipedia domains
- Performance degrades on test sets, indicating potential overfitting on small corpora
- GCN captures global context through word co-occurrence patterns, complementing LSTM's local sequential modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph convolution aggregates co-occurrence neighborhoods to encode global semantic context into word embeddings.
- Mechanism: A word co-occurrence graph G=(V,E) is constructed where nodes=unique words, edges=co-occurrence in text. Two-layer GCN performs message-passing to compute weighted aggregations of 1-hop and 2-hop neighbors per node (Equation 1: h_{i+1}=σ(wh_iA)). This produces 64-dim embeddings that reflect each word's global context—the set of words it commonly appears near across the corpus.
- Core assumption: Co-occurrence patterns in the training corpus capture meaningful semantic relationships that generalize to unseen sequences.
- Evidence anchors:
  - [abstract] "constructs a word co-occurrence graph from Wikipedia articles and uses Graph Convolutional Networks (GCNs) to learn global context embeddings"
  - [section] "In each layer of GCN, graph convolution captures a node's neighborhood information by one hop. Therefore, in the case of language modeling, graph convolution helps to preserve the context of a word."
  - [corpus] Weak direct validation—no corpus papers verify this specific graph-language mechanism; GraphSense uses graph embeddings for code but different domain.

### Mechanism 2
- Claim: LSTM processes ordered n-gram sequences to capture local sequential dependencies for next-word prediction.
- Mechanism: Many-to-one LSTM takes padded n-gram sequences (1-gram through 10-gram) as input, where each token is represented by its GCN-learned embedding. The LSTM's gating mechanisms (forget gate f_t, input gate i_t, output gate o_t per Equations 2-6) selectively retain relevant context across the sequence to predict the next word.
- Core assumption: Local word order contains predictive signal beyond what global co-occurrence captures.
- Evidence anchors:
  - [section] "LSTM networks are a special kind of RNN that is capable of learning long-term dependencies by remembering the information/sequence for a longer period of time."
  - [section] "To capture the local context efficiently, many-to-one LSTM can be trained on n-gram data."
  - [corpus] Once Upon a Time paper confirms LSTM/next-word prediction paradigms but focuses on interactive learning, not graph-LSTM hybrid.

### Mechanism 3
- Claim: Separating global context learning (GCN) from local sequence learning (LSTM) provides complementary signals that jointly improve prediction.
- Mechanism: Two-stage architecture—GCN is first trained independently on link prediction to learn context-aware node embeddings; these frozen embeddings then serve as input representations for LSTM training. This factorization allows global semantic structure to be learned once and reused, while LSTM focuses purely on sequential patterns.
- Core assumption: Global and local context are separable and their combination is additive rather than requiring joint optimization.
- Evidence anchors:
  - [abstract] "combines Graph Neural Networks (GNNs) and LSTMs to capture both global and local context from text"
  - [section] "As shown in the figure, we train two models independently. For the GCN model, we initialize the embeddings... and perform two-hop Graph convolution. We train the GCN model for the Link Prediction task."
  - [corpus] No direct corpus validation of GCN-LSTM factorization for language; related GNN works focus on other domains (POI recommendation, code analysis).

## Foundational Learning

- **Graph Neural Networks (GCN fundamentals)**
  - Why needed here: Understanding how message-passing aggregates neighbor information is essential to grasp why GCN embeddings encode "global context" vs. just local word vectors.
  - Quick check question: Given a 2-layer GCN, what is the effective receptive field for any node's embedding?

- **LSTM gating mechanics (forget/input/output gates)**
  - Why needed here: The paper relies on LSTM's ability to selectively retain context over n-gram sequences; knowing what each gate controls helps debug sequence modeling issues.
  - Quick check question: If the forget gate outputs values near 0 for all timesteps, what happens to long-range dependencies?

- **Word embeddings and distributional semantics**
  - Why needed here: The entire approach builds on representing words as dense vectors where proximity implies semantic similarity; this is the baseline the paper compares against (random embeddings).
  - Quick check question: Why would a random embedding baseline underperform compared to learned embeddings even with the same LSTM architecture?

## Architecture Onboarding

- **Component map**:
Text Corpus → Preprocessing → Word Co-occurrence Graph → GCN (2 layers, 64-dim) → Node Embeddings
                                                              ↓
N-gram Sequences (1-10 gram) → Embedding Lookup → LSTM (200 hidden units) → Softmax → Next Word

- **Critical path**:
  1. Graph construction quality—sparse graphs produce disconnected components with no neighborhood signal
  2. GCN link prediction training—if GCN doesn't learn meaningful structure, embeddings remain near-random
  3. Vocabulary alignment—words in LSTM sequences must exist in GCN graph or you have OOV failures

- **Design tradeoffs**:
  - Two-stage training (GCN→LSTM) vs. end-to-end: Current design is simpler but embeddings can't adapt to prediction task
  - Fixed n-gram padding vs. variable length: Post-padding with zeros enables batching but introduces noise
  - Small corpus (60-76K words) vs. generalization: Limited vocabulary constrains real-world applicability but enables resource-constrained experimentation

- **Failure signatures**:
  - Training accuracy high (~67%) but test accuracy low (~22-36%): Classic overfitting on small corpus
  - Predictions semantically related but syntactically wrong (e.g., "the new → bestselling"): GCN capturing topical similarity but LSTM missing grammatical patterns
  - Disconnected graph components: Words in sparse neighborhoods get near-random embeddings

- **First 3 experiments**:
  1. **Baseline sanity check**: Train LSTM with random embeddings (RE) on same n-gram data—confirm CE outperforms RE as paper reports (it does: CE-test 36.21% vs RE-test 24.8-26.43%)
  2. **Ablation on GCN depth**: Test 1-layer vs 2-layer GCN—hypothesis: 1-layer captures only immediate co-occurrence, should degrade performance
  3. **Embedding analysis**: Apply t-SNE to learned GCN embeddings—verify semantically similar words cluster together (paper explicitly notes this as future work needed to validate assumption)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the node embeddings learned by the Graph Convolutional Network effectively capture semantic context?
- **Basis in paper:** [explicit] The authors state in the Discussion: "We will need to perform analysis like T-SNE plot on the learned node embeddings from GCN and run empirical tests to verify if the node embeddings encode the context or not."
- **Why unresolved:** The study assumes the GCN captures "global context" via co-occurrence, but the qualitative validity of these embeddings remains unverified.
- **What evidence would resolve it:** Visualizations (e.g., t-SNE) showing clustering of semantically similar words or performance gains on standard word analogy benchmarks.

### Open Question 2
- **Question:** Would alternative Graph Neural Network architectures improve performance over standard GCNs?
- **Basis in paper:** [explicit] The authors list this as a future direction: "Another direction that we would like to explore is to try different types of Graph Neural Networks like GraphSAGE, GIN, GAT, and others."
- **Why unresolved:** The current implementation relies on standard GraphConv layers; alternative architectures might handle the dynamic or structural nature of word co-occurrence graphs more efficiently.
- **What evidence would resolve it:** Comparative benchmarks showing test accuracy improvements when substituting the GCN layer with GraphSAGE, GIN, or GAT layers on the same corpus.

### Open Question 3
- **Question:** Can the generalization gap be reduced without significantly increasing the dataset size?
- **Basis in paper:** [inferred] The paper notes training accuracies up to 67.3% versus test accuracies of only 36.21%, attributing this to limited data, but leaving open whether architectural changes could mitigate overfitting.
- **Why unresolved:** While the authors cite resource constraints, it is unclear if the decoupled training (GCN trained on link prediction, LSTM on n-grams) contributes to the poor generalization compared to end-to-end approaches.
- **What evidence would resolve it:** Experiments applying regularization (dropout, weight decay) or end-to-end training to observe if the test accuracy increases relative to the training accuracy.

## Limitations
- Extremely small dataset size (60-76K words) causes severe overfitting with large gap between training (67.3%) and test (36.21%) accuracy
- Co-occurrence graph construction method underspecified - exact window definition for edge creation remains unclear
- GCN link prediction training details (loss function, negative sampling) omitted, preventing faithful reproduction

## Confidence
- **High confidence**: GCN embeddings capture global context through message-passing over co-occurrence neighborhoods - well-grounded in established GNN literature
- **Medium confidence**: Separating GCN and LSTM training provides complementary signals - plausible but not empirically validated
- **Low confidence**: Context Embedding method significantly outperforms random embeddings - difficult to verify due to missing implementation details

## Next Checks
1. **Corpus size sensitivity analysis**: Systematically evaluate model performance as corpus size increases from 60-76K words to larger datasets (100K, 500K, 1M words) to quantify overfitting and determine minimum viable corpus size

2. **End-to-end vs. two-stage training comparison**: Implement and compare the proposed two-stage architecture against an end-to-end trainable model where GCN embeddings are updated during LSTM optimization to validate factorization assumption

3. **Graph quality and connectivity audit**: Analyze co-occurrence graph structural properties including node degree distribution, connected components, and clustering coefficient; verify semantically similar words cluster in embedding space using t-SNE visualization