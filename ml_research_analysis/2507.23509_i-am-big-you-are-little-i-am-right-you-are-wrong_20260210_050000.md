---
ver: rpa2
title: I Am Big, You Are Little; I Am Right, You Are Wrong
arxiv_id: '2507.23509'
source_url: https://arxiv.org/abs/2507.23509
tags:
- image
- different
- size
- classification
- mpss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the decision-making processes of different
  image classification models by analyzing their minimal sufficient pixel sets (MPSs),
  which represent the smallest pixel sets required for classification. Using the ReX
  tool, the authors compare 15 models across five architectures (ConvNext, EV A, Inception,
  ResNet, and ViT) on ImageNet-1k.
---

# I Am Big, You Are Little; I Am Right, You Are Wrong

## Quick Facts
- arXiv ID: 2507.23509
- Source URL: https://arxiv.org/abs/2507.23509
- Authors: David A. Kelly; Akchunya Chanchal; Nathan Blake
- Reference count: 40
- Primary result: Larger models produce smaller minimal sufficient pixel sets (MPSs) that may indicate overfitting

## Executive Summary
This study analyzes decision-making processes across 15 image classification models by examining their minimal sufficient pixel sets (MPSs) using the ReX tool. The research reveals statistically significant differences in MPS size and location across five architectures, with larger models like ConvNext and EVA producing smaller, more spatially distinct MPSs. The analysis shows that incorrect classifications are associated with larger MPSs than correct classifications (2.6% increase on average). These findings suggest that large models may be more prone to overfitting, as they can classify images using very few pixels. The research provides insights into model behavior that could inform future model development and evaluation.

## Method Summary
The study uses ReX to extract minimal sufficient pixel sets from 15 models across five architectures (ConvNext, EVA, Inception, ResNet, and ViT) on ImageNet-1k. ReX treats models as black-box causal systems, iteratively partitioning images into superpixels and masking combinations to identify the smallest pixel sets that maintain classification. The tool computes MPS size ratios, spatial overlap using Sørensen-Dice coefficient, and Hausdorff distances between architectures. Statistical analysis employs Kruskal-Wallis H tests for cross-architecture comparisons and Friedman tests for within-architecture comparisons, examining differences between correct and incorrect classifications.

## Key Results
- ConvNext-V2 and EVA models produce MPSs averaging 5-7% of input image size, while Inception models use ~24%
- Incorrect classifications have statistically larger MPSs than correct classifications (2.6% increase on average)
- MPSs from different architectures show low spatial overlap (Dice coefficients ~0.5) and high Hausdorff distances
- Larger models trained on bigger corpora tend to be "myopic," relying on very small regions for classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimal sufficient pixel sets (MPSs) reveal model "concentration" — the smallest pixel subset that reproduces the original classification.
- Mechanism: ReX treats the model as a black-box causal model in the Halpern-Pearl sense. It iteratively partitions images into superpixels, creates mutants by masking combinations with a baseline value (0), and distributes causal responsibility based on which superpixels appear in "passing" mutants that maintain classification. The process refines toward pixels with highest responsibility.
- Core assumption: A minimal pixel set that reproduces classification reveals genuine model decision-making rather than artifact of the perturbation method. Also assumes baseline value of 0 produces meaningful OOD mutants.
- Evidence anchors:
  - [abstract] "we propose using minimal sufficient pixels sets to gauge a model's 'concentration': the pixels that capture the essence of an image through the lens of the model"
  - [section 3] "ReX views a model as a black-box causal model in the Halpern-Pearl sense... with its inputs being the individual pixels of an image"
  - [corpus] Related work on causal attacks (arxiv:2512.03730) uses similar black-box perturbation strategies for object detectors, suggesting cross-domain validity of perturbation-based analysis.
- Break condition: If models behave fundamentally differently under baseline masking vs. blur-based perturbations, MPS characteristics may not generalize. If multiple disjoint explanations exist (as noted in Section 3), the "best" MPS may not capture full decision logic.

### Mechanism 2
- Claim: Larger models pre-trained on bigger corpora produce smaller MPSs, indicating "myopic" decision-making that may signal overfitting.
- Mechanism: Large-scale pre-training appears to concentrate model attention on fewer, highly discriminative regions. EVA-Giant (1B parameters) uses only 5.4% of input pixels on average vs. Inception models using ~24%.
- Core assumption: Smaller MPS indicates overfitting rather than more efficient feature learning. The paper notes this "may indicate overfitting" but does not prove it.
- Evidence anchors:
  - [abstract] "larger models trained on more data tend to be 'myopic,' relying on very small regions of images for classification, which may indicate overfitting"
  - [section 4.4] "These very small MPSs (see Figure 4b) may indicate overfitting"
  - [corpus] Weak direct evidence. Neighbors don't address model size vs. explanation sparsity relationships.
- Break condition: If small MPSs actually reflect more efficient/robust feature learning rather than overfitting, the safety concern framing collapses. Would need to test MPS size against adversarial robustness or distribution shift performance.

### Mechanism 3
- Claim: Misclassified images have statistically larger MPSs than correctly classified ones (2.6% average increase).
- Mechanism: When models struggle with classification (output disagrees with ground truth), they require more pixels to reach a decision threshold, suggesting diffuse or conflicting feature representations.
- Core assumption: Ground truth labels in ImageNet-1K are accurate. The paper acknowledges labels are "slightly noisy" which may confound this analysis.
- Evidence anchors:
  - [abstract] "misclassified images are associated with larger pixels sets than correct classifications"
  - [section 4.2] "Our second null hypothesis is that incorrect classifications have the same size MPS as correct classifications... having the incorrect classification increased the explanation size by 2.6% (standard error 0.4%, p < 0.01)"
  - [corpus] No direct corpus support for this specific finding.
- Break condition: If label noise in ImageNet is substantial, the "correct vs. incorrect" partition may mischaracterize the relationship. Effect size (2.6%) is statistically significant but may lack practical significance.

## Foundational Learning

- Concept: **Halpern-Pearl Causal Models and Actual Causality**
  - Why needed here: ReX is built on actual causality formalism; understanding Boolean causal variables, sufficiency, and responsibility is essential to interpret MPS validity.
  - Quick check question: Can you explain why a pixel being "minimally sufficient" for classification differs from a pixel being "necessary" for classification?

- Concept: **Black-box vs. White-box XAI Methods**
  - Why needed here: The paper positions ReX against GradCAM, LIME, SHAP, and RISE. Understanding why patch-based or gradient-based methods produce different explanations clarifies MPS advantages.
  - Quick check question: Why might SHAP's pixel ranking not isolate a minimal sufficient set for classification reproduction?

- Concept: **Non-parametric Statistical Testing (Kruskal-Wallis, Friedman)**
  - Why needed here: The paper's claims rest on statistical significance across architectures. Understanding these tests prevents misinterpreting p-values as effect sizes.
  - Quick check question: Why use Kruskal-Wallis instead of ANOVA for comparing MPS sizes across architectures?

## Architecture Onboarding

- Component map:
  ReX Engine -> Model Wrapper -> Comparison Metrics -> Statistical Analysis

- Critical path:
  1. Convert PyTorch models to ONNX (opset 20)
  2. Run ReX with fixed hyperparameters and random seed across all models
  3. Compute MPS size ratios relative to input image size
  4. Apply Kruskal-Wallis H test (cross-architecture) and Friedman test (intra-architecture)
  5. Calculate DC and Hausdorff on best-performing model from each architecture

- Design tradeoffs:
  - **Baseline value (0) vs. blur**: Paper uses 0, creating OOD mutants. Different baselines may yield different MPS characteristics — unexplored.
  - **Single MPS vs. multiple explanations**: Paper uses only "best" MPS (highest responsibility), ignoring multiple disjoint explanations that may exist.
  - **Patch-free vs. patch-based**: ReX is not bound to fixed patches (unlike SAG), enabling smaller MPSs but with NP-complete exact computation.

- Failure signatures:
  - Very small MPSs (<6% of image) with high model confidence may indicate overfitting
  - Large Hausdorff distances between architectures suggest fundamentally different decision strategies
  - MPS not overlapping human segmentation (noted in prior work) suggests misaligned feature learning

- First 3 experiments:
  1. Reproduce MPS extraction on 50 images across ConvNext-V2 and Inception-V4 to verify the 3.6× size difference reported in Table 1.
  2. Test sensitivity to baseline value: run ReX with blur baseline instead of 0 on 20 images and compare MPS size/location.
  3. Validate "myopia" hypothesis: correlate MPS size with adversarial robustness scores from existing benchmarks for the same 15 models.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on ImageNet-1k labels, which are acknowledged as "slightly noisy," potentially confounding the correct/incorrect classification analysis
- The use of baseline value 0 for masking may not represent the most natural or robust perturbation strategy, and alternative baselines could yield different MPS characteristics
- While statistical significance is established, effect sizes (2.6% increase for incorrect classifications, 3.6× size difference between architectures) may lack practical significance

## Confidence
- **High confidence**: Cross-architecture MPS size differences (Kruskal-Wallis results), correct vs. incorrect classification differences (Friedman test results), and general ReX methodology framework
- **Medium confidence**: MPS size differences within architectures, spatial overlap/centrality differences, and the myopia-overfitting hypothesis
- **Low confidence**: Causal claims about what MPS characteristics reveal about model decision-making, practical implications of MPS differences for real-world deployment

## Next Checks
1. **Label noise sensitivity test**: Repeat the correct/incorrect MPS analysis using a subset of ImageNet images with verified ground truth labels to assess robustness to label noise.
2. **Baseline sensitivity analysis**: Run ReX with alternative perturbation strategies (blur, mean pixel value, adversarial perturbation) on 100 images to measure MPS size/location sensitivity to perturbation method.
3. **Generalization across tasks**: Apply ReX to non-ImageNet datasets (medical imaging, satellite imagery) to test whether MPS characteristics generalize beyond natural image classification.