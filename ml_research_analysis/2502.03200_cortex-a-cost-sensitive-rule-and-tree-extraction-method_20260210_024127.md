---
ver: rpa2
title: 'CORTEX: A Cost-Sensitive Rule and Tree Extraction Method'
arxiv_id: '2502.03200'
source_url: https://arxiv.org/abs/2502.03200
tags:
- cortex
- rule
- methods
- rules
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces CORTEX, a cost-sensitive rule and tree extraction
  method designed to enhance the interpretability of machine learning models. CORTEX
  extends the cost-sensitive decision tree (CSDT) method to handle multi-class classification
  problems by incorporating an n-dimensional class-dependent cost matrix.
---

# CORTEX: A Cost-Sensitive Rule and Tree Extraction Method

## Quick Facts
- arXiv ID: 2502.03200
- Source URL: https://arxiv.org/abs/2502.03200
- Reference count: 11
- CORTEX generates interpretable IF-THEN rules from neural networks with competitive fidelity and compact rule sets

## Executive Summary
CORTEX is a cost-sensitive rule and tree extraction method designed to enhance the interpretability of machine learning models, particularly neural networks. The method extends the cost-sensitive decision tree (CSDT) framework to handle multi-class classification problems by incorporating an n-dimensional class-dependent cost matrix. This approach addresses class imbalance by favoring minority classes during tree construction. CORTEX generates decision trees and extracts human-understandable IF-THEN rules, balancing interpretability and predictive performance. Experimental results demonstrate that CORTEX produces smaller rule sets with shorter average rule lengths compared to other rule-extraction methods, while maintaining competitive accuracy, fidelity, and completeness across diverse datasets.

## Method Summary
CORTEX is a post-hoc rule-based XAI surrogate model that explains trained neural networks by extracting IF-THEN rules from a cost-sensitive decision tree (CSDT) extended to multi-class classification. The method builds a K×K cost matrix from class imbalance ratios (Cij = (Ni + Nj)/Ni), trains a multi-class CSDT on test set predictions using cost-sensitive node labeling, and converts root-to-leaf paths to rules. Default cost matrix and one-hot encoding for nominal features are used. The algorithm is evaluated on 8 UCI datasets against 5 baseline extractors using 6 metrics: completeness, correctness, fidelity, robustness, rule count, and average rule length across 100 random splits.

## Key Results
- CORTEX achieves 100% completeness across all datasets, ensuring all samples are covered by rules
- Produces smaller rule sets with shorter average rule lengths compared to baselines (e.g., 12 rules with 4 antecedents on credit dataset)
- Maintains competitive fidelity (~0.875 on credit) while outperforming baselines on high-cardinality targets like abalone (29 classes)

## Why This Works (Mechanism)

### Mechanism 1: N-Dimensional Class-Dependent Cost Matrix Extension
Multi-class classification is enabled by generalizing the binary cost-sensitive decision tree framework to K classes through an n-dimensional cost matrix. The algorithm builds a K×K cost matrix C where C(i,j) encodes the cost of predicting class j when the true class is i. Terminal nodes are labeled by minimizing total misclassification cost, not by majority voting. The default cost matrix Cij = (Ni + Nj)/Ni encodes class imbalance ratios between class pairs.

### Mechanism 2: Cost-Sensitive Soft-Labeling for Minority Class Protection
Cost-sensitive probabilities computed at terminal nodes allow minority classes to dominate node labels even with fewer samples, if misclassification costs justify it. Instead of hard-labeling nodes by sample count, CORTEX computes pmk = 1 - avgcost(f(k))/Σavgcost(f(i)). High misclassification costs for minority classes yield higher probabilities, enabling minority-class terminal labels despite fewer samples.

### Mechanism 3: Tree-Path-to-Rule Conversion for Human Comprehensibility
Converting tree paths to IF-THEN rules improves stakeholder comprehensibility while preserving complete decision logic. Each root-to-leaf path becomes a rule with feature conditions as antecedents (AND-conjoined) and terminal class as consequent. This transforms hierarchical visualization into linear textual rules.

## Foundational Learning

- Concept: Cost Matrix Fundamentals in Classification
  - Why needed here: CORTEX's core innovation is the cost matrix; understanding how C(i,j) encodes asymmetric penalties is essential for configuration
  - Quick check question: For a 3-class problem (A=100 samples, B=50, C=10), what does C(A,C)=11 vs C(C,A)=1.1 imply about misclassification priorities?

- Concept: Surrogate Model Fidelity vs. Correctness
  - Why needed here: CORTEX is evaluated by how well it mimics the neural network (fidelity) and ground truth (correctness); these differ
  - Quick check question: If a neural network has 70% accuracy and CORTEX achieves 95% fidelity, what is the maximum possible correctness?

- Concept: Rule Extraction Evaluation Metrics
  - Why needed here: Six metrics (completeness, correctness, fidelity, robustness, # rules, avg rule length) capture different quality dimensions with inherent tradeoffs
  - Quick check question: Why might minimizing rules conflict with robustness to input perturbations?

## Architecture Onboarding

- Component map: Cost Matrix Constructor -> Multi-class CSDT Builder -> Probability Calculator -> Rule Extractor -> Evaluation Suite
- Critical path: Pre-trained neural network + test data -> NN predictions -> Build cost matrix -> Train CSDT on (X_test, y_pred_NN) -> Extract rules -> Evaluate against y_true and y_pred_NN
- Design tradeoffs:
  - Rule simplicity vs. fidelity: CORTEX ranks 2nd-3rd for # rules/length but 3rd for fidelity; tree-based methods lead fidelity
  - Robustness vs. compactness: CORTEX ranks 4th for robustness (sensitive to noise) despite simpler rules
  - Manual vs. automatic cost matrix: Tuning intractable for K>3 classes; default matrix may misalign with domain priorities
- Failure signatures:
  - Low completeness (<100%): Indicates uncovered samples (CORTEX achieves 100%)
  - Rule count >50 for small datasets: Overfitting or insufficient pruning
  - Fidelity <70%: Rules poorly approximate NN behavior
  - Low robustness (~0.2 on abalone/yeast): Observed weakness—noise flips predictions
- First 3 experiments:
  1. Reproduce credit dataset results: Train NN-1 (1 hidden layer), apply CORTEX, verify ~12 rules with ~4 antecedents, compare fidelity against DT baseline (expect ~0.875 vs 1.0)
  2. Multi-class stress test on abalone (29 classes): Validate CORTEX outperforms DT on high-cardinality targets (results show CORTEX ranked highest for this dataset)
  3. Cost matrix ablation: Replace default Cij with uniform costs; expect reduced minority-class sensitivity and fidelity drop—quantifies cost matrix contribution

## Open Questions the Paper Calls Out

### Open Question 1
Can the CORTEX algorithm be refined to improve its robustness to minor input perturbations without compromising rule simplicity? The conclusion states that to enhance reliability, the algorithm needs refinement so that "minor input modifications do not lead to significant changes in the model’s predictions." The results show CORTEX ranks only 4th in robustness compared to other methods like TREPAN, indicating a specific weakness in stability. Evidence would require a modified CORTEX implementation demonstrating statistically significantly higher robustness scores while maintaining high fidelity and low rule counts.

### Open Question 2
How does the definition of the cost matrix impact CORTEX's ability to balance fidelity and rule complexity? The authors note that the default cost matrix is based on class imbalance ratios, but "future work could explore modifications of the cost matrix to enhance the proposed CORTEX’s performance." The current approach relies on a specific heuristic for costs; it is unclear if domain-specific or tuned cost matrices yield better surrogate models. Evidence would come from comparative experiments showing that custom cost matrices significantly improve fidelity or correctness metrics over the default imbalance-ratio-based matrix on specific datasets.

### Open Question 3
Does CORTEX maintain its competitive performance when explaining more complex neural network architectures? The conclusion outlines that "various neural network architectures will be examined based on the complexity of new datasets" in future work. The current study is restricted to feed-forward networks with one or two hidden layers (NN-1 and NN-2). Evidence would come from benchmarks of CORTEX on deep architectures (e.g., CNNs or RNNs) demonstrating that it retains high fidelity and compact rule sets compared to baseline extractors.

## Limitations

- CORTEX shows poor robustness (~0.2) on complex datasets like abalone and yeast, indicating sensitivity to input perturbations
- Default cost matrix Cij = (Ni + Nj)/Ni may not align with domain priorities, especially for K > 3 classes where manual tuning becomes intractable
- Rule extraction inherently involves fidelity tradeoffs—CORTEX achieves 100% completeness but only ~0.875 fidelity on credit dataset versus 1.0 for decision trees

## Confidence

- High: Rule extraction from tree paths is a well-established mechanism with clear implementation paths
- Medium: Cost-sensitive soft-labeling and multi-class cost matrix extension are novel but untested outside this study
- Low: Default cost matrix generalization across diverse domains without manual calibration is unproven

## Next Checks

1. Cost matrix sensitivity analysis: Systematically vary Cij parameters to quantify impact on minority class protection and overall fidelity
2. Robustness enhancement: Implement noise-robust tree pruning or rule smoothing techniques to improve stability metrics
3. Cross-domain applicability: Apply CORTEX to non-UCI domains (medical, financial) with known class imbalance to test default cost matrix effectiveness