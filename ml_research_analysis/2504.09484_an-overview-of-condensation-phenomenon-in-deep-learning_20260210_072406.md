---
ver: rpa2
title: An overview of condensation phenomenon in deep learning
arxiv_id: '2504.09484'
source_url: https://arxiv.org/abs/2504.09484
tags:
- neural
- training
- condensation
- network
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The condensation phenomenon describes how neurons in the same layer
  of deep neural networks tend to group into clusters with similar outputs during
  nonlinear training. This behavior becomes more pronounced as training progresses,
  with neurons aligning into fewer distinct groups early in training and increasing
  in number as training continues.
---

# An overview of condensation phenomenon in deep learning

## Quick Facts
- **arXiv ID**: 2504.09484
- **Source URL**: https://arxiv.org/abs/2504.09484
- **Reference count**: 13
- **Primary result**: Neurons in deep neural networks group into clusters with similar outputs during nonlinear training, a phenomenon called condensation that improves generalization and enables pruning.

## Executive Summary
The condensation phenomenon describes how neurons in the same layer of deep neural networks tend to group into clusters with similar outputs during nonlinear training. This behavior becomes more pronounced as training progresses, with neurons aligning into fewer distinct groups early in training and increasing in number as training continues. The authors demonstrate that small weight initializations and dropout optimization facilitate this condensation process. Through theoretical analysis and empirical experiments across various network architectures including CNNs and residual networks, they show that condensation offers insights into neural network generalization and reasoning abilities. The phenomenon suggests that over-parameterized networks can behave like much narrower networks, providing a potential pruning strategy.

## Method Summary
The study uses synthetic 1D functions, MNIST, ImageNet, and composite function tasks for Transformers to demonstrate condensation. The primary metric is cosine similarity between neuron weight vectors, with feature visualization tracking neuron orientations (θk, Ak) over time. The method employs small initialization N(0, 1/m^γ) with γ ≥ 4 or dropout (p≈0.9) with normal initialization. Training uses full batch gradient descent for CNN experiments and standard optimizers for other cases. The minimum viable reproduction involves implementing a two-layer ReLU network on synthetic data, tracking feature maps over training, and computing cosine similarity heatmaps.

## Key Results
- Small weight initialization (γ > 1) and dropout optimization both facilitate condensation
- Condensation enables over-parameterized networks to behave like narrower networks, suggesting pruning strategies
- The authors introduce an optimistic estimation method for sample size requirements based on perfectly condensed networks

## Why This Works (Mechanism)

### Mechanism 1: Small Initialization Forces Condensation
Small weight initialization forces the network into a nonlinear "condensation regime" where neurons rapidly align into a few functional clusters. In the small initialization limit (γ > 1), neurons within a layer follow similar dynamics governed by the leading-order term of the activation function's Taylor expansion (defined by "multiplicity p"). Because they share symmetric ODEs and evolve toward a finite number of stable points, they collapse into specific orientations (clusters) rather than moving independently. If initialization is too large (γ < 1), the system enters the linear regime (NTK); neurons move minimally from initialization, and condensation does not occur.

### Mechanism 2: Dropout Induces Condensation
Dropout optimization induces condensation by enforcing functional redundancy, even without small initializations. Dropout randomly deactivates neurons during training. To maintain performance, surviving neurons must compensate for the missing ones. This pressure drives the network toward a solution where neurons within a layer are functionally interchangeable (similar outputs), resulting in condensation. If the dropout rate is 0% or the network is severely under-parameterized, neurons may specialize rather than generalize into clusters.

### Mechanism 3: Embedding Principle
The "Embedding Principle" explains why over-parameterized networks find condensed solutions; the loss landscape of a wide network contains the critical points of all narrower networks. A wide network can simulate a narrow one by clustering neurons (e.g., two neurons having identical weights and split output weights act as one). These "embedded" solutions are critical points (saddles or minima) in the high-dimensional loss landscape. Explicit regularization (e.g., forcing orthogonal weights) could prevent the network from settling into these embedded/condensed states.

## Foundational Learning

- **Neural Tangent Kernel (NTK) / Linear Regime vs. Nonlinear Regime**: The paper distinguishes condensation as a phenomenon exclusive to the "nonlinear regime." Without understanding that small initialization moves the system away from NTK behavior (kernel methods), the emergence of feature learning (condensation) is unintuitive. Quick check: Does the network behave like a fixed kernel (linear regime) or do features evolve significantly during training (nonlinear regime)?

- **Cosine Similarity**: This is the primary metric used to quantify "condensation." It measures the alignment of weight vectors. Quick check: If two weight vectors have a cosine similarity of 1.0, are they functionally redundant for a ReLU activation?

- **Multiplicity of Activation Functions (p)**: The paper uses the derivative order at zero (p) to theoretically predict the number of initial clusters (max 2p). Quick check: For a standard Tanh activation where σ'(0) ≠ 0, what is the multiplicity p, and how many initial cluster directions does the theory predict?

## Architecture Onboarding

- **Component map**: Input/Hidden Layers -> Neurons (grouped by features (θk, Ak)) -> Metric Layer (compute Cosine Similarity Matrices)

- **Critical path**:
  1. **Initialization Strategy**: Select initialization scale γ (e.g., N(0, 1/m^γ))
     - γ ≤ 1: Linear regime (no condensation expected)
     - γ > 1: Nonlinear regime (condensation expected)
  2. **Optimizer Choice**: Standard SGD/Adam for nonlinear dynamics; or SGD + Dropout for implicit condensation

- **Design tradeoffs**:
  - **Speed vs. Interpretability**: Small initialization (high γ) guarantees condensation but significantly slows convergence. Dropout offers a middle ground, achieving condensation with faster convergence than small init.
  - **Generalization vs. Cluster Count**: Early training = few clusters (low complexity). Late training = many clusters (better fit). You must tune γ to balance the "simplicity bias" with the capacity to fit data.

- **Failure signatures**:
  - **Static Neurons**: (Section 3) Neurons that never activate (zero gradient). This is specific to ReLU and indicates wasted capacity.
  - **Linear Collapse**: If γ is too small, the network stays in the NTK regime; neurons remain random and dispersed, failing to learn features or condense.

- **First 3 experiments**:
  1. **Visualize Condensation**: Train a 2-layer ReLU network on 1D synthetic data. Plot the features (θk, Ak) over time to confirm neurons align into discrete orientations (replicate Fig 1).
  2. **Initialization Ablation**: Train the same network varying γ ∈ {0.5, 1.0, 2.0, 4.0}. Plot the cosine similarity heatmap at the final layer for each. Verify that higher γ yields "bluer" blocks (higher similarity).
  3. **Dropout Equivalence**: Compare two runs: (A) Small Init (γ=4) vs. (B) Normal Init (γ=1) + Dropout. Plot loss curves and final cosine similarity to verify if Dropout recovers the condensation behavior of Small Init faster (replicate Fig 9).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the precise theoretical conditions that determine when early condensation improves versus harms final convergence?
- **Basis in paper**: The authors note that initial condensation "will do harm to the final convergence with the initialization that |aj| ≥ ||wj||" while also showing condensation facilitates learning.
- **Why unresolved**: The paper identifies both beneficial and harmful cases of condensation but lacks a unified theoretical framework predicting which outcome occurs under specific conditions.
- **What evidence would resolve it**: A theoretical characterization of initialization regimes and hyperparameter settings that predict condensation's effect on convergence, validated across architectures.

### Open Question 2
- **Question**: How can the optimal degree of condensation be quantified and controlled for pruning without performance degradation?
- **Basis in paper**: The authors state "if a neural network is not in an extremely condensed state, such reduction can potentially harm performance, depending on the degree of condensation."
- **Why unresolved**: No quantitative metric exists for determining when condensation is sufficient for safe pruning, and the trade-off between condensation degree and retained accuracy remains empirical.
- **What evidence would resolve it**: Development of a condensation sufficiency metric that predicts pruning success rates across tasks and architectures.

### Open Question 3
- **Question**: What mechanisms causally link condensation to improved reasoning abilities in transformers?
- **Basis in paper**: The paper observes correlation between condensation and reasoning but notes only "A straightforward rationale is as follows" without formal proof.
- **Why unresolved**: The observed correlation between condensation and OOD generalization lacks causal mechanistic explanation.
- **What evidence would resolve it**: Intervention experiments that manipulate condensation independently of other factors to isolate its causal effect on reasoning metrics.

## Limitations
- The theoretical framework relies heavily on asymptotic analysis in the small initialization limit, which may not fully capture behavior in practical networks
- The multiplicity p theory for predicting cluster counts has been validated empirically for ReLU but lacks formal proofs for more complex activations
- Direct mathematical verification of the Hessian zero-eigenvalue claims in the Embedding Principle remains limited

## Confidence

**High Confidence**: The empirical observations of condensation across multiple architectures (CNNs, ResNets, Transformers) and the basic mechanism of small initialization forcing nonlinear dynamics. The cosine similarity measurements showing neuron clustering are straightforward and reproducible.

**Medium Confidence**: The theoretical predictions about cluster count based on activation function multiplicity, and the mathematical framework connecting initialization scale to the phase diagram (linear vs. nonlinear regime). These require careful parameter tuning and may be sensitive to implementation details.

**Low Confidence**: The specific claims about the Embedding Principle and the exact relationship between dropout rate and condensation speed. The corpus lacks direct validation of these theoretical constructs, relying instead on indirect evidence from related work.

## Next Checks

1. **Initialization Phase Diagram Verification**: Systematically vary initialization scale γ across a wide range (0.1 to 10) for a fixed architecture and plot the final cosine similarity heatmap. This would empirically validate the theoretical phase transition between linear and nonlinear regimes.

2. **Dropout Rate Sensitivity Analysis**: Train networks with normal initialization across dropout rates from 0% to 90% and measure both training speed and final condensation quality. This would quantify the trade-off between convergence speed and condensation strength that the paper claims exists.

3. **Cross-Architecture Generalization Test**: Apply the same condensation analysis to architectures not mentioned in the paper (e.g., Vision Transformers, MLPs with different activation functions like Swish or GELU). This would test whether condensation is truly a universal phenomenon or specific to certain network families.