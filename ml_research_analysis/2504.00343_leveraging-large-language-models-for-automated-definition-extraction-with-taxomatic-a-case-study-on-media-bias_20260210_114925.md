---
ver: rpa2
title: Leveraging Large Language Models for Automated Definition Extraction with TaxoMatic
  A Case Study on Media Bias
arxiv_id: '2504.00343'
source_url: https://arxiv.org/abs/2504.00343
tags:
- bias
- media
- nitions
- prompting
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TaxoMatic is an LLM-based framework for automated definition extraction
  from academic literature. It processes scientific articles through relevance classification
  and definition extraction steps, validated on a media bias dataset of 2,398 manually
  rated papers.
---

# Leveraging Large Language Models for Automated Definition Extraction with TaxoMatic A Case Study on Media Bias

## Quick Facts
- arXiv ID: 2504.00343
- Source URL: https://arxiv.org/abs/2504.00343
- Reference count: 20
- LLMs can extract definitions from academic papers, with TaxoMatic achieving F1=0.381 for relevance classification and cosine similarity of 0.557 for definition extraction

## Executive Summary
TaxoMatic is an LLM-based framework designed to automate definition extraction from scientific literature, addressing the challenge of inconsistent conceptual terminology across disciplines. The framework processes academic articles through a two-stage pipeline: first identifying papers containing relevant definitions (relevance classification), then extracting and refining those definitions. Validation on a media bias dataset of 2,398 manually rated papers demonstrated the framework's potential while revealing areas for improvement. The study shows that while LLMs can effectively identify relevant papers and extract definitions, performance varies significantly based on prompting strategies and model selection.

## Method Summary
The TaxoMatic framework employs a two-stage approach to definition extraction. First, it classifies academic papers as containing or not containing relevant definitions using various prompting strategies and LLM models. The study tested six different prompts with four different LLM models, ultimately finding that Claude-3-sonnet with Chain-of-Thought prompting achieved the highest F1-score of 0.381 for relevance classification. In the second stage, the framework extracts definitions from classified relevant papers using refined prompts, comparing extracted definitions against ground truth using cosine similarity. The system successfully processed 113 relevant papers, extracting 123 definitions, though performance showed significant variability depending on the specific prompting approach and model used.

## Key Results
- Claude-3-sonnet achieved highest F1-score of 0.381 for relevance classification
- Chain-of-Thought and Role prompting delivered best definition extraction performance with median cosine similarity of 0.557
- Framework successfully extracted 123 definitions from 113 relevant articles out of 2,398 total papers

## Why This Works (Mechanism)
The TaxoMatic framework leverages LLMs' natural language understanding capabilities to identify contextual patterns that signal definition presence and content. By employing multiple prompting strategies including Chain-of-Thought and Role prompting, the system can guide LLMs to focus on specific linguistic markers and structural elements typical of academic definitions. The two-stage architecture allows for specialized processing at each step, with relevance classification filtering the dataset before definition extraction applies more detailed linguistic analysis to identified candidates.

## Foundational Learning
The framework builds upon established techniques in natural language processing and machine learning for text classification and information extraction. It extends previous work on definition extraction by incorporating modern LLM capabilities while maintaining the structured approach of separating relevance classification from content extraction. The validation methodology follows established practices in information retrieval evaluation, using precision, recall, and cosine similarity metrics to quantify performance against ground truth annotations.

## Architecture Onboarding
TaxoMatic operates through a sequential pipeline where input documents first undergo relevance classification before definition extraction. The system accepts academic papers in standard formats, processes them through the two-stage LLM pipeline, and outputs extracted definitions with associated confidence scores. The modular design allows for substitution of different LLM models and prompting strategies, with performance optimization achieved through systematic comparison of model-prompt combinations. The framework maintains clear separation between classification and extraction stages while sharing document processing infrastructure.

## Open Questions the Paper Calls Out
The study identifies several areas requiring further investigation, including how the framework would perform across different academic disciplines beyond media bias, whether alternative class balancing techniques could improve relevance classification, and what additional evaluation metrics might better capture definition extraction quality. The authors also question how different LLM architectures might affect performance and whether more sophisticated prompting strategies could further improve results.

## Limitations
- Class imbalance (98 relevant vs. 2,300 non-relevant papers) may have constrained performance metrics
- Validation performed on single domain (media bias) limits generalizability conclusions
- Performance metrics (F1-score 0.381, cosine similarity 0.557) indicate framework functions as research aid rather than fully autonomous system
- Limited exploration of alternative evaluation metrics beyond cosine similarity
- Single dataset validation raises questions about cross-disciplinary applicability

## Confidence
- Medium for relevance classification performance: Limited by dataset size and class imbalance
- Medium for definition extraction quality: Cosine similarity provides useful but incomplete assessment
- Medium for generalizability: Single domain validation raises cross-disciplinary questions

## Next Checks
1. Evaluate TaxoMatic across three additional academic domains (psychology, computer science, economics) to assess domain transfer
2. Conduct human expert evaluation comparing LLM-extracted definitions against ground truth across multiple quality dimensions beyond cosine similarity
3. Test alternative class balancing techniques and expanded evaluation metrics for relevance classification stage