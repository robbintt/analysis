---
ver: rpa2
title: Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion
  Battery Simulation
arxiv_id: '2508.08087'
source_url: https://arxiv.org/abs/2508.08087
tags:
- neural
- operator
- pe-fno
- battery
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks three operator-learning surrogates for the
  Single Particle Model (SPM) of lithium-ion batteries: DeepONets, Fourier Neural
  Operators (FNOs), and a newly proposed parameter-embedded Fourier Neural Operator
  (PE-FNO). The PE-FNO conditions each spectral layer on particle radius and solid-phase
  diffusivity, enabling generalization to varying material parameters.'
---

# Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation

## Quick Facts
- arXiv ID: 2508.08087
- Source URL: https://arxiv.org/abs/2508.08087
- Reference count: 40
- Benchmarks three operator-learning surrogates for SPM battery simulation with parameter generalization

## Executive Summary
This paper presents a parameter-embedded Fourier Neural Operator (PE-FNO) for fast lithium-ion battery simulation that generalizes across particle radii and solid-phase diffusivities. The method achieves ~200× speedup over classical solvers while maintaining voltage errors under 1.7 mV and concentration errors below 1%. When applied to Bayesian optimization for parameter estimation, the PE-FNO recovers anode and cathode diffusivities with 1.14% and 8.4% mean absolute percentage error, respectively, demonstrating practical utility for inverse problems despite higher error than classical methods.

## Method Summary
The study benchmarks DeepONets, basic Fourier Neural Operators (FNOs), and a newly proposed parameter-embedded Fourier Neural Operator (PE-FNO) on the Single Particle Model (SPM) of lithium-ion batteries. Models are trained on 33,000 Sobol-sampled trajectories spanning four current families (constant, triangular, pulse-train, and Gaussian-random-field) with full SOC range from 0% to 100%. PE-FNO conditions spectral layers on particle radius and solid-phase diffusivity via channel-wise modulation, enabling generalization to varying material parameters. The framework uses JAX/Flax with Adam optimizer and cosine learning rate decay.

## Key Results
- Basic FNO maintains mesh invariance and keeps concentration errors below 1%, with voltage MAE under 1.7 mV across all load types
- PE-FNO enables generalization to varying radii and diffusivities while achieving approximately 200× faster execution than 16-thread SPM solver
- Bayesian optimization using PE-FNO recovers anode diffusivity with 1.14% MAPE and cathode diffusivity with 8.4% MAPE
- DeepONet accurately replicates constant-current behavior but struggles with dynamic loads (up to 42% error for pulse profiles)

## Why This Works (Mechanism)

### Mechanism 1: Spectral Convolution Captures Global Dependencies Efficiently
The FNO learns solution operators for diffusion-dominated PDEs by representing integral kernels in Fourier space, avoiding expensive spatial convolutions. The input field is transformed to frequency domain via FFT, multiplied by learned complex weights for the lowest k_{max} modes, then inverse-transformed. This captures long-range spatial correlations in O(n log n) time rather than O(n²) for naive convolutions. The underlying operator exhibits translation-invariance, making it representable as a convolution kernel—reasonable for diffusion equations with homogeneous material properties.

### Mechanism 2: Parameter Embedding Enables Zero-Shot Generalization to Material Variations
Conditioning Fourier layers on scalar parameters (D_k, R_k) via channel-wise modulation allows a single trained network to generalize across a two-decade range of diffusivities and varying particle radii without retraining. Parameters pass through a 2-layer MLP producing scale-shift factors. These modulate the lifted input through three parallel branches: (1) 1×1 convolution for channel mixing, (2) 3×3 depth-wise convolution injecting local neighborhood information (analogous to finite-difference diffusion operators), (3) Fourier layer for global interaction. The parameter space is sufficiently sampled during training (33,000 Sobol-sampled trajectories) to interpolate smoothly between parameter configurations.

### Mechanism 3: Differentiable Surrogate Enables Gradient-Based Inverse Problems with Speed-Accuracy Trade-off
The PE-FNO's end-to-end differentiability in JAX supports Bayesian optimization for parameter estimation, achieving ~200× speedup over classical solvers at the cost of ~1 decade higher inverse error. Forward pass outputs concentration fields, which are converted to voltage via the analytical expression. The voltage mismatch objective is minimized over log-diffusivity space using Gaussian-process-based Bayesian optimization with expected improvement acquisition. The surrogate's smooth interpolation enables efficient gradient-free optimization.

## Foundational Learning

- **Concept: Operator Learning vs. Function Approximation**
  - **Why needed here:** DeepONet learns point-wise input-output mappings (function approximation), while FNO learns the integral kernel of the PDE solution operator. Understanding this distinction explains why FNO generalizes better to mesh refinements and dynamic inputs.
  - **Quick check question:** Given a trained neural operator on a coarse (r, t) grid, can you evaluate it at arbitrarily fine query points without retraining? (FNO: yes via mesh invariance; DeepONet: limited by trunk-net discretization)

- **Concept: Fourier Transforms for PDEs**
  - **Why needed here:** The FNO's core operation—FFT → learned kernel multiplication → inverse FFT—requires understanding why diffusion operators are simpler in frequency space (become diagonal multiplication).
  - **Quick check question:** Why does truncating high-frequency modes (k_{max}) affect the representation of sharp gradients in concentration fields? (Answer: high frequencies encode rapid spatial changes; truncation smooths solutions)

- **Concept: Single Particle Model (SPM) Physics**
  - **Why needed here:** The SPM reduces full porous-electrode models to radial diffusion in each electrode particle, with voltage computed from surface concentrations via OCP curves. Understanding these physics clarifies what the neural operator must learn.
  - **Quick check question:** If electrolyte resistance became significant (violating SPM assumptions), would a surrogate trained on SPM data generalize to real cell behavior? (Answer: no—model mismatch would cause systematic errors)

## Architecture Onboarding

- **Component map:** Input tensor [I(t), c₀(r), r/Rₖ, t/T] → Lifting layer P ∈ ℝ^(4→d) → PE-FNO only: Parameter MLP(Dₖ, Rₖ) → 1×1 conv + 3×3 DW conv + Fourier layer → multiply → Fourier layers (6-8 stacked) → Projection head Q ∈ ℝ^(d→1) → predicted c(r,t) → Voltage computation via analytical OCP expressions in JAX

- **Critical path:** The parameter embedding block must inject material parameters before the main Fourier stack—placing it after would prevent parameter-dependent spatial processing. The 3×3 depth-wise convolution is critical for modeling diffusion-like local interactions.

- **Design tradeoffs:**
  - Training data vs. parameter coverage: PE-FNO requires 33,000 samples (vs. 11,000 for FNO) to populate 4D parameter space—insufficient sampling leads to interpolation errors
  - Width vs. accuracy: PE-FNO uses width=64 vs. FNO width=32, partially compensating for parameter embedding overhead
  - Inverse accuracy vs. speed: ~200× speedup comes with ~10× higher parameter estimation error—acceptable for rapid screening, not for precision diagnostics

- **Failure signatures:**
  - Saturation at concentration limits: Paper notes out-of-domain samples (c exceeding c_{max}) were retained for edge coverage—expect saturation artifacts near stoichiometric limits
  - Flat OCP sensitivity: Cathode diffusivity estimation fails for LFP (8.4% MAPE vs. 1.14% for anode) due to flat OCP curve—not a model failure but physics limitation
  - Dynamic load degradation: DeepONet's error explodes for pulse/GRF profiles (up to 42% nL₂)—if you see spatial artifacts or 100s of mV errors, check if architecture is DeepONet on non-constant current

- **First 3 experiments:**
  1. Reproduce CC/TRI baseline: Train basic FNO on constant-current and triangular profiles only; target nL₂ < 0.5% and voltage MAE < 2 mV
  2. Parameter embedding ablation: Train PE-FNO with parameter embedding removed on the 33,000-sample dataset; compare accuracy to paper's FNO results
  3. Inverse problem validation: Run Bayesian optimization on a held-out test trajectory with known ground-truth diffusivities; verify anode MAPE ~1-2% and cathode MAPE ~8-10%

## Open Questions the Paper Calls Out

- Can the PE-FNO architecture be extended to higher-fidelity models like the Doyle-Fuller-Newman (DFN) model? Future work should focus on extending them to more complex electrochemical models (e.g., DFN).

- Can joint forward–adjoint training close the accuracy gap between forward simulation and parameter estimation? Incorporating inverse awareness through joint forward–adjoint training offers a promising route to narrowing the remaining gap in parameter-estimation accuracy.

- Is it possible to train a single operator that generalizes across all current families without family-specific data sampling? A single operator that copes with constant-current, pulse and stochastic load families would remove the need for family-specific training.

## Limitations
- Inverse problem accuracy is ~10× worse than forward prediction, limiting utility for precision parameter estimation
- Parameter embedding generalization untested beyond training ranges; extrapolation likely fails
- 3D electrode microstructures and electrolyte resistance effects not addressed

## Confidence
- **High Confidence:** FNO forward prediction accuracy, mesh invariance properties, PE-FNO voltage computation implementation
- **Medium Confidence:** Parameter embedding generalization mechanism, inverse problem speed-accuracy tradeoff
- **Low Confidence:** Out-of-distribution parameter generalization, complex geometry performance

## Next Checks
1. Parameter extrapolation test: Train PE-FNO on reduced parameter ranges and evaluate accuracy at training bounds to quantify extrapolation limits
2. DeepONet architecture isolation: Implement and compare multiple trunk network architectures to determine if poor dynamic load performance is architecture-dependent
3. 3D generalization experiment: Extend PE-FNO to simple 3D geometries to test zero-padding and boundary condition handling