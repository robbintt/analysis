---
ver: rpa2
title: 'Understanding QA generation: Extracting Parametric and Contextual Knowledge
  with CQA for Low Resource Bangla Language'
arxiv_id: '2602.01451'
source_url: https://arxiv.org/abs/2602.01451
tags:
- parametric
- counterfactual
- contextual
- dataset
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BanglaCQA, the first Bengali counterfactual
  QA dataset designed to disentangle parametric and contextual knowledge in language
  models. By extending BanglaRQA with 6.3K counterfactual contexts and answerability
  annotations, the dataset enables evaluation of whether models rely on pre-encoded
  knowledge or contextual input.
---

# Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language

## Quick Facts
- arXiv ID: 2602.01451
- Source URL: https://arxiv.org/abs/2602.01451
- Reference count: 14
- Primary result: First Bengali counterfactual QA dataset (BanglaCQA) that disentangles parametric and contextual knowledge in language models

## Executive Summary
This work introduces BanglaCQA, the first Bengali counterfactual QA dataset designed to disentangle parametric and contextual knowledge in language models. By extending BanglaRQA with 6.3K counterfactual contexts and answerability annotations, the dataset enables evaluation of whether models rely on pre-encoded knowledge or contextual input. Experiments with encoder-decoder models (BanglaT5, mT5) and decoder-only LLMs (Qwen-2.5, DeepSeek-R1, LLAMA-3.3, Mistral-3-small) under fine-tuning and prompting strategies show that Chain-of-Thought prompting significantly improves parametric similarity (+0.42–0.44 factual, +0.38–0.39 counterfactual) compared to few-shot prompting. Qwen-2.5 achieved the best overall performance (F parametric: 0.81, CF parametric: 0.74). Encoder-decoder models showed limited parametric generalization in counterfactual contexts, while decoder-only LLMs demonstrated stronger parametric reasoning due to broader pre-encoded knowledge. Human evaluation revealed cases where models produced more accurate or multiple valid answers than dataset references, highlighting limitations of single-reference evaluation. The study establishes a framework for analyzing knowledge sources in low-resource languages and underscores the effectiveness of structured reasoning prompts for counterfactual QA.

## Method Summary
The study extends the BanglaRQA dataset with counterfactual contexts through NER-based entity substitution (PER, LOC, ORG, GPE, DATE, NUM) to create controlled factual contradictions. Four encoder-decoder models (BanglaT5-small/base, mT5-small) are fine-tuned on factual+answer (F+A) or factual+counterfactual+answer (F+CF+A) configurations. Four decoder-only LLMs (Qwen-2.5-32B, DeepSeek-R1-32B, LLaMA-3.3-72B, Mistral-3-small-24B) are evaluated using few-shot and Chain-of-Thought prompting. Parametric and contextual similarity scores are computed using Gemini-2.0-Flash and GPT-4.1 as semantic evaluators, with human evaluation (Cohen's Kappa: 0.73) validating results.

## Key Results
- Chain-of-Thought prompting improves parametric similarity by +0.42–0.44 for factual and +0.38–0.39 for counterfactual contexts compared to few-shot prompting
- Qwen-2.5 achieves highest overall performance with F parametric: 0.81 and CF parametric: 0.74
- Encoder-decoder models show limited parametric generalization in counterfactual contexts (max CF parametric: 0.23)
- Human evaluation identifies cases where models produce more accurate or multiple valid answers than single-reference dataset answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought (CoT) prompting improves parametric knowledge extraction in counterfactual QA settings by enforcing intermediate reasoning steps.
- Mechanism: CoT prompts explicitly instruct models to generate reasoning chains before final answers. This structured reasoning allows models to distinguish between counterfactual context and pre-encoded parametric knowledge, effectively increasing computational depth.
- Core assumption: Models trained on CoT-style reasoning formats can leverage structured inference templates even for low-resource languages lacking such supervision during pretraining.
- Evidence anchors:
  - [abstract]: "Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios"
  - [section 4, RQ2]: "CoT prompting leads to statistically significant improvements in parametric similarity for both factual (+0.42–0.44) and counterfactual (+0.38–0.39) settings" with "extremely large effect sizes (Cohen's d > 5)"
  - [corpus]: Weak direct corpus support for CoT mechanisms specific to low-resource languages; most related papers focus on code generation rather than QA disentanglement.
- Break condition: If models lack exposure to CoT-style formats during training, intermediate reasoning may not form coherent chains, reducing gains.

### Mechanism 2
- Claim: Counterfactual context substitution via NER reveals knowledge source prioritization by creating controlled factual contradictions.
- Mechanism: Named entities (PER, LOC, ORG, GPE, DATE, NUM) are replaced with type-consistent alternatives while preserving sentence structure. When models encounter contradictory inputs, their responses indicate whether they ground answers in context or default to parametric knowledge.
- Core assumption: The substitution process creates semantically coherent but factually contradictory passages that genuinely test knowledge source reliance.
- Evidence anchors:
  - [section 2.1]: "These controlled modifications construct hypothetical contradictions while retaining the original sentence structure and allow us to test whether models truly ground their answers in the input context or default to memorized (parametric) knowledge."
  - [section 4, RQ1]: "BanglaT5 Small drops from 0.70 (F Parametric) to 0.11 (CF Parametric)" showing encoder-decoder models' difficulty generalizing to counterfactual knowledge
  - [corpus]: No direct corpus evidence for counterfactual QA mechanisms in Bangla; this appears novel to the dataset.
- Break condition: If NER substitutions produce incoherent contexts or models lack relevant parametric knowledge, the disentanglement signal degrades.

### Mechanism 3
- Claim: Decoder-only LLMs outperform encoder-decoder models in counterfactual parametric reasoning due to broader pre-encoded knowledge bases.
- Mechanism: Decoder-only models (24B–72B parameters) access extensive pretraining knowledge through prompting rather than task-specific fine-tuning. This enables better parametric answer generation when contexts contradict world knowledge.
- Core assumption: Scale and pretraining diversity correlate with parametric knowledge breadth applicable to low-resource languages.
- Evidence anchors:
  - [section 4, RQ1]: "Decoder-only LLMs utilize prompting to access a broader and more comprehensive pre-encoded knowledge base... enables LLMs to better generate accurate parametric answers, particularly in counterfactual contexts."
  - [section 4, RQ3]: "Qwen-2.5 achieves high similarity scores (F parametric: 0.81, CF parametric: 0.74)" vs encoder-decoder CF parametric scores of 0.09–0.23
  - [corpus]: Related work (TigerCoder, PyBangla) confirms decoder-only LLM advantages for Bangla tasks, though focused on code generation.
- Break condition: If pretraining data lacks sufficient Bangla or relevant factual content, decoder-only advantages diminish.

## Foundational Learning

- Concept: Parametric vs. Contextual Knowledge Distinction
  - Why needed here: The entire framework depends on understanding that models can answer from internal weights (parametric) or provided passages (contextual). Without this distinction, disentanglement evaluation is meaningless.
  - Quick check question: Given a counterfactual context stating "Dhaka is in India," would a parametric answer say "Bangladesh" while a contextual answer says "India"?

- Concept: Counterfactual QA Design
  - Why needed here: Creating valid counterfactuals requires systematic entity substitution that preserves grammatical and semantic coherence while introducing factual contradictions.
  - Quick check question: If you replace "Barack Obama" with "Donald Trump" in a passage, what other entities in the Q/A pair might need consistent updates?

- Concept: Semantic Similarity Evaluation with LLMs
  - Why needed here: Traditional metrics (BLEU, ROUGE) fail for Bangla QA; LLM-based evaluators (Gemini, GPT-4.1) provide more reliable semantic alignment scores.
  - Quick check question: Why might exact match fail when a model outputs "approximately 10 million" against a reference of "5.5 million"?

## Architecture Onboarding

- Component map:
  BanglaRQA -> NER-based counterfactual generation -> Human annotation (4 reviewers, κ=0.73) -> BanglaCQA (6.3K CF contexts)
  BanglaT5/mT5 -> Fine-tune on F+A or F+CF+A -> Generate parametric and contextual outputs
  Qwen/DeepSeek/LLaMA/Mistral -> Few-shot or CoT prompting -> Generate dual outputs
  Gemini-2.0-Flash and GPT-4.1 -> Semantic similarity scoring + Human evaluation for validation

- Critical path: Counterfactual context quality -> CoT prompting design -> Parametric similarity scoring. The paper identifies single-reference evaluation and temporal staleness as key failure points.

- Design tradeoffs:
  - Fine-tuning (F+CF+A) improves contextual scores but not parametric CF reasoning (0.23 max for encoder-decoders)
  - CoT requires more inference tokens but yields +0.38–0.44 parametric gains
  - Quantization (LLaMA-3.3 FP16) enables resource efficiency but may affect precision

- Failure signatures:
  - Temporal mismatch: Model produces updated facts penalized against stale references (~4% of cases)
  - Multiple valid answers: Different but correct answers scored low against single references (~7% of cases)
  - Encoder-decoder CF parametric collapse: Scores drop to 0.09–0.13 without F+CF+A training

- First 3 experiments:
  1. Replicate few-shot vs. CoT comparison on a 100-example subset using Qwen-2.5, measuring parametric similarity delta with Gemini evaluation.
  2. Validate counterfactual generation quality by sampling 50 NER-substituted passages and verifying semantic coherence through human review.
  3. Test whether encoder-decoder CF parametric scores improve with larger Bangla pretraining corpora (compare BanglaT5-base vs. mT5-small on identical F+CF+A splits).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks be adapted to handle multi-reference answers and temporally dynamic knowledge in low-resource counterfactual QA?
- Basis in paper: [explicit] The authors explicitly state in the Limitations that evaluation relied on single references, which penalized valid variations, and that static references failed to account for up-to-date parametric knowledge.
- Why unresolved: Human evaluation revealed that static datasets unfairly punish models for producing factually superior or multiple valid answers (e.g., complementary angles) that differ from the single ground truth.
- What evidence would resolve it: Development and testing of a multi-reference evaluation protocol or a temporally-adaptive scoring metric on the BanglaCQA dataset.

### Open Question 2
- Question: Can fine-tuning strategies be developed to enhance parametric generalization in encoder-decoder models, or is the reliance on "constrained internal knowledge" an insurmountable architectural limitation?
- Basis in paper: [inferred] The authors conclude in RQ1 that fine-tuning T5 variants improves contextual extraction but fails to boost parametric reasoning in counterfactual settings, unlike decoder-only LLMs.
- Why unresolved: The paper demonstrates the performance gap but does not propose a method to bridge the specific deficit in parametric reasoning for encoder-decoder architectures.
- What evidence would resolve it: Experiments applying targeted knowledge-enhancing fine-tuning methods to encoder-decoder models to observe if parametric similarity scores in counterfactual settings can match those of decoder-only models.

### Open Question 3
- Question: To what extent do alternative prompting strategies or fine-tuned reasoning templates improve the disentanglement of knowledge sources beyond Few-shot and Chain-of-Thought methods?
- Basis in paper: [explicit] The Limitations section notes that the study focused on few-shot and CoT prompting, but "further exploration of other prompting strategies... could provide additional gains."
- Why unresolved: The paper establishes CoT as effective but leaves the potential of other complex prompting techniques (e.g., Tree-of-Thought) or specialized fine-tuning unexplored.
- What evidence would resolve it: A comparative analysis of advanced reasoning strategies applied to the BanglaCQA benchmark.

### Open Question 4
- Question: Does the use of quantized weights (e.g., FP16) significantly alter the balance between parametric and contextual reasoning in decoder-only LLMs?
- Basis in paper: [explicit] The authors acknowledge in the Limitations that experiments used quantized weights due to resource constraints and that "results may differ for full-precision inference."
- Why unresolved: It is unclear if the observed strong performance of quantized decoder-only models is consistent with their full-precision counterparts or if quantization introduces specific biases in knowledge retrieval.
- What evidence would resolve it: A controlled comparison of quantized versus full-precision inference runs for the decoder-only models evaluated in the study.

## Limitations

- Single-reference evaluation framework may unfairly penalize valid alternative answers (7% of cases) and updated factual knowledge (4% of cases)
- Encoder-decoder models show architectural limitations for counterfactual parametric reasoning, achieving only 0.23 maximum CF parametric scores
- Counterfactual QA design through NER-based entity substitution novelty lacks independent validation of semantic coherence at scale

## Confidence

- High confidence: Chain-of-Thought prompting effectiveness for parametric knowledge extraction (statistically significant with extremely large effect sizes)
- Medium confidence: Decoder-only LLM superiority in counterfactual parametric reasoning (empirical results but dependent on pretraining data quality)
- Medium confidence: Counterfactual QA design through NER-based entity substitution (lacks independent validation of semantic coherence)

## Next Checks

1. **Human evaluation validation**: Conduct human evaluation on a stratified sample (n=200) of model outputs to quantify the frequency of valid alternative answers being penalized by single-reference metrics.

2. **Temporal fact verification**: Implement a fact-checking layer to identify and exclude cases where model outputs reflect more current knowledge than dataset references.

3. **Encoder-decoder scaling study**: Test whether larger encoder-decoder models (mGENIE-2.5, 1.8B parameters) or domain-specific pretraining improves counterfactual parametric reasoning beyond the observed 0.23 ceiling.