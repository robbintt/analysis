---
ver: rpa2
title: Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem
  Solving
arxiv_id: '2508.09099'
source_url: https://arxiv.org/abs/2508.09099
tags:
- reasoning
- language
- formal
- geometry
- area
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of geometry problem solving (GPS)
  in vision language models, which struggle with unreliable diagram interpretation
  and pure natural language reasoning. The core method introduces a hybrid reasoning
  framework that interleaves natural language reasoning with formal language program
  generation, enabling models to leverage the flexibility of natural language for
  diagram interpretation while using formal programs for precise geometric computations.
---

# Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving

## Quick Facts
- arXiv ID: 2508.09099
- Source URL: https://arxiv.org/abs/2508.09099
- Reference count: 40
- Introduces GF-Reasoner, a hybrid natural-formal reasoning framework that improves geometry problem-solving accuracy by up to 15% over 7B-scale peers.

## Executive Summary
This work tackles the challenge of geometry problem solving in vision language models, which struggle with unreliable diagram interpretation and pure natural language reasoning. The core innovation is a hybrid reasoning framework that interleaves natural language reasoning with formal program generation, enabling models to leverage the flexibility of natural language for diagram interpretation while using formal programs for precise geometric computations. The approach combines supervised fine-tuning on a newly developed 11K-sample synthetic dataset of interleaved reasoning and reinforcement learning with solver-in-the-loop feedback. The resulting model, GF-Reasoner, achieves significant accuracy improvements on standard GPS benchmarks compared to 7B-scale peers and even outperforms the much larger Qwen2.5-VL-72B model. The hybrid approach produces shorter, cleaner reasoning traces while reducing computational and reasoning errors through offloading symbolic computation to external solvers.

## Method Summary
The method introduces a two-stage training procedure to teach vision language models hybrid natural-formal reasoning for geometry problem solving. First, supervised fine-tuning (SFT) is performed on an 11K-sample synthetic dataset of interleaved natural language reasoning and formal program generation, created using a teacher model and forward/backward synthesis techniques. This establishes basic formal language syntax and auto-formalization skills. Second, reinforcement learning with a solver-in-the-loop is applied, where the model generates responses that are executed by a geometry solver, and binary rewards guide policy updates via the GRPO algorithm. The formal language includes 34 operators and 55 operands for geometric computations, with the solver (implemented in SymPy) performing symbolic calculations to eliminate arithmetic errors. The approach is validated on standard GPS benchmarks including PGPS9K, UniGeo, MathVista, and MathVerse.

## Key Results
- GF-Reasoner achieves up to 15% accuracy improvements on GPS benchmarks compared to 7B-scale peers.
- Outperforms the much larger Qwen2.5-VL-72B model on standard benchmarks.
- Reduces reasoning errors from 23.0% to 14.3% and computation errors from 1.7% to 0.3% compared to natural language baseline.
- Produces shorter, cleaner reasoning traces while maintaining or improving accuracy.

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Natural-Formal Interleaved Reasoning
- Claim: Interleaving natural language reasoning with incremental formal program generation improves geometry problem-solving accuracy and token efficiency compared to pure natural language or direct formal program generation.
- Mechanism: Natural language handles flexible, interpretive tasks (diagram understanding, problem formalization), while formal language provides precise, executable operators for critical derivations, offloading symbolic computation to an external solver.
- Core assumption: Large Vision-Language Models (LVLMs) lack inherent knowledge of geometric formal language syntax and usage patterns; they must be explicitly trained via post-training strategies.
- Evidence anchors:
  - [abstract]: "The model interleaves natural language reasoning with incremental emission of solver-executable code, producing a hybrid reasoning trace in which critical derivations are expressed in formal language."
  - [section 3.1]: "Our goal is to teach LVLMs to use this formal language and geometry solver. A straightforward approach involves designing prompt strategies... LVLMs... frequently revert to natural language explanations."
  - [corpus]: Neighbor papers like "AutoGPS" and "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration" similarly advocate for symbolic-neural integration, supporting the hybrid paradigm.
- Break condition: Fails if the model cannot reliably parse diagrams to extract geometric primitives, or if the formal language operators lack coverage for required theorems (e.g., auxiliary line constructions).

### Mechanism 2: Two-Stage Training with Solver-in-the-Loop Feedback
- Claim: A cold-start supervised fine-tuning (SFT) phase followed by reinforcement learning (RL) with solver feedback is necessary and effective for learning hybrid reasoning.
- Mechanism: SFT on a synthetic dataset teaches auto-formalization and basic formal language syntax. RL then refines the policy by executing generated programs in a solver and using outcome-based rewards to optimize both the natural language narrative and the formal program.
- Core assumption: High-quality, interleaved natural-formal reasoning data is scarce; it must be synthesized using capable teacher models (e.g., GPT-4o) and existing GPS datasets.
- Evidence anchors:
  - [abstract]: "To teach this behavior at scale, we combine (1) supervised fine-tuning on an 11K newly developed synthetic dataset... and (2) solver-in-the-loop reinforcement learning..."
  - [section 4.3.3]: "Our empirical results show that RL increases Pass@1 accuracy by 26%. Notably, without RL, our model fails to match the performance of natural language reasoning alone."
  - [corpus]: "TrustGeoGen" discusses formal-verified data engines, highlighting the critical role of data quality and synthesis for GPS tasks.
- Break condition: Fails if SFT over-trains and reduces policy diversity (hindering RL exploration), or if the solver provides sparse, binary rewards insufficient for guiding complex reasoning steps.

### Mechanism 3: Error Reduction via Computational Offloading
- Claim: The hybrid approach significantly reduces reasoning and computation errors by delegating symbolic calculation to an external solver and using formal operators for knowledge application.
- Mechanism: Formal language operators encapsulate geometric theorems, ensuring correct principle application. The solver performs algebraic manipulation (e.g., equation solving, root extraction), eliminating arithmetic errors common in natural language reasoning.
- Core assumption: The formal language's operator set and the solver's symbolic engine are correct and complete for the target problem domain.
- Evidence anchors:
  - [section 4.3.1, Table 2]: Shows formal-integrated reasoning reduces reasoning errors from 23.0% to 14.3% and computation errors from 1.7% to 0.3% compared to a natural language baseline.
  - [abstract]: "...reducing computational and reasoning errors through offloading symbolic computation to external solvers."
  - [corpus]: "GeoSense" evaluates identification and application of geometric principles in multimodal reasoning, a core capability addressed by the formal operators.
- Break condition: Fails if the model misinterprets the diagram leading to incorrect variable binding (visual perception error), or if the required theorem is not implemented as an operator.

## Foundational Learning

- **Formal Language Syntax & Semantics**
  - Why needed here: The core method requires the model to generate code in a specific formal language with 34 operators and 55 operands. Understanding the grammar (e.g., `Operator Operand1 Operand2 ResultVariable`) is a prerequisite for data synthesis, SFT, and evaluation.
  - Quick check question: Given the operator `Gougu a b c`, which represents the Pythagorean theorem, what do variables `a`, `b`, and `c` correspond to? (Answer: `a` and `b` are legs, `c` is the hypotenuse).

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The hybrid framework explicitly builds upon CoT, interleaving reasoning steps with formal code. Understanding how CoT breaks down complex problems is essential to see how formal language is integrated into intermediate steps.
  - Quick check question: How does the paper's hybrid reasoning differ from standard CoT? (Answer: It integrates executable formal code into the reasoning trace, not just natural language steps).

- **Reinforcement Learning from Verifiable Rewards**
  - Why needed here: The second training stage uses RL (GRPO algorithm) where the reward is binary feedback from a geometry solver. Grasping how a verifiable, outcome-based reward signal can shape a generative policy is key.
  - Quick check question: What is the reward function `r(z, p)` used in the RL stage? (Answer: It is 1 if the solver-executed result from program `p` matches the ground truth, and 0 otherwise).

## Architecture Onboarding

- **Component map**: Data Synthesis Engine -> Supervised Fine-Tuning Module -> Solver-Integrated RL Loop -> Geometry Solver
- **Critical path**:
  1. Define formal language specification (operators, operands).
  2. Develop synthesis prompts and generate the FI-CoT training dataset.
  3. Perform short-epoch (e.g., 2) SFT on the base model.
  4. Setup the solver-in-the-loop RL environment with the GRPO algorithm.
  5. Run RL training on the combined GPS training sets.
  6. Evaluate the final model on benchmarks (PGPS9K, UniGeo, MathVista, MathVerse).

- **Design tradeoffs**:
  - **SFT Epochs**: Too few may leave syntax unstable; too many (e.g., 8 epochs) can reduce policy entropy and hinder RL exploration. Moderate epochs (2-4) are optimal.
  - **Data Synthesis Method**: Forward synthesis is flexible but low-accuracy (~20%). Backward synthesis with ground-truth programs achieves higher accuracy (~50%) but requires program annotations.
  - **Reward Signal**: Binary (correct/incorrect) reward is simple but sparse. Multi-step or partial credit rewards could provide richer gradients but were not explored in this work.

- **Failure signatures**:
  - **Regress to Pure NL**: The model ignores the formal language specification and generates only natural language solutions.
  - **Hallucinate Operators**: The model uses operators not defined in the formal language (e.g., `Solve`, `Set` as seen in Appendix A.3).
  - **Poor RL Convergence**: Accuracy plateaus early or degrades, potentially due to low initial policy entropy from over-aggressive SFT.
  - **Visual Perception Errors**: Incorrect identification of geometric elements in the diagram, leading to wrong variable bindings.

- **First 3 experiments**:
  1. **Synthesis Accuracy Ablation**: Compare the quality (solver pass rate) of data generated via forward synthesis, backward synthesis, and customized backward synthesis. This validates the data generation pipeline.
  2. **SFT Duration Ablation**: Train separate SFT models with 2, 4, 6, and 8 epochs on the same data, then run identical RL training. Measure final accuracy and track policy entropy during RL to find the optimal SFT duration.
  3. **Hybrid vs. Pure Paradigm Comparison**: Train and evaluate three models: (a) pure natural language CoT, (b) direct formal program generation, and (c) hybrid reasoning. Compare Pass@1 accuracy, error type distribution, and average token count to quantify the benefits of the hybrid approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the formal-integrated reasoning framework be extended to handle problems requiring diagrammatic constructions, such as adding auxiliary lines?
- Basis in paper: [explicit] The conclusion states, "the current framework cannot handle problems requiring diagrammatic constructions (e.g., adding auxiliary lines), which restricts its applicability to certain classes of geometry problems."
- Why unresolved: The current formal language and solver interface are designed to parse existing elements but lack the capability to hypothesize and integrate new geometric elements into the reasoning trace.
- What evidence would resolve it: Demonstrating successful problem-solving accuracy on a benchmark of geometry problems that specifically mandate auxiliary constructions, or the introduction of new operators for geometric extension.

### Open Question 2
- Question: Can incorporating self-reflection mechanisms into the reasoning loop enhance the performance of formal-integrated reasoning models?
- Basis in paper: [explicit] The conclusion notes, "our method cannot perform long-horizon reasoning with self-reflection... We believe empowering our framework with this feature would further enhance the performance."
- Why unresolved: The current GF-Reasoner architecture produces a linear reasoning chain without an internal feedback loop to detect and correct logical or syntax errors in the formal program before execution.
- What evidence would resolve it: An ablation study comparing the performance of the current model against a variant capable of self-correction, specifically analyzing error rates in complex multi-step derivations.

### Open Question 3
- Question: How does replacing binary outcome rewards with rich executor feedback and multi-turn interactions affect the efficiency of the reinforcement learning phase?
- Basis in paper: [explicit] The paper states, "our approach receives limited reward supervision during training, in contrast to the rich executor feedback and multi-turn interaction mechanisms used by [Li et al., 2025a]. Exploring these enhancements is a promising direction."
- Why unresolved: The current RL training relies on a binary signal (match/mismatch with ground truth), ignoring potential intermediate feedback from the solver regarding execution errors or partial correctness.
- What evidence would resolve it: Comparative training curves and final accuracy metrics between the current binary-reward RL framework and a setup utilizing detailed error traces from the geometry solver as reward signals.

### Open Question 4
- Question: Why does reinforcement learning training saturate for high-complexity problems (operator count $\ge$ 6), and how can this limitation be mitigated?
- Basis in paper: [inferred] Figure 9 and Section 4.3.3 show that RL training improves Pass@8 for medium-difficulty problems but fails to yield gains for problems with 6 or more operators.
- Why unresolved: This suggests the current RL strategy may struggle to explore sparse solution spaces effectively or that the base model lacks the capacity to refine strategies for high-complexity problems via the current reward structure.
- What evidence would resolve it: An ablation study using curriculum learning or specific reward shaping for high-complexity instances that results in statistically significant accuracy improvements for problems with $\ge$ 6 operators.

## Limitations
- Cannot handle problems requiring diagrammatic constructions (e.g., adding auxiliary lines), restricting applicability to certain geometry problem classes.
- The formal language's coverage of geometric reasoning is assumed sufficient but not rigorously tested against out-of-distribution problems.
- The synthetic data pipeline, while effective, is validated only on constrained datasets and may not generalize to more diverse or complex geometry problems.

## Confidence
- **High Confidence**: The hybrid reasoning framework (Mechanism 1) and the two-stage training procedure (Mechanism 2) are clearly articulated and empirically validated, with ablation studies directly supporting their contributions.
- **Medium Confidence**: The error reduction mechanism (Mechanism 3) is demonstrated via error type analysis, but the attribution of improvements to specific formal operators versus overall model training is not fully disentangled.
- **Low Confidence**: The long-tail generalization of the model to novel geometric constructs or problems requiring extensive auxiliary line construction remains speculative, as such cases are not explicitly tested.

## Next Checks
1. **Formal Language Coverage Test**: Systematically evaluate the model on a benchmark containing problems requiring auxiliary constructions or theorems not covered by the current 34 operators, to quantify robustness to out-of-distribution reasoning.
2. **Reward Signal Ablation**: Replace the binary solver reward with a multi-step reward that credits intermediate correct derivations, and measure the impact on both accuracy and sample efficiency during RL.
3. **Data Synthesis Stress Test**: Generate a synthetic dataset using only forward synthesis (no backward-synthesized data) and train the same two-stage pipeline; compare final accuracy to the published model to isolate the impact of high-quality ground-truth programs.