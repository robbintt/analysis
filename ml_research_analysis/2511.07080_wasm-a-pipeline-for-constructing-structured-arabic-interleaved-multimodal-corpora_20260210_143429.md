---
ver: rpa2
title: 'Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal
  Corpora'
arxiv_id: '2511.07080'
source_url: https://arxiv.org/abs/2511.07080
tags:
- arabic
- multimodal
- text
- filtering
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents Wasm, the first Arabic multimodal processing\
  \ pipeline that preserves document structure while creating interleaved text-image\
  \ datasets. The authors adapted the OBELICS framework for Arabic, introducing Arabic-specific\
  \ perplexity modeling, dialectal coverage, and node-level deduplication using Needleman\u2013\
  Wunsch algorithm."
---

# Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora

## Quick Facts
- arXiv ID: 2511.07080
- Source URL: https://arxiv.org/abs/2511.07080
- Reference count: 16
- First Arabic multimodal pipeline preserving document structure while creating interleaved text-image datasets

## Executive Summary
This paper presents Wasm, the first Arabic multimodal processing pipeline that preserves document structure while creating interleaved text-image datasets. The authors adapted the OBELICS framework for Arabic, introducing Arabic-specific perplexity modeling, dialectal coverage, and node-level deduplication using Needleman–Wunsch algorithm. The pipeline outputs structured Markdown with interleaved images, maintaining document-level coherence and cross-modal alignments. A comparative analysis shows Wasm achieves higher corpus diversity and efficiency than conventional approaches, with the perplexity model trained on diverse Arabic content outperforming Wikipedia-based alternatives in filtering quality.

## Method Summary
Wasm adapts the OBELICS framework for Arabic by implementing Arabic-specific preprocessing and filtering strategies. The pipeline processes Common Crawl WARC files filtered for Arabic content, converts HTML to structured Markdown preserving document hierarchy, and applies tag-level and document-level filtering with calibrated perplexity thresholds. A custom KenLM language model trained on diverse Arabic dialects enables quality assessment, while Needleman–Wunsch sequence alignment performs node-level deduplication at 80% similarity threshold. The pipeline outputs structured Markdown with interleaved text and image references, maintaining cross-modal alignments needed for multimodal training.

## Key Results
- Wasm pipeline demonstrates superior corpus diversity and filtering efficiency compared to conventional approaches
- Custom KenLM perplexity model trained on diverse Arabic content outperforms Wikipedia-based models in filtering quality
- Node-level deduplication using Needleman–Wunsch algorithm achieves more accurate removal of near-duplicate Arabic content while preserving document-level unique content

## Why This Works (Mechanism)

### Mechanism 1: Structured Markdown Preservation
- Claim: Converting web HTML to structured Markdown with interleaved images preserves document-level coherence and cross-modal alignments needed for multimodal training.
- Mechanism: Pipeline extracts DOM tree structure, converts to Markdown maintaining headers/paragraphs/lists/tables hierarchy, preserves image positions relative to text, and outputs in flexible format usable for both text-only and multimodal training.
- Core assumption: Document structure encodes semantic relationships (caption-image, section hierarchy) that sequential token representations lose.
- Evidence anchors:
  - [abstract] "our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios"
  - [section 1.5] "converted into Markdown format to facilitate downstream text processing, preserve the document's basic structural hierarchy"
  - [corpus] Limited direct corpus evidence on downstream task improvement from structure preservation
- Break condition: If downstream models cannot parse Markdown formatting or if structure introduces noise that degrades training convergence.

### Mechanism 2: Arabic-Dialect-Aware Perplexity Filtering
- Claim: A KenLM perplexity model trained on diverse Arabic content (dialects, topics) filters low-quality text more effectively than Wikipedia-trained models.
- Mechanism: Custom KenLM language model trained on curated multilingual Arabic corpus → perplexity scoring → threshold-based filtering with Arabic-calibrated thresholds (1900 at document level vs. OBELICS 1500).
- Core assumption: Arabic web quality signals differ from English; Wikipedia-based models underrepresent dialectal variation and flag valid content.
- Evidence anchors:
  - [section 1.6.1] "model was trained on a carefully curated dataset that emphasizes high-quality content and spans a wide spectrum of Arabic dialects and topics"
  - [Table 5] Shows perplexity comparison where Wasm model scores 1360.5 vs. Wikipedia model 7880.8 on sample text, indicating better fit
  - [corpus] Weak corpus evidence - no cited benchmarks comparing perplexity model performance across Arabic corpora
- Break condition: If perplexity thresholds over-filter scarce Arabic data or if dialectal variation is so wide that no single model captures it adequately.

### Mechanism 3: Node-Level Needleman-Wunsch Deduplication
- Claim: Deduplicating at HTML node level using sequence alignment preserves documents with unique content alongside boilerplate, improving corpus diversity.
- Mechanism: Needleman-Wunsch global sequence alignment algorithm applied to text nodes → 80% similarity threshold → remove near-duplicate nodes while keeping parent document.
- Core assumption: Web documents contain mixed unique + duplicated content (ads, navigation); document-level deduplication discards valuable unique portions.
- Evidence anchors:
  - [section 1.6.3] "implemented the Needleman–Wunsch algorithm with a similarity threshold of 80% to efficiently identify and remove nearly duplicate content"
  - [Table 3] "More accurate removal of near-duplicate Arabic content" vs. OBELICS N/A for deduplication
  - [corpus] No corpus evidence comparing node-level vs. document-level deduplication on downstream metrics
- Break condition: If 80% threshold is too aggressive for Arabic morphological variation or too lenient for actual near-duplicates.

## Foundational Learning

- Concept: **Interleaved multimodal corpora**
  - Why needed here: Wasm's core output format differs from image-text pairs by preserving sequence order of text and images as they appear in documents.
  - Quick check question: Can you explain why interleaved data might improve a model's ability to follow multi-step visual instructions compared to isolated image-caption pairs?

- Concept: **KenLM perplexity scoring**
  - Why needed here: Pipeline uses KenLM for quality filtering; understanding how perplexity relates to text quality is essential for threshold calibration.
  - Quick check question: Given a perplexity score of 1900 vs. 5000 for two Arabic documents, which would likely be retained and why might this differ for dialectal vs. MSA content?

- Concept: **Needleman-Wunsch sequence alignment**
  - Why needed here: Node-level deduplication uses this bioinformatics algorithm; understanding gap penalties and similarity scoring is needed to modify thresholds.
  - Quick check question: If two Arabic paragraphs share 85% word overlap but differ in a key sentence, would the 80% threshold retain or remove the duplicate?

## Architecture Onboarding

- Component map: Metadata Extraction → HTML Processing → Structure Conversion → Tag-Level Filtering → Visual Filtering → Node Deduplication → Document-Level Filtering

- Critical path: Metadata extraction → HTML retrieval → Markdown conversion → Tag filtering → Node deduplication → Document filtering → Output. Language ID and perplexity filtering at tag level are the primary quality gates.

- Design tradeoffs:
  - Conservative filtering (higher thresholds) preserves scarce Arabic data but may retain noise
  - URL-only image collection reduces storage but limits image-content quality verification
  - Node-level deduplication is more compute-intensive than document-level but preserves more unique content
  - Markdown output requires downstream parsers but enables flexible text/multimodal use

- Failure signatures:
  - Over-filtering: Exclusion rate >20% on valid Arabic corpora (see Table 1 dataset_101 at 19.757%)
  - Under-deduplication: Repetitive boilerplate appearing across documents
  - Structure loss: Markdown output flattened to plain text losing headers/lists
  - Perplexity mismatch: Valid dialectal text flagged as low-quality (check Wikipedia model vs. custom model divergence in Table 5)

- First 3 experiments:
  1. **Threshold calibration run**: Process 10K Arabic pages with default thresholds, manually inspect 100 filtered examples to validate perplexity cutoff (1900) and language ID threshold (85%)
  2. **Deduplication A/B test**: Compare output diversity metrics (unique n-gram ratio, vocabulary size) between node-level (Needleman-Wunsch 80%) and document-level deduplication on same corpus slice
  3. **Perplexity model comparison**: Score held-out samples from existing Arabic corpora (FineWeb2, CulturaX, Ara24) with both Wikipedia-KenLM and Wasm-custom model; analyze exclusion rate differences and qualitatively inspect divergence cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does training Arabic large multimodal models on Wasm's interleaved corpus compare to training on translated pairwise datasets in downstream task performance?
- Basis in paper: [explicit] The authors note "Arabic multimodal resources have mainly been based on pairwise translated datasets" and release their dataset to enable future research, but do not present comparative downstream evaluations.
- Why unresolved: The paper describes dataset construction but does not train and evaluate models comparing Wasm against existing Arabic multimodal resources.
- What evidence would resolve it: Benchmark results from LMMs trained on Wasm versus translated datasets across standard Arabic multimodal tasks.

### Open Question 2
- Question: Why does the Wikipedia-based KenLM model outperform the custom Arabic perplexity model on certain text samples, and what characteristics distinguish these cases?
- Basis in paper: [explicit] Table 5 shows instances where the Wikipedia-based model yields lower perplexity values, which the authors flag as "a potential issue that requires inspection."
- Why unresolved: The paper acknowledges the anomaly but does not analyze the linguistic or structural patterns causing it.
- What evidence would resolve it: Systematic analysis of perplexity discrepancies identifying text types, dialects, or domains where Wikipedia training outperforms diverse-corpus training.

### Open Question 3
- Question: What is the impact of URL-only image filtering on multimodal training quality compared to content-based image assessment?
- Basis in paper: [inferred] The pipeline collects image URLs rather than downloading images, applying filtering at the site level via blacklists, but this approach cannot detect quality issues in the actual image content.
- Why unresolved: No validation is presented comparing URL-level filtering against downloaded image quality assessment for Arabic web content.
- What evidence would resolve it: Comparative analysis of training outcomes using URL-filtered versus content-filtered image corpora.

## Limitations
- Dataset composition: Custom KenLM model training corpus composition is not publicly available, limiting reproducibility and validation of dialectal coverage
- Resource access: Exact Common Crawl dump versions used are not specified, preventing exact reproduction of exclusion rates and quality metrics
- Image handling: URL-only collection prevents validation of image-text semantic alignment quality, relying solely on site-level blacklists

## Confidence

**High confidence**: The Markdown conversion and node-level deduplication mechanisms (Needleman–Wunsch at 80% threshold) are well-specified with clear algorithmic descriptions. The OBELICS framework adaptation approach is documented and reproducible.

**Medium confidence**: Perplexity-based filtering effectiveness depends on the unpublished training corpus. The paper shows better perplexity scores than Wikipedia models on sample text, but without benchmark comparisons or validation datasets, the actual quality improvement is uncertain.

**Low confidence**: Claims about downstream utility (improved Arabic multimodal training) lack empirical validation. No ablation studies compare structure preservation vs. flattened text, or node-level vs. document-level deduplication on actual Arabic NLP tasks.

## Next Checks
1. **Perplexity model validation**: Score a balanced sample of Modern Standard Arabic, Egyptian, Levantine, and Gulf dialect texts using both the Wasm custom KenLM model and the Wikipedia-based baseline. Calculate precision/recall of filtering decisions and identify systematic biases toward or against specific dialects.

2. **Structure preservation impact**: Process the same Arabic corpus with Wasm pipeline and a modified version that flattens all structure to plain text. Train identical text-only models on both outputs and measure performance differences on Arabic language understanding benchmarks (ARCD, Arabic-SQuAD).

3. **Deduplication threshold optimization**: Systematically vary the Needleman–Wunsch similarity threshold (70%, 80%, 90%) and document-level perplexity cutoffs (1700, 1900, 2100) on a held-out Arabic corpus. Measure the tradeoff between corpus size retention and repetition rate, identifying the optimal balance for scarce Arabic web data.