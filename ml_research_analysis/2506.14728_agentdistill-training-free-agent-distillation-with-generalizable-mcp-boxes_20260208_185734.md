---
ver: rpa2
title: 'AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes'
arxiv_id: '2506.14728'
source_url: https://arxiv.org/abs/2506.14728
tags:
- agent
- distillation
- arxiv
- student
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentDistill, a training-free agent distillation
  framework that transfers task-solving capabilities from large teacher agents to
  small student agents using distilled Model-Context-Protocols (MCPs). Instead of
  trajectory replay or gradient updates, the method abstracts, clusters, and consolidates
  reusable tool-use strategies into an executable MCP-Box, which is directly mounted
  into student agents at inference time.
---

# AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes

## Quick Facts
- arXiv ID: 2506.14728
- Source URL: https://arxiv.org/abs/2506.14728
- Reference count: 40
- One-line primary result: Training-free agent distillation achieves teacher-comparable performance on biomedical and mathematical tasks by mounting distilled MCP-Boxes into student agents without gradient updates.

## Executive Summary
AgentDistill introduces a training-free framework for transferring tool-use capabilities from large teacher agents to small student agents through distilled Model-Context-Protocols (MCPs). The method abstracts, clusters, and consolidates reusable tool-use strategies into an executable MCP-Box, which is directly mounted into student agents at inference time. Experiments on biomedical (PathVQA, SLAKE) and mathematical (Game of 24) benchmarks show that MCP-equipped student agents achieve performance comparable to teacher agents and outperform retrieval-based systems like OctoTools with GPT-4o.

## Method Summary
AgentDistill transfers capabilities from teacher to student agents without training by distilling successful tool-use trajectories into reusable MCPs. The teacher agent executes tasks and generates MCPs, which are then abstracted to remove example-specific details, clustered by functionality, and consolidated into general-purpose protocols. These MCPs are mounted as an executable MCP-Box into student agents using the SmolAgents framework. The student agent selects from this pre-validated toolset rather than generating tool code from scratch, reducing generation complexity while preserving high-level planning autonomy.

## Key Results
- On SLAKE, student models improved by up to 10% accuracy after MCP-Box mounting
- On Game of 24, GPT-3.5-turbo gained 48.4 percentage points when equipped with distilled MCP-Box
- MCP-equipped student agents achieved performance comparable to teacher agents and outperformed retrieval-based OctoTools with GPT-4o across all benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling teacher-generated tool-use strategies into reusable MCPs enables student agents to inherit capabilities without gradient updates.
- Mechanism: Teacher agents autonomously generate MCPs during task execution → MCPs are abstracted to remove example-specific phrases, clustered by functionality, and consolidated into general-purpose protocols → Student agents directly mount the MCP-Box at inference time with no training.
- Core assumption: Student agents can effectively select appropriate MCPs and fill arguments without understanding implementation details.
- Evidence anchors:
  - [abstract] "transfers task-solving capabilities... using distilled Model-Context-Protocols (MCPs)... directly mounted into student agents at inference time"
  - [section 4.2] "The MCP-Box serves as an external library of executable protocols... allowing the student agent to bypass low-level code generation"
  - [corpus] No direct corpus support for MCP-based agent distillation; related work focuses on trajectory imitation.
- Break condition: If student agents cannot reliably select MCPs or bind parameters correctly, performance degrades to baseline.

### Mechanism 2
- Claim: Constraining the tool-calling space to verified, functional options reduces student generation complexity.
- Mechanism: Student agents select from a pre-validated MCP-Box rather than generating tool code from scratch → This constrains the action space while preserving high-level planning autonomy.
- Core assumption: The primary bottleneck for small models is low-level code/tool generation, not high-level task decomposition.
- Evidence anchors:
  - [abstract] "Instead of trajectory replay or gradient updates, the method abstracts, clusters, and consolidates reusable tool-use strategies"
  - [section 4.2] "benefit arises from constraining the tool-calling space to a set of functional, verified options. This reduces generation complexity without interfering with the agent's core reasoning process"
  - [corpus] Related distillation papers (e.g., trajectory-level, structure-level) do not address tool-space constraining directly.
- Break condition: If tasks require novel tools outside MCP-Box coverage, student cannot complete them.

### Mechanism 3
- Claim: Abstracting task-specific MCPs into parameterized templates enables cross-task generalization.
- Mechanism: Raw MCPs are rewritten to expose up to three critical parameters → Similar MCPs are consolidated into single general-purpose versions → Same MCP applies across scenarios via argument adjustment.
- Core assumption: Task variability can be captured through parameter tuning rather than new tool creation.
- Evidence anchors:
  - [section 3.3] "each raw MCP is rewritten into a concise, task-agnostic form... makes up to three critical parameters configurable"
  - [section 4.2] "decouples task semantics from implementation logic, allowing the same MCP to be reused across new clinical scenarios... with no code change"
  - [corpus] No corpus evidence directly supports parameterization-based agent generalization.
- Break condition: If new tasks require fundamentally different tool logic not parameterizable from existing MCPs.

## Foundational Learning

- Concept: Knowledge Distillation vs Agent Distillation
  - Why needed here: Traditional KD (logit alignment, CoT distillation) does not address planning, memory, and tool use in agents.
  - Quick check question: Why is trajectory replay insufficient for teaching student agents to plan in novel environments?

- Concept: Model-Context-Protocol (MCP) Structure
  - Why needed here: MCPs are the core transfer units; understanding their self-contained, executable format is essential.
  - Quick check question: What components must a self-contained MCP include to be executable by a student agent without external context?

- Concept: Tool-Augmented Agent Reasoning
  - Why needed here: The framework assumes students can already invoke tools and integrate results into reasoning.
  - Quick check question: How does an agent decide when to call a tool versus reasoning directly from context?

## Architecture Onboarding

- Component map:
  - Teacher Agent (Claude-Sonnet-4 + GPT-4o) -> Manager Agent + Image Captioner + MCP Creation Module
  - MCP Creation Module: Brainstorming -> Open-Source Search -> Script Generation -> Virtual Env Execution
  - MCP-Box Construction: Abstraction -> Clustering -> Consolidation (via Claude-Sonnet-4)
  - Student Agent (GPT-3.5-turbo/Qwen-8B/LLaMA-8B) -> Manager Agent + Image Captioner + MCP-Box mounted

- Critical path:
  1. Run teacher agent on training examples → collect MCPs from successful trajectories
  2. Abstract, cluster, and consolidate MCPs into MCP-Box
  3. Mount MCP-Box to student agent → evaluate on test set
  4. No gradient updates anywhere in the pipeline

- Design tradeoffs:
  - MCP-Box size vs selection clarity: More MCPs increase coverage but may confuse student selection
  - Parameter count vs usability: More parameters increase flexibility but raise binding error risk
  - Teacher strength vs cost: Stronger teachers yield better MCPs at higher inference cost
  - No retrieval vs retrieval: Paper shows no-retrieval MCP-Box outperforms retrieval-based OctoTools (GPT-4o)

- Failure signatures:
  - Student fails to invoke MCP when appropriate (planning failure)
  - Student selects wrong MCP from cluster (selection confusion)
  - Student binds arguments incorrectly (parameter error)
  - MCP-Box lacks required tool (coverage gap)

- First 3 experiments:
  1. Replicate MCP-Box construction on 100 PathVQA examples; verify abstraction removes example-specific phrases
  2. Test student agent (e.g., LLaMA-8B) before/after MCP-Box mounting on held-out PathVQA; measure accuracy gain
  3. Ablate consolidation: compare performance using raw abstracted MCPs vs consolidated MCP-Box

## Open Questions the Paper Calls Out

- Question: How does inference latency and decision-making accuracy degrade as the size of the MCP-Box scales, given the framework mounts the entire toolset without retrieval?
- Question: Does the performance gap in the Game of 24 task (teacher 99% vs. student 75.5%) stem from the student's inability to plan or the MCP-Box's inability to encapsulate search strategies?
- Question: Can the MCP creation pipeline effectively generalize to environments with dynamic state changes (e.g., web navigation) or is it restricted to self-contained code generation?

## Limitations

- Framework effectiveness depends critically on teacher agent's ability to generate correct, diverse MCPs; failures in complex tasks create inherited gaps
- Abstraction and consolidation steps are heuristic and may lose essential task nuances when simplifying MCPs into parameterized templates
- No evaluation of scenarios requiring novel tools outside MCP-Box coverage or fundamentally different tool logic

## Confidence

- High confidence: That the MCP-Box approach enables zero-training capability transfer, as demonstrated empirically across three datasets with clear performance gains over baselines
- Medium confidence: That abstraction + consolidation preserves sufficient task flexibility for novel scenarios, since the paper shows generalization but doesn't extensively probe failure modes or parameter coverage
- Medium confidence: That MCP selection is the primary bottleneck for small agents, as the method assumes student agents can already plan and reason; insufficient evidence is provided that tool generation was the limiting factor versus other reasoning capabilities

## Next Checks

1. Stress-test MCP-Box generalization by evaluating on out-of-distribution tasks not represented in the 100-sample distillation set to measure parameter template flexibility
2. Perform ablation on consolidation quality by comparing performance using raw abstracted MCPs versus consolidated versions to quantify information loss
3. Analyze student MCP invocation patterns to determine whether failures stem from selection confusion, parameter binding errors, or coverage gaps in the MCP-Box