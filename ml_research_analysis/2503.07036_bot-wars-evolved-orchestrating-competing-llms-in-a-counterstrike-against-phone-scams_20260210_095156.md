---
ver: rpa2
title: 'Bot Wars Evolved: Orchestrating Competing LLMs in a Counterstrike Against
  Phone Scams'
arxiv_id: '2503.07036'
source_url: https://arxiv.org/abs/2503.07036
tags:
- deepseek
- mixtral
- dialogue
- while
- scam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces "Bot Wars," a framework using LLMs to simulate
  adversarial phone scam dialogues for proactive defense. The core innovation is a
  two-layer prompt architecture enabling strategy emergence through chain-of-thought
  reasoning without explicit optimization.
---

# Bot Wars Evolved: Orchestrating Competing LLMs in a Counterstrike Against Phone Scams

## Quick Facts
- arXiv ID: 2503.07036
- Source URL: https://arxiv.org/abs/2503.07036
- Authors: Nardine Basta; Conor Atkins; Dali Kaafar
- Reference count: 14
- Primary result: LLM framework simulates adversarial scam dialogues using chain-of-thought reasoning without optimization metrics

## Executive Summary
Bot Wars introduces a framework using competing LLMs to simulate adversarial phone scam dialogues for proactive defense research. The core innovation is a two-layer prompt architecture enabling strategy emergence through chain-of-thought reasoning without explicit optimization. The framework creates demographically authentic victim personas while maintaining strategic coherence against scammers. Evaluation across 3,200 dialogues (validated against 179 hours of human interactions) shows GPT-4 excels in dialogue naturalness (2.99/3.0) and persona authenticity, while Deepseek demonstrates superior engagement sustainability (maintaining 83.33% distinct scammer names across scenarios).

## Method Summary
The framework orchestrates adversarial dialogues between LLM agents representing scammers and victims using a two-layer prompt architecture with base layer (demographic bounds) and behavioral layer (tactical sequences). Four scam types are implemented with opposing objective functions: scammers maximize PII extraction while victims maximize dialogue length with minimal disclosure. The system uses 20-utterance sliding context windows, 50-turn limits, and 30-word response bounds. Eight model combinations (GPT-4, GPT-3.5, Mixtral, DeepSeek) are evaluated across 3,200 dialogues using cognitive (G-Eval), quantitative (length, repetition, duration), and content analysis (persona authenticity, PII handling, tactic detection).

## Key Results
- GPT-4 achieves highest cognitive naturalness score (2.99/3.0) and statistically significant persona alignment with ACCC demographic profiles
- DeepSeek demonstrates superior engagement sustainability with 83.33% distinct scammer names across scenarios versus 60% for GPT-4
- Framework creates coherent multi-turn adversarial strategies through chain-of-thought reasoning without optimization metrics
- Response bounds (≤30 words) and turn limits (≤50) constrain dialogue quality but match human scam-baiting patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strategy emergence occurs through chain-of-thought reasoning without explicit optimization metrics.
- Mechanism: Decomposes social engineering into hierarchical reasoning chains (e.g., problem_establish → solution_propose → compliance_induce) that mirror human cognitive processes. LLMs generate adaptive responses by following structured reasoning templates rather than maximizing reward signals.
- Core assumption: LLMs can maintain coherent multi-turn adversarial strategies through prompt-guided reasoning alone, without requiring reinforcement learning or explicit objective maximization.
- Evidence anchors: [abstract] "formal foundation for strategy emergence through chain-of-thought reasoning without explicit optimization"; [section 3.3] Equations 5-10 formalize dual-stream CoT processing; [corpus] Weak corpus support—neighbor papers focus on adversarial attacks on detectors rather than strategy emergence in dialogue.
- Break condition: Strategies become incoherent beyond 20-utterance context windows; Mixtral showed "premature interaction termination" indicating limitations in strategic adaptation without optimization.

### Mechanism 2
- Claim: Two-layer prompt architecture enables demographically authentic victim personas while maintaining tactical coherence.
- Mechanism: Base layer provides high-level trait guidelines (age range, professional demeanor) without rigid characteristics, allowing persona variation. Behavioral layer implements scam-specific tactics. This separation allows flexible persona generation within bounded tactical constraints.
- Core assumption: Separating demographic guidelines from tactical instructions prevents persona drift while enabling natural variation across dialogues.
- Evidence anchors: [abstract] "two-layer prompt architecture enables LLMs to craft demographically authentic victim personas while maintaining strategic coherence"; [section 6.3, Table 3] GPT-4 and Mixtral achieve statistically significant alignment with ACCC profiles; [corpus] No direct corpus validation of two-layer prompt architectures for adversarial dialogue.
- Break condition: Response bounds (length ≤ 30 words) may constrain natural conversation flow; Mixtral's "constrained responses (1.1-1.33) deviate significantly from natural conversation patterns."

### Mechanism 3
- Claim: Opposing objective functions create sustained adversarial engagement that depletes scammer resources.
- Mechanism: Scammer agent maximizes PII_extraction subject to dialogue length constraints; victim agent maximizes dialogue length subject to minimal PII disclosure. This creates a zero-sum dynamic where victim agents deploy delay tactics while scammers escalate pressure.
- Core assumption: Engaging scammers in extended dialogue measurably depletes their operational resources and prevents targeting of real victims.
- Evidence anchors: [section 3.1] Equations 1-2 formalize opposing objectives; [section 6.2, Figure 2] GPT-4 maintains "extended engagements comparable to human scam-baiters"; [corpus] No corpus validation of resource depletion effectiveness.
- Break condition: Ethical constraints prevent GPT-3.5/GPT-4 from scammer roles; Mixtral's premature termination indicates objective misalignment in some configurations.

## Foundational Learning

- Concept: **Chain-of-thought (CoT) reasoning in LLMs**
  - Why needed here: The entire framework relies on decomposing social engineering into sequential reasoning steps that LLMs follow. Without understanding CoT, you cannot modify tactical sequences or debug strategy failures.
  - Quick check question: Can you explain how CoT differs from standard prompting in terms of intermediate reasoning visibility?

- Concept: **Adversarial dialogue systems with opposing objectives**
  - Why needed here: The scammer-victim dynamic requires understanding how to formalize competing goals (PII extraction vs. dialogue prolongation) and how these create emergent strategic behaviors.
  - Quick check question: How would you formalize a new adversarial scenario (e.g., tech support vs. persistent customer) using the paper's objective function notation?

- Concept: **Prompt engineering with behavioral bounds**
  - Why needed here: The two-layer architecture requires understanding how to separate demographic/identity constraints from tactical/behavioral instructions without causing persona collapse or tactical drift.
  - Quick check question: If victim responses become repetitive after 15 turns, which layer (base or behavioral) would you modify first and why?

## Architecture Onboarding

- Component map: LLM API Module -> Prompting Module -> Interaction Module -> Evaluation Layer
- Critical path: 1. Define scam type → selects tactical sequence 2. Initialize agents with prompt templates 3. Per-turn: Load context → Generate via CoT → Validate bounds → Update history 4. Terminate at turn limit or objective completion
- Design tradeoffs:
  - Context window (20 turns): Balances memory coherence vs. API costs; may lose long-term strategy signals
  - Response length (≤30 words): Matches human baiter patterns but may truncate complex tactical responses
  - Temperature settings: Higher values increase persona diversity but risk coherence loss
  - Model restrictions: GPT models ethically constrained to victim roles only; limits scammer simulation fidelity
- Failure signatures:
  - Premature termination: Mixtral ends dialogues early (<10 turns)—check response repetition scores and tactical sequence adherence
  - Persona drift: Inconsistent demographics across turns—verify base layer constraints not overwritten by behavioral instructions
  - Verbose responses: Length scores <2 indicate >45 words—tune temperature down or strengthen length constraints
  - PII over-disclosure: Victim agents revealing real patterns—audit Pv template for information leakage paths
- First 3 experiments:
  1. Baseline replication: Run Deepseek-Deepseek configuration on SSN scam type (100 dialogues); validate average PII requests and dialogue duration match Table 3
  2. Ablation study: Remove behavioral layer Ts from scammer prompt; measure impact on social engineering tactic emergence
  3. Context window stress test: Extend sliding window from 20 to 40 turns; measure coherence scores and API latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can safety-aligned proprietary models (e.g., GPT-4) effectively simulate the adversarial "scammer" agent without triggering refusal mechanisms, compared to open-source alternatives?
- Basis in paper: [Explicit] The authors restrict GPT-3.5 and GPT-4 to victim roles "due to ethical constraints," leaving their potential performance as adversarial agents unmeasured.
- Why unresolved: The ethical guardrails of leading LLMs may prevent the generation of malicious content required for realistic scammer simulation, limiting the framework's ability to use the highest-quality models for red-teaming.
- What evidence would resolve it: A comparative study evaluating the success rate and tactical diversity of GPT-4 in the scammer role versus Mixtral and Deepseek, specifically measuring safety refusal rates.

### Open Question 2
- Question: Does the observed "strategy emergence" represent generalizable reasoning, or is it contingent upon the specific social engineering tactics present in the models' pre-training data?
- Basis in paper: [Inferred] The paper attributes strategy emergence to the CoT architecture, but high performance in "SSN" and "Support" scams may rely on model memorization rather than novel adversarial reasoning.
- Why unresolved: Without testing against entirely novel or low-prevalence scam scenarios, it is difficult to distinguish between sophisticated reasoning and pattern retrieval.
- What evidence would resolve it: Evaluating the framework using "zero-day" social engineering scenarios not present in public datasets to see if coherent strategies still emerge.

### Open Question 3
- Question: To what extent does the text-based cognitive naturalness (score 2.99/3.0) transfer to voice-based channels where latency and prosody are factors?
- Basis in paper: [Inferred] The framework targets "Phone Scams" and evaluates dialogue naturalness, yet the experimental setup relies entirely on text-based LLM generation without integrating TTS or conversational latency.
- Why unresolved: A text dialogue that scores high on coherence may fail in a voice channel if the processing delay or synthesized voice lacks the emotional nuance required to maintain the "human" illusion.
- What evidence would resolve it: Deployment of the system via a telephony interface (e.g., SIP/VoIP) to measure scammer hang-up rates compared to the text-only benchmarks.

## Limitations

- The framework's effectiveness in actual scam prevention remains untested—current validation relies entirely on synthetic dialogue quality metrics rather than real-world deployment outcomes.
- The two-layer prompt architecture's scalability to novel scam types beyond the four studied (support, SSN, refund, reward) represents an open question, as does its performance across different cultural contexts and scam variants.
- The ethical constraints limiting GPT models to victim-only roles create an asymmetric evaluation where scammer behavior quality depends entirely on open models (Mixtral, DeepSeek) with documented limitations in strategic coherence.

## Confidence

**High Confidence:** The cognitive evaluation methodology (G-Eval scoring 1-3 scale) and quantitative metrics (response length, repetition, dialogue duration) are methodologically sound and reproducible. The comparative performance rankings (GPT-4 > DeepSeek > Mixtral > GPT-3.5) align with established LLM capability hierarchies.

**Medium Confidence:** The two-layer prompt architecture's theoretical benefits for persona authenticity and tactical coherence are supported by statistical alignment with ACCC demographic profiles, but the causal relationship between architectural design and emergent strategy quality requires further validation across diverse scam scenarios.

**Low Confidence:** The claim that extended dialogue engagement measurably depletes scammer resources lacks empirical validation beyond dialogue length metrics. The framework's practical impact on reducing real scam victimization remains unproven.

## Next Checks

1. **Resource Depletion Validation:** Deploy Bot Wars-generated dialogues in controlled environments with volunteer scammers (ethical IRB approval required) to measure actual time/resource consumption versus baseline human interaction patterns.

2. **Cross-Cultural Generalization:** Test framework performance on scam types targeting non-Western demographics using culturally-specific prompt modifications and evaluate persona authenticity against region-specific fraud pattern databases.

3. **Real-World Impact Assessment:** Partner with telecom providers or consumer protection agencies to deploy Bot Wars as an automated call screening system, measuring reduction in successful scam completions compared to traditional call-blocking methods.