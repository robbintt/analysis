---
ver: rpa2
title: Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision
arxiv_id: '2509.11476'
source_url: https://arxiv.org/abs/2509.11476
tags:
- fusion
- image
- infrared
- visible
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FusionNet, a novel end-to-end framework for
  infrared and visible image fusion that integrates modality-aware attention and pixel-wise
  alpha blending. FusionNet dynamically adjusts the contribution of infrared and visible
  features using a modality attention module and enables fine-grained, spatially-varying
  fusion via an alpha blending module.
---

# Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision

## Quick Facts
- arXiv ID: 2509.11476
- Source URL: https://arxiv.org/abs/2509.11476
- Authors: Tianyao Sun; Dawei Xiang; Tianqi Ding; Xiang Fang; Yijiashun Qi; Zunduo Zhao
- Reference count: 35
- Key outcome: FusionNet achieves SSIM 0.87, MSE 0.012, Entropy 7.42, and ROI-SSIM 0.84 on M3FD dataset, demonstrating superior semantic preservation and perceptual quality

## Executive Summary
This paper introduces FusionNet, an end-to-end framework for infrared and visible image fusion that addresses the limitations of existing methods in preserving semantic information and handling task-critical regions. The proposed approach integrates modality-aware attention mechanisms with pixel-wise alpha blending to dynamically adjust feature contributions from infrared and visible inputs. Additionally, a target-aware loss function is introduced that leverages weak ROI supervision to emphasize semantic preservation in critical regions, making the system more interpretable and task-oriented.

## Method Summary
FusionNet employs a two-stream encoder-decoder architecture where modality-aware attention dynamically weights infrared and visible features based on their contextual relevance. The modality attention module learns to assign higher weights to infrared features in regions with significant thermal information while emphasizing visible features where texture and detail are important. Pixel-wise alpha blending then combines these weighted features to produce the final fused output. The target-aware loss function incorporates weak ROI supervision to guide the network toward preserving semantic information in task-critical regions, addressing the limitation of traditional fusion metrics that don't account for specific downstream tasks or region-of-interest preservation.

## Key Results
- Achieves SSIM of 0.87 on M3FD dataset, outperforming existing methods
- Maintains MSE of 0.012 while achieving Entropy of 7.42, demonstrating both accuracy and detail preservation
- ROI-SSIM of 0.84 shows effective semantic preservation in target regions
- Superior interpretability through attention visualization and region-specific performance metrics

## Why This Works (Mechanism)
The framework's effectiveness stems from its adaptive feature weighting mechanism that recognizes the complementary nature of infrared and visible modalities. The modality attention module learns to identify which modality contains more relevant information for each spatial location, allowing the network to preserve infrared thermal signatures in target-rich regions while maintaining visible texture details in other areas. The pixel-wise alpha blending provides spatially-varying fusion that adapts to local content rather than applying uniform weighting across the entire image. The target-aware loss function ensures that the network optimizes for task-relevant performance rather than generic quality metrics, making the fusion results more suitable for downstream applications like object detection or tracking.

## Foundational Learning
- **Modality-aware attention**: Needed to dynamically weight infrared vs visible features based on content relevance; quick check: verify attention maps highlight expected regions (thermal for targets, visible for texture)
- **Pixel-wise alpha blending**: Required for spatially-varying fusion instead of uniform weighting; quick check: confirm alpha values vary smoothly across semantically meaningful boundaries
- **Weak ROI supervision**: Enables task-specific optimization without dense annotations; quick check: validate performance improves when ROI annotations are accurate and degrades gracefully when sparse
- **End-to-end training**: Necessary for joint optimization of attention, blending, and loss components; quick check: ensure gradient flow through all modules during backpropagation
- **Multi-scale feature fusion**: Important for preserving both global context and local details; quick check: verify features from different scales contribute appropriately to final output
- **Task-aware evaluation metrics**: Required to measure semantic preservation beyond traditional image quality metrics; quick check: confirm ROI-SSIM correlates with downstream task performance

## Architecture Onboarding

**Component Map**: Input Images -> Two-stream Encoder -> Modality Attention Module -> Alpha Blending Module -> Decoder -> Fused Output -> Target-Aware Loss

**Critical Path**: Input -> Encoder Feature Extraction -> Modality Attention (weighted combination) -> Alpha Blending (pixel-wise fusion) -> Decoder (reconstruction) -> Output

**Design Tradeoffs**: The framework balances between preserving modality-specific information (through attention) and achieving smooth fusion (through alpha blending). While the two-stream architecture increases parameter count compared to single-stream approaches, it allows for better modality-specific feature extraction. The use of weak ROI supervision reduces annotation burden but may limit performance in highly dynamic scenes.

**Failure Signatures**: Poor attention maps that consistently favor one modality regardless of content, alpha values that don't vary spatially indicating improper blending, or fused outputs that lose either thermal information or visible texture depending on the scene content.

**First Experiments**: 1) Visualize attention maps on diverse image pairs to verify modality-specific weighting; 2) Ablate the target-aware loss to quantify its contribution to semantic preservation; 3) Test on cross-dataset scenarios to evaluate generalization beyond M3FD.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does FusionNet maintain temporal consistency when extended to video-level infrared and visible image fusion?
- Basis in paper: [explicit] The Conclusion states that "future work" will extend the framework to "video-level fusion."
- Why unresolved: The current methodology is evaluated strictly in a "single-frame fusion setting" (Section IV.A), processing each image pair independently without utilizing temporal information.
- What evidence would resolve it: Quantitative results on video datasets measuring temporal stability (e.g., optical flow consistency) alongside standard fusion metrics.

### Open Question 2
- Question: How effectively does FusionNet generalize to unseen environmental conditions or sensor configurations without retraining?
- Basis in paper: [explicit] The Conclusion identifies "domain adaptation for robust scene understanding in complex environments" as a necessary future extension.
- Why unresolved: Evaluation is currently limited to the M3FD dataset (Section IV.A), leaving cross-domain performance unverified.
- What evidence would resolve it: Cross-dataset benchmarking results (e.g., training on M3FD, testing on TNO or RoadScene) demonstrating stable performance.

### Open Question 3
- Question: Is the proposed target-aware loss robust to sparse or noisy bounding-box annotations?
- Basis in paper: [inferred] The method relies on "weak ROI supervision" (Section III.D), but Section IV.A does not evaluate performance degradation when annotations are missing or inaccurate.
- Why unresolved: It is unclear if the loss function creates a dependency on dense, high-quality annotations, which may not be available in real-world deployment.
- What evidence would resolve it: An ablation study measuring ROI-SSIM and detection accuracy as the density of ground-truth bounding boxes is systematically reduced.

## Limitations
- Evaluation is limited to a single dataset (M3FD), potentially limiting generalizability across different imaging conditions
- The framework's computational efficiency and real-time performance capabilities are not assessed
- Dependency on ROI annotations, even if weak, may limit applicability in scenarios where such annotations are unavailable

## Confidence
- **High confidence**: The architectural design incorporating modality-aware attention and pixel-wise alpha blending is technically sound and well-motivated by prior work in attention mechanisms and multi-modal fusion
- **Medium confidence**: The target-aware loss function with weak ROI supervision is innovative but requires validation on more diverse datasets to confirm robustness across different target types and imaging conditions
- **Medium confidence**: The comparative performance claims are based on standard evaluation metrics, though the absence of real-time performance metrics and computational efficiency analysis limits assessment of practical deployment viability

## Next Checks
1. Test FusionNet on multiple infrared-visible fusion datasets with varying environmental conditions (weather, time of day, sensor configurations) to verify robustness beyond M3FD
2. Conduct ablation studies to isolate the contribution of each component (modality attention, alpha blending, target-aware loss) to overall performance
3. Evaluate real-time performance and computational efficiency on resource-constrained platforms to assess practical deployment feasibility