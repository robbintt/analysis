---
ver: rpa2
title: 'IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages'
arxiv_id: '2512.00333'
source_url: https://arxiv.org/abs/2512.00333
tags:
- languages
- language
- question
- indic
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IndicParam is a human-curated benchmark of over 13,000 multiple-choice
  questions covering 11 low- and extremely low-resource Indic languages. It evaluates
  LLM performance on language understanding and general knowledge tasks using graduate-level
  exam questions.
---

# IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages

## Quick Facts
- arXiv ID: 2512.00333
- Source URL: https://arxiv.org/abs/2512.00333
- Reference count: 40
- Primary result: 58% average accuracy for best-performing model (Gemini-2.5) on 13,000+ questions across 11 low-resource Indic languages

## Executive Summary
IndicParam is a human-curated benchmark designed to evaluate large language models on low- and extremely low-resource Indic languages. The benchmark comprises over 13,000 multiple-choice questions across 11 languages, focusing on both language understanding and general knowledge tasks using graduate-level exam questions. Even state-of-the-art models like Gemini-2.5 achieve only 58% average accuracy, demonstrating the benchmark's challenging nature and its effectiveness in assessing cross-lingual transfer capabilities in underrepresented languages. The dataset distinguishes between knowledge-oriented and linguistic questions and supports diverse question formats, making it a valuable resource for evaluating LLM performance in low-resource contexts.

## Method Summary
The benchmark was created through human curation of graduate-level exam questions across 11 low- and extremely low-resource Indic languages. Questions were carefully selected to cover both language understanding and general knowledge domains, with a total of 13,000+ multiple-choice questions. The dataset supports diverse question formats and explicitly distinguishes between knowledge-oriented and linguistic questions to provide comprehensive evaluation of language capabilities. The questions were designed to test both linguistic proficiency and domain knowledge, making the benchmark particularly challenging for LLMs trained on limited data for these language pairs.

## Key Results
- Gemini-2.5 achieves only 58% average accuracy across all languages and question types
- Benchmark covers 11 low- and extremely low-resource Indic languages
- Dataset contains over 13,000 human-curated multiple-choice questions
- Questions span both knowledge-oriented and linguistic domains at graduate-level difficulty

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on low-resource languages where LLMs typically struggle due to limited training data. By using graduate-level exam questions, it ensures high linguistic and cognitive complexity. The human curation process guarantees question quality and relevance, while the distinction between knowledge and linguistic questions allows for nuanced evaluation of different capabilities. The diverse question formats prevent models from exploiting format-specific patterns, ensuring genuine language understanding is tested.

## Foundational Learning
- **Multilingual LLM Evaluation**: Understanding how models perform across language families is crucial for assessing cross-lingual transfer - quick check: verify model performance consistency across language clusters
- **Low-Resource Language Challenges**: Recognizing data scarcity impacts model capabilities - quick check: compare performance gaps between high-resource and low-resource languages
- **Human-Curated Datasets**: Ensures quality control but introduces potential subjectivity - quick check: validate inter-annotator agreement rates
- **Question Classification Systems**: Distinguishing knowledge vs linguistic questions enables targeted evaluation - quick check: confirm classification accuracy through blind reviews
- **Multiple-Choice Format Design**: Balances assessment rigor with automated evaluation feasibility - quick check: test for format-specific model exploitation
- **Graduate-Level Assessment Standards**: Ensures cognitive complexity matches real-world language proficiency requirements - quick check: benchmark against established academic standards

## Architecture Onboarding

**Component Map**: Human curators -> Question classification -> Dataset compilation -> Model evaluation -> Performance analysis

**Critical Path**: Human curation → Question classification → Dataset compilation → Model testing → Results analysis

**Design Tradeoffs**: Human curation ensures quality but limits scalability; multiple-choice format enables automated evaluation but may not capture all linguistic nuances; focusing on low-resource languages addresses a critical gap but may not generalize to high-resource language performance.

**Failure Signatures**: Poor cross-lingual transfer on linguistically distant languages; over-reliance on format-specific patterns; inability to handle complex grammatical structures; knowledge gaps in culturally specific domains.

**First 3 Experiments**:
1. Evaluate model performance on knowledge-oriented vs linguistic questions separately to identify capability gaps
2. Test cross-lingual transfer by training on high-resource languages and evaluating on low-resource targets
3. Assess question format robustness by introducing variations in multiple-choice presentation

## Open Questions the Paper Calls Out
None

## Limitations
- Human curation introduces potential subjectivity in question selection and difficulty calibration
- Reported 58% accuracy may not fully represent model limitations due to limited model testing
- Distinction between knowledge-oriented and linguistic questions lacks detailed validation effectiveness
- Coverage of only 11 Indic languages may not capture full linguistic diversity of the language family

## Confidence

**High confidence**: Benchmark creation process and dataset size (13,000+ questions across 11 languages) are well-documented and verifiable.

**Medium confidence**: Claim of distinguishing between knowledge and linguistic questions is supported by methodology but requires further validation in actual use cases.

**Medium confidence**: Reported 58% accuracy for Gemini-2.5 is based on limited model testing and may not generalize to other evaluations.

## Next Checks
1. Conduct cross-validation with additional multilingual models to confirm benchmark's difficulty and consistency in evaluating low-resource language understanding
2. Perform inter-annotator agreement studies to assess reliability of human-curated question classification between knowledge-oriented and linguistic categories
3. Expand evaluation to include more diverse question types and formats to ensure benchmark's robustness across different linguistic phenomena in Indic languages