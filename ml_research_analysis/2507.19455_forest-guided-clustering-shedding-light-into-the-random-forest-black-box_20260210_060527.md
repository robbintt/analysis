---
ver: rpa2
title: Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box
arxiv_id: '2507.19455'
source_url: https://arxiv.org/abs/2507.19455
tags:
- clustering
- feature
- cluster
- clusters
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Forest-Guided Clustering (FGC) is a novel explainability method
  that reveals both local and global structure in Random Forests by grouping instances
  according to shared decision paths. Unlike traditional feature-centric explanations,
  FGC clusters samples based on model-informed proximity, enabling interpretable segmentation
  aligned with the RF's internal logic.
---

# Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box
## Quick Facts
- arXiv ID: 2507.19455
- Source URL: https://arxiv.org/abs/2507.19455
- Reference count: 40
- Primary result: FGC clusters samples based on RF decision paths, revealing structure missed by feature-centric methods

## Executive Summary
Forest-Guided Clustering (FGC) is a novel explainability method that reveals both local and global structure in Random Forests by grouping instances according to shared decision paths. Unlike traditional feature-centric explanations, FGC clusters samples based on model-informed proximity, enabling interpretable segmentation aligned with the RF's internal logic. Applied to a simulated dataset, FGC accurately recovered latent subclass structure (ARI = 0.98) and outperformed both classical clustering and post-hoc explanation methods. In an AML transcriptomic case study, FGC uncovered biologically coherent subpopulations, disentangled disease-relevant signals from confounders, and recovered known and novel gene expression patterns. FGC bridges the gap between performance and interpretability by providing structure-aware insights that go beyond feature-level attribution.

## Method Summary
FGC constructs a proximity matrix from Random Forest decision paths, where each entry represents the frequency with which two samples co-occur in the same terminal node across all trees. This matrix captures the RF's learned notion of similarity between instances. Standard clustering algorithms are then applied to this proximity matrix to identify groups of samples that follow similar decision paths through the forest. The resulting clusters reflect the model's internal organization of the data, revealing both local decision boundaries and global structure that may be obscured by traditional feature-based analysis. FGC can be applied to both classification and regression RFs, with the proximity measure adapted appropriately for each case.

## Key Results
- On synthetic data with known subclass structure, FGC achieved ARI = 0.98 and successfully recovered the latent groupings
- FGC identified biologically coherent subpopulations in AML transcriptomic data that aligned with known disease subtypes
- The method uncovered gene expression patterns distinguishing disease-relevant signals from confounding factors, including both validated and novel findings

## Why This Works (Mechanism)
FGC leverages the Random Forest's learned decision boundaries to create a similarity metric that reflects the model's internal representation of the data. By counting co-occurrences in terminal nodes, the method captures not just feature similarity but the actual decision logic the RF uses to separate instances. This approach naturally accounts for complex, non-linear interactions and hierarchical relationships that may be missed by feature-based distance metrics. The clustering step then groups samples that the RF treats similarly, revealing structure that aligns with the model's learned patterns rather than arbitrary feature spaces.

## Foundational Learning
- Random Forest decision paths: Why needed - to understand how FGC derives its proximity measure; Quick check - can you trace a sample's path through multiple trees?
- Proximity matrices: Why needed - core data structure for FGC; Quick check - does your proximity matrix have zeros on diagonal and symmetry?
- Clustering algorithms: Why needed - to group samples based on RF proximity; Quick check - have you tested multiple clustering methods on your proximity matrix?
- Model interpretability trade-offs: Why needed - to understand FGC's contribution to explainable AI; Quick check - can you articulate how FGC differs from feature importance methods?
- Evaluation metrics (ARI, NMI): Why needed - to quantify clustering quality against ground truth; Quick check - do you understand the difference between ARI and NMI?
- Biological validation methods: Why needed - for interpreting case study results; Quick check - can you identify which AML subtypes were recovered by FGC?

## Architecture Onboarding
- Component map: RF training -> Proximity matrix construction -> Clustering algorithm -> Result interpretation
- Critical path: The proximity matrix computation is the bottleneck, scaling with n_samples Ã— n_trees
- Design tradeoffs: Deeper trees capture more nuance but risk overfitting; more trees improve stability but increase computation
- Failure signatures: Poor clustering may indicate inadequate tree depth, insufficient number of trees, or inappropriate clustering algorithm choice
- First experiments: 1) Verify proximity matrix properties (symmetry, diagonal = 1), 2) Test clustering with varying numbers of clusters, 3) Compare results with ground truth on synthetic data

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on RF architecture choices (tree depth, number of trees)
- Assumes decision paths capture meaningful structure, which may not hold for all datasets
- Computational complexity scales with number of trees and instances, limiting scalability

## Confidence
- Synthetic dataset results: High confidence due to clear ground truth validation
- Biological case study findings: Medium confidence based on domain knowledge and literature alignment
- Generalizability claims: Medium confidence requiring broader empirical validation

## Next Checks
1. Test FGC's stability across different Random Forest hyperparameters (tree depth, number of trees) to establish robustness bounds
2. Compare FGC against other model-agnostic clustering approaches on multiple benchmark datasets with known structure
3. Validate biological findings from the AML case study using independent datasets or experimental follow-up to confirm discovered subpopulations and gene expression patterns