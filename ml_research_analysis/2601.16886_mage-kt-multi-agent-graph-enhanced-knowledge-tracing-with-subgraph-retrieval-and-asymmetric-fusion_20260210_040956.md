---
ver: rpa2
title: 'MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval
  and Asymmetric Fusion'
arxiv_id: '2601.16886'
source_url: https://arxiv.org/abs/2601.16886
tags:
- knowledge
- graph
- student
- tracing
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAGE-KT improves knowledge tracing by using a multi-agent pipeline
  to extract richer, more accurate inter-knowledge concept (KC) relations and by retrieving
  compact, student-relevant subgraphs for efficient prediction. It combines a heterogeneous
  graph with a multi-relational KC graph and a student-question interaction graph,
  enhanced with IRT-based ability and difficulty scores.
---

# MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion

## Quick Facts
- arXiv ID: 2601.16886
- Source URL: https://arxiv.org/abs/2601.16886
- Reference count: 0
- MAGE-KT achieves state-of-the-art accuracy (83.06-87.29%) and AUC (87.89-91.79%) on three knowledge tracing datasets

## Executive Summary
MAGE-KT introduces a multi-agent pipeline for knowledge tracing that extracts richer inter-knowledge concept relations and retrieves student-relevant subgraphs for efficient prediction. The framework combines a heterogeneous graph with multi-relational knowledge concept graphs and student-question interaction graphs, enhanced with IRT-based ability and difficulty scores. The asymmetric cross-attention fusion module integrates these views while reducing noise and irrelevant attention. Experiments demonstrate significant improvements over both sequence-based and graph-based baselines across three datasets.

## Method Summary
MAGE-KT employs a multi-agent pipeline where GPT-4 generates knowledge concept relations, which are then refined through self-reflection and voting. The model constructs a heterogeneous graph combining knowledge concepts, questions, and interactions, enhanced with IRT-based ability and difficulty scores. A student-conditioned subgraph retrieval mechanism extracts compact, relevant subgraphs based on historical interactions. The asymmetric cross-attention fusion module integrates graph and sequence representations, using bidirectional attention between the sequence encoder and subgraph, but unidirectional attention from the subgraph to the sequence encoder. This design reduces noise and irrelevant attention while preserving critical information for knowledge tracing predictions.

## Key Results
- Achieves highest accuracy (83.06-87.29%) and AUC (87.89-91.79%) across ASSIST09, Junyi, and Statics2011 datasets
- Outperforms both sequence-based and graph-based baselines in all evaluation metrics
- Ablation studies confirm significant contributions from multi-agent KC relation extraction, student-conditioned subgraph retrieval, and asymmetric fusion

## Why This Works (Mechanism)
The multi-agent pipeline generates more accurate and diverse knowledge concept relations compared to manual curation, capturing richer semantic relationships between concepts. Student-conditioned subgraph retrieval ensures the model focuses on compact, relevant information rather than processing entire graphs, improving efficiency and reducing noise. The asymmetric fusion mechanism addresses the limitation of bidirectional attention by preventing irrelevant information from the subgraph affecting the sequence representation, while still allowing the sequence to attend to relevant subgraph features.

## Foundational Learning

**Heterogeneous Graph Construction**
- Why needed: To capture multiple types of relationships (knowledge concepts, questions, student interactions) in a unified structure
- Quick check: Verify node types and edge relationships are correctly defined in the final graph

**IRT Parameter Estimation**
- Why needed: To incorporate student ability and question difficulty as node attributes, providing additional signal for prediction
- Quick check: Confirm IRT scores are properly scaled and normalized before being added to the graph

**Graph Attention Networks**
- Why needed: To learn node representations by aggregating information from neighbors while weighting their importance
- Quick check: Ensure attention coefficients are properly normalized and can be interpreted as importance weights

## Architecture Onboarding

**Component Map**
Student Interaction History -> Multi-Agent KC Relation Extractor -> Heterogeneous Graph Builder -> Subgraph Retriever -> Asymmetric Fusion Module -> Knowledge Tracing Predictor

**Critical Path**
Student interaction sequence → Graph attention encoding → Subgraph retrieval → Asymmetric cross-attention fusion → Final prediction

**Design Tradeoffs**
- Complexity vs. performance: Multi-agent extraction and asymmetric fusion add complexity but provide significant performance gains
- Interpretability vs. accuracy: The asymmetric mechanism improves accuracy but makes the attention flow harder to interpret
- Computational cost vs. efficiency: Subgraph retrieval reduces computation compared to full graph processing

**Failure Signatures**
- Poor performance on datasets with limited interaction history
- Overfitting when the multi-agent extraction generates overly specific relations
- Degradation when IRT parameters are poorly estimated or unavailable

**First Experiments**
1. Compare performance with and without multi-agent KC relation extraction on a validation set
2. Test different subgraph sizes (k values) to find optimal trade-off between relevance and coverage
3. Evaluate the impact of removing asymmetric fusion by switching to bidirectional attention

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Multi-agent pipeline relies on GPT-4, introducing computational cost and dependency on proprietary model
- Asymmetric fusion mechanism adds architectural complexity that may limit interpretability
- Model requires accurately estimated IRT parameters, which may not be available for all educational datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| Performance improvements on tested datasets | High |
| Architectural innovations contribution | Medium |
| Generalizability to other domains | Low |

## Next Checks
1. Conduct reproducibility tests using open-source alternatives to GPT-4 for multi-agent KC relation extraction
2. Perform cross-domain validation on non-mathematics educational datasets
3. Implement a simplified version of the asymmetric fusion mechanism to assess performance trade-offs