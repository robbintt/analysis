---
ver: rpa2
title: 'XAI-Units: Benchmarking Explainability Methods with Unit Tests'
arxiv_id: '2506.01059'
source_url: https://arxiv.org/abs/2506.01059
tags:
- methods
- feature
- attribution
- xai-units
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XAI-Units is a benchmark for evaluating feature attribution methods
  in explainable AI. It provides synthetic datasets and handcrafted models with known
  internal mechanisms to establish ground truth expectations for desirable attribution
  scores.
---

# XAI-Units: Benchmarking Explainability Methods with Unit Tests

## Quick Facts
- arXiv ID: 2506.01059
- Source URL: https://arxiv.org/abs/2506.01059
- Reference count: 31
- Primary result: Synthetic benchmark reveals FA methods struggle with gradient discontinuities, conflicting features, and non-linear interactions despite success on simple weighted models

## Executive Summary
XAI-Units introduces a benchmarking framework for evaluating feature attribution methods using synthetic datasets with handcrafted models that have known internal mechanisms. The benchmark consists of seven tabular, two image, and one text dataset-model pairs designed to test specific model behaviors like feature interactions, cancellations, and discontinuous outputs. By establishing ground truth through ablation methods, the benchmark enables systematic comparison of FA methods and reveals implementation discrepancies, such as a normalization bug in DeepLIFT's Captum implementation. Results show FA methods perform well on simple weighted models but struggle with gradient discontinuities, conflicting features, and non-linear interactions.

## Method Summary
The benchmark uses procedurally generated synthetic datasets paired with handcrafted neural networks whose mathematical formulations are explicitly known. Ground truth attributions are derived through ablation (M(x) - M(x₋ᵢ)) rather than heuristic proxy metrics. The framework includes ten dataset generators, a model zoo with both handcrafted and trained variants, FA method wrappers around Captum, built-in evaluation metrics (MSE, Mask Error, SensitivityMax, Infidelity), and an evaluation pipeline. Each dataset-model pair isolates a specific atomic behavior to test, analogous to software unit tests. Experiments run for five trials on test sets of size 1000, with default metrics specified for each dataset.

## Key Results
- All FA methods performed well on Weighted Continuous models (MSE ~0) but struggled on models with gradient discontinuities like Shattered Gradients and Pertinent Negatives
- DeepLIFT showed implementation discrepancy: 62.9 ± 80.5 Mask Error with Captum defaults vs. 0.000 when targeting logits with proper normalization
- Gradient-based methods performed significantly worse on handcrafted Conflicting Feature models (MSE ~1.3-4.5) compared to trained equivalents (MSE ~0.1-0.2)
- KernelSHAP and LIME struggled on non-linear interaction cases due to linear surrogate assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic dataset-model pairs with known internal mechanisms establish verifiable ground truth attributions.
- Mechanism: By procedurally generating neural networks with explicit mathematical formulations (e.g., y = Wx for Weighted Continuous), ground truth becomes derivable via ablation to baseline references rather than relying on heuristic proxy metrics.
- Core assumption: Handcrafted models faithfully represent targeted atomic behavior in a way that generalizes to trained models exhibiting similar patterns.
- Evidence anchors:
  - [abstract] "Our benchmark provides a set of paired datasets and models with known internal mechanisms, establishing clear expectations for desirable attribution scores."
  - [Section 3.1] "Each dataset and model pair is analogous to a unit test that isolates a single type of input behaviour to test for."
  - [corpus] Related papers focus on post-hoc evaluation but lack the ground-truth-by-construction approach.

### Mechanism 2
- Claim: Isolating atomic behaviors enables systematic identification of FA method failure modes.
- Mechanism: Each dataset-model pair targets a specific challenge (gradient discontinuities, feature cancellation, non-linear interactions). Evaluating FA methods against these isolated cases reveals failure patterns obscured in complex real-world datasets.
- Core assumption: Failure on atomic synthetic cases indicates likely failure on real-world models with similar characteristics.
- Evidence anchors:
  - [Section 4.1] "All FA methods performed well on the simplest test case, the Weighted Continuous models, but struggled on models with gradient discontinuities."
  - [Section 4.1 Case Study] Gradient-based FA methods showed higher error on handcrafted Conflicting Feature models than trained models.
  - [corpus] No direct corpus evidence on this specific unit-test isolation approach.

### Mechanism 3
- Claim: Benchmarking can surface implementation discrepancies in FA libraries.
- Mechanism: When a theoretically correct FA method produces unexpected results on a well-defined test case, the discrepancy can be traced to implementation choices rather than method design, as demonstrated by the DeepLIFT normalization issue.
- Core assumption: The benchmark's ground truth is correct and the theoretical expectations of the FA method are well-specified.
- Evidence anchors:
  - [Section 4.1, Table 4] DeepLIFT achieved high Mask Error (62.9 ± 80.5) with Captum's default implementation but 0.000 error when targeting logits with normalization applied correctly.
  - [Section 4.1] "Our analysis demonstrates the utility of XAI-Units not only for comparing different FA methods, but also for surfacing and diagnosing issues with their implementation."

## Foundational Learning

- Concept: Feature Attribution Ground Truth via Ablation
  - Why needed here: Understanding how the paper defines "correct" attributions (M(x) − M(x₋ᵢ) for most tabular datasets) is essential for interpreting all evaluation results.
  - Quick check question: For the Conflicting Features model, why does the ground truth attribution for feature xᵢ require first ablating the cancellation feature cᵢ?

- Concept: Gradient Discontinuities in Neural Networks
  - Why needed here: Shattered Gradients and Pertinent Negatives test cases specifically target FA method failures near ReLU discontinuities; understanding why gradients become uninformative at these points explains performance differences.
  - Quick check question: Why would InputXGradient fail on a Pertinent Negatives model where the input value is 0 but meaningfully affects output?

- Concept: Softmax Translation Invariance
  - Why needed here: The Uncertainty Model exploits softmax properties to create features that contribute to logits but not class predictions; understanding this is necessary to interpret the DeepLIFT implementation bug findings.
  - Quick check question: If a feature adds a constant k to all class logits, why should its attribution for the predicted class be zero?

## Architecture Onboarding

- Component map:
  Data Generators -> Model Zoo -> FA Method Wrappers -> Metrics Suite -> Evaluation Pipeline

- Critical path:
  1. Select dataset-model pair matching the behavior you want to test
  2. Configure FA method with appropriate baseline (zero for most tabular, whitespace token for text)
  3. Run evaluation with dataset-specific default metric
  4. Compare results across handcrafted vs. trained model variants

- Design tradeoffs:
  - Synthetic data enables ground truth but may not reflect real-world complexity
  - Handcrafted models guarantee alignment with expected behavior but may over-represent specific architectural patterns
  - Single-behavior isolation improves diagnostic clarity but requires multiple tests for comprehensive assessment

- Failure signatures:
  - High variance across seeds suggests sensitivity to model initialization rather than FA method properties
  - Large performance gap between handcrafted and trained models may indicate training artifacts rather than FA limitations
  - Zero attribution on features known to affect output indicates gradient-based method failure (common with ReLU)

- First 3 experiments:
  1. Run DeepLIFT and IntegratedGradients on Weighted Continuous (baseline sanity check—should achieve ~0 MSE on handcrafted model)
  2. Compare KernelSHAP vs. LIME on Conflicting Features (expect both to struggle due to linear surrogate assumptions)
  3. Reproduce DeepLIFT Uncertainty Model discrepancy: test Captum default vs. logits-with-normalization to verify implementation bug diagnosis

## Open Questions the Paper Calls Out

- **Extending to semi-synthetic/real-world data**: The authors state they envision future extensions to include semi-synthetic or real-world applications while preserving ground truth verification capability. This remains unresolved because the current framework relies entirely on handcrafted models and synthetic datasets to establish atomic units of behavior with known truths, which is lost with real-world data.

- **Human-centric evaluation**: The paper focuses on technical evaluation (e.g., verifying implementation correctness) rather than cognitive utility or complexity of explanations for human users. User studies comparing FA methods ranked highly by XAI-Units against human performance metrics in downstream tasks would be needed.

- **Mechanistic reasons for FA failures**: The paper speculates that gradient-based methods perform worse on handcrafted Conflicting Feature models due to conflict behavior being distributed across multiple neurons, but stops short of confirming this hypothesis through theoretical analysis or ablation studies.

## Limitations

- The benchmark relies on synthetic, handcrafted models that may not fully capture the complexity and diversity of real-world model behaviors
- The Rashomon effect (multiple models achieving similar performance with different internal mechanisms) creates potential gaps between handcrafted ground truth and trained model behaviors
- Single-behavior isolation improves diagnostic clarity but may miss emergent failure modes that arise from combinations of behaviors in real-world scenarios

## Confidence

- **High confidence**: The mechanism by which synthetic datasets with known ground truth enable systematic FA method comparison (supported by clear mathematical formulations and reproducible results)
- **Medium confidence**: The claim that FA methods struggle with specific behaviors like gradient discontinuities (supported by consistent performance patterns across multiple methods)
- **Medium confidence**: The implementation verification utility (demonstrated by one clear case but lacking systematic exploration of this benefit)

## Next Checks

1. Conduct a systematic study quantifying the divergence rate between handcrafted and trained model attributions across all benchmark datasets to establish typical vs. exceptional performance gaps
2. Test FA method performance on composite datasets that combine multiple atomic behaviors to identify emergent failure modes not visible in isolated tests
3. Perform ablation studies on the handcrafted model constructions to measure sensitivity of ground truth attributions to small parameter variations