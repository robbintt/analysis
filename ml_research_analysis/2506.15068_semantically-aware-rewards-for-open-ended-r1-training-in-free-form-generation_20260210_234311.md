---
ver: rpa2
title: Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation
arxiv_id: '2506.15068'
source_url: https://arxiv.org/abs/2506.15068
tags:
- answer
- reward
- grpo
- qwen2
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating and improving open-ended,
  long-form text generation using reinforcement learning from verifiable rewards (RLVR),
  particularly in Group Relative Policy Optimization (GRPO) training. Traditional
  evaluation metrics like ROUGE-L and BERTScore fail to capture semantic quality and
  human preferences, while large reward models are computationally expensive.
---

# Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation

## Quick Facts
- arXiv ID: 2506.15068
- Source URL: https://arxiv.org/abs/2506.15068
- Reference count: 16
- PrefBERT (150M ModernBERT) provides semantically-aware rewards that outperform traditional metrics and large reward models in GRPO training for long-form generation

## Executive Summary
This paper addresses the challenge of evaluating and improving open-ended, long-form text generation using reinforcement learning from verifiable rewards (RLVR), particularly in Group Relative Policy Optimization (GRPO) training. Traditional evaluation metrics like ROUGE-L and BERTScore fail to capture semantic quality and human preferences, while large reward models are computationally expensive. The authors introduce PrefBERT, a lightweight ModernBERT model trained on human-annotated Likert ratings to score generated responses against references. Evaluated across three long-form datasets, models trained with PrefBERT in GRPO outperformed those trained with traditional metrics and matched larger models in quality while being significantly more efficient.

## Method Summary
PrefBERT uses a ModernBERT base (150M parameters) with a linear regressor and sigmoid activation to predict normalized quality scores (0-1) from reference-generated text pairs. The model is trained on human Likert ratings (1-5) from Prometheus-preference and MOCHA datasets, minimizing MSE loss on normalized scores. For GRPO training, group-normalized advantages are computed using PrefBERT rewards, and the OpenRLHF framework optimizes the policy model. The approach replaces traditional metrics and large reward models in the RL loop, enabling semantically-aware reward signals for long-form generation tasks.

## Key Results
- PrefBERT-trained models outperformed traditional metric-based models (ROUGE-L, BERTScore) on human evaluations for coherence, relevance, and conciseness
- PrefBERT matched or exceeded larger reward models (GRM-llama-3B) while being 20x smaller and using 4x less training data
- Human evaluators preferred PrefBERT-trained outputs (258 words average) over GRM-llama-3B outputs (710 words average), indicating resistance to verbosity bias
- Reward curves showed meaningful progression with PrefBERT versus flat curves with traditional metrics

## Why This Works (Mechanism)

### Mechanism 1
PrefBERT provides semantically discriminative reward signals that distinguish high-quality from low-quality long-form generations better than traditional metrics. Trained on human Likert ratings from diverse long-form datasets, it learns to map reference-generated pairs to normalized quality scores via MSE regression on ModernBERT embeddings. The core assumption is that human Likert ratings capture semantic quality dimensions (coherence, relevance, conciseness) that n-gram overlap and embedding similarity metrics miss.

### Mechanism 2
Group-normalized advantages in GRPO enable stable learning from scalar rewards without a separate value function. For each prompt, GRPO samples G responses, computes group-relative advantages A(x,yi) = (r(x,yi) - r̄) / σr, then applies clipped surrogate optimization with KL regularization to the reference model. The core assumption is that within-group variance provides meaningful relative quality signal even when absolute reward scale is noisy.

### Mechanism 3
Lightweight evaluator fine-tuning avoids reward hacking behaviors (verbosity bias) observed in larger general-purpose reward models. PrefBERT's training on Likert-rated quality targets absolute quality calibration rather than relative preference, reducing correlation with response length. The core assumption is that Likert ratings encode conciseness as a quality dimension while preference pair training can conflate length with preference.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: Understanding how relative advantages enable RL training without a learned value function is essential for debugging reward signal quality
  - Quick check: Given 4 sampled responses with rewards [0.2, 0.4, 0.6, 0.8], what are the normalized advantages?

- **Concept: Reward hacking in RLHF**
  - Why needed: Recognizing when models exploit reward model blindspots (e.g., generating verbose responses) is critical for evaluation design
  - Quick check: If a reward model scores longer responses higher regardless of content, what failure mode will emerge during training?

- **Concept: Reference-based vs. reference-free evaluation**
  - Why needed: PrefBERT uses reference answers; understanding when references are available vs. when they're not determines applicability
  - Quick check: For a creative writing task with no single correct answer, what challenges arise for reference-based evaluation?

## Architecture Onboarding

- **Component map:** `[CLS] reference [SEP] generated` → ModernBERT → pooled embedding → linear regressor + sigmoid → normalized score (0-1) → GRPO training loop
- **Critical path:** 1) Curate training data with (reference, generated, Likert) triplets ensuring quality rating balance; 2) Train PrefBERT with MSE loss on normalized scores; 3) Integrate PrefBERT as reward function in GRPO; 4) Verify reward curves show meaningful progression; 5) Evaluate policy models with multiple judges
- **Design tradeoffs:** PrefBERT (150M) vs. GRM-llama-3B (3B): 20x parameter reduction, 4x less training data, but requires reference answers; Reference-based vs. reference-free: PrefBERT needs references but avoids length bias; Training data size: 19K examples sufficient for PrefBERT
- **Failure signatures:** Reward curve flat (no improvement): traditional metric provides insufficient gradient signal; Reward strongly correlates with length: reward model has length bias (GRM pattern); High LLM-judge scores but low human ratings: evaluator bias toward verbosity; SFT-style shallow outputs: policy model not receiving discriminative rewards
- **First 3 experiments:** 1) Validate PrefBERT correlates with human judgments: compute Spearman correlation between PrefBERT scores and held-out human Likert ratings; 2) Ablate training data composition: train PrefBERT on only Prometheus-preference vs. only MOCHA vs. combined; 3) Test reward hacking robustness: track response length and repetition rate during GRPO training with PrefBERT vs. GRM-llama-3B

## Open Questions the Paper Calls Out

- **Question:** Would using a larger evaluator model (e.g., 7B-scale) as the verifiable reward provider significantly improve GRPO performance for open-ended generation tasks?
  - Basis: Computational constraints prevented testing larger evaluators; PrefBERT uses only 150M parameters
  - Evidence needed: Training and evaluating GRPO with 7B-scale reward models, comparing alignment quality against PrefBERT

- **Question:** Can PrefBERT-based GRPO training generalize effectively to creative writing, open-ended math problems, and design tasks?
  - Basis: Current experiments only cover ELI5, Alpaca, and LongForm datasets—domains with factual/instructional responses
  - Evidence needed: Evaluation on benchmarks for creative writing, open-ended mathematical reasoning, and design generation tasks

- **Question:** How can reward models be designed to prevent verbosity-based reward hacking, as observed with GRM-llama-3B?
  - Basis: GRM-llama-3B produced excessively verbose outputs (710 words vs. 258 for PrefBERT), correlated reward with length
  - Evidence needed: Ablation studies testing explicit length-penalty terms or multi-objective reward designs

## Limitations

- Reference-dependency restricts applicability to tasks where ground truth answers exist
- Training data (19K examples) may not capture all semantic failure modes for highly creative or subjective generation tasks
- Focus on Qwen2.5 models raises questions about generalizability across different base architectures

## Confidence

- **High Confidence:** PrefBERT outperforms traditional metrics (ROUGE-L, BERTScore) in semantic quality alignment, demonstrated through human evaluation and reward curve analysis
- **Medium Confidence:** PrefBERT matches or exceeds larger reward models (GRM-llama-3B) while being significantly more efficient, though this comparison relies on a single large reward model baseline
- **Medium Confidence:** The claim that Likert rating training prevents length bias requires more direct comparison studies

## Next Checks

1. Test PrefBERT's generalization to reference-free generation tasks (creative writing, dialogue) by measuring performance drop when references are unavailable
2. Conduct ablation studies on training data composition to identify minimum viable dataset size and determine whether diversity or quantity matters more for semantic coverage
3. Evaluate PrefBERT across multiple base model families (LLaMA, Mistral, GPT) to assess architectural dependence and identify potential model-specific optimizations