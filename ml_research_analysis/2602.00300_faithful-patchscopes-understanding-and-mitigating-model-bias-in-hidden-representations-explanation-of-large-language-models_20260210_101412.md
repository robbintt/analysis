---
ver: rpa2
title: 'Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations
  Explanation of Large Language Models'
arxiv_id: '2602.00300'
source_url: https://arxiv.org/abs/2602.00300
tags:
- prompt
- bias
- hidden
- color
- balor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that Patchscopes, a framework for interpreting
  hidden representations of large language models, is affected by model bias arising
  from imbalanced linguistic patterns. The authors show that when decoding from hidden
  representations, models often rely on high-frequency associations rather than the
  contextual information actually encoded.
---

# Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models

## Quick Facts
- arXiv ID: 2602.00300
- Source URL: https://arxiv.org/abs/2602.00300
- Authors: Xilin Gong; Shu Yang; Zehua Cao; Lynne Billard; Di Wang
- Reference count: 40
- Primary result: Proposes BALOR to suppress model bias in Patchscopes, achieving up to 33% relative improvement in explanation faithfulness

## Executive Summary
This paper identifies that Patchscopes, a framework for interpreting hidden representations of large language models, is affected by model bias arising from imbalanced linguistic patterns. The authors show that when decoding from hidden representations, models often rely on high-frequency associations rather than the contextual information actually encoded. To address this, they propose Bias Alignment through Logit Recalibration (BALOR), which uses contrastive prompts to recalibrate logits during decoding, suppressing model bias and amplifying contextual information. Experiments across four tasks and four models demonstrate that BALOR significantly improves explanation faithfulness, achieving up to 33% relative performance improvement over existing baselines.

## Method Summary
BALOR addresses model bias in Patchscopes by introducing contrastive logit recalibration. The method creates a contrastive prompt pair where one contains the patched contextual information and the other contains only the prior bias. During decoding, logits are recalibrated using the formula p_balor(y|T) = softmax[(1+α)l_θ(y|T) − αl_θ(y|T*)], where α controls the amplification strength. Layer selection is performed using combined Logit Difference (LD) and Gradient Similarity Alignment (GSA) metrics, with optimal layers varying by architecture (Llama2-7b=13, Llama3.2-1b=11, Llama3-8b=21, Qwen3-4b=24). Two decoding modes are implemented: Shared (synchronized recalibrated distribution) and Divided (only target prompt uses recalibration).

## Key Results
- Model bias causes 11.84%-28.47% faithfulness decrease on biased vs non-biased data across four models
- BALOR achieves up to 33% relative improvement over vanilla Patchscopes across four tasks
- Shared mode decoding is more stable than Divided mode while maintaining comparable performance
- Layer selection via LD+GSA metrics successfully identifies bias-sensitive layers where contextual attributes are encoded but priors haven't fully dominated

## Why This Works (Mechanism)

### Mechanism 1
Imbalanced linguistic patterns cause model bias that overrides contextual information in hidden representations. Training data with skewed co-occurrence frequencies (e.g., "broccoli" → "green" >> "purple") creates strong prior associations. During decoding, these priors dominate even when hidden representations encode contradictory contextual attributes, as the model's output logits reflect empirical frequencies rather than patched information. Logistic regression shows +1 SD in frequency difference increases bias odds by factor of 9.338 (Llama3.2-1b), ROC-AUC 0.619. 11.84%-28.47% faithfulness decrease observed on biased vs non-biased data across four models.

### Mechanism 2
BALOR's contrastive logit subtraction amplifies contextual information by isolating and suppressing the bias component. Create contrastive prompt pair: target prompt T (patched with contextual information) and contrastive prompt T* (unpatched, containing only priors). The logit difference l_θ(y|T) - l_θ(y|T*) isolates the contribution of patched context. Recalibrated distribution: p_balor(y|T) = softmax[(1+α)l_θ(y|T) - αl_θ(y|T*)], where α controls amplification strength. Formal proof shows if token y₁ is more supported by patched evidence than contrastive prior, BALOR monotonically increases its relative preference as α grows. Up to 33% relative improvement observed across four tasks and four models.

### Mechanism 3
Optimal patching occurs at bias-sensitive layers where contextual attributes are encoded but priors haven't fully dominated. Combine Logit Difference (LD) - measures whether layer differentiates context-aligned vs biased attributes - with Gradient Similarity Alignment (GSA) - measures whether patching affects output gradients. Score = w·LD_norm + (1-w)·GSA_norm (w=0.8), select layer maximizing score. Bias emerges gradually in intermediate layers; early layers lack full encoding, late layers have committed to priors. Layer selection varies by architecture: L13 for Llama2-7b, L11 for Llama3.2-1b, L21 for Llama3-8b, L24 for Qwen3-4b. Spearman correlation ρ=-0.147 between LD and GSA (p<0.001).

## Foundational Learning

- **Logit-lens analysis**: Understanding how hidden representations map to vocabulary logits at each layer. The logit-lens projects intermediate activations through the unembedding matrix to inspect what tokens the model "would predict" at that layer. Quick check: Given hidden state h^(l) and unembedding matrix W_o, how would you compute the logit difference between two candidate tokens?

- **Contrastive decoding methods**: BALOR builds on contrastive approaches where one forward pass provides a baseline (bias) distribution to subtract from a target distribution. Understanding when subtraction vs. division is appropriate in log-odds space is critical. Quick check: Why does BALOR use (1+α)l_target - αl_contrastive instead of simple subtraction l_target - l_contrastive?

- **Patching interventions in transformers**: Patchscopes replaces hidden representations during forward passes. Understanding the causal role of specific activations requires grasping how information flows through residual connections and when patching can alter downstream computation. Quick check: If you patch at layer l, which subsequent computations can potentially change, and which are guaranteed unchanged?

## Architecture Onboarding

- **Component map**:
Source Prompt → Source Model → Extract h_i^(l) at noun position
↓
Target Prompt (with placeholder x) → Target Model → Patch h_i^(l) into x position at layer l*
↓
[BALOR: Parallel forward pass with contrastive prompt]
↓
Logit subtraction: (1+α)l_target - αl_contrastive → Sample output

- **Critical path**:
1. Identify bias-sensitive layer using LD+GSA metrics on validation subset
2. Construct source prompt with contextual attribute, extract hidden representation
3. Create target/contrastive prompt pair; run parallel decoding with BALOR logit fusion
4. Use Shared mode (synchronized decoding) for stability; Divided mode only if task requires divergent generation

- **Design tradeoffs**:
  - **α value**: Higher α = stronger bias suppression but risk of overcorrection. Paper finds α∈[0.4, 2.4] robust; recommend starting at α=0.0
  - **Shared vs Divided mode**: Shared synchronizes both prompts with recalibrated distribution (more stable); Divided lets contrastive prompt diverge (higher variance, occasionally better)
  - **Layer selection cost**: Full LD+GSA scan is O(n) forward passes; can cache results per model or use heuristics (middle layers) for faster deployment

- **Failure signatures**:
  - **Negative SR gain**: α too high (overcorrection) or patching layer too late (priors already committed). Check Figure 6 curves for model-specific α tolerance.
  - **Unstable Divided mode**: Prompts diverge semantically, making logit comparison meaningless. Switch to Shared mode.
  - **Low baseline SR**: Patched representation may not encode target attribute. Verify with linear probe before patching.
  - **Position bias in target prompt**: Model favors first option regardless of content. Use option-swapping and average results.

- **First 3 experiments**:
1. **Validate bias exists**: Run vanilla Patchscopes on color task with biased nouns (e.g., "purple broccoli"). Measure SR - should be ~20-40%. Compare against non-biased nouns - should see 12-28% gap.
2. **Ablate α**: Fix layer (use paper's recommended values), sweep α from 0.4 to 2.4. Plot SR gain curve. Confirm Shared mode is monotonic/stable; Divided mode shows variance.
3. **Cross-task generalization**: Apply BALOR to a task not in paper (e.g., sentiment flip with contradictory context). Assess whether contrastive logit subtraction transfers or if task-specific tuning is needed.

## Open Questions the Paper Calls Out

### Open Question 1
Can BALOR effectively mitigate bias in tasks requiring complex, multi-hop reasoning rather than simple attribute extraction? The experimental validation is restricted to four specific tasks (color, gender, culture, age) which primarily involve retrieving single attribute values from constrained output spaces. It is unclear if the logit recalibration strategy scales to tasks where the "contextual information" is distributed across many tokens or requires synthesizing multiple facts, as the contrastive signal might become noisy.

### Open Question 2
To what extent does the specific phrasing of the contrastive prompt T* influence the stability and effectiveness of logit recalibration? The paper notes that prompt-based debiasing methods generally suffer from instability due to lexical changes, yet BALOR relies on a specific construction of T* (replacing the placeholder with the original noun) to capture model priors. If the contrastive prompt inadvertently encodes specific syntactic structures that interfere with the bias isolation, the recalibration might suppress valid features along with bias.

### Open Question 3
Can the "Divided" (Mode D) decoding strategy be stabilized to prevent sequence divergence while preserving its potential for higher faithfulness? Section 5.2 explicitly notes that Mode D is less stable than Mode S because the target and contrastive prompts follow different next-token prediction strategies, causing the sequences to diverge progressively. Mode D occasionally yields higher Show Rates (SR) but is unreliable; the divergence introduces discrepancies in logit distributions that reduce the method's robustness.

## Limitations

- The approach assumes linear separability of attributes via probing, which may not hold for complex, multi-dimensional contextual information
- Reliance on option-swapping QA for bias detection may not capture all forms of contextual suppression
- The method does not address scenarios where patched representations encode multiple conflicting attributes

## Confidence

- **High confidence**: The existence of model bias in Patchscopes outputs (Section 3 evidence is robust across multiple models)
- **Medium confidence**: The BALOR mechanism effectiveness (formal proof exists but relies on specific architectural assumptions)
- **Low confidence**: Generalization to models beyond the four tested architectures and tasks beyond the four evaluated domains

## Next Checks

1. **Cross-architecture validation**: Apply BALOR to models outside the tested family (e.g., Mistral, Grok, or Claude) using the same four tasks. Measure whether layer selection patterns (LD+GSA peaks) remain consistent and whether SR improvements generalize.

2. **Multi-attribute stress test**: Create synthetic prompts where hidden representations encode multiple attributes (e.g., "green broccoli that is small") and test whether BALOR can suppress bias while preserving all contextual signals. Compare against single-attribute baselines.

3. **Temporal stability analysis**: Run BALOR on the same model after fine-tuning on different datasets. Measure whether layer selection shifts significantly and whether previously optimal α values remain effective, assessing the approach's robustness to distributional shifts.