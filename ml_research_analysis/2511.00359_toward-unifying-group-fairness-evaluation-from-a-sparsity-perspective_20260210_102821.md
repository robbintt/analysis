---
ver: rpa2
title: Toward Unifying Group Fairness Evaluation from a Sparsity Perspective
arxiv_id: '2511.00359'
source_url: https://arxiv.org/abs/2511.00359
tags:
- fairness
- index
- sparsity
- gini
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for evaluating group fairness
  using sparsity measures. The authors theoretically connect sparsity indices (particularly
  the PQ Index) to fairness metrics by showing that reduced sparsity corresponds to
  higher fairness.
---

# Toward Unifying Group Fairness Evaluation from a Sparsity Perspective

## Quick Facts
- arXiv ID: 2511.00359
- Source URL: https://arxiv.org/abs/2511.00359
- Authors: Zhecheng Sheng; Jiawei Zhang; Enmao Diao
- Reference count: 40
- One-line primary result: Sparsity-based fairness metrics (S-SP, S-EO) align with traditional MPD measures while offering broader applicability across classification and regression tasks, particularly in multi-group and intersectional fairness settings.

## Executive Summary
This paper proposes a unified framework for evaluating group fairness using sparsity measures, specifically connecting the PQ Index to fairness metrics. The authors demonstrate that reduced sparsity corresponds to higher fairness and that the PQ Index satisfies desirable properties for fairness measurement. Experiments across multiple datasets (UCI Adult, COMPAS, ACSIncome, Communities & Crimes, LawSchool) with various bias mitigation algorithms show that sparsity-based metrics align well with traditional MPD-based fairness measures while offering broader applicability across classification and regression tasks, including multi-class and multi-group scenarios.

## Method Summary
The framework reformulates traditional fairness criteria (Statistical Parity, Equalized Odds) by replacing Maximum Pairwise Difference (MPD) with sparsity measures over group-level statistics. The PQ Index, defined as I_{p,q}(w) = 1 - d^(1/q-1/p) × ||w||_p / ||w||_q with p=1 and q=2, serves as the primary sparsity measure. For classification, S-Statistical Parity computes the maximum sparsity across all classes, while S-Equalized Odds extends this to confusion matrix elements. The framework applies exponential transformation to ensure positivity for numerical stability. Experiments use logistic regression and linear regression baselines with bias mitigation algorithms including Reweight, FairRR, Reduction, Rejection, and others, evaluated on 80/20 splits across 10 random seeds.

## Key Results
- Sparsity-based metrics (S-SP, S-EO) align well with traditional MPD-based fairness measures across binary and multi-class classification tasks
- The framework proves particularly robust in intersectional fairness settings with many sensitive groups, providing more stable evaluations than MPD-based approaches when class distributions are severely imbalanced
- S-EO captures subtle group disparities overlooked by MPD and maintains consistent trade-off patterns across different performance metrics
- PQ Index satisfies all six ideal sparsity properties (Robin Hood, Scaling, Rising Tide, Cloning, Bill Gates, Babies) and provides scale-invariant, smooth optimization surfaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsity measures can quantify fairness by capturing distributional inequality across groups.
- Mechanism: The PQ Index measures how unequally mass is distributed across vector components. When applied to group-level outcomes, higher sparsity indicates greater disparity (lower fairness).
- Core assumption: Fairness violations manifest as inequality in the full distribution of group-wise outputs, not just worst-case pairwise differences.
- Evidence anchors:
  - [abstract] "connects various sparsity measures—including the PQ Index, Gini Index, and Maximum Pairwise Difference—with existing fairness criteria"
  - [section 3] "sparsity embodies the idea that a vector's magnitude is primarily determined by a few large components, reflecting inequality in the distribution"
- Break condition: If group outcomes are uniformly distributed but still unfair due to non-distributional factors, sparsity metrics will not capture the violation.

### Mechanism 2
- Claim: Replacing MPD with sparsity measures generalizes fairness criteria to multi-class/multi-group settings.
- Mechanism: Traditional fairness metrics use MPD: max_{i,j} |w_i - w_j|. The framework substitutes MPD with S([m_i]_{i=1}^{|A|}) where S is a sparsity measure, enabling consideration of all groups simultaneously rather than only extreme pairs.
- Core assumption: Fairness evaluation should incorporate information from intermediate groups, not just the worst-off pair.
- Evidence anchors:
  - [section 4] "In general, we replace the Maximum Pairwise Difference used in existing fairness metrics with a sparsity measure over w"
  - [section 5.3] "S-SP captures the addition of groups with class imbalances smaller than the maximum difference"
- Break condition: When intermediate group distributions are irrelevant to the fairness criterion (e.g., strict worst-case guarantees required), the sparsity approach may over-constrain.

### Mechanism 3
- Claim: The PQ Index provides theoretically grounded, scale-invariant fairness measurement with numerical stability advantages.
- Mechanism: The PQ Index satisfies all six ideal sparsity properties, is scale-invariant, and provides smooth gradients unlike the piecewise-linear Gini Index.
- Core assumption: Scale invariance and smooth optimization surfaces are desirable for fairness evaluation and mitigation.
- Evidence anchors:
  - [section 3.1] "PQ Index satisfies the six properties of an ideal sparsity measure"
  - [section 4.2, Figure 5] "exponential transformation to ensure positivity" addresses numerical instability
- Break condition: When input values contain zeros or negative numbers without transformation, PQ Index becomes unstable; MPD-based metrics remain robust.

## Foundational Learning

- Concept: **ℓ_p norms and norm ratios**
  - Why needed here: The PQ Index is defined as a ratio of ℓ_p norms. Understanding how different p values affect sensitivity to outliers is essential.
  - Quick check question: Why does ||w||_1 / ||w||_2 increase as the vector becomes sparser (more concentrated in fewer components)?

- Concept: **Group fairness criteria (Statistical Parity, Equalized Odds)**
  - Why needed here: The framework reformulates these standard criteria using sparsity. Understanding their original definitions clarifies what's being unified.
  - Quick check question: What's the key difference between Statistical Parity (independence of prediction from sensitive attribute) and Equalized Odds (independence conditional on true label)?

- Concept: **CDF-based distance metrics (Kolmogorov-Smirnov)**
  - Why needed here: Regression fairness uses KS distance over CDFs; the sparsity extension requires understanding distributional comparisons.
  - Quick check question: Why does KS distance take the supremum over all threshold values y ∈ Y?

## Architecture Onboarding

- Component map:
  - Compute per-group evaluation metrics → Apply positivity transformation if needed → Compute sparsity measure using PQ Index → Aggregate across classes if multi-class

- Critical path:
  1. Compute per-group evaluation metrics (e.g., E[f(X_a) = y] for each group a)
  2. Apply positivity transformation if needed (w = exp(w))
  3. Compute sparsity measure S([m_i]) using PQ Index (p=1, q=2)
  4. Aggregate across classes if multi-class (max operation default)

- Design tradeoffs:
  - **PQ Index vs. Gini Index**: PQ is smooth (differentiable); Gini is piecewise linear. PQ preferred for optimization, Gini for interpretability.
  - **MPD vs. Sparsity**: MPD is robust to zeros/negatives; sparsity captures full distribution. Use sparsity when intermediate groups matter.
  - **max vs. mean aggregation**: max is conservative (worst-class focus); mean dilutes severe class-specific violations.

- Failure signatures:
  - **Extreme values in confusion matrix**: S-EO produces erratic curves (Figure 5); apply exp transformation
  - **All-zero group outcomes**: PQ Index undefined; add small constant or use MPD fallback
  - **Many groups with empty classes**: SP produces extreme values; S-SP more stable (Section 5.3)

- First 3 experiments:
  1. Reproduce binary classification comparison (Adult dataset, |Y|=2, |A|=2) between S-SP and SP across 3 bias mitigation methods to validate alignment.
  2. Test intersectional fairness scenario: increase group count from 2 to 50 on Adult (gender × race × age) to verify S-SP stability vs. SP volatility.
  3. Ablate positivity transformation: compare S-EO with and without exp() on regression task (LawSchool) to quantify numerical instability impact.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical connection between sparsity and fairness relies heavily on PQ Index properties but doesn't extensively validate whether these properties remain meaningful when applied to fairness-specific group statistics
- The exponential transformation for numerical stability is mentioned but not empirically justified—sensitivity to transformation threshold is unknown
- The framework doesn't test scenarios with extremely imbalanced group sizes where some groups might have very few samples

## Confidence
- **High Confidence**: Empirical demonstration that S-SP and S-EO align with traditional MPD-based metrics in binary classification settings with clear baselines and multiple datasets
- **Medium Confidence**: Claim about superior performance in intersectional fairness with many groups; while Section 5.3 shows stability vs. SP volatility, lacks theoretical justification
- **Low Confidence**: Numerical stability claims for PQ Index vs Gini Index; corpus contains no direct evidence about PQ Index's smooth gradients or scale invariance benefits in practice

## Next Checks
1. **Theoretical Stress Test**: Apply the framework to a synthetic dataset where intermediate group distributions are known to be irrelevant (e.g., only extreme groups matter). Verify that sparsity metrics don't produce misleading fairness scores.
2. **Transformation Sensitivity**: Systematically vary the exponential transformation threshold on the LawSchool regression dataset and measure impact on S-EO scores and mitigation algorithm rankings.
3. **Extreme Group Imbalance**: Construct a dataset with 50+ groups where 48 groups have near-identical outcomes and 2 groups differ significantly. Compare SP, S-SP, and MPD-based metrics to identify which best captures the true fairness violation.