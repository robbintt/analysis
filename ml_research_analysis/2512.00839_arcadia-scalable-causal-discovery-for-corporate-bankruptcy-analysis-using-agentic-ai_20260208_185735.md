---
ver: rpa2
title: 'ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using
  Agentic AI'
arxiv_id: '2512.00839'
source_url: https://arxiv.org/abs/2512.00839
tags:
- causal
- arcadia
- discovery
- bankruptcy
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARCADIA is an agentic AI framework that uses iterative LLM reasoning
  and statistical validation to discover causal DAGs for corporate bankruptcy prediction.
  Unlike purely data-driven methods, ARCADIA proposes and refines theory-informed
  causal structures while enforcing temporal ordering, identifiability, and validity
  constraints.
---

# ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI

## Quick Facts
- arXiv ID: 2512.00839
- Source URL: https://arxiv.org/abs/2512.00839
- Reference count: 40
- Primary result: ARCADIA produces more reliable causal DAGs for corporate bankruptcy prediction than NOTEARS, GOLEM, and DirectLiNGAM, with minimal post-hoc pruning and high structural validity.

## Executive Summary
ARCADIA is an agentic AI framework that uses iterative LLM reasoning and statistical validation to discover causal DAGs for corporate bankruptcy prediction. Unlike purely data-driven methods, ARCADIA proposes and refines theory-informed causal structures while enforcing temporal ordering, identifiability, and validity constraints. Experiments on Italian corporate data show ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM, with stable, interpretable structures and minimal post-hoc pruning. ARCADIA consistently meets structural validity criteria, maintains moderate-sized graphs, and prioritizes causal understanding over raw predictive fit. The approach advances explainable AI by demonstrating how agentic LLMs can drive autonomous causal discovery in high-stakes financial domains.

## Method Summary
ARCADIA implements a four-node LangGraph workflow: Initialize (sample balanced data, set hyperparameters), Propose (LLM generates/refines DAG with temporal and assumption constraints), Evaluate (run statistical tests for identifiability, positivity, global validity), and Finish (persist best DAG). The framework iterates up to T_max times, using failure memos to guide LLM refinements. Hyperparameters include R² thresholds, VIF limits, and edge pruning rules. Baselines (NOTEARS, GOLEM, DirectLiNGAM) use gCastle defaults. Experiments use AIDA data (Italian firms, 2015-2019), stratified undersampling to 434 balanced observations, and 150 features including temporal deltas.

## Key Results
- ARCADIA achieves near-perfect structural validity (100% valid DAGs) versus 0-50% for baselines across multiple feature set sizes
- Zero temporal pruning required for ARCADIA versus 5-32% for baselines, indicating superior temporal ordering adherence
- Moderate-sized DAGs (mean 10 nodes, 20 edges) with R² ~0.10 prioritize interpretability over predictive fit
- Runtime 500-900 seconds balances robustness against computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based proposal injects domain priors into causal discovery, producing structurally coherent DAGs without post-hoc pruning.
- Mechanism: A language model receives a bootstrap prompt (first iteration) or refinement prompt (subsequent iterations) with variable descriptions, temporal structure, and an explicit causal-assumption checklist. It outputs a JSON DAG with `reasoning`, `assumptions`, and `edges`, encoding theory-informed constraints such as temporal ordering and unobserved-confounding proxies, which guide the search toward causally plausible structures.
- Core assumption: The LLM's parametric knowledge encodes sufficient financial-domain causal intuition to propose better initial structures than unconstrained statistical learners.
- Evidence anchors: [abstract] "ARCADIA employs an iterative process where an LLM agent proposes theory-informed causal structures, evaluates them against statistical criteria, and refines the model based on diagnostic feedback." [section] §3.2 describes the JSON schema and the hierarchical decision framework prioritizing identification over metric optimization.
- Break condition: If LLM priors are misaligned with the data-generating process (e.g., outdated or biased domain theories), proposals may systematically omit key confounders or include spurious edges.

### Mechanism 2
- Claim: Multi-layer statistical evaluation enforces causal guardrails before acceptance, producing "born valid" DAGs.
- Mechanism: The `Evaluate` node runs edge-level tests (residual correlation, FDR-adjusted p-values, ΔBIC for orientation), node-level regressions (R², VIF), and global metrics (significant-edge ratio, direction accuracy). It also verifies identifiability via Pearl's back-door criterion and positivity via propensity-score overlap. DAGs must pass all six criteria to be accepted.
- Core assumption: Observational data and the proposed DAG jointly satisfy the Causal Markov condition and faithfulness where applicable; identifiability can be established via the back-door criterion given measured adjustment sets.
- Evidence anchors: [abstract] "ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM, with stable, interpretable structures and minimal post-hoc pruning." [section] Table 3 and Algorithm 2 detail the six verification criteria and the hierarchical evaluation logic.
- Break condition: If latent confounding is severe and proxy variables poorly capture it, the back-door adjustment may remain biased despite passing identifiability checks.

### Mechanism 3
- Claim: Iterative refinement with constrained budgets and conversational memory yields stable mid-complexity DAGs.
- Mechanism: After each `Evaluate` step, a failure memo and proposal history are fed back to the LLM, which may add/swap/remove up to `k_refine` variables per round. The agent must articulate theory-driven rationale for each change. This loop runs up to `T_max` iterations or until all criteria pass.
- Core assumption: The search space contains at least one DAG that satisfies all validity constraints within the iteration budget.
- Evidence anchors: [abstract] "ARCADIA consistently meets structural validity criteria, maintains moderate-sized graphs, and prioritizes causal understanding over raw predictive fit." [section] §3.2 and Algorithm 1 describe the propose–evaluate loop, memory log `H`, and refinement budget.
- Break condition: If the refinement budget is too tight or the initial proposal space is far from valid structures, the loop may exhaust iterations without convergence.

## Foundational Learning

- Concept: **Directed Acyclic Graphs (DAGs) and Causal Identification**
  - Why needed here: ARCADIA outputs and validates DAGs; understanding d-separation, back-door criterion, and identifiability is essential to interpret results.
  - Quick check question: Given a DAG with X → Y ← Z and an arrow X → Z, is the causal effect of X on Y identifiable without adjustment?

- Concept: **LLM Prompt Engineering for Structured Output**
  - Why needed here: The agent must emit valid JSON with `reasoning`, `assumptions`, and `edges` each iteration; prompt design controls output fidelity.
  - Quick check question: How would you enforce that an LLM always returns a parseable JSON DAG in a single completion?

- Concept: **Statistical Diagnostics for Regression Models**
  - Why needed here: Evaluate step computes R², adjusted R², VIF, and FDR-corrected p-values; interpreting these guides refinement.
  - Quick check question: If a node's VIF exceeds the threshold, what are two theory-aligned ways the agent could respond?

## Architecture Onboarding

- Component map: Initialse -> Propose -> Evaluate -> Finish
- Critical path:
  1. Temporally balanced feature sampling (prevents spurious correlations across years)
  2. LLM proposal respecting temporal ordering and assumption checklist (reduces pruning)
  3. Evaluate → if pass, Finish; else, build failure memo → Propose (repeat up to `T_max`)
- Design tradeoffs:
  - Stability vs. fit: Mid-sized DAGs (~10 nodes, ~20 edges) with mean node R² ≈ 0.10 prioritize interpretability and identifiability over maximizing variance explained
  - Runtime vs. robustness: ARCADIA's iterative LLM calls increase wall-clock time (~500–900s) but reduce post-hoc pruning compared to baselines
  - Generality vs. domain specificity: Prompts encode finance-specific context (Italian firms, 2015–2019); adapting to new domains requires prompt and assumption revision
- Failure signatures:
  - Exhausted `T_max` without passing all six criteria → check if initial proposal space excludes viable DAGs
  - High temporal-pruning or disconnected-node counts → LLM ignoring ordering constraints; revisit prompt checklist
  - Null minimal adjustment set repeatedly → consider adding proxy confounders or alternative theoretical frameworks
- First 3 experiments:
  1. Replicate Table 4 at M ∈ {20, 50, 150} with fixed hyperparameters; confirm near-perfect validity and zero temporal pruning for ARCADIA
  2. Ablate the assumption checklist in the prompt; measure increase in post-hoc pruning and decrease in valid-DAG rate
  3. Vary `k_refine` (e.g., 2, 5, 10) and `T_max` (e.g., 5, 10, 20) to characterize convergence sensitivity and runtime tradeoffs

## Open Questions the Paper Calls Out

- **Question:** How does ARCADIA's causal validity vs. predictive accuracy trade-off manifest, and can both be simultaneously optimized?
  - Basis in paper: [explicit] "Future work could examine the trade-off between causal validity and predictive performance more systematically."
  - Why unresolved: The paper deliberately prioritizes causal validity (R² ~0.10) over prediction fit; no joint optimization was attempted.
  - What evidence would resolve it: Experiments comparing counterfactual accuracy and downstream bankruptcy prediction AUC across differently tuned ARCADIA configurations.

- **Question:** Do ARCADIA's discovered causal structures generalize across jurisdictions with differing bankruptcy laws and accounting standards?
  - Basis in paper: [explicit] "Cross-market validation is critical... The generalizability to other geographic regions, time periods, or financial distress definitions remains untested."
  - Why unresolved: Only Italian firms (2015–2019) were evaluated; no cross-national replication was conducted.
  - What evidence would resolve it: Replicating the framework on US, German, or Asian corporate datasets and comparing DAG overlap and identifiability rates.

- **Question:** Can adaptive or principled hyperparameter selection improve ARCADIA's convergence and graph quality?
  - Basis in paper: [explicit] "The framework would benefit from principled hyperparameter selection methods or adaptive threshold adjustment based on data properties."
  - Why unresolved: Fixed thresholds (Θglobal, ΘVIF, krefine) were used across all experiments without sensitivity analysis.
  - What evidence would resolve it: Systematic ablation over threshold ranges and comparison of discovered DAG stability and validity.

- **Question:** How robust is ARCADIA under realistic class imbalance compared to the balanced sampling used in experiments?
  - Basis in paper: [inferred] "The balanced sampling strategy...may not reflect the severe class imbalance encountered in practice, potentially affecting the causal structures discovered."
  - Why unresolved: The 50/50 balanced dataset artificially inflates minority-class representation.
  - What evidence would resolve it: Re-running ARCADIA on naturally imbalanced corporate populations and assessing DAG stability.

## Limitations

- Proprietary AIDA data prevents independent verification of preprocessing and results.
- GPT-5's behavior is unknown due to its non-public status, requiring surrogate models whose outputs may differ.
- The iterative refinement's success hinges on the LLM's ability to generate valid causal structures, but the reliability of LLM-based causal reasoning in finance remains under-validated.

## Confidence

- **High:** ARCADIA's design and workflow (agentic LLM-driven causal discovery with statistical validation) are well-specified and logically coherent.
- **Medium:** The reported improvements over baselines (validity, stability, interpretability) are plausible given the design but depend on data and model behavior that cannot be fully verified.
- **Low:** Claims about LLM reliability for causal reasoning in finance are exploratory; the evidence base is thin and the approach's generalizability is untested.

## Next Checks

1. Replicate Table 4 at M∈{20,50,150} with fixed hyperparameters; confirm near-perfect validity and zero temporal pruning for ARCADIA versus baselines.
2. Ablate the assumption checklist in the LLM prompt; measure the impact on post-hoc pruning and valid-DAG rate.
3. Vary `k_refine` and `T_max` to characterize convergence sensitivity and runtime tradeoffs.